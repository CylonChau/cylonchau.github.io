<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ceph重新平衡 - Rebalance | Cylon's Collection</title><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NP3JNCPR" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><meta name=keywords content="cephfs,fscache"><meta name=description content="Rebalance 当 Ceph 集群在扩容/缩容后，Ceph会更新 Cluster map, 在更新时会更新 Cluster map 也会更新 “对象的放置” CRUSH 会平均但随机的将对象放置在现有的 OSD 之上，在 Rebalancing 时，只有少量数据进行移动而不是全部数据进行移动，直到达到 OSD 与 对象 之间的平衡，这个过程就叫做 Ceph 的 Rebalance。
需要注意的是，当集群中的 OSD 数量越多，那么在做 Rebalance 时所移动的就越少。例如，在具有 50 个 OSD 的集群中，在添加 OSD 时可能会移动 1/50th 或 2% 的数据。
如下图所示，当前集群有两个 OSD，当在集群中添加一个 OSD，使其数量达到3时，这个时候会触发 Rebalance，所移动的数量为 OSD1 上的 PG3 与 OSD2 上的 PG 6和9
图：Ceph Rebalancing 示意图 Source：https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/4/html/architecture_guide/ceph-rebalancing-and-recovery_arch
Balancer 执行 Rebalance 的模块时 Balancer，其可以优化 OSD 上的放置组 (PG) ，以实现平衡分配。
可以通过命令查看 balancer 的状态
bash 1 ceph balancer status https://docs."><meta name=author content="cylon"><link rel=canonical href=https://www.oomkill.com/2023/09/6-1-ceph-rebalance/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://www.oomkill.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.oomkill.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.oomkill.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.oomkill.com/favicon.ico><link rel=mask-icon href=https://www.oomkill.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://www.oomkill.com/2023/09/6-1-ceph-rebalance/><noscript><style>#theme-toggle,#top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=/assets/css/pe.min.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/pe.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/v4-shims.min.css><script defer src=https://cdn.staticfile.net/jquery/3.5.1/jquery.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.css><script defer src=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.js></script><script id=MathJax-script async src=https://cdn.staticfile.net/mathjax/3.2.2/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["\\$","\\$"]]}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-H94HZ5S19Y"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H94HZ5S19Y")</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><meta property="og:title" content="Ceph重新平衡 - Rebalance"><meta property="og:description" content="Rebalance 当 Ceph 集群在扩容/缩容后，Ceph会更新 Cluster map, 在更新时会更新 Cluster map 也会更新 “对象的放置” CRUSH 会平均但随机的将对象放置在现有的 OSD 之上，在 Rebalancing 时，只有少量数据进行移动而不是全部数据进行移动，直到达到 OSD 与 对象 之间的平衡，这个过程就叫做 Ceph 的 Rebalance。
需要注意的是，当集群中的 OSD 数量越多，那么在做 Rebalance 时所移动的就越少。例如，在具有 50 个 OSD 的集群中，在添加 OSD 时可能会移动 1/50th 或 2% 的数据。
如下图所示，当前集群有两个 OSD，当在集群中添加一个 OSD，使其数量达到3时，这个时候会触发 Rebalance，所移动的数量为 OSD1 上的 PG3 与 OSD2 上的 PG 6和9
图：Ceph Rebalancing 示意图 Source：https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/4/html/architecture_guide/ceph-rebalancing-and-recovery_arch
Balancer 执行 Rebalance 的模块时 Balancer，其可以优化 OSD 上的放置组 (PG) ，以实现平衡分配。
可以通过命令查看 balancer 的状态
bash 1 ceph balancer status https://docs."><meta property="og:type" content="article"><meta property="og:url" content="https://www.oomkill.com/2023/09/6-1-ceph-rebalance/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-09-03T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-03T23:10:36+08:00"><meta property="og:site_name" content="Cylon's Collection"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ceph重新平衡 - Rebalance"><meta name=twitter:description content="Rebalance 当 Ceph 集群在扩容/缩容后，Ceph会更新 Cluster map, 在更新时会更新 Cluster map 也会更新 “对象的放置” CRUSH 会平均但随机的将对象放置在现有的 OSD 之上，在 Rebalancing 时，只有少量数据进行移动而不是全部数据进行移动，直到达到 OSD 与 对象 之间的平衡，这个过程就叫做 Ceph 的 Rebalance。
需要注意的是，当集群中的 OSD 数量越多，那么在做 Rebalance 时所移动的就越少。例如，在具有 50 个 OSD 的集群中，在添加 OSD 时可能会移动 1/50th 或 2% 的数据。
如下图所示，当前集群有两个 OSD，当在集群中添加一个 OSD，使其数量达到3时，这个时候会触发 Rebalance，所移动的数量为 OSD1 上的 PG3 与 OSD2 上的 PG 6和9
图：Ceph Rebalancing 示意图 Source：https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/4/html/architecture_guide/ceph-rebalancing-and-recovery_arch
Balancer 执行 Rebalance 的模块时 Balancer，其可以优化 OSD 上的放置组 (PG) ，以实现平衡分配。
可以通过命令查看 balancer 的状态
bash 1 ceph balancer status https://docs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.oomkill.com/posts/"},{"@type":"ListItem","position":2,"name":"Ceph重新平衡 - Rebalance","item":"https://www.oomkill.com/2023/09/6-1-ceph-rebalance/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ceph重新平衡 - Rebalance","name":"Ceph重新平衡 - Rebalance","description":"Rebalance 当 Ceph 集群在扩容/缩容后，Ceph会更新 Cluster map, 在更新时会更新 Cluster map 也会更新 “对象的放置” CRUSH 会平均但随机的将对象放置在现有的 OSD 之上，在 Rebalancing 时，只有少量数据进行移动而不是全部数据进行移动，直到达到 OSD 与 对象 之间的平衡，这个过程就叫做 Ceph 的 Rebalance。\n需要注意的是，当集群中的 OSD 数量越多，那么在做 Rebalance 时所移动的就越少。例如，在具有 50 个 OSD 的集群中，在添加 OSD 时可能会移动 1/50th 或 2% 的数据。\n如下图所示，当前集群有两个 OSD，当在集群中添加一个 OSD，使其数量达到3时，这个时候会触发 Rebalance，所移动的数量为 OSD1 上的 PG3 与 OSD2 上的 PG 6和9\n图：Ceph Rebalancing 示意图 Source：https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/4/html/architecture_guide/ceph-rebalancing-and-recovery_arch\nBalancer 执行 Rebalance 的模块时 Balancer，其可以优化 OSD 上的放置组 (PG) ，以实现平衡分配。\n可以通过命令查看 balancer 的状态\nbash 1 ceph balancer status https://docs.","keywords":["cephfs","fscache"],"articleBody":"Rebalance 当 Ceph 集群在扩容/缩容后，Ceph会更新 Cluster map, 在更新时会更新 Cluster map 也会更新 “对象的放置” CRUSH 会平均但随机的将对象放置在现有的 OSD 之上，在 Rebalancing 时，只有少量数据进行移动而不是全部数据进行移动，直到达到 OSD 与 对象 之间的平衡，这个过程就叫做 Ceph 的 Rebalance。\n需要注意的是，当集群中的 OSD 数量越多，那么在做 Rebalance 时所移动的就越少。例如，在具有 50 个 OSD 的集群中，在添加 OSD 时可能会移动 1/50th 或 2% 的数据。\n如下图所示，当前集群有两个 OSD，当在集群中添加一个 OSD，使其数量达到3时，这个时候会触发 Rebalance，所移动的数量为 OSD1 上的 PG3 与 OSD2 上的 PG 6和9\n图：Ceph Rebalancing 示意图 Source：https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/4/html/architecture_guide/ceph-rebalancing-and-recovery_arch\nBalancer 执行 Rebalance 的模块时 Balancer，其可以优化 OSD 上的放置组 (PG) ，以实现平衡分配。\n可以通过命令查看 balancer 的状态\nbash 1 ceph balancer status https://docs.ceph.com/en/latest/rados/operations/balancer/\nBackfill Ceph 回填 (Backfill) 指的是每当删除 OSD 时，Ceph 都会使用 “Backfill” 和 “recovery” 来重新 rebalance 存储集群。这样做是为了根据PG 策略保留数据的多个副本。这两个操作都会占用系统资源，因此当 Ceph 存储集群处于负载状态时，Ceph 的性能将会下降，因为 Ceph 将资源转移到 “回填” 和 “恢复” 过程。\n有时为了在删除 OSD 时保持 Ceph 存储可接受的性能，需要先降低 “Backfill” 和 “recovery” 操作的优先级。降低优先级的代价是，较长时间内的数据副本较少，这将会导致数据面临风险。\n回填和恢复的发生是发生在 OSD/节点 故障或新增时被触发，如果所有的回填同时发生，会对OSD带来很大的负载，这个现象叫做 ”雷群效应“ (“thundering herd” effect)\nConfigration backfill paramter 回填的参数通常位于 OSD 参数下，在 Ceph OSD 中关于 backfill [1] 的参数如下：\n参数 类型 默认值 说明 osd_max_backfills uint 1 允许回填到单个 OSD 或从单个 OSD 回填的最大数量。请注意，这对于读和写操作是分开应用的。 osd_backfill_scan_min int 64 每次回填扫描的最小对象数 osd_backfill_scan_max int 512 每次回填扫描的最大对象数 osd_backfill_retry_interval float 30.0 重试回填请求之前等待的秒数。 查看当前参数\n查看配置之前需要确定 OSD 所在的节点，例如 OSD.1 可以通过 ceph osd tree 获取所有 OSD 列表\nbash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 13.09845 root default -3 4.36615 host PMX1 0 nvme 0.72769 osd.0 up 1.00000 1.00000 1 nvme 0.72769 osd.1 up 1.00000 1.00000 2 nvme 0.72769 osd.2 up 1.00000 1.00000 3 nvme 0.72769 osd.3 up 1.00000 1.00000 4 nvme 0.72769 osd.4 up 1.00000 1.00000 5 nvme 0.72769 osd.5 up 1.00000 1.00000 -5 4.36615 host PMX2 6 nvme 0.72769 osd.6 up 1.00000 1.00000 7 nvme 0.72769 osd.7 up 1.00000 1.00000 8 nvme 0.72769 osd.8 up 1.00000 1.00000 9 nvme 0.72769 osd.9 up 1.00000 1.00000 10 nvme 0.72769 osd.10 up 1.00000 1.00000 11 nvme 0.72769 osd.11 up 1.00000 1.00000 -7 4.36615 host PMX3 12 nvme 0.72769 osd.12 up 1.00000 1.00000 13 nvme 0.72769 osd.13 up 1.00000 1.00000 14 nvme 0.72769 osd.14 up 1.00000 1.00000 15 nvme 0.72769 osd.15 up 1.00000 1.00000 16 nvme 0.72769 osd.16 up 1.00000 1.00000 17 nvme 0.72769 osd.17 up 1.00000 1.00000 在拿到 OSD 坐在节点可以通过下面命令查看对应的 OSD 配置\nbash 1 2 3 4 5 6 7 8 $ ceph daemon osd.1 config get osd_max_backfills { \"osd_max_backfills\": \"1\" } $ ceph daemon osd.1 config get osd_recovery_max_active { \"osd_recovery_max_active\": \"0\" } 接下来可以根据 OSD 类型(SSD, HDD, nvme) 的不同，来相应的调整，例如 NVMes 比 HDD 更好的性能，那么可以设置大的回填\nbash 1 2 ceph config show osd.0 osd_recovery_max_active ceph config set osd osd_max_backfills 16 Recovery 如果 Ceph OSD 守护进程崩溃并重新上线，通常这个OSD会与 PG 中包含更新版本对象的其他 Ceph OSD 守护进程不同步。发生这种情况时，Ceph OSD 守护进程会进入恢复模式 (Recovery)，并寻求获取最新的数据副本并使其映射恢复到最新状态。根据 Ceph OSD daemon 关闭的时间长短，OSD 的对象和 PG 可能会明显过时。此外，如果一个故障域（机架）发生故障，多个 Ceph OSD 守护进程可能会同时恢复在线状态。这会使恢复过程耗时且占用资源。\n为了维持操作性能，Ceph 在执行恢复时限制恢复请求数量、线程和对象块大小，这使得 Ceph 在降级状态下也能良好运行。\n恢复的参数通常位于 OSD 参数下，在 Ceph OSD 中关于 recovery [2] 的参数如下：\n参数 类型 默认值 说明 osd_recovery_delay_start float 0.0 peer互连完成后，Ceph 将延迟指定的秒数，然后再开始恢复 RADOS 对象。 osd_recovery_max_active uint 0 每个 OSD 一次的活动恢复请求数。更多请求将加速恢复，但请求会增加集群的负载 osd_recovery_max_active_hdd uint 3 如果主设备是旋转设备（HDD），则每个 OSD 一次的活动恢复请求数。 osd_recovery_max_active_ssd uint 10 如果主设备是非旋转设备（即 SSD），则每个 OSD 一次的活动恢复请求数。 osd_recovery_max_chunk size 8Mi 恢复操作可以携带的数据块的最大总大小，需要注意单位。 osd_recovery_max_single_start uint 1 当 OSD (daemon)恢复时，每个 OSD 新启动的恢复操作的最大数量。 osd_recovery_sleep float 0.0 在下一次“恢复”或“回填”操作之前休眠的时间（以秒为单位）。增加此值将减慢恢复操作，而客户端操作受影响较小。 osd_recovery_sleep_hdd float 0.1 HDD 下次恢复或回填操作之前的睡眠时间（以秒为单位）。 osd_recovery_sleep_ssd float 0.0 SSD 下一次恢复或回填操作之前的睡眠时间（以秒为单位）。 osd_recovery_sleep_hybrid float 0.025 当 OSD 数据位于 HDD 上并且 OSD 日志/WAL+DB 位于 SSD 上时，在下一次恢复或回填操作之前休眠的时间（以秒为单位）。 osd_recovery_priority uint 5 为恢复工作队列设置的默认优先级。与 Pool 无关 Ceph backfill 和 recovery 也可以在 Ceph dashboard 中进行配置\n异步恢复 在 Nautilus 版本之前 “恢复” 动作是同步的，同步最显著的一个特征就是 “同步时会阻止对 RADOS 对象的写入，直到恢复为止”。\n回填操作与恢复操作有些不同，回填会临时分配不同的活动集(Active set, PG的一个属性)，并回填活动集之外的 OSD 来允许继续写入\n而为了避免 “同步恢复” 的问题 Ceph 提供了一种可以异步恢复的配置，当异步恢复发生时，对活动集成员可继续写入，有关于更多的异步说明，可以参考 Ceph 文档 asynchronous recovery 部分\nReference [1] backfilling\n[2] recovery\n[3] ASYNCHRONOUS RECOVERY\n[4] Advantages and Disadvantages of SAN\n","wordCount":"558","inLanguage":"zh","datePublished":"2023-09-03T00:00:00Z","dateModified":"2023-09-03T23:10:36+08:00","author":{"@type":"Person","name":"cylon"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.oomkill.com/2023/09/6-1-ceph-rebalance/"},"publisher":{"@type":"Organization","name":"Cylon's Collection","logo":{"@type":"ImageObject","url":"https://www.oomkill.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.oomkill.com/><img src=https://www.oomkill.com/favicon.ico alt aria-label=logo height=20>Cylon's Collection</a><div class=logo-switches><button id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.oomkill.com/archives><span>归档</span></a></li><li><a href=https://www.oomkill.com/tags><span>标签</span></a></li><li><a href=https://www.oomkill.com/search><span>搜索</span></a></li><li><a href=https://www.oomkill.com/about accesskey=/><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Ceph重新平衡 - Rebalance</h1><div class=post-meta><span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2023-09-03</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg><span>558 字</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><span>3 分钟</span></span>
<span class=pe-post-meta-item>&nbsp;·&nbsp;<svg t="1714036239378" fill="currentcolor" class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6659" width="256" height="256"><path d="M690 78.2c-18.6-18.8-49-19-67.8-.4s-19 49-.4 67.8l255.4 258.6c67.8 68.6 67.8 178.8.0 247.4L653.4 878.2c-18.6 18.8-18.4 49.2.4 67.8s49.2 18.4 67.8-.4l224-226.4c104.8-106 104.8-276.4.0-382.4L690 78.2zM485.4 101.4c-24-24-56.6-37.4-90.6-37.4H96C43 64 0 107 0 160v299c0 34 13.4 66.6 37.4 90.6l336 336c50 50 131 50 181 0l267-267c50-50 50-131 0-181l-336-336zM96 160h299c8.4.0 16.6 3.4 22.6 9.4l336 336c12.4 12.4 12.4 32.8.0 45.2l-267 267c-12.4 12.4-32.8 12.4-45.2.0l-336-336c-6-6-9.4-14.2-9.4-22.6V160zm192 128a64 64 0 10-128 0 64 64 0 10128 0z" p-id="6660"/></svg></span><ul class=pe-post-meta-item><a href=https://www.oomkill.com/tags/storage/>#Storage</a></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary><span class=details>目录</span></summary><div class=inner><ul><li><a href=#rebalance aria-label=Rebalance>Rebalance</a><ul><li><a href=#balancer aria-label=Balancer>Balancer</a></ul><li><a href=#backfill aria-label=Backfill>Backfill</a><ul><li><a href=#configration-backfill-paramter aria-label="Configration backfill paramter">Configration backfill paramter</a></ul><li><a href=#recovery aria-label=Recovery>Recovery</a><ul><li><a href=#%e5%bc%82%e6%ad%a5%e6%81%a2%e5%a4%8d aria-label=异步恢复>异步恢复</a></ul><li><a href=#reference aria-label=Reference>Reference</a></li></div></details></div></aside><script src=/js/pe-toc.min.445eb1bfc5e85dd13b9519fcc2a806522e9629b6224a2974052789ba00ab78af.js integrity="sha256-RF6xv8XoXdE7lRn8wqgGUi6WKbYiSil0BSeJugCreK8="></script><div class=post-content><h2 id=rebalance>Rebalance<a hidden class=anchor aria-hidden=true href=#rebalance>#</a></h2><p>当 Ceph 集群在扩容/缩容后，Ceph会更新 Cluster map, 在更新时会更新 Cluster map 也会更新 “对象的放置” CRUSH 会平均但随机的将对象放置在现有的 OSD 之上，在 Rebalancing 时，只有少量数据进行移动而不是全部数据进行移动，直到达到 OSD 与 对象 之间的平衡，这个过程就叫做 Ceph 的 Rebalance。</p><p>需要注意的是，当集群中的 OSD 数量越多，那么在做 Rebalance 时所移动的就越少。例如，在具有 50 个 OSD 的集群中，在添加 OSD 时可能会移动 1/50th 或 2% 的数据。</p><p>如下图所示，当前集群有两个 OSD，当在集群中添加一个 OSD，使其数量达到3时，这个时候会触发 Rebalance，所移动的数量为 OSD1 上的 PG3 与 OSD2 上的 PG 6和9</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20230830233602421.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20230830233602421.png#center alt=image-20230830233602421 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><center>图：Ceph Rebalancing 示意图</center><center><em>Source：</em>https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/4/html/architecture_guide/ceph-rebalancing-and-recovery_arch</center><br><h3 id=balancer>Balancer<a hidden class=anchor aria-hidden=true href=#balancer>#</a></h3><p>执行 Rebalance 的模块时 Balancer，其可以优化 OSD 上的放置组 (PG) ，以实现平衡分配。</p><p>可以通过命令查看 balancer 的状态</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph balancer status</span></span></code></pre></td></tr></table></div></div></div></div><p><a href=https://docs.ceph.com/en/latest/rados/operations/balancer/ target=_blank rel="noopener nofollow noreferrer">https://docs.ceph.com/en/latest/rados/operations/balancer/</a></p><h2 id=backfill>Backfill<a hidden class=anchor aria-hidden=true href=#backfill>#</a></h2><p>Ceph 回填 (Backfill) 指的是每当删除 OSD 时，Ceph 都会使用 “Backfill” 和 “recovery” 来重新 rebalance 存储集群。这样做是为了根据PG 策略保留数据的多个副本。这两个操作都会占用系统资源，因此当 Ceph 存储集群处于负载状态时，Ceph 的性能将会下降，因为 Ceph 将资源转移到 “回填” 和 “恢复” 过程。</p><p>有时为了在删除 OSD 时保持 Ceph 存储可接受的性能，需要先降低 “Backfill” 和 “recovery” 操作的优先级。降低优先级的代价是，较长时间内的数据副本较少，这将会导致数据面临风险。</p><p>回填和恢复的发生是发生在 OSD/节点 故障或新增时被触发，如果所有的回填同时发生，会对OSD带来很大的负载，这个现象叫做 ”雷群效应“ (&ldquo;thundering herd&rdquo; effect)</p><h3 id=configration-backfill-paramter>Configration backfill paramter<a hidden class=anchor aria-hidden=true href=#configration-backfill-paramter>#</a></h3><p>回填的参数通常位于 OSD 参数下，在 Ceph OSD 中关于 backfill <sup><a href=#1>[1]</a></sup> 的参数如下：</p><table><thead><tr><th>参数</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>osd_max_backfills</td><td>uint</td><td>1</td><td>允许回填到单个 OSD 或从单个 OSD 回填的最大数量。请注意，这对于读和写操作是分开应用的。</td></tr><tr><td>osd_backfill_scan_min</td><td>int</td><td>64</td><td>每次回填扫描的最小对象数</td></tr><tr><td>osd_backfill_scan_max</td><td>int</td><td>512</td><td>每次回填扫描的最大对象数</td></tr><tr><td>osd_backfill_retry_interval</td><td>float</td><td>30.0</td><td>重试回填请求之前等待的秒数。</td></tr></tbody></table><p><strong>查看当前参数</strong></p><p>查看配置之前需要确定 OSD 所在的节点，例如 OSD.1 可以通过 <code>ceph osd tree</code> 获取所有 OSD 列表</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ceph osd tree
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>ID  CLASS  WEIGHT    TYPE NAME       STATUS  REWEIGHT  PRI-AFF
</span></span><span class=line><span class=cl>-1         13.09845  root default                             
</span></span><span class=line><span class=cl>-3          4.36615      host PMX1                            
</span></span><span class=line><span class=cl> <span class=m>0</span>   nvme   0.72769          osd.0       up   1.00000  1.00000
</span></span><span class=line><span class=cl> <span class=m>1</span>   nvme   0.72769          osd.1       up   1.00000  1.00000
</span></span><span class=line><span class=cl> <span class=m>2</span>   nvme   0.72769          osd.2       up   1.00000  1.00000
</span></span><span class=line><span class=cl> <span class=m>3</span>   nvme   0.72769          osd.3       up   1.00000  1.00000
</span></span><span class=line><span class=cl> <span class=m>4</span>   nvme   0.72769          osd.4       up   1.00000  1.00000
</span></span><span class=line><span class=cl> <span class=m>5</span>   nvme   0.72769          osd.5       up   1.00000  1.00000
</span></span><span class=line><span class=cl>-5          4.36615      host PMX2                            
</span></span><span class=line><span class=cl> <span class=m>6</span>   nvme   0.72769          osd.6       up   1.00000  1.00000
</span></span><span class=line><span class=cl> <span class=m>7</span>   nvme   0.72769          osd.7       up   1.00000  1.00000
</span></span><span class=line><span class=cl> <span class=m>8</span>   nvme   0.72769          osd.8       up   1.00000  1.00000
</span></span><span class=line><span class=cl> <span class=m>9</span>   nvme   0.72769          osd.9       up   1.00000  1.00000
</span></span><span class=line><span class=cl><span class=m>10</span>   nvme   0.72769          osd.10      up   1.00000  1.00000
</span></span><span class=line><span class=cl><span class=m>11</span>   nvme   0.72769          osd.11      up   1.00000  1.00000
</span></span><span class=line><span class=cl>-7          4.36615      host PMX3                            
</span></span><span class=line><span class=cl><span class=m>12</span>   nvme   0.72769          osd.12      up   1.00000  1.00000
</span></span><span class=line><span class=cl><span class=m>13</span>   nvme   0.72769          osd.13      up   1.00000  1.00000
</span></span><span class=line><span class=cl><span class=m>14</span>   nvme   0.72769          osd.14      up   1.00000  1.00000
</span></span><span class=line><span class=cl><span class=m>15</span>   nvme   0.72769          osd.15      up   1.00000  1.00000
</span></span><span class=line><span class=cl><span class=m>16</span>   nvme   0.72769          osd.16      up   1.00000  1.00000
</span></span><span class=line><span class=cl><span class=m>17</span>   nvme   0.72769          osd.17      up   1.00000  1.00000</span></span></code></pre></td></tr></table></div></div></div></div><p>在拿到 OSD 坐在节点可以通过下面命令查看对应的 OSD 配置</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ceph daemon osd.1 config get osd_max_backfills
</span></span><span class=line><span class=cl><span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;osd_max_backfills&#34;</span>: <span class=s2>&#34;1&#34;</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>$  ceph daemon osd.1 config get osd_recovery_max_active
</span></span><span class=line><span class=cl><span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;osd_recovery_max_active&#34;</span>: <span class=s2>&#34;0&#34;</span>
</span></span><span class=line><span class=cl><span class=o>}</span></span></span></code></pre></td></tr></table></div></div></div></div><p>接下来可以根据 OSD 类型(SSD, HDD, nvme) 的不同，来相应的调整，例如 NVMes 比 HDD 更好的性能，那么可以设置大的回填</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph config show osd.0 osd_recovery_max_active
</span></span><span class=line><span class=cl>ceph config <span class=nb>set</span> osd osd_max_backfills <span class=m>16</span></span></span></code></pre></td></tr></table></div></div></div></div><h2 id=recovery>Recovery<a hidden class=anchor aria-hidden=true href=#recovery>#</a></h2><p>如果 Ceph OSD 守护进程崩溃并重新上线，通常这个OSD会与 PG 中包含更新版本对象的其他 Ceph OSD 守护进程不同步。发生这种情况时，Ceph OSD 守护进程会进入恢复模式 (Recovery)，并寻求获取最新的数据副本并使其映射恢复到最新状态。根据 Ceph OSD daemon 关闭的时间长短，OSD 的对象和 PG 可能会明显过时。此外，如果一个故障域（机架）发生故障，多个 Ceph OSD 守护进程可能会同时恢复在线状态。这会使恢复过程耗时且占用资源。</p><p>为了维持操作性能，Ceph 在执行恢复时限制恢复请求数量、线程和对象块大小，这使得 Ceph 在降级状态下也能良好运行。</p><p>恢复的参数通常位于 OSD 参数下，在 Ceph OSD 中关于 recovery <sup><a href=#2>[2]</a></sup> 的参数如下：</p><table><thead><tr><th>参数</th><th>类型</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>osd_recovery_delay_start</td><td>float</td><td>0.0</td><td>peer互连完成后，Ceph 将延迟指定的秒数，然后再开始恢复 RADOS 对象。</td></tr><tr><td>osd_recovery_max_active</td><td>uint</td><td>0</td><td>每个 OSD 一次的活动恢复请求数。更多请求将加速恢复，但请求会增加集群的负载</td></tr><tr><td>osd_recovery_max_active_hdd</td><td>uint</td><td>3</td><td>如果主设备是旋转设备（HDD），则每个 OSD 一次的活动恢复请求数。</td></tr><tr><td>osd_recovery_max_active_ssd</td><td>uint</td><td>10</td><td>如果主设备是非旋转设备（即 SSD），则每个 OSD 一次的活动恢复请求数。</td></tr><tr><td>osd_recovery_max_chunk</td><td>size</td><td>8Mi</td><td>恢复操作可以携带的数据块的最大总大小，需要注意单位。</td></tr><tr><td>osd_recovery_max_single_start</td><td>uint</td><td>1</td><td>当 OSD (daemon)恢复时，每个 OSD 新启动的恢复操作的最大数量。</td></tr><tr><td>osd_recovery_sleep</td><td>float</td><td>0.0</td><td>在下一次“恢复”或“回填”操作之前休眠的时间（以秒为单位）。增加此值将减慢恢复操作，而客户端操作受影响较小。</td></tr><tr><td>osd_recovery_sleep_hdd</td><td>float</td><td>0.1</td><td>HDD 下次恢复或回填操作之前的睡眠时间（以秒为单位）。</td></tr><tr><td>osd_recovery_sleep_ssd</td><td>float</td><td>0.0</td><td>SSD 下一次恢复或回填操作之前的睡眠时间（以秒为单位）。</td></tr><tr><td>osd_recovery_sleep_hybrid</td><td>float</td><td>0.025</td><td>当 OSD 数据位于 HDD 上并且 OSD 日志/WAL+DB 位于 SSD 上时，在下一次恢复或回填操作之前休眠的时间（以秒为单位）。</td></tr><tr><td>osd_recovery_priority</td><td>uint</td><td>5</td><td>为恢复工作队列设置的默认优先级。与 Pool 无关</td></tr></tbody></table><blockquote><p>Ceph backfill 和 recovery 也可以在 Ceph dashboard 中进行配置</p></blockquote><h3 id=异步恢复>异步恢复<a hidden class=anchor aria-hidden=true href=#异步恢复>#</a></h3><p>在 Nautilus 版本之前 “恢复” 动作是同步的，同步最显著的一个特征就是 “同步时会阻止对 RADOS 对象的写入，直到恢复为止”。</p><p>回填操作与恢复操作有些不同，回填会临时分配不同的活动集(Active set, PG的一个属性)，并回填活动集之外的 OSD 来允许继续写入</p><p>而为了避免 “同步恢复” 的问题 Ceph 提供了一种可以异步恢复的配置，当异步恢复发生时，对活动集成员可继续写入，有关于更多的异步说明，可以参考 Ceph 文档 asynchronous recovery 部分</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><blockquote><p><sup id=1>[1]</sup> <a href=https://docs.ceph.com/en/reef/rados/configuration/osd-config-ref/#backfilling target=_blank rel="noopener nofollow noreferrer"><em><strong>backfilling</strong></em></a></p><p><sup id=2>[2]</sup> <a href=https://docs.ceph.com/en/latest/rados/configuration/osd-config-ref/#recovery target=_blank rel="noopener nofollow noreferrer"><em><strong>recovery</strong></em></a></p><p><sup id=3>[3]</sup> <a href=https://docs.ceph.com/en/reef/dev/osd_internals/async_recovery/#asynchronous-recovery target=_blank rel="noopener nofollow noreferrer"><em><strong>ASYNCHRONOUS RECOVERY</strong></em></a></p><p><sup id=4>[4]</sup> <a href=https://www.ecstuff4u.com/2021/04/advantages-and-disadvantages-of-san.html target=_blank rel="noopener nofollow noreferrer"><em><strong>Advantages and Disadvantages of SAN</strong></em></a></p></blockquote></div><div class=pe-copyright><hr><blockquote><p>本文为原创内容，版权归作者所有。如需转载，请在文章中声明本文标题及链接。</p><p>文章标题：Ceph重新平衡 - Rebalance</p><p>文章链接：<a href=https://www.oomkill.com/2023/09/6-1-ceph-rebalance/ target=_blank>https://www.oomkill.com/2023/09/6-1-ceph-rebalance/</a></p><p>许可协议：<a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></blockquote></div><div class=comments-separator></div><h3 class=relatedContentTitle>相关阅读</h3><ul class=relatedContent><li><a href=/2023/08/acquaintance-stroage/><span>存储概念 - 存储类型对比</span></a></li><li><a href=/2023/07/02-1-install-ceph-with-cephadm/><span>使用cephadm纯离线安装Ceph集群</span></a></li><li><a href=/2019/11/02-2-install-ceph-with-ceph-deploy/><span>Ceph集群安装 - ceph-deploy</span></a></li><li><a href=/2019/09/03-1-acquaintance-rdb/><span>Ceph RBD - 初识块存储RBD</span></a></li><li><a href=/2019/07/03-2-rbd-management/><span>Ceph RBD - 关于RBD的操作与管理</span></a></li></ul><div class=comments-separator></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.oomkill.com/tags/storage/>Storage</a></li></ul><nav class=paginav><a class=prev href=https://www.oomkill.com/2023/09/11-1-ceph-common-cmd/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></polyline></svg>&nbsp;</span>
<span>ceph常用命令</span>
</a><a class=next href=https://www.oomkill.com/2023/08/ch29-volumemanager/><span class=title></span>
<span>深入理解kubelet - VolumeManager源码解析&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span></a></nav></footer><div class=pe-comments-decoration><p class=pe-comments-title></p><p class=pe-comments-subtitle></p></div><div id=pe-comments></div><script src=/js/pe-go-comment.min.86a214102576ba5f9b7bdc29eed8d58dd56e34aef80b3c65c73ea9cc88443696.js integrity="sha256-hqIUECV2ul+be9wp7tjVjdVuNK74Czxlxz6pzIhENpY="></script><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="dark"?"dark":"light",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"cylonchau/blogs","data-repo-id":"R_kgDOIRlNSQ","data-category":"Announcements","data-category-id":"DIC_kwDOIRlNSc4CXy1U","data-mapping":"pathname","data-term":"posts/6-1-ceph-rebalance","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getStoredTheme(),"data-lang":"zh-TW","data-loading":"lazy",crossorigin:"anonymous",async:""},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#pe-comments").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.oomkill.com/>Cylon's Collection</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> on
<a href=https://pages.github.com/ rel=noopener target=_blank>GitHub Pages</a> & Theme
        <a href=https://github.com/tofuwine/PaperMod-PE rel=noopener target=_blank>PaperMod-PE</a></span></footer><div class=pe-right-sidebar><a href=javascript:void(0); id=theme-toggle-float class=pe-float-btn><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a><a href=#top class=pe-float-btn id=top-link><span id=pe-read-progress></span></a></div><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>