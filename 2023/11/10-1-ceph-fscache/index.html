<!doctype html><html lang=zh dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>当cephfs和fscache结合时在K8s环境下的全集群规模故障 | Cylon's Collection</title><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NP3JNCPR" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><meta name=keywords content="cephfs,fscache"><meta name=description content="本文记录了在 kubernetes 环境中，使用 cephfs 时当启用了 fscache 时，由于网络问题，或者 ceph 集群问题导致的整个 k8s 集群规模的挂载故障问题。
结合fscache的kubernetes中使用cephfs造成的集群规模故障 在了解了上面的基础知识后，就可以引入故障了，下面是故障产生环境的配置
故障发生环境 软件 版本 Centos 7.9 Ceph nautilus (14.20) Kernel 4.18.16 故障现象 在 k8s 集群中挂在 cephfs 的场景下，新启动的 Pod 报错无法启动，报错信息如下
bash 1 ContainerCannotRun: error while creating mount source path /var/lib/kubelet/pods/5446c441-9162-45e8-0e93-b59be74d13b/volumes/kubernetesio-cephfs/{dir name} mkcir /var/lib/kubelet/pods/5446c441-9162-45e8-de93-b59bte74d13b/volumes/kubernetes.io~cephfs/ip-ib file existe 主要表现的现象大概为如下三个特征
对于该节点故障之前运行的 Pod 是正常运行，但是无法写入和读取数据
无法写入数据 permission denied
无法读取数据
kublet 的日志报错截图如下
彻底解决方法 需要驱逐该节点上所有挂在 cephfs 的 Pod，之后新调度来的 Pod 就可以正常启动了
故障的分析 当网络出现问题时，如果使用了 cephfs 的 Pod 就会出现大量故障，具体故障表现方式有下面几种
新部署的 Pod 处于 Waiting 状态"><meta name=author content="cylon"><link rel=canonical href=http://localhost:1313/2023/11/10-1-ceph-fscache/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/favicon.ico><link rel=mask-icon href=http://localhost:1313/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=http://localhost:1313/2023/11/10-1-ceph-fscache/><noscript><style>#theme-toggle,#top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=/assets/css/pe.min.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/pe.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/v4-shims.min.css><script defer src=https://cdn.staticfile.net/jquery/3.5.1/jquery.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.css><script defer src=https://cdn.staticfile.net/fancybox/3.5.7/jquery.fancybox.min.js></script><script id=MathJax-script async src=https://cdn.staticfile.net/mathjax/3.2.2/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["\\$","\\$"]]}}</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-NP3JNCPR")</script><meta property="og:title" content="当cephfs和fscache结合时在K8s环境下的全集群规模故障"><meta property="og:description" content="本文记录了在 kubernetes 环境中，使用 cephfs 时当启用了 fscache 时，由于网络问题，或者 ceph 集群问题导致的整个 k8s 集群规模的挂载故障问题。
结合fscache的kubernetes中使用cephfs造成的集群规模故障 在了解了上面的基础知识后，就可以引入故障了，下面是故障产生环境的配置
故障发生环境 软件 版本 Centos 7.9 Ceph nautilus (14.20) Kernel 4.18.16 故障现象 在 k8s 集群中挂在 cephfs 的场景下，新启动的 Pod 报错无法启动，报错信息如下
bash 1 ContainerCannotRun: error while creating mount source path /var/lib/kubelet/pods/5446c441-9162-45e8-0e93-b59be74d13b/volumes/kubernetesio-cephfs/{dir name} mkcir /var/lib/kubelet/pods/5446c441-9162-45e8-de93-b59bte74d13b/volumes/kubernetes.io~cephfs/ip-ib file existe 主要表现的现象大概为如下三个特征
对于该节点故障之前运行的 Pod 是正常运行，但是无法写入和读取数据
无法写入数据 permission denied
无法读取数据
kublet 的日志报错截图如下
彻底解决方法 需要驱逐该节点上所有挂在 cephfs 的 Pod，之后新调度来的 Pod 就可以正常启动了
故障的分析 当网络出现问题时，如果使用了 cephfs 的 Pod 就会出现大量故障，具体故障表现方式有下面几种
新部署的 Pod 处于 Waiting 状态"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/2023/11/10-1-ceph-fscache/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-11-11T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-12T23:10:36+08:00"><meta property="og:site_name" content="Cylon's Collection"><meta name=twitter:card content="summary"><meta name=twitter:title content="当cephfs和fscache结合时在K8s环境下的全集群规模故障"><meta name=twitter:description content="本文记录了在 kubernetes 环境中，使用 cephfs 时当启用了 fscache 时，由于网络问题，或者 ceph 集群问题导致的整个 k8s 集群规模的挂载故障问题。
结合fscache的kubernetes中使用cephfs造成的集群规模故障 在了解了上面的基础知识后，就可以引入故障了，下面是故障产生环境的配置
故障发生环境 软件 版本 Centos 7.9 Ceph nautilus (14.20) Kernel 4.18.16 故障现象 在 k8s 集群中挂在 cephfs 的场景下，新启动的 Pod 报错无法启动，报错信息如下
bash 1 ContainerCannotRun: error while creating mount source path /var/lib/kubelet/pods/5446c441-9162-45e8-0e93-b59be74d13b/volumes/kubernetesio-cephfs/{dir name} mkcir /var/lib/kubelet/pods/5446c441-9162-45e8-de93-b59bte74d13b/volumes/kubernetes.io~cephfs/ip-ib file existe 主要表现的现象大概为如下三个特征
对于该节点故障之前运行的 Pod 是正常运行，但是无法写入和读取数据
无法写入数据 permission denied
无法读取数据
kublet 的日志报错截图如下
彻底解决方法 需要驱逐该节点上所有挂在 cephfs 的 Pod，之后新调度来的 Pod 就可以正常启动了
故障的分析 当网络出现问题时，如果使用了 cephfs 的 Pod 就会出现大量故障，具体故障表现方式有下面几种
新部署的 Pod 处于 Waiting 状态"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"当cephfs和fscache结合时在K8s环境下的全集群规模故障","item":"http://localhost:1313/2023/11/10-1-ceph-fscache/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"当cephfs和fscache结合时在K8s环境下的全集群规模故障","name":"当cephfs和fscache结合时在K8s环境下的全集群规模故障","description":"本文记录了在 kubernetes 环境中，使用 cephfs 时当启用了 fscache 时，由于网络问题，或者 ceph 集群问题导致的整个 k8s 集群规模的挂载故障问题。\n结合fscache的kubernetes中使用cephfs造成的集群规模故障 在了解了上面的基础知识后，就可以引入故障了，下面是故障产生环境的配置\n故障发生环境 软件 版本 Centos 7.9 Ceph nautilus (14.20) Kernel 4.18.16 故障现象 在 k8s 集群中挂在 cephfs 的场景下，新启动的 Pod 报错无法启动，报错信息如下\nbash 1 ContainerCannotRun: error while creating mount source path /var/lib/kubelet/pods/5446c441-9162-45e8-0e93-b59be74d13b/volumes/kubernetesio-cephfs/{dir name} mkcir /var/lib/kubelet/pods/5446c441-9162-45e8-de93-b59bte74d13b/volumes/kubernetes.io~cephfs/ip-ib file existe 主要表现的现象大概为如下三个特征\n对于该节点故障之前运行的 Pod 是正常运行，但是无法写入和读取数据\n无法写入数据 permission denied\n无法读取数据\nkublet 的日志报错截图如下\n彻底解决方法 需要驱逐该节点上所有挂在 cephfs 的 Pod，之后新调度来的 Pod 就可以正常启动了\n故障的分析 当网络出现问题时，如果使用了 cephfs 的 Pod 就会出现大量故障，具体故障表现方式有下面几种\n新部署的 Pod 处于 Waiting 状态","keywords":["cephfs","fscache"],"articleBody":"本文记录了在 kubernetes 环境中，使用 cephfs 时当启用了 fscache 时，由于网络问题，或者 ceph 集群问题导致的整个 k8s 集群规模的挂载故障问题。\n结合fscache的kubernetes中使用cephfs造成的集群规模故障 在了解了上面的基础知识后，就可以引入故障了，下面是故障产生环境的配置\n故障发生环境 软件 版本 Centos 7.9 Ceph nautilus (14.20) Kernel 4.18.16 故障现象 在 k8s 集群中挂在 cephfs 的场景下，新启动的 Pod 报错无法启动，报错信息如下\nbash 1 ContainerCannotRun: error while creating mount source path /var/lib/kubelet/pods/5446c441-9162-45e8-0e93-b59be74d13b/volumes/kubernetesio-cephfs/{dir name} mkcir /var/lib/kubelet/pods/5446c441-9162-45e8-de93-b59bte74d13b/volumes/kubernetes.io~cephfs/ip-ib file existe 主要表现的现象大概为如下三个特征\n对于该节点故障之前运行的 Pod 是正常运行，但是无法写入和读取数据\n无法写入数据 permission denied\n无法读取数据\nkublet 的日志报错截图如下\n彻底解决方法 需要驱逐该节点上所有挂在 cephfs 的 Pod，之后新调度来的 Pod 就可以正常启动了\n故障的分析 当网络出现问题时，如果使用了 cephfs 的 Pod 就会出现大量故障，具体故障表现方式有下面几种\n新部署的 Pod 处于 Waiting 状态\n新部署的 Pod 可以启动成功，但是无法读取 cephfs 的挂载目录，主要故障表现为下面几种形式：\nceph mount error 5 = input/output error [3] cephfs mount failure.permission denied 旧 Pod 无法被删除\n新部署的 Pod 无法启动\n注：上面故障引用都是在网络上找到相同报错的一些提示，并不完全切合本文中故障描述\n去对应节点查看节点内核日志会发现有下面几个特征\n图1：故障发生的节点报错 图2：故障发生的节点报错 图3：故障发生的节点报错 log [ 1815.029831] ceph: mds0 closed our session [ 1815.029833] ceph: mds0 reconnect start [ 1815.052219] ceph: mds0 reconnect denied [ 1815.052229] ceph: dropping dirty Fw state for ffff9d9085da1340 1099512175611 [ 1815.052231] ceph: dropping dirty+flushing Fw state for ffff9d9085da1340 1099512175611 [ 1815.273008] libceph: mds0 10.99.10.4:6801 socket closed (con state NEGOTIATING) [ 1816.033241] ceph: mds0 rejected session [ 1829.018643] ceph: mds0 hung [ 1880.088504] ceph: mds0 came back [ 1880.088662] ceph: mds0 caps renewed [ 1880.094018] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm [ 1881.100367] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm [ 2046.768969] conntrack: generic helper won't handle protocol 47. Please consider loading the specific helper module. [ 2061.731126] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm 故障分析 由上面的三张图我们可以得到几个关键点\nconnection reset session lost, hunting for new mon ceph: get_quota_realm() reconnection denied mds1 hung mds1 caps stale 这三张图上的日志是一个故障恢复的顺序，而问题节点（通常为整个集群 Node）内核日志都会在刷 ceph: get_quota_realm() 这种日志，首先我们需要确认第一个问题，ceph: get_quota_realm() 是什么原因导致，在互联网上找了一个 linux kernel 关于修复这个问题的提交记录，通过 commit，我们可以看到这个函数产生的原因\nget_quota_realm() enters infinite loop if quota inode has no caps. This can happen after client gets evicted. [4]\n这里可以看到，修复的内容是当客户端被驱逐时，这个函数会进入无限 loop ，当 inode 配额没有被授权的用户，常常发生在客户端被驱逐。\n通过这个 commit，我们可以确定了后面 4 - 6 问题的疑问，即客户端被 ceph mds 驱逐（加入了黑名单），在尝试重连时就会发生 reconnection denied 接着发生陈腐的被授权认证的用户 (caps stale)。接着由于本身没有真实的卸载，而是使用了一个共享的 cookie 这个时候就会发生节点新挂载的目录是没有权限写，或者是 input/output error 的错误，这些错误表象是根据不同情况下而定，比如说被拉黑和丢失的会话。\nkubelet的错误日志 此时当新的使用了 volumes 去挂载 cephfs时，由于旧的 Pod 产生的工作目录 (/var/lib/kubelet) 下的 Pod 挂载会因为 cephfs caps stale 而导致无法卸载，这是就会存在 “孤儿Pod”，“不能同步 Pod 的状态”，“不能创建新的Pod挂载，因为目录已存在”。\nkubelet 日志如下所示：\nbash 1 2 3 kubelet_volumes.go:66] pod \"5446c441-9162-45e8-e11f46893932\" found, but error stat /var/lib/kubelet/pods/5446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxx: permission denied occurred during checking mounted volumes from disk pod_workers.go:119] Error syncing pod \"5446c441-9162-45e8-e11f46893932\" (\"xxxxx-xxx-xxx-xxxx-xxx_xxxxx(5446c441-9162-45e8-e11f46893932)\", skipping: failed to \"StartContainer\" for \"xxxxx-xxx-xxx\" with RunContainerError: \"failed to start container \\\"719346531es654113s3216e1456313d51as132156\\\": Error response from daemon: error while createing mount source path '/var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx': mkdir /var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx: file exists\" 问题复现 节点：192.168.155.70\n操作步骤，手动删除掉这个节点的会话复现问题：\n操作前日志\nbash 1 2 3 4 5 Nov 09 15:16:01 node88.itnet.com kernel: libceph: mon0 192.168.20.299:6789 session established Nov 09 15:16:01 node88.itnet.com kernel: libceph: mon0 192.168.20.299:6789 socket closed (con state OPEN) Nov 09 15:16:01 node88.itnet.com kernel: libceph: mon0 192.168.20.299:6789 session lost, hunting for new mon Nov 09 15:16:01 node88.itnet.com kernel: libceph: mon1 192.168.20.134:6789 session established Nov 09 15:16:01 node88.itnet.com kernel: libceph: client176873 fsid bf9495f9-726d-42d3-ac43-53938496bb29 步骤一：查找客户端id\nbash 1 2 3 4 5 $ ceph tell mds.0 client ls|grep 155.70 2023-11-09 18:07:37.063 7f204dffb700 0 client.177035 ms_handle_reset on v2:192.168.20.299:6800/1124232159 2023-11-09 18:07:37.089 7f204effd700 0 client.177041 ms_handle_reset on v2:192.168.20.299:6800/1124232159 \"addr\": \"192.168.155.70:0\", \"inst\": \"client.176873 v1:192.168.155.70:0/144083785\", 步骤二：驱逐该客户端\nbash 1 2 3 $ ceph tell mds.0 client evict id=176873 2023-11-09 18:09:13.726 7fc3cffff700 0 client.177074 ms_handle_reset on v2:192.168.20.299:6800/1124232159 2023-11-09 18:09:14.790 7fc3d97fa700 0 client.177080 ms_handle_reset on v2:192.168.20.299:6800/1124232159 步骤三：检查客户端\n查看日志，与 Openstack 全机房故障出现时日志内容一致\nbash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Nov 09 18:09:14 node88.itnet.com kernel: libceph: mds0 192.168.20.299:6801 socket closed (con state OPEN) Nov 09 18:09:16 node88.itnet.com kernel: libceph: mds0 192.168.20.299:6801 connection reset Nov 09 18:09:16 node88.itnet.com kernel: libceph: reset on mds0 Nov 09 18:09:16 node88.itnet.com kernel: ceph: mds0 closed our session Nov 09 18:09:16 node88.itnet.com kernel: ceph: mds0 reconnect start Nov 09 18:09:16 node88.itnet.com kernel: ceph: mds0 reconnect denied Nov 09 18:09:20 node88.itnet.com kernel: libceph: mds0 192.168.20.299:6801 socket closed (con state NEGOTIATING) Nov 09 18:09:21 node88.itnet.com kernel: ceph: mds0 rejected session Nov 09 18:09:21 node88.itnet.com kernel: ceph: mds1 rejected session Nov 09 18:09:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:09:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:15:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:15:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:21:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:21:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:27:22 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:27:22 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:33:22 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:33:22 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:39:23 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:39:23 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:45:23 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:45:23 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:51:24 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:51:24 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:57:24 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:57:24 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm 问题建议与解决 取消fscache，但fscache的存在是为了实现大并发连接ceph的机制，取消比较困难 调整参数（还没有测试）：当网络异常时，保证不要拉黑客户端使其可以重连，这里理论上可以解决客户端缓存问题 首先上面我们阐述了问题出现背景以及原因，要想解决这些错误，要分为两个步骤：\n首先驱逐 Kubernetes Node 节点上所有挂载 cephfs 的 Pod，这步骤是为了优雅的结束 fscache 的 cookie cache 机制，使节点可以正常的提供服务 解决使用 fscache 因网络问题导致的会话丢失问题的重连现象 这里主要以步骤2来阐述，解决这个问题就是通过两个方式，一个是不使用 fscache，另一个则是不让 mds 拉黑客户端，关闭 fscache 的成本很难，至今没有尝试成功，这里通过配置 ceph 服务使得 ceph mds 不会拉黑因出现网络问题丢失连接的客户端。\nceph 中阐述了驱逐的概念 “当某个文件系统客户端不响应或者有其它异常行为时，有必要强制切断它到文件系统的访问，这个过程就叫做驱逐。” [5]\n问题的根本原因为：ceph mds 把客户端拉入了黑名单，缓存导致客户端无法卸载连接，但接入了 fscache 的概念导致旧 session 无法释放，新连接会被 reject。\n要想解决这个问题，ceph 提供了一个参数来解决这个问题，mds_session_blacklist_on_timeout\nIt is possible to respond to slow clients by simply dropping their MDS sessions, but permit them to re-open sessions and permit them to continue talking to OSDs. To enable this mode, set mds_session_blacklist_on_timeout to false on your MDS nodes. [6]\n最终在配置后，上述问题解决\n解决问题后测试故障是否存在 测试过程\nceph 参数的配置\n图4：故障发生的节点报错 操作驱逐 192.168.155.70 的客户端连接，用以模拟 ceph 运行的底层出现故障而非正常断开 session 的场景\n图5：驱逐客户端的操作 重新运行 Pod 检查 session 是缓存还是会重连\n图6：检查节点日志 附：ceph mds 管理客户端 查看一个客户端的连接\nbash 1 2 3 ceph daemon mds.xxxxxxxx session ls |grep -E 'inst|hostname|kernel_version'|grep xxxx \"inst\": \"client.105123 v1:192.168.0.0:0/11243531\", \"hostname\": \"xxxxxxxxxxxxxxxxxx\" 手动驱逐一个客户端\nbash 1 2 3 ceph tell mds.0 client evict id=105123 2023-11-12 13:25:23:381 7fa3a67fc700 0 client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231 2023-11-12 13:25:23:421 7fa3a67fc700 0 client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231 查看 ceph 的配置参数\nbash 1 2 3 4 5 6 ceph config dump WHO MASK LEVEL OPTION VALUE RO mon advanced auth_allow_insecure_global_id_reclaim false mon advanced mon_allow_pool_delete false mds advanced mds_session_blacklist_on_evict false mds advanced mds_session_blacklist_on_timeout false 当出现问题无法卸载时应如何解决？\n当我们遇到问题时，卸载目录会出现被占用情况，通过 mount 和 fuser 都无法卸载\nbash 1 2 3 4 5 6 7 umount -f /tmp/998 umount： /tmp/998: target is buy. (In some cases useful info about processes that use th device is found by losf(8) or fuser(1)) the device is found by losf(8) or fuser(1) fuser -v1 /root/test Cannot stat /root/test: Input/output error 这个时候由于 cephfs 挂载问题会导致整个文件系统不可用，例如 df -h, ls dir 等，此时可以使用 umount 的懒卸载模式 umount -l，这会告诉内核当不占用时被卸载，由于这个问题是出现问题，而不是长期占用，这里用懒卸载后会立即卸载，从而解决了 stuck 的问题。\n什么是fscache fscache 是网络文件系统的通用缓存，例如 NFS, CephFS都可以使用其进行缓存从而提高 IO\nFS-Cache是在访问之前，将整个打开的每个 netfs 文件完全加载到 Cache 中，之后的挂载是从该缓存而不是 netfs 的 inode 中提供\nfscache主要提供了下列功能：\n一次可以使用多个缓存 可以随时添加/删除缓存 Cookie 分为 “卷”, “数据文件”, “缓存” 缓存 cookie 代表整个缓存，通常不可见到“网络文件系统” 卷 cookie 来表示一组 文件 数据文件 cookie 用于缓存数据 下图是一个 NFS 使用 fscache 的示意图，CephFS 原理与其类似\n图7：FS-Cache 架构 Source：https://computingforgeeks.com/how-to-cache-nfs-share-data-with-fs-cache-on-linux/\nCephFS 也是可以被缓存的一种网络文件系统，可以通过其内核模块看到对应的依赖\nbash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 root@client:~# lsmod | grep ceph ceph 376832 1 libceph 315392 1 ceph fscache 65536 1 ceph libcrc32c 16384 3 xfs,raid456,libceph root@client:~# modinfo ceph filename: /lib/modules/4.15.0-112-generic/kernel/fs/ceph/ceph.ko license: GPL description: Ceph filesystem for Linux author: Patience Warnick author: Yehuda Sadeh author: Sage Weil alias: fs-ceph srcversion: B2806F4EAACAC1E19EE7AFA depends: libceph,fscache retpoline: Y intree: Y name: ceph vermagic: 4.15.0-112-generic SMP mod_unload signat: PKCS#7 signer: sig_key: sig_hashalgo: md4 在启用了fs-cache后，内核日志可以看到对应 cephfs 挂载时 ceph 被注册到 fscache中\nbash 1 2 3 4 5 6 [ 11457.592011] FS-Cache: Loaded [ 11457.617265] Key type ceph registered [ 11457.617686] libceph: loaded (mon/osd proto 15/24) [ 11457.640554] FS-Cache: Netfs 'ceph' registered for caching [ 11457.640558] ceph: loaded (mds proto 32) [ 11457.640978] libceph: parse_ips bad ip 'mon1.ichenfu.com:6789,mon2.ichenfu.com:6789,mon3.ichenfu.com:6789' 当 monitor / OSD 拒绝连接时，所有该节点后续创建的挂载均会使用缓存，除非 umount 所有挂载后重新挂载才可以重新与 ceph mon 建立连接\ncephfs 中的 fscache ceph 官方在 2023年11月5日的一篇博客 [1] 中介绍了，cephfs 与 fscache 结合的介绍。这个功能的加入最显著的成功就是 ceph node 流向 OSD 网络被大大减少，尤其是在读取多的情况下。\n这个机制可以在代码 commit 中看到其原理：“在第一次通过文件引用inode时创建缓存cookie。之后，直到我们处理掉inode，我们都不会摆脱cookie” [2]\nReference [1] First Impressions Through Fscache and Ceph\n[2] ceph: persistent caching with fscache\n[3] Cannot Mount CephFS No Timeout, mount error 5 = Input/output error\n[4] ceph: fix infinite loop in get_quota_realm()\n[5] Ceph 文件系统客户端的驱逐\n[6] advanced-configuring-blacklisting\n","wordCount":"1326","inLanguage":"zh","datePublished":"2023-11-11T00:00:00Z","dateModified":"2024-09-12T23:10:36+08:00","author":{"@type":"Person","name":"cylon"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/2023/11/10-1-ceph-fscache/"},"publisher":{"@type":"Organization","name":"Cylon's Collection","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/><img src=http://localhost:1313/favicon.ico alt aria-label=logo height=20>Cylon's Collection</a><div class=logo-switches><button id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives><span>归档</span></a></li><li><a href=http://localhost:1313/tags><span>标签</span></a></li><li><a href=http://localhost:1313/search><span>搜索</span></a></li><li><a href=http://localhost:1313/about accesskey=/><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">当cephfs和fscache结合时在K8s环境下的全集群规模故障</h1><div class=post-meta><span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2023-11-11</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg><span>1326 字</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><span>7 分钟</span></span>
<span class=pe-post-meta-item>&nbsp;·&nbsp;<svg t="1714036239378" fill="currentcolor" class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6659" width="256" height="256"><path d="M690 78.2c-18.6-18.8-49-19-67.8-.4s-19 49-.4 67.8l255.4 258.6c67.8 68.6 67.8 178.8.0 247.4L653.4 878.2c-18.6 18.8-18.4 49.2.4 67.8s49.2 18.4 67.8-.4l224-226.4c104.8-106 104.8-276.4.0-382.4L690 78.2zM485.4 101.4c-24-24-56.6-37.4-90.6-37.4H96C43 64 0 107 0 160v299c0 34 13.4 66.6 37.4 90.6l336 336c50 50 131 50 181 0l267-267c50-50 50-131 0-181l-336-336zM96 160h299c8.4.0 16.6 3.4 22.6 9.4l336 336c12.4 12.4 12.4 32.8.0 45.2l-267 267c-12.4 12.4-32.8 12.4-45.2.0l-336-336c-6-6-9.4-14.2-9.4-22.6V160zm192 128a64 64 0 10-128 0 64 64 0 10128 0z" p-id="6660"/></svg></span><ul class=pe-post-meta-item><a href=http://localhost:1313/tags/storage/>#Storage</a>
<a href=http://localhost:1313/tags/troubleshooting/>#Troubleshooting</a></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e7%bb%93%e5%90%88fscache%e7%9a%84kubernetes%e4%b8%ad%e4%bd%bf%e7%94%a8cephfs%e9%80%a0%e6%88%90%e7%9a%84%e9%9b%86%e7%be%a4%e8%a7%84%e6%a8%a1%e6%95%85%e9%9a%9c aria-label=结合fscache的kubernetes中使用cephfs造成的集群规模故障>结合fscache的kubernetes中使用cephfs造成的集群规模故障</a><ul><li><a href=#%e6%95%85%e9%9a%9c%e5%8f%91%e7%94%9f%e7%8e%af%e5%a2%83 aria-label=故障发生环境>故障发生环境</a><li><a href=#%e6%95%85%e9%9a%9c%e7%8e%b0%e8%b1%a1 aria-label=故障现象>故障现象</a><li><a href=#%e5%bd%bb%e5%ba%95%e8%a7%a3%e5%86%b3%e6%96%b9%e6%b3%95 aria-label=彻底解决方法>彻底解决方法</a><li><a href=#%e6%95%85%e9%9a%9c%e7%9a%84%e5%88%86%e6%9e%90 aria-label=故障的分析>故障的分析</a><li><a href=#%e6%95%85%e9%9a%9c%e5%88%86%e6%9e%90 aria-label=故障分析>故障分析</a><li><a href=#kubelet%e7%9a%84%e9%94%99%e8%af%af%e6%97%a5%e5%bf%97 aria-label=kubelet的错误日志>kubelet的错误日志</a></ul><li><a href=#%e9%97%ae%e9%a2%98%e5%a4%8d%e7%8e%b0 aria-label=问题复现>问题复现</a><li><a href=#%e9%97%ae%e9%a2%98%e5%bb%ba%e8%ae%ae%e4%b8%8e%e8%a7%a3%e5%86%b3 aria-label=问题建议与解决>问题建议与解决</a><ul><li><a href=#%e8%a7%a3%e5%86%b3%e9%97%ae%e9%a2%98%e5%90%8e%e6%b5%8b%e8%af%95%e6%95%85%e9%9a%9c%e6%98%af%e5%90%a6%e5%ad%98%e5%9c%a8 aria-label=解决问题后测试故障是否存在>解决问题后测试故障是否存在</a><li><a href=#%e9%99%84ceph-mds-%e7%ae%a1%e7%90%86%e5%ae%a2%e6%88%b7%e7%ab%af aria-label="附：ceph mds 管理客户端">附：ceph mds 管理客户端</a></ul><li><a href=#%e4%bb%80%e4%b9%88%e6%98%affscache aria-label=什么是fscache>什么是fscache</a><li><a href=#cephfs-%e4%b8%ad%e7%9a%84-fscache aria-label="cephfs 中的 fscache">cephfs 中的 fscache</a><li><a href=#reference aria-label=Reference>Reference</a></li></div></details></div></aside><script src=/js/pe-toc.min.445eb1bfc5e85dd13b9519fcc2a806522e9629b6224a2974052789ba00ab78af.js integrity="sha256-RF6xv8XoXdE7lRn8wqgGUi6WKbYiSil0BSeJugCreK8="></script><div class=post-content><p>本文记录了在 kubernetes 环境中，使用 cephfs 时当启用了 fscache 时，由于网络问题，或者 ceph 集群问题导致的整个 k8s 集群规模的挂载故障问题。</p><h2 id=结合fscache的kubernetes中使用cephfs造成的集群规模故障>结合fscache的kubernetes中使用cephfs造成的集群规模故障<a hidden class=anchor aria-hidden=true href=#结合fscache的kubernetes中使用cephfs造成的集群规模故障>#</a></h2><p>在了解了上面的基础知识后，就可以引入故障了，下面是故障产生环境的配置</p><h3 id=故障发生环境>故障发生环境<a hidden class=anchor aria-hidden=true href=#故障发生环境>#</a></h3><table><thead><tr><th>软件</th><th>版本</th></tr></thead><tbody><tr><td>Centos</td><td>7.9</td></tr><tr><td>Ceph</td><td>nautilus (14.20)</td></tr><tr><td>Kernel</td><td>4.18.16</td></tr></tbody></table><h3 id=故障现象>故障现象<a hidden class=anchor aria-hidden=true href=#故障现象>#</a></h3><p>在 k8s 集群中挂在 cephfs 的场景下，新启动的 Pod 报错无法启动，报错信息如下</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ContainerCannotRun: error <span class=k>while</span> creating mount <span class=nb>source</span> path /var/lib/kubelet/pods/5446c441-9162-45e8-0e93-b59be74d13b/volumes/kubernetesio-cephfs/<span class=o>{</span>dir name<span class=o>}</span> mkcir /var/lib/kubelet/pods/5446c441-9162-45e8-de93-b59bte74d13b/volumes/kubernetes.io~cephfs/ip-ib file existe</span></span></code></pre></td></tr></table></div></div></div></div><p>主要表现的现象大概为如下三个特征</p><p>对于该节点故障之前运行的 Pod 是正常运行，但是无法写入和读取数据</p><p>无法写入数据 permission denied</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914165711395.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914165711395.png#center alt=image-202410914165711395 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>无法读取数据</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914162323295.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914162323295.png#center alt=image-202410914162323295 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><p>kublet 的日志报错截图如下</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914112451395.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914112451395.png#center alt=image-202410914112451395 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><h3 id=彻底解决方法>彻底解决方法<a hidden class=anchor aria-hidden=true href=#彻底解决方法>#</a></h3><p>需要驱逐该节点上所有挂在 cephfs 的 Pod，之后新调度来的 Pod 就可以正常启动了</p><h3 id=故障的分析>故障的分析<a hidden class=anchor aria-hidden=true href=#故障的分析>#</a></h3><p>当网络出现问题时，如果使用了 cephfs 的 Pod 就会出现大量故障，具体故障表现方式有下面几种</p><ul><li><p>新部署的 Pod 处于 Waiting 状态</p></li><li><p>新部署的 Pod 可以启动成功，但是无法读取 cephfs 的挂载目录，主要故障表现为下面几种形式：</p><ul><li>ceph mount error 5 = input/output error <sup><a href=#3>[3]</a></sup></li><li>cephfs mount failure.permission denied</li></ul></li><li><p>旧 Pod 无法被删除</p></li><li><p>新部署的 Pod 无法启动</p></li></ul><blockquote><p>注：上面故障引用都是在网络上找到相同报错的一些提示，并不完全切合本文中故障描述</p></blockquote><p>去对应节点查看节点内核日志会发现有下面几个特征</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20241091412343242.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20241091412343242.png#center alt=image-20241091412343242 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><center>图1：故障发生的节点报错</center><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914112309809.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914112309809.png#center alt=image-202410914112309809 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><center>图2：故障发生的节点报错</center><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914312554309.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-202410914312554309.png#center alt=image-202410914312554309 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><center>图3：故障发生的节点报错</center><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>log</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><pre tabindex=0><code class=language-log data-lang=log>[ 1815.029831] ceph: mds0 closed our session
[ 1815.029833] ceph: mds0 reconnect start
[ 1815.052219] ceph: mds0 reconnect denied
[ 1815.052229] ceph:  dropping dirty Fw state for ffff9d9085da1340 1099512175611
[ 1815.052231] ceph:  dropping dirty+flushing Fw state for ffff9d9085da1340 1099512175611
[ 1815.273008] libceph: mds0 10.99.10.4:6801 socket closed (con state NEGOTIATING)
[ 1816.033241] ceph: mds0 rejected session
[ 1829.018643] ceph: mds0 hung
[ 1880.088504] ceph: mds0 came back
[ 1880.088662] ceph: mds0 caps renewed
[ 1880.094018] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm
[ 1881.100367] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm
[ 2046.768969] conntrack: generic helper won&#39;t handle protocol 47. Please consider loading the specific helper module.
[ 2061.731126] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm</code></pre></div></div><h3 id=故障分析>故障分析<a hidden class=anchor aria-hidden=true href=#故障分析>#</a></h3><p>由上面的三张图我们可以得到几个关键点</p><ol><li>connection reset</li><li>session lost, hunting for new mon</li><li>ceph: get_quota_realm()</li><li>reconnection denied</li><li>mds1 hung</li><li>mds1 caps stale</li></ol><p>这三张图上的日志是一个故障恢复的顺序，而问题节点（通常为整个集群 Node）内核日志都会在刷 <code>ceph: get_quota_realm()</code> 这种日志，首先我们需要确认第一个问题，<code>ceph: get_quota_realm()</code> 是什么原因导致，在互联网上找了一个 linux kernel 关于修复这个问题的提交记录，通过 commit，我们可以看到这个函数产生的原因</p><blockquote><p>get_quota_realm() enters infinite loop if quota inode has no caps.
This can happen after client gets evicted. <sup><a href=#4>[4]</a></sup></p></blockquote><p>这里可以看到，修复的内容是当客户端被驱逐时，这个函数会进入无限 loop ，当 inode 配额没有被授权的用户，常常发生在客户端被驱逐。</p><p>通过这个 commit，我们可以确定了后面 4 - 6 问题的疑问，即客户端被 ceph mds 驱逐（加入了黑名单），在尝试重连时就会发生 <code>reconnection denied</code> 接着发生陈腐的被授权认证的用户 (caps stale)。<font color=#f8070d size=3>接着由于本身没有真实的卸载，而是使用了一个共享的 cookie 这个时候就会发生节点新挂载的目录是没有权限写，或者是 input/output error 的错误</font>，这些错误表象是根据不同情况下而定，比如说被拉黑和丢失的会话。</p><h3 id=kubelet的错误日志>kubelet的错误日志<a hidden class=anchor aria-hidden=true href=#kubelet的错误日志>#</a></h3><p>此时当新的使用了 volumes 去挂载 cephfs时，由于旧的 Pod 产生的工作目录 (/var/lib/kubelet) 下的 Pod 挂载会因为 cephfs caps stale 而导致无法卸载，这是就会存在 “孤儿Pod”，“不能同步 Pod 的状态”，“不能创建新的Pod挂载，因为目录已存在”。</p><p>kubelet 日志如下所示：</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kubelet_volumes.go:66<span class=o>]</span> pod <span class=s2>&#34;5446c441-9162-45e8-e11f46893932&#34;</span> found, but error stat /var/lib/kubelet/pods/5446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxx: permission denied occurred during checking mounted volumes from disk
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>pod_workers.go:119<span class=o>]</span> Error syncing pod <span class=s2>&#34;5446c441-9162-45e8-e11f46893932&#34;</span> <span class=o>(</span><span class=s2>&#34;xxxxx-xxx-xxx-xxxx-xxx_xxxxx(5446c441-9162-45e8-e11f46893932)&#34;</span>, skipping: failed to <span class=s2>&#34;StartContainer&#34;</span> <span class=k>for</span> <span class=s2>&#34;xxxxx-xxx-xxx&#34;</span> with RunContainerError: <span class=s2>&#34;failed to start container \&#34;719346531es654113s3216e1456313d51as132156\&#34;: Error response from daemon: error while createing mount source path &#39;/var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx&#39;: mkdir /var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx: file exists&#34;</span></span></span></code></pre></td></tr></table></div></div></div></div><h2 id=问题复现>问题复现<a hidden class=anchor aria-hidden=true href=#问题复现>#</a></h2><p>节点：192.168.155.70</p><p>操作步骤，手动删除掉这个节点的会话复现问题：</p><p>操作前日志</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Nov <span class=m>09</span> 15:16:01 node88.itnet.com kernel: libceph: mon0 192.168.20.299:6789 session established
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 15:16:01 node88.itnet.com kernel: libceph: mon0 192.168.20.299:6789 socket closed <span class=o>(</span>con state OPEN<span class=o>)</span>
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 15:16:01 node88.itnet.com kernel: libceph: mon0 192.168.20.299:6789 session lost, hunting <span class=k>for</span> new mon
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 15:16:01 node88.itnet.com kernel: libceph: mon1 192.168.20.134:6789 session established
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 15:16:01 node88.itnet.com kernel: libceph: client176873 fsid bf9495f9-726d-42d3-ac43-53938496bb29</span></span></code></pre></td></tr></table></div></div></div></div><p>步骤一：查找客户端id</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ceph tell mds.0 client ls<span class=p>|</span>grep 155.70
</span></span><span class=line><span class=cl>2023-11-09 18:07:37.063 7f204dffb700  <span class=m>0</span> client.177035 ms_handle_reset on v2:192.168.20.299:6800/1124232159
</span></span><span class=line><span class=cl>2023-11-09 18:07:37.089 7f204effd700  <span class=m>0</span> client.177041 ms_handle_reset on v2:192.168.20.299:6800/1124232159
</span></span><span class=line><span class=cl>                <span class=s2>&#34;addr&#34;</span>: <span class=s2>&#34;192.168.155.70:0&#34;</span>,
</span></span><span class=line><span class=cl>        <span class=s2>&#34;inst&#34;</span>: <span class=s2>&#34;client.176873 v1:192.168.155.70:0/144083785&#34;</span>,</span></span></code></pre></td></tr></table></div></div></div></div><p>步骤二：驱逐该客户端</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ceph tell mds.0 client evict <span class=nv>id</span><span class=o>=</span><span class=m>176873</span>
</span></span><span class=line><span class=cl>2023-11-09 18:09:13.726 7fc3cffff700  <span class=m>0</span> client.177074 ms_handle_reset on v2:192.168.20.299:6800/1124232159
</span></span><span class=line><span class=cl>2023-11-09 18:09:14.790 7fc3d97fa700  <span class=m>0</span> client.177080 ms_handle_reset on v2:192.168.20.299:6800/1124232159</span></span></code></pre></td></tr></table></div></div></div></div><p>步骤三：检查客户端</p><p>查看日志，与 Openstack 全机房故障出现时日志内容一致</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:14 node88.itnet.com kernel: libceph: mds0 192.168.20.299:6801 socket closed <span class=o>(</span>con state OPEN<span class=o>)</span>
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:16 node88.itnet.com kernel: libceph: mds0 192.168.20.299:6801 connection reset
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:16 node88.itnet.com kernel: libceph: reset on mds0
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:16 node88.itnet.com kernel: ceph: mds0 closed our session
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:16 node88.itnet.com kernel: ceph: mds0 reconnect start
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:16 node88.itnet.com kernel: ceph: mds0 reconnect denied
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:20 node88.itnet.com kernel: libceph: mds0 192.168.20.299:6801 socket closed <span class=o>(</span>con state NEGOTIATING<span class=o>)</span>
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:21 node88.itnet.com kernel: ceph: mds0 rejected session
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:21 node88.itnet.com kernel: ceph: mds1 rejected session
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:21 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:09:21 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:15:21 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:15:21 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:21:21 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:21:21 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:27:22 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:27:22 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:33:22 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:33:22 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:39:23 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:39:23 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:45:23 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:45:23 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:51:24 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:51:24 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:57:24 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm
</span></span><span class=line><span class=cl>Nov <span class=m>09</span> 18:57:24 node88.itnet.com kernel: ceph: get_quota_realm: ino <span class=o>(</span>10000000006.fffffffffffffffe<span class=o>)</span> null i_snap_realm</span></span></code></pre></td></tr></table></div></div></div></div><h2 id=问题建议与解决>问题建议与解决<a hidden class=anchor aria-hidden=true href=#问题建议与解决>#</a></h2><ul><li>取消fscache，但fscache的存在是为了实现大并发连接ceph的机制，取消比较困难</li><li>调整参数（还没有测试）：当网络异常时，保证不要拉黑客户端使其可以重连，这里理论上可以解决客户端缓存问题</li></ul><p>首先上面我们阐述了问题出现背景以及原因，要想解决这些错误，要分为两个步骤：</p><ol><li>首先驱逐 Kubernetes Node 节点上所有挂载 cephfs 的 Pod，这步骤是为了优雅的结束 fscache 的 cookie cache 机制，使节点可以正常的提供服务</li><li>解决使用 fscache 因网络问题导致的会话丢失问题的重连现象</li></ol><p>这里主要以步骤2来阐述，解决这个问题就是通过两个方式，一个是不使用 fscache，另一个则是不让 mds 拉黑客户端，关闭 fscache 的成本很难，至今没有尝试成功，这里通过配置 ceph 服务使得 ceph mds 不会拉黑因出现网络问题丢失连接的客户端。</p><p>ceph 中阐述了驱逐的概念 “当某个文件系统客户端不响应或者有其它异常行为时，有必要强制切断它到文件系统的访问，这个过程就叫做<em>驱逐</em>。” <sup><a href=#5>[5]</a></sup></p><p>问题的根本原因为：ceph mds 把客户端拉入了黑名单，缓存导致客户端无法卸载连接，但接入了 fscache 的概念导致旧 session 无法释放，新连接会被 reject。</p><p>要想解决这个问题，ceph 提供了一个参数来解决这个问题，<em><strong>mds_session_blacklist_on_timeout</strong></em></p><blockquote><p>It is possible to respond to slow clients by simply dropping their MDS sessions, but permit them to re-open sessions and permit them to continue talking to OSDs. To enable this mode, set <code>mds_session_blacklist_on_timeout</code> to false on your MDS nodes. <sup><a href=#6>[6]</a></sup></p></blockquote><p>最终在配置后，上述问题解决</p><h3 id=解决问题后测试故障是否存在>解决问题后测试故障是否存在<a hidden class=anchor aria-hidden=true href=#解决问题后测试故障是否存在>#</a></h3><p>测试过程</p><p>ceph 参数的配置</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20241091412343123.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20241091412343123.png#center alt=image-20241091412343123 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><center>图4：故障发生的节点报错</center><p>操作驱逐 192.168.155.70 的客户端连接，用以模拟 ceph 运行的底层出现故障而非正常断开 session 的场景</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20241130011556065.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-20241130011556065.png#center alt=image-20241130011556065 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><center>图5：驱逐客户端的操作</center><p>重新运行 Pod 检查 session 是缓存还是会重连</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-2024109141123756109.png><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/image-2024109141123756109.png#center alt=image-2024109141123756109 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><center>图6：检查节点日志</center><h3 id=附ceph-mds-管理客户端>附：ceph mds 管理客户端<a hidden class=anchor aria-hidden=true href=#附ceph-mds-管理客户端>#</a></h3><p>查看一个客户端的连接</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph daemon mds.xxxxxxxx session ls <span class=p>|</span>grep -E <span class=s1>&#39;inst|hostname|kernel_version&#39;</span><span class=p>|</span>grep xxxx
</span></span><span class=line><span class=cl>        <span class=s2>&#34;inst&#34;</span>: <span class=s2>&#34;client.105123 v1:192.168.0.0:0/11243531&#34;</span>,
</span></span><span class=line><span class=cl>            <span class=s2>&#34;hostname&#34;</span>: <span class=s2>&#34;xxxxxxxxxxxxxxxxxx&#34;</span></span></span></code></pre></td></tr></table></div></div></div></div><p>手动驱逐一个客户端</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph tell mds.0 client evict <span class=nv>id</span><span class=o>=</span><span class=m>105123</span>
</span></span><span class=line><span class=cl>2023-11-12 13:25:23:381 7fa3a67fc700 <span class=m>0</span> client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231
</span></span><span class=line><span class=cl>2023-11-12 13:25:23:421 7fa3a67fc700 <span class=m>0</span> client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231</span></span></code></pre></td></tr></table></div></div></div></div><p>查看 ceph 的配置参数</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph config dump
</span></span><span class=line><span class=cl>WHO     MASK  LEVEL     OPTION                                VALUE RO
</span></span><span class=line><span class=cl>  mon         advanced  auth_allow_insecure_global_id_reclaim <span class=nb>false</span>
</span></span><span class=line><span class=cl>  mon         advanced  mon_allow_pool_delete                 <span class=nb>false</span>
</span></span><span class=line><span class=cl>  mds         advanced  mds_session_blacklist_on_evict        <span class=nb>false</span>
</span></span><span class=line><span class=cl>  mds         advanced  mds_session_blacklist_on_timeout      false</span></span></code></pre></td></tr></table></div></div></div></div><p>当出现问题无法卸载时应如何解决？</p><p>当我们遇到问题时，卸载目录会出现被占用情况，通过 mount 和 fuser 都无法卸载</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>umount -f /tmp/998
</span></span><span class=line><span class=cl>umount： /tmp/998: target is buy.
</span></span><span class=line><span class=cl>        <span class=o>(</span>In some cases useful info about processes that use th device is found by losf<span class=o>(</span>8<span class=o>)</span> or fuser<span class=o>(</span>1<span class=o>))</span>
</span></span><span class=line><span class=cl>        the device is found by losf<span class=o>(</span>8<span class=o>)</span> or fuser<span class=o>(</span>1<span class=o>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>fuser -v1 /root/test
</span></span><span class=line><span class=cl>Cannot stat /root/test: Input/output error</span></span></code></pre></td></tr></table></div></div></div></div><p>这个时候由于 cephfs 挂载问题会导致整个文件系统不可用，例如 df -h, ls dir 等，此时可以使用 umount 的懒卸载模式 <code>umount -l</code>，这会告诉内核当不占用时被卸载，由于这个问题是出现问题，而不是长期占用，这里用懒卸载后会立即卸载，从而解决了 stuck 的问题。</p><h2 id=什么是fscache>什么是fscache<a hidden class=anchor aria-hidden=true href=#什么是fscache>#</a></h2><p>fscache 是网络文件系统的通用缓存，例如 NFS, CephFS都可以使用其进行缓存从而提高 IO</p><p>FS-Cache是在访问之前，将整个打开的每个 netfs 文件完全加载到 Cache 中，之后的挂载是从该缓存而不是 netfs 的 inode 中提供</p><p>fscache主要提供了下列功能：</p><ul><li>一次可以使用多个缓存</li><li>可以随时添加/删除缓存</li><li>Cookie 分为 “卷”, “数据文件”, “缓存”<ul><li>缓存 cookie 代表整个缓存，通常不可见到“网络文件系统”</li><li>卷 cookie 来表示一组 文件</li><li>数据文件 cookie 用于缓存数据</li></ul></li></ul><p>下图是一个 NFS 使用 fscache 的示意图，CephFS 原理与其类似</p><p><div class=pe-fancybox><a data-fancybox=gallery href=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/Cache-NFS-Share-Data-with-FS-Cache-1.webp><img src=https://cdn.jsdelivr.net/gh/cylonchau/blogs@img/img/Cache-NFS-Share-Data-with-FS-Cache-1.webp#center alt=Cache-NFS-Share-Data-with-FS-Cache-1 onerror='this.onerror=null,this.src="/placeholder.svg",this.className="pe-image-placeholder"'></a></div></p><center>图7：FS-Cache 架构</center><center><em>Source：</em>https://computingforgeeks.com/how-to-cache-nfs-share-data-with-fs-cache-on-linux/</center><br><p>CephFS 也是可以被缓存的一种网络文件系统，可以通过其内核模块看到对应的依赖</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@client:~# lsmod <span class=p>|</span> grep ceph
</span></span><span class=line><span class=cl>ceph                  <span class=m>376832</span>  <span class=m>1</span>
</span></span><span class=line><span class=cl>libceph               <span class=m>315392</span>  <span class=m>1</span> ceph
</span></span><span class=line><span class=cl>fscache                <span class=m>65536</span>  <span class=m>1</span> ceph
</span></span><span class=line><span class=cl>libcrc32c              <span class=m>16384</span>  <span class=m>3</span> xfs,raid456,libceph
</span></span><span class=line><span class=cl>root@client:~# modinfo ceph
</span></span><span class=line><span class=cl>filename:       /lib/modules/4.15.0-112-generic/kernel/fs/ceph/ceph.ko
</span></span><span class=line><span class=cl>license:        GPL
</span></span><span class=line><span class=cl>description:    Ceph filesystem <span class=k>for</span> Linux
</span></span><span class=line><span class=cl>author:         Patience Warnick &lt;patience@newdream.net&gt;
</span></span><span class=line><span class=cl>author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;
</span></span><span class=line><span class=cl>author:         Sage Weil &lt;sage@newdream.net&gt;
</span></span><span class=line><span class=cl>alias:          fs-ceph
</span></span><span class=line><span class=cl>srcversion:     B2806F4EAACAC1E19EE7AFA
</span></span><span class=line><span class=cl>depends:        libceph,fscache
</span></span><span class=line><span class=cl>retpoline:      Y
</span></span><span class=line><span class=cl>intree:         Y
</span></span><span class=line><span class=cl>name:           ceph
</span></span><span class=line><span class=cl>vermagic:       4.15.0-112-generic SMP mod_unload
</span></span><span class=line><span class=cl>signat:         PKCS#7
</span></span><span class=line><span class=cl>signer:        
</span></span><span class=line><span class=cl>sig_key:       
</span></span><span class=line><span class=cl>sig_hashalgo:   md4</span></span></code></pre></td></tr></table></div></div></div></div><p>在启用了fs-cache后，内核日志可以看到对应 cephfs 挂载时 ceph 被注册到 fscache中</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>[</span>  11457.592011<span class=o>]</span> FS-Cache: Loaded
</span></span><span class=line><span class=cl><span class=o>[</span>  11457.617265<span class=o>]</span> Key <span class=nb>type</span> ceph registered
</span></span><span class=line><span class=cl><span class=o>[</span>  11457.617686<span class=o>]</span> libceph: loaded <span class=o>(</span>mon/osd proto 15/24<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>[</span>  11457.640554<span class=o>]</span> FS-Cache: Netfs <span class=s1>&#39;ceph&#39;</span> registered <span class=k>for</span> caching
</span></span><span class=line><span class=cl><span class=o>[</span>  11457.640558<span class=o>]</span> ceph: loaded <span class=o>(</span>mds proto 32<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>[</span>  11457.640978<span class=o>]</span> libceph: parse_ips bad ip <span class=s1>&#39;mon1.ichenfu.com:6789,mon2.ichenfu.com:6789,mon3.ichenfu.com:6789&#39;</span></span></span></code></pre></td></tr></table></div></div></div></div><blockquote><p>当 monitor / OSD 拒绝连接时，所有该节点后续创建的挂载均会使用缓存，除非 umount 所有挂载后重新挂载才可以重新与 ceph mon 建立连接</p></blockquote><h2 id=cephfs-中的-fscache>cephfs 中的 fscache<a hidden class=anchor aria-hidden=true href=#cephfs-中的-fscache>#</a></h2><p>ceph 官方在 2023年11月5日的一篇博客 <sup><a href=#1>[1]</a></sup> 中介绍了，cephfs 与 fscache 结合的介绍。这个功能的加入最显著的成功就是 ceph node 流向 OSD 网络被大大减少，尤其是在读取多的情况下。</p><p>这个机制可以在代码 commit 中看到其原理：“在第一次通过文件引用inode时创建缓存cookie。之后，直到我们处理掉inode，我们都不会摆脱cookie” <sup><a href=#2>[2]</a></sup></p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li><p><sup id=1>[1]</sup> <a href=https://ceph.io/en/news/blog/2013/first-impressions-through-fscache-and-ceph/ target=_blank rel="noopener nofollow noreferrer">First Impressions Through Fscache and Ceph</a></p></li><li><p><sup id=2>[2]</sup> <a href=https://lwn.net/Articles/563146/ target=_blank rel="noopener nofollow noreferrer">ceph: persistent caching with fscache</a></p></li><li><p><sup id=3>[3]</sup> <a href=https://tracker.ceph.com/issues/51191 target=_blank rel="noopener nofollow noreferrer">Cannot Mount CephFS No Timeout, mount error 5 = Input/output error</a></p></li><li><p><sup id=4>[4]</sup> <a href=https://patchwork.kernel.org/project/ceph-devel/patch/20190531122802.12814-3-zyan@redhat.com/ target=_blank rel="noopener nofollow noreferrer">ceph: fix infinite loop in get_quota_realm()</a></p></li><li><p><sup id=5>[5]</sup> <a href=https://drunkard.github.io/cephfs/eviction/ target=_blank rel="noopener nofollow noreferrer">Ceph 文件系统客户端的驱逐</a></p></li><li><p><sup id=6>[6]</sup> <a href=https://docs.ceph.com/en/mimic/cephfs/eviction/#advanced-configuring-blacklisting target=_blank rel="noopener nofollow noreferrer">advanced-configuring-blacklisting</a></p></li></ul></div><div class=pe-copyright><hr><blockquote><p>本文为原创内容，版权归作者所有。如需转载，请在文章中声明本文标题及链接。</p><p>文章标题：当cephfs和fscache结合时在K8s环境下的全集群规模故障</p><p>文章链接：<a href=http://localhost:1313/2023/11/10-1-ceph-fscache/ target=_blank>http://localhost:1313/2023/11/10-1-ceph-fscache/</a></p><p>许可协议：<a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></blockquote></div><div class=comments-separator></div><h3 class=relatedContentTitle>相关阅读</h3><ul class=relatedContent><li><a href=/2023/09/6-1-ceph-rebalance/><span>Ceph重新平衡 - Rebalance</span></a></li><li><a href=/2023/09/11-1-ceph-common-cmd/><span>ceph常用命令</span></a></li><li><a href=/2023/09/05-4-s3cmd-in-windows/><span>Ceph对象存储 - windows上安装s3cmd</span></a></li><li><a href=/2023/09/05-3-s3cmd/><span>Ceph对象存储 - 使用s3cmd管理对象存储</span></a></li><li><a href=/2023/09/05-2-bucket-policy/><span>Ceph对象存储 - 桶策略 Bucket Policy</span></a></li></ul><div class=comments-separator></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/storage/>Storage</a></li><li><a href=http://localhost:1313/tags/troubleshooting/>Troubleshooting</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/2023/11/ch07-in-cluster-pod/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></polyline></svg>&nbsp;</span>
<span>client-go - Pod使用in-cluster方式访问集群</span>
</a><a class=next href=http://localhost:1313/2023/11/ch03-argo-add-cluster/><span class=title></span>
<span>初识Argo cd - 注册/删除k8s集群&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span></a></nav></footer><div class=pe-comments-decoration><p class=pe-comments-title></p><p class=pe-comments-subtitle></p></div><div id=pe-comments></div><script src=/js/pe-go-comment.min.86a214102576ba5f9b7bdc29eed8d58dd56e34aef80b3c65c73ea9cc88443696.js integrity="sha256-hqIUECV2ul+be9wp7tjVjdVuNK74Czxlxz6pzIhENpY="></script><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="dark"?"dark":"light",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"cylonchau/cylonchau.github.io","data-repo-id":"R_kgDOIRlNSQ","data-category":"Announcements","data-category-id":"DIC_kwDOIRlNSc4CXy1U","data-mapping":"pathname","data-term":"posts/10-1-ceph-fscache","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":getStoredTheme(),"data-lang":"zh-TW","data-loading":"lazy",crossorigin:"anonymous",async:""},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#pe-comments").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Cylon's Collection</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> on
<a href=https://pages.github.com/ rel=noopener target=_blank>GitHub Pages</a> & Theme
        <a href=https://github.com/tofuwine/PaperMod-PE rel=noopener target=_blank>PaperMod-PE</a></span></footer><div class=pe-right-sidebar><a href=javascript:void(0); id=theme-toggle-float class=pe-float-btn><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a><a href=#top class=pe-float-btn id=top-link><span id=pe-read-progress></span></a></div><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>