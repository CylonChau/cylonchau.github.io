<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>决策树 | Cylon&#39;s Collection</title>
<meta name="keywords" content="MachineLearning, algorithm, CS">
<meta name="description" content="决策树 - Cylon&#39;s Collection">
<meta name="author" content="cylon">
<link rel="canonical" href="https://www.oomkill.com/2022/06/decision-tree/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.41a8706089174fae1769fc26da4d1d354fa88083db604a95688ff58852dd9006.css" integrity="sha256-QahwYIkXT64Xafwm2k0dNU&#43;ogIPbYEqVaI/1iFLdkAY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.oomkill.com/favicon.ico">
<link rel="apple-touch-icon" href="https://www.oomkill.com/apple-touch-icon.png">

<meta name="twitter:title" content="决策树 | Cylon&#39;s Collection" />
<meta name="twitter:description" content="" />
<meta property="og:title" content="决策树 | Cylon&#39;s Collection" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.oomkill.com/2022/06/decision-tree/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2022-06-01T00:00:00&#43;00:00" />
  <meta property="article:modified_time" content="2023-03-22T23:00:36&#43;08:00" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://www.oomkill.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "决策树",
      "item": "https://www.oomkill.com/2022/06/decision-tree/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "决策树 | Cylon's Collection",
  "name": "决策树",
  "description": "",
  "keywords": [
    "MachineLearning", "algorithm", "CS"
  ],
  "wordCount" : "9180",
  "inLanguage": "zh",
  "datePublished": "2022-06-01T00:00:00Z",
  "dateModified": "2023-03-22T23:00:36+08:00",
  "author":{
    "@type": "Person",
    "name": "cylon"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.oomkill.com/2022/06/decision-tree/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Cylon's Collection",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.oomkill.com/favicon.ico"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary-bg: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list-page {
                background: var(--theme);
            }

            .list-page:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list-page:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

</head>

<body class=" type-posts kind-page layout-" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'auto';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.oomkill.com" accesskey="h" title="Cylon&#39;s Collection (Alt + H)">Cylon&#39;s Collection</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.oomkill.com/archives/" title="归档"
                >归档
                </a>
            </li>
            <li>
                <a href="https://www.oomkill.com/tags/" title="标签"
                >标签
                </a>
            </li>
            <li>
                <a href="https://www.oomkill.com/search/" title="搜索 (Alt &#43; /)"data-no-instant accesskey=/
                >搜索
                </a>
            </li>
            <li>
                <a href="https://www.oomkill.com/about/" title="关于"
                >关于
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header"><h1 class="post-title">决策树</h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>2022-06-01</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>Edited on 2023-03-22</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select: text;"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z" style="user-select: text;"></path><line x1="7" y1="7" x2="7" y2="7" style="user-select: text;"></line></svg>
  <span class="post-tags"><a href="https://www.oomkill.com/tags/machinelearning/">MachineLearning</a><a href="https://www.oomkill.com/tags/algorithm/">algorithm</a><a href="https://www.oomkill.com/tags/cs/">CS</a></span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>19 分钟</span></span>

      
      
    </div>
  </header> <div class="toc side right">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e7%86%b5%e5%92%8c%e5%9f%ba%e5%b0%bc%e6%8c%87%e6%95%b0" aria-label="熵和基尼指数">熵和基尼指数</a><ul>
                        
                <li>
                    <a href="#%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a" aria-label="信息增益">信息增益</a></li>
                <li>
                    <a href="#%e7%86%b5" aria-label="熵">熵</a></li>
                <li>
                    <a href="#%e5%9f%ba%e5%b0%bc%e6%8c%87%e6%95%b0" aria-label="基尼指数">基尼指数</a><ul>
                        
                <li>
                    <a href="#%e8%ae%a1%e7%ae%97%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a%e7%a4%ba%e4%be%8b" aria-label="计算信息增益示例">计算信息增益示例</a></li></ul>
                </li>
                <li>
                    <a href="#%e8%ae%a1%e7%ae%97%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a%e7%a4%ba%e4%be%8b-1" aria-label="计算信息增益示例">计算信息增益示例</a></li></ul>
                </li>
                <li>
                    <a href="#python%e8%ae%a1%e7%ae%97%e5%86%b3%e7%ad%96%e6%a0%91%e5%ae%9e%e4%be%8b" aria-label="python计算决策树实例">python计算决策树实例</a><ul>
                        
                <li>
                    <a href="#%e5%9f%ba%e4%ba%8e%e5%9f%ba%e5%b0%bc%e6%8c%87%e6%95%b0%e7%9a%84%e5%86%b3%e7%ad%96%e6%a0%91" aria-label="基于基尼指数的决策树">基于基尼指数的决策树</a><ul>
                        
                <li>
                    <a href="#%e5%9f%ba%e5%b0%bc%e6%8c%87%e6%95%b0-1" aria-label="基尼指数">基尼指数</a></li>
                <li>
                    <a href="#%e6%8b%86%e5%88%86" aria-label="拆分">拆分</a><ul>
                        
                <li>
                    <a href="#%e6%95%b0%e6%8d%ae%e6%8b%86%e5%88%86" aria-label="数据拆分">数据拆分</a></li>
                <li>
                    <a href="#%e8%af%84%e4%bc%b0%e6%8b%86%e5%88%86%e7%9a%84%e6%95%b0%e6%8d%ae" aria-label="评估拆分的数据">评估拆分的数据</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%a6%82%e4%bd%95%e6%9e%84%e5%bb%ba%e6%a0%91" aria-label="如何构建树">如何构建树</a><ul>
                        
                <li>
                    <a href="#%e7%bb%88%e7%ab%af%e8%8a%82%e7%82%b9" aria-label="终端节点">终端节点</a></li>
                <li>
                    <a href="#%e9%80%92%e5%bd%92%e6%8b%86%e5%88%86" aria-label="递归拆分">递归拆分</a></li>
                <li>
                    <a href="#%e5%88%9b%e5%bb%ba%e6%a0%91" aria-label="创建树">创建树</a></li>
                <li>
                    <a href="#%e6%95%b4%e5%90%88" aria-label="整合">整合</a></li>
                <li>
                    <a href="#%e9%a2%84%e6%b5%8b" aria-label="预测">预测</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%a5%97%e7%94%a8%e7%9c%9f%e5%ae%9e%e6%95%b0%e6%8d%ae%e9%9b%86%e6%9d%a5%e6%b5%8b%e8%af%95" aria-label="套用真实数据集来测试">套用真实数据集来测试</a><ul>
                        
                <li>
                    <a href="#%e4%bb%80%e4%b9%88%e6%98%af-k%e6%8a%98%e4%ba%a4%e5%8f%89%e9%aa%8c%e8%af%81" aria-label="什么是 K折交叉验证">什么是 K折交叉验证</a></li></ul>
                </li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content">
    





<div class="copyrightTopBlock">
    <p>本文发布于<a href="https://www.oomkill.com/about" target="_blank">Cylon的收藏册</a>，转载请著名原文链接~</p>
    <div class="articleSuffix-bg"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 147.78 155.96"> <path d="M10.5,99.81a1.9,1.9,0,0,0-.53-.09,1.66,1.66,0,0,0-1.64,1.65A1.64,1.64,0,0,0,10,103a1.57,1.57,0,0,0,.87-.25l26.76,26.82.45-1.08L11.52,101.91A1.65,1.65,0,0,0,10.5,99.81Zm-.13,2a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.58.58,0,0,1,.57-.57h0a.57.57,0,0,1,.56.58A.55.55,0,0,1,10.37,101.77Z" style="fill:#c5c9e0"></path><path d="M56.15,117.58H39.06l0-.09a1.65,1.65,0,0,0-1.36-1H37.5a1.65,1.65,0,1,0,1.56,2.19H55.7L92.92,156h41.44v-1.08h-41Zm-18.25.94a.56.56,0,0,1-.79,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h0a.58.58,0,0,1,.57.58A.54.54,0,0,1,37.9,118.52Z" style="fill:#c5c9e0"></path><path d="M23.52,50.32a1.65,1.65,0,0,0,1.55-1.11H55.28l48-48.13h31.06V0H102.85l-48,48.13H25.07a1.64,1.64,0,0,0-2.09-1,1.64,1.64,0,0,0,.54,3.2Zm0-2.21a.57.57,0,0,1,0,1.13.57.57,0,1,1,0-1.13Z" style="fill:#c5c9e0"></path><polygon points="102.86 0 102.86 0 102.86 0 102.86 0" style="fill:#c5c9e0"></polygon><path d="M107.72,12.14h26.64V11.07H107.27L57.4,61H3.09a1.66,1.66,0,0,0-1.45-.86H1.52A1.65,1.65,0,1,0,2.81,63a1.59,1.59,0,0,0,.45-.87H57.85ZM2.05,62.23a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h.09a.57.57,0,0,1,.32,1Z" style="fill:#c5c9e0"></path><path d="M134.36,43.22V42.14h-22.3l-9.62,9.63a1.64,1.64,0,0,0-2.19.77,1.61,1.61,0,0,0-.17.71,1.65,1.65,0,1,0,3.29,0,1.61,1.61,0,0,0-.16-.72l9.3-9.32Zm-32.64,10.6a.57.57,0,0,1,0-1.13.57.57,0,0,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M147,52.3l-9,9H111.48a1.64,1.64,0,0,0-1.61-1.33h-.14a1.65,1.65,0,1,0,1.6,2.41h27.19l9.26-9.29L147,52.3Zm-37.15,9.85a.56.56,0,0,1-.56-.57h0a.56.56,0,0,1,.56-.56h0a.57.57,0,1,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M66.79,75.35l11,11.06h56.53V85.33H78.27l-11-11.06H49.49L37.12,86.67a1.64,1.64,0,0,0-2.09,1,1.61,1.61,0,0,0-.09.54,1.65,1.65,0,0,0,3.29,0,1.68,1.68,0,0,0-.26-.89l12-12ZM36.58,88.79a.57.57,0,1,1,.57-.56A.57.57,0,0,1,36.58,88.79Z" style="fill:#c5c9e0"></path><path d="M110.61,95.55,92.8,113.4a1.62,1.62,0,1,0,.77.76l17.49-17.53h23.31V95.55ZM92.49,115.28a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.57-.57h0a.58.58,0,0,1,.56.58A.55.55,0,0,1,92.49,115.28Z" style="fill:#c5c9e0"></path><path d="M97.89,122.3H76.62L64.2,109.85a1.65,1.65,0,0,0-.77-2.2,1.77,1.77,0,0,0-.72-.17h-.14a1.65,1.65,0,0,0,.15,3.29,1.58,1.58,0,0,0,.71-.17l12.74,12.77H98.34l17.48-17.52h18.54v-1.08h-19ZM63.12,109.53a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,1.14,0A.54.54,0,0,1,63.12,109.53Z" style="fill:#c5c9e0"></path> </svg> </div>
</div>
<br><h2 id="熵和基尼指数">熵和基尼指数<a hidden class="anchor" aria-hidden="true" href="#熵和基尼指数">¶</a></h2>
<h3 id="信息增益">信息增益<a hidden class="anchor" aria-hidden="true" href="#信息增益">¶</a></h3>
<p>信息增益 <code>information gain</code> 是用于训练决策树的指标。具体来说，是指这些指标衡量<strong>拆分的质量</strong>。通俗来说是通过根据随机变量的给定值拆分数据集来衡量熵。</p>
<p>通过描述一个事件是否&quot;惊讶&quot;，通常低概率事件更令人惊讶，因此具有更大的信息量。而具有相同可能性的事件的概率分布更&quot;惊讶&quot;并且具有更大的熵。</p>
<p><strong>定义</strong>：熵 <strong>entropy</strong>是一组例子中<strong>杂质</strong>、<strong>无序</strong>或<strong>不确定性</strong>的度量。熵控制决策树如何决定<strong>拆分</strong>数据。它实际上影响了决策树如何绘制边界。</p>
<h3 id="熵">熵<a hidden class="anchor" aria-hidden="true" href="#熵">¶</a></h3>
<p>熵的计算公式为：$E=-\sum^i_{i=1}(p_i\times\log_2(p_i))$ ；$P_i$ 是类别 $i$ 的概率。我们来举一个例子来更好地理解熵及其计算。假设有一个由三种颜色组成的数据集，红色、紫色和黄色。如果我们的集合中有一个红色、三个紫色和四个黄色的观测值，我们的方程变为：$E=-(p_r \times \log_2(p_r) + p_p \times \log_2(p_p) + p_y \times \log_2(p_y)$</p>
<p>其中 $p_r$ 、$p_p$ 和 $p_y$ 分别是选择红色、紫色和黄色的概率。假设 $p_r=\frac{1}{8}$，$p_p=\frac{3}{8}$ ，$p_y=\frac{4}{8}$ 现在等式变为变为：</p>
<ul>
<li>$E=-(\frac{1}{8} \times \log_2(\frac{1}{8}) + \frac{3}{8} \times \log_2(\frac{3}{8}) + \frac{4}{8} \times \log_2(\frac{4}{8}))$</li>
<li>$0.125 \times log_2(0.125) + 0.375 \times log_2(0.375) + 0.5 \times log_2(0.375)$</li>
<li>$0.125 \times -3 + 0.375 \times -1.415 + 0.5 \times -1 = -0.375+-0.425 +-0.5 = 1.41$</li>
</ul>
<p>==当所有观测值都属于同一类时会发生什么？== 在这种情况下，熵将始终为零。$E=-(1log_21)=0$ ；这种情况下的数据集没有杂质，这就意味着没有数据集没有意义。又如果有两类数据集，一半是黄色，一半是紫色，那么熵为1，推导过程是：$E=−(\ (0.5\log_2(0.5))+(0.5\times \log_2(0.5))\ ) = 1$</p>
<h3 id="基尼指数">基尼指数<a hidden class="anchor" aria-hidden="true" href="#基尼指数">¶</a></h3>
<p>基尼指数 <code>Gini index</code> 和熵 <code>entropy </code> 是计算信息增益的标准。决策树算法使用信息增益来拆分节点。</p>
<p>基尼指数计算特定变量在随机选择时被错误分类的概率程度以及基尼系数的变化。它适用于分类变量，提供“成功”或“失败”的结果，因此仅进行二元拆分（二叉树结构）。基尼指数在 0 和 1 之间变化，其中，1 表示元素在各个类别中的随机分布。基尼指数为 0.5 表示元素在某些类别中分布均匀。：</p>
<ul>
<li>0 表示为所有元素都与某个类相关联，或只存在一个类。</li>
<li>1 表示所有元素随机分布在各个类中，并且0.5 表示元素均匀分布到某些类中</li>
</ul>
<p>基尼指数公式：$1− \sum_n^{i=1}(p_i)^2$ ； $P_i$ 为分类到特定类别的概率。在构建决策树时，更愿意选择具有最小基尼指数的属性作为根节点。</p>
<p>通过实例了解公式</p>
<table>
<thead>
<tr>
<th><strong>Past Trend</strong></th>
<th><strong>Open Interest</strong></th>
<th><strong>Trading Volume</strong></th>
<th><strong>Return</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive</td>
<td>Low</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Negative</td>
<td>High</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Positive</td>
<td>High</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Negative</td>
<td>Low</td>
<td>High</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Negative</td>
<td>High</td>
<td>High</td>
<td>Down</td>
</tr>
<tr>
<td>Negative</td>
<td>Low</td>
<td>High</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>High</td>
<td>High</td>
<td>Up</td>
</tr>
</tbody>
</table>
<p>计算基尼指数</p>
<p>已知条件</p>
<ul>
<li>
<p>$P(Past\ Trend=Positive) = \frac{6}{10}$</p>
</li>
<li>
<p>$P(Past\ Trend=Negative) = \frac{4}{10}$</p>
</li>
</ul>
<p>过去趋势基尼指数计算</p>
<p>如果过去趋势为正面，回报为上涨，概率为：$P(Past\ Trend=Positive\ &amp;\ Return=Up) = \frac{4}{6}$</p>
<p>如果过去趋势为正面，回报为下降，概率为：$P(Past\ Trend=Positive\ &amp;\ Return=Down) = \frac{2}{6}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Past\ Trend) = 1-(\frac{4}{6}^2+\frac{2}{6}^2) = 0.45$</li>
</ul>
<p>如果过去趋势为负面，回报为上涨，概率为：$P(Past\ Trend=Negative\ &amp;\ Return=Up) = 0$</p>
<p>如果过去趋势为负面，回报为下降，概率为：$P(Past\ Trend=Negative\ &amp;\ Return=Down) = \frac{4}{4}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Past\ Trend=Negative) = 1-(0^2+\frac{4}{4}^2) = 1-(0+1)=0$</li>
</ul>
<p>那么过去交易量的的基尼指数加权 = $\frac{6}{10} \times 0.45 + \frac{4}{10}\times 0 = 0.27$</p>
<p>未平仓量基尼指数计算</p>
<p>已知条件</p>
<ul>
<li>$P(Open\ Interest=High): \frac{4}{10}$</li>
<li>$P(Open\ Interest=Low): \frac{6}{10}$</li>
</ul>
<p>如果未平仓量为 <code>high</code> 并且回报为上涨，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Up)=\frac{2}{4}$</p>
<p>如果未平仓量为 <code>high</code> 并且回报为下降，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Down)=\frac{2}{4}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Open\ Interest=High) = 1-(\frac{2}{4}^2+\frac{2}{4}^2) = 0.5$</li>
</ul>
<p>如果未平仓量为 <code>low</code> 并且回报为上涨，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Up)=\frac{2}{6}$</p>
<p>如果未平仓量为 <code>low</code> 并且回报为下降，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Down)=\frac{4}{6}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Open\ Interest=Low) = 1-(\frac{2}{6}^2+\frac{4}{6}^2) = 0.45$</li>
</ul>
<p>那么未平仓量基尼指数加权 = $\frac{4}{10} \times 0.5 + \frac{6}{10}\times 0.45 = 0.47$</p>
<p>计算交易量基尼指数</p>
<p>已知条件</p>
<ul>
<li>$P(Trading\ Volume=High): \frac{7}{10}$</li>
<li>$P(Trading\ Volume=Low): \frac{3}{10}$</li>
</ul>
<p>如果交易量为 <code>high</code> 并且回报为上涨，概率为：$P(Trading\ Volume=High\ &amp;\ Return\ = Up)=\frac{4}{7}$</p>
<p>如果交易量为 <code>high</code> 并且回报为下降，概率为：$P(Trading\ Volume = High\ &amp;\ Return\ = Down)=\frac{3}{7}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Trading\ Volume=High) = 1-(\frac{4}{7}^2+\frac{3}{7}^2) = 0.49$</li>
</ul>
<p>如果交易量为 <code>low</code> 并且回报为上涨，概率为：$P(Trading\ Volume = Low\ &amp;\ Return\ = Up)=0$</p>
<p>如果交易量为 <code>low</code> 并且回报为下降，概率为：$P(Trading\ Volume = Low\ &amp;\ Return\ = Down)=\frac{3}{3}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Trading\ Volume=Low) = 1-(0^2+1^2) = 0$</li>
</ul>
<p>那么交易量基尼指数加权 = $\frac{7}{10} \times 0.49 + \frac{3}{10}\times 0 = 0.34$</p>
<p>最终计算出的基尼指数列表如下，在表中可以观察到“<strong>Past Trend</strong>”的基尼指数最低，因此它将被选为决策树的根节点。</p>
<table>
<thead>
<tr>
<th><strong>Attributes</strong></th>
<th><strong>Gini Index</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Past Trend</td>
<td>0.27</td>
</tr>
<tr>
<td>Open Interest</td>
<td>0.47</td>
</tr>
<tr>
<td>Trading Volume</td>
<td>0.34</td>
</tr>
</tbody>
</table>
<p>这里将重复的过程来确定决策树的子节点或分支。将通过计算”<strong>Past Trend</strong>“的“<strong>Positive</strong>”分支的基尼指数如下：</p>
<table>
<thead>
<tr>
<th><strong>Past Trend</strong></th>
<th><strong>Open Interest</strong></th>
<th><strong>Trading Volume</strong></th>
<th><strong>Return</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive</td>
<td>Low</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Positive</td>
<td>High</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>High</td>
<td>High</td>
<td>Up</td>
</tr>
</tbody>
</table>
<p>针对过去正面趋势计算未平仓量的基尼指数</p>
<p>已知条件</p>
<ul>
<li>$P(Open\ Interest=High): \frac{2}{6}$</li>
<li>$P(Open\ Interest=Low): \frac{4}{6}$</li>
</ul>
<p>如果未平仓量为 <code>high</code> 并且回报为上涨，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Up)=\frac{2}{2}$</p>
<p>如果未平仓量为 <code>high</code> 并且回报为下降，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Down)=0$</p>
<ul>
<li>那么这个基尼指数为：$gini(Open\ Interest=High) = 1-(\frac{2}{2}^2+0^2) = 0$</li>
</ul>
<p>如果未平仓量为 <code>low</code> 并且回报为上涨，概率为：$P(Open\ Interest = Low\ &amp;\ Return\ = Up)=\frac{2}{4}$</p>
<p>如果未平仓量为 <code>low</code> 并且回报为下降，概率为：$P(Open\ Interest = Low\ &amp;\ Return\ = Down)=\frac{2}{4}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Open\ Interest=Low) = 1-(\frac{2}{4}^2+\frac{2}{4}^2) = 0.5$</li>
</ul>
<p>那么未平仓量基尼指数加权 = $\frac{2}{6} \times 0 + \frac{4}{6}\times 0.5 = 0.33$</p>
<p>计算交易量基尼指数</p>
<p>已知条件</p>
<ul>
<li>$P(Trading\ Volume=High): \frac{4}{6}$</li>
<li>$P(Trading\ Volume=Low): \frac{2}{6}$</li>
</ul>
<p>如果交易量为 <code>high</code> 并且回报为上涨，概率为：$P(Trading\ Volume=High\ &amp;\ Return\ = Up)=\frac{4}{4}$</p>
<p>如果交易量为 <code>high</code> 并且回报为下降，概率为：$P(Trading\ Volume = High\ &amp;\ Return\ = Down)=0$</p>
<ul>
<li>那么这个基尼指数为：$gini(Trading\ Volume=High) = 1-(\frac{4}{4}^2+0^2) = 0$</li>
</ul>
<p>如果交易量为 <code>low</code> 并且回报为上涨，概率为：$P(Trading\ Volume = Low\ &amp;\ Return\ = Up)=0$</p>
<p>如果交易量为 <code>low</code> 并且回报为下降，概率为：$P(Trading\ Volume = Low\ &amp;\ Return\ = Down)=\frac{2}{2}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Trading\ Volume=Low) = 1-(0^2+\frac{2}{2}^2) = 0$</li>
</ul>
<p>那么交易量基尼指数加权 = $\frac{4}{6} \times 0 + \frac{2}{6}\times 0 = 0$</p>
<p>最终计算出的基尼指数列表如下，这里将使用“<strong>Trading Volume</strong>”进一步拆分节点，因为它具有最小的基尼指数。</p>
<table>
<thead>
<tr>
<th><strong>Attributes/Features</strong></th>
<th><strong>Gini Index</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Open Interest</td>
<td>0.33</td>
</tr>
<tr>
<td>Trading Volume</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>最终的模型就如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220602000050768.png" alt="image-20220602000050768" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="计算信息增益示例">计算信息增益示例<a hidden class="anchor" aria-hidden="true" href="#计算信息增益示例">¶</a></h4>
<p>我们可以根据属于一类数据的概率分布来考虑数据集的熵，例如，在二进制分类数据集的情况下为两个类。计算样本的熵如 $Entropy = -(P_0 \times log(P_0) + P_1 \times log(P_1)$ 。</p>
<p>两类的样本拆分为 <code>50/50</code> 的数据集将具最大熵（最惊讶），而拆分为 <code>10/90</code> 的不平衡数据集将具有较小的熵。可以通过在 Python 中计算这个不平衡数据集的熵的例子来证明这一点。</p>
<pre><code class="language-python">from math import log2
# 概率
class0 = 10/100
class1 = 90/100
# entropy formula
entropy = -(class0 * log2(class0) + class1 * log2(class1))
# print the result
print('entropy: %.3f bits' % entropy)
</code></pre>
<p>运行示例，可以看到用于二分类的数据集的熵小于 1 。也就是说，对来自数据集中的任意示例类进行编码所需的信息不到1。通过这种方式，熵可以用作数据集纯度的计算，例如类别分布的平衡程度。</p>
<p>熵为 0 位表示数据集包含一个类；1或更大位的熵表示平衡数据集的最大熵（取决于类别的数量），介于两者之间的值表示这些极端之间的水平。</p>
<h3 id="计算信息增益示例-1">计算信息增益示例<a hidden class="anchor" aria-hidden="true" href="#计算信息增益示例-1">¶</a></h3>
<p>要求：定义一个函数来根据属于 0 类和 1 类的样本的比率来计算一组样本的熵。</p>
<p>假设有一个20 个示例的数据集，13 个为0 类，7 个为1 类。我们可以计算该数据集的熵，它的熵小于 1 位。</p>
<pre><code class="language-python">from math import log2
# calculate the entropy for the split in the dataset
def entropy(class0, class1):
	return -(class0 * log2(class0) + class1 * log2(class1))

# split of the main dataset
class0 = 13 / 20
class1 = 7 / 20
# calculate entropy before the change
s_entropy = entropy(class0, class1)
print('Dataset Entropy: %.3f bits' % s_entropy)

# Dataset Entropy: 0.934 bits
</code></pre>
<p>假设按照 value1 分割数据集，有一组 8 个样本的数据集，7 个为第 0 类，1 个用于第 1 类。然后我们可以计算这组样本的熵。</p>
<pre><code class="language-python">from math import log2
# calculate the entropy for the split in the dataset
def entropy(class0, class1):
	return -(class0 * log2(class0) + class1 * log2(class1))

# split of the main dataset
s1_class0 = 7 / 8
s1_class1 = 1 / 8
# calculate entropy before the change
s_entropy = entropy(s1_class0, s1_class1)
print('Dataset Entropy: %.3f bits' % s_entropy)

# Dataset Entropy: 0.544 bits
</code></pre>
<p>假设现在按 value2 分割数据集；一组 12 个样本数据集，每组 6 个。我们希望这个组的熵为 1。</p>
<pre><code class="language-python">from math import log2
# calculate the entropy for the split in the dataset
def entropy(class0, class1):
	return -(class0 * log2(class0) + class1 * log2(class1))

# split of the main dataset
s1_class0 = 6 / 12
s1_class1 = 6 / 12
# calculate entropy before the change
s_entropy = entropy(s1_class0, s1_class1)
print('Dataset Entropy: %.3f bits' % s_entropy)

# Dataset Entropy: 1.000 bits
</code></pre>
<p>最后，可以根据为变量的每个值创建的组和计算的熵来计算该变量的信息增益。例如：</p>
<p>第一个变量从数据集中产生一组 8 个样本，第二组在数据集中有12 个样本。在这种情况下，信息增益计算：</p>
<ul>
<li>$Entropy(Dataset) – (\frac{(Count(Group1)}{Count(Dataset)} \times Entropy(Group1) + \frac{Count(Group2)}{Count(Dataset)} \times Entropy(Group2)))$</li>
</ul>
<p>这里是因为在每个子节点重复这个分裂过程直到空叶节点。这意味着每个节点的样本都属于同一类。但是，这种情况下会导致具有许多节点使非常<strong>深的树</strong>，这很容易导致过度拟合。因此，我们通常希望通过设置树的最大深度来修剪树。IG就是我们想确定给定训练特征向的量集中的<strong>哪个属性最有用</strong>，那么上面的公式推理就为：</p>
<ul>
<li>$IG(D_p) = I(D_p) − \frac{N_{left}}{N_p}I(D_{left})−\frac{N_{right}}{N_p}I(D_{right})$
<ul>
<li>$IG(D_P)$：数据集的信息增益</li>
<li>$I(D)$：叶子的熵或基尼指数</li>
<li>$\frac{N}{N_P}$ ：页数据集占总数据集的比例</li>
</ul>
</li>
</ul>
<p>我们将使用它来决定<strong>决策树</strong> 节点中<strong>属性的顺序</strong>。该行为在python中表示为：</p>
<pre><code class="language-python">from math import log2
 
# calculate the entropy for the split in the dataset
def entropy(class0, class1):
	return -(class0 * log2(class0) + class1 * log2(class1))
 
# split of the main dataset
class0 = 13 / 20
class1 = 7 / 20
# calculate entropy before the change
s_entropy = entropy(class0, class1)
print('Dataset Entropy: %.3f bits' % s_entropy)
 
# split 1 (split via value1)
s1_class0 = 7 / 8
s1_class1 = 1 / 8
# calculate the entropy of the first group
s1_entropy = entropy(s1_class0, s1_class1)
print('Group1 Entropy: %.3f bits' % s1_entropy)
 
# split 2  (split via value2)
s2_class0 = 6 / 12
s2_class1 = 6 / 12
# calculate the entropy of the second group
s2_entropy = entropy(s2_class0, s2_class1)
print('Group2 Entropy: %.3f bits' % s2_entropy)
 
# calculate the information gain
gain = s_entropy - (8/20 * s1_entropy + 12/20 * s2_entropy)
print('Information Gain: %.3f bits' % gain)


# Dataset Entropy: 0.934 bits
# Group1 Entropy: 0.544 bits
# Group2 Entropy: 1.000 bits
# Information Gain: 0.117 bits
</code></pre>
<p>通过实例，就可以很清楚的明白了，信息增益的概念：<strong>信息熵-条件熵</strong>，换句话来说就是==信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度==。</p>
<h2 id="python计算决策树实例">python计算决策树实例<a hidden class="anchor" aria-hidden="true" href="#python计算决策树实例">¶</a></h2>
<h3 id="基于基尼指数的决策树">基于基尼指数的决策树<a hidden class="anchor" aria-hidden="true" href="#基于基尼指数的决策树">¶</a></h3>
<p>钞票数据集涉及根据从照片中采取的一系列措施来预测给定钞票是否是真实的。数据是取自真钞和伪钞样样本的图像中提取的。对于数字化，使用了通常用于印刷检查的工业相机，从图像中提取特征。</p>
<p>该数据集包含 1372 行和 5 个数值变量。这是一个二元分类的问题。</p>
<h4 id="基尼指数-1">基尼指数<a hidden class="anchor" aria-hidden="true" href="#基尼指数-1">¶</a></h4>
<p>假设有两组数据，每组有 2 行。第一组的行都属于 0 类，第二组的行都属于 1 类，所以这是一个完美的拆分。</p>
<p>首先需要计算每个组中类的比例。</p>
<pre><code class="language-python">proportion = count(class_value) / count(rows)
</code></pre>
<p>这个比例是</p>
<pre><code class="language-python">group_1_class_0 = 2 / 2 = 1
group_1_class_1 = 0 / 2 = 0
group_2_class_0 = 0 / 2 = 0
group_2_class_1 = 2 / 2 = 1
</code></pre>
<p>为每个子节点计算 Gini index</p>
<pre><code class="language-python">gini_index = sum(proportion * (1.0 - proportion))
gini_index = 1.0 - sum(proportion * proportion)
</code></pre>
<p>然后对每组的基尼指数按组的大小加权，例如当前正在分组的所有样本。我们可以将此权重添加到组的基尼指数计算中，如下所示：</p>
<pre><code class="language-python">gini_index = (1.0 - sum(proportion * proportion)) * (group_size/total_samples)
</code></pre>
<p>在该案例中，每个组的基尼指数为：</p>
<pre><code class="language-python">Gini(group_1) = (1 - (1*1 + 0*0)) * 2/4
Gini(group_1) = 0.0 * 0.5 
Gini(group_1) = 0.0 # 分类1的基尼指数
Gini(group_2) = (1 - (0*0 + 1*1)) * 2/4
Gini(group_2) = 0.0 * 0.5 
Gini(group_2) = 0.0 # 分类2的基尼指数
</code></pre>
<p>然后在分割点的每个子节点上添加分数，以给出分割点的最终 Gini 分数，该分数可以与其他候选分割点进行比较。如该分割点的基尼系数为 $0.0 + 0.0$ 或完美的基尼系数 0.0。</p>
<p>编写一个 <code>gini_index()</code> 的函数，用于计算组列表和已知类值列表的基尼指数。</p>
<pre><code class="language-python">def gini_index(groups, classes):
    print(&quot;------------&quot;)
    # 计算所有样本的分割点，计算样本的总长度
    n_instances = float(sum([len(group) for group in groups]))
    # 计算每个组的总基尼指数
    gini = 0.0
    for group in groups:
        size = float(len(group))
        if size == 0: # avoid divide by zero
            continue
        score = 0.0
        # score the group based on the score for each class
        for class_val in classes:
            # row[-1] 代表每个样本的最后一个值，是否存在分类 class_val
            p = [row[-1] for row in group]
            p1 = p.count(class_val) / size
            score += p1 * p1
        # 按照对应的样本分割点，加权重
        gini += (1.0 - score) * (size / n_instances)
    return gini

print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))
print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))
</code></pre>
<p>运行该示例会打印两组的Gini index，最差情况的为 0.5，最少情况的指数为 0.0。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220602215655808.png" alt="image-20220602215655808" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="拆分">拆分<a hidden class="anchor" aria-hidden="true" href="#拆分">¶</a></h4>
<h5 id="数据拆分">数据拆分<a hidden class="anchor" aria-hidden="true" href="#数据拆分">¶</a></h5>
<p>拆分是由数据集中的一个属性和一个值组成。可以将其总结为要拆分的属性的索引和拆分该属性上的行的值。这只是索引数据行的有用简写。</p>
<p>创建拆分涉及三个部分，我们已经看过的第一个部分是计算基尼分数。剩下的两部分是：</p>
<ul>
<li>拆分数据集。</li>
<li>评估所有拆分。</li>
</ul>
<p>拆分数据是给定数据集索引和拆分值，将数据集拆分为两个行列表形成一个分类。具体是拆分数据集涉及遍历每一行，检查属性值是否低于或高于拆分值，并将其分别分配给左组或右组。当存在两个组时，可以按照基尼指数进行评估</p>
<p>编写一个**test_split()**函数，它实现了拆分。</p>
<pre><code class="language-python">def test_split(index, value, dataset):
	left, right = list(), list()
	for row in dataset:
		if row[index] &lt; value:
			left.append(row)
		else:
			right.append(row)
	return left, right
</code></pre>
<h5 id="评估拆分的数据">评估拆分的数据<a hidden class="anchor" aria-hidden="true" href="#评估拆分的数据">¶</a></h5>
<p>给定一个数据集，必须检查每个属性上的每个值作为候选拆分，评估拆分的成本并找到我们可以进行的最佳拆分。一旦找到最佳值，就可以将其用作决策树中的节点。</p>
<p>这里使用 <code>dict</code> 作为决策树中的节点，因为这样可以按名称存储数据。选择最佳基尼指数并将其用作树的新节点。</p>
<p>每组数据都是其小数据集，其中仅包含通过拆分过程分配给左组或右组的那些行。可以想象我们如何在构建决策树时递归地再次拆分每个组。</p>
<pre><code class="language-python">def get_split(dataset):
	class_values = list(set(row[-1] for row in dataset))
	b_index, b_value, b_score, b_groups = 999, 999, 999, None
	for index in range(len(dataset[0])-1):
		for row in dataset:
			groups = test_split(index, row[index], dataset)
			gini = gini_index(groups, class_values)
			if gini &lt; b_score:
				b_index, b_value, b_score, b_groups = index, row[index], gini, groups
	return {'index':b_index, 'value':b_value, 'groups':b_groups}
</code></pre>
<p>之后准备一些测试数据集进行测试，其中 $Y$ 是测试集的分类</p>
<pre><code>X1				X2				Y
2.771244718		1.784783929		0
1.728571309		1.169761413		0
3.678319846		2.81281357		0
3.961043357		2.61995032		0
2.999208922		2.209014212		0
7.497545867		3.162953546		1
9.00220326		3.339047188		1
7.444542326		0.476683375		1
10.12493903		3.234550982		1
6.642287351		3.319983761		1
</code></pre>
<p>将上述代码整合为一起，运行该代码后会打印所有基尼指数，基尼指数为 0.0 或完美分割。</p>
<pre><code class="language-python"># Split a dataset based on an attribute and an attribute value
def test_split(index, value, dataset):
    left, right = list(), list()
    for row in dataset:
        if row[index] &lt; value:
            left.append(row)
        else:
            right.append(row)
    return left, right

# Calculate the Gini index for a split dataset
def gini_index(groups, classes):
    # 计算两组数据集的总数每个种类的列表数量和
    n_instances = float(sum([len(group) for group in groups]))
    # 计算每组的基尼值
    gini = 0.0
    for group in groups:
        size = float(len(group))
        # avoid divide by zero
        if size == 0:
            continue
        score = 0.0
        # score the group based on the score for each class
        for class_val in classes:
            # 拿出数据集中每行的类型，拆开是为了更好的了解结构
            p = [row[-1] for row in group]
            # print(&quot;%f / %f = %f&quot; % (p.count(class_val), size, p.count(class_val) / size ))
            # 这里计算的是当前的分类在总数据集中占比
            p1 = p.count(class_val) / size
            score += p1 * p1 # gini index formula = 1 - sum(p_i^2)
        # 计算总的基尼指数，权重：当前分组占总数据集中的数量
        gini += (1.0 - score) * (size / n_instances)
    return gini

# Select the best split point for a dataset
def get_split(dataset):
    class_values = list(set(row[-1] for row in dataset))
    b_index, b_value, b_score, b_groups = 999, 999, 999, None
    for index in range(len(dataset[0])-1): # 最后分类不计算
        for row in dataset:
            # 根据每个值分类计算出最优基尼值，这个值就作为决策树的节点
            groups = test_split(index, row[index], dataset)
            gini = gini_index(groups, class_values)
            print('X%d &lt; %.3f Gini=%.3f' % ((index+1), row[index], gini))
            if gini &lt; b_score:
                b_index, b_value, b_score, b_groups = index, row[index], gini, groups
    return {'index':b_index, 'value':b_value, 'groups':b_groups}

dataset = [
    [2.771244718,1.784783929,0],
    [1.728571309,1.169761413,0],
    [3.678319846,2.81281357,0],
    [3.961043357,2.61995032,0],
    [2.999208922,2.209014212,0],
    [7.497545867,3.162953546,1],
    [9.00220326,3.339047188,1],
    [7.444542326,0.476683375,1],
    [10.12493903,3.234550982,1],
    [6.642287351,3.319983761,1]
]

split = get_split(dataset)
print('Split: [X%d &lt; %.3f]' % ((split['index']+1), split['value']))
</code></pre>
<p>通过执行结果可以看出，<code>X1 &lt; 6.642 Gini=0.000 </code>基尼指数为 0.0 为完美分割。</p>
<h4 id="如何构建树">如何构建树<a hidden class="anchor" aria-hidden="true" href="#如何构建树">¶</a></h4>
<p>构建树主要分为 3 个部分</p>
<ul>
<li>终端节点 <code>Terminal Nodes</code> 零度节点称为终端节点或叶节点</li>
<li>递归拆分</li>
<li>建造一棵树</li>
</ul>
<h5 id="终端节点">终端节点<a hidden class="anchor" aria-hidden="true" href="#终端节点">¶</a></h5>
<p>需要决定何时停止种植树，这里可以使用节点在训练数据集中负责的<strong>深度</strong>和<strong>行数</strong>来做到。</p>
<ul>
<li><strong>树的最大深度</strong>：从树的根节点开始的最大节点数。一旦达到树的最大深度，停止拆分新节点。</li>
<li><strong>最小节点</strong>：对一个节点的要训练的最小值。一旦达到或低于此最小值，则停止拆分和添加新节点。</li>
</ul>
<p>这两种方法将是构建树的过程时用户的指定参数。当在给定点停止增长时，该节点称为终端节点，用于进行最终预测。</p>
<p>编写一个函数<strong>to_terminal()</strong>，这个函数将为一组行选择一类。它返回行列表中最常见的输出值。</p>
<pre><code class="language-python">def to_terminal(group):
	outcomes = [row[-1] for row in group]
	return max(set(outcomes), key=outcomes.count)
</code></pre>
<h5 id="递归拆分">递归拆分<a hidden class="anchor" aria-hidden="true" href="#递归拆分">¶</a></h5>
<p>构建决策树会在为每个节点创建的组上一遍又一遍地调用 <code>get_split()</code> 函数。</p>
<p>添加到现有节点的新节点称为子节点。一个节点可能有零个子节点（一个终端节点）、一个子节点或两个子节点，这里将在给定节点的字典表示中将子节点称为左和右。当一旦创建出一个节点，则通过再次调用相同的函数来递归地从拆分的每组数据以创建子节点。</p>
<p>下面需要实现这个过程（递归函数）。函数接受一个节点作为参数，以及节点中的最大深度、最小模式数、节点的当前深度。</p>
<p>调用的过程分步为。设置，传入根节点和深度1：</p>
<ul>
<li>首先，将拆分后的两组数据提取出来使用，当处理过这些组时，节点不再需要访问这些数据。</li>
<li>接下来，我们检查左或右两组是否为空，如果是，则使用我们拥有的记录创建一个终端节点。</li>
<li>不为空的情况下，检查是否达到了最大深度，如果是，则创建一个终端节点。</li>
<li>然后我们处理左子节点，如果行组太小，则创建一个终端节点，否则以深度优先的方式创建并添加左节点，直到在该分支上到达树的底部。最后再以相同的方式处理右侧。</li>
</ul>
<pre><code class="language-python"># 创建子拆分或者终端节点
def split(node, max_depth, min_size, depth):
    &quot;&quot;&quot;
    :param node: {},分割好的的{'index':b_index, 'value':b_value, 'groups':b_groups}
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :param depth:int， 当前深度
    :return: None
    &quot;&quot;&quot;
    left, right = node['groups']
    del(node['groups'])
    # 检查两边的分割问题
    if not left or not right:
        node['left'] = node['right'] = to_terminal(left + right)
        return
    # 检查最大的深度
    if depth &gt;= max_depth:
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    # 处理左分支，数量要小于最小模式数为terminal node
    if len(left) &lt;= min_size:
        node['left'] = to_terminal(left)
    else:
        node['left'] = get_split(left)
        split(node['left'], max_depth, min_size, depth+1) # 否则递归
    # 处理左右支，数量要小于最小模式数为terminal node
    if len(right) &lt;= min_size:
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_split(right)
        split(node['right'], max_depth, min_size, depth+1)
</code></pre>
<h5 id="创建树">创建树<a hidden class="anchor" aria-hidden="true" href="#创建树">¶</a></h5>
<p>构建一个树就是一个上面的步骤的合并，通过**split()**函数打分并确定树的根节点，然后通过递归来构建出整个树；下面代码是实现此过程的函数 <strong>build_tree()</strong>。</p>
<pre><code class="language-python"># Build a decision tree
def build_tree(train, max_depth, min_size):
    &quot;&quot;&quot;
    :param train: list, 数据集，可以是训练集
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :return: None
    &quot;&quot;&quot;
    root = get_split(train) # 对整个数据集进行打分
    split(root, max_depth, min_size, 1)
    return root
</code></pre>
<h5 id="整合">整合<a hidden class="anchor" aria-hidden="true" href="#整合">¶</a></h5>
<p>将全部代码整合为一个</p>
<pre><code class="language-python"># Split a dataset based on an attribute and an attribute value
def test_split(index, value, dataset):
    left, right = list(), list()
    for row in dataset:
        if row[index] &lt; value:
            left.append(row)
        else:
            right.append(row)
    return left, right

# Calculate the Gini index for a split dataset
def gini_index(groups, classes):
    # 计算两组数据集的总数每个种类的列表数量和
    n_instances = float(sum([len(group) for group in groups]))
    # 计算每组的基尼值
    gini = 0.0
    for group in groups:
        size = float(len(group))
        # avoid divide by zero
        if size == 0:
            continue
        score = 0.0
        # score the group based on the score for each class
        for class_val in classes:
            # 拿出数据集中每行的类型，拆开是为了更好的了解结构
            p = [row[-1] for row in group]
            # print(&quot;%f / %f = %f&quot; % (p.count(class_val), size, p.count(class_val) / size ))
            # 这里计算的是当前的分类在总数据集中占比
            p1 = p.count(class_val) / size
            score += p1 * p1 # gini index formula = 1 - sum(p_i^2)
        # 计算总的基尼指数，权重：当前分组占总数据集中的数量
        gini += (1.0 - score) * (size / n_instances)
    return gini

# Select the best split point for a dataset
def get_split(dataset):
    class_values = list(set(row[-1] for row in dataset))
    b_index, b_value, b_score, b_groups = 999, 999, 999, None
    for index in range(len(dataset[0])-1): # 最后分类不计算
        for row in dataset:
            # 根据每个值分类计算出最优基尼值，这个值就作为决策树的节点
            groups = test_split(index, row[index], dataset)
            gini = gini_index(groups, class_values)
            # print('X%d &lt; %.3f Gini=%.3f' % ((index+1), row[index], gini))
            if gini &lt; b_score: # 拿到最小的gini index那列
                b_index, b_value, b_score, b_groups = index, row[index], gini, groups
    return {'index':b_index, 'value':b_value, 'groups':b_groups}

# 创建子拆分或者终端节点
def split(node, max_depth, min_size, depth):
    &quot;&quot;&quot;
    :param node: {},分割好的的{'index':b_index, 'value':b_value, 'groups':b_groups}
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :param depth:int， 当前深度
    :return: None
    &quot;&quot;&quot;
    left, right = node['groups']
    del(node['groups'])
    # 检查两边的分割问题
    if not left or not right:
        node['left'] = node['right'] = to_terminal(left + right)
        return
    # 检查最大的深度
    if depth &gt;= max_depth:
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    # 处理左分支，数量要小于最小模式数为terminal node
    if len(left) &lt;= min_size:
        node['left'] = to_terminal(left)
    else:
        node['left'] = get_split(left)
        split(node['left'], max_depth, min_size, depth+1) # 否则递归
    # 处理左右支，数量要小于最小模式数为terminal node
    if len(right) &lt;= min_size:
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_split(right)
        split(node['right'], max_depth, min_size, depth+1)

# Build a decision tree
def build_tree(train, max_depth, min_size):
    &quot;&quot;&quot;
    :param train: list, 数据集，可以是训练集
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :return: None
    &quot;&quot;&quot;
    root = get_split(train) # 对整个数据集进行打分
    split(root, max_depth, min_size, 1)
    return root

# 打印树
def print_tree(node, depth=0):
    if isinstance(node, dict):
        print('%s[X%d &lt; %.3f]' % ( (depth*' ', (node['index']+1), node['value']) ))
        print_tree(node['left'], depth+1) # 递归打印左右
        print_tree(node['right'], depth+1)
    else:
        print('%s[%s]' % ((depth*' ', node))) # 不是对象就是terminal node

# 创建一个terminal node
def to_terminal(group):
    outcomes = [row[-1] for row in group]
    return max(set(outcomes), key=outcomes.count)

dataset = [
    [2.771244718,1.784783929,0],
    [1.728571309,1.169761413,0],
    [3.678319846,2.81281357,0],
    [3.961043357,2.61995032,0],
    [2.999208922,2.209014212,0],
    [7.497545867,3.162953546,1],
    [9.00220326,3.339047188,1],
    [7.444542326,0.476683375,1],
    [10.12493903,3.234550982,1],
    [6.642287351,3.319983761,1]
]

if __name__=='__main__':
    tree = build_tree(dataset, 4, 2)
    print_tree(tree)
</code></pre>
<p>可以看到打印结果是一个类似二叉树的</p>
<pre><code>[X1 &lt; 6.642]
 [X1 &lt; 2.771]
  [0]
  [X1 &lt; 2.771]
   [0]
   [0]
 [X1 &lt; 7.498]
  [X1 &lt; 7.445]
   [1]
   [1]
  [X1 &lt; 7.498]
   [1]
   [1]
</code></pre>
<h5 id="预测">预测<a hidden class="anchor" aria-hidden="true" href="#预测">¶</a></h5>
<p>预测是预测数据是该向右还是向左，是作为对数据进行导航的方式。这里可以使用递归来实现，其中使用左侧或右侧子节点再次调用相同的预测，具体取决于拆分如何影响提供的数据。</p>
<p>我们必须检查子节点是否是要作为预测返回的终端值，或者它是否是包含要考虑的树的另一个级别的字典节点。</p>
<p>下面是实现此过程的函数 <strong>predict()</strong>。</p>
<pre><code class="language-python"># Make a prediction with a decision tree
def predict(node, row):
	if row[node['index']] &lt; node['value']:
		if isinstance(node['left'], dict):
			return predict(node['left'], row)
		else:
			return node['left']
	else:
		if isinstance(node['right'], dict):
			return predict(node['right'], row)
		else:
			return node['right']
</code></pre>
<p>下面是一个使用硬编码决策树的示例，该决策树具有一个最好地分割数据的节点（决策树桩，这个就是gini index的最优质值）。通过对上面的测试数据集例来对每一行进行预测。</p>
<pre><code class="language-python">def predict(node, row):
    # 如果gini index与对应属性的值小于则向左，
	if row[node['index']] &lt; node['value']:
		if isinstance(node['left'], dict):
			return predict(node['left'], row) # 递归处理完整个树
		else:
			return node['left']
	else: # 否则的话，则为右
		if isinstance(node['right'], dict):
			return predict(node['right'], row)
		else:
			return node['right']
 
dataset = [[2.771244718,1.784783929,0],
	[1.728571309,1.169761413,0],
	[3.678319846,2.81281357,0],
	[3.961043357,2.61995032,0],
	[2.999208922,2.209014212,0],
	[7.497545867,3.162953546,1],
	[9.00220326,3.339047188,1],
	[7.444542326,0.476683375,1],
	[10.12493903,3.234550982,1],
	[6.642287351,3.319983761,1]]
 
#  这是之前用于计算出最优的gini index
stump = {'index': 0, 'right': 1, 'value': 6.642287351, 'left': 0}
for row in dataset:
	prediction = predict(stump, row)
	print('Expected=%d, Got=%d' % (row[-1], prediction))
</code></pre>
<p>通过观察可以看出预测结果和实际结果一样</p>
<pre><code class="language-python">Expected=0, Got=0
Expected=0, Got=0
Expected=0, Got=0
Expected=0, Got=0
Expected=0, Got=0
Expected=1, Got=1
Expected=1, Got=1
Expected=1, Got=1
Expected=1, Got=1
Expected=1, Got=1
</code></pre>
<h4 id="套用真实数据集来测试">套用真实数据集来测试<a hidden class="anchor" aria-hidden="true" href="#套用真实数据集来测试">¶</a></h4>
<p>这里将使用 <code>CART</code> 算法对<a href="https://archive.ics.uci.edu/ml/datasets/banknote&#43;authentication" target="_blank"
   rel="noopener nofollow noreferrer" >银行钞票数据集</a>进行预测。大概的流程为：</p>
<ul>
<li>加载数据集并转换格式。</li>
<li>编写拆分算法与准确度计算算法；这里使用 5折的k折交叉验证（<code>k-fold cross validation</code>）用于评估算法</li>
<li>编写 CART 算法，从训练数据集，创建树，对测试数据集进行预测操作</li>
</ul>
<h5 id="什么是-k折交叉验证">什么是 K折交叉验证<a hidden class="anchor" aria-hidden="true" href="#什么是-k折交叉验证">¶</a></h5>
<p>K折较差验证（<strong>K-Fold CV</strong>）是将给定的数据集分成<strong>K</strong>个部分，其中每个折叠在某时用作测试集。以 5 折（K=5）为例。这种情况下，数据集被分成5份。在第一次迭代中，第一份用于测试模型，其余用于训练模型。在第二次迭代中，第 2 份用作测试集，其余用作训练集。重复这个过程，直到 5 个折叠中的每个折叠都被用作测试集。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1IjKy-Zc9zVOHFzMw2GXaQw.png" alt="K-Fold CV" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>下面来开始编写函数，函数的整个过程为</p>
<ul>
<li><code>evaluate_algorithm()</code> 作为最外层调用
<ul>
<li>使用五折交叉进行评估 <code>cross_validation_split()</code></li>
<li>使用决策树算法作为算法根据 <code>decision_tree()</code></li>
<li>构建树：<code>build_tree()</code>
<ul>
<li>拿到最优基尼指数作为叶子 <code>get_split()</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="language-python">from random import seed
from random import randrange
from csv import reader
 
# 加载csv文件
def load_csv(filename):
	file = open(filename, &quot;rt&quot;)
	lines = reader(file)
	dataset = list(lines)
	return dataset
 
# 将所有字段转换为float类型便于计算
def str_column_to_float(dataset, column):
	for row in dataset:
		row[column] = float(row[column].strip())
 
# k-folds CV函数
def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
    # 平均分位折数n_folds
	fold_size = int(len(dataset) / n_folds)
	for i in range(n_folds):
		fold = list()
		while len(fold) &lt; fold_size:
			index = randrange(len(dataset_copy)) # 随机
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split
 
# 计算精确度
def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0
 
# Evaluate an algorithm using a cross validation split
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
	folds = cross_validation_split(dataset, n_folds)
	scores = list()
	for fold in folds:
		train_set = list(folds)
		train_set.remove(fold)
		train_set = sum(train_set, [])
		test_set = list()
		for row in fold:
			row_copy = list(row)
			test_set.append(row_copy)
			row_copy[-1] = None
		predicted = algorithm(train_set, test_set, *args)
		actual = [row[-1] for row in fold]
		accuracy = accuracy_metric(actual, predicted)
		scores.append(accuracy)
	return scores
 
# 根据基尼指数划分value是应该在树的哪边？
def test_split(index, value, dataset):
	left, right = list(), list()
	for row in dataset:
		if row[index] &lt; value:
			left.append(row)
		else:
			right.append(row)
	return left, right
 
# 基尼指数打分
def gini_index(groups, classes):
    # 计算数据集中的多组数据的总个数
    n_instances = float(sum([len(group) for group in groups]))
    # 计算每组中的最优基尼指数
    gini = 0.0
    for group in groups:
        size = float(len(group))
        if size == 0:
            continue
        score = 0.0
        # 总基尼指数
        for class_val in classes:
            # 拿出数据集中每行的类型，拆开是为了更好的了解结构

            # 计算的是当前的分类在总数据集中占比
            p = [row[-1] for row in group]
            p1 = p.count(class_val) / size
            score += p1 * p1
        # 计算总的基尼指数，并根据相应大小增加权重。权重：当前分组占总数据集中的数量
        gini += (1.0 - score) * (size / n_instances)
    return gini
 
# 从数据集中获得基尼指数最佳的值
def get_split(dataset):
	class_values = list(set(row[-1] for row in dataset))
	b_index, b_value, b_score, b_groups = 999, 999, 999, None
	for index in range(len(dataset[0])-1):
		for row in dataset:
			groups = test_split(index, row[index], dataset)
			gini = gini_index(groups, class_values)
			if gini &lt; b_score:
				b_index, b_value, b_score, b_groups = index, row[index], gini, groups
	return {'index':b_index, 'value':b_value, 'groups':b_groups}
 
# 创建终端节点
def to_terminal(group):
	outcomes = [row[-1] for row in group]
	return max(set(outcomes), key=outcomes.count)
 
# 创建子节点，为终端节点或子节点
def split(node, max_depth, min_size, depth):
    &quot;&quot;&quot;
    :param node: {},分割好的的{'index':b_index, 'value':b_value, 'groups':b_groups}
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :param depth:int， 当前深度
    :return: None
    &quot;&quot;&quot;
    left, right = node['groups']
    del(node['groups'])
    # check for a no split
    if not left or not right:
        node['left'] = node['right'] = to_terminal(left + right)
        return
    # check for max depth
    if depth &gt;= max_depth:
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    # process left child
    if len(left) &lt;= min_size:
        node['left'] = to_terminal(left)
    else:
        node['left'] = get_split(left)
        split(node['left'], max_depth, min_size, depth+1)
    # process right child
    if len(right) &lt;= min_size:
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_split(right)
        split(node['right'], max_depth, min_size, depth+1)
 
# 构建树
def build_tree(train, max_depth, min_size):
    &quot;&quot;&quot;
    :param train: list, 数据集，可以是训练集
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :ret
    &quot;&quot;&quot;
    root = get_split(train)
    split(root, max_depth, min_size, 1)
    return root

# 打印树
def print_tree(node, depth=0):
    if isinstance(node, dict):
        print('%s[X%d &lt; %.3f]' % ( (depth*' ', (node['index']+1), node['value']) ))
        print_tree(node['left'], depth+1) # 递归打印左右
        print_tree(node['right'], depth+1)
    else:
        print('%s[%s]' % ((depth*' ', node))) # 不是对象就是terminal node
 
# 预测，预测方式为当前基尼指数与最优基尼指数相比较，然后放入树两侧
def predict(node, row):
    &quot;&quot;&quot;
    :param node: {} 叶子值
    :param row: {}, 需要预测值
    :ret
    &quot;&quot;&quot;
    if row[node['index']] &lt; node['value']:
        if isinstance(node['left'], dict):
            return predict(node['left'], row)
        else:
            return node['left']
    else:
        if isinstance(node['right'], dict):
            return predict(node['right'], row)
        else:
            return node['right']
 

def decision_tree(train, test, max_depth, min_size):
	tree = build_tree(train, max_depth, min_size)
	predictions = list()
	for row in test:
		prediction = predict(tree, row)
		predictions.append(prediction)
	return(predictions)


# Test CART on Bank Note dataset
seed(1)
# 加载数据
filename = 'data_banknote_authentication.csv'
dataset = load_csv(filename)
# 转换格式
for i in range(len(dataset[0])):
	str_column_to_float(dataset, i)
# 评估算法
n_folds = 5
max_depth = 5
min_size = 10
scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)
print('Scores: %s' % scores)
print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))
</code></pre>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">¶</a></h2>
<blockquote>
<p><a href="https://www.bogotobogo.com/python/scikit-learn/scikt_machine_learning_Decision_Tree_Learning_Informatioin_Gain_IG_Impurity_Entropy_Gini_Classification_Error.php" target="_blank"
   rel="noopener nofollow noreferrer" >Informatioin Gain</a></p>
<p><a href="https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/" target="_blank"
   rel="noopener nofollow noreferrer" >implement decision tree algorithm</a></p>
<p><a href="https://machinelearningmastery.com/information-gain-and-mutual-information/" target="_blank"
   rel="noopener nofollow noreferrer" >inplement information gain</a></p>
</blockquote>


    
    


<div class="copyrightBlock" >
    <div class="articleSuffix-bg"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 147.78 155.96"> <path d="M10.5,99.81a1.9,1.9,0,0,0-.53-.09,1.66,1.66,0,0,0-1.64,1.65A1.64,1.64,0,0,0,10,103a1.57,1.57,0,0,0,.87-.25l26.76,26.82.45-1.08L11.52,101.91A1.65,1.65,0,0,0,10.5,99.81Zm-.13,2a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.58.58,0,0,1,.57-.57h0a.57.57,0,0,1,.56.58A.55.55,0,0,1,10.37,101.77Z" style="fill:#c5c9e0"></path><path d="M56.15,117.58H39.06l0-.09a1.65,1.65,0,0,0-1.36-1H37.5a1.65,1.65,0,1,0,1.56,2.19H55.7L92.92,156h41.44v-1.08h-41Zm-18.25.94a.56.56,0,0,1-.79,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h0a.58.58,0,0,1,.57.58A.54.54,0,0,1,37.9,118.52Z" style="fill:#c5c9e0"></path><path d="M23.52,50.32a1.65,1.65,0,0,0,1.55-1.11H55.28l48-48.13h31.06V0H102.85l-48,48.13H25.07a1.64,1.64,0,0,0-2.09-1,1.64,1.64,0,0,0,.54,3.2Zm0-2.21a.57.57,0,0,1,0,1.13.57.57,0,1,1,0-1.13Z" style="fill:#c5c9e0"></path><polygon points="102.86 0 102.86 0 102.86 0 102.86 0" style="fill:#c5c9e0"></polygon><path d="M107.72,12.14h26.64V11.07H107.27L57.4,61H3.09a1.66,1.66,0,0,0-1.45-.86H1.52A1.65,1.65,0,1,0,2.81,63a1.59,1.59,0,0,0,.45-.87H57.85ZM2.05,62.23a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h.09a.57.57,0,0,1,.32,1Z" style="fill:#c5c9e0"></path><path d="M134.36,43.22V42.14h-22.3l-9.62,9.63a1.64,1.64,0,0,0-2.19.77,1.61,1.61,0,0,0-.17.71,1.65,1.65,0,1,0,3.29,0,1.61,1.61,0,0,0-.16-.72l9.3-9.32Zm-32.64,10.6a.57.57,0,0,1,0-1.13.57.57,0,0,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M147,52.3l-9,9H111.48a1.64,1.64,0,0,0-1.61-1.33h-.14a1.65,1.65,0,1,0,1.6,2.41h27.19l9.26-9.29L147,52.3Zm-37.15,9.85a.56.56,0,0,1-.56-.57h0a.56.56,0,0,1,.56-.56h0a.57.57,0,1,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M66.79,75.35l11,11.06h56.53V85.33H78.27l-11-11.06H49.49L37.12,86.67a1.64,1.64,0,0,0-2.09,1,1.61,1.61,0,0,0-.09.54,1.65,1.65,0,0,0,3.29,0,1.68,1.68,0,0,0-.26-.89l12-12ZM36.58,88.79a.57.57,0,1,1,.57-.56A.57.57,0,0,1,36.58,88.79Z" style="fill:#c5c9e0"></path><path d="M110.61,95.55,92.8,113.4a1.62,1.62,0,1,0,.77.76l17.49-17.53h23.31V95.55ZM92.49,115.28a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.57-.57h0a.58.58,0,0,1,.56.58A.55.55,0,0,1,92.49,115.28Z" style="fill:#c5c9e0"></path><path d="M97.89,122.3H76.62L64.2,109.85a1.65,1.65,0,0,0-.77-2.2,1.77,1.77,0,0,0-.72-.17h-.14a1.65,1.65,0,0,0,.15,3.29,1.58,1.58,0,0,0,.71-.17l12.74,12.77H98.34l17.48-17.52h18.54v-1.08h-19ZM63.12,109.53a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,1.14,0A.54.54,0,0,1,63.12,109.53Z" style="fill:#c5c9e0"></path> </svg> </div>
    <p>本文发布于<a href="https://www.oomkill.com/about" target="_blank">Cylon的收藏册</a>，转载请著名原文链接~</p>
    <p>链接：<a href="https://www.oomkill.com/2022/06/decision-tree/" target="_blank">https://www.oomkill.com/2022/06/decision-tree/</a></p>
    <p style="margin-bottom: 0px;">版权：本作品采用<a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">「署名-非商业性使用-相同方式共享 4.0 国际」</a> 许可协议进行许可。</p>
    </div>
</div>
  </div>

  <footer class="post-footer">
    
<nav class="paginav">
  <a class="prev" href="https://www.oomkill.com/2022/06/decision-boundary/">
    <span class="title"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select: text;"><line x1="19" y1="12" x2="5" y2="12" style="user-select: text;"></line><polyline points="12 19 5 12 12 5" style="user-select: text;"></polyline>
      </polyline></svg>&nbsp; </span>
    
    <span>决策边界算法</span>
  </a>
  <a class="next" href="https://www.oomkill.com/2022/06/logistic-regression/" >
    <span class="title"> </span>
    
    <span>逻辑回归&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select: text;"><line x1="5" y1="12" x2="19" y2="12" style="user-select: text;"></line><polyline points="12 5 19 12 12 19" style="user-select: text;"></polyline></svg></span>
  </a>
</nav>

  </footer>

  
  <div class="pagination__title">
    <span class="pagination__title-h"></span>
  </div>
  
  
  
  
    <div class="comments-separator"></div>
    

<h3 class="relatedContentTitle" >相关阅读</h3>
<ul class="relatedContent">
	
	<li><a href="/2022/06/knn/"><span>KNN算法</span></a></li>
	
	<li><a href="/2022/06/decision-boundary/"><span>决策边界算法</span></a></li>
	
	<li><a href="/2022/06/naive-bayes/"><span>朴素贝叶斯算法</span></a></li>
	
	<li><a href="/2022/06/logistic-regression/"><span>逻辑回归</span></a></li>
	
	<li><a href="/2016/09/consistent-hash/"><span>一致性hash在memcache中的应用</span></a></li>
	
</ul>

  

  
    
      <div class="comments-separator"></div>
<div class="comments">
    <script>
    function loadComment() {
        let theme = localStorage.getItem('pref-theme') === 'dark' ? 'dark' : 'light';
        let s = document.createElement('script');
        s.src = 'https://giscus.app/client.js';
        s.setAttribute('data-repo', 'cylonchau\/cylonchau.github.io');
        s.setAttribute('data-repo-id', 'R_kgDOIRlNSQ');
        s.setAttribute('data-category', 'Announcements');
        s.setAttribute('data-category-id', 'DIC_kwDOIRlNSc4CXy1U');
        s.setAttribute('data-mapping', 'title');
        s.setAttribute('data-reactions-enabled', '1');
        s.setAttribute('data-emit-metadata', '1');
        s.setAttribute('data-input-position', 'top');
        s.setAttribute('data-lang', 'zh-TW');
        s.setAttribute('data-theme', theme);
        s.setAttribute('crossorigin', 'anonymous');
        s.setAttribute('async', '');
        document.querySelector('div.comments').innerHTML = '';
        document.querySelector('div.comments').appendChild(s);
    }
    loadComment();
    </script>
</div>
</article>
    </main>
    
<footer class="footer">
  <p>
  Copyright
  <span>&copy; 2024 <a href="https://www.oomkill.com">Cylon&#39;s Collection</a></span></p>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> on github-page & Theme
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '1' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>

<script>
  document.addEventListener('scroll', function (e) {
      const readProgress = document.getElementById("read_progress");
      const scrollHeight = document.documentElement.scrollHeight;
      const clientHeight = document.documentElement.clientHeight;
      const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
      readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
  })
</script>

<script>
  var menu = document.getElementById('menu')
  if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
          localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
  }

  document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
          e.preventDefault();
          var id = this.getAttribute("href").substr(1);
          if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
              document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                  behavior: "smooth"
              });
          } else {
              document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
          }
          if (id === "top") {
              history.replaceState(null, null, " ");
          } else {
              history.pushState(null, null, `#${id}`);
          }
      });
  });
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
      if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
          mybutton.style.visibility = "visible";
          mybutton.style.opacity = "1";
      } else {
          mybutton.style.visibility = "hidden";
          mybutton.style.opacity = "0";
      }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'copy';

    function copyingDone() {
      copybutton.innerText = 'copied';
      setTimeout(() => {
        copybutton.innerText = 'copy';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

<script src="/js/instantclick.min.js" data-no-instant
></script>
<script data-no-instant>
  
  
  
  
  
  
  InstantClick.init();
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.6.0/mermaid.min.js" crossorigin="anonymous"></script>
<script>
    mermaid.init(undefined, '.language-mermaid');
</script>
</body>

</html>
