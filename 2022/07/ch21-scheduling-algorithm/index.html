<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>如何理解kubernetes调度框架与插件？ | Cylon&#39;s Collection</title>
<meta name="keywords" content="kubernetes, develop">
<meta name="description" content="如何理解kubernetes调度框架与插件？ - Cylon&#39;s Collection">
<meta name="author" content="cylon">
<link rel="canonical" href="https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.41a8706089174fae1769fc26da4d1d354fa88083db604a95688ff58852dd9006.css" integrity="sha256-QahwYIkXT64Xafwm2k0dNU&#43;ogIPbYEqVaI/1iFLdkAY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.oomkill.com/favicon.ico">
<link rel="apple-touch-icon" href="https://www.oomkill.com/apple-touch-icon.png">

<meta name="twitter:title" content="如何理解kubernetes调度框架与插件？ | Cylon&#39;s Collection" />
<meta name="twitter:description" content="" />
<meta property="og:title" content="如何理解kubernetes调度框架与插件？ | Cylon&#39;s Collection" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2022-07-27T00:00:00&#43;00:00" />
  <meta property="article:modified_time" content="2022-07-27T00:00:00&#43;00:00" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://www.oomkill.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "如何理解kubernetes调度框架与插件？",
      "item": "https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "如何理解kubernetes调度框架与插件？ | Cylon's Collection",
  "name": "如何理解kubernetes调度框架与插件？",
  "description": "",
  "keywords": [
    "kubernetes", "develop"
  ],
  "wordCount" : "12395",
  "inLanguage": "zh",
  "datePublished": "2022-07-27T00:00:00Z",
  "dateModified": "2022-07-27T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "cylon"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Cylon's Collection",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.oomkill.com/favicon.ico"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary-bg: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list-page {
                background: var(--theme);
            }

            .list-page:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list-page:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

</head>

<body class=" type-posts kind-page layout-" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'auto';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.oomkill.com" accesskey="h" title="Cylon&#39;s Collection (Alt + H)">Cylon&#39;s Collection</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.oomkill.com/archives/" title="归档"
                >归档
                </a>
            </li>
            <li>
                <a href="https://www.oomkill.com/tags/" title="标签"
                >标签
                </a>
            </li>
            <li>
                <a href="https://www.oomkill.com/search/" title="搜索 (Alt &#43; /)"data-no-instant accesskey=/
                >搜索
                </a>
            </li>
            <li>
                <a href="https://www.oomkill.com/about/" title="关于"
                >关于
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header"><h1 class="post-title">如何理解kubernetes调度框架与插件？</h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>2022-07-27</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>Edited on 2022-07-27</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select: text;"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z" style="user-select: text;"></path><line x1="7" y1="7" x2="7" y2="7" style="user-select: text;"></line></svg>
  <span class="post-tags"><a href="https://www.oomkill.com/tags/kubernetes-develop/">kubernetes develop</a><a href="https://www.oomkill.com/tags/kubernetes/">kubernetes</a></span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>25 分钟</span></span>

      
      
    </div>
  </header> <div class="toc side right">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e8%b0%83%e5%ba%a6%e6%a1%86%e6%9e%b6-supa-href11asup" aria-label="调度框架 [1]">调度框架 <sup><a href="#1">[1]</a></sup></a></li>
                <li>
                    <a href="#%e6%8f%92%e4%bb%b6-supa-href44asup" aria-label="插件 [4]">插件 <sup><a href="#4">[4]</a></sup></a><ul>
                        
                <li>
                    <a href="#%e6%8f%92%e4%bb%b6%e7%9a%84%e8%bd%bd%e5%85%a5%e8%bf%87%e7%a8%8b" aria-label="插件的载入过程">插件的载入过程</a><ul>
                        
                <li>
                    <a href="#newscheduler" aria-label="NewScheduler">NewScheduler</a></li>
                <li>
                    <a href="#profilenewmap" aria-label="profile.NewMap">profile.NewMap</a></li>
                <li>
                    <a href="#newframework" aria-label="NewFramework">NewFramework</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%8f%92%e4%bb%b6%e7%9a%84%e6%89%a7%e8%a1%8c" aria-label="插件的执行">插件的执行</a><ul>
                        
                <li>
                    <a href="#nodeports" aria-label="NodePorts">NodePorts</a></li>
                <li>
                    <a href="#noderesourcesfit--supa-href55asup" aria-label="NodeResourcesFit [5]">NodeResourcesFit  <sup><a href="#5">[5]</a></sup></a></li>
                <li>
                    <a href="#fit" aria-label="Fit">Fit</a></li>
                <li>
                    <a href="#leastallocate" aria-label="leastAllocate">leastAllocate</a></li></ul>
                </li>
                <li>
                    <a href="#topology-supa-href66asup" aria-label="Topology [6]">Topology <sup><a href="#6">[6]</a></sup></a><ul>
                        
                <li>
                    <a href="#concept" aria-label="Concept">Concept</a></li>
                <li>
                    <a href="#%e5%ae%9a%e4%b9%89%e4%b8%80%e4%b8%aatopology" aria-label="定义一个Topology">定义一个Topology</a></li>
                <li>
                    <a href="#%e5%af%b9%e4%ba%8e%e6%8b%93%e6%89%91%e5%9f%9f%e7%9a%84%e7%90%86%e8%a7%a3" aria-label="对于拓扑域的理解">对于拓扑域的理解</a></li>
                <li>
                    <a href="#%e5%a6%82%e4%bd%95%e4%b8%ba%e9%9b%86%e7%be%a4%e8%ae%be%e7%bd%ae%e4%b8%80%e4%b8%aa%e9%bb%98%e8%ae%a4%e6%8b%93%e6%89%91%e5%9f%9f%e7%ba%a6%e6%9d%9f" aria-label="如何为集群设置一个默认拓扑域约束">如何为集群设置一个默认拓扑域约束</a></li>
                <li>
                    <a href="#%e9%bb%98%e8%ae%a4%e7%ba%a6%e6%9d%9f%e7%ad%96%e7%95%a5" aria-label="默认约束策略">默认约束策略</a></li></ul>
                </li>
                <li>
                    <a href="#%e9%80%9a%e8%bf%87%e6%ba%90%e7%a0%81%e5%ad%a6%e4%b9%a0topology" aria-label="通过源码学习Topology">通过源码学习Topology</a><ul>
                        
                <li>
                    <a href="#prefilter" aria-label="PreFilter">PreFilter</a></li>
                <li>
                    <a href="#filter" aria-label="Filter">Filter</a></li>
                <li>
                    <a href="#prescore" aria-label="PreScore">PreScore</a></li>
                <li>
                    <a href="#score" aria-label="Score">Score</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content">
    





<div class="copyrightTopBlock">
    <p>本文发布于<a href="https://www.oomkill.com/about" target="_blank">Cylon的收藏册</a>，转载请著名原文链接~</p>
    <div class="articleSuffix-bg"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 147.78 155.96"> <path d="M10.5,99.81a1.9,1.9,0,0,0-.53-.09,1.66,1.66,0,0,0-1.64,1.65A1.64,1.64,0,0,0,10,103a1.57,1.57,0,0,0,.87-.25l26.76,26.82.45-1.08L11.52,101.91A1.65,1.65,0,0,0,10.5,99.81Zm-.13,2a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.58.58,0,0,1,.57-.57h0a.57.57,0,0,1,.56.58A.55.55,0,0,1,10.37,101.77Z" style="fill:#c5c9e0"></path><path d="M56.15,117.58H39.06l0-.09a1.65,1.65,0,0,0-1.36-1H37.5a1.65,1.65,0,1,0,1.56,2.19H55.7L92.92,156h41.44v-1.08h-41Zm-18.25.94a.56.56,0,0,1-.79,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h0a.58.58,0,0,1,.57.58A.54.54,0,0,1,37.9,118.52Z" style="fill:#c5c9e0"></path><path d="M23.52,50.32a1.65,1.65,0,0,0,1.55-1.11H55.28l48-48.13h31.06V0H102.85l-48,48.13H25.07a1.64,1.64,0,0,0-2.09-1,1.64,1.64,0,0,0,.54,3.2Zm0-2.21a.57.57,0,0,1,0,1.13.57.57,0,1,1,0-1.13Z" style="fill:#c5c9e0"></path><polygon points="102.86 0 102.86 0 102.86 0 102.86 0" style="fill:#c5c9e0"></polygon><path d="M107.72,12.14h26.64V11.07H107.27L57.4,61H3.09a1.66,1.66,0,0,0-1.45-.86H1.52A1.65,1.65,0,1,0,2.81,63a1.59,1.59,0,0,0,.45-.87H57.85ZM2.05,62.23a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h.09a.57.57,0,0,1,.32,1Z" style="fill:#c5c9e0"></path><path d="M134.36,43.22V42.14h-22.3l-9.62,9.63a1.64,1.64,0,0,0-2.19.77,1.61,1.61,0,0,0-.17.71,1.65,1.65,0,1,0,3.29,0,1.61,1.61,0,0,0-.16-.72l9.3-9.32Zm-32.64,10.6a.57.57,0,0,1,0-1.13.57.57,0,0,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M147,52.3l-9,9H111.48a1.64,1.64,0,0,0-1.61-1.33h-.14a1.65,1.65,0,1,0,1.6,2.41h27.19l9.26-9.29L147,52.3Zm-37.15,9.85a.56.56,0,0,1-.56-.57h0a.56.56,0,0,1,.56-.56h0a.57.57,0,1,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M66.79,75.35l11,11.06h56.53V85.33H78.27l-11-11.06H49.49L37.12,86.67a1.64,1.64,0,0,0-2.09,1,1.61,1.61,0,0,0-.09.54,1.65,1.65,0,0,0,3.29,0,1.68,1.68,0,0,0-.26-.89l12-12ZM36.58,88.79a.57.57,0,1,1,.57-.56A.57.57,0,0,1,36.58,88.79Z" style="fill:#c5c9e0"></path><path d="M110.61,95.55,92.8,113.4a1.62,1.62,0,1,0,.77.76l17.49-17.53h23.31V95.55ZM92.49,115.28a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.57-.57h0a.58.58,0,0,1,.56.58A.55.55,0,0,1,92.49,115.28Z" style="fill:#c5c9e0"></path><path d="M97.89,122.3H76.62L64.2,109.85a1.65,1.65,0,0,0-.77-2.2,1.77,1.77,0,0,0-.72-.17h-.14a1.65,1.65,0,0,0,.15,3.29,1.58,1.58,0,0,0,.71-.17l12.74,12.77H98.34l17.48-17.52h18.54v-1.08h-19ZM63.12,109.53a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,1.14,0A.54.54,0,0,1,63.12,109.53Z" style="fill:#c5c9e0"></path> </svg> </div>
</div>
<br><h2 id="调度框架-supa-href11asup">调度框架 <sup><a href="#1">[1]</a></sup><a hidden class="anchor" aria-hidden="true" href="#调度框架-supa-href11asup">¶</a></h2>
<p>本文基于 <a href="https://github.com/kubernetes/kubernetes/tree/release-1.24/pkg/scheduler" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes 1.24</a> 进行分析</p>
<p>调度框架（<code>Scheduling Framework</code>）是Kubernetes 的调度器 <code>kube-scheduler</code> 设计的的可插拔架构，将插件（调度算法）嵌入到调度上下文的每个扩展点中，并编译为 <code>kube-scheduler</code></p>
<p>在 <code>kube-scheduler</code> 1.22 之后，在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/interface.go" target="_blank"
   rel="noopener nofollow noreferrer" >pkg/scheduler/framework/interface.go</a> 中定义了一个 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/interface.go#L295-L297" target="_blank"
   rel="noopener nofollow noreferrer" >Plugin</a> 的 <em>interface</em>，这个 <em>interface</em> 作为了所有插件的父级。而每个未调度的 Pod，Kubernetes 调度器会根据一组规则尝试在集群中寻找一个节点。</p>
<pre><code class="language-go">type Plugin interface {
	Name() string
}
</code></pre>
<p>下面会对每个算法是如何实现的进行分析</p>
<p>在初始化 <em>scheduler</em> 时，会创建一个 <code>profile</code>，profile是关于 <em>scheduler</em> 调度配置相关的定义</p>
<pre><code class="language-go">func New(client clientset.Interface,
...
	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)
	if err != nil {
		return nil, fmt.Errorf(&quot;initializing profiles: %v&quot;, err)
	}

	if len(profiles) == 0 {
		return nil, errors.New(&quot;at least one profile is required&quot;)
	}
....
}
</code></pre>
<p>关于 <code>profile</code> 的实现，则为 <code>KubeSchedulerProfile</code>，也是作为 yaml生成时传入的配置</p>
<pre><code class="language-go">// KubeSchedulerProfile 是一个 scheduling profile.
type KubeSchedulerProfile struct {
	// SchedulerName 是与此配置文件关联的调度程序的名称。
    // 如果 SchedulerName 与 pod “spec.schedulerName”匹配，则使用此配置文件调度 pod。
	SchedulerName string

	// Plugins指定应该启用或禁用的插件集。
    // 启用的插件是除了默认插件之外应该启用的插件。禁用插件应是禁用的任何默认插件。
    // 当没有为扩展点指定启用或禁用插件时，将使用该扩展点的默认插件（如果有）。
    // 如果指定了 QueueSort 插件，
    // 则必须为所有配置文件指定相同的 QueueSort Plugin 和 PluginConfig。
    // 这个Plugins展现的形式则是调度上下文中的所有扩展点(这是抽象)，实际中会表现为多个扩展点
	Plugins *Plugins

	// PluginConfig 是每个插件的一组可选的自定义插件参数。
    // 如果省略PluginConfig参数等同于使用该插件的默认配置。
	PluginConfig []PluginConfig
}
</code></pre>
<p>对于 <code>profile.NewMap</code> 就是根据给定的配置来构建这个framework，因为配置可能是存在多个的。而 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/registry.go#L70" target="_blank"
   rel="noopener nofollow noreferrer" >Registry</a> 则是所有可用插件的集合，内部构造则是 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/registry.go#L30" target="_blank"
   rel="noopener nofollow noreferrer" >PluginFactory</a> ,通过函数来构建出对应的 plugin</p>
<pre><code class="language-go">func NewMap(cfgs []config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory,
	stopCh &lt;-chan struct{}, opts ...frameworkruntime.Option) (Map, error) {
	m := make(Map)
	v := cfgValidator{m: m}

	for _, cfg := range cfgs {
		p, err := newProfile(cfg, r, recorderFact, stopCh, opts...)
		if err != nil {
			return nil, fmt.Errorf(&quot;creating profile for scheduler name %s: %v&quot;, cfg.SchedulerName, err)
		}
		if err := v.validate(cfg, p); err != nil {
			return nil, err
		}
		m[cfg.SchedulerName] = p
	}
	return m, nil
}

// newProfile 给的配置构建出一个profile
func newProfile(cfg config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory,
	stopCh &lt;-chan struct{}, opts ...frameworkruntime.Option) (framework.Framework, error) {
	recorder := recorderFact(cfg.SchedulerName)
	opts = append(opts, frameworkruntime.WithEventRecorder(recorder))
	return frameworkruntime.NewFramework(r, &amp;cfg, stopCh, opts...)
}

</code></pre>
<p>可以看到最终返回的是一个 <code>Framework</code> 。那么来看下这个 <code>Framework</code></p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/interface.go#L495-L566" target="_blank"
   rel="noopener nofollow noreferrer" >Framework</a> 是一个抽象，管理着调度过程中所使用的所有插件，并在调度上下文中适当的位置去运行对应的插件</p>
<pre><code class="language-go">type Framework interface {
	Handle
	// QueueSortFunc 返回对调度队列中的 Pod 进行排序的函数
    // 也就是less，在Sort打分阶段的打分函数
	QueueSortFunc() LessFunc
    
    // RunPreFilterPlugins 运行配置的一组PreFilter插件。
    // 如果这组插件中，任何一个插件失败，则返回 *Status 并设置为non-success。
    // 如果返回状态为non-success，则调度周期中止。
    // 它还返回一个 PreFilterResult，它可能会影响到要评估下游的节点。
    
	RunPreFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod) (*PreFilterResult, *Status)

    // RunPostFilterPlugins 运行配置的一组PostFilter插件。 
    // PostFilter 插件是通知性插件，在这种情况下应配置为先执行并返回 Unschedulable 状态，
    // 或者尝试更改集群状态以使 pod 在未来的调度周期中可能会被调度。
	RunPostFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, filteredNodeStatusMap NodeToStatusMap) (*PostFilterResult, *Status)

    // RunPreBindPlugins 运行配置的一组 PreBind 插件。
    // 如果任何一个插件返回错误，则返回 *Status 并且code设置为non-success。
    // 如果code为“Unschedulable”，则调度检查失败，
    // 则认为是内部错误。在任何一种情况下，Pod都不会被bound。
	RunPreBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

    // RunPostBindPlugins 运行配置的一组PostBind插件
	RunPostBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string)

    // RunReservePluginsReserve运行配置的一组Reserve插件的Reserve方法。
    // 如果在这组调用中的任何一个插件返回错误，则不会继续运行剩余调用的插件并返回错误。
    // 在这种情况下，pod将不能被调度。
	RunReservePluginsReserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

    // RunReservePluginsUnreserve运行配置的一组Reserve插件的Unreserve方法。
	RunReservePluginsUnreserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string)

    // RunPermitPlugins运行配置的一组Permit插件。
    // 如果这些插件中的任何一个返回“Success”或“Wait”之外的状态，则它不会继续运行其余插件并返回错误。
    // 否则，如果任何插件返回 “Wait”，则此函数将创建等待pod并将其添加到当前等待pod的map中，
    // 并使用“Wait” code返回状态。 Pod将在Permit插件返回的最短持续时间内保持等待pod。
	RunPermitPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

    // 如果pod是waiting pod，WaitOnPermit 将阻塞，直到等待的pod被允许或拒绝。
	WaitOnPermit(ctx context.Context, pod *v1.Pod) *Status

    // RunBindPlugins运行配置的一组bind插件。 Bind插件可以选择是否处理Pod。
    // 如果 Bind 插件选择跳过binding，它应该返回 code=5(&quot;skip&quot;)状态。
    // 否则，它应该返回“Error”或“Success”。
    // 如果没有插件处理绑定，则RunBindPlugins返回code=5(&quot;skip&quot;)的状态。
	RunBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

	// 如果至少定义了一个filter插件，则HasFilterPlugins返回true
	HasFilterPlugins() bool

    // 如果至少定义了一个PostFilter插件，则HasPostFilterPlugins返回 true。
	HasPostFilterPlugins() bool

	// 如果至少定义了一个Score插件，则HasScorePlugins返回 true。
	HasScorePlugins() bool

    // ListPlugins将返回map。key为扩展点名称，value则是配置的插件列表。
	ListPlugins() *config.Plugins

    // ProfileName则是与profile name关联的framework
	ProfileName() string
}
</code></pre>
<p>而实现这个抽象的则是 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/framework.go#L73-L102" target="_blank"
   rel="noopener nofollow noreferrer" >frameworkImpl</a>；<code>frameworkImpl</code> 是初始化与运行 <em>scheduler plugins</em> 的组件，并在调度上下文中会运行这些扩展点</p>
<pre><code>type frameworkImpl struct {
   registry             Registry
   snapshotSharedLister framework.SharedLister
   waitingPods          *waitingPodsMap
   scorePluginWeight    map[string]int
   queueSortPlugins     []framework.QueueSortPlugin
   preFilterPlugins     []framework.PreFilterPlugin
   filterPlugins        []framework.FilterPlugin
   postFilterPlugins    []framework.PostFilterPlugin
   preScorePlugins      []framework.PreScorePlugin
   scorePlugins         []framework.ScorePlugin
   reservePlugins       []framework.ReservePlugin
   preBindPlugins       []framework.PreBindPlugin
   bindPlugins          []framework.BindPlugin
   postBindPlugins      []framework.PostBindPlugin
   permitPlugins        []framework.PermitPlugin

   clientSet       clientset.Interface
   kubeConfig      *restclient.Config
   eventRecorder   events.EventRecorder
   informerFactory informers.SharedInformerFactory

   metricsRecorder *metricsRecorder
   profileName     string

   extenders []framework.Extender
   framework.PodNominator

   parallelizer parallelize.Parallelizer
}
</code></pre>
<p>那么来看下 Registry ，<code>Registry </code> 是作为一个可用插件的集合。<code>framework</code> 使用 <code>registry</code> 来启用和对插件配置的初始化。在初始化框架之前，所有插件都必须在注册表中。表现形式就是一个 <code>map[]</code>；<em>key</em> 是插件的名称，value是 <code>PluginFactory</code> 。</p>
<pre><code class="language-go">type Registry map[string]PluginFactory
</code></pre>
<p>而在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/plugins/registry.go" target="_blank"
   rel="noopener nofollow noreferrer" >pkg\scheduler\framework\plugins\registry.go</a> 中会将所有的 <code>in-tree plugin</code> 注册进来。通过 <code>NewInTreeRegistry</code> 。后续如果还有插件要注册，可以通过 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/scheduler.go#L176-L180" target="_blank"
   rel="noopener nofollow noreferrer" >WithFrameworkOutOfTreeRegistry</a>  来注册其他的插件。</p>
<pre><code class="language-go">func NewInTreeRegistry() runtime.Registry {
	fts := plfeature.Features{
		EnableReadWriteOncePod:                       feature.DefaultFeatureGate.Enabled(features.ReadWriteOncePod),
		EnableVolumeCapacityPriority:                 feature.DefaultFeatureGate.Enabled(features.VolumeCapacityPriority),
		EnableMinDomainsInPodTopologySpread:          feature.DefaultFeatureGate.Enabled(features.MinDomainsInPodTopologySpread),
		EnableNodeInclusionPolicyInPodTopologySpread: feature.DefaultFeatureGate.Enabled(features.NodeInclusionPolicyInPodTopologySpread),
	}

	return runtime.Registry{
		selectorspread.Name:                  selectorspread.New,
		imagelocality.Name:                   imagelocality.New,
		tainttoleration.Name:                 tainttoleration.New,
		nodename.Name:                        nodename.New,
		nodeports.Name:                       nodeports.New,
		nodeaffinity.Name:                    nodeaffinity.New,
		podtopologyspread.Name:               runtime.FactoryAdapter(fts, podtopologyspread.New),
		nodeunschedulable.Name:               nodeunschedulable.New,
		noderesources.Name:                   runtime.FactoryAdapter(fts, noderesources.NewFit),
		noderesources.BalancedAllocationName: runtime.FactoryAdapter(fts, noderesources.NewBalancedAllocation),
		volumebinding.Name:                   runtime.FactoryAdapter(fts, volumebinding.New),
		volumerestrictions.Name:              runtime.FactoryAdapter(fts, volumerestrictions.New),
		volumezone.Name:                      volumezone.New,
		nodevolumelimits.CSIName:             runtime.FactoryAdapter(fts, nodevolumelimits.NewCSI),
		nodevolumelimits.EBSName:             runtime.FactoryAdapter(fts, nodevolumelimits.NewEBS),
		nodevolumelimits.GCEPDName:           runtime.FactoryAdapter(fts, nodevolumelimits.NewGCEPD),
		nodevolumelimits.AzureDiskName:       runtime.FactoryAdapter(fts, nodevolumelimits.NewAzureDisk),
		nodevolumelimits.CinderName:          runtime.FactoryAdapter(fts, nodevolumelimits.NewCinder),
		interpodaffinity.Name:                interpodaffinity.New,
		queuesort.Name:                       queuesort.New,
		defaultbinder.Name:                   defaultbinder.New,
		defaultpreemption.Name:               runtime.FactoryAdapter(fts, defaultpreemption.New),
	}
}
</code></pre>
<blockquote>
<p>这里插入一个题外话，关于 <em>in-tree plugin</em></p>
<p>在这里没有找到关于，<em>kube-scheduler</em> ，只是找到有关的概念，大概可以解释为，in-tree表示为随kubernetes官方提供的二进制构建的 <em>plugin</em> 则为 <code>in-tree</code>，而独立于kubernetes代码库之外的为 <code>out-of-tree</code> <sup><a href="#3">[3]</a></sup> 。这种情况下，可以理解为，AA则是 <code>out-of-tree</code> 而 <code>Pod</code>, <code>DeplymentSet</code> 等是 <code>in-tree</code>。</p>
</blockquote>
<p>接下来回到初始化 <em>scheduler</em> ，在初始化一个 <em>scheduler</em> 时，会通过<code>NewInTreeRegistry</code> 来初始化</p>
<pre><code class="language-go">func New(client clientset.Interface,
	....
	registry := frameworkplugins.NewInTreeRegistry()
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}
         
	...

	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)
	...
}
</code></pre>
<p>接下来在调度上下文 <code>scheduleOne</code> 中 <code>schedulePod</code> 时，会通过 <code>framework</code> 调用对应的插件来处理这个扩展点工作。具体的体现在，pkg\scheduler\schedule_one.go 中的预选阶段</p>
<pre><code class="language-go">func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New(&quot;Scheduling&quot;, utiltrace.Field{Key: &quot;namespace&quot;, Value: pod.Namespace}, utiltrace.Field{Key: &quot;name&quot;, Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)

	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
		return result, err
	}
	trace.Step(&quot;Snapshotting scheduler cache and node infos done&quot;)

	if sched.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}

	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step(&quot;Computing predicates done&quot;)

</code></pre>
<p>与其他扩展点部分，在调度上下文 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L66-L278" target="_blank"
   rel="noopener nofollow noreferrer" >scheduleOne</a> 中可以很好的看出，功能都是 <code>framework</code> 提供的。</p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {

    ...
    
	scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)

    ...
    
	// Run the Reserve method of reserve plugins.
	if sts := fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {
	}

    ...
    
	// Run &quot;permit&quot; plugins.
	runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
	
		// One of the plugins returned status different than success or wait.
		fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)

...
    
	// bind the pod to its host asynchronously (we can do this b/c of the assumption step above).
	go func() {
		...
		waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod)
		if !waitOnPermitStatus.IsSuccess() {
			...
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		}

		// Run &quot;prebind&quot; plugins.
		preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		
        ...
        
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
	
        ...

		...
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			
        ...

		// Run &quot;postbind&quot; plugins.
		fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)

	...
}
</code></pre>
<h2 id="插件-supa-href44asup">插件 <sup><a href="#4">[4]</a></sup><a hidden class="anchor" aria-hidden="true" href="#插件-supa-href44asup">¶</a></h2>
<p>插件（<code>Plugins</code>）（也可以算是调度策略）在 <code>kube-scheduler</code> 中的实现为 <code>framework plugin</code>，插件API的实现分为两个步骤**：register** 和 <strong>configured</strong>，然后都实现了其父方法 <code>Plugin</code>。然后可以通过配置（kube-scheduler <code>--config</code> 提供）启动或禁用插件；除了默认插件外，还可以实现自定义调度插件与默认插件进行绑定。</p>
<pre><code class="language-go">type Plugin interface {
    Name() string
}
// sort扩展点
type QueueSortPlugin interface {
    Plugin
    Less(*v1.pod, *v1.pod) bool
}
// PreFilter扩展点
type PreFilterPlugin interface {
    Plugin
    PreFilter(context.Context, *framework.CycleState, *v1.pod) error
}

</code></pre>
<h3 id="插件的载入过程">插件的载入过程<a hidden class="anchor" aria-hidden="true" href="#插件的载入过程">¶</a></h3>
<p>在 <em>scheduler</em> 被启动时，会 <code>scheduler.New(cc.Client..</code> 这个时候会传入 <code>profiles</code>，整个的流如下：</p>
<ul>
<li><code>NewScheduler</code> ：<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/cmd/kube-scheduler/app/server.go#L327-L346" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/cmd/kube-scheduler/app/server.go</a></li>
<li><code>profile.NewMap</code>：<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/profile/profile.go#L48-L64" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/pkg/scheduler/scheduler.go</a>
<ul>
<li><code>newProfile</code>：<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/profile/profile.go#L37-L42" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/pkg/scheduler/scheduler.go</a></li>
</ul>
</li>
<li><code>frameworkruntime.NewFramework</code>：<a href="kubernetes/pkg/scheduler/framework/runtime/framework.go">kubernetes/pkg/scheduler/framework/runtime/framework.go</a>
<ul>
<li><code>pluginsNeeded</code>：<a href="kubernetes/pkg/scheduler/framework/runtime/framework.go">kubernetes/pkg/scheduler/framework/runtime/framework.go</a></li>
</ul>
</li>
</ul>
<h4 id="newscheduler">NewScheduler<a hidden class="anchor" aria-hidden="true" href="#newscheduler">¶</a></h4>
<p>我们了解如何 New 一个 <em>scheduler</em> 即为 <code>Setup</code> 中去配置这些参数，</p>
<pre><code class="language-go">func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) {

    ...
    
	// Create the scheduler.
	sched, err := scheduler.New(cc.Client,
		cc.InformerFactory,
		cc.DynInformerFactory,
		recorderFactory,
		ctx.Done(),
		scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion),
		scheduler.WithKubeConfig(cc.KubeConfig),
		scheduler.WithProfiles(cc.ComponentConfig.Profiles...),
		scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore),
		scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry),
		scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds),
		scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds),
		scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration),
		scheduler.WithExtenders(cc.ComponentConfig.Extenders...),
		scheduler.WithParallelism(cc.ComponentConfig.Parallelism),
		scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) {
			// Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging
			completedProfiles = append(completedProfiles, profile)
		}),
	)
    ...
}
</code></pre>
<h4 id="profilenewmap">profile.NewMap<a hidden class="anchor" aria-hidden="true" href="#profilenewmap">¶</a></h4>
<p>在 <code>scheduler.New</code> 中，会根据配置生成profile，而 <code>profile.NewMap</code> 会完成这一步</p>
<pre><code class="language-go">func New(client clientset.Interface,
	...
         
	clusterEventMap := make(map[framework.ClusterEvent]sets.String)

	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)

         ...
}
</code></pre>
<h4 id="newframework">NewFramework<a hidden class="anchor" aria-hidden="true" href="#newframework">¶</a></h4>
<p><code>newProfile</code> 返回的则是一个创建好的 framework</p>
<pre><code class="language-go">func newProfile(cfg config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory,
	stopCh &lt;-chan struct{}, opts ...frameworkruntime.Option) (framework.Framework, error) {
	recorder := recorderFact(cfg.SchedulerName)
	opts = append(opts, frameworkruntime.WithEventRecorder(recorder))
	return frameworkruntime.NewFramework(r, &amp;cfg, stopCh, opts...)
}
</code></pre>
<p>最终会走到 <code>pluginsNeeded</code>，这里会根据配置中开启的插件而返回一个插件集，这个就是最终在每个扩展点中药执行的插件。</p>
<pre><code class="language-go">func (f *frameworkImpl) pluginsNeeded(plugins *config.Plugins) sets.String {
	pgSet := sets.String{}

	if plugins == nil {
		return pgSet
	}

	find := func(pgs *config.PluginSet) {
		for _, pg := range pgs.Enabled {
			pgSet.Insert(pg.Name)
		}
	}
	// 获取到所有的扩展点，找到为Enabled的插件加入到pgSet
	for _, e := range f.getExtensionPoints(plugins) {
		find(e.plugins)
	}
	// Parse MultiPoint separately since they are not returned by f.getExtensionPoints()
	find(&amp;plugins.MultiPoint)

	return pgSet
}
</code></pre>
<h3 id="插件的执行">插件的执行<a hidden class="anchor" aria-hidden="true" href="#插件的执行">¶</a></h3>
<p>在对插件源码部分分析，会找几个典型的插件进行分析，而不会对全部的进行分析，因为总的来说是大同小异，分析的插件有 <code>NodePorts</code>，<code>NodeResourcesFit</code>，<code>podtopologyspread</code></p>
<h4 id="nodeports">NodePorts<a hidden class="anchor" aria-hidden="true" href="#nodeports">¶</a></h4>
<p>这里以一个简单的插件来分析；<code>NodePorts</code> 插件用于检查Pod请求的端口，在节点上是否为空闲端口。</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L30" target="_blank"
   rel="noopener nofollow noreferrer" >NodePorts</a> 实现了 <code>FilterPlugin</code> 和 <code>PreFilterPlugin</code></p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L77-L81" target="_blank"
   rel="noopener nofollow noreferrer" >PreFilter</a>  将会被 <code>framework</code> 中 <code>PreFilter</code> 扩展点被调用。</p>
<pre><code class="language-go">func (pl *NodePorts) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	s := getContainerPorts(pod) // 或得Pod得端口
    // 写入状态
	cycleState.Write(preFilterStateKey, preFilterState(s))
	return nil, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L113-L125" target="_blank"
   rel="noopener nofollow noreferrer" >Filter</a> 将会被 <code>framework</code> 中 <code>Filter</code> 扩展点被调用。</p>
<pre><code class="language-go">// Filter invoked at the filter extension point.
func (pl *NodePorts) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
   wantPorts, err := getPreFilterState(cycleState)
   if err != nil {
      return framework.AsStatus(err)
   }

   fits := fitsPorts(wantPorts, nodeInfo)
   if !fits {
      return framework.NewStatus(framework.Unschedulable, ErrReason)
   }

   return nil
}

func fitsPorts(wantPorts []*v1.ContainerPort, nodeInfo *framework.NodeInfo) bool {
	// 对比existingPorts 和 wantPorts是否冲突，冲突则调度失败
	existingPorts := nodeInfo.UsedPorts
	for _, cp := range wantPorts {
		if existingPorts.CheckConflict(cp.HostIP, string(cp.Protocol), cp.HostPort) {
			return false
		}
	}
	return true
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L144-L146" target="_blank"
   rel="noopener nofollow noreferrer" >New</a> ，初始化新插件，在 <code>register</code> 中注册得</p>
<pre><code class="language-go">func New(_ runtime.Object, _ framework.Handle) (framework.Plugin, error) {
	return &amp;NodePorts{}, nil
}
</code></pre>
<p>在调用中，如果有任何一个插件返回错误，则跳过该扩展点注册得其他插件，返回失败。</p>
<pre><code class="language-go">func (f *frameworkImpl) RunFilterPlugins(
	ctx context.Context,
	state *framework.CycleState,
	pod *v1.Pod,
	nodeInfo *framework.NodeInfo,
) framework.PluginToStatus {
	statuses := make(framework.PluginToStatus)
	for _, pl := range f.filterPlugins {
		pluginStatus := f.runFilterPlugin(ctx, pl, state, pod, nodeInfo)
		if !pluginStatus.IsSuccess() {
			if !pluginStatus.IsUnschedulable() 
				errStatus := framework.AsStatus(fmt.Errorf(&quot;running %q filter plugin: %w&quot;, pl.Name(), pluginStatus.AsError())).WithFailedPlugin(pl.Name())
				return map[string]*framework.Status{pl.Name(): errStatus}
			}
			pluginStatus.SetFailedPlugin(pl.Name())
			statuses[pl.Name()] = pluginStatus
		}
	}

	return statuses
}
</code></pre>
<p>返回得状态是一个 Status 结构体，该结构体表示了插件运行的结果。由 <code>Code</code>、<code>reasons</code>、（可选）<code>err</code> 和 <code>failedPlugin</code> （失败的那个插件名）组成。当 <em>code</em> 不是 <code>Success</code> 时，应说明原因。而且，当 <em>code</em> 为 <code>Success</code> 时，其他所有字段都应为空。<code>nil</code> 状态也被视为成功。</p>
<pre><code class="language-go">type Status struct {
	code    Code
	reasons []string
	err     error
	// failedPlugin is an optional field that records the plugin name a Pod failed by.
	// It's set by the framework when code is Error, Unschedulable or UnschedulableAndUnresolvable.
	failedPlugin string
}
</code></pre>
<h4 id="noderesourcesfit--supa-href55asup">NodeResourcesFit  <sup><a href="#5">[5]</a></sup><a hidden class="anchor" aria-hidden="true" href="#noderesourcesfit--supa-href55asup">¶</a></h4>
<p><code>NodeResourcesFit</code> 扩展检查节点是否拥有 Pod 请求的所有资源。分数可以使用以下三种策略之一，扩展点为：<code>preFilter</code>， <code>filter</code>，<code>score</code></p>
<ul>
<li><code>LeastAllocated</code> （默认）</li>
<li><code>MostAllocated</code></li>
<li><code>RequestedToCapacityRatio</code></li>
</ul>
<h4 id="fit">Fit<a hidden class="anchor" aria-hidden="true" href="#fit">¶</a></h4>
<p><code>NodeResourcesFit  </code> PreFilter 可以看到调用得 <code>computePodResourceRequest</code></p>
<pre><code>// PreFilter invoked at the prefilter extension point.
func (f *Fit) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
   cycleState.Write(preFilterStateKey, computePodResourceRequest(pod))
   return nil, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/noderesources/fit.go#L133-L175" target="_blank"
   rel="noopener nofollow noreferrer" >computePodResourceRequest</a> 这里有一个注释，总体解释起来是这样得：<code>computePodResourceRequest</code> ，返回值（ <code>framework.Resource</code>）覆盖了每一个维度中资源的最大宽度。因为将按照 <code>init-containers</code> , <code>containers</code> 得顺序运行，会通过迭代方式收集每个维度中的最大值。计算时会对常规容器的资源向量求和，因为<code>containers</code> 运行会同时运行多个容器。计算示例为：</p>
<pre><code class="language-yaml">Pod:
  InitContainers
    IC1:
      CPU: 2
      Memory: 1G
    IC2:
      CPU: 2
      Memory: 3G
  Containers
    C1:
      CPU: 2
      Memory: 1G
    C2:
      CPU: 1
      Memory: 1G
</code></pre>
<p>在维度1中（<code>InitContainers</code>）所需资源最大值时，CPU=2, Memory=3G；而维度2（<code>Containers</code>）所需资源最大值为：CPU=2, Memory=1G；那么最终结果为 CPU=3, Memory=3G，因为在维度1，最大资源时Memory=3G；而维度2最大资源是CPU=1+2, Memory=1+1，取每个维度中最大资源最大宽度即为 CPU=3, Memory=3G。</p>
<p>下面则看下代码得实现</p>
<pre><code class="language-go">func computePodResourceRequest(pod *v1.Pod) *preFilterState {
	result := &amp;preFilterState{}
	for _, container := range pod.Spec.Containers {
		result.Add(container.Resources.Requests)
	}

	// 取最大得资源
	for _, container := range pod.Spec.InitContainers {
		result.SetMaxResource(container.Resources.Requests)
	}

	// 如果Overhead正在使用，需要将其计算到总资源中
	if pod.Spec.Overhead != nil {
		result.Add(pod.Spec.Overhead)
	}
	return result
}

// SetMaxResource 是比较ResourceList并为每个资源取最大值。
func (r *Resource) SetMaxResource(rl v1.ResourceList) {
	if r == nil {
		return
	}

	for rName, rQuantity := range rl {
		switch rName {
		case v1.ResourceMemory:
			r.Memory = max(r.Memory, rQuantity.Value())
		case v1.ResourceCPU:
			r.MilliCPU = max(r.MilliCPU, rQuantity.MilliValue())
		case v1.ResourceEphemeralStorage:
			if utilfeature.DefaultFeatureGate.Enabled(features.LocalStorageCapacityIsolation) {
				r.EphemeralStorage = max(r.EphemeralStorage, rQuantity.Value())
			}
		default:
			if schedutil.IsScalarResourceName(rName) {
				r.SetScalar(rName, max(r.ScalarResources[rName], rQuantity.Value()))
			}
		}
	}
}
</code></pre>
<h4 id="leastallocate">leastAllocate<a hidden class="anchor" aria-hidden="true" href="#leastallocate">¶</a></h4>
<p>LeastAllocated 是 NodeResourcesFit 的打分策略 ，<code>LeastAllocated</code> 打分的标准是更偏向于请求资源较少的Node。将会先计算出Node上调度的pod请求的内存、CPU与其他资源的百分比，然后并根据请求的比例与容量的平均值的最小值进行优先级排序。</p>
<p>计算公式是这样的：$\frac{\frac{cpu((capacity-requested) \times MaxNodeScore \times cpuWeight)}{capacity} + \frac{memory((capacity-requested) \times MaxNodeScore \times memoryWeight}{capacity}) + &hellip;}{weightSum}$</p>
<p>下面来看下实现</p>
<pre><code class="language-go">func leastResourceScorer(resToWeightMap resourceToWeightMap) func(resourceToValueMap, resourceToValueMap) int64 {
	return func(requested, allocable resourceToValueMap) int64 {
		var nodeScore, weightSum int64
		for resource := range requested {
			weight := resToWeightMap[resource]
            //  计算出的资源分数乘weight
			resourceScore := leastRequestedScore(requested[resource], allocable[resource])
			nodeScore += resourceScore * weight
			weightSum += weight
		}
		if weightSum == 0 {
			return 0
		}
        // 最终除weightSum
		return nodeScore / weightSum
	}
}
</code></pre>
<p>leastRequestedScore 计算标准为<strong>未使用容量</strong>的计算范围为 <code>0~MaxNodeScore</code>，0 为最低优先级，<code>MaxNodeScore</code> 为最高优先级。未使用的资源越多，得分越高。</p>
<pre><code class="language-go">func leastRequestedScore(requested, capacity int64) int64 {
	if capacity == 0 {
		return 0
	}
	if requested &gt; capacity {
		return 0
	}
	// 容量 - 请求的 x 预期值（100）/ 容量
	return ((capacity - requested) * int64(framework.MaxNodeScore)) / capacity
}
</code></pre>
<h3 id="topology-supa-href66asup">Topology <sup><a href="#6">[6]</a></sup><a hidden class="anchor" aria-hidden="true" href="#topology-supa-href66asup">¶</a></h3>
<h4 id="concept">Concept<a hidden class="anchor" aria-hidden="true" href="#concept">¶</a></h4>
<p>在对 <code>podtopologyspread</code> 插件进行分析前，先需要掌握Pod拓扑的概念。</p>
<p>Pod拓扑（<code>Pod Topology</code>）是Kubernetes Pod调度机制，可以将Pod分布在集群中不同 <code>Zone</code> ，以及用户自定义的各种拓扑域 （<code>topology domains</code>）。当有了拓扑域后，用户可以更高效的利用集群资源。</p>
<p>如何来解释拓扑域，首先需要提及为什么需要拓扑域，在集群有3个节点，并且当Pod副本数为2时，又不希望两个Pod在同一个Node上运行。在随着扩大Pod的规模，副本数扩展到到15个时，这时候最理想的方式是每个Node运行5个Pod，在这种背景下，用户希望对集群中Zone的安排为相似的副本数量，并且在集群存在部分问题时可以更好的自愈（也是按照相似的副本数量均匀的分布在Node上）。在这种情况下Kubernetes 提供了Pod 拓扑约束来解决这个问题。</p>
<h4 id="定义一个topology">定义一个Topology<a hidden class="anchor" aria-hidden="true" href="#定义一个topology">¶</a></h4>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  # Configure a topology spread constraint
  topologySpreadConstraints:
    - maxSkew: &lt;integer&gt; # 
      minDomains: &lt;integer&gt; # optional; alpha since v1.24
      topologyKey: &lt;string&gt;
      whenUnsatisfiable: &lt;string&gt;
      labelSelector: &lt;object&gt;
</code></pre>
<p><strong>参数的描述</strong>：</p>
<ul>
<li><strong>maxSkew</strong>：Required，Pod分布不均的程度，并且数字必须大于零
<ul>
<li>当 <code>whenUnsatisfiable: DoNotSchedule</code>，则定义目标拓扑中匹配 pod 的数量与 <strong>全局最小值</strong>（<em>拓扑域中的标签选择器匹配的 pod 的最小数量</em> ）<code>maxSkew</code>之间的最大允许差异。例如有 3 个 <code>Zone</code>，分别具有 2、4 和 5 个匹配的 pod，则全局最小值为 2</li>
<li>当 <code>whenUnsatisfiable: ScheduleAnyway</code>，<em>scheduler</em> 会为减少倾斜的拓扑提供更高的优先级。</li>
</ul>
</li>
<li><strong>minDomains</strong>：optional，符合条件的域的最小数量。
<ul>
<li>如果不指定该选项 <code>minDomains</code>，则约束的行为 <code>minDomains: 1</code> 。</li>
<li><code>minDomains</code>必须大于 0。<code>minDomains</code>与 <code>whenUnsatisfiable</code> 一起时为<code>whenUnsatisfiable: DoNotSchedule</code>。</li>
</ul>
</li>
<li><strong>topologyKey</strong>：Node label的key，如果多个Node都使用了这个lable key那么 <em>scheduler</em> 将这些 Node 看作为相同的拓扑域。</li>
<li><strong>whenUnsatisfiable</strong>：当 Pod 不满足分布的约束时，怎么去处理
<ul>
<li><code>DoNotSchedule</code>（默认）不要调度。</li>
<li><code>ScheduleAnyway</code>仍然调度它，同时优先考虑最小化倾斜节点</li>
</ul>
</li>
<li><strong>labelSelector</strong>：查找匹配的 Pod label选择器的node进行技术，以计算Pod如何分布在拓扑域中</li>
</ul>
<h4 id="对于拓扑域的理解">对于拓扑域的理解<a hidden class="anchor" aria-hidden="true" href="#对于拓扑域的理解">¶</a></h4>
<p>对于拓扑域，官方是这么说明的，假设有一个带有以下lable的 4 节点集群：</p>
<pre><code class="language-bash">NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre>
<p>那么集群拓扑如图：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725223516451.png" alt="image-20220725223516451" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图1：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>假设一个 4 节点集群，其中 3个label被标记为<code>foo: bar</code>的 Pod 分别位于Node1、Node2 和 Node3：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725224602667.png" alt="image-20220725224602667" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图2：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>这种情况下，新部署一个Pod，并希望新Pod与现有Pod跨 <code>Zone</code>均匀分布，资源清单文件如下：</p>
<pre><code class="language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1
</code></pre>
<p>这个清单对于拓扑域来说，<code>topologyKey: zone</code> 表示对Pod均匀分布仅应用于已标记的节点（如 <code>foo: bar</code>），将会跳过没有标签的节点（如<code>zone: &lt;any value&gt;</code>）。如果 <em>scheduler</em> 找不到满足约束的方法，<code>whenUnsatisfiable: DoNotSchedule</code> 设置的策略则是 <em>scheduler</em> 对新部署的Pod保持 <code>Pendding</code></p>
<p>如果此时 <em>scheduler</em> 将新Pod 调度至 $Zone_A$，此时Pod分布在拓扑域间为 $[3,1]$ ，而 <code>maxSkew</code> 配置的值是1。此时倾斜值为 $Zone_A - Zone_B = 3-1=2$，不满足 <code>maxSkew=1</code>，故这个Pod只能被调度到 $Zone_B$。</p>
<p>此时Pod调度拓扑图为图3或图4</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725230358777.png" alt="image-20220725230358777" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图3：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725230515969.png" alt="image-20220725230515969" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图4：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>如果需要将Pod调度到 $Zone_A$ ,可以按照如下方式进行：</p>
<ul>
<li>修改 <code>maxSkew=2</code></li>
<li>修改 <code>topologyKey: node</code> 而不是 <code>Zone</code> ，这种模式下可以将 Pod 均匀分布在Node而不是Zone之间。</li>
<li>修改 <code>whenUnsatisfiable: DoNotSchedule</code> 为 <code>whenUnsatisfiable: ScheduleAnyway</code> 确保新的Pod始终可被调度</li>
</ul>
<p>下面再通过一个例子增强对拓扑域了解</p>
<p><strong>多拓扑约束</strong></p>
<p>设拥有一个 4 节点集群，其中 3 个现有 Pod 标记 <code>foo: bar </code>分别位于 <code>node1</code>、<code>node2</code> 和 <code>node3</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725231905415.png" alt="image-20220725231905415" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图5：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>部署的资源清单如下：可以看出拓扑分布约束配置了多个</p>
<pre><code class="language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  - maxSkew: 1
    topologyKey: node
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1
</code></pre>
<p>在这种情况下，为了匹配第一个约束条件，新Pod 只能放置在 $Zone_B$ ；而就第二个约束条件，新Pod只能调度到 <code>node4</code>。在这种配置多约束条件下， <em>scheduler</em> 只考虑满足所有约束的值，因此唯一有效的是 <code>node4</code>。</p>
<h4 id="如何为集群设置一个默认拓扑域约束">如何为集群设置一个默认拓扑域约束<a hidden class="anchor" aria-hidden="true" href="#如何为集群设置一个默认拓扑域约束">¶</a></h4>
<p>默认情况下，拓扑域约束也作 <em>scheduler</em> 的为 <em>scheduler configurtion</em> 中的一部分参数，这也意味着，可以通过profile为整个集群级别指定一个默认的拓扑域调度约束，</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List
</code></pre>
<h4 id="默认约束策略">默认约束策略<a hidden class="anchor" aria-hidden="true" href="#默认约束策略">¶</a></h4>
<p>如果在没有配置集群级别的约束策略时，<em>kube-scheduler</em> 内部 <code>topologyspread</code> 插件提供了一个默认的拓扑约束策略，大致上如下列清单所示</p>
<pre><code>defaultConstraints:
  - maxSkew: 3
    topologyKey: &quot;kubernetes.io/hostname&quot;
    whenUnsatisfiable: ScheduleAnyway
  - maxSkew: 5
    topologyKey: &quot;topology.kubernetes.io/zone&quot;
    whenUnsatisfiable: ScheduleAnyway
</code></pre>
<p>上述清单中内容可以在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/plugin.go#L42-L53" target="_blank"
   rel="noopener nofollow noreferrer" >pkg\scheduler\framework\plugins\podtopologyspread\plugin.go</a></p>
<pre><code class="language-go">var systemDefaultConstraints = []v1.TopologySpreadConstraint{
	{
		TopologyKey:       v1.LabelHostname,
		WhenUnsatisfiable: v1.ScheduleAnyway,
		MaxSkew:           3,
	},
	{
		TopologyKey:       v1.LabelTopologyZone,
		WhenUnsatisfiable: v1.ScheduleAnyway,
		MaxSkew:           5,
	},
}
</code></pre>
<p>可以通过在配置文件中留空，来禁用默认配置</p>
<ul>
<li><code>defaultConstraints: []</code></li>
<li><code>defaultingType: List</code></li>
</ul>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints: []
          defaultingType: List
</code></pre>
<h3 id="通过源码学习topology">通过源码学习Topology<a hidden class="anchor" aria-hidden="true" href="#通过源码学习topology">¶</a></h3>
<p><code>podtopologyspread</code> 实现了4种扩展点方法，包含 <code>filter</code> 和 <code>score</code></p>
<h4 id="prefilter">PreFilter<a hidden class="anchor" aria-hidden="true" href="#prefilter">¶</a></h4>
<p>可以看到 <code>PreFilter</code> 的核心为 <code>calPreFilterState</code></p>
<pre><code class="language-go">func (pl *PodTopologySpread) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	s, err := pl.calPreFilterState(ctx, pod)
	if err != nil {
		return nil, framework.AsStatus(err)
	}
	cycleState.Write(preFilterStateKey, s)
	return nil, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L225-L307" target="_blank"
   rel="noopener nofollow noreferrer" >calPreFilterState</a> 主要功能是用在计算如何在拓扑域中分布Pod，首先看段代码时，需要掌握下属几个概念</p>
<ul>
<li><a href="#preFilterState">preFilterState</a></li>
<li><a href="#criticalPaths">criticalPaths</a></li>
<li><a href="#update">update</a></li>
</ul>
<pre><code class="language-go">func (pl *PodTopologySpread) calPreFilterState(ctx context.Context, pod *v1.Pod) (*preFilterState, error) {
    // 获取Node
	allNodes, err := pl.sharedLister.NodeInfos().List()
	if err != nil {
		return nil, fmt.Errorf(&quot;listing NodeInfos: %w&quot;, err)
	}
	var constraints []topologySpreadConstraint
	if len(pod.Spec.TopologySpreadConstraints) &gt; 0 {
		// 这里会构建出TopologySpreadConstraints，因为约束是不确定的
		constraints, err = filterTopologySpreadConstraints(
			pod.Spec.TopologySpreadConstraints,
			v1.DoNotSchedule,
			pl.enableMinDomainsInPodTopologySpread,
			pl.enableNodeInclusionPolicyInPodTopologySpread,
		)
		if err != nil {
			return nil, fmt.Errorf(&quot;obtaining pod's hard topology spread constraints: %w&quot;, err)
		}
	} else {
        // buildDefaultConstraints使用&quot;.DefaultConstraints&quot;与pod匹配的
        // service、replication controllers、replica sets 
        // 和stateful sets的选择器为pod构建一个约束。
		constraints, err = pl.buildDefaultConstraints(pod, v1.DoNotSchedule)
		if err != nil {
			return nil, fmt.Errorf(&quot;setting default hard topology spread constraints: %w&quot;, err)
		}
	}
	if len(constraints) == 0 { // 如果是空的，则返回空preFilterState
		return &amp;preFilterState{}, nil
	}
    // 初始化一个 preFilterState 状态
	s := preFilterState{
		Constraints:          constraints,
		TpKeyToCriticalPaths: make(map[string]*criticalPaths, len(constraints)),
		TpPairToMatchNum:     make(map[topologyPair]int, sizeHeuristic(len(allNodes), constraints)),
	}
	// 根据node统计拓扑域数量
	tpCountsByNode := make([]map[topologyPair]int, len(allNodes))
	// 获取pod亲和度配置
	requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod)
	processNode := func(i int) {
		nodeInfo := allNodes[i]
		node := nodeInfo.Node()
		if node == nil {
			klog.ErrorS(nil, &quot;Node not found&quot;)
			return
		}
		// 通过spreading去过滤node以用作filters，错误解析以向后兼容
		if !pl.enableNodeInclusionPolicyInPodTopologySpread {
			if match, _ := requiredNodeAffinity.Match(node); !match {
				return
			}
		}

		// 确保node的lable 包含topologyKeys定义的值
		if !nodeLabelsMatchSpreadConstraints(node.Labels, constraints) {
			return
		}

		tpCounts := make(map[topologyPair]int, len(constraints))
		for _, c := range constraints { // 对应的约束列表
			if pl.enableNodeInclusionPolicyInPodTopologySpread &amp;&amp;
				!c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) {
				continue
			}
			// 构建出 topologyPair 以key value形式，
			// 通常情况下TopologyKey属于什么类型的拓扑
			//  node.Labels[c.TopologyKey] 则是属于这个拓扑中那个子域
			pair := topologyPair{key: c.TopologyKey, value: node.Labels[c.TopologyKey]}
			// 计算与标签选择器相匹配的pod有多少个
			count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace)
			tpCounts[pair] = count
		}
		tpCountsByNode[i] = tpCounts // 最终形成的拓扑结构
	}
	// 执行上面的定义的processNode，执行的数量就是node的数量
	pl.parallelizer.Until(ctx, len(allNodes), processNode)
	// 最后构建出 TpPairToMatchNum
	// 表示每个拓扑域中的每个子域各分布多少Pod，如图6所示
	for _, tpCounts := range tpCountsByNode {
		for tp, count := range tpCounts {
			s.TpPairToMatchNum[tp] += count
		}
	}
	if pl.enableMinDomainsInPodTopologySpread {
		// 根据状态进行构建 preFilterState
		s.TpKeyToDomainsNum = make(map[string]int, len(constraints))
		for tp := range s.TpPairToMatchNum {
			s.TpKeyToDomainsNum[tp.key]++
		}
	}

	// 计算最小匹配出的拓扑对
	for i := 0; i &lt; len(constraints); i++ {
		key := constraints[i].TopologyKey
		s.TpKeyToCriticalPaths[key] = newCriticalPaths()
	}
	for pair, num := range s.TpPairToMatchNum {
		s.TpKeyToCriticalPaths[pair.key].update(pair.value, num)
	}

	return &amp;s, nil // 返回的值则包含最小的分布
}
</code></pre>
<p class="preFilterState">preFilterState</p>
<pre><code class="language-go">// preFilterState 是在PreFilter处计算并在Filter处使用。
// 它结合了 “TpKeyToCriticalPaths” 和 “TpPairToMatchNum” 来表示：
//（1）在每个分布约束上匹配最少pod的criticalPaths。 
// (2) 在每个分布约束上匹配的pod的数量。
// “nil preFilterState” 则表示没有设置（在PreFilter阶段）；
// empty “preFilterState”对象则表示它是一个合法的状态，并在PreFilter阶段设置。

type preFilterState struct {
	Constraints []topologySpreadConstraint

    // 这里记录2条关键路径而不是所有关键路径。 
    // criticalPaths[0].MatchNum 始终保存最小匹配数。 
    // criticalPaths[1].MatchNum 总是大于或等于criticalPaths[0].MatchNum，但不能保证是第二个最小匹配数。
	TpKeyToCriticalPaths map[string]*criticalPaths
	
    // TpKeyToDomainsNum 以 “topologyKey” 作为key ，并以zone的数量作为值。
	TpKeyToDomainsNum map[string]int
	
    // TpPairToMatchNum 以 “topologyPair作为key” ，并以匹配到pod的数量作为value。
	TpPairToMatchNum map[topologyPair]int
}
</code></pre>
<p class="criticalPaths">criticalPaths</p>
<pre><code class="language-go">// [2]criticalPath能够工作的原因是基于当前抢占算法的实现，特别是以下两个事实
// 事实 1：只抢占同一节点上的Pod，而不是多个节点上的 Pod。
// 事实 2：每个节点在其抢占周期期间在“preFilterState”的单独副本上进行评估。如果我们计划转向更复杂的算法，例如“多个节点上的任意pod”时则需要重新考虑这种结构。
type criticalPaths [2]struct {
	// TopologyValue代表映射到拓扑键的拓扑值。
	TopologyValue string
	// MatchNum代表匹配到的pod数量
	MatchNum int
}
</code></pre>
<p>单元测试中的测试案例，具有两个约束条件的场景，通过表格来解析如下：</p>
<p>Node列表与标签如下表：</p>
<table>
<thead>
<tr>
<th>Node Name</th>
<th>🏷️Lable-zone</th>
<th>🏷️Lable-node</th>
</tr>
</thead>
<tbody>
<tr>
<td>node-a</td>
<td>zone1</td>
<td>node-a</td>
</tr>
<tr>
<td>node-b</td>
<td>zone1</td>
<td>node-b</td>
</tr>
<tr>
<td>node-x</td>
<td>zone2</td>
<td>node-x</td>
</tr>
<tr>
<td>node-y</td>
<td>zone2</td>
<td>node-y</td>
</tr>
</tbody>
</table>
<p>Pod列表与标签如下表：</p>
<table>
<thead>
<tr>
<th>Pod Name</th>
<th>Node</th>
<th>🏷️Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>p-a1</td>
<td>node-a</td>
<td>foo:</td>
</tr>
<tr>
<td>p-a2</td>
<td>node-a</td>
<td>foo:</td>
</tr>
<tr>
<td>p-b1</td>
<td>node-b</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y1</td>
<td>node-y</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y2</td>
<td>node-y</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y3</td>
<td>node-y</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y4</td>
<td>node-y</td>
<td>foo:</td>
</tr>
</tbody>
</table>
<p>对应的拓扑约束</p>
<pre><code class="language-yaml">spec:
  topologySpreadConstraints:
  - MaxSkew: 1
	TopologyKey: zone
	labelSelector:
	  matchLabels:
	    foo: bar
	MinDomains: 1
	NodeAffinityPolicy: Honor
	NodeTaintsPolicy: Ignore
  - MaxSkew: 1
	TopologyKey: node
	labelSelector:
	  matchLabels:
	    foo: bar
	MinDomains: 1
	NodeAffinityPolicy: Honor
	NodeTaintsPolicy: Ignore
</code></pre>
<p>那么整个分布如下：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220726214255638.png" alt="image-20220726214255638" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图6：具有两个场景的分布图</center>
<p id="prefiltertesting">实现的测试代码如下</p>
<pre><code class="language-go">...
{
    name: &quot;normal case with two spreadConstraints&quot;,
    pod: st.MakePod().Name(&quot;p&quot;).Label(&quot;foo&quot;, &quot;&quot;).
    SpreadConstraint(1, &quot;zone&quot;, v1.DoNotSchedule, fooSelector, nil, nil, nil).
    SpreadConstraint(1, &quot;node&quot;, v1.DoNotSchedule, fooSelector, nil, nil, nil).
    Obj(),
    nodes: []*v1.Node{
        st.MakeNode().Name(&quot;node-a&quot;).Label(&quot;zone&quot;, &quot;zone1&quot;).Label(&quot;node&quot;, &quot;node-a&quot;).Obj(),
        st.MakeNode().Name(&quot;node-b&quot;).Label(&quot;zone&quot;, &quot;zone1&quot;).Label(&quot;node&quot;, &quot;node-b&quot;).Obj(),
        st.MakeNode().Name(&quot;node-x&quot;).Label(&quot;zone&quot;, &quot;zone2&quot;).Label(&quot;node&quot;, &quot;node-x&quot;).Obj(),
        st.MakeNode().Name(&quot;node-y&quot;).Label(&quot;zone&quot;, &quot;zone2&quot;).Label(&quot;node&quot;, &quot;node-y&quot;).Obj(),
    },
    existingPods: []*v1.Pod{
        st.MakePod().Name(&quot;p-a1&quot;).Node(&quot;node-a&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-a2&quot;).Node(&quot;node-a&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-b1&quot;).Node(&quot;node-b&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y1&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y2&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y3&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y4&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
    },
    want: &amp;preFilterState{
        Constraints: []topologySpreadConstraint{
            {
                MaxSkew:            1,
                TopologyKey:        &quot;zone&quot;,
                Selector:           mustConvertLabelSelectorAsSelector(t, fooSelector),
                MinDomains:         1,
                NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,
                NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore,
            },
            {
                MaxSkew:            1,
                TopologyKey:        &quot;node&quot;,
                Selector:           mustConvertLabelSelectorAsSelector(t, fooSelector),
                MinDomains:         1,
                NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,
                NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore,
            },
        },
        TpKeyToCriticalPaths: map[string]*criticalPaths{
            &quot;zone&quot;: {{&quot;zone1&quot;, 3}, {&quot;zone2&quot;, 4}},
            &quot;node&quot;: {{&quot;node-x&quot;, 0}, {&quot;node-b&quot;, 1}},
        },
        for pair, num := range s.TpPairToMatchNum {
		s.TpKeyToCriticalPaths[pair.key].update(pair.value, num)
	}
        TpPairToMatchNum: map[topologyPair]int{
            {key: &quot;zone&quot;, value: &quot;zone1&quot;}:  3,
            {key: &quot;zone&quot;, value: &quot;zone2&quot;}:  4,
            {key: &quot;node&quot;, value: &quot;node-a&quot;}: 2,
            {key: &quot;node&quot;, value: &quot;node-b&quot;}: 1,
            {key: &quot;node&quot;, value: &quot;node-x&quot;}: 0,
            {key: &quot;node&quot;, value: &quot;node-y&quot;}: 4,
        },
    },
}
...
</code></pre>
<p class="update">update</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L120-L148" target="_blank"
   rel="noopener nofollow noreferrer" >update</a> 函数实际上时用于计算 <a href="#criticalPaths">criticalPaths</a> 中的第一位始终保持为是一个最小Pod匹配值</p>
<pre><code class="language-go">func (p *criticalPaths) update(tpVal string, num int) {
	// first verify if `tpVal` exists or not
	i := -1
	if tpVal == p[0].TopologyValue {
		i = 0
	} else if tpVal == p[1].TopologyValue {
		i = 1
	}

	if i &gt;= 0 {
		// `tpVal` 表示已经存在
		p[i].MatchNum = num
		if p[0].MatchNum &gt; p[1].MatchNum {
			// swap paths[0] and paths[1]
			p[0], p[1] = p[1], p[0]
		}
	} else {
		// `tpVal` 表示不存在，如一个新初始化的值
        // num对应子域分布的pod
        // 说明第一个元素不是最小的，则作为交换
		if num &lt; p[0].MatchNum {
			// update paths[1] with paths[0]
			p[1] = p[0]
			// update paths[0]
			p[0].TopologyValue, p[0].MatchNum = tpVal, num
		} else if num &lt; p[1].MatchNum {
			// 如果小于 paths[1]，则更新它，永远保证元素0是最小，1是次小的
			p[1].TopologyValue, p[1].MatchNum = tpVal, num
		}
	}
}
</code></pre>
<p>综合来讲 <code>Prefilter</code> 主要做的工作是。循环所有的节点，先根据 <code>NodeAffinity</code> 或者 <code>NodeSelector</code> 进行过滤，然后根据约束中定义的 <code>topologyKeys</code> （拓扑划分的依据） 来选择节点。</p>
<p>接下来会计算出每个拓扑域下的拓扑对（可以理解为子域）匹配的 Pod 数量，存入 <code>TpPairToMatchNum</code> 中，最后就是要把所有约束中匹配的 Pod 数量最小（第二小）匹配出来的路径（代码是这么定义的，理解上可以看作是分布图）放入 <code>TpKeyToCriticalPaths</code> 中保存起来。整个 <code>preFilterState</code> 保存下来传递到后续的 <code>filter</code> 插件中使用。</p>
<h4 id="filter">Filter<a hidden class="anchor" aria-hidden="true" href="#filter">¶</a></h4>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L178" target="_blank"
   rel="noopener nofollow noreferrer" >preFilter</a> 中 最后的计算结果会保存在 <code>CycleState</code> 中</p>
<pre><code class="language-go">cycleState.Write(preFilterStateKey, s)
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L310-L362" target="_blank"
   rel="noopener nofollow noreferrer" >Filter</a> 主要是从 <code>PreFilter</code> 处理的过程中拿到状态 <code>preFilterState</code>，然后看下每个拓扑约束中的 <code>MaxSkew</code> 是否合法，具体的计算公式为：$matchNum + selfMatchNum - minMatchNum$</p>
<ul>
<li><code>matchNum</code>：Prefilter 中计算出的对应的拓扑分布数量，可以在<a href="#prefiltertesting">Prefilter</a>中参考对应的内容
<ul>
<li><code>if tpCount, ok := s.TpPairToMatchNum[pair]; ok {</code></li>
</ul>
</li>
<li><code>selfMatchNum</code>：匹配到label的数量，匹配到则是1，否则为0</li>
<li><code> minMatchNum</code>：获的 <code>Prefilter </code> 中计算出来的最小匹配的值</li>
</ul>
<pre><code class="language-go">func (pl *PodTopologySpread) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	node := nodeInfo.Node()
	if node == nil {
		return framework.AsStatus(fmt.Errorf(&quot;node not found&quot;))
	}
	// 拿到 prefilter处理的s，即preFilterState
	s, err := getPreFilterState(cycleState)
	if err != nil {
		return framework.AsStatus(err)
	}

	// 一个 空类型的 preFilterState是合法的，这种情况下将容忍每一个被调度的 Pod
	if len(s.Constraints) == 0 {
		return nil
	}

	podLabelSet := labels.Set(pod.Labels) // 设置标签
	for _, c := range s.Constraints { // 因为拓扑约束允许多个所以
		tpKey := c.TopologyKey
		tpVal, ok := node.Labels[c.TopologyKey]
		if !ok {
			klog.V(5).InfoS(&quot;Node doesn't have required label&quot;, &quot;node&quot;, klog.KObj(node), &quot;label&quot;, tpKey)
			return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReasonNodeLabelNotMatch)
		}

		// 判断标准
		// 现有的匹配数量 + 子匹配（1|0） - 全局minimum &lt;= maxSkew
		minMatchNum, err := s.minMatchNum(tpKey, c.MinDomains, pl.enableMinDomainsInPodTopologySpread)
		if err != nil {
			klog.ErrorS(err, &quot;Internal error occurred while retrieving value precalculated in PreFilter&quot;, &quot;topologyKey&quot;, tpKey, &quot;paths&quot;, s.TpKeyToCriticalPaths)
			continue
		}

		selfMatchNum := 0
		if c.Selector.Matches(podLabelSet) {
			selfMatchNum = 1
		}

		pair := topologyPair{key: tpKey, value: tpVal}
		matchNum := 0
		if tpCount, ok := s.TpPairToMatchNum[pair]; ok {
			matchNum = tpCount
		}
		skew := matchNum + selfMatchNum - minMatchNum
		if skew &gt; int(c.MaxSkew) {
			klog.V(5).InfoS(&quot;Node failed spreadConstraint: matchNum + selfMatchNum - minMatchNum &gt; maxSkew&quot;, &quot;node&quot;, klog.KObj(node), &quot;topologyKey&quot;, tpKey, &quot;matchNum&quot;, matchNum, &quot;selfMatchNum&quot;, selfMatchNum, &quot;minMatchNum&quot;, minMatchNum, &quot;maxSkew&quot;, c.MaxSkew)
			return framework.NewStatus(framework.Unschedulable, ErrReasonConstraintsNotMatch)
		}
	}

	return nil
}
</code></pre>
<p>minMatchNum</p>
<pre><code class="language-go">// minMatchNum用于计算 倾斜的全局最小值，同时考虑 MinDomains。
func (s *preFilterState) minMatchNum(tpKey string, minDomains int32, enableMinDomainsInPodTopologySpread bool) (int, error) {
	paths, ok := s.TpKeyToCriticalPaths[tpKey]
	if !ok {
		return 0, fmt.Errorf(&quot;failed to retrieve path by topology key&quot;)
	}
	// 通常来说最小值是第一个
	minMatchNum := paths[0].MatchNum
	if !enableMinDomainsInPodTopologySpread { // 就是plugin的配置的 enableMinDomainsInPodTopologySpread
		return minMatchNum, nil
	}

	domainsNum, ok := s.TpKeyToDomainsNum[tpKey]
	if !ok {
		return 0, fmt.Errorf(&quot;failed to retrieve the number of domains by topology key&quot;)
	}

	if domainsNum &lt; int(minDomains) {
		// 当有匹配拓扑键的符合条件的域的数量小于 配置的&quot;minDomains&quot;(每个约束条件的这个配置) 时，
		//它将全局“minimum” 设置为0。
		// 因为minimum默认就为1，如果他小于1，就让他为0
		minMatchNum = 0
	}

	return minMatchNum, nil
}
</code></pre>
<h4 id="prescore">PreScore<a hidden class="anchor" aria-hidden="true" href="#prescore">¶</a></h4>
<p>与 Filter 类似， <code>PreScore</code> 也是类似 <code>PreFilter</code> 的构成。 <code>initPreScoreState</code> 来完成过滤。</p>
<p>有了 <code>PreFilter</code> 基础后，对于 Score 来说大同小异</p>
<pre><code class="language-go">func (pl *PodTopologySpread) PreScore(
	ctx context.Context,
	cycleState *framework.CycleState,
	pod *v1.Pod,
	filteredNodes []*v1.Node,
) *framework.Status {
	allNodes, err := pl.sharedLister.NodeInfos().List()
	if err != nil {
		return framework.AsStatus(fmt.Errorf(&quot;getting all nodes: %w&quot;, err))
	}

	if len(filteredNodes) == 0 || len(allNodes) == 0 {
		// No nodes to score.
		return nil
	}

	state := &amp;preScoreState{
		IgnoredNodes:            sets.NewString(),
		TopologyPairToPodCounts: make(map[topologyPair]*int64),
	}
	// Only require that nodes have all the topology labels if using
	// non-system-default spreading rules. This allows nodes that don't have a
	// zone label to still have hostname spreading.
	// 如果使用非系统默认分布规则，则仅要求节点具有所有拓扑标签。
	// 这将允许没有zone标签的节点仍然具有hostname分布。
	requireAllTopologies := len(pod.Spec.TopologySpreadConstraints) &gt; 0 || !pl.systemDefaulted
	err = pl.initPreScoreState(state, pod, filteredNodes, requireAllTopologies)
	if err != nil {
		return framework.AsStatus(fmt.Errorf(&quot;calculating preScoreState: %w&quot;, err))
	}

	// return if incoming pod doesn't have soft topology spread Constraints.
	if len(state.Constraints) == 0 {
		cycleState.Write(preScoreStateKey, state)
		return nil
	}

	// Ignore parsing errors for backwards compatibility.
	requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod)
	processAllNode := func(i int) {
		nodeInfo := allNodes[i]
		node := nodeInfo.Node()
		if node == nil {
			return
		}

		if !pl.enableNodeInclusionPolicyInPodTopologySpread {
			// `node` should satisfy incoming pod's NodeSelector/NodeAffinity
			if match, _ := requiredNodeAffinity.Match(node); !match {
				return
			}
		}

		// All topologyKeys need to be present in `node`
		if requireAllTopologies &amp;&amp; !nodeLabelsMatchSpreadConstraints(node.Labels, state.Constraints) {
			return
		}

		for _, c := range state.Constraints {
			if pl.enableNodeInclusionPolicyInPodTopologySpread &amp;&amp;
				!c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) {
				continue
			}

			pair := topologyPair{key: c.TopologyKey, value: node.Labels[c.TopologyKey]}
			// If current topology pair is not associated with any candidate node,
			// continue to avoid unnecessary calculation.
			// Per-node counts are also skipped, as they are done during Score.
			tpCount := state.TopologyPairToPodCounts[pair]
			if tpCount == nil {
				continue
			}
			count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace)
			atomic.AddInt64(tpCount, int64(count))
		}
	}
	pl.parallelizer.Until(ctx, len(allNodes), processAllNode)
	// 保存状态给后面sorce调用
	cycleState.Write(preScoreStateKey, state)
	return nil
}
</code></pre>
<p>与Filter中Update使用的函数一样，这里也会到这一步，这里会构建出TopologySpreadConstraints，因为约束是不确定的</p>
<pre><code class="language-go">func filterTopologySpreadConstraints(constraints []v1.TopologySpreadConstraint, action v1.UnsatisfiableConstraintAction, enableMinDomainsInPodTopologySpread, enableNodeInclusionPolicyInPodTopologySpread bool) ([]topologySpreadConstraint, error) {
	var result []topologySpreadConstraint
	for _, c := range constraints {
		if c.WhenUnsatisfiable == action { // 始终调度时
			selector, err := metav1.LabelSelectorAsSelector(c.LabelSelector)
			if err != nil {
				return nil, err
			}
			tsc := topologySpreadConstraint{
				MaxSkew:            c.MaxSkew,
				TopologyKey:        c.TopologyKey,
				Selector:           selector,
				MinDomains:         1,                            // If MinDomains is nil, we treat MinDomains as 1.
				NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,  // If NodeAffinityPolicy is nil, we treat NodeAffinityPolicy as &quot;Honor&quot;.
				NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore, // If NodeTaintsPolicy is nil, we treat NodeTaintsPolicy as &quot;Ignore&quot;.
			}
			if enableMinDomainsInPodTopologySpread &amp;&amp; c.MinDomains != nil {
				tsc.MinDomains = *c.MinDomains
			}
			if enableNodeInclusionPolicyInPodTopologySpread {
				if c.NodeAffinityPolicy != nil {
					tsc.NodeAffinityPolicy = *c.NodeAffinityPolicy
				}
				if c.NodeTaintsPolicy != nil {
					tsc.NodeTaintsPolicy = *c.NodeTaintsPolicy
				}
			}
			result = append(result, tsc)
		}
	}
	return result, nil
}
</code></pre>
<h4 id="score">Score<a hidden class="anchor" aria-hidden="true" href="#score">¶</a></h4>
<pre><code class="language-GO">// 在分数扩展点调用分数。该函数返回的“score”是 `nodeName` 上匹配的 pod 数量，稍后会进行归一化。
func (pl *PodTopologySpread) Score(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	nodeInfo, err := pl.sharedLister.NodeInfos().Get(nodeName)
	if err != nil {
		return 0, framework.AsStatus(fmt.Errorf(&quot;getting node %q from Snapshot: %w&quot;, nodeName, err))
	}

	node := nodeInfo.Node()
	s, err := getPreScoreState(cycleState)
	if err != nil {
		return 0, framework.AsStatus(err)
	}

	// Return if the node is not qualified.
	if s.IgnoredNodes.Has(node.Name) {
		return 0, nil
	}

	// 对于每个当前的 &lt;pair&gt;，当前节点获得 &lt;matchSum&gt; 的信用分。
	// 计算 &lt;matchSum&gt;总和 并将其作为该节点的分数返回。
	var score float64
	for i, c := range s.Constraints {
		if tpVal, ok := node.Labels[c.TopologyKey]; ok {
			var cnt int64
			if c.TopologyKey == v1.LabelHostname {
				cnt = int64(countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace))
			} else {
				pair := topologyPair{key: c.TopologyKey, value: tpVal}
				cnt = *s.TopologyPairToPodCounts[pair]
			}
			score += scoreForCount(cnt, c.MaxSkew, s.TopologyNormalizingWeight[i])
		}
	}
	return int64(math.Round(score)), nil
}
</code></pre>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/framework.go#L940-L952" target="_blank"
   rel="noopener nofollow noreferrer" >Framework</a> 中会运行 <code>ScoreExtension</code> ，即 <code>NormalizeScore</code></p>
<pre><code class="language-go">// Run NormalizeScore method for each ScorePlugin in parallel.
f.Parallelizer().Until(ctx, len(f.scorePlugins), func(index int) {
    pl := f.scorePlugins[index]
    nodeScoreList := pluginToNodeScores[pl.Name()]
    if pl.ScoreExtensions() == nil {
        return
    }
    status := f.runScoreExtension(ctx, pl, state, pod, nodeScoreList)
    if !status.IsSuccess() {
        err := fmt.Errorf(&quot;plugin %q failed with: %w&quot;, pl.Name(), status.AsError())
        errCh.SendErrorWithCancel(err, cancel)
        return
    }
})
if err := errCh.ReceiveError(); err != nil {
    return nil, framework.AsStatus(fmt.Errorf(&quot;running Normalize on Score plugins: %w&quot;, err))
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/scoring.go#L216-L255" target="_blank"
   rel="noopener nofollow noreferrer" >NormalizeScore</a> 会为所有的node根据之前计算出的权重进行打分</p>
<pre><code class="language-go">func (pl *PodTopologySpread) NormalizeScore(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
	s, err := getPreScoreState(cycleState)
	if err != nil {
		return framework.AsStatus(err)
	}
	if s == nil {
		return nil
	}

	// 计算 &lt;minScore&gt; 和 &lt;maxScore&gt;
	var minScore int64 = math.MaxInt64
	var maxScore int64
	for i, score := range scores {
		// it's mandatory to check if &lt;score.Name&gt; is present in m.IgnoredNodes
		if s.IgnoredNodes.Has(score.Name) {
			scores[i].Score = invalidScore
			continue
		}
		if score.Score &lt; minScore {
			minScore = score.Score
		}
		if score.Score &gt; maxScore {
			maxScore = score.Score
		}
	}

	for i := range scores {
		if scores[i].Score == invalidScore {
			scores[i].Score = 0
			continue
		}
		if maxScore == 0 {
			scores[i].Score = framework.MaxNodeScore
			continue
		}
		s := scores[i].Score
		scores[i].Score = framework.MaxNodeScore * (maxScore + minScore - s) / maxScore
	}
	return nil
}
</code></pre>
<p>到此，对于pod拓扑插件功能大概可以明了了，</p>
<ul>
<li>Filter 部分（<code>PreFilter</code>，<code>Filter</code>）完成拓扑对(<code>Topology Pair</code>)划分</li>
<li>Score部分（<code>PreScore</code>, <code>Score</code> , <code>NormalizeScore</code> ）主要是对拓扑对（可以理解为拓扑结构划分）来选择一个最适合的pod的节点（即分数最优的节点）</li>
</ul>
<p>而在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/plugins/podtopologyspread/scoring_test.go" target="_blank"
   rel="noopener nofollow noreferrer" >scoring_test.go</a> 给了很多用例，可以更深入的了解这部分算法</p>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">¶</a></h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling code hierarchy</a></p>
<p><sup id="2">[2]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_algorithm.md" target="_blank"
   rel="noopener nofollow noreferrer" >scheduler algorithm</a></p>
<p><sup id="3">[3]</sup> <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#in-tree-vs-out-of-tree-volume-plugins" target="_blank"
   rel="noopener nofollow noreferrer" >in tree VS out of tree volume plugins</a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_framework_plugins.md" target="_blank"
   rel="noopener nofollow noreferrer" >scheduler_framework_plugins</a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/reference/scheduling/config/" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling config</a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/" target="_blank"
   rel="noopener nofollow noreferrer" >topology spread constraints</a></p>
</blockquote>


    
    


<div class="copyrightBlock" >
    <div class="articleSuffix-bg"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 147.78 155.96"> <path d="M10.5,99.81a1.9,1.9,0,0,0-.53-.09,1.66,1.66,0,0,0-1.64,1.65A1.64,1.64,0,0,0,10,103a1.57,1.57,0,0,0,.87-.25l26.76,26.82.45-1.08L11.52,101.91A1.65,1.65,0,0,0,10.5,99.81Zm-.13,2a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.58.58,0,0,1,.57-.57h0a.57.57,0,0,1,.56.58A.55.55,0,0,1,10.37,101.77Z" style="fill:#c5c9e0"></path><path d="M56.15,117.58H39.06l0-.09a1.65,1.65,0,0,0-1.36-1H37.5a1.65,1.65,0,1,0,1.56,2.19H55.7L92.92,156h41.44v-1.08h-41Zm-18.25.94a.56.56,0,0,1-.79,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h0a.58.58,0,0,1,.57.58A.54.54,0,0,1,37.9,118.52Z" style="fill:#c5c9e0"></path><path d="M23.52,50.32a1.65,1.65,0,0,0,1.55-1.11H55.28l48-48.13h31.06V0H102.85l-48,48.13H25.07a1.64,1.64,0,0,0-2.09-1,1.64,1.64,0,0,0,.54,3.2Zm0-2.21a.57.57,0,0,1,0,1.13.57.57,0,1,1,0-1.13Z" style="fill:#c5c9e0"></path><polygon points="102.86 0 102.86 0 102.86 0 102.86 0" style="fill:#c5c9e0"></polygon><path d="M107.72,12.14h26.64V11.07H107.27L57.4,61H3.09a1.66,1.66,0,0,0-1.45-.86H1.52A1.65,1.65,0,1,0,2.81,63a1.59,1.59,0,0,0,.45-.87H57.85ZM2.05,62.23a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h.09a.57.57,0,0,1,.32,1Z" style="fill:#c5c9e0"></path><path d="M134.36,43.22V42.14h-22.3l-9.62,9.63a1.64,1.64,0,0,0-2.19.77,1.61,1.61,0,0,0-.17.71,1.65,1.65,0,1,0,3.29,0,1.61,1.61,0,0,0-.16-.72l9.3-9.32Zm-32.64,10.6a.57.57,0,0,1,0-1.13.57.57,0,0,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M147,52.3l-9,9H111.48a1.64,1.64,0,0,0-1.61-1.33h-.14a1.65,1.65,0,1,0,1.6,2.41h27.19l9.26-9.29L147,52.3Zm-37.15,9.85a.56.56,0,0,1-.56-.57h0a.56.56,0,0,1,.56-.56h0a.57.57,0,1,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M66.79,75.35l11,11.06h56.53V85.33H78.27l-11-11.06H49.49L37.12,86.67a1.64,1.64,0,0,0-2.09,1,1.61,1.61,0,0,0-.09.54,1.65,1.65,0,0,0,3.29,0,1.68,1.68,0,0,0-.26-.89l12-12ZM36.58,88.79a.57.57,0,1,1,.57-.56A.57.57,0,0,1,36.58,88.79Z" style="fill:#c5c9e0"></path><path d="M110.61,95.55,92.8,113.4a1.62,1.62,0,1,0,.77.76l17.49-17.53h23.31V95.55ZM92.49,115.28a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.57-.57h0a.58.58,0,0,1,.56.58A.55.55,0,0,1,92.49,115.28Z" style="fill:#c5c9e0"></path><path d="M97.89,122.3H76.62L64.2,109.85a1.65,1.65,0,0,0-.77-2.2,1.77,1.77,0,0,0-.72-.17h-.14a1.65,1.65,0,0,0,.15,3.29,1.58,1.58,0,0,0,.71-.17l12.74,12.77H98.34l17.48-17.52h18.54v-1.08h-19ZM63.12,109.53a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,1.14,0A.54.54,0,0,1,63.12,109.53Z" style="fill:#c5c9e0"></path> </svg> </div>
    <p>本文发布于<a href="https://www.oomkill.com/about" target="_blank">Cylon的收藏册</a>，转载请著名原文链接~</p>
    <p>链接：<a href="https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/" target="_blank">https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/</a></p>
    <p style="margin-bottom: 0px;">版权：本作品采用<a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">「署名-非商业性使用-相同方式共享 4.0 国际」</a> 许可协议进行许可。</p>
    </div>
</div>
  </div>

  <footer class="post-footer">
    
<nav class="paginav">
  <a class="prev" href="https://www.oomkill.com/2022/08/ch22-custom-scheduler/">
    <span class="title"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select: text;"><line x1="19" y1="12" x2="5" y2="12" style="user-select: text;"></line><polyline points="12 19 5 12 12 5" style="user-select: text;"></polyline>
      </polyline></svg>&nbsp; </span>
    
    <span>基于Prometheus的Kubernetes网络调度器</span>
  </a>
  <a class="next" href="https://www.oomkill.com/2022/07/ch20-schedule-workflow/" >
    <span class="title"> </span>
    
    <span>kube-scheduler的调度上下文&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select: text;"><line x1="5" y1="12" x2="19" y2="12" style="user-select: text;"></line><polyline points="12 5 19 12 12 19" style="user-select: text;"></polyline></svg></span>
  </a>
</nav>

  </footer>

  
  <div class="pagination__title">
    <span class="pagination__title-h"></span>
  </div>
  
  
  
  
    <div class="comments-separator"></div>
    

<h3 class="relatedContentTitle" >相关阅读</h3>
<ul class="relatedContent">
	
	<li><a href="/2022/07/ch20-schedule-workflow/"><span>kube-scheduler的调度上下文</span></a></li>
	
	<li><a href="/2022/07/ch16-scheduler/"><span>kubernetes的决策组件 - kube-scheduler原理分析</span></a></li>
	
	<li><a href="/2022/07/ch33-admission-webhook/"><span>深入理解Kubernetes 4A - Admission Control源码解析</span></a></li>
	
	<li><a href="/2022/06/ch27-leader-election/"><span>源码分析Kubernetes HA机制 - leader election</span></a></li>
	
	<li><a href="/2022/06/ch15-controller-runtime/"><span>源码分析Kubernetes controller组件 - controller-runtime</span></a></li>
	
</ul>

  

  
    
      <div class="comments-separator"></div>
<div class="comments">
    <script>
    function loadComment() {
        let theme = localStorage.getItem('pref-theme') === 'dark' ? 'dark' : 'light';
        let s = document.createElement('script');
        s.src = 'https://giscus.app/client.js';
        s.setAttribute('data-repo', 'cylonchau\/cylonchau.github.io');
        s.setAttribute('data-repo-id', 'R_kgDOIRlNSQ');
        s.setAttribute('data-category', 'Announcements');
        s.setAttribute('data-category-id', 'DIC_kwDOIRlNSc4CXy1U');
        s.setAttribute('data-mapping', 'title');
        s.setAttribute('data-reactions-enabled', '1');
        s.setAttribute('data-emit-metadata', '1');
        s.setAttribute('data-input-position', 'top');
        s.setAttribute('data-lang', 'zh-TW');
        s.setAttribute('data-theme', theme);
        s.setAttribute('crossorigin', 'anonymous');
        s.setAttribute('async', '');
        document.querySelector('div.comments').innerHTML = '';
        document.querySelector('div.comments').appendChild(s);
    }
    loadComment();
    </script>
</div>
</article>
    </main>
    
<footer class="footer">
  <p>
  Copyright
  <span>&copy; 2024 <a href="https://www.oomkill.com">Cylon&#39;s Collection</a></span></p>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> on github-page & Theme
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '1' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>

<script>
  document.addEventListener('scroll', function (e) {
      const readProgress = document.getElementById("read_progress");
      const scrollHeight = document.documentElement.scrollHeight;
      const clientHeight = document.documentElement.clientHeight;
      const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
      readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
  })
</script>

<script>
  var menu = document.getElementById('menu')
  if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
          localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
  }

  document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
          e.preventDefault();
          var id = this.getAttribute("href").substr(1);
          if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
              document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                  behavior: "smooth"
              });
          } else {
              document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
          }
          if (id === "top") {
              history.replaceState(null, null, " ");
          } else {
              history.pushState(null, null, `#${id}`);
          }
      });
  });
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
      if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
          mybutton.style.visibility = "visible";
          mybutton.style.opacity = "1";
      } else {
          mybutton.style.visibility = "hidden";
          mybutton.style.opacity = "0";
      }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'copy';

    function copyingDone() {
      copybutton.innerText = 'copied';
      setTimeout(() => {
        copybutton.innerText = 'copy';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

<script src="/js/instantclick.min.js" data-no-instant
></script>
<script data-no-instant>
  
  
  
  
  
  
  InstantClick.init();
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.6.0/mermaid.min.js" crossorigin="anonymous"></script>
<script>
    mermaid.init(undefined, '.language-mermaid');
</script>
</body>

</html>
