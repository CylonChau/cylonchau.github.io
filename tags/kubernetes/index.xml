<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>kubernetes on Cylon&#39;s Collection</title>
    <link>https://www.oomkill.com/tags/kubernetes/</link>
    <description>Recent content in kubernetes on Cylon&#39;s Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 24 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.oomkill.com/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>debian12 - 高版本系统安装旧版本k8s异常处理</title>
      <link>https://www.oomkill.com/2024/11/debian12-install-k8s-1.16/</link>
      <pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/11/debian12-install-k8s-1.16/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>今日在部署旧版本 k8s 集群 (1.16.10) 时出现错误，主要是在新版本操作系统上部署老版本 k8s，kubelet会出现如下错误</p>
<pre><code class="language-bash">W1123 22:31:47.383423    3686 server.go:605] failed to get the kubelet's cgroup: mountpoint for cpu not found.  Kubelet system container metrics may be missing.
W1123 22:31:47.383572    3686 server.go:612] failed to get the container runtime's cgroup: failed to get container name for docker process: mountpoint for cpu not found. Runtime system container metrics may be missing.
</code></pre>
<h2 id="错误原因">错误原因</h2>
<p>上面的报错是 Kubelet 无法正确访问 Docker 容器运行时的 cgroup 信息，特别是关于 CPU 使用的 cgroup 信息</p>
<blockquote>
<p>从 Linux 内核 4.5 起，cgroup v2 被引入并逐渐取代了 cgroup v1。如果系统启用了 cgroup v2，而 Docker 或 Kubelet 并没有正确配置，可能会导致 Kubelet 无法访问 cgroup 信息。</p>
</blockquote>
<p>查看系统 cgroup 版本</p>
<pre><code class="language-bash">$ mount | grep cgroup
cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)
</code></pre>
<h2 id="更改docker和kubelet的cgroup版本">更改Docker和Kubelet的cgroup版本</h2>
<p>更改 Docker 的 cgroup 版本</p>
<pre><code class="language-bash">$ docker info |grep Cgroup
 Cgroup Driver: systemd
 Cgroup Version: 2
</code></pre>
<p>强制 Docker 使用 cgroup v1</p>
<pre><code class="language-bash">{
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=cgroupfs&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;
}
</code></pre>
<p>查看结果</p>
<pre><code class="language-bash">root@debian-template:~# docker info|grep cgroup
 Cgroup Driver: cgroupfs
</code></pre>
<p>配置 kubelet 使用 cgroup v1</p>
<p>修改 kubelet 参数</p>
<pre><code class="language-bash">--cgroup-driver=cgroupfs
</code></pre>
<h2 id="让操作系统挂载-cgroup-v1">让操作系统挂载 cgroup v1</h2>
<p>查看系统 cgroup 版本</p>
<pre><code class="language-bash">$ mount | grep cgroup
cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)
</code></pre>
<p>手动挂载</p>
<p>通常手动挂载是不成功的</p>
<pre><code class="language-bash">mount: /sys/fs/cgroup: cgroup already mounted or mount point busy.
       dmesg(1) may have more information after failed mount system call.
</code></pre>
<p>需要修改启动参数让操作系统挂载上 cgroup v1</p>
<pre><code class="language-bash">vi /etc/default/grub
</code></pre>
<p>在 <code>GRUB_CMDLINE_LINUX_DEFAULT</code> 行中，添加 <code>cgroup_no_v2=1</code> 和 <code>systemd.unified_cgroup_hierarchy=0</code> 选项，确保系统使用 <strong>cgroup v1</strong></p>
<pre><code class="language-bash">GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet cgroup_no_v2=1 systemd.unified_cgroup_hierarchy=0&quot;
</code></pre>
<p>更新 GRUB 配置</p>
<pre><code class="language-bash">update-grub
</code></pre>
<p>重启后查看挂载信息</p>
<pre><code class="language-bash">$ mount | grep group
tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,size=4096k,nr_inodes=1024,mode=755,inode64)
cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate)
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
cgroup on /sys/fs/cgroup/misc type cgroup (rw,nosuid,nodev,noexec,relatime,misc)
</code></pre>
<p>此时 kubelet 可以正常启动了</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>构建集群kubernetes v1.28并使用kine和mysql替换etcd</title>
      <link>https://www.oomkill.com/2024/06/kubernetes-without-etcd-step-by-step/</link>
      <pubDate>Sun, 30 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/06/kubernetes-without-etcd-step-by-step/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>在本文中，将探讨使用 k3s 的 kine 项目来替换掉 etcd，并通过实验使用 kubeadm 去 run 一个 k8s 集群，并用 k3s 的 kine 项目来替换掉 etcd。</p>
<h2 id="为什么使用-kine">为什么使用 kine</h2>
<p>etcd 在 Kubernetes 之外基本上没有应用的场景，并且 etcd 迭代也比较慢，由于没有人愿意维护因此一直在衰退 <sup><a href="#1">[1]</a></sup>，并且，Kubernetes 集群中，etcd 也是一个影响集群规模的重大因素。并且 K3S 存在一个项目 Kine 可以使用关系型数据库运行，这样对集群维护者来说可以不需要维护复杂的 etcd 集群，由于关系型数据库有很多高可用方案，这将使得 k8s 集群规模变成了无限可能。</p>
<h2 id="kine-介绍">Kine 介绍</h2>
<p>前文提到，kubernetes (kube-apiserver) 与 etcd 是耦合的，如果我们要使用 RDBMS 去替换 etcd 就需要实现 etcd 的接口，那么这个项目就是 Kine <sup><a href="#2">[2]</a></sup>。</p>
<p>Kine 是一个 etcdshim，处于 kube-apiserver 和 RDBMS 的中间层，它实现了 etcdAPI的子集（不是etcd的全部功能），Kine 在 RDBMS  数据库之上实现了简单的多版本并发控制；将所有信息存储在一个表中；每行存储此 key 的修订, key, 当前值, 先前值, 先前修订，以及表示该 Key 是已创建还是已删除的标记，通过这种机制可以作为 shim 层来替换 etcd。</p>
<blockquote>
<p>简单提一句，shim 是计算机程序设计中的术语，表现为一个小型函数库，服务等，通过截取 API 调用，修改传入参数，来处理自行处理对应操作或者将操作交由其它地方执行。</p>
<p>总的来说 shim 是一种可以在新环境中支持老 API，也可以在老环境里支持新 API 辅助运行库或服务，在云原生场景中，我们经常看到 docker-shim，cri-shim 等。</p>
</blockquote>
<h2 id="前提条件">前提条件</h2>
<p>本文实验环境使用的软件版本如下</p>
<table>
<thead>
<tr>
<th>软件/硬件</th>
<th>版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>操作系统</td>
<td>Debian 11(bullseye) 2C/4G</td>
</tr>
<tr>
<td>Kubernetes版本</td>
<td>v1.28.11(截至文章编写时间的最新版)</td>
</tr>
<tr>
<td>Kubernetes集群部署工具</td>
<td>kubeadm</td>
</tr>
<tr>
<td>Kine</td>
<td>v0.11.10 (截至文章编写时间的最新版)</td>
</tr>
<tr>
<td>MySQL</td>
<td>Docker运行，镜像 <code>mysql:5.7</code></td>
</tr>
</tbody>
</table>
<h2 id="使用-kubeadm-构建控制平面">使用 kubeadm 构建控制平面</h2>
<p>为了展现 kine 的作用，首先我们需要准备一个 k8s 集群，这里简单使用 kubeadm + containerd 来构建一个 kuebrnetes 集群。</p>
<h3 id="安装-containerd">安装 containerd</h3>
<h4 id="载入内核依赖项">载入内核依赖项</h4>
<p>containerd 或 docker 的安装都需要内核支持 <code>overlay</code> 和  <code>br_netfilter</code>  模块，overlay 为 containerd 运行的文件系统，netfiler 用于维护容器内 (<em>inter-container</em>) 的网络。所以我们需要加载对应的内核模块。</p>
<pre><code class="language-bash">cat &lt;&lt;EOF | tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
</code></pre>
<p>手动执行下面命令</p>
<pre><code class="language-bash">modprobe overlay &amp;&amp; \
modprobe br_netfilter
</code></pre>
<h4 id="通过仓库-containerd">通过仓库 containerd</h4>
<p>contanerd 是作为 docker-ce 的下层，所以很多 Linux 发行版都有对应的包管理工具的仓库，这里面维护了基本上比较新的版本，可以直接在对应操作系统下载</p>
<p>CentOS</p>
<pre><code class="language-bash">yum install yum-utils -y &amp;&amp; \
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo &amp;&amp; \
yum install containerd.io -y
</code></pre>
<p>Debian</p>
<p>Debian仓库中通常都有比较新版本的 containerd，可以直接安装</p>
<pre><code class="language-bash">apt list|grep containerd
</code></pre>
<p>安装</p>
<pre><code class="language-bash">apt -y install containerd
</code></pre>
<h4 id="离线安装">离线安装</h4>
<p>如果需要离线环境安装的话，可以在手动下载 containerd 和 runc 后传入内网</p>
<p>下载 Containerd 的二进制包，这里下载<code>containerd-&lt;VERSION&gt;-&lt;OS&gt;-&lt;ARCH&gt;.tar.gz </code> 格式名称的发行版，后边在单独下载安装 runc</p>
<pre><code class="language-bash">wget https://github.com/containerd/containerd/releases/download/v1.7.3/containerd-1.7.3-linux-amd64.tar.gz
</code></pre>
<p>将其解压缩到 <em><strong>/usr/local</strong></em> 下:</p>
<pre><code class="language-bash">tar Cxzvf /usr/local containerd-1.7.3-linux-amd64.tar.gz
</code></pre>
<p>接下来从 runc 的 github 上下载安装 runc，该二进制文件是静态构建的，并且应该适用于任何Linux发行版。</p>
<pre><code class="language-bash">wget https://github.com/opencontainers/runc/releases/download/v1.1.9/runc.amd64
install -m 755 runc.amd64 /usr/local/sbin/runc
</code></pre>
<p>为了通过 systemd 管理 containerd，请还需要从仓库中下载 <a href="https://raw.githubusercontent.com/containerd/containerd/main/containerd.service" target="_blank"
   rel="noopener nofollow noreferrer" >containerd.service</a> 单元文件</p>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target

[Service]
#uncomment to enable the experimental sbservice (sandboxed) version of containerd/cri integration
#Environment=&quot;ENABLE_CRI_SANDBOXES=sandboxed&quot;
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd

Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
LimitNOFILE=infinity
# Comment TasksMax if your systemd version does not supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h4 id="配置配置文件">配置配置文件</h4>
<pre><code class="language-bash">mkdir -p /etc/containerd &amp;&amp; \
containerd config default | sudo tee /etc/containerd/config.toml
</code></pre>
<h4 id="配置驱动为-systemd">配置驱动为 systemd</h4>
<p>将配置文件修改为实例所述</p>
<pre><code class="language-yaml">[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]
[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]
          runtime_type = &quot;io.containerd.runc.v2&quot;
          runtime_engine = &quot;&quot;
          runtime_root = &quot;&quot;
          privileged_without_host_devices = false
          base_runtime_spec = &quot;&quot;

[plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]
          SystemdCgroup = true
</code></pre>
<p>一键修改命令</p>
<pre><code class="language-bash">sed -i &quot;s/SystemdCgroup = false/SystemdCgroup = true/g&quot;  &quot;${CONTAINDERD_CONFIG_PATH}&quot;
</code></pre>
<h4 id="启动服务">启动服务</h4>
<pre><code class="language-bash">systemctl enable --now containerd &amp;&amp; \
systemctl restart containerd
</code></pre>
<h3 id="使用kubeadm构建集群">使用kubeadm构建集群</h3>
<h4 id="加载内核依赖项">加载内核依赖项</h4>
<pre><code class="language-bash">cat &gt; /etc/modules-load.d/kubernetes.conf &lt;&lt;EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
EOF
</code></pre>
<p>执行以下命令使配置立即生效:</p>
<pre><code class="language-bash">modprobe ip_vs &amp;&amp; \
modprobe ip_vs_rr &amp;&amp; \
modprobe ip_vs_wrr &amp;&amp; \
modprobe ip_vs_sh
</code></pre>
<h4 id="安装kubeadm-kubelet-kubectl">安装kubeadm kubelet kubectl</h4>
<p>安装 kubeadm 可以参考官网的步骤来 <sup><a href="#3">[3]</a></sup></p>
<h4 id="使用基于debian-包管理仓库">使用基于debian 包管理仓库</h4>
<p>使用 Kubernetes apt 仓库</p>
<pre><code class="language-bash">apt-get install -y apt-transport-https ca-certificates curl gpg
</code></pre>
<p>下载公共签名key</p>
<pre><code class="language-bash">curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
</code></pre>
<p>添加适合的 k8s 版本仓库，这里是 1.28</p>
<pre><code class="language-bash"># This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
</code></pre>
<p>更新包索引</p>
<pre><code class="language-bash">apt update &amp;&amp; \
apt install -y kubelet=1.28.11-1.1 kubeadm=1.28.11-1.1 kubectl=1.28.11-1.1
</code></pre>
<h4 id="不使用包管理工具">不使用包管理工具</h4>
<p>下载  <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code>  二进制文件</p>
<pre><code class="language-bash"># 这个文件内包含的是 kubernetes 最新稳定版的版本号，如果要安装最新版可以取消掉这行注释
# RELEASE=&quot;$(curl -sSL https://dl.k8s.io/release/stable.txt)&quot;
RELEASE=&quot;v1.28.11&quot;
ARCH=&quot;amd64&quot;
DOWNLOAD_DIR=&quot;/usr/local/bin&quot;
mkdir -p &quot;$DOWNLOAD_DIR&quot;

cd $DOWNLOAD_DIR
sudo curl -L --remote-name-all https://dl.k8s.io/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet}
sudo chmod +x {kubeadm,kubelet}
</code></pre>
<p>下载 kubelet 的 system单元文件 或手动添加所需的 systemd 单元文件</p>
<pre><code class="language-bash"># v0.16.2 是一个固定的版本号，不是 kubernetes 版本
RELEASE_VERSION=&quot;v0.16.2&quot;
curl -sSL &quot;https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/krel/templates/latest/kubelet/kubelet.service&quot; | sed &quot;s:/usr/bin:${DOWNLOAD_DIR}:g&quot; | sudo tee /etc/systemd/system/kubelet.service

# kubelet.service 是一个单元文件
# systemd 的 service.d 目录是一个固定写法，这里表示可以使用 .conf 结尾的文件来覆盖这个服务的单元文件
mkdir -p /etc/systemd/system/kubelet.service.d
curl -sSL &quot;https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/krel/templates/latest/kubeadm/10-kubeadm.conf&quot; | sed &quot;s:/usr/bin:${DOWNLOAD_DIR}:g&quot; | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre>
<p>或者手动创建 kubelet.serivce 的 systemd 的单元文件</p>
<blockquote>
<p>这个文件是将 rpm 或 dpkg 包的 kubelet.service 和上述 10-kubeadm.conf 融合为一起的，效果是相同的</p>
</blockquote>
<pre><code class="language-bash">cat &lt;&lt; EOF &gt; /usr/lib/systemd/system/kubelet.serivce
[Unit]
Description=kubelet: The Kubernetes Node Agent
Documentation=https://kubernetes.io/docs/
Wants=network-online.target
After=network-online.target

# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;
Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;
# This is a file that &quot;kubeadm init&quot; and &quot;kubeadm join&quot; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/sysconfig/kubelet
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
Restart=always
StartLimitInterval=0
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF
</code></pre>
<h4 id="离线环境镜像下载">离线环境镜像下载</h4>
<p>列出所使用的镜像</p>
<pre><code class="language-bash">$ kubeadm config images list --kubernetes-version=1.28.8
registry.k8s.io/kube-apiserver:v1.28.8
registry.k8s.io/kube-controller-manager:v1.28.8
registry.k8s.io/kube-scheduler:v1.28.8
registry.k8s.io/kube-proxy:v1.28.8
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.10.1
</code></pre>
<p>下载对应镜像，并上传到私有仓库</p>
<pre><code class="language-bash">for n in `./kubeadm config images list --kubernetes-version=1.28.11`;
do
    docker pull $n; docker tag $n `echo $n | sed 's|registry.k8s.io|img.xxxx.com/system|'`
    docker push `echo $n | sed 's|registry.k8s.io|img.xxx.com/system|'`
done
</code></pre>
<p>生成配置文件</p>
<pre><code class="language-bash">./kubeadm config images list --image-repository img.xxx.com/system --kubernetes-version=v1.28.11

# 生成对应组件的的 kubeconfig
# kubelet
kubeadm config print init-defaults --component-configs KubeletConfiguration|grep -A 1000 'apiVersion: kubelet.config.k8s.io'|sed 's|0s|30s|g'
# kube-proxy
kubeadm config print init-defaults --component-configs KubeProxyConfiguration|grep -A 1000 'kubeproxy.config.k8s.io/'|sed 's|0s|30s|g'
</code></pre>
<p>使用配置文件安装</p>
<pre><code class="language-bash">kubeadm init --config kube.yaml  -v 10
</code></pre>
<p>使用命令初始化</p>
<pre><code class="language-bash">kubeadm init \
    --image-repository=img.xxx.com/system \
    --pod-network-cidr=10.10.0.0/16 \
    --service-cidr=10.11.0.0/24 \
    --kubernetes-version=v1.28.11 \
    --control-plane-endpoint=`hostname -I` \
    --apiserver-advertise-address=`hostname -I` \
    --apiserver-cert-extra-sans=`hostname -I` \
    --v=10
</code></pre>
<p>这个时候控制平面已经可以正常工作了</p>
<pre><code class="language-bash">$ kubectl --kubeconfig /etc/kubernetes/admin.conf  get pods -n kube-system
NAME                           READY   STATUS    RESTARTS   AGE
coredns-5dd5756b68-nvqwf       0/1     Pending   0          16h
coredns-5dd5756b68-t2tj5       0/1     Pending   0          16h
etcd-node                      1/1     Running   0          16h
kube-apiserver-node            1/1     Running   0          16h
kube-controller-manager-node   1/1     Running   0          16h
kube-proxy-g6fpc               1/1     Running   0          16h
kube-scheduler-node            1/1     Running   0          16h
</code></pre>
<h2 id="使用-kine-来替换-etcd">使用 kine 来替换 etcd</h2>
<h3 id="查看官方示例">查看官方示例</h3>
<p>首先根据 kine 官方 example 来查看最小示例的来学习如何使用 kine <sup><a href="#4">[4]</a></sup>，通过文章得知，kine 运行有两种方式，kine 与数据库之间的使用 ssl 链接。</p>
<p>mysql</p>
<pre><code class="language-bash">kine --endpoint &quot;mysql://root:$PASSWORD@tcp(localhost:3306)/kine&quot;
--ca-file ca.crt --cert-file server.crt --key-file server.key
</code></pre>
<p>postgres</p>
<pre><code class="language-bash">kine --endpoint=&quot;postgres://$(POSTGRES_USERNAME):$(POSTGRES_PASSWORD)@localhost:5432/postgres&quot;
      --ca-file=/var/lib/postgresql/ca.crt
      --cert-file=/var/lib/postgresql/server.crt
      --key-file=/var/lib/postgresql/server.key
</code></pre>
<p>这时我们需要查看一下 kine 的参数</p>
<pre><code class="language-bash">GLOBAL OPTIONS:
   --listen-address value                     (default: &quot;0.0.0.0:2379&quot;)
   --endpoint value                           Storage endpoint (default is sqlite)
   --ca-file value                            CA cert for DB connection
   --cert-file value                          Certificate for DB connection
   --server-cert-file value                   Certificate for etcd connection
   --server-key-file value                    Key file for etcd connection
   --datastore-max-idle-connections value     Maximum number of idle connections retained by datastore. If value = 0, the system default will be used. If value &lt; 0, idle connections will not be reused. (default: 0)
   --datastore-max-open-connections value     Maximum number of open connections used by datastore. If value &lt;= 0, then there is no limit (default: 0)
   --datastore-connection-max-lifetime value  Maximum amount of time a connection may be reused. If value &lt;= 0, then there is no limit. (default: 0s)
   --key-file value                           Key file for DB connection
   --metrics-bind-address value               The address the metric endpoint binds to. Default :8080, set 0 to disable metrics serving. (default: &quot;:8080&quot;)
   --slow-sql-threshold value                 The duration which SQL executed longer than will be logged. Default 1s, set &lt;= 0 to disable slow SQL log. (default: 1s)
   --metrics-enable-profiling                 Enable net/http/pprof handlers on the metrics bind address. Default is false. (default: false)
   --watch-progress-notify-interval value     Interval between periodic watch progress notifications. Default is 10m. (default: 10m0s)
   --debug                                    (default: false)
   --help, -h                                 show help
   --version, -v                              print the version
</code></pre>
<p>通过参数得知，上面的除了官方给出的，kine 与数据库之间的连接也可以不使用 ssl，并通过 <code>--server-cert-file</code>  与 <code>--server-key-file</code>  来作为 kube-apiserver 连接 etcd 所使用的证书指定给 kine 就可以启动了。</p>
<h3 id="编写静态文件">编写静态文件</h3>
<p>这里我们只需要删除 <code>/etc/kubernetes/manifests/etcd.yaml</code> 并将 etcd 使用的证书挂载到 kine pod 中，那么我们编写 <code>/etc/kubernetes/manifests/kine.yaml</code> 文件。</p>
<pre><code class="language-yaml">cat /etc/kubernetes/manifests/kine.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kine
    tier: control-plane
  name: kine
  namespace: kube-system
spec:
  containers:
  - name: kine
    command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;--&quot; ]
    args: [ 'kine --endpoint=&quot;mysql://root:111@tcp(10.0.0.1:3306)/kine&quot;
 --server-cert-file=/etc/kubernetes/pki/etcd/server.crt
 --server-key-file=/etc/kubernetes/pki/etcd/server.key' ]
    image: docker.io/rancher/kine:v0.11.10-amd64
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 250m
    volumeMounts:
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
status: {}
</code></pre>
<p>kubuadm 生成的 kubelet 的 KubeletConfiguration 文件，中静态文件得路径参数 “staticPodPath”</p>
<pre><code class="language-yaml">$ cat /var/lib/kubelet/config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
...
staticPodPath: /etc/kubernetes/manifests
</code></pre>
<p>这个时候可以启动 kubelet 服务，然后查看静态 Pod，此时可以看到， kube-system 名称空间 已经没有 etcd pod了</p>
<pre><code class="language-bash">$ kubectl --kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system
NAMESPACE     NAME                           READY   STATUS    RESTARTS   AGE
kube-system   kine-node                      1/1     Running   0          17s
kube-system   kube-apiserver-node            1/1     Running   19         7m15s
kube-system   kube-controller-manager-node   1/1     Running   6          7m5s
kube-system   kube-scheduler-node            1/1     Running   6          7m2s
</code></pre>
<p>此时就可以继续部署 k8s 的 worker 节点和 CNI 了</p>
<h2 id="探索-kine">探索 kine</h2>
<p>我们可以查看数据库表结构，来探索 kine 是如何实现的 etcdAPI 转换的，我们可以看到，kine 会在启动参数中配置的库名 创建对应的数据库，并且仅有一个表 kine</p>
<pre><code class="language-bash">mysql&gt; show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| kine               |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
5 rows in set (0.01 sec)

mysql&gt; show tables;
+----------------+
| Tables_in_kine |
+----------------+
| kine           |
+----------------+
</code></pre>
<p>观察表结构</p>
<pre><code class="language-bash">mysql&gt; desc kine;
+-----------------+---------------------+------+-----+---------+----------------+
| Field           | Type                | Null | Key | Default | Extra          |
+-----------------+---------------------+------+-----+---------+----------------+
| id              | bigint(20) unsigned | NO   | PRI | NULL    | auto_increment |
| name            | varchar(630)        | YES  | MUL | NULL    |                |
| created         | int(11)             | YES  |     | NULL    |                |
| deleted         | int(11)             | YES  |     | NULL    |                |
| create_revision | bigint(20) unsigned | YES  |     | NULL    |                |
| prev_revision   | bigint(20) unsigned | YES  | MUL | NULL    |                |
| lease           | int(11)             | YES  |     | NULL    |                |
| value           | mediumblob          | YES  |     | NULL    |                |
| old_value       | mediumblob          | YES  |     | NULL    |                |
+-----------------+---------------------+------+-----+---------+----------------+
9 rows in set (0.00 sec)
</code></pre>
<p>查看数据是如何存储的</p>
<pre><code class="language-bash">mysql&gt; select count(id),name from kine group by name;
+-----------+---------------------------------------------------------------------------------------------+
| count(id) | name                                                                                        |
+-----------+---------------------------------------------------------------------------------------------+
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.                                            |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.admissionregistration.k8s.io                |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.apiextensions.k8s.io                        |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.apps                                        |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.authentication.k8s.io                       |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.authorization.k8s.io                        |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.autoscaling                                 |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.batch                                       |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.certificates.k8s.io                         |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.coordination.k8s.io                         |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.discovery.k8s.io                            |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.events.k8s.io                               |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.networking.k8s.io                           |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.node.k8s.io                                 |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.policy                                      |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.rbac.authorization.k8s.io                   |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.scheduling.k8s.io                           |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1.storage.k8s.io                              |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1beta2.flowcontrol.apiserver.k8s.io           |
|         1 | /registry/apiregistration.k8s.io/apiservices/v1beta3.flowcontrol.apiserver.k8s.io           |
|         1 | /registry/apiregistration.k8s.io/apiservices/v2.autoscaling                                 |
|         1 | /registry/clusterrolebindings/cluster-admin                                                 |
|         1 | /registry/clusterrolebindings/system:basic-user                                             |
|         1 | /registry/clusterrolebindings/system:controller:attachdetach-controller                     |
|         1 | /registry/clusterrolebindings/system:controller:certificate-controller                      |
|         1 | /registry/clusterrolebindings/system:controller:clusterrole-aggregation-controller          |
|         1 | /registry/clusterrolebindings/system:controller:cronjob-controller                          |
|         1 | /registry/clusterrolebindings/system:controller:daemon-set-controller                       |
|         1 | /registry/clusterrolebindings/system:controller:deployment-controller                       |
|         1 | /registry/clusterrolebindings/system:controller:disruption-controller                       |
|         1 | /registry/clusterrolebindings/system:controller:endpoint-controller                         |
|         1 | /registry/clusterrolebindings/system:controller:endpointslice-controller                    |
|         1 | /registry/clusterrolebindings/system:controller:endpointslicemirroring-controller           |
|         1 | /registry/clusterrolebindings/system:controller:ephemeral-volume-controller                 |
|         1 | /registry/clusterrolebindings/system:controller:expand-controller                           |
|         1 | /registry/clusterrolebindings/system:controller:generic-garbage-collector                   |
|         1 | /registry/clusterrolebindings/system:controller:horizontal-pod-autoscaler                   |
|         1 | /registry/clusterrolebindings/system:controller:job-controller                              |
|         1 | /registry/clusterrolebindings/system:controller:namespace-controller                        |
|         1 | /registry/clusterrolebindings/system:controller:node-controller                             |
|         1 | /registry/clusterrolebindings/system:controller:persistent-volume-binder                    |
|         1 | /registry/clusterrolebindings/system:controller:pod-garbage-collector                       |
|         1 | /registry/clusterrolebindings/system:controller:pv-protection-controller                    |
|         1 | /registry/clusterrolebindings/system:controller:pvc-protection-controller                   |
|         1 | /registry/clusterrolebindings/system:controller:replicaset-controller                       |
|         1 | /registry/clusterrolebindings/system:controller:replication-controller                      |
|         1 | /registry/clusterrolebindings/system:controller:resourcequota-controller                    |
|         1 | /registry/clusterrolebindings/system:controller:root-ca-cert-publisher                      |
|         1 | /registry/clusterrolebindings/system:controller:route-controller                            |
|         1 | /registry/clusterrolebindings/system:controller:service-account-controller                  |
|         1 | /registry/clusterrolebindings/system:controller:service-controller                          |
|         1 | /registry/clusterrolebindings/system:controller:statefulset-controller                      |
|         1 | /registry/clusterrolebindings/system:controller:ttl-after-finished-controller               |
|         1 | /registry/clusterrolebindings/system:controller:ttl-controller                              |
|         1 | /registry/clusterrolebindings/system:discovery                                              |
|         1 | /registry/clusterrolebindings/system:kube-controller-manager                                |
|         1 | /registry/clusterrolebindings/system:kube-dns                                               |
|         1 | /registry/clusterrolebindings/system:kube-scheduler                                         |
|         1 | /registry/clusterrolebindings/system:monitoring                                             |
|         1 | /registry/clusterrolebindings/system:node                                                   |
|         1 | /registry/clusterrolebindings/system:node-proxier                                           |
|         1 | /registry/clusterrolebindings/system:public-info-viewer                                     |
|         1 | /registry/clusterrolebindings/system:service-account-issuer-discovery                       |
|         1 | /registry/clusterrolebindings/system:volume-scheduler                                       |
|         1 | /registry/clusterroles/admin                                                                |
|         1 | /registry/clusterroles/cluster-admin                                                        |
|         1 | /registry/clusterroles/edit                                                                 |
|         1 | /registry/clusterroles/system:aggregate-to-admin                                            |
|         1 | /registry/clusterroles/system:aggregate-to-edit                                             |
|         1 | /registry/clusterroles/system:aggregate-to-view                                             |
|         1 | /registry/clusterroles/system:auth-delegator                                                |
|         1 | /registry/clusterroles/system:basic-user                                                    |
|         1 | /registry/clusterroles/system:certificates.k8s.io:certificatesigningrequests:nodeclient     |
|         1 | /registry/clusterroles/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient |
|         1 | /registry/clusterroles/system:certificates.k8s.io:kube-apiserver-client-approver            |
|         1 | /registry/clusterroles/system:certificates.k8s.io:kube-apiserver-client-kubelet-approver    |
|         1 | /registry/clusterroles/system:certificates.k8s.io:kubelet-serving-approver                  |
|         1 | /registry/clusterroles/system:certificates.k8s.io:legacy-unknown-approver                   |
|         1 | /registry/clusterroles/system:controller:attachdetach-controller                            |
|         1 | /registry/clusterroles/system:controller:certificate-controller                             |
|         1 | /registry/clusterroles/system:controller:clusterrole-aggregation-controller                 |
|         1 | /registry/clusterroles/system:controller:cronjob-controller                                 |
|         1 | /registry/clusterroles/system:controller:daemon-set-controller                              |
|         1 | /registry/clusterroles/system:controller:deployment-controller                              |
|         1 | /registry/clusterroles/system:controller:disruption-controller                              |
|         1 | /registry/clusterroles/system:controller:endpoint-controller                                |
|         1 | /registry/clusterroles/system:controller:endpointslice-controller                           |
|         1 | /registry/clusterroles/system:controller:endpointslicemirroring-controller                  |
|         1 | /registry/clusterroles/system:controller:ephemeral-volume-controller                        |
|         1 | /registry/clusterroles/system:controller:expand-controller                                  |
|         1 | /registry/clusterroles/system:controller:generic-garbage-collector                          |
|         1 | /registry/clusterroles/system:controller:horizontal-pod-autoscaler                          |
|         1 | /registry/clusterroles/system:controller:job-controller                                     |
|         1 | /registry/clusterroles/system:controller:namespace-controller                               |
|         1 | /registry/clusterroles/system:controller:node-controller                                    |
|         1 | /registry/clusterroles/system:controller:persistent-volume-binder                           |
|         1 | /registry/clusterroles/system:controller:pod-garbage-collector                              |
|         1 | /registry/clusterroles/system:controller:pv-protection-controller                           |
|         1 | /registry/clusterroles/system:controller:pvc-protection-controller                          |
|         1 | /registry/clusterroles/system:controller:replicaset-controller                              |
|         1 | /registry/clusterroles/system:controller:replication-controller                             |
|         1 | /registry/clusterroles/system:controller:resourcequota-controller                           |
|         1 | /registry/clusterroles/system:controller:root-ca-cert-publisher                             |
|         1 | /registry/clusterroles/system:controller:route-controller                                   |
|         1 | /registry/clusterroles/system:controller:service-account-controller                         |
|         1 | /registry/clusterroles/system:controller:service-controller                                 |
|         1 | /registry/clusterroles/system:controller:statefulset-controller                             |
|         1 | /registry/clusterroles/system:controller:ttl-after-finished-controller                      |
|         1 | /registry/clusterroles/system:controller:ttl-controller                                     |
|         1 | /registry/clusterroles/system:discovery                                                     |
|         1 | /registry/clusterroles/system:heapster                                                      |
|         1 | /registry/clusterroles/system:kube-aggregator                                               |
|         1 | /registry/clusterroles/system:kube-controller-manager                                       |
|         1 | /registry/clusterroles/system:kube-dns                                                      |
|         1 | /registry/clusterroles/system:kube-scheduler                                                |
|         1 | /registry/clusterroles/system:kubelet-api-admin                                             |
|         1 | /registry/clusterroles/system:monitoring                                                    |
|         1 | /registry/clusterroles/system:node                                                          |
|         1 | /registry/clusterroles/system:node-bootstrapper                                             |
|         1 | /registry/clusterroles/system:node-problem-detector                                         |
|         1 | /registry/clusterroles/system:node-proxier                                                  |
|         1 | /registry/clusterroles/system:persistent-volume-provisioner                                 |
|         1 | /registry/clusterroles/system:public-info-viewer                                            |
|         1 | /registry/clusterroles/system:service-account-issuer-discovery                              |
|         1 | /registry/clusterroles/system:volume-scheduler                                              |
|         1 | /registry/clusterroles/view                                                                 |
|         1 | /registry/configmaps/default/kube-root-ca.crt                                               |
|         1 | /registry/configmaps/kube-node-lease/kube-root-ca.crt                                       |
|         1 | /registry/configmaps/kube-public/kube-root-ca.crt                                           |
|         1 | /registry/configmaps/kube-system/extension-apiserver-authentication                         |
|         1 | /registry/configmaps/kube-system/kube-apiserver-legacy-service-account-token-tracking       |
|         1 | /registry/configmaps/kube-system/kube-root-ca.crt                                           |
|         1 | /registry/csinodes/node                                                                     |
|         1 | /registry/endpointslices/default/kubernetes                                                 |
|         1 | /registry/events/default/node.17de1624f3c1624f                                              |
|         1 | /registry/events/default/node.17de1624f3c1e6bb                                              |
|         1 | /registry/events/default/node.17de1624f3c25c4f                                              |
|         1 | /registry/events/default/node.17de1624f5b37dfb                                              |
|         1 | /registry/events/default/node.17de1639e7890c71                                              |
|         1 | /registry/events/default/node.17de168dce4cdb68                                              |
|         1 | /registry/events/default/node.17de16a194521b80                                              |
|         1 | /registry/events/kube-system/kine-node.17de162525650d9b                                     |
|         1 | /registry/events/kube-system/kine-node.17de1625275ca2d7                                     |
|         1 | /registry/events/kube-system/kine-node.17de16252f773864                                     |
|         1 | /registry/events/kube-system/kine-node.17de1625a5af90c0                                     |
|         1 | /registry/events/kube-system/kine-node.17de169d120062cc                                     |
|         1 | /registry/events/kube-system/kine-node.17de169d1361dab8                                     |
|         1 | /registry/events/kube-system/kine-node.17de169d1855aee6                                     |
|         1 | /registry/events/kube-system/kine-node.17de16a1969b1ed6                                     |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de162513417e64                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de1625158e863e                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de162525e8ebd2                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de1629c37f0b35                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de1629f6bc718f                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de162ecf004a1d                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de162eff4060dd                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de1637f005507c                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de1661f1bd6879                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de16620f441326                           |
|         1 | /registry/events/kube-system/kube-apiserver-node.17de16a1985f63bd                           |
|         1 | /registry/events/kube-system/kube-controller-manager-node.17de162511a4b3be                  |
|         1 | /registry/events/kube-system/kube-controller-manager-node.17de162512f837ca                  |
|         1 | /registry/events/kube-system/kube-controller-manager-node.17de16251d8b658b                  |
|         1 | /registry/events/kube-system/kube-controller-manager-node.17de169b0537b6d0                  |
|         1 | /registry/events/kube-system/kube-controller-manager-node.17de16a1971f3999                  |
|         1 | /registry/events/kube-system/kube-controller-manager.17de1638e6568ffc                       |
|         1 | /registry/events/kube-system/kube-controller-manager.17de168d8ed8b9cc                       |
|         1 | /registry/events/kube-system/kube-controller-manager.17de16a150704739                       |
|         1 | /registry/events/kube-system/kube-scheduler-node.17de162512917b00                           |
|         1 | /registry/events/kube-system/kube-scheduler-node.17de16251515909b                           |
|         1 | /registry/events/kube-system/kube-scheduler-node.17de16252295ae29                           |
|         1 | /registry/events/kube-system/kube-scheduler-node.17de162a7ee366d4                           |
|         1 | /registry/events/kube-system/kube-scheduler-node.17de169c038ba9cc                           |
|         1 | /registry/events/kube-system/kube-scheduler-node.17de169cf8755bf3                           |
|         1 | /registry/events/kube-system/kube-scheduler-node.17de16a19797a620                           |
|         1 | /registry/events/kube-system/kube-scheduler.17de1643dd024555                                |
|         1 | /registry/events/kube-system/kube-scheduler.17de168dbd6f19b1                                |
|         1 | /registry/events/kube-system/kube-scheduler.17de16a24a03d6c8                                |
|         1 | /registry/flowschemas/catch-all                                                             |
|         1 | /registry/flowschemas/endpoint-controller                                                   |
|         1 | /registry/flowschemas/exempt                                                                |
|         1 | /registry/flowschemas/global-default                                                        |
|         1 | /registry/flowschemas/kube-controller-manager                                               |
|         1 | /registry/flowschemas/kube-scheduler                                                        |
|         1 | /registry/flowschemas/kube-system-service-accounts                                          |
|         1 | /registry/flowschemas/probes                                                                |
|         1 | /registry/flowschemas/service-accounts                                                      |
|         1 | /registry/flowschemas/system-leader-election                                                |
|         1 | /registry/flowschemas/system-node-high                                                      |
|         1 | /registry/flowschemas/system-nodes                                                          |
|         1 | /registry/flowschemas/workload-leader-election                                              |
|         1 | /registry/health                                                                            |
|        97 | /registry/leases/kube-node-lease/node                                                       |
|        97 | /registry/leases/kube-system/apiserver-6cazmjvz5glfjbabvahmi5cwfy                           |
|       484 | /registry/leases/kube-system/kube-controller-manager                                        |
|       485 | /registry/leases/kube-system/kube-scheduler                                                 |
|        99 | /registry/masterleases/10.0.0.14                                                            |
|        33 | /registry/minions/node                                                                      |
|         1 | /registry/namespaces/default                                                                |
|         1 | /registry/namespaces/kube-node-lease                                                        |
|         1 | /registry/namespaces/kube-public                                                            |
|         1 | /registry/namespaces/kube-system                                                            |
|         1 | /registry/pods/kube-system/kine-node                                                        |
|         1 | /registry/pods/kube-system/kube-apiserver-node                                              |
|         1 | /registry/pods/kube-system/kube-controller-manager-node                                     |
|         1 | /registry/pods/kube-system/kube-scheduler-node                                              |
|         1 | /registry/priorityclasses/system-cluster-critical                                           |
|         1 | /registry/priorityclasses/system-node-critical                                              |
|         1 | /registry/prioritylevelconfigurations/catch-all                                             |
|         1 | /registry/prioritylevelconfigurations/exempt                                                |
|         1 | /registry/prioritylevelconfigurations/global-default                                        |
|         1 | /registry/prioritylevelconfigurations/leader-election                                       |
|         1 | /registry/prioritylevelconfigurations/node-high                                             |
|         1 | /registry/prioritylevelconfigurations/system                                                |
|         1 | /registry/prioritylevelconfigurations/workload-high                                         |
|         1 | /registry/prioritylevelconfigurations/workload-low                                          |
|         1 | /registry/ranges/serviceips                                                                 |
|         1 | /registry/ranges/servicenodeports                                                           |
|         1 | /registry/rolebindings/kube-public/system:controller:bootstrap-signer                       |
|         1 | /registry/rolebindings/kube-system/system::extension-apiserver-authentication-reader        |
|         1 | /registry/rolebindings/kube-system/system::leader-locking-kube-controller-manager           |
|         1 | /registry/rolebindings/kube-system/system::leader-locking-kube-scheduler                    |
|         1 | /registry/rolebindings/kube-system/system:controller:bootstrap-signer                       |
|         1 | /registry/rolebindings/kube-system/system:controller:cloud-provider                         |
|         1 | /registry/rolebindings/kube-system/system:controller:token-cleaner                          |
|         1 | /registry/roles/kube-public/system:controller:bootstrap-signer                              |
|         1 | /registry/roles/kube-system/extension-apiserver-authentication-reader                       |
|         1 | /registry/roles/kube-system/system::leader-locking-kube-controller-manager                  |
|         1 | /registry/roles/kube-system/system::leader-locking-kube-scheduler                           |
|         1 | /registry/roles/kube-system/system:controller:bootstrap-signer                              |
|         1 | /registry/roles/kube-system/system:controller:cloud-provider                                |
|         1 | /registry/roles/kube-system/system:controller:token-cleaner                                 |
|         1 | /registry/serviceaccounts/default/default                                                   |
|         1 | /registry/serviceaccounts/kube-node-lease/default                                           |
|         1 | /registry/serviceaccounts/kube-public/default                                               |
|         1 | /registry/serviceaccounts/kube-system/attachdetach-controller                               |
|         1 | /registry/serviceaccounts/kube-system/bootstrap-signer                                      |
|         1 | /registry/serviceaccounts/kube-system/certificate-controller                                |
|         1 | /registry/serviceaccounts/kube-system/clusterrole-aggregation-controller                    |
|         1 | /registry/serviceaccounts/kube-system/cronjob-controller                                    |
|         1 | /registry/serviceaccounts/kube-system/daemon-set-controller                                 |
|         1 | /registry/serviceaccounts/kube-system/default                                               |
|         1 | /registry/serviceaccounts/kube-system/deployment-controller                                 |
|         1 | /registry/serviceaccounts/kube-system/disruption-controller                                 |
|         1 | /registry/serviceaccounts/kube-system/endpoint-controller                                   |
|         1 | /registry/serviceaccounts/kube-system/endpointslice-controller                              |
|         1 | /registry/serviceaccounts/kube-system/endpointslicemirroring-controller                     |
|         1 | /registry/serviceaccounts/kube-system/ephemeral-volume-controller                           |
|         1 | /registry/serviceaccounts/kube-system/expand-controller                                     |
|         1 | /registry/serviceaccounts/kube-system/generic-garbage-collector                             |
|         1 | /registry/serviceaccounts/kube-system/horizontal-pod-autoscaler                             |
|         1 | /registry/serviceaccounts/kube-system/job-controller                                        |
|         1 | /registry/serviceaccounts/kube-system/namespace-controller                                  |
|         1 | /registry/serviceaccounts/kube-system/node-controller                                       |
|         1 | /registry/serviceaccounts/kube-system/persistent-volume-binder                              |
|         1 | /registry/serviceaccounts/kube-system/pod-garbage-collector                                 |
|         1 | /registry/serviceaccounts/kube-system/pv-protection-controller                              |
|         1 | /registry/serviceaccounts/kube-system/pvc-protection-controller                             |
|         1 | /registry/serviceaccounts/kube-system/replicaset-controller                                 |
|         1 | /registry/serviceaccounts/kube-system/replication-controller                                |
|         1 | /registry/serviceaccounts/kube-system/resourcequota-controller                              |
|         1 | /registry/serviceaccounts/kube-system/root-ca-cert-publisher                                |
|         1 | /registry/serviceaccounts/kube-system/service-account-controller                            |
|         1 | /registry/serviceaccounts/kube-system/service-controller                                    |
|         1 | /registry/serviceaccounts/kube-system/statefulset-controller                                |
|         1 | /registry/serviceaccounts/kube-system/token-cleaner                                         |
|         1 | /registry/serviceaccounts/kube-system/ttl-after-finished-controller                         |
|         1 | /registry/serviceaccounts/kube-system/ttl-controller                                        |
|         1 | /registry/services/endpoints/default/kubernetes                                             |
|         1 | /registry/services/specs/default/kubernetes                                                 |
|         1 | compact_rev_key                                                                             |
+-----------+---------------------------------------------------------------------------------------------+
271 rows in set (0.00 sec)
</code></pre>
<p>如上所示，有一个名为的表 “kine”包含所有数据。Kine 使用数据库作为日志结构存储，因此来自 API 服务器的每次写入都会创建一个新行来存储已创建或更新的 Kubernetes 对象，“name” 列使用与 etcd 相同的存储结构 “<strong>/registry/RESOURCE_TYPE/NAMESPACE/NAME</strong>” 表示集群中对象。</p>
<h2 id="k3s-资源分析">k3s 资源分析</h2>
<p>k3s 官方提供了 Resource Profiling <sup><a href="#5">[5]</a></sup> 来对比了 RDBMS 与 etcd 的性能对比。</p>
<h2 id="总结">总结</h2>
<p>因为 RDBMS 大家都很熟悉，并且更高性能的分布式解决方案也有很多，例如 YugabyteDB (PostgreSQL兼容的分布式数据库)，也可以预创建 kine 表，通过分区形式将不同数据存储到不同的分区内。而且 k8s 对象的历史数据也是可以根据一定的规则进行删除，因为 kubernetes 中的对象都是实时协调的，所以也不怕误删除，这样就会使得 kubernetes 规模有更大扩展的可能。</p>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://groups.google.com/a/kubernetes.io/g/steering/c/e-O-tVSCJOk/m/N9IkiWLEAgAJ" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Worrying state of Etcd community</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://github.com/k3s-io/kine" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Kine (Kine is not etcd)</em></a></p>
<p><sup id="2">[3]</sup> <a href="https://v1-28.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Installing kubeadm, kubelet and kubectl</em></a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/k3s-io/kine/blob/v0.11.10/examples/minimal.md" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Minimal example of using kine</em></a></p>
<p><sup id="5">[5]</sup> <a href="https://docs.k3s.io/reference/resource-profiling" target="_blank"
   rel="noopener nofollow noreferrer" ><em>resource-profiling</em></a></p>
<p><sup id="6">[6]</sup> <a href="https://martinheinz.dev/blog/100" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Goodbye etcd, Hello PostgreSQL: Running Kubernetes with an SQL Database</em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入解析Kubernetes监控体系与prometheus-adapter</title>
      <link>https://www.oomkill.com/2024/05/prometheus-adapter-intro/</link>
      <pubDate>Fri, 31 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/05/prometheus-adapter-intro/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="kubernetes监控架构设计">Kubernetes监控架构设计</h2>
<h3 id="k8s监控设计背景说明">k8s监控设计背景说明</h3>
<p>根据 Kubernetes监控架构 <sup><a href="#1">1</a></sup>，Kubernetes 集群中的 metrcis 可以分为 <strong>系统指标</strong> (Core Metrics) 和 <strong>服务指标</strong> (service metrics) ; 系统指标(System metrics) 是通用的指标，通常可以从每一个被监控的实体中获得（例如，容器和节点的CPU和内存使用情况）。服务指标(Service metrics) 是在应用程序代码中显式定义并暴露的 (例如，API Server 处理的 500 错误数量)。</p>
<p>Kubernetes将系统指标分为两部分：</p>
<ul>
<li>核心指标 (core metrics) 是 Kubernetes 理解和用于其内部组件和核心工具操作的指标，例如：用于调度的指标 (包括资源估算算法的输入, 初始资源/VPA (vertical autoscaling)，集群自动扩缩 (cluster autoscaling)，水平Pod自动扩缩 (horizontal pod autoscaling ) 除自定义指标之外的指标)；Kube Dashboard 使用的指标，以及 “kubectl top” 命令使用的指标。</li>
<li>非核心指标 (non-core metrics) 是指不被 Kubernetes 解释的指标。我们一般假设这些指标包含核心指标 (但不一定是 Kubernetes 可理解的格式)，以及其他额外的指标。</li>
</ul>
<p>所以，kubernetes monitoring 的架构被设计拥有如下特点：</p>
<ul>
<li>通过标准的主 API (当前为主监控 API) 提供关于Node, Pod 和容器的核心系统指标，使得核心 Kubernetes 功能不依赖于非核心组件</li>
<li>kubelet 只导出有限的指标集，即核心 Kubernetes 组件正常运行所需的指标。</li>
<li>&hellip;</li>
</ul>
<h3 id="监控管道">监控管道</h3>
<p>Kubernetes 监控管道分为两个：</p>
<ul>
<li>核心指标管道 (<strong>core metrics pipeline</strong>) 由 Kubelet、资源估算器, 一个精简版 Heapster (metrics-server)，以及 api-server 中 master metrics API 组成。这些指标被核心系统组件使用，例如调度逻辑（如调度器和基于系统指标的HPA）和一些简单 UI 组件（如 kubectl top），这个管道并不打算与第三方监控系统集成。</li>
<li>监控管道：一个用于收集系统中的各种指标并将其暴露给最终用户端，以及通过适配器暴露给 HPA(用于自定义指标) 和  Infrastore 的。用户可以选择多种监控系统供应商（例如 Prometheus, metric-server），也可以完全不使用。</li>
</ul>
<h4 id="core-metrics-pipeline">Core Metrics Pipeline</h4>
<p>根据 kubernetes 监控设计文档可以得知，核心指标指</p>
<ul>
<li>使用这组核心指标，由Kubelet收集，并仅供 Kubernetes 系统组件使用，支持&quot;第一类资源隔离和利用特性&quot;。</li>
<li>不设计成面向用户的 API，而是尽可能通用，以支持未来的用户级组件。</li>
</ul>
<p>核心指标的包含三类：</p>
<ul>
<li>CpuUsage: 记录从创建对象开始的累计CPU使用时间。</li>
<li>MemoryUsage: 记录工作集内存使用量。</li>
<li>FilesystemUsage: 记录文件系统使用情况,包括已用字节数和已用Inode数。</li>
</ul>
<h4 id="monitoring-pipeline">Monitoring Pipeline</h4>
<p>根据 Kubernetes 监控设计文档 <sup><a href="#1">1</a></sup> 得知，监控管道用于与核心Kubernetes组件分开的系统，可以更加灵活。并且监控管道可以收集不同类型的指标：</p>
<ul>
<li>Core system metrics</li>
<li>Non-core system metrics</li>
<li>Service metrics from user application containers</li>
<li>Service metrics from Kubernetes infrastructure containers (using Prometheus instrumentation)</li>
</ul>
<p>监控管道主要用于根据自定义指标进行 HPA，监控管道提供了一个无状态的 API Adapter，用于拉去监控给 HPA</p>
<h3 id="指标api">指标API</h3>
<h4 id="api类别">API类别</h4>
<p>根据监控架构设计文档，Kubernetes 定义了两套指标 API，资源指标 API 和 自定义指标 API；Kubernetes 为资源指标 API 提供了两种实现：Heapster 和 metrics-server，而自定义指标 API 由不同的监控供应商实现。下面将详细描述每个 API。</p>
<ul>
<li>
<p>资源指标 API (Resource Metrics API)：该 API 允许消费者访问 Pod 和 Node 的资源指标（CPU &amp; Memory）</p>
<ul>
<li>The API is implemented by metrics-server and prometheus-adapter.</li>
</ul>
</li>
<li>
<p>自定义指标 API (Custom Metrics API)：该 API 允许消费者访问描述 Kubernetes 资源的任意指标。</p>
<ul>
<li>用户可以根据 <a href="https://github.com/kubernetes-sigs/custom-metrics-apiserver" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes-sigs/custom-metrics-apiserver </a> 仓库来自定义 API-server</li>
</ul>
</li>
</ul>
<h4 id="api的访问">API的访问</h4>
<p>资源指标，该 API 是在 <code>/apis/metrics.k8s.io/</code> ，可以使用 <code>kubectl proxy --port 8080</code> 代理后进行访问，</p>
<pre><code class="language-bash">$ kubectl proxy --port=8080
$ curl localhost:8080/apis/metrics.k8s.io/v1beta1/nodes
</code></pre>
<p>或者使用 <code>kubectl get --raw</code> 进行获取</p>
<pre><code class="language-bash">$ kubectl get --raw &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot; | jq 
</code></pre>
<p>自定义指标，该 API 是在 <code>/apis/custom.metrics.k8s.io/</code> ，访问的方式相同，用户通过该 PATH 进行访问。</p>
<pre><code class="language-bash"># 查看有哪些指标可用
$ kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/&quot; | jq
</code></pre>
<h2 id="prometheus-adapter">Prometheus-adapter</h2>
<p>通过上一章节介绍了kubernetes监控体系，这已经可以了解到了 prometheus-adapter 的定位；prometheus-adapter 是通过 kubernetes custom-metrics-apiserver 标准实现的一个 custom.metrics.k8s.io API，用于提供给 HPA 的一种指标适配器，可以将任何指标转化为 HPA 可用的指标。他全名为 <strong>Kubernetes Custom Metrics Adapter for Prometheus</strong>。</p>
<h3 id="prometheus-adapter配置文件详解">prometheus-adapter配置文件详解</h3>
<p>prometheus-adapter负责确定哪些指标以及如何去发现这些指标，根据这个标准，配置文件分为四个步骤来完成这套 “发现” 规则</p>
<p>每一个指标可以大致分为四个部分，对应在配置文件中：</p>
<ul>
<li><em>Discovery</em>  ，用于指定 adapter 应如何查找此规则的所有Prometheus指标。</li>
<li><em>Association</em>  ，用于指定 adapter 应如何确定特定指标与哪些 Kubernetes 资源相关联。</li>
<li><em>Naming</em> ，用于指定 adapter 应如何在自定义指标 API 中公开该指标。</li>
<li><em>Querying</em> ，用于指定如何将针对一个或多个 Kubernetes 对象的特定指标请求转换为对 Prometheus 的查询。</li>
</ul>
<p>配置文件如下所示，这是官方给出的样板<a href="https://github.com/kubernetes-sigs/prometheus-adapter/blob/v0.12.0/docs/sample-config.yaml" target="_blank"
   rel="noopener nofollow noreferrer" >配置文件</a>（文章编写时版本为0.12）</p>
<pre><code class="language-yaml">rules:
# Each rule represents a some naming and discovery logic.
# Each rule is executed independently of the others, so
# take care to avoid overlap.  As an optimization, rules
# with the same `seriesQuery` but different
# `name` or `seriesFilters` will use only one query to
# Prometheus for discovery.

# some of these rules are taken from the &quot;default&quot; configuration, which
# can be found in pkg/config/default.go

# this rule matches cumulative cAdvisor metrics measured in seconds
- seriesQuery: '{__name__=~&quot;^container_.*&quot;,container!=&quot;POD&quot;,namespace!=&quot;&quot;,pod!=&quot;&quot;}'
  resources:
    # skip specifying generic resource&lt;-&gt;label mappings, and just
    # attach only pod and namespace resources by mapping label names to group-resources
    overrides:
      namespace: {resource: &quot;namespace&quot;}
      pod: {resource: &quot;pod&quot;}
  # specify that the `container_` and `_seconds_total` suffixes should be removed.
  # this also introduces an implicit filter on metric family names
  name:
    # we use the value of the capture group implicitly as the API name
    # we could also explicitly write `as: &quot;$1&quot;`
    matches: &quot;^container_(.*)_seconds_total$&quot;
  # specify how to construct a query to fetch samples for a given series
  # This is a Go template where the `.Series` and `.LabelMatchers` string values
  # are available, and the delimiters are `&lt;&lt;` and `&gt;&gt;` to avoid conflicts with
  # the prometheus query language
  metricsQuery: &quot;sum(rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;,container!=&quot;POD&quot;}[2m])) by (&lt;&lt;.GroupBy&gt;&gt;)&quot;

# this rule matches cumulative cAdvisor metrics not measured in seconds
- seriesQuery: '{__name__=~&quot;^container_.*_total&quot;,container!=&quot;POD&quot;,namespace!=&quot;&quot;,pod!=&quot;&quot;}'
  resources:
    overrides:
      namespace: {resource: &quot;namespace&quot;}
      pod: {resource: &quot;pod&quot;}
  seriesFilters:
  # since this is a superset of the query above, we introduce an additional filter here
  - isNot: &quot;^container_.*_seconds_total$&quot;
  name: {matches: &quot;^container_(.*)_total$&quot;}
  metricsQuery: &quot;sum(rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;,container!=&quot;POD&quot;}[2m])) by (&lt;&lt;.GroupBy&gt;&gt;)&quot;

# this rule matches cumulative non-cAdvisor metrics
- seriesQuery: '{namespace!=&quot;&quot;,__name__!=&quot;^container_.*&quot;}'
  name: {matches: &quot;^(.*)_total$&quot;}
  resources:
    # specify an a generic mapping between resources and labels.  This
    # is a template, like the `metricsQuery` template, except with the `.Group`
    # and `.Resource` strings available.  It will also be used to match labels,
    # so avoid using template functions which truncate the group or resource.
    # Group will be converted to a form acceptible for use as a label automatically.
    template: &quot;&lt;&lt;.Resource&gt;&gt;&quot;
    # if we wanted to, we could also specify overrides here
  metricsQuery: &quot;sum(rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;,container!=&quot;POD&quot;}[2m])) by (&lt;&lt;.GroupBy&gt;&gt;)&quot;

# this rule matches only a single metric, explicitly naming it something else
# It's series query *must* return only a single metric family
- seriesQuery: 'cheddar{sharp=&quot;true&quot;}'
  # this metric will appear as &quot;cheesy_goodness&quot; in the custom metrics API
  name: {as: &quot;cheesy_goodness&quot;}
  resources:
    overrides:
      # this should still resolve in our cluster
      brand: {group: &quot;cheese.io&quot;, resource: &quot;brand&quot;}
  metricsQuery: 'count(cheddar{sharp=&quot;true&quot;})'

# external rules are not tied to a Kubernetes resource and can reference any metric
# https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects
externalRules:
- seriesQuery: '{__name__=&quot;queue_consumer_lag&quot;,name!=&quot;&quot;}'
  metricsQuery: sum(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (name)
- seriesQuery: '{__name__=&quot;queue_depth&quot;,topic!=&quot;&quot;}'
  metricsQuery: sum(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;}) by (name)
  # Kubernetes metric queries include a namespace in the query by default
  # but you can explicitly disable namespaces if needed with &quot;namespaced: false&quot;
  # this is useful if you have an HPA with an external metric in namespace A
  # but want to query for metrics from namespace B
  resources:
    namespaced: false

# TODO: should we be able to map to a constant instance of a resource
# (e.g. `resources: {constant: [{resource: &quot;namespace&quot;, name: &quot;kube-system&quot;}}]`)?
</code></pre>
<h4 id="discovery">Discovery</h4>
<p>Discovery 部分控制了查找要在自定义指标 API 中公开的指标的过程。其中有两个关键字段：<code>seriesQuery</code> 和 <code>seriesFilters</code>。</p>
<p><strong>seriesQuery</strong> 指定了用于查找某些 Prometheus series 的 Prometheus series 查询(作为传递给  Prometheus  <code> /api/v1/series</code>)。适配器将从这些系列中剥离标签值，然后在后续步骤中使用得到的“指标名称—标签名称”的组合。</p>
<p>在许多情况下，seriesQuery 就足以缩小 Prometheus series 的列表。但有时(特别是当两个规则可能重叠时)，对指标名称进行额外的过滤是很有用的。在这种情况下,可以使用  seriesFilters。在从 seriesQuery 返回 series  列表后，每个 series 的指标名称都会通过指定的任何过滤器进行过滤。</p>
<p>过滤器可以是以下两种形式之一:</p>
<ul>
<li>is: <regex>，匹配名称符合指定正则表达式的任何序列。</li>
<li>isNot: <regex>，匹配名称不符合指定正则表达式的任何序列。</li>
</ul>
<p>例如</p>
<pre><code class="language-yaml"># match all cAdvisor metrics that aren't measured in seconds
seriesQuery: '{__name__=~&quot;^container_.*_total&quot;,container!=&quot;POD&quot;,namespace!=&quot;&quot;,pod!=&quot;&quot;}'
seriesFilters:
  - isNot: &quot;^container_.*_seconds_total&quot;
</code></pre>
<h4 id="association">Association</h4>
<p>Association 部分控制了确定序列指标可以附加到哪些 Kubernetes 资源的过程。resources 字段控制了这个过程。</p>
<p>有两种方式来关联资源与特定指标。在这两种情况下,标签的值都会成为特定对象的名称。</p>
<p>一种方式是指定，任何符合某个特定模式的标签名称都指向基于标签名称的某个“group_resource”。这可以使用 template 字段来完成。pattern 被指定为一个 Go 模板，其中 Group 和 Resource 字段分别代表“组”和“资源”。</p>
<pre><code class="language-yaml"># any label `kube_&lt;group&gt;_&lt;resource&gt;` becomes &lt;group&gt;.&lt;resource&gt; in Kubernetes
resources:
  template: &quot;kube_&lt;&lt;.Group&gt;&gt;_&lt;&lt;.Resource&gt;&gt;&quot;
</code></pre>
<p>另一种方式是指定某个特定标签代表某个特定的 Kubernetes 资源。这可以使用 overrides 字段来完成。每个 override 将一个 Prometheus 标签映射到一个 Kubernetes group-resource。例如:</p>
<pre><code class="language-yaml"># the microservice label corresponds to the apps.deployment resource
resources:
  overrides:
    microservice: 
      group: &quot;apps&quot;
      resource: &quot;deployment&quot;
</code></pre>
<p>Association 部分提供了两种关联 Prometheus 指标和 Kubernetes 资源的方式，可以根据需要灵活地组合使用。这是实现自定义指标 API 的关键一环。</p>
<h4 id="naming">Naming</h4>
<p>Naming 部分控制了将 Prometheus 指标名称转换为自定义指标 API 中的指标，这是通过 name 字段来实现的。</p>
<p>Naming 的控制通过指定一个从 Prometheus 名称中提取 API 名称的模式，以及对提取值进行的可选转换来实现。</p>
<p>模式由 <code>matches</code> 字段指定，这是一个正则表达式。如果没有指定,它默认为 .* 。</p>
<p>转换由 <code>as</code> 字段指定。你可以使用 <code>matches</code> 字段中定义的任何捕获组。如果 <code>matches</code> 字段没有捕获组，as 字段默认为 <code>$0</code> 。如果只包含一个捕获组，as 字段默认为 <code>$1</code> 。否则，如果没有指定 as 字段就会出错。例如</p>
<pre><code class="language-yaml"># match turn any name &lt;name&gt;_total to &lt;name&gt;_per_second
# e.g. http_requests_total becomes http_requests_per_second
name:
  matches: &quot;^(.*)_total$&quot;
  as: &quot;${1}_per_second&quot;
</code></pre>
<h4 id="querying">Querying</h4>
<p>Querying 部分控制了实际获取特定指标值的过程。它由 <code>metricsQuery</code> 字段来控制。</p>
<p>metricsQuery 字段是一个 Go 模板,它会被转换成一个 Prometheus 查询，使用从特定的自定义指标 API  调用获取的输入数据。对自定义指标 API  的一次调用会被简化为一个指标名称、一个 “group-resource” 和一个或多个该 “group-resource” 的对象。这些会被转换成模板中的以下字段:</p>
<ul>
<li>Series: 指标名称</li>
<li>LabelMatchers: 一个逗号分隔的标签匹配器列表，匹配给定的对象。当前包括特定的 “group-resource” 标签，以及 namespace 标。</li>
<li>GroupBy: 一个逗号分隔的用于分组的标签列表。当前包括用于 LabelMatchers 的组-资源标签。</li>
</ul>
<p>例如，假设我们有一个 <code>http_requests_total</code> 序列 (在 API 中公开为  http_requests_per_second )，具有 service、pod、ingress、namespace 和 verb  标签。前四个对应于 Kubernetes 资源。那么,如果有人请求了 <code>pods/http_request_per_second</code> 指标，那么针对  somens 命名空间中的 pod1 和 pod2，我们会有:</p>
<ul>
<li>Series: &ldquo;http_requests_total&rdquo;</li>
<li>LabelMatchers: <code>&quot;pod=~&quot;pod1|pod2&quot;,namespace=&quot;somens&quot;&quot;</code></li>
<li>GroupBy: pod</li>
</ul>
<p>对应 prometheus promql 如下所示</p>
<pre><code class="language-bash">sum(http_requests_total{pod=~&quot;pod1|pod2&quot;,namespace=&quot;somens&quot;}) by (pod)
</code></pre>
<p>此外,还有两个高级字段是其他字段的&quot;原始&quot;形式:</p>
<ul>
<li>LabelValuesByName: 映射。将 LabelMatchers 字段中的标签和值对应起来。值是用 <code>|</code> 预先连接的 (用于在 Prometheus 中使用 =~ 匹配器)。</li>
<li>GroupBySlice: GroupBy 字段的切片形式。</li>
</ul>
<p>通常，我们可能会想使用 Series、LabelMatchers 和 GroupBy 字段。其他两个是用于高级用法的。</p>
<p>Querying 预计会为每个请求的对象返回一个值。适配器会使用返回的系列上的标签，将给定的系列关联回其相应的对象。例如:</p>
<pre><code class="language-bash"># convert cumulative cAdvisor metrics into rates calculated over 2 minutes
metricsQuery: &quot;sum(rate(&lt;&lt;.Series&gt;&gt;{&lt;&lt;.LabelMatchers&gt;&gt;,container!=&quot;POD&quot;}[2m])) by (&lt;&lt;.GroupBy&gt;&gt;)&quot;
</code></pre>
<h3 id="完整的配置文件实例">完整的配置文件实例</h3>
<p>例如，我们想使用 springboot 的 actuator 提供的 <code>jvm_memory_used_bytes</code> 和 <code>jvm_memory_max_bytes</code> 计算内存使用率，如下所式</p>
<pre><code class="language-yaml">rules:
- seriesQuery: 'jvm_memory_used_bytes'
  resources:
    overrides:
      namespace:
        resource: &quot;namespace&quot;
      pod:
        resource: &quot;pod&quot;
  name:
    matches: 'jvm_memory_used_bytes'
    as: memory_percent
  metricsQuery: 'sum(jvm_memory_used_bytes{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;) / sum(jvm_memory_max_bytes{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;) * 100'
- seriesQuery: 'process_cpu_usage'
  resources:
    overrides:
      namespace:
        resource: &quot;namespace&quot;
      pod:
        resource: &quot;pod&quot;  
  name:
    matches: 'process_cpu_usage'
    as: process_cpu_percent
  metricsQuery: 'sum(avg_over_time(process_cpu_usage{&lt;&lt;.LabelMatchers&gt;&gt;}[1m])) by (&lt;&lt;.GroupBy&gt;&gt;)'
</code></pre>
<p>这里用到了一个技巧，就是使用查询多个指标，这里参考了 prometheus-adapter 的说明 <sup><a href="#4">4</a></sup></p>
<blockquote>
<p>这很好理解,虽然一开始可能看起来不太明显。</p>
<p>基本上，你只需要选择一个指标作为 &ldquo;Discovery&rdquo; 和 &ldquo;naming&rdquo; 指标，然后使用它来配置配置中的 &ldquo;discovery&rdquo; 和 &ldquo;naming&rdquo; 部分。之后，你就可以在 metricsQuery 中写任何你想要的指标了！ ==<strong>Querying 的序列可以包含任何你想要的指标，只要它们有正确的标签集合即可</strong>==。</p>
</blockquote>
<p>例如，假设你有两个指标 foo_total 和 foo_count，它们都有一个标签 <code>system_name</code>，用于表示节点资源，那么如下配置所示</p>
<pre><code class="language-yaml">rules:
- seriesQuery: 'foo_total'
  resources: {overrides: {system_name: {resource: &quot;node&quot;}}}
  name:
    matches: 'foo_total'
    as: 'foo'
  metricsQuery: 'sum(foo_total{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;) / sum(foo_count{&lt;&lt;.LabelMatchers&gt;&gt;}) by (&lt;&lt;.GroupBy&gt;&gt;)'
</code></pre>
<p>由于我们使用了 <code>jvm_memory_used_bytes</code> 和 <code>jvm_memory_max_bytes</code> ，那么我们可以在 &ldquo;discovery&rdquo; 和 &ldquo;naming&rdquo; 部分写任意指标，在 ”quering“ 中使用真是的指标进行替换，就可以完成</p>
<h4 id="查询-kubernetes-的指标">查询 kubernetes 的指标</h4>
<p>完成配置后，可以使用下面命令进行查询</p>
<pre><code class="language-bash">kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1&quot;|jq
{
  &quot;kind&quot;: &quot;APIResourceList&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;groupVersion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;,
  &quot;resources&quot;: [
    {
      &quot;name&quot;: &quot;pods/process_cpu_percent&quot;,
      &quot;singularName&quot;: &quot;&quot;,
      &quot;namespaced&quot;: true,
      &quot;kind&quot;: &quot;MetricValueList&quot;,
      &quot;verbs&quot;: [
        &quot;get&quot;
      ]
    },
    {
      &quot;name&quot;: &quot;pods/memory_percent&quot;,
      &quot;singularName&quot;: &quot;&quot;,
      &quot;namespaced&quot;: true,
      &quot;kind&quot;: &quot;MetricValueList&quot;,
      &quot;verbs&quot;: [
        &quot;get&quot;
      ]
    },
    {
      &quot;name&quot;: &quot;namespaces/memory_percent&quot;,
      &quot;singularName&quot;: &quot;&quot;,
      &quot;namespaced&quot;: false,
      &quot;kind&quot;: &quot;MetricValueList&quot;,
      &quot;verbs&quot;: [
        &quot;get&quot;
      ]
    },
    {
      &quot;name&quot;: &quot;namespaces/process_cpu_percent&quot;,
      &quot;singularName&quot;: &quot;&quot;,
      &quot;namespaced&quot;: false,
      &quot;kind&quot;: &quot;MetricValueList&quot;,
      &quot;verbs&quot;: [
        &quot;get&quot;
      ]
    }
  ]
}
</code></pre>
<p>可以通过 custom API 进程查询具体获取的值，如下所示</p>
<pre><code class="language-bash">$ kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/public/pods/*/process_cpu_percent&quot;|jq
{
  &quot;kind&quot;: &quot;MetricValueList&quot;,
  &quot;apiVersion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;,
  &quot;metadata&quot;: {},
  &quot;items&quot;: [
    {
      &quot;describedObject&quot;: {
        &quot;kind&quot;: &quot;Pod&quot;,
        &quot;namespace&quot;: &quot;msg&quot;,
        &quot;name&quot;: &quot;message-gateway-api-78c4d5cdbf-9k2g7&quot;,
        &quot;apiVersion&quot;: &quot;/v1&quot;
      },
      &quot;metricName&quot;: &quot;process_cpu_percent&quot;,
      &quot;timestamp&quot;: &quot;2024-05-31T11:40:25Z&quot;,
      &quot;value&quot;: &quot;404m&quot;,
      &quot;selector&quot;: null
    },
    
    ...
    
    {
      &quot;describedObject&quot;: {
        &quot;kind&quot;: &quot;Pod&quot;,
        &quot;namespace&quot;: &quot;msg&quot;,
        &quot;name&quot;: &quot;message-core-79fdc6fdd-lkpdm&quot;,
        &quot;apiVersion&quot;: &quot;/v1&quot;
      },
      &quot;metricName&quot;: &quot;process_cpu_percent&quot;,
      &quot;timestamp&quot;: &quot;2024-05-31T11:40:25Z&quot;,
      &quot;value&quot;: &quot;31m&quot;,
      &quot;selector&quot;: null
    },
    {
      &quot;describedObject&quot;: {
        &quot;kind&quot;: &quot;Pod&quot;,
        &quot;namespace&quot;: &quot;msg&quot;,
        &quot;name&quot;: &quot;message-push-admin-554f5d96fd-xlnhj&quot;,
        &quot;apiVersion&quot;: &quot;/v1&quot;
      },
      &quot;metricName&quot;: &quot;process_cpu_percent&quot;,
      &quot;timestamp&quot;: &quot;2024-05-31T11:40:25Z&quot;,
      &quot;value&quot;: &quot;487m&quot;,
      &quot;selector&quot;: null
    }
  ]
}
</code></pre>
<p>我们可以看到，返回值是带有 ”m“ 的单位，这里 issue 是这样回答的</p>
<blockquote>
<p>The <code>m</code>-suffix means milli, Quantity Values are explained here: <a href="https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/walkthrough.md#quantity-values" target="_blank"
   rel="noopener nofollow noreferrer" >https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/walkthrough.md#quantity-values</a>  <sup><a href="#5">5</a></sup></p>
</blockquote>
<p>在指标 API 中最常见的是 m 后缀,它表示毫单位，即单位的千分之一；由于我们返回值是一个百分比，例如 4.87%，那么实际值是 0.0487，那么他的毫单位为就是 “487m” ，和上面返回值一样。</p>
<h3 id="prometheus-adapter的安装">Prometheus-adapter的安装</h3>
<p>在这里采用 helm 方式进行安装，只需要修改对应参数即可</p>
<pre><code class="language-bash">helm install prometheus-adapter -n monitoring  prometheus-community/prometheus-adapter \
	--set prometheus.url=http://prometheus.default.svc \
	--set logLevel=2 \
	--set rules.external=xxx # 如果使用外部规则替换默认的config.yaml,则需要提前创建一个configmap，然后这里指定这个名称
</code></pre>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://github.com/kubernetes/design-proposals-archive/blob/main/instrumentation/monitoring_architecture.md" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes monitoring architecture</a></p>
<p><sup id="2">[2]</sup> <a href="https://github.com/kubernetes/design-proposals-archive/blob/main/instrumentation/core-metrics-pipeline.md" target="_blank"
   rel="noopener nofollow noreferrer" >core-metrics-pipeline</a></p>
<p><sup id="3">[3]</sup> <a href="https://github.com/kubernetes/metrics" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/metrics</a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/kubernetes-sigs/prometheus-adapter/tree/v0.12.0?tab=readme-ov-file#my-query-contains-multiple-metrics-how-do-i-make-that-work" target="_blank"
   rel="noopener nofollow noreferrer" >my-query-contains-multiple-metrics-how-do-i-make-that-work</a></p>
<p><sup id="5">[5]</sup> <a href="https://github.com/kubernetes-sigs/prometheus-adapter/issues/376" target="_blank"
   rel="noopener nofollow noreferrer" >why i request rest-api, returned requeslt for item value has &rsquo;m&rsquo; unit!!  #376</a></p>
<p><sup id="6">[6]</sup> <a href="https://medium.com/@congliu.thu/complete-guide-to-kubernetes-metrics-24a8782c34cd" target="_blank"
   rel="noopener nofollow noreferrer" >Guide to Kubernetes Metrics</a></p>
<p><sup id="7">[7]</sup>  <a href="https://www.geekgame.site/post/k8s/monitoring_arch/" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes 监控架构(译)</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>探索kubectl - kubectl诊断命令</title>
      <link>https://www.oomkill.com/2024/04/kubernetes-kubectl.md/</link>
      <pubDate>Sat, 20 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/04/kubernetes-kubectl.md/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="pod">Pod</h2>
<p>检查 Pod 就绪探针</p>
<pre><code class="language-bash">kubectl get pods &lt;pod-name&gt; -n &lt;namespace&gt; -o jsonpath='{.status.conditions[?(@.type==&quot;Ready&quot;)].status}'
</code></pre>
<p>查看 Pod 事件</p>
<pre><code class="language-bash">kubectl get events -n &lt;namespace&gt; --field-selector involvedObject.name=&lt;pod-name&gt;
</code></pre>
<p>获取 Pod Affinity 和 Anti-Affinity</p>
<pre><code class="language-bash">kubectl get pod &lt;pod-name&gt; -n &lt;namespace&gt; -o=jsonpath='{.spec.affinity}'
</code></pre>
<p>列出 Pod 的 anti-affinity 规则</p>
<pre><code class="language-bash">kubectl get pod &lt;pod-name&gt; -n &lt;namespace&gt; -o=jsonpath='{.spec.affinity.podAntiAffinity}'
</code></pre>
<h2 id="pod-network">Pod Network</h2>
<p>运行 Debug Pod 进行调试</p>
<pre><code class="language-bash">kubectl run -it --rm --restart=Never --image=busybox net-debug-pod -- /bin/sh
</code></pre>
<p>测试从 Pod 到 Endpoint 的连接性</p>
<pre><code class="language-bash">kubectl exec -it &lt;pod-name&gt; -n &lt;namespace&gt; -- curl &lt;endpoint-url&gt;
</code></pre>
<p>trace 一个 Pod 到另一个 Pod 的网络</p>
<pre><code class="language-bash">kubectl exec -it &lt;source-pod-name&gt; -n &lt;namespace&gt; -- traceroute &lt;destination-pod-ip&gt;
</code></pre>
<p>检查 Pod DNS配置</p>
<pre><code class="language-bash">kubectl exec -it &lt;pod-name&gt; -n &lt;namespace&gt; -- cat /etc/resolv.conf
</code></pre>
<h2 id="deployment">Deployment</h2>
<p>查看 rollout 状态</p>
<pre><code class="language-bash">kubectl rollout status deployment/&lt;deployment-name&gt; -n &lt;namespace&gt;
</code></pre>
<p>查看 rollout 历史记录</p>
<pre><code class="language-bash">kubectl rollout history deployment/&lt;deployment-name&gt; -n &lt;namespace&gt;
</code></pre>
<p>调整 deployment 数量</p>
<pre><code class="language-bash">kubectl scale deployment &lt;deployment-name&gt; --replicas=&lt;replica-count&gt; -n &lt;namespace&gt;
</code></pre>
<p>设置 deployment 的自动缩放</p>
<pre><code class="language-bash">kubectl autoscale deployment &lt;deployment-name&gt; \
	--min=&lt;min-pods&gt; \
	--max=&lt;max-pods&gt; \
	--cpu-percent=&lt;cpu-percent&gt; \
	-n &lt;namespace&gt;
</code></pre>
<p>检查 HAP 状态</p>
<pre><code class="language-bash">kubectl get hpa -n &lt;namespace&gt;
</code></pre>
<h2 id="networking">Networking</h2>
<p>显示命名空间中 Pod 的 IP 地址：</p>
<pre><code class="language-bash">kubectl get pods -n &lt;namespace -o custom-columns=POD:metadata.name,IP:status.podIP --no-headers
</code></pre>
<h2 id="node">Node</h2>
<p>获取特定 Node 上运行的 Pod 列表</p>
<pre><code class="language-bash">kubectl get pods --field-selector spec.nodeName=&lt;node-name&gt; -n &lt;namespace&gt;
</code></pre>
<pre><code class="language-bash">kubectl get pod -n {ns} -o \ 
    jsonpath='{range $.items[?(@.spec.nodeName == &quot;xxxx&quot;)]}{.metadata.name}{&quot;\n&quot;}'
</code></pre>
<p>获取 Node 的 IP</p>
<pre><code class="language-bash">kubectl get pod -n {ns} -o \ 
    -o=jsonpath='{range $.items[*]}{.metadata.name}{&quot;\t&quot;}{.status.addresses[0].address}{&quot;\n&quot;}{end}'
</code></pre>
<p>获取 Node 上 指定标签</p>
<pre><code class="language-bash">kubectl get nodes \
    -o=jsonpath='{range .items[*]}{.metadata.name}{&quot;\t&quot;}{.metadata.labels.&lt;xxx&gt;}{&quot;\n&quot;}{end}'
</code></pre>
<p>查找属于这个标签 Node 的数量</p>
<pre><code class="language-bash">kubectl get node -l xxx=xxxx --no-headers | wc -l
</code></pre>
<p>查看node上所有的标签，json格式</p>
<pre><code class="language-bash">kubectl get node -o json | jq '.items[] | {name: .metadata.name, labels: .metadata.labels }' --compact-output | jq -r
</code></pre>
<p>根据 NodeSelector 查询 application</p>
<pre><code class="language-bash">kubectl get deployment -A \
    -o=jsonpath='{range .items[*]}{@.metadata.name}{&quot;\t&quot;}{range @.spec.template.spec.nodeSelector}{.xxxx}{&quot;\n&quot;}{end}{end}'
</code></pre>
<p>根据条件列出 Node 1.17+</p>
<pre><code class="language-bash">kubectl get nodes -o \
	custom-columns=NODE:.metadata.name,READY:.status.conditions[?(@.type==&quot;Ready&quot;)].status -l 'node-role.kubernetes.io/worker=xxx'
</code></pre>
<p>获取 Node 的操作系统信息</p>
<pre><code class="language-bash">kubectl get node &lt;node-name&gt; -o jsonpath='{.status.nodeInfo.osImage}'
</code></pre>
<p>查询应用 (Deployment) 使用的 Node</p>
<pre><code class="language-bash">kubectl deployment -n {ns} -o \
    jsonpath='{range $.item[*]}{&quot;Deployment: &quot;}{.metadata.name}{&quot;\n&quot;}{&quot;\t&quot;}{.spec.template.spec.nodeSelector.srv-grp}{&quot;\n&quot;}{end}'
</code></pre>
<p>仅获取 Node 的指定标签和 Pod 名称</p>
<pre><code class="language-bash">k get nodes -o \
jsonpath='{range .items[*]}{.metadata.labels.nodeSelector.xxxxx}{&quot;\t&quot;}{.status.addresses[?(@.type==&quot;InternalIP&quot;)].address}{&quot;\n&quot;}{end}'
</code></pre>
<p>仅获取 Node IP</p>
<pre><code class="language-bash">k get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==&quot;InternalIP&quot;)].address}'
</code></pre>
<p>获取指定标签的 Node，并输出他的标签和 IP</p>
<pre><code class="language-bash">k get nodes -o \
	jsonpath='{range .items[?(@.metadata.labels.xxxx==&quot;xxxx&quot;)]}{.metadata.labels.srv-grp}{&quot;\t&quot;}{.status.addresses[?(@.type==&quot;InternalIP&quot;)].address}{&quot;\n&quot;}{end}'
</code></pre>
<p>获取指定 Role 的 Node 列表，只输出他的主机名</p>
<pre><code class="language-bash">k get node --all-namespaces \
	-o=jsonpath='{range .items[?(@.metadata.labels.kubernetes.io/role==&quot;xxxx&quot;)]}{.metadata.name}{&quot;\n&quot;}{end}'
</code></pre>
<p>修改指定 Role 的 Node 的角色值 (jq)</p>
<pre><code class="language-bash"># 假设将 k8s role 为 aaa 的主机修改为 role=bbb
for n in `k get nodes -o json |jq -r '.items[] | select(.metadata.labels.&quot;kubernetes.io/role&quot; == &quot;aaa&quot;) | .metadata.name'`;
do 
	k label node $n kubernetes.io/role=bbb --overwrite
done
</code></pre>
<p>修改指定 Role 的 Node 的角色值 (jsonpath)</p>
<pre><code class="language-bash"># 假设将 k8s node 标签 xxx 为 aaa 的主机修改为 xxx=bbb
for n in `k get nodes -o jsonpath='{range .items[?(@.metadata.labels.xxx==&quot;aaa&quot;)]}{.metadata.name}{&quot;\n&quot;}{end}'`;\
do
	k label node $n xxx=bbb  --overwrite
done
</code></pre>
<h2 id="resource-quotas">Resource Quotas</h2>
<p>列出命名空间中的资源配额</p>
<pre><code class="language-bash">kubectl get resourcequotas -n &lt;namespace&gt;
</code></pre>
<p>查看资源配额详情</p>
<pre><code class="language-bash">kubectl describe resourcequota &lt;resource-quota-name&gt; -n &lt;namespace&gt;
</code></pre>
<h2 id="volumes">Volumes</h2>
<p>按容量排序的列出PV</p>
<pre><code class="language-bash">kubectl get pv --sort-by=.spec.capacity.storage
</code></pre>
<p>检查 PV reclaim policy</p>
<pre><code class="language-bash">kubectl get pv &lt;pv-name&gt; -o=jsonpath='{.spec.persistentVolumeReclaimPolicy}'
</code></pre>
<h2 id="ephemeral-containers">Ephemeral Containers</h2>
<p>1.18+</p>
<p>运行一个 ephemeral debugging 容器</p>
<pre><code class="language-bash">kubectl debug -it &lt;pod-name&gt; -n &lt;namespace&gt; --image=&lt;debug-image&gt; -- /bin/sh
</code></pre>
<h2 id="pod-disruption-budget-pdb"><strong>Pod Disruption Budget (PDB)</strong></h2>
<p>列出一个ns内的PDB</p>
<pre><code class="language-bash">kubectl get pdb -n &lt;namespace&gt;
</code></pre>
<h2 id="configmap">ConfigMap</h2>
<p>批量备份 configmap</p>
<pre><code class="language-bash"></code></pre>
<h2 id="secret">Secret</h2>
<p>从一个 namespace 导出所有 secret 到另一个 namespace</p>
<pre><code class="language-bash">for n in `kubectl get secret -n xxxx -o jsonpath='{range $.items[*]}{.metadata.name}{&quot;\n&quot;}{end}'`;do
    kubectl get secret -n xxxx $n -o yaml | \
     sed -e '/resourceVersion:/d;/uid:/d; /selfLink:/d; /creationTimestamp:/d;' \
         -e 's/namespace: &lt;xxxxxx&gt;/namespace: &lt;xxxxx&gt;/' \
    kubectl create -f -
done
</code></pre>
<p>查看证书是否过期</p>
<pre><code class="language-bash">ns=xxx
secret_name=xxx
keywords=xxx
_command=xxxx
openssl x509 -in &lt;(
    ${_command} get secret -n $ns $secret_name -ojson | \
    jq --arg secret_key $(
        ${_command} get secret -n ${ns} ${secret_name} -ojson | \
        jq -r '.data | keys[]' | \
        awk -v keywords=${keywords} '$0 ~ keywords { print }'
    ) -r '.data | .[$secret_key]' | base64 -d
) -text -noout | \
grep -i &quot;subject: &quot;
</code></pre>
<p>批量备份集群内指定secret命令</p>
<pre><code class="language-bash"># 指定域名的secret
keywords=xxx
_command=xxxx
for ns in `${_command} get ns -o jsonpath='{range $.items[*]}{.metadata.name}{&quot;\n&quot;}{&quot;end&quot;}'`
do
    for secret_name in `${_command} get secret -n $ns -o json|jq -r '.items[] | select( .type == &quot;kubernetes.io/tls&quot;) | .metadata.name'`
    do  
        openssl x509 -in &lt;(
            ${_command} get secret -n ${ns} ${secret_name} -ojson | \
            jq --arg secret_key $(
                    ${_command} get secret -n ${ns} ${secret_name} -ojson | 
                    jq -r '.data | keys[]' | \
                    awk '$0 ~ /crt/ { print }'
                ) -r '.data|.[$secret_key]'|base64 -d
        ) -text -noout | \
        grep -i &quot;subject: &quot;| \
        awk -v ns=${ns} -v secret_name=${secret_name} -v cmd=${_command} -v kw=${keywords} \
            '$0 ~ kw { system(
                cmd&quot; get secret -n &quot;ns&quot; &quot;secret_name&quot; -oyaml &gt; primetive/&quot;ns&quot;.&quot;secret_name&quot;.yaml&quot;
            )}'
    done
done
</code></pre>
<p>查询 tls 签发域名，并进行替换</p>
<pre><code class="language-bash">keywords=xxx
_command=xxxx
for ns in `${_command} get ns -o jsonpath='range $.items[*]}{.metadata.name}{&quot;\n&quot;}{&quot;end&quot;}'
do
    for secret_name in `${_command} get secret -n $ns -o json|jq -r '.items[] | select( .type == &quot;kubernetes.io/tls&quot;) | .metadta.name'`
    do  
        openssl x509 -n &lt;(
            ${_command} get secret -n ${ns} ${secret_name} -ojson | \
            jq --arg secret_key $(${_command} get secret -n ${ns} ${secret_name} -ojson | \
            jq -r '.data | .keys[]' | \
            awk '$0 ~ /crt/ { print }' -r '.data|.[$secret_key]'|base64 -d
        ) -text -noout | \
        grep -i &quot;subject: &quot;| \
        awk -v ns=${ns} -v secret_name=${secret_name} -v commond=${_command}\
            '$0 ~ ${keywords} { system(
                command&quot; create secret tls -n &quot;ns&quot; &quot;secret_name&quot; --cert=ca.crt --key=key.key --dry-run -oyaml|command&quot; replace -f -&quot;
            )}'
    done
done
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>批量更新harbor版本 1.10 to 2.10</title>
      <link>https://www.oomkill.com/2024/03/upgrade-harbor/</link>
      <pubDate>Fri, 29 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/03/upgrade-harbor/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>本文将介绍 Harbor 从 v1.10.7 升级到 v2.10.0，以及如何将 Harbor 从 v2.10 回滚到 v1.10.7。</p>
<h2 id="升级条件">升级条件</h2>
<ul>
<li>Linux服务器</li>
<li>4 个 CPU 和 8 GB 内存（强要求），100G可用空间（跨多版本时存放备份文件以及镜像文件，这部分要求）</li>
<li>Docker-compose &gt; 1.19.0+</li>
<li>备份现有的 Harbor  <strong>/data/database</strong> 目录</li>
</ul>
<p>本次升级主要是使用了 harbor 内置的数据库，所以升级步骤比较容易。</p>
<h2 id="官方升级路线">官方升级路线</h2>
<p>harbor 的升级，是不能跨很多版本进行升级，官方对此有详细说明 <sup><a href="#1">[1]</a></sup> ，可以看到路线为：</p>
<p>1.10.0 <sup><a href="#1">[1]</a></sup> =&gt; 2.4.0 <sup><a href="#2">[2]</a></sup> =&gt; 2.6.0 <sup><a href="#3">[3]</a></sup> =&gt; 2.8.0 <sup><a href="#4">[4]</a></sup> =&gt; 2.10.0 <sup><a href="#5">[5]</a></sup></p>
<h2 id="模拟升级步骤">模拟升级步骤</h2>
<p>github release 页下载对应的安装包</p>
<p>解压</p>
<pre><code class="language-bash"># 命令主要为将harbor压缩包内文件解压到指定目录中，由于 harbor 解压后文件名无论版本如何都为“harbor”
$ mkdir ./harbor-v1.10 &amp;&amp; tar -xf harbor-offline-installer-v1.10.0.tgz -C ./harbor-v1.10 --strip-components 1
</code></pre>
<p>备份默认的配置文件（仅限于 v1.10.x，v2.x均为 harbor.tmpl）</p>
<pre><code class="language-bash">cp harbor.yml harbor.yaml.backup
</code></pre>
<p>清除注释</p>
<pre><code class="language-bash">grep -Ev '^#|^$|^\s*(#|//)' harbor.yml.tmpl &gt; harbor.yml.clean_annotation
</code></pre>
<p>修改一些默认配置</p>
<pre><code class="language-bash">\cp -a harbor.yml.clean_annotation harbor.yml
# 如果需要则关闭https
sed -i  '/https:/,+3s/.*/# &amp;/' harbor.yml
# 替换默认harbor郁闷
sed -i &quot;s@hostname: reg.mydomain.com@hostname: img.test.com@g&quot; harbor.yml
# 修改默认目录
sed -i &quot;s@data_volume: /data@data_volume: /data/harbor@g&quot; harbor.yml
</code></pre>
<p>启动服务</p>
<pre><code class="language-bash">./install.sh
</code></pre>
<h2 id="升级步骤">升级步骤</h2>
<p>在升级前首先要缕清升级的内容，官方在升级时存在两个步骤，配置文件升级与数据库 schema 升级；并且需要知道升级的路线图，这里是从 1.10.0 升级至本文撰写时最新版本 2.10.0，所以查看官方升级路线为 1.10.0 =&gt; 2.4.0 =&gt; 2.6.0 =&gt; 2.8.0 =&gt; 2.10.0。总结升级所需变更如下：</p>
<ul>
<li>harbor的配置文件升级</li>
<li>数据库 schema 升级，由 harbor-core 组件自动完成</li>
<li>升级路线：1.10.0 =&gt; 2.4.0 =&gt; 2.6.0 =&gt; 2.8.0 =&gt; 2.10.0</li>
</ul>
<h3 id="备份当前-harbor-版本">备份当前 harbor 版本</h3>
<p>备份当前版本是为了如果需要回滚的话，可以快速的回滚到所需的版本</p>
<pre><code class="language-bash">cd harbor 
docker-compose down 
</code></pre>
<blockquote>
<p>备份 Harbor 的当前文件，以便您可以在必要时回滚到当前版本。</p>
</blockquote>
<h3 id="备份数据库文件">备份数据库文件</h3>
<p>我们知道了，升级主要是对数据库 schema 进行reschema，Harbor 的每次新版本发布时新的功能及对老功能、代码的重构都会导致数据库模型的变更，因此几乎每次升级都需要升级数据库模式。配置文件数据，是指 Harbor 组件的配置文件，在部分新功能或者新的组件出现时，都需要在配置文件中新增其参数；在老功能、组件重构或者废弃时，也会对配置文件进行更新。</p>
<pre><code class="language-bash">cp -r /data/database /my_backup_dir/
</code></pre>
<blockquote>
<p>reschema的工作是由 harbor-core 完成的，所以我们只需要备份即可，当新版本在启动时，第一次会 reschema，这个步骤的时间会随着 harbor 的使用量而增加，这里数据库目录为 13G，1.10.7 =&gt; 2.4.0 时间大概在20分钟左右。</p>
</blockquote>
<h3 id="harbor的配置文件升级">harbor的配置文件升级</h3>
<p>harbor 配置的升级是需要手动执行的，命令是包含在 offline 安装包中，被包含在 “goharbor/prepare:v2.x.0” 镜像中。用户可以在 Harbor 的离线安装包中找到它，也可以在 Docker Hub 上获取，官方给出升级指南中的命令如下</p>
<pre><code class="language-bash"># 1.10
docker run -it --rm -v ./harbor.yml:/harbor-migration/harbor-cfg/harbor.yml goharbor/harbor-migrator:v1.10.0 --cfg up


# 2.4
# 后的yaml文件必须是旧版本的
# 这步骤是将旧的 harbor.yaml 配置文件升级到新版本
# 升级后旧版本的配置文件就没有了，如果需要需要自行备份
# docker run -it --rm -v /:/hostfs goharbor/prepare:[tag] migrate -i ${path to harbor.yml}
docker run -it --rm -v /:/hostfs goharbor/prepare:v2.4.0 migrate -i ./harbor.yml
</code></pre>
<p>“-v  /:/hostfs” 是将主机的根目录 “/” 挂载到容器中的 “/hostfs” 目录中。因为命令是运行在容器中的，而文件是在宿主机上的，为了能在容器中访问到指定的文件，需要这样挂载，之后 prepare 会对 “/hostfs” 这个文件做特殊处理，使得在容器里也能访问主机上的指定文件。</p>
<p>”-i“ 是指定旧版本的 harbor 配置文件</p>
<p>migrate 命令有如下3个参数。</p>
<p>​	&ndash;input（缩写形式为“-i”）：是输入文件的绝对路径，也就是需要升级的原配置文件。</p>
<p>​	&ndash;output（缩写形式为“-o”）：是输出文件的绝对路径，也是升级后的配置文件，是可选参数，如果取默认值，则升级后的文件会被写回输入文件中。</p>
<p>​	&ndash;target（缩写形式为“-t”）：是目标版本，也就是打算升级到的版本，也是可选参数，如果取默认值，则版本为此工具发布时所支持的最新版本。</p>
<p>这里我们可以使用如下命令</p>
<pre><code class="language-bash">docker run -v :/hostfs goharbor/prepare:v2.4.0 migrate -i home/harbor/upgrade/harbor.yml
</code></pre>
<p>升级成功会有如下输出</p>
<pre><code class="language-bash">migrating to version 2.0.0
migrating to version 2.1.0
migrating to version 2.2.0
migrating to version 2.3.0
migrating to version 2.4.0
Written new values to home/harbor/upgrade/harbor.yml
</code></pre>
<h3 id="启动新服务">启动新服务</h3>
<p>在新版本 harbor 目录中，运行 <code>./install.sh </code>脚本来安装新的 Harbor 实例，这里会导入离线安装包，生成配置文件，启动服务等操作</p>
<h3 id="替换-docker-compose-文件">替换 docker-compose 文件</h3>
<p>docker-compose 的生成是在 prepare 脚本中执行的，可以看出，是调用的 prepare 镜像</p>
<pre><code class="language-bash"># Run prepare script
docker run --rm -v $input_dir:/input \
                    -v $data_path:/data \
                    -v $harbor_prepare_path:/compose_location \
                    -v $config_dir:/config \
                    -v /:/hostfs \
                    --privileged \
                    goharbor/prepare:dev prepare $@
</code></pre>
<p>这种情况下，如果我们需要自定义的 docker-compose.yaml 就可以挂在到对应目录即可，模板文件可以在 <a href="photon/prepare/templates/docker_compose">photon/prepare/templates/docker_compose</a> 处下载进行替换。</p>
<p>替换后，使用 sed 命令，替换 prepare 脚本中的启动命令即可。</p>
<pre><code class="language-bash">sed -i &quot;/-v \/:\/hostfs/a \\\t\\t -v /root/docker_compose/docker-compose.yml.jinjia.${version}:/usr/src/app/templates/docker_compose/dockercompose.yml.jinjia \\\\&quot; ./prepare
</code></pre>
<h2 id="批量升级脚本">批量升级脚本</h2>
<pre><code class="language-bash">#!/bin/bash

# 根据当前版本和目标版本选择对应的下载地址
declare -A harbor_versions=(
  [&quot;v1.10.0&quot;]=&quot;https://github.com/goharbor/harbor/releases/download/v1.10.0/harbor-offline-installer-v1.10.0.tgz&quot;
  [&quot;v2.4.0&quot;]=&quot;https://github.com/goharbor/harbor/releases/download/v2.4.0/harbor-offline-installer-v2.4.0.tgz&quot;
  [&quot;v2.6.1&quot;]=&quot;https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz&quot;
  [&quot;v2.8.0&quot;]=&quot;https://github.com/goharbor/harbor/releases/download/v2.8.0/harbor-offline-installer-v2.8.0.tgz&quot;
  [&quot;v2.10.0&quot;]=&quot;https://github.com/goharbor/harbor/releases/download/v2.10.0/harbor-offline-installer-v2.10.0.tgz&quot;
)

#
# Set Colors
#

bold=$(tput bold)
underline=$(tput sgr 0 1)
reset=$(tput sgr0)

red=$(tput setaf 1)
green=$(tput setaf 76)
white=$(tput setaf 7)
tan=$(tput setaf 202)
blue=$(tput setaf 25)

#
# Headers and Logging
#

underline() { printf &quot;${underline}${bold}%s${reset}\n&quot; &quot;$@&quot;
}
h1() { printf &quot;\n${underline}${bold}${blue}%s${reset}\n&quot; &quot;$@&quot;
}
h2() { printf &quot;\n${underline}${bold}${white}%s${reset}\n&quot; &quot;$@&quot;
}
debug() { printf &quot;${white}%s${reset}\n&quot; &quot;$@&quot;
}
info() { printf &quot;${white}➜ %s${reset}\n&quot; &quot;$@&quot;
}
success() { printf &quot;${green}✔ %s${reset}\n&quot; &quot;$@&quot;
}
error() { printf &quot;${red}✖ %s${reset}\n&quot; &quot;$@&quot;
}
warn() { printf &quot;${tan}➜ %s${reset}\n&quot; &quot;$@&quot;
}
bold() { printf &quot;${bold}%s${reset}\n&quot; &quot;$@&quot;
}
note() { printf &quot;\n${underline}${bold}${blue}Note:${reset} ${blue}%s${reset}\n&quot; &quot;$@&quot;
}

set -e
# 设置根目录和工作目录
ROOT_DIR=$(cd $(dirname $0); pwd)
export ROOT_DIR
# 设置步骤变量

usage()
{
cat &lt;&lt;EOF
Usage: ${CMD} [ OPTION ]
Commands:
   Some commands take arguments or -h for usage.
     -d        download harbor rolling dependencies offline installer package
     -u        upgrade harbor to latest
     -r        rollback to old version
     -h        this message
EOF
    return 0
}

compare_versions()
{
  local current_version=$1
  local target_version=$2

  if [[ $current_version == v* ]]; then
    current_version=${current_version#v}
  fi

  if [[ $target_version == v* ]]; then
    target_version=${target_version#v}
  fi

  if [[ $current_version == &quot;$target_version&quot; ]]; then
    return 2
  fi

  IFS='.' read -ra current_version_parts &lt;&lt;&lt; &quot;$current_version&quot;
  IFS='.' read -ra target_version_parts &lt;&lt;&lt; &quot;$target_version&quot;

  for (( i=0; i&lt;${#current_version_parts[@]}; i++ )); do
    if (( ${target_version_parts[$i]} &gt; ${current_version_parts[$i]} )); then
      return 0
    elif (( ${target_version_parts[$i]} &lt; ${current_version_parts[$i]} )); then
      return 1
    fi
  done

  return 2
}

set_env()
{
	# 设置工作目录
	WORK_DIR=&quot;${ROOT_DIR}/_work&quot;
	
	# 获取当前版本和目标版本
	read -p &quot;Please input current version [1.10.0]: &quot; CURRENT_VERSION
	CURRENT_VERSION=${CURRENT_VERSION:-v1.10.0}

	if [[ ${CURRENT_VERSION:0:1} != &quot;v&quot; ]]; then
		CURRENT_VERSION=&quot;v${CURRENT_VERSION}&quot; 
	fi

	read -p &quot;Please input target version [2.10.0]: &quot; TARGET_VERSION
	TARGET_VERSION=${TARGET_VERSION:-v2.10.0}

	if [[ ${TARGET_VERSION:0:1} != &quot;v&quot; ]]; then
		TARGET_VERSION=&quot;v${TARGET_VERSION}&quot; 
	fi

	read -p &quot;Please input harbor path [/root/harbor-v1.10]: &quot; CURR_HARBOR_PATH
	CURR_HARBOR_PATH=${CURR_HARBOR_PATH:-/root/harbor-v1.10}

	read -p &quot;Please input harbor database [/data/harbor]: &quot; DATA_DIR
	DATA_DIR=${DATA_DIR:-/data/harbor}
	
	# 设置备份目录
	BACKUP_DIR=&quot;${ROOT_DIR}/harbor_backup&quot;
	
	# 当前版本的备份路径
	CURRENT_VERSION_BACKUP_PATH=&quot;$BACKUP_DIR/${CURRENT_VERSION}&quot;
	
	export WORK_DIR CURRENT_VERSION TARGET_VERSION CURR_HARBOR_PATH DATA_DIR BACKUP_DIR CURRENT_VERSION_BACKUP_PATH

    versions=&quot;&quot;
    sorted_versions=&quot;&quot;
    export versions sorted_versions
}

set_env_rollback()
{
	# 设置工作目录
	WORK_DIR=&quot;${ROOT_DIR}/_work&quot;
	
	# 获取当前版本和目标版本
	read -p &quot;Please input current version [2.10.0]: &quot; CURRENT_VERSION
	CURRENT_VERSION=${CURRENT_VERSION:-v2.10.0}

	if [[ ${CURRENT_VERSION:0:1} != &quot;v&quot; ]]; then
		CURRENT_VERSION=&quot;v${CURRENT_VERSION}&quot; 
	fi

	read -p &quot;Please input target version [1.10.0]: &quot; TARGET_VERSION
	TARGET_VERSION=${TARGET_VERSION:-v1.10.0}

	if [[ ${TARGET_VERSION:0:1} != &quot;v&quot; ]]; then
		TARGET_VERSION=&quot;v${TARGET_VERSION}&quot; 
	fi

	read -p &quot;Please input old harbor dirname [harbor-v1.10.0]: &quot; CURR_HARBOR_PATH
	CURR_HARBOR_PATH=${CURR_HARBOR_PATH:-harbor-v1.10.0}

	read -p &quot;Please input harbor database [/data/harbor]: &quot; DATA_DIR
	DATA_DIR=${DATA_DIR:-/data/harbor}
	
	# 设置备份目录
	BACKUP_DIR=&quot;${ROOT_DIR}/harbor_rollback_backup&quot;
	
	# 当前版本的备份路径
	CURRENT_VERSION_BACKUP_PATH=&quot;$BACKUP_DIR/${CURRENT_VERSION}&quot;
	
	export WORK_DIR CURRENT_VERSION TARGET_VERSION CURR_HARBOR_PATH DATA_DIR BACKUP_DIR CURRENT_VERSION_BACKUP_PATH
}

initialize_workspace()
{
	# 创建工作目录（如果不存在）
	[ -d ${WORK_DIR} ] || mkdir -pv ${WORK_DIR}
	
	# 创建备份目录（如果不存在）
	[ -d ${BACKUP_DIR} ] || mkdir -pv ${BACKUP_DIR}
}

clean_annotation()
{
    find ${WORK_DIR}/${version}/ \
        -name &quot;harbor.yml.*&quot; \
        ! -name &quot;harbor.yml.clean_annotation&quot; \
        ! -name &quot;harbor.yml.tmpl&quot; -type f -exec \
        grep -Ev '^#|^$|^\s*(#|//)' {} + &gt; ${WORK_DIR}/${version}/harbor.yml.clean_annotation
}

replace_configuration()
{
    cp -a ${WORK_DIR}/${version}/harbor.yml.clean_annotation ${WORK_DIR}/${version}/harbor.yml
    sed -i &quot;s@hostname: reg.mydomain.com@hostname: your-harbor-domain.com@g&quot; ${WORK_DIR}/${version}/harbor.yml
    sed -i &quot;s@data_volume: /data@data_volume: ${DATA_DIR}@g&quot; harbor.yml
}

compare_versions() 
{
    local current_version=$1
    local target_version=$2

    if [[ $current_version == v* ]]; then
        current_version=${current_version#v}
    fi

    if [[ $target_version == v* ]]; then
        target_version=${target_version#v}
    fi

    if [[ $current_version == $target_version ]]; then
        return 2
    fi

    IFS='.' read -ra current_version_parts &lt;&lt;&lt; &quot;$current_version&quot;
    IFS='.' read -ra target_version_parts &lt;&lt;&lt; &quot;$target_version&quot;

    for (( i=0; i&lt;${#current_version_parts[@]}; i++ )); do
        if (( ${target_version_parts[$i]} &gt; ${current_version_parts[$i]} )); then
            return 0
        elif (( ${target_version_parts[$i]} &lt; ${current_version_parts[$i]} )); then
            return 1
        fi
    done

    return 2
}

check_version_number()
{
	set +e
	# 校验版本号
	compare_versions &quot;$version&quot; &quot;${CURRENT_VERSION}&quot;

	if [ $? -eq 0 ]; then
		warn &quot;${CURRENT_VERSION} Greater than ${version}.&quot;
		continue
	fi

	compare_versions &quot;$version&quot; &quot;$CURRENT_VERSION&quot;
	if [ $? -eq 2 ]; then
		warn &quot;${TARGET_VERSION} equal ${version}.&quot;
		continue
	fi
	set -e
}

swtich_version()
{
    CURRENT_VERSION=${version:-$CURRENT_VERSION}
    # 当前版本的备份路径
    CURRENT_VERSION_BACKUP_PATH=&quot;${BACKUP_DIR}/${CURRENT_VERSION}&quot;
    # 当前harbor的启动路径
    CURR_HARBOR_PATH=&quot;${WORK_DIR}/${CURRENT_VERSION}&quot;
    export CURRENT_VERSION CURRENT_VERSION_BACKUP_PATH CURR_HARBOR_PATH
}

pause() 
{
    success &quot;Press enter to continue...&quot;
    read -r
}

sorted_version()
{
	# 获取harbor版本列表
	versions=(&quot;${!harbor_versions[@]}&quot;)

	# 对版本列表进行排序
	sorted_versions=($(printf '%s\n' &quot;${versions[@]}&quot; | sort -V))
	export versions sorted_versions
}

initial_backup_dir()
{
    h2 &quot;[Backup initialization]: Starting backup ${CURRENT_VERSION} ...&quot;
    # 创建备份目录（如果不存在）
    [ -d ${CURRENT_VERSION_BACKUP_PATH} ] || mkdir -pv ${CURRENT_VERSION_BACKUP_PATH}
    [ -d ${CURRENT_VERSION_BACKUP_PATH}&quot;_database&quot; ] || mkdir -pv ${CURRENT_VERSION_BACKUP_PATH}&quot;_database&quot;
    [ -d ${CURRENT_VERSION_BACKUP_PATH}&quot;_redis&quot; ] || mkdir -pv ${CURRENT_VERSION_BACKUP_PATH}&quot;_redis&quot;
    note &quot;backup dir is: ${CURRENT_VERSION_BACKUP_PATH} is checked.&quot; 
    note &quot;backup database dir ${CURRENT_VERSION_BACKUP_PATH}_database is checked.&quot; 
}

backup_current_version()
{
    h2 &quot;[Backup progess]: starting backup harbor ${CURRENT_VERSION} ...&quot;
    # 备份harbor
    cp -r ${CURR_HARBOR_PATH} ${CURRENT_VERSION_BACKUP_PATH}/
    # 备份harbor数据目录的database
    cp -Rpf ${DATA_DIR}/database ${CURRENT_VERSION_BACKUP_PATH}&quot;_database&quot;
    cp -Rpf ${DATA_DIR}/redis ${CURRENT_VERSION_BACKUP_PATH}&quot;_redis&quot;
    # 备份 harbor.yml 防止升级被覆盖从而无法回滚
    cp ${CURR_HARBOR_PATH}/harbor.yml ${CURRENT_VERSION_BACKUP_PATH}/harbor.yml.$(date +%F)
    note &quot;harbor ${CURRENT_VERSION} is backup completed, in ${CURRENT_VERSION_BACKUP_PATH}&quot;
}

stop_old_harbor_progress()
{
    # 停止旧版容器组
    h2 &quot;[Progress stop]: stopping ${CURRENT_VERSION} ...&quot;
    cd ${CURR_HARBOR_PATH} &amp;&amp; docker-compose down &amp;&amp; cd ${ROOT_DIR} &amp;&amp; note &quot;harbor ${CURRENT_VERSION} is stopped ...&quot;
}

upgrade_configfile()
{   
    h2 &quot;[Upgrade]: upgrade ${CURRENT_VERSION} to ${version} ...&quot;
    docker run -v /:/hostfs goharbor/prepare:${version} migrate -i ${CURRENT_VERSION_BACKUP_PATH}/harbor.yml.$(date +%F) -o ${WORK_DIR}/${version}/harbor.yml
    note &quot;harbor config version is ${version}&quot;
}

rollback()
{
    step=0
    h2 &quot;[Step $step]: set up rollback env ...&quot;; let step+=1
    set_env_rollback

    # 停止容器
    h2 &quot;[Progress stop]: stopping ${CURRENT_VERSION} ...&quot;
    cd ${WORK_DIR}/${CURRENT_VERSION}/ &amp;&amp; docker-compose down &amp;&amp; cd ${ROOT_DIR} &amp;&amp; note &quot;harbor ${CURRENT_VERSION} is stopped ...&quot;

    # 备份当前版本
    initial_backup_dir
    backup_current_version

    FILE_NAME=${WORK_DIR}/${CURRENT_VERSION}

    # 检查工作目录是否存在已下载文件
    if [ ! -d ${FILE_NAME} ]; then
        error &quot;${CURRENT_VERSION} not found&quot;
        exit 1
    fi

    h2 &quot;[Step $step]: starting switch harbor ${TARGET_VERSION} ...&quot; ; let step+=1
    rm -fr ${DATA_DIR}/database &amp;&amp; rm -fr ${DATA_DIR}/redis &amp;&amp; \
        cp -Rpf &quot;${ROOT_DIR}/harbor_backup/${TARGET_VERSION}_database/database&quot; ${DATA_DIR}/database
        cp -Rpf &quot;${ROOT_DIR}/harbor_backup/${TARGET_VERSION}_redis/redis&quot; ${DATA_DIR}/redis
    note &quot;Switch database to ${TARGET_VERSION} is completed.&quot;

    sleep $((RANDOM % 6 + 10))
    h2 &quot;[Step $step]: starting harbor ${TARGET_VERSION} ...&quot;
    cd &quot;${ROOT_DIR}/harbor_backup/${TARGET_VERSION}/${CURR_HARBOR_PATH}&quot; &amp;&amp; ./install.sh
    success $&quot;----Harbor ${CURRENT_VERSION} to ${TARGET_VERSION} has been rollback.----&quot;

}

download()
{   
    sorted_version
	# 更新harbor
    for version in &quot;${sorted_versions[@]}&quot;; do
        DOWNLOAD_URL=${harbor_versions[$version]}
        FILE_NAME=$(basename $DOWNLOAD_URL)
        FOLDER_NAME=${FILE_NAME%.*}
        ARCHIVE_FILE=&quot;${ROOT_DIR}/$FILE_NAME&quot;

        if [ -f ${ARCHIVE_FILE} ]; then
            warn &quot;${FILE_NAME} existed, skip download..&quot;
            continue
        fi

        note &quot;[Downloader]: ${harbor_versions[$version]}&quot;
        wget &quot;${harbor_versions[$version]}&quot;
    done
}

rotate_upgrade_harbor_versions()
{
	# 获取harbor版本列表
    sorted_version

	# 更新harbor
    for version in &quot;${sorted_versions[@]}&quot;; do
        # 向下传递变量
        export version
        
        h2 &quot;[Install ${version}]: starting upgrade ...&quot; 
        
        # 检查更新是否合法
        # 如果当前版本等于要更新的版本，则不更新
        check_version_number

        # 停止旧版本的服务
        stop_old_harbor_progress

        # 备份当前版本 harbor
        initial_backup_dir
        backup_current_version
        
        h2 &quot;[install checking]: Starting install checking ...&quot;
        # 检查offline安装包是否下载
        DOWNLOAD_URL=${harbor_versions[$version]}
        FILE_NAME=$(basename $DOWNLOAD_URL)
        FOLDER_NAME=${FILE_NAME%.*}
        ARCHIVE_FILE=&quot;${ROOT_DIR}/$FILE_NAME&quot;

        # 检查工作目录是否存在已下载文件
        if [ ! -f ${ARCHIVE_FILE} ]; then
            error &quot;Offline installer ${FILE_NAME} not found，Please download first ${DOWNLOAD_URL}&quot;
            exit 1
        fi
        
        # 创建对应版本的工作目录（如果不存在）

        [ -d &quot;${WORK_DIR}&quot;/&quot;${version}&quot; ] || mkdir -pv &quot;${WORK_DIR}&quot;/&quot;${version}&quot;
        note &quot;install checked&quot;

        h2 &quot;[Uncompress]: starting uncompress harbor offline installer ...&quot;
        # 解压对应版本安装包
        mkdir -pv &quot;${WORK_DIR}&quot;/&quot;${version}&quot; &amp;&amp; tar -xf &quot;${ROOT_DIR}&quot;/&quot;${FILE_NAME}&quot; -C &quot;${WORK_DIR}&quot;/&quot;${version}&quot;/ --strip-components 1
        note &quot;harbor ${version}: ${WORK_DIR}/${version}&quot;

        # 导入镜像
        set +e
        h2 &quot;[Load image]: starting load harbor ${version} ...&quot;
        cd &quot;${WORK_DIR}/${version}&quot; &amp;&amp; docker image load -i harbor.&quot;${version}&quot;.tar.gz
        note &quot;harbor image loaded&quot;
        set -e
        
        h2 &quot;[Replace configration]: start replace default config file ...&quot;
        
        # 清除 harbor.yml 中的注释
        clean_annotation
        # 替换为所需的config
        replace_configuration
        note &quot;replaced&quot;
        
        # 更新操作
        upgrade_configfile

        h2 &quot;[Installer]: start install harbor ${version} ...&quot;
        # 如果使用了定制化 docker-compose 则开启这行
        # 使用准备好的模板来更换容器内部的模板
        # 这样保证了可以随意定制 docker-compose的文件
        cd &quot;${WORK_DIR}/${version}&quot; &amp;&amp; sed -i &quot;/-v \/:\/hostfs/a \\\t\\t -v /root/docker_compose/docker-compose.yml.jinjia.${version}:/usr/src/app/templates/docker_compose/dockercompose.yml.jinjia \\\\&quot; ./prepare
        cd &quot;${WORK_DIR}/${version}&quot; &amp;&amp; ./install.sh
        note &quot;${version} installed&quot;

        success $&quot;----Harbor ${CURRENT_VERSION} to ${version} has been upgraded.----&quot;
        # 切换变量，把这次更新好的版本作为下次要更新的旧版本进行传递
        swtich_version
        pause
    done

    # 完成升级
    success $&quot;----Harbor ${TARGET_VERSION} has been installed and started successfully.----&quot;
}

upgrade()
{
    step=0
    h2 &quot;[Step $step]: set up env ...&quot;; (( step+1 ))
    set_env

    # 初始化目录
    h2 &quot;[Step $step]: initailizaion workspace ...&quot;; (( step+1 ))
    initialize_workspace

    # 滚动版本更新harbor
    h2 &quot;[Step $step]: Starting upgrade gradually ...&quot;; (( step+1 ))
    rotate_upgrade_harbor_versions
}

MAIN(){

    if [ $# -eq 0 ]; then
        warn &quot;Non option ...&quot;
        usage
        exit 1
    fi

    while getopts &quot;duhr&quot; option; do
        case ${option} in
            d)
                download
                R=$?
                ;;
            u)
                upgrade
                R=$?
                ;;
            r)
                rollback
                R=$?
                ;;
            h)
                usage
                R=$?
                ;;
            \?)
                usage
                ;;
        esac
    done
    exit ${R}
}

MAIN ${@}
</code></pre>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://web.archive.org/web/20230923103738/https://goharbor.io/docs/1.10/administration/upgrade/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Upgrade Harbor and Migrate Data - v1.10.0</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://web.archive.org/web/20231207205057/https://goharbor.io/docs/2.4.0/administration/upgrade/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Upgrade Harbor and Migrate Data - v2.4.0</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://web.archive.org/web/20230923113547/https://goharbor.io/docs/2.6.0/administration/upgrade/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Upgrade Harbor and Migrate Data - v2.6.0</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://web.archive.org/web/20231207201739/https://goharbor.io/docs/2.8.0/administration/upgrade/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Upgrade Harbor and Migrate Data - v2.8.0</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://web.archive.org/web/20240313095530/https://goharbor.io/docs/2.10.0/administration/upgrade/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Upgrade Harbor and Migrate Data - v2.10.0</strong></em></a></p>
<p><sup id="6">[6]</sup> <a href="https://github.com/goharbor/harbor/blob/main/make/prepare" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Prepare 脚本</strong></em></a></p>
<p><sup id="6">[6]</sup> <a href="https://archive.is/RsQzK" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>生产系统中升级 Harbor 的完整流程</strong></em></a></p>
<p><sup id="6">[6]</sup> <a href="https://web.archive.org/web/20230925022643/https://www.fosstechnix.com/upgrade-harbor-from-v1-10-7-to-v2-4-0-then-2-6/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Upgrade Harbor from v1.10.7 to v2.4.0 then 2.6.0</strong></em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes维护 - secret批量更新</title>
      <link>https://www.oomkill.com/2024/02/kubernetes-update-secert/</link>
      <pubDate>Tue, 20 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/02/kubernetes-update-secert/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>tls 证书在 k8s 集群上大量使用的话，当到期时会存在批量替换的难度，比如说每个名称空间，多个业务的使用，在这篇博文中，将尝试批量替换整个集群的证书（前提，在没有使用 vault, cert-manager这类组件的集群之上）。</p>
<h2 id="基本操作">基本操作</h2>
<p>步骤1：首先不知道有多少个名称空间使用了这个证书，所以需要遍历所有的名称空间，这里使用 kubectl 的 json path 实现</p>
<pre><code class="language-bash">$ kubectl get ns -o jsonpath='{range $.items[*]}{.metadata.name}{&quot;\n&quot;}{end}'
</code></pre>
<p>步骤2：拿到名称空间的名字后，就需要循环这个名称空间下所有的 secret</p>
<pre><code class="language-bash">for ns in `kubectl get ns -o jsonpath='{range $.items[*]}{.metadata.name}{&quot;\n&quot;}{end}'`
do
	kubectl get secret -n $ns -o jsonpath='{range $.items[*]}{.metadata.name}{&quot;\n&quot;}{end}'
done
</code></pre>
<p>步骤3：找到与这个匹配的证书进行替换</p>
<p>把步骤3拆解为下面几个步骤：</p>
<ul>
<li>拿去到符合要求的secret的名字</li>
<li>匹配名称是否为修改的secret</li>
<li>做替换操作</li>
</ul>
<p>由于步骤2使用的是 jsonpath 拿到的 secret name，由于 kubectl 并不支持高级jsonpath语法，官方推荐使用jq，那么使用jq获取名字</p>
<pre><code class="language-bash">kubectl get secret -n $ns -o json|jq -r .items[].metadata.name
</code></pre>
<p>使用 awk 做字符串匹配，匹配是否包含对应的字符串关键词</p>
<pre><code class="language-bash">awk '$1 ~ /xxxx/ { print }'
</code></pre>
<p>最后使用 kubectl replace 替换现有的 secret</p>
<pre><code class="language-bash">kubectl create secret tls $secertName -n $ns --cert=xxx.crt --key=xxx.key --dry-run -o yaml| kubectl replace -f -
</code></pre>
<p>三个步骤整个为一个命令，如下所示：</p>
<pre><code class="language-bash">for ns in `kubectl get ns -o jsonpath='{range $.items[*]}{.metadata.name}{&quot;\n&quot;}{end}'`
do
	for secertName in `kubectl get secret -n $ns -o json|jq -r .items[].metadata.name|awk '$1 ~ /xxxx/ { print }';
	do
		kubectl create secret tls $secertName -n $ns --cert=xxx.crt --key=xxx.key --dry-run -o yaml| kubectl replace -f -
	done
done
</code></pre>
<blockquote>
<p>注意：</p>
<ul>
<li>ingress 使用的证书会立即更新</li>
<li>如果是作为 secret 挂在到 Pod 中需要重启，这是因为 kubelet volume 机制导致的。</li>
</ul>
</blockquote>
<h2 id="高级用法">高级用法</h2>
<p>查询证书内容，并根据域名进行替换</p>
<p>查看证书域名的命令</p>
<pre><code class="language-bash">ns=xxx
secret_name=xxx
keywords=xxx
_command=xxxx
openssl x509 -in &lt;(
    ${_command} get secret -n $ns $secret_name -ojson | \
    jq --arg secret_key $(
        ${_command} get secret -n ${ns} ${secret_name} -ojson | \
        jq -r '.data | keys[]' | \
        awk -v keywords=${keywords} '$0 ~ keywords { print }'
    ) -r '.data | .[$secret_key]' | base64 -d
) -text -noout | \
grep -i &quot;subject: &quot;
</code></pre>
<p>步骤拆解</p>
<pre><code class="language-bash"># 替换的名称空间
ns=kube-system
# 替换的secret_name
secret_name=test
# 替换key的关键词，通常secret作为tls存在时，为xxx.crt
keywords=crt
# kubectl命令
_command=kubectl --kubeconfig=kubeconfig

# shell语法，将一个命令的输出作为openssl的输入
openssl x509 -in &lt;(
    ${_command} get secret -n $ns $secret_name -ojson | \
    # jq --arg secret_key 是 jq语法，将 $() 命名为secret_key作为变量传入到下个命令使用
    jq --arg secret_key $(
        ${_command} get secret -n ${ns} ${secret_name} -ojson | \
        jq -r '.data | keys[]' | \
        # awk -v keywords=${keywords} 是awk语法，将keyworkd作为变量传递到awk内部
        # '$0 ~ keywords { print }' 打印出符合条件的行
        awk -v keywords=${keywords} '$0 ~ keywords { print }'
    ) -r '.data | .[$secret_key]' | base64 -d
) -text -noout | \
grep -i &quot;subject: &quot;
</code></pre>
<p>更新前没出意外需要备份下对应资源，批量备份集群内指定域名的 secret 命令</p>
<pre><code class="language-bash">```bash
# 指定域名的secret
keywords=xxx
_command=xxxx
for ns in `${_command} get ns -o jsonpath='{range $.items[*]}{.metadata.name}{&quot;\n&quot;}{&quot;end&quot;}'`
do
    for secret_name in `${_command} get secret -n $ns -o json|jq -r '.items[] | select( .type == &quot;kubernetes.io/tls&quot;) | .metadata.name'`
    do  
        openssl x509 -in &lt;(
            ${_command} get secret -n ${ns} ${secret_name} -ojson | \
            jq --arg secret_key $(
                    ${_command} get secret -n ${ns} ${secret_name} -ojson | 
                    jq -r '.data | keys[]' | \
                    awk '$0 ~ /crt/ { print }'
                ) -r '.data|.[$secret_key]'|base64 -d
        ) -text -noout | \
        grep -i &quot;subject: &quot;| \
        awk -v ns=${ns} -v secret_name=${secret_name} -v cmd=${_command} -v kw=${keywords} \
            '$0 ~ kw { system(
                cmd&quot; get secret -n &quot;ns&quot; &quot;secret_name&quot; -oyaml &gt; primetive/&quot;ns&quot;.&quot;secret_name&quot;.yaml&quot;
            )}'
    done
done
</code></pre>
<p>批量替换命令，查询 tls 签发域名，并进行替换</p>
<pre><code class="language-bash">keywords=xxx
_command=xxxx
for ns in `${_command} get ns -o jsonpath='range $.items[*]}{.metadata.name}{&quot;\n&quot;}{&quot;end&quot;}'
do
    for secret_name in `${_command} get secret -n $ns -o json|jq -r '.items[] | select( .type == &quot;kubernetes.io/tls&quot;) | .metadta.name'`
    do  
        openssl x509 -n &lt;(
            ${_command} get secret -n ${ns} ${secret_name} -ojson | \
            jq --arg secret_key $(${_command} get secret -n ${ns} ${secret_name} -ojson | \
            jq -r '.data | .keys[]' | \
            awk '$0 ~ /crt/ { print }' -r '.data|.[$secret_key]'|base64 -d
        ) -text -noout | \
        grep -i &quot;subject: &quot;| \
        awk -v ns=${ns} -v secret_name=${secret_name} -v commond=${_command}\
            '$0 ~ ${keywords} { system(
                command&quot; create secret tls -n &quot;ns&quot; &quot;secret_name&quot; --cert=ca.crt --key=key.key --dry-run -oyaml|command&quot; replace -f -&quot;
            )}'
    done
done
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>k8s - jsonnet从入门到放弃</title>
      <link>https://www.oomkill.com/2024/02/k8s-jsonnet/</link>
      <pubDate>Sun, 18 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/02/k8s-jsonnet/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="什么是jsonnet">什么是jsonnet</h2>
<p>jsonnet是用于app或开发工具的数据模板语言，主要用于json的扩展，具有下面功能：</p>
<ul>
<li>生成配置数据</li>
<li>无副作用</li>
<li>组织化，简化，统一化</li>
<li>管理无序的配置</li>
</ul>
<p>jsonnet可以通过面向对象消除重复。或者，使用函数。与现有/自定义应用程序集成。生成 JSON、YAML、INI 和其他格式。</p>
<h3 id="安装jsonnet">安装jsonnet</h3>
<p>Jsonnet 有两种实现（C++ 和 Go）</p>
<p>在 Debian/Ubuntu 之上，可以直接使用 apt 源来安装</p>
<pre><code class="language-bash">apt install jsonnet -y
</code></pre>
<p>安装 go 实现的，可以用下面命令，前提是安装了go</p>
<pre><code class="language-bash">go get github.com/google/go-jsonnet/cmd/jsonnet
</code></pre>
<h2 id="什么是jsonnet-bundler">什么是jsonnet-bundler</h2>
<p>jsonnet-bundler 是 Jsonnet 的包管理器，用于个简化 jsonnet 项目中依赖关系管理的工具。使用 Jsonnet Bundler 可以带来下面便利之处：</p>
<ul>
<li>使用 <code>jsonnetfile.json</code> 作为依赖关系管理</li>
<li>自动安装和更新依赖项。</li>
<li>版本选择，使用 <code>jsonnetfile.json</code> 可以确保项目使用正确版本的依赖。</li>
<li>可重复构建， 使用 <code>jsonnetfile.json</code> 管理的项目可以在不同环境中构建并确保结果的一致性。</li>
<li>更方便的与 GitOps 结合， Jsonnet Bundler 提供 与 GitOps 的集成。</li>
</ul>
<h3 id="jsonnet-bundler-安装">jsonnet-bundler 安装</h3>
<p>Jsonnet Bundler 有两种安装模式，实际上就是 Go 程序通用的安装方式：</p>
<ul>
<li>Jsonnet Bundler 二进制文件 （预构建）</li>
<li>go install 安装</li>
</ul>
<p>在 jsonnet bundler <a href="https://github.com/jsonnet-bundler/jsonnet-bundler/releases" target="_blank"
   rel="noopener nofollow noreferrer" >release</a> 页面下载对应版本</p>
<p>使用 go install 安装</p>
<pre><code class="language-bash">go install -a github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb@latest
</code></pre>
<blockquote>
<p><strong>注意</strong>：</p>
<ul>
<li>jsonnet bundler 要求 Go 版本最少是 Go 1.13 或更高版本。 <sup><a href="#1">[1]</a></sup></li>
<li>使用 go install 安装的，需要将 <code>$(go env GOPATH)/bin</code>  加入到 <code>$PATH</code></li>
</ul>
</blockquote>
<h3 id="jsonnetfilejson">jsonnetfile.json</h3>
<p><a href="https://jsonnet.movatech.today/blog/structure-of-the-jsonnetfile.json-file/" target="_blank"
   rel="noopener nofollow noreferrer" >https://jsonnet.movatech.today/blog/structure-of-the-jsonnetfile.json-file/</a></p>
<p><a href="https://jsonnet.org/learning/tutorial.html" target="_blank"
   rel="noopener nofollow noreferrer" >https://jsonnet.org/learning/tutorial.html</a></p>
<p><a href="https://jsonnet-libs.github.io/jsonnet-training-course/" target="_blank"
   rel="noopener nofollow noreferrer" >https://jsonnet-libs.github.io/jsonnet-training-course/</a></p>
<p><a href="https://medium.com/@pmspraveen8/k8s-libsonnet-generating-manifests-33a5e5aff277" target="_blank"
   rel="noopener nofollow noreferrer" >https://medium.com/@pmspraveen8/k8s-libsonnet-generating-manifests-33a5e5aff277</a></p>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://github.com/jsonnet-bundler/jsonnet-bundler" target="_blank"
   rel="noopener nofollow noreferrer" ><em>jsonnet</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://support.smartbear.com/alertsite/docs/monitors/api/endpoint/jsonpath.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em>JSONPath Syntax</em></a></p>
<p><sup id="2">[3]</sup> <a href="https://www.ctyun.cn/zhishi/p-168130" target="_blank"
   rel="noopener nofollow noreferrer" ><em>k8s学习-kubectl命令行 jsonpath的使用</em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes - 基于OOMKill的QoS的设计</title>
      <link>https://www.oomkill.com/2024/01/ch30-oomkill/</link>
      <pubDate>Tue, 30 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/01/ch30-oomkill/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>阅读完本文，您当了解</p>
<ul>
<li>Linux oom kill</li>
<li>Kubernetes oom 算法</li>
<li>Kubernetes QoS</li>
</ul>
<blockquote>
<p>本文只是个人理解，如果有大佬觉得不是这样的可以留言一起讨论，参考源码版本为 1.18.20，与高版本相差不大</p>
</blockquote>
<h2 id="什么是oom-kill">什么是OOM Kill</h2>
<p>当你的Linux机器内存不足时，内核会调用Out of Memory (OOM) killer来释放一些内存。这经常在运行许多内存密集型进程的服务器上遇到。</p>
<h3 id="oom-killer是如何选择要杀死的进程的">OOM Killer是如何选择要杀死的进程的？</h3>
<p>Linux内核为每个运行的进程分配一个分数，称为 <code>oom_score</code>，==显示在内存紧张时终止该进程的可能性有多大==。该 Score 与进程使用的内存量成比例。 Score 是进程使用内存的百分比乘以10。因此，最大分数是 $100% \times 10 = 1000$。此外，如果一个进程以特权用户身份运行，那么与普通用户进程相比，它的 <code>oom_score</code> 会稍低。</p>
<p>在主发行版内核会将 <code>/proc/sys/vm/overcommit_memory</code> 的默认值设置为零，这意味着进程可以请求比系统中当前可用的内存更多的内存。这是基于以下启发式完成的：分配的内存不会立即使用，并且进程在其生命周期内也不会使用它们分配的所有内存。如果没有过度使用，系统将无法充分利用其内存，从而浪费一些内存。过量使用内存允许系统以更有效的方式使用内存，但存在 OOM 情况的风险。占用内存的程序会耗尽系统内存，使整个系统陷入瘫痪。当内存太低时，这可能会导致这样的情况：即使是单个页面也无法分配给用户进程，从而允许管理员终止适当的任务，或者内核执行重要操作，例如释放内存。在这种情况下，OOM Killer 就会介入，并将该进程识别为牺牲品，以保证系统其余部分的利益。</p>
<p>用户和系统管理员经常询问控制 OOM Killer 行为的方法。为了方便控制，引入了 <code>/proc/&lt;pid&gt;/oom_adj</code> 来防止系统中的重要进程被杀死，并定义进程被杀死的顺序。 oom_adj 的可能值范围为 -17 到 +15。Score 越高，相关进程就越有可能被 OOM-killer Kill。如果 <code>oom_adj</code> 设置为 -17，则 OOM Killer 不会  Kill 该进程。</p>
<p>oom_score 分数为 1 ~ 1000，值越低，程序被杀死的机会就越小。</p>
<ul>
<li>oom_score 0 表示该进程未使用任何可用内存。</li>
<li>oom_score  1000 表示该进程正在使用 100% 的可用内存，大于1000，也取1000。</li>
</ul>
<h3 id="谁是糟糕的进程">谁是糟糕的进程？</h3>
<p>在内存不足的情况下选择要被终止的进程是基于其  <em>oom_score</em> 。糟糕进程 Score 被记录在 <code>/proc/&lt;pid&gt;/oom_score</code> 文件中。该值是基于系统损失的最小工作量、回收的大量内存、不终止任何消耗大量内存的无辜进程以及终止的进程数量最小化（如果可能限制在一个）等因素来确定的。糟糕程度得分是使用进程的原始内存大小、其 CPU 时间（utime + stime）、运行时间（uptime - 启动时间）以及其 <code>oom_adj</code> 值计算的。进程使用的内存越多，得分越高。进程在系统中存在的时间越长，得分越小。</p>
<h3 id="列出所有正在运行的进程的oom-score">列出所有正在运行的进程的OOM Score</h3>
<pre><code class="language-bash">printf 'PID\tOOM Score\tOOM Adj\tCommand\n'
while read -r pid comm; do [ -f /proc/$pid/oom_score ] &amp;&amp; [ $(cat /proc/$pid/oom_score) != 0 ] &amp;&amp; printf '%d\t%d\t\t%d\t%s\n' &quot;$pid&quot; &quot;$(cat /proc/$pid/oom_score)&quot; &quot;$(cat /proc/$pid/oom_score_adj)&quot; &quot;$comm&quot;; done &lt; &lt;(ps -e -o pid= -o comm=) | sort -k 2nr
</code></pre>
<h3 id="如何检查进程是否已被-oom-终止">如何检查进程是否已被 OOM 终止</h3>
<p>最简单的方法是查看<code>grep</code>系统日志。在 Ubuntu 中：<code>grep -i kill /var/log/syslog</code>。如果进程已被终止，您可能会得到类似的结果</p>
<pre><code class="language-bash">my_process invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0
</code></pre>
<h2 id="kubernetes的qos是如何设计的">Kubernetes的QoS是如何设计的</h2>
<p>Kubernetes 中 Pod 存在一个 “服务质量等级” (<em>QoS</em>)，它保证了Kubernetes 在 Node 资源不足时使用 QoS 类来就驱逐 Pod 作出决定。这个 QoS 就是基于 OOM Kill Score 和 Adj 来设计的。</p>
<p>对于用户来讲，Kubernetes Pod 的 QoS 有三类，这些设置是被自动设置的，除此之外还有两种单独的等级：“Worker 组件”，总共 Pod QoS 的级别有5种</p>
<ul>
<li>Kubelet</li>
<li>KubeProxy</li>
<li>Guaranteed</li>
<li>Besteffort</li>
<li>Burstable</li>
</ul>
<p>这些在 <a href="pkg/kubelet/qos/policy.go">pkg/kubelet/qos/policy.go</a> 中可以看到，其中 Burstable 属于一个动态的级别。</p>
<pre><code class="language-go">const (
	// KubeletOOMScoreAdj is the OOM score adjustment for Kubelet
	KubeletOOMScoreAdj int = -999
	// KubeProxyOOMScoreAdj is the OOM score adjustment for kube-proxy
	KubeProxyOOMScoreAdj  int = -999
	guaranteedOOMScoreAdj int = -998
	besteffortOOMScoreAdj int = 1000
)
</code></pre>
<p>其中最重要的分数就是 Burstable，这保证了驱逐的优先级，他的算法为：$1000 - \frac{1000 \times Request}{memoryCapacity}$ ，Request 为 Deployment 这类清单中配置的 <em>Memory Request</em> 的部分，<em>memoryCapacity</em> 则为 Node 的内存数量。</p>
<p>例如 Node 为 64G，Pod Request 值配置了 2G，那么最终 <code>oom_score_adj</code> 的值为 $1000 - \frac{1000 \times Request}{memoryCapacity} = 1000 - \frac{1000\times2}{64} = 968$</p>
<p>这部分可以在下面代码中看到，其中算出的值将被写入 /proc/{pid}/oom_score_adj 文件内</p>
<pre><code class="language-go">func GetContainerOOMScoreAdjust(pod *v1.Pod, container *v1.Container, memoryCapacity int64) int {
	if types.IsNodeCriticalPod(pod) {
		// Only node critical pod should be the last to get killed.
		return guaranteedOOMScoreAdj
	}

	switch v1qos.GetPodQOS(pod) {
	case v1.PodQOSGuaranteed:
		// Guaranteed containers should be the last to get killed.
		return guaranteedOOMScoreAdj
	case v1.PodQOSBestEffort:
		return besteffortOOMScoreAdj
	}

	// Burstable containers are a middle tier, between Guaranteed and Best-Effort. Ideally,
	// we want to protect Burstable containers that consume less memory than requested.
	// The formula below is a heuristic. A container requesting for 10% of a system's
	// memory will have an OOM score adjust of 900. If a process in container Y
	// uses over 10% of memory, its OOM score will be 1000. The idea is that containers
	// which use more than their request will have an OOM score of 1000 and will be prime
	// targets for OOM kills.
	// Note that this is a heuristic, it won't work if a container has many small processes.
	memoryRequest := container.Resources.Requests.Memory().Value()
	if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) {
		if cs, ok := podutil.GetContainerStatus(pod.Status.ContainerStatuses, container.Name); ok {
			memoryRequest = cs.AllocatedResources.Memory().Value()
		}
	}
	oomScoreAdjust := 1000 - (1000*memoryRequest)/memoryCapacity
	// A guaranteed pod using 100% of memory can have an OOM score of 10. Ensure
	// that burstable pods have a higher OOM score adjustment.
	if int(oomScoreAdjust) &lt; (1000 + guaranteedOOMScoreAdj) {
		return (1000 + guaranteedOOMScoreAdj)
	}
	// Give burstable pods a higher chance of survival over besteffort pods.
	if int(oomScoreAdjust) == besteffortOOMScoreAdj {
		return int(oomScoreAdjust - 1)
	}
	return int(oomScoreAdjust)
}
</code></pre>
<p>到此可以了解到 Pod QoS 级别为</p>
<ul>
<li>
<p>Kubelet = KubeProxy = -999</p>
</li>
<li>
<p>Guaranteed = -998</p>
</li>
<li>
<p>1000(<em>Besteffort</em>) &gt; Burstable &gt; -998 (<em>Guaranteed</em>)</p>
</li>
<li>
<p>Besteffort = 1000</p>
</li>
</ul>
<p>那么在当 Node 节点内存不足时，发生驱逐的条件就会根据  <code>oom_score_adj</code> 完成，但当 Pod 中程序使用内存达到了 Limits 限制，此时的OOM Killed和上面阐述的无关。</p>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://lwn.net/Articles/317814/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Taming the OOM killer</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://unix.stackexchange.com/questions/153585/how-does-the-oom-killer-decide-which-process-to-kill-first" target="_blank"
   rel="noopener nofollow noreferrer" ><em>How does the OOM killer decide which process to kill first?</em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes上jprofiler自动映射 - 项目设计</title>
      <link>https://www.oomkill.com/2023/12/stackstorm-rules/</link>
      <pubDate>Mon, 11 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/12/stackstorm-rules/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="项目设计">项目设计</h2>
<p>需求：外部分析器工具连接到运行在 kubernetes 集群上 Java pod 的 JVM，通过 jprofiler 暴露其接口，可以直接连接至这个 java pod</p>
<h2 id="问题解决">问题解决</h2>
<p>Jprofiler 如何在 kubernetes 集群中运行：</p>
<ul>
<li>方法1：打包至业务Pod容器内
<ul>
<li>缺点：需要侵入业务Pod内，不方便</li>
</ul>
</li>
<li>方法2：使用 Init Container 将 JProfiler 安装复制到 Init Container 和将在 Pod 中启动的其他容器之间共享的卷</li>
<li>方法3：使用 sidecar 方式 共享业务Pod与 sidecar 共享名称空间
<ul>
<li>缺点：涉及到容器共享进程空间，与 jprofiler-agent 机制问题，所以需要共享 /tmp 目录</li>
</ul>
</li>
</ul>
<blockquote>
<p>JProfiler finds JVMs via the &ldquo;Attach API&rdquo; that is part of the JDK. Have a look at the <code>$TMP/hsperfdata_$USER</code> directory, which is created by the hot spot JVM. It should contain PID files for all running JVMs. If not, delete the directory and restart all JVMs.</p>
</blockquote>
<h2 id="使用-init-container-实施步骤">使用 Init Container 实施步骤</h2>
<h3 id="先决条件">先决条件</h3>
<p>假设已存在 Java 应用程序 deployment，我们还需要一个 JProfiler 镜像。如果您没有 JProfiler 镜像，这里有一个可用于构建映像的Dockerfile示例</p>
<pre><code class="language-yaml">FROM centos:7

# Switch to root
USER 0

ENV \
 JPROFILER_DISTRO=&quot;jprofiler_linux_10_1_1.tar.gz&quot; \
 STAGING_DIR=&quot;/jprofiler-staging&quot; \
 HOME=&quot;/jprofiler&quot;

LABEL \
 io.k8s.display-name=&quot;JProfiler from ${JPROFILER_DISTRO}&quot;

RUN yum -y update \
 &amp;&amp; yum -y install ca-certificates curl \
 &amp;&amp; mkdir -p ${HOME} ${STAGING_DIR} \
 &amp;&amp; cd ${STAGING_DIR} \
 # curl is expected to be available; wget would work, too
 # Add User-Agent header to pretend to be a browser and avoid getting HTTP 404 response
 &amp;&amp; curl -v -OL &quot;https://download-keycdn.ej-technologies.com/jprofiler/${JPROFILER_DISTRO}&quot; -H &quot;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36&quot; \
 &amp;&amp; tar -xzf ${JPROFILER_DISTRO} \
 &amp;&amp; rm -f ${JPROFILER_DISTRO} \
 # Eliminate the version-specific directory
 &amp;&amp; cp -R */* ${HOME} \
 &amp;&amp; rm -Rf ${STAGING_DIR} \
 &amp;&amp; chmod -R 0775 ${HOME} \
 &amp;&amp; yum clean all

# chown and switch user as needed

WORKDIR ${HOME}
</code></pre>
<h3 id="与业务pod配置">与业务Pod配置</h3>
<p>更改应用程序的部署配置如下：</p>
<ul>
<li>如果尚未定义，请在 “spec.template.spec” 下添加 “volumes” 部分并定义一个新卷：</li>
</ul>
<pre><code class="language-yaml">volumes:
  - name: jprofiler-share-tmp
    emptyDir: {}
</code></pre>
<p>如果尚未定义，请在“spec.template.spec” 下添加 “initContainers”（Kubernetes 1.6+），并使用 JProfiler 的镜像定义 Init Container 将 Init container 中的文件复制到共享目录</p>
<pre><code class="language-yaml">initContainers:
  - name: jprofiler-init
    image: &lt;JPROFILER_IMAGE:TAG&gt;
    command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cp -R /jprofiler/ /tmp/&quot;]
    volumeMounts:
      - name: jprofiler
        mountPath: &quot;/tmp/jprofiler&quot;
</code></pre>
<p>将 jprofiler-agent 添加到 JVM 启动参数。</p>
<pre><code class="language-yaml">-agentpath:/jprofiler/bin/linux-x64/libjprofilerti.so=port=8849
</code></pre>
<p>完整的Deployment 示例</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: jprofiler-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jprofiler
  template:
    metadata:
      labels:
        app: jprofiler
    spec:
      volumes:
      - name: jprofiler-share-tmp
        emptyDir: {}
      shareProcessNamespace: true
      initContainers:
      - name: jprofiler-init
        image: jprofiler:14_0
        command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;cp -R /jprofiler/ /tmp/&quot;]
        volumeMounts:
          - name: jprofiler-share-tmp
            mountPath: &quot;/tmp&quot;
      containers:
        - name: sprintboot-test
          image:javaweb:3
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: jprofiler-share-tmp
            mountPath: /tmp
          env:
          - name: JAVA_OPTS
          # nowait 表示启动时不需要手动确认，如果不加会stuck到 jprofiler，使得业务容器不能启动
          # -agentpath 必须加到java参数后，而不是 java -jar xxx -agentpath 这样
            value: &quot;-agentpath:/tmp/jprofiler/jprofiler14.0/bin/linux-x64/libjprofilerti.so=port=8849,nowait&quot; 
          command: 
          - &quot;java&quot;
          - &quot;-jar&quot;
          - demo-0.0.1-SNAPSHOT.jar 
          args:
          - --server.port=8085
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>client-go - Pod使用in-cluster方式访问集群</title>
      <link>https://www.oomkill.com/2023/11/ch07-in-cluster-pod/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/11/ch07-in-cluster-pod/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>在我们基于 Kubernetes 编写云原生 GoLang 代码时，通常在本地调试时，使用 kubeconfig 文件，以构建基于 clientSet 的客户端。而在将代码作为容器部署到集群时，则会使用集群 (in-cluster) 内的配置。</p>
<p>clientcmd 模块用于通过传递本地 kubeconfig 文件构建 clientSet。因此，在容器内使用相同模块构建 clientSet 将需要维护容器进程可访问的 kubeconfig 文件，并设置具有访问 Kubernetes 资源权限的 serviceaccount token。</p>
<p>下面是一个基于 kubeconfig 访问集群的代码模式</p>
<pre><code class="language-go">var (
    k8sconfig  *string //使用kubeconfig配置文件进行集群权限认证
    restConfig *rest.Config
    err        error
)
if home := homedir.HomeDir(); home != &quot;&quot; {
    k8sconfig = flag.String(&quot;kubeconfig&quot;, fmt.Sprintf(&quot;./admin.conf&quot;), &quot;kubernetes auth config&quot;)
}

flag.Parse()
if _, err := os.Stat(*k8sconfig); err != nil {
    panic(err)
}
clientset,err := kubernetes.NewConfig(k8sconfig)
if err != nil {
    panic(err)
}
</code></pre>
<p>这样做可能导致 serviceaccount token 本身被潜在地暴露出去。如果任何用户能够执行到使用 kubeconfig 与集群通信的容器，那么就可以获取该 token，并可以伪装成服务账号从集群外部与 kube-apiserver 进行通信。</p>
<p>为了避免这种情况，我们在 client-go 模块中使用了 rest 包。这将帮助我们从集群内部与集群通信，前提是使用适当的服务账号运行。但这需要对代码进行重写，以适应从集群外部构建 client-set 的方式。</p>
<p>下面代码时使用 in-cluster 方式进行通讯的模式</p>
<pre><code class="language-go">	var (
		k8sconfig  *string //使用kubeconfig配置文件进行集群权限认证
		restConfig *rest.Config
		err        error
	)
	if home := homedir.HomeDir(); home != &quot;&quot; {
		k8sconfig = flag.String(&quot;kubeconfig&quot;, fmt.Sprintf(&quot;./admin.conf&quot;), &quot;kubernetes auth config&quot;)
	}
	k8sconfig = k8sconfig
	flag.Parse()
	if _, err := os.Stat(*k8sconfig); err != nil {
		panic(err)
	}

	if restConfig, err = rest.InClusterConfig(); err != nil {
		// 这里是从masterUrl 或者 kubeconfig传入集群的信息，两者选一
        // 先从 in-cluster 方式获取，如果不能获取，再执行这里
		restConfig, err = clientcmd.BuildConfigFromFlags(&quot;&quot;, *k8sconfig)
		if err != nil {
			panic(err)
		}
	}
	restset, err := kubernetes.NewForConfig(restConfig)
</code></pre>
<p>除了这些之外，还需要创建对应的 serviceaccount 来让 Pod 在 in-cluster 有权限获取到自己要的资源，下面是一个完整的 deployment 创建这些资源的清单</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: infra
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-proxier-secret-reader
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods&quot;]
    verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pod-proxier-rolebinding
subjects:
  - kind: ServiceAccount
    name: pod-proxier-secret-sa
    namespace: infra
roleRef:
  kind: ClusterRole
  name: pod-proxier-secret-reader
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: infra
  name: pod-proxier-secret-sa
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-proxier
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-proxier
  template:
    metadata:
      labels:
        app: pod-proxier
    spec:
      serviceAccount: pod-proxier-secret-sa # 使用上面定义的 sa 进行in-cluster 访问
      containers:
        - name: container-1
          image: haproxytech/haproxy-debian:2.6
          ports:
            - containerPort: 80
          hostPort: 8080  # 添加 hostPort 字段
        - name: container-2
          image: container-2-image:tag
          ports:
            - containerPort: 8080
          hostPort: 8081  # 添加 hostPort 字段
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>K8S Admission Webhook官方扩展版 - ValidatingAdmissionPolicy</title>
      <link>https://www.oomkill.com/2023/11/kubernetes-validatingadmissionpolicy/</link>
      <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/11/kubernetes-validatingadmissionpolicy/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p>相关阅读：<a href="https://cylonchau.github.io/ch3.7-admission-webhook.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Admission Control源码解析</a></p>
</blockquote>
<p>准入 (<em>Admission</em>) 是 Kubernetes 提供 4A 安全认证中的一个步骤，在以前版本中 (1,26-)，官方提供了 webhook 功能，使用户可以自行的定义 Kubernetes 资源准入规则，但这些是有成本的，需要自行开发 webhook，下图是 Kubernetes准入控制流程。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/admission-controller-phases.png" alt="准入控制器阶段" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图</center>
<center><em>Source：</em>https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/</center>
<p>在 Kubernetes 1.26 时 引入了 ValidatingAdmissionPolicy alpha 版，这个功能等于将  Admission Webhook controller 作为了一个官方扩展版，通过资源进行自行扩展，通过这种方式带来下面优势：</p>
<ul>
<li>减少了准入请求延迟，提高可靠性和可用性</li>
<li>能够在不影响可用性的情况下失败关闭</li>
<li>避免 webhooks 的操作负担</li>
</ul>
<h2 id="validatingadmissionpolicy-说明">ValidatingAdmissionPolicy 说明</h2>
<p>验证准入策略提供一种声明式的、进程内的替代方案来验证准入 Webhook。</p>
<p>验证准入策略使用通用表达语言 (Common Expression Language，CEL) 来声明策略的验证规则。 验证准入策略是高度可配置的，使配置策略的作者能够根据集群管理员的需要， 定义可以参数化并限定到资源的策略</p>
<p>下面是一个 ValidatingAdmissionPolicy 的示例，配置 Deployment 必须拥有的副本数的限制</p>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1alpha1
kind: ValidatingAdmissionPolicy
metadata:
  name: &quot;demo-policy.example.com&quot;
spec:
  matchConstraints:
    resourceRules:
    - apiGroups:   [&quot;apps&quot;]
      apiVersions: [&quot;v1&quot;]
      operations:  [&quot;CREATE&quot;, &quot;UPDATE&quot;]
      resources:   [&quot;deployments&quot;]
  validations:
    - expression: &quot;object.spec.replicas &lt;= 5&quot;
</code></pre>
<p>这里 规格 Spec 中有两个关键属性：</p>
<ul>
<li><code>expression</code> 字段包含用于验证的 CEL 表达式</li>
<li><code>matchConstraints</code> 声明什么表达式应用的类型</li>
</ul>
<p>在声明完规则后还需要应用到资源上才生效，这里 Kubernetes 还有另外一个资源类型 <code>ValidatingAdmissionPolicyBinding</code> 可以声明 “<strong>资源</strong>” 和 “<strong>策略</strong>” 绑定到一起，例如下面示例</p>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1alpha1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: &quot;demo-binding-test.example.com&quot;
spec:
  policyName: &quot;demo-policy.example.com&quot;
  validationActions: [Deny]
  matchResources:
    namespaceSelector:
      matchLabels:
        environment: test
</code></pre>
<p>在这里需要注意的是，每个 <em><strong>ValidatingAdmissionPolicyBinding</strong></em> 必须指定一个或多个 <em><strong>validationActions</strong></em> 来声明如何执行策略的 <code>validations</code>，其中 <em>validationActions</em> 包括：</p>
<ul>
<li><strong>Deny</strong>:  验证失败会导致请求被拒绝。</li>
<li><strong>Warn</strong>: 验证失败会作为警告报告给请求客户端。</li>
<li><strong>Audit</strong>: 验证失败会包含在 API 请求的审计事件中。</li>
</ul>
<p>这三个值可以同时设置，表示同时生效，例如：同时向客户端发出验证失败的警告并记录验证失败的审计记录，可以按照下面配置</p>
<pre><code class="language-yaml">validationActions: [Warn, Audit]
</code></pre>
<p>其中，<code>Deny</code> 和 <code>Warn</code> 不能一起使用，因为这种组合会不必要地将验证失败重复输出到 API 响应体和 HTTP 警告头中。</p>
<h2 id="validatingadmissionpolicy-的高级可用性">ValidatingAdmissionPolicy 的高级可用性</h2>
<p>ValidatingAdmissionPolicy 也是一种高度可自由配置的功能，这种方式使策略维护者能够自定义==可根据需要参数化并限定资源范围的策略== 。 例如下面一个策略示例</p>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1alpha1
kind: ValidatingAdmissionPolicy
metadata:
  name: &quot;demo-policy.example.com&quot;
spec:
  paramKind:
    apiVersion: rules.example.com/v1 # 这个资源是通过CRD自定义的集群资源
    kind: ReplicaLimit
  matchConstraints:
    resourceRules:
    - apiGroups:   [&quot;apps&quot;]
      apiVersions: [&quot;v1&quot;]
      operations:  [&quot;CREATE&quot;, &quot;UPDATE&quot;]
      resources:   [&quot;deployments&quot;]
  validations:
    - expression: &quot;object.spec.replicas &lt;= params.maxReplicas&quot;
</code></pre>
<p>在这个示例中，使用了 <em>paramKind</em>，这个可以使得管理员可以通过 CRD 的形式扩展策略本身，而这个 CRD 资源可以定义为下面示例所提到的</p>
<pre><code class="language-yaml">apiVersion: rules.example.com/v1 # 使用 CRD 方式定义策略本身参数
kind: ReplicaLimit
metadata:
  name: &quot;demo-params-production.example.com&quot;
maxReplicas: 1000
</code></pre>
<p>最终使用了 <em>ValidatingAdmissionPolicyBinding</em> 资源将 “策略” , “规则” , “限制参数” 进行了解耦合，更灵活性的引用了 <em>ValidatingAdmissionPolicy</em></p>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1alpha1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: &quot;demo-binding-production.example.com&quot;
spec:
  policyName: &quot;demo-policy.example.com&quot;
  paramRef:
    name: &quot;demo-params-production.example.com&quot;
  matchResources:
    namespaceSelector:
      matchExpressions:
      - key: environment
        operator: In
        values:
        - production
</code></pre>
<h2 id="如何启用-validatingadmissionpolicy">如何启用 ValidatingAdmissionPolicy</h2>
<ul>
<li>确保 <strong>ValidatingAdmissionPolicy</strong>  启用特性门控 (<em>feature gates</em>)。</li>
<li>确保 <strong>admissionregistration.k8s.io/v1beta1</strong> API 启用。</li>
</ul>
<pre><code class="language-bash">--feature-gates=ValidatingAdmissionPolicy=true
</code></pre>
<h2 id="总结">总结</h2>
<p>ValidatingAdmissionPolicy 作为 kube-apiserver 内置的功能，减少了 Kubernetes 使用者的维护成本，避免了 webhook 不可控因素影响整个集群，并带来个更便捷的管理方式，使得 Kubernetes 越来越像从工具传变成一个产品，大大加强了 Kubernetes 使用者灵活管控集群的方式。更高级的用法，以及 CEL 的使用可以参考附录官方文档</p>
<blockquote>
<p><a href="https://cylonchau.github.io/ch3.7-admission-webhook.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Admission Control源码解析</a></p>
</blockquote>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/validating-admission-policy/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>验证准入策略（ValidatingAdmissionPolicy）</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://www.armosec.io/glossary/kubernetes-validation-admission-policies/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Kubernetes validation admission policies</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/blog/2022/12/20/validating-admission-policies-alpha/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Kubernetes 1.26: Introducing Validating Admission Policies</strong></em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes集群中的IP伪装 - ip-masq-agent</title>
      <link>https://www.oomkill.com/2023/10/ch24-ip-masq/</link>
      <pubDate>Sat, 28 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/10/ch24-ip-masq/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>“IP 伪装” 通常应用于云环境中，例如 GKE, AWS, CCE 等云厂商都有使用 “IP伪装” 技术，本文将围绕 “IP伪装” 技术本身，以及这项技术在 Kubernetes 集群中的实现应用 <em>ip-masq-agent</em> 的源码分析，以及 ”IP伪装“ 能为 Kubernetes 带来什么作用，这三个方向阐述。</p>
<h2 id="什么是ip伪装">什么是IP伪装？</h2>
<p>IP 伪装 (<em>IP Masquerade</em>)  是 Linux 中的一个网络功能，一对多 (1 to Many) 的网络地址转换 (NAT) 的功能 。</p>
<p>IP 伪装允许一组计算机通过 “伪装” 网关无形地访问互联网。对于互联网上的其他计算机，出站流量将看起来来自于 IP MASQ 服务器本身。互联网上任何希望发回数据包（作为答复）的主机必须将该数据包发送到网关 （IP MASQ 服务器本身）。记住，网关（IP MASQ 服务器本身）是互联网上唯一可见的主机。网关重写目标地址，用被伪装的机器的 IP 地址替换自己的地址，并将该数据包转发到本地网络进行传递。</p>
<p>除了增加的功能之外，IP Masquerade 为创建一个高度安全的网络环境提供了基础。通过良好构建的防火墙，突破经过良好配置的伪装系统和内部局域网的安全性应该会相当困难。</p>
<p>IP Masquerade 从 Linux 1.3.x 开始支持，目前基本所有 Linux 发行版都带有 IP 伪装的功能</p>
<h3 id="什么情况下不需要ip伪装">什么情况下不需要IP伪装</h3>
<ul>
<li>已经连接到互联网的独立主机</li>
<li>为其他主机分配了多个公共地址</li>
</ul>
<h2 id="ip伪装在kubernetes集群中的应用">IP伪装在Kubernetes集群中的应用</h2>
<p>IP 伪装通常应用在大规模 Kubernetes 集群中，主要用于解决 “地址冲突” 的问题，例如在 GCP 中，通常是一种 IP 可路由的网络模型，例如分配给 Pod service 的 ClusterIP 只能在 Kubernetes 集群内部可用，而分配 IP CIDR 又是一种不可控的情况，假设，我们为 k8s 分配的 IP CIDR 段如下表所示：</p>
<table>
<thead>
<tr>
<th>角色</th>
<th>IP CIDR</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kubernetes Nodes</td>
<td>10.0.0.0/16</td>
</tr>
<tr>
<td>Kubernetes Services</td>
<td>10.1.0.0/16</td>
</tr>
<tr>
<td>Kubernetes Pods</td>
<td>192.168.0.0/24</td>
</tr>
<tr>
<td>其他不可控业务网段</td>
<td>192.168.0.0/24</td>
</tr>
</tbody>
</table>
<p>通过上表可以看出，通常管理员在管理  Kubernetes 集群会配置三个网段，此时的配置，如果 Pod 需要与其他节点网络进行通讯（如我需要连接数据库），那么可能会出现 ”IP 重叠“ 的现象，尤其是在公有云环境中，<font color="#f8070d" size=3>用户在配置 Kubernetes 集群网络时不知道数据中心所保留的 CIDR 是什么</font>，在这种情况下就很容易产生 ”IP 重叠“ 的现象，为了解决这个问题，Kubernetes 提出了一种使用 “IP伪装” 技术来解决这个问题。</p>
<blockquote>
<p>在不使用 IP Masquerade 的情况下， Kubernetes 集群管理员如果在规划集群 CIDR 时，必须要了解了解整个组织中已预留/未使用的 CIDR 规划。</p>
</blockquote>
<h2 id="ip-masquerade-agent">IP Masquerade Agent</h2>
<p>IP伪装在 kubernetes 中的应用是名为 <em>ip-masq-agent</em> 的项目， <em>ip-masq-agent</em> 是用于配置 iptables 规则，以便在将流量发送到集群节点的 IP 和集群 IP 范围之外的目标时处理伪装节点或 Pod 的 IP 地址。这本质上隐藏了集群节点 IP 地址后面的 Pod IP 地址。在某些环境中，去往&quot;外部&quot;地址的流量必须从已知的机器地址发出。 例如，在 GCP 中，任何到互联网的流量都必须来自 VM 的 IP。 使用容器时，如 GKE，从 Pod IP 发出的流量将被拒绝出站。 为了避免这种情况，我们必须将 Pod IP 隐藏在 VM 自己的 IP 地址后面 - 通常称为&quot;伪装&quot;。 默认情况下，代理配置为将 RFC 1918指定的三个私有 IP 范围视为非伪装 CIDR。 这些范围是 <code>10.0.0.0/8</code>、<code>172.16.0.0/12</code> 和 <code>192.168.0.0/16</code>。 默认情况下，代理还将链路本地地址（169.254.0.0/16）视为非伪装 CIDR。 代理程序配置为每隔 60 秒从 <strong>/etc/config/ip-masq-agent</strong> 重新加载其配置， 这也是可修改的。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/ip-masq.png" alt="masq/non-masq example" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：ip-masq-agent工作原理</center>
<center><em>Source：</em>https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/ip-masq-agent/</center><br>
<p>默认情况下，CIDR <em>10.0.0.0/8</em>，<em>172.16.0.0/12</em>, <em>192.168.0.0/16</em> 范围内的流量不会被伪装。 任何其他 CIDR 流量将被伪装。 Pod 访问本地目的地的例子，可以是其节点 (Node) 的 IP 地址，另一节点 (Node) 的地址或集群的 IP 地址 (ClusterIP) 范围内的一个 IP 地址。 默认情况下，任何其他流量都将伪装。以下条目展示了 ip-masq-agent 的默认使用的规则：</p>
<pre><code class="language-bash">$ iptables -t nat -L IP-MASQ-AGENT
target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             172.16.0.0/12        /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             192.168.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre>
<h2 id="部署-ip-masq-agent">部署 ip-masq-agent</h2>
<p>ip-masq-agent 的部署可以直接使用官方提供的资源清单 <sup><a href="#1">[1]</a></sup></p>
<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml
</code></pre>
<p>清除 ip-masq-agent</p>
<pre><code class="language-yaml">kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml
</code></pre>
<p>部署后需要同时将对应的节点标签应用于集群中希望代理运行的任何节点</p>
<pre><code class="language-bash">kubectl label nodes my-node node.kubernetes.io/masq-agent-ds-ready=true
</code></pre>
<p>配置好之后，需要创建配置，以对不伪装的地址增加白名单</p>
<pre><code class="language-bash">nonMasqueradeCIDRs:
  - 10.0.0.0/8
resyncInterval: 60s
</code></pre>
<h2 id="ip-masq-agent-深入解析">ip-masq-agent 深入解析</h2>
<p>ip-masq-agent 的代码很少，只有400多行，但是作用却很大，直接可以解决管理员集群网络规划与大拓扑网络的网络冲突问题，下面就分析他的原理，以及如何完成集群 IP 伪装功能</p>
<h3 id="ip-masq-agent源码的分析">ip-masq-agent源码的分析</h3>
<p>ip-masq-agent 只有这一个文件 cmd/ip-masq-agent/ip-masq-agent.go，包含了整个的业务逻辑</p>
<p>首先在 <a href="https://github.com/kubernetes-sigs/ip-masq-agent/blob/master/cmd/ip-masq-agent/ip-masq-agent.go#L139" target="_blank"
   rel="noopener nofollow noreferrer" >main()</a> 启动时，定义了这个链的名称，之后调用 Run()</p>
<pre><code class="language-go">masqChain = utiliptables.Chain(*masqChainFlag)

..

m.Run()
</code></pre>
<p>在 <a href="https://github.com/kubernetes-sigs/ip-masq-agent/blob/1a093b8aad7c372c535c1fb104377728c1e54a7a/cmd/ip-masq-agent/ip-masq-agent.go#L153-L175" target="_blank"
   rel="noopener nofollow noreferrer" >Run()</a> 中，只是做了周期性同步</p>
<pre><code class="language-go">func (m *MasqDaemon) Run() {
	// Periodically resync to reconfigure or heal from any rule decay
	for {
		func() {
			defer time.Sleep(time.Duration(m.config.ResyncInterval))
			// resync config
			if err := m.osSyncConfig(); err != nil {
				glog.Errorf(&quot;error syncing configuration: %v&quot;, err)
				return
			}
			// resync rules
			if err := m.syncMasqRules(); err != nil {
				glog.Errorf(&quot;error syncing masquerade rules: %v&quot;, err)
				return
			}
			// resync ipv6 rules
			if err := m.syncMasqRulesIPv6(); err != nil {
				glog.Errorf(&quot;error syncing masquerade rules for ipv6: %v&quot;, err)
				return
			}
		}()
	}
}
</code></pre>
<p>重点就在 m.osSyncConfig() , 这里做的是同步实际的规则</p>
<pre><code class="language-go">func (m *MasqDaemon) syncMasqRules() error {
    // 指定的链是否存在，如果不存在则创建，masqChain全局变量 是 main() 中初始化的名称，默认为IP-MASQ-AGENT
	m.iptables.EnsureChain(utiliptables.TableNAT, masqChain)

	// ensure that any non-local in POSTROUTING jumps to masqChain
	if err := m.ensurePostroutingJump(); err != nil {
		return err
	}

	// build up lines to pass to iptables-restore
	lines := bytes.NewBuffer(nil)
	writeLine(lines, &quot;*nat&quot;)
	writeLine(lines, utiliptables.MakeChainLine(masqChain)) // effectively flushes masqChain atomically with rule restore

	// local-link cidr 不伪装（&quot;169.254.0.0/16&quot;） 固定值
	if !m.config.MasqLinkLocal {
		writeNonMasqRule(lines, linkLocalCIDR)
	}

	// 用户定义的不伪装的 CIDR 部分
	for _, cidr := range m.config.NonMasqueradeCIDRs {
		if !isIPv6CIDR(cidr) {
			writeNonMasqRule(lines, cidr)
		}
	}

	// masquerade all other traffic that is not bound for a --dst-type LOCAL destination
	writeMasqRule(lines)

	writeLine(lines, &quot;COMMIT&quot;)

	if err := m.iptables.RestoreAll(lines.Bytes(), utiliptables.NoFlushTables, utiliptables.NoRestoreCounters); err != nil {
		return err
	}
	return nil
}

</code></pre>
<p>看完同步规则后，了解到上面就是两个操作，”伪装“ 和 “不伪装” 的操作如下所示</p>
<p>不伪装部分实际上就是关键词 RETURN</p>
<pre><code class="language-go">func writeNonMasqRule(lines *bytes.Buffer, cidr string) {
	writeRule(lines, utiliptables.Append, masqChain, nonMasqRuleComment, &quot;-d&quot;, cidr, &quot;-j&quot;, &quot;RETURN&quot;)
}
</code></pre>
<p>伪装部分实际上就是关键词 MASQUERADE</p>
<pre><code class="language-go">func writeMasqRule(lines *bytes.Buffer) {
	writeRule(lines, utiliptables.Append, masqChain, masqRuleComment, &quot;-j&quot;, &quot;MASQUERADE&quot;, &quot;--random-fully&quot;)
}
</code></pre>
<h3 id="伪装网络包的分析">伪装网络包的分析</h3>
<p>创建一个 ip-masq-agent 的配置文件</p>
<pre><code class="language-bash">tee &gt; config &lt;&lt;EOF
nonMasqueradeCIDRs:
  - 10.244.0.0/16
  - 192.0.0.0/8
resyncInterval: 60s
EOF
</code></pre>
<p>创建 configmap</p>
<pre><code class="language-yaml">kubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system
</code></pre>
<p>验证规则是否生效</p>
<pre><code class="language-bash">$ iptables -t nat -L IP-MASQ-AGENT
Chain IP-MASQ-AGENT (1 references)
target     prot opt source               destination         
RETURN     all  --  anywhere             link-local/16        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
RETURN     all  --  anywhere             10.244.0.0/16        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
RETURN     all  --  anywhere             192.0.0.0/8          /* ip-masq-agent: local traffic is not subject to MASQUERADE */
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic is subject to MASQUERADE (must be last in chain) */
</code></pre>
<p>抓包查看包是否被伪装</p>
<pre><code class="language-bash">$ cpid=`docker inspect --format '{{.State.Pid}}' 6b0a92ca4327`
$ nsenter -t $cpid -n ifconfig eth0|grep inet
        inet 10.244.196.132  netmask 255.255.255.255  broadcast 10.244.196.132
$ nsenter -t $cpid -n ping 10.0.0.2
$ tcpdump -i any icmp and host 10.0.0.2 -w icap.cap
</code></pre>
<p>通过导出的 wireshark 包，可以很清楚的看到，去往 10.0.0.2 的已经被伪装了</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20231105222224315.png" alt="image-20231105222224315" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes集群节点IP伪装抓包</center>
<h2 id="reference"><strong>Reference</strong></h2>
<p><sup id="1">[1]</sup> <a href="https://github.com/kubernetes-sigs/ip-masq-agent/blob/master/ip-masq-agent.yaml" target="_blank"
   rel="noopener nofollow noreferrer" >ip-masq-agent.yaml</a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/ip-masq-agent/" target="_blank"
   rel="noopener nofollow noreferrer" >IP Masquerade Agent 用户指南</a></p>
<p><sup id="3">[3]</sup> <a href="https://medium.com/google-cloud/ip-address-management-strategy-a-crucial-aspect-of-running-gke-f063fe90cfbd" target="_blank"
   rel="noopener nofollow noreferrer" >IP address management strategy — a crucial aspect of running GKE</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>探索kubectl - 巧用jsonpath提取有用数据</title>
      <link>https://www.oomkill.com/2023/09/kubectl-jsonpath/</link>
      <pubDate>Mon, 25 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/09/kubectl-jsonpath/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>kubernetes集群工具 kubect 提供了一种强大的数据提取的模式，<strong>jsonpath</strong>，相对于 yaml 来说，jsonpath 拥有高度的自制提取功能，以及一些更便于提取字段的模式，使得过去 kubernetes 资源信息时更便捷，在本文中将解开 <strong>jsonpath</strong> 的神秘面纱。</p>
<h2 id="什么是jsonpath">什么是jsonpath</h2>
<p>JSONPath 是一种用于查询 JSON 数据结构中特定元素的查询语言。它类似于 XPath 用于 XML 数据的查询。JSONPath 允许您以一种简单而灵活的方式从 JSON 对象中提取数据，而不需要编写复杂的代码来解析 JSON 结构。</p>
<p>JSONPath 使用路径表达式来指定您要检索的 JSON 数据的位置。这些路径表达式类似于文件系统中的路径，但用于导航 JSON 结构。以下是一些常见的 JSONPath 表达式示例：</p>
<ol>
<li><code>$</code>：表示 JSON 根对象。</li>
<li><code>$.store</code>：表示从根对象中获取名为 &ldquo;store&rdquo; 的属性。</li>
<li><code>$.store.book</code>：表示从根对象中获取 &ldquo;store&rdquo; 属性中的 &ldquo;book&rdquo; 属性。</li>
<li><code>$.store.book[0]</code>：表示获取 &ldquo;store&rdquo; 属性中的 &ldquo;book&rdquo; 属性的第一个元素。</li>
<li><code>$.store.book[?(@.price &lt; 10)]</code>：表示选择 &ldquo;store&rdquo; 属性中的 &ldquo;book&rdquo; 属性中价格小于 10 的所有元素。</li>
</ol>
<table>
<thead>
<tr>
<th>Function</th>
<th>Description</th>
<th>Example</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>text</td>
<td>the plain text</td>
<td>kind is {.kind}</td>
<td>kind is List</td>
</tr>
<tr>
<td>@</td>
<td>the current object</td>
<td>{@}</td>
<td>the same as input</td>
</tr>
<tr>
<td>. or []</td>
<td>child operator</td>
<td>{.kind} or {[‘kind’]}</td>
<td>List</td>
</tr>
<tr>
<td>..</td>
<td>recursive descent</td>
<td>{..name}</td>
<td>127.0.0.1 127.0.0.2 myself e2e</td>
</tr>
<tr>
<td>*</td>
<td>wildcard. Get all objects</td>
<td>{.items[*].metadata.name}</td>
<td>[127.0.0.1 127.0.0.2]</td>
</tr>
<tr>
<td>[start:end :step]</td>
<td>subscript operator</td>
<td>{.users[0].name}</td>
<td>myself</td>
</tr>
<tr>
<td>[,]</td>
<td>union operator</td>
<td>{.items[*][‘metadata.name’, ‘status.capacity’]}</td>
<td>127.0.0.1 127.0.0.2 map[cpu:4] map[cpu:8]</td>
</tr>
<tr>
<td>?()</td>
<td>filter</td>
<td>{.users[?(@.name==“e2e”)].user.password}</td>
<td>secret</td>
</tr>
<tr>
<td>range, end</td>
<td>iterate list</td>
<td>{range .items[*]}[{.metadata.name}, {.status.capacity}] {end}</td>
<td>[127.0.0.1, map[cpu:4]] [127.0.0.2, map[cpu:8]]</td>
</tr>
<tr>
<td>“</td>
<td>quote interpreted string</td>
<td>{range .items[*]}{.metadata.name}{’\t’}{end}</td>
<td>127.0.0.1    127.0.0.2</td>
</tr>
</tbody>
</table>
<p>JSONPath 支持各种操作符和函数，以便更复杂地筛选和操作 JSON 数据。它在 JSON 数据的导航和过滤方面非常强大，通常用于从 JSON 数据中提取所需的信息。</p>
<p>JSONPath 在各种编程语言和工具中都有实现，包括 JavaScript、Python、Java 等，因此您可以根据需要选择适合您项目的工具来使用 JSONPath 查询 JSON 数据。</p>
<h3 id="kubectl中对jsonpath的支持">kubectl中对jsonpath的支持</h3>
<p>例如，通常在生产环境中处理 Kubernetes 问题时，您将需要查看数百个节点和数千个 Pod 的信息，例如 Deployment, Pod, Replicat, Service, Secret 等资源信息，但要获取这些类型的资源，通常会使用 kubectl 命令，然而在50%以上的高级场景下，是过滤信息并进行整理。在这种场景下，使用 kubectl + shell 命令进行整理的却没有 jsonpath 来的实在。假设在一个大规模集群中，例如 10 万个 Node 节点，这时如果你想获得一些节点信息，或者 Pod 信息，再或者某些需要循环的条件，这时候多次的请求对你在统计数据上造成的时间成本及频繁请求API都会造成压力，这个时候 <em>jsonpath</em> 的功能就很好的解决了这个问题，通过一次请求，快速循环可以在很短时间内得出结果，并减少了大量请求 <em>kube-apiserver</em> 的压力。</p>
<h3 id="kubectl-jsonpath-示例">kubectl jsonpath 示例</h3>
<h3 id="仅获取某个资源的名称">仅获取某个资源的名称</h3>
<pre><code class="language-bash"># 语法
kubectl -n &lt;my_namespace&gt; get deploy/&lt;my_deployment&gt; -o jsonpath='{.metadata.name}'
</code></pre>
<p>获取一个 deployment的 信息</p>
<pre><code class="language-bash">$ kubectl get deploy/traefik -o jsonpath='{.metadata.name}'
traefik
</code></pre>
<p>通常使用 json path 不会获取一个资源的信息，而是获取所有资源的信息，例如获取所有 Pod 的 name</p>
<pre><code class="language-bash">$ kubectl get pod -A -o jsonpath=&quot;{.items[*]['metadata.name']}&quot;
echo-hello-world-task-run-1-pod-ksdtm echo-hello-world-task-run-pod-4sx9n traefik-679bf6459c-sz9jv calico-kube-controllers-577f77cb5c-kwhph calico-node-59d5x calico-node-82zgm coredns-6b9bb479b9-wnc8n minio spin-clouddriver-88df48858-dfzkg spin-deck-5dc8f847b8-m4tbk spin-echo-69868fd866-nxn8g spin-front50-54fb4b6d67-jfkds spin-gate-7b6f4d4566-gdjkd spin-orca-7765fb5c96-9gmvr spin-redis-8485df6b88-bgrgm spin-rosco-6d77f8cb-bf2nj tekton-pipelines-controller-5cdb46974f-8rjbx tekton-pipelines-webhook-6479d769ff-756gq
</code></pre>
<blockquote>
<p>Tips：jsonpath 的输出是以字符串方式输出，不会携带换行之类的</p>
</blockquote>
<h3 id="-的用法">@ 的用法</h3>
<p><em>@</em> 表示当前对象，例如 <em>kubectl get pods</em> 这获取的是一个 list 列表，那么 <em>@</em> 就代表这个 list，例如</p>
<pre><code class="language-bash">$ kubectl get pods -o=jsonpath='{@}'|jq
{
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;items&quot;: [
    {
      &quot;apiVersion&quot;: &quot;v1&quot;,
      &quot;kind&quot;: &quot;Pod&quot;,
      &quot;metadata&quot;: {
        &quot;annotations&quot;: {
          &quot;cni.projectcalico.org/containerID&quot;: &quot;aedb0d3f11b2572d82a7ccb456cec393f88de2c8befa4d19e69a577bb8c0e20f&quot;,
          &quot;cni.projectcalico.org/podIP&quot;: &quot;&quot;,
          &quot;cni.projectcalico.org/podIPs&quot;: &quot;&quot;,
          &quot;kubectl.kubernetes.io/last-applied-configuration&quot;: &quot;{\&quot;apiVersion\&quot;:\&quot;tekton.dev/v1beta1\&quot;,\&quot;kind\&quot;:\&quot;Task\&quot;,\&quot;metadata\&quot;:{\&quot;annotations\&quot;:{},\&quot;name\&quot;:\&quot;echo-hello-world\&quot;,\&quot;namespace\&quot;:\&quot;default\&quot;},\&quot;spec\&quot;:{\&quot;steps\&quot;:[{\&quot;args\&quot;:[\&quot;Hello World\&quot;],\&quot;command\&quot;:[\&quot;echo\&quot;],\&quot;image\&quot;:\&quot;busybox\&quot;,\&quot;name\&quot;:\&quot;echo\&quot;}]}}\n&quot;,
          &quot;pipeline.tekton.dev/release&quot;: &quot;v0.19.0&quot;,
          &quot;tekton.dev/ready&quot;: &quot;READY&quot;
        },
        &quot;creationTimestamp&quot;: &quot;2023-06-26T15:05:54Z&quot;,
        &quot;labels&quot;: {
          &quot;app.kubernetes.io/managed-by&quot;: &quot;tekton-pipelines&quot;,
          &quot;tekton.dev/task&quot;: &quot;echo-hello-world&quot;,
          &quot;tekton.dev/taskRun&quot;: &quot;echo-hello-world-task-run-1&quot;
        },
        &quot;managedFields&quot;: [
          {
            &quot;apiVersion&quot;: &quot;v1&quot;,
            &quot;fieldsType&quot;: &quot;FieldsV1&quot;,
            &quot;fieldsV1&quot;: {
              &quot;f:metadata&quot;: {
                &quot;f:annotations&quot;: {
                  &quot;f:cni.projectcalico.org/containerID&quot;: {},
                  &quot;f:cni.projectcalico.org/podIP&quot;: {},
                  &quot;f:cni.projectcalico.org/podIPs&quot;: {}
                }
              }
            },
            &quot;manager&quot;: &quot;calico&quot;,
            &quot;operation&quot;: &quot;Update&quot;,
            &quot;time&quot;: &quot;2023-06-26T15:05:55Z&quot;
          },
          {
            &quot;apiVersion&quot;: &quot;v1&quot;,
            &quot;fieldsType&quot;: &quot;FieldsV1&quot;,
            &quot;fieldsV1&quot;: {
              &quot;f:metadata&quot;: {
                &quot;f:annotations&quot;: {
                  &quot;.&quot;: {},
                  &quot;f:kubectl.kubernetes.io/last-applied-configuration&quot;: {},
                  &quot;f:pipeline.tekton.dev/release&quot;: {},
                  &quot;f:tekton.dev/ready&quot;: {}
 ...
        &quot;nodeName&quot;: &quot;node01&quot;,
        &quot;preemptionPolicy&quot;: &quot;PreemptLowerPriority&quot;,
        &quot;priority&quot;: 0,
        &quot;restartPolicy&quot;: &quot;Never&quot;,
        &quot;schedulerName&quot;: &quot;default-scheduler&quot;,
        &quot;securityContext&quot;: {},
        &quot;serviceAccount&quot;: &quot;default&quot;,
        &quot;serviceAccountName&quot;: &quot;default&quot;,
        &quot;terminationGracePeriodSeconds&quot;: 30,
        &quot;tolerations&quot;: [
          {
            &quot;effect&quot;: &quot;NoExecute&quot;,
            &quot;key&quot;: &quot;node.kubernetes.io/not-ready&quot;,
            &quot;operator&quot;: &quot;Exists&quot;,
            &quot;tolerationSeconds&quot;: 300
          },
          {
            &quot;effect&quot;: &quot;NoExecute&quot;,
            &quot;key&quot;: &quot;node.kubernetes.io/unreachable&quot;,
            &quot;operator&quot;: &quot;Exists&quot;,
            &quot;tolerationSeconds&quot;: 300
          }

</code></pre>
<h3 id="-和--的用法">. 和 [] 的用法</h3>
<p><em>.</em> 和 <em>[]</em> 是子操作符，用于获取到列表的元素，返回值也是 list，例如获取 Pod 列表中第一个 Pod，下面是一个 [] 的使用示例，可以获取某个元素</p>
<pre><code class="language-bash">kubectl get pods -o=jsonpath='{.items[0]}' | jq
{
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;kind&quot;: &quot;Pod&quot;,
  &quot;metadata&quot;: {
    &quot;annotations&quot;: {
      &quot;cni.projectcalico.org/containerID&quot;: &quot;aedb0d3f11b2572d82a7ccb456cec393f88de2c8befa4d19e69a577bb8c0e20f&quot;,

...

  },
  &quot;status&quot;: {
    &quot;conditions&quot;: [
      {
        &quot;lastProbeTime&quot;: null,
     
     ...
     
    &quot;hostIP&quot;: &quot;10.0.0.5&quot;,
    &quot;initContainerStatuses&quot;: [
      {
        &quot;containerID&quot;: &quot;docker://8129d155a9e9f204a22947ae3268513a1bf4d1ce98012120ab30cfd0eca04564&quot;,
        &quot;image&quot;: &quot;sha256:5d54c55f19bc6fdda7629a4f2015255ec1bed2750a81909817bede45a4d360b5&quot;,
        &quot;imageID&quot;: &quot;docker-pullable://gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/entrypoint@sha256:67fceb87f3f76baefcfdb35fd04d0ebfc8d91117dccb7f3194056d6727bac636&quot;,
        &quot;lastState&quot;: {},
        &quot;name&quot;: &quot;place-tools&quot;,
        &quot;ready&quot;: true,
        &quot;restartCount&quot;: 0,
        &quot;state&quot;: {
          &quot;terminated&quot;: {
            &quot;containerID&quot;: &quot;docker://8129d155a9e9f204a22947ae3268513a1bf4d1ce98012120ab30cfd0eca04564&quot;,
            &quot;exitCode&quot;: 0,
            &quot;finishedAt&quot;: &quot;2023-06-26T15:05:55Z&quot;,
            &quot;reason&quot;: &quot;Completed&quot;,
            &quot;startedAt&quot;: &quot;2023-06-26T15:05:55Z&quot;
          }
        }
      }
    ],
    &quot;phase&quot;: &quot;Succeeded&quot;,
    &quot;podIP&quot;: &quot;10.244.196.131&quot;,
    &quot;podIPs&quot;: [
      {
        &quot;ip&quot;: &quot;10.244.196.131&quot;
      }
    ],
    &quot;qosClass&quot;: &quot;BestEffort&quot;,
    &quot;startTime&quot;: &quot;2023-06-26T15:05:54Z&quot;
  }
}
</code></pre>
<p>. 是 [] 的子操作符，可以获取某一个元素下的某个值，与 json 中语法相同，例如获取 Pod 列表中==第一个 Pod 的名称==</p>
<pre><code class="language-bash">$ kubectl get pods -o=jsonpath='{.items[0].metadata.name}'
echo-hello-world-task-run-1-pod-ksdtm
</code></pre>
<h3 id="下标操作符-">下标操作符 :</h3>
<p>下标操作符 “:” 可以视为一个切片，获取列表中某一些元素，语法为 <code>[start:end :step]</code></p>
<p>例如获取前三个元素的名称</p>
<pre><code class="language-bash">kubectl get pods -o=jsonpath='{.items[0:2].metadata.name}'
</code></pre>
<p>获取最后两个元素</p>
<pre><code class="language-bash'">$ kubectl get pods -n spinnaker -o jsonpath='{range .items[*]}{.metadata.name}{&quot;\n&quot;}{end}'
spin-clouddriver-88df48858-dfzkg
spin-deck-5dc8f847b8-m4tbk
spin-echo-69868fd866-nxn8g
spin-front50-54fb4b6d67-jfkds
spin-gate-7b6f4d4566-gdjkd
spin-orca-7765fb5c96-9gmvr
spin-redis-8485df6b88-bgrgm
spin-rosco-6d77f8cb-bf2nj

$ kubectl get pods -n spinnaker -o jsonpath='{range .items[-2:]}{.metadata.name}{&quot;\n&quot;}{end}'
spin-redis-8485df6b88-bgrgm
spin-rosco-6d77f8cb-bf2nj
</code></pre>
<h3 id="过滤表达式">过滤表达式</h3>
<p>如果设置了 <em>limit</em> 参数则打印其名称</p>
<pre><code class="language-bash">kubectl get pods -n spinnaker -o jsonpath='{.items[?(@.spec.containers[*].resources.limits.memory != &quot;&quot;)].metadata.name}'
</code></pre>
<p>价格大于 10 的 元素的 name</p>
<pre><code class="language-bash">&quot;$.store.book[?(@.price &gt; 10)].name&quot;
</code></pre>
<p>json path 支持下列过滤操作符参考  <sup><a href="#2">[2]</a></sup></p>
<table>
<thead>
<tr>
<th>Operator</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>==</td>
<td>等于。字符串值必须用单引号（而不是双引号）括起来：<code>[?(@.color=='red')]</code>。注意：数字与字符串比较的工作方式因播放引擎而异。在 TestEngine 中，1 不等于 “1”。在 ReadyAPI 1.9 及更早版本中，1 等于 “1”。</td>
</tr>
<tr>
<td>!=</td>
<td>不等于，字符串值必须用单引号括起来：<code>[?(@.color!='red')]</code>。</td>
</tr>
<tr>
<td>&gt;</td>
<td>大于</td>
</tr>
<tr>
<td>&gt;=</td>
<td>大于或等于</td>
</tr>
<tr>
<td>&lt;</td>
<td>小于</td>
</tr>
<tr>
<td>&lt;=</td>
<td>小于或等于</td>
</tr>
<tr>
<td>=~</td>
<td>匹配 JavaScript 正则表达式。例如，<code>[?(@.description =~ /cat.*/i)]</code>  匹配描述以 cat 开头的项目（不区分大小写）。注意：如果使用 ReadyAPI 1.1 作为播放引擎，则不支持。</td>
</tr>
<tr>
<td>!</td>
<td>用于否定过滤器：<code>[?(!@.isbn)]</code>  匹配不具有 isbn 属性的项目。注意：如果使用 ReadyAPI 1.1 作为播放引擎，则不支持。</td>
</tr>
<tr>
<td>&amp;&amp;</td>
<td>逻辑与 AND，用于组合多个过滤表达式:  <code>[?(@.category=='fiction' &amp;&amp; @.price &lt; 10)]</code></td>
</tr>
<tr>
<td>||</td>
<td>逻辑或 OR Logical OR, 用于组合多个过滤表达式： `[?(@.category==&lsquo;fiction&rsquo;</td>
</tr>
<tr>
<td>in</td>
<td>检查左侧值是否存在于右侧列表中。类似于 SQL IN 运算符。字符串比较区分大小写。 <code>[?(@.size in ['M', 'L'])]   			[?('S' in @.sizes)]</code> 注意：仅由 TestEngine 播放引擎支持。</td>
</tr>
<tr>
<td>nin</td>
<td>与 in 相反。检查左侧值是否不存在于右侧列表中。字符串比较区分大小写。 <code>[?(@.size nin ['M', 'L'])]   	[?('S' nin @.sizes)]</code> 注意：仅由 TestEngine 播放引擎支持。</td>
</tr>
<tr>
<td>contains</td>
<td>检查字符串是否包含指定的子字符串（区分大小写），或者数组是否包含指定的元素。<br><code>[?(@.name contains 'Alex')]</code>   								<code>[?(@.numbers contains 7)]</code>   								<code>[?('ABCDEF' contains @.character)]</code>  <br>注意：仅由 TestEngine 播放引擎支持。</td>
</tr>
<tr>
<td>size</td>
<td>检查数组或字符串是否具有指定的长度。 <code>[?(@.name size 4)]</code> 注意：仅由 TestEngine 播放引擎支持。</td>
</tr>
<tr>
<td>empty true</td>
<td>匹配空数组或字符串。 <code>[?(@.name empty true)]</code> 注意：仅由 TestEngine 播放引擎支持。</td>
</tr>
<tr>
<td>empty false</td>
<td>匹配非空数组或字符串。 <code>[?(@.name empty false)]</code>  注意：仅由 TestEngine 播放引擎支持。</td>
</tr>
</tbody>
</table>
<h3 id="正则表达式">正则表达式</h3>
<p>kubectl jsonpath 不支持正则表达式，如果需要使用正则表达式可以使用 <em><strong>jq</strong></em> 替换</p>
<p>例如获取 <em>spin</em> 开头的 Pod 是否配置了 <em>resources</em></p>
<pre><code class="language-bash">kubectl get pods -n spinnaker -o json | jq -r '.items[] | select(.metadata.name | test(&quot;spin-&quot;)).spec.resources'
</code></pre>
<p>example</p>
<pre><code class="language-bash">$ kubectl get pods -n spinnaker -o json | jq -r '.items[] | select(.metadata.name | test(&quot;spin-&quot;)).spec.resources'
null
null
null
null
null
null
null
null
</code></pre>
<h2 id="kubectl-example-示例">kubectl example 示例</h2>
<h3 id="获取-pod-为-web-的-pod-name">获取 Pod 为 web 的 Pod name</h3>
<pre><code class="language-bash">kubectl get pods -o  jsonpath  =  '{.items[?(@.metadata.labels.name==&quot;web&quot;)].metadata.name}' 
</code></pre>
<h3 id="过滤一个元素">过滤一个元素</h3>
<p>过滤 Node 地址模式</p>
<pre><code class="language-bash">kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==&quot;InternalIP&quot;)]}'
</code></pre>
<p>过滤元素并只打印想要的属性</p>
<pre><code class="language-bash">kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==&quot;InternalIP&quot;)].address}'
</code></pre>
<h3 id="从数组中获取每个元素的单个字段">从数组中获取每个元素的单个字段</h3>
<pre><code class="language-bash">kubectl get pods -o jsonpath={$.items[*].status.hostIP}
</code></pre>
<h3 id="换行">换行</h3>
<p>jsonpath 在获取元素后，是一个单行字符串，如果需要换行操作可以使用下面示例</p>
<pre><code class="language-bash">$ kubectl get pods -o jsonpath='{range .items[*]}{.status.hostIP}{&quot;\n&quot;}{end}'
10.0.0.5
10.0.0.5
10.0.0.5
</code></pre>
<h3 id="循环并获得多个元素">循环并获得多个元素</h3>
<p>如果想获取两个元素，可以使用下面示例</p>
<pre><code class="language-bash">$ kubectl get pods -o jsonpath={range .items[*]}{.status.hostIP}{&quot;\t&quot;}{.status.phase}{&quot;\n&quot;}{end}
10.154.196.228	Running
10.154.202.136	Running
10.154.201.54	Running
</code></pre>
<h3 id="递归">递归</h3>
<p>如果想获取一个元素下所有相同名称的字段，可以使用下面示例</p>
<p>递归获取所有 name 字段</p>
<pre><code class="language-bash">$ kubectl get pod -A -o jsonpath='{..name}'
step-echo place-tools echo-hello-world-task-run-1-pod-ksdtm echo-hello-world-task-run-1 tekton-internal-workspace tekton-internal-home tekton-internal-results tekton-internal-tools tekton-internal-downward tekton-creds-init-home-tlvg8 default-token-6762j step-echo HOME tekton-internal-tools tekton-internal-downward tekton-creds-init-home-tlvg8 tekton-internal-workspace tekton-i..
...
</code></pre>
<p>巧用递归简化语句，如果一个元素 (元素名称) 是独有的，那么可以使用递归直接获取到这个元素的值，例如，过去所有 containers</p>
<pre><code class="language-bash">$ kubectl get pods -A -o=jsonpath='{range .items[*]}{&quot;pod: &quot;}{.metadata.name} {&quot;\n&quot;}{range ..containers[*]}{&quot;\tcontainer: &quot;}{.name}{&quot;\n\timage: &quot;}{.image}{&quot;\n&quot;}{end}{end}'
pod: echo-hello-world-task-run-1-pod-ksdtm 
	container: step-echo
	image: busybox
pod: echo-hello-world-task-run-pod-4sx9n 
	container: step-echo
</code></pre>
<h3 id="获取每个-pod-limit-值">获取每个 Pod limit 值</h3>
<pre><code class="language-bash">kubectl  get pod -A -o jsonpath='{range $.items[*]}{&quot;Pod: &quot;}{.metadata.name}{&quot;\n&quot;} {&quot;  limit_mem: &quot;}{.spec.containers[*].resources.limits.memory}{&quot;\n&quot;}{end}'
Pod: echo-hello-world-task-run-1-pod-ksdtm
   limit_mem: 
Pod: echo-hello-world-task-run-pod-4sx9n
   limit_mem: 
Pod: traefik-679bf6459c-sz9jv
   limit_mem: 
Pod: calico-kube-controllers-577f77cb5c-kwhph
   limit_mem: 
Pod: calico-node-59d5x
   limit_mem: 
Pod: calico-node-82zgm
   limit_mem: 
Pod: coredns-6b9bb479b9-wnc8n
   limit_mem: 170Mi
Pod: minio
   limit_mem: 
Pod: spin-clouddriver-88df48858-dfzkg
   limit_mem: 
Pod: spin-deck-7fbf94d8bc-k7zrk
   limit_mem: 200Mi
Pod: spin-echo-69868fd866-nxn8g
   limit_mem: 
Pod: spin-front50-54fb4b6d67-jfkds
   limit_mem: 
Pod: spin-gate-6d7dbf74b9-4l89r
   limit_mem: 600Mi
Pod: spin-orca-7765fb5c96-9gmvr
   limit_mem: 
Pod: spin-redis-8485df6b88-bgrgm
   limit_mem: 
Pod: spin-rosco-6d77f8cb-bf2nj
   limit_mem: 
Pod: tekton-pipelines-controller-5cdb46974f-8rjbx
   limit_mem: 
Pod: tekton-pipelines-webhook-6479d769ff-756gq
   limit_mem: 500Mi
</code></pre>
<h3 id="获取容器的-ip">获取容器的 IP</h3>
<p>获取单独一个 Pod</p>
<pre><code class="language-bash">kubectl  get pod nginx-67d5fc57d8-jkfjp -n quota-example  -o jsonpath='{.status.podIPs[].ip}{&quot;\n&quot;}'
</code></pre>
<p>循环获取</p>
<pre><code class="language-bash">kubectl  get pod  -o jsonpath='{range $.items[*]}{.status.podIPs[].ip}{&quot;\n&quot;}{end}'
10.244.196.131
10.244.196.129
10.244.196.186
</code></pre>
<h3 id="获取所有的-container-id-和-pod-ip">获取所有的 Container ID 和 Pod IP</h3>
<pre><code class="language-bash">kubectl get pods --all-namespaces    -o=jsonpath='{range .items[*]}[{.status.containerStatuses[0].containerID}, {.status.podIP}]{&quot;\n&quot;}{end}'
</code></pre>
<h3 id="获取所有的容器名称和镜像名称">获取所有的容器名称和镜像名称</h3>
<pre><code class="language-bash">kubectl get pods -n kube-system -o=jsonpath='{range .items[*]}[{.metadata.name},{.status.containerStatuses[0].image}]{&quot;\n&quot;}{end}'
</code></pre>
<h3 id="获取所有状态条件中的类型">获取所有状态条件中的类型</h3>
<pre><code class="language-bash">kubectl get pod cm-test-pod -o jsonpath='{.status.conditions[*].type}'
</code></pre>
<h3 id="获取-pod-的-apiversion">获取 Pod 的 apiversion</h3>
<pre><code class="language-bash">kubectl get pod cm-test-pod -o jsonpath='{.apiVersion}'
</code></pre>
<h3 id="从第一个状态条件开始到最后一个结束每隔2个获取一次">从第一个状态条件开始到最后一个结束，每隔2个获取一次</h3>
<pre><code class="language-bash">kubectl get pod cm-test-pod -o jsonpath='{.status.conditions[0:3:2].type}'
</code></pre>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/reference/kubectl/jsonpath/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>jsonpath</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://support.smartbear.com/alertsite/docs/monitors/api/endpoint/jsonpath.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em>JSONPath Syntax</em></a></p>
<p><sup id="2">[3]</sup> <a href="https://www.ctyun.cn/zhishi/p-168130" target="_blank"
   rel="noopener nofollow noreferrer" ><em>k8s学习-kubectl命令行 jsonpath的使用</em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>kube-proxy参数ClusterCIDR做什么</title>
      <link>https://www.oomkill.com/2023/09/ch26-kube-proxy-clustercidr/</link>
      <pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/09/ch26-kube-proxy-clustercidr/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>我们可以看到，<em>kube-proxy</em> 有一个 <em>&ndash;cluster-cidr</em> 的参数，我们就来解开这个参数究竟有没有用</p>
<pre><code class="language-bash">$ kube-proxy -h|grep cidr
      --cluster-cidr string                          The CIDR range of pods in the cluster. When configured, traffic sent to a Service cluster IP from outside this range will be masqueraded and traffic sent from pods to an external LoadBalancer IP will be directed to the respective cluster IP instead
</code></pre>
<p>可以看到，参数说明是说，如果配置，那么从外部发往 Service Cluster IP 的流量将被伪装，从 Pod 发往外部 LB 将被直接发往对应的 cluster IP。但实际上做了什么并不知道，那么就从源码解决这个问题。</p>
<p>首先我们知道，参数是作为 kube-proxy server 的参数，位于 cmd/kube-proxy 下，而对应的逻辑则位于 pkg/kube-proxy 下，参数很明显，就是 clusterCIDR，那么我们就寻找这个参数的调用即可。</p>
<p>在 API <em>KubeProxyConfiguration</em> 中我们找到的对应的 <em>ClusterCIDR</em> ，在这里的注释又变为 ”用于桥接集群外部流量“。这里涉及到关于 <em>kube-proxy</em> 的两个模式 “LocalMode” 和 “ProxyMode“。</p>
<ul>
<li><em><strong>LocalMode</strong></em>：表示是来自节点本地流量的模式，包含 ClusterCIDR, NodeCIDR</li>
<li><em><strong>ProxyMode</strong></em>：就是 kube-proxy 最常用的模式，包含 iptables, IPVS, user namespace, kernelspace</li>
</ul>
<p>而参数 <em>&ndash;cluster-cidr</em> 是作为选择使用的 “本地网络检测器” (Local Network Detector)，这里起到的作用就是 “将集群外部的流量伪装成 service VIP” ，从代码中我们可以看到 Detector 将决定了你使用的是什么网络，无论是 <em>LocalMode</em> 还是 <em>ProxyMode</em>。</p>
<p>在代码 <a href="cmd/kube-proxy/app/server_others.go">cmd/kube-proxy/app/server_others.go</a> 中可以看到是如何选择的 <em>LocalMode</em> 方式，可以看出在存在三种模式：</p>
<ul>
<li>没有配置 <em>&ndash;cluster-cidr</em> 则会返回一个 <em>NoOpLocalDetector</em>；</li>
<li>在配置了 <em>&ndash;cluster-cidr</em> ，则将会使用 CIDR 的本地模式；</li>
<li>如果  <em>&ndash;cluster-cidr</em> 没有配置，但配置了 LocalModeNodeCIDR，则会设置为 CNI 为该 Node 配置的 POD CIDR 的地址 (使用参数 <em>&ndash;proxy-mode</em> 指定的模式，如果为空，那么会检测对应操作系统默认 Linux 为 iptables，如果内核开启 IPVS 那么则使用 IPVS，windows 默认为 kernelspace)</li>
</ul>
<pre><code class="language-go">func getLocalDetector(mode proxyconfigapi.LocalMode, config *proxyconfigapi.KubeProxyConfiguration, ipt utiliptables.Interface, nodeInfo *v1.Node) (proxyutiliptables.LocalTrafficDetector, error) {
	switch mode {
	case proxyconfigapi.LocalModeClusterCIDR:
		if len(strings.TrimSpace(config.ClusterCIDR)) == 0 {
			klog.Warning(&quot;detect-local-mode set to ClusterCIDR, but no cluster CIDR defined&quot;)
			break
		}
		return proxyutiliptables.NewDetectLocalByCIDR(config.ClusterCIDR, ipt)
	case proxyconfigapi.LocalModeNodeCIDR:
		if len(strings.TrimSpace(nodeInfo.Spec.PodCIDR)) == 0 {
			klog.Warning(&quot;detect-local-mode set to NodeCIDR, but no PodCIDR defined at node&quot;)
			break
		}
		return proxyutiliptables.NewDetectLocalByCIDR(nodeInfo.Spec.PodCIDR, ipt)
	}
	klog.V(0).Info(&quot;detect-local-mode: &quot;, string(mode), &quot; , defaulting to no-op detect-local&quot;)
	return proxyutiliptables.NewNoOpLocalDetector(), nil
}
</code></pre>
<p>这里我们以 IPVS 为例，如果开启了 localDetector 在 这个 <em>ipvs proxier</em> 中做了什么? 在代码 <a href="pkg/proxy/ipvs/proxier.go">pkg/proxy/ipvs/proxier.go</a> 可以看到</p>
<pre><code class="language-go">	if !proxier.ipsetList[kubeClusterIPSet].isEmpty() {
		args = append(args[:0],
			&quot;-A&quot;, string(kubeServicesChain),
			&quot;-m&quot;, &quot;comment&quot;, &quot;--comment&quot;, proxier.ipsetList[kubeClusterIPSet].getComment(),
			&quot;-m&quot;, &quot;set&quot;, &quot;--match-set&quot;, proxier.ipsetList[kubeClusterIPSet].Name,
		)
		if proxier.masqueradeAll {
			writeLine(proxier.natRules, append(args, &quot;dst,dst&quot;, &quot;-j&quot;, string(KubeMarkMasqChain))...)
		} else if proxier.localDetector.IsImplemented() {
			// This masquerades off-cluster traffic to a service VIP.  The idea
			// is that you can establish a static route for your Service range,
			// routing to any node, and that node will bridge into the Service
			// for you.  Since that might bounce off-node, we masquerade here.
			// If/when we support &quot;Local&quot; policy for VIPs, we should update this.
			writeLine(proxier.natRules, proxier.localDetector.JumpIfNotLocal(append(args, &quot;dst,dst&quot;), string(KubeMarkMasqChain))...)
		} else {
			// Masquerade all OUTPUT traffic coming from a service ip.
			// The kube dummy interface has all service VIPs assigned which
			// results in the service VIP being picked as the source IP to reach
			// a VIP. This leads to a connection from VIP:&lt;random port&gt; to
			// VIP:&lt;service port&gt;.
			// Always masquerading OUTPUT (node-originating) traffic with a VIP
			// source ip and service port destination fixes the outgoing connections.
			writeLine(proxier.natRules, append(args, &quot;src,dst&quot;, &quot;-j&quot;, string(KubeMarkMasqChain))...)
		}
	}
</code></pre>
<p>可以看到“不管使用了什么模式，都会更新一条 iptables 规则” 这就代表了使用了什么模式，而这个则被称之为 <em>LocalTrafficDetector</em>，也就是本地流量的检测，那我们看一下这个做了什么。</p>
<p>在使用 IPVS 的日志中，可以看到这样一条规则，这个是来自集群外部的 IP 去访问集群 CLUSTER IP (<em>KUBE-CLUSTER-IP</em>，即集群内所有 service IP) 时, 将非集群 IP 地址，转换为集群内的 IP 地址 (做源地址转换)</p>
<pre><code class="language-bash">[DetectLocalByCIDR (10.244.0.0/16)] Jump Not Local: [-A KUBE-SERVICES -m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot; -m set --match-set KUBE-CLUSTER-IP dst,dst ! -s 10.244.0.0/16 -j KUBE-MARK-MASQ]
</code></pre>
<p>而这个步骤分布在所有模式下 (iptables&amp;ipvs)，这里还是没说到两个概念 <em><strong>LocalMode</strong></em> 和 <em><strong>ProxyMode</strong></em>，实际上这两个模式的区别为：</p>
<ul>
<li><strong>LocalMode</strong>：集群 IP 伪装采用 <em>ClusterCIDR</em> 还是 <em>NodeCIDR</em>，<em>ClusterCIDR</em> 是使用集群 Pod IP 的地址段 (IP Range)，而 <em>LocalCIDR</em> 只仅仅使用被分配给该 kubernetes node 上的 Pod 做地址伪装</li>
<li><strong>ProxyMode</strong>：和 <em><strong>LocalMode</strong></em> 没有任何关系，是 <em>kube-proxy</em> 在运行时使用什么为集群 service 做代理，例如 iptables, ipvs ，而在这些模式下将采用什么 <em>LocalMode</em> 为集群外部地址作伪装，大概分为三种类型：
<ul>
<li>为来自集群外部地址 (<em>cluster-off</em>)：所有非 Pod 地址的请求执行跳转 (<em>KUBE-POSTROUTING</em>)</li>
<li>没有操作 ：在非 iptables/ipvs 模式下，不做伪装</li>
<li>masqueradeAll：为所有访问 cluster ip 的地址做伪装</li>
</ul>
</li>
</ul>
<h2 id="clustercidr-原理">ClusterCIDR 原理</h2>
<p><em>kube-proxy</em> 为 kube node 上生成一些 NAT 规则，如下所示</p>
<pre><code class="language-bash">-A KUBE-FIREWALL -j KUBE-MARK-DROP
-A KUBE-LOAD-BALANCER -j KUBE-MARK-MASQ
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-NODE-PORT -p tcp -m comment --comment &quot;Kubernetes nodeport TCP port for masquerade purpose&quot; -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ
-A KUBE-POSTROUTING -m comment --comment &quot;Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose&quot; -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE
-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN
-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0
-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -j MASQUERADE
-A KUBE-SERVICES ! -s 10.244.0.0/16 -m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot; -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ
-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT
-A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT
</code></pre>
<p>可以看到这里做了几个链，在 <em>KUBE-SERVICES</em> 链中指明了非来自 ClusterCIDR 的 IP 都做一个，并且访问的目的地址是 <em>KUBE-CLUSTER-IP</em> (ipset 里配置的地址) 那么将跳转到 <em>KUBE-MARK-MASQ</em> 链做一个 <code> --set-xmark 0x4000/0x4000</code> ，而在 <em>KUBE-POSTROUTING</em> 中对没有被标记 <code>0x4000/0x4000</code> 的操作不做处理</p>
<p>具体来说，<code>-A KUBE-NODE-PORT -p tcp -m comment --comment &quot;Kubernetes nodeport TCP port for masquerade purpose&quot; -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ</code> 做了如下操作：</p>
<ul>
<li><code>-A KUBE-SERVICES</code>：将这条规则附加到名为<code>KUBE-SERVICES</code>的iptables链。</li>
<li><code>! -s 10.244.0.0/16</code>：排除源IP地址为<code>10.244.0.0/16</code>的流量（即来自Kubernetes服务集群IP的流量）。</li>
<li><code>-m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot;</code>：添加一条注释，说明这个规则的用途。</li>
<li><code>-m set --match-set KUBE-CLUSTER-IP dst,dst</code>：使用IP集合<code>KUBE-CLUSTER-IP</code>来匹配目标IP地址和目标端口。</li>
<li><code>-j KUBE-MARK-MASQ</code>：如果流量匹配了前面的条件，将流量传递到名为<code>KUBE-MARK-MASQ</code>的目标。</li>
</ul>
<blockquote>
<p><code>iptables -j RETURN</code> 是用于iptables规则中的一个目标动作，它不是用于拒绝或接受数据包的动作，而是用于从当前规则链中返回（返回到调用链）的动作。</p>
<p>具体来说，当规则链中的数据包被标记为 <code>RETURN</code> 时，它们将不再受到当前链中后续规则的影响，而会立即返回到调用链，以便继续进行后续规则的处理。这通常用于某些高级设置，例如在自定义规则链中执行特定的操作后返回到主要的防火墙链。</p>
</blockquote>
<p>从代码中可以看到，对应执行 jump 的操作的链就是 <em>KUBE-MARK-MASQ</em></p>
<pre><code class="language-go">} else if proxier.localDetector.IsImplemented() {
			// This masquerades off-cluster traffic to a service VIP.  The idea
			// is that you can establish a static route for your Service range,
			// routing to any node, and that node will bridge into the Service
			// for you.  Since that might bounce off-node, we masquerade here.
			// If/when we support &quot;Local&quot; policy for VIPs, we should update this.
			writeLine(proxier.natRules, proxier.localDetector.JumpIfNotLocal(append(args, &quot;dst,dst&quot;), string(KubeMarkMasqChain))...)	

// KubeMarkMasqChain is the mark-for-masquerade chain
KubeMarkMasqChain utiliptables.Chain = &quot;KUBE-MARK-MASQ&quot;
    
// 具体拼接的就是 -j 链名的操作
func (d *detectLocalByCIDR) JumpIfNotLocal(args []string, toChain string) []string {
	line := append(args, &quot;!&quot;, &quot;-s&quot;, d.cidr, &quot;-j&quot;, toChain)
	klog.V(4).Info(&quot;[DetectLocalByCIDR (&quot;, d.cidr, &quot;)]&quot;, &quot; Jump Not Local: &quot;, line)
	return line
}
</code></pre>
<p>继续往下 <em>KUBE-POSTROUTING</em> 可以看到对应伪装是一个动态的源地址改造，而 <em>RETURN</em> 则不是被标记的请求</p>
<pre><code class="language-go">Chain KUBE-POSTROUTING (1 references)
target     prot opt source               destination         
MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0            /* Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose */ match-set KUBE-LOOP-BACK dst,dst,src
RETURN     all  --  0.0.0.0/0            0.0.0.0/0            mark match ! 0x4000/0x4000
MARK       all  --  0.0.0.0/0            0.0.0.0/0            MARK xor 0x4000
MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */
</code></pre>
<p>这整体就是 ClusterCIDR 在 <em>kube-proxy</em> 中的应用，换句话说还需要关注一个 LocalCIDR</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解kubelet - VolumeManager源码解析</title>
      <link>https://www.oomkill.com/2023/08/ch29-volumemanager/</link>
      <pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/08/ch29-volumemanager/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>阅读完本文，您当了解</p>
<ul>
<li>Kubernetes 卷</li>
<li>CephFS 在 kubernetes 中的挂载</li>
<li>Kubelet VolumeManager</li>
</ul>
<blockquote>
<p>本文只是个人理解，如果有大佬觉得不是这样的可以留言一起讨论，参考源码版本为 1.18.20，与高版本相差不大</p>
</blockquote>
<h2 id="volumemanager">VolumeManager</h2>
<p>VolumeManager VM 是在 kubelet 启动时被初始化的一个异步进程，主要是维护 “Pod&quot; 卷的两个状态，”desiredStateOfWorld“ 和 ”actualStateOfWorld“； 这两个状态用于将节点上的卷 “协调” 到所需的状态。</p>
<p>VM 实际上包含三个 “异步进程” (goroutine)，其中有一个 reconciler 就是用于协调与挂载的，下面就来阐述 VM 的挂载过程。</p>
<h3 id="vm中的重要组件">VM中的重要组件</h3>
<ul>
<li>actualStateOfWorld</li>
<li>mountedPod</li>
<li>desiredStateOfWorld</li>
<li>VolumeToMount</li>
<li>podToMount</li>
</ul>
<h3 id="vm的组成">VM的组成</h3>
<p>VM 的代码位于，由图可以看出，主要包含三个重要部分：</p>
<ul>
<li>reconciler：协调器</li>
<li>populator：填充器</li>
<li>cache：包含 ”desiredStateOfWorld“ 和 ”actualStateOfWorld“</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230820221712742.png" alt="image-20230820221712742" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：VM的目录组成</center>
<p>在代码结构上，<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/volume_manager.go#L216-L257" target="_blank"
   rel="noopener nofollow noreferrer" >volumeManager</a> 如下所示</p>
<pre><code class="language-go">// volumeManager implements the VolumeManager interface
type volumeManager struct {
    // DesiredStateOfWorldPopulator 用来与 API 服务器通信以获取 PV 和 PVC 对象的 API 客户端
	kubeClient clientset.Interface
    
    // VolumePluginMgr 是用于访问 VolumePlugin 插件的 VolumePlugin 管理器。它必须预初始化。
	volumePluginMgr *volume.VolumePluginMgr

    // desiredStateOfWorld 是一个数据结构，包含根据 VM 所需的状态：即应附加哪些卷以及 &quot;哪些pod” 正在引用这些卷。
    // 使用 kubelet pod manager 根据 world populator 的所需状态填充数据结构。
	desiredStateOfWorld cache.DesiredStateOfWorld

	// 与 desiredStateOfWorld 相似，是实际状态：即哪些卷被 attacted 到该 Node 以及 volume 被 mounted 到哪些 pod。
    // 成功完成 reconciler attach,detach, mount, 和 unmount 操作后，将填充数据结构。
	actualStateOfWorld cache.ActualStateOfWorld

	// operationExecutor 用于启动异步 attach,detach, mount, 和 unmount 操作。
	operationExecutor operationexecutor.OperationExecutor

	// reconciler reconciler 运行异步周期性循环，通过使用操作执行器触发 attach,detach, mount, 和 unmount操作
    // 来协调 desiredStateOfWorld 与 actualStateOfWorld。
	reconciler reconciler.Reconciler

    // desiredStateOfWorldPopulator 运行异步周期性循环以使用 kubelet Pod Manager 填充desiredStateOfWorld。
	desiredStateOfWorldPopulator populator.DesiredStateOfWorldPopulator

	// csiMigratedPluginManager keeps track of CSI migration status of plugins
	csiMigratedPluginManager csimigration.PluginManager

	// intreeToCSITranslator translates in-tree volume specs to CSI
	intreeToCSITranslator csimigration.InTreeToCSITranslator
}
</code></pre>
<h3 id="vm的初始化">VM的初始化</h3>
<ul>
<li>入口：“volumeManager”(vm) 的 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/kubelet.go#L1436" target="_blank"
   rel="noopener nofollow noreferrer" >初始化</a> 操作发生在 kubelet Run 时被作为一个异步进程启动。</li>
<li>VM 初始化：
<ul>
<li>如代码1所示，VM在初始化阶段创建了两个 cache 对象 “desiredStateOfWorld”（dsw）和“actualStateOfWorld”（asw）以及一个 “operationExecutor”，用于启动异步的线程操作 attach,detach, mount, 和 unmount</li>
<li>如代码2所示：VM在初始化阶段还创建了 “desiredStateOfWorldPopulator” (dswp) 与 “reconciler”
<ul>
<li>“reconciler” 通过使用上面的 “operationExecutor”  触发 attach,detach, mount 和 unmount来协调 dsw 与 asw</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>代码1</p>
<pre><code class="language-go">vm := &amp;volumeManager{
    kubeClient:          kubeClient,
    volumePluginMgr:     volumePluginMgr,
    desiredStateOfWorld: cache.NewDesiredStateOfWorld(volumePluginMgr),
    actualStateOfWorld:  cache.NewActualStateOfWorld(nodeName, volumePluginMgr),
    operationExecutor: operationexecutor.NewOperationExecutor(operationexecutor.NewOperationGenerator(
        kubeClient,
        volumePluginMgr,
        recorder,
        checkNodeCapabilitiesBeforeMount,
        blockVolumePathHandler)),
}
</code></pre>
<p>代码2：</p>
<pre><code class="language-go">vm.intreeToCSITranslator = intreeToCSITranslator
vm.csiMigratedPluginManager = csiMigratedPluginManager
vm.desiredStateOfWorldPopulator = populator.NewDesiredStateOfWorldPopulator(
    kubeClient,
    desiredStateOfWorldPopulatorLoopSleepPeriod,
    desiredStateOfWorldPopulatorGetPodStatusRetryDuration,
    podManager,
    podStatusProvider,
    vm.desiredStateOfWorld,
    vm.actualStateOfWorld,
    kubeContainerRuntime,
    keepTerminatedPodVolumes,
    csiMigratedPluginManager,
    intreeToCSITranslator)
vm.reconciler = reconciler.NewReconciler(
    kubeClient,
    controllerAttachDetachEnabled,
    reconcilerLoopSleepPeriod,
    waitForAttachTimeout,
    nodeName,
    vm.desiredStateOfWorld,
    vm.actualStateOfWorld,
    vm.desiredStateOfWorldPopulator.HasAddedPods,
    vm.operationExecutor,
    mounter,
    hostutil,
    volumePluginMgr,
    kubeletPodsDir)
</code></pre>
<h3 id="vm-的-run">VM 的 Run</h3>
<p>VM 是在 Kubelet 启动时作为异步线程启动，如代码1所示</p>
<p>如下面代码2所示，VM 在运行时会启动 <strong>三个</strong> 异步线程</p>
<ul>
<li>第一个调用是 第二个是调用 “dswp” 填充其的“ <a href="https://github.com/kubernetes/kubernetes/blob/v1.21.2/pkg/kubelet/volumemanager/volume_manager.go#L268" target="_blank"
   rel="noopener nofollow noreferrer" >Run</a> ”，这里主要做的操作是从 API 拿到 Pod 列表，根据对应条件来决定 attach,detach, mount, 和 unmount</li>
<li>第二个调用的是，reconciler 来协调应该安装的卷是否已安装以及应该卸载的卷是否已卸载。</li>
<li>第三个调用的是，volumePluginMgr，启用 CSI  informer</li>
</ul>
<p><a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/kubelet.go#L1435-L1436" target="_blank"
   rel="noopener nofollow noreferrer" >代码1</a></p>
<pre><code>// Start volume manager
go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/volume_manager.go#L259-L277" target="_blank"
   rel="noopener nofollow noreferrer" >代码2</a></p>
<pre><code class="language-go">func (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh &lt;-chan struct{}) {
	defer runtime.HandleCrash()

	go vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh)
	klog.V(2).Infof(&quot;The desired_state_of_world populator starts&quot;)

	klog.Infof(&quot;Starting Kubelet Volume Manager&quot;)
	go vm.reconciler.Run(stopCh)

	metrics.Register(vm.actualStateOfWorld, vm.desiredStateOfWorld, vm.volumePluginMgr)

	if vm.kubeClient != nil {
		// start informer for CSIDriver
		vm.volumePluginMgr.Run(stopCh)
	}

	&lt;-stopCh
	klog.Infof(&quot;Shutting down Kubelet Volume Manager&quot;)
}
</code></pre>
<h3 id="vm-的调用流程">VM 的调用流程</h3>
<h4 id="desiredstateofworldpopulator">desiredStateOfWorldPopulator</h4>
<p><code>DesiredStateOfWorldPopulator</code> 是一个周期 Loop，会定期循环遍历 Active Pod 列表，并确保每个 Pod 都处于所需状态（如果有卷，World state）。它还会验证 World cache 中处于所需状态的 pod 是否仍然存在，如果不存在，则会将其删除。</p>
<p>desiredStateOfWorldPopulator 结构包含两个方法，ReprocessPod 和 HasAddedPods；<strong><code>ReprocessPod</code></strong> 负责将 processedPods 中指定 pod 的值设置为false，强制重新处理它。这是在 Pod 更新时启用重新挂载卷所必需的。而 <strong><code>HasAddedPods</code></strong> 返回 填充器 是否已循环遍历 Active Pod 列表并将它们添加到 world cache 的所需状态。</p>
<p>在期待填充器 desiredStateOfWorldPopulator 启动时，会运行一个 populatorLoop，这里主要负责运行两个函数，</p>
<ul>
<li><code>findAndAddNewPods</code> 负责迭代所有 pod，如果它们不存在添加到  desired state of world (desiredStateOfWorld)</li>
<li><code>findAndRemoveDeletedPods</code> 负责迭代 <em><code>desiredStateOfWorld</code></em> 下的所有 Pod，如果它们不再存在则将其删除</li>
</ul>
<h4 id="reconciler">reconciler</h4>
<p>reconciler Run 的过程是通过一个 Loop 函数 <code>reconciliationLoopFunc</code> 完成的，正如下列 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/reconciler/reconciler.go#L128-L179" target="_blank"
   rel="noopener nofollow noreferrer" >代码</a> 所示</p>
<pre><code class="language-go">func (rc *reconciler) Run(stopCh &lt;-chan struct{}) {
	wait.Until(rc.reconciliationLoopFunc(), rc.loopSleepDuration, stopCh)
}

func (rc *reconciler) reconciliationLoopFunc() func() {
	return func() {
		rc.reconcile()

		// Sync the state with the reality once after all existing pods are added to the desired state from all sources.
		// Otherwise, the reconstruct process may clean up pods' volumes that are still in use because
		// desired state of world does not contain a complete list of pods.
		if rc.populatorHasAddedPods() &amp;&amp; !rc.StatesHasBeenSynced() {
			klog.Infof(&quot;Reconciler: start to sync state&quot;)
			rc.sync()
		}
	}
}

func (rc *reconciler) reconcile() {
   // Unmounts are triggered before mounts so that a volume that was
   // referenced by a pod that was deleted and is now referenced by another
   // pod is unmounted from the first pod before being mounted to the new
   // pod.
   // 卸载会在挂载之前触发，以便已删除的 Pod 引用的卷现在被另一个 Pod 引用，
   // 然后再挂载到新 Pod 之前从第一个 Pod 中卸载。
   rc.unmountVolumes()

   // Next we mount required volumes. This function could also trigger
   // attach if kubelet is responsible for attaching volumes.
   // If underlying PVC was resized while in-use then this function also handles volume
   // resizing.
   // 接下来我们安装所需的卷。如果 kubelet 负责附加卷，
   // 则此函数还可以触发附加。如果底层 PVC 在使用时调整了大小，则此函数还可以处理卷大小调整。
   rc.mountAttachVolumes()

   // Ensure devices that should be detached/unmounted are detached/unmounted.
   // 确保应 detached/unmounted 的设备已完成 detached/unmounted。
   rc.unmountDetachDevices()
}
</code></pre>
<p>Reconciler <strong>是挂载部分最重要的角色</strong>，用于协调应该安装的卷是否已安装以及应该卸载的卷是否已卸载；对应的，实际上执行的为三个函数：“unmountVolumes”、“mountAttachVolumes” 和 “unmountDetachDevices”。</p>
<h5 id="mountattachvolumes">mountAttachVolumes</h5>
<ol>
<li>
<p>首先，“<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/reconciler/reconciler.go#L202-L292" target="_blank"
   rel="noopener nofollow noreferrer" >mountAttachVolumes</a>” 会调用 “dsw” (desiredStateOfWorld) 的函数 “GetVolumesToMount” 来检索所有 “volumesToMount” 并迭代它们，这里主要是为了确保 “volumes” 应完成了 “attached/mounted”</p>
</li>
<li>
<p>接下来这个循环做的工作是，对于每个 Volume 和 Pod，都会检查该 Volume 或 Pod 是否存在于 “asw” 的 “attachedVolumes” 中。如果 Volume 不存在，则“asw”返回 “<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/cache/actual_state_of_world.go#L662" target="_blank"
   rel="noopener nofollow noreferrer" >newVolumeNotAttachedError</a> ”，否则它检查指定的 pod 是否存在并根据状态返回结果。这里存在 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_executor.go#L400-L409" target="_blank"
   rel="noopener nofollow noreferrer" >三个状态</a>，返回也是根据这个状态返回。<strong>这里主要为了得到挂载路径和是否挂载</strong></p>
<ul>
<li>
<p>VolumeMounted：表示 Volume 已挂载到 pod 的本地路径中</p>
</li>
<li>
<p>VolumeMountUncertain：表示 Volume 可能会也可能不会安装在 Pod 的本地路径中</p>
</li>
<li>
<p>VolumeNotMounted：表示 Volume  还未挂载到 pod 的本地路径中</p>
</li>
</ul>
</li>
<li>
<p>当“ asw” 返回 “ <a href="https://github.com/kubernetes/kubernetes/blob/v1.21.2/pkg/kubelet/volumemanager/reconciler/reconciler.go#L207" target="_blank"
   rel="noopener nofollow noreferrer" >newVolumeNotAttachedError</a> ” 时，“reconciler” 会检查 “controllerAttachDetachEnabled” 是否启用，或 “<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_executor.go#L336" target="_blank"
   rel="noopener nofollow noreferrer" >volumeToMount</a>” 没有实现了对应插件，这里面如果其中任何一个为 true，“reconciler” 将调用 “operationExecutor” 来执行操作“ ，走到这里代表了 Volume 没有被 attach，或者没有实现 attacher，例如 cephfs 没有实现 attacher；或者是 kubelet 禁用了 attach  <sup><a href="#1">[1]</a></sup> （默认是开启状态），将进入 “ <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_executor.go#L909-L921" target="_blank"
   rel="noopener nofollow noreferrer" >VerifyControllerAttachedVolume</a> ”</p>
<ul>
<li>
<p>在此期间，“operationExecutor” 生成一个名为 “<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_generator.go#L1278-L1352" target="_blank"
   rel="noopener nofollow noreferrer" >verifyControllerAttachedVolumeFunc</a>” 的函数来实际实现。在此函数中，如果 “volumeToMount” 的 “PluginIsAttachable” 为 <em>false</em>（没有实现），则假设其已经实现并标记 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_generator.go#L1288C17-L1301" target="_blank"
   rel="noopener nofollow noreferrer" >attached</a>，标记出错时进行重试（这是一个函数用于后面的调用，这里只是定义）</p>
</li>
<li>
<p>如果还没有将 Node attached 到 Volume 节点列表状态中，则返回错误进行重试（这是一个函数用于后面的调用，这里只是定义）</p>
</li>
<li>
<p>上面两个步骤是为了组装这个操作，返回的是操作的内容，包含执行的函数，完成的hook等，最后运行这个函数并返回</p>
</li>
</ul>
</li>
<li>
<p>这是步骤3的另外一个分支，即 kubelet 启用了 ADController，并且实现了对应的 attcher，那么将执行附加操作</p>
<ul>
<li>拼接对象</li>
<li>执行函数 ”AttachVolume“</li>
<li>AttachVolume 如上面步骤一样，拼接出最后的执行的动作，进行执行操作（将 node 附加到 volume 之上）</li>
</ul>
</li>
<li>
<p><a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_executor.go#L798-L958" target="_blank"
   rel="noopener nofollow noreferrer" >步骤5</a> 表示 3, 4 条件均不满足，也就是 Attached，目前状态为 ”未挂载“  或者 ”已挂载“，将执行这个步骤，未挂载的进行挂载，已挂载的进行 remount</p>
</li>
<li>
<p>在该分支中（也就是 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_generator.go#L489-L694" target="_blank"
   rel="noopener nofollow noreferrer" >步骤5</a> 执行的）执行的是名为 “<a href="https://github.com/kubernetes/kubernetes/blob/v1.21.2/pkg/volume/util/operationexecutor/operation_generator.go#L500" target="_blank"
   rel="noopener nofollow noreferrer" >GenerateMountVolumeFunc</a>“ 的函数，在此函数中，会获取 Plugin ，并通过 Plugin 创建出一个 volumeMounter，在通过 Plugin 获取一个 deviceMouter（能够挂载块设备的）；当然我们这里挂载的是 ”cephfs“ 所以没有 ”deviceMouter“ 这里不被执行。</p>
<ul>
<li>如果 ”deviceMounter“ 定义了，那么则执行这个 plugin 的 &ldquo;MountDevice&rdquo; 函数</li>
<li>如果没有定义，那么执行 volumeMounter 的 SetUp 进行挂载（因为不是块设备）</li>
</ul>
</li>
<li>
<p>执行 SetUp 函数，通常 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/nfs/nfs.go#L240-L242" target="_blank"
   rel="noopener nofollow noreferrer" >NFS</a>, <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/cephfs/cephfs.go#L223-L225" target="_blank"
   rel="noopener nofollow noreferrer" >CephFS</a>, <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/local/local.go#L472-L474" target="_blank"
   rel="noopener nofollow noreferrer" >HostPath</a>，都实现了这个函数，那么就会通过这个函数挂载到 Node 对应的目录</p>
</li>
<li>
<p>最后通过 Overlay2 文件系统附加到容器里</p>
</li>
</ol>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>command-line-tools-reference kubelet</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://shuanglu1993.medium.com/what-happens-when-volumemanager-in-the-kubelet-starts-1fea623ac6ce" target="_blank"
   rel="noopener nofollow noreferrer" ><em>What happens when volumeManager in the kubelet starts?</em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析 - Kubernetes中的事件通知机制</title>
      <link>https://www.oomkill.com/2023/06/kubernetes-event/</link>
      <pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/06/kubernetes-event/</guid>
      <description>文章分析了Kuberentes事件记录及事件是如何调度的</description>
      <content:encoded><![CDATA[<p>在 Kubernetes 中 事件 ( <em>Event</em> ） 通常被大家认知为是展示集群中发生的情况，通常用作 Pod 的查看，例如为什么 CrashBackOff, 为什么 Pendding，而很少有人知道事件在 Kubernetes 整个系统中的设计是非常巧妙的，可以通过各组件间的传递，使得用户可以知道集群中的情况，文章中将一地揭开Kubernetes to神秘面纱。</p>
<h2 id="为什么需要事件">为什么需要事件</h2>
<p>Kubernetes 在设计时就是 “声明式”，而声明式的最大特点就是 “多组件的协同工作”，而在多组件协同工作时，势必需要传递一些事件，以告知用户任务的状态如何；而事件本身上是一种资源，在很早版本就以及被移入 api/v1 中。下面是 “事件” 资源的定义。</p>
<p>位于 <a href="vendor/k8s.io/api/core/v1/types.go">vendor/k8s.io/api/core/v1/types.go</a> ，因为 <code>vendor/k8s.io</code> 实际上是做了一个软连接，那么真实的实际上位于 <code>{kubernetes_repo}/staging/src/k8s.io/api/core/v1</code></p>
<pre><code class="language-go">type Event struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// 标准的元数据
	metav1.ObjectMeta `json:&quot;metadata&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

	// 事件涉及的对象
	InvolvedObject ObjectReference `json:&quot;involvedObject&quot; protobuf:&quot;bytes,2,opt,name=involvedObject&quot;`

	// 这里表示的是事件原因，通常为简短的的一种状态名称
	// TODO: provide exact specification for format.
	// +optional
	Reason string `json:&quot;reason,omitempty&quot; protobuf:&quot;bytes,3,opt,name=reason&quot;`

	// 以人类可读取的方式描述，类似于 tcmpdump -A
	// TODO: decide on maximum length.
	// +optional
	Message string `json:&quot;message,omitempty&quot; protobuf:&quot;bytes,4,opt,name=message&quot;`

	// 报告事件的组件，通常包含这个结构体包含 “组件+主机名” 的结构
	// +optional
	Source EventSource `json:&quot;source,omitempty&quot; protobuf:&quot;bytes,5,opt,name=source&quot;`

	// 首次上报事件的事件
	// +optional
	FirstTimestamp metav1.Time `json:&quot;firstTimestamp,omitempty&quot; protobuf:&quot;bytes,6,opt,name=firstTimestamp&quot;`

	// 最近一次记录事件的事件
	// +optional
	LastTimestamp metav1.Time `json:&quot;lastTimestamp,omitempty&quot; protobuf:&quot;bytes,7,opt,name=lastTimestamp&quot;`

	// 事件发生的次数
	// +optional
	Count int32 `json:&quot;count,omitempty&quot; protobuf:&quot;varint,8,opt,name=count&quot;`

    // 事件的类型(Normal, Warning)
	// +optional
	Type string `json:&quot;type,omitempty&quot; protobuf:&quot;bytes,9,opt,name=type&quot;`

	// 首次观察到事件的.
	// +optional
	EventTime metav1.MicroTime `json:&quot;eventTime,omitempty&quot; protobuf:&quot;bytes,10,opt,name=eventTime&quot;`

    // 事件相关的序列，如果事件为单例事件，那么则为nil
	// +optional
	Series *EventSeries `json:&quot;series,omitempty&quot; protobuf:&quot;bytes,11,opt,name=series&quot;`

	// 对事件对象采取的行动
	// +optional
	Action string `json:&quot;action,omitempty&quot; protobuf:&quot;bytes,12,opt,name=action&quot;`

	// Optional secondary object for more complex actions.
	// +optional
	Related *ObjectReference `json:&quot;related,omitempty&quot; protobuf:&quot;bytes,13,opt,name=related&quot;`

	// 发出事件的对应的控制器，也可以理解为组件，因为通常controller-manager 包含多个控制器
    // e.g. `kubernetes.io/kubelet`.
	// +optional
	ReportingController string `json:&quot;reportingComponent&quot; protobuf:&quot;bytes,14,opt,name=reportingComponent&quot;`

	// 控制器实例的ID, e.g. `kubelet-xyzf`.
	// +optional
	ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;`
}
</code></pre>
<h2 id="事件管理器">事件管理器</h2>
<p>通过上面知道了事件这个资源的设计，里面存在一个 ”发出事件的对应的控制器“ 那么必然是作为每一个组件的内置功能，也就是说这可以作为 client-go 中的一个组件。</p>
<p>代码 <a href="vendor/k8s.io/client-go/tools/events/interfaces.go">vendor/k8s.io/client-go/tools/events/interfaces.go</a> 中定义了一个事件管理器，这将定义了如何接收或发送事件到任何地方，例如事件接收器 (<em>EventSink</em>) 或 log</p>
<pre><code class="language-go">type EventBroadcaster interface {
    // 发送从指定的eventBroadcaster接收到的事件
	StartRecordingToSink(stopCh &lt;-chan struct{})

    // 返回一个 EventRecorder 并可以使用发送事件到 EventBroadcaster，并将事件源设置为给定的事件源。
	NewRecorder(scheme *runtime.Scheme, reportingController string) EventRecorder

    // StartEventWatcher 可以使在不使用 StartRecordingToSink 的情况下发送事件
    // 这使得可以通过自定义方式记录事件
    // NOTE: 在使用 eventHandler 接收到的事件时应先进行复制一份。
	// TODO: figure out if this can be removed.
	StartEventWatcher(eventHandler func(event runtime.Object)) func()

    // StartStructuredLogging 可以接收 EventBroadcaster 发送的结构化日志功能
    // 如果需要可以忽略返回值或使用于停止记录
	StartStructuredLogging(verbosity klog.Level) func()

    // 关闭广播
	Shutdown()
}
</code></pre>
<p>EventBroadcaster 的实现只有一个 eventBroadcasterImpl</p>
<pre><code>type eventBroadcasterImpl struct {
   *watch.Broadcaster
   mu            sync.Mutex
   eventCache    map[eventKey]*eventsv1.Event
   sleepDuration time.Duration
   sink          EventSink
}
</code></pre>
<p>这里面最重要的就是 sink，sink就是决定如何去存储事件的一个组件，他返回的是一组 client-go 的 REST 客户端。</p>
<h3 id="事件管理器的设计">事件管理器的设计</h3>
<h3 id="事件生产者">事件生产者</h3>
<p>事件生产者在事件管理器中是作为</p>
<h2 id="控制器">控制器</h2>
<p>service的资源创建很奇妙，继不属于 <code>controller-manager</code> 组件，也不属于 <code>kube-proxy</code> 组件，而是存在于 <code>apiserver</code> 中的一个被成为控制器的组件；而这个控制器又区别于准入控制器。更准确来说，准入控制器是位于kubeapiserver中的组件，而 <strong>控制器</strong> 则是存在于单独的一个包，这里包含了很多kubernetes集群的公共组件的功能，其中就有service。这也就是在操作kubernetes时 当 <code>controller-manager</code> 于  <code>kube-proxy</code> 未工作时，也可以准确的为service分配IP。</p>
<p>首先在构建出apiserver时，也就是代码 <a href="cmd/kube-apiserver/app/server.go">cmd/kube-apiserver/app/server.go</a></p>
<pre><code class="language-go">serviceIPRange, apiServerServiceIP, err := master.ServiceIPRange(s.PrimaryServiceClusterIPRange)
if err != nil {
    return nil, nil, nil, nil, err
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/pkg/master/services.go#L34-L54" target="_blank"
   rel="noopener nofollow noreferrer" >master.ServiceIPRange</a> 承接了为service分配IP的功能，这部分逻辑就很简单了</p>
<pre><code class="language-go">func ServiceIPRange(passedServiceClusterIPRange net.IPNet) (net.IPNet, net.IP, error) {
	serviceClusterIPRange := passedServiceClusterIPRange
	if passedServiceClusterIPRange.IP == nil {
		klog.Warningf(&quot;No CIDR for service cluster IPs specified. Default value which was %s is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.&quot;, kubeoptions.DefaultServiceIPCIDR.String())
		serviceClusterIPRange = kubeoptions.DefaultServiceIPCIDR
	}

	size := integer.Int64Min(utilnet.RangeSize(&amp;serviceClusterIPRange), 1&lt;&lt;16)
	if size &lt; 8 {
		return net.IPNet{}, net.IP{}, fmt.Errorf(&quot;the service cluster IP range must be at least %d IP addresses&quot;, 8)
	}

	// Select the first valid IP from ServiceClusterIPRange to use as the GenericAPIServer service IP.
	apiServerServiceIP, err := utilnet.GetIndexedIP(&amp;serviceClusterIPRange, 1)
	if err != nil {
		return net.IPNet{}, net.IP{}, err
	}
	klog.V(4).Infof(&quot;Setting service IP to %q (read-write).&quot;, apiServerServiceIP)

	return serviceClusterIPRange, apiServerServiceIP, nil
}
</code></pre>
<p>而后kube-apiserver为service分为两类</p>
<ul>
<li>apiserver 地址在集群内的service，在代码中表示为 <a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L351" target="_blank"
   rel="noopener nofollow noreferrer" >APIServerServiceIP</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L352" target="_blank"
   rel="noopener nofollow noreferrer" >Service</a>，<code>--service-cluster-ip-range</code> 配置指定的ip，通过『逗号』分割可以为两个</li>
</ul>
<p>有了对 service 更好的理解后，接下来开始本系列第二节<a href="https://cylonchau.github.io/kubernetes-without-service.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - kube-proxy软件架构分析</a></p>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>dual-stack service</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>在Kubernetes集群上安装 Calico cni 的注意事项</title>
      <link>https://www.oomkill.com/2023/06/calico-cni-deplyment/</link>
      <pubDate>Wed, 21 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/06/calico-cni-deplyment/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="开始前的实验环境">开始前的实验环境</h2>
<table>
<thead>
<tr>
<th style="text-align:center">Resources</th>
<th style="text-align:center">controller</th>
<th style="text-align:center">worker-1</th>
<th style="text-align:center">worker-2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">OS</td>
<td style="text-align:center">CentOS 7.9</td>
<td style="text-align:center">CentOS 7.9</td>
<td style="text-align:center">CentOS 7.9</td>
</tr>
<tr>
<td style="text-align:center">Storage</td>
<td style="text-align:center">20GB</td>
<td style="text-align:center">20GB</td>
<td style="text-align:center">20GB</td>
</tr>
<tr>
<td style="text-align:center">vCPU</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">RAM</td>
<td style="text-align:center">4GB</td>
<td style="text-align:center">4GB</td>
<td style="text-align:center">4GB</td>
</tr>
<tr>
<td style="text-align:center">NIC</td>
<td style="text-align:center">10.0.0.4</td>
<td style="text-align:center">10.0.0.4</td>
<td style="text-align:center">10.0.0.4</td>
</tr>
<tr>
<td style="text-align:center">Kubernetes Version</td>
<td style="text-align:center">1.19.10</td>
<td style="text-align:center">1.19.10</td>
<td style="text-align:center">1.19.10</td>
</tr>
</tbody>
</table>
<h2 id="选择匹配-kubernetes-版本的-calico-版本">选择匹配 Kubernetes 版本的 Calico 版本</h2>
<p>通常情况下，查看 Calico 所支持的 Kubernetes 版本，可以通过路径 Install Calico ==&gt; Kubernetes ==&gt; System requirements 可以找到自己的 Kubernetes 集群所支持的 Calico 版本。</p>
<p>例如在实验环境中，Kubernetes 1.19 版本所支持的版本有 Calico 3.20，这个时候直接 apply 这个版本提供的资源清单即可</p>
<h2 id="如何开启纯-bgp-模式">如何开启纯 BGP 模式</h2>
<p>默认情况下下，Calico 使用的是 full mesh 和 IPIP， 如果想通过在部署时就修改关闭 IPIP 模式，可以通过修改资源清单中的环境变量来关闭 <code>CALICO_IPV4POOL_IPIP: Never</code>。</p>
<p>如果需要在安装时配置Pod 的 CIDR，需要修改 <code>CALICO_IPV4POOL_CIDR</code></p>
<h2 id="如果你需要切换-cni">如果你需要切换 CNI</h2>
<p>如果你的集群不是空的，而是存在很多 Pod 的集群，请注意，这个时候你的 flannel 或者其他 CNI 生成的网络接口是不会被销毁的，Pod 的 IP也是旧 CNI 生成的网段，此时 Calico 会按照原有的 IP 进行维护路由，可能会存在访问不了的情况，这时候不要随意切换 CNI</p>
<h2 id="如何检查-calico-使用的是什么模式">如何检查 Calico 使用的是什么模式</h2>
<p>在使用默认的资源清单安装完 Calico 后，实际上此时会表现为 BGP + IPIP 隧道模式，同节点 Pod 使用直连方式，跨节点 Pod 通讯使用 tunnel 隧道完成，表现形式为 <code>ip addr</code> 会看到 <code>tunl0</code> 设备</p>
<p>如果是纯 BGP 模式，那么表现形式为 <code>route -n</code> 看到的路由跨节点的都应该是 <code>eth0</code> 这样子的，如下所示</p>
<pre><code class="language-bash">$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.0.0.2        0.0.0.0         UG    0      0        0 eth0
10.0.0.0        0.0.0.0         255.255.255.0   U     0      0        0 eth0
10.244.196.128  0.0.0.0         255.255.255.255 UH    0      0        0 calia5f3c234a97
10.244.196.128  0.0.0.0         255.255.255.192 U     0      0        0 *
10.244.214.0    10.0.0.4        255.255.255.192 UG    0      0        0 eth0
169.254.0.0     0.0.0.0         255.255.0.0     U     1002   0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
</code></pre>
<p>当然此时一定是不存在 <code>tunl0</code> 设备的。</p>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://docs.tigera.io/archive/v3.20/reference/resources/bgpconfig" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>BGP configuration</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://docs.tigera.io/archive/v3.20/reference/resources/bgppeer" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>BGP peer</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://docs.tigera.io/archive/v3.20/reference/resources/ippool" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>IP pool</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://docs.tigera.io/archive/v3.20/getting-started/kubernetes/requirements" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>System requirements</strong></em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes中的资源限制 - Request&amp;Limit</title>
      <link>https://www.oomkill.com/2023/06/kubernetes-limit-request/</link>
      <pubDate>Sat, 17 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/06/kubernetes-limit-request/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p>原作者 <a href="https://sysdig.com/blog/author/javier-martinez/" target="_blank"
   rel="noopener nofollow noreferrer" >Javier Martínez</a></p>
</blockquote>
<h2 id="背景">背景</h2>
<p>在学习 Kubernetes 调度时，有两个重要的概念，&ldquo;request &ldquo;与 &ldquo;limit&rdquo;，而对应的资源就是“内存” 与 “CPU” ，而这两个决定了 Pod 将如何调度；&ldquo;request &ldquo;与 &ldquo;limit&rdquo; 也是整个调度系统中的基数因子。</p>
<h2 id="什么是-request--和--limit">什么是 request  和  limit</h2>
<p>在 Kubernetes 中，Limit 是容器可以使用的<strong>最大资源量</strong>，这表示 “容器” 的内存或 CPU 的使用，永远不会超过 Limit 配置的值。</p>
<p>而另一方面，Request 则是为 “容器”  保留的最低资源保障；换句话来说，Request 则是在调度时，容器被允许所需的配置。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/Kubernetes-Limits-and-Request-04-1-1170x585.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes 中Limit 和 Request 图示</center>
<center><em>Source：</em>https://sysdig.com/blog/kubernetes-limits-requests/</center>
<h2 id="如何配置-request--和--limit">如何配置 request  和  limit</h2>
<p>下列清单是 Deployment 的部署清单，他将部署一个 redis 与 一个 busybox</p>
<pre><code class="language-yaml">kind: Deployment
apiVersion: extensions/v1beta1
…
template:
  spec:
    containers:
      - name: redis
        image: redis:5.0.3-alpine
        resources:
          limits:
            memory: 600Mi
            cpu: 1
          requests:
            memory: 300Mi
            cpu: 500m
      - name: busybox
        image: busybox:1.28
        resources:
          limits:
            memory: 200Mi
            cpu: 300m
          requests:
            memory: 100Mi
            cpu: 100m
</code></pre>
<p>假设现有集群，具有 4 核CPU 和 16GB RAM 节点。此时可以提取出的信息如下：</p>
<ol>
<li>
<p><strong>Pod  Request</strong> 是 400 MiB 内存和 600 毫核 CPU (Redis+busybox)。而调度需要一具有足够可用可分配空间的Node</p>
<p>来调度 Pod。</p>
</li>
<li>
<p><strong>redis的 CPU</strong> 为 512，busybox 容器的 CPU 份额为 102，Kubernetes 为每个核心分配 1024 个份额，因此 redis：1024 * 0.5 个核心 ≅ 512 与 busybox：1024 * 0.1 个核心 ≅ 102</p>
</li>
<li>
<p>如果 Redis 容器尝试分配超过 600MB 的内存，则它会被 OOM 终止</p>
</li>
<li>
<p>如果 Redis 每 100 毫秒超过超过 100 毫秒时的 CPU，（Node有 4 个核，可用时间为每 100 毫秒 400 毫秒时），Redis 将受到 CPU 限制，从而导致性能下降**。**</p>
</li>
<li>
<p>如果 Busybox 容器试图分配超过 200MB 的内存，它将被 OOM 终止</p>
</li>
<li>
<p>如果Busybox尝试每 100 毫秒使用超过 30 毫秒的 CPU</p>
</li>
</ol>
<h2 id="request">Request</h2>
<p>通过上面示例，可以下定义了，Kuberentes 将 Request 定义为容器的 <strong>最小资源量</strong>。</p>
<p>当一个 Pod 被调度时，<code>kube-scheduler</code> 将检查 Kubernetes 请求，以便将该 Pod 分配到最佳节点，该节点至少可以满足 Pod 中所有容器的数量。如果 Request 的数量高于可用资源，则 Pod 将不会被调度并保持在 Pending 状态。</p>
<p>例如下列例子</p>
<pre><code class="language-yaml">resources:
   requests:
        cpu: 0.1
        memory: 4Mi
</code></pre>
<p>使用请求：</p>
<ul>
<li>
<p>将 Pod 分配给 Node 时，满足 Pod 中容器指示的 Request 。</p>
</li>
<li>
<p>在运行时，指示的 Request 将保证为该 Pod 中的容器的最小 Request 。</p>
</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image4.png" alt="如何设置良好的 CPU 请求" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：如何最佳的分配资源</center>
<center><em>Source：</em>https://sysdig.com/blog/kubernetes-limits-requests/</center>
<h2 id="limit">Limit</h2>
<p><strong>Limit</strong> 在 Kubernetes 为定义容器可以使用的<strong>最大资源量</strong>。这代表容器永远不会超过 Limit 配置的 内存 或 CPU 。</p>
<pre><code class="language-yaml">resources:
  limits:
    cpu: 0.5
    memory: 100Mi
</code></pre>
<ul>
<li>在调度时，如果没有配置 Request，默认 Kubernetes 将设置 requests = limits。</li>
<li>调度后，运行时，kubelet 检查 Pod 中的容器是否消耗了比 Limit 中配置的更多的资源。</li>
</ul>
<h2 id="cpu-和-内存的特性">CPU 和 内存的特性</h2>
<p>CPU 是一种 “<strong>可压缩资源</strong>”，这意味着它可以被拉伸以满足所有需求。如果进程申请了太多 CPU，其中一些将被限制。</p>
<ul>
<li>可以使用 millicores (m) 来表示比1核心更小的数量</li>
<li>CPU 最小量为 1m</li>
</ul>
<p><strong>内存</strong>是一种 “<strong>不可压缩的</strong>” 资源，这意味着内存资源不能像 CPU 资源那样被拉伸。如果一个进程没有足够的内存来工作，这个进程就会被 OOM。</p>
<p>内存资源在 Kubernetes 中的单位是以字节为<strong>单位</strong>，可以使用大写的 E、P、T、G、M、k 来表示 Exabyte、Petabyte、Terabyte、Gigabyte、Megabyte 和 kilobyte，例如 4G, 500M；也可以使用 Ei、Pi、Ti，例如 500Mi</p>
<p>**G 和 Gi ** 的区别：**G 和 Gi **区别主要在计算方式上，G是按照 2 的 n 次方进行计算，例如 1KB = $2^{10}$，而 Gi 计算方式是按照 10 的 n 次方，例如 1Mi = $10^3$</p>
<blockquote>
<p>Note：不要使用小写的 “m” ，这代表 Millibytes</p>
</blockquote>
<h2 id="resourcelimitquota---基于名称空间的资源限制">Resource/LimitQuota - 基于名称空间的资源限制</h2>
<p><strong>ResourceQuotas</strong> 在 Kubernetes 集群中提供了基于名称空间的资源隔离，我们可以将资源隔离到不同的名称空间中，也称为租户；例如可以 <strong>为整个命名空间设置内存或 CPU 限制</strong>，确保名称空间内的业务不能从使用更多的系统资源。</p>
<p>下列是一个 ResourceQuotas 的配置</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
spec:
  hard:
    requests.cpu: 2
    requests.memory: 1Gi
    limits.cpu: 3
    limits.memory: 2Gi
</code></pre>
<ul>
<li><code>requests.cpu</code>：名称空间中所有 Request 的最低 CPU 数量</li>
<li><code>requests.memory</code>：名称空间中所有 Request 的 最低内存数量</li>
<li><code>limits.cpu</code>：名称空间中所有 Limit 最大 CPU 数量</li>
<li><code>limits.memory</code>：名称空间中所有 Limit 最大内存量</li>
</ul>
<p><strong>ResourceQuotas</strong> 可限制名称空间的资源总量，如果我们想给里名称空间里的 Pod 配置限制可以使用 “<strong>LimitRange</strong>”</p>
<p>下列是一个 <strong>LimitRanges</strong> 的配置</p>
<pre><code class="language-yaml">apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default:
      cpu: 500m
    defaultRequest:
      cpu: 500m
    min:
      cpu: 100m
    max:
      cpu: &quot;1&quot;
    type: Container
</code></pre>
<ul>
<li><code>default</code>:  如果未指定，创建的容器将具有此值。</li>
<li><code>min</code>：创建的容器不能有小于此的限制或请求。</li>
<li><code>max</code>: 创建的容器不能有比这更大的限制或请求。</li>
</ul>
<blockquote>
<p>Note:  在默认情况下，即使未设置 <code>LimitRanges</code>，Pod  中的所有容器也会有效地请求 100m 的 CPU。</p>
</blockquote>
<h2 id="总结">总结</h2>
<ul>
<li>Request 和 Limit 是在 Kubernetes 集群中控制成本的关键配置</li>
<li>只有巧用 Request 和 Limit 才可以为集群提供最佳配额</li>
<li>专用的 容器不应该设置 Request 和 Limit，这将导致容器无法正常允许，甚至被驱逐</li>
<li>只有对 Request 和 Limit 进行精细的设置才可以使 Kubernetes 集群最佳化，否则 “弊大于利”</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Awesome kubernetes</title>
      <link>https://www.oomkill.com/2023/05/awesome-kubernetes/</link>
      <pubDate>Wed, 03 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/05/awesome-kubernetes/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="deployment">Deployment</h2>
<ul>
<li><a href="https://ranchermanager.docs.rancher.com/v2.5/how-to-guides/new-user-guides/kubernetes-clusters-in-rancher-setup/checklist-for-production-ready-clusters/recommended-cluster-architecture" target="_blank"
   rel="noopener nofollow noreferrer" >Recommended Cluster Architecture - rancher</a></li>
<li><a href="https://etcd.io/docs/v3.5/op-guide/hardware/" target="_blank"
   rel="noopener nofollow noreferrer" >Hardware recommendations - etcd</a></li>
<li><a href="https://kubernetes.io/docs/setup/best-practices/cluster-large/" target="_blank"
   rel="noopener nofollow noreferrer" >Considerations for large clusters - kubernetes cluster</a></li>
<li><a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/" target="_blank"
   rel="noopener nofollow noreferrer" >Operating etcd clusters for Kubernetes</a></li>
<li><a href="https://docs.okd.io/4.12/scalability_and_performance/recommended-host-practices.html" target="_blank"
   rel="noopener nofollow noreferrer" >Recommended performance and scalability practices</a></li>
<li><a href="https://github.com/cylonchau/kubernetes-generator" target="_blank"
   rel="noopener nofollow noreferrer" >Binary deploy script pure shell</a></li>
<li><a href="https://www.google.ru/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwif4ImmtuD-AhVoUqQEHRGkBTIQFnoECAwQAQ&amp;url=http%3A%2F%2Fthuvien.utt.edu.vn%3A8080%2Fjspui%2Fbitstream%2F123456789%2F1098%2F1%2Fthuvienpdf.commanaging-kubernetes--traffic-with-f5-nginx.pdf&amp;usg=AOvVaw3qLVyYOrwcWpZ8I1h3ddIW" target="_blank"
   rel="noopener nofollow noreferrer" >Managing Kubernetes Traffic with F5 NGINX</a></li>
<li><a href="https://github.com/Azure/eraser" target="_blank"
   rel="noopener nofollow noreferrer" >Eraser -  Cleaning up Images from Kubernetes Nodes</a></li>
<li><a href="https://etcd.io/docs/v3.5/op-guide/hardware/#example-hardware-configurations" target="_blank"
   rel="noopener nofollow noreferrer" >对于不同规模的 Kubernetes 集群所需要的 etcd 规模推荐</a></li>
<li><a href="https://hub.datree.io" target="_blank"
   rel="noopener nofollow noreferrer" >datree: allowing you to scan your k8s configs during development</a></li>
</ul>
<h2 id="performance">Performance</h2>
<ul>
<li><a href="https://zendesk.engineering/etcd-getting-30-more-write-s-318bcdbf7774" target="_blank"
   rel="noopener nofollow noreferrer" >etcd: getting 30% more write/s</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/396335220" target="_blank"
   rel="noopener nofollow noreferrer" >蚂蚁集团万级规模 K8s 集群 etcd 高可用建设之路</a></li>
<li><a href="https://hex108.gitbook.io/kubernetes-notes/zui-jia-shi-jian/components_configure" target="_blank"
   rel="noopener nofollow noreferrer" >各组件参数配置调优</a></li>
<li><a href="https://www.jiqizhixin.com/articles/2020-07-21-17" target="_blank"
   rel="noopener nofollow noreferrer" >万级K8s集群背后etcd稳定性及性能优化实践</a></li>
<li><a href="https://arthurchiao.art/blog/k8s-reliability-list-data-zh/" target="_blank"
   rel="noopener nofollow noreferrer" >K8s 集群稳定性：LIST 请求源码分析、性能评估与大规模基础服务部署调优</a></li>
</ul>
<h2 id="comparing">Comparing</h2>
<ul>
<li><a href="https://blog.palark.com/comparing-ingress-controllers-for-kubernetes/" target="_blank"
   rel="noopener nofollow noreferrer" >comparing Kubernetes ingress controller</a></li>
</ul>
<h2 id="troubleshooting">Troubleshooting</h2>
<ul>
<li><a href="https://code2life.top/2018/04/12/0023-etcd-thinking/" target="_blank"
   rel="noopener nofollow noreferrer" >一次Etcd集群宕机引发的思考</a></li>
<li><a href="https://github.com/stern/stern" target="_blank"
   rel="noopener nofollow noreferrer" >Stern:  allows you to tail multiple pods on Kubernetes</a></li>
</ul>
<h2 id="diagnosis">Diagnosis</h2>
<ul>
<li>Kubernetes 自动化诊断工具：<a href="https://github.com/k8sgpt-ai/k8sgpt-operator" target="_blank"
   rel="noopener nofollow noreferrer" >k8sgpt-operator</a></li>
<li>ktop: <a href="https://github.com/vladimirvivien/ktop/" target="_blank"
   rel="noopener nofollow noreferrer" >displays useful metrics information about kubernetes cluster</a></li>
</ul>
<h2 id="dashboard">Dashboard</h2>
<ul>
<li><a href="https://github.com/kdash-rs/kdash" target="_blank"
   rel="noopener nofollow noreferrer" >KDash - A fast and simple dashboard for Kubernetes</a></li>
</ul>
<h2 id="security">Security</h2>
<ul>
<li><a href="https://lib.jimmysong.io/kubernetes-hardening-guidance/" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes 加固指南</a></li>
<li><a href="https://github.com/derailed/popeye" target="_blank"
   rel="noopener nofollow noreferrer" >Popeye 扫描实时 Kubernetes 集群并报告已部署资源和配置的潜在问题</a></li>
</ul>
<h2 id="test">Test</h2>
<ul>
<li><a href="https://github.com/asobti/kube-monkey" target="_blank"
   rel="noopener nofollow noreferrer" >kube-monkey It randomly deletes Kubernetes (k8s) pods in the cluster encouraging and validating the development of failure-resilient services.</a></li>
</ul>
<h2 id="study">Study</h2>
<ul>
<li><a href="https://kwok.sigs.k8s.io/" target="_blank"
   rel="noopener nofollow noreferrer" >KWOK (Kubernetes With Out Kubelet)</a></li>
</ul>
<h2 id="backup">Backup</h2>
<ul>
<li><a href="https://github.com/gardener/etcd-backup-restore" target="_blank"
   rel="noopener nofollow noreferrer" >etcd 备份与恢复工具</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>kube-proxy如何保证规则的一致性</title>
      <link>https://www.oomkill.com/2023/03/ch24-kube-proxy-performance/</link>
      <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/03/ch24-kube-proxy-performance/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
</blockquote>
<h2 id="前景">前景</h2>
<p>这里谈 <code>kube-proxy</code> 如何保证规则的一致性以及提升 <code>kube-proxy</code> 性能点的地方，这也是 kubernetes 使用稳定性的一部分。</p>
<h2 id="kube-proxy-如何做到的crud">kube-proxy 如何做到的CRUD</h2>
<p><code>kube-proxy</code>  实际上与其他内置 controller 架构是相同的，实际上也属于一个 controller ，但它属于一个 service, endpoints 的可读可写的控制器，node的读控制器。对于CRUD方面，kube-proxy，在设计上分为 增/改 两方面。正如下面代码所示 <a href="pkg/proxy/ipvs/proxier.go">pkg/proxy/ipvs/proxier.go</a></p>
<pre><code class="language-go">func (proxier *Proxier) OnServiceAdd(service *v1.Service) {
	proxier.OnServiceUpdate(nil, service)
}

// OnServiceUpdate is called whenever modification of an existing service object is observed.
func (proxier *Proxier) OnServiceUpdate(oldService, service *v1.Service) {
	if proxier.serviceChanges.Update(oldService, service) &amp;&amp; proxier.isInitialized() {
		proxier.Sync()
	}
}

// OnServiceDelete is called whenever deletion of an existing service object is observed.
func (proxier *Proxier) OnServiceDelete(service *v1.Service) {
	proxier.OnServiceUpdate(service, nil)
}
</code></pre>
<p>可以看到代码最终调用的都是 <code>OnServiceUpdate</code>，最终 调用的是 <code>proxier.Sync()</code>。对于 Sync()，这里会调用在 Proxier 初始化时注入的那个函数，而 Sync() 本质上是 一个异步有限的函数管理器，这里将实现两个方面，一是，定时去触发执行这个函数；二是满足规则去触发这个函数；而 Sync() 属于条件2</p>
<p>对于注入的函数，则是 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L490" target="_blank"
   rel="noopener nofollow noreferrer" >proxier.syncProxyRules</a> ，由下列代码可以看到</p>
<pre><code class="language-go">proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs)
</code></pre>
<p>这样就是说，kube-proxy 通过 syncProxyRules 实现了整个 service 与 endpoint 的增删改查</p>
<h2 id="性能提升点1">性能提升点1</h2>
<p>由上面有限的函数管理器 runner，可以作为性能提升点，而该runner初始化时提供了minSyncPeriod, syncPeriod 两个函数，这两个函数代表的意思为，minSyncPeriod是runner允许你在最小多少时间内可以调用，如果你的集群规模大，那么则可以适当配置该参数小写，因为service的频繁更改会被这个参数限制。</p>
<h2 id="如何通过一个函数做crud">如何通过一个函数做CRUD</h2>
<p>对于增改，存在三个资源，ClusterIP, NodePort, Ingress,当这些资源被触发时，会同步这个service与endpoint，如代码所示 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1398-L1406" target="_blank"
   rel="noopener nofollow noreferrer" >syncProxyRules</a></p>
<pre><code class="language-go">if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil {
    activeIPVSServices[serv.String()] = true
    activeBindAddrs[serv.Address.String()] = true
    if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil {
        klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
    }
} else {
    klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
}
</code></pre>
<p>由上面代码可知，所有的 service 与 endpoint 的更新，都会触发 <code>Sync()</code>，而 <code>Sync()</code> 执行的是 <code>syncProxyRules()</code> ，当service有变动时，就会通过 syncService/syncEndpoint 进行同步</p>
<p>而对于删除动作来说，kube-proxy 提供了 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1626" target="_blank"
   rel="noopener nofollow noreferrer" >cleanLegacyService</a> 函数在变动做完时，进行清理遗留的service规则，如下列代码所示。</p>
<pre><code class="language-go">proxier.cleanLegacyService(activeIPVSServices, currentIPVSServices, legacyBindAddrs)
</code></pre>
<p>并且通过两个数组来维护两个 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1091-L1094" target="_blank"
   rel="noopener nofollow noreferrer" >activeIPVSServices</a> , 与 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1091-L1094" target="_blank"
   rel="noopener nofollow noreferrer" >currentIPVSServices</a> 为主，来维护删除的数据</p>
<h2 id="crud实际实现">CRUD实际实现</h2>
<p>上面了解到了删除与添加的逻辑，下面分析这些是如何进行的</p>
<p>当添加被触发时，会触发 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1936-L1974" target="_blank"
   rel="noopener nofollow noreferrer" >proxier.syncService()</a> ，首先会进行本机上ipvs规则是否存在这个规则，存在则更改，而后返回一个 error, 这个 error 取决于是否更新 endpoints，如下列代码所示</p>
<pre><code class="language-go">func (proxier *Proxier) syncService(svcName string, vs *utilipvs.VirtualServer, bindAddr bool, bindedAddresses sets.String) error {
	appliedVirtualServer, _ := proxier.ipvs.GetVirtualServer(vs)
	if appliedVirtualServer == nil || !appliedVirtualServer.Equal(vs) {
		if appliedVirtualServer == nil {
			// IPVS service is not found, create a new service
			klog.V(3).Infof(&quot;Adding new service %q %s:%d/%s&quot;, svcName, vs.Address, vs.Port, vs.Protocol)
			if err := proxier.ipvs.AddVirtualServer(vs); err != nil {
				klog.Errorf(&quot;Failed to add IPVS service %q: %v&quot;, svcName, err)
				return err
			}
		} else {
			// IPVS service was changed, update the existing one
			// During updates, service VIP will not go down
			klog.V(3).Infof(&quot;IPVS service %s was changed&quot;, svcName)
			if err := proxier.ipvs.UpdateVirtualServer(vs); err != nil {
				klog.Errorf(&quot;Failed to update IPVS service, err:%v&quot;, err)
				return err
			}
		}
	}

	// bind service address to dummy interface
	if bindAddr {
		// always attempt to bind if bindedAddresses is nil,
		// otherwise check if it's already binded and return early
		if bindedAddresses != nil &amp;&amp; bindedAddresses.Has(vs.Address.String()) {
			return nil
		}

		klog.V(4).Infof(&quot;Bind addr %s&quot;, vs.Address.String())
		_, err := proxier.netlinkHandle.EnsureAddressBind(vs.Address.String(), DefaultDummyDevice)
		if err != nil {
			klog.Errorf(&quot;Failed to bind service address to dummy device %q: %v&quot;, svcName, err)
			return err
		}
	}

	return nil
}
</code></pre>
<p>接下来通过后会 触发 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1976-L2084" target="_blank"
   rel="noopener nofollow noreferrer" >proxier.syncEndpoint()</a> 这里传入了当前的 service, 这里是为了与 IPVS 概念相吻合，如IPVS 中存在 RealServers/VirtualServers，首先会拿到本机这个VirtualServer下的所有RealServer，而后进行添加，而后对比 传入的 Endpoints 列表 与 本机这个VirtualServer下的所有RealServer，不相等的则被删除；删除的动作是一个异步操作。由 gracefuldeleteManager 维护</p>
<pre><code class="language-go">func (proxier *Proxier) syncEndpoint(svcPortName proxy.ServicePortName, onlyNodeLocalEndpoints bool, vs *utilipvs.VirtualServer) error {
	appliedVirtualServer, err := proxier.ipvs.GetVirtualServer(vs)
	if err != nil || appliedVirtualServer == nil {
		klog.Errorf(&quot;Failed to get IPVS service, error: %v&quot;, err)
		return err
	}

	// curEndpoints represents IPVS destinations listed from current system.
	curEndpoints := sets.NewString()
	// newEndpoints represents Endpoints watched from API Server.
	newEndpoints := sets.NewString()

	curDests, err := proxier.ipvs.GetRealServers(appliedVirtualServer)
	if err != nil {
		klog.Errorf(&quot;Failed to list IPVS destinations, error: %v&quot;, err)
		return err
	}
	for _, des := range curDests {
		curEndpoints.Insert(des.String())
	}

	endpoints := proxier.endpointsMap[svcPortName]

	// Service Topology will not be enabled in the following cases:
	// 1. externalTrafficPolicy=Local (mutually exclusive with service topology).
	// 2. ServiceTopology is not enabled.
	// 3. EndpointSlice is not enabled (service topology depends on endpoint slice
	// to get topology information).
	if !onlyNodeLocalEndpoints &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.ServiceTopology) &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.EndpointSliceProxying) {
		endpoints = proxy.FilterTopologyEndpoint(proxier.nodeLabels, proxier.serviceMap[svcPortName].TopologyKeys(), endpoints)
	}

	for _, epInfo := range endpoints {
		if onlyNodeLocalEndpoints &amp;&amp; !epInfo.GetIsLocal() {
			continue
		}
		newEndpoints.Insert(epInfo.String())
	}

	// Create new endpoints
	for _, ep := range newEndpoints.List() {
		ip, port, err := net.SplitHostPort(ep)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint: %v, error: %v&quot;, ep, err)
			continue
		}
		portNum, err := strconv.Atoi(port)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint port %s, error: %v&quot;, port, err)
			continue
		}

		newDest := &amp;utilipvs.RealServer{
			Address: net.ParseIP(ip),
			Port:    uint16(portNum),
			Weight:  1,
		}

		if curEndpoints.Has(ep) {
			// check if newEndpoint is in gracefulDelete list, if true, delete this ep immediately
			uniqueRS := GetUniqueRSName(vs, newDest)
			if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
				continue
			}
			klog.V(5).Infof(&quot;new ep %q is in graceful delete list&quot;, uniqueRS)
			err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS)
			if err != nil {
				klog.Errorf(&quot;Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v&quot;, ep, err)
				continue
			}
		}
		err = proxier.ipvs.AddRealServer(appliedVirtualServer, newDest)
		if err != nil {
			klog.Errorf(&quot;Failed to add destination: %v, error: %v&quot;, newDest, err)
			continue
		}
	}
	// Delete old endpoints
	for _, ep := range curEndpoints.Difference(newEndpoints).UnsortedList() {
		// if curEndpoint is in gracefulDelete, skip
		uniqueRS := vs.String() + &quot;/&quot; + ep
		if proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
			continue
		}
		ip, port, err := net.SplitHostPort(ep)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint: %v, error: %v&quot;, ep, err)
			continue
		}
		portNum, err := strconv.Atoi(port)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint port %s, error: %v&quot;, port, err)
			continue
		}

		delDest := &amp;utilipvs.RealServer{
			Address: net.ParseIP(ip),
			Port:    uint16(portNum),
		}

		klog.V(5).Infof(&quot;Using graceful delete to delete: %v&quot;, uniqueRS)
		err = proxier.gracefuldeleteManager.GracefulDeleteRS(appliedVirtualServer, delDest)
		if err != nil {
			klog.Errorf(&quot;Failed to delete destination: %v, error: %v&quot;, uniqueRS, err)
			continue
		}
	}
	return nil
}
</code></pre>
<p>删除 service 将删除所有的 RealServer，这点上面提到过，kube-proxy 通过 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L2086-L2114" target="_blank"
   rel="noopener nofollow noreferrer" >cleanLegacyService</a> 进行删除，如下列代码所示</p>
<pre><code class="language-go">func (proxier *Proxier) cleanLegacyService(activeServices map[string]bool, currentServices map[string]*utilipvs.VirtualServer, legacyBindAddrs map[string]bool) {
	isIPv6 := utilnet.IsIPv6(proxier.nodeIP)
	for cs := range currentServices {
		svc := currentServices[cs]
		if proxier.isIPInExcludeCIDRs(svc.Address) {
			continue
		}
		if utilnet.IsIPv6(svc.Address) != isIPv6 {
			// Not our family
			continue
		}
		if _, ok := activeServices[cs]; !ok {
			klog.V(4).Infof(&quot;Delete service %s&quot;, svc.String())
			if err := proxier.ipvs.DeleteVirtualServer(svc); err != nil {
				klog.Errorf(&quot;Failed to delete service %s, error: %v&quot;, svc.String(), err)
			}
			addr := svc.Address.String()
			if _, ok := legacyBindAddrs[addr]; ok {
				klog.V(4).Infof(&quot;Unbinding address %s&quot;, addr)
				if err := proxier.netlinkHandle.UnbindAddress(addr, DefaultDummyDevice); err != nil {
					klog.Errorf(&quot;Failed to unbind service addr %s from dummy interface %s: %v&quot;, addr, DefaultDummyDevice, err)
				} else {
					// In case we delete a multi-port service, avoid trying to unbind multiple times
					delete(legacyBindAddrs, addr)
				}
			}
		}
	}
}
</code></pre>
<h2 id="性能提升点2">性能提升点2</h2>
<p>由上面的讲解可知，CRUD动作是每一个事件 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1398-L1406" target="_blank"
   rel="noopener nofollow noreferrer" >syncProxyRules</a> 被触发时都会进行执行，而删除动作存在多组循环（如构建维护的两个列表；进行循环删除）即每一次 Endpoints 变动也会触发 大量的 Service 的循环，从而检测 是否由遗留的Service资源，而这个操作保留到kubernetes 1.26版本</p>
<p>假设你的集群节点是5000个，service资源是两万个，那么当你更新一个Service资源循环的次数，会至少循环多达数万次（Service, EndpointSpilt, currentServices, NodeIP, Ingress）其中无用的为currentServices，因为这个只有在删除Service本身才会有效（如果存在20000个service，其中currentServices在构建与对比的过程超过4万次）</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>扫盲Kubernetes负载均衡 - 从Ingress聊到LB</title>
      <link>https://www.oomkill.com/2023/02/kubernetes-lb/</link>
      <pubDate>Sun, 26 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/02/kubernetes-lb/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="概述">概述</h2>
<p>在之前有一个系列提到了扩展proxier，但可能细心的同学注意到，作为一个外部的LB，市场上存在一些开源的为kubernetes集群提供的LB，这不是舍近求远吗？而 Google工程师 <em>Adam Dunstan</em> 的 文章 <sup><a href="#1">[1]</a></sup> 对比了这些LB的区别（中文翻译备份 <sup><a href="#2">[2]</a></sup> ），例如：</p>
<ul>
<li><a href="https://metallb.universe.tf/" target="_blank"
   rel="noopener nofollow noreferrer" >MetalLB</a>：最流行的 负载均衡控制器</li>
<li><a href="https://purelb.gitlab.io/docs/" target="_blank"
   rel="noopener nofollow noreferrer" >PureLB</a>：新产品 (文章作者 Adam Dunstan 参与了 PureLB的开发工作)</li>
<li><a href="https://github.com/kubesphere/openelb" target="_blank"
   rel="noopener nofollow noreferrer" >OpenELB</a>：相对较新的产品，最初该LB仅关注路由方向</li>
</ul>
<p>文章提出了一个LB实现的基本目标为：必要的简单网络组件，与可扩展的集群操作</p>
<ul>
<li>启动受控的集群service/应用的外部访问</li>
<li>外部资源的预配置</li>
<li>易于整合自动化的工作流程（CI/CD）</li>
</ul>
<p>那么这些LB与 kube-proxy 甚至于 IPVS/IPTables 有什么区别呢？</p>
<p>这些LB的核心是为集群service提供一个外部IP，而service功能本身还是由 kube-proxy,IPVS 提供，在这点 MetalLB 介绍中提到了这个问题</p>
<blockquote>
<p>In layer 2 mode, all traffic for a service IP goes to one node. From there, <code>kube-proxy</code> spreads the traffic to all the service’s pods. <sup><a href="#3">[3]</a></sup></p>
<p>After the packets arrive at the node, <code>kube-proxy</code> is responsible for the final hop of traffic routing, to get the packets to one specific pod in the service.  <sup><a href="#4">[4]</a></sup></p>
</blockquote>
<p>通过kubernetes service 资源方向表示，是为EXTERNAL-IP部分分配一个IP地址，而从来不是说内部的LB</p>
<pre><code class="language-bash">$ kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   192.168.0.1     &lt;none&gt;        443/TCP   196d
</code></pre>
<p>关于MetalLb的使用可以参考视频：<a href="https://www.youtube.com/watch?v=xYiYIjlAgHY" target="_blank"
   rel="noopener nofollow noreferrer" >Set up MetalLB Load Balancing for Bare Metal Kubernetes</a></p>
<p>正如注解所说，工作与L2的模式下，流量会到达一个Node,接下来通过kube-proxy广播至所有的Pod，这种模式下是否可以做到HA，还有待测试。</p>
<p>接下来提到 Proixer，Proixer是对于集群（内）正常工作的一个保证，最基础的一点则是Kubernetes service 需要每个Pod可以访问到，所以 kube-proxy 则完全不同于 kubernetes LB</p>
<h2 id="总结">总结</h2>
<p>Kubernetes LB是Kubernetes的扩展功能，主要特点体现在下列方面：</p>
<ul>
<li>LB 是要作用是为service提供一个外部IP</li>
<li>通常情况下，LB支持的都是 L2, L3 网络，而非传统的L4, L7 LB</li>
<li>kube-proxy并不可以被这类LB所替代，因为这类LB的端点是到service</li>
<li>目前开源的 kube-proxy repleacement 应该只有 eBPF</li>
</ul>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Kubernetes Ingress 架构说明</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://archive.ph/XDM1A#selection-1538.0-1547.110" target="_blank"
   rel="noopener nofollow noreferrer" ><em>「译文」比较开源 k8s LoadBalancer-MetalLB vs PureLB vs OpenELB</em></a></p>
<p><sup id="3">[3]</sup> <a href="https://metallb.universe.tf/concepts/layer2/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>METALLB IN LAYER 2 MODE</em></a></p>
<p><sup id="4">[4]</sup> <a href="https://metallb.universe.tf/concepts/bgp/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>METALLB IN BGP MODE</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes service - EndpointSlices做了什么？</title>
      <link>https://www.oomkill.com/2023/02/ch18-endpointslices/</link>
      <pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/02/ch18-endpointslices/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="endpoint">Endpoint</h2>
<p>Endpoints 就是 service 中后端的server，通常来说 endpoint 与 service是关联的，例如下面的一个endpoints 资源。</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Endpoints
metadata:
  name: nginx
subsets:
  - addresses:
      - ip: 172.17.0.2
      - ip: 172.17.0.3
    ports:
      - port: 80
        name: &quot;111&quot; # 多个端口需要用name
      - port: 88
        name: &quot;222&quot;
</code></pre>
<p>而 Endpoints 资源是由控制平面的  Endpoints controller 进行管理的，主要用于将外部server引入至集群内时使用的，例如Kube-apiserver 在集群外的地址，以及external service所需要创建的。</p>
<p>我们看到 Endpoints controller 代码中，在对 该 informer 监听的包含 service 与 Pod，位于 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/controller/endpoint/endpoints_controller.go#L79-L128" target="_blank"
   rel="noopener nofollow noreferrer" >NewEndpointController()</a></p>
<pre><code class="language-go">// NewEndpointController returns a new *EndpointController.
func NewEndpointController(podInformer coreinformers.PodInformer, serviceInformer coreinformers.ServiceInformer,
	endpointsInformer coreinformers.EndpointsInformer, client clientset.Interface, endpointUpdatesBatchPeriod time.Duration) *EndpointController {
	broadcaster := record.NewBroadcaster()
	broadcaster.StartStructuredLogging(0)
	broadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl{Interface: client.CoreV1().Events(&quot;&quot;)})
	recorder := broadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: &quot;endpoint-controller&quot;})

	if client != nil &amp;&amp; client.CoreV1().RESTClient().GetRateLimiter() != nil {
		ratelimiter.RegisterMetricAndTrackRateLimiterUsage(&quot;endpoint_controller&quot;, client.CoreV1().RESTClient().GetRateLimiter())
	}
	e := &amp;EndpointController{
		client:           client,
		queue:            workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;endpoint&quot;),
		workerLoopPeriod: time.Second,
	}

	serviceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: e.onServiceUpdate,
		UpdateFunc: func(old, cur interface{}) {
			e.onServiceUpdate(cur)
		},
		DeleteFunc: e.onServiceDelete,
	})
	e.serviceLister = serviceInformer.Lister()
	e.servicesSynced = serviceInformer.Informer().HasSynced

	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    e.addPod,
		UpdateFunc: e.updatePod,
		DeleteFunc: e.deletePod,
	})
	e.podLister = podInformer.Lister()
	e.podsSynced = podInformer.Informer().HasSynced

	endpointsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		DeleteFunc: e.onEndpointsDelete,
	})
	e.endpointsLister = endpointsInformer.Lister()
	e.endpointsSynced = endpointsInformer.Informer().HasSynced

    ....
    
}
</code></pre>
<h2 id="endpointslices-supa-href11asup">EndpointSlices <sup><a href="#1">[1]</a></sup></h2>
<p>EndpointSlices 是提供为集群内用于替换 Endpoints 资源的一种灵活并具有扩展性的一种资源，由控制平面的 EndpointSlices Controller 来创建和管理的，默认情况下 EndpointSlices Controller 创建和管理的EndpointSlices 资源将不大于100个Endpoints；可以通过 <code>kube-controller-manager</code> 的参数  <code>--max-endpoints-per-slice</code>  设置，该参数最大为1000 <sup><a href="#2">[2]</a></sup></p>
<p>通常情况下无需自行创建该资源，因为在创建 service 资源时 通常是通过 label 来匹配到对应的 backend server</p>
<p>下面是一个完整的 EndpointSlices 资源清单</p>
<pre><code class="language-yaml">addressType: IPv4
apiVersion: discovery.k8s.io/v1beta1 #注意版本 1.21后是 v1
endpoints:
- addresses:
  - 192.168.1.241
  conditions:
    ready: true
  targetRef:
    kind: Pod
    name: netbox-ff6dd9445-kxr4s
    namespace: default
    resourceVersion: &quot;1994535&quot;
  topology:
    kubernetes.io/hostname: master-machine
- addresses:
  - 192.168.1.242
  conditions:
    ready: true
  targetRef:
    kind: Pod
    name: netbox-ff6dd9445-566tj
    namespace: default
  topology:
    kubernetes.io/hostname: master-machine
kind: EndpointSlice
metadata:
  annotations:
    endpoints.kubernetes.io/last-change-trigger-time: &quot;2023-02-24T22:40:20+08:00&quot;
  labels:
    endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io
    kubernetes.io/service-name: netbox
  name: netbox-l489z
  namespace: default
ports:
- name: &quot;&quot;
  port: 80
  protocol: TCP
</code></pre>
<p>在代码中 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/apis/discovery/types.go#L29-L54" target="_blank"
   rel="noopener nofollow noreferrer" >EndpointSlices</a> 资源是这么呈现的，可以看到主要的就是包含一组 Endpoints 资源</p>
<pre><code class="language-go">type EndpointSlice struct {
	metav1.TypeMeta
	metav1.ObjectMeta
	AddressType AddressType
	Endpoints []Endpoint
	Ports []EndpointPort
}
</code></pre>
<h2 id="endpointslices-在-kube-proxy中的应用">EndpointSlices 在 kube-proxy中的应用</h2>
<p>Google 工程师 <em>Rob Scott</em> 在2020年一文 <sup><a href="#3">[3]</a></sup> 中提到了 EndpointSlices 的作用，从kubernetes 1.19 开始EndpointSlices 默认被开启，而开启后的kube-proxy将使用 EndpointSlices 读取集群内的service的 Endpoints，而这个最大的变化就是『拓扑感知路由』(<em><strong>Topology Aware Routing</strong></em>)</p>
<p><em>Rob Scott</em> 在文中提到 EndpointSlice API 是为了提升 Endpoints API 的限制，例如，etcd的存储大小，以及pod规模变动时最大产生的超过22TB数据的问题</p>
<p>而这些问题可以通过文中变化图来说明，开启功能后会将所有匹配到的 Endpoint，划分为多个EndpointSlices，而在大规模集群环境场景下，每次的变更只需要修改其中一个 EndpointSlices 即可，这将带给 kube-proxy 提供超Endpoint模式 10倍的性能</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/endpoint-slices.png" alt="端点切片" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes EndpointSlices </center>
<center><em>Source：</em>https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices</center><br>
<blockquote>
<p><em><strong>Notes</strong></em>：该文中没有提到的一点是：”EndpointSlices资源解决的是集群内的service节点问题，如你使用了endpoint类资源，那么不会触发到EndpointSlices的资源，这部分在 <code>kube-proxy</code> 源码中可以很清晰的看到</p>
</blockquote>
<p>下面的 kube-proxy 日志可以看到获取 server是通过 Endpoints 还是 EndpointSlices</p>
<pre><code class="language-log">endpointslicecache.go:322] Setting endpoints for &quot;default/netbox&quot; to [192.168.1.241:80 192.168.1.242:80]
10008 proxier.go:1057] Syncing ipvs Proxier rules
10008 iptables.go:343] running iptables-save [-t filter]
10008 iptables.go:343] running iptables-save [-t nat]
10008 ipset.go:173] Successfully add entry: 192.168.1.241,tcp:80,192.168.1.241 to ip set: KUBE-LOOP-BACK
</code></pre>
<h2 id="总结">总结</h2>
<ul>
<li>Endpoints 与 EndpointSlices 均是为service提供端点的</li>
<li>Service规模越大，那么Endpoints中的 Pod 数量越大，传输的 EndPoints 对象就越大。集群中 Pod 更改的频率越高，也意味着传输在网络中发生的频率就越高</li>
<li>Endpoints 对象在大规模集群场景下存在下列问题：
<ul>
<li>增加网络流量</li>
<li>超大规模的 service 理论上会无法存储 该 Endpoints</li>
<li>处理Endpoints资源的 worker 会消耗更多的计算资源</li>
<li>隐性增加对控制平面的影响，service的可扩展性将降低</li>
</ul>
</li>
<li>Endpointslices 解决了：
<ul>
<li>部分更新，更少的网络流量</li>
<li>Worker 处理 Endpoints 更新所需的资源更少</li>
<li>减少对控制平面的影响，提升的性能和 service 规模</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>EndpointSlices</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/#address-types" target="_blank"
   rel="noopener nofollow noreferrer" ><em>EndpointSlice API</em></a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/#address-types" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Scaling Kubernetes Networking With EndpointSlices</em></a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/#scalability-limitations-of-the-endpoints-api" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Scalability Limitations of the Endpoints API</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？</title>
      <link>https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/</link>
      <pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p>本文是关于Kubernetes service解析的第四章</p>
<ul>
<li><a href="https://cylonchau.github.io/kubernetes-service-controller.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - 你真的理解service吗?</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-endpointslices.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - EndpointSlices做了什么？</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-kube-proxy-code.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - kube-proxy架构分析</a></li>
<li>深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？</li>
</ul>
<p>所有关于Kubernetes service 部分代码上传至仓库 <a href="https://github.com/cylonchau/kube-haproxy" target="_blank"
   rel="noopener nofollow noreferrer" >github.com/cylonchau/kube-haproxy</a></p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>在前两部分中，学习了一些 service,于kube-proxy在设计架构，但存在扩展问题将引入了一些问题：</p>
<ul>
<li>为什么需要了解这部分内容呢？</li>
<li>与传统架构有什么区别呢？</li>
<li>于eBPF 的 cilium又有什么区别呢？</li>
<li>既然eBPF可以做到，那为什么要这部分内容呢？</li>
</ul>
<p>接下来的内容将围绕这四个问题展开来讲，而不是代码的讲解，代码可以看置顶</p>
<h2 id="ipvs与iptables在kubernetes中应用时的问题">IPVS与iptables在kubernetes中应用时的问题</h2>
<p>对于在使用了kubernetes用户以及了解 kube-proxy 架构后，知道当集群规模过大时，service必将增多，而一个service未必是一条iptables/ipvs规则，对于kubernetes这种分布式架构来说，集群规模越大，集群状态就越不可控，尤其时kube-proxy。</p>
<p>为什么单指kube-proxy呢？想想可以知道，pod的故障 或 node 的故障对于kubernetes集群来说却不是致命的，因为 这些资源集群中存在 避免方案，例如Pod的驱逐。而kube-proxy或iptables/IPVS问题将导致服务的不可控 『抖动』例如规则生成的快慢和Pod就绪的快慢不一致，部分节点不存在 service 此时服务必然抖动。</p>
<p>再例如 iptables/IPVS 排查的难度对于普通运维工程师或开发工程师的技术水平有很高的要求，网上随处可见分析该类问题的帖子：</p>
<ul>
<li>
<p><a href="https://www.bilibili.com/video/BV1yK411V7oa/?spm_id_from=333.337.search-card.all.click&amp;vd_source=80a7f916d4f5b3fd494735dbc609331f" target="_blank"
   rel="noopener nofollow noreferrer" >kube-proxy源码分析与问题定位</a></p>
</li>
<li>
<p><a href="http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/%E5%8A%A0%E9%A4%9001%20%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90%EF%BC%9A%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E6%B5%B7%E9%87%8FIPVS%E8%A7%84%E5%88%99%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%BD%91%E7%BB%9C%E5%BB%B6%E6%97%B6%E6%8A%96%E5%8A%A8%E9%97%AE%E9%A2%98%EF%BC%9F.md" target="_blank"
   rel="noopener nofollow noreferrer" >案例分析：怎么解决海量IPVS规则带来的网络延时抖动问题？</a></p>
</li>
<li>
<p><a href="https://imroc.cc/kubernetes/networking/faq/ipvs-conn-reuse-mode.html#ipvs-%e8%bf%9e%e6%8e%a5%e5%a4%8d%e7%94%a8%e5%bc%95%e5%8f%91%e7%9a%84%e7%b3%bb%e5%88%97%e9%97%ae%e9%a2%98" target="_blank"
   rel="noopener nofollow noreferrer" >ipvs 连接复用引发的系列问题</a></p>
</li>
<li>
<p><a href="https://www.diva-portal.org/smash/get/diva2:1610208/FULLTEXT01.pdf" target="_blank"
   rel="noopener nofollow noreferrer" >Investigating Causes of Jitter in Container Networking</a></p>
</li>
<li>
<p><a href="https://main.qcloudimg.com/raw/document/intl/product/pdf/457_37358_en.pdf" target="_blank"
   rel="noopener nofollow noreferrer" >ContainerNative network LoadBalancer IPVS jitter</a></p>
</li>
</ul>
<p>对于上述问题，相信遇到了很难定位处理，虽然现在已fixed，并有eBPF技术的加入减少了此类问题的发生，但是eBPF实际同理于IPVS 都是需要对Linux内核有一定了解后才可以，这也就是为什么需要了解这部分</p>
<h2 id="如果需要自定义proxier为什么会解决这个问题">如果需要自定义proxier为什么会解决这个问题</h2>
<p>这里就是放大到kubernetes意外的传统架构中，当直接部署于Linux系统上使用nginx等传统LB时就很少有人提到这些问题了，而这些问题存在一个关键字「Container」；而引发这个问题的则是 service。去除 service 的功能，传统架构于Kubernetes架构部署的应用则是相同的，只是区分了名称空间。</p>
<p>抓入关键的核心之后就做接下来的事情了，我称之为「shed kube-proxy, fetch service」；即把service提取到集群外部的LB之上，例如F5, nginx等。</p>
<p>这里会存在一个疑问：「这个不是ingress吗？」，这个问题会在下一章讲到 <a href="https://cylonchau.github.io/kubernetes-auditing.html" target="_blank"
   rel="noopener nofollow noreferrer" >proxier与ingress有什么区别?</a></p>
<h2 id="软件的设计">软件的设计</h2>
<p>既然拿到了核心问题就该定义软件工作模式，这里将软件架构设计为三种：</p>
<ul>
<li>only fetch：任然需要 kube-proxy 组件，通过定义 contoller 将流量引入，即不过service，这种场景不会破坏现有的集群架构，从而去除service的功能，如果需要service功能配置外部service即可</li>
<li>SK (similar kube-proxy)：通过效仿kube-proxy + ipvs架构，将LB于proxier部署在每个worker节点上，让浏览都走本地</li>
<li>replacement kube-proxy：完全取代kube-proxy 这于cilium类似了，但不同的是，<strong>proxier</strong> 可以于 <code>kube-controller-manager</code>；<code>kube-scheduler</code> 作为控制平面为集群提供 <code>service </code> 功能，而无需为所有worker节点都部署一个 <code>kube-proxy</code> 或 <code>cilium</code> 这种架构</li>
</ul>
<h2 id="最后一个问题">最后一个问题</h2>
<p>此时可以引入最后一个问题了：「既然eBPF可以做到，那为什么要这部分内容呢？」。</p>
<p>答：其一简单，每个运维人员无需额外知识都可以对 service 问题进行排错，简便了运维复杂度。另外这一部分其实是对于完整企业生态来讲，统一的流量转发平台是所必须的，有了这个就不需要单独的 service 功能了</p>
<h2 id="实践基于haproxy的proxier">实践：基于haproxy的proxier</h2>
<p>在扩展proxier需要对 kube-proxy 有一定的了解，并且，kube-proxy 在可扩展性来说做的也是相当不错的，我们只需要实现一个 proxier.go 就可以基本上完成了对 kube-proxy ；而 proxier.go 的核心方法只需要三个函数即可（==这里是根据iptables/ipvs的设计进行的，也可以整合为一个方法==）</p>
<p>除了这三个函数外，其他的函数全都是 kube-proxy 已经实现好的通用的，这里直接使用或者按照其他内置proxier的方法即可</p>
<h3 id="满足条件">满足条件</h3>
<ul>
<li>haproxy工作与proxier相同的节点，可以是集群内也可以是集群外，整个集群只需要一个</li>
<li>实现方法：syncProxyRules(), syncService(),  syncEndpoint()</li>
</ul>
<p>查看当前的service</p>
<pre><code class="language-bash">$ kubectl get endpointslices
NAME           ADDRESSTYPE   PORTS   ENDPOINTS                     AGE
kubernetes     IPv4          6443    10.0.0.4                      195d
netbox-l489z   IPv4          80      192.168.1.241,192.168.1.242   2d1h
</code></pre>
<p>查看service 配置</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: netbox
spec:
  clusterIP: 192.168.129.5
  ports:
  - port: 88
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
</code></pre>
<p>通过 proxier 生成了对应的 backend 与 frontend，这样就可以通过 haproxy 作为一个外部LB来跨过 service 与 IPVS/IPTables，通过这种情况下，我们可以将集群拉出一个平面至传统架构上，而又不影响集群的功能</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20230226234851398.png" alt="image-20230226234851398" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>在这种场景下需要注意的是：</p>
<ul>
<li>OF模式下，我们需要 kube-proxy 组件，而使用 kube-proxy 组件</li>
<li>所有模式下，haproxy worker和kubernetes nodes需处于一个网络平面</li>
<li>非OF模式下需要自行修改 <code>kube-apiserver</code> 源代码（主要是使kubernetes service分配机制）</li>
</ul>
<h2 id="proxier与ingress的区别">Proxier与Ingress的区别</h2>
<p>肯定有人会问，kubernetes提供了Ingress功能不是和这个一样吗？</p>
<p>答：对比一个LB是Proxier还是Ingress最好的区别就是“舍去kube-proxy”可以工作正常吗？</p>
<p>而kubernetes官方也提供说明，Ingress的后端是service，service的后端则是IPVS/IPTables，而IPVS的后端才是Pod；相对于Proxier LB，他的后端则直接是Pod，跨越了Service。</p>
<ul>
<li>Kubernetes Ingress 架构说明 <sup><a href="#1">[1]</a></sup></li>
<li>Traefik Ingress 架构说明 <sup><a href="#2">[2]</a></sup></li>
<li>APISIX Ingress 架构说明 <sup><a href="#3">[3]</a></sup></li>
</ul>
<p>而相对的本文的学习思路，haproxy官方提供了对应的解决方案 <sup><a href="#4">[4]</a></sup> ；而由此，可以灵活的为Kubernetes提供更多的LB方案</p>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Kubernetes Ingress 架构说明</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://traefik.io/solutions/kubernetes-ingress/#architecture" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Traefik Ingress 架构说明</em></a></p>
<p><sup id="3">[3]</sup> <a href="https://github.com/apache/apisix-ingress-controller" target="_blank"
   rel="noopener nofollow noreferrer" ><em>APISIX Ingress 架构说明</em></a></p>
<p><sup id="4">[4]</sup> <a href="https://haproxy-ingress.github.io/docs/examples/external-haproxy/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>External haproxy</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes service - kube-proxy架构分析</title>
      <link>https://www.oomkill.com/2023/02/ch19-kube-proxy-code/</link>
      <pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/02/ch19-kube-proxy-code/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="前提概述">前提概述</h2>
<p>kubernetes集群中运行在每个Worker节点上的组件 <strong>kube-proxy</strong>，本文讲解的是如何快速的了解 kube-proxy 的软件架构，而不是流程的分析，专注于 proxy 层面的设计讲解，而不会贴大量的代码</p>
<h2 id="kube-proxy软件设计">kube-proxy软件设计</h2>
<p>kube-proxy 在设计上分为三个模块 server 于 <a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/kube-proxy" target="_blank"
   rel="noopener nofollow noreferrer" >proxy</a>：</p>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/kube-proxy" target="_blank"
   rel="noopener nofollow noreferrer" >server</a>: 是一个常驻进程用于处理service的事件</li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/v1.24.5/pkg/proxy" target="_blank"
   rel="noopener nofollow noreferrer" >proxy</a>: 是 kube-proxy 的工作核心，实际上的角色是一个 service controller，通过监听 node, service, endpoint 而生成规则</li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.24.5/pkg/proxy/ipvs/proxier.go" target="_blank"
   rel="noopener nofollow noreferrer" >proxier</a>: 是实现service的组件，例如iptables, ipvs&hellip;.</li>
</ul>
<h2 id="如何快速读懂kube-proxy源码">如何快速读懂kube-proxy源码</h2>
<p>要想快速读懂 kube-proxy 源码就需要对 kube-proxy 设计有深刻的了解，例如需要看 kube-proxy 的实现，我们就可以看 proxy的部分，下列是 proxy 部分的目录结构</p>
<pre><code class="language-bash">$ tree -L 1
.
├── BUILD
├── OWNERS
├── apis
├── config
├── healthcheck
├── iptables
├── ipvs
├── metaproxier
├── metrics
├── userspace
├── util
├── winuserspace
├── winkernel
├── doc.go
├── endpoints.go
├── endpoints_test.go
├── endpointslicecache.go
├── endpointslicecache_test.go
├── service.go
├── service_test.go
├── topology.go
├── topology_test.go
└── types.go
</code></pre>
<ul>
<li>目录 ipvs, iptables, 就是所知的 kube-proxy 提供的两种 load balancer</li>
<li>目录 apis, 则是kube-proxy 配置文件资源类型的定义，<code>--config=/etc/kubernetes/kube-proxy-config.yaml</code> 所指定问题的shema</li>
<li>目录config: 定义了每种资源的handler需要实现什么</li>
<li>service.go, endpoints.go：是controller的实现</li>
<li>type.go: 是每个资源的interface定义，例如：
<ul>
<li>Provider: 规定了每个proxier需要实现什么</li>
<li>ServicePort: service 控制器需要实现什么</li>
<li>Endpoint: service 控制器需要实现什么</li>
</ul>
</li>
</ul>
<p>上述是整个 proxy 的一级结构</p>
<h2 id="service-controller">service controller</h2>
<p>service控制器换句话说，就是工作内容类似于kubernetes集群中的pod控制器那些，所作的工作就是监听对应资源做出相应事件处理，而这个处理被定义为 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/types.go#L30-L41" target="_blank"
   rel="noopener nofollow noreferrer" >handler</a></p>
<pre><code class="language-go">type Provider interface {
	config.EndpointsHandler
	config.EndpointSliceHandler
	config.ServiceHandler
	config.NodeHandler

	// Sync immediately synchronizes the Provider's current state to proxy rules.
	Sync()
	// SyncLoop runs periodic work.
	// This is expected to run as a goroutine or as the main loop of the app.
	// It does not return.
	SyncLoop()
}
</code></pre>
<p>由上面代码可以看到，每一个Provider 即 proxier （用于实现的LB的控制器）都需要包含对应资源的事件处理函数于 一个 <code>Sync()</code> 和 <code>SyncLoop()</code>，所以这里将总结为 controller 而不是用于这里给到的术语</p>
<p>同理，其他类型的 controller 则是相同与 service controller</p>
<h2 id="深入理解proxier">深入理解proxier</h2>
<p>这里将以 ipvs 为例，如图所示，这将是一个 proxier 的实现，而 proxier.go 则是真实的 proxier 实现</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20230212221556051.png" alt="image-20230212221556051" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图（详细）</center>
<p>而 ipvs 的 <a href="https://github.com/kubernetes/kubernetes/blob/v1.24.5/pkg/proxy/ipvs/proxier.go" target="_blank"
   rel="noopener nofollow noreferrer" >proxier</a> 则是如下定义的</p>
<pre><code class="language-go">type Proxier struct {
	// endpointsChanges and serviceChanges contains all changes to endpoints and
	// services that happened since last syncProxyRules call. For a single object,
	// changes are accumulated, i.e. previous is state from before all of them,
	// current is state after applying all of those.
	endpointsChanges *proxy.EndpointChangeTracker
	serviceChanges   *proxy.ServiceChangeTracker

	mu           sync.Mutex // protects the following fields
	serviceMap   proxy.ServiceMap
	endpointsMap proxy.EndpointsMap
	portsMap     map[utilproxy.LocalPort]utilproxy.Closeable
	nodeLabels   map[string]string
	// endpointsSynced, endpointSlicesSynced, and servicesSynced are set to true when
	// corresponding objects are synced after startup. This is used to avoid updating
	// ipvs rules with some partial data after kube-proxy restart.
	endpointsSynced      bool
	endpointSlicesSynced bool
	servicesSynced       bool
	initialized          int32
	syncRunner           *async.BoundedFrequencyRunner // governs calls to syncProxyRules

	// These are effectively const and do not need the mutex to be held.
	syncPeriod    time.Duration
	minSyncPeriod time.Duration
	// Values are CIDR's to exclude when cleaning up IPVS rules.
	excludeCIDRs []*net.IPNet
	// Set to true to set sysctls arp_ignore and arp_announce
	strictARP      bool
	iptables       utiliptables.Interface
	ipvs           utilipvs.Interface
	ipset          utilipset.Interface
	exec           utilexec.Interface
	masqueradeAll  bool
	masqueradeMark string
	localDetector  proxyutiliptables.LocalTrafficDetector
	hostname       string
	nodeIP         net.IP
	portMapper     utilproxy.PortOpener
	recorder       record.EventRecorder

	serviceHealthServer healthcheck.ServiceHealthServer
	healthzServer       healthcheck.ProxierHealthUpdater

	ipvsScheduler string
	// Added as a member to the struct to allow injection for testing.
	ipGetter IPGetter
	// The following buffers are used to reuse memory and avoid allocations
	// that are significantly impacting performance.
	iptablesData     *bytes.Buffer
	filterChainsData *bytes.Buffer
	natChains        *bytes.Buffer
	filterChains     *bytes.Buffer
	natRules         *bytes.Buffer
	filterRules      *bytes.Buffer
	// Added as a member to the struct to allow injection for testing.
	netlinkHandle NetLinkHandle
	// ipsetList is the list of ipsets that ipvs proxier used.
	ipsetList map[string]*IPSet
	// Values are as a parameter to select the interfaces which nodeport works.
	nodePortAddresses []string
	// networkInterfacer defines an interface for several net library functions.
	// Inject for test purpose.
	networkInterfacer     utilproxy.NetworkInterfacer
	gracefuldeleteManager *GracefulTerminationManager
}
</code></pre>
<p>再看这个结构体的 structure，发现他实现了上述提到的 Handler 和 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L843-L849" target="_blank"
   rel="noopener nofollow noreferrer" >Sync()</a></p>
<p>可以看到 Sync() 的实现是调用 runner.Run()</p>
<pre><code class="language-go">func (proxier *Proxier) Sync() {
	if proxier.healthzServer != nil {
		proxier.healthzServer.QueuedUpdate()
	}
	metrics.SyncProxyRulesLastQueuedTimestamp.SetToCurrentTime()
	proxier.syncRunner.Run()
}
</code></pre>
<p>而 handler 中的任意事件的触发则是调用 Sync()</p>
<pre><code class="language-go">// handler 不同的事件均指向 On{rs}Update() 函数
func (proxier *Proxier) OnEndpointsAdd(endpoints *v1.Endpoints) {
	proxier.OnEndpointsUpdate(nil, endpoints)
}
// OnEndpointsDelete is called whenever deletion of an existing endpoints object is observed.
func (proxier *Proxier) OnEndpointsDelete(endpoints *v1.Endpoints) {
	proxier.OnEndpointsUpdate(endpoints, nil)
}

...

// 而 update 调用的则是 Sync()
func (proxier *Proxier) OnEndpointsUpdate(oldEndpoints, endpoints *v1.Endpoints) {
	if proxier.endpointsChanges.Update(oldEndpoints, endpoints) &amp;&amp; proxier.isInitialized() {
		proxier.Sync()
	}
}
</code></pre>
<p>到了这里，明了了 runner 才是这个 proxier 的核心，被定义于 proxier 结构图的 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L232" target="_blank"
   rel="noopener nofollow noreferrer" >syncRunner</a> 在初始化时被注入了函数 syncProxyRules()</p>
<pre><code class="language-go">func NewProxier(ipt utiliptables.Interface,

...
	
    proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs)
	proxier.gracefuldeleteManager.Run()
	return proxier, nil
}
</code></pre>
<h2 id="syncproxyrules">syncProxyRules()</h2>
<p>而这个 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1004-L1621" target="_blank"
   rel="noopener nofollow noreferrer" >syncProxyRules()</a> 则是完成了整个 ipvs 以及 service 的生命周期</p>
<p>对于了解kubernetes架构的同学来说，kube-proxy 完成的功能就是 ipvs 的规则管理，那么换句话说就是干预 ipvs 规则的生命周期，也就是分析函数 syncProxyRules() 是如何干预这些规则的。</p>
<p>syncProxyRules() 将动作分为两部分，一是对 ipvs 资源的增改，二是对资源的销毁；引入完概念后，就开始进行分析吧。</p>
<p>600多行的代码看起来很困难，那就拆分成步骤进行分析</p>
<h3 id="step1-前期准备工作">step1 前期准备工作</h3>
<p>为什么这么叫第一部分呢？看下列代码就知道，做的工作和 ipvs rules 没多大关系</p>
<pre><code class="language-go">func (proxier *Proxier) syncProxyRules() {
    // 互斥锁
	proxier.mu.Lock()
	defer proxier.mu.Unlock()
	
	// don't sync rules till we've received services and endpoints
    // 在等待接收完信息前，不同步收到的 services和endpoints资源
	if !proxier.isInitialized() {
		klog.V(2).Info(&quot;Not syncing ipvs rules until Services and Endpoints have been received from master&quot;)
		return
	}

	// Keep track of how long syncs take.
    // 记录同步耗时
	start := time.Now()
	defer func() {
		metrics.SyncProxyRulesLatency.Observe(metrics.SinceInSeconds(start))
		klog.V(4).Infof(&quot;syncProxyRules took %v&quot;, time.Since(start))
	}()
	// 获取本地多个IP地址
	localAddrs, err := utilproxy.GetLocalAddrs()
	if err != nil {
		klog.Errorf(&quot;Failed to get local addresses during proxy sync: %v, assuming external IPs are not local&quot;, err)
	} else if len(localAddrs) == 0 {
		klog.Warning(&quot;No local addresses found, assuming all external IPs are not local&quot;)
	}

	localAddrSet := utilnet.IPSet{}
	localAddrSet.Insert(localAddrs...)

	// We assume that if this was called, we really want to sync them,
	// even if nothing changed in the meantime. In other words, callers are
	// responsible for detecting no-op changes and not calling this function.
    // 这两步骤正如注释所讲，如果在资源修改的前提下需要同步，也就是update操作包含了增改
	serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges)
	endpointUpdateResult := proxier.endpointsMap.Update(proxier.endpointsChanges)
	// 陈腐的UDP信息处理
	staleServices := serviceUpdateResult.UDPStaleClusterIP
    // merge stale services gathered from updateEndpointsMap
	for _, svcPortName := range endpointUpdateResult.StaleServiceNames {
		if svcInfo, ok := proxier.serviceMap[svcPortName]; ok &amp;&amp; svcInfo != nil &amp;&amp; conntrack.IsClearConntrackNeeded(svcInfo.Protocol()) {
			klog.V(2).Infof(&quot;Stale %s service %v -&gt; %s&quot;, strings.ToLower(string(svcInfo.Protocol())), svcPortName, svcInfo.ClusterIP().String())
			staleServices.Insert(svcInfo.ClusterIP().String())
			for _, extIP := range svcInfo.ExternalIPStrings() {
				staleServices.Insert(extIP)
			}
		}
	}
</code></pre>
<h3 id="step2构建ipvs规则">step2：构建IPVS规则</h3>
<p>首先会经历一些预处理的操作 这部分掠过了 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1042-L1140" target="_blank"
   rel="noopener nofollow noreferrer" >L1042-L1140</a></p>
<pre><code class="language-go">	klog.V(3).Infof(&quot;Syncing ipvs Proxier rules&quot;)

	// Begin install iptables

	// Reset all buffers used later.
	// This is to avoid memory reallocations and thus improve performance.
	proxier.natChains.Reset()
	proxier.natRules.Reset()
	proxier.filterChains.Reset()
	proxier.filterRules.Reset()

	// Write table headers.
	writeLine(proxier.filterChains, &quot;*filter&quot;)
	writeLine(proxier.natChains, &quot;*nat&quot;)

	proxier.createAndLinkeKubeChain()

	// make sure dummy interface exists in the system where ipvs Proxier will bind service address on it
	_, err = proxier.netlinkHandle.EnsureDummyDevice(DefaultDummyDevice)
	if err != nil {
		klog.Errorf(&quot;Failed to create dummy interface: %s, error: %v&quot;, DefaultDummyDevice, err)
		return
	}

	// make sure ip sets exists in the system.
	for _, set := range proxier.ipsetList {
		if err := ensureIPSet(set); err != nil {
			return
		}
		set.resetEntries()
	}

	// Accumulate the set of local ports that we will be holding open once this update is complete
	replacementPortsMap := map[utilproxy.LocalPort]utilproxy.Closeable{}
	// activeIPVSServices represents IPVS service successfully created in this round of sync
	activeIPVSServices := map[string]bool{}
	// currentIPVSServices represent IPVS services listed from the system
	currentIPVSServices := make(map[string]*utilipvs.VirtualServer)
	// activeBindAddrs represents ip address successfully bind to DefaultDummyDevice in this round of sync
	activeBindAddrs := map[string]bool{}

	bindedAddresses, err := proxier.ipGetter.BindedIPs()
	if err != nil {
		klog.Errorf(&quot;error listing addresses binded to dummy interface, error: %v&quot;, err)
	}
	// 检查是否是nodeport类型
	hasNodePort := false
	for _, svc := range proxier.serviceMap {
		svcInfo, ok := svc.(*serviceInfo)
		if ok &amp;&amp; svcInfo.NodePort() != 0 {
			hasNodePort = true
			break
		}
	}

	// Both nodeAddresses and nodeIPs can be reused for all nodePort services
	// and only need to be computed if we have at least one nodePort service.
	var (
		// List of node addresses to listen on if a nodePort is set.
		nodeAddresses []string
		// List of node IP addresses to be used as IPVS services if nodePort is set.
		nodeIPs []net.IP
	)

	if hasNodePort {
		nodeAddrSet, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer)
		if err != nil {
			klog.Errorf(&quot;Failed to get node ip address matching nodeport cidr: %v&quot;, err)
		} else {
			nodeAddresses = nodeAddrSet.List()
			for _, address := range nodeAddresses {
				if utilproxy.IsZeroCIDR(address) {
					nodeIPs, err = proxier.ipGetter.NodeIPs()
					if err != nil {
						klog.Errorf(&quot;Failed to list all node IPs from host, err: %v&quot;, err)
					}
					break
				}
				nodeIPs = append(nodeIPs, net.ParseIP(address))
			}
		}
	}
</code></pre>
<p>接下来是整个构建ipvs规则的关键部分，大概200-300行代码，通过循环 serviceMap 拿到每一个 service 的信息，然后在通过 循环 endpointsMap[svcName] 得到每个 service下所属的 endpoint ，然后构建 ipvs 的规则</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1141-L1542" target="_blank"
   rel="noopener nofollow noreferrer" >L1141-L1542</a> 这里也包含了 nodeport, clusterIP, ingress等不同的service类型</p>
<pre><code class="language-go">	// Build IPVS rules for each service.
	for svcName, svc := range proxier.serviceMap {
		svcInfo, ok := svc.(*serviceInfo)
		if !ok {
			klog.Errorf(&quot;Failed to cast serviceInfo %q&quot;, svcName.String())
			continue
		}
		isIPv6 := utilnet.IsIPv6(svcInfo.ClusterIP())
		protocol := strings.ToLower(string(svcInfo.Protocol()))
		// Precompute svcNameString; with many services the many calls
		// to ServicePortName.String() show up in CPU profiles.
		svcNameString := svcName.String()
		
        // 循环endpoint
		// Handle traffic that loops back to the originator with SNAT.
		for _, e := range proxier.endpointsMap[svcName] {
			ep, ok := e.(*proxy.BaseEndpointInfo)
			if !ok {
				klog.Errorf(&quot;Failed to cast BaseEndpointInfo %q&quot;, e.String())
				continue
			}
			if !ep.IsLocal {
				continue
			}
			epIP := ep.IP()
			epPort, err := ep.Port()
			// Error parsing this endpoint has been logged. Skip to next endpoint.
			if epIP == &quot;&quot; || err != nil {
				continue
			}
			entry := &amp;utilipset.Entry{
				IP:       epIP,
				Port:     epPort,
				Protocol: protocol,
				IP2:      epIP,
				SetType:  utilipset.HashIPPortIP,
			}
			if valid := proxier.ipsetList[kubeLoopBackIPSet].validateEntry(entry); !valid {
				klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoopBackIPSet].Name))
				continue
			}
			proxier.ipsetList[kubeLoopBackIPSet].activeEntries.Insert(entry.String())
		}

		// Capture the clusterIP.
		// ipset call
		entry := &amp;utilipset.Entry{
			IP:       svcInfo.ClusterIP().String(),
			Port:     svcInfo.Port(),
			Protocol: protocol,
			SetType:  utilipset.HashIPPort,
		}
		// add service Cluster IP:Port to kubeServiceAccess ip set for the purpose of solving hairpin.
		// proxier.kubeServiceAccessSet.activeEntries.Insert(entry.String())
		if valid := proxier.ipsetList[kubeClusterIPSet].validateEntry(entry); !valid {
			klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeClusterIPSet].Name))
			continue
		}
		proxier.ipsetList[kubeClusterIPSet].activeEntries.Insert(entry.String())
		// ipvs call
		serv := &amp;utilipvs.VirtualServer{
			Address:   svcInfo.ClusterIP(),
			Port:      uint16(svcInfo.Port()),
			Protocol:  string(svcInfo.Protocol()),
			Scheduler: proxier.ipvsScheduler,
		}
		// Set session affinity flag and timeout for IPVS service
		if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP {
			serv.Flags |= utilipvs.FlagPersistent
			serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds())
		}
		// We need to bind ClusterIP to dummy interface, so set `bindAddr` parameter to `true` in syncService()
		if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil {
			activeIPVSServices[serv.String()] = true
			activeBindAddrs[serv.Address.String()] = true
			// ExternalTrafficPolicy only works for NodePort and external LB traffic, does not affect ClusterIP
			// So we still need clusterIP rules in onlyNodeLocalEndpoints mode.
			if err := proxier.syncEndpoint(svcName, false, serv); err != nil {
				klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
			}
		} else {
			klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
		}

		// Capture externalIPs.
		for _, externalIP := range svcInfo.ExternalIPStrings() {
			// If the &quot;external&quot; IP happens to be an IP that is local to this
			// machine, hold the local port open so no other process can open it
			// (because the socket might open but it would never work).
			if (svcInfo.Protocol() != v1.ProtocolSCTP) &amp;&amp; localAddrSet.Has(net.ParseIP(externalIP)) {
				// We do not start listening on SCTP ports, according to our agreement in the SCTP support KEP
				lp := utilproxy.LocalPort{
					Description: &quot;externalIP for &quot; + svcNameString,
					IP:          externalIP,
					Port:        svcInfo.Port(),
					Protocol:    protocol,
				}
				if proxier.portsMap[lp] != nil {
					klog.V(4).Infof(&quot;Port %s was open before and is still needed&quot;, lp.String())
					replacementPortsMap[lp] = proxier.portsMap[lp]
				} else {
					socket, err := proxier.portMapper.OpenLocalPort(&amp;lp, isIPv6)
					if err != nil {
						msg := fmt.Sprintf(&quot;can't open %s, skipping this externalIP: %v&quot;, lp.String(), err)

						proxier.recorder.Eventf(
							&amp;v1.ObjectReference{
								Kind:      &quot;Node&quot;,
								Name:      proxier.hostname,
								UID:       types.UID(proxier.hostname),
								Namespace: &quot;&quot;,
							}, v1.EventTypeWarning, err.Error(), msg)
						klog.Error(msg)
						continue
					}
					replacementPortsMap[lp] = socket
				}
			} // We're holding the port, so it's OK to install IPVS rules.

			// ipset call
			entry := &amp;utilipset.Entry{
				IP:       externalIP,
				Port:     svcInfo.Port(),
				Protocol: protocol,
				SetType:  utilipset.HashIPPort,
			}

			if utilfeature.DefaultFeatureGate.Enabled(features.ExternalPolicyForExternalIP) &amp;&amp; svcInfo.OnlyNodeLocalEndpoints() {
				if valid := proxier.ipsetList[kubeExternalIPLocalSet].validateEntry(entry); !valid {
					klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeExternalIPLocalSet].Name))
					continue
				}
				proxier.ipsetList[kubeExternalIPLocalSet].activeEntries.Insert(entry.String())
			} else {
				// We have to SNAT packets to external IPs.
				if valid := proxier.ipsetList[kubeExternalIPSet].validateEntry(entry); !valid {
					klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeExternalIPSet].Name))
					continue
				}
				proxier.ipsetList[kubeExternalIPSet].activeEntries.Insert(entry.String())
			}

			// ipvs call
			serv := &amp;utilipvs.VirtualServer{
				Address:   net.ParseIP(externalIP),
				Port:      uint16(svcInfo.Port()),
				Protocol:  string(svcInfo.Protocol()),
				Scheduler: proxier.ipvsScheduler,
			}
			if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP {
				serv.Flags |= utilipvs.FlagPersistent
				serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds())
			}
			if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil {
				activeIPVSServices[serv.String()] = true
				activeBindAddrs[serv.Address.String()] = true

				onlyNodeLocalEndpoints := false
				if utilfeature.DefaultFeatureGate.Enabled(features.ExternalPolicyForExternalIP) {
					onlyNodeLocalEndpoints = svcInfo.OnlyNodeLocalEndpoints()
				}
				if err := proxier.syncEndpoint(svcName, onlyNodeLocalEndpoints, serv); err != nil {
					klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
				}
			} else {
				klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
			}
		}

		// Capture load-balancer ingress.
		for _, ingress := range svcInfo.LoadBalancerIPStrings() {
			if ingress != &quot;&quot; {
				// ipset call
				entry = &amp;utilipset.Entry{
					IP:       ingress,
					Port:     svcInfo.Port(),
					Protocol: protocol,
					SetType:  utilipset.HashIPPort,
				}
				// add service load balancer ingressIP:Port to kubeServiceAccess ip set for the purpose of solving hairpin.
				// proxier.kubeServiceAccessSet.activeEntries.Insert(entry.String())
				// If we are proxying globally, we need to masquerade in case we cross nodes.
				// If we are proxying only locally, we can retain the source IP.
				if valid := proxier.ipsetList[kubeLoadBalancerSet].validateEntry(entry); !valid {
					klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSet].Name))
					continue
				}
				proxier.ipsetList[kubeLoadBalancerSet].activeEntries.Insert(entry.String())
				// insert loadbalancer entry to lbIngressLocalSet if service externaltrafficpolicy=local
				if svcInfo.OnlyNodeLocalEndpoints() {
					if valid := proxier.ipsetList[kubeLoadBalancerLocalSet].validateEntry(entry); !valid {
						klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerLocalSet].Name))
						continue
					}
					proxier.ipsetList[kubeLoadBalancerLocalSet].activeEntries.Insert(entry.String())
				}
				if len(svcInfo.LoadBalancerSourceRanges()) != 0 {
					// The service firewall rules are created based on ServiceSpec.loadBalancerSourceRanges field.
					// This currently works for loadbalancers that preserves source ips.
					// For loadbalancers which direct traffic to service NodePort, the firewall rules will not apply.
					if valid := proxier.ipsetList[kubeLoadbalancerFWSet].validateEntry(entry); !valid {
						klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadbalancerFWSet].Name))
						continue
					}
					proxier.ipsetList[kubeLoadbalancerFWSet].activeEntries.Insert(entry.String())
					allowFromNode := false
					for _, src := range svcInfo.LoadBalancerSourceRanges() {
						// ipset call
						entry = &amp;utilipset.Entry{
							IP:       ingress,
							Port:     svcInfo.Port(),
							Protocol: protocol,
							Net:      src,
							SetType:  utilipset.HashIPPortNet,
						}
						// enumerate all white list source cidr
						if valid := proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].validateEntry(entry); !valid {
							klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].Name))
							continue
						}
						proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].activeEntries.Insert(entry.String())

						// ignore error because it has been validated
						_, cidr, _ := net.ParseCIDR(src)
						if cidr.Contains(proxier.nodeIP) {
							allowFromNode = true
						}
					}
					// generally, ip route rule was added to intercept request to loadbalancer vip from the
					// loadbalancer's backend hosts. In this case, request will not hit the loadbalancer but loop back directly.
					// Need to add the following rule to allow request on host.
					if allowFromNode {
						entry = &amp;utilipset.Entry{
							IP:       ingress,
							Port:     svcInfo.Port(),
							Protocol: protocol,
							IP2:      ingress,
							SetType:  utilipset.HashIPPortIP,
						}
						// enumerate all white list source ip
						if valid := proxier.ipsetList[kubeLoadBalancerSourceIPSet].validateEntry(entry); !valid {
							klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSourceIPSet].Name))
							continue
						}
						proxier.ipsetList[kubeLoadBalancerSourceIPSet].activeEntries.Insert(entry.String())
					}
				}

				// ipvs call
				serv := &amp;utilipvs.VirtualServer{
					Address:   net.ParseIP(ingress),
					Port:      uint16(svcInfo.Port()),
					Protocol:  string(svcInfo.Protocol()),
					Scheduler: proxier.ipvsScheduler,
				}
				if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP {
					serv.Flags |= utilipvs.FlagPersistent
					serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds())
				}
				if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil {
					activeIPVSServices[serv.String()] = true
					activeBindAddrs[serv.Address.String()] = true
					if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil {
						klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
					}
				} else {
					klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
				}
			}
		}

		if svcInfo.NodePort() != 0 {
			if len(nodeAddresses) == 0 || len(nodeIPs) == 0 {
				// Skip nodePort configuration since an error occurred when
				// computing nodeAddresses or nodeIPs.
				continue
			}

			var lps []utilproxy.LocalPort
			for _, address := range nodeAddresses {
				lp := utilproxy.LocalPort{
					Description: &quot;nodePort for &quot; + svcNameString,
					IP:          address,
					Port:        svcInfo.NodePort(),
					Protocol:    protocol,
				}
				if utilproxy.IsZeroCIDR(address) {
					// Empty IP address means all
					lp.IP = &quot;&quot;
					lps = append(lps, lp)
					// If we encounter a zero CIDR, then there is no point in processing the rest of the addresses.
					break
				}
				lps = append(lps, lp)
			}

			// For ports on node IPs, open the actual port and hold it.
			for _, lp := range lps {
				if proxier.portsMap[lp] != nil {
					klog.V(4).Infof(&quot;Port %s was open before and is still needed&quot;, lp.String())
					replacementPortsMap[lp] = proxier.portsMap[lp]
					// We do not start listening on SCTP ports, according to our agreement in the
					// SCTP support KEP
				} else if svcInfo.Protocol() != v1.ProtocolSCTP {
					socket, err := proxier.portMapper.OpenLocalPort(&amp;lp, isIPv6)
					if err != nil {
						klog.Errorf(&quot;can't open %s, skipping this nodePort: %v&quot;, lp.String(), err)
						continue
					}
					if lp.Protocol == &quot;udp&quot; {
						conntrack.ClearEntriesForPort(proxier.exec, lp.Port, isIPv6, v1.ProtocolUDP)
					}
					replacementPortsMap[lp] = socket
				} // We're holding the port, so it's OK to install ipvs rules.
			}

			// Nodeports need SNAT, unless they're local.
			// ipset call

			var (
				nodePortSet *IPSet
				entries     []*utilipset.Entry
			)

			switch protocol {
			case &quot;tcp&quot;:
				nodePortSet = proxier.ipsetList[kubeNodePortSetTCP]
				entries = []*utilipset.Entry{{
					// No need to provide ip info
					Port:     svcInfo.NodePort(),
					Protocol: protocol,
					SetType:  utilipset.BitmapPort,
				}}
			case &quot;udp&quot;:
				nodePortSet = proxier.ipsetList[kubeNodePortSetUDP]
				entries = []*utilipset.Entry{{
					// No need to provide ip info
					Port:     svcInfo.NodePort(),
					Protocol: protocol,
					SetType:  utilipset.BitmapPort,
				}}
			case &quot;sctp&quot;:
				nodePortSet = proxier.ipsetList[kubeNodePortSetSCTP]
				// Since hash ip:port is used for SCTP, all the nodeIPs to be used in the SCTP ipset entries.
				entries = []*utilipset.Entry{}
				for _, nodeIP := range nodeIPs {
					entries = append(entries, &amp;utilipset.Entry{
						IP:       nodeIP.String(),
						Port:     svcInfo.NodePort(),
						Protocol: protocol,
						SetType:  utilipset.HashIPPort,
					})
				}
			default:
				// It should never hit
				klog.Errorf(&quot;Unsupported protocol type: %s&quot;, protocol)
			}
			if nodePortSet != nil {
				entryInvalidErr := false
				for _, entry := range entries {
					if valid := nodePortSet.validateEntry(entry); !valid {
						klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, nodePortSet.Name))
						entryInvalidErr = true
						break
					}
					nodePortSet.activeEntries.Insert(entry.String())
				}
				if entryInvalidErr {
					continue
				}
			}

			// Add externaltrafficpolicy=local type nodeport entry
			if svcInfo.OnlyNodeLocalEndpoints() {
				var nodePortLocalSet *IPSet
				switch protocol {
				case &quot;tcp&quot;:
					nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetTCP]
				case &quot;udp&quot;:
					nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetUDP]
				case &quot;sctp&quot;:
					nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetSCTP]
				default:
					// It should never hit
					klog.Errorf(&quot;Unsupported protocol type: %s&quot;, protocol)
				}
				if nodePortLocalSet != nil {
					entryInvalidErr := false
					for _, entry := range entries {
						if valid := nodePortLocalSet.validateEntry(entry); !valid {
							klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, nodePortLocalSet.Name))
							entryInvalidErr = true
							break
						}
						nodePortLocalSet.activeEntries.Insert(entry.String())
					}
					if entryInvalidErr {
						continue
					}
				}
			}

			// Build ipvs kernel routes for each node ip address
			for _, nodeIP := range nodeIPs {
				// ipvs call
				serv := &amp;utilipvs.VirtualServer{
					Address:   nodeIP,
					Port:      uint16(svcInfo.NodePort()),
					Protocol:  string(svcInfo.Protocol()),
					Scheduler: proxier.ipvsScheduler,
				}
				if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP {
					serv.Flags |= utilipvs.FlagPersistent
					serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds())
				}
				// There is no need to bind Node IP to dummy interface, so set parameter `bindAddr` to `false`.
				if err := proxier.syncService(svcNameString, serv, false, bindedAddresses); err == nil {
					activeIPVSServices[serv.String()] = true
					if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil {
						klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
					}
				} else {
					klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
				}
			}
		}
	}
</code></pre>
<p>其中有两个非常重要的函数 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1215-L1229" target="_blank"
   rel="noopener nofollow noreferrer" >syncService()</a> 于 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1215-L1229" target="_blank"
   rel="noopener nofollow noreferrer" >syncEndpoint()</a>  这两个定义了同步的过程</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1921-L1959" target="_blank"
   rel="noopener nofollow noreferrer" >syncService()</a> 函数表示了增加或删除一个service，如果存在则修改，如果存在则添加</p>
<pre><code class="language-go">func (proxier *Proxier) syncService(svcName string, vs *utilipvs.VirtualServer, bindAddr bool, bindedAddresses sets.String) error {
	appliedVirtualServer, _ := proxier.ipvs.GetVirtualServer(vs)
	if appliedVirtualServer == nil || !appliedVirtualServer.Equal(vs) {
		if appliedVirtualServer == nil {
			// IPVS service is not found, create a new service
			klog.V(3).Infof(&quot;Adding new service %q %s:%d/%s&quot;, svcName, vs.Address, vs.Port, vs.Protocol)
			if err := proxier.ipvs.AddVirtualServer(vs); err != nil {
				klog.Errorf(&quot;Failed to add IPVS service %q: %v&quot;, svcName, err)
				return err
			}
		} else {
			// IPVS service was changed, update the existing one
			// During updates, service VIP will not go down
			klog.V(3).Infof(&quot;IPVS service %s was changed&quot;, svcName)
			if err := proxier.ipvs.UpdateVirtualServer(vs); err != nil {
				klog.Errorf(&quot;Failed to update IPVS service, err:%v&quot;, err)
				return err
			}
		}
	}

	// bind service address to dummy interface
	if bindAddr {
		// always attempt to bind if bindedAddresses is nil,
		// otherwise check if it's already binded and return early
		if bindedAddresses != nil &amp;&amp; bindedAddresses.Has(vs.Address.String()) {
			return nil
		}

		klog.V(4).Infof(&quot;Bind addr %s&quot;, vs.Address.String())
		_, err := proxier.netlinkHandle.EnsureAddressBind(vs.Address.String(), DefaultDummyDevice)
		if err != nil {
			klog.Errorf(&quot;Failed to bind service address to dummy device %q: %v&quot;, svcName, err)
			return err
		}
	}

	return nil
}
</code></pre>
<p>同理 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1961-L2090" target="_blank"
   rel="noopener nofollow noreferrer" >syncEndpoint()</a> 也是相同的操作</p>
<pre><code class="language-go">func (proxier *Proxier) syncEndpoint(svcPortName proxy.ServicePortName, onlyNodeLocalEndpoints bool, vs *utilipvs.VirtualServer) error {
	appliedVirtualServer, err := proxier.ipvs.GetVirtualServer(vs)
	if err != nil || appliedVirtualServer == nil {
		klog.Errorf(&quot;Failed to get IPVS service, error: %v&quot;, err)
		return err
	}

	// curEndpoints represents IPVS destinations listed from current system.
	curEndpoints := sets.NewString()
	// newEndpoints represents Endpoints watched from API Server.
	newEndpoints := sets.NewString()

	curDests, err := proxier.ipvs.GetRealServers(appliedVirtualServer)
	if err != nil {
		klog.Errorf(&quot;Failed to list IPVS destinations, error: %v&quot;, err)
		return err
	}
	for _, des := range curDests {
		curEndpoints.Insert(des.String())
	}

	endpoints := proxier.endpointsMap[svcPortName]

	// Service Topology will not be enabled in the following cases:
	// 1. externalTrafficPolicy=Local (mutually exclusive with service topology).
	// 2. ServiceTopology is not enabled.
	// 3. EndpointSlice is not enabled (service topology depends on endpoint slice
	// to get topology information).
	if !onlyNodeLocalEndpoints &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.ServiceTopology) &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.EndpointSliceProxying) {
		endpoints = proxy.FilterTopologyEndpoint(proxier.nodeLabels, proxier.serviceMap[svcPortName].TopologyKeys(), endpoints)
	}

	for _, epInfo := range endpoints {
		if onlyNodeLocalEndpoints &amp;&amp; !epInfo.GetIsLocal() {
			continue
		}
		newEndpoints.Insert(epInfo.String())
	}

	// Create new endpoints
	for _, ep := range newEndpoints.List() {
		ip, port, err := net.SplitHostPort(ep)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint: %v, error: %v&quot;, ep, err)
			continue
		}
		portNum, err := strconv.Atoi(port)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint port %s, error: %v&quot;, port, err)
			continue
		}

		newDest := &amp;utilipvs.RealServer{
			Address: net.ParseIP(ip),
			Port:    uint16(portNum),
			Weight:  1,
		}

		if curEndpoints.Has(ep) {
			// check if newEndpoint is in gracefulDelete list, if true, delete this ep immediately
			uniqueRS := GetUniqueRSName(vs, newDest)
			if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
				continue
			}
			klog.V(5).Infof(&quot;new ep %q is in graceful delete list&quot;, uniqueRS)
			err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS)
			if err != nil {
				klog.Errorf(&quot;Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v&quot;, ep, err)
				continue
			}
		}
		err = proxier.ipvs.AddRealServer(appliedVirtualServer, newDest)
		if err != nil {
			klog.Errorf(&quot;Failed to add destination: %v, error: %v&quot;, newDest, err)
			continue
		}
	}
	// Delete old endpoints
	for _, ep := range curEndpoints.Difference(newEndpoints).UnsortedList() {
		// if curEndpoint is in gracefulDelete, skip
		uniqueRS := vs.String() + &quot;/&quot; + ep
		if proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
			continue
		}
		ip, port, err := net.SplitHostPort(ep)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint: %v, error: %v&quot;, ep, err)
			continue
		}
		portNum, err := strconv.Atoi(port)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint port %s, error: %v&quot;, port, err)
			continue
		}

		delDest := &amp;utilipvs.RealServer{
			Address: net.ParseIP(ip),
			Port:    uint16(portNum),
		}

		klog.V(5).Infof(&quot;Using graceful delete to delete: %v&quot;, uniqueRS)
		err = proxier.gracefuldeleteManager.GracefulDeleteRS(appliedVirtualServer, delDest)
		if err != nil {
			klog.Errorf(&quot;Failed to delete destination: %v, error: %v&quot;, uniqueRS, err)
			continue
		}
	}
	return nil
}
</code></pre>
<p>接下来就是同步规则的步骤了，<a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1544-L1621" target="_blank"
   rel="noopener nofollow noreferrer" >L1544-L1621</a></p>
<h3 id="step-3规则的删除">step 3：规则的删除</h3>
<p>粗略翻到这里可能有一个疑问？没有提到删除，删除时包含在 syncXX() 函数中的</p>
<p>例如在 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L2063-L2088" target="_blank"
   rel="noopener nofollow noreferrer" >syncEndpoint()</a> 中会看是否在 终止列表中，如果在跳过，如果不在加入</p>
<pre><code class="language-go">if curEndpoints.Has(ep) {
    // check if newEndpoint is in gracefulDelete list, if true, delete this ep immediately
    uniqueRS := GetUniqueRSName(vs, newDest)
    if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
        continue
    }
    klog.V(5).Infof(&quot;new ep %q is in graceful delete list&quot;, uniqueRS)
    err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS)
    if err != nil {
        klog.Errorf(&quot;Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v&quot;, ep, err)
        continue
    }
}
</code></pre>
<p>而 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L275" target="_blank"
   rel="noopener nofollow noreferrer" >gracefuldeleteManager</a> 是一个 一直运行的协程，在初始化 proxier 时被 Run()</p>
<pre><code class="language-go">// Run start a goroutine to try to delete rs in the graceful delete rsList with an interval 1 minute
func (m *GracefulTerminationManager) Run() {
	go wait.Until(m.tryDeleteRs, rsCheckDeleteInterval, wait.NeverStop)
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L508" target="_blank"
   rel="noopener nofollow noreferrer" >proxier.go</a></p>
<pre><code class="language-go">proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs)
proxier.gracefuldeleteManager.Run()
return proxier, nil
</code></pre>
<h2 id="总结">总结</h2>
<p>到这里已经清楚的掌握了 kube-proxy 的架构，接下来的会为扩展kubernetes中service架构，以及手撸一个 kube-proxy做准备；本系列第三部分：<a href="https://cylonchau.github.io/ch3.7-admission-webhook.html" target="_blank"
   rel="noopener nofollow noreferrer" >如何扩展现有的kube-proxy架构</a></p>
<p>文中的知识都是个人根据理解整理的，如有不对的地方欢迎指出，感谢各位大佬</p>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="5">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>dual-stack service</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes service - 你真的理解service吗？</title>
      <link>https://www.oomkill.com/2023/02/ch17-service-controller/</link>
      <pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/02/ch17-service-controller/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p>本文是关于Kubernetes service解析的第一章</p>
<ul>
<li>深入理解Kubernetes service - 你真的理解service吗?</li>
<li><a href="https://cylonchau.github.io/kubernetes-endpointslices.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - EndpointSlices做了什么？</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-kube-proxy-code.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - kube-proxy架构分析</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-extend-kube-proxy.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - 如何扩展现有的kube-proxy架构</a></li>
</ul>
<p>所有关于Kubernetes service 部分代码上传至仓库 <a href="https://github.com/cylonchau/kube-haproxy" target="_blank"
   rel="noopener nofollow noreferrer" >github.com/cylonchau/kube-haproxy</a></p>
</blockquote>
<h2 id="前景">前景</h2>
<p>对于了解kubernetes架构时，已知的是 <code>service</code> 是kubernetes在设计时为了避免Pod在频繁创建和销毁时IP变更问题，从而给集群内服务（一组Pod）提供访问的一个入口。而Pod在这里的角色是 『后端』( <em><strong>backend</strong></em> ) ，而 service 的角色是 『前端』( <em><strong>frontend</strong></em> )。本文将阐述service的生命周期</p>
<h2 id="为什么需要了解这部分内容呢">为什么需要了解这部分内容呢</h2>
<p>对于 without kube-proxy来说，这部分是最重要的部分，因为service的生成不是kube-proxy来完成的，而这部分也就是service ip定义的核心。</p>
<h2 id="控制器">控制器</h2>
<p>service的资源创建很奇妙，继不属于 <code>controller-manager</code> 组件，也不属于 <code>kube-proxy</code> 组件，而是存在于 <code>apiserver</code> 中的一个被成为控制器的组件；而这个控制器又区别于准入控制器。更准确来说，准入控制器是位于kubeapiserver中的组件，而 <strong>控制器</strong> 则是存在于单独的一个包，这里包含了很多kubernetes集群的公共组件的功能，其中就有service。这也就是在操作kubernetes时 当 <code>controller-manager</code> 于  <code>kube-proxy</code> 未工作时，也可以准确的为service分配IP。</p>
<p>首先在构建出apiserver时，也就是代码 <a href="cmd/kube-apiserver/app/server.go">cmd/kube-apiserver/app/server.go</a></p>
<pre><code class="language-go">serviceIPRange, apiServerServiceIP, err := master.ServiceIPRange(s.PrimaryServiceClusterIPRange)
if err != nil {
    return nil, nil, nil, nil, err
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/pkg/master/services.go#L34-L54" target="_blank"
   rel="noopener nofollow noreferrer" >master.ServiceIPRange</a> 承接了为service分配IP的功能，这部分逻辑就很简单了</p>
<pre><code class="language-go">func ServiceIPRange(passedServiceClusterIPRange net.IPNet) (net.IPNet, net.IP, error) {
	serviceClusterIPRange := passedServiceClusterIPRange
	if passedServiceClusterIPRange.IP == nil {
		klog.Warningf(&quot;No CIDR for service cluster IPs specified. Default value which was %s is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.&quot;, kubeoptions.DefaultServiceIPCIDR.String())
		serviceClusterIPRange = kubeoptions.DefaultServiceIPCIDR
	}

	size := integer.Int64Min(utilnet.RangeSize(&amp;serviceClusterIPRange), 1&lt;&lt;16)
	if size &lt; 8 {
		return net.IPNet{}, net.IP{}, fmt.Errorf(&quot;the service cluster IP range must be at least %d IP addresses&quot;, 8)
	}

	// Select the first valid IP from ServiceClusterIPRange to use as the GenericAPIServer service IP.
	apiServerServiceIP, err := utilnet.GetIndexedIP(&amp;serviceClusterIPRange, 1)
	if err != nil {
		return net.IPNet{}, net.IP{}, err
	}
	klog.V(4).Infof(&quot;Setting service IP to %q (read-write).&quot;, apiServerServiceIP)

	return serviceClusterIPRange, apiServerServiceIP, nil
}
</code></pre>
<p>而后kube-apiserver为service分为两类</p>
<ul>
<li>apiserver 地址在集群内的service，在代码中表示为 <a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L351" target="_blank"
   rel="noopener nofollow noreferrer" >APIServerServiceIP</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L352" target="_blank"
   rel="noopener nofollow noreferrer" >Service</a>，<code>--service-cluster-ip-range</code> 配置指定的ip，通过『逗号』分割可以为两个</li>
</ul>
<p>有了对 service 更好的理解后，接下来开始本系列第二节<a href="https://cylonchau.github.io/kubernetes-without-service.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - kube-proxy软件架构分析</a></p>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>dual-stack service</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>理解Kubernetes驱逐核心 - Pod QoS</title>
      <link>https://www.oomkill.com/2022/12/kubernetes-pod-qos/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/12/kubernetes-pod-qos/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>服务质量 Quality of Service (QoS)，在Kubernetes是用于解决资源抢占，延迟等方向的一种技术，是服务于调度与抢占之间的条件。</p>
<h2 id="qos-级别">QoS 级别</h2>
<p>QoS 与 资源限制紧密相关，正如下属展示，是一个Pod资源限制部分的配置</p>
<pre><code class="language-yaml">resources:
  limits:
    cpu: 200m
    memory: 1G
  requests:
    cpu: 500m
    memory: 1G
</code></pre>
<p>而Kubernetes 将Pod QoS 根据 CPU 与 内存的配置，将QoS分为三个等级：</p>
<ul>
<li><strong>Guaranteed</strong>：确保的，只设置 <code>limits</code> 或者 <code>requests</code> 与 <code>limits</code> 为相同时则为该等级</li>
<li><strong>Burstable</strong>：可突发的，只设置 <code>requests</code> 或 <code>requests</code> 低于  <code>limits</code> 的场景</li>
<li><strong>Best-effort</strong>： 默认值，如果不设置则为这个等级</li>
</ul>
<h2 id="为什么要关心pod-qos级别">为什么要关心Pod QoS级别</h2>
<p>在Kubernetes中，将资源分为两类：可压缩性资源 “CPU”，不可压缩性资源 “内存”。当可压缩性资源用尽时，不会被终止与驱逐，而不可压缩性资源用尽时，即Pod内存不足，此时会被OOMKiller杀掉，也就是被驱逐等操作，而了解Pod 的QoS级别可以有效避免关键Pod被驱逐。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221201203746111.png" alt="image-20221201203746111" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Pod QoS分类</center>
<center><em>Source：</em>https://doc.kaas.thalesdigital.io/docs/BestPractices/QOS</center><br>
<p>有上图可知，<strong>BestEffort</strong> 级别的 Pod 能够使用节点上所有资源，浙江导致其他 Pod 出现资源问题。所以这类 Pod 优先级<strong>最低</strong>，如果系统没有内存，将首先被杀死。</p>
<h2 id="pod是如何被驱逐的">Pod是如何被驱逐的</h2>
<p>当节点的计算资源不足时，<em>kubelet</em> 会发起驱逐，这个操作是为了避免系统OOM事件，而QoS的等级决定了驱逐的优先级，没有限制资源的 <strong>BestEffort</strong> 类型的Pod最先被驱逐，接下来资源使用率低于 <code>Requests</code> 的 <strong>Guaranteed</strong> 与 <strong>Burstable</strong> 将不会被其他Pod的资源使用量而驱逐，其次对于此类Pod而言，如果Pod使用了比配置（<code>Requests</code>）更多的资源时，会根据这两个级别Pod的优先级进行驱逐。 <strong>BestEffort</strong> 与  **Burstable **将按照先优先级，后资源使用率顺序进行驱逐</p>
<p>对于磁盘压力来讲，驱逐顺序根据 <strong>BestEffort</strong> ==》<strong>Burstable</strong> ==》<strong>Guaranteed</strong> 进行驱逐</p>
<h2 id="如何查看pod的qos等级">如何查看Pod的QoS等级</h2>
<p>Pod资源清单中 Status 字段代表Pod QoS等级</p>
<pre><code class="language-bash">kubectl get pod &lt;pod_name&gt; -o jsonpath='{.status.qosClass}'
</code></pre>
<h2 id="如何配置qos默认级别">如何配置QoS默认级别</h2>
<p>如果不想对每个Pod都配置资源限制，Kubernetes提供了一个API <code>LimitRange</code> 可以指定默认的QoS，为Pod提供默认的资源限制，然后准入控制器会增加默认的资源限制 k8s.io/kubernetes/plugin/pkg/admission/limitranger，正如官方给出的<a href="https://kubernetes.io/docs/concepts/policy/limit-range/" target="_blank"
   rel="noopener nofollow noreferrer" >实例</a>一样</p>
<blockquote>
<p>Notes：准入控制器与Pod控制器概念不同，准入控制器是 <em>kube-apiserver</em> 请求时的hander chain</p>
</blockquote>
<pre><code class="language-yaml">apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default: # this section defines default limits
      cpu: 500m
    defaultRequest: # this section defines default requests
      cpu: 500m
    max: # max and min define the limit range
      cpu: &quot;1&quot;
    min:
      cpu: 100m
</code></pre>
<p>准入控制器会进行检查，而 <code>LimitRange</code> 也是一个标准，限制所有Pod的资源限制标准（<code>Request</code> 与 <code>limits</code> ）必须小于等于 <code>LimitRange</code> 配置的格式，例如下列配置将不会被准入</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: example-conflict-with-limitrange-cpu
spec:
  containers:
  - name: demo
    image: registry.k8s.io/pause:2.0
    resources:
      requests:
        cpu: 700m
</code></pre>
<p>由于该Pod没有配置 <strong>Limits</strong> ，不符合规范，该Pod不会被调度，错误如下</p>
<pre><code class="language-bash">Pod &quot;example-conflict-with-limitrange-cpu&quot; is invalid: spec.containers[0].resources.requests: Invalid value: &quot;700m&quot;: must be less than or equal to cpu limit
</code></pre>
<p>如果同时设置 <code>request</code> 和 <code>limit</code> ，即使大于<code>LimitRange</code> 的配置，新的 Pod 也会被成功调度：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: example-no-conflict-with-limitrange-cpu
spec:
  containers:
  - name: demo
    image: registry.k8s.io/pause:2.0
    resources:
      requests:
        cpu: 700m
      limits:
        cpu: 700m
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - 理解Kubernetes的驱逐机制</title>
      <link>https://www.oomkill.com/2022/11/kubernetes-eviction/</link>
      <pubDate>Tue, 29 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/11/kubernetes-eviction/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>驱逐 (<em><strong>eviction</strong></em>) 是指终止在Node上运行的Pod，保证workload的可用性，对于使用Kubernetes，了解驱逐机制是很有必要性的，因为通常情况下，Pod被驱逐是需要解决驱逐背后导致的问题，而想要快速定位就需要对驱逐机制进行了解。</p>
<h2 id="pod被驱逐原因">Pod被驱逐原因</h2>
<p>Kubernetes官方给出了下属Pod被驱逐的原因：</p>
<ul>
<li>抢占驱逐 (<em><strong>Preemption and Eviction</strong></em>) <sup><a href="#1">[1]</a></sup></li>
<li>节点压力驱逐 (<em><strong>Node-pressure</strong></em>) <sup><a href="#2">[2]</a></sup></li>
<li>污点驱逐 (<em><strong>Taints</strong></em>) <sup><a href="#3">[3]</a></sup></li>
<li>使用API发起驱逐 (<em><strong>API-initiated</strong></em>) <sup><a href="#4">[4]</a></sup></li>
<li>排出Node上的Pod (<em><strong>drain</strong></em>) <sup><a href="#5">[5]</a></sup></li>
<li>被 controller-manager 驱逐</li>
</ul>
<h3 id="抢占和优先级">抢占和优先级</h3>
<p>抢占是指当节点资源不足以运行新添加的Pod时，<em>kube-scheduler</em> 会检查低优先级Pod而后驱逐掉这些Pod以将资源分配给优先级高的Pod。这个过程称为 “抢占” 例如这个实例是 <a href="https://cylonchau.github.io/ch00.0-pod-network-troubleshooting.html#%E9%9B%86%E7%BE%A4pod%E8%AE%BF%E9%97%AE%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E8%B6%85%E6%97%B6" target="_blank"
   rel="noopener nofollow noreferrer" ><em>kube-proxy</em> 被驱逐的场景</a></p>
<h3 id="节点压力驱逐">节点压力驱逐</h3>
<p>节点压力驱逐是指，Pod所在节点的资源，如CPU, 内存, inode等，这些资源被分为可压缩资源CPU (<em><strong>compressible resources</strong></em>) 与不可压缩资源 (<em><strong>incompressible resources</strong></em>) 磁盘IO, 内存等，当不可压缩资源不足时，Pod会被驱逐。对于此类问题的驱逐 是每个计算节点的 kubelet 通过捕获 cAdvisor 指标来监控节点的资源使用情况。</p>
<h3 id="被-controller-manager-驱逐">被 controller-manager 驱逐</h3>
<p><em><strong>kube-controller-manager</strong></em> 会定期检查节点的状态，如节点处于 <code>NotReady</code>  超过一定时间，或Pod部署长时间失败，这些Pod由控制平面 <em><strong>controller-manager</strong></em> 创建新的Pod已替换存在问题的Pod</p>
<h3 id="通过api发起驱逐">通过API发起驱逐</h3>
<p>Kubernetes为用户提供了驱逐的API，用户可以通过调用API来实现自定义的驱逐。</p>
<p>对于 1.22 以上版本，可以通过API <code>policy/v1</code> 进行驱逐</p>
<pre><code class="language-bash">curl -v \
	-H 'Content-type: application/json' \
	https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d '\
	{
        &quot;apiVersion&quot;: &quot;policy/v1&quot;,
        &quot;kind&quot;: &quot;Eviction&quot;,
        &quot;metadata&quot;: {
            &quot;name&quot;: &quot;quux&quot;,
            &quot;namespace&quot;: &quot;default&quot;
        }
    }'
</code></pre>
<p>例如，要驱逐Pod <code>netbox-85865d5556-hfg6v</code>，可以通过下述命令</p>
<pre><code class="language-bash"># 1.22+
$ curl -v 'https://10.0.0.4:6443/api/v1/namespaces/default/pods/netbox-85865d5556-hfg6v/eviction' \
--header 'Content-Type: application/json' \
--cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \
--key /etc/kubernetes/pki/apiserver-kubelet-client.key \
--cacert /etc/kubernetes/pki/ca.crt \
-d '{
    &quot;apiVersion&quot;: &quot;policy/v1&quot;,
    &quot;kind&quot;: &quot;Eviction&quot;,
    &quot;metadata&quot;: {
        &quot;name&quot;: &quot;netbox-85865d5556-hfg6v&quot;,
        &quot;namespace&quot;: &quot;default&quot;
    }
}'

# 1.22-
curl -v 'https://10.0.0.4:6443/api/v1/namespaces/default/pods/netbox-85865d5556-hfg6v/eviction' \
--header 'Content-Type: application/json' \
--cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \
--key /etc/kubernetes/pki/apiserver-kubelet-client.key \
--cacert /etc/kubernetes/pki/ca.crt \
-d '{
    &quot;apiVersion&quot;: &quot;policy/v1beta1&quot;,
    &quot;kind&quot;: &quot;Eviction&quot;,
    &quot;metadata&quot;: {
        &quot;name&quot;: &quot;netbox-85865d5556-hfg6v&quot;,
        &quot;namespace&quot;: &quot;default&quot;
    }
}'
</code></pre>
<p>可以看到结果，旧Pod被驱逐，而新Pod被创建，在这里实验环境节点较少，所以体现为没有更换节点</p>
<pre><code class="language-bash">$ kubectl get pods -o wide
NAME                      READY   STATUS        RESTARTS   AGE    IP              NODE             NOMINATED NODE   READINESS GATES
netbox-85865d5556-hfg6v   1/1     Terminating   0          101d   192.168.1.213   master-machine   &lt;none&gt;           &lt;none&gt;
netbox-85865d5556-vlgr4   1/1     Running       0          101d   192.168.0.4     node01           &lt;none&gt;           &lt;none&gt;
netbox-85865d5556-z6vqx   1/1     Running       0          11s    192.168.1.220   master-machine   &lt;none&gt;           &lt;none&gt;
</code></pre>
<h4 id="通过api驱逐返回状态">通过API驱逐返回状态</h4>
<ul>
<li><strong>200 OK|201 Success</strong>：允许驱逐，<code>Eviction</code> 类似于向Pod URL发送 <code>DELETE</code> 请求</li>
<li><strong>429 Too Many Requests</strong>：由于API限速可能会看到该相应，另外也为配置原因，不允许驱逐 <code>poddisruptionbudget</code> (PDB是一种保护机制，将总是确保一定数量或百分比的Pod 被自愿驱逐)</li>
<li><strong>500 Internal Server Error</strong>：不允许驱逐，存在错误配置，如多个PDB引用一个 Pod</li>
</ul>
<h3 id="排出node上的pod">排出Node上的Pod</h3>
<p><em><strong>drain</strong></em> 是kubernetes 1.5+之后提供给用户维护命令，通过这个命令 (<code>kubectl drain &lt;node_name&gt;</code>) 可以驱逐该节点上运行的所有Pod，已用来对节点主机进行操作（如内核升级，重启）</p>
<blockquote>
<p>Notes：<code>kubectl drain &lt;node_name&gt;</code> 一次只能接一个nodename <sup><a href="#6">[6]</a></sup></p>
</blockquote>
<h3 id="污点驱逐">污点驱逐</h3>
<p>污点通常与容忍度同时使用，拥有污点的node，Pod将不会被调度至该节点，而容忍度将允许一定的污点来调度 pod。</p>
<p>在Kubernetes 1.18+后，允许基于污点的驱逐机制，即kubelet在某些情况下会自动添加节点从而进行驱逐：</p>
<p>Kubernetes内置了一些污点，此时 <em>Controller</em> 会自动污染节点：</p>
<ul>
<li><code>node.kubernetes.io/not-ready</code>: Node故障。对应 NodeCondition 的<code>Ready</code> =  <code>False</code>。</li>
<li><code>node.kubernetes.io/unreachable</code>：Node控制器无法访问节点。对应 NodeCondition <code>Ready</code>= <code>Unknown</code>。</li>
<li><code>node.kubernetes.io/memory-pressure</code>：Node内存压力。</li>
<li><code>node.kubernetes.io/disk-pressure</code>：Node磁盘压力。</li>
<li><code>node.kubernetes.io/pid-pressure</code>：Node有PID压力。</li>
<li><code>node.kubernetes.io/network-unavailable</code>：Node网络不可用。</li>
<li><code>node.kubernetes.io/unschedulable</code>：Node不可调度。</li>
</ul>
<h2 id="转实例pod被驱逐故障排除过程-supa-href77asup">【转】实例：Pod被驱逐故障排除过程 <sup><a href="#7">[7]</a></sup></h2>
<p>设想一个场景：，有三个工作节点的Kubernetes 集群，版本为 v1.19.0。发现在 worker 1 上运行的一些 pod 被驱逐了</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221129225911143.png" alt="image-20221129225911143" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Pod被驱逐的日志</center>
<center><em>Source：</em>https://www.containiq.com/post/kubernetes-pod-evictions</center><br>
<p>从上图可以看出有很多pod被驱逐了，报错信息也很清楚。由于节点上存储资源不足，导致kubelet触发驱逐过程。</p>
<h3 id="方法1启用auto-scaler">方法1：启用auto-scaler</h3>
<ul>
<li>
<p>向集群添加工作节点，要么部署cluster-autoscaler以根据配置的条件自动扩缩容。</p>
</li>
<li>
<p>只增加worker的本地存储空间，这涉及到虚拟机的扩容，会导致worker节点暂时不可用。</p>
</li>
</ul>
<h3 id="方法2保护关键pod">方法2：保护关键Pod</h3>
<p>在资源清单中指定资源请求和限制，配置QoS (<em><strong>Quality of Service</strong></em>)。当kubelet触发驱逐时，将至少保证这些 pod 不受影响。</p>
<p>这种施在一定程度上保证了一些关键Pod的可用性。如果节点出现问题时 Pod 没有被驱逐，这将需要执行更多步骤来查找故障。</p>
<p>运行命令 <code>kubectl get pods</code> 结果显示很多 pod 处于 evicted 状态。检查结果将保存在节点的kubelet日志中。查找对应日志使用 <code>cat /var/paas/sys/log/kubernetes/kubelet.log | grep -i Evicted -C3</code>。</p>
<h3 id="检查思路">检查思路</h3>
<h4 id="查看pod容忍度">查看Pod容忍度</h4>
<p>当Pod故障无法连接或节点无法响应时，可以使用 <code>tolerationSeconds</code> 配置对应时长长短</p>
<pre><code class="language-yaml">tolerations:
  - key: &quot;node.kubernetes.io/unreachable&quot;
    operator: &quot;Exists&quot;
    effect: &quot;NoExecute&quot;
    tolerationSeconds: 6000
</code></pre>
<h4 id="查看防止-pod-驱逐的条件">查看防止 Pod 驱逐的条件</h4>
<p>如果<strong>集群中的节点数小于50，并且故障节点数超过总节点数的55%</strong>，则暂停 Pod 驱逐。在这种情况下，Kubernetes 将尝试驱逐故障节点的工作负载（运行在kubernetes中的APP）。</p>
<p>下属json描述了一个健康的节点</p>
<pre><code class="language-json">&quot;conditions&quot;: [
    {
        &quot;type&quot;: &quot;Ready&quot;,
        &quot;status&quot;: &quot;True&quot;,
        &quot;reason&quot;: &quot;KubeletReady&quot;,
        &quot;message&quot;: &quot;kubelet is posting ready status&quot;,
        &quot;lastHearbeatTime&quot;: &quot;2019-06-05T18:38:35Z&quot;,
        &quot;lastTransitionTime&quot;: &quot;2019-06-05T11:41:27Z&quot;
    }
]
</code></pre>
<p>如果就绪条件为 <code>Unknown </code> 或 <code>False</code> 的时间超过了 pod-eviction-timeout，node controller 将对分配给该节点上的所有 Pod 执行 <code>API-initiated</code> 类型驱逐。</p>
<h4 id="检查pod的已分配资源">检查Pod的已分配资源</h4>
<p>Pod会根据节点的资源使用情况被逐出。被逐出的Pod将会根据分配给Pod的节点资源进行调度。管理驱逐”和“调度”的条件由不同的规则组成。这种结果会导致，被逐出的容器可能会被重新安排到原始节点。因此，要合理分配资源给每个容器。</p>
<h4 id="检查pod-是否定期失败">检查Pod 是否定期失败</h4>
<p>Pod 可以被驱逐多次。即如果在 Pod 被驱逐并调度到新节点后该节点中的 Pod 也被驱逐，则该 Pod 将再次被驱逐。</p>
<p>如果驱逐动作是由 <em>kube-controller-manager</em> 触发的，则保留处于 <em>Terminating</em> 状态的 Pod 。在节点恢复后，Pod将被 自动销毁。如果节点已经被删除或者其他原因无法恢复，可以强制删除Pod。</p>
<p>如果是由 <em>kubelet</em> 触发的驱逐，Pod 状态将保留为 Evicted 状态。仅用于后期故障定位，可直接删除。</p>
<p>删除被逐出的 Pod 命令为：</p>
<pre><code class="language-bash">kubectl get pods | grep Evicted | awk ‘{print $1}’ | xargs kubectl delete pod
</code></pre>
<blockquote>
<p>Notes：</p>
<ul>
<li>被Kubernetes驱逐的Pod，不会被自动重新创建 pod。如果要重新创建Pod，需要使用replicationcontroller、replicaset和 deployment 机制，这也是上述提到的Kubernetes的工作负载。</li>
<li>Pod控制器是协调一组Pod始终为理想状态的控制器，所以会删除后重建，也是Kubernetes 声明式API的特点</li>
</ul>
</blockquote>
<h2 id="如何监控被驱逐的pod">如何监控被驱逐的Pod</h2>
<h3 id="使用prometheus">使用Prometheus</h3>
<pre><code class="language-bash">kube_pod_status_reason{reason=&quot;Evicted&quot;} &gt; 0
</code></pre>
<h3 id="使用-containiq">使用 ContainIQ</h3>
<p>ContainIQ 是为Kubernetes设计的可观测性工具，其中包含Kubernetes 事件仪表板，这就包括 Pod 驱逐事件</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Scheduling, Preemption and Eviction</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Node-pressure Eviction</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Taints and Tolerations</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>API-initiated Eviction</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Safely Drain a Node</strong></em></a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#draining-multiple-nodes-in-parallel" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Draining multiple nodes in parallel</strong></em></a></p>
<p><sup id="7">[7]</sup> <a href="https://www.containiq.com/post/kubernetes-pod-evictions" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Kubernetes Pod Evictions | Troubleshooting and Examples</strong></em></a></p>
<p><sup id="8">[8]</sup> <a href="https://sysdig.com/blog/kubernetes-pod-evicted/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>kubernetes pod evicted</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes 4A - Audit源码解析</title>
      <link>https://www.oomkill.com/2022/11/ch34-auditing/</link>
      <pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/11/ch34-auditing/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<blockquote>
<p>本文是关于Kubernetes 4A解析的第四章</p>
<ul>
<li><a href="https://cylonchau.github.io/kubernetes-authentication.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authentication源码解析</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-authorization.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authorization源码解析</a></li>
<li><a href="https://cylonchau.github.io/ch3.7-admission-webhook.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Admission Control源码解析</a></li>
<li>深入理解Kubernetes 4A - Audit源码解析</li>
</ul>
<p>所有关于Kubernetes 4A四部分代码上传至仓库 github.com/cylonchau/hello-k8s-4A</p>
</blockquote>
<p>审计是信息系统中非常重要的一部分，Kubernetes 1.11中也增加了审计 (<em><strong>Auditing</strong></em>) 功能，通过审计功能获得 deployment, ns,等资源操作的事件。</p>
<p><strong>objective</strong>：</p>
<ul>
<li>从设计角度了解Auditing在kubernets中是如何实现的</li>
<li>了解kubernetes auditing webhook</li>
<li>完成实验，通过webhook来收集审计日志</li>
</ul>
<p>如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大。</p>
<h2 id="kubernetes-auditing">Kubernetes Auditing</h2>
<p>根据Kubernetes官方描述审计在kubernetes中是有控制平面 <em>kube-apiserver</em> 中产生的一个事件，记录了集群中所操作的资源，审计围绕下列几个维度来记录事件的：</p>
<ul>
<li>发生了什么</li>
<li>发生的事件</li>
<li>谁触发的</li>
<li>发生动作的对象</li>
<li>在哪里检查到动作的</li>
<li>从哪触发的</li>
<li>处理行为是什么</li>
</ul>
<p>审计生命周期开始于组件 <em>kube-apiserver</em> 准入控制阶段，在每个阶段内都会产生审计事件并经过预处理后写入后端，目前后端包含webhook与日志文件。</p>
<blockquote>
<p>审计日志功能增加了 <em>kube-apiserver</em> 的内存消耗，因为会为每个请求存储了审计所需的上下文。内存的消耗取决于审计日志配置 <sup><a href="#1">[1]</a></sup>。</p>
</blockquote>
<h2 id="审计事件设计">审计事件设计</h2>
<p>审计的schema不同于资源API的设计，没有 <code>metav1.ObjectMeta</code> 属性，Event是一个事件的结构体，Policy是事件配置，属于kubernetes资源，在代码 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/apis/audit/types.go#L79-L148" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/apis/audit/types.go</a> 可以看到</p>
<pre><code class="language-go">type Event struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	Level Level `json:&quot;level&quot; protobuf:&quot;bytes,1,opt,name=level,casttype=Level&quot;
	AuditID types.UID `json:&quot;auditID&quot; protobuf:&quot;bytes,2,opt,name=auditID,casttype=k8s.io/apimachinery/pkg/types.UID&quot;`
	
	Stage Stage `json:&quot;stage&quot; protobuf:&quot;bytes,3,opt,name=stage,casttype=Stage&quot;`
	RequestURI string `json:&quot;requestURI&quot; protobuf:&quot;bytes,4,opt,name=requestURI&quot;`
	Verb string `json:&quot;verb&quot; protobuf:&quot;bytes,5,opt,name=verb&quot;`
	User authnv1.UserInfo `json:&quot;user&quot; protobuf:&quot;bytes,6,opt,name=user&quot;`
	ImpersonatedUser *authnv1.UserInfo `json:&quot;impersonatedUser,omitempty&quot; protobuf:&quot;bytes,7,opt,name=impersonatedUser&quot;`
	SourceIPs []string `json:&quot;sourceIPs,omitempty&quot; protobuf:&quot;bytes,8,rep,name=sourceIPs&quot;`
	UserAgent string `json:&quot;userAgent,omitempty&quot; protobuf:&quot;bytes,16,opt,name=userAgent&quot;`
	ObjectRef *ObjectReference `json:&quot;objectRef,omitempty&quot; protobuf:&quot;bytes,9,opt,name=objectRef&quot;`
	// +optional
	ResponseStatus *metav1.Status `json:&quot;responseStatus,omitempty&quot; protobuf:&quot;bytes,10,opt,name=responseStatus&quot;`

...
	
}
</code></pre>
<p>对于记录的认证事件来说，会根据请求阶段记录审计的阶段，主要分为下属集中情况，每个请求会记录其中一个验证阶段，如代码所示 <sup><a href="#1">[1]</a></sup></p>
<pre><code class="language-go">const (
    // 这个阶段是audit handler收到请求后立即生成事件的阶段，然后委托handler chain处理。
	StageRequestReceived Stage = &quot;RequestReceived&quot;
    // 这个阶段阶段仅对长时间运行的请求如 watch
	// 将在发送响应标头后，响应正文之前生成的阶段
	StageResponseStarted Stage = &quot;ResponseStarted&quot;
    // 这个阶段是发送相应体后的事件。
	StageResponseComplete Stage = &quot;ResponseComplete&quot;
	// 如果程序出现panic，则触发这个阶段
	StagePanic Stage = &quot;Panic&quot;
)
</code></pre>
<h2 id="审计工作流程">审计工作流程</h2>
<p>审计真正工作的地方在 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go#L42-L119" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/endpoints/filters/audit.go.WithAudit</a> 函数，下面对与官方文档说明与这个实际代码进行结合</p>
<pre><code class="language-go">func WithAudit(handler http.Handler, sink audit.Sink, policy audit.PolicyRuleEvaluator, longRunningCheck request.LongRunningRequestCheck) http.Handler {
    // sink是一个backend（webhook 或 日志），policy则是自定义的事件配置
    // 如果两者之一未配置，则不会使用审计功能
	if sink == nil || policy == nil {
		return handler
	}
	return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) {
        // 通过给定的配置与请求构建出一个事件 context
        // 这里可以看到
		auditContext, err := evaluatePolicyAndCreateAuditEvent(req, policy)
		if err != nil {
			utilruntime.HandleError(fmt.Errorf(&quot;failed to create audit event: %v&quot;, err))
			responsewriters.InternalError(w, req, errors.New(&quot;failed to create audit event&quot;))
			return
		}
		// 下面代码可以看出是对事件context进行构建，与拿到来自请求Context
		ev := auditContext.Event
		if ev == nil || req.Context() == nil {
			handler.ServeHTTP(w, req)
			return
		}

		req = req.WithContext(audit.WithAuditContext(req.Context(), auditContext))

		ctx := req.Context()
		omitStages := auditContext.RequestAuditConfig.OmitStages
		
        // 这里到StageRequestReceived阶段，如果是收到请求阶段则通过注入的后端进行处理
		ev.Stage = auditinternal.StageRequestReceived
		if processed := processAuditEvent(ctx, sink, ev, omitStages); !processed {
            audit.ApiserverAuditDroppedCounter.WithContext(ctx).Inc()
			responsewriters.InternalError(w, req, errors.New(&quot;failed to store audit event&quot;))
			return
		}

		// 拦截watch类长请求的状态码
		var longRunningSink audit.Sink
		if longRunningCheck != nil {
			ri, _ := request.RequestInfoFrom(ctx)
			if longRunningCheck(req, ri) {
				longRunningSink = sink
			}
		}
		respWriter := decorateResponseWriter(ctx, w, ev, longRunningSink, omitStages)

		// send audit event when we leave this func, either via a panic or cleanly. In the case of long
		// running requests, this will be the second audit event.
        // 在离开函数前会处理 ResponseStarted、ResponseComplete、Panic这三个阶段
		defer func() {
			if r := recover(); r != nil {
				defer panic(r)
                // 当前发生panic的请求
				ev.Stage = auditinternal.StagePanic
				ev.ResponseStatus = &amp;metav1.Status{
					Code:    http.StatusInternalServerError,
					Status:  metav1.StatusFailure,
					Reason:  metav1.StatusReasonInternalError,
					Message: fmt.Sprintf(&quot;APIServer panic'd: %v&quot;, r),
				}
				processAuditEvent(ctx, sink, ev, omitStages)
				return
			}

			// if no StageResponseStarted event was sent b/c neither a status code nor a body was sent, fake it here
			// But Audit-Id http header will only be sent when http.ResponseWriter.WriteHeader is called.
			fakedSuccessStatus := &amp;metav1.Status{
				Code:    http.StatusOK,
				Status:  metav1.StatusSuccess,
				Message: &quot;Connection closed early&quot;,
			}
			if ev.ResponseStatus == nil &amp;&amp; longRunningSink != nil {
				ev.ResponseStatus = fakedSuccessStatus
				ev.Stage = auditinternal.StageResponseStarted
				processAuditEvent(ctx, longRunningSink, ev, omitStages)
			}
			// ResponseStarted 在响应头发送后，响应体发送前的事件。watch会触发他
            
			ev.Stage = auditinternal.StageResponseComplete
			if ev.ResponseStatus == nil {
                // 没有相应状态 正是上面构造的fakedSuccessStatus
				ev.ResponseStatus = fakedSuccessStatus
			}
            // 将事件发送到后端
			processAuditEvent(ctx, sink, ev, omitStages)
		}()
		handler.ServeHTTP(respWriter, req)
	})
}
</code></pre>
<ul>
<li>
<p>在评估请求时，会调用 <code>GetAuthorizerAttributes(ctx)</code> ，这里通过授权记录然后来通过给定的审计配置来</p>
</li>
<li>
<p>当在将事件发送到后端时，使用 <code>processAuditEvent()</code> 函数，最终修改时间后会转交至后端函数，例如webhook，会请求后端配置的webhook url的客户端，最终被执行 <code>return sink.ProcessEvents(ev)</code></p>
<pre><code class="language-go">func (b *backend) processEvents(ev ...*auditinternal.Event) error {
	var list auditinternal.EventList
	for _, e := range ev {
		list.Items = append(list.Items, *e)
	}
	return b.w.WithExponentialBackoff(context.Background(), func() rest.Result {
		trace := utiltrace.New(&quot;Call Audit Events webhook&quot;,
			utiltrace.Field{&quot;name&quot;, b.name},
			utiltrace.Field{&quot;event-count&quot;, len(list.Items)})
		// Only log audit webhook traces that exceed a 25ms per object limit plus a 50ms request overhead allowance. The high per object limit used here is primarily to allow enough time for the serialization/deserialization of audit events, which contain nested request and response objects plus additional event fields.
		defer trace.LogIfLong(time.Duration(50+25*len(list.Items)) * time.Millisecond)
		return b.w.RestClient.Post().Body(&amp;list).Do(context.TODO())
	}).Error()
}
</code></pre>
</li>
</ul>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go#L125-L155" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/endpoints/filters/audit.go.evaluatePolicyAndCreateAuditEvent</a> 会评估请求的级别和规则，而 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/audit/policy/checker.go#L64-L84" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/audit/policy/checker.go</a></p>
<pre><code class="language-go">func (p *policyRuleEvaluator) EvaluatePolicyRule(attrs authorizer.Attributes) auditinternal.RequestAuditConfigWithLevel {
	for _, rule := range p.Rules {
        // 评估则是评估用户与用户组，verb，ns,非API资源 /metrics /healthz
		if ruleMatches(&amp;rule, attrs) {
            // 通过后，则将这条规则与配置返回
			return auditinternal.RequestAuditConfigWithLevel{
				Level: rule.Level,
				RequestAuditConfig: auditinternal.RequestAuditConfig{
					OmitStages:        rule.OmitStages,
					OmitManagedFields: isOmitManagedFields(&amp;rule, p.OmitManagedFields),
				},
			}
		}
	}
	// 如果条件都不满足，则构建一个
	return auditinternal.RequestAuditConfigWithLevel{
		Level: DefaultAuditLevel,
		RequestAuditConfig: auditinternal.RequestAuditConfig{
			OmitStages:        p.OmitStages,
			OmitManagedFields: p.OmitManagedFields,
		},
	}
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/audit/request.go#L48-L93" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/audit/request.go.NewEventFromRequest</a> 创建出审计事件对象被上面 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go#L125-L155" target="_blank"
   rel="noopener nofollow noreferrer" >evaluatePolicyAndCreateAuditEvent</a> 返回</p>
<pre><code class="language-go">// evaluatePolicyAndCreateAuditEvent is responsible for evaluating the audit
// policy configuration applicable to the request and create a new audit
// event that will be written to the API audit log.
// - error if anything bad happened
func evaluatePolicyAndCreateAuditEvent(req *http.Request, policy audit.PolicyRuleEvaluator) (*audit.AuditContext, error) {
	ctx := req.Context()

	attribs, err := GetAuthorizerAttributes(ctx)
	if err != nil {
		return nil, fmt.Errorf(&quot;failed to GetAuthorizerAttributes: %v&quot;, err)
	}

	ls := policy.EvaluatePolicyRule(attribs)
	audit.ObservePolicyLevel(ctx, ls.Level)
	if ls.Level == auditinternal.LevelNone {
		// Don't audit.
		return &amp;audit.AuditContext{
			RequestAuditConfig: ls.RequestAuditConfig,
		}, nil
	}

	requestReceivedTimestamp, ok := request.ReceivedTimestampFrom(ctx)
	if !ok {
		requestReceivedTimestamp = time.Now()
	}
	ev, err := audit.NewEventFromRequest(req, requestReceivedTimestamp, ls.Level, attribs)
	if err != nil {
		return nil, fmt.Errorf(&quot;failed to complete audit event from request: %v&quot;, err)
	}

	return &amp;audit.AuditContext{
		RequestAuditConfig: ls.RequestAuditConfig,
		Event:              ev,
	}, nil
}
</code></pre>
<p>到这里，已经清楚的了解到，Kubernetes审计工作与什么位置了，而对于Kubernetes准入给出的登录（<em><strong>Authentication</strong></em>），授权 (<em><strong>Authorization</strong></em>) 与 准入控制 (<em><strong>Admission control</strong></em>) 三个阶段来说，Audition 位于授权之后，正如下图所示，而这个真正的流程在kubernetes中有个属于叫 <em><strong>handler chain</strong></em> 整个链条中，准入与审计只是其中一部分。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221128001225515.png" alt="image-20221128001225515" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes 4A 的 handler chain</center><br>
<p>由图再结合代码可以看出，所有的客户端访问API都需要经过完整经由整个链条，而 Auditing 事件的构建是需要获取经由验证过的用户等资源构建出的事件，首次发生的为 <code>StageRequestReceived</code> ，这将在收到请求后执行，而由代码又可知，因为在最终结束掉整个请求时会执行 <code>WithAudit</code> 函数，这就为 <code>StageResponseComplete</code> 与 <code>StageResponseStarted</code> 这两个阶段被执行，而这个将发生在被注册的 handler 完成后，也就是  <em><strong>Admission control</strong></em> 后因为 AC 是在每个真实REST中被执行。TODO</p>
<h2 id="审计策略级别-supa-href22asup">审计策略级别 <sup><a href="#2">[2]</a></sup></h2>
<p>审计策略级别是控制审计记录将记录那些对象的数据内容，当事件被处理时，会按照配置的审计规则进行比较。而使用该功能需要 <em>kube-apiserver</em> 开启参数 <code>--audit-policy-file</code> 指定对应的配置，如果未指定则默认不记录任何事件，可供定义的级别有四个，被定义在 k8s.io/apiserver/pkg/apis/audit/v1/types.go 中</p>
<pre><code class="language-go">const (
	// LevelNone disables auditing
	LevelNone Level = &quot;None&quot;
	// LevelMetadata provides the basic level of auditing.
	LevelMetadata Level = &quot;Metadata&quot;
	// LevelRequest provides Metadata level of auditing, and additionally
	// logs the request object (does not apply for non-resource requests).
	LevelRequest Level = &quot;Request&quot;
	// LevelRequestResponse provides Request level of auditing, and additionally
	// logs the response object (does not apply for non-resource requests).
	LevelRequestResponse Level = &quot;RequestResponse&quot;
)
</code></pre>
<ul>
<li><strong>None</strong>： 不记录符合该规则的事件</li>
<li><strong>Metadata</strong>：只记录请求元数据（如User, timestamp, resources, verb），不记录请求和响应体。</li>
<li><strong>Request</strong>：记录事件元数据和请求体，不记录响应体。</li>
<li><strong>RequestResponse</strong>： 记录事件元数据，请求和响应体</li>
</ul>
<p>下面是Kubernetes官网给出的 Policy 的配置 <sup><a href="#2">[2]</a></sup></p>
<pre><code class="language-yaml">apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
# omitStages 代表忽略该阶段所有请求事件
# RequestReceived 这里配置的指在RequestReceived阶段忽略所有请求事件
omitStages:
  - &quot;RequestReceived&quot;
rules:
  # 记录将以RequestResponse级别的格式记录pod更改
  - level: RequestResponse
    resources:
    - group: &quot;&quot;
      # 这里资源的配置必须与RBAC配置的一致，pods将不支持pods/log这类子资源
      resources: [&quot;pods&quot;]
  # 如果需要配置子资源按照下列方式
  - level: Metadata
    resources:
    - group: &quot;&quot;
      resources: [&quot;pods/log&quot;, &quot;pods/status&quot;]

  # 不记录的资源为controller-leader的configmaps资源的请求
  - level: None
    resources:
    - group: &quot;&quot;
      resources: [&quot;configmaps&quot;]
      resourceNames: [&quot;controller-leader&quot;]

  # 不记录用户为 &quot;system:kube-proxy&quot; 发起的对 endpoints与services资源的watch请求事件
  - level: None
    users: [&quot;system:kube-proxy&quot;]
    verbs: [&quot;watch&quot;]
    resources:
    - group: &quot;&quot; # core API group
      resources: [&quot;endpoints&quot;, &quot;services&quot;]

  # 每个登录成功的用户，都会被追加一个用户组为 &quot;system:authenticated&quot;
  # 下述规则为不记录包含非资源类型的URL的已认证请求
  - level: None
    userGroups: [&quot;system:authenticated&quot;]
    nonResourceURLs:
    - &quot;/api*&quot; # Wildcard matching.
    - &quot;/version&quot;

  # 记录kube-system名称空间configmap更改事件的请求体与元数据
  - level: Request
    resources:
    - group: &quot;&quot; # core API group
      resources: [&quot;configmaps&quot;]
    # This rule only applies to resources in the &quot;kube-system&quot; namespace.
    # The empty string &quot;&quot; can be used to select non-namespaced resources.
    namespaces: [&quot;kube-system&quot;]

  # 事件将记录所有名称空间内对于configmap与secret资源改变的元数据
  - level: Metadata
    resources:
    - group: &quot;&quot; # core API group
      resources: [&quot;secrets&quot;, &quot;configmaps&quot;]

  # 记录对group为core与extensions下的资源类型请求的 请求体与元数据（request级别）
  - level: Request
    resources:
    - group: &quot;&quot; # core API group
    - group: &quot;extensions&quot; # Version of group should NOT be included.

  # 这种属于泛规则，会记录所有上述其他之外的所有类型请求的元数据
  # 类似于授权，小权限在前，* 最后
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - &quot;RequestReceived&quot;
</code></pre>
<h2 id="backend-supa-href33asup">Backend <sup><a href="#3">[3]</a></sup></h2>
<p>kubernetes目前为Auditing 提供了两个后端，日志方式与webhook方式，kubernetes审计事件会遵循 <code>audit.k8s.io</code> 结构写入到后端。</p>
<h3 id="日志模式配置">日志模式配置</h3>
<p>启用日志模式只需要配置几个参数 <sup><a href="#4">[4]</a></sup></p>
<ul>
<li><code>--audit-log-path</code> 写入审计事件的日志路径。这个是必须配置的否则默认输出到STDOUT</li>
<li><code>--audit-log-maxage</code>  审计日志文件保留的最大天数</li>
<li><code>--audit-log-maxbackup</code> 审计日志保留的的最大数量</li>
<li><code>--audit-log-maxsize</code> 审计日志文件最大大小（单位M）大于会切割</li>
</ul>
<p>例如配置</p>
<pre><code class="language-bash">--audit-policy-file=/etc/kubernetes/audit-policy.yaml \
--audit-log-path=/var/log/kubernetes/audit.log \
--audit-log-maxsize=20M
</code></pre>
<h3 id="webhook-supa-href55asup">webhook <sup><a href="#5">[5]</a></sup></h3>
<p>webhook是指审计事件将由 <em>kube-apiserver</em> 发送到webhook服务中记录，开启webhook只需要配置 <code>--audit-webhook-config-file</code> 与 <code>--audit-policy-file</code> 两个参数，而其他的则是对该模式的辅助</p>
<ul>
<li>
<p><code>--audit-webhook-config-file</code> ：webhook的配置文件，格式是kubeconfig类型，所有的信息不是kubernetes api配置，而是webhook相关信息</p>
</li>
<li>
<p><code>--audit-webhook-initial-backoff </code> ：第一次失败后重试事件，随后仍失败后将以指数方式退避重试</p>
</li>
<li>
<p><code>--audit-webhook-mode</code> ：发送至webhook的模式。 <em>batch</em>, <em>blocking</em>, <em>blocking-strict</em> 。</p>
</li>
</ul>
<p>例如配置</p>
<pre><code class="language-yaml">--audit-policy-file=/etc/kubernetes/audit-policy.yaml \
--audit-webhook-config-file=/etc/kubernetes/auth/audit-webhook.yaml \
--audit-webhook-mode=batch \
</code></pre>
<p>对于initialBackoff 的退避重试则如代码所示 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/server/options/audit.go#L584-L594" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/options/audit.go</a></p>
<pre><code class="language-go">func (o *AuditWebhookOptions) newUntruncatedBackend(customDial utilnet.DialFunc) (audit.Backend, error) {
	groupVersion, _ := schema.ParseGroupVersion(o.GroupVersionString)
	webhook, err := pluginwebhook.NewBackend(o.ConfigFile, groupVersion, webhook.DefaultRetryBackoffWithInitialDelay(o.InitialBackoff), customDial)
	if err != nil {
		return nil, fmt.Errorf(&quot;initializing audit webhook: %v&quot;, err)
	}
	webhook = o.BatchOptions.wrapBackend(webhook)
	return webhook, nil
}
</code></pre>
<p>在函数 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go#L42-L49" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/util/webhook/webhook.go.DefaultRetryBackoffWithInitialDelay</a> 中看到 通过 wait.Backoff 进行的</p>
<pre><code class="language-go">return wait.Backoff{
    // 时间间隔，用于调用 Step 方法时返回的时间间隔
    Duration: initialBackoffDelay,  
   	
    // 用于计算下次的时间间隔，不能为负数
    // Factor 大于 0 时，Backoff 在计算下次的时间间隔时都会根据 
    // Duration * Factor，Factor * Duration 不能大于 Cap
    Factor:   1.5,
    
    // 抖动，Jitter &gt; 0 时，每次迭代的时间间隔都会额外加上 0 - Duration * Jitter 的随机时间,
    // 并且抖动出的时间不会设置为 Duration，而且不受 Caps 的限制
    Jitter:   0.2,
    
    // 进行指数回退(*Factor) 操作的次数
    // 当 Factor * Duration &gt; Cap 时 Steps 会被设置为 0, Duration 设置为 Cap
    // 也就是说后续的迭代时间间隔都会返回 Duration
    Steps:    5,
    
    // 还有一个cap（Cap time.Duration），是最大的时间间隔
}
</code></pre>
<h3 id="批处理">批处理</h3>
<p>日志后端与webhook后端都支持批处理模式，默认值为webhook默认开启batch，而log则被禁用</p>
<ul>
<li><code>--audit-log-mode/--audit-webhook-mode</code> ：参数通过将webhook替换为log则为对应的 batch 模式的参数，可以通过 <code>kube-apiserver --help|grep &quot;audit&quot;|grep batch</code> 查看
<ul>
<li><code>batch</code> 默认值，缓冲事件进行异步批量处理
<ul>
<li><code>--audit-webhook-batch-buffer-size</code>：批处理之前要缓冲的事件数。如果传入事件的溢出，则被丢弃。</li>
<li><code>--audit-webhook-batch-max-size</code>：定义每一批中的最大事件数</li>
<li><code>--audit-webhook-batch-max-wait</code>：批处理队列未满时等待的事件，到时强制写入一次</li>
<li><code>--audit-webhook-batch-throttle-qps</code>：定义每秒最大平均批次</li>
<li><code>--audit-webhook-batch-throttle-burst</code>：如果之前还没使用throttle-qps之前，发送的最大批数，通常情况下为第一次启动时生效的参数</li>
</ul>
</li>
<li><code>blocking</code> 阻止 apiserver 处理每个单独事件</li>
<li><code>blocking-strict</code>：与 <em>blocking</em> 相同，但当 <em>RequestReceived</em> 阶段的审计日志记录失败时，对 kube-apiserver 的整个请求都将失败</li>
</ul>
</li>
</ul>
<h2 id="参数调整">参数调整</h2>
<p>适当的调整参数与策略可以有效适应 <em>kuber-apiserver</em> 的负载，如在记录日志时应只记录所需的事件，而不是所有的事件，这样可以避免 APIServer不必要开销，例如：</p>
<ul>
<li>每个请求存在多个阶段，而审计时其实不关心响应等信息，可以只记录 <code>RequestReceived</code> 的 <code>metadata</code> 级别。</li>
<li>&ldquo;pods/log&rdquo;, &ldquo;pods/status&rdquo; 在记录时应该区分子资源类型，而不要直接写 <em><code>pods</code></em> 或 <em><code>pods/*</code></em></li>
<li>kubernetes系统组件内的事件如果没有特殊要求可以不记录</li>
<li>对于资源类型，如configmap的请求其实没必要记录</li>
<li>审计记录应严格按照外部用户记录，而不是所有请求</li>
</ul>
<p>如何适配APIServer的负载能力，正如官方给的示例一样，如果 <em>kube-apiserver</em> 每秒收到100个请求，而记录事件为 <code>ResponseStarted</code> 和<code>ResponseComplete</code> 阶段，此时会记录的条数约 200/s ，如果batch缓冲区为100，那么需要配置的参数至少2Qps/s。再假设后端处理能力为5秒，那么缓冲区需要配置的大小至少为5秒的事件，即1000条evnet，10个batch。正如下图所示：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/624219-20220802143739996-2062420228.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：审计批处理参数调优结构图</center>
<center><em>Source：</em>https://www.cnblogs.com/zhangmingcheng/p/16539514.html</center><br>
<p>而kube-apiserver提供了两个Prometheus指标可以用于监控审计子系统的状态</p>
<ul>
<li><code>apiserver_audit_event_total</code> 审计事件的总数</li>
<li><code>apiserver_audit_error_total</code> 由于错误而被丢弃的审计事件总数，例如panic类型事件</li>
</ul>
<h2 id="实验audit-webhook">实验：Audit Webhook</h2>
<p>编写一个webhook，用于处理接收到的日志，这里直接打印</p>
<pre><code class="language-go">func serveAudit(w http.ResponseWriter, r *http.Request) {
	b, err := ioutil.ReadAll(r.Body)
	if err != nil {
		httpError(w, err)
		return
	}

	var eventList audit.EventList
	err = json.Unmarshal(b, &amp;eventList)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	for _, event := range eventList.Items {

		// here is your logic

		fmt.Printf(&quot;审计ID %s: 用户&lt;%s&gt;, 请求对象&lt;%s&gt;, 操作&lt;%s&gt;, 请求阶段&lt;%s&gt;\n&quot;,
			event.AuditID,
			event.User.UID,
			event.RequestURI,
			event.Verb,
			event.Stage,
		)
	}
	w.WriteHeader(http.StatusOK)
}
</code></pre>
<p>当使用命令执行查看Pod的操作时，会看到webhook收到的下述审计日志</p>
<p>操作命令</p>
<pre><code class="language-bash">for n in `seq 1 100`; do kubectl get pod --user=admin; done
</code></pre>
<p>审计日志</p>
<pre><code class="language-log">审计ID c0313416-f950-4361-9823-7c4792b143fd: 用户&lt;admin&gt;, 请求对象&lt;/api/v1/namespaces/default/pods?limit=500&gt;, 操作&lt;list
&gt;, 请求阶段&lt;ResponseComplete&gt;
审计ID db2390c1-83cf-42e7-b589-70cd04003d0e: 用户&lt;admin&gt;, 请求对象&lt;/api/v1/namespaces/default/pods?limit=500&gt;, 操作&lt;list
&gt;, 请求阶段&lt;ResponseComplete&gt;
审计ID a8fc2ff9-d0c5-4263-901c-b5974fd58026: 用户&lt;admin&gt;, 请求对象&lt;/api/v1/namespaces/default/pods?limit=500&gt;, 操作&lt;list
&gt;, 请求阶段&lt;ResponseComplete&gt;
</code></pre>
<h2 id="总结">总结</h2>
<p>kubernetes通过插件的方式提供了提供了一个IT系统 4A模型为集群提供了安全保障，与传统的4A (<em><strong>Authentication</strong></em>, <em><strong>Authorization</strong></em>, <em><strong>Accounting</strong></em>, <em><strong>Auditing</strong></em>) 不同的是，对于 <em>Accounting</em> 与 <em>Authentication</em> 在kubernetes中设计来说 Kubernetes没有用户的实现而是一个抽象，这使得Kubernetes可以更灵活使用任意的用户系统完成登录（OID, X.509, webhook, proxy, SA&hellip;.），而对于授权来说，Kubernetes 通过多种授权模型(RBAC, ABAC, Node, Webhook)，为集群提供了灵活的权限；而不同的是，通过 <em><strong>Admission Control</strong></em> 可以为集群提供更多的安全策略，例如镜像策略，通过三方提供的控制器来自定义更多的安全策略，如OPA。而这种设计为Kubernetes集群提供了一种更灵活的安全。</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Auditing</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Audit policy</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-backends" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Audit backends</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#log-backend" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Log backend</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#webhook-backend" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Webhook backend</strong></em></a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#batching" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Event batching</strong></em></a></p>
<p><sup id="7">[7]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#parameter-tuning" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Parameter tuning</strong></em></a></p>
<p><sup id="8">[8]</sup> <a href="https://ldapwiki.com/wiki/Privilege%20Management%20Infrastructure" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Privilege Management Infrastructure</strong></em></a></p>
<p><sup id="9">[9]</sup> <a href="https://www.cnblogs.com/zhangmingcheng/p/16539514.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Kubernetes 审计（Auditing）功能详解</strong></em></a></p>
<p><sup id="10">[10]</sup> <a href="https://blog.tianfeiyu.com/2019/01/30/k8s-audit-webhook/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>kubernetes 审计日志功能</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes 4A - Authorization源码解析</title>
      <link>https://www.oomkill.com/2022/11/ch32-authorization/</link>
      <pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/11/ch32-authorization/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>在 Kubernetes 中，当一个访问请求通过了登录阶段（<em><strong>Authentication</strong></em>），必须还需要请求拥有该对象的访问权限，而授权部分也是Kubernetes API 访问控制中的第二个部分 <em><strong>Authorization</strong></em> .</p>
<p><em><strong>Authorization</strong></em> 在 Kubernetes中是以评估发起请求的用户，根据其身份特性评估这次请求是被 ”拒绝“ 还是 “允许”，同访问控制三部曲中其他两个插件 (<em><strong>Authentication</strong></em>, <em><strong>Adminssion Control</strong></em>) 一样，<em><strong>Authorization</strong></em> 也可以同时配置多个，当收到用户的请求时，会依次检查这个阶段配置的所有模块，如果任何一个模块对该请求授予权限（拒绝或允许），那么该阶段会直接返回，当所有模块都没有该用户所属的权限时，默认是拒绝，在Kubernetes中，被该插件拒绝的用户显示为HTTP 403。</p>
<p>如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大</p>
<p><strong>objective</strong>：</p>
<ul>
<li>了解kubernetes Authorization机制</li>
<li>了解授权系统的设计</li>
<li>完成实验，使用 OPA 作为 Kubernetes 外部用户，权限认证模型 <em>RBAC</em> 的替代品</li>
</ul>
<h2 id="kubernetes是如何对用户授权的">Kubernetes是如何对用户授权的</h2>
<p>kubernetes对用户授权需要遵守的shema必须拥有下列属性，代码位于<a href="https://github.com/kubernetes/kubernetes/blob/57eb5d631ccd615cd161b6da36afc759af004b93/pkg/apis/authorization/types.go#L27-L36" target="_blank"
   rel="noopener nofollow noreferrer" >pkg\apis\authorization\types.go</a></p>
<pre><code class="language-go">type SubjectAccessReview struct {
    // API必须实现的部分
	metav1.TypeMeta
	metav1.ObjectMeta
	// 请求需要遵守的属性
	Spec SubjectAccessReviewSpec
	// 请求被授权的状态
	Status SubjectAccessReviewStatus
}
</code></pre>
<p>这里可以看到数据模型是</p>
<pre><code class="language-go">type SubjectAccessReviewSpec struct {
	// ResourceAttributes describes information for a resource access request
	ResourceAttributes *ResourceAttributes
	// NonResourceAttributes describes information for a non-resource access request
	NonResourceAttributes *NonResourceAttributes

	// 请求的用户，必填
    // 如果只传递 User，而没有Group，那么权限必须与用户对应，例如rolebinding/clusterrolebing
    // 如果传递了User与Group，那么rolebinding/clusterrolebing权限最大为Group，最小为User
	User string
	// Groups是用户所属组，可以有多个
	Groups []string
	// Extra corresponds to the user.Info.GetExtra() method from the authenticator.  Since that is input to the authorizer
	// 这里通常对于验证和授权阶段，没有特别的需求
	Extra map[string]ExtraValue
	// UID 请求用户的UID，通常来说与User相同，Authentication中也是这么做的
	UID string
}
</code></pre>
<p>由此可得知，在授权部分，kubernetes要求请求必须存在</p>
<ul>
<li><strong>用户类属性</strong>：<strong>user</strong>，<strong>group</strong> ，<strong>extra</strong>  由 <em><strong>Authentication</strong></em> 提供的用户信息</li>
<li><strong>请求类属性</strong>：
<ul>
<li><strong>API资源</strong>： <code>curl $API_SERVER_URL/api/v1/namespaces</code></li>
<li><strong>请求路径</strong>： 非API资源格式的路径，<code>/api</code>，<code>/healthz</code></li>
<li><strong>verb</strong>：HTTP请求方法，GET，POST..</li>
</ul>
</li>
<li>资源类属性：
<ul>
<li>访问的资源的名称或ID，如Pod名</li>
<li>要访问的名称空间</li>
<li>资源所属组，Kubernetes资源有GVR组成</li>
</ul>
</li>
</ul>
<p>那么，<code>SubjectAccessReview.Spec</code> 为要审查的对象，<code>SubjectAccessReview.Status</code> 为审查结果，通常在每个请求到来时，入库前必定被审查</p>
<h2 id="kubernetes中的授权模式">Kubernetes中的授权模式</h2>
<p>知道授权的对象，就需要知道如何对该对象进行授权，Kubernetes authorizer 提供了下列授权模式</p>
<p><a href="pkg/kubeapiserver/authorizer/modes/modes.go">pkg/kubeapiserver/authorizer/modes/modes.go</a></p>
<pre><code class="language-go">const (
	// ModeAlwaysAllow is the mode to set all requests as authorized
	ModeAlwaysAllow string = &quot;AlwaysAllow&quot;
	// ModeAlwaysDeny is the mode to set no requests as authorized
	ModeAlwaysDeny string = &quot;AlwaysDeny&quot;
	// ModeABAC is the mode to use Attribute Based Access Control to authorize
	ModeABAC string = &quot;ABAC&quot;
	// ModeWebhook is the mode to make an external webhook call to authorize
	ModeWebhook string = &quot;Webhook&quot;
	// ModeRBAC is the mode to use Role Based Access Control to authorize
	ModeRBAC string = &quot;RBAC&quot;
	// ModeNode is an authorization mode that authorizes API requests made by kubelets.
	ModeNode string = &quot;Node&quot;
)
</code></pre>
<p>可以看出，大致遵循模式进行授权</p>
<ul>
<li>ModeABAC (<em><strong>Attribute-based access control</strong></em>)：是一种将属性分组，而后属性组分配给用户的模型，通常情况下这种模型很少使用</li>
<li>ModeRBAC (<em><strong>Role Based Access Control</strong></em>) ：是kubernetes主流的授权模型，是将用户分组，将属性分配给用户组的一种模型</li>
<li>ModeNode：对kubelet授权的方式</li>
<li>ModeWebhook：用户注入给Kubernetes 授权插件进行回调的一种授权模式</li>
</ul>
<h2 id="kubernetes-授权生命周期">Kubernetes 授权生命周期</h2>
<p>在启动 <code>kube-apiserver</code> 是都会初始化被注入一个 <code>Authorizer</code> 而这个被上面模式进行实现，如 <code>RBACAuthorizer</code> ,  <code>WebhookAuthorizer</code> <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/staging/src/k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go</a></p>
<pre><code class="language-go">type Authorizer interface {
	Authorize(ctx context.Context, a Attributes) (authorized Decision, reason string, err error)
}
</code></pre>
<p>在 Run 中会创建一个CreateServerChain，这里面可以看到对应注册进来的  <code>Authorizer </code>  <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/cmd/kube-apiserver/app/server.go#L155-L173" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io\kubernetes\cmd\kube-apiserver\app\server.go</a></p>
<pre><code class="language-go">// Run runs the specified APIServer.  This should never exit.
func Run(completeOptions completedServerRunOptions, stopCh &lt;-chan struct{}) error {
	// To help debugging, immediately log version
	klog.Infof(&quot;Version: %+v&quot;, version.Get())

	klog.InfoS(&quot;Golang settings&quot;, &quot;GOGC&quot;, os.Getenv(&quot;GOGC&quot;), &quot;GOMAXPROCS&quot;, os.Getenv(&quot;GOMAXPROCS&quot;), &quot;GOTRACEBACK&quot;, os.Getenv(&quot;GOTRACEBACK&quot;))

	server, err := CreateServerChain(completeOptions)
	if err != nil {
		return err
	}

	prepared, err := server.PrepareRun()
	if err != nil {
		return err
	}

	return prepared.Run(stopCh)
}
</code></pre>
<p>可以看到在创建这个 <code>Authorizer </code>  时会调用一个 <code>BuildAuthorizer</code> 构建这个 <code>Authorizer </code></p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/cmd/kube-apiserver/app/server.go#L448" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/kubernetes/cmd/kube-apiserver/app/server.go</a></p>
<pre><code class="language-go">func buildGenericConfig(
	s *options.ServerRunOptions,
	proxyTransport *http.Transport,
) (
	genericConfig *genericapiserver.Config,
	versionedInformers clientgoinformers.SharedInformerFactory,
	serviceResolver aggregatorapiserver.ServiceResolver,
	pluginInitializers []admission.PluginInitializer,
	admissionPostStartHook genericapiserver.PostStartHookFunc,
	storageFactory *serverstorage.DefaultStorageFactory,
	lastErr error,
) {
    
	...

	genericConfig.Authorization.Authorizer, genericConfig.RuleResolver, err = BuildAuthorizer(s, genericConfig.EgressSelector, versionedInformers)
	if err != nil {
		lastErr = fmt.Errorf(&quot;invalid authorization config: %v&quot;, err)
		return
	}
	...
}
</code></pre>
<p>在代码 <code>BuildAuthorizer</code> 中构建了这个 <code>Authorizer</code> 其中可以看到 s 为 <code>kube-apiserver</code> 对于授权阶段的参数，例如参数，使用哪些模式 <code>--authorization-mode</code>，使用的webhook的配置 <code>--authentication-token-webhook-config-file</code> 等，通过传入的参数来决定这些</p>
<pre><code class="language-go">// BuildAuthorizer constructs the authorizer
func BuildAuthorizer(s *options.ServerRunOptions, EgressSelector *egressselector.EgressSelector, versionedInformers clientgoinformers.SharedInformerFactory) (authorizer.Authorizer, authorizer.RuleResolver, error) {
   // 这里构建出  authorizer.Config
	authorizationConfig := s.Authorization.ToAuthorizationConfig(versionedInformers)

	if EgressSelector != nil {
		egressDialer, err := EgressSelector.Lookup(egressselector.ControlPlane.AsNetworkContext())
		if err != nil {
			return nil, nil, err
		}
		authorizationConfig.CustomDial = egressDialer
	}
	
    // 然后返回你开启的每一个webhook的模式的 authorizer
	return authorizationConfig.New()
}
</code></pre>
<p>而对应这部分的数据结构如下所示  <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/pkg/kubeapiserver/options/authorization.go#L34-L46" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/pkg/kubeapiserver/options/authorization.go</a></p>
<pre><code class="language-go">// BuiltInAuthorizationOptions contains all build-in authorization options for API Server
type BuiltInAuthorizationOptions struct {
	Modes                       []string
	PolicyFile                  string
	WebhookConfigFile           string
	WebhookVersion              string
	WebhookCacheAuthorizedTTL   time.Duration
	WebhookCacheUnauthorizedTTL time.Duration
	// WebhookRetryBackoff specifies the backoff parameters for the authorization webhook retry logic.
	// This allows us to configure the sleep time at each iteration and the maximum number of retries allowed
	// before we fail the webhook call in order to limit the fan out that ensues when the system is degraded.
	WebhookRetryBackoff *wait.Backoff
}
</code></pre>
<p>例如在客户端部分，如果需要授权，都会使用该操作，可以在代码 <a href="pkg/registry/authorization/subjectaccessreview/rest.go">k8s.io/pkg/registry/authorization/subjectaccessreview/rest.go</a>  中可以看到REST中会 authorizer.Authorize 去验证是否有权限操作</p>
<pre><code class="language-go">func (r *REST) Create(ctx context.Context, obj runtime.Object, createValidation rest.ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) {
	subjectAccessReview, ok := obj.(*authorizationapi.SubjectAccessReview)
	if !ok {
		return nil, apierrors.NewBadRequest(fmt.Sprintf(&quot;not a SubjectAccessReview: %#v&quot;, obj))
	}
	if errs := authorizationvalidation.ValidateSubjectAccessReview(subjectAccessReview); len(errs) &gt; 0 {
		return nil, apierrors.NewInvalid(authorizationapi.Kind(subjectAccessReview.Kind), &quot;&quot;, errs)
	}

	if createValidation != nil {
		if err := createValidation(ctx, obj.DeepCopyObject()); err != nil {
			return nil, err
		}
	}

	authorizationAttributes := authorizationutil.AuthorizationAttributesFrom(subjectAccessReview.Spec)
	decision, reason, evaluationErr := r.authorizer.Authorize(ctx, authorizationAttributes)

	subjectAccessReview.Status = authorizationapi.SubjectAccessReviewStatus{
		Allowed: (decision == authorizer.DecisionAllow),
		Denied:  (decision == authorizer.DecisionDeny),
		Reason:  reason,
	}
	if evaluationErr != nil {
		subjectAccessReview.Status.EvaluationError = evaluationErr.Error()
	}

	return subjectAccessReview, nil
}
</code></pre>
<p>authorizer.Authorize 会被实现在每一个该阶段的模式下，在 withAuthentication 构建了一个授权的 http.Handler 函数</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go#L45-L79" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/endpoints/filters/authorization.go</a></p>
<pre><code class="language-go">func WithAuthorization(handler http.Handler, a authorizer.Authorizer, s runtime.NegotiatedSerializer) http.Handler {
	if a == nil {
		klog.Warning(&quot;Authorization is disabled&quot;)
		return handler
	}
	return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) {
		ctx := req.Context()

		attributes, err := GetAuthorizerAttributes(ctx)
		if err != nil {
			responsewriters.InternalError(w, req, err)
			return
		}
        // 这里调用了authorizer.Authorizer传入的authorizer来进行鉴权
		authorized, reason, err := a.Authorize(ctx, attributes)
		// an authorizer like RBAC could encounter evaluation errors and still allow the request, so authorizer decision is checked before error here.
		if authorized == authorizer.DecisionAllow {
			audit.AddAuditAnnotations(ctx,
				decisionAnnotationKey, decisionAllow,
				reasonAnnotationKey, reason)
			handler.ServeHTTP(w, req)
			return
		}
		if err != nil {
			audit.AddAuditAnnotation(ctx, reasonAnnotationKey, reasonError)
			responsewriters.InternalError(w, req, err)
			return
		}

		klog.V(4).InfoS(&quot;Forbidden&quot;, &quot;URI&quot;, req.RequestURI, &quot;Reason&quot;, reason)
		audit.AddAuditAnnotations(ctx,
			decisionAnnotationKey, decisionForbid,
			reasonAnnotationKey, reason)
		responsewriters.Forbidden(ctx, attributes, w, req, reason, s)
	})
}
</code></pre>
<p>接下来在 createAggregatorConfig 调用了 BuildHandlerChainWithStorageVersionPrecondition 而又调用了</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/cmd/kube-apiserver/app/aggregator.go#L56-L78" target="_blank"
   rel="noopener nofollow noreferrer" >cmd/kube-apiserver/app/aggregator.go</a></p>
<pre><code class="language-go">func createAggregatorConfig(
	kubeAPIServerConfig genericapiserver.Config,
	commandOptions *options.ServerRunOptions,
	externalInformers kubeexternalinformers.SharedInformerFactory,
	serviceResolver aggregatorapiserver.ServiceResolver,
	proxyTransport *http.Transport,
	pluginInitializers []admission.PluginInitializer,
) (*aggregatorapiserver.Config, error) {
	// make a shallow copy to let us twiddle a few things
	// most of the config actually remains the same.  We only need to mess with a couple items related to the particulars of the aggregator
	genericConfig := kubeAPIServerConfig
	genericConfig.PostStartHooks = map[string]genericapiserver.PostStartHookConfigEntry{}
	genericConfig.RESTOptionsGetter = nil
	// prevent generic API server from installing the OpenAPI handler. Aggregator server
	// has its own customized OpenAPI handler.
	genericConfig.SkipOpenAPIInstallation = true

	if utilfeature.DefaultFeatureGate.Enabled(genericfeatures.StorageVersionAPI) &amp;&amp;
		utilfeature.DefaultFeatureGate.Enabled(genericfeatures.APIServerIdentity) {
		// Add StorageVersionPrecondition handler to aggregator-apiserver.
		// The handler will block write requests to built-in resources until the
		// target resources' storage versions are up-to-date.
		genericConfig.BuildHandlerChainFunc = genericapiserver.BuildHandlerChainWithStorageVersionPrecondition
	}
</code></pre>
<p>而 <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/pkg/server/config.go#L601-L655" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/config.go</a> 返回这个函数 BuildHandlerChainWithStorageVersionPrecondition</p>
<pre><code class="language-go">handlerChainBuilder := func(handler http.Handler) http.Handler {
    return c.BuildHandlerChainFunc(handler, c.Config)
}

apiServerHandler := NewAPIServerHandler(name, c.Serializer, handlerChainBuilder, delegationTarget.UnprotectedHandler())

s := &amp;GenericAPIServer{
		discoveryAddresses:         c.DiscoveryAddresses,
		LoopbackClientConfig:       c.LoopbackClientConfig,
		legacyAPIGroupPrefixes:     c.LegacyAPIGroupPrefixes,
		admissionControl:           c.AdmissionControl,
		Serializer:                 c.Serializer,
		AuditBackend:               c.AuditBackend,
		Authorizer:                 c.Authorization.Authorizer,
		delegationTarget:           delegationTarget,
		EquivalentResourceRegistry: c.EquivalentResourceRegistry,
		HandlerChainWaitGroup:      c.HandlerChainWaitGroup,
		Handler:                    apiServerHandler,

		listedPathProvider: apiServerHandler,
</code></pre>
<p>只要知道哪里调用了 handlerChainBuilder 就知道了鉴权步骤在哪里了，可以看到 handlerChainBuilder 被传入了 apiServerHandler，而后被作为参数返回给 <code>listedPathProvider: &amp;GenericAPIServer{}</code></p>
<p>listedPathProvider在 <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go#L282-L284" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/genericapiserver.go</a></p>
<pre><code class="language-go">func (s *GenericAPIServer) ListedPaths() []string {
	return s.listedPathProvider.ListedPaths()
}
</code></pre>
<p>ListedPaths() 又在代码  <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/pkg/server/routes/index.go#L38-L47" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/routes/index.go</a> 中被 构建成这个http服务</p>
<pre><code class="language-go">// ListedPaths returns the paths that should be shown under /
func (a *APIServerHandler) ListedPaths() []string {
	var handledPaths []string
	// Extract the paths handled using restful.WebService
	for _, ws := range a.GoRestfulContainer.RegisteredWebServices() {
		handledPaths = append(handledPaths, ws.RootPath())
	}
	handledPaths = append(handledPaths, a.NonGoRestfulMux.ListedPaths()...)
	sort.Strings(handledPaths)

	return handledPaths
}


// ServeHTTP serves the available paths.
func (i IndexLister) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	responsewriters.WriteRawJSON(i.StatusCode, metav1.RootPaths{Paths: i.PathProvider.ListedPaths()}, w)
}
</code></pre>
<p>至此，可以知道，每次请求时，我们在配置 <em>kube-apiserver</em> 配置的授权插件 <code>.authorizer.Authorize</code> ，而这个参数会被带至 <code>subjectAccessReview</code> 向下传递，其中 User,Group,Extra,UID 为 authentication 部分提供</p>
<h2 id="authorization-webhook">Authorization webhook</h2>
<p>Authorization webhook 位于 <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go#L166-L247" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go</a>，是通过 <em>kube-apiserver</em> 注入进来的配置，就是上面讲到的如果提供了配置就会加入这种类型的 Authorization 插件来认证。当配置此类型的授权插件，Authorize 会被调用，通过向注入的 URL 发起 REST 请求进行授权，请求对象是 <code>v1beta1.SubjectAccessReview</code></p>
<p>下面是请求的实例</p>
<pre><code>{
  &quot;apiVersion&quot;: &quot;authorization.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;SubjectAccessReview&quot;,
  &quot;spec&quot;: {
	&quot;resourceAttributes&quot;: {
	  &quot;namespace&quot;: &quot;kittensandponies&quot;,
	  &quot;verb&quot;: &quot;GET&quot;,
	  &quot;group&quot;: &quot;group3&quot;,
	  &quot;resource&quot;: &quot;pods&quot;
	},
	&quot;user&quot;: &quot;jane&quot;,
	&quot;group&quot;: [
	  &quot;group1&quot;,
	  &quot;group2&quot;
	]
  }
}
</code></pre>
<p>webhook 返回的格式</p>
<pre><code class="language-json">// 如果允许这个用户访问则返回这个格式
{
  &quot;apiVersion&quot;: &quot;authorization.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;SubjectAccessReview&quot;,
  &quot;status&quot;: {
	&quot;allowed&quot;: true
  }
}

// 如果拒绝这个用户访问则返回这个格式
{
  &quot;apiVersion&quot;: &quot;authorization.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;SubjectAccessReview&quot;,
  &quot;status&quot;: {
	&quot;allowed&quot;: false,
	&quot;reason&quot;: &quot;user does not have read access to the namespace&quot;
  }
}
</code></pre>
<p>对于webhook来讲，只要接受请求保持上面格式，而返回格式为下属格式，就可以很好的将Kubernetes 权限体系接入到三方系统中，例如 <em><strong>open policy agent</strong></em>。</p>
<p>同样 webhook 也提供了  <code>Authorize</code> 函数，如同上面一样会被注入到每个handler中被执行</p>
<pre><code class="language-go">func (w *WebhookAuthorizer) Authorize(ctx context.Context, attr authorizer.Attributes) (decision authorizer.Decision, reason string, err error) {
	r := &amp;authorizationv1.SubjectAccessReview{}
	if user := attr.GetUser(); user != nil {
		r.Spec = authorizationv1.SubjectAccessReviewSpec{
			User:   user.GetName(),
			UID:    user.GetUID(),
			Groups: user.GetGroups(),
			Extra:  convertToSARExtra(user.GetExtra()),
		}
	}

	if attr.IsResourceRequest() {
		r.Spec.ResourceAttributes = &amp;authorizationv1.ResourceAttributes{
			Namespace:   attr.GetNamespace(),
			Verb:        attr.GetVerb(),
			Group:       attr.GetAPIGroup(),
			Version:     attr.GetAPIVersion(),
			Resource:    attr.GetResource(),
			Subresource: attr.GetSubresource(),
			Name:        attr.GetName(),
		}
	} else {
		r.Spec.NonResourceAttributes = &amp;authorizationv1.NonResourceAttributes{
			Path: attr.GetPath(),
			Verb: attr.GetVerb(),
		}
	}
	key, err := json.Marshal(r.Spec)
	if err != nil {
		return w.decisionOnError, &quot;&quot;, err
	}
	if entry, ok := w.responseCache.Get(string(key)); ok {
		r.Status = entry.(authorizationv1.SubjectAccessReviewStatus)
	} else {
		var result *authorizationv1.SubjectAccessReview
		// WithExponentialBackoff will return SAR create error (sarErr) if any.
		if err := webhook.WithExponentialBackoff(ctx, w.retryBackoff, func() error {
			var sarErr error
			var statusCode int

			start := time.Now()
			result, statusCode, sarErr = w.subjectAccessReview.Create(ctx, r, metav1.CreateOptions{})
			latency := time.Since(start)

			if statusCode != 0 {
				w.metrics.RecordRequestTotal(ctx, strconv.Itoa(statusCode))
				w.metrics.RecordRequestLatency(ctx, strconv.Itoa(statusCode), latency.Seconds())
				return sarErr
			}

			if sarErr != nil {
				w.metrics.RecordRequestTotal(ctx, &quot;&lt;error&gt;&quot;)
				w.metrics.RecordRequestLatency(ctx, &quot;&lt;error&gt;&quot;, latency.Seconds())
			}

			return sarErr
		}, webhook.DefaultShouldRetry); err != nil {
			klog.Errorf(&quot;Failed to make webhook authorizer request: %v&quot;, err)
			return w.decisionOnError, &quot;&quot;, err
		}

		r.Status = result.Status
		if shouldCache(attr) {
			if r.Status.Allowed {
				w.responseCache.Add(string(key), r.Status, w.authorizedTTL)
			} else {
				w.responseCache.Add(string(key), r.Status, w.unauthorizedTTL)
			}
		}
	}
	switch {
	case r.Status.Denied &amp;&amp; r.Status.Allowed:
		return authorizer.DecisionDeny, r.Status.Reason, fmt.Errorf(&quot;webhook subject access review returned both allow and deny response&quot;)
	case r.Status.Denied:
		return authorizer.DecisionDeny, r.Status.Reason, nil
	case r.Status.Allowed:
		return authorizer.DecisionAllow, r.Status.Reason, nil
	default:
		return authorizer.DecisionNoOpinion, r.Status.Reason, nil
	}

}
</code></pre>
<p>执行 <code>webhook.Authorize()</code> 会执行 <code>w.subjectAccessReview.Create()</code> 在这里可以看到会发起一个POST请求将 <code>v1beta1.SubjectAccessReview</code> 传入给webhook</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go#L317-L329" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go.Create</a></p>
<pre><code class="language-go">func (t *subjectAccessReviewV1Client) Create(ctx context.Context, subjectAccessReview *authorizationv1.SubjectAccessReview, opts metav1.CreateOptions) (result *authorizationv1.SubjectAccessReview, statusCode int, err error) {
	result = &amp;authorizationv1.SubjectAccessReview{}

	restResult := t.client.Post().
		Resource(&quot;subjectaccessreviews&quot;).
		VersionedParams(&amp;opts, scheme.ParameterCodec).
		Body(subjectAccessReview).
		Do(ctx)

	restResult.StatusCode(&amp;statusCode)
	err = restResult.Into(result)
	return
}
</code></pre>
<h2 id="实验基于opa的rbac模型">实验：基于OPA的RBAC模型</h2>
<p>通过上面阐述，大致了解到kubernetes认证框架中的用户的分类以及认证的策略由哪些，实验的目的也是为了阐述一个结果，就是使用OIDC/webhook 是比其他方式更好的保护，管理kubernetes集群。首先在安全上，假设网络环境是不安全的，那么任意node节点遗漏 bootstrap  token文件，就意味着拥有了集群中最高权限；其次在管理上，越大的团队，人数越多，不可能每个用户都提供单独的证书或者token，要知道传统教程中讲到token在kubernetes集群中是永久有效的，除非你删除了这个secret/sa；而Kubernetes提供的插件就很好的解决了这些问题。</p>
<h3 id="实验环境">实验环境</h3>
<ul>
<li>一个kubernetes集群</li>
<li>了解OPA相关技术</li>
</ul>
<p><strong>实验大致分为以下几个步骤</strong>：</p>
<ul>
<li>建立一个HTTP服务器用于返回给kubernetes Authorization服务</li>
<li>查询用户操作是否有权限</li>
</ul>
<h3 id="实验开始">实验开始</h3>
<h4 id="编写webhook-authorization">编写webhook Authorization</h4>
<p>这里做的就是接收 <code>subjectAccessReview</code> ，将授权结果赋予 <code>subjectAccessReview.Status.Allowed</code> ，true/false，然后返回 <code>subjectAccessReview</code>  即可</p>
<pre><code class="language-go">func serveAuthorization(w http.ResponseWriter, r *http.Request) {
	b, err := ioutil.ReadAll(r.Body)
	if err != nil {
		httpError(w, err)
		return
	}
	klog.V(4).Info(&quot;Receied: &quot;, string(b))

	var subjectAccessReview authoV1.SubjectAccessReview
	err = json.Unmarshal(b, &amp;subjectAccessReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	subjectAccessReview.Status.Allowed = rbac.RBACChek(&amp;subjectAccessReview)
	b, err = json.Marshal(subjectAccessReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	w.Write(b)
	klog.V(3).Info(&quot;Returning: &quot;, string(b))
}
</code></pre>
<h4 id="编写rego">编写rego</h4>
<p>这里简单配置了两个权限，<em>admin</em> 组拥有所有操作权限，不包含 <code>watch</code> ，而 <em>conf</em> 组只能 <em>list</em>，在访问控制三部曲中，已授权的会增加一个组，例如 <code>system:authenticated</code> 代表被 <em>Authentication</em> 授予通过的用户，所以 Groups 为一个数组格式，这里检查为两个数组的交集 &gt; 1，则肯定代表这个用户拥有该组的权限。</p>
<p>实验中 <em>rego</em> 部分可以在 <a href="https://play.openpolicyagent.org/p/JFdryx8eqW" target="_blank"
   rel="noopener nofollow noreferrer" >playground</a> 中测试</p>
<pre><code class="language-go">var module = `package k8s
import future.keywords.in

default allow = false
admin_verbs := {&quot;create&quot;, &quot;list&quot;, &quot;delete&quot;, &quot;update&quot;}
admin_groups := {&quot;admin&quot;}
conf_groups := {&quot;conf&quot;}
conf_verbs := {&quot;list&quot;}
allow  {
	groups := {v | v := input.spec.groups[_]}
	count(admin_groups &amp; groups) &gt; 0
	input.spec.resourceAttributes.verb in admin_verbs
}

allow  {
	groups := {v | v := input.spec.groups[_]}
	count(conf_groups &amp; groups) &gt; 0
	input.spec.resourceAttributes.verb in conf_verbs
}
`
</code></pre>
<p>下面编写 RBACChek 函数，由于go1.16提供了embed功能，就可以直接将 rego embed go中，最后<code>result.Allowed()</code>  如果 <em>input</em> 通过评估则为 <code>true</code> ，反之亦然</p>
<pre><code class="language-go">func RBACChek(req *authoV1.SubjectAccessReview) bool {
	fmt.Printf(&quot;\n%+v\n&quot;, req)
	query, err := rego.New(
        // query是要检查的模块，data是固定格式，这与playground中不一样，需要.allow
        // k8s是package
		rego.Query(&quot;data.k8s.allow&quot;),
		rego.Module(&quot;k8s.allow&quot;, module),
	).PrepareForEval(context.TODO())

	if err != nil {
		klog.V(4).Info(err)
		return false
	}
	result, err := query.Eval(context.TODO(), rego.EvalInput(req))

	if err != nil {
		klog.V(4).Info(&quot;evaluation error:&quot;, err)
		return false
	} else if len(result) == 0 {
		klog.V(4).Info(&quot;undefined result&quot;, err)
		return false
	}
	return result.Allowed()
}
</code></pre>
<h4 id="配置kube-apiserver">配置kube-apiserver</h4>
<p>Authorization webhook 与其他 webhook 一样，启用的方法也是修改 <em>kube-apiserver</em> 参数，并指定 <code>kubeconfig</code> 类型的配置文件，其中对于 Kubernetes 集群来说 <code>kubeconfig</code> 是 kubernetes 客户端访问的信息，而 webhook 这里的 <code>kubeconfig</code> 配置文件要填写的则是 webhook的信息，其中 user,cluster,contexts 属性均为 webhook的配置信息 <sup><a href="#1">[1]</a></sup>。</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Config
clusters:
- cluster:
    server: http://10.0.0.1:88/authorization
    insecure-skip-tls-verify: true
  name: authorizator
users:
- name: webhook-authorizator
current-context: webhook-authorizator@authorizator
contexts:
- context:
    cluster: authorizator
    user: webhook-authorizator
  name: webhook-authorizator@authorizator
</code></pre>
<p>修改 <em>kube-apiserver</em> 参数</p>
<pre><code class="language-yaml">--authorization-webhook-config-file=/etc/kubernetes/auth/authorization-webhook.conf \
# 1s 是为了在测试时减少等待的时间，否则缓存太长不会走webhook
--authorization-webhook-cache-authorized-ttl=1s \
--authorization-webhook-cache-unauthorized-ttl=1s \
# api版本建议还是指定下，因为v1与v1beta1的 subjectAccessReview 内容不同rego因为格式问题会为空从而false
# 代码中schema v1与v1beta1相同，测试时收到的请求的格式不一样，没找到原因 TODO
--authorization-webhook-version=v1 \
</code></pre>
<h3 id="验证结果">验证结果</h3>
<p>准备三个外部用户，admin,admin1,searchUser，admin,admin1 为 admin 组，拥有所有权限，searchUser 为 conf 组，仅能 list 操作</p>
<pre><code class="language-yaml">- name: admin
  user: 
    token: admin@111
- name: admin1
  user:
    token: admin1@111
- name: searchUser
  user:
    token: searchUser@111
</code></pre>
<p>测试用户 searchUser ，可以看到只能list操作</p>
<pre><code class="language-bash">$ kubectl get pod --user=searchUser
NAME                      READY   STATUS             RESTARTS   AGE
netbox-85865d5556-hfg6v   1/1     Running            0          96d
netbox-85865d5556-vlgr4   1/1     Running            0          96d
pod                       0/1     CrashLoopBackOff   95         22h

$ kubectl delete pod pod --user=searchUser
Error from server (Forbidden): pods &quot;pod&quot; is forbidden: User &quot;searchUser&quot; cannot delete resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>
<p>测试用户 admin，可以看出可以进行写与查看的操作</p>
<pre><code class="language-bash">$ kubectl delete pod pod --user=admin
pod &quot;pod&quot; deleted

$ kubectl get pod --user=admin
NAME                      READY   STATUS    RESTARTS   AGE
netbox-85865d5556-hfg6v   1/1     Running   0          96d
netbox-85865d5556-vlgr4   1/1     Running   0          96d
</code></pre>
<h2 id="总结">总结</h2>
<p>kubernetes 提供了 Authentication,Authorization,Adminsion Control,Audit 几种webhook，可以自行在Kubernetes之上实现一个4A的标准，Authorization部分提供了一个并行与，但脱离Kubernetes的授权系统，使得外部用户可以很灵活的被授权，而不是手动管理多个clusterrolebinding,rolebingding 之类的资源。</p>
<p>实验中使用了OPA，这里是将rego静态文件embed入go中，在正常情况下OPA给出的架构如下图所示，存在一个 <em><strong>OPA Service</strong></em>，来进行验证，而实验中是直接嵌入到go中，OPA本身也提供了 <em><strong>HTTP Service</strong></em>，可以直接编译运行为 HTTP服务  <sup><a href="#2">[2]</a></sup>。 TODO</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221124224618920.png" alt="image-20221124224618920" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：OPA 架构</center>
<center><em>Source：</em>https://www.openpolicyagent.org/docs/latest/</center><br>
<p>OPA本身提供了 Gatekeeper ，可以作为Kubernetes 资源使用，官方示例是作为为一个kubernetes准入网关，也提供了ingress浏览的验证 <sup><a href="#3">[3]</a></sup></p>
<blockquote>
<p>Notes：实验中还需要注意的一点则是，如果RBAC与webhook同时验证时，需要合理的规划权限，例如集群组件的账户，coreDNS，flannel等，也会被拒绝（在OPA设置的 <code>default allow = false</code> ）。</p>
</blockquote>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Webhook Mode</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://www.openpolicyagent.org/docs/latest/http-api-authorization/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>HTTP APIs</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://www.openpolicyagent.org/docs/latest/kubernetes-introduction/#what-is-opa-gatekeeper" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>What is OPA Gatekeeper?</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://blog.haohtml.com/archives/31514" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>用 Goalng 开发 OPA 策略</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://blog.wu-boy.com/2021/04/setup-rbac-role-based-access-control-using-open-policy-agent/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>初探 Open Policy Agent 實作 RBAC (Role-based access control) 權限控管</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes 4A - Authentication源码解析</title>
      <link>https://www.oomkill.com/2022/11/ch31-authentication/</link>
      <pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/11/ch31-authentication/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<blockquote>
<p>本文是关于Kubernetes 4A解析的第一章</p>
<ul>
<li>深入理解Kubernetes 4A - Authentication源码解析</li>
<li><a href="https://cylonchau.github.io/kubernetes-authorization.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authorization源码解析</a></li>
<li><a href="https://cylonchau.github.io/ch3.7-admission-webhook.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Admission Control源码解析</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-auditing.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Audit源码解析</a></li>
</ul>
<p>所有关于Kubernetes 4A部分代码上传至仓库 github.com/cylonchau/hello-k8s-4A</p>
</blockquote>
<p>本章主要简单阐述kubernetes 认证相关原理，最后以实验来阐述kubernetes用户系统的思路</p>
<p><strong>objective</strong>：</p>
<ul>
<li>了解kubernetes 各种认证机制的原理</li>
<li>了解kubernetes 用户的概念</li>
<li>了解kubernetes authentication webhook</li>
<li>完成实验，如何将其他用户系统接入到kubernetes中的一个思路</li>
</ul>
<p>如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大。</p>
<h2 id="kubernetes-认证">Kubernetes 认证</h2>
<p>在Kubernetes apiserver对于认证部分所描述的，对于所有用户访问Kubernetes API（通过任何客户端，客户端库，<code>kubectl</code> 等）时都会经历 验证 (<em><strong>Authentication</strong></em>) , 授权 (<em><strong>Authorization</strong></em>), 和准入控制 (<em><strong>Admission control</strong></em>) 三个阶段来完成对 “用户” 进行授权，整个流程正如下图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/vhesGDFN3dLdzXwS7vzPdXkI3aglQYZgGhjc-Cx_boaV6URKFFoe8mFRZZUuJyGHywa_bOkeUlIkm-nJkCVMHPk9dr2dXFwNzAQJKzft2phsTcEDjdObjmugBcYtpdPLpLIYuIGzeFYvtsR2Lw.jpeg" alt="image-20221025003822017" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图</center>
<center><em>Source：</em>https://www.armosec.io/blog/kubernetes-admission-controller/</center><br>
<p>其中在大多数教程中，在对这三个阶段所做的工作大致上为：</p>
<ul>
<li>
<p><em><strong>Authentication</strong></em> 阶段所指用于确认请求访问Kubernetes API 用户是否为合法用户，拒绝为401</p>
</li>
<li>
<p><em><strong>Authorization</strong></em> 阶段所指的将是这个用户是否有对操作的资源的权限，拒绝为403</p>
</li>
<li>
<p><em><strong>Admission control</strong></em> 阶段所指控制对请求资源进行控制，通俗来说，就是一票否决权，即使前两个步骤完成</p>
</li>
</ul>
<p>到这里了解到了Kubernetes API实际上做的工作就是 “人类用户” 与 “kubernetes service account&quot; <sup><a href="#2">[2]</a></sup>；那么就引出了一个重要概念就是 “用户” 在Kubernetes中是什么，以及用户在认证中的也是本章节的中心。</p>
<p>在Kubernetes官方手册中给出了 ”用户“ 的概念，Kubernetes集群中存在的用户包括 ”普通用户“ 与 “service account” 但是 Kubernetes 没有普通用户的管理方式，只是将使用集群的证书CA签署的有效证书的用户都被视为合法用户 <sup><a href="#3">[3]</a></sup></p>
<p>那么对于使得Kubernetes集群有一个真正的用户系统，就可以根据上面给出的概念将Kubernetes用户分为 ”外部用户“ 与 ”内部用户“。如何理解外部与内部用户呢？实际上就是有Kubernetes管理的用户，即在kubernetes定义用户的数据模型这种为 “内部用户” ，正如 service account；反之，非Kubernetes托管的用户则为 ”外部用户“ 这中概念也更好的对kubernetes用户的阐述。</p>
<p>对于外部用户来说，实际上Kubernetes给出了多种用户概念 <sup><a href="#3">[3]</a></sup>，例如：</p>
<ul>
<li>拥有kubernetes集群证书的用户</li>
<li>拥有Kubernetes集群token的用户（<code>--token-auth-file</code> 指定的静态token）</li>
<li>用户来自外部用户系统，例如 <em>OpenID</em>，<em>LDAP</em>，<em>QQ connect</em>, <em>google identity platform</em> 等</li>
</ul>
<h2 id="向外部用户授权集群访问的示例">向外部用户授权集群访问的示例</h2>
<h3 id="场景1通过证书请求k8s">场景1：通过证书请求k8s</h3>
<p>该场景中kubernetes将使用证书中的cn作为用户，ou作为组，如果对应 <code>rolebinding/clusterrolebinding</code> 给予该用户权限，那么请求为合法</p>
<pre><code class="language-bash">$ curl https://hostname:6443/api/v1/pods \
	--cert ./client.pem \
	--key ./client-key.pem \
	--cacert ./ca.pem 
</code></pre>
<p>接下来浅析下在代码中做的事情</p>
<p>确认用户是 <em><strong>apiserver</strong></em> 在 <em><strong>Authentication</strong></em> 阶段 做的事情，而对应代码在 <a href="pkg/kubeapiserver/authenticator">pkg/kubeapiserver/authenticator</a> 下，整个文件就是构建了一系列的认证器，而x.509证书指是其中一个</p>
<pre><code class="language-go">// 创建一个认证器，返回请求或一个k8s认证机制的标准错误
func (config Config) New() (authenticator.Request, *spec.SecurityDefinitions, error) {
    
...

	// X509 methods
    // 可以看到这里就是将x509证书解析为user
	if config.ClientCAContentProvider != nil {
		certAuth := x509.NewDynamic(config.ClientCAContentProvider.VerifyOptions, x509.CommonNameUserConversion)
		authenticators = append(authenticators, certAuth)
	}
...
</code></pre>
<p>接下来看实现原理，NewDynamic函数位于代码 <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/staging/src/k8s.io/apiserver/pkg/authentication/request/x509/x509.go#L126-L130" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/authentication/request/x509/x509.go</a></p>
<p>通过代码可以看出，是通过一个验证函数与用户来解析为一个 <em>Authenticator</em></p>
<pre><code class="language-go">// NewDynamic returns a request.Authenticator that verifies client certificates using the provided
// VerifyOptionFunc (which may be dynamic), and converts valid certificate chains into user.Info using the provided UserConversion
func NewDynamic(verifyOptionsFn VerifyOptionFunc, user UserConversion) *Authenticator {
	return &amp;Authenticator{verifyOptionsFn, user}
}
</code></pre>
<p>验证函数为 CAContentProvider 的方法，而x509部分实现为 <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/staging/src/k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go#L253-L261" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go.VerifyOptions</a>；可以看出返回是一个 <code>x509.VerifyOptions</code> + 与认证的状态</p>
<pre><code class="language-go">// VerifyOptions provides verifyoptions compatible with authenticators
func (c *DynamicFileCAContent) VerifyOptions() (x509.VerifyOptions, bool) {
	uncastObj := c.caBundle.Load()
	if uncastObj == nil {
		return x509.VerifyOptions{}, false
	}

	return uncastObj.(*caBundleAndVerifier).verifyOptions, true
}
</code></pre>
<p>而用户的获取则位于  <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/staging/src/k8s.io/apiserver/pkg/authentication/request/x509/x509.go#L248-L258" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/authentication/request/x509/x509.go</a>；可以看出，用户正是拿的证书的CN，而组则是为证书的OU</p>
<pre><code class="language-go">// CommonNameUserConversion builds user info from a certificate chain using the subject's CommonName
var CommonNameUserConversion = UserConversionFunc(func(chain []*x509.Certificate) (*authenticator.Response, bool, error) {
	if len(chain[0].Subject.CommonName) == 0 {
		return nil, false, nil
	}
	return &amp;authenticator.Response{
		User: &amp;user.DefaultInfo{
			Name:   chain[0].Subject.CommonName,
			Groups: chain[0].Subject.Organization,
		},
	}, true, nil
})
</code></pre>
<p>由于授权不在本章范围内，直接忽略至入库阶段，入库阶段由 <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/pkg/controlplane/instance.go#L561" target="_blank"
   rel="noopener nofollow noreferrer" >RESTStorageProvider</a> 实现 这里，每一个Provider都提供了 <code>Authenticator</code> 这里包含了已经允许的请求，将会被对应的REST客户端写入到库中</p>
<pre><code class="language-go">type RESTStorageProvider struct {
	Authenticator authenticator.Request
	APIAudiences  authenticator.Audiences
}
// RESTStorageProvider is a factory type for REST storage.
type RESTStorageProvider interface {
	GroupName() string
	NewRESTStorage(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter) (genericapiserver.APIGroupInfo, error)
}
</code></pre>
<h3 id="场景2通过token">场景2：通过token</h3>
<p>该场景中，当 <em><strong>kube-apiserver</strong></em> 开启了 <code>--enable-bootstrap-token-auth</code> 时，就可以使用 Bootstrap Token 进行认证，通常如下列命令，在请求头中增加 <code>Authorization: Bearer &lt;token&gt;</code> 标识</p>
<pre><code class="language-bash">$ curl https://hostname:6443/api/v1/pods \
  --cacert ${CACERT} \
  --header &quot;Authorization: Bearer &lt;token&gt;&quot; \
</code></pre>
<p>接下来浅析下在代码中做的事情</p>
<p>可以看到，在代码 <a href="pkg/kubeapiserver/authenticator">pkg/kubeapiserver/authenticator.New()</a> 中当 <em><strong>kube-apiserver</strong></em> 指定了参数 <code>--token-auth-file=/etc/kubernetes/token.csv&quot;</code> 这种认证会被激活</p>
<pre><code class="language-go">if len(config.TokenAuthFile) &gt; 0 {
    tokenAuth, err := newAuthenticatorFromTokenFile(config.TokenAuthFile)
    if err != nil {
        return nil, nil, err
    }
    tokenAuthenticators = append(tokenAuthenticators, authenticator.WrapAudienceAgnosticToken(config.APIAudiences, tokenAuth))
}
</code></pre>
<p>此时打开 token.csv 查看下token长什么样</p>
<pre><code class="language-bash">$ cat /etc/kubernetes/token.csv
12ba4f.d82a57a4433b2359,&quot;system:bootstrapper&quot;,10001,&quot;system:bootstrappers&quot;
</code></pre>
<p>这里回到代码 <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/staging/src/k8s.io/apiserver/pkg/authentication/token/tokenfile/tokenfile.go#L45-L91" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/authentication/token/tokenfile/tokenfile.go.NewCSV</a> ，这里可以看出，就是读取 <code>--token-auth-file=</code> 参数指定的tokenfile，然后解析为用户，<code>record[1]</code> 作为用户名，<code>record[2]</code> 作为UID</p>
<pre><code class="language-go">// NewCSV returns a TokenAuthenticator, populated from a CSV file.
// The CSV file must contain records in the format &quot;token,username,useruid&quot;
func NewCSV(path string) (*TokenAuthenticator, error) {
	file, err := os.Open(path)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	recordNum := 0
	tokens := make(map[string]*user.DefaultInfo)
	reader := csv.NewReader(file)
	reader.FieldsPerRecord = -1
	for {
		record, err := reader.Read()
		if err == io.EOF {
			break
		}
		if err != nil {
			return nil, err
		}
		if len(record) &lt; 3 {
			return nil, fmt.Errorf(&quot;token file '%s' must have at least 3 columns (token, user name, user uid), found %d&quot;, path, len(record))
		}

		recordNum++
		if record[0] == &quot;&quot; {
			klog.Warningf(&quot;empty token has been found in token file '%s', record number '%d'&quot;, path, recordNum)
			continue
		}

		obj := &amp;user.DefaultInfo{
			Name: record[1],
			UID:  record[2],
		}
		if _, exist := tokens[record[0]]; exist {
			klog.Warningf(&quot;duplicate token has been found in token file '%s', record number '%d'&quot;, path, recordNum)
		}
		tokens[record[0]] = obj

		if len(record) &gt;= 4 {
			obj.Groups = strings.Split(record[3], &quot;,&quot;)
		}
	}

	return &amp;TokenAuthenticator{
		tokens: tokens,
	}, nil
}
</code></pre>
<p>而token file中配置的格式正是以逗号分隔的一组字符串，</p>
<pre><code class="language-go">type DefaultInfo struct {
	Name   string
	UID    string
	Groups []string
	Extra  map[string][]string
}
</code></pre>
<p>这种用户最常见的方式就是 <em><strong>kubelet</strong></em> 通常会以此类用户向控制平面进行身份认证，例如下列配置</p>
<pre><code class="language-bash">KUBELET_ARGS=&quot;--v=0 \
    --logtostderr=true \
    --config=/etc/kubernetes/kubelet-config.yaml \
    --kubeconfig=/etc/kubernetes/auth/kubelet.conf \
    --network-plugin=cni \
    --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \
    --bootstrap-kubeconfig=/etc/kubernetes/auth/bootstrap.conf&quot;
</code></pre>
<p><code>/etc/kubernetes/auth/bootstrap.conf</code> 内容，这里就用到了 <em><strong>kube-apiserver</strong></em> 配置的 <code>--token-auth-file=</code> 用户名，组必须为 <code>system:bootstrappers</code></p>
<pre><code class="language-yaml">apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: ......
    server: https://10.0.0.4:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:bootstrapper
  name: system:bootstrapper@kubernetes
current-context: system:bootstrapper@kubernetes
kind: Config
preferences: {}
users:
- name: system:bootstrapper
</code></pre>
<p>而通常在二进制部署时会出现的问题，例如下列错误</p>
<pre><code class="language-log">Unable to register node &quot;hostname&quot; with API server: nodes is forbidden: User &quot;system:anonymous&quot; cannot create resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope
</code></pre>
<p>而通常解决方法是执行下列命令，这里就是将 <em><strong>kubelet</strong></em> 与 <em><strong>kube-apiserver</strong></em> 通讯时的用户授权，因为kubernetes官方给出的条件是，用户组必须为 <code>system:bootstrappers</code>  <sup><a href="#4">[4]</a></sup></p>
<pre><code class="language-bash">$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
</code></pre>
<p>生成的clusterrolebinding 如下</p>
<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: &quot;2022-08-14T22:26:51Z&quot;
  managedFields:
  - apiVersion: rbac.authorization.k8s.io/v1
    fieldsType: FieldsV1
   ...
    time: &quot;2022-08-14T22:26:51Z&quot;
  name: kubelet-bootstrap
  resourceVersion: &quot;158&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubelet-bootstrap
  uid: b4d70f4f-4ae0-468f-86b7-55e9351e4719
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node-bootstrapper
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:bootstrappers
</code></pre>
<p>上述就是 bootstrap token，翻译后就是引导token，因为其做的工作就是将节点载入Kubernetes系统过程提供认证机制的用户。</p>
<blockquote>
<p>Notes：这种用户不存在与kubernetes内，可以算属于一个外部用户，但认证机制中存在并绑定了最高权限，也可以用来做其他访问时的认证</p>
</blockquote>
<h3 id="场景3serviceaccount">场景3：serviceaccount</h3>
<p>serviceaccount通常为API自动创建的，但在用户中，实际上认证存在两个方向，一个是 <code>--service-account-key-file</code> 这个参数可以指定多个，指定对应的证书文件公钥或私钥，用以办法sa的token</p>
<p>首先会根据指定的公钥或私钥文件生成token</p>
<pre><code class="language-go">if len(config.ServiceAccountKeyFiles) &gt; 0 {
    serviceAccountAuth, err := newLegacyServiceAccountAuthenticator(config.ServiceAccountKeyFiles, config.ServiceAccountLookup, config.APIAudiences, config.ServiceAccountTokenGetter)
    if err != nil {
        return nil, nil, err
    }
    tokenAuthenticators = append(tokenAuthenticators, serviceAccountAuth)
}
if len(config.ServiceAccountIssuers) &gt; 0 {
    serviceAccountAuth, err := newServiceAccountAuthenticator(config.ServiceAccountIssuers, config.ServiceAccountKeyFiles, config.APIAudiences, config.ServiceAccountTokenGetter)
    if err != nil {
        return nil, nil, err
    }
    tokenAuthenticators = append(tokenAuthenticators, serviceAccountAuth)
}
</code></pre>
<p>对于  <code>--service-account-key-file</code>  他生成的用户都是 “kubernetes/serviceaccount”  , 而对于 <code>--service-account-issuer</code> 只是对sa颁发者提供了一个称号标识是谁，而不是统一的 “kubernetes/serviceaccount” ，这里可以从代码中看到，两者是完全相同的，只是称号不同罢了</p>
<pre><code class="language-go">// newLegacyServiceAccountAuthenticator returns an authenticator.Token or an error
func newLegacyServiceAccountAuthenticator(keyfiles []string, lookup bool, apiAudiences authenticator.Audiences, serviceAccountGetter serviceaccount.ServiceAccountTokenGetter) (authenticator.Token, error) {
	allPublicKeys := []interface{}{}
	for _, keyfile := range keyfiles {
		publicKeys, err := keyutil.PublicKeysFromFile(keyfile)
		if err != nil {
			return nil, err
		}
		allPublicKeys = append(allPublicKeys, publicKeys...)
	}
// 唯一的区别 这里使用了常量 serviceaccount.LegacyIssuer
	tokenAuthenticator := serviceaccount.JWTTokenAuthenticator([]string{serviceaccount.LegacyIssuer}, allPublicKeys, apiAudiences, serviceaccount.NewLegacyValidator(lookup, serviceAccountGetter))
	return tokenAuthenticator, nil
}

// newServiceAccountAuthenticator returns an authenticator.Token or an error
func newServiceAccountAuthenticator(issuers []string, keyfiles []string, apiAudiences authenticator.Audiences, serviceAccountGetter serviceaccount.ServiceAccountTokenGetter) (authenticator.Token, error) {
	allPublicKeys := []interface{}{}
	for _, keyfile := range keyfiles {
		publicKeys, err := keyutil.PublicKeysFromFile(keyfile)
		if err != nil {
			return nil, err
		}
		allPublicKeys = append(allPublicKeys, publicKeys...)
	}
// 唯一的区别 这里根据kube-apiserver提供的称号指定名称
	tokenAuthenticator := serviceaccount.JWTTokenAuthenticator(issuers, allPublicKeys, apiAudiences, serviceaccount.NewValidator(serviceAccountGetter))
	return tokenAuthenticator, nil
}
</code></pre>
<p>最后根据ServiceAccounts，Secrets等值签发一个token，也就是通过下列命令获取的值</p>
<pre><code class="language-go">$ kubectl get secret multus-token-v6bfg -n kube-system -o jsonpath={&quot;.data.token&quot;}
</code></pre>
<h3 id="场景4openid">场景4：openid</h3>
<p>OpenID Connect是 OAuth2 风格，允许用户授权三方网站访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方网站或分享他们数据的所有内容，下面是一张kubernetes 使用 OID 认证的逻辑图</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/kube-login-oidc-ad4caf57f124e622897e0781fe1e3d6e1ecb5c6099776e6677ca800c4458f1de.jpg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes OID认证</center>
<center><em>Source：</em>https://developer.okta.com/blog/2021/11/08/k8s-api-server-oidc</center><br>
<h3 id="场景5webhook">场景5：webhook</h3>
<p>webhook是kubernetes提供自定义认证的其中一种，主要是用于认证 “<em><strong>不记名 token</strong></em>“ 的钩子，“<em><strong>不记名 token</strong></em>“ 将 由身份验证服务创建。当用户对kubernetes访问时，会触发准入控制，当对kubernetes集群注册了 authenticaion webhook时，将会使用该webhook提供的方式进行身份验证时，此时会为您生成一个 token 。</p>
<p>如代码 <a href="pkg/kubeapiserver/authenticator">pkg/kubeapiserver/authenticator.New()</a>  中所示 newWebhookTokenAuthenticator 会通过提供的config (<code>--authentication-token-webhook-config-file</code>) 来创建出一个 WebhookTokenAuthenticator</p>
<pre><code class="language-go">if len(config.WebhookTokenAuthnConfigFile) &gt; 0 {
    webhookTokenAuth, err := newWebhookTokenAuthenticator(config)
    if err != nil {
        return nil, nil, err
    }

    tokenAuthenticators = append(tokenAuthenticators, webhookTokenAuth)
}
</code></pre>
<p>下图是kubernetes 中 WebhookToken 验证的工作原理</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/25d075712ff343ce492a5db30733cd93.svg" alt="Webhook 令牌认证插件" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：kubernetes WebhookToken验证原理</center>
<center><em>Source：</em>https://learnk8s.io/kubernetes-custom-authentication</center><br>
<p>最后由token中的authHandler，循环所有的Handlers在运行 <code>AuthenticateToken</code> 去进行获取用户的信息</p>
<pre><code class="language-go">func (authHandler *unionAuthTokenHandler) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) {
   var errlist []error
   for _, currAuthRequestHandler := range authHandler.Handlers {
      info, ok, err := currAuthRequestHandler.AuthenticateToken(ctx, token)
      if err != nil {
         if authHandler.FailOnError {
            return info, ok, err
         }
         errlist = append(errlist, err)
         continue
      }

      if ok {
         return info, ok, err
      }
   }

   return nil, false, utilerrors.NewAggregate(errlist)
}
</code></pre>
<p>而webhook插件也实现了这个方法 <code>AuthenticateToken</code> ,这里会通过POST请求，调用注入的webhook，该请求携带一个JSON 格式的 <code>TokenReview</code> 对象，其中包含要验证的令牌</p>
<pre><code class="language-go">func (w *WebhookTokenAuthenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) {

    ....

		start := time.Now()
		result, statusCode, tokenReviewErr = w.tokenReview.Create(ctx, r, metav1.CreateOptions{})
		latency := time.Since(start)
...
}
</code></pre>
<p>webhook token认证服务要返回<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#userinfo-v1beta1-authentication-k8s-io" target="_blank"
   rel="noopener nofollow noreferrer" >用户的身份信息</a>，就是上面token部分提到的数据结构（webhook来决定接受还是拒绝该用户）</p>
<pre><code class="language-go">type DefaultInfo struct {
	Name   string
	UID    string
	Groups []string
	Extra  map[string][]string
}
</code></pre>
<h3 id="场景6代理认证">场景6：代理认证</h3>
<h2 id="实验基于ldap的身份认证">实验：基于LDAP的身份认证</h2>
<p>通过上面阐述，大致了解到kubernetes认证框架中的用户的分类以及认证的策略由哪些，实验的目的也是为了阐述一个结果，就是使用OIDC/webhook 是比其他方式更好的保护，管理kubernetes集群。首先在安全上，假设网络环境是不安全的，那么任意node节点遗漏 bootstrap token文件，就意味着拥有了集群中最高权限；其次在管理上，越大的团队，人数越多，不可能每个用户都提供单独的证书或者token，要知道传统教程中讲到token在kubernetes集群中是永久有效的，除非你删除了这个secret/sa；而Kubernetes提供的插件就很好的解决了这些问题。</p>
<h3 id="实验环境">实验环境</h3>
<ul>
<li>一个kubernetes集群</li>
<li>一个openldap服务，建议可以是集群外部的，因为webhook不像SSSD有缓存机制，并且集群不可用，那么认证不可用，当认证不可用时会导致集群不可用，这样事故影响的范围可以得到控制，也叫最小化半径</li>
<li>了解ldap相关技术，并了解go ldap客户端</li>
</ul>
<p><strong>实验大致分为以下几个步骤</strong>：</p>
<ul>
<li>建立一个HTTP服务器用于返回给kubernetes Authenticaion服务</li>
<li>查询ldap该用户是否合法
<ul>
<li>查询用户是否合法</li>
<li>查询用户所属组是否拥有权限</li>
</ul>
</li>
</ul>
<h3 id="实验开始">实验开始</h3>
<h4 id="初始化用户数据">初始化用户数据</h4>
<p>首先准备openldap初始化数据，创建三个 posixGroup 组，与5个用户 admin, admin1, admin11, searchUser, syncUser 密码均为111，组与用户关联使用的 <code>memberUid</code></p>
<pre><code class="language-bash">cat &lt;&lt; EOF | ldapdelete -r  -H ldap://10.0.0.3 -D &quot;cn=admin,dc=test,dc=com&quot; -w 111
dn: dc=test,dc=com
objectClass: top
objectClass: organizationalUnit
objectClass: extensibleObject
description: US Organization
ou: people

dn: ou=tvb,dc=test,dc=com
objectClass: organizationalUnit
description: Television Broadcasts Limited
ou: tvb

dn: cn=admin,ou=tvb,dc=test,dc=com
objectClass: posixGroup
gidNumber: 10000
cn: admin

dn: cn=conf,ou=tvb,dc=test,dc=com
objectClass: posixGroup
gidNumber: 10001
cn: conf

dn: cn=dir,ou=tvb,dc=test,dc=com
objectClass: posixGroup
gidNumber: 10002
cn: dir

dn: uid=syncUser,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
pwdAttribute: userPassword
uid: syncUser
cn: syncUser
uidNumber: 10006
gidNumber: 10002
homeDirectory: /home/syncUser
loginShell: /bin/bash
sn: syncUser
givenName: syncUser
memberOf: cn=confGroup,ou=tvb,dc=test,dc=com

dn: uid=searchUser,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
pwdAttribute: userPassword
uid: searchUser
cn: searchUser
uidNumber: 10005
gidNumber: 10001
homeDirectory: /home/searchUser
loginShell: /bin/bash
sn: searchUser
givenName: searchUser
memberOf: cn=dirGroup,ou=tvb,dc=test,dc=com

dn: uid=admin1,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
pwdAttribute: userPassword
uid: admin1
sn: admin1
cn: admin
uidNumber: 10010
gidNumber: 10000
homeDirectory: /home/admin
loginShell: /bin/bash
givenName: admin
memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com

dn: uid=admin11,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
sn: admin11
pwdAttribute: userPassword
uid: admin11
cn: admin11
uidNumber: 10011
gidNumber: 10000
homeDirectory: /home/admin
loginShell: /bin/bash
givenName: admin11
memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com

dn: uid=admin,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
pwdAttribute: userPassword
uid: admin
cn: admin
uidNumber: 10009
gidNumber: 10000
homeDirectory: /home/admin
loginShell: /bin/bash
sn: admin
givenName: admin
memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com
EOF
</code></pre>
<p>接下来需要确定如何为认证成功的用户，上面讲到对于kubernetes中用户格式为 <code>v1.UserInfo</code> 的格式，即要获得用户，即用户组，假设需要查找的用户为，admin，那么在openldap中查询filter如下：</p>
<pre><code class="language-bash">&quot;(|(&amp;(objectClass=posixAccount)(uid=admin))(&amp;(objectClass=posixGroup)(memberUid=admin)))&quot;
</code></pre>
<p>上面语句意思是，找到 <code>objectClass=posixAccount</code> 并且 <code>uid=admin</code> 或者 <code>objectClass=posixGroup</code> 并且 <code>memberUid=admin</code> 的条目信息，这里使用 ”|“ 与 ”&amp;“ 是为了要拿到这两个结果。</p>
<h4 id="编写webhook查询用户部分">编写webhook查询用户部分</h4>
<p>这里由于openldap配置密码保存格式不是明文的，如果直接使用 ”=“ 来验证是查询不到内容的，故直接多用了一次登录来验证用户是否合法</p>
<pre><code class="language-go">func ldapSearch(username, password string) (*v1.UserInfo, error) {
	ldapconn, err := ldap.DialURL(ldapURL)
	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}
	defer ldapconn.Close()

	// Authenticate as LDAP admin user
	err = ldapconn.Bind(&quot;uid=searchUser,ou=tvb,dc=test,dc=com&quot;, &quot;111&quot;)
	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	// Execute LDAP Search request
	result, err := ldapconn.Search(ldap.NewSearchRequest(
		&quot;ou=tvb,dc=test,dc=com&quot;,
		ldap.ScopeWholeSubtree,
		ldap.NeverDerefAliases,
		0,
		0,
		false,
		fmt.Sprintf(&quot;(&amp;(objectClass=posixGroup)(memberUid=%s))&quot;, username), // Filter
		nil,
		nil,
	))

	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	userResult, err := ldapconn.Search(ldap.NewSearchRequest(
		&quot;ou=tvb,dc=test,dc=com&quot;,
		ldap.ScopeWholeSubtree,
		ldap.NeverDerefAliases,
		0,
		0,
		false,
		fmt.Sprintf(&quot;(&amp;(objectClass=posixAccount)(uid=%s))&quot;, username), // Filter
		nil,
		nil,
	))

	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	if len(result.Entries) == 0 {
		klog.V(3).Info(&quot;User does not exist&quot;)
		return nil, errors.New(&quot;User does not exist&quot;)
	} else {
		// 验证用户名密码是否正确
		if err := ldapconn.Bind(userResult.Entries[0].DN, password); err != nil {
			e := fmt.Sprintf(&quot;Failed to auth. %s\n&quot;, err)
			klog.V(3).Info(e)
			return nil, errors.New(e)
		} else {
			klog.V(3).Info(fmt.Sprintf(&quot;User %s Authenticated successfuly!&quot;, username))
		}
		// 拼接为kubernetes authentication 的用户格式
		user := new(v1.UserInfo)
		for _, v := range result.Entries {
			attrubute := v.GetAttributeValue(&quot;objectClass&quot;)
			if strings.Contains(attrubute, &quot;posixGroup&quot;) {
				user.Groups = append(user.Groups, v.GetAttributeValue(&quot;cn&quot;))
			}
		}

		u := userResult.Entries[0].GetAttributeValue(&quot;uid&quot;)
		user.UID = u
		user.Username = u
		return user, nil
	}
}
</code></pre>
<h4 id="编写http部分">编写HTTP部分</h4>
<p>这里有几个需要注意的部分，即用户或者理解为要认证的token的定义，此处使用了 ”username@password“ 格式作为用户的辨别，即登录kubernetes时需要直接输入 ”username@password“ 来作为登录的凭据。</p>
<p>第二个部分为返回值，返回给Kubernetes的格式必须为 <code>api/authentication/v1.TokenReview</code> 格式，<code>Status.Authenticated</code> 表示用户身份验证结果，如果该用户合法，则设置 <code>tokenReview.Status.Authenticated = true</code> 反之亦然。如果验证成功还需要 <code>Status.User</code> 这就是在<code>ldapSearch</code></p>
<pre><code class="language-go">func serve(w http.ResponseWriter, r *http.Request) {
	b, err := ioutil.ReadAll(r.Body)
	if err != nil {
		httpError(w, err)
		return
	}
	klog.V(4).Info(&quot;Receiving: %s\n&quot;, string(b))

	var tokenReview v1.TokenReview
	err = json.Unmarshal(b, &amp;tokenReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}

	// 提取用户名与密码
	s := strings.SplitN(tokenReview.Spec.Token, &quot;@&quot;, 2)
	if len(s) != 2 {
		klog.V(3).Info(fmt.Errorf(&quot;badly formatted token: %s&quot;, tokenReview.Spec.Token))
		httpError(w, fmt.Errorf(&quot;badly formatted token: %s&quot;, tokenReview.Spec.Token))
		return
	}
	username, password := s[0], s[1]
	// 查询ldap，验证用户是否合法
	userInfo, err := ldapSearch(username, password)
	if err != nil {
		// 这里不打印日志的原因是 ldapSearch 中打印过了
		return
	}

	// 设置返回的tokenReview
	if userInfo == nil {
		tokenReview.Status.Authenticated = false
	} else {
		tokenReview.Status.Authenticated = true
		tokenReview.Status.User = *userInfo
	}

	b, err = json.Marshal(tokenReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	w.Write(b)
	klog.V(3).Info(&quot;Returning: &quot;, string(b))
}

func httpError(w http.ResponseWriter, err error) {
	err = fmt.Errorf(&quot;Error: %v&quot;, err)
	w.WriteHeader(http.StatusInternalServerError) // 500
	fmt.Fprintln(w, err)
	klog.V(4).Info(&quot;httpcode 500: &quot;, err)
}
</code></pre>
<p>下面是完整的代码</p>
<pre><code class="language-go">package main

import (
	&quot;encoding/json&quot;
	&quot;errors&quot;
	&quot;flag&quot;
	&quot;fmt&quot;
	&quot;io/ioutil&quot;
	&quot;net/http&quot;
	&quot;strings&quot;

	&quot;github.com/go-ldap/ldap&quot;
	&quot;k8s.io/api/authentication/v1&quot;
	&quot;k8s.io/klog/v2&quot;
)

var ldapURL string

func main() {
	klog.InitFlags(nil)
	flag.Parse()
	http.HandleFunc(&quot;/authenticate&quot;, serve)
	klog.V(4).Info(&quot;Listening on port 443 waiting for requests...&quot;)
	klog.V(4).Info(http.ListenAndServe(&quot;:443&quot;, nil))
	ldapURL = &quot;ldap://10.0.0.10:389&quot;
	ldapSearch(&quot;admin&quot;, &quot;1111&quot;)
}

func serve(w http.ResponseWriter, r *http.Request) {
	b, err := ioutil.ReadAll(r.Body)
	if err != nil {
		httpError(w, err)
		return
	}
	klog.V(4).Info(&quot;Receiving: %s\n&quot;, string(b))

	var tokenReview v1.TokenReview
	err = json.Unmarshal(b, &amp;tokenReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}

	// 提取用户名与密码
	s := strings.SplitN(tokenReview.Spec.Token, &quot;@&quot;, 2)
	if len(s) != 2 {
		klog.V(3).Info(fmt.Errorf(&quot;badly formatted token: %s&quot;, tokenReview.Spec.Token))
		httpError(w, fmt.Errorf(&quot;badly formatted token: %s&quot;, tokenReview.Spec.Token))
		return
	}
	username, password := s[0], s[1]
	// 查询ldap，验证用户是否合法
	userInfo, err := ldapSearch(username, password)
	if err != nil {
		// 这里不打印日志的原因是 ldapSearch 中打印过了
		return
	}

	// 设置返回的tokenReview
	if userInfo == nil {
		tokenReview.Status.Authenticated = false
	} else {
		tokenReview.Status.Authenticated = true
		tokenReview.Status.User = *userInfo
	}

	b, err = json.Marshal(tokenReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	w.Write(b)
	klog.V(3).Info(&quot;Returning: &quot;, string(b))
}

func httpError(w http.ResponseWriter, err error) {
	err = fmt.Errorf(&quot;Error: %v&quot;, err)
	w.WriteHeader(http.StatusInternalServerError) // 500
	fmt.Fprintln(w, err)
	klog.V(4).Info(&quot;httpcode 500: &quot;, err)
}

func ldapSearch(username, password string) (*v1.UserInfo, error) {

	ldapconn, err := ldap.DialURL(ldapURL)
	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}
	defer ldapconn.Close()

	// Authenticate as LDAP admin user
	err = ldapconn.Bind(&quot;cn=admin,dc=test,dc=com&quot;, &quot;111&quot;)
	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	// Execute LDAP Search request
	result, err := ldapconn.Search(ldap.NewSearchRequest(
		&quot;ou=tvb,dc=test,dc=com&quot;,
		ldap.ScopeWholeSubtree,
		ldap.NeverDerefAliases,
		0,
		0,
		false,
		fmt.Sprintf(&quot;(&amp;(objectClass=posixGroup)(memberUid=%s))&quot;, username), // Filter
		nil,
		nil,
	))

	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	userResult, err := ldapconn.Search(ldap.NewSearchRequest(
		&quot;ou=tvb,dc=test,dc=com&quot;,
		ldap.ScopeWholeSubtree,
		ldap.NeverDerefAliases,
		0,
		0,
		false,
		fmt.Sprintf(&quot;(&amp;(objectClass=posixAccount)(uid=%s))&quot;, username), // Filter
		nil,
		nil,
	))

	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	if len(result.Entries) == 0 {
		klog.V(3).Info(&quot;User does not exist&quot;)
		return nil, errors.New(&quot;User does not exist&quot;)
	} else {
		// 验证用户名密码是否正确
		if err := ldapconn.Bind(userResult.Entries[0].DN, password); err != nil {
			e := fmt.Sprintf(&quot;Failed to auth. %s\n&quot;, err)
			klog.V(3).Info(e)
			return nil, errors.New(e)
		} else {
			klog.V(3).Info(fmt.Sprintf(&quot;User %s Authenticated successfuly!&quot;, username))
		}
		// 拼接为kubernetes authentication 的用户格式
		user := new(v1.UserInfo)
		for _, v := range result.Entries {
			attrubute := v.GetAttributeValue(&quot;objectClass&quot;)
			if strings.Contains(attrubute, &quot;posixGroup&quot;) {
				user.Groups = append(user.Groups, v.GetAttributeValue(&quot;cn&quot;))
			}
		}

		u := userResult.Entries[0].GetAttributeValue(&quot;uid&quot;)
		user.UID = u
		user.Username = u
		return user, nil
	}
}
</code></pre>
<h3 id="部署webhook">部署webhook</h3>
<p>kubernetes官方手册中指出，启用webhook认证的标记是在 <em><strong>kube-apiserver</strong></em> 指定参数 <code>--authentication-token-webhook-config-file</code> 。而这个配置文件是一个 <em>kubeconfig</em> 类型的文件格式 <sup><a href="#5">[5]</a></sup></p>
<p>下列是部署在kubernetes集群外部的配置</p>
<p>创建一个给 <em>kube-apiserver</em> 使用的配置文件 <code>/etc/kubernetes/auth/authentication-webhook.conf</code></p>
<pre><code class="language-yaml">apiVersion: v1
kind: Config
clusters:
- cluster:
    server: http://10.0.0.1:88/authenticate
  name: authenticator
users:
- name: webhook-authenticator
current-context: webhook-authenticator@authenticator
contexts:
- context:
    cluster: authenticator
    user: webhook-authenticator
  name: webhook-authenticator@authenticator
</code></pre>
<p>修改 <em>kube-apiserver</em> 参数</p>
<pre><code class="language-bash"># 指向对应的配置文件
--authentication-token-webhook-config-file=/etc/kubernetes/auth/authentication-webhook.conf
# 这个是token缓存时间，指的是用户在访问API时验证通过后在一定时间内无需在请求webhook进行认证了
--authentication-token-webhook-cache-ttl=30m
# 版本指定为API使用哪个版本？authentication.k8s.io/v1或v1beta1
--authentication-token-webhook-version=v1
</code></pre>
<p>启动服务后，创建一个 kubeconfig 中的用户用于验证结果</p>
<pre><code class="language-conf">apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: 
    server: https://10.0.0.4:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: k8s-admin
  name: k8s-admin@kubernetes
current-context: k8s-admin@kubernetes
kind: Config
preferences: {}
users:
- name: admin
  user: 
    token: admin@111
</code></pre>
<h3 id="验证结果">验证结果</h3>
<p><strong>当密码不正确时，使用用户admin请求集群</strong></p>
<pre><code class="language-bash">$ kubectl get pods --user=admin
error: You must be logged in to the server (Unauthorized)
</code></pre>
<p><strong>当密码正确时，使用用户admin请求集群</strong></p>
<pre><code class="language-bash">$ kubectl get pods --user=admin
Error from server (Forbidden): pods is forbidden: User &quot;admin&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>
<p>可以看到admin用户是一个不存在与集群中的用户，并且提示没有权限操作对应资源，此时将admin用户与集群中的cluster-admin绑定，测试结果</p>
<pre><code class="language-bash">$ kubectl create clusterrolebinding admin \
	--clusterrole=cluster-admin \
	--group=admin
</code></pre>
<p>此时再尝试使用admin用户访问集群</p>
<pre><code class="language-bash">$ kubectl get pods --user=admin
NAME                      READY   STATUS    RESTARTS   AGE
netbox-85865d5556-hfg6v   1/1     Running   0          91d
netbox-85865d5556-vlgr4   1/1     Running   0          91d
</code></pre>
<h2 id="总结">总结</h2>
<p>kubernetes authentication 插件提供的功能可以注入一个认证系统，这样可以完美解决了kubernetes中用户的问题，而这些用户并不存在与kubernetes中，并且也无需为多个用户准备大量serviceaccount或者证书，也可以完成鉴权操作。首先返回值标准如下所示，如果kubernetes集群有对在其他用户系统中获得的 <code>Groups</code> 并建立了 <code>clusterrolebinding</code> 或 <code>rolebinding</code> 那么这个组的所有用户都将有这些权限。管理员只需要维护与公司用户系统中组同样多的 clusterrole 与 clusterrolebinding 即可</p>
<pre><code class="language-go">type DefaultInfo struct {
	Name   string
	UID    string
	Groups []string
	Extra  map[string][]string
}
</code></pre>
<p>对于如何将 kubernetes 与其他平台进行融合可以参考 <a href="https://cylonchau.github.io/kubernetes-dashborad-based.html" target="_blank"
   rel="noopener nofollow noreferrer" >文章</a></p>
<blockquote>
<p>Notes：Kubernetes原生就支持OID，完全不用自己开发webhook从而实现接入其他系统，这里展示的只是一个思路</p>
</blockquote>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://learnk8s.io/kubernetes-custom-authentication" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Implementing a custom Kubernetes authentication method</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/concepts/security/controlling-access/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Controlling Access to the Kubernetes API</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#users-in-kubernetes" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Users in Kubernetes</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#bootstrap-tokens" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>bootstrap tokens</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Webhook Token Authentication</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>漏桶算法与令牌桶算法</title>
      <link>https://www.oomkill.com/2022/08/ch10-token-bucket-algorithm/</link>
      <pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/08/ch10-token-bucket-algorithm/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="principle-of-token-bucket">Principle of token bucket</h2>
<p>随着互联网的发展，在处理流量的方法也不仅仅为 first-come，first-served，而在共享网络中实现流量管理的基本机制就是排队。而公平算法则是实现在优先级队列中基于哪些策略来排队的 “<strong>公平队列</strong>” 。<code>Token Bucket</code> 则是为公平排队提供了替代方案。Fair Queue 与 Token Bucket的区别主要在，对于Fair Queue来讲，如果请求者目前空闲，Queue会将该请求者的带宽分配给其他请求者；而 Token Bucket 则是分配给请求者的带宽是带宽的上限。</p>
<p><strong>通过例子了解算法原理</strong></p>
<p>假设出站带宽是 4个数据包/ms，此时有一个需求为，为一个特定的发送端 <strong>A</strong> 来分配 1个数据包/ms的带宽。此时可以使用公平排队的方法分给发送 <strong>A</strong> 25%的带宽。</p>
<p>此时存在的问题是我们希望可以灵活地允许 <strong>A</strong>  的数据包以无规则的时间间隔发送。例如假设 <strong>A</strong>  在每个数据包发送后等待1毫秒后再开始下一个数据包的发送。</p>
<ul>
<li>sence1：此时假设  <strong>A</strong>  以 1ms 的间隔去发送数据包，而由于某种原因导致应该在 t=6 到达的数据包却在 t=6.5 到达。随后的数据包在 t=7 准时到达，在这种情况下是否应该保留到t=7.5？</li>
<li>sence2：或者是否允许在 t=6.5 发送一个迟到的数据包，在 t=7 发送下一个数据包，此时理论上平均速率仍然还是 1 个数据包/ms？</li>
</ul>
<p>显然sence2是合理的，这个场景的解决方法就是<strong>令牌桶</strong>算法，规定 <strong>A</strong> 的配额，允许指定平均速率和突发容量。当数据包不符合令牌桶规范，那么就认为其不合理，此时会做出一下相应：</p>
<ul>
<li>delay，直到桶准备好</li>
<li>drop</li>
<li>mark，标记为不合规的数据包</li>
</ul>
<p>delay 被称为 <strong>整形</strong> <code>shaping</code> , <strong>shaping</strong> 是指在某个时间间隔内发送超过 <strong>Bc</strong>（Committed Burst）的大小，<strong>Bc</strong> 在这里指桶的尺寸。由于数据流量是突发性的，当在一段时间内不活动后，再次激活后的在一个间隔内发送的数量大于 <strong>Bc <strong>，那么额外的流量被称为</strong>Be</strong> （burst excess）。</p>
<p>将流量丢弃或标记超额流量，保持在一个流量速率限制称为 “<strong>管制</strong>” <code>policing</code>。</p>
<h3 id="definition">Definition</h3>
<p>令牌桶的定义是指，有一个桶，以稳定的速度填充令牌；桶中的任何一个溢出都会被丢弃。当要发送一个数据包，需要能够从桶中取出一个令牌；如果桶是空的那么此时数据包是不合规的数据包，必须进行 <code>delay</code> , <code>drop</code> , <code>mark</code> 操作。如果桶是满的，则会发送与桶容量相对应的突发（短时间内的高带宽传输），这是桶是空的。</p>
<p>令牌桶的规范：$TB(r,B_{max})$</p>
<ul>
<li>$r$ ：r个token每秒的令牌填充率，表示桶填充令牌的速率</li>
<li>$B$ ：桶容量，$B_{mac} &gt; 0$</li>
</ul>
<p>那么公式则表示，桶以指定的速率填充令牌，最大为 $B_{max}$ 。这就说明了为了使大小为 S 的数据包合规，桶内必须至少有 S 个令牌，即 $B \ge S$，否则数据包不合规，在发送时，桶为 $B=B-S$</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/fb2fa6c0c5aea327f72d2e67ed19c801.jpg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="examples">Examples</h3>
<p>场景1：假设令牌桶规范为 $TB(\frac{1}{3}\ packet/ms, 4\ packet)$，桶最初是满的，数据包在以下时间到达 <code>[0, 0, 0, 2, 3, 6, 9, 12]</code></p>
<p>在处理完所有 <code>T=0</code> 的数据包后，桶中还剩 1 个令牌。到第四个数据包 <code>T=2</code> 到达时，桶内已经有1个令牌 + $\frac{2}{3}$ 个令牌；当发送完第四个数据包时，桶内令牌数为  $\frac{2}{3}$ 。到 <code>T=3</code> 数据包时，桶内令牌为1，满足发送第 5 个数据包。万松完成后桶是空的，在后面 6 9 12时，都满足3/ms 一个数据包，都可以发送成功</p>
<p>场景2：另外一个实例，在同样的令牌桶规范下 $TB(\frac{1}{3}, 4)$，数据包到达时间为 <code>[0, 0, 0, 0, 12, 12, 12, 12, 24, 24, 24, 24]</code> ，可以看到在这个场景下，数据到达为3个突发，每个突发4个数据包，此时每次发送完成后桶被清空，当再次填满时需要12ms，此时另外一组突发达。故这组数据是合规的。、</p>
<p>场景3：在同样的令牌桶规范下 $TB(\frac{1}{3}, 4)$，数据包到达时间为 <code>[0, 1, 2, 3, 4, 5]</code> , 这组数据是不合规的</p>
<p>用表格形式表示如下：</p>
<table>
<thead>
<tr>
<th>数据包到达时间</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>发送前桶内令牌</td>
<td>4</td>
<td>3 $\frac{1}{3}$</td>
<td>2 $\frac{2}{3}$</td>
<td>2</td>
<td>1 $\frac{1}{3}$</td>
<td>$\frac{2}{3}$</td>
</tr>
<tr>
<td>发送后桶内令牌</td>
<td>3</td>
<td>2 $\frac{1}{3}$</td>
<td>1 $\frac{2}{3}$</td>
<td>1</td>
<td>$\frac{1}{3}$</td>
<td>$\frac{2}{3}$</td>
</tr>
</tbody>
</table>
<p>如果一个数据包在桶中没有足够的令牌来发送它时到达，可以进行整形或管制，整形使数据包等到足够的令牌积累。管制会丢弃数据包。或者发送方可以立即发送数据包，但将其标记为不合规。</p>
<h2 id="principle-of-leaky-bucket">Principle of leaky bucket</h2>
<p><strong>漏桶</strong> （leaky bucket）是一种临时存储可变数量的请求并将它们组织成设定速率输出的数据包的方法。漏桶的概念与令牌桶比起是相反的，漏桶可以理解为是一个具有恒定服务时间的队列。</p>
<p>由下图可以看出，漏桶的概念是一个底部有孔的桶。无论水进入桶的速度是多少，它都会以恒定的速度通过孔从桶中泄漏出来。如果桶中没有水，则流速为零，如果桶已满，则多余的水溢出并丢失。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/leaky-bucket.png" alt="漏桶算法" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>和令牌桶一样，漏桶用于流量整形和流量管制</p>
<h2 id="difference-between-token-and-leaky">Difference between Token and Leaky</h2>
<table>
<thead>
<tr>
<th>Leaky</th>
<th>Token</th>
</tr>
</thead>
<tbody>
<tr>
<td>桶中存放的是所有到达的数据包，必须入桶</td>
<td>桶中存放的是定期生成的令牌</td>
</tr>
<tr>
<td>桶以恒定速率泄漏</td>
<td>桶有最大容量 $B_{max}$</td>
</tr>
<tr>
<td>突发流量入桶转换为恒定流量发送</td>
<td>发送数据包需要小号对应的token</td>
</tr>
</tbody>
</table>
<p><strong>token较leaky的优势</strong>：</p>
<ul>
<li>在令牌桶中，如果桶已满，处理的方式有 shaping和policing两种模型三种方式（延迟、丢弃、标记），而漏桶中的流量仅为shaping。
<ul>
<li>通俗来说，就是令牌桶已满，丢弃的是令牌，漏桶中丢弃的则是数据包</li>
</ul>
</li>
<li>令牌桶可以更快的速率发送大突发流量，而漏桶仅是恒定速率</li>
</ul>
<h2 id="implementation-with-go">Implementation with go</h2>
<h3 id="token">Token</h3>
<p>在golang中，内置的 <code>rate</code> 包实现了一个令牌桶算法，通过 <code>rate.NewLimiter(r,B)</code> 进行构造。与公式$TB(r,B_{max})$ 意思相同。</p>
<pre><code class="language-go">type Limiter struct {
	limit Limit // 向桶中放置令牌的速率
	burst int // 桶的容量
	mu     sync.Mutex
	tokens float64 // 可用令牌容量
	last time.Time // 上次放入token的时间
	lastEvent time.Time
}
</code></pre>
<p>Limiter中带有三种方法， <code>Allow</code>、<code>Reserve</code>、<code>Wait</code> 分别表示Token Bucket中的 <code>shaping</code> 和 <code>policing</code>：</p>
<ul>
<li>Allow：丢弃超过速率的事件，类似 <code>drop</code></li>
<li>Wait：等待，直到获取到令牌或者取消或deadline/timeout</li>
<li>Reserve：等待或减速，不丢弃事件，类似于 <code>delay</code></li>
</ul>
<h3 id="reservereserven">Reserve/ReserveN</h3>
<ul>
<li><code>Reserve()</code>  返回了 <code>ReserveN(time.Now(), 1)</code></li>
<li><code>ReserveN()</code> 无论如何都会返回一个 Reservation，指定了调用者在 n 个事件发生之前必须等待多长时间。</li>
<li>Reservation 是一个令牌桶事件信息</li>
<li>Reservation 中的 <code>Delay()</code> 方法返回了需要等待的时间，如果时间为0则不需要等待</li>
<li>Reservation 中的 <code>Cancel()</code> 将取消等待</li>
</ul>
<p>wait/waitN</p>
<h3 id="allowallown">Allow/AllowN</h3>
<ul>
<li>在获取不到令牌是丢弃对应的事件</li>
<li>返回的是一个 <code>reserveN()</code> 拿到token是合规的，并消耗掉token</li>
</ul>
<p>AllowN 为截止到某一时刻，当前桶内桶中数目是否至少为 n 个，满足则返回 true，同时从桶中消费 n 个 token。反之不消费 Token，false。</p>
<pre><code class="language-go">func (lim *Limiter) AllowN(now time.Time, n int) bool {
	return lim.reserveN(now, n, 0).ok // 由于仅需要一个合规否，顾合规的通过，不合规的丢弃
}
</code></pre>
<p><code>reserveN()</code> 是三个行为的核心，AllowN中指定的为 <strong>0</strong> ，因为 <code>maxFutureReserve</code> 是最大的等待时间，AllowN给定的是0，即如果突发大的情况下丢弃额外的 <strong>Bc</strong>。</p>
<pre><code class="language-go">func (lim *Limiter) reserveN(now time.Time, n int, maxFutureReserve time.Duration) Reservation {
	lim.mu.Lock()

	if lim.limit == Inf {
		lim.mu.Unlock()
		return Reservation{
			ok:        true,
			lim:       lim,
			tokens:    n,
			timeToAct: now,
		}
	}
	// 这里拿到的是now，上次更新token时间和桶内token数量
	now, last, tokens := lim.advance(now)
	// 计算剩余的token
	tokens -= float64(n)

	// Calculate the wait duration
	var waitDuration time.Duration
	if tokens &lt; 0 {
		waitDuration = lim.limit.durationFromTokens(-tokens)
	}

	// 确定是否合规，n是token
    // token 的数量要小于桶的容量，并且 等待时间小于最大等待时间
	ok := n &lt;= lim.burst &amp;&amp; waitDuration &lt;= maxFutureReserve

	// Prepare reservation
	r := Reservation{
		ok:    ok,
		lim:   lim,
		limit: lim.limit,
	}
	if ok {
		r.tokens = n
		r.timeToAct = now.Add(waitDuration)
	}

	// Update state
	if ok {
		lim.last = now
		lim.tokens = tokens
		lim.lastEvent = r.timeToAct
	} else {
		lim.last = last
	}

	lim.mu.Unlock()
	return r
}
</code></pre>
<p>在reserveN中调用了一个 <code>advance()</code> 函数，</p>
<pre><code class="language-go">func (lim *Limiter) advance(now time.Time) (newNow time.Time, newLast time.Time, newTokens float64) {
   last := lim.last
   if now.Before(last) { // 计算上次放入token是否在传入now之前
      last = now
   }

   // 当 last 很旧时，避免在下面进行 delta 溢出。
   // maxElapsed 计算装满需要多少时间
   maxElapsed := lim.limit.durationFromTokens(float64(lim.burst) - lim.tokens)
   elapsed := now.Sub(last) // 上次装入到现在的时差
   if elapsed &gt; maxElapsed { // 上次如果放入token时间超长，就让他与装满时间相等
      elapsed = maxElapsed // 即，让桶为满的
   }

   // 装桶的动作，下面函数表示，elapsed时间内可以生成多少个token
   delta := lim.limit.tokensFromDuration(elapsed)
   tokens := lim.tokens + delta // 当前的token
   if burst := float64(lim.burst); tokens &gt; burst {
      tokens = burst // 这里表示token溢出，让他装满就好
   }

   return now, last, tokens
}
</code></pre>
<h3 id="waitwaitn">wait/waitN</h3>
<ul>
<li>桶内令牌可以&gt;N时，返回，在获取不到令牌是阻塞，等待context取消或者超时</li>
<li>返回的是一个 <code>reserveN()</code> 拿到token是合规的，并消耗掉token</li>
</ul>
<pre><code class="language-go">func (lim *Limiter) WaitN(ctx context.Context, n int) (err error) {
	if n &gt; lim.burst &amp;&amp; lim.limit != Inf {
		return fmt.Errorf(&quot;rate: Wait(n=%d) exceeds limiter's burst %d&quot;, n, lim.burst)
	}
	// 外部已取消
	select {
	case &lt;-ctx.Done():
		return ctx.Err()
	default:
	}
	// Determine wait limit
	now := time.Now()
	waitLimit := InfDuration
	if deadline, ok := ctx.Deadline(); ok {
		waitLimit = deadline.Sub(now)
	}
	// 三个方法的核心，这里给定了deatline
	r := lim.reserveN(now, n, waitLimit)
	if !r.ok {
		return fmt.Errorf(&quot;rate: Wait(n=%d) would exceed context deadline&quot;, n)
	}
	// Wait if necessary
	delay := r.DelayFrom(now)
	if delay == 0 {
		return nil
	}
	t := time.NewTimer(delay)
	defer t.Stop()
	select {
	case &lt;-t.C:
		// We can proceed.
		return nil
	case &lt;-ctx.Done():
		// Context was canceled before we could proceed.  Cancel the
		// reservation, which may permit other events to proceed sooner.
		r.Cancel()
		return ctx.Err()
	}
}
</code></pre>
<h3 id="dynamic-adjustment">Dynamic Adjustment</h3>
<p>在 <code>rate.limiter</code> 中，支持调整速率和桶大小，这样就可以根据现有环境和条件，来动态的改变 Token生成速率和桶容量</p>
<ul>
<li><code>SetLimit(Limit)</code>  更改生成 Token 的速率</li>
<li><code>SetBurst(int)</code>  改变桶容量</li>
</ul>
<h3 id="example">Example</h3>
<h4 id="一个流量整形的场景">一个流量整形的场景</h4>
<pre><code class="language-go">package main

import (
	&quot;log&quot;
	&quot;strconv&quot;
	&quot;time&quot;

	&quot;golang.org/x/time/rate&quot;
)

func main() {
	timeLayout := &quot;2006-01-02:15:04:05.0000&quot;
	limiter := rate.NewLimiter(1, 5) // BT(1,5)
	log.Println(&quot;bucket current capacity: &quot; + strconv.Itoa(limiter.Burst()))
	length := 20 // 一共请求20次
	chs := make([]chan string, length)
	for i := 0; i &lt; length; i++ {
		chs[i] = make(chan string, 1)
		go func(taskId string, ch chan string, r *rate.Limiter) {
			err := limiter.Allow()
			if !err {
				ch &lt;- &quot;Task-&quot; + taskId + &quot; unallow &quot; + time.Now().Format(timeLayout)
			}

			time.Sleep(time.Duration(5) * time.Millisecond)
			ch &lt;- &quot;Task-&quot; + taskId + &quot; run success  &quot; + time.Now().Format(timeLayout)
			return

		}(strconv.FormatInt(int64(i), 10), chs[i], limiter)
	}
	for _, ch := range chs {
		log.Println(&quot;task start at &quot; + &lt;-ch)
	}
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220615200407126.png" alt="image-20220615200407126" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>通过执行结果可以看出，在突发为20的情况下，allow仅允许了获得token的事件执行，，这种场景下实现了流量整形的特性。</p>
<h4 id="一个流量管制的场景">一个流量管制的场景</h4>
<pre><code class="language-go">package main

import (
	&quot;context&quot;
	&quot;log&quot;
	&quot;strconv&quot;
	&quot;time&quot;

	&quot;golang.org/x/time/rate&quot;
)

func main() {
	timeLayout := &quot;2006-01-02:15:04:05.0000&quot;
	limiter := rate.NewLimiter(1, 5) // BT(1,5)
	log.Println(&quot;bucket current capacity: &quot; + strconv.Itoa(limiter.Burst()))
	length := 20 // 一共请求20次
	chs := make([]chan string, length)
	for i := 0; i &lt; length; i++ {
		chs[i] = make(chan string, 1)
		go func(taskId string, ch chan string, r *rate.Limiter) {
			err := limiter.Wait(context.TODO())
			if err != nil {
				ch &lt;- &quot;Task-&quot; + taskId + &quot; unallow &quot; + time.Now().Format(timeLayout)
			}
			ch &lt;- &quot;Task-&quot; + taskId + &quot; run success  &quot; + time.Now().Format(timeLayout)
			return

		}(strconv.FormatInt(int64(i), 10), chs[i], limiter)
	}
	for _, ch := range chs {
		log.Println(&quot;task start at &quot; + &lt;-ch)
	}
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220615201137581.png" alt="image-20220615201137581" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>结果可以看出，在大突发的情况下，在拿到token的任务会立即执行，没有拿到token的会等待拿到token后继续执行，这种场景下实现了流量管制的特性</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="http://intronetworks.cs.luc.edu/current/html/tokenbucket.html" target="_blank"
   rel="noopener nofollow noreferrer" >tokenbucket</a></li>
<li><a href="https://content.cisco.com/chapter.sjs?uri=/searchable/chapter/content/en/us/td/docs/routers/ncs4000/software/qos/configuration_guide/b-qos-cg/b-qos-cg_chapter_0111.html.xml" target="_blank"
   rel="noopener nofollow noreferrer" >QoS Policing</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes Pod网络排错思路</title>
      <link>https://www.oomkill.com/2022/08/pod-network-troubleshooting/</link>
      <pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/08/pod-network-troubleshooting/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>本文将引入一个思路：“在Kubernetes集群发生网络异常时如何排查”。文章将引入Kubernetes 集群中网络排查的思路，包含网络异常模型，常用工具，并且提出一些案例以供学习。</p>
<ul>
<li>Pod常见网络异常分类</li>
<li>网络排查工具</li>
<li>Pod网络异常排查思路及流程模型</li>
<li>CNI网络异常排查步骤</li>
<li>案例学习</li>
</ul>
<h2 id="pod网络异常">Pod网络异常</h2>
<p>网络异常大概分为如下几类：</p>
<ul>
<li>
<p><strong>网络不可达</strong>，主要现象为ping不通，其可能原因为：</p>
<ul>
<li>源端和目的端防火墙（<code>iptables</code>, <code>selinux</code>）限制</li>
<li>网络路由配置不正确</li>
<li>源端和目的端的系统负载过高，网络连接数满，网卡队列满</li>
<li>网络链路故障</li>
</ul>
</li>
<li>
<p><strong>端口不可达</strong>：主要现象为可以ping通，但telnet端口不通，其可能原因为：</p>
<ul>
<li>源端和目的端防火墙限制</li>
<li>源端和目的端的系统负载过高，网络连接数满，网卡队列满，端口耗尽</li>
<li>目的端应用未正常监听导致（应用未启动，或监听为127.0.0.1等）</li>
</ul>
</li>
<li>
<p><strong>DNS解析异常</strong>：主要现象为基础网络可以连通，访问域名报错无法解析，访问IP可以正常连通。其可能原因为</p>
<ul>
<li>Pod的DNS配置不正确</li>
<li>DNS服务异常</li>
<li>pod与DNS服务通讯异常</li>
</ul>
</li>
<li>
<p><strong>大数据包丢包</strong>：主要现象为基础网络和端口均可以连通，小数据包收发无异常，大数据包丢包。可能原因为：</p>
<ul>
<li>数据包的大小超过了 <em>dockero</em>，<em>CNI</em> 插件，或者宿主机网卡的 <em>MTU</em> 值。
<ul>
<li>可使用 <code>ping -s</code> 指定数据包大小进行测试</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>CNI异常</strong>：主要现象为Node可以通，但Pod无法访问集群地址，可能原因有：</p>
<ul>
<li><em>kube-proxy</em> 服务异常，没有生成 <em>iptables</em> 策略或者 <em>ipvs</em> 规则导致无法访问</li>
<li>CIDR耗尽，无法为Node注入 <code>PodCIDR</code> 导致 <em>CNI</em> 插件异常</li>
<li>其他 <em>CNI</em> 插件问题</li>
</ul>
</li>
</ul>
<p>那么整个Pod网络异常分类可以如下图所示：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821164836806.png" alt="image-20220821164836806" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Pod network trouble hirarchy</center><br>
<p>总结一下，Pod最常见的网络故障有，网络不可达（ping不通）；端口不可达（telnet不通）；DNS解析异常（域名不通）与大数据包丢失（大包不通）。</p>
<h2 id="常用网络排查工具">常用网络排查工具</h2>
<p>在了解到常见的网络异常后，在排查时就需要使用到一些网络工具才可以很有效的定位到网络故障原因，下面会介绍一些网络排查工具。</p>
<h3 id="tcpdump-supa-href11asup">tcpdump <sup><a href="#1">[1]</a></sup></h3>
<p>tcpdump网络嗅探器，将强大和简单结合到一个单一的命令行界面中，能够将网络中的报文抓取，输出到屏幕或者记录到文件中。</p>
<blockquote>
<p><strong>各系统下的安装</strong></p>
<ul>
<li>Ubuntu/Debian: <code>tcpdump</code>  ；<code>apt-get install -y tcpdump</code></li>
<li>Centos/Fedora: <code>tcpdump</code> ；<code>yum install -y tcpdump</code></li>
<li>Apline：<code>tcpdump </code> ；<code>apk add tcpdump --no-cache</code></li>
</ul>
</blockquote>
<p>查看指定接口上的所有通讯</p>
<p>语法</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>-i [interface]</td>
<td></td>
</tr>
<tr>
<td> -w [flle]</td>
<td>第一个n表示将地址解析为数字格式而不是主机名，第二个N表示将端口解析为数字格式而不是服务名</td>
</tr>
<tr>
<td>-n</td>
<td>不显示IP地址</td>
</tr>
<tr>
<td>-X</td>
<td>hex and ASCII</td>
</tr>
<tr>
<td>-A</td>
<td>ASCII（实际上是以人类可读懂的包进行显示）</td>
</tr>
<tr>
<td>-XX</td>
<td></td>
</tr>
<tr>
<td>-v</td>
<td>详细信息</td>
</tr>
<tr>
<td>-r</td>
<td>读取文件而不是实时抓包</td>
</tr>
<tr>
<td></td>
<td><strong>关键字</strong></td>
</tr>
<tr>
<td><strong>type</strong></td>
<td>host（主机名，域名，IP地址）, net, port, portrange</td>
</tr>
<tr>
<td><strong>direction</strong></td>
<td>src, dst, src or dst , src and ds</td>
</tr>
<tr>
<td><strong>protocol</strong></td>
<td>ether, ip，arp, tcp, udp, wlan</td>
</tr>
</tbody>
</table>
<h4 id="捕获所有网络接口">捕获所有网络接口</h4>
<pre><code class="language-bash">tcpdump -D
</code></pre>
<p>####按IP查找流量</p>
<p>最常见的查询之一 <code>host</code>，可以看到来往于 <code>1.1.1.1</code> 的流量。</p>
<pre><code class="language-bash">tcpdump host 1.1.1.1
</code></pre>
<p>####按源/目的 地址过滤</p>
<p>如果只想查看来自/向某方向流量，可以使用 <code>src</code> 和 <code>dst</code>。</p>
<pre><code class="language-bash">tcpdump src|dst 1.1.1.1
</code></pre>
<h4 id="通过网络查找数据包">通过网络查找数据包</h4>
<p>使用 <code>net</code> 选项，来要查找出/入某个网络或子网的数据包。</p>
<pre><code class="language-bash">tcpdump net 1.2.3.0/24
</code></pre>
<h4 id="使用十六进制输出数据包内容">使用十六进制输出数据包内容</h4>
<p><code>hex</code> 可以以16进制输出包的内容</p>
<pre><code class="language-bash">tcpdump -c 1 -X icmp
</code></pre>
<h4 id="查看特定端口的流量">查看特定端口的流量</h4>
<p>使用 <code>port</code> 选项来查找特定的端口流量。</p>
<pre><code class="language-bash">tcpdump port 3389
tcpdump src port 1025
</code></pre>
<h4 id="查找端口范围的流量">查找端口范围的流量</h4>
<pre><code class="language-bash">tcpdump portrange 21-23
</code></pre>
<h4 id="过滤包的大小">过滤包的大小</h4>
<p>如果需要查找特定大小的数据包，可以使用以下选项。你可以使用 <code>less</code>，<code>greater</code>。</p>
<pre><code class="language-bash">tcpdump less 32
tcpdump greater 64
tcpdump &lt;= 128
</code></pre>
<h4 id="捕获流量输出为文件">捕获流量输出为文件</h4>
<p><code>-w</code>  可以将数据包捕获保存到一个文件中以便将来进行分析。这些文件称为<code>PCAP</code>（PEE-cap）文件，它们可以由不同的工具处理，包括 <code>Wireshark</code> 。</p>
<pre><code class="language-bash">tcpdump port 80 -w capture_file
</code></pre>
<h4 id="组合条件">组合条件</h4>
<p>tcpdump也可以结合逻辑运算符进行组合条件查询</p>
<ul>
<li>
<p><strong>AND</strong>
<em><code>and</code></em> or <code>&amp;&amp;</code></p>
</li>
<li>
<p><strong>OR</strong>
<em><code>or</code></em> or <code>||</code></p>
</li>
<li>
<p><strong>EXCEPT</strong>
<em><code>not</code></em> or <code>!</code></p>
</li>
</ul>
<pre><code class="language-bash">tcpdump -i eth0 -nn host 220.181.57.216 and 10.0.0.1  # 主机之间的通讯
tcpdump -i eth0 -nn host 220.181.57.216 or 10.0.0.1
# 获取10.0.0.1与 10.0.0.9或 10.0.0.1 与10.0.0.3之间的通讯
tcpdump -i eth0 -nn host 10.0.0.1 and \(10.0.0.9 or 10.0.0.3\)
</code></pre>
<h4 id="原始输出">原始输出</h4>
<p>并显示人类可读的内容进行输出包（不包含内容）。</p>
<pre><code class="language-bash">tcpdump -ttnnvvS -i eth0 
tcpdump -ttnnvvS -i eth0 
</code></pre>
<h4 id="ip到端口">IP到端口</h4>
<p>让我们查找从某个IP到端口任何主机的某个端口所有流量。</p>
<pre><code class="language-bash">tcpdump -nnvvS src 10.5.2.3 and dst port 3389
</code></pre>
<h4 id="去除特定流量">去除特定流量</h4>
<p>可以将指定的流量排除，如这显示所有到192.168.0.2的 非ICMP的流量。</p>
<pre><code class="language-bash">tcpdump dst 192.168.0.2 and src net and not icmp
</code></pre>
<p>来自非指定端口的流量，如，显示来自不是SSH流量的主机的所有流量。</p>
<pre><code class="language-bash">tcpdump -vv src mars and not dst port 22
</code></pre>
<h4 id="选项分组">选项分组</h4>
<p>在构建复杂查询时，必须使用单引号 <code>'</code>。单引号用于忽略特殊符号 <code>()</code> ，以便于使用其他表达式（如host, port, net等）进行分组。</p>
<pre><code class="language-bash">tcpdump 'src 10.0.2.4 and (dst port 3389 or 22)'
</code></pre>
<h4 id="过滤tcp标记位">过滤TCP标记位</h4>
<p>TCP RST</p>
<p>The filters below find these various packets because tcp[13] looks at offset 13 in the TCP header, the number represents the location within the byte, and the !=0 means that the flag in question is set to 1, i.e. it’s on.</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 4!=0'
tcpdump 'tcp[tcpflags] == tcp-rst'
</code></pre>
<p>TCP SYN</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 2!=0'
tcpdump 'tcp[tcpflags] == tcp-syn'
</code></pre>
<p>同时忽略SYN和ACK标志的数据包</p>
<pre><code class="language-bash">tcpdump 'tcp[13]=18'
</code></pre>
<p>TCP URG</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 32!=0'
tcpdump 'tcp[tcpflags] == tcp-urg'
</code></pre>
<p>TCP ACK</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 16!=0'
tcpdump 'tcp[tcpflags] == tcp-ack'
</code></pre>
<p>TCP PSH</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 8!=0'
tcpdump 'tcp[tcpflags] == tcp-push'
</code></pre>
<p>TCP FIN</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 1!=0'
tcpdump 'tcp[tcpflags] == tcp-fin'
</code></pre>
<h4 id="查找http包">查找http包</h4>
<p>查找 <code>user-agent</code> 信息</p>
<pre><code class="language-bash">tcpdump -vvAls0 | grep 'User-Agent:'
</code></pre>
<p>查找只是 <code>GET</code> 请求的流量</p>
<pre><code class="language-bash">tcpdump -vvAls0 | grep 'GET'
</code></pre>
<p>查找http客户端IP</p>
<pre><code class="language-bash">tcpdump -vvAls0 | grep 'Host:'
</code></pre>
<p>查询客户端cookie</p>
<pre><code class="language-bash">tcpdump -vvAls0 | grep 'Set-Cookie|Host:|Cookie:'
</code></pre>
<h4 id="查找dns流量">查找DNS流量</h4>
<pre><code class="language-bash">tcpdump -vvAs0 port 53
</code></pre>
<h4 id="查找对应流量的明文密码">查找对应流量的明文密码</h4>
<pre><code class="language-bash">tcpdump port http or port ftp or port smtp or port imap or port pop3 or port telnet -lA | egrep -i -B5 'pass=|pwd=|log=|login=|user=|username=|pw=|passw=|passwd= |password=|pass:|user:|username:|password:|login:|pass |user '
</code></pre>
<h4 id="wireshark追踪流">wireshark追踪流</h4>
<p>wireshare追踪流可以很好的了解出在一次交互过程中都发生了那些问题。</p>
<p>wireshare选中包，右键选择 “追踪流“ 如果该包是允许的协议是可以打开该选项的</p>
<h4 id="关于抓包节点和抓包设备">关于抓包节点和抓包设备</h4>
<p>如何抓取有用的包，以及如何找到对应的接口，有以下建议</p>
<p><strong>抓包节点</strong>：</p>
<p>通常情况下会在==源端==和==目的端==两端同时抓包，观察数据包是否从源端正常发出，目的端是否接收到数据包并给源端回包，以及源端是否正常接收到回包。如果有丢包现象，则沿网络链路上各节点抓包排查。例如，A节点经过c节点到B节点，先在AB两端同时抓包，如果B节点未收到A节点的包，则在c节点同时抓包。</p>
<p><strong>抓包设备</strong>：</p>
<p>对于 Kubernetes 集群中的Pod，由于容器内不便于抓包，通常视情况在Pod数据包经过的veth设备，<em>docker0</em> 网桥，<em>CNI</em> 插件设备（如cni0，flannel.1 etc..）及Pod所在节点的网卡设备上指定Pod IP进行抓包。选取的设备根据怀疑导致网络问题的原因而定，比如范围由大缩小，从源端逐渐靠近目的端，比如怀疑是 <em>CNI</em> 插件导致，则在 <em>CNI</em> 插件设备上抓包。从pod发出的包逐一经过veth设备，<em>cni0</em> 设备，<em>flannel0</em>，宿主机网卡，到达对端，抓包时可按顺序逐一抓包，定位问题节点。</p>
<blockquote>
<p>需要注意在不同设备上抓包时指定的源目IP地址需要转换，如抓取某Pod时，ping <em>{host}</em> 的包，在 <em>veth</em> 和 <em>cni0</em> 上可以指定 Pod IP抓包，而在宿主机网卡上如果仍然指定Pod IP会发现抓不到包，因为此时Pod IP已被转换为宿主机网卡IP。</p>
</blockquote>
<p>下图是一个使用 <em>VxLAN</em> 模式的 <em>flannel</em> 的跨界点通讯的网络模型，在抓包时需要注意对应的网络接口</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821200501496.png" alt="image-20220821200501496" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：VxLAN in kubernetes</center><br>
<h3 id="nsenter">nsenter</h3>
<p>nsenter是一款可以进入进程的名称空间中。例如，如果一个容器以非 root 用户身份运行，而使用 <code>docker exec</code> 进入其中后，但该容器没有安装 <code>sudo</code> 或未 <code>netstat</code> ，并且您想查看其当前的网络属性，如开放端口，这种场景下将如何做到这一点？<em><strong>nsenter</strong></em> 就是用来解决这个问题的。</p>
<p><strong>nsenter</strong> (<em>namespace enter</em>) 可以在容器的宿主机上使用 <em>nsenter</em> 命令进入容器的命名空间，以容器视角使用宿主机上的相应网络命令进行操作。==当然需要拥有 <em>root</em> 权限==</p>
<blockquote>
<p><strong>各系统下的安装</strong> <sup><a href="#2">[2]</a></sup></p>
<ul>
<li>Ubuntu/Debian: <code>util-linux</code>  ；<code>apt-get install -y util-linux</code></li>
<li>Centos/Fedora: <code>util-linux</code> ；<code>yum install -y util-linux</code></li>
<li>Apline：<code>util-linux</code> ；<code>apk add util-linux --no-cache</code></li>
</ul>
</blockquote>
<p><em>nsenter</em> 的使用语法为，<code>nsenter -t pid -n &lt;commond&gt;</code>，<code>-t</code> 接 进程ID号，<code>-n</code> 表示进入名称空间内，<code>&lt;commond&gt;</code> 为执行的命令。更多的内容可以参考 <sup><a href="#3">[3]</a></sup></p>
<p>实例：如我们有一个Pod进程ID为30858，进入该Pod名称空间内执行 <code>ifconfig</code> ，如下列所示</p>
<pre><code class="language-bash">$ ps -ef|grep tail
root      17636  62887  0 20:19 pts/2    00:00:00 grep --color=auto tail
root      30858  30838  0 15:55 ?        00:00:01 tail -f

$ nsenter -t 30858 -n ifconfig
eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1480
        inet 192.168.1.213  netmask 255.255.255.0  broadcast 192.168.1.255
        ether 5e:d5:98:af:dc:6b  txqueuelen 0  (Ethernet)
        RX packets 92  bytes 9100 (8.8 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 92  bytes 8422 (8.2 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 5  bytes 448 (448.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 5  bytes 448 (448.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

net1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 10.1.0.201  netmask 255.255.255.0  broadcast 10.1.0.255
        ether b2:79:f9:dd:2a:10  txqueuelen 0  (Ethernet)
        RX packets 228  bytes 21272 (20.7 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 216  bytes 20272 (19.7 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>
<h4 id="如何定位pod名称空间">如何定位Pod名称空间</h4>
<p>首先需要确定Pod所在的节点名称</p>
<pre><code class="language-bash">$ kubectl get pods -owide |awk '{print $1,$7}'
NAME NODE
netbox-85865d5556-hfg6v master-machine
netbox-85865d5556-vlgr4 node01
</code></pre>
<p>如果Pod不在当前节点还需要用IP登录则还需要查看IP（可选）</p>
<pre><code class="language-bash">$ kubectl get pods -owide |awk '{print $1,$6,$7}'
NAME IP NODE
netbox-85865d5556-hfg6v 192.168.1.213 master-machine
netbox-85865d5556-vlgr4 192.168.0.4 node01
</code></pre>
<p>接下来，登录节点，获取容器lD，如下列所示，每个pod默认有一个 <em>pause</em> 容器，其他为用户yaml文件中定义的容器，理论上所有容器共享相同的网络命名空间，排查时可任选一个容器。</p>
<pre><code class="language-bash">$ docker ps |grep netbox-85865d5556-hfg6v
6f8c58377aae   f78dd05f11ff                                                    &quot;tail -f&quot;                45 hours ago   Up 45 hours             k8s_netbox_netbox-85865d5556-hfg6v_default_4a8e2da8-05d1-4c81-97a7-3d76343a323a_0
b9c732ee457e   registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1   &quot;/pause&quot;                 45 hours ago   Up 45 hours             k8s_POD_netbox-85865d5556-hfg6v_default_4a8e2da8-05d1-4c81-97a7-3d76343a323a_0
</code></pre>
<p>接下来获得获取容器在节点系统中对应的进程号，如下所示</p>
<pre><code class="language-bash">$ docker inspect --format &quot;{{ .State.Pid }}&quot; 6f8c58377aae
30858
</code></pre>
<p>最后就可以通过 <em>nsenter</em> 进入容器网络空间执行命令了</p>
<h3 id="paping">paping</h3>
<p><strong>paping</strong> 命令可对目标地址指定端口以TCP协议进行连续ping，通过这种特性可以弥补 <em>ping</em> ICMP协议，以及 <em>nmap</em> , <em>telnet</em> 只能进行一次操作的的不足；通常情况下会用于测试端口连通性和丢包率</p>
<p>paping download：<a href="https://code.google.com/archive/p/paping/" target="_blank"
   rel="noopener nofollow noreferrer" >paping</a></p>
<p><em>paping</em> 还需要安装以下依赖，这取决于你安装的 <em>paping</em> 版本</p>
<ul>
<li>RedHat/CentOS：<code>yum install -y libstdc++.i686 glibc.i686</code></li>
<li>Ubuntu/Debian：最小化安装无需依赖</li>
</ul>
<pre><code class="language-bash">$ paping -h
paping v1.5.5 - Copyright (c) 2011 Mike Lovell

Syntax: paping [options] destination

Options:
 -?, --help     display usage
 -p, --port N   set TCP port N (required)
     --nocolor  Disable color output
 -t, --timeout  timeout in milliseconds (default 1000)
 -c, --count N  set number of checks to N
</code></pre>
<h3 id="mtr">mtr</h3>
<p><strong>mtr</strong> 是一个跨平台的网络诊断工具，将 <strong>traceroute</strong> 和 <strong>ping</strong> 的功能结合到一个工具。与 <em>traceroute</em> 不同的是 <em>mtr</em> 显示的信息比起 <em>traceroute</em> 更加丰富：通过 <em>mtr</em> 可以确定网络的条数，并且可以同时打印响应百分比以及网络中各跳跃点的响应时间。</p>
<blockquote>
<p><strong>各系统下的安装</strong> <sup><a href="#2">[2]</a></sup></p>
<ul>
<li>Ubuntu/Debian: <code>mtr</code>  ；<code>apt-get install -y mtr</code></li>
<li>Centos/Fedora: <code>mtr</code> ；<code>yum install -y mtr</code></li>
<li>Apline：<code>mtr</code> ；<code>apk add mtr --no-cache</code></li>
</ul>
</blockquote>
<h4 id="简单的使用示例">简单的使用示例</h4>
<p>最简单的示例，就是后接域名或IP，这将跟踪整个路由</p>
<pre><code class="language-bash">$ mtr google.com

Start: Thu Jun 28 12:10:13 2018
HOST: TecMint                     Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- 192.168.0.1                0.0%     5    0.3   0.3   0.3   0.4   0.0
  2.|-- 5.5.5.211                  0.0%     5    0.7   0.9   0.7   1.3   0.0
  3.|-- 209.snat-111-91-120.hns.n 80.0%     5    7.1   7.1   7.1   7.1   0.0
  4.|-- 72.14.194.226              0.0%     5    1.9   2.9   1.9   4.4   1.1
  5.|-- 108.170.248.161            0.0%     5    2.9   3.5   2.0   4.3   0.7
  6.|-- 216.239.62.237             0.0%     5    3.0   6.2   2.9  18.3   6.7
  7.|-- bom05s12-in-f14.1e100.net  0.0%     5    2.1   2.4   2.0   3.8   0.5
</code></pre>
<p><code>-n</code> 强制 <em>mtr</em> 打印 IP地址而不是主机名</p>
<pre><code class="language-bash">$ mtr -n google.com

Start: Thu Jun 28 12:12:58 2018
HOST: TecMint                     Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- 192.168.0.1                0.0%     5    0.3   0.3   0.3   0.4   0.0
  2.|-- 5.5.5.211                  0.0%     5    0.9   0.9   0.8   1.1   0.0
  3.|-- ???                       100.0     5    0.0   0.0   0.0   0.0   0.0
  4.|-- 72.14.194.226              0.0%     5    2.0   2.0   1.9   2.0   0.0
  5.|-- 108.170.248.161            0.0%     5    2.3   2.3   2.2   2.4   0.0
  6.|-- 216.239.62.237             0.0%     5    3.0   3.2   3.0   3.3   0.0
  7.|-- 172.217.160.174            0.0%     5    3.7   3.6   2.0   5.3   1.4
</code></pre>
<p><code>-b</code> 同时显示IP地址与主机名</p>
<pre><code class="language-bash">$ mtr -b google.com

Start: Thu Jun 28 12:14:36 2018
HOST: TecMint                     Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- 192.168.0.1                0.0%     5    0.3   0.3   0.3   0.4   0.0
  2.|-- 5.5.5.211                  0.0%     5    0.7   0.8   0.6   1.0   0.0
  3.|-- 209.snat-111-91-120.hns.n  0.0%     5    1.4   1.6   1.3   2.1   0.0
  4.|-- 72.14.194.226              0.0%     5    1.8   2.1   1.8   2.6   0.0
  5.|-- 108.170.248.209            0.0%     5    2.0   1.9   1.8   2.0   0.0
  6.|-- 216.239.56.115             0.0%     5    2.4   2.7   2.4   2.9   0.0
  7.|-- bom07s15-in-f14.1e100.net  0.0%     5    3.7   2.2   1.7   3.7   0.9
</code></pre>
<p><code>-c</code> 跟一个具体的值，这将限制 <em>mtr</em> ping的次数，到达次数后会退出</p>
<pre><code class="language-bash">$ mtr -c5 google.com
</code></pre>
<p>如果需要指定次数，并且在退出后保存这些数据，使用 <code>-r</code> flag</p>
<pre><code class="language-bash">$ mtr -r -c 5 google.com &gt;  1
$ cat 1
Start: Sun Aug 21 22:06:49 2022
HOST: xxxxx.xxxxx.xxxx.xxxx Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- gateway                    0.0%     5    0.6 146.8   0.6 420.2 191.4
  2.|-- 212.xx.21.241              0.0%     5    0.4   1.0   0.4   2.3   0.5
  3.|-- 188.xxx.106.124            0.0%     5    0.7   1.1   0.7   2.1   0.5
  4.|-- ???                       100.0     5    0.0   0.0   0.0   0.0   0.0
  5.|-- 72.14.209.89               0.0%     5   43.2  43.3  43.1  43.3   0.0
  6.|-- 108.xxx.250.33             0.0%     5   43.2  43.1  43.1  43.2   0.0
  7.|-- 108.xxx.250.34             0.0%     5   43.7  43.6  43.5  43.7   0.0
  8.|-- 142.xxx.238.82             0.0%     5   60.6  60.9  60.6  61.2   0.0
  9.|-- 142.xxx.238.64             0.0%     5   59.7  67.5  59.3  89.8  13.2
 10.|-- 142.xxx.37.81              0.0%     5   62.7  62.9  62.6  63.5   0.0
 11.|-- 142.xxx.229.85             0.0%     5   61.0  60.9  60.7  61.3   0.0
 12.|-- xx-in-f14.1e100.net  0.0%     5   59.0  58.9  58.9  59.0   0.0
</code></pre>
<p>默认使用的是 ICMP 协议 <code>-i</code> ，可以指定 <code>-u</code>,  <code>-t</code> 使用其他协议</p>
<pre><code class="language-bash">mtr --tcp google.com
</code></pre>
<p><code>-m</code> 指定最大的跳数</p>
<pre><code class="language-bash">mtr -m 35 216.58.223.78
</code></pre>
<p><code>-s</code> 指定包的大小</p>
<h4 id="mtr输出的数据">mtr输出的数据</h4>
<table>
<thead>
<tr>
<th>colum</th>
<th>describe</th>
</tr>
</thead>
<tbody>
<tr>
<td>last</td>
<td>最近一次的探测延迟值</td>
</tr>
<tr>
<td>avg</td>
<td>探测延迟的平均值</td>
</tr>
<tr>
<td>best</td>
<td>探测延迟的最小值</td>
</tr>
<tr>
<td>wrst</td>
<td>探测延迟的最大值</td>
</tr>
<tr>
<td>stdev</td>
<td>标准偏差。越大说明相应节点越不稳定</td>
</tr>
</tbody>
</table>
<h4 id="丢包判断">丢包判断</h4>
<p>任一节点的 <code>Loss%</code>（丢包率）如果不为零，则说明这一跳网络可能存在问题。导致相应节点丢包的原因通常有两种。</p>
<ul>
<li>运营商基于安全或性能需求，人为限制了节点的ICMP发送速率，导致丢包。</li>
<li>节点确实存在异常，导致丢包。可以结合异常节点及其后续节点的丢包情况，来判定丢包原因。</li>
</ul>
<blockquote>
<p>Notes:</p>
<ul>
<li>如果随后节点均没有丢包，则通常说明异常节点丢包是由于运营商策略限制所致。可以忽略相关丢包。</li>
<li>如果随后节点也出现丢包，则通常说明节点确实存在网络异常，导致丢包。对于这种情况，如果异常节点及其后续节点连续出现丢包，而且各节点的丢包率不同，则通常以最后几跳的丢包率为准。如链路测试在第5、6、7跳均出现了丢包。最终丢包情况以第7跳作为参考。</li>
</ul>
</blockquote>
<h4 id="延迟判断">延迟判断</h4>
<p>由于链路抖动或其它因素的影响，节点的 <em>Best</em> 和 <em>Worst</em> 值可能相差很大。而 <em>Avg</em>（平均值）统计了自链路测试以来所有探测的平均值，所以能更好的反应出相应节点的网络质量。而 <em>StDev</em>（标准偏差值）越高，则说明数据包在相应节点的延时值越不相同（越离散）。所以标准偏差值可用于协助判断 <em>Avg</em> 是否真实反应了相应节点的网络质量。例如，如果标准偏差很大，说明数据包的延迟是不确定的。可能某些数据包延迟很小（例如：25ms），而另一些延迟却很大（例如：350ms），但最终得到的平均延迟反而可能是正常的。所以此时 <em>Avg</em> 并不能很好的反应出实际的网络质量情况。</p>
<p>这就需要结合如下情况进行判断：</p>
<ul>
<li>如果 <em>StDev</em> 很高，则同步观察相应节点的 <em>Best</em> 和 <em>wrst</em>，来判断相应节点是否存在异常。</li>
<li>如果<em>StDev</em> 不高，则通过Avg来判断相应节点是否存在异常。</li>
</ul>
<blockquote>
<p>Tips：对于更多的网络工具的使用可以参考这篇<a href="https://www.cnblogs.com/Cylon/p/14946935.html" target="_blank"
   rel="noopener nofollow noreferrer" >文章</a></p>
</blockquote>
<h2 id="pod网络排查流程">Pod网络排查流程</h2>
<p>Pod网络异常时排查思路，可以按照下图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821225201747.png" alt="image-20220821225201747" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Pod network exception troubleshooting idea</center><br>
<h2 id="案例学习">案例学习</h2>
<h3 id="扩容节点访问service地址不通">扩容节点访问service地址不通</h3>
<p><strong>测试环境k8s节点扩容后无法访问集群clusterlP类型的registry服务</strong></p>
<p>环境信息：</p>
<table>
<thead>
<tr>
<th>IP</th>
<th>Hostname</th>
<th>role</th>
</tr>
</thead>
<tbody>
<tr>
<td>10.153.204.15</td>
<td>yxxx-xxx-xxxfu12</td>
<td>worknode节点（本次扩容的问题节点）</td>
</tr>
<tr>
<td>10.153.203.14</td>
<td>yxxx-xxx-xxxxfu31</td>
<td>master节点</td>
</tr>
<tr>
<td>10.61.187.42</td>
<td>yxxx-xxx-xxxxxxxxf8e9</td>
<td>master节点</td>
</tr>
<tr>
<td>10.61.187.48</td>
<td>yxxx-xxx-xxxxxx61e25</td>
<td>master节点（本次registry服务pod所在<br/>节点）</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>cni插件：flannel vxlan</p>
</li>
<li>
<p>kube-proxy工作模式为iptables</p>
</li>
<li>
<p>registry服务</p>
<ul>
<li>单实例部署在10.61.187.48:5000</li>
<li>Pod IP：10.233.65.46，</li>
<li>Cluster IP：10.233.0.100</li>
</ul>
</li>
</ul>
<p>现象：</p>
<ul>
<li>
<p>所有节点之间的pod通信正常</p>
</li>
<li>
<p>任意节点和Pod curl registry的Pod 的 <em>IP:5000</em> 均可以连通</p>
</li>
<li>
<p>新扩容节点10.153.204.15 curl registry服务的 Cluster lP 10.233.0.100:5000不通，其他节点curl均可以连通</p>
</li>
</ul>
<p>分析思路：</p>
<ul>
<li>
<p>根据现象1可以初步判断 <em>CNI</em> 插件无异常</p>
</li>
<li>
<p>根据现象2可以判断 <em>registry</em> 的 <em>Pod</em> 无异常</p>
</li>
<li>
<p>根据现象3可以判断 <em>registry</em> 的 <em>service</em> 异常的可能性不大，可能是新扩容节点访问 <em>registry</em> 的 <em>service</em> 存在异常</p>
</li>
</ul>
<p>怀疑方向：</p>
<ul>
<li>问题节点的kube-proxy存在异常</li>
<li>问题节点的iptables规则存在异常</li>
<li>问题节点到service的网络层面存在异常</li>
</ul>
<p>排查过程：</p>
<ul>
<li>排查问题节点的<code> kube-proxy</code></li>
<li>执行 <code>kubectl get pod -owide -nkube-system l grep kube-proxy </code>查看 <em>kube-proxy</em> Pod的状态，问题节点上的 <em>kube-proxy</em> Pod为 <em><strong>running</strong></em> 状态</li>
<li>执行 <code>kubecti logs &lt;nodename&gt; &lt;kube-proxy pod name&gt; -nkube-system</code> 查看问题节点 <em>kube-proxy</em>的Pod日志，没有异常报错</li>
<li>在问题节点操作系统上执行 <code>iptables -S -t nat</code> 查看 <code>iptables </code>规则</li>
</ul>
<p>排查过程：</p>
<p>确认存在到 <em>registry</em> 服务的 Cluster lP <em>10.233.0.100</em> 的 <em>KUBE-SERVICES</em> 链，跳转至 <em>KUBE-SVC-*</em> 链做负载均衡，再跳转至 <em>KUBE-SEP-*</em> 链通过 <em>DNAT</em> 替换为服务后端Pod的IP 10.233.65.46。因此判断iptables规则无异常执行route-n查看问题节点存在访问10.233.65.46所在网段的路由，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821231943753.png" alt="image-20220821231943753" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：10.233.65.46路由</center><br>
<p>查看对端的回程路由</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232111046.png" alt="image-20220821232111046" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：回程路由</center><br>
<p>以上排查证明问题原因不是 <em>cni</em> 插件或者 <em>kube-proxy</em> 异常导致，因此需要在访问链路上抓包，判断问题原因、问题节点执行 <code>curl 10.233.0.100:5000</code>，在问题节点和后端pod所在节点的flannel.1上同时抓包发包节点一直在重传，Cluster lP已 <em>DNAT</em> 转换为后端Pod IP，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821231845672.png" alt="image-20220821231845672" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：抓包过程，发送端</center><br>
<p>后端Pod（ <em>registry</em> 服务）所在节点的 <em>flannel.1</em> 上未抓到任何数据包，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821231730846.png" alt="image-20220821231730846" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：抓包过程，服务端</center><br>
<p>请求 <em>service</em> 的 <em>ClusterlP</em> 时，在两端物理机网卡抓包，发包端如图所示，封装的源端节点IP是10.153.204.15，但一直在重传</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232249344.png" alt="image-20220821232249344" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：包传送过程，发送端</center><br>
<p>收包端收到了包，但未回包，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232338410.png" alt="image-20220821232338410" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：包传送过程，服务端</center><br>
<p>由此可以知道，NAT的动作已经完成，而只是后端Pod（ <em>registry</em> 服务）没有回包，接下来在问题节点执行 <code>curl 10.233.65.46:5000</code>，在问题节点和后端（ <em>registry</em> 服务）Pod所在节点的 <em>flannel.1</em> 上同时抓包，两节点收发正常，发包如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232550959.png" alt="image-20220821232550959" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：正常包发送端</center><br>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232827589.png" alt="image-20220821232827589" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：正常包接收端</center><br>
<p>接下来在两端物理机网卡接口抓包，因为数据包通过物理机网卡会进行 <em>vxlan</em> 封装，需要抓 <em>vxlan</em> 设备的8472端口，发包端如图所示</p>
<p>发现网络链路连通，==但封装的IP不对==，封装的源端节点IP是10.153.204.228，但是存在问题节点的IP是10.153.204.15</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821233107112.png" alt="image-20220821233107112" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：问题节点物理机网卡接口抓包</center><br>
<p>后端Pod所在节点的物理网卡上抓包，注意需要过滤其他正常节点的请求包，如图所示；发现收到的数据包，源地址是10.153.204.228，但是问题节点的IP是10.153.204.15。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821233258211.png" alt="image-20220821233258211" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：对端节点物理机网卡接口抓包</center><br>
<p>此时问题以及清楚了，是一个Pod存在两个IP，导致发包和回包时无法通过隧道设备找到对端的接口，所以发可以收到，但不能回。</p>
<p>问题节点执行<code>ip addr</code>，发现网卡 <em>enp26s0f0</em>上配置了两个IP，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821233642903.png" alt="image-20220821233642903" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：问题节点IP</center><br>
<p>进一步查看网卡配置文件，发现网卡既配置了静态IP，又配置了dhcp动态获取IP。如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821233741211.png" alt="image-20220821233741211" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：问题节点网卡配置</center><br>
<p>最终定位原因为问题节点既配置了dhcp 获取IP，又配置了静态IP，导致IP冲突，引发网络异常</p>
<p>解决方法：修改网卡配置文件 <code>/etc/sysconfig/network-scripts/ifcfg-enp26s0f0</code> 里 <code>BOOTPROTO=&quot;dhcp&quot;</code>
为 <code>BOOTPROTO=&quot;none&quot;</code>；重启 <em>docker</em> 和 <em>kubelet</em> 问题解决。</p>
<h3 id="集群外云主机调用集群内应用超时">集群外云主机调用集群内应用超时</h3>
<p>问题现象：Kubernetes 集群外云主机以 http post 方式访问Kubernetes 集群应用接口超时</p>
<p>环境信息：Kubernetes 集群：calicoIP-IP模式，应用接口以nodeport方式对外提供服务</p>
<p>客户端：Kubernetes 集群之外的云主机</p>
<p>排查过程：</p>
<ul>
<li>
<p>在云主机telnet应用接口地址和端口，可以连通，证明网络连通正常，如图所示</p>
</li>
<li>
<p>云主机上调用接口不通，在云主机和Pod所在 Kubernetes节点同时抓包，使用wireshark分析数据包</p>
</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821234238398.png" alt="image-20220821234238398" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>通过抓包结果分析结果为TCP链接建立没有问题，但是在传输大数据的时候会一直重传 **1514 **大小的第一个数据包直至超时。怀疑是链路两端MTU大小不一致导致（现象：某一个固定大小的包一直超时的情况）。如图所示，1514大小的包一直在重传。</p>
<p>报文1-3 TCP三次握手正常</p>
<p>报文1 info中MSS字段可以看到MSS协商为1460，MTU=1460+20bytes（IP包头）+20bytes（TCP包头）=1500</p>
<p>报文7 k8s主机确认了包4的数据包，但是后续再没有对数据的ACK</p>
<p>报文21-29 可以看到云主机一直在发送后面的数据，但是没有收到k8s节点的ACK，结合pod未收到任何报文，表明是k8s节点和POD通信出现了问题。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821234410926.png" alt="image-20220821234410926" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：wireshark分析</center><br>
<p>在云主机上使用 <code>ping -s</code> 指定数据包大小，发现超过1400大小的数据包无法正常发送。结合以上情况，定位是云主机网卡配置的MTU是1500，<em>tunl0</em> 配置的MTU是1440，导致大数据包无法发送至 <em>tunl0</em> ，因此Pod没有收到报文，接口调用失败。</p>
<p>解决方法：修改云主机网卡MTU值为1440，或者修改calico的MTU值为1500，保持链路两端MTU值一致。</p>
<h3 id="集群pod访问对象存储超时">集群pod访问对象存储超时</h3>
<p>环境信息：公有云环境，Kubernetes 集群节点和对象存储在同一私有网络下，网络链路无防火墙限制k8s集群开启了节点自动弹缩（CA）和Pod自动弹缩（HPA），通过域名访问对象存储，Pod使用集群DNS服务，集群DNS服务配置了用户自建上游DNS服务器</p>
<p>排查过程：</p>
<ul>
<li>
<p>使用nsenter工具进入pod容器网络命名空间测试，ping对象存储域名不通，报错unknown server name，ping对象存储lP可以连通。</p>
</li>
<li>
<p><code>telnet</code> 对象存储80/443端口可以连通。</p>
</li>
<li>
<p><code>paping</code> 对象存储 80/443 端口无丢包。</p>
</li>
<li>
<p>为了验证Pod创建好以后的初始阶段网络连通性，将以上测试动作写入dockerfile，重新生成容器镜像并创pod，测试结果一致。</p>
</li>
</ul>
<p>通过上述步骤，判断Pod网络连通性无异常，超时原因为域名解析失败，怀疑问题如下：</p>
<ul>
<li>集群DNS服务存在异常</li>
<li>上游DNS服务存在异常</li>
<li>集群DNS服务与上游DNS通讯异常</li>
<li>pod访问集群DNS服务异常</li>
</ul>
<p>根据上述方向排查，集群DNS服务状态正常，无报错。测试Pod分别使用集群DNS服务和上游DNS服务解析域名，前者解析失败，后者解析成功。至此，证明上游DNS服务正常，并且集群DNS服务日志中没有与上游DNS通讯超时的报错。定位到的问题：==Pod访问集群DNS服务超时==</p>
<p>此时发现，出现问题的Pod集中在新弹出的 Kubernetes 节点上。这些节点的 <code>kube-proxy</code> Pod状态全部为<em>pending</em>，没有正常调度到节点上。因此导致该节点上其他Pod无法访问包括 dns 在内的所有Kubernetes service。</p>
<p>再进一步排查发现 <code>kube-proxy</code> Pod没有配置priorityclass为最高优先级，导致节点资源紧张时为了将高优先级的应用Pod调度到该节点，将原本已运行在该节点的kube-proxy驱逐。</p>
<p>解决方法：将 <code>kube-proxy</code> 设置 <code>priorityclass</code> 值为 <code>system-node-critical</code> 最高优先级，同时建议应用Pod配置就绪探针，测试可以正常连通对象存储域名后再分配任务。</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://danielmiessler.com/study/tcpdump/#basic-communication" target="_blank"
   rel="noopener nofollow noreferrer" ><em>A tcpdump Tutorial with Examples</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://laramatic.com/how-to-install-nsenter-in-debian-ubuntu-alpine-arch-kali-centos-fedora-raspbian-and-macos/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>How to install nsenter</em></a></p>
<p><sup id="3">[3]</sup> <a href="https://man7.org/linux/man-pages/man1/nsenter.1.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em>man nsenter</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>详述Kubernetes网络模型</title>
      <link>https://www.oomkill.com/2022/08/kubernetes-network-model/</link>
      <pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/08/kubernetes-network-model/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>本文将深入探讨Kubernetes中的网络模型，以及对各种网络模型进行分析。</p>
<h2 id="underlay-network-model">Underlay Network Model</h2>
<h3 id="什么是underlay-network">什么是Underlay Network</h3>
<p>底层网络 <em>Underlay Network</em> 顾名思义是指网络设备基础设施，如交换机，路由器, <em>DWDM</em> 使用网络介质将其链接成的物理网络拓扑，负责网络之间的数据包传输。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/download.png" alt="典型的底层网络" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Underlay network topology</center>
<center><em>Source：</em>https://community.cisco.com/t5/data-center-switches/understanding-underlay-and-overlay-networks/td-p/4295870</center><br>
<p><em>underlay network</em> 可以是二层，也可以是三层；二层 <em>underlay network</em> 的典型例子是以太网 <em>Ethernet</em>，三层是 <em>underlay network</em> 的典型例子是互联网 <em>Internet</em>。</p>
<p>而工作与二层的技术是 <em>vlan</em>，工作在三层的技术是由 <em>OSPF</em>, <em>BGP</em> 等协议组成</p>
<h3 id="kubernetes中的underlay-network">kubernetes中的underlay network</h3>
<p>在kubernetes中，<em>underlay network</em> 是将宿主机作为路由器设备而，Pod 的网络则通过学习成路由条目从而实现跨节点通讯。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220820230021593.png" alt="image-20220820230021593" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：underlay network topology in kubernetes</center><br>
<p>这种模型下典型的有 <em>flannel</em> 的 <em>host-gw</em> 模式与 <em>calico</em> <em>BGP</em> 模式。</p>
<h4 id="flannel-host-gw-supa-href11asup">flannel host-gw <sup><a href="#1">[1]</a></sup></h4>
<p><em>flannel host-gw</em> 模式中每个Node需要在同一个二层网络中，并将Node作为一个路由器，跨节点通讯将通过路由表方式进行，这样方式下将网络模拟成一个<em>underlay network</em>。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/SwitchDiagram1_update_feb16.jpeg" alt="网络交换机图 1" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：layer2 ethernet topology</center>
<center><em>Source：</em>https://www.auvik.com/franklyit/blog/layer-3-switches-layer-2/</center><br>
<blockquote>
<p>Notes：因为是通过路由方式，集群的cidr至少要配置16，因为这样可以保证，跨节点的Node作为一层网络，同节点的Pod作为一个网络。如果不是这种用情况，路由表处于相同的网络中，会存在网络不可达</p>
</blockquote>
<h4 id="calico-bgp-supa-href22asup">Calico BGP <sup><a href="#2">[2]</a></sup></h4>
<p>BGP（<em>Border Gateway Protocol</em>）是去中心化自治路由协议。它是通过维护IP路由表或&rsquo;前缀&rsquo;表来实现AS （<em>Autonomous System</em>）之间的可访问性，属于向量路由协议。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/ti020109.gif" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：BGP network topology</center>
<center><em>Source：</em>https://infocenter.nokia.com/public/7705SAR214R1A/index.jsp?topic=%2Fcom.sar.routing_protocols%</center><br>
<p>与 <em>flannel</em> 不同的是，<em>Calico</em> 提供了的 <em>BGP</em> 网络解决方案，在网络模型上，<em>Calico</em> 与 <em>Flannel host-gw</em> 是近似的，但在软件架构的实现上，<em>flannel</em> 使用 <em>flanneld</em> 进程来维护路由信息；而 <em>Calico</em> 是包含多个守护进程的，其中 <em>Brid</em> 进程是一个 <em>BGP</em> 的客户端 与路由反射器(<em>Router Reflector</em>)，<em>BGP</em> 客户端负责从 <em>Felix</em> 中获取路由并分发到其他 <em>BGP Peer</em>，而反射器在BGP中起了优化的作用。在同一个IBGP中，BGP客户端仅需要和一个 <em>RR</em> 相连，这样减少了<em>AS</em>内部维护的大量的BGP连接。通常情况下，<em>RR</em> 是真实的路由设备，而 <em>Bird</em> 作为 <em>BGP</em> 客户端工作。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/cisco-nx-os-calico-network-design_0.jpeg" alt="相关图片、图表或屏幕截图" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Calico Network Architecture</center>
<center><em>Source：</em>https://www.cisco.com/c/en/us/td/docs/dcn/whitepapers/cisco-nx-os-calico-network-design.html</center><br>
<h4 id="ipvlan--macvlan-supa-href44asup">IPVLAN &amp; MACVLAN <sup><a href="#4">[4]</a></sup></h4>
<p><em>IPVLAN</em> 和 <em>MACVLAN</em> 是一种网卡虚拟化技术，两者之间的区别为， <em>IPVLAN</em> 允许一个物理网卡拥有多个IP地址，并且所有的虚拟接口用同一个MAC地址；而 <em>MACVLAN</em> 则是相反的，其允许同一个网卡拥有多个MAC地址，而虚拟出的网卡可以没有IP地址。</p>
<p>因为是网卡虚拟化技术，而不是网络虚拟化技术，本质上来说属于 <em>Overlay network</em>，这种方式在虚拟化环境中与<em>Overlay network</em> 相比最大的特点就是可以将Pod的网络拉平到Node网络同级，从而提供更高的性能、低延迟的网络接口。本质上来说其网络模型属于下图中第二个。</p>
<ul>
<li>虚拟网桥：创建一个虚拟网卡对(veth pair)，一头栽容器内，一头栽宿主机的root namespaces内。这样一来容器内发出的数据包可以通过网桥直接进入宿主机网络栈，而发往容器的数据包也可以经过网桥进入容器。</li>
<li>多路复用：使用一个中间网络设备，暴露多个虚拟网卡接口，容器网卡都可以介入这个中间设备，并通过MAC/IP地址来区分packet应该发往哪个容器设备。</li>
<li>硬件交换，为每个Pod分配一个虚拟网卡，这样一来，Pod与Pod之间的连接关系就会变得非常清晰，因为近乎物理机之间的通信基础。如今大多数网卡都支持SR-IOV功能，该功能将单一的物理网卡虚拟成多个VF接口，每个VF接口都有单独的虚拟PCIe通道，这些虚拟的PCIe通道共用物理网卡的PCIe通道。</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/7a021d86-virtual-networking-1024x380.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Virtual networking modes: bridging, multiplexing and SR-IOV</center>
<center><em>Source：</em>https://thenewstack.io/hackers-guide-kubernetes-networking/</center><br>
<p>在kubernetes中 <em>IPVLAN</em> 这种网络模型下典型的CNI有，multus 与 danm。</p>
<h5 id="multus">multus</h5>
<p><em>multus</em> 是 intel 开源的CNI方案，是由传统的 <em>cni</em> 与 <em>multus</em>，并且提供了 SR-IOV CNI 插件使 K8s pod 能够连接到 SR-IOV VF 。这是使用了 <em>IPVLAN/MACVLAN</em> 的功能。</p>
<p>当创建新的Pod后，SR-IOV 插件开始工作。配置 VF 将被移动到新的 CNI 名称空间。该插件根据 CNI 配置文件中的 “name” 选项设置接口名称。最后将VF状态设置为UP。</p>
<p>下图是一个 Multus 和 SR-IOV CNI 插件的网络环境，具有三个接口的 pod。</p>
<ul>
<li><em>eth0</em> 是 <em>flannel</em> 网络插件，也是作为Pod的默认网络</li>
<li>VF 是主机的物理端口 <em>ens2f0</em> 的实例化。这是英特尔X710-DA4上的一个端口。 在Pod端的 VF 接口名称为 <em>south0</em> 。</li>
<li>这个VF使用了 DPDK 驱动程序，此 VF 是从主机的物理端口 <em>ens2f1</em> 实例化出的。这个是英特尔® X710-DA4上另外一个端口。 Pod 内的 VF 接口名称为 <em>north0</em>。该接口绑定到 DPDK 驱动程序 <em>vfio-pci</em> 。</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220817154334081.png" alt="image-20220817154334081" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Mutus networking Architecture overlay and SR-IOV</center>
<center><em>Source：</em>https://builders.intel.com/docs/networkbuilders/enabling_new_features_in_kubernetes_for_NFV.pdf</center><br>
<blockquote>
<p><strong>Notes：terminology</strong></p>
<ul>
<li>NIC：network interface card，网卡</li>
<li>SR-IOV：single root I/O virtualization，硬件实现的功能，允许各虚拟机间共享PCIe设备。</li>
<li>VF：Virtual Function，基于PF，与PF或者其他VF共享一个物理资源。</li>
<li>PF：PCIe Physical Function，拥有完全控制PCIe资源的能力</li>
<li>DPDK：Data Plane Development Kit</li>
</ul>
</blockquote>
<p>于此同时，也可以将主机接口直接移动到Pod的网络名称空间，当然这个接口是必须存在，并且不能是与默认网络使用同一个接口。这种情况下，在普通网卡的环境中，就直接将Pod网络与Node网络处于同一个平面内了。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/multus02.png" alt="布鲁吉" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Mutus networking Architecture overlay and ipvlan</center>
<center><em>Source：</em>https://devopstales.github.io/kubernetes/multus/</center><br>
<h5 id="danm">danm</h5>
<p>DANM是诺基亚开源的CNI项目，目的是将电信级网络引入kubernetes中，与multus相同的是，也提供了SR-IOV/DPDK 的硬件技术，并且支持IPVLAN.</p>
<h2 id="overlay-network-model">Overlay Network Model</h2>
<h3 id="什么是overlay">什么是Overlay</h3>
<p>叠加网络是使用网络虚拟化技术，在 <em>underlay</em> 网络上构建出的虚拟逻辑网络，而无需对物理网络架构进行更改。本质上来说，<em>overlay network</em> 使用的是一种或多种隧道协议 (<em>tunneling</em>)，通过将数据包封装，实现一个网络到另一个网络中的传输，具体来说隧道协议关注的是数据包（帧）。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/Example-Overlay-Network-built-on-top-of-an-Internet-style-Underlay.png" alt="图 4：建立在 Internet 样式底层之上的示例 Overlay 网络" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：overlay network topology</center>
<center><em>Source：</em>https://www.researchgate.net/figure/Example-Overlay-Network-built-on-top-of-an-Internet-style-Underlay_fig4_230774628</center><br>
<h3 id="常见的网络隧道技术">常见的网络隧道技术</h3>
<ul>
<li>通用路由封装 ( <em>Generic Routing Encapsulation</em> ) 用于将来自 IPv4/IPv6的数据包封装为另一个协议的数据包中，通常工作与L3网络层中。</li>
<li>VxLAN (<em>Virtual Extensible LAN</em>)，是一个简单的隧道协议，本质上是将L2的以太网帧封装为L4中UDP数据包的方法，使用 <strong>4789</strong> 作为默认端口。<em>VxLAN</em> 也是 <em>VLAN</em> 的扩展对于 4096（$2^{12}$ 位 <em>VLAN ID</em>） 扩展为1600万（$2^{24}$ 位 <em>VNID</em> ）个逻辑网络。</li>
</ul>
<p>这种工作在 <em>overlay</em> 模型下典型的有 <em>flannel</em> 与 <em>calico</em> 中的的 <em>VxLAN</em>, <em>IPIP</em> 模式。</p>
<h3 id="ipip">IPIP</h3>
<p><em>IP in IP</em> 也是一种隧道协议，与 <em>VxLAN</em> 类似的是，<em>IPIP</em> 的实现也是通过Linux内核功能进行的封装。<em>IPIP</em> 需要内核模块 <code>ipip.ko</code>  使用命令查看内核是否加载IPIP模块<code>lsmod | grep ipip</code> ；使用命令<code>modprobe ipip</code> 加载。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/IPIP_Process.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：A simple IPIP network workflow</center>
<center><em>Source：</em>https://ssup2.github.io/theory_analysis/IPIP_GRE_Tunneling/</center><br>
<p>Kubernetes中 <em>IPIP</em> 与 <em>VxLAN</em> 类似，也是通过网络隧道技术实现的。与 <em>VxLAN</em> 差别就是，<em>VxLAN</em> 本质上是一个 UDP包，而 <em>IPIP</em> 则是将包封装在本身的报文包上。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220817163743182.png" alt="image-20220817163743182" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：IPIP in kubernetes</center><br>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220817163814698.png" alt="image-20220817163814698" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：IPIP packet with wireshark unpack</center><br>
<blockquote>
<p>Notes：公有云可能不允许IPIP流量，例如Azure</p>
</blockquote>
<h3 id="vxlan">VxLAN</h3>
<p>kubernetes中不管是 <em>flannel</em> 还是 <em>calico</em> VxLAN的实现都是使用Linux内核功能进行的封装，Linux 对 vxlan 协议的支持时间并不久，2012 年 Stephen Hemminger 才把相关的工作合并到 kernel 中，并最终出现在 kernel 3.7.0 版本。为了稳定性和很多的功能，你可以会看到某些软件推荐在 3.9.0 或者 3.10.0 以后版本的 kernel 上使用 <em>VxLAN</em>。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220815232351298.png" alt="image-20220815232351298" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：A simple VxLAN network topology</center><br>
<p>在kubernetes中vxlan网络，例如 <em>flannel</em>，守护进程会根据kubernetes的Node而维护 <em>VxLAN</em>，名称为 <code>flannel.1</code> 这是 <em>VNID</em>，并维护这个网络的路由，当发生跨节点的流量时，本地会维护对端 <em>VxLAN</em> 设备的MAC地址，通过这个地址可以知道发送的目的端，这样就可以封包发送到对端，收到包的对端 VxLAN设备 <code>flannel.1</code>  解包后得到真实的目的地址。</p>
<p>查看 <em>Forwarding database</em> 列表</p>
<pre><code class="language-bash">$ bridge fdb 
26:5e:87:90:91:fc dev flannel.1 dst 10.0.0.3 self permanent
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220816161418748.png" alt="image-20220816161418748" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：VxLAN in kubernetes</center><br>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220816164301428.png" alt="image-20220816164301428" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：VxLAN packet with wireshark unpack</center><br>
<blockquote>
<p>Notes：VxLAN使用的4789端口，wireshark应该是根据端口进行分析协议的，而flannel在linux中默认端口是8472，此时抓包仅能看到是一个UDP包。</p>
</blockquote>
<p>通过上述的架构可以看出，隧道实际上是一个抽象的概念，并不是建立的真实的两端的隧道，而是通过将数据包封装成另一个数据包，通过物理设备传输后，经由相同的设备（网络隧道）进行解包实现网络的叠加。</p>
<h3 id="weave-vxlan-supa-href33asup">weave vxlan <sup><a href="#3">[3]</a></sup></h3>
<p>weave也是使用了 <em>VxLAN</em> 技术完成的包的封装，这个技术在 <em>weave</em> 中称之为 <em>fastdp (fast data path)</em>，与 <em>calico</em> 和 <em>flannel</em> 中用到的技术不同的，这里使用的是 Linux 内核中的 <em>openvswitch datapath module</em>。与其他的 <em>VxLAN</em> 模型不同的是，weave对网络流量进行了加密。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/weave-net-fdp1-1024x454.png" alt="Weave Net Encapsulation" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：weave fastdp network topology</center>
<center><em>Source：</em>https://www.weave.works/docs/net/latest/concepts/fastdp-how-it-works/</center><br>
<blockquote>
<p>Notes：fastdp工作在Linux 内核版本 3.12 及更高版本，如果低于此版本的例如CentOS7，weave将工作在用户空间，weave中称之为 <em>sleeve mode</em></p>
</blockquote>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#host-gw" target="_blank"
   rel="noopener nofollow noreferrer" >flannel host-gw</a></p>
<p><sup id="2">[2]</sup> <a href="https://projectcalico.docs.tigera.io/networking/bgp" target="_blank"
   rel="noopener nofollow noreferrer" >calico bgp networking</a></p>
<p><sup id="3">[3]</sup> <a href="https://www.weave.works/docs/net/latest/concepts/router-encapsulation/" target="_blank"
   rel="noopener nofollow noreferrer" >calico bgp networking</a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin" target="_blank"
   rel="noopener nofollow noreferrer" >sriov network</a></p>
<p><sup id="4">[5]</sup> <a href="https://github.com/nokia/danm" target="_blank"
   rel="noopener nofollow noreferrer" >danm</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>基于Prometheus的Kubernetes网络调度器</title>
      <link>https://www.oomkill.com/2022/08/ch22-custom-scheduler/</link>
      <pubDate>Mon, 08 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/08/ch22-custom-scheduler/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>本文将深入讲解 如何扩展 Kubernetes scheduler 中各个扩展点如何使用，与扩展scheduler的原理，这些是作为扩展 <em>scheduler</em> 的所需的知识点。最后会完成一个实验，基于网络流量的调度器。</p>
<h2 id="kubernetes调度配置">kubernetes调度配置</h2>
<p>kubernetes集群中允许运行多个不同的 <em>scheduler</em>  ，也可以为Pod指定不同的调度器进行调度。在一般的Kubernetes调度教程中并没有提到这点，这也就是说，对于亲和性，污点等策略实际上并没有完全的使用kubernetes调度功能，在之前的文章中提到的一些调度插件，如基于端口占用的调度 <code>NodePorts</code> 等策略一般情况下是没有使用到的，本章节就是对这部分内容进行讲解，这也是作为扩展调度器的一个基础。</p>
<h3 id="scheduler-configuration-supa-href11asup">Scheduler Configuration <sup><a href="#1">[1]</a></sup></h3>
<p><em>kube-scheduler</em> 提供了配置文件的资源，作为给 <em>kube-scheduler</em> 的配置文件，启动时通过 <code>--onfig=</code> 来指定文件。目前各个kubernetes版本中使用的 <code>KubeSchedulerConfiguration</code> 为，</p>
<ul>
<li>1.21 之前版本使用 <code>v1beta1</code></li>
<li>1.22 版本使用 <code>v1beta2</code> ，但保留了 <code>v1beta1</code></li>
<li>1.23, 1.24, 1.25 版本使用 <code>v1beta3</code> ，但保留了  <code>v1beta2</code>，删除了 <code>v1beta1</code></li>
</ul>
<p>下面是一个简单的 <em>kubeSchedulerConfiguration</em> 示例，其中 <code>kubeconfig</code> 与启动参数 <code>--kubeconfig</code> 是相同的功效。而 <em>kubeSchedulerConfiguration</em> 与其他组件的配置文件类似，如 <em>kubeletConfiguration</em> 都是作为服务启动的配置文件。</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: /etc/srv/kubernetes/kube-scheduler/kubeconfig
</code></pre>
<blockquote>
<p>Notes: <code>--kubeconfig</code> 与 <code>--config</code> 是不可以同时指定的，指定了 <code>--config</code> 则其他参数自然失效 <sup><a href="#2">[2]</a></sup></p>
</blockquote>
<h3 id="kubeschedulerconfiguration使用">kubeSchedulerConfiguration使用</h3>
<p>通过配置文件，用户可以自定义多个调度器，以及配置每个阶段的扩展点。而插件就是通过这些扩展点来提供在整个调度上下文中的调度行为。</p>
<p>下面配置是对于配置扩展点的部分的一个示例，关于扩展点的讲解可以参考kubernetes官方文档调度上下文部分</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
profiles:
  - plugins:
      score:
        disabled:
        - name: PodTopologySpread
        enabled:
        - name: MyCustomPluginA
          weight: 2
        - name: MyCustomPluginB
          weight: 1
</code></pre>
<blockquote>
<p>Notes: 如果name=&quot;*&quot; 的话，这种情况下将禁用/启用对应扩展点的所有插件</p>
</blockquote>
<p>既然kubernetes提供了多调度器，那么对于配置文件来说自然支持多个配置文件，profile也是列表形式，只要指定多个配置列表即可，下面是多配置文件示例，其中，如果存在多个扩展点，也可以为每个调度器配置多个扩展点。</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: default-scheduler
  	plugins:
      preScore:
        disabled:
        - name: '*'
      score:
        disabled:
        - name: '*'
  - schedulerName: no-scoring-scheduler
    plugins:
      preScore:
        disabled:
        - name: '*'
      score:
        disabled:
        - name: '*'
</code></pre>
<h3 id="scheduler调度插件-supa-href33asup">scheduler调度插件 <sup><a href="#3">[3]</a></sup></h3>
<p><em>kube-scheduler</em> 默认提供了很多插件作为调度方法，默认不配置的情况下会启用这些插件，如：</p>
<ul>
<li><em><strong>ImageLocality</strong></em>：调度将更偏向于Node存在容器镜像的节点。扩展点：<code>score</code>.</li>
<li><em><strong>TaintToleration</strong></em>：实现污点与容忍度功能。扩展点：<code>filter</code>, <code>preScore</code>, <code>score</code>.</li>
<li><em><strong>NodeName</strong></em>：实现调度策略中最简单的调度方法 <code>NodeName</code> 的实现。扩展点：<code>filter</code>.</li>
<li><em><strong>NodePorts</strong></em>：调度将检查Node端口是否已占用。扩展点：<code>preFilter</code>, <code>filter</code>.</li>
<li><em><strong>NodeAffinity</strong></em>：提供节点亲和性相关功能。扩展点：<code>filter</code>, <code>score</code>.</li>
<li><em><strong>PodTopologySpread</strong></em>：实现Pod拓扑域的功能。扩展点：<code>preFilter</code>, <code>filter</code>, <code>preScore</code>, <code>score</code>.</li>
<li><em><strong>NodeResourcesFit</strong></em>：该插件将检查节点是否拥有 Pod 请求的所有资源。使用以下三种策略之一：<code>LeastAllocated</code> （默认）<code>MostAllocated</code> 和 <code>RequestedToCapacityRatio</code>。扩展点：<code>preFilter</code>, <code>filter</code>, <code>score</code>.</li>
<li><em><strong>VolumeBinding</strong></em>：检查节点是否有或是否可以绑定请求的 <a href="https://kubernetes.io/docs/concepts/storage/volumes/" target="_blank"
   rel="noopener nofollow noreferrer" >卷</a>. 扩展点：<code>preFilter</code>, <code>filter</code>, <code>reserve</code>, <code>preBind</code>, <code>score</code>.</li>
<li><em><strong>VolumeRestrictions</strong></em>：检查安装在节点中的卷是否满足特定于卷提供程序的限制。扩展点：<code>filter</code>.</li>
<li><em><strong>VolumeZone</strong></em>：检查请求的卷是否满足它们可能具有的任何区域要求。扩展点：<code>filter</code>.</li>
<li><em><strong>InterPodAffinity</strong></em>： 实现Pod 间的亲和性与反亲和性的功能。扩展点：<code>preFilter</code>, <code>filter</code>, <code>preScore</code>, <code>score</code>.</li>
<li><em><strong>PrioritySort</strong></em>：提供基于默认优先级的排序。扩展点：<code>queueSort</code>.</li>
</ul>
<p>对于更多配置文件使用案例可以参考官方给出的文档</p>
<h2 id="如何扩展kube-scheduler-supa-href44asup">如何扩展kube-scheduler <sup><a href="#4">[4]</a></sup></h2>
<p>当在第一次考虑编写调度程序时，通常会认为扩展 <em>kube-scheduler</em> 是一件非常困难的事情，其实这些事情 kubernetes 官方早就想到了，kubernetes为此在 1.15 版本引入了framework的概念，framework旨在使 <em>scheduler</em> 更具有扩展性。</p>
<p><em>framework</em> 通过重新定义 各扩展点，将其作为 <em>plugins</em> 来使用，并且支持用户注册 <code>out of tree</code> 的扩展，使其可以被注册到 <em>kube-scheduler</em> 中，下面将对这些步骤进行分析。</p>
<h3 id="定义入口">定义入口</h3>
<p><em>scheduler</em> 允许进行自定义，但是对于只需要引用对应的 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/cmd/kube-scheduler/app/server.go#L64-L117" target="_blank"
   rel="noopener nofollow noreferrer" >NewSchedulerCommand</a>，并且实现自己的 <em>plugins</em> 的逻辑即可。</p>
<pre><code class="language-go">import (
    scheduler &quot;k8s.io/kubernetes/cmd/kube-scheduler/app&quot;
)

func main() {
    command := scheduler.NewSchedulerCommand(
            scheduler.WithPlugin(&quot;example-plugin1&quot;, ExamplePlugin1),
            scheduler.WithPlugin(&quot;example-plugin2&quot;, ExamplePlugin2))
    if err := command.Execute(); err != nil {
        fmt.Fprintf(os.Stderr, &quot;%v\n&quot;, err)
        os.Exit(1)
    }
}
</code></pre>
<p>而 <strong>NewSchedulerCommand</strong> 允许注入 out of tree plugins，也就是注入外部的自定义 plugins，这种情况下就无需通过修改源码方式去定义一个调度器，而仅仅通过自行实现即可完成一个自定义调度器。</p>
<pre><code class="language-go">// WithPlugin 用于注入out of tree plugins 因此scheduler代码中没有其引用。
func WithPlugin(name string, factory runtime.PluginFactory) Option {
	return func(registry runtime.Registry) error {
		return registry.Register(name, factory)
	}
}
</code></pre>
<h3 id="插件实现">插件实现</h3>
<p>对于插件的实现仅仅需要实现对应的扩展点接口。下面通过内置插件进行分析</p>
<p>对于内置插件 <code>NodeAffinity</code> ,我们通过观察他的结构可以发现，实现插件就是实现对应的扩展点抽象 <em>interface</em> 即可。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220807212221684.png" alt="image-20220807212221684" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="定义插件结构体">定义插件结构体</h4>
<p>其中 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/framework/v1alpha1/interface.go#L495-L524" target="_blank"
   rel="noopener nofollow noreferrer" >framework.FrameworkHandle</a> 是提供了Kubernetes API与 <em>scheduler</em> 之间调用使用的，通过结构可以看出包含 lister，informer等等，这个参数也是必须要实现的。</p>
<pre><code class="language-go">type NodeAffinity struct {
	handle framework.FrameworkHandle
}
</code></pre>
<h4 id="实现对应的扩展点">实现对应的扩展点</h4>
<pre><code class="language-go">func (pl *NodeAffinity) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	nodeInfo, err := pl.handle.SnapshotSharedLister().NodeInfos().Get(nodeName)
	if err != nil {
		return 0, framework.NewStatus(framework.Error, fmt.Sprintf(&quot;getting node %q from Snapshot: %v&quot;, nodeName, err))
	}

	node := nodeInfo.Node()
	if node == nil {
		return 0, framework.NewStatus(framework.Error, fmt.Sprintf(&quot;getting node %q from Snapshot: %v&quot;, nodeName, err))
	}

	affinity := pod.Spec.Affinity

	var count int64
	// A nil element of PreferredDuringSchedulingIgnoredDuringExecution matches no objects.
	// An element of PreferredDuringSchedulingIgnoredDuringExecution that refers to an
	// empty PreferredSchedulingTerm matches all objects.
	if affinity != nil &amp;&amp; affinity.NodeAffinity != nil &amp;&amp; affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution != nil {
		// Match PreferredDuringSchedulingIgnoredDuringExecution term by term.
		for i := range affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution {
			preferredSchedulingTerm := &amp;affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution[i]
			if preferredSchedulingTerm.Weight == 0 {
				continue
			}

			// TODO: Avoid computing it for all nodes if this becomes a performance problem.
			nodeSelector, err := v1helper.NodeSelectorRequirementsAsSelector(preferredSchedulingTerm.Preference.MatchExpressions)
			if err != nil {
				return 0, framework.NewStatus(framework.Error, err.Error())
			}

			if nodeSelector.Matches(labels.Set(node.Labels)) {
				count += int64(preferredSchedulingTerm.Weight)
			}
		}
	}

	return count, nil
}
</code></pre>
<p>最后在通过实现一个 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/framework/plugins/nodeaffinity/node_affinity.go#L116-L118" target="_blank"
   rel="noopener nofollow noreferrer" >New</a> 函数来提供注册这个扩展的方法。通过这个 <em>New</em> 函数可以在 <code>main.go</code> 中将其作为 out of tree plugins 注入到 <em>scheduler</em> 中即可</p>
<pre><code class="language-go">// New initializes a new plugin and returns it.
func New(_ runtime.Object, h framework.FrameworkHandle) (framework.Plugin, error) {
	return &amp;NodeAffinity{handle: h}, nil
}
</code></pre>
<h2 id="实验基于网络流量的调度-supa-href77asup">实验：基于网络流量的调度 <sup><a href="#7">[7]</a></sup></h2>
<p>通过上面阅读了解到了如何扩展 <em>scheduler</em> 插件，下面实验将完成一个基于流量的调度，通常情况下，网络一个Node在一段时间内使用的网络流量也是作为生产环境中很常见的情况。例如在配置均衡的多个主机中，主机A作为业务拉单脚本运行，主机B作为计算服务运行。通常来说计算服务会使用更多的系统资源，而拉单需要更多的是网络流量，此时在调度时，默认调度器有限选择的是系统空闲资源多的节点，这种情况下如果有Pod被调度到该节点上，那么可能双方业务都会收到影响（前端代理觉得这个节点连接数少会被大量调度，而拉单脚本因为网络带宽的占用降低了效能）。</p>
<h3 id="实验环境">实验环境</h3>
<ul>
<li>一个kubernetes集群，至少保证有两个节点。</li>
<li>提供的kubernetes集群都需要安装prometheus node_exporter，可以是集群内部的，也可以是集群外部的，这里使用的是集群外部的。</li>
<li>对 <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/" target="_blank"
   rel="noopener nofollow noreferrer" >promQL</a> 与 <a href="https://github.com/prometheus/client_golang" target="_blank"
   rel="noopener nofollow noreferrer" >client_golang</a> 有所了解</li>
</ul>
<p><strong>实验大致分为以下几个步骤</strong>：</p>
<ul>
<li>定义插件API
<ul>
<li>插件命名为 <code>NetworkTraffic</code></li>
</ul>
</li>
<li>定义扩展点
<ul>
<li>这里使用了 Score 扩展点，并且定义评分的算法</li>
</ul>
</li>
<li>定义分数获取途径（从prometheus指标中拿到对应的数据）</li>
<li>定义对自定义调度器的参数传入</li>
<li>将项目部署到集群中（集群内部署与集群外部署）</li>
<li>实验的结果验证</li>
</ul>
<p>实验将仿照内置插件 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/framework/plugins/nodeaffinity/node_affinity.go" target="_blank"
   rel="noopener nofollow noreferrer" >nodeaffinity</a> 完成代码编写，为什么选择这个插件，只是因为这个插件相对比较简单，并且与我们实验目的基本相同，其实其他插件也是同样的效果。</p>
<p>整个实验的代码上传至 github.com/CylonChau/customScheduler</p>
<h3 id="实验开始">实验开始</h3>
<h4 id="错误处理">错误处理</h4>
<p>在初始化项目时，<code>go mod tidy</code> 等操作时，会遇到大量下面的错误</p>
<pre><code class="language-bash">go: github.com/GoogleCloudPlatform/spark-on-k8s-operator@v0.0.0-20210307184338-1947244ce5f4 requires
        k8s.io/apiextensions-apiserver@v0.0.0: reading k8s.io/apiextensions-apiserver/go.mod at revision v0.0.0: unknown revision v0.0.0
</code></pre>
<p>kubernetes issue #79384 <sup><a href="#5">[5]</a></sup> 中有提到这个问题，粗略浏览下没有说明为什么会出现这个问题，在最下方有个大佬提供了一个脚本，出现上述问题无法解决时直接运行该脚本后正常。</p>
<pre><code class="language-bash">#!/bin/sh
set -euo pipefail

VERSION=${1#&quot;v&quot;}
if [ -z &quot;$VERSION&quot; ]; then
    echo &quot;Must specify version!&quot;
    exit 1
fi
MODS=($(
    curl -sS https://raw.githubusercontent.com/kubernetes/kubernetes/v${VERSION}/go.mod |
    sed -n 's|.*k8s.io/\(.*\) =&gt; ./staging/src/k8s.io/.*|k8s.io/\1|p'
))
for MOD in &quot;${MODS[@]}&quot;; do
    V=$(
        go mod download -json &quot;${MOD}@kubernetes-${VERSION}&quot; |
        sed -n 's|.*&quot;Version&quot;: &quot;\(.*\)&quot;.*|\1|p'
    )
    go mod edit &quot;-replace=${MOD}=${MOD}@${V}&quot;
done
go get &quot;k8s.io/kubernetes@v${VERSION}&quot;
</code></pre>
<h4 id="定义插件api">定义插件API</h4>
<p>通过上面内容描述了解到了定义插件只需要实现对应的扩展点抽象 <em>interface</em> ，那么可以初始化项目目录 <code>pkg/networtraffic/networktraffice.go</code>。</p>
<p>定义插件名称与变量</p>
<pre><code class="language-go">const Name = &quot;NetworkTraffic&quot;
var _ = framework.ScorePlugin(&amp;NetworkTraffic{})
</code></pre>
<p>定义插件的结构体</p>
<pre><code class="language-go">type NetworkTraffic struct {
    // 这个作为后面获取node网络流量使用
	prometheus *PrometheusHandle
	// FrameworkHandle 提供插件可以使用的数据和一些工具。
	// 它在插件初始化时传递给 plugin 工厂类。
	// plugin 必须存储和使用这个handle来调用framework函数。
	handle framework.FrameworkHandle
}
</code></pre>
<h4 id="定义扩展点">定义扩展点</h4>
<p>因为选用 Score 扩展点，需要定义对应的方法，来实现对应的抽象</p>
<pre><code class="language-go">func (n *NetworkTraffic) Score(ctx context.Context, state *framework.CycleState, p *corev1.Pod, nodeName string) (int64, *framework.Status) {
    // 通过promethes拿到一段时间的node的网络使用情况
	nodeBandwidth, err := n.prometheus.GetGauge(nodeName)
	if err != nil {
		return 0, framework.NewStatus(framework.Error, fmt.Sprintf(&quot;error getting node bandwidth measure: %s&quot;, err))
	}
	bandWidth := int64(nodeBandwidth.Value)
	klog.Infof(&quot;[NetworkTraffic] node '%s' bandwidth: %s&quot;, nodeName, bandWidth)
	return bandWidth, nil // 这里直接返回就行
}
</code></pre>
<p>接下来需要对结果归一化，这里就回到了调度框架中扩展点的执行问题上了，通过源码可以看出，Score 扩展点需要实现的并不只是这单一的方法。</p>
<pre><code class="language-go">// Run NormalizeScore method for each ScorePlugin in parallel.
parallelize.Until(ctx, len(f.scorePlugins), func(index int) {
    pl := f.scorePlugins[index]
    nodeScoreList := pluginToNodeScores[pl.Name()]
    if pl.ScoreExtensions() == nil {
        return
    }
    status := f.runScoreExtension(ctx, pl, state, pod, nodeScoreList)
    if !status.IsSuccess() {
        err := fmt.Errorf(&quot;normalize score plugin %q failed with error %v&quot;, pl.Name(), status.Message())
        errCh.SendErrorWithCancel(err, cancel)
        return
    }
})
</code></pre>
<p>通过上面代码了解到，实现 <code>Score </code> 就必须实现 <code>ScoreExtensions</code>，如果没有实现则直接返回。而根据 <code>nodeaffinity</code> 中示例发现这个方法仅仅返回的是这个扩展点对象本身，而具体的归一化也就是真正进行打分的操作在 <code>NormalizeScore</code> 中。</p>
<pre><code class="language-go">// NormalizeScore invoked after scoring all nodes.
func (pl *NodeAffinity) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
	return pluginhelper.DefaultNormalizeScore(framework.MaxNodeScore, false, scores)
}

// ScoreExtensions of the Score plugin.
func (pl *NodeAffinity) ScoreExtensions() framework.ScoreExtensions {
	return pl
}
</code></pre>
<p>而在 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/framework/runtime/framework.go#L692-L700" target="_blank"
   rel="noopener nofollow noreferrer" >framework</a> 中，真正执行的操作的方法也是 <code>NormalizeScore()</code></p>
<pre><code class="language-go">func (f *frameworkImpl) runScoreExtension(ctx context.Context, pl framework.ScorePlugin, state *framework.CycleState, pod *v1.Pod, nodeScoreList framework.NodeScoreList) *framework.Status {
	if !state.ShouldRecordPluginMetrics() {
		return pl.ScoreExtensions().NormalizeScore(ctx, state, pod, nodeScoreList)
	}
	startTime := time.Now()
	status := pl.ScoreExtensions().NormalizeScore(ctx, state, pod, nodeScoreList)
	f.metricsRecorder.observePluginDurationAsync(scoreExtensionNormalize, pl.Name(), status, metrics.SinceInSeconds(startTime))
	return status
}
</code></pre>
<p>下面来实现对应的方法</p>
<p>在 <em>NormalizeScore</em> 中需要实现具体的选择node的算法，因为对node打分结果的区间为 $[0,100]$ ，所以这里实现的算法公式将为 $最高分 - (当前带宽 / 最高最高带宽 * 100)$，这样就保证了，带宽占用越大的机器，分数越低。</p>
<p>例如，最高带宽为200000，而当前Node带宽为140000，那么这个Node分数为：$max - \frac{140000}{200000}\times 100 = 100 - (0.7\times100)=30$</p>
<pre><code class="language-go">// 如果返回framework.ScoreExtensions 就需要实现framework.ScoreExtensions
func (n *NetworkTraffic) ScoreExtensions() framework.ScoreExtensions {
	return n
}

// NormalizeScore与ScoreExtensions是固定格式
func (n *NetworkTraffic) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *corev1.Pod, scores framework.NodeScoreList) *framework.Status {
	var higherScore int64
	for _, node := range scores {
		if higherScore &lt; node.Score {
			higherScore = node.Score
		}
	}
	// 计算公式为，满分 - (当前带宽 / 最高最高带宽 * 100)
	// 公式的计算结果为，带宽占用越大的机器，分数越低
	for i, node := range scores {
		scores[i].Score = framework.MaxNodeScore - (node.Score * 100 / higherScore)
		klog.Infof(&quot;[NetworkTraffic] Nodes final score: %v&quot;, scores)
	}

	klog.Infof(&quot;[NetworkTraffic] Nodes final score: %v&quot;, scores)
	return nil
}
</code></pre>
<blockquote>
<p>Notes：在kubernetes中最大的node数支持5000个，岂不是在获取最大分数时循环就占用了大量的性能，其实不必担心。<em>scheduler</em> 提供了一个参数 <code>percentageOfNodesToScore</code>。这个参数决定了这里要循环的数量。更多的细节可以参考官方文档对这部分的说明 <sup><a href="#6">[6]</a></sup></p>
</blockquote>
<p><strong>配置插件名称</strong></p>
<p>为了使插件注册时候使用，还需要为其配置一个名称</p>
<pre><code class="language-go">// Name returns name of the plugin. It is used in logs, etc.
func (n *NetworkTraffic) Name() string {
	return Name
}
</code></pre>
<h4 id="定义prometheushandle">定义PrometheusHandle</h4>
<p>网络插件的扩展中还存在一个 <code>prometheusHandle</code>，这个就是操作prometheus-server拿去指标的动作。</p>
<p>首先需要定义一个 <em>PrometheusHandle</em> 的结构体</p>
<pre><code class="language-go">type PrometheusHandle struct {
	deviceName string // 网络接口名称
	timeRange  time.Duration // 抓取的时间段
	ip         string // prometheus server的连接地址
	client     v1.API // 操作prometheus的客户端
}
</code></pre>
<p>有了结构就需要查询的动作和指标，对于指标来说，这里使用了 <code>node_network_receive_bytes_total</code> 作为获取Node的网络流量的计算方式。由于环境是部署在集群之外的，没有node的主机名，通过 <code>promQL</code> 获取，整个语句如下：</p>
<pre><code class="language-bash">sum_over_time(node_network_receive_bytes_total{device=&quot;eth0&quot;}[1s]) * on(instance) group_left(nodename) (node_uname_info{nodename=&quot;node01&quot;})
</code></pre>
<p>整个 <em>Prometheus</em> 部分如下：</p>
<pre><code class="language-go">type PrometheusHandle struct {
	deviceName string
	timeRange  time.Duration
	ip         string
	client     v1.API
}

func NewProme(ip, deviceName string, timeRace time.Duration) *PrometheusHandle {
	client, err := api.NewClient(api.Config{Address: ip})
	if err != nil {
		klog.Fatalf(&quot;[NetworkTraffic] FatalError creating prometheus client: %s&quot;, err.Error())
	}
	return &amp;PrometheusHandle{
		deviceName: deviceName,
		ip:         ip,
		timeRange:  timeRace,
		client:     v1.NewAPI(client),
	}
}

func (p *PrometheusHandle) GetGauge(node string) (*model.Sample, error) {
	value, err := p.query(fmt.Sprintf(nodeMeasureQueryTemplate, node, p.deviceName, p.timeRange))
	fmt.Println(fmt.Sprintf(nodeMeasureQueryTemplate, p.deviceName, p.timeRange, node))
	if err != nil {
		return nil, fmt.Errorf(&quot;[NetworkTraffic] Error querying prometheus: %w&quot;, err)
	}

	nodeMeasure := value.(model.Vector)
	if len(nodeMeasure) != 1 {
		return nil, fmt.Errorf(&quot;[NetworkTraffic] Invalid response, expected 1 value, got %d&quot;, len(nodeMeasure))
	}
	return nodeMeasure[0], nil
}

func (p *PrometheusHandle) query(promQL string) (model.Value, error) {
    // 通过promQL查询并返回结果
	results, warnings, err := p.client.Query(context.Background(), promQL, time.Now())
	if len(warnings) &gt; 0 {
		klog.Warningf(&quot;[NetworkTraffic Plugin] Warnings: %v\n&quot;, warnings)
	}

	return results, err
}
</code></pre>
<h4 id="定义调度器传入的参数">定义调度器传入的参数</h4>
<p>因为需要指定 <em>prometheus</em> 的地址，网卡名称，和获取数据的大小，故整个结构体如下，另外，参数结构必须遵循<code>&lt;Plugin Name&gt;Args</code> 格式的名称。</p>
<pre><code class="language-go">type NetworkTrafficArgs struct {
	IP         string `json:&quot;ip&quot;`
	DeviceName string `json:&quot;deviceName&quot;`
	TimeRange  int    `json:&quot;timeRange&quot;`
}
</code></pre>
<p>为了使这个类型的数据作为 <code>KubeSchedulerConfiguration</code> 可以解析的结构，还需要做一步操作，就是在扩展APIServer时扩展对应的资源类型。在这里kubernetes中提供两种方法来扩展 <code>KubeSchedulerConfiguration</code> 的资源类型。</p>
<p>一种是旧版中提供了 <a href="https://github.com/kubernetes/kubernetes/blob/7a98bb2b7c9112935387825f2fce1b7d40b76236/pkg/scheduler/framework/plugins/nodelabel/node_label.go#L65-L80" target="_blank"
   rel="noopener nofollow noreferrer" >framework.DecodeInto</a> 函数可以做这个操作</p>
<pre><code class="language-go">func New(plArgs *runtime.Unknown, handle framework.FrameworkHandle) (framework.Plugin, error) {
	args := Args{}
	if err := framework.DecodeInto(plArgs, &amp;args); err != nil {
		return nil, err
	}
	...
}
</code></pre>
<p>另外一种方式是必须实现对应的深拷贝方法，例如 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/apis/config/types_pluginargs.go#L37-L49" target="_blank"
   rel="noopener nofollow noreferrer" >NodeLabel</a> 中的</p>
<pre><code class="language-go">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// NodeLabelArgs holds arguments used to configure the NodeLabel plugin.
type NodeLabelArgs struct {
	metav1.TypeMeta

	// PresentLabels should be present for the node to be considered a fit for hosting the pod
	PresentLabels []string
	// AbsentLabels should be absent for the node to be considered a fit for hosting the pod
	AbsentLabels []string
	// Nodes that have labels in the list will get a higher score.
	PresentLabelsPreference []string
	// Nodes that don't have labels in the list will get a higher score.
	AbsentLabelsPreference []string
}
</code></pre>
<p>最后将其注册到register中，整个行为与扩展APIServer是类似的</p>
<pre><code class="language-go">// addKnownTypes registers known types to the given scheme
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&amp;KubeSchedulerConfiguration{},
		&amp;Policy{},
		&amp;InterPodAffinityArgs{},
		&amp;NodeLabelArgs{},
		&amp;NodeResourcesFitArgs{},
		&amp;PodTopologySpreadArgs{},
		&amp;RequestedToCapacityRatioArgs{},
		&amp;ServiceAffinityArgs{},
		&amp;VolumeBindingArgs{},
		&amp;NodeResourcesLeastAllocatedArgs{},
		&amp;NodeResourcesMostAllocatedArgs{},
	)
	scheme.AddKnownTypes(schema.GroupVersion{Group: &quot;&quot;, Version: runtime.APIVersionInternal}, &amp;Policy{})
	return nil
}
</code></pre>
<blockquote>
<p>Notes：对于生成深拷贝函数及其他文件，可以使用 kubernetes 代码库中的脚本 <a href="https://github.com/kubernetes/kubernetes/blob/v1.24.3/hack/update-codegen.sh" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/hack/update-codegen.sh</a></p>
</blockquote>
<p>这里为了方便使用了 <em>framework.DecodeInto</em> 的方式。</p>
<h4 id="项目部署">项目部署</h4>
<p>准备 scheduler 的 profile，可以看到，我们自定义的参数，就可以被识别为 <em>KubeSchedulerConfiguration</em> 的资源类型了。</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: /mnt/d/src/go_work/customScheduler/scheduler.conf
profiles:
- schedulerName: custom-scheduler
  plugins:
    score:
      enabled:
      - name: &quot;NetworkTraffic&quot;
      disabled:
      - name: &quot;*&quot;
  pluginConfig:
    - name: &quot;NetworkTraffic&quot;
      args:
        ip: &quot;http://10.0.0.4:9090&quot;
        deviceName: &quot;eth0&quot;
        timeRange: 60
</code></pre>
<p>如果需要部署到集群内部，可以打包成镜像</p>
<pre><code class="language-dockerfile">FROM golang:alpine AS builder
MAINTAINER cylon
WORKDIR /scheduler
COPY ./ /scheduler
ENV GOPROXY https://goproxy.cn,direct
RUN \
    sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories &amp;&amp; \
    apk add upx  &amp;&amp; \
    GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -ldflags &quot;-s -w&quot; -o scheduler main.go &amp;&amp; \
    upx -1 scheduler &amp;&amp; \
    chmod +x scheduler

FROM alpine AS runner
WORKDIR /go/scheduler
COPY --from=builder /scheduler/scheduler .
COPY --from=builder /scheduler/scheduler.yaml /etc/
VOLUME [&quot;./scheduler&quot;]
</code></pre>
<p>部署在集群内部所需的资源清单</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: scheduler-sa
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: scheduler
subjects:
  - kind: ServiceAccount
    name: scheduler-sa
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-scheduler
  namespace: kube-system
  labels:
    component: custom-scheduler
spec:
  selector:
    matchLabels:
      component: custom-scheduler
  template:
    metadata:
      labels:
        component: custom-scheduler
    spec:
      serviceAccountName: scheduler-sa
      priorityClassName: system-cluster-critical
      containers:
        - name: scheduler
          image: cylonchau/custom-scheduler:v0.0.1
          imagePullPolicy: IfNotPresent
          command:
            - ./scheduler
            - --config=/etc/scheduler.yaml
            - --v=3
          livenessProbe:
            httpGet:
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
          readinessProbe:
            httpGet:
              path: /healthz
              port: 10251
</code></pre>
<p>启动自定义 <em>scheduler</em>，这里通过简单的二进制方式启动，所以需要一个kubeconfig做认证文件</p>
<pre><code class="language-bash">./main --logtostderr=true \
	--address=127.0.0.1 \
	--v=3 \
	--config=`pwd`/scheduler.yaml \
	--kubeconfig=`pwd`/scheduler.conf
</code></pre>
<p>启动后为了验证方便性，关闭了原来的 <em>kube-scheduler</em> 服务，因为原来的  <em>kube-scheduler</em> 已经作为HA中的master，所以不会使用自定义的 <em>scheduler</em> 导致pod pending。</p>
<h4 id="验证结果">验证结果</h4>
<p>准备一个需要部署的Pod，指定使用的调度器名称</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
      schedulerName: custom-scheduler
</code></pre>
<p>这里实验环境为2个节点的kubernetes集群，master与node01，因为master的服务比node01要多，这种情况下不管怎样，调度结果永远会被调度到node01上。</p>
<pre><code class="language-bash">$ kubectl get pods -o wide
NAME                                READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx-deployment-69f76b454c-lpwbl   1/1     Running   0          43s   192.168.0.17   node01   &lt;none&gt;           &lt;none&gt;
nginx-deployment-69f76b454c-vsb7k   1/1     Running   0          43s   192.168.0.16   node01   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>而调度器的日志如下</p>
<pre><code class="language-log">I0808 01:56:31.098189   27131 networktraffic.go:83] [NetworkTraffic] node 'node01' bandwidth: %!s(int64=12541068340)
I0808 01:56:31.098461   27131 networktraffic.go:70] [NetworkTraffic] Nodes final score: [{master-machine 0} {node01 12541068340}]
I0808 01:56:31.098651   27131 networktraffic.go:70] [NetworkTraffic] Nodes final score: [{master-machine 0} {node01 71}]
I0808 01:56:31.098911   27131 networktraffic.go:73] [NetworkTraffic] Nodes final score: [{master-machine 0} {node01 71}]
I0808 01:56:31.099275   27131 default_binder.go:51] Attempting to bind default/nginx-deployment-69f76b454c-vsb7k to node01
I0808 01:56:31.101414   27131 eventhandlers.go:225] add event for scheduled pod default/nginx-deployment-69f76b454c-lpwbl
I0808 01:56:31.101414   27131 eventhandlers.go:205] delete event for unscheduled pod default/nginx-deployment-69f76b454c-lpwbl
I0808 01:56:31.103604   27131 scheduler.go:609] &quot;Successfully bound pod to node&quot; pod=&quot;default/nginx-deployment-69f76b454c-lpwbl&quot; node=&quot;no
de01&quot; evaluatedNodes=2 feasibleNodes=2
I0808 01:56:31.104540   27131 scheduler.go:609] &quot;Successfully bound pod to node&quot; pod=&quot;default/nginx-deployment-69f76b454c-vsb7k&quot; node=&quot;no
de01&quot; evaluatedNodes=2 feasibleNodes=2
</code></pre>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/reference/scheduling/config/" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling config</a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank"
   rel="noopener nofollow noreferrer" >kube-scheduler</a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling-plugins</a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md#custom-scheduler-plugins-out-of-tree" target="_blank"
   rel="noopener nofollow noreferrer" >custom scheduler plugins</a></p>
<p><sup id="5">[5]</sup> <a href="https://github.com/kubernetes/kubernetes/issues/79384" target="_blank"
   rel="noopener nofollow noreferrer" >ssues #79384</a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/scheduler-perf-tuning/" target="_blank"
   rel="noopener nofollow noreferrer" >scheduler perf tuning</a></p>
<p><sup id="7">[7]</sup> <a href="https://medium.com/@juliorenner123/k8s-creating-a-kube-scheduler-plugin-8a826c486a1" target="_blank"
   rel="noopener nofollow noreferrer" >creating a kube-scheduler plugin</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>如何理解kubernetes调度框架与插件？</title>
      <link>https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/</link>
      <pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="调度框架-supa-href11asup">调度框架 <sup><a href="#1">[1]</a></sup></h2>
<p>本文基于 <a href="https://github.com/kubernetes/kubernetes/tree/release-1.24/pkg/scheduler" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes 1.24</a> 进行分析</p>
<p>调度框架（<code>Scheduling Framework</code>）是Kubernetes 的调度器 <code>kube-scheduler</code> 设计的的可插拔架构，将插件（调度算法）嵌入到调度上下文的每个扩展点中，并编译为 <code>kube-scheduler</code></p>
<p>在 <code>kube-scheduler</code> 1.22 之后，在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/interface.go" target="_blank"
   rel="noopener nofollow noreferrer" >pkg/scheduler/framework/interface.go</a> 中定义了一个 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/interface.go#L295-L297" target="_blank"
   rel="noopener nofollow noreferrer" >Plugin</a> 的 <em>interface</em>，这个 <em>interface</em> 作为了所有插件的父级。而每个未调度的 Pod，Kubernetes 调度器会根据一组规则尝试在集群中寻找一个节点。</p>
<pre><code class="language-go">type Plugin interface {
	Name() string
}
</code></pre>
<p>下面会对每个算法是如何实现的进行分析</p>
<p>在初始化 <em>scheduler</em> 时，会创建一个 <code>profile</code>，profile是关于 <em>scheduler</em> 调度配置相关的定义</p>
<pre><code class="language-go">func New(client clientset.Interface,
...
	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)
	if err != nil {
		return nil, fmt.Errorf(&quot;initializing profiles: %v&quot;, err)
	}

	if len(profiles) == 0 {
		return nil, errors.New(&quot;at least one profile is required&quot;)
	}
....
}
</code></pre>
<p>关于 <code>profile</code> 的实现，则为 <code>KubeSchedulerProfile</code>，也是作为 yaml生成时传入的配置</p>
<pre><code class="language-go">// KubeSchedulerProfile 是一个 scheduling profile.
type KubeSchedulerProfile struct {
	// SchedulerName 是与此配置文件关联的调度程序的名称。
    // 如果 SchedulerName 与 pod “spec.schedulerName”匹配，则使用此配置文件调度 pod。
	SchedulerName string

	// Plugins指定应该启用或禁用的插件集。
    // 启用的插件是除了默认插件之外应该启用的插件。禁用插件应是禁用的任何默认插件。
    // 当没有为扩展点指定启用或禁用插件时，将使用该扩展点的默认插件（如果有）。
    // 如果指定了 QueueSort 插件，
    // 则必须为所有配置文件指定相同的 QueueSort Plugin 和 PluginConfig。
    // 这个Plugins展现的形式则是调度上下文中的所有扩展点(这是抽象)，实际中会表现为多个扩展点
	Plugins *Plugins

	// PluginConfig 是每个插件的一组可选的自定义插件参数。
    // 如果省略PluginConfig参数等同于使用该插件的默认配置。
	PluginConfig []PluginConfig
}
</code></pre>
<p>对于 <code>profile.NewMap</code> 就是根据给定的配置来构建这个framework，因为配置可能是存在多个的。而 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/registry.go#L70" target="_blank"
   rel="noopener nofollow noreferrer" >Registry</a> 则是所有可用插件的集合，内部构造则是 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/registry.go#L30" target="_blank"
   rel="noopener nofollow noreferrer" >PluginFactory</a> ,通过函数来构建出对应的 plugin</p>
<pre><code class="language-go">func NewMap(cfgs []config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory,
	stopCh &lt;-chan struct{}, opts ...frameworkruntime.Option) (Map, error) {
	m := make(Map)
	v := cfgValidator{m: m}

	for _, cfg := range cfgs {
		p, err := newProfile(cfg, r, recorderFact, stopCh, opts...)
		if err != nil {
			return nil, fmt.Errorf(&quot;creating profile for scheduler name %s: %v&quot;, cfg.SchedulerName, err)
		}
		if err := v.validate(cfg, p); err != nil {
			return nil, err
		}
		m[cfg.SchedulerName] = p
	}
	return m, nil
}

// newProfile 给的配置构建出一个profile
func newProfile(cfg config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory,
	stopCh &lt;-chan struct{}, opts ...frameworkruntime.Option) (framework.Framework, error) {
	recorder := recorderFact(cfg.SchedulerName)
	opts = append(opts, frameworkruntime.WithEventRecorder(recorder))
	return frameworkruntime.NewFramework(r, &amp;cfg, stopCh, opts...)
}

</code></pre>
<p>可以看到最终返回的是一个 <code>Framework</code> 。那么来看下这个 <code>Framework</code></p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/interface.go#L495-L566" target="_blank"
   rel="noopener nofollow noreferrer" >Framework</a> 是一个抽象，管理着调度过程中所使用的所有插件，并在调度上下文中适当的位置去运行对应的插件</p>
<pre><code class="language-go">type Framework interface {
	Handle
	// QueueSortFunc 返回对调度队列中的 Pod 进行排序的函数
    // 也就是less，在Sort打分阶段的打分函数
	QueueSortFunc() LessFunc
    
    // RunPreFilterPlugins 运行配置的一组PreFilter插件。
    // 如果这组插件中，任何一个插件失败，则返回 *Status 并设置为non-success。
    // 如果返回状态为non-success，则调度周期中止。
    // 它还返回一个 PreFilterResult，它可能会影响到要评估下游的节点。
    
	RunPreFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod) (*PreFilterResult, *Status)

    // RunPostFilterPlugins 运行配置的一组PostFilter插件。 
    // PostFilter 插件是通知性插件，在这种情况下应配置为先执行并返回 Unschedulable 状态，
    // 或者尝试更改集群状态以使 pod 在未来的调度周期中可能会被调度。
	RunPostFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, filteredNodeStatusMap NodeToStatusMap) (*PostFilterResult, *Status)

    // RunPreBindPlugins 运行配置的一组 PreBind 插件。
    // 如果任何一个插件返回错误，则返回 *Status 并且code设置为non-success。
    // 如果code为“Unschedulable”，则调度检查失败，
    // 则认为是内部错误。在任何一种情况下，Pod都不会被bound。
	RunPreBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

    // RunPostBindPlugins 运行配置的一组PostBind插件
	RunPostBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string)

    // RunReservePluginsReserve运行配置的一组Reserve插件的Reserve方法。
    // 如果在这组调用中的任何一个插件返回错误，则不会继续运行剩余调用的插件并返回错误。
    // 在这种情况下，pod将不能被调度。
	RunReservePluginsReserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

    // RunReservePluginsUnreserve运行配置的一组Reserve插件的Unreserve方法。
	RunReservePluginsUnreserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string)

    // RunPermitPlugins运行配置的一组Permit插件。
    // 如果这些插件中的任何一个返回“Success”或“Wait”之外的状态，则它不会继续运行其余插件并返回错误。
    // 否则，如果任何插件返回 “Wait”，则此函数将创建等待pod并将其添加到当前等待pod的map中，
    // 并使用“Wait” code返回状态。 Pod将在Permit插件返回的最短持续时间内保持等待pod。
	RunPermitPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

    // 如果pod是waiting pod，WaitOnPermit 将阻塞，直到等待的pod被允许或拒绝。
	WaitOnPermit(ctx context.Context, pod *v1.Pod) *Status

    // RunBindPlugins运行配置的一组bind插件。 Bind插件可以选择是否处理Pod。
    // 如果 Bind 插件选择跳过binding，它应该返回 code=5(&quot;skip&quot;)状态。
    // 否则，它应该返回“Error”或“Success”。
    // 如果没有插件处理绑定，则RunBindPlugins返回code=5(&quot;skip&quot;)的状态。
	RunBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

	// 如果至少定义了一个filter插件，则HasFilterPlugins返回true
	HasFilterPlugins() bool

    // 如果至少定义了一个PostFilter插件，则HasPostFilterPlugins返回 true。
	HasPostFilterPlugins() bool

	// 如果至少定义了一个Score插件，则HasScorePlugins返回 true。
	HasScorePlugins() bool

    // ListPlugins将返回map。key为扩展点名称，value则是配置的插件列表。
	ListPlugins() *config.Plugins

    // ProfileName则是与profile name关联的framework
	ProfileName() string
}
</code></pre>
<p>而实现这个抽象的则是 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/framework.go#L73-L102" target="_blank"
   rel="noopener nofollow noreferrer" >frameworkImpl</a>；<code>frameworkImpl</code> 是初始化与运行 <em>scheduler plugins</em> 的组件，并在调度上下文中会运行这些扩展点</p>
<pre><code>type frameworkImpl struct {
   registry             Registry
   snapshotSharedLister framework.SharedLister
   waitingPods          *waitingPodsMap
   scorePluginWeight    map[string]int
   queueSortPlugins     []framework.QueueSortPlugin
   preFilterPlugins     []framework.PreFilterPlugin
   filterPlugins        []framework.FilterPlugin
   postFilterPlugins    []framework.PostFilterPlugin
   preScorePlugins      []framework.PreScorePlugin
   scorePlugins         []framework.ScorePlugin
   reservePlugins       []framework.ReservePlugin
   preBindPlugins       []framework.PreBindPlugin
   bindPlugins          []framework.BindPlugin
   postBindPlugins      []framework.PostBindPlugin
   permitPlugins        []framework.PermitPlugin

   clientSet       clientset.Interface
   kubeConfig      *restclient.Config
   eventRecorder   events.EventRecorder
   informerFactory informers.SharedInformerFactory

   metricsRecorder *metricsRecorder
   profileName     string

   extenders []framework.Extender
   framework.PodNominator

   parallelizer parallelize.Parallelizer
}
</code></pre>
<p>那么来看下 Registry ，<code>Registry </code> 是作为一个可用插件的集合。<code>framework</code> 使用 <code>registry</code> 来启用和对插件配置的初始化。在初始化框架之前，所有插件都必须在注册表中。表现形式就是一个 <code>map[]</code>；<em>key</em> 是插件的名称，value是 <code>PluginFactory</code> 。</p>
<pre><code class="language-go">type Registry map[string]PluginFactory
</code></pre>
<p>而在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/plugins/registry.go" target="_blank"
   rel="noopener nofollow noreferrer" >pkg\scheduler\framework\plugins\registry.go</a> 中会将所有的 <code>in-tree plugin</code> 注册进来。通过 <code>NewInTreeRegistry</code> 。后续如果还有插件要注册，可以通过 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/scheduler.go#L176-L180" target="_blank"
   rel="noopener nofollow noreferrer" >WithFrameworkOutOfTreeRegistry</a>  来注册其他的插件。</p>
<pre><code class="language-go">func NewInTreeRegistry() runtime.Registry {
	fts := plfeature.Features{
		EnableReadWriteOncePod:                       feature.DefaultFeatureGate.Enabled(features.ReadWriteOncePod),
		EnableVolumeCapacityPriority:                 feature.DefaultFeatureGate.Enabled(features.VolumeCapacityPriority),
		EnableMinDomainsInPodTopologySpread:          feature.DefaultFeatureGate.Enabled(features.MinDomainsInPodTopologySpread),
		EnableNodeInclusionPolicyInPodTopologySpread: feature.DefaultFeatureGate.Enabled(features.NodeInclusionPolicyInPodTopologySpread),
	}

	return runtime.Registry{
		selectorspread.Name:                  selectorspread.New,
		imagelocality.Name:                   imagelocality.New,
		tainttoleration.Name:                 tainttoleration.New,
		nodename.Name:                        nodename.New,
		nodeports.Name:                       nodeports.New,
		nodeaffinity.Name:                    nodeaffinity.New,
		podtopologyspread.Name:               runtime.FactoryAdapter(fts, podtopologyspread.New),
		nodeunschedulable.Name:               nodeunschedulable.New,
		noderesources.Name:                   runtime.FactoryAdapter(fts, noderesources.NewFit),
		noderesources.BalancedAllocationName: runtime.FactoryAdapter(fts, noderesources.NewBalancedAllocation),
		volumebinding.Name:                   runtime.FactoryAdapter(fts, volumebinding.New),
		volumerestrictions.Name:              runtime.FactoryAdapter(fts, volumerestrictions.New),
		volumezone.Name:                      volumezone.New,
		nodevolumelimits.CSIName:             runtime.FactoryAdapter(fts, nodevolumelimits.NewCSI),
		nodevolumelimits.EBSName:             runtime.FactoryAdapter(fts, nodevolumelimits.NewEBS),
		nodevolumelimits.GCEPDName:           runtime.FactoryAdapter(fts, nodevolumelimits.NewGCEPD),
		nodevolumelimits.AzureDiskName:       runtime.FactoryAdapter(fts, nodevolumelimits.NewAzureDisk),
		nodevolumelimits.CinderName:          runtime.FactoryAdapter(fts, nodevolumelimits.NewCinder),
		interpodaffinity.Name:                interpodaffinity.New,
		queuesort.Name:                       queuesort.New,
		defaultbinder.Name:                   defaultbinder.New,
		defaultpreemption.Name:               runtime.FactoryAdapter(fts, defaultpreemption.New),
	}
}
</code></pre>
<blockquote>
<p>这里插入一个题外话，关于 <em>in-tree plugin</em></p>
<p>在这里没有找到关于，<em>kube-scheduler</em> ，只是找到有关的概念，大概可以解释为，in-tree表示为随kubernetes官方提供的二进制构建的 <em>plugin</em> 则为 <code>in-tree</code>，而独立于kubernetes代码库之外的为 <code>out-of-tree</code> <sup><a href="#3">[3]</a></sup> 。这种情况下，可以理解为，AA则是 <code>out-of-tree</code> 而 <code>Pod</code>, <code>DeplymentSet</code> 等是 <code>in-tree</code>。</p>
</blockquote>
<p>接下来回到初始化 <em>scheduler</em> ，在初始化一个 <em>scheduler</em> 时，会通过<code>NewInTreeRegistry</code> 来初始化</p>
<pre><code class="language-go">func New(client clientset.Interface,
	....
	registry := frameworkplugins.NewInTreeRegistry()
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}
         
	...

	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)
	...
}
</code></pre>
<p>接下来在调度上下文 <code>scheduleOne</code> 中 <code>schedulePod</code> 时，会通过 <code>framework</code> 调用对应的插件来处理这个扩展点工作。具体的体现在，pkg\scheduler\schedule_one.go 中的预选阶段</p>
<pre><code class="language-go">func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New(&quot;Scheduling&quot;, utiltrace.Field{Key: &quot;namespace&quot;, Value: pod.Namespace}, utiltrace.Field{Key: &quot;name&quot;, Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)

	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
		return result, err
	}
	trace.Step(&quot;Snapshotting scheduler cache and node infos done&quot;)

	if sched.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}

	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step(&quot;Computing predicates done&quot;)

</code></pre>
<p>与其他扩展点部分，在调度上下文 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L66-L278" target="_blank"
   rel="noopener nofollow noreferrer" >scheduleOne</a> 中可以很好的看出，功能都是 <code>framework</code> 提供的。</p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {

    ...
    
	scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)

    ...
    
	// Run the Reserve method of reserve plugins.
	if sts := fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {
	}

    ...
    
	// Run &quot;permit&quot; plugins.
	runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
	
		// One of the plugins returned status different than success or wait.
		fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)

...
    
	// bind the pod to its host asynchronously (we can do this b/c of the assumption step above).
	go func() {
		...
		waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod)
		if !waitOnPermitStatus.IsSuccess() {
			...
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		}

		// Run &quot;prebind&quot; plugins.
		preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		
        ...
        
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
	
        ...

		...
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			
        ...

		// Run &quot;postbind&quot; plugins.
		fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)

	...
}
</code></pre>
<h2 id="插件-supa-href44asup">插件 <sup><a href="#4">[4]</a></sup></h2>
<p>插件（<code>Plugins</code>）（也可以算是调度策略）在 <code>kube-scheduler</code> 中的实现为 <code>framework plugin</code>，插件API的实现分为两个步骤**：register** 和 <strong>configured</strong>，然后都实现了其父方法 <code>Plugin</code>。然后可以通过配置（kube-scheduler <code>--config</code> 提供）启动或禁用插件；除了默认插件外，还可以实现自定义调度插件与默认插件进行绑定。</p>
<pre><code class="language-go">type Plugin interface {
    Name() string
}
// sort扩展点
type QueueSortPlugin interface {
    Plugin
    Less(*v1.pod, *v1.pod) bool
}
// PreFilter扩展点
type PreFilterPlugin interface {
    Plugin
    PreFilter(context.Context, *framework.CycleState, *v1.pod) error
}

</code></pre>
<h3 id="插件的载入过程">插件的载入过程</h3>
<p>在 <em>scheduler</em> 被启动时，会 <code>scheduler.New(cc.Client..</code> 这个时候会传入 <code>profiles</code>，整个的流如下：</p>
<ul>
<li><code>NewScheduler</code> ：<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/cmd/kube-scheduler/app/server.go#L327-L346" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/cmd/kube-scheduler/app/server.go</a></li>
<li><code>profile.NewMap</code>：<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/profile/profile.go#L48-L64" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/pkg/scheduler/scheduler.go</a>
<ul>
<li><code>newProfile</code>：<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/profile/profile.go#L37-L42" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/pkg/scheduler/scheduler.go</a></li>
</ul>
</li>
<li><code>frameworkruntime.NewFramework</code>：<a href="kubernetes/pkg/scheduler/framework/runtime/framework.go">kubernetes/pkg/scheduler/framework/runtime/framework.go</a>
<ul>
<li><code>pluginsNeeded</code>：<a href="kubernetes/pkg/scheduler/framework/runtime/framework.go">kubernetes/pkg/scheduler/framework/runtime/framework.go</a></li>
</ul>
</li>
</ul>
<h4 id="newscheduler">NewScheduler</h4>
<p>我们了解如何 New 一个 <em>scheduler</em> 即为 <code>Setup</code> 中去配置这些参数，</p>
<pre><code class="language-go">func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) {

    ...
    
	// Create the scheduler.
	sched, err := scheduler.New(cc.Client,
		cc.InformerFactory,
		cc.DynInformerFactory,
		recorderFactory,
		ctx.Done(),
		scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion),
		scheduler.WithKubeConfig(cc.KubeConfig),
		scheduler.WithProfiles(cc.ComponentConfig.Profiles...),
		scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore),
		scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry),
		scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds),
		scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds),
		scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration),
		scheduler.WithExtenders(cc.ComponentConfig.Extenders...),
		scheduler.WithParallelism(cc.ComponentConfig.Parallelism),
		scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) {
			// Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging
			completedProfiles = append(completedProfiles, profile)
		}),
	)
    ...
}
</code></pre>
<h4 id="profilenewmap">profile.NewMap</h4>
<p>在 <code>scheduler.New</code> 中，会根据配置生成profile，而 <code>profile.NewMap</code> 会完成这一步</p>
<pre><code class="language-go">func New(client clientset.Interface,
	...
         
	clusterEventMap := make(map[framework.ClusterEvent]sets.String)

	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)

         ...
}
</code></pre>
<h4 id="newframework">NewFramework</h4>
<p><code>newProfile</code> 返回的则是一个创建好的 framework</p>
<pre><code class="language-go">func newProfile(cfg config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory,
	stopCh &lt;-chan struct{}, opts ...frameworkruntime.Option) (framework.Framework, error) {
	recorder := recorderFact(cfg.SchedulerName)
	opts = append(opts, frameworkruntime.WithEventRecorder(recorder))
	return frameworkruntime.NewFramework(r, &amp;cfg, stopCh, opts...)
}
</code></pre>
<p>最终会走到 <code>pluginsNeeded</code>，这里会根据配置中开启的插件而返回一个插件集，这个就是最终在每个扩展点中药执行的插件。</p>
<pre><code class="language-go">func (f *frameworkImpl) pluginsNeeded(plugins *config.Plugins) sets.String {
	pgSet := sets.String{}

	if plugins == nil {
		return pgSet
	}

	find := func(pgs *config.PluginSet) {
		for _, pg := range pgs.Enabled {
			pgSet.Insert(pg.Name)
		}
	}
	// 获取到所有的扩展点，找到为Enabled的插件加入到pgSet
	for _, e := range f.getExtensionPoints(plugins) {
		find(e.plugins)
	}
	// Parse MultiPoint separately since they are not returned by f.getExtensionPoints()
	find(&amp;plugins.MultiPoint)

	return pgSet
}
</code></pre>
<h3 id="插件的执行">插件的执行</h3>
<p>在对插件源码部分分析，会找几个典型的插件进行分析，而不会对全部的进行分析，因为总的来说是大同小异，分析的插件有 <code>NodePorts</code>，<code>NodeResourcesFit</code>，<code>podtopologyspread</code></p>
<h4 id="nodeports">NodePorts</h4>
<p>这里以一个简单的插件来分析；<code>NodePorts</code> 插件用于检查Pod请求的端口，在节点上是否为空闲端口。</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L30" target="_blank"
   rel="noopener nofollow noreferrer" >NodePorts</a> 实现了 <code>FilterPlugin</code> 和 <code>PreFilterPlugin</code></p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L77-L81" target="_blank"
   rel="noopener nofollow noreferrer" >PreFilter</a>  将会被 <code>framework</code> 中 <code>PreFilter</code> 扩展点被调用。</p>
<pre><code class="language-go">func (pl *NodePorts) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	s := getContainerPorts(pod) // 或得Pod得端口
    // 写入状态
	cycleState.Write(preFilterStateKey, preFilterState(s))
	return nil, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L113-L125" target="_blank"
   rel="noopener nofollow noreferrer" >Filter</a> 将会被 <code>framework</code> 中 <code>Filter</code> 扩展点被调用。</p>
<pre><code class="language-go">// Filter invoked at the filter extension point.
func (pl *NodePorts) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
   wantPorts, err := getPreFilterState(cycleState)
   if err != nil {
      return framework.AsStatus(err)
   }

   fits := fitsPorts(wantPorts, nodeInfo)
   if !fits {
      return framework.NewStatus(framework.Unschedulable, ErrReason)
   }

   return nil
}

func fitsPorts(wantPorts []*v1.ContainerPort, nodeInfo *framework.NodeInfo) bool {
	// 对比existingPorts 和 wantPorts是否冲突，冲突则调度失败
	existingPorts := nodeInfo.UsedPorts
	for _, cp := range wantPorts {
		if existingPorts.CheckConflict(cp.HostIP, string(cp.Protocol), cp.HostPort) {
			return false
		}
	}
	return true
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L144-L146" target="_blank"
   rel="noopener nofollow noreferrer" >New</a> ，初始化新插件，在 <code>register</code> 中注册得</p>
<pre><code class="language-go">func New(_ runtime.Object, _ framework.Handle) (framework.Plugin, error) {
	return &amp;NodePorts{}, nil
}
</code></pre>
<p>在调用中，如果有任何一个插件返回错误，则跳过该扩展点注册得其他插件，返回失败。</p>
<pre><code class="language-go">func (f *frameworkImpl) RunFilterPlugins(
	ctx context.Context,
	state *framework.CycleState,
	pod *v1.Pod,
	nodeInfo *framework.NodeInfo,
) framework.PluginToStatus {
	statuses := make(framework.PluginToStatus)
	for _, pl := range f.filterPlugins {
		pluginStatus := f.runFilterPlugin(ctx, pl, state, pod, nodeInfo)
		if !pluginStatus.IsSuccess() {
			if !pluginStatus.IsUnschedulable() 
				errStatus := framework.AsStatus(fmt.Errorf(&quot;running %q filter plugin: %w&quot;, pl.Name(), pluginStatus.AsError())).WithFailedPlugin(pl.Name())
				return map[string]*framework.Status{pl.Name(): errStatus}
			}
			pluginStatus.SetFailedPlugin(pl.Name())
			statuses[pl.Name()] = pluginStatus
		}
	}

	return statuses
}
</code></pre>
<p>返回得状态是一个 Status 结构体，该结构体表示了插件运行的结果。由 <code>Code</code>、<code>reasons</code>、（可选）<code>err</code> 和 <code>failedPlugin</code> （失败的那个插件名）组成。当 <em>code</em> 不是 <code>Success</code> 时，应说明原因。而且，当 <em>code</em> 为 <code>Success</code> 时，其他所有字段都应为空。<code>nil</code> 状态也被视为成功。</p>
<pre><code class="language-go">type Status struct {
	code    Code
	reasons []string
	err     error
	// failedPlugin is an optional field that records the plugin name a Pod failed by.
	// It's set by the framework when code is Error, Unschedulable or UnschedulableAndUnresolvable.
	failedPlugin string
}
</code></pre>
<h4 id="noderesourcesfit--supa-href55asup">NodeResourcesFit  <sup><a href="#5">[5]</a></sup></h4>
<p><code>NodeResourcesFit</code> 扩展检查节点是否拥有 Pod 请求的所有资源。分数可以使用以下三种策略之一，扩展点为：<code>preFilter</code>， <code>filter</code>，<code>score</code></p>
<ul>
<li><code>LeastAllocated</code> （默认）</li>
<li><code>MostAllocated</code></li>
<li><code>RequestedToCapacityRatio</code></li>
</ul>
<h4 id="fit">Fit</h4>
<p><code>NodeResourcesFit  </code> PreFilter 可以看到调用得 <code>computePodResourceRequest</code></p>
<pre><code>// PreFilter invoked at the prefilter extension point.
func (f *Fit) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
   cycleState.Write(preFilterStateKey, computePodResourceRequest(pod))
   return nil, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/noderesources/fit.go#L133-L175" target="_blank"
   rel="noopener nofollow noreferrer" >computePodResourceRequest</a> 这里有一个注释，总体解释起来是这样得：<code>computePodResourceRequest</code> ，返回值（ <code>framework.Resource</code>）覆盖了每一个维度中资源的最大宽度。因为将按照 <code>init-containers</code> , <code>containers</code> 得顺序运行，会通过迭代方式收集每个维度中的最大值。计算时会对常规容器的资源向量求和，因为<code>containers</code> 运行会同时运行多个容器。计算示例为：</p>
<pre><code class="language-yaml">Pod:
  InitContainers
    IC1:
      CPU: 2
      Memory: 1G
    IC2:
      CPU: 2
      Memory: 3G
  Containers
    C1:
      CPU: 2
      Memory: 1G
    C2:
      CPU: 1
      Memory: 1G
</code></pre>
<p>在维度1中（<code>InitContainers</code>）所需资源最大值时，CPU=2, Memory=3G；而维度2（<code>Containers</code>）所需资源最大值为：CPU=2, Memory=1G；那么最终结果为 CPU=3, Memory=3G，因为在维度1，最大资源时Memory=3G；而维度2最大资源是CPU=1+2, Memory=1+1，取每个维度中最大资源最大宽度即为 CPU=3, Memory=3G。</p>
<p>下面则看下代码得实现</p>
<pre><code class="language-go">func computePodResourceRequest(pod *v1.Pod) *preFilterState {
	result := &amp;preFilterState{}
	for _, container := range pod.Spec.Containers {
		result.Add(container.Resources.Requests)
	}

	// 取最大得资源
	for _, container := range pod.Spec.InitContainers {
		result.SetMaxResource(container.Resources.Requests)
	}

	// 如果Overhead正在使用，需要将其计算到总资源中
	if pod.Spec.Overhead != nil {
		result.Add(pod.Spec.Overhead)
	}
	return result
}

// SetMaxResource 是比较ResourceList并为每个资源取最大值。
func (r *Resource) SetMaxResource(rl v1.ResourceList) {
	if r == nil {
		return
	}

	for rName, rQuantity := range rl {
		switch rName {
		case v1.ResourceMemory:
			r.Memory = max(r.Memory, rQuantity.Value())
		case v1.ResourceCPU:
			r.MilliCPU = max(r.MilliCPU, rQuantity.MilliValue())
		case v1.ResourceEphemeralStorage:
			if utilfeature.DefaultFeatureGate.Enabled(features.LocalStorageCapacityIsolation) {
				r.EphemeralStorage = max(r.EphemeralStorage, rQuantity.Value())
			}
		default:
			if schedutil.IsScalarResourceName(rName) {
				r.SetScalar(rName, max(r.ScalarResources[rName], rQuantity.Value()))
			}
		}
	}
}
</code></pre>
<h4 id="leastallocate">leastAllocate</h4>
<p>LeastAllocated 是 NodeResourcesFit 的打分策略 ，<code>LeastAllocated</code> 打分的标准是更偏向于请求资源较少的Node。将会先计算出Node上调度的pod请求的内存、CPU与其他资源的百分比，然后并根据请求的比例与容量的平均值的最小值进行优先级排序。</p>
<p>计算公式是这样的：$\frac{\frac{cpu((capacity-requested) \times MaxNodeScore \times cpuWeight)}{capacity} + \frac{memory((capacity-requested) \times MaxNodeScore \times memoryWeight}{capacity}) + &hellip;}{weightSum}$</p>
<p>下面来看下实现</p>
<pre><code class="language-go">func leastResourceScorer(resToWeightMap resourceToWeightMap) func(resourceToValueMap, resourceToValueMap) int64 {
	return func(requested, allocable resourceToValueMap) int64 {
		var nodeScore, weightSum int64
		for resource := range requested {
			weight := resToWeightMap[resource]
            //  计算出的资源分数乘weight
			resourceScore := leastRequestedScore(requested[resource], allocable[resource])
			nodeScore += resourceScore * weight
			weightSum += weight
		}
		if weightSum == 0 {
			return 0
		}
        // 最终除weightSum
		return nodeScore / weightSum
	}
}
</code></pre>
<p>leastRequestedScore 计算标准为<strong>未使用容量</strong>的计算范围为 <code>0~MaxNodeScore</code>，0 为最低优先级，<code>MaxNodeScore</code> 为最高优先级。未使用的资源越多，得分越高。</p>
<pre><code class="language-go">func leastRequestedScore(requested, capacity int64) int64 {
	if capacity == 0 {
		return 0
	}
	if requested &gt; capacity {
		return 0
	}
	// 容量 - 请求的 x 预期值（100）/ 容量
	return ((capacity - requested) * int64(framework.MaxNodeScore)) / capacity
}
</code></pre>
<h3 id="topology-supa-href66asup">Topology <sup><a href="#6">[6]</a></sup></h3>
<h4 id="concept">Concept</h4>
<p>在对 <code>podtopologyspread</code> 插件进行分析前，先需要掌握Pod拓扑的概念。</p>
<p>Pod拓扑（<code>Pod Topology</code>）是Kubernetes Pod调度机制，可以将Pod分布在集群中不同 <code>Zone</code> ，以及用户自定义的各种拓扑域 （<code>topology domains</code>）。当有了拓扑域后，用户可以更高效的利用集群资源。</p>
<p>如何来解释拓扑域，首先需要提及为什么需要拓扑域，在集群有3个节点，并且当Pod副本数为2时，又不希望两个Pod在同一个Node上运行。在随着扩大Pod的规模，副本数扩展到到15个时，这时候最理想的方式是每个Node运行5个Pod，在这种背景下，用户希望对集群中Zone的安排为相似的副本数量，并且在集群存在部分问题时可以更好的自愈（也是按照相似的副本数量均匀的分布在Node上）。在这种情况下Kubernetes 提供了Pod 拓扑约束来解决这个问题。</p>
<h4 id="定义一个topology">定义一个Topology</h4>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  # Configure a topology spread constraint
  topologySpreadConstraints:
    - maxSkew: &lt;integer&gt; # 
      minDomains: &lt;integer&gt; # optional; alpha since v1.24
      topologyKey: &lt;string&gt;
      whenUnsatisfiable: &lt;string&gt;
      labelSelector: &lt;object&gt;
</code></pre>
<p><strong>参数的描述</strong>：</p>
<ul>
<li><strong>maxSkew</strong>：Required，Pod分布不均的程度，并且数字必须大于零
<ul>
<li>当 <code>whenUnsatisfiable: DoNotSchedule</code>，则定义目标拓扑中匹配 pod 的数量与 <strong>全局最小值</strong>（<em>拓扑域中的标签选择器匹配的 pod 的最小数量</em> ）<code>maxSkew</code>之间的最大允许差异。例如有 3 个 <code>Zone</code>，分别具有 2、4 和 5 个匹配的 pod，则全局最小值为 2</li>
<li>当 <code>whenUnsatisfiable: ScheduleAnyway</code>，<em>scheduler</em> 会为减少倾斜的拓扑提供更高的优先级。</li>
</ul>
</li>
<li><strong>minDomains</strong>：optional，符合条件的域的最小数量。
<ul>
<li>如果不指定该选项 <code>minDomains</code>，则约束的行为 <code>minDomains: 1</code> 。</li>
<li><code>minDomains</code>必须大于 0。<code>minDomains</code>与 <code>whenUnsatisfiable</code> 一起时为<code>whenUnsatisfiable: DoNotSchedule</code>。</li>
</ul>
</li>
<li><strong>topologyKey</strong>：Node label的key，如果多个Node都使用了这个lable key那么 <em>scheduler</em> 将这些 Node 看作为相同的拓扑域。</li>
<li><strong>whenUnsatisfiable</strong>：当 Pod 不满足分布的约束时，怎么去处理
<ul>
<li><code>DoNotSchedule</code>（默认）不要调度。</li>
<li><code>ScheduleAnyway</code>仍然调度它，同时优先考虑最小化倾斜节点</li>
</ul>
</li>
<li><strong>labelSelector</strong>：查找匹配的 Pod label选择器的node进行技术，以计算Pod如何分布在拓扑域中</li>
</ul>
<h4 id="对于拓扑域的理解">对于拓扑域的理解</h4>
<p>对于拓扑域，官方是这么说明的，假设有一个带有以下lable的 4 节点集群：</p>
<pre><code class="language-bash">NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre>
<p>那么集群拓扑如图：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725223516451.png" alt="image-20220725223516451" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图1：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>假设一个 4 节点集群，其中 3个label被标记为<code>foo: bar</code>的 Pod 分别位于Node1、Node2 和 Node3：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725224602667.png" alt="image-20220725224602667" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图2：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>这种情况下，新部署一个Pod，并希望新Pod与现有Pod跨 <code>Zone</code>均匀分布，资源清单文件如下：</p>
<pre><code class="language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1
</code></pre>
<p>这个清单对于拓扑域来说，<code>topologyKey: zone</code> 表示对Pod均匀分布仅应用于已标记的节点（如 <code>foo: bar</code>），将会跳过没有标签的节点（如<code>zone: &lt;any value&gt;</code>）。如果 <em>scheduler</em> 找不到满足约束的方法，<code>whenUnsatisfiable: DoNotSchedule</code> 设置的策略则是 <em>scheduler</em> 对新部署的Pod保持 <code>Pendding</code></p>
<p>如果此时 <em>scheduler</em> 将新Pod 调度至 $Zone_A$，此时Pod分布在拓扑域间为 $[3,1]$ ，而 <code>maxSkew</code> 配置的值是1。此时倾斜值为 $Zone_A - Zone_B = 3-1=2$，不满足 <code>maxSkew=1</code>，故这个Pod只能被调度到 $Zone_B$。</p>
<p>此时Pod调度拓扑图为图3或图4</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725230358777.png" alt="image-20220725230358777" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图3：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725230515969.png" alt="image-20220725230515969" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图4：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>如果需要将Pod调度到 $Zone_A$ ,可以按照如下方式进行：</p>
<ul>
<li>修改 <code>maxSkew=2</code></li>
<li>修改 <code>topologyKey: node</code> 而不是 <code>Zone</code> ，这种模式下可以将 Pod 均匀分布在Node而不是Zone之间。</li>
<li>修改 <code>whenUnsatisfiable: DoNotSchedule</code> 为 <code>whenUnsatisfiable: ScheduleAnyway</code> 确保新的Pod始终可被调度</li>
</ul>
<p>下面再通过一个例子增强对拓扑域了解</p>
<p><strong>多拓扑约束</strong></p>
<p>设拥有一个 4 节点集群，其中 3 个现有 Pod 标记 <code>foo: bar </code>分别位于 <code>node1</code>、<code>node2</code> 和 <code>node3</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725231905415.png" alt="image-20220725231905415" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图5：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>部署的资源清单如下：可以看出拓扑分布约束配置了多个</p>
<pre><code class="language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  - maxSkew: 1
    topologyKey: node
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1
</code></pre>
<p>在这种情况下，为了匹配第一个约束条件，新Pod 只能放置在 $Zone_B$ ；而就第二个约束条件，新Pod只能调度到 <code>node4</code>。在这种配置多约束条件下， <em>scheduler</em> 只考虑满足所有约束的值，因此唯一有效的是 <code>node4</code>。</p>
<h4 id="如何为集群设置一个默认拓扑域约束">如何为集群设置一个默认拓扑域约束</h4>
<p>默认情况下，拓扑域约束也作 <em>scheduler</em> 的为 <em>scheduler configurtion</em> 中的一部分参数，这也意味着，可以通过profile为整个集群级别指定一个默认的拓扑域调度约束，</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List
</code></pre>
<h4 id="默认约束策略">默认约束策略</h4>
<p>如果在没有配置集群级别的约束策略时，<em>kube-scheduler</em> 内部 <code>topologyspread</code> 插件提供了一个默认的拓扑约束策略，大致上如下列清单所示</p>
<pre><code>defaultConstraints:
  - maxSkew: 3
    topologyKey: &quot;kubernetes.io/hostname&quot;
    whenUnsatisfiable: ScheduleAnyway
  - maxSkew: 5
    topologyKey: &quot;topology.kubernetes.io/zone&quot;
    whenUnsatisfiable: ScheduleAnyway
</code></pre>
<p>上述清单中内容可以在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/plugin.go#L42-L53" target="_blank"
   rel="noopener nofollow noreferrer" >pkg\scheduler\framework\plugins\podtopologyspread\plugin.go</a></p>
<pre><code class="language-go">var systemDefaultConstraints = []v1.TopologySpreadConstraint{
	{
		TopologyKey:       v1.LabelHostname,
		WhenUnsatisfiable: v1.ScheduleAnyway,
		MaxSkew:           3,
	},
	{
		TopologyKey:       v1.LabelTopologyZone,
		WhenUnsatisfiable: v1.ScheduleAnyway,
		MaxSkew:           5,
	},
}
</code></pre>
<p>可以通过在配置文件中留空，来禁用默认配置</p>
<ul>
<li><code>defaultConstraints: []</code></li>
<li><code>defaultingType: List</code></li>
</ul>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints: []
          defaultingType: List
</code></pre>
<h3 id="通过源码学习topology">通过源码学习Topology</h3>
<p><code>podtopologyspread</code> 实现了4种扩展点方法，包含 <code>filter</code> 和 <code>score</code></p>
<h4 id="prefilter">PreFilter</h4>
<p>可以看到 <code>PreFilter</code> 的核心为 <code>calPreFilterState</code></p>
<pre><code class="language-go">func (pl *PodTopologySpread) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	s, err := pl.calPreFilterState(ctx, pod)
	if err != nil {
		return nil, framework.AsStatus(err)
	}
	cycleState.Write(preFilterStateKey, s)
	return nil, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L225-L307" target="_blank"
   rel="noopener nofollow noreferrer" >calPreFilterState</a> 主要功能是用在计算如何在拓扑域中分布Pod，首先看段代码时，需要掌握下属几个概念</p>
<ul>
<li><a href="#preFilterState">preFilterState</a></li>
<li><a href="#criticalPaths">criticalPaths</a></li>
<li><a href="#update">update</a></li>
</ul>
<pre><code class="language-go">func (pl *PodTopologySpread) calPreFilterState(ctx context.Context, pod *v1.Pod) (*preFilterState, error) {
    // 获取Node
	allNodes, err := pl.sharedLister.NodeInfos().List()
	if err != nil {
		return nil, fmt.Errorf(&quot;listing NodeInfos: %w&quot;, err)
	}
	var constraints []topologySpreadConstraint
	if len(pod.Spec.TopologySpreadConstraints) &gt; 0 {
		// 这里会构建出TopologySpreadConstraints，因为约束是不确定的
		constraints, err = filterTopologySpreadConstraints(
			pod.Spec.TopologySpreadConstraints,
			v1.DoNotSchedule,
			pl.enableMinDomainsInPodTopologySpread,
			pl.enableNodeInclusionPolicyInPodTopologySpread,
		)
		if err != nil {
			return nil, fmt.Errorf(&quot;obtaining pod's hard topology spread constraints: %w&quot;, err)
		}
	} else {
        // buildDefaultConstraints使用&quot;.DefaultConstraints&quot;与pod匹配的
        // service、replication controllers、replica sets 
        // 和stateful sets的选择器为pod构建一个约束。
		constraints, err = pl.buildDefaultConstraints(pod, v1.DoNotSchedule)
		if err != nil {
			return nil, fmt.Errorf(&quot;setting default hard topology spread constraints: %w&quot;, err)
		}
	}
	if len(constraints) == 0 { // 如果是空的，则返回空preFilterState
		return &amp;preFilterState{}, nil
	}
    // 初始化一个 preFilterState 状态
	s := preFilterState{
		Constraints:          constraints,
		TpKeyToCriticalPaths: make(map[string]*criticalPaths, len(constraints)),
		TpPairToMatchNum:     make(map[topologyPair]int, sizeHeuristic(len(allNodes), constraints)),
	}
	// 根据node统计拓扑域数量
	tpCountsByNode := make([]map[topologyPair]int, len(allNodes))
	// 获取pod亲和度配置
	requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod)
	processNode := func(i int) {
		nodeInfo := allNodes[i]
		node := nodeInfo.Node()
		if node == nil {
			klog.ErrorS(nil, &quot;Node not found&quot;)
			return
		}
		// 通过spreading去过滤node以用作filters，错误解析以向后兼容
		if !pl.enableNodeInclusionPolicyInPodTopologySpread {
			if match, _ := requiredNodeAffinity.Match(node); !match {
				return
			}
		}

		// 确保node的lable 包含topologyKeys定义的值
		if !nodeLabelsMatchSpreadConstraints(node.Labels, constraints) {
			return
		}

		tpCounts := make(map[topologyPair]int, len(constraints))
		for _, c := range constraints { // 对应的约束列表
			if pl.enableNodeInclusionPolicyInPodTopologySpread &amp;&amp;
				!c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) {
				continue
			}
			// 构建出 topologyPair 以key value形式，
			// 通常情况下TopologyKey属于什么类型的拓扑
			//  node.Labels[c.TopologyKey] 则是属于这个拓扑中那个子域
			pair := topologyPair{key: c.TopologyKey, value: node.Labels[c.TopologyKey]}
			// 计算与标签选择器相匹配的pod有多少个
			count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace)
			tpCounts[pair] = count
		}
		tpCountsByNode[i] = tpCounts // 最终形成的拓扑结构
	}
	// 执行上面的定义的processNode，执行的数量就是node的数量
	pl.parallelizer.Until(ctx, len(allNodes), processNode)
	// 最后构建出 TpPairToMatchNum
	// 表示每个拓扑域中的每个子域各分布多少Pod，如图6所示
	for _, tpCounts := range tpCountsByNode {
		for tp, count := range tpCounts {
			s.TpPairToMatchNum[tp] += count
		}
	}
	if pl.enableMinDomainsInPodTopologySpread {
		// 根据状态进行构建 preFilterState
		s.TpKeyToDomainsNum = make(map[string]int, len(constraints))
		for tp := range s.TpPairToMatchNum {
			s.TpKeyToDomainsNum[tp.key]++
		}
	}

	// 计算最小匹配出的拓扑对
	for i := 0; i &lt; len(constraints); i++ {
		key := constraints[i].TopologyKey
		s.TpKeyToCriticalPaths[key] = newCriticalPaths()
	}
	for pair, num := range s.TpPairToMatchNum {
		s.TpKeyToCriticalPaths[pair.key].update(pair.value, num)
	}

	return &amp;s, nil // 返回的值则包含最小的分布
}
</code></pre>
<p class="preFilterState">preFilterState</p>
<pre><code class="language-go">// preFilterState 是在PreFilter处计算并在Filter处使用。
// 它结合了 “TpKeyToCriticalPaths” 和 “TpPairToMatchNum” 来表示：
//（1）在每个分布约束上匹配最少pod的criticalPaths。 
// (2) 在每个分布约束上匹配的pod的数量。
// “nil preFilterState” 则表示没有设置（在PreFilter阶段）；
// empty “preFilterState”对象则表示它是一个合法的状态，并在PreFilter阶段设置。

type preFilterState struct {
	Constraints []topologySpreadConstraint

    // 这里记录2条关键路径而不是所有关键路径。 
    // criticalPaths[0].MatchNum 始终保存最小匹配数。 
    // criticalPaths[1].MatchNum 总是大于或等于criticalPaths[0].MatchNum，但不能保证是第二个最小匹配数。
	TpKeyToCriticalPaths map[string]*criticalPaths
	
    // TpKeyToDomainsNum 以 “topologyKey” 作为key ，并以zone的数量作为值。
	TpKeyToDomainsNum map[string]int
	
    // TpPairToMatchNum 以 “topologyPair作为key” ，并以匹配到pod的数量作为value。
	TpPairToMatchNum map[topologyPair]int
}
</code></pre>
<p class="criticalPaths">criticalPaths</p>
<pre><code class="language-go">// [2]criticalPath能够工作的原因是基于当前抢占算法的实现，特别是以下两个事实
// 事实 1：只抢占同一节点上的Pod，而不是多个节点上的 Pod。
// 事实 2：每个节点在其抢占周期期间在“preFilterState”的单独副本上进行评估。如果我们计划转向更复杂的算法，例如“多个节点上的任意pod”时则需要重新考虑这种结构。
type criticalPaths [2]struct {
	// TopologyValue代表映射到拓扑键的拓扑值。
	TopologyValue string
	// MatchNum代表匹配到的pod数量
	MatchNum int
}
</code></pre>
<p>单元测试中的测试案例，具有两个约束条件的场景，通过表格来解析如下：</p>
<p>Node列表与标签如下表：</p>
<table>
<thead>
<tr>
<th>Node Name</th>
<th>🏷️Lable-zone</th>
<th>🏷️Lable-node</th>
</tr>
</thead>
<tbody>
<tr>
<td>node-a</td>
<td>zone1</td>
<td>node-a</td>
</tr>
<tr>
<td>node-b</td>
<td>zone1</td>
<td>node-b</td>
</tr>
<tr>
<td>node-x</td>
<td>zone2</td>
<td>node-x</td>
</tr>
<tr>
<td>node-y</td>
<td>zone2</td>
<td>node-y</td>
</tr>
</tbody>
</table>
<p>Pod列表与标签如下表：</p>
<table>
<thead>
<tr>
<th>Pod Name</th>
<th>Node</th>
<th>🏷️Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>p-a1</td>
<td>node-a</td>
<td>foo:</td>
</tr>
<tr>
<td>p-a2</td>
<td>node-a</td>
<td>foo:</td>
</tr>
<tr>
<td>p-b1</td>
<td>node-b</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y1</td>
<td>node-y</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y2</td>
<td>node-y</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y3</td>
<td>node-y</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y4</td>
<td>node-y</td>
<td>foo:</td>
</tr>
</tbody>
</table>
<p>对应的拓扑约束</p>
<pre><code class="language-yaml">spec:
  topologySpreadConstraints:
  - MaxSkew: 1
	TopologyKey: zone
	labelSelector:
	  matchLabels:
	    foo: bar
	MinDomains: 1
	NodeAffinityPolicy: Honor
	NodeTaintsPolicy: Ignore
  - MaxSkew: 1
	TopologyKey: node
	labelSelector:
	  matchLabels:
	    foo: bar
	MinDomains: 1
	NodeAffinityPolicy: Honor
	NodeTaintsPolicy: Ignore
</code></pre>
<p>那么整个分布如下：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220726214255638.png" alt="image-20220726214255638" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图6：具有两个场景的分布图</center>
<p id="prefiltertesting">实现的测试代码如下</p>
<pre><code class="language-go">...
{
    name: &quot;normal case with two spreadConstraints&quot;,
    pod: st.MakePod().Name(&quot;p&quot;).Label(&quot;foo&quot;, &quot;&quot;).
    SpreadConstraint(1, &quot;zone&quot;, v1.DoNotSchedule, fooSelector, nil, nil, nil).
    SpreadConstraint(1, &quot;node&quot;, v1.DoNotSchedule, fooSelector, nil, nil, nil).
    Obj(),
    nodes: []*v1.Node{
        st.MakeNode().Name(&quot;node-a&quot;).Label(&quot;zone&quot;, &quot;zone1&quot;).Label(&quot;node&quot;, &quot;node-a&quot;).Obj(),
        st.MakeNode().Name(&quot;node-b&quot;).Label(&quot;zone&quot;, &quot;zone1&quot;).Label(&quot;node&quot;, &quot;node-b&quot;).Obj(),
        st.MakeNode().Name(&quot;node-x&quot;).Label(&quot;zone&quot;, &quot;zone2&quot;).Label(&quot;node&quot;, &quot;node-x&quot;).Obj(),
        st.MakeNode().Name(&quot;node-y&quot;).Label(&quot;zone&quot;, &quot;zone2&quot;).Label(&quot;node&quot;, &quot;node-y&quot;).Obj(),
    },
    existingPods: []*v1.Pod{
        st.MakePod().Name(&quot;p-a1&quot;).Node(&quot;node-a&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-a2&quot;).Node(&quot;node-a&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-b1&quot;).Node(&quot;node-b&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y1&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y2&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y3&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y4&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
    },
    want: &amp;preFilterState{
        Constraints: []topologySpreadConstraint{
            {
                MaxSkew:            1,
                TopologyKey:        &quot;zone&quot;,
                Selector:           mustConvertLabelSelectorAsSelector(t, fooSelector),
                MinDomains:         1,
                NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,
                NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore,
            },
            {
                MaxSkew:            1,
                TopologyKey:        &quot;node&quot;,
                Selector:           mustConvertLabelSelectorAsSelector(t, fooSelector),
                MinDomains:         1,
                NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,
                NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore,
            },
        },
        TpKeyToCriticalPaths: map[string]*criticalPaths{
            &quot;zone&quot;: {{&quot;zone1&quot;, 3}, {&quot;zone2&quot;, 4}},
            &quot;node&quot;: {{&quot;node-x&quot;, 0}, {&quot;node-b&quot;, 1}},
        },
        for pair, num := range s.TpPairToMatchNum {
		s.TpKeyToCriticalPaths[pair.key].update(pair.value, num)
	}
        TpPairToMatchNum: map[topologyPair]int{
            {key: &quot;zone&quot;, value: &quot;zone1&quot;}:  3,
            {key: &quot;zone&quot;, value: &quot;zone2&quot;}:  4,
            {key: &quot;node&quot;, value: &quot;node-a&quot;}: 2,
            {key: &quot;node&quot;, value: &quot;node-b&quot;}: 1,
            {key: &quot;node&quot;, value: &quot;node-x&quot;}: 0,
            {key: &quot;node&quot;, value: &quot;node-y&quot;}: 4,
        },
    },
}
...
</code></pre>
<p class="update">update</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L120-L148" target="_blank"
   rel="noopener nofollow noreferrer" >update</a> 函数实际上时用于计算 <a href="#criticalPaths">criticalPaths</a> 中的第一位始终保持为是一个最小Pod匹配值</p>
<pre><code class="language-go">func (p *criticalPaths) update(tpVal string, num int) {
	// first verify if `tpVal` exists or not
	i := -1
	if tpVal == p[0].TopologyValue {
		i = 0
	} else if tpVal == p[1].TopologyValue {
		i = 1
	}

	if i &gt;= 0 {
		// `tpVal` 表示已经存在
		p[i].MatchNum = num
		if p[0].MatchNum &gt; p[1].MatchNum {
			// swap paths[0] and paths[1]
			p[0], p[1] = p[1], p[0]
		}
	} else {
		// `tpVal` 表示不存在，如一个新初始化的值
        // num对应子域分布的pod
        // 说明第一个元素不是最小的，则作为交换
		if num &lt; p[0].MatchNum {
			// update paths[1] with paths[0]
			p[1] = p[0]
			// update paths[0]
			p[0].TopologyValue, p[0].MatchNum = tpVal, num
		} else if num &lt; p[1].MatchNum {
			// 如果小于 paths[1]，则更新它，永远保证元素0是最小，1是次小的
			p[1].TopologyValue, p[1].MatchNum = tpVal, num
		}
	}
}
</code></pre>
<p>综合来讲 <code>Prefilter</code> 主要做的工作是。循环所有的节点，先根据 <code>NodeAffinity</code> 或者 <code>NodeSelector</code> 进行过滤，然后根据约束中定义的 <code>topologyKeys</code> （拓扑划分的依据） 来选择节点。</p>
<p>接下来会计算出每个拓扑域下的拓扑对（可以理解为子域）匹配的 Pod 数量，存入 <code>TpPairToMatchNum</code> 中，最后就是要把所有约束中匹配的 Pod 数量最小（第二小）匹配出来的路径（代码是这么定义的，理解上可以看作是分布图）放入 <code>TpKeyToCriticalPaths</code> 中保存起来。整个 <code>preFilterState</code> 保存下来传递到后续的 <code>filter</code> 插件中使用。</p>
<h4 id="filter">Filter</h4>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L178" target="_blank"
   rel="noopener nofollow noreferrer" >preFilter</a> 中 最后的计算结果会保存在 <code>CycleState</code> 中</p>
<pre><code class="language-go">cycleState.Write(preFilterStateKey, s)
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L310-L362" target="_blank"
   rel="noopener nofollow noreferrer" >Filter</a> 主要是从 <code>PreFilter</code> 处理的过程中拿到状态 <code>preFilterState</code>，然后看下每个拓扑约束中的 <code>MaxSkew</code> 是否合法，具体的计算公式为：$matchNum + selfMatchNum - minMatchNum$</p>
<ul>
<li><code>matchNum</code>：Prefilter 中计算出的对应的拓扑分布数量，可以在<a href="#prefiltertesting">Prefilter</a>中参考对应的内容
<ul>
<li><code>if tpCount, ok := s.TpPairToMatchNum[pair]; ok {</code></li>
</ul>
</li>
<li><code>selfMatchNum</code>：匹配到label的数量，匹配到则是1，否则为0</li>
<li><code> minMatchNum</code>：获的 <code>Prefilter </code> 中计算出来的最小匹配的值</li>
</ul>
<pre><code class="language-go">func (pl *PodTopologySpread) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	node := nodeInfo.Node()
	if node == nil {
		return framework.AsStatus(fmt.Errorf(&quot;node not found&quot;))
	}
	// 拿到 prefilter处理的s，即preFilterState
	s, err := getPreFilterState(cycleState)
	if err != nil {
		return framework.AsStatus(err)
	}

	// 一个 空类型的 preFilterState是合法的，这种情况下将容忍每一个被调度的 Pod
	if len(s.Constraints) == 0 {
		return nil
	}

	podLabelSet := labels.Set(pod.Labels) // 设置标签
	for _, c := range s.Constraints { // 因为拓扑约束允许多个所以
		tpKey := c.TopologyKey
		tpVal, ok := node.Labels[c.TopologyKey]
		if !ok {
			klog.V(5).InfoS(&quot;Node doesn't have required label&quot;, &quot;node&quot;, klog.KObj(node), &quot;label&quot;, tpKey)
			return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReasonNodeLabelNotMatch)
		}

		// 判断标准
		// 现有的匹配数量 + 子匹配（1|0） - 全局minimum &lt;= maxSkew
		minMatchNum, err := s.minMatchNum(tpKey, c.MinDomains, pl.enableMinDomainsInPodTopologySpread)
		if err != nil {
			klog.ErrorS(err, &quot;Internal error occurred while retrieving value precalculated in PreFilter&quot;, &quot;topologyKey&quot;, tpKey, &quot;paths&quot;, s.TpKeyToCriticalPaths)
			continue
		}

		selfMatchNum := 0
		if c.Selector.Matches(podLabelSet) {
			selfMatchNum = 1
		}

		pair := topologyPair{key: tpKey, value: tpVal}
		matchNum := 0
		if tpCount, ok := s.TpPairToMatchNum[pair]; ok {
			matchNum = tpCount
		}
		skew := matchNum + selfMatchNum - minMatchNum
		if skew &gt; int(c.MaxSkew) {
			klog.V(5).InfoS(&quot;Node failed spreadConstraint: matchNum + selfMatchNum - minMatchNum &gt; maxSkew&quot;, &quot;node&quot;, klog.KObj(node), &quot;topologyKey&quot;, tpKey, &quot;matchNum&quot;, matchNum, &quot;selfMatchNum&quot;, selfMatchNum, &quot;minMatchNum&quot;, minMatchNum, &quot;maxSkew&quot;, c.MaxSkew)
			return framework.NewStatus(framework.Unschedulable, ErrReasonConstraintsNotMatch)
		}
	}

	return nil
}
</code></pre>
<p>minMatchNum</p>
<pre><code class="language-go">// minMatchNum用于计算 倾斜的全局最小值，同时考虑 MinDomains。
func (s *preFilterState) minMatchNum(tpKey string, minDomains int32, enableMinDomainsInPodTopologySpread bool) (int, error) {
	paths, ok := s.TpKeyToCriticalPaths[tpKey]
	if !ok {
		return 0, fmt.Errorf(&quot;failed to retrieve path by topology key&quot;)
	}
	// 通常来说最小值是第一个
	minMatchNum := paths[0].MatchNum
	if !enableMinDomainsInPodTopologySpread { // 就是plugin的配置的 enableMinDomainsInPodTopologySpread
		return minMatchNum, nil
	}

	domainsNum, ok := s.TpKeyToDomainsNum[tpKey]
	if !ok {
		return 0, fmt.Errorf(&quot;failed to retrieve the number of domains by topology key&quot;)
	}

	if domainsNum &lt; int(minDomains) {
		// 当有匹配拓扑键的符合条件的域的数量小于 配置的&quot;minDomains&quot;(每个约束条件的这个配置) 时，
		//它将全局“minimum” 设置为0。
		// 因为minimum默认就为1，如果他小于1，就让他为0
		minMatchNum = 0
	}

	return minMatchNum, nil
}
</code></pre>
<h4 id="prescore">PreScore</h4>
<p>与 Filter 类似， <code>PreScore</code> 也是类似 <code>PreFilter</code> 的构成。 <code>initPreScoreState</code> 来完成过滤。</p>
<p>有了 <code>PreFilter</code> 基础后，对于 Score 来说大同小异</p>
<pre><code class="language-go">func (pl *PodTopologySpread) PreScore(
	ctx context.Context,
	cycleState *framework.CycleState,
	pod *v1.Pod,
	filteredNodes []*v1.Node,
) *framework.Status {
	allNodes, err := pl.sharedLister.NodeInfos().List()
	if err != nil {
		return framework.AsStatus(fmt.Errorf(&quot;getting all nodes: %w&quot;, err))
	}

	if len(filteredNodes) == 0 || len(allNodes) == 0 {
		// No nodes to score.
		return nil
	}

	state := &amp;preScoreState{
		IgnoredNodes:            sets.NewString(),
		TopologyPairToPodCounts: make(map[topologyPair]*int64),
	}
	// Only require that nodes have all the topology labels if using
	// non-system-default spreading rules. This allows nodes that don't have a
	// zone label to still have hostname spreading.
	// 如果使用非系统默认分布规则，则仅要求节点具有所有拓扑标签。
	// 这将允许没有zone标签的节点仍然具有hostname分布。
	requireAllTopologies := len(pod.Spec.TopologySpreadConstraints) &gt; 0 || !pl.systemDefaulted
	err = pl.initPreScoreState(state, pod, filteredNodes, requireAllTopologies)
	if err != nil {
		return framework.AsStatus(fmt.Errorf(&quot;calculating preScoreState: %w&quot;, err))
	}

	// return if incoming pod doesn't have soft topology spread Constraints.
	if len(state.Constraints) == 0 {
		cycleState.Write(preScoreStateKey, state)
		return nil
	}

	// Ignore parsing errors for backwards compatibility.
	requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod)
	processAllNode := func(i int) {
		nodeInfo := allNodes[i]
		node := nodeInfo.Node()
		if node == nil {
			return
		}

		if !pl.enableNodeInclusionPolicyInPodTopologySpread {
			// `node` should satisfy incoming pod's NodeSelector/NodeAffinity
			if match, _ := requiredNodeAffinity.Match(node); !match {
				return
			}
		}

		// All topologyKeys need to be present in `node`
		if requireAllTopologies &amp;&amp; !nodeLabelsMatchSpreadConstraints(node.Labels, state.Constraints) {
			return
		}

		for _, c := range state.Constraints {
			if pl.enableNodeInclusionPolicyInPodTopologySpread &amp;&amp;
				!c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) {
				continue
			}

			pair := topologyPair{key: c.TopologyKey, value: node.Labels[c.TopologyKey]}
			// If current topology pair is not associated with any candidate node,
			// continue to avoid unnecessary calculation.
			// Per-node counts are also skipped, as they are done during Score.
			tpCount := state.TopologyPairToPodCounts[pair]
			if tpCount == nil {
				continue
			}
			count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace)
			atomic.AddInt64(tpCount, int64(count))
		}
	}
	pl.parallelizer.Until(ctx, len(allNodes), processAllNode)
	// 保存状态给后面sorce调用
	cycleState.Write(preScoreStateKey, state)
	return nil
}
</code></pre>
<p>与Filter中Update使用的函数一样，这里也会到这一步，这里会构建出TopologySpreadConstraints，因为约束是不确定的</p>
<pre><code class="language-go">func filterTopologySpreadConstraints(constraints []v1.TopologySpreadConstraint, action v1.UnsatisfiableConstraintAction, enableMinDomainsInPodTopologySpread, enableNodeInclusionPolicyInPodTopologySpread bool) ([]topologySpreadConstraint, error) {
	var result []topologySpreadConstraint
	for _, c := range constraints {
		if c.WhenUnsatisfiable == action { // 始终调度时
			selector, err := metav1.LabelSelectorAsSelector(c.LabelSelector)
			if err != nil {
				return nil, err
			}
			tsc := topologySpreadConstraint{
				MaxSkew:            c.MaxSkew,
				TopologyKey:        c.TopologyKey,
				Selector:           selector,
				MinDomains:         1,                            // If MinDomains is nil, we treat MinDomains as 1.
				NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,  // If NodeAffinityPolicy is nil, we treat NodeAffinityPolicy as &quot;Honor&quot;.
				NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore, // If NodeTaintsPolicy is nil, we treat NodeTaintsPolicy as &quot;Ignore&quot;.
			}
			if enableMinDomainsInPodTopologySpread &amp;&amp; c.MinDomains != nil {
				tsc.MinDomains = *c.MinDomains
			}
			if enableNodeInclusionPolicyInPodTopologySpread {
				if c.NodeAffinityPolicy != nil {
					tsc.NodeAffinityPolicy = *c.NodeAffinityPolicy
				}
				if c.NodeTaintsPolicy != nil {
					tsc.NodeTaintsPolicy = *c.NodeTaintsPolicy
				}
			}
			result = append(result, tsc)
		}
	}
	return result, nil
}
</code></pre>
<h4 id="score">Score</h4>
<pre><code class="language-GO">// 在分数扩展点调用分数。该函数返回的“score”是 `nodeName` 上匹配的 pod 数量，稍后会进行归一化。
func (pl *PodTopologySpread) Score(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	nodeInfo, err := pl.sharedLister.NodeInfos().Get(nodeName)
	if err != nil {
		return 0, framework.AsStatus(fmt.Errorf(&quot;getting node %q from Snapshot: %w&quot;, nodeName, err))
	}

	node := nodeInfo.Node()
	s, err := getPreScoreState(cycleState)
	if err != nil {
		return 0, framework.AsStatus(err)
	}

	// Return if the node is not qualified.
	if s.IgnoredNodes.Has(node.Name) {
		return 0, nil
	}

	// 对于每个当前的 &lt;pair&gt;，当前节点获得 &lt;matchSum&gt; 的信用分。
	// 计算 &lt;matchSum&gt;总和 并将其作为该节点的分数返回。
	var score float64
	for i, c := range s.Constraints {
		if tpVal, ok := node.Labels[c.TopologyKey]; ok {
			var cnt int64
			if c.TopologyKey == v1.LabelHostname {
				cnt = int64(countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace))
			} else {
				pair := topologyPair{key: c.TopologyKey, value: tpVal}
				cnt = *s.TopologyPairToPodCounts[pair]
			}
			score += scoreForCount(cnt, c.MaxSkew, s.TopologyNormalizingWeight[i])
		}
	}
	return int64(math.Round(score)), nil
}
</code></pre>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/framework.go#L940-L952" target="_blank"
   rel="noopener nofollow noreferrer" >Framework</a> 中会运行 <code>ScoreExtension</code> ，即 <code>NormalizeScore</code></p>
<pre><code class="language-go">// Run NormalizeScore method for each ScorePlugin in parallel.
f.Parallelizer().Until(ctx, len(f.scorePlugins), func(index int) {
    pl := f.scorePlugins[index]
    nodeScoreList := pluginToNodeScores[pl.Name()]
    if pl.ScoreExtensions() == nil {
        return
    }
    status := f.runScoreExtension(ctx, pl, state, pod, nodeScoreList)
    if !status.IsSuccess() {
        err := fmt.Errorf(&quot;plugin %q failed with: %w&quot;, pl.Name(), status.AsError())
        errCh.SendErrorWithCancel(err, cancel)
        return
    }
})
if err := errCh.ReceiveError(); err != nil {
    return nil, framework.AsStatus(fmt.Errorf(&quot;running Normalize on Score plugins: %w&quot;, err))
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/scoring.go#L216-L255" target="_blank"
   rel="noopener nofollow noreferrer" >NormalizeScore</a> 会为所有的node根据之前计算出的权重进行打分</p>
<pre><code class="language-go">func (pl *PodTopologySpread) NormalizeScore(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
	s, err := getPreScoreState(cycleState)
	if err != nil {
		return framework.AsStatus(err)
	}
	if s == nil {
		return nil
	}

	// 计算 &lt;minScore&gt; 和 &lt;maxScore&gt;
	var minScore int64 = math.MaxInt64
	var maxScore int64
	for i, score := range scores {
		// it's mandatory to check if &lt;score.Name&gt; is present in m.IgnoredNodes
		if s.IgnoredNodes.Has(score.Name) {
			scores[i].Score = invalidScore
			continue
		}
		if score.Score &lt; minScore {
			minScore = score.Score
		}
		if score.Score &gt; maxScore {
			maxScore = score.Score
		}
	}

	for i := range scores {
		if scores[i].Score == invalidScore {
			scores[i].Score = 0
			continue
		}
		if maxScore == 0 {
			scores[i].Score = framework.MaxNodeScore
			continue
		}
		s := scores[i].Score
		scores[i].Score = framework.MaxNodeScore * (maxScore + minScore - s) / maxScore
	}
	return nil
}
</code></pre>
<p>到此，对于pod拓扑插件功能大概可以明了了，</p>
<ul>
<li>Filter 部分（<code>PreFilter</code>，<code>Filter</code>）完成拓扑对(<code>Topology Pair</code>)划分</li>
<li>Score部分（<code>PreScore</code>, <code>Score</code> , <code>NormalizeScore</code> ）主要是对拓扑对（可以理解为拓扑结构划分）来选择一个最适合的pod的节点（即分数最优的节点）</li>
</ul>
<p>而在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/plugins/podtopologyspread/scoring_test.go" target="_blank"
   rel="noopener nofollow noreferrer" >scoring_test.go</a> 给了很多用例，可以更深入的了解这部分算法</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling code hierarchy</a></p>
<p><sup id="2">[2]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_algorithm.md" target="_blank"
   rel="noopener nofollow noreferrer" >scheduler algorithm</a></p>
<p><sup id="3">[3]</sup> <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#in-tree-vs-out-of-tree-volume-plugins" target="_blank"
   rel="noopener nofollow noreferrer" >in tree VS out of tree volume plugins</a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_framework_plugins.md" target="_blank"
   rel="noopener nofollow noreferrer" >scheduler_framework_plugins</a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/reference/scheduling/config/" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling config</a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/" target="_blank"
   rel="noopener nofollow noreferrer" >topology spread constraints</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kube-scheduler的调度上下文</title>
      <link>https://www.oomkill.com/2022/07/ch20-schedule-workflow/</link>
      <pubDate>Thu, 21 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/07/ch20-schedule-workflow/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="scheduler">Scheduler</h2>
<p><a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L64-L102" target="_blank"
   rel="noopener nofollow noreferrer" >Scheduler</a> 是整个 <code>kube-scheduler</code> 的一个 structure，提供了 <code>kube-scheduler</code> 运行所需的组件。</p>
<pre><code class="language-go">type Scheduler struct {
	// Cache是一个抽象，会缓存pod的信息，作为scheduler进行查找，操作是基于Pod进行增加
	Cache internalcache.Cache
	// Extenders 算是调度框架中提供的调度插件，会影响kubernetes中的调度策略
	Extenders []framework.Extender

	// NextPod 作为一个函数提供，会阻塞获取下一个ke'diao'du
	NextPod func() *framework.QueuedPodInfo

	// Error is called if there is an error. It is passed the pod in
	// question, and the error
	Error func(*framework.QueuedPodInfo, error)

	// SchedulePod 尝试将给出的pod调度到Node。
	SchedulePod func(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (ScheduleResult, error)

	// 关闭scheduler的信号
	StopEverything &lt;-chan struct{}

	// SchedulingQueue保存要调度的Pod
	SchedulingQueue internalqueue.SchedulingQueue

	// Profiles中是多个调度框架
	Profiles profile.Map
	client clientset.Interface
	nodeInfoSnapshot *internalcache.Snapshot
	percentageOfNodesToScore int32
	nextStartNodeIndex int
}
</code></pre>
<p>作为实际执行的两个核心，<code>SchedulingQueue</code> ，与 <code>scheduleOne</code> 将会分析到这两个</p>
<h2 id="schedulingqueue">SchedulingQueue</h2>
<p>在知道 <code>kube-scheduler</code> 初始化过程后，需要对 <code>kube-scheduler</code> 的整个 <em>structure</em> 和 <em>workflow</em> 进行分析</p>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L336-L340" target="_blank"
   rel="noopener nofollow noreferrer" >Run</a> 中，运行的是 一个 <code>SchedulingQueue</code> 与 一个 <code>scheduleOne</code> ，从结构上看是属于 <em>Scheduler</em></p>
<pre><code class="language-go">func (sched *Scheduler) Run(ctx context.Context) {
	sched.SchedulingQueue.Run()

	// We need to start scheduleOne loop in a dedicated goroutine,
	// because scheduleOne function hangs on getting the next item
	// from the SchedulingQueue.
	// If there are no new pods to schedule, it will be hanging there
	// and if done in this goroutine it will be blocking closing
	// SchedulingQueue, in effect causing a deadlock on shutdown.
	go wait.UntilWithContext(ctx, sched.scheduleOne, 0)

	&lt;-ctx.Done()
	sched.SchedulingQueue.Close()
}

</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L81-L110" target="_blank"
   rel="noopener nofollow noreferrer" >SchedulingQueue</a> 是一个队列的抽象，用于存储等待调度的Pod。该接口遵循类似于 cache.FIFO 和 cache.Heap 的模式。</p>
<pre><code class="language-go">type SchedulingQueue interface {
	framework.PodNominator
	Add(pod *v1.Pod) error
	// Activate moves the given pods to activeQ iff they're in unschedulablePods or backoffQ.
	// The passed-in pods are originally compiled from plugins that want to activate Pods,
	// by injecting the pods through a reserved CycleState struct (PodsToActivate).
	Activate(pods map[string]*v1.Pod)
	// 将不可调度的Pod重入到队列中
	AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error
	// SchedulingCycle returns the current number of scheduling cycle which is
	// cached by scheduling queue. Normally, incrementing this number whenever
	// a pod is popped (e.g. called Pop()) is enough.
	SchedulingCycle() int64
	// Pop会弹出一个pod，并从head优先级队列中删除
	Pop() (*framework.QueuedPodInfo, error)
	Update(oldPod, newPod *v1.Pod) error
	Delete(pod *v1.Pod) error
	MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck)
	AssignedPodAdded(pod *v1.Pod)
	AssignedPodUpdated(pod *v1.Pod)
	PendingPods() []*v1.Pod
	// Close closes the SchedulingQueue so that the goroutine which is
	// waiting to pop items can exit gracefully.
	Close()
	// Run starts the goroutines managing the queue.
	Run()
}
</code></pre>
<p>而 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L134-L175" target="_blank"
   rel="noopener nofollow noreferrer" >PriorityQueue</a> 是 <code>SchedulingQueue</code> 的实现，该部分的核心构成是两个子队列与一个数据结构，即 <code>activeQ</code>、<code>backoffQ</code> 和 <code>unschedulablePods</code></p>
<ul>
<li><code>activeQ</code>：是一个 <em>heap</em> 类型的优先级队列，是 <em>sheduler</em> 从中获得优先级最高的Pod进行调度</li>
<li><code>backoffQ</code>：也是一个 <em>heap</em> 类型的优先级队列，存放的是不可调度的Pod</li>
<li><code>unschedulablePods </code>：保存确定不可被调度的Pod</li>
</ul>
<pre><code class="language-GO">type SchedulingQueue interface {
	framework.PodNominator
	Add(pod *v1.Pod) error
	// Activate moves the given pods to activeQ iff they're in unschedulablePods or backoffQ.
	// The passed-in pods are originally compiled from plugins that want to activate Pods,
	// by injecting the pods through a reserved CycleState struct (PodsToActivate).
	Activate(pods map[string]*v1.Pod)
	// AddUnschedulableIfNotPresent adds an unschedulable pod back to scheduling queue.
	// The podSchedulingCycle represents the current scheduling cycle number which can be
	// returned by calling SchedulingCycle().
	AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error
	// SchedulingCycle returns the current number of scheduling cycle which is
	// cached by scheduling queue. Normally, incrementing this number whenever
	// a pod is popped (e.g. called Pop()) is enough.
	SchedulingCycle() int64
	// Pop removes the head of the queue and returns it. It blocks if the
	// queue is empty and waits until a new item is added to the queue.
	Pop() (*framework.QueuedPodInfo, error)
	Update(oldPod, newPod *v1.Pod) error
	Delete(pod *v1.Pod) error
	MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck)
	AssignedPodAdded(pod *v1.Pod)
	AssignedPodUpdated(pod *v1.Pod)
	PendingPods() []*v1.Pod
	// Close closes the SchedulingQueue so that the goroutine which is
	// waiting to pop items can exit gracefully.
	Close()
	// Run starts the goroutines managing the queue.
	Run()
}
</code></pre>
<p>在New <em>scheduler</em> 时可以看到会初始化这个queue</p>
<pre><code class="language-go">podQueue := internalqueue.NewSchedulingQueue(
    // 实现pod对比的一个函数即less
    profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
    informerFactory,
    internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
    internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
    internalqueue.WithPodNominator(nominator),
    internalqueue.WithClusterEventMap(clusterEventMap),
    internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
)
</code></pre>
<p>而 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L252-L289" target="_blank"
   rel="noopener nofollow noreferrer" >NewSchedulingQueue</a> 则是初始化这个 PriorityQueue</p>
<pre><code class="language-go">// NewSchedulingQueue initializes a priority queue as a new scheduling queue.
func NewSchedulingQueue(
	lessFn framework.LessFunc,
	informerFactory informers.SharedInformerFactory,
	opts ...Option) SchedulingQueue {
	return NewPriorityQueue(lessFn, informerFactory, opts...)
}

// NewPriorityQueue creates a PriorityQueue object.
func NewPriorityQueue(
	lessFn framework.LessFunc,
	informerFactory informers.SharedInformerFactory,
	opts ...Option,
) *PriorityQueue {
	options := defaultPriorityQueueOptions
	for _, opt := range opts {
		opt(&amp;options)
	}
	// 这个就是 less函数，作为打分的一部分
	comp := func(podInfo1, podInfo2 interface{}) bool {
		pInfo1 := podInfo1.(*framework.QueuedPodInfo)
		pInfo2 := podInfo2.(*framework.QueuedPodInfo)
		return lessFn(pInfo1, pInfo2)
	}

	if options.podNominator == nil {
		options.podNominator = NewPodNominator(informerFactory.Core().V1().Pods().Lister())
	}

	pq := &amp;PriorityQueue{
		PodNominator:                      options.podNominator,
		clock:                             options.clock,
		stop:                              make(chan struct{}),
		podInitialBackoffDuration:         options.podInitialBackoffDuration,
		podMaxBackoffDuration:             options.podMaxBackoffDuration,
		podMaxInUnschedulablePodsDuration: options.podMaxInUnschedulablePodsDuration,
		activeQ:                           heap.NewWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()),
		unschedulablePods:                 newUnschedulablePods(metrics.NewUnschedulablePodsRecorder()),
		moveRequestCycle:                  -1,
		clusterEventMap:                   options.clusterEventMap,
	}
	pq.cond.L = &amp;pq.lock
	pq.podBackoffQ = heap.NewWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder())
	pq.nsLister = informerFactory.Core().V1().Namespaces().Lister()

	return pq
}
</code></pre>
<p>了解了Queue的结构，就需要知道 入队列与出队列是在哪里操作的。在初始化时，需要注册一个 <code>addEventHandlerFuncs</code> 这个时候，会注入三个动作函数，也就是controller中的概念；而在AddFunc中可以看到会入队列。</p>
<p>注入是对 Pod 的<a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/eventhandlers.go#L302-L305" target="_blank"
   rel="noopener nofollow noreferrer" >informer</a>注入的，注入的函数 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/eventhandlers.go#L114-L120" target="_blank"
   rel="noopener nofollow noreferrer" >addPodToSchedulingQueue</a>  就是入栈</p>
<pre><code class="language-go">Handler: cache.ResourceEventHandlerFuncs{
    AddFunc:    sched.addPodToSchedulingQueue,
    UpdateFunc: sched.updatePodInSchedulingQueue,
    DeleteFunc: sched.deletePodFromSchedulingQueue,
},

func (sched *Scheduler) addPodToSchedulingQueue(obj interface{}) {
	pod := obj.(*v1.Pod)
	klog.V(3).InfoS(&quot;Add event for unscheduled pod&quot;, &quot;pod&quot;, klog.KObj(pod))
	if err := sched.SchedulingQueue.Add(pod); err != nil {
		utilruntime.HandleError(fmt.Errorf(&quot;unable to queue %T: %v&quot;, obj, err))
	}
}
</code></pre>
<p>而这个 <code>SchedulingQueue</code> 的实现就是 <code>PriorityQueue</code> ，而Add中则对 activeQ进行的操作</p>
<pre><code class="language-go">func (p *PriorityQueue) Add(pod *v1.Pod) error {
	p.lock.Lock()
	defer p.lock.Unlock()
    // 格式化入栈数据，包含podinfo，里会包含v1.Pod
    // 初始化的时间，创建的时间，以及不能被调度时的记录其plugin的名称
	pInfo := p.newQueuedPodInfo(pod)
    // 入栈
	if err := p.activeQ.Add(pInfo); err != nil {
		klog.ErrorS(err, &quot;Error adding pod to the active queue&quot;, &quot;pod&quot;, klog.KObj(pod))
		return err
	}
	if p.unschedulablePods.get(pod) != nil {
		klog.ErrorS(nil, &quot;Error: pod is already in the unschedulable queue&quot;, &quot;pod&quot;, klog.KObj(pod))
		p.unschedulablePods.delete(pod)
	}
	// Delete pod from backoffQ if it is backing off
	if err := p.podBackoffQ.Delete(pInfo); err == nil {
		klog.ErrorS(nil, &quot;Error: pod is already in the podBackoff queue&quot;, &quot;pod&quot;, klog.KObj(pod))
	}
	metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;active&quot;, PodAdd).Inc()
	p.PodNominator.AddNominatedPod(pInfo.PodInfo, nil)
	p.cond.Broadcast()

	return nil
}
</code></pre>
<p>在上面看 <em>scheduler</em> 结构时，可以看到有一个 nextPod的，nextPod就是从队列中弹出一个pod，这个在<em>scheduler</em> 时会传入 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L952-L965" target="_blank"
   rel="noopener nofollow noreferrer" >MakeNextPodFunc</a> 就是这个 nextpod</p>
<pre><code class="language-go">func MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo {
	return func() *framework.QueuedPodInfo {
		podInfo, err := queue.Pop()
		if err == nil {
			klog.V(4).InfoS(&quot;About to try and schedule pod&quot;, &quot;pod&quot;, klog.KObj(podInfo.Pod))
			for plugin := range podInfo.UnschedulablePlugins {
				metrics.UnschedulableReason(plugin, podInfo.Pod.Spec.SchedulerName).Dec()
			}
			return podInfo
		}
		klog.ErrorS(err, &quot;Error while retrieving next pod from scheduling queue&quot;)
		return nil
	}
}
</code></pre>
<p>而这个 <code>queue.Pop()</code> 对应的就是 <code>PriorityQueue</code> 的 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L483-L503" target="_blank"
   rel="noopener nofollow noreferrer" >Pop()</a> ，在这里会将作为 activeQ 的消费端</p>
<pre><code class="language-go">func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
   p.lock.Lock()
   defer p.lock.Unlock()
   for p.activeQ.Len() == 0 {
      // When the queue is empty, invocation of Pop() is blocked until new item is enqueued.
      // When Close() is called, the p.closed is set and the condition is broadcast,
      // which causes this loop to continue and return from the Pop().
      if p.closed {
         return nil, fmt.Errorf(queueClosed)
      }
      p.cond.Wait()
   }
   obj, err := p.activeQ.Pop()
   if err != nil {
      return nil, err
   }
   pInfo := obj.(*framework.QueuedPodInfo)
   pInfo.Attempts++
   p.schedulingCycle++
   return pInfo, nil
}
</code></pre>
<p>在上面入口部分也看到了，scheduleOne 和 scheduler，scheduleOne 就是去消费一个Pod，他会调用 NextPod，NextPod就是在初始化传入的 <code>MakeNextPodFunc</code> ，至此回到对应的 Pop来做消费。</p>
<p>schedulerOne是为一个Pod做调度的流程。</p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	podInfo := sched.NextPod()
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
	fwk, err := sched.frameworkForPod(pod)
	if err != nil {
		// This shouldn't happen, because we only accept for scheduling the pods
		// which specify a scheduler name that matches one of the profiles.
		klog.ErrorS(err, &quot;Error occurred&quot;)
		return
	}
	if sched.skipPodSchedule(fwk, pod) {
		return
	}
...
</code></pre>
<h2 id="调度上下文">调度上下文</h2>
<p>当了解了scheduler结构后，下面分析下调度上下文的过程。看看扩展点是怎么工作的。这个时候又需要提到官网的调度上下文的图。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/scheduling-framework-extensions.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图1：Pod的调度上下文</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework</center>
<p>而 <em>scheduler</em> 对于调度上下文来就是这个 <code>scheduleOne </code> ，下面就是看这个调度上下文</p>
<h3 id="sort">Sort</h3>
<p><code>Sort</code> 插件提供了排序功能，用于对在调度队列中待处理 Pod 进行排序。一次只能启用一个队列排序。</p>
<p>在进入 <code>scheduleOne</code> 后，<code>NextPod</code> 从 <code>activeQ</code> 中队列中得到一个Pod，然后的 <code>frameworkForPod</code> 会做打分的动作就是调度上下文的第一个扩展点 <code>sort</code></p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	podInfo := sched.NextPod()
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
	fwk, err := sched.frameworkForPod(pod)
...
    
func (sched *Scheduler) frameworkForPod(pod *v1.Pod) (framework.Framework, error) {
    // 获取指定的profile
	fwk, ok := sched.Profiles[pod.Spec.SchedulerName]
	if !ok {
		return nil, fmt.Errorf(&quot;profile not found for scheduler name %q&quot;, pod.Spec.SchedulerName)
	}
	return fwk, nil
}
</code></pre>
<p>回顾，因为在New scheduler时会初始化这个 sort 函数</p>
<pre><code class="language-go">podQueue := internalqueue.NewSchedulingQueue(
    profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
    informerFactory,
    internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
    internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
    internalqueue.WithPodNominator(nominator),
    internalqueue.WithClusterEventMap(clusterEventMap),
    internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
)
</code></pre>
<h3 id="prefilter">preFilter</h3>
<p>preFilter作为第一个扩展点，是用于在过滤之前预处理或检查 Pod 或集群的相关信息。这里会终止调度</p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	podInfo := sched.NextPod()
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
	fwk, err := sched.frameworkForPod(pod)
	if err != nil {
		// This shouldn't happen, because we only accept for scheduling the pods
		// which specify a scheduler name that matches one of the profiles.
		klog.ErrorS(err, &quot;Error occurred&quot;)
		return
	}
	if sched.skipPodSchedule(fwk, pod) {
		return
	}

	klog.V(3).InfoS(&quot;Attempting to schedule pod&quot;, &quot;pod&quot;, klog.KObj(pod))

	// Synchronously attempt to find a fit for the pod.
	start := time.Now()
	state := framework.NewCycleState()
	state.SetRecordPluginMetrics(rand.Intn(100) &lt; pluginMetricsSamplePercent)
	// Initialize an empty podsToActivate struct, which will be filled up by plugins or stay empty.
	podsToActivate := framework.NewPodsToActivate()
	state.Write(framework.PodsToActivateKey, podsToActivate)

	schedulingCycleCtx, cancel := context.WithCancel(ctx)
	defer cancel()
    // 这里将进入prefilter
	scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L311-L360" target="_blank"
   rel="noopener nofollow noreferrer" >schedulePod</a> 尝试将给定的 pod 调度到节点列表中的节点之一。如果成功，它将返回节点的名称。</p>
<pre><code class="language-go">func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New(&quot;Scheduling&quot;, utiltrace.Field{Key: &quot;namespace&quot;, Value: pod.Namespace}, utiltrace.Field{Key: &quot;name&quot;, Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)
	// 用于将cache更新为当前内容
	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
		return result, err
	}
	trace.Step(&quot;Snapshotting scheduler cache and node infos done&quot;)

	if sched.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}
	// 找到一个合适的pod时，会执行扩展点
	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	
    ...
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L364-L426" target="_blank"
   rel="noopener nofollow noreferrer" >findNodesThatFitPod</a> 会执行对应的过滤插件来找到最适合的Node，包括备注，以及方法名都可以看到，这里运行的插件😁😁，后面会分析算法内容，只对workflow学习。</p>
<pre><code class="language-go">func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
	diagnosis := framework.Diagnosis{
		NodeToStatusMap:      make(framework.NodeToStatusMap),
		UnschedulablePlugins: sets.NewString(),
	}

	// Run &quot;prefilter&quot; plugins.
	preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod)
	allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List()
	if err != nil {
		return nil, diagnosis, err
	}
	if !s.IsSuccess() {
		if !s.IsUnschedulable() {
			return nil, diagnosis, s.AsError()
		}
		// All nodes will have the same status. Some non trivial refactoring is
		// needed to avoid this copy.
		for _, n := range allNodes {
			diagnosis.NodeToStatusMap[n.Node().Name] = s
		}
		// Status satisfying IsUnschedulable() gets injected into diagnosis.UnschedulablePlugins.
		if s.FailedPlugin() != &quot;&quot; {
			diagnosis.UnschedulablePlugins.Insert(s.FailedPlugin())
		}
		return nil, diagnosis, nil
	}

	// &quot;NominatedNodeName&quot; can potentially be set in a previous scheduling cycle as a result of preemption.
	// This node is likely the only candidate that will fit the pod, and hence we try it first before iterating over all nodes.
	if len(pod.Status.NominatedNodeName) &gt; 0 {
		feasibleNodes, err := sched.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis)
		if err != nil {
			klog.ErrorS(err, &quot;Evaluation failed on nominated node&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, pod.Status.NominatedNodeName)
		}
		// Nominated node passes all the filters, scheduler is good to assign this node to the pod.
		if len(feasibleNodes) != 0 {
			return feasibleNodes, diagnosis, nil
		}
	}

	nodes := allNodes
	if !preRes.AllNodes() {
		nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames))
		for n := range preRes.NodeNames {
			nInfo, err := sched.nodeInfoSnapshot.NodeInfos().Get(n)
			if err != nil {
				return nil, diagnosis, err
			}
			nodes = append(nodes, nInfo)
		}
	}
	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)
	if err != nil {
		return nil, diagnosis, err
	}

	feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
	if err != nil {
		return nil, diagnosis, err
	}
	return feasibleNodes, diagnosis, nil
}
</code></pre>
<h3 id="filter">filter</h3>
<p>filter插件相当于<em>调度上下文</em>中的 <code>Predicates</code>，用于排除不能运行 Pod 的节点。Filter 会按配置的顺序进行调用。如果有一个filter将节点标记位不可用，则将 Pod 标记为不可调度（即不会向下执行）。</p>
<p>对于代码中来讲，filter还是处于 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L364-L426" target="_blank"
   rel="noopener nofollow noreferrer" >findNodesThatFitPod</a> 函数中，<code>findNodesThatPassFilters</code> 就是获取到 FN，即可行节点，而这个过程就是 <em>filter</em> 扩展点</p>
<pre><code class="language-go">func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
	...
    
	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)
	if err != nil {
		return nil, diagnosis, err
	}

	feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
	if err != nil {
		return nil, diagnosis, err
	}
	return feasibleNodes, diagnosis, nil
}
</code></pre>
<h3 id="postfilter">Postfilter</h3>
<p>当没有为 pod 找到<em>FN</em>时，该插件会按照配置的顺序进行调用。如果任何<code>postFilter</code>插件将 Pod 标记为<em>schedulable</em>，则不会调用其余插件。即 <code>filter</code> 成功后不会进行这步骤，那我们来验证下这里把😊</p>
<p>还是在 scheduleOne 中，当我们运行的 SchedulePod 完成后（成功或失败），这时会返回一个err，而 <code>postfilter</code> 会根据这个 err进行选择执行或不执行，符合官方给出的说法。</p>
<pre><code class="language-go">scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
	if err != nil {
		// SchedulePod() may have failed because the pod would not fit on any host, so we try to
		// preempt, with the expectation that the next time the pod is tried for scheduling it
		// will fit due to the preemption. It is also possible that a different pod will schedule
		// into the resources that were preempted, but this is harmless.
		var nominatingInfo *framework.NominatingInfo
		if fitError, ok := err.(*framework.FitError); ok {
			if !fwk.HasPostFilterPlugins() {
				klog.V(3).InfoS(&quot;No PostFilter plugins are registered, so no preemption will be performed&quot;)
			} else {
				// Run PostFilter plugins to try to make the pod schedulable in a future scheduling cycle.
				result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap)
				if status.Code() == framework.Error {
					klog.ErrorS(nil, &quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, status)
				} else {
					fitError.Diagnosis.PostFilterMsg = status.Message()
					klog.V(5).InfoS(&quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, status)
				}
				if result != nil {
					nominatingInfo = result.NominatingInfo
				}
			}
			// Pod did not fit anywhere, so it is counted as a failure. If preemption
			// succeeds, the pod should get counted as a success the next time we try to
			// schedule it. (hopefully)
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else if err == ErrNoNodesAvailable {
			nominatingInfo = clearNominatedNode
			// No nodes available is counted as unschedulable rather than an error.
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else {
			nominatingInfo = clearNominatedNode
			klog.ErrorS(err, &quot;Error selecting node for pod&quot;, &quot;pod&quot;, klog.KObj(pod))
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
		}
		sched.handleSchedulingFailure(ctx, fwk, podInfo, err, v1.PodReasonUnschedulable, nominatingInfo)
		return
	}
</code></pre>
<h3 id="prescorescore">PreScore,Score</h3>
<p>可用于进行预Score工作，作为通知性的扩展点，会在在filter完之后直接会关联 preScore 插件进行继续工作，而不是返回，如果配置的这些插件有任何一个返回失败，则Pod将被拒绝。</p>
<pre><code class="language-go">
func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New(&quot;Scheduling&quot;, utiltrace.Field{Key: &quot;namespace&quot;, Value: pod.Namespace}, utiltrace.Field{Key: &quot;name&quot;, Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)

	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
		return result, err
	}
	trace.Step(&quot;Snapshotting scheduler cache and node infos done&quot;)

	if sched.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}

	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step(&quot;Computing predicates done&quot;)

	if len(feasibleNodes) == 0 {
		return result, &amp;framework.FitError{
			Pod:         pod,
			NumAllNodes: sched.nodeInfoSnapshot.NumNodes(),
			Diagnosis:   diagnosis,
		}
	}

	// When only one node after predicate, just use it.
	if len(feasibleNodes) == 1 {
		return ScheduleResult{
			SuggestedHost:  feasibleNodes[0].Name,
			EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap),
			FeasibleNodes:  1,
		}, nil
	}
	// 这里会完成prescore，score
	priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes)
	if err != nil {
		return result, err
	}

	host, err := selectHost(priorityList)
	trace.Step(&quot;Prioritizing done&quot;)

	return ScheduleResult{
		SuggestedHost:  host,
		EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap),
		FeasibleNodes:  len(feasibleNodes),
	}, err
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L605-L705" target="_blank"
   rel="noopener nofollow noreferrer" >priorityNodes</a> 会通过配置的插件给Node打分，并返回每个Node的分数，将每个插件打分结果计算总和获得Node的分数，最后获得节点的加权总分数。</p>
<pre><code class="language-go">func prioritizeNodes(
	ctx context.Context,
	extenders []framework.Extender,
	fwk framework.Framework,
	state *framework.CycleState,
	pod *v1.Pod,
	nodes []*v1.Node,
) (framework.NodeScoreList, error) {
	// If no priority configs are provided, then all nodes will have a score of one.
	// This is required to generate the priority list in the required format
	if len(extenders) == 0 &amp;&amp; !fwk.HasScorePlugins() {
		result := make(framework.NodeScoreList, 0, len(nodes))
		for i := range nodes {
			result = append(result, framework.NodeScore{
				Name:  nodes[i].Name,
				Score: 1,
			})
		}
		return result, nil
	}

	// Run PreScore plugins.
	preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes)
	if !preScoreStatus.IsSuccess() {
		return nil, preScoreStatus.AsError()
	}

	// Run the Score plugins.
	scoresMap, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes)
	if !scoreStatus.IsSuccess() {
		return nil, scoreStatus.AsError()
	}

	// Additional details logged at level 10 if enabled.
	klogV := klog.V(10)
	if klogV.Enabled() {
		for plugin, nodeScoreList := range scoresMap {
			for _, nodeScore := range nodeScoreList {
				klogV.InfoS(&quot;Plugin scored node for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;plugin&quot;, plugin, &quot;node&quot;, nodeScore.Name, &quot;score&quot;, nodeScore.Score)
			}
		}
	}

	// Summarize all scores.
	result := make(framework.NodeScoreList, 0, len(nodes))

	for i := range nodes {
		result = append(result, framework.NodeScore{Name: nodes[i].Name, Score: 0})
		for j := range scoresMap {
			result[i].Score += scoresMap[j][i].Score
		}
	}

	if len(extenders) != 0 &amp;&amp; nodes != nil {
		var mu sync.Mutex
		var wg sync.WaitGroup
		combinedScores := make(map[string]int64, len(nodes))
		for i := range extenders {
			if !extenders[i].IsInterested(pod) {
				continue
			}
			wg.Add(1)
			go func(extIndex int) {
				metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Inc()
				defer func() {
					metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Dec()
					wg.Done()
				}()
				prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes)
				if err != nil {
					// Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities
					klog.V(5).InfoS(&quot;Failed to run extender's priority function. No score given by this extender.&quot;, &quot;error&quot;, err, &quot;pod&quot;, klog.KObj(pod), &quot;extender&quot;, extenders[extIndex].Name())
					return
				}
				mu.Lock()
				for i := range *prioritizedList {
					host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score
					if klogV.Enabled() {
						klogV.InfoS(&quot;Extender scored node for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;extender&quot;, extenders[extIndex].Name(), &quot;node&quot;, host, &quot;score&quot;, score)
					}
					combinedScores[host] += score * weight
				}
				mu.Unlock()
			}(i)
		}
		// wait for all go routines to finish
		wg.Wait()
		for i := range result {
			// MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore,
			// therefore we need to scale the score returned by extenders to the score range used by the scheduler.
			result[i].Score += combinedScores[result[i].Name] * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority)
		}
	}

	if klogV.Enabled() {
		for i := range result {
			klogV.InfoS(&quot;Calculated node's final score for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, result[i].Name, &quot;score&quot;, result[i].Score)
		}
	}
	return result, nil
}
</code></pre>
<h3 id="reserve">Reserve</h3>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L153-L163" target="_blank"
   rel="noopener nofollow noreferrer" >Reserve</a> 因为绑定事件时异步发生的，该插件是为了避免Pod在绑定到节点前时，调度到新的Pod，使节点使用资源超过可用资源情况。如果后续阶段发生错误或失败，将触发 <code>UnReserve</code> 回滚（通知性扩展点）。这也是作为调度周期中最后一个状态，要么成功到 <code>postBind</code> ，要么失败触发 <code>UnReserve</code>。</p>
<pre><code class="language-go">// Run the Reserve method of reserve plugins.
if sts := fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() { // 当处理不成功时
    metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
    // 触发 un-reserve 来清理相关Pod的状态
    fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
    if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
        klog.ErrorS(forgetErr, &quot;Scheduler cache ForgetPod failed&quot;)
    }
    sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, sts.AsError(), SchedulerError, clearNominatedNode)
    return
}
</code></pre>
<h3 id="permit">permit</h3>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L165-L183" target="_blank"
   rel="noopener nofollow noreferrer" >Permit</a> 插件可以阻止或延迟 Pod 的绑定</p>
<pre><code class="language-go">	// Run &quot;permit&quot; plugins.
	runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
	if !runPermitStatus.IsWait() &amp;&amp; !runPermitStatus.IsSuccess() {
		var reason string
		if runPermitStatus.IsUnschedulable() {
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
			reason = v1.PodReasonUnschedulable
		} else {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
			reason = SchedulerError
		}
        // 只要其中一个插件返回的状态不是 success 或者 wait
		fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
        // 从cache中忘掉pod
		if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
			klog.ErrorS(forgetErr, &quot;Scheduler cache ForgetPod failed&quot;)
		}
		sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, runPermitStatus.AsError(), reason, clearNominatedNode)
		return
	}

</code></pre>
<h3 id="binding-cycle">Binding Cycle</h3>
<p>在选择好 <em>FN</em> 后则做一个假设绑定，并更新到cache中，接下来回去执行真正的bind操作，也就是 <code>binding cycle</code></p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	...
    ...
	// binding cycle 是一个异步的操作，这里表现就是go协程
	go func() {
		bindingCycleCtx, cancel := context.WithCancel(ctx)
		defer cancel()
		metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Inc()
		defer metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Dec()
		// 运行WaitOnPermit插件，如果失败则，unReserve回滚
		waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod)
		if !waitOnPermitStatus.IsSuccess() {
			var reason string
			if waitOnPermitStatus.IsUnschedulable() {
				metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
				reason = v1.PodReasonUnschedulable
			} else {
				metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
				reason = SchedulerError
			}
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
				klog.ErrorS(forgetErr, &quot;scheduler cache ForgetPod failed&quot;)
			} else {
				// &quot;Forget&quot;ing an assumed Pod in binding cycle should be treated as a PodDelete event,
				// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
				// TODO(#103853): de-duplicate the logic.
				// Avoid moving the assumed Pod itself as it's always Unschedulable.
				// It's intentional to &quot;defer&quot; this operation; otherwise MoveAllToActiveOrBackoffQueue() would
				// update `q.moveRequest` and thus move the assumed pod to backoffQ anyways.
				defer sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, func(pod *v1.Pod) bool {
					return assumedPod.UID != pod.UID
				})
			}
			sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, waitOnPermitStatus.AsError(), reason, clearNominatedNode)
			return
		}

	// 运行Prebind 插件
		preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		if !preBindStatus.IsSuccess() {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
				klog.ErrorS(forgetErr, &quot;scheduler cache ForgetPod failed&quot;)
			} else {
				// &quot;Forget&quot;ing an assumed Pod in binding cycle should be treated as a PodDelete event,
				// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
				// TODO(#103853): de-duplicate the logic.
				sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, nil)
			}
			sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, preBindStatus.AsError(), SchedulerError, clearNominatedNode)
			return
		}
		// bind是真正的绑定操作
		err := sched.bind(bindingCycleCtx, fwk, assumedPod, scheduleResult.SuggestedHost, state)
		if err != nil {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
			// 如果失败了就触发 un-reserve plugins 
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			if err := sched.Cache.ForgetPod(assumedPod); err != nil {
				klog.ErrorS(err, &quot;scheduler cache ForgetPod failed&quot;)
			} else {
				// &quot;Forget&quot;ing an assumed Pod in binding cycle should be treated as a PodDelete event,
				// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
				// TODO(#103853): de-duplicate the logic.
				sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, nil)
			}
			sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, fmt.Errorf(&quot;binding rejected: %w&quot;, err), SchedulerError, clearNominatedNode)
			return
		}
		// Calculating nodeResourceString can be heavy. Avoid it if klog verbosity is below 2.
		klog.V(2).InfoS(&quot;Successfully bound pod to node&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, scheduleResult.SuggestedHost, &quot;evaluatedNodes&quot;, scheduleResult.EvaluatedNodes, &quot;feasibleNodes&quot;, scheduleResult.FeasibleNodes)
		metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start))
		metrics.PodSchedulingAttempts.Observe(float64(podInfo.Attempts))
		metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(podInfo)).Observe(metrics.SinceInSeconds(podInfo.InitialAttemptTimestamp))

		// 运行 &quot;postbind&quot; 插件
        // 是通知性的扩展点，该插件在绑定 Pod 后调用，可用于清理相关资源（）。
		fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)

		// At the end of a successful binding cycle, move up Pods if needed.
		if len(podsToActivate.Map) != 0 {
			sched.SchedulingQueue.Activate(podsToActivate.Map)
			// Unlike the logic in scheduling cycle, we don't bother deleting the entries
			// as `podsToActivate.Map` is no longer consumed.
		}
	}()
}
</code></pre>
<h2 id="调度上下文中的失败流程">调度上下文中的失败流程</h2>
<p>上面说到的都是正常的请求，下面会对失败的请求是如何重试的进行分析，而 <em>scheduler</em> 中关于失败处理方面相关的属性会涉及到上面 <em>scheduler</em> 结构中的  <code>backoffQ</code> 与 <code>unschedulablePods </code></p>
<ul>
<li><code>backoffQ</code>：也是一个 <em>heap</em> 类型的优先级队列，存放的是不可调度的Pod</li>
<li><code>unschedulablePods </code>：保存确定不可被调度的Pod，一个map类型</li>
</ul>
<p>backoffQ 与  unschedulablePods 会在初始化 <em>scheduler</em> 时初始化，</p>
<pre><code class="language-go">func NewPriorityQueue(
	lessFn framework.LessFunc,
	informerFactory informers.SharedInformerFactory,
	opts ...Option,
) *PriorityQueue {
	options := defaultPriorityQueueOptions
	for _, opt := range opts {
		opt(&amp;options)
	}

	comp := func(podInfo1, podInfo2 interface{}) bool {
		pInfo1 := podInfo1.(*framework.QueuedPodInfo)
		pInfo2 := podInfo2.(*framework.QueuedPodInfo)
		return lessFn(pInfo1, pInfo2)
	}

	if options.podNominator == nil {
		options.podNominator = NewPodNominator(informerFactory.Core().V1().Pods().Lister())
	}

	pq := &amp;PriorityQueue{
		PodNominator:                      options.podNominator,
		clock:                             options.clock,
		stop:                              make(chan struct{}),
		podInitialBackoffDuration:         options.podInitialBackoffDuration,
		podMaxBackoffDuration:             options.podMaxBackoffDuration,
		podMaxInUnschedulablePodsDuration: options.podMaxInUnschedulablePodsDuration,
		activeQ:                           heap.NewWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()),
		unschedulablePods:                 newUnschedulablePods(metrics.NewUnschedulablePodsRecorder()),
		moveRequestCycle:                  -1,
		clusterEventMap:                   options.clusterEventMap,
	}
	pq.cond.L = &amp;pq.lock
    // 初始化backoffQ
    // NewWithRecorder作为一个可选的 metricRecorder 的 Heap 对象。
    // podInfoKeyFunc是一个函数，返回错误与字符串
    // pq.podsCompareBackoffCompleted 比较两个pod的回退时间，如果第一个在第二个之前为true，
    // 反之 false
	pq.podBackoffQ = heap.NewWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder())
	pq.nsLister = informerFactory.Core().V1().Namespaces().Lister()

	return pq
}
</code></pre>
<p>对于初始化 backoffQ 会产生的两个函数，<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L757-L761" target="_blank"
   rel="noopener nofollow noreferrer" >getBackoffTime</a> 与 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L765-L775" target="_blank"
   rel="noopener nofollow noreferrer" >calculateBackoffDuration</a></p>
<pre><code class="language-go">// getBackoffTime returns the time that podInfo completes backoff
func (p *PriorityQueue) getBackoffTime(podInfo *framework.QueuedPodInfo) time.Time {
	duration := p.calculateBackoffDuration(podInfo)
	backoffTime := podInfo.Timestamp.Add(duration)
	return backoffTime
}

// calculateBackoffDuration is a helper function for calculating the backoffDuration
// based on the number of attempts the pod has made.
func (p *PriorityQueue) calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration {
	duration := p.podInitialBackoffDuration
	for i := 1; i &lt; podInfo.Attempts; i++ {
		// Use subtraction instead of addition or multiplication to avoid overflow.
		if duration &gt; p.podMaxBackoffDuration-duration {
			return p.podMaxBackoffDuration
		}
		duration += duration
	}
	return duration
}
</code></pre>
<p>对于整个故障错误会按照如下流程进行，在初始化 <em>scheduler</em> 会注册一个 Error 函数，这个函数用作对不可调度Pod进行处理，实际上被注册的函数是 MakeDefaultErrorFunc。这个函数将作为 Error 函数被调用。</p>
<pre><code class="language-go">sched := newScheduler(
    schedulerCache,
    extenders,
    internalqueue.MakeNextPodFunc(podQueue),
    MakeDefaultErrorFunc(client, podLister, podQueue, schedulerCache),
    stopEverything,
    podQueue,
    profiles,
    client,
    snapshot,
    options.percentageOfNodesToScore,
)
</code></pre>
<p>而在 调度周期中，也就是 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L66-L132" target="_blank"
   rel="noopener nofollow noreferrer" >scheduleOne</a> 可以看到，每个扩展点操作失败后都会调用 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L812-L834" target="_blank"
   rel="noopener nofollow noreferrer" >handleSchedulingFailure</a> 而该函数，使用了注册的 <em>Error</em> 函数来处理Pod</p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	...
	defer cancel()
	scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
	if err != nil {

		var nominatingInfo *framework.NominatingInfo
		if fitError, ok := err.(*framework.FitError); ok {
			if !fwk.HasPostFilterPlugins() {
				klog.V(3).InfoS(&quot;No PostFilter plugins are registered, so no preemption will be performed&quot;)
			} else {
			
				result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap)
				if status.Code() == framework.Error {
					klog.ErrorS(nil, &quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, status)
				} else {
					fitError.Diagnosis.PostFilterMsg = status.Message()
					klog.V(5).InfoS(&quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, status)
				}
				if result != nil {
					nominatingInfo = result.NominatingInfo
				}
			}
	
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else if err == ErrNoNodesAvailable {
			nominatingInfo = clearNominatedNode
			// No nodes available is counted as unschedulable rather than an error.
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else {
			nominatingInfo = clearNominatedNode
			klog.ErrorS(err, &quot;Error selecting node for pod&quot;, &quot;pod&quot;, klog.KObj(pod))
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
		}
        // 处理不可调度Pod
		sched.handleSchedulingFailure(ctx, fwk, podInfo, err, v1.PodReasonUnschedulable, nominatingInfo)
		return
	}

</code></pre>
<p>来到了注册的 <em>Error</em> 函数 <code>MakeDefaultErrorFunc</code></p>
<pre><code class="language-go">func MakeDefaultErrorFunc(client clientset.Interface, podLister corelisters.PodLister, podQueue internalqueue.SchedulingQueue, schedulerCache internalcache.Cache) func(*framework.QueuedPodInfo, error) {
	return func(podInfo *framework.QueuedPodInfo, err error) {
		pod := podInfo.Pod
		if err == ErrNoNodesAvailable {
			klog.V(2).InfoS(&quot;Unable to schedule pod; no nodes are registered to the cluster; waiting&quot;, &quot;pod&quot;, klog.KObj(pod))
		} else if fitError, ok := err.(*framework.FitError); ok {
			// Inject UnschedulablePlugins to PodInfo, which will be used later for moving Pods between queues efficiently.
			podInfo.UnschedulablePlugins = fitError.Diagnosis.UnschedulablePlugins
			klog.V(2).InfoS(&quot;Unable to schedule pod; no fit; waiting&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, err)
		} else if apierrors.IsNotFound(err) {
			klog.V(2).InfoS(&quot;Unable to schedule pod, possibly due to node not found; waiting&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, err)
			if errStatus, ok := err.(apierrors.APIStatus); ok &amp;&amp; errStatus.Status().Details.Kind == &quot;node&quot; {
				nodeName := errStatus.Status().Details.Name
				// when node is not found, We do not remove the node right away. Trying again to get
				// the node and if the node is still not found, then remove it from the scheduler cache.
				_, err := client.CoreV1().Nodes().Get(context.TODO(), nodeName, metav1.GetOptions{})
				if err != nil &amp;&amp; apierrors.IsNotFound(err) {
					node := v1.Node{ObjectMeta: metav1.ObjectMeta{Name: nodeName}}
					if err := schedulerCache.RemoveNode(&amp;node); err != nil {
						klog.V(4).InfoS(&quot;Node is not found; failed to remove it from the cache&quot;, &quot;node&quot;, node.Name)
					}
				}
			}
		} else {
			klog.ErrorS(err, &quot;Error scheduling pod; retrying&quot;, &quot;pod&quot;, klog.KObj(pod))
		}

		// Check if the Pod exists in informer cache.
		cachedPod, err := podLister.Pods(pod.Namespace).Get(pod.Name)
		if err != nil {
			klog.InfoS(&quot;Pod doesn't exist in informer cache&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, err)
			return
		}

		// In the case of extender, the pod may have been bound successfully, but timed out returning its response to the scheduler.
		// It could result in the live version to carry .spec.nodeName, and that's inconsistent with the internal-queued version.
		if len(cachedPod.Spec.NodeName) != 0 {
			klog.InfoS(&quot;Pod has been assigned to node. Abort adding it back to queue.&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, cachedPod.Spec.NodeName)
			return
		}

		// As &lt;cachedPod&gt; is from SharedInformer, we need to do a DeepCopy() here.
		podInfo.PodInfo = framework.NewPodInfo(cachedPod.DeepCopy())
        // 添加到unschedulable队列中
		if err := podQueue.AddUnschedulableIfNotPresent(podInfo, podQueue.SchedulingCycle()); err != nil {
			klog.ErrorS(err, &quot;Error occurred&quot;)
		}
	}
}
</code></pre>
<p>下面来到 <code>AddUnschedulableIfNotPresent</code> ，这个也是操作 <code>backoffQ</code> 和 <code>unschedulablePods </code> 的真正的动作</p>
<p><code>AddUnschedulableIfNotPresent</code> 函数会吧无法调度的 pod 插入队列，除非它已经在队列中。通常情况下，<code>PriorityQueue</code> 将不可调度的 Pod 放在 <code>unschedulablePods</code> 中。但如果最近有 move request，则将 pod 放入 <code>podBackoffQ</code> 中。</p>
<pre><code class="language-go">func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error {
	p.lock.Lock()
	defer p.lock.Unlock()
	pod := pInfo.Pod
    // 如果已经存在则不添加
	if p.unschedulablePods.get(pod) != nil {
		return fmt.Errorf(&quot;Pod %v is already present in unschedulable queue&quot;, klog.KObj(pod))
	}
	// 检查是否在activeQ中
	if _, exists, _ := p.activeQ.Get(pInfo); exists {
		return fmt.Errorf(&quot;Pod %v is already present in the active queue&quot;, klog.KObj(pod))
	}
    // 检查是否在podBackoffQ中
	if _, exists, _ := p.podBackoffQ.Get(pInfo); exists {
		return fmt.Errorf(&quot;Pod %v is already present in the backoff queue&quot;, klog.KObj(pod))
	}

	// 在重新添加时，会刷新 Pod时间为最新操作的时间
	pInfo.Timestamp = p.clock.Now()

	for plugin := range pInfo.UnschedulablePlugins {
		metrics.UnschedulableReason(plugin, pInfo.Pod.Spec.SchedulerName).Inc()
	}
    // 如果接受到move request那么则放入BackoffQ
	if p.moveRequestCycle &gt;= podSchedulingCycle {
		if err := p.podBackoffQ.Add(pInfo); err != nil {
			return fmt.Errorf(&quot;error adding pod %v to the backoff queue: %v&quot;, pod.Name, err)
		}
		metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;backoff&quot;, ScheduleAttemptFailure).Inc()
	} else {
        // 否则将放入到 unschedulablePods
		p.unschedulablePods.addOrUpdate(pInfo)
		metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;unschedulable&quot;, ScheduleAttemptFailure).Inc()

	}

	p.PodNominator.AddNominatedPod(pInfo.PodInfo, nil)
	return nil
}
</code></pre>
<p>在启动 <em>scheduler</em> 时，会将这两个队列异步启用两个loop来操作队列。表现在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L292-L295" target="_blank"
   rel="noopener nofollow noreferrer" >Run()</a></p>
<pre><code class="language-go">func (p *PriorityQueue) Run() {
	go wait.Until(p.flushBackoffQCompleted, 1.0*time.Second, p.stop)
	go wait.Until(p.flushUnschedulablePodsLeftover, 30*time.Second, p.stop)
}
</code></pre>
<p>可以看到 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L431-L458" target="_blank"
   rel="noopener nofollow noreferrer" >flushBackoffQCompleted</a> 作为 <code>BackoffQ</code> 实现；而 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L462-L478" target="_blank"
   rel="noopener nofollow noreferrer" >flushUnschedulablePodsLeftover</a> 作为 <code>UnschedulablePods</code> 实现。</p>
<p><code>flushBackoffQCompleted</code> 是用于将所有已完成回退的 pod 从 <code>backoffQ</code> 移到 <code>activeQ</code> 中</p>
<pre><code class="language-go">func (p *PriorityQueue) flushBackoffQCompleted() {
	p.lock.Lock()
	defer p.lock.Unlock()
	broadcast := false
	for { // 这就是heap实现的方法，窥视下，但不弹出
		rawPodInfo := p.podBackoffQ.Peek()
		if rawPodInfo == nil {
			break
		}
		pod := rawPodInfo.(*framework.QueuedPodInfo).Pod
		boTime := p.getBackoffTime(rawPodInfo.(*framework.QueuedPodInfo))
		if boTime.After(p.clock.Now()) {
			break
		}
		_, err := p.podBackoffQ.Pop() // 弹出一个
		if err != nil {
			klog.ErrorS(err, &quot;Unable to pop pod from backoff queue despite backoff completion&quot;, &quot;pod&quot;, klog.KObj(pod))
			break
		}
		p.activeQ.Add(rawPodInfo) // 放入到活动队列中
		metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;active&quot;, BackoffComplete).Inc()
		broadcast = true
	}

	if broadcast {
		p.cond.Broadcast()
	}
}
</code></pre>
<p><code>flushUnschedulablePodsLeftover </code> 函数用于将在 <code>unschedulablePods</code> 中的存放时间超过 <code>podMaxInUnschedulablePodsDuration</code> 值的 pod 移动到 <code>backoffQ</code> 或 <code>activeQ</code> 中。</p>
<p><code>podMaxInUnschedulablePodsDuration</code> 会根据配置传入，当没有传入，也就是使用了 <em><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/cmd/kube-scheduler/app/options/options.go#L77-L84" target="_blank"
   rel="noopener nofollow noreferrer" >Deprecated</a></em> 那么会为5分钟。</p>
<pre><code class="language-go">func NewOptions() *Options {
	o := &amp;Options{
		SecureServing:  apiserveroptions.NewSecureServingOptions().WithLoopback(),
		Authentication: apiserveroptions.NewDelegatingAuthenticationOptions(),
		Authorization:  apiserveroptions.NewDelegatingAuthorizationOptions(),
		Deprecated: &amp;DeprecatedOptions{
			PodMaxInUnschedulablePodsDuration: 5 * time.Minute,
		},
</code></pre>
<p>对于 <code>flushUnschedulablePodsLeftover </code> 就是做一个时间对比，然后添加到对应的队列中</p>
<pre><code class="language-go">func (p *PriorityQueue) flushUnschedulablePodsLeftover() {
	p.lock.Lock()
	defer p.lock.Unlock()

	var podsToMove []*framework.QueuedPodInfo
	currentTime := p.clock.Now()
	for _, pInfo := range p.unschedulablePods.podInfoMap {
		lastScheduleTime := pInfo.Timestamp
		if currentTime.Sub(lastScheduleTime) &gt; p.podMaxInUnschedulablePodsDuration {
			podsToMove = append(podsToMove, pInfo)
		}
	}

	if len(podsToMove) &gt; 0 {
		p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout)
	}
}
</code></pre>
<h2 id="总结调度上下文流程">总结调度上下文流程</h2>
<ul>
<li>在构建一个 <em>scheduler</em> 时经历如下步骤：
<ul>
<li>准备cache，informer，queue，错误处理函数等</li>
<li>添加事件函数，会监听资源（如Pod），当有变动则触发对应事件函数，这是入站 <code>activeQ</code></li>
</ul>
</li>
<li>构建完成后会 run，run时会run一个 <code>SchedulingQueue</code>，这个是作为不可调度队列
<ul>
<li><code>BackoffQ</code></li>
<li><code>UnschedulablePods</code></li>
<li>不可调度队列会根据注册时定期消费队列中Pod将其添加到 <code>activeQ</code> 中</li>
</ul>
</li>
<li>启动一个 <code>scheduleOne</code> 的loop，这个是调度上下文中所有的扩展点的执行，也是 <code>activeQ</code> 的消费端
<ul>
<li><code>scheduleOne</code> 获取 pod</li>
<li>执行各个扩展点，如果出错则 <em>Error</em> 函数 <code>MakeDefaultErrorFunc</code> 将其添加到不可调度队列中</li>
<li>回到不可调度队列中消费部分</li>
</ul>
</li>
</ul>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://www.sobyte.net/post/2022-02/kubernetes-scheduling-framework-and-extender/" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes scheduler extender</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes的决策组件 - kube-scheduler原理分析</title>
      <link>https://www.oomkill.com/2022/07/ch16-scheduler/</link>
      <pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/07/ch16-scheduler/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview-supa-href11asup">Overview <sup><a href="#1">[1]</a></sup></h2>
<p>kubernetes集群中的调度程序 <code>kube-scheduler</code> 会 <code>watch</code> 未分配节点的新创建的Pod，并未该Pod找到可运行的最佳（特定）节点。那么这些动作或者说这些原理是怎么实现的呢，让我们往下剖析下。</p>
<p>对于新创建的 pod 或其他未调度的 pod来讲，kube-scheduler 选择一个最佳节点供它们运行。但是，Pod 中的每个容器对资源的要求都不同，每个 Pod 也有不同的要求。因此，需要根据具体的调度要求对现有节点进行过滤。</p>
<p>在Kubernetes集群中，满足 Pod 调度要求的节点称为可行节点 （<code> feasible nodes</code> <em>FN</em>） 。如果没有合适的节点，则 pod 将保持未调度状态，直到调度程序能够放置它。也就是说，当我们创建Pod时，如果长期处于 <code>Pending</code> 状态，这个时候应该看你的集群调度器是否因为某些问题没有合适的节点了</p>
<p>调度器为 Pod 找到 FN 后，然后运行一组函数对 FN 进行评分，并在 FN 中找到得分最高的节点来运行 Pod。</p>
<p>调度策略在决策时需要考虑的因素包括个人和集体资源需求、硬件/软件/策略约束 （<code>constraints</code>）、亲和性 (<code>affinity</code>) 和反亲和性（ <code>anti-affinity</code> ）规范、数据局部性、工作负载间干扰等。</p>
<h3 id="如何为pod选择节点">如何为pod选择节点？</h3>
<p><code>kube-scheduler</code> 为pod选择节点会分位两部：</p>
<ul>
<li>过滤 (<code>Filtering</code>)</li>
<li>打分 (<code>Scoring</code>)</li>
</ul>
<p>过滤也被称为预选 （<code>Predicates</code>），该步骤会找到可调度的节点集，然后通过是否满足特定资源的请求，例如通过 <code>PodFitsResources</code> 过滤器检查候选节点是否有足够的资源来满足 Pod 资源的请求。这个步骤完成后会得到一个包含合适的节点的列表（通常为多个），如果列表为空，则Pod不可调度。</p>
<p>打分也被称为优选（<code>Priorities</code>），在该步骤中，会对上一个步骤的输出进行打分，Scheduer 通过打分的规则为每个通过 <code>Filtering</code> 步骤的节点计算出一个分数。</p>
<p>完成上述两个步骤之后，<code>kube-scheduler</code> 会将Pod分配给分数最高的 Node，如果存在多个相同分数的节点，会随机选择一个。</p>
<h2 id="kubernetes的调度策略">kubernetes的调度策略</h2>
<p>Kubernetes 1.21之前版本可以在代码 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.21/pkg/scheduler/algorithmprovider/registry.go" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes\pkg\scheduler\algorithmprovider\registry.go</a> 中看到对应的注册模式，在1.22 scheduler 更换了其路径，对于registry文件更换到了<a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/plugins/registry.go" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes\pkg\scheduler\framework\plugins\registry.go</a> ；对于kubernetes官方说法为，<em>调度策略是用于“预选” (<code>Predicates </code>)或 过滤（<code>filtering </code>） 和 用于 优选（<code>Priorities</code>）或 评分 (<code>scoring</code>)的</em></p>
<blockquote>
<p>注：kubernetes官方没有找到预选和优选的概念，而Predicates和filtering 是处于预选阶段的动词，而Priorities和scoring是优选阶段的动词。后面用PF和PS代替这个两个词。</p>
</blockquote>
<h3 id="为pod预选节点-supa-href22asup">为Pod预选节点 <sup><a href="#2">[2]</a></sup></h3>
<p>上面也提到了，<code>filtering</code> 的目的是为了排除（过滤）掉不满足 Pod 要求的节点。例如，某个节点上的闲置资源小于 Pod 所需资源，则该节点不会被考虑在内，即被过滤掉。在 <em>“Predicates”</em> 阶段实现的 <em>filtering</em> 策略，包括：</p>
<ul>
<li><code>NoDiskConflict</code> ：评估是否有合适Pod请求的卷</li>
<li><code>NoVolumeZoneConflict</code>：在给定zone限制情况下，评估Pod请求所需的卷在Node上是否可用</li>
<li><code>PodFitsResources</code>：检查空闲资源（CPU、内存）是否满足Pod请求</li>
<li><code>PodFitsHostPorts</code>：检查Pod所需端口在Node上是否被占用</li>
<li><code>HostName</code>： 过滤除去，<code>PodSpec</code> 中 <code>NodeName</code> 字段中指定的Node之外的所有Node。</li>
<li><code>MatchNodeSelector</code>：检查Node的 <em>label</em> 是否与 <em>Pod</em> 配置中 <code>nodeSelector</code>字段中指定的 <em>label</em> 匹配，并且从 Kubernetes v1.2 开始， 如果存在 <code>nodeAffinity</code> 也会匹配。</li>
<li><code>CheckNodeMemoryPressure</code>：检查是否可以在已出现内存压力情况节点上调度 Pod。</li>
<li><code>CheckNodeDiskPressure</code>：检查是否可以在报告磁盘压力情况的节点上调度 Pod</li>
</ul>
<p>具体对应得策略可以在 kubernetes\pkg\scheduler\framework\plugins\registry.go 看到</p>
<h3 id="对预选节点打分-supa-href22asup">对预选节点打分 <sup><a href="#2">[2]</a></sup></h3>
<p>通过上面步骤过滤过得列表则是适合托管的Pod，这个结果通常来说是一个列表，如何选择最优Node进行调度，则是接下来打分的步骤步骤。</p>
<p>例如：Kubernetes对剩余节点进行优先级排序，优先级由一组函数计算；优先级函数将为剩余节点给出从<code>0~10</code> 的分数，10 表示最优，0 表示最差。每个优先级函数由一个正数加权组成，每个Node的得分是通过将所有加权得分相加来计算的。设有两个优先级函数，<code>priorityFunc1</code> 和 <code>priorityFunc2</code> 加上权重因子 <code>weight1</code> 和<code>weight2</code>，那么这个Node的最终得分为：$finalScore = (w1 \times priorityFunc1) + (w2 \times priorityFunc2)$。计算完分数后，选择最高分数的Node做为Pod的宿主机，存在多个相同分数Node情况下会随机选择一个Node。</p>
<p>目前kubernetes提供了一些在打分 <em>Scoring</em> 阶段算法：</p>
<ul>
<li><code>LeastRequestedPriority</code>：Node的优先级基于Node的空闲部分$\frac{capacity\ -\  Node上所有存在的Pod\ -\ 正在调度的Pod请求}{capacity}$，通过计算具有最高分数的Node是FN</li>
<li><code>BalancedResourceAllocation</code> ：该算法会将 Pod 放在一个Node上，使得在Pod 部署后 CPU 和内存的使用率为平衡的</li>
<li><code>SelectorSpreadPriority</code>：通过最小化资源方式，将属于同一种服务、控制器或同一Node上的Replica的 Pod的数量来分布Pod。如果节点上存在Zone，则会调整优先级，以便 pod可以分布在Zone之上。</li>
<li><code>CalculateAntiAffinityPriority</code>：根据label来分布，按照相同service上相同label值的pod进行分配</li>
<li><code>ImageLocalityPriority</code> ：根据Node上镜像进行打分，Node上存在Pod请求所需的镜像优先级较高。</li>
</ul>
<h3 id="在代码中查看上述的代码">在代码中查看上述的代码</h3>
<p>以 <code>PodFitsHostPorts</code> 算法为例，因为是Node类算法，在<a href="https://github.com/kubernetes/kubernetes/tree/release-1.23/pkg/scheduler/framework/plugins/nodeports" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes\pkg\scheduler\framework\plugins\nodeports</a></p>
<h2 id="调度框架-supa-href33asup">调度框架 <sup><a href="#3">[3]</a></sup></h2>
<p>调度框架 (<code>scheduling framework</code> <em>SF</em> ) 是kubernetes为 scheduler设计的一个pluggable的架构。SF 将scheduler设计为 <em>Plugin</em> 式的 API，API将上一章中提到的一些列调度策略实现为 <code>Plugin</code>。</p>
<p>在 <em>SF</em> 中，定义了一些扩展点 （<code>extension points</code> <em>EP</em> ），而被实现为Plugin的调度程序将被注册在一个或多个 <em>EP</em> 中，换句话来说，在这些 <em>EP</em> 的执行过程中如果注册在多个 <em>EP</em> 中，将会在多个 <em>EP</em> 被调用。</p>
<p>每次调度都分为两个阶段，调度周期（<code>Scheduling Cycel</code>）与绑定周期（<code>Binding Cycle</code>）。</p>
<ul>
<li><em>SC</em> 表示为，为Pod选择一个节点；<em>SC</em> 是串行运行的。</li>
<li><em>BC</em> 表示为，将 <em>SC</em> 决策结果应用于集群中；<em>BC</em> 可以同时运行。</li>
</ul>
<p>调度周期与绑定周期结合一起，被称为<strong>调度上下文</strong> （<code>Scheduling Context</code>）,下图则是调度上下文的工作流</p>
<blockquote>
<p>注：如果决策结果为Pod的调度结果无可用节点，或存在内部错误，则中止 <em>SC</em> 或 <em>BC</em>。Pod将重入队列重试</p>
</blockquote>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/scheduling-framework-extensions.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图1：Pod的调度上下文</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework</center>
<h3 id="扩展点-supa-href44asup">扩展点 <sup><a href="#4">[4]</a></sup></h3>
<p>扩展点（<code>Extension points</code>）是指在<em>调度上下文</em>中的每个可扩展API，通过图提现为<a href="#podsc">[图1]</a>。其中 <code>Filter</code> 相当于 <code>Predicate</code> 而 <code>Scoring</code> 相当于 <code>Priority</code>。</p>
<p>对于调度阶段会通过以下扩展点：</p>
<ul>
<li>
<p><code>Sort</code>：该插件提供了排序功能，用于对在调度队列中待处理 Pod 进行排序。一次只能启用一个队列排序。</p>
</li>
<li>
<p><code>preFilter</code>：该插件用于在过滤之前预处理或检查 Pod 或集群的相关信息。这里会终止调度</p>
</li>
<li>
<p><code>filter</code>：该插件相当于<em>调度上下文</em>中的 <code>Predicates</code>，用于排除不能运行 Pod 的节点。Filter 会按配置的顺序进行调用。如果有一个filter将节点标记位不可用，则将 Pod 标记为不可调度（即不会向下执行）。</p>
</li>
<li>
<p><code>postFilter</code>：当没有为 pod 找到<em>FN</em>时，该插件会按照配置的顺序进行调用。如果任何<code>postFilter</code>插件将 Pod 标记为<em>schedulable</em>，则不会调用其余插件。即 <code>filter</code> 成功后不会进行这步骤</p>
</li>
<li>
<p><code>preScore</code>：可用于进行预Score工作（通知性的扩展点）。</p>
</li>
<li>
<p><code>score</code>：该插件为每个通过 <code>filter</code> 阶段的Node提供打分服务。然后Scheduler将选择具有最高加权分数总和的Node。</p>
</li>
<li>
<p><code>reserve</code>：因为绑定事件时异步发生的，该插件是为了避免Pod在绑定到节点前时，调度到新的Pod，使节点使用资源超过可用资源情况。如果后续阶段发生错误或失败，将触发 <code>UnReserve</code> 回滚（通知性扩展点）。这也是作为调度周期中最后一个状态，要么成功到 <code>postBind</code> ，要么失败触发 <code>UnReserve</code>。</p>
</li>
<li>
<p><code>permit</code>：该插件可以阻止或延迟 Pod 的绑定，一般情况下这步骤会做三件事：</p>
<ul>
<li><code>appove</code> ：调度器继续绑定过程</li>
<li><code>Deny</code>：如果任何一个Premit拒绝了Pod与节点的绑定，那么将触发 <code>UnReserve</code> ，并重入队列</li>
<li><code>Wait</code>： 如果 Permit 插件返回 <code>Wait</code>，该 Pod 将保留在内部 <code>Wait</code> Pod 列表中，直到被 <code>Appove</code>。如果发生超时，<code>wait</code> 变为 <code>deny</code> ，将Pod放回至调度队列中，并触发 <code>Unreserve</code> 回滚 。</li>
</ul>
</li>
<li>
<p><code>preBind</code>：该插件用于在 bind Pod 之前执行所需的前置工作。如，<code>preBind</code> 可能会提供一个网络卷并将其挂载到目标节点上。如果在该步骤中的任意插件返回错误，则Pod 将被 <code>deny</code> 并放置到调度队列中。</p>
</li>
<li>
<p><code>bind</code>：在所有的 <code>preBind</code> 完成后，该插件将用于将Pod绑定到Node，并按顺序调用绑定该步骤的插件。如果有一个插件处理了这个事件，那么则忽略其余所有插件。</p>
</li>
<li>
<p><code>postBind</code>：该插件在绑定 Pod 后调用，可用于清理相关资源（通知性的扩展点）。</p>
</li>
<li>
<p><code>multiPoint</code>：这是一个仅配置字段，允许同时为所有适用的扩展点启用或禁用插件。</p>
</li>
</ul>
<h2 id="kube-scheduler工作流分析">kube-scheduler工作流分析</h2>
<p>对于 <code>kube-scheduler</code> 组件的分析，包含 <code>kube-scheduler</code> 启动流程，以及scheduler调度流程。这里会主要针对启动流程分析，后面算法及二次开发部分会切入调度分析。</p>
<p>对于我们部署时使用的 <code>kube-scheduler</code> 位于 <a href="https://github.com/kubernetes/kubernetes/tree/release-1.24/cmd/kube-scheduler" target="_blank"
   rel="noopener nofollow noreferrer" >cmd/kube-scheduler</a> ，在 <em>Alpha (1.16)</em> 版本提供了调度框架的模式，到 <em>Stable (1.19)</em> ，从代码结构上是相似的；直到1.22后改变了代码风格。</p>
<p>首先看到的是 <code>kube-scheduler</code> 的入口 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/cmd/kube-scheduler/scheduler.go" target="_blank"
   rel="noopener nofollow noreferrer" >cmd/kube-scheduler</a> ，这里主要作为两部分，构建参数与启动<code>server</code> ,这里严格来讲 <code>kube-scheduer</code> 是作为一个server，而调度框架等部分是另外的。</p>
<pre><code class="language-go">func main() {
	command := app.NewSchedulerCommand()
	code := cli.Run(command)
	os.Exit(code)
}
</code></pre>
<p><code>cli.Run</code> 提供了cobra构成的命令行cli，日志将输出为标准输出</p>
<pre><code class="language-go">// 这里是main中执行的Run
func Run(cmd *cobra.Command) int {
	if logsInitialized, err := run(cmd); err != nil {
		if !logsInitialized {
			fmt.Fprintf(os.Stderr, &quot;Error: %v\n&quot;, err)
		} else {
			klog.ErrorS(err, &quot;command failed&quot;)
		}
		return 1
	}
	return 0
}
// 这个run作为
func run(cmd *cobra.Command) (logsInitialized bool, err error) {
	rand.Seed(time.Now().UnixNano())
	defer logs.FlushLogs()

	cmd.SetGlobalNormalizationFunc(cliflag.WordSepNormalizeFunc)

	if !cmd.SilenceUsage {
		cmd.SilenceUsage = true
		cmd.SetFlagErrorFunc(func(c *cobra.Command, err error) error {
			// Re-enable usage printing.
			c.SilenceUsage = false
			return err
		})
	}

	// In all cases error printing is done below.
	cmd.SilenceErrors = true

	// This is idempotent.
	logs.AddFlags(cmd.PersistentFlags())

	// Inject logs.InitLogs after command line parsing into one of the
	// PersistentPre* functions.
	switch {
	case cmd.PersistentPreRun != nil:
		pre := cmd.PersistentPreRun
		cmd.PersistentPreRun = func(cmd *cobra.Command, args []string) {
			logs.InitLogs()
			logsInitialized = true
			pre(cmd, args)
		}
	case cmd.PersistentPreRunE != nil:
		pre := cmd.PersistentPreRunE
		cmd.PersistentPreRunE = func(cmd *cobra.Command, args []string) error {
			logs.InitLogs()
			logsInitialized = true
			return pre(cmd, args)
		}
	default:
		cmd.PersistentPreRun = func(cmd *cobra.Command, args []string) {
			logs.InitLogs()
			logsInitialized = true
		}
	}

	err = cmd.Execute()
	return
}
</code></pre>
<p>可以看到最终是调用 <code>command.Execute() </code> 执行，这个是执行本身构建的命令，而真正被执行的则是上面的 <code>app.NewSchedulerCommand()</code> ,那么来看看这个是什么</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L72-L114" target="_blank"
   rel="noopener nofollow noreferrer" >app.NewSchedulerCommand()</a> 构建了一个cobra.Commond对象， <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L117-L142" target="_blank"
   rel="noopener nofollow noreferrer" >runCommand()</a> 被封装在内，这个是作为启动scheduler的函数</p>
<pre><code class="language-go">func NewSchedulerCommand(registryOptions ...Option) *cobra.Command {
	opts := options.NewOptions()

	cmd := &amp;cobra.Command{
		Use: &quot;kube-scheduler&quot;,
		Long: `The Kubernetes scheduler is a control plane process which assigns
Pods to Nodes. The scheduler determines which Nodes are valid placements for
each Pod in the scheduling queue according to constraints and available
resources. The scheduler then ranks each valid Node and binds the Pod to a
suitable Node. Multiple different schedulers may be used within a cluster;
kube-scheduler is the reference implementation.
See [scheduling](https://kubernetes.io/docs/concepts/scheduling-eviction/)
for more information about scheduling and the kube-scheduler component.`,
		RunE: func(cmd *cobra.Command, args []string) error {
			return runCommand(cmd, opts, registryOptions...)
		},
		Args: func(cmd *cobra.Command, args []string) error {
			for _, arg := range args {
				if len(arg) &gt; 0 {
					return fmt.Errorf(&quot;%q does not take any arguments, got %q&quot;, cmd.CommandPath(), args)
				}
			}
			return nil
		},
	}

	nfs := opts.Flags
	verflag.AddFlags(nfs.FlagSet(&quot;global&quot;))
	globalflag.AddGlobalFlags(nfs.FlagSet(&quot;global&quot;), cmd.Name(), logs.SkipLoggingConfigurationFlags())
	fs := cmd.Flags()
	for _, f := range nfs.FlagSets {
		fs.AddFlagSet(f)
	}

	cols, _, _ := term.TerminalSize(cmd.OutOrStdout())
	cliflag.SetUsageAndHelpFunc(cmd, *nfs, cols)

	if err := cmd.MarkFlagFilename(&quot;config&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;json&quot;); err != nil {
		klog.ErrorS(err, &quot;Failed to mark flag filename&quot;)
	}

	return cmd
}
</code></pre>
<p>下面来看下 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L117-L142" target="_blank"
   rel="noopener nofollow noreferrer" >runCommand()</a> 在启动 <em>scheduler</em> 时提供了什么功能。</p>
<p>在新版中已经没有 <code>algorithmprovider</code> 的概念，所以在 <code>runCommand</code> 中做的也就是仅仅启动这个 <code>scheduler</code> ，而 scheduler 作为kubernetes组件，也是会watch等操作，自然少不了informer。其次作为和 <code>controller-manager</code> 相同的工作特性，<code>kube-scheduler</code> 也是 基于Leader选举的。</p>
<pre><code class="language-go">func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error {
	// To help debugging, immediately log version
	klog.InfoS(&quot;Starting Kubernetes Scheduler&quot;, &quot;version&quot;, version.Get())

	klog.InfoS(&quot;Golang settings&quot;, &quot;GOGC&quot;, os.Getenv(&quot;GOGC&quot;), &quot;GOMAXPROCS&quot;, os.Getenv(&quot;GOMAXPROCS&quot;), &quot;GOTRACEBACK&quot;, os.Getenv(&quot;GOTRACEBACK&quot;))

	// Configz registration.
	if cz, err := configz.New(&quot;componentconfig&quot;); err == nil {
		cz.Set(cc.ComponentConfig)
	} else {
		return fmt.Errorf(&quot;unable to register configz: %s&quot;, err)
	}

	// Start events processing pipeline.
	cc.EventBroadcaster.StartRecordingToSink(ctx.Done())
	defer cc.EventBroadcaster.Shutdown()

	// Setup healthz checks.
	var checks []healthz.HealthChecker
	if cc.ComponentConfig.LeaderElection.LeaderElect {
		checks = append(checks, cc.LeaderElection.WatchDog)
	}

	waitingForLeader := make(chan struct{})
	isLeader := func() bool {
		select {
		case _, ok := &lt;-waitingForLeader:
			// if channel is closed, we are leading
			return !ok
		default:
			// channel is open, we are waiting for a leader
			return false
		}
	}

	// Start up the healthz server.
	if cc.SecureServing != nil {
		handler := buildHandlerChain(newHealthzAndMetricsHandler(&amp;cc.ComponentConfig, cc.InformerFactory, isLeader, checks...), cc.Authentication.Authenticator, cc.Authorization.Authorizer)
		// TODO: handle stoppedCh and listenerStoppedCh returned by c.SecureServing.Serve
		if _, _, err := cc.SecureServing.Serve(handler, 0, ctx.Done()); err != nil {
			// fail early for secure handlers, removing the old error loop from above
			return fmt.Errorf(&quot;failed to start secure server: %v&quot;, err)
		}
	}

	// Start all informers.
	cc.InformerFactory.Start(ctx.Done())
	// DynInformerFactory can be nil in tests.
	if cc.DynInformerFactory != nil {
		cc.DynInformerFactory.Start(ctx.Done())
	}

	// Wait for all caches to sync before scheduling.
	cc.InformerFactory.WaitForCacheSync(ctx.Done())
	// DynInformerFactory can be nil in tests.
	if cc.DynInformerFactory != nil {
		cc.DynInformerFactory.WaitForCacheSync(ctx.Done())
	}

	// If leader election is enabled, runCommand via LeaderElector until done and exit.
	if cc.LeaderElection != nil {
		cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{
			OnStartedLeading: func(ctx context.Context) {
				close(waitingForLeader)
				sched.Run(ctx)
			},
			OnStoppedLeading: func() {
				select {
				case &lt;-ctx.Done():
					// We were asked to terminate. Exit 0.
					klog.InfoS(&quot;Requested to terminate, exiting&quot;)
					os.Exit(0)
				default:
					// We lost the lock.
					klog.ErrorS(nil, &quot;Leaderelection lost&quot;)
					klog.FlushAndExit(klog.ExitFlushTimeout, 1)
				}
			},
		}
		leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection)
		if err != nil {
			return fmt.Errorf(&quot;couldn't create leader elector: %v&quot;, err)
		}

		leaderElector.Run(ctx)

		return fmt.Errorf(&quot;lost lease&quot;)
	}

	// Leader election is disabled, so runCommand inline until done.
	close(waitingForLeader)
	sched.Run(ctx)
	return fmt.Errorf(&quot;finished without leader elect&quot;)
}
</code></pre>
<p>上面看到了 <code>runCommend</code> 是作为启动 <em>scheduler</em> 的工作，那么通过参数构建一个 <em>scheduler</em> 则是在 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L298-L355" target="_blank"
   rel="noopener nofollow noreferrer" >Setup</a> 中完成的。</p>
<pre><code class="language-go">// Setup creates a completed config and a scheduler based on the command args and options
func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) {
	if cfg, err := latest.Default(); err != nil {
		return nil, nil, err
	} else {
		opts.ComponentConfig = cfg
	}
	// 验证参数
	if errs := opts.Validate(); len(errs) &gt; 0 {
		return nil, nil, utilerrors.NewAggregate(errs)
	}
	// 构建一个config对象
	c, err := opts.Config()
	if err != nil {
		return nil, nil, err
	}

	// 返回一个config对象，包含了scheduler所需的配置，如informer，leader selection
	cc := c.Complete()

	outOfTreeRegistry := make(runtime.Registry)
	for _, option := range outOfTreeRegistryOptions {
		if err := option(outOfTreeRegistry); err != nil {
			return nil, nil, err
		}
	}

	recorderFactory := getRecorderFactory(&amp;cc)
	completedProfiles := make([]kubeschedulerconfig.KubeSchedulerProfile, 0)
	// 创建出来的scheduler
	sched, err := scheduler.New(cc.Client,
		cc.InformerFactory,
		cc.DynInformerFactory,
		recorderFactory,
		ctx.Done(),
		scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion),
		scheduler.WithKubeConfig(cc.KubeConfig),
		scheduler.WithProfiles(cc.ComponentConfig.Profiles...),
		scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore),
		scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry),
		scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds),
		scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds),
		scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration),
		scheduler.WithExtenders(cc.ComponentConfig.Extenders...),
		scheduler.WithParallelism(cc.ComponentConfig.Parallelism),
		scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) {
			// Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging
			completedProfiles = append(completedProfiles, profile)
		}),
	)
	if err != nil {
		return nil, nil, err
	}
	if err := options.LogOrWriteConfig(opts.WriteConfigTo, &amp;cc.ComponentConfig, completedProfiles); err != nil {
		return nil, nil, err
	}

	return &amp;cc, sched, nil
}
</code></pre>
<p>上面了解到了 <em>scheduler</em> 是如何被构建出来的，下面就看看 构建时参数是如何传递进来的，而对象 option就是对应需要的配置结构，而 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/options/options.go#L203-L243" target="_blank"
   rel="noopener nofollow noreferrer" >ApplyTo</a> 则是将启动时传入的参数转化为构建 <em>scheduler</em> 所需的配置。</p>
<blockquote>
<p>对于Deprecated flags可以参考官方对于kube-scheduler启动参数的说明 <sup><a href="#5">[5]</a></sup></p>
<p>对于如何编写一个scheduler config请参考 <a href="#6">[6]</a> 与 <a href="#7">[7]</a></p>
</blockquote>
<pre><code class="language-go">func (o *Options) ApplyTo(c *schedulerappconfig.Config) error {
	if len(o.ConfigFile) == 0 {
		// 在没有指定 --config时会找到 Deprecated flags:参数
        // 通过kube-scheduler --help可以看到这些被弃用的参数
		o.ApplyDeprecated()
		o.ApplyLeaderElectionTo(o.ComponentConfig)
		c.ComponentConfig = *o.ComponentConfig
	} else {
        // 这里就是指定了--config
		cfg, err := loadConfigFromFile(o.ConfigFile)
		if err != nil {
			return err
		}
		// 这里会将leader选举的参数附加到配置中
		o.ApplyLeaderElectionTo(cfg)

		if err := validation.ValidateKubeSchedulerConfiguration(cfg); err != nil {
			return err
		}

		c.ComponentConfig = *cfg
	}

	if err := o.SecureServing.ApplyTo(&amp;c.SecureServing, &amp;c.LoopbackClientConfig); err != nil {
		return err
	}
	if o.SecureServing != nil &amp;&amp; (o.SecureServing.BindPort != 0 || o.SecureServing.Listener != nil) {
		if err := o.Authentication.ApplyTo(&amp;c.Authentication, c.SecureServing, nil); err != nil {
			return err
		}
		if err := o.Authorization.ApplyTo(&amp;c.Authorization); err != nil {
			return err
		}
	}
	o.Metrics.Apply()

	// Apply value independently instead of using ApplyDeprecated() because it can't be configured via ComponentConfig.
	if o.Deprecated != nil {
		c.PodMaxInUnschedulablePodsDuration = o.Deprecated.PodMaxInUnschedulablePodsDuration
	}

	return nil
}
</code></pre>
<p><code>Setup</code> 后会new一个 <code>schedueler</code> , <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L234-L333" target="_blank"
   rel="noopener nofollow noreferrer" >New</a> 则是这个动作，在里面可以看出，会初始化一些informer与 Pod的list等操作。</p>
<pre><code class="language-go">func New(client clientset.Interface,
	informerFactory informers.SharedInformerFactory,
	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
	recorderFactory profile.RecorderFactory,
	stopCh &lt;-chan struct{},
	opts ...Option) (*Scheduler, error) {

	stopEverything := stopCh
	if stopEverything == nil {
		stopEverything = wait.NeverStop
	}

	options := defaultSchedulerOptions // 默认调度策略，如percentageOfNodesToScore
	for _, opt := range opts {
		opt(&amp;options) // opt 是传入的函数，会返回一个schedulerOptions即相应的一些配置
	}

	if options.applyDefaultProfile { // 这个是个bool类型，默认scheduler会到这里
        // Profile包含了调度器的名称与调度器在两个过程中使用的插件
		var versionedCfg v1beta3.KubeSchedulerConfiguration
		scheme.Scheme.Default(&amp;versionedCfg)
		cfg := schedulerapi.KubeSchedulerConfiguration{} // 初始化一个配置，这个是--config传入的类型。因为默认的调度策略会初始化
        // convert 会将in转为out即versionedCfg转换为cfg
		if err := scheme.Scheme.Convert(&amp;versionedCfg, &amp;cfg, nil); err != nil {
			return nil, err
		}
		options.profiles = cfg.Profiles
	}

	registry := frameworkplugins.NewInTreeRegistry() // 调度框架的注册
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}

	metrics.Register() // 指标类

	extenders, err := buildExtenders(options.extenders, options.profiles)
	if err != nil {
		return nil, fmt.Errorf(&quot;couldn't build extenders: %w&quot;, err)
	}

	podLister := informerFactory.Core().V1().Pods().Lister()
	nodeLister := informerFactory.Core().V1().Nodes().Lister()

	// The nominator will be passed all the way to framework instantiation.
	nominator := internalqueue.NewPodNominator(podLister)
	snapshot := internalcache.NewEmptySnapshot()
	clusterEventMap := make(map[framework.ClusterEvent]sets.String)

	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)
	if err != nil {
		return nil, fmt.Errorf(&quot;initializing profiles: %v&quot;, err)
	}

	if len(profiles) == 0 {
		return nil, errors.New(&quot;at least one profile is required&quot;)
	}

	podQueue := internalqueue.NewSchedulingQueue(
		profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
		informerFactory,
		internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
		internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
		internalqueue.WithPodNominator(nominator),
		internalqueue.WithClusterEventMap(clusterEventMap),
		internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
	)

	schedulerCache := internalcache.New(durationToExpireAssumedPod, stopEverything)

	// Setup cache debugger.
	debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue)
	debugger.ListenForSignal(stopEverything)

	sched := newScheduler(
		schedulerCache,
		extenders,
		internalqueue.MakeNextPodFunc(podQueue),
		MakeDefaultErrorFunc(client, podLister, podQueue, schedulerCache),
		stopEverything,
		podQueue,
		profiles,
		client,
		snapshot,
		options.percentageOfNodesToScore,
	)
	// 这个就是controller中onAdd等那三个必须的事件函数
	addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap))

	return sched, nil
}
</code></pre>
<p>接下来会启动这个 <em>scheduler</em>， 在上面我们看到 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L72-L114" target="_blank"
   rel="noopener nofollow noreferrer" >NewSchedulerCommand</a> 构建了一个cobra.Commond对象， <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L117-L142" target="_blank"
   rel="noopener nofollow noreferrer" >runCommand()</a> 最终会返回个 Run，而这个Run就是启动这个 <em>sche</em> 的。</p>
<p>下面这个 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L336-L340" target="_blank"
   rel="noopener nofollow noreferrer" >run</a> 是 <em>sche</em> 的运行，他运行并watch资源，直到上下文完成。</p>
<pre><code class="language-go">func (sched *Scheduler) Run(ctx context.Context) {
	sched.SchedulingQueue.Run()

	// We need to start scheduleOne loop in a dedicated goroutine,
	// because scheduleOne function hangs on getting the next item
	// from the SchedulingQueue.
	// If there are no new pods to schedule, it will be hanging there
	// and if done in this goroutine it will be blocking closing
	// SchedulingQueue, in effect causing a deadlock on shutdown.
	go wait.UntilWithContext(ctx, sched.scheduleOne, 0)

	&lt;-ctx.Done()
	sched.SchedulingQueue.Close()
}
</code></pre>
<p>而调用这个 <em>Run</em> 的部分则是作为server的 <em>kube-scheduler</em> 中的 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L145-L237" target="_blank"
   rel="noopener nofollow noreferrer" >run</a></p>
<pre><code class="language-go">// Run executes the scheduler based on the given configuration. It only returns on error or when context is done.
func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error {
	// To help debugging, immediately log version
	klog.InfoS(&quot;Starting Kubernetes Scheduler&quot;, &quot;version&quot;, version.Get())

	klog.InfoS(&quot;Golang settings&quot;, &quot;GOGC&quot;, os.Getenv(&quot;GOGC&quot;), &quot;GOMAXPROCS&quot;, os.Getenv(&quot;GOMAXPROCS&quot;), &quot;GOTRACEBACK&quot;, os.Getenv(&quot;GOTRACEBACK&quot;))

	// Configz registration.
	if cz, err := configz.New(&quot;componentconfig&quot;); err == nil {
		cz.Set(cc.ComponentConfig)
	} else {
		return fmt.Errorf(&quot;unable to register configz: %s&quot;, err)
	}

	// Start events processing pipeline.
	cc.EventBroadcaster.StartRecordingToSink(ctx.Done())
	defer cc.EventBroadcaster.Shutdown()

	// Setup healthz checks.
	var checks []healthz.HealthChecker
	if cc.ComponentConfig.LeaderElection.LeaderElect {
		checks = append(checks, cc.LeaderElection.WatchDog)
	}

	waitingForLeader := make(chan struct{})
	isLeader := func() bool {
		select {
		case _, ok := &lt;-waitingForLeader:
			// if channel is closed, we are leading
			return !ok
		default:
			// channel is open, we are waiting for a leader
			return false
		}
	}

	// Start up the healthz server.
	if cc.SecureServing != nil {
		handler := buildHandlerChain(newHealthzAndMetricsHandler(&amp;cc.ComponentConfig, cc.InformerFactory, isLeader, checks...), cc.Authentication.Authenticator, cc.Authorization.Authorizer)
		// TODO: handle stoppedCh and listenerStoppedCh returned by c.SecureServing.Serve
		if _, _, err := cc.SecureServing.Serve(handler, 0, ctx.Done()); err != nil {
			// fail early for secure handlers, removing the old error loop from above
			return fmt.Errorf(&quot;failed to start secure server: %v&quot;, err)
		}
	}

	// Start all informers.
	cc.InformerFactory.Start(ctx.Done())
	// DynInformerFactory can be nil in tests.
	if cc.DynInformerFactory != nil {
		cc.DynInformerFactory.Start(ctx.Done())
	}

	// Wait for all caches to sync before scheduling.
	cc.InformerFactory.WaitForCacheSync(ctx.Done())
	// DynInformerFactory can be nil in tests.
	if cc.DynInformerFactory != nil {
		cc.DynInformerFactory.WaitForCacheSync(ctx.Done())
	}

	// If leader election is enabled, runCommand via LeaderElector until done and exit.
	if cc.LeaderElection != nil {
		cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{
			OnStartedLeading: func(ctx context.Context) {
				close(waitingForLeader)
				sched.Run(ctx)
			},
			OnStoppedLeading: func() {
				select {
				case &lt;-ctx.Done():
					// We were asked to terminate. Exit 0.
					klog.InfoS(&quot;Requested to terminate, exiting&quot;)
					os.Exit(0)
				default:
					// We lost the lock.
					klog.ErrorS(nil, &quot;Leaderelection lost&quot;)
					klog.FlushAndExit(klog.ExitFlushTimeout, 1)
				}
			},
		}
		leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection)
		if err != nil {
			return fmt.Errorf(&quot;couldn't create leader elector: %v&quot;, err)
		}

		leaderElector.Run(ctx)

		return fmt.Errorf(&quot;lost lease&quot;)
	}

	// Leader election is disabled, so runCommand inline until done.
	close(waitingForLeader)
	sched.Run(ctx)
	return fmt.Errorf(&quot;finished without leader elect&quot;)
}
</code></pre>
<p>而上面的 <em>server.Run</em> 会被 <code>runCommand</code> 也就是在 <code>NewSchedulerCommand</code> 时被返回，在 <code>kube-scheduler</code> 的入口文件中被执行。</p>
<pre><code class="language-go">cc, sched, err := Setup(ctx, opts, registryOptions...)
if err != nil {
    return err
}

return Run(ctx, cc, sched)
</code></pre>
<p>至此，整个 <code>kube-scheduler</code> 启动流就分析完了，这个的流程可以用下图表示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220718174104043.png" alt="image-20220718174104043" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图2：scheduler server运行流程</center>
<blockquote>
<p>Reference</p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" target="_blank"
   rel="noopener nofollow noreferrer" >kube scheduler</a></p>
<p><sup id="2">[2]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_algorithm.md#filtering-the-nodes" target="_blank"
   rel="noopener nofollow noreferrer" >Scheduler Algorithm in Kubernetes</a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling framework</a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#permit" target="_blank"
   rel="noopener nofollow noreferrer" >permit</a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank"
   rel="noopener nofollow noreferrer" >kube-scheduler parmater</a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/docs/reference/config-api/kube-scheduler-config.v1beta3/" target="_blank"
   rel="noopener nofollow noreferrer" >kube-scheduler config.v1beta3/</a></p>
<p><sup id="7">[7]</sup> <a href="https://kubernetes.io/docs/reference/scheduling/config/" target="_blank"
   rel="noopener nofollow noreferrer" >kube-scheduler config</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes 4A - Admission Control源码解析</title>
      <link>https://www.oomkill.com/2022/07/ch33-admission-webhook/</link>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/07/ch33-admission-webhook/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p>本文是关于Kubernetes 4A解析的第三章</p>
<ul>
<li><a href="https://cylonchau.github.io/kubernetes-authentication.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authentication源码解析</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-authorization.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authorization源码解析</a></li>
<li>深入理解Kubernetes 4A - Admission Control源码解析</li>
<li><a href="https://cylonchau.github.io/kubernetes-auditing.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Audit源码解析</a></li>
</ul>
<p>所有关于Kubernetes 4A部分代码上传至仓库 github.com/cylonchau/hello-k8s-4A</p>
</blockquote>
<p>如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大</p>
<h2 id="background">BACKGROUND</h2>
<p><strong>admission controllers的特点</strong>：</p>
<ul>
<li>可定制性：准入功能可针对不同的场景进行调整。</li>
<li>可预防性：审计则是为了检测问题，而准入控制器可以预防问题发生</li>
<li>可扩展性：在kubernetes自有的验证机制外，增加了另外的防线，弥补了RBAC仅能对资源提供安全保证。</li>
</ul>
<p>下图，显示了用户操作资源的流程，可以看出 <em>admission controllers</em> 作用是在通过身份验证资源持久化之前起到拦截作用。在准入控制器的加入会使kubernetes增加了更高级的安全功能。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/admission-controller-phases.png" alt="准入控制器阶段" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图</center>
<center><em>Source：</em>https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/</center>
<p>这里找到一个大佬博客画的图，通过两张图可以很清晰的了解到admission webhook流程，与官方给出的不一样的地方在于，这里清楚地定位了kubernetes admission webhook 处于准入控制中，RBAC之后，push 之前。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/vhesGDFN3dLdzXwS7vzPdXkI3aglQYZgGhjc-Cx_boaV6URKFFoe8mFRZZUuJyGHywa_bOkeUlIkm-nJkCVMHPk9dr2dXFwNzAQJKzft2phsTcEDjdObjmugBcYtpdPLpLIYuIGzeFYvtsR2Lw.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图（详细）</center>
<center><em>Source：</em>https://www.armosec.io/blog/kubernetes-admission-controller/</center>
<h3 id="两种控制器有什么区别">两种控制器有什么区别？</h3>
<p>根据官方提供的说法是</p>
<blockquote>
<p>Mutating controllers may modify related objects to the requests they admit; validating controllers may not</p>
</blockquote>
<p>从结构图中也可以看出，<code>validating </code> 是在持久化之前，而 <code>Mutating </code> 是在结构验证前，根据这些特性我们可以使用 <code>Mutating</code> 修改这个资源对象内容（如增加验证的信息），在 <code>validating</code> 中验证是否合法。</p>
<h3 id="composition-of-admission-controllers">composition of admission controllers</h3>
<p>kubernetes中的  <em>admission controllers</em> 由两部分组成：</p>
<ul>
<li>内置在APIServer中的准入控制器 <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do" target="_blank"
   rel="noopener nofollow noreferrer" >build-in li.st</a></li>
<li>特殊的控制器；也是内置在APIServer中，但提供一些自定义的功能
<ul>
<li>MutatingAdmission</li>
<li>ValidatingAdmission</li>
</ul>
</li>
</ul>
<p>Mutating 控制器可以修改他们处理的资源对象，Validating 控制器不会。当在任何一个阶段中的任何控制器拒绝这个了请求，则会立即拒绝整个请求，并将错误返回。</p>
<h3 id="admission-webhook">admission webhook</h3>
<p>由于准入控制器是内置在 <code>kube-apiserver</code> 中的，这种情况下就限制了admission controller的可扩展性。在这种背景下，kubernetes提供了一种可扩展的准入控制器 <code>extensible admission controllers</code>，这种行为叫做动态准入控制 <code>Dynamic Admission Control</code>，而提供这个功能的就是 <code>admission webhook</code> 。</p>
<p><code>admission webhook</code>  通俗来讲就是 HTTP 回调，通过定义一个http server，接收准入请求并处理。用户可以通过kubernetes提供的两种类型的 <code>admission webhook</code>，<em>validating admission webhook</em> 和 <em>mutating admission webhook</em>。来完成自定义的准入策略的处理。</p>
<p>webhook 就是</p>
<blockquote>
<p>注：从上面的流程图也可以看出，admission webhook 也是有顺序的。首先调用mutating webhook，然后会调用validating webhook。</p>
</blockquote>
<h2 id="如何使用准入控制器">如何使用准入控制器</h2>
<p><strong>使用条件</strong>：kubernetes v1.16 使用 <code>admissionregistration.k8s.io/v1</code> ；kubernetes v1.9 使用 <code>admissionregistration.k8s.io/v1beta1</code>。</p>
<p><strong>如何在集群中开启准入控制器?</strong> ：查看kube-apiserver 的启动参数 <code>--enable-admission-plugins</code> ；通过该参数来配置要启动的准入控制器，如 <code>--enable-admission-plugins=NodeRestriction</code> 多个准入控制器以 <code>,</code> 分割，顺序无关紧要。 反之使用 <code>--disable-admission-plugins</code> 参数可以关闭相应的准入控制器（Refer to <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options" target="_blank"
   rel="noopener nofollow noreferrer" >apiserver opts</a>）。</p>
<p>通过 <code>kubectl</code> 命令可以看到，当前kubernetes集群所支持准入控制器的版本</p>
<pre><code class="language-bash">$ kubectl api-versions | grep admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1beta1
</code></pre>
<h2 id="webhook工作原理">webhook工作原理</h2>
<p>通过上面的学习，已经了解到了两种webhook的工作原理如下所示：</p>
<blockquote>
<p>mutating webhook，会在持久化前拦截在 MutatingWebhookConfiguration 中定义的规则匹配的请求。MutatingAdmissionWebhook 通过向 mutating webhook 服务器发送准入请求来执行验证。</p>
<p>validaing webhook，会在持久化前拦截在 <code>ValidatingWebhookConfiguration</code> 中定义的规则匹配的请求。ValidatingAdmissionWebhook 通过将准入请求发送到 validating webhook server来执行验证。</p>
</blockquote>
<p>那么接下来将从源码中看这个在这个工作流程中，究竟做了些什么？</p>
<h3 id="资源类型">资源类型</h3>
<p>对于 1.9 版本之后，也就是 <code>v1</code> 版本 ，admission 被定义在 <a href="https://github.com/kubernetes/kubernetes/blob/v1.18.20/staging/src/k8s.io/api/admissionregistration/v1/types.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io\api\admissionregistration\v1\types.go</a> ，大同小异，因为本地只有1.18集群，所以以这个讲解。</p>
<p>对于 <code>Validating Webhook</code> 来讲实现主要都在webhook中</p>
<pre><code class="language-go">type ValidatingWebhookConfiguration struct {
    // 每个api必须包含下列的metadata，这个是kubernetes规范，可以在注释中的url看到相关文档
	metav1.TypeMeta `json:&quot;,inline&quot;`
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`
	// Webhooks在这里被表示为[]ValidatingWebhook，表示我们可以注册多个
	// +optional
	// +patchMergeKey=name
	// +patchStrategy=merge
	Webhooks []ValidatingWebhook `json:&quot;webhooks,omitempty&quot; patchStrategy:&quot;merge&quot; patchMergeKey:&quot;name&quot; protobuf:&quot;bytes,2,rep,name=Webhooks&quot;`
}
</code></pre>
<p>webhook，则是对这种类型的webhook提供的操作、资源等。对于这部分不做过多的注释了，因为这里本身为kubernetes API资源，官网有很详细的例子与说明。这里更多字段的意思的可以参考官方 <a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#webhook-configuration" target="_blank"
   rel="noopener nofollow noreferrer" >doc</a></p>
<pre><code class="language-go">type ValidatingWebhook struct {
	//  admission webhook的名词，Required
	Name string `json:&quot;name&quot; protobuf:&quot;bytes,1,opt,name=name&quot;`

	// ClientConfig 定义了与webhook通讯的方式 Required
	ClientConfig WebhookClientConfig `json:&quot;clientConfig&quot; protobuf:&quot;bytes,2,opt,name=clientConfig&quot;`

	// rule表示了webhook对于哪些资源及子资源的操作进行关注
	Rules []RuleWithOperations `json:&quot;rules,omitempty&quot; protobuf:&quot;bytes,3,rep,name=rules&quot;`

	// FailurePolicy 对于无法识别的value将如何处理，allowed/Ignore optional
	FailurePolicy *FailurePolicyType `json:&quot;failurePolicy,omitempty&quot; protobuf:&quot;bytes,4,opt,name=failurePolicy,casttype=FailurePolicyType&quot;`

	// matchPolicy 定义了如何使用“rules”列表来匹配传入的请求。
	MatchPolicy *MatchPolicyType `json:&quot;matchPolicy,omitempty&quot; protobuf:&quot;bytes,9,opt,name=matchPolicy,casttype=MatchPolicyType&quot;`
	NamespaceSelector *metav1.LabelSelector `json:&quot;namespaceSelector,omitempty&quot; protobuf:&quot;bytes,5,opt,name=namespaceSelector&quot;`
	SideEffects *SideEffectClass `json:&quot;sideEffects&quot; protobuf:&quot;bytes,6,opt,name=sideEffects,casttype=SideEffectClass&quot;`
	AdmissionReviewVersions []string `json:&quot;admissionReviewVersions&quot; protobuf:&quot;bytes,8,rep,name=admissionReviewVersions&quot;`
}
</code></pre>
<p>到这里了解了一个webhook资源的定义，那么这个如何使用呢？通过 <code>Find Usages</code> 找到一个 <a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/accessors.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/admission/plugin/webhook/accessors.go</a> 在使用它。这里没有注释，但在结构上可以看出，包含客户端与一系列选择器组成</p>
<pre><code class="language-go">type mutatingWebhookAccessor struct {
	*v1.MutatingWebhook
	uid               string
	configurationName string

	initObjectSelector sync.Once
	objectSelector     labels.Selector
	objectSelectorErr  error

	initNamespaceSelector sync.Once
	namespaceSelector     labels.Selector
	namespaceSelectorErr  error

	initClient sync.Once
	client     *rest.RESTClient
	clientErr  error
}
</code></pre>
<p><code>accessor</code> 因为包含了整个webhookconfig定义的一些动作（这里个人这么觉得）。</p>
<p><code>accessor.go</code> 下面 有一个 <code>GetRESTClient</code> 方法 ，通过这里可以看出，这里做的就是使用根据 <code>accessor</code> 构造一个客户端。</p>
<pre><code class="language-go">func (m *mutatingWebhookAccessor) GetRESTClient(clientManager *webhookutil.ClientManager) (*rest.RESTClient, error) {
	m.initClient.Do(func() {
		m.client, m.clientErr = clientManager.HookClient(hookClientConfigForWebhook(m))
	})
	return m.client, m.clientErr
}
</code></pre>
<p>到这步骤已经没必要往下看了，因已经知道这里是请求webhook前的步骤了，下面就是何时请求了。</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/dispatcher.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io\apiserver\pkg\admission\plugin\webhook\validating\dispatcher.go</a> 下面有两个方法，Dispatch去请求我们自己定义的webhook</p>
<pre><code class="language-go">func (d *validatingDispatcher) Dispatch(ctx context.Context, attr admission.Attributes, o admission.ObjectInterfaces, hooks []webhook.WebhookAccessor) error {
	var relevantHooks []*generic.WebhookInvocation
	// Construct all the versions we need to call our webhooks
	versionedAttrs := map[schema.GroupVersionKind]*generic.VersionedAttributes{}
	for _, hook := range hooks {
		invocation, statusError := d.plugin.ShouldCallHook(hook, attr, o)
		if statusError != nil {
			return statusError
		}
		if invocation == nil {
			continue
		}
		relevantHooks = append(relevantHooks, invocation)
		// If we already have this version, continue
		if _, ok := versionedAttrs[invocation.Kind]; ok {
			continue
		}
		versionedAttr, err := generic.NewVersionedAttributes(attr, invocation.Kind, o)
		if err != nil {
			return apierrors.NewInternalError(err)
		}
		versionedAttrs[invocation.Kind] = versionedAttr
	}

	if len(relevantHooks) == 0 {
		// no matching hooks
		return nil
	}

	// Check if the request has already timed out before spawning remote calls
	select {
	case &lt;-ctx.Done():
		// parent context is canceled or timed out, no point in continuing
		return apierrors.NewTimeoutError(&quot;request did not complete within requested timeout&quot;, 0)
	default:
	}

	wg := sync.WaitGroup{}
	errCh := make(chan error, len(relevantHooks))
	wg.Add(len(relevantHooks))
    // 循环所有相关的注册的hook
	for i := range relevantHooks {
		go func(invocation *generic.WebhookInvocation) {
			defer wg.Done()
            // invacation 中有一个 Accessor,Accessor注册了一个相关的webhookconfig
            // 也就是我们 kubectl -f 注册进来的那个webhook的相关配置
			hook, ok := invocation.Webhook.GetValidatingWebhook()
			if !ok {
				utilruntime.HandleError(fmt.Errorf(&quot;validating webhook dispatch requires v1.ValidatingWebhook, but got %T&quot;, hook))
				return
			}
			versionedAttr := versionedAttrs[invocation.Kind]
			t := time.Now()
            // 调用了callHook去请求我们自定义的webhook
			err := d.callHook(ctx, hook, invocation, versionedAttr)
			ignoreClientCallFailures := hook.FailurePolicy != nil &amp;&amp; *hook.FailurePolicy == v1.Ignore
			rejected := false
			if err != nil {
				switch err := err.(type) {
				case *webhookutil.ErrCallingWebhook:
					if !ignoreClientCallFailures {
						rejected = true
						admissionmetrics.Metrics.ObserveWebhookRejection(hook.Name, &quot;validating&quot;, string(versionedAttr.Attributes.GetOperation()), admissionmetrics.WebhookRejectionCallingWebhookError, 0)
					}
				case *webhookutil.ErrWebhookRejection:
					rejected = true
					admissionmetrics.Metrics.ObserveWebhookRejection(hook.Name, &quot;validating&quot;, string(versionedAttr.Attributes.GetOperation()), admissionmetrics.WebhookRejectionNoError, int(err.Status.ErrStatus.Code))
				default:
					rejected = true
					admissionmetrics.Metrics.ObserveWebhookRejection(hook.Name, &quot;validating&quot;, string(versionedAttr.Attributes.GetOperation()), admissionmetrics.WebhookRejectionAPIServerInternalError, 0)
				}
			}
			admissionmetrics.Metrics.ObserveWebhook(time.Since(t), rejected, versionedAttr.Attributes, &quot;validating&quot;, hook.Name)
			if err == nil {
				return
			}

			if callErr, ok := err.(*webhookutil.ErrCallingWebhook); ok {
				if ignoreClientCallFailures {
					klog.Warningf(&quot;Failed calling webhook, failing open %v: %v&quot;, hook.Name, callErr)
					utilruntime.HandleError(callErr)
					return
				}

				klog.Warningf(&quot;Failed calling webhook, failing closed %v: %v&quot;, hook.Name, err)
				errCh &lt;- apierrors.NewInternalError(err)
				return
			}

			if rejectionErr, ok := err.(*webhookutil.ErrWebhookRejection); ok {
				err = rejectionErr.Status
			}
			klog.Warningf(&quot;rejected by webhook %q: %#v&quot;, hook.Name, err)
			errCh &lt;- err
		}(relevantHooks[i])
	}
	wg.Wait()
	close(errCh)

	var errs []error
	for e := range errCh {
		errs = append(errs, e)
	}
	if len(errs) == 0 {
		return nil
	}
	if len(errs) &gt; 1 {
		for i := 1; i &lt; len(errs); i++ {
			// TODO: merge status errors; until then, just return the first one.
			utilruntime.HandleError(errs[i])
		}
	}
	return errs[0]
}

</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/6adee9d4fb7a0cf3eec148448792e2ab091e1720/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/dispatcher.go#L216-L301" target="_blank"
   rel="noopener nofollow noreferrer" >callHook</a> 可以理解为真正去请求我们自定义的webhook服务的动作</p>
<pre><code class="language-go">func (d *validatingDispatcher) callHook(ctx context.Context, h *v1.ValidatingWebhook, invocation *generic.WebhookInvocation, attr *generic.VersionedAttributes) error {
   if attr.Attributes.IsDryRun() {
      if h.SideEffects == nil {
         return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: fmt.Errorf(&quot;Webhook SideEffects is nil&quot;)}
      }
      if !(*h.SideEffects == v1.SideEffectClassNone || *h.SideEffects == v1.SideEffectClassNoneOnDryRun) {
         return webhookerrors.NewDryRunUnsupportedErr(h.Name)
      }
   }

   uid, request, response, err := webhookrequest.CreateAdmissionObjects(attr, invocation)
   if err != nil {
      return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err}
   }
   // 发生请求，可以看到，这里从上面的讲到的地方获取了一个客户端
   client, err := invocation.Webhook.GetRESTClient(d.cm)
   if err != nil {
      return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err}
   }
   trace := utiltrace.New(&quot;Call validating webhook&quot;,
      utiltrace.Field{&quot;configuration&quot;, invocation.Webhook.GetConfigurationName()},
      utiltrace.Field{&quot;webhook&quot;, h.Name},
      utiltrace.Field{&quot;resource&quot;, attr.GetResource()},
      utiltrace.Field{&quot;subresource&quot;, attr.GetSubresource()},
      utiltrace.Field{&quot;operation&quot;, attr.GetOperation()},
      utiltrace.Field{&quot;UID&quot;, uid})
   defer trace.LogIfLong(500 * time.Millisecond)

   // 这里设置超时，超时时长就是在yaml资源清单中设置的那个值
   if h.TimeoutSeconds != nil {
      var cancel context.CancelFunc
      ctx, cancel = context.WithTimeout(ctx, time.Duration(*h.TimeoutSeconds)*time.Second)
      defer cancel()
   }
   // 直接用post请求我们自己定义的webhook接口
   r := client.Post().Body(request)

   // if the context has a deadline, set it as a parameter to inform the backend
   if deadline, hasDeadline := ctx.Deadline(); hasDeadline {
      // compute the timeout
      if timeout := time.Until(deadline); timeout &gt; 0 {
         // if it's not an even number of seconds, round up to the nearest second
         if truncated := timeout.Truncate(time.Second); truncated != timeout {
            timeout = truncated + time.Second
         }
         // set the timeout
         r.Timeout(timeout)
      }
   }

   if err := r.Do(ctx).Into(response); err != nil {
      return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err}
   }
   trace.Step(&quot;Request completed&quot;)

   result, err := webhookrequest.VerifyAdmissionResponse(uid, false, response)
   if err != nil {
      return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err}
   }

   for k, v := range result.AuditAnnotations {
      key := h.Name + &quot;/&quot; + k
      if err := attr.Attributes.AddAnnotation(key, v); err != nil {
         klog.Warningf(&quot;Failed to set admission audit annotation %s to %s for validating webhook %s: %v&quot;, key, v, h.Name, err)
      }
   }
   if result.Allowed {
      return nil
   }
   return &amp;webhookutil.ErrWebhookRejection{Status: webhookerrors.ToStatusErr(h.Name, result.Result)}
}
</code></pre>
<p>走到这里基本上对 <code>admission webhook</code> 有了大致的了解，可以知道这个操作是由 apiserver 完成的。下面就实际操作下自定义一个webhook。</p>
<p>这里还有两个概念，就是请求参数 <a href="https://github.com/kubernetes/kubernetes/blob/6adee9d4fb7a0cf3eec148448792e2ab091e1720/staging/src/k8s.io/api/admission/v1/types.go#L40-L113" target="_blank"
   rel="noopener nofollow noreferrer" >AdmissionRequest</a> 和相应参数 <a href="https://github.com/kubernetes/kubernetes/blob/6adee9d4fb7a0cf3eec148448792e2ab091e1720/staging/src/k8s.io/api/admission/v1/types.go#L116-L150" target="_blank"
   rel="noopener nofollow noreferrer" >AdmissionResponse</a>，这些可以在 <code>callHook</code> 中看到，这两个参数被定义在 <a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/admission/v1/types.go#L29-L37" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io\api\admission\v1\types.go</a> ；这两个参数也就是我们在自定义 webhook 时需要处理接收到的body的结构，以及我们响应内容数据结构。</p>
<h2 id="如何编写一个自定义的admission-webhook">如何编写一个自定义的admission webhook</h2>
<p>通过上面的学习了解到了，自定义的webhook就是做为kubernetes提供给用户两种admission controller来验证自定义业务的一个中间件 admission webhook。本质上他是一个HTTP Server，用户可以使用任何语言来完成这部分功能。当然，如果涉及到需要对kubernetes集群资源操作的话，还是建议使用kubernetes官方提供了SDK的编程语言来完成自定义的webhook。</p>
<p>那么完成一个自定义admission webhook需要两个步骤：</p>
<ul>
<li>将相关的webhook config注册给kubernetes，也就是让kubernetes知道你的webhook</li>
<li>准备一个http server来处理 apiserver发过来验证的信息</li>
</ul>
<blockquote>
<p>注：这里使用go net/http包，本身不区分方法处理HTTP的何种请求，如果用其他框架实现的，如django，需要指定对应方法需要为POST</p>
</blockquote>
<h3 id="向kubernetes注册webhook对象">向kubernetes注册webhook对象</h3>
<p>kubernetes提供的两种类型可自定义的准入控制器，和其他资源一样，可以利用资源清单，动态配置那些资源要被adminssion webhook处理。 kubernetes将这种形式抽象为两种资源：</p>
<ul>
<li>
<p>ValidatingWebhookConfiguration</p>
</li>
<li>
<p>MutatingWebhookConfiguration</p>
</li>
</ul>
<h4 id="validatingadmission">ValidatingAdmission</h4>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: &quot;pod-policy.example.com&quot;
webhooks:
- name: &quot;pod-policy.example.com&quot;
  rules:
  - apiGroups:   [&quot;&quot;] # 拦截资源的Group &quot;&quot; 表示 core。&quot;*&quot; 表示所有。
    apiVersions: [&quot;v1&quot;] # 拦截资源的版本
    operations:  [&quot;CREATE&quot;] # 什么请求下拦截
    resources:   [&quot;pods&quot;]  # 拦截什么资源
    scope:       &quot;Namespaced&quot; # 生效的范围，cluster还是namespace &quot;*&quot;表示没有范围限制。
  clientConfig: # 我们部署的webhook服务，
    service: # service是在cluster-in模式下
      namespace: &quot;example-namespace&quot;
      name: &quot;example-service&quot;
      port: 443 # 服务的端口
      path: &quot;/validate&quot; # path是对应用于验证的接口
    # caBundle是提供给 admission webhook CA证书  
    caBundle: &quot;Ci0tLS0tQk...&lt;base64-encoded PEM bundle containing the CA that signed the webhook's serving certificate&gt;...tLS0K&quot;
  admissionReviewVersions: [&quot;v1&quot;, &quot;v1beta1&quot;]
  sideEffects: None
  timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间
</code></pre>
<h4 id="mutatingadmission">MutatingAdmission</h4>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: &quot;valipod-policy.example.com&quot;
webhooks:
- name: &quot;valipod-policy.example.com&quot;
  rules:
    - apiGroups:   [&quot;apps&quot;] # 拦截资源的Group &quot;&quot; 表示 core。&quot;*&quot; 表示所有。
      apiVersions: [&quot;v1&quot;] # 拦截资源的版本
      operations:  [&quot;CREATE&quot;] # 什么请求下拦截
      resources:   [&quot;deployments&quot;]  # 拦截什么资源
      scope:       &quot;Namespaced&quot; # 生效的范围，cluster还是namespace &quot;*&quot;表示没有范围限制。
  clientConfig: # 我们部署的webhook服务，
    url: &quot;https://10.0.0.1:81/validate&quot; # 这里是外部模式
    #      service: # service是在cluster-in模式下
    #        namespace: &quot;default&quot;
    #        name: &quot;admission-webhook&quot;
    #        port: 81 # 服务的端口
    #        path: &quot;/mutate&quot; # path是对应用于验证的接口
    # caBundle是提供给 admission webhook CA证书
    caBundle: &quot;Ci0tLS0tQk...&lt;base64-encoded PEM bundle containing the CA that signed the webhook's serving certificate&gt;...tLS0K&quot;
  admissionReviewVersions: [&quot;v1&quot;]
  sideEffects: None
  timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间
</code></pre>
<blockquote>
<p>注：对于webhook，也可以引入外部的服务，并非必须部署到集群内部</p>
</blockquote>
<p>对于外部服务来讲，需要 <code>clientConfig</code> 中的 <code>service</code> , 更换为 <code>url</code> ; 通过 <code>url</code> 参数可以将一个外部的服务引入</p>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
...
webhooks:
- name: my-webhook.example.com
  clientConfig:
    url: &quot;https://my-webhook.example.com:9443/my-webhook-path&quot;
  ...
</code></pre>
<blockquote>
<p>注：这里的url规则必须准守下列形式：</p>
<ul>
<li><code>scheme://host:port/path</code></li>
<li>使用了url 时，这里不应填写集群内的服务</li>
<li><code>scheme</code> 必须是 https，不能为http，这就意味着，引入外部时也需要</li>
<li>配置时使用了，<code>?xx=xx</code> 的参数也是不被允许的（官方说法是这样的，通过源码学习了解到因为会发送特定的请求体，所以无需管参数）</li>
</ul>
</blockquote>
<p>更多的配置可以参考kubernetes官方提供的 <a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#webhook-configuration" target="_blank"
   rel="noopener nofollow noreferrer" >doc</a></p>
<h3 id="准备一个webhook">准备一个webhook</h3>
<p>让我们编写我们的 webhook  server。将创建两个钩子，<code>/mutate</code> 与 <code>/validate</code>；</p>
<ul>
<li><code>/mutate</code> 将在创建deployment资源时，基于版本，给资源加上注释 <code>webhook.example.com/allow: true</code></li>
<li><code>/validate</code> 将对 <code>/mutate</code>  增加的 <code>allow:true</code> 那么则继续，否则拒绝。</li>
</ul>
<p>这里为了方便，全部写在一起了，实际上不符合软件的设计。在kubernetes代码库中也提供了一个<a href="https://github.com/kubernetes/kubernetes/blob/release-1.21/test/images/agnhost/webhook/main.go" target="_blank"
   rel="noopener nofollow noreferrer" >webhook server</a>，可以参考他这个webhook server来学习具体要做什么</p>
<pre><code class="language-go">package main

import (
	&quot;context&quot;
	&quot;crypto/tls&quot;
	&quot;encoding/json&quot;
	&quot;fmt&quot;
	&quot;io/ioutil&quot;
	&quot;net/http&quot;
	&quot;os&quot;
	&quot;os/signal&quot;
	&quot;strings&quot;
	&quot;syscall&quot;

	v1admission &quot;k8s.io/api/admission/v1&quot;
	&quot;k8s.io/apimachinery/pkg/runtime&quot;
	&quot;k8s.io/apimachinery/pkg/runtime/serializer&quot;

	appv1 &quot;k8s.io/api/apps/v1&quot;
	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	&quot;k8s.io/klog&quot;
)

type patch struct {
	Op    string            `json:&quot;op&quot;`
	Path  string            `json:&quot;path&quot;`
	Value map[string]string `json:&quot;value&quot;`
}

func serve(w http.ResponseWriter, r *http.Request) {

	var body []byte
	if data, err := ioutil.ReadAll(r.Body); err == nil {
		body = data
	}
	klog.Infof(fmt.Sprintf(&quot;receive request: %v....&quot;, string(body)[:130]))
	if len(body) == 0 {
		klog.Error(fmt.Sprintf(&quot;admission request body is empty&quot;))
		http.Error(w, fmt.Errorf(&quot;admission request body is empty&quot;).Error(), http.StatusBadRequest)
		return
	}
	var admission v1admission.AdmissionReview
	codefc := serializer.NewCodecFactory(runtime.NewScheme())
	decoder := codefc.UniversalDeserializer()
	_, _, err := decoder.Decode(body, nil, &amp;admission)

	if err != nil {
		msg := fmt.Sprintf(&quot;Request could not be decoded: %v&quot;, err)
		klog.Error(msg)
		http.Error(w, msg, http.StatusBadRequest)
		return
	}

	if admission.Request == nil {
		klog.Error(fmt.Sprintf(&quot;admission review can't be used: Request field is nil&quot;))
		http.Error(w, fmt.Errorf(&quot;admission review can't be used: Request field is nil&quot;).Error(), http.StatusBadRequest)
		return
	}

	switch strings.Split(r.RequestURI, &quot;?&quot;)[0] {
	case &quot;/mutate&quot;:
		req := admission.Request
		var admissionResp v1admission.AdmissionReview
		admissionResp.APIVersion = admission.APIVersion
		admissionResp.Kind = admission.Kind
		klog.Infof(&quot;AdmissionReview for Kind=%v, Namespace=%v Name=%v UID=%v Operation=%v&quot;,
			req.Kind.Kind, req.Namespace, req.Name, req.UID, req.Operation)
		switch req.Kind.Kind {
		case &quot;Deployment&quot;:
			var (
				respstr []byte
				err     error
				deploy  appv1.Deployment
			)
			if err = json.Unmarshal(req.Object.Raw, &amp;deploy); err != nil {
				respStructure := v1admission.AdmissionResponse{Result: &amp;metav1.Status{
					Message: fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err),
					Code:    http.StatusInternalServerError,
				}}
				klog.Error(fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err))
				if respstr, err = json.Marshal(respStructure); err != nil {
					klog.Error(fmt.Errorf(&quot;could not unmarshal resouces review response: %v&quot;, err))
					http.Error(w, fmt.Errorf(&quot;could not unmarshal resouces review response: %v&quot;, err).Error(), http.StatusInternalServerError)
					return
				}
				http.Error(w, string(respstr), http.StatusBadRequest)
				return
			}

			current_annotations := deploy.GetAnnotations()
			pl := []patch{}
			for k, v := range current_annotations {
				pl = append(pl, patch{
					Op:   &quot;add&quot;,
					Path: &quot;/metadata/annotations&quot;,
					Value: map[string]string{
						k: v,
					},
				})
			}
			pl = append(pl, patch{
				Op:   &quot;add&quot;,
				Path: &quot;/metadata/annotations&quot;,
				Value: map[string]string{
					deploy.Name + &quot;/Allow&quot;: &quot;true&quot;,
				},
			})

			annotationbyte, err := json.Marshal(pl)

			if err != nil {
				http.Error(w, err.Error(), http.StatusInternalServerError)
				return
			}
			respStructure := &amp;v1admission.AdmissionResponse{
				UID:     req.UID,
				Allowed: true,
				Patch:   annotationbyte,
				PatchType: func() *v1admission.PatchType {
					t := v1admission.PatchTypeJSONPatch
					return &amp;t
				}(),
				Result: &amp;metav1.Status{
					Message: fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err),
					Code:    http.StatusOK,
				},
			}
			admissionResp.Response = respStructure

			klog.Infof(&quot;sending response: %s....&quot;, admissionResp.Response.String()[:130])
			respByte, err := json.Marshal(admissionResp)
			if err != nil {
				klog.Errorf(&quot;Can't encode response messages: %v&quot;, err)
				http.Error(w, err.Error(), http.StatusInternalServerError)
			}
			klog.Infof(&quot;prepare to write response...&quot;)
			w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
			if _, err := w.Write(respByte); err != nil {
				klog.Errorf(&quot;Can't write response: %v&quot;, err)
				http.Error(w, fmt.Sprintf(&quot;could not write response: %v&quot;, err), http.StatusInternalServerError)
			}

		default:
			klog.Error(fmt.Sprintf(&quot;unsupport resouces review request type&quot;))
			http.Error(w, &quot;unsupport resouces review request type&quot;, http.StatusBadRequest)
		}

	case &quot;/validate&quot;:
		req := admission.Request
		var admissionResp v1admission.AdmissionReview
		admissionResp.APIVersion = admission.APIVersion
		admissionResp.Kind = admission.Kind
		klog.Infof(&quot;AdmissionReview for Kind=%v, Namespace=%v Name=%v UID=%v Operation=%v&quot;,
			req.Kind.Kind, req.Namespace, req.Name, req.UID, req.Operation)
		var (
			deploy  appv1.Deployment
			respstr []byte
		)
		switch req.Kind.Kind {
		case &quot;Deployment&quot;:
			if err = json.Unmarshal(req.Object.Raw, &amp;deploy); err != nil {
				respStructure := v1admission.AdmissionResponse{Result: &amp;metav1.Status{
					Message: fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err),
					Code:    http.StatusInternalServerError,
				}}
				klog.Error(fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err))
				if respstr, err = json.Marshal(respStructure); err != nil {
					klog.Error(fmt.Errorf(&quot;could not unmarshal resouces review response: %v&quot;, err))
					http.Error(w, fmt.Errorf(&quot;could not unmarshal resouces review response: %v&quot;, err).Error(), http.StatusInternalServerError)
					return
				}
				http.Error(w, string(respstr), http.StatusBadRequest)
				return
			}
		}
		al := deploy.GetAnnotations()
		respStructure := v1admission.AdmissionResponse{
			UID: req.UID,
		}
		if al[fmt.Sprintf(&quot;%s/Allow&quot;, deploy.Name)] == &quot;true&quot; {
			respStructure.Allowed = true
			respStructure.Result = &amp;metav1.Status{
				Code: http.StatusOK,
			}
		} else {
			respStructure.Allowed = false
			respStructure.Result = &amp;metav1.Status{
				Code: http.StatusForbidden,
				Reason: func() metav1.StatusReason {
					return metav1.StatusReasonForbidden
				}(),
				Message: fmt.Sprintf(&quot;the resource %s couldn't to allow entry.&quot;, deploy.Kind),
			}
		}

		admissionResp.Response = &amp;respStructure

		klog.Infof(&quot;sending response: %s....&quot;, admissionResp.Response.String()[:130])
		respByte, err := json.Marshal(admissionResp)
		if err != nil {
			klog.Errorf(&quot;Can't encode response messages: %v&quot;, err)
			http.Error(w, err.Error(), http.StatusInternalServerError)
		}
		klog.Infof(&quot;prepare to write response...&quot;)
		w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
		if _, err := w.Write(respByte); err != nil {
			klog.Errorf(&quot;Can't write response: %v&quot;, err)
			http.Error(w, fmt.Sprintf(&quot;could not write response: %v&quot;, err), http.StatusInternalServerError)
		}
	}
}

func main() {
	var (
		cert, key string
	)

	if cert = os.Getenv(&quot;TLS_CERT&quot;); len(cert) == 0 {
		cert = &quot;./tls/tls.crt&quot;
	}

	if key = os.Getenv(&quot;TLS_KEY&quot;); len(key) == 0 {
		key = &quot;./tls/tls.key&quot;
	}

	ca, err := tls.LoadX509KeyPair(cert, key)
	if err != nil {
		klog.Error(err.Error())
		return
	}

	server := &amp;http.Server{
		Addr: &quot;:81&quot;,
		TLSConfig: &amp;tls.Config{
			Certificates: []tls.Certificate{
				ca,
			},
		},
	}

	httpserver := http.NewServeMux()

	httpserver.HandleFunc(&quot;/validate&quot;, serve)
	httpserver.HandleFunc(&quot;/mutate&quot;, serve)
	httpserver.HandleFunc(&quot;/ping&quot;, func(w http.ResponseWriter, r *http.Request) {
		klog.Info(fmt.Sprintf(&quot;%s %s&quot;, r.RequestURI, &quot;pong&quot;))
		fmt.Fprint(w, &quot;pong&quot;)
	})
	server.Handler = httpserver

	go func() {
		if err := server.ListenAndServeTLS(&quot;&quot;, &quot;&quot;); err != nil {
			klog.Errorf(&quot;Failed to listen and serve webhook server: %v&quot;, err)
		}
	}()

	klog.Info(&quot;starting serve.&quot;)
	signalChan := make(chan os.Signal, 1)
	signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM)
	&lt;-signalChan

	klog.Infof(&quot;Got shut signal, shutting...&quot;)
	if err := server.Shutdown(context.Background()); err != nil {
		klog.Errorf(&quot;HTTP server Shutdown: %v&quot;, err)
	}
}
</code></pre>
<p>对应的Dockerfile</p>
<pre><code class="language-docker">FROM golang:alpine AS builder
MAINTAINER cylon
WORKDIR /admission
COPY ./ /admission
ENV GOPROXY https://goproxy.cn,direct
RUN \
    sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories &amp;&amp; \
    apk add upx  &amp;&amp; \
    GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -ldflags &quot;-s -w&quot; -o webhook main.go &amp;&amp; \
    upx -1 webhook &amp;&amp; \
    chmod +x webhook

FROM alpine AS runner
WORKDIR /go/admission
COPY --from=builder /admission/webhook .
VOLUME [&quot;/admission&quot;]
</code></pre>
<p>集群内部部署所需的资源清单</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: admission-webhook
  labels:
    app: admission-webhook
spec:
  ports:
    - port: 81
      targetPort: 81
  selector:
    app: simple-webhook
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: simple-webhook
  name: simple-webhook
spec:
  replicas: 1
  selector:
    matchLabels:
      app: simple-webhook
  template:
    metadata:
      labels:
        app: simple-webhook
    spec:
      containers:
        - image: cylonchau/simple-webhook:v0.0.2
          imagePullPolicy: IfNotPresent
          name: webhook
          command: [&quot;./webhook&quot;]
          env:
            - name: &quot;TLS_CERT&quot;
              value: &quot;./tls/tls.crt&quot;
            - name: &quot;TLS_KEY&quot;
              value: &quot;./tls/tls.key&quot;
            - name: NS_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          ports:
            - containerPort: 81
          volumeMounts:
            - name: tlsdir
              mountPath: /go/admission/tls
              readOnly: true
      volumes:
        - name: tlsdir
          secret:
            secretName: webhook
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: &quot;pod-policy.example.com&quot;
webhooks:
  - name: &quot;pod-policy.example.com&quot;
    rules:
      - apiGroups:   [&quot;apps&quot;] # 拦截资源的Group &quot;&quot; 表示 core。&quot;*&quot; 表示所有。
        apiVersions: [&quot;v1&quot;] # 拦截资源的版本
        operations:  [&quot;CREATE&quot;] # 什么请求下拦截
        resources:   [&quot;deployments&quot;]  # 拦截什么资源
        scope:       &quot;Namespaced&quot; # 生效的范围，cluster还是namespace &quot;*&quot;表示没有范围限制。
    clientConfig: # 我们部署的webhook服务，
      url: &quot;https://10.0.0.1:81/mutate&quot;
#      service: # service是在cluster-in模式下
#        namespace: &quot;default&quot;
#        name: &quot;admission-webhook&quot;
#        port: 81 # 服务的端口
#        path: &quot;/mutate&quot; # path是对应用于验证的接口
      # caBundle是提供给 admission webhook CA证书
      caBundle: Put you CA (base64 encode) in here
    admissionReviewVersions: [&quot;v1&quot;]
    sideEffects: None
    timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: &quot;valipod-policy.example.com&quot;
webhooks:
- name: &quot;valipod-policy.example.com&quot;
  rules:
    - apiGroups:   [&quot;apps&quot;] # 拦截资源的Group &quot;&quot; 表示 core。&quot;*&quot; 表示所有。
      apiVersions: [&quot;v1&quot;] # 拦截资源的版本
      operations:  [&quot;CREATE&quot;] # 什么请求下拦截
      resources:   [&quot;deployments&quot;]  # 拦截什么资源
      scope:       &quot;Namespaced&quot; # 生效的范围，cluster还是namespace &quot;*&quot;表示没有范围限制。
  clientConfig: # 我们部署的webhook服务，
    #      service: # service是在cluster-in模式下
    #        namespace: &quot;default&quot;
    #        name: &quot;admission-webhook&quot;
    #        port: 81 # 服务的端口
    #        path: &quot;/mutate&quot; # path是对应用于验证的接口
    # caBundle是提供给 admission webhook CA证书
    caBundle: Put you CA (base64 encode) in here
  admissionReviewVersions: [&quot;v1&quot;]
  sideEffects: None
  timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间
</code></pre>
<h4 id="这里需要主义的问题">这里需要主义的问题</h4>
<p><strong>证书问题</strong></p>
<p>如果需要 <code>cluster-in</code> ，那么则需要对对应webhookconfig资源配置 <code>service</code> ；如果使用的是外部部署，则需要配置对应访问地址，如：<em>&ldquo;https://xxxx:port/method&rdquo;</em></p>
<p>这两种方式的证书均需要对应的 <code>subjectAltName</code> ，<code>cluster-in</code> 模式 需要对应service名称，如，至少包含<code>serviceName.NS.svc</code> 这一个域名。</p>
<p>下面就是证书类问题的错误</p>
<pre><code>Failed calling webhook, failing closed pod-policy.example.com: failed calling webhook &quot;pod-policy.example.com&quot;: Post https://admission-webhook.default.svc:81/mutate?timeout=5s: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;admission-webhook-ca&quot;)
</code></pre>
<p><strong>相应信息问题</strong></p>
<p>上面我们了解到的APIServer是去发出 <code>v1admission.AdmissionReview</code> 也就是 Request 和 Response类型的，所以，为了更清晰的表示出问题所在，需要对响应格式中的 <code>Reason</code> 与 <code>Message</code>  配置，这也就是我们在客户端看到的报错信息。</p>
<pre><code class="language-go">&amp;metav1.Status{
    Code: http.StatusForbidden,
    Reason: func() metav1.StatusReason {
        return metav1.StatusReasonForbidden
    }(),
    Message: fmt.Sprintf(&quot;the resource %s couldn't to allow entry.&quot;, deploy.Kind),
}
</code></pre>
<p>通过上面的设置用户可以看到下列错误</p>
<pre><code class="language-bash">$ kubectl apply -f nginx.yaml 
Error from server (Forbidden): error when creating &quot;nginx.yaml&quot;: admission webhook &quot;valipod-policy.example.com&quot; denied the request: the resource Deployment couldn't to allow entry.
</code></pre>
<blockquote>
<p>注：必须的参数还包含，UID，allowed，这两个是必须的，上面阐述的只是对用户友好的提示信息</p>
</blockquote>
<p>下面的报错就是对相应格式设置错误</p>
<pre><code class="language-go">Error from server (InternalError): error when creating &quot;nginx.yaml&quot;: Internal error occurred: failed calling webhook &quot;pod-policy.example.com&quot;: the server rejected our request for an unknown reason
</code></pre>
<p><strong>相应信息版本问题</strong></p>
<p>相应信息也需要指定一个版本，这个与请求来的结构中拿即可</p>
<pre><code class="language-go">admissionResp.APIVersion = admission.APIVersion
admissionResp.Kind = admission.Kind
</code></pre>
<p>下面是没有为对应相应信息配置对应KV的值出现的报错</p>
<pre><code>Error from server (InternalError): error when creating &quot;nginx.yaml&quot;: Internal error occurred: failed calling webhook &quot;pod-policy.example.com&quot;: expected webhook response of admission.k8s.io/v1, Kind=AdmissionReview, got /, Kind=
</code></pre>
<p><strong>关于patch</strong></p>
<p>kubernetes中patch使用的是特定的规范，如 <code>jsonpatch</code></p>
<blockquote>
<p>kubernetes当前唯一支持的 <code>patchType</code> 是 <code>JSONPatch</code>。 有关更多详细信息，请参见 <a href="https://jsonpatch.com/" target="_blank"
   rel="noopener nofollow noreferrer" >JSON patch</a></p>
<p>对于 <code>jsonpatch</code> 是一个固定的类型，在go中必须定义其结构体</p>
<pre><code class="language-json">{
	&quot;op&quot;: &quot;add&quot;, // 做什么操作
	&quot;path&quot;: &quot;/spec/replicas&quot;, // 操作的路径
	&quot;value&quot;: 3 // 对应添加的key value
}
</code></pre>
</blockquote>
<p>下面就是字符串类型设置为布尔型产生的报错</p>
<pre><code class="language-bash">Error from server (InternalError): error when creating &quot;nginx.yaml&quot;: Internal error occurred: v1.Deployment.ObjectMeta: v1.ObjectMeta.Annotations: ReadString: expects &quot; or n, but found t, error found in #10 byte of ...|t/Allow&quot;:true},&quot;crea|..., bigger context ...|tadata&quot;:{&quot;annotations&quot;:{&quot;nginx-deployment/Allow&quot;:true},&quot;creationTimestamp&quot;:null,&quot;managedFields&quot;:[{&quot;m|..
</code></pre>
<h3 id="准备证书">准备证书</h3>
<p>Ubuntu</p>
<pre><code class="language-bash">touch ./demoCAindex.txt
touch ./demoCA/serial 
touch ./demoCA/crlnumber
echo 01 &gt; ./demoCA/serial
mkdir ./demoCA/newcerts

openssl genrsa -out cakey.pem 2048

openssl req -new \
	-x509 \
	-key cakey.pem \
	-out cacert.pem \
	-days 3650 \
	-subj &quot;/CN=admission webhook ca&quot;

openssl genrsa -out tls.key 2048

openssl req -new \
	-key tls.key \
	-subj &quot;/CN=admission webhook client&quot; \
	-reqexts webhook \
	-config &lt;(cat /etc/ssl/openssl.cnf \
	&lt;(printf &quot;[webhook]\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1,  IP:10.0.0.4&quot;)) \
	-out tls.csr

sed -i 's/= match/= optional/g' /etc/ssl/openssl.cnf

openssl ca \
	-in tls.csr \
	-cert cacert.pem \
	-keyfile cakey.pem \
	-out tls.crt \
	-days 300 \
	-extensions webhook \
	-extfile &lt;(cat /etc/ssl/openssl.cnf \
    &lt;(printf &quot;[webhook]\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1,  IP:10.0.0.4&quot;))
</code></pre>
<p>CentOS</p>
<pre><code class="language-bash">touch /etc/pki/CA/index.txt
touch /etc/pki/CA/serial # 下一个要颁发的编号 16进制
touch /etc/pki/CA/crlnumber
echo 01 &gt; /etc/pki/CA/serial

openssl req -new \
	-x509 \
	-key cakey.pem \
	-out cacert.pem \
	-days 3650 \
	-subj &quot;/CN=admission webhook ca&quot;

openssl genrsa -out tls.key 2048

openssl req -new \
	-key tls.key \
	-subj &quot;/CN=admission webhook client&quot; \
	-reqexts webhook \
	-config &lt;(cat /etc/pki/tls/openssl.cnf \
	&lt;(printf &quot;[webhook]\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1,  IP:10.0.0.4&quot;)) \
	-out tls.csr

sed -i 's/= match/= optional/g' /etc/ssl/openssl.cnf

openssl ca \
	-in tls.csr \
	-cert cacert.pem \
	-keyfile cakey.pem \
	-out tls.crt \
	-days 300 \
	-extensions webhook \
	-extfile &lt;(cat /etc/pki/tls/openssl.cnf \
    &lt;(printf &quot;[webhook]\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1,  IP:10.0.0.4&quot;))
</code></pre>
<h3 id="通过部署测试结果">通过部署测试结果</h3>
<p>可以看到我们自己注入的 annotation <code>nginx-deployment/Allow: true</code>，在该示例中，仅为演示过程，而不是真的策略，实际环境中可以根据情况进行定制自己的策略。</p>
<p>结果可以看出，当在 <code>mutating</code> 中不通过，即缺少对应的 annotation 标签 , 则 <code>validating</code> 会不允许准入</p>
<pre><code class="language-bash">$ kubectl describe deploy nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Mon, 11 Jul 2022 20:25:16 +0800
Labels:                 &lt;none&gt;
Annotations:            deployment.kubernetes.io/revision: 1
                        nginx-deployment/Allow: true
Selector:               app=nginx
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.14.2
</code></pre>
<blockquote>
<p>Reference</p>
<p><a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank"
   rel="noopener nofollow noreferrer" >extensible admission controllers</a></p>
<p><a href="https://developer.aliyun.com/article/703438" target="_blank"
   rel="noopener nofollow noreferrer" >K8S client-go Patch example</a></p>
<p><a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#response" target="_blank"
   rel="noopener nofollow noreferrer" >admission controllers response</a></p>
<p><a href="https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/" target="_blank"
   rel="noopener nofollow noreferrer" >a guide to kubernetes admission controllers</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>利用kubernetes中的leader选举机制自定义HA应用</title>
      <link>https://www.oomkill.com/2022/06/ch28-leader-election-eg/</link>
      <pubDate>Wed, 29 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch28-leader-election-eg/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="backgroud">Backgroud</h2>
<p>前一章中，对kubernetes的选举原理进行了深度剖析，下面就通过一个example来实现一个，利用kubernetes提供的选举机制完成的高可用应用。</p>
<p>对于此章需要提前对一些概念有所了解后才可以继续看下去</p>
<ul>
<li>leader election mechanism</li>
<li>RBCA</li>
<li>Pod runtime mechanism</li>
</ul>
<h2 id="implementation">Implementation</h2>
<h3 id="代码实现">代码实现</h3>
<p>如果仅仅是使用Kubernetes中的锁，实现的代码也只有几行而已。</p>
<pre><code class="language-go">package main

import (
	&quot;context&quot;
	&quot;flag&quot;
	&quot;fmt&quot;
	&quot;os&quot;
	&quot;os/signal&quot;
	&quot;syscall&quot;
	&quot;time&quot;

	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	clientset &quot;k8s.io/client-go/kubernetes&quot;
	&quot;k8s.io/client-go/rest&quot;
	&quot;k8s.io/client-go/tools/clientcmd&quot;
	&quot;k8s.io/client-go/tools/leaderelection&quot;
	&quot;k8s.io/client-go/tools/leaderelection/resourcelock&quot;
	&quot;k8s.io/klog/v2&quot;
)

func buildConfig(kubeconfig string) (*rest.Config, error) {
	if kubeconfig != &quot;&quot; {
		cfg, err := clientcmd.BuildConfigFromFlags(&quot;&quot;, kubeconfig)
		if err != nil {
			return nil, err
		}
		return cfg, nil
	}

	cfg, err := rest.InClusterConfig()
	if err != nil {
		return nil, err
	}
	return cfg, nil
}

func main() {
	klog.InitFlags(nil)

	var kubeconfig string
	var leaseLockName string
	var leaseLockNamespace string
	var id string
	// 初始化客户端的部分
	flag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;absolute path to the kubeconfig file&quot;)
	flag.StringVar(&amp;id, &quot;id&quot;, &quot;&quot;, &quot;the holder identity name&quot;)
	flag.StringVar(&amp;leaseLockName, &quot;lease-lock-name&quot;, &quot;&quot;, &quot;the lease lock resource name&quot;)
	flag.StringVar(&amp;leaseLockNamespace, &quot;lease-lock-namespace&quot;, &quot;&quot;, &quot;the lease lock resource namespace&quot;)
	flag.Parse()

	if leaseLockName == &quot;&quot; {
		klog.Fatal(&quot;unable to get lease lock resource name (missing lease-lock-name flag).&quot;)
	}
	if leaseLockNamespace == &quot;&quot; {
		klog.Fatal(&quot;unable to get lease lock resource namespace (missing lease-lock-namespace flag).&quot;)
	}
	config, err := buildConfig(kubeconfig)
	if err != nil {
		klog.Fatal(err)
	}
	client := clientset.NewForConfigOrDie(config)

	run := func(ctx context.Context) {
		// 实现的业务逻辑，这里仅仅为实验，就直接打印了
		klog.Info(&quot;Controller loop...&quot;)

		for {
			fmt.Println(&quot;I am leader, I was working.&quot;)
			time.Sleep(time.Second * 5)
		}
	}

	// use a Go context so we can tell the leaderelection code when we
	// want to step down
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// 监听系统中断
	ch := make(chan os.Signal, 1)
	signal.Notify(ch, os.Interrupt, syscall.SIGTERM)
	go func() {
		&lt;-ch
		klog.Info(&quot;Received termination, signaling shutdown&quot;)
		cancel()
	}()

	// 创建一个资源锁
	lock := &amp;resourcelock.LeaseLock{
		LeaseMeta: metav1.ObjectMeta{
			Name:      leaseLockName,
			Namespace: leaseLockNamespace,
		},
		Client: client.CoordinationV1(),
		LockConfig: resourcelock.ResourceLockConfig{
			Identity: id,
		},
	}

	// 开启一个选举的循环
	leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
		Lock:            lock,
		ReleaseOnCancel: true,
		LeaseDuration:   60 * time.Second,
		RenewDeadline:   15 * time.Second,
		RetryPeriod:     5 * time.Second,
		Callbacks: leaderelection.LeaderCallbacks{
			OnStartedLeading: func(ctx context.Context) {
				// 当选举为leader后所运行的业务逻辑
				run(ctx)
			},
			OnStoppedLeading: func() {
				// we can do cleanup here
				klog.Infof(&quot;leader lost: %s&quot;, id)
				os.Exit(0)
			},
			OnNewLeader: func(identity string) { // 申请一个选举时的动作
				if identity == id {
					return
				}
				klog.Infof(&quot;new leader elected: %s&quot;, identity)
			},
		},
	})
}
</code></pre>
<blockquote>
<p>注：这种lease锁只能在in-cluster模式下运行，如果需要类似二进制部署的程序，可以选择endpoint类型的资源锁。</p>
</blockquote>
<h3 id="生成镜像">生成镜像</h3>
<p>这里已经制作好了镜像并上传到dockerhub（<code>cylonchau/leaderelection:v0.0.2</code>）上了，如果只要学习运行原理，则忽略此步骤</p>
<pre><code class="language-docker">FROM golang:alpine AS builder
MAINTAINER cylon
WORKDIR /election
COPY . /election
ENV GOPROXY https://goproxy.cn,direct
RUN GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -o elector main.go

FROM alpine AS runner
WORKDIR /go/elector
COPY --from=builder /election/elector .
VOLUME [&quot;/election&quot;]
ENTRYPOINT [&quot;./elector&quot;]
</code></pre>
<h3 id="准备资源清单">准备资源清单</h3>
<p>默认情况下，Kubernetes运行的pod在请求Kubernetes集群内资源时，默认的账户是没有权限的，默认服务帐户无权访问协调 API，因此我们需要创建另一个serviceaccount并相应地设置 对应的RBAC权限绑定；在清单中配置上这个sa，此时所有的pod就会有协调锁的权限了</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-leaderelection
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: leaderelection
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: leaderelection
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: leaderelection
subjects:
  - kind: ServiceAccount
    name: sa-leaderelection
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: leaderelection
  name: leaderelection
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: leaderelection
  template:
    metadata:
      labels:
        app: leaderelection
    spec:
      containers:
        - image: cylonchau/leaderelection:v0.0.2
          imagePullPolicy: IfNotPresent
          command: [&quot;./elector&quot;]
          args:
          - &quot;-id=$(POD_NAME)&quot;
          - &quot;-lease-lock-name=test&quot;
          - &quot;-lease-lock-namespace=default&quot;
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          name: elector
      serviceAccountName: sa-leaderelection
</code></pre>
<h3 id="集群中运行">集群中运行</h3>
<p>执行完清单后，当pod启动后，可以看到会创建出一个 lease</p>
<pre><code class="language-bash">$ kubectl get lease
NAME   HOLDER                            AGE
test   leaderelection-5644c5f84f-frs5n   1s


$ kubectl describe lease
Name:         test
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  coordination.k8s.io/v1
Kind:         Lease
Metadata:
  Creation Timestamp:  2022-06-28T16:39:45Z
  Managed Fields:
    API Version:  coordination.k8s.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        f:acquireTime:
        f:holderIdentity:
        f:leaseDurationSeconds:
        f:leaseTransitions:
        f:renewTime:
    Manager:         elector
    Operation:       Update
    Time:            2022-06-28T16:39:45Z
  Resource Version:  131693
  Self Link:         /apis/coordination.k8s.io/v1/namespaces/default/leases/test
  UID:               bef2b164-a117-44bd-bad3-3e651c94c97b
Spec:
  Acquire Time:            2022-06-28T16:39:45.931873Z
  Holder Identity:         leaderelection-5644c5f84f-frs5n
  Lease Duration Seconds:  60
  Lease Transitions:       0
  Renew Time:              2022-06-28T16:39:55.963537Z
Events:                    &lt;none&gt;
</code></pre>
<p>通过其持有者的信息查看对应pod（因为程序中对holder Identity设置的是pod的名称），实际上是工作的pod。</p>
<p>如上实例所述，这是利用Kubernetes集群完成的leader选举的方案，虽然这不是最完美解决方案，但这是一种简单的方法，因为可以无需在集群上部署更多东西或者进行大量的代码工作就可以利用Kubernetes集群来完成一个高可用的HA应用。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析Kubernetes HA机制 - leader election</title>
      <link>https://www.oomkill.com/2022/06/ch27-leader-election/</link>
      <pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch27-leader-election/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>在 Kubernetes的 <code>kube-controller-manager</code> , <code>kube-scheduler</code>, 以及使用 <code>Operator</code> 的底层实现 <code>controller-rumtime</code> 都支持高可用系统中的leader选举，本文将以理解 <code>controller-rumtime</code> （底层的实现是 <code>client-go</code>） 中的leader选举以在kubernetes controller中是如何实现的。</p>
<h2 id="background">Background</h2>
<p>在运行 <code>kube-controller-manager</code> 时，是有一些参数提供给cm进行leader选举使用的，可以参考官方文档提供的 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank"
   rel="noopener nofollow noreferrer" >参数</a> 来了解相关参数。</p>
<pre><code class="language-bash">--leader-elect                               Default: true
--leader-elect-renew-deadline duration       Default: 10s
--leader-elect-resource-lock string          Default: &quot;leases&quot;
--leader-elect-resource-name string     	 Default: &quot;kube-controller-manager&quot;
--leader-elect-resource-namespace string     Default: &quot;kube-system&quot;
--leader-elect-retry-period duration         Default: 2s
...
</code></pre>
<p>本身以为这些组件的选举动作时通过etcd进行的，但是后面对 <code>controller-runtime</code> 学习时，发现并没有配置其相关的etcd相关参数，这就引起了对选举机制的好奇。怀着这种好奇心搜索了下有关于 kubernetes的选举，发现官网是这么介绍的，下面是对官方的说明进行一个通俗总结。<a href="https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/" target="_blank"
   rel="noopener nofollow noreferrer" >simple leader election with kubernetes</a></p>
<blockquote>
<p>通过阅读文章得知，kubernetes API 提供了一中选举机制，只要运行在集群内的容器，都是可以实现选举功能的。</p>
<p>Kubernetes API通过提供了两个属性来完成选举动作的</p>
<ul>
<li>ResourceVersions：每个API对象唯一一个ResourceVersion</li>
<li>Annotations：每个API对象都可以对这些key进行注释</li>
</ul>
<p>注：这种选举会增加APIServer的压力。也就对etcd会产生影响</p>
</blockquote>
<p>那么有了这些信息之后，我们来看一下，在Kubernetes集群中，谁是cm的leader（我们提供的集群只有一个节点，所以本节点就是leader）</p>
<p>在Kubernetes中所有启用了leader选举的服务都会生成一个 <code>EndPoint</code> ，在这个 <code>EndPoint</code> 中会有上面提到的label（<em>Annotations</em>）来标识谁是leader。</p>
<pre><code class="language-bash">$ kubectl get ep -n kube-system
NAME                      ENDPOINTS   AGE
kube-controller-manager   &lt;none&gt;      3d4h
kube-dns                              3d4h
kube-scheduler            &lt;none&gt;      3d4h
</code></pre>
<p>这里以 <code>kube-controller-manager</code> 为例，来看下这个 <code>EndPoint</code> 有什么信息</p>
<pre><code class="language-bash">$ kubectl describe ep kube-controller-manager -n kube-system
Name:         kube-controller-manager
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  control-plane.alpha.kubernetes.io/leader:
                {&quot;holderIdentity&quot;:&quot;master-machine_06730140-a503-487d-850b-1fe1619f1fe1&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2022-06-27T15:30:46Z&quot;,&quot;re...
Subsets:
Events:
  Type    Reason          Age    From                     Message
  ----    ------          ----   ----                     -------
  Normal  LeaderElection  2d22h  kube-controller-manager  master-machine_76aabcb5-49ff-45ff-bd18-4afa61fbc5af became leader
  Normal  LeaderElection  9m     kube-controller-manager  master-machine_06730140-a503-487d-850b-1fe1619f1fe1 became leader
</code></pre>
<p>可以看出 <code>Annotations:  control-plane.alpha.kubernetes.io/leader:</code> 标出了哪个node是leader。</p>
<h2 id="election-in-controller-runtime">election in controller-runtime</h2>
<p><code>controller-runtime</code> 有关leader选举的部分在 <a href="https://github.com/kubernetes-sigs/controller-runtime/tree/master/pkg/leaderelection" target="_blank"
   rel="noopener nofollow noreferrer" >pkg/leaderelection</a> 下面，总共100行代码，我们来看下做了些什么？</p>
<p>可以看到，这里只提供了创建资源锁的一些选项</p>
<pre><code class="language-go">type Options struct {
	// 在manager启动时，决定是否进行选举
	LeaderElection bool
	// 使用那种资源锁 默认为租用 lease
	LeaderElectionResourceLock string
	// 选举发生的名称空间
	LeaderElectionNamespace string
	// 该属性将决定持有leader锁资源的名称
	LeaderElectionID string
}
</code></pre>
<p>通过 <code>NewResourceLock</code> 可以看到，这里是走的 <a href="https://github.com/kubernetes/client-go/tree/v0.24.0/tools/leaderelection" target="_blank"
   rel="noopener nofollow noreferrer" >client-go/tools/leaderelection</a>下面，而这个leaderelection也有一个 <a href="https://github.com/kubernetes/client-go/blob/v0.24.0/examples/leader-election/main.go" target="_blank"
   rel="noopener nofollow noreferrer" >example</a> 来学习如何使用它。</p>
<p>通过 example 可以看到，进入选举的入口是一个 RunOrDie() 的函数</p>
<pre><code class="language-go">// 这里使用了一个lease锁，注释中说愿意为集群中存在lease的监听较少
lock := &amp;resourcelock.LeaseLock{
    LeaseMeta: metav1.ObjectMeta{
        Name:      leaseLockName,
        Namespace: leaseLockNamespace,
    },
    Client: client.CoordinationV1(),
    LockConfig: resourcelock.ResourceLockConfig{
        Identity: id,
    },
}

// 开启选举循环
leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
    Lock: lock,
    // 这里必须保证拥有的租约在调用cancel()前终止，否则会仍有一个loop在运行
    ReleaseOnCancel: true,
    LeaseDuration:   60 * time.Second,
    RenewDeadline:   15 * time.Second,
    RetryPeriod:     5 * time.Second,
    Callbacks: leaderelection.LeaderCallbacks{
        OnStartedLeading: func(ctx context.Context) {
            // 这里填写你的代码，
            // usually put your code
            run(ctx)
        },
        OnStoppedLeading: func() {
            // 这里清理你的lease
            klog.Infof(&quot;leader lost: %s&quot;, id)
            os.Exit(0)
        },
        OnNewLeader: func(identity string) {
            // we're notified when new leader elected
            if identity == id {
                // I just got the lock
                return
            }
            klog.Infof(&quot;new leader elected: %s&quot;, identity)
        },
    },
})
</code></pre>
<p>到这里，我们了解了锁的概念和如何启动一个锁，下面看下，client-go都提供了那些锁。</p>
<p>在代码 <a href="tools/leaderelection/resourcelock/interface.go">tools/leaderelection/resourcelock/interface.go</a> 定义了一个锁抽象，interface提供了一个通用接口，用于锁定leader选举中使用的资源。</p>
<pre><code class="language-go">type Interface interface {
	// Get 返回选举记录
	Get(ctx context.Context) (*LeaderElectionRecord, []byte, error)

	// Create 创建一个LeaderElectionRecord
	Create(ctx context.Context, ler LeaderElectionRecord) error

	// Update will update and existing LeaderElectionRecord
	Update(ctx context.Context, ler LeaderElectionRecord) error

	// RecordEvent is used to record events
	RecordEvent(string)

	// Identity 返回锁的标识
	Identity() string

	// Describe is used to convert details on current resource lock into a string
	Describe() string
}
</code></pre>
<p>那么实现这个抽象接口的就是，实现的资源锁，我们可以看到，client-go提供了四种资源锁</p>
<ul>
<li>leaselock</li>
<li>configmaplock</li>
<li>multilock</li>
<li>endpointlock</li>
</ul>
<h3 id="leaselock">leaselock</h3>
<p>Lease是kubernetes控制平面中的通过ETCD来实现的一个Leases的资源，主要为了提供分布式租约的一种控制机制。相关对这个API的描述可以参考于：<a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/" target="_blank"
   rel="noopener nofollow noreferrer" >Lease</a> 。</p>
<p>在Kubernetes集群中，我们可以使用如下命令来查看对应的lease</p>
<pre><code class="language-bash">$ kubectl get leases -A
NAMESPACE         NAME                      HOLDER                                                AGE
kube-node-lease   master-machine            master-machine                                        3d19h
kube-system       kube-controller-manager   master-machine_06730140-a503-487d-850b-1fe1619f1fe1   3d19h
kube-system       kube-scheduler            master-machine_1724e2d9-c19c-48d7-ae47-ee4217b27073   3d19h

$ kubectl describe leases kube-controller-manager -n kube-system
Name:         kube-controller-manager
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  coordination.k8s.io/v1
Kind:         Lease
Metadata:
  Creation Timestamp:  2022-06-24T11:01:51Z
  Managed Fields:
    API Version:  coordination.k8s.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        f:acquireTime:
        f:holderIdentity:
        f:leaseDurationSeconds:
        f:leaseTransitions:
        f:renewTime:
    Manager:         kube-controller-manager
    Operation:       Update
    Time:            2022-06-24T11:01:51Z
  Resource Version:  56012
  Self Link:         /apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager
  UID:               851a32d2-25dc-49b6-a3f7-7a76f152f071
Spec:
  Acquire Time:            2022-06-27T15:30:46.000000Z
  Holder Identity:         master-machine_06730140-a503-487d-850b-1fe1619f1fe1
  Lease Duration Seconds:  15
  Lease Transitions:       2
  Renew Time:              2022-06-28T06:09:26.837773Z
Events:                    &lt;none&gt;
</code></pre>
<p>下面来看下leaselock的实现，leaselock会实现了作为资源锁的抽象</p>
<pre><code class="language-go">type LeaseLock struct {
	// LeaseMeta 就是类似于其他资源类型的属性，包含name ns 以及其他关于lease的属性
	LeaseMeta  metav1.ObjectMeta
	Client     coordinationv1client.LeasesGetter // Client 就是提供了informer中的功能
	// lockconfig包含上面通过 describe 看到的 Identity与recoder用于记录资源锁的更改
    LockConfig ResourceLockConfig
    // lease 就是 API中的Lease资源，可以参考下上面给出的这个API的使用
	lease      *coordinationv1.Lease
}
</code></pre>
<p>下面来看下leaselock实现了那些方法？</p>
<h4 id="get">Get</h4>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/resourcelock/leaselock.go#L41-L53" target="_blank"
   rel="noopener nofollow noreferrer" >Get</a> 是从spec中返回选举的记录</p>
<pre><code class="language-go">func (ll *LeaseLock) Get(ctx context.Context) (*LeaderElectionRecord, []byte, error) {
	var err error
	ll.lease, err = ll.Client.Leases(ll.LeaseMeta.Namespace).Get(ctx, ll.LeaseMeta.Name, metav1.GetOptions{})
	if err != nil {
		return nil, nil, err
	}
	record := LeaseSpecToLeaderElectionRecord(&amp;ll.lease.Spec)
	recordByte, err := json.Marshal(*record)
	if err != nil {
		return nil, nil, err
	}
	return record, recordByte, nil
}

// 可以看出是返回这个资源spec里面填充的值
func LeaseSpecToLeaderElectionRecord(spec *coordinationv1.LeaseSpec) *LeaderElectionRecord {
	var r LeaderElectionRecord
	if spec.HolderIdentity != nil {
		r.HolderIdentity = *spec.HolderIdentity
	}
	if spec.LeaseDurationSeconds != nil {
		r.LeaseDurationSeconds = int(*spec.LeaseDurationSeconds)
	}
	if spec.LeaseTransitions != nil {
		r.LeaderTransitions = int(*spec.LeaseTransitions)
	}
	if spec.AcquireTime != nil {
		r.AcquireTime = metav1.Time{spec.AcquireTime.Time}
	}
	if spec.RenewTime != nil {
		r.RenewTime = metav1.Time{spec.RenewTime.Time}
	}
	return &amp;r
}
</code></pre>
<h4 id="create">Create</h4>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/resourcelock/leaselock.go#L56-L66" target="_blank"
   rel="noopener nofollow noreferrer" >Create</a> 是在kubernetes集群中尝试去创建一个租约，可以看到，Client就是API提供的对应资源的REST客户端，结果会在Kubernetes集群中创建这个Lease</p>
<pre><code class="language-go">func (ll *LeaseLock) Create(ctx context.Context, ler LeaderElectionRecord) error {
	var err error
	ll.lease, err = ll.Client.Leases(ll.LeaseMeta.Namespace).Create(ctx, &amp;coordinationv1.Lease{
		ObjectMeta: metav1.ObjectMeta{
			Name:      ll.LeaseMeta.Name,
			Namespace: ll.LeaseMeta.Namespace,
		},
		Spec: LeaderElectionRecordToLeaseSpec(&amp;ler),
	}, metav1.CreateOptions{})
	return err
}
</code></pre>
<h4 id="update">Update</h4>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/resourcelock/leaselock.go#L69-L82" target="_blank"
   rel="noopener nofollow noreferrer" >Update</a> 是更新Lease的spec</p>
<pre><code class="language-go">func (ll *LeaseLock) Update(ctx context.Context, ler LeaderElectionRecord) error {
	if ll.lease == nil {
		return errors.New(&quot;lease not initialized, call get or create first&quot;)
	}
	ll.lease.Spec = LeaderElectionRecordToLeaseSpec(&amp;ler)

	lease, err := ll.Client.Leases(ll.LeaseMeta.Namespace).Update(ctx, ll.lease, metav1.UpdateOptions{})
	if err != nil {
		return err
	}

	ll.lease = lease
	return nil
}
</code></pre>
<h4 id="recordevent">RecordEvent</h4>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/resourcelock/leaselock.go#L85-L95" target="_blank"
   rel="noopener nofollow noreferrer" >RecordEvent</a> 是记录选举时出现的事件，这时候我们回到上部分 在kubernetes集群中查看 ep 的信息时可以看到的event中存在 <code>became leader</code> 的事件，这里就是将产生的这个event添加到 <code>meta-data</code> 中。</p>
<pre><code>func (ll *LeaseLock) RecordEvent(s string) {
   if ll.LockConfig.EventRecorder == nil {
      return
   }
   events := fmt.Sprintf(&quot;%v %v&quot;, ll.LockConfig.Identity, s)
   subject := &amp;coordinationv1.Lease{ObjectMeta: ll.lease.ObjectMeta}
   // Populate the type meta, so we don't have to get it from the schema
   subject.Kind = &quot;Lease&quot;
   subject.APIVersion = coordinationv1.SchemeGroupVersion.String()
   ll.LockConfig.EventRecorder.Eventf(subject, corev1.EventTypeNormal, &quot;LeaderElection&quot;, events)
}
</code></pre>
<p>到这里大致上了解了资源锁究竟是什么了，其他种类的资源锁也是相同的实现的方式，这里就不过多阐述了；下面的我们来看看选举的过程。</p>
<h3 id="election-workflow">election workflow</h3>
<p>选举的代码入口是在 <a href="https://github.com/kubernetes/client-go/blob/v0.24.0/tools/leaderelection/leaderelection.go" target="_blank"
   rel="noopener nofollow noreferrer" >leaderelection.go</a> ，这里会继续上面的 example 向下分析整个选举的过程。</p>
<p>前面我们看到了进入选举的入口是一个 <a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/examples/leader-election/main.go#L122" target="_blank"
   rel="noopener nofollow noreferrer" >RunOrDie()</a> 的函数，那么就继续从这里开始来了解。进入 RunOrDie，看到其实只有几行而已，大致上了解到了RunOrDie会使用提供的配置来启动选举的客户端，之后会阻塞，直到 ctx 退出，或停止持有leader的租约。</p>
<pre><code class="language-go">func RunOrDie(ctx context.Context, lec LeaderElectionConfig) {
	le, err := NewLeaderElector(lec)
	if err != nil {
		panic(err)
	}
	if lec.WatchDog != nil {
		lec.WatchDog.SetLeaderElection(le)
	}
	le.Run(ctx)
}
</code></pre>
<p>下面看下 <a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/leaderelection.go#L77-L110" target="_blank"
   rel="noopener nofollow noreferrer" >NewLeaderElector</a> 做了些什么？可以看到，LeaderElector是一个结构体，这里只是创建他，这个结构体提供了我们选举中所需要的一切（LeaderElector就是RunOrDie创建的选举客户端）。</p>
<pre><code class="language-go">func NewLeaderElector(lec LeaderElectionConfig) (*LeaderElector, error) {
	if lec.LeaseDuration &lt;= lec.RenewDeadline {
		return nil, fmt.Errorf(&quot;leaseDuration must be greater than renewDeadline&quot;)
	}
	if lec.RenewDeadline &lt;= time.Duration(JitterFactor*float64(lec.RetryPeriod)) {
		return nil, fmt.Errorf(&quot;renewDeadline must be greater than retryPeriod*JitterFactor&quot;)
	}
	if lec.LeaseDuration &lt; 1 {
		return nil, fmt.Errorf(&quot;leaseDuration must be greater than zero&quot;)
	}
	if lec.RenewDeadline &lt; 1 {
		return nil, fmt.Errorf(&quot;renewDeadline must be greater than zero&quot;)
	}
	if lec.RetryPeriod &lt; 1 {
		return nil, fmt.Errorf(&quot;retryPeriod must be greater than zero&quot;)
	}
	if lec.Callbacks.OnStartedLeading == nil {
		return nil, fmt.Errorf(&quot;OnStartedLeading callback must not be nil&quot;)
	}
	if lec.Callbacks.OnStoppedLeading == nil {
		return nil, fmt.Errorf(&quot;OnStoppedLeading callback must not be nil&quot;)
	}

	if lec.Lock == nil {
		return nil, fmt.Errorf(&quot;Lock must not be nil.&quot;)
	}
	le := LeaderElector{
		config:  lec,
		clock:   clock.RealClock{},
		metrics: globalMetricsFactory.newLeaderMetrics(),
	}
	le.metrics.leaderOff(le.config.Name)
	return &amp;le, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/leaderelection.go#L177-L195" target="_blank"
   rel="noopener nofollow noreferrer" >LeaderElector</a> 是建立的选举客户端，</p>
<pre><code class="language-go">type LeaderElector struct {
	config LeaderElectionConfig // 这个的配置，包含一些时间参数，健康检查
	// recoder相关属性
	observedRecord    rl.LeaderElectionRecord
	observedRawRecord []byte
	observedTime      time.Time
	// used to implement OnNewLeader(), may lag slightly from the
	// value observedRecord.HolderIdentity if the transition has
	// not yet been reported.
	reportedLeader string
	// clock is wrapper around time to allow for less flaky testing
	clock clock.Clock
	// 锁定 observedRecord
	observedRecordLock sync.Mutex
	metrics leaderMetricsAdapter
}
</code></pre>
<p>可以看到 Run 实现的选举逻辑就是在初始化客户端时传入的 三个 callback</p>
<pre><code class="language-go">func (le *LeaderElector) Run(ctx context.Context) {
	defer runtime.HandleCrash()
	defer func() { // 退出时执行callbacke的OnStoppedLeading
		le.config.Callbacks.OnStoppedLeading()
	}()

	if !le.acquire(ctx) {
		return
	}
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()
	go le.config.Callbacks.OnStartedLeading(ctx) // 选举时，执行 OnStartedLeading
	le.renew(ctx)
}
</code></pre>
<p>在 Run 中调用了 acquire，这个是 通过一个loop去调用 tryAcquireOrRenew，直到ctx传递过来结束信号</p>
<pre><code class="language-go">func (le *LeaderElector) acquire(ctx context.Context) bool {
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()
	succeeded := false
	desc := le.config.Lock.Describe()
	klog.Infof(&quot;attempting to acquire leader lease %v...&quot;, desc)
    // jitterUntil是执行定时的函数 func() 是定时任务的逻辑
    // RetryPeriod是周期间隔
    // JitterFactor 是重试系数，类似于延迟队列中的系数 （duration + maxFactor * duration）
    // sliding 逻辑是否计算在时间内
    // 上下文传递
	wait.JitterUntil(func() {
		succeeded = le.tryAcquireOrRenew(ctx)
		le.maybeReportTransition()
		if !succeeded {
			klog.V(4).Infof(&quot;failed to acquire lease %v&quot;, desc)
			return
		}
		le.config.Lock.RecordEvent(&quot;became leader&quot;)
		le.metrics.leaderOn(le.config.Name)
		klog.Infof(&quot;successfully acquired lease %v&quot;, desc)
		cancel()
	}, le.config.RetryPeriod, JitterFactor, true, ctx.Done())
	return succeeded
}
</code></pre>
<p>这里实际上选举动作在 tryAcquireOrRenew 中，下面来看下tryAcquireOrRenew；tryAcquireOrRenew 是尝试获得一个leader租约，如果已经获得到了，则更新租约；否则可以得到租约则为true，反之false</p>
<pre><code class="language-go">func (le *LeaderElector) tryAcquireOrRenew(ctx context.Context) bool {
	now := metav1.Now() // 时间
	leaderElectionRecord := rl.LeaderElectionRecord{ // 构建一个选举record
		HolderIdentity:       le.config.Lock.Identity(), // 选举人的身份特征，ep与主机名有关
		LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second), // 默认15s
		RenewTime:            now, // 重新获取时间
		AcquireTime:          now, // 获得时间
	}

	// 1. 从API获取或创建一个recode，如果可以拿到则已经有租约，反之创建新租约
	oldLeaderElectionRecord, oldLeaderElectionRawRecord, err := le.config.Lock.Get(ctx)
	if err != nil {
		if !errors.IsNotFound(err) {
			klog.Errorf(&quot;error retrieving resource lock %v: %v&quot;, le.config.Lock.Describe(), err)
			return false
		}
		// 创建租约的动作就是新建一个对应的resource，这个lock就是leaderelection提供的四种锁，
		// 看你在runOrDie中初始化传入了什么锁
		if err = le.config.Lock.Create(ctx, leaderElectionRecord); err != nil {
			klog.Errorf(&quot;error initially creating leader election record: %v&quot;, err)
			return false
		}
		// 到了这里就已经拿到或者创建了租约，然后记录其一些属性，LeaderElectionRecord
		le.setObservedRecord(&amp;leaderElectionRecord)

		return true
	}

	// 2. 获取记录检查身份和时间
	if !bytes.Equal(le.observedRawRecord, oldLeaderElectionRawRecord) {
		le.setObservedRecord(oldLeaderElectionRecord)

		le.observedRawRecord = oldLeaderElectionRawRecord
	}
	if len(oldLeaderElectionRecord.HolderIdentity) &gt; 0 &amp;&amp;
		le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp;
		!le.IsLeader() { // 不是leader，进行HolderIdentity比较，再加上时间，这个时候没有到竞选其，跳出
		klog.V(4).Infof(&quot;lock is held by %v and has not yet expired&quot;, oldLeaderElectionRecord.HolderIdentity)
		return false
	}

	// 3.我们将尝试更新。 在这里leaderElectionRecord设置为默认值。让我们在更新之前更正它。
	if le.IsLeader() { // 到这就说明是leader，修正他的时间
		leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime
		leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions
	} else { // LeaderTransitions 就是指leader调整（转变为其他）了几次，如果是，
		// 则为发生转变，保持原有值
		// 反之，则+1
		leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1
	}
	// 完事之后更新APIServer中的锁资源，也就是更新对应的资源的属性信息
	if err = le.config.Lock.Update(ctx, leaderElectionRecord); err != nil {
		klog.Errorf(&quot;Failed to update lock: %v&quot;, err)
		return false
	}
	// setObservedRecord 是通过一个新的record来更新这个锁中的record
	// 操作是安全的，会上锁保证临界区仅可以被一个线程/进程操作
	le.setObservedRecord(&amp;leaderElectionRecord)
	return true
}
</code></pre>
<h2 id="summary">summary</h2>
<p>到这里，已经完整知道利用kubernetes进行选举的流程都是什么了；下面简单回顾下，上述leader选举所有的步骤：</p>
<ul>
<li>首选创建的服务就是该服务的leader，锁可以为 <code>lease</code> , <code>endpoint</code> 等资源进行上锁</li>
<li>已经是leader的实例会不断续租，租约的默认值是15秒 （<code>leaseDuration</code>）；leader在租约满时更新租约时间（<code>renewTime</code>）。</li>
<li>其他的follower，会不断检查对应资源锁的存在，如果已经有leader，那么则检查 <code>renewTime</code>，如果超过了租用时间（），则表明leader存在问题需要重新启动选举，直到有follower提升为leader。</li>
<li>而为了避免资源被抢占，Kubernetes API使用了 <code>ResourceVersion</code> 来避免被重复修改（如果版本号与请求版本号不一致，则表示已经被修改了，那么APIServer将返回错误）</li>
</ul>
<blockquote>
<p>Reference</p>
<p><a href="https://juejin.cn/post/6844903709336420360" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes 并发控制与数据一致性的实现原理</a></p>
<p><a href="http://liubin.org/blog/2018/04/28/how-to-build-controller-manager-high-available/" target="_blank"
   rel="noopener nofollow noreferrer" >Controller manager 的高可用实现方式</a></p>
<p><a href="https://medium.com/michaelbi-22303/deep-dive-into-kubernetes-simple-leader-election-3712a8be3a99" target="_blank"
   rel="noopener nofollow noreferrer" >deep dive into kubernetes simple leader election</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析Kubernetes controller组件 - controller-runtime</title>
      <link>https://www.oomkill.com/2022/06/ch15-controller-runtime/</link>
      <pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch15-controller-runtime/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p><a href="https://github.com/kubernetes-sigs/controller-runtime" target="_blank"
   rel="noopener nofollow noreferrer" >controller-runtime</a> 是 Kubernetes 社区提供可供快速搭建一套  实现了controller 功能的工具，无需自行实现Controller的功能了；在 <code>Kubebuilder</code>  与  <code>Operator SDK</code> 也是使用 <code>controller-runtime</code> 。本文将对 <code>controller-runtime</code> 的工作原理以及在不同场景下的使用方式进行简要的总结和介绍。</p>
<h2 id="controller-runtime-structure">controller-runtime structure</h2>
<p><code>controller-runtime</code> 主要组成是需要用户创建的 <code>Manager</code> 和 <code>Reconciler</code> 以及 <code>Controller Runtime</code> 自己启动的 <code>Cache</code> 和 <code>Controller </code>。</p>
<ul>
<li><strong>Manager</strong>：是用户在初始化时创建的，用于启动 <code>Controller Runtime</code> 组件</li>
<li><strong>Reconciler</strong>：是用户需要提供来处理自己的业务逻辑的组件（即在通过 <code>code-generator</code> 生成的api-like而实现的controller中的业务处理部分）。</li>
<li><strong>Cache</strong>：一个缓存，用来建立 <code>Informer</code> 到 <code>ApiServer </code>的连接来监听资源并将被监听的对象推送到queue中。</li>
<li><strong>Controller</strong>： 一方面向 Informer 注册 <code>eventHandler</code>，另一方面从队列中获取数据。controller 将从队列中获取数据并执行用户自定义的 <code>Reconciler</code> 功能。</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220625221632675.png" alt="image-20220625221632675" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：controller-runtime structure</center>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220625221548958.png" alt="image-20220625221548958" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：controller-runtime flowchart</center>
<p>由图可知，Controller会向 Informer 注册一些列eventHandler；然后Cache启动Informer（informer属于cache包中），与ApiServer建立监听；当Informer检测到资源变化时，将对象加入queue，Controller 将元素取出并在用户端执行 Reconciler。</p>
<h2 id="controller引入">Controller引入</h2>
<p>我们从 controller-rumtime项目的 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/master/examples/crd/main.go" target="_blank"
   rel="noopener nofollow noreferrer" >example</a> 进行引入看下，整个架构都是如何实现的。</p>
<p>可以看到 example 下的实际上实现了一个 <code>reconciler </code> 的结构体，实现了 <code>Reconciler</code> 抽象和 <code>Client</code> 结构体</p>
<pre><code class="language-go">type reconciler struct {
	client.Client
	scheme *runtime.Scheme
}
</code></pre>
<p>那么来看下 抽象的 Reconciler 是什么，可以看到就是抽象了 <code>Reconcile </code>方法，这个是具体处理的逻辑过程</p>
<pre><code class="language-go">type Reconciler interface {
	Reconcile(context.Context, Request) (Result, error)
}
</code></pre>
<p>下面在看下谁来实现了这个 Reconciler 抽象</p>
<pre><code class="language-go">type Controller interface {
	reconcile.Reconciler // 协调的具体步骤，通过ns/name\
    // 通过predicates来评估来源数据，并加入queue中（放入队列的是reconcile.Requests）
	Watch(src source.Source, eventhandler handler.EventHandler, predicates ...predicate.Predicate) error
    // 启动controller，类似于自定义的Run()
	Start(ctx context.Context) error
	GetLogger() logr.Logger
}
</code></pre>
<h3 id="controller-structure">controller structure</h3>
<p>在 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/internal/controller/controller.go#L42" target="_blank"
   rel="noopener nofollow noreferrer" >controller-runtime\pkg\internal\controller\controller.go</a> 中实现了这个 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/controller/controller.go#L66" target="_blank"
   rel="noopener nofollow noreferrer" >Controller</a></p>
<pre><code class="language-go">type Controller struct {
	Name string // controller的标识
    
	MaxConcurrentReconciles int // 并发运行Reconciler的数量，默认1
	// 实现了reconcile.Reconciler的调节器， 默认DefaultReconcileFunc
	Do reconcile.Reconciler
	// makeQueue会构建一个对应的队列，就是返回一个限速队列
	MakeQueue func() workqueue.RateLimitingInterface
	// MakeQueue创造出来的，在出入队列就是操作的这个
	Queue workqueue.RateLimitingInterface

	// 用于注入其他内容
    // 已弃用
	SetFields func(i interface{}) error

	mu sync.Mutex
	// 标识开始的状态
	Started bool
	// 在启动时传递的上下文，用于停止控制器
	ctx context.Context
	// 等待缓存同步的时间 默认2分钟
	CacheSyncTimeout time.Duration

	// 维护了eventHandler predicates，在控制器启动时启动
	startWatches []watchDescription

	// 日志构建器，输出入日志
	LogConstructor func(request *reconcile.Request) logr.Logger

	// RecoverPanic为是否对reconcile引起的panic恢复
	RecoverPanic bool
}
</code></pre>
<p>看完了controller的structure，接下来看看controller是如何使用的</p>
<h3 id="injection">injection</h3>
<p><a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/internal/controller/controller.go#L125-L152" target="_blank"
   rel="noopener nofollow noreferrer" >Controller.Watch</a> 实现了注入的动作，可以看到 <code>watch()</code> 通过参数将 对应的事件函数传入到内部</p>
<pre><code class="language-go">func (c *Controller) Watch(src source.Source, evthdler handler.EventHandler, prct ...predicate.Predicate) error {
	c.mu.Lock()
	defer c.mu.Unlock()

	// 使用SetFields来完成注入操作
	if err := c.SetFields(src); err != nil {
		return err
	}
	if err := c.SetFields(evthdler); err != nil {
		return err
	}
	for _, pr := range prct {
		if err := c.SetFields(pr); err != nil {
			return err
		}
	}

	// 如果Controller还未启动，那么将这些动作缓存到本地
	if !c.Started {
		c.startWatches = append(c.startWatches, watchDescription{src: src, handler: evthdler, predicates: prct})
		return nil
	}

	c.LogConstructor(nil).Info(&quot;Starting EventSource&quot;, &quot;source&quot;, src)
	return src.Start(c.ctx, evthdler, c.Queue, prct...)
}
</code></pre>
<p>启动操作实际上为informer注入事件函数</p>
<pre><code class="language-go">type Source interface {
	// start 是Controller 调用，用以向 Informer 注册 EventHandler， 将 reconcile.Requests（一个入队列的动作） 排入队列。
	Start(context.Context, handler.EventHandler, workqueue.RateLimitingInterface, ...predicate.Predicate) error
}

func (is *Informer) Start(ctx context.Context, handler handler.EventHandler, queue workqueue.RateLimitingInterface,
	prct ...predicate.Predicate) error {
	// Informer should have been specified by the user.
	if is.Informer == nil {
		return fmt.Errorf(&quot;must specify Informer.Informer&quot;)
	}

	is.Informer.AddEventHandler(internal.EventHandler{Queue: queue, EventHandler: handler, Predicates: prct})
	return nil
}
</code></pre>
<p>我们知道对于 eventHandler，实际上应该是一个 <code>onAdd</code>，<code>onUpdate</code> 这种类型的函数，queue则是workqueue，那么 <code>Predicates</code> 是什么呢？</p>
<p>通过追踪可以看到定义了 Predicate 抽象，可以看出Predicate 是Watch到的事件时什么类型的，当对于每个类型的事件，对应的函数就为 true，在 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/source/internal/eventsource.go#L87-L91" target="_blank"
   rel="noopener nofollow noreferrer" >eventHandler</a> 中，这些被用作，事件的过滤。</p>
<pre><code class="language-go">// Predicate filters events before enqueuing the keys.
type Predicate interface {
	// Create returns true if the Create event should be processed
	Create(event.CreateEvent) bool

	// Delete returns true if the Delete event should be processed
	Delete(event.DeleteEvent) bool

	// Update returns true if the Update event should be processed
	Update(event.UpdateEvent) bool

	// Generic returns true if the Generic event should be processed
	Generic(event.GenericEvent) bool
}
</code></pre>
<p>在对应的动作中，可以看到这里作为过滤操作</p>
<pre><code class="language-go">func (e EventHandler) OnAdd(obj interface{}) {
	c := event.CreateEvent{}

	// Pull Object out of the object
	if o, ok := obj.(client.Object); ok {
		c.Object = o
	} else {
		log.Error(nil, &quot;OnAdd missing Object&quot;,
			&quot;object&quot;, obj, &quot;type&quot;, fmt.Sprintf(&quot;%T&quot;, obj))
		return
	}

	for _, p := range e.Predicates {
		if !p.Create(c) {
			return
		}
	}

	// Invoke create handler
	e.EventHandler.Create(c, e.Queue)
}
</code></pre>
<p>上面就看到了，对应是 <code>EventHandler.Create</code> 进行添加的，那么这些动作具体是在做什么呢？</p>
<p>在代码 <a href="https://github.com/kubernetes-sigs/controller-runtime/tree/master/pkg/handler" target="_blank"
   rel="noopener nofollow noreferrer" >pkg/handler</a> ,可以看到这些操作，类似于create，这里将ns/name放入到队列中。</p>
<pre><code class="language-go">func (e *EnqueueRequestForObject) Create(evt event.CreateEvent, q workqueue.RateLimitingInterface) {
	if evt.Object == nil {
		enqueueLog.Error(nil, &quot;CreateEvent received with no metadata&quot;, &quot;event&quot;, evt)
		return
	}
	q.Add(reconcile.Request{NamespacedName: types.NamespacedName{
		Name:      evt.Object.GetName(),
		Namespace: evt.Object.GetNamespace(),
	}})
}
</code></pre>
<h3 id="unqueue">unqueue</h3>
<p>上面看到了，入队的动作实际上都是将 <code>ns/name</code> 加入到队列中，那么出队列时又做了些什么呢？</p>
<p>通过 <code>controller.Start()</code>  可以看到controller在启动后都做了些什么动作</p>
<pre><code class="language-go">func (c *Controller) Start(ctx context.Context) error {
	c.mu.Lock()
	if c.Started {
		return errors.New(&quot;controller was started more than once. This is likely to be caused by being added to a manager multiple times&quot;)
	}

	c.initMetrics()

	// Set the internal context.
	c.ctx = ctx

	c.Queue = c.MakeQueue() // 初始化queue
	go func() { // 退出时，让queue关闭
		&lt;-ctx.Done()
		c.Queue.ShutDown()
	}()

	wg := &amp;sync.WaitGroup{}
	err := func() error {
		defer c.mu.Unlock()
		defer utilruntime.HandleCrash()

		// 启动informer前，将之前准备好的 evnetHandle predictates source注册
		for _, watch := range c.startWatches {
			c.LogConstructor(nil).Info(&quot;Starting EventSource&quot;, &quot;source&quot;, fmt.Sprintf(&quot;%s&quot;, watch.src))
				// 上面我们看过了，start就是真正的注册动作
			if err := watch.src.Start(ctx, watch.handler, c.Queue, watch.predicates...); err != nil {
				return err
			}
		}

		// Start the SharedIndexInformer factories to begin populating the SharedIndexInformer caches
		c.LogConstructor(nil).Info(&quot;Starting Controller&quot;)
		 // startWatches上面我们也看到了，是evnetHandle predictates source被缓存到里面，
        // 这里是拿出来将其启动
		for _, watch := range c.startWatches {
			syncingSource, ok := watch.src.(source.SyncingSource)
			if !ok {
				continue
			}

			if err := func() error {
				// use a context with timeout for launching sources and syncing caches.
				sourceStartCtx, cancel := context.WithTimeout(ctx, c.CacheSyncTimeout)
				defer cancel()

				// WaitForSync waits for a definitive timeout, and returns if there
				// is an error or a timeout
				if err := syncingSource.WaitForSync(sourceStartCtx); err != nil {
					err := fmt.Errorf(&quot;failed to wait for %s caches to sync: %w&quot;, c.Name, err)
					c.LogConstructor(nil).Error(err, &quot;Could not wait for Cache to sync&quot;)
					return err
				}

				return nil
			}(); err != nil {
				return err
			}
		}

		// which won't be garbage collected if we hold a reference to it.
		c.startWatches = nil

		// Launch workers to process resources
		c.LogConstructor(nil).Info(&quot;Starting workers&quot;, &quot;worker count&quot;, c.MaxConcurrentReconciles)
		wg.Add(c.MaxConcurrentReconciles)
        // 启动controller消费端的线程
		for i := 0; i &lt; c.MaxConcurrentReconciles; i++ {
			go func() {
				defer wg.Done()
				for c.processNextWorkItem(ctx) {
				}
			}()
		}

		c.Started = true
		return nil
	}()
	if err != nil {
		return err
	}

	&lt;-ctx.Done() // 阻塞，直到上下文关闭
	c.LogConstructor(nil).Info(&quot;Shutdown signal received, waiting for all workers to finish&quot;)
	wg.Wait() // 等待所有线程都关闭
	c.LogConstructor(nil).Info(&quot;All workers finished&quot;)
	return nil
}
</code></pre>
<p>通过上面的分析，可以看到，每个消费的worker线程，实际上调用的是 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/internal/controller/controller.go#L255-L275" target="_blank"
   rel="noopener nofollow noreferrer" >processNextWorkItem</a> 下面就来看看他究竟做了些什么？</p>
<pre><code class="language-go">func (c *Controller) processNextWorkItem(ctx context.Context) bool {
	obj, shutdown := c.Queue.Get() // 从队列中拿取数据
	if shutdown {
		return false
	}

	defer c.Queue.Done(obj)
	// 下面应该是prometheus指标的一些东西
	ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(1)
	defer ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(-1)
	// 获得的对象通过reconcileHandler处理
	c.reconcileHandler(ctx, obj)
	return true
}
</code></pre>
<p>那么下面看看 reconcileHandler 做了些什么</p>
<pre><code class="language-go">func (c *Controller) reconcileHandler(ctx context.Context, obj interface{}) {
	// Update metrics after processing each item
	reconcileStartTS := time.Now()
	defer func() {
		c.updateMetrics(time.Since(reconcileStartTS))
	}()

	// 检查下取出的数据是否为reconcile.Request，在之前enqueue时了解到是插入的这个类型的值
	req, ok := obj.(reconcile.Request)
	if !ok {
		// 如果错了就忘记
		c.Queue.Forget(obj)
		c.LogConstructor(nil).Error(nil, &quot;Queue item was not a Request&quot;, &quot;type&quot;, fmt.Sprintf(&quot;%T&quot;, obj), &quot;value&quot;, obj)
		return
	}

	log := c.LogConstructor(&amp;req)

	log = log.WithValues(&quot;reconcileID&quot;, uuid.NewUUID())
	ctx = logf.IntoContext(ctx, log)

	// 这里调用了自己在实现controller实现的Reconcile的动作
	result, err := c.Reconcile(ctx, req)
	switch {
	case err != nil:
		c.Queue.AddRateLimited(req)
		ctrlmetrics.ReconcileErrors.WithLabelValues(c.Name).Inc()
		ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelError).Inc()
		log.Error(err, &quot;Reconciler error&quot;)
	case result.RequeueAfter &gt; 0:
		c.Queue.Forget(obj)
		c.Queue.AddAfter(req, result.RequeueAfter)
		ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelRequeueAfter).Inc()
	case result.Requeue:
		c.Queue.AddRateLimited(req)
		ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelRequeue).Inc()
	default:
		c.Queue.Forget(obj)
		ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelSuccess).Inc()
	}
}
</code></pre>
<p>通过对example中的 <em>Reconcile</em> 查找其使用，可以看到，调用他的就是上面我们说道的 <code>reconcileHandler</code> ，到这里我们就知道了，controller 的运行流为 <code>Controller.Start()</code> &gt; <code>Controller.processNextWorkItem</code> &gt; <code>Controller.reconcileHandler</code> &gt; <code>Controller.Reconcile</code> 最终到达了我们自定义的业务逻辑处理 Reconcile</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220626231657599.png" alt="image-20220626231657599" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="manager">Manager</h2>
<p>在上面学习 <code>controller-runtime</code> 时了解到，有一个 <code>Manager</code> 的组件，这个组件是做什么呢？我们来分析下。</p>
<p><code>Manager</code> 是用来创建与启动 <code>controller</code> 的（允许多个 <code>controller</code> 与 一个 <code>manager</code> 关联），Manager会启动分配给他的所有controller，以及其他可启动的对象。</p>
<p>在 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/examples/crd/main.go#L104-L145" target="_blank"
   rel="noopener nofollow noreferrer" >example</a> 看到，会初始化一个 <code>ctrl.NewManager</code></p>
<pre><code class="language-go">func main() {
   ctrl.SetLogger(zap.New())

   mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{})
   if err != nil {
      setupLog.Error(err, &quot;unable to start manager&quot;)
      os.Exit(1)
   }

   // in a real controller, we'd create a new scheme for this
   err = api.AddToScheme(mgr.GetScheme())
   if err != nil {
      setupLog.Error(err, &quot;unable to add scheme&quot;)
      os.Exit(1)
   }

   err = ctrl.NewControllerManagedBy(mgr).
      For(&amp;api.ChaosPod{}).
      Owns(&amp;corev1.Pod{}).
      Complete(&amp;reconciler{
         Client: mgr.GetClient(),
         scheme: mgr.GetScheme(),
      })
   if err != nil {
      setupLog.Error(err, &quot;unable to create controller&quot;)
      os.Exit(1)
   }

   err = ctrl.NewWebhookManagedBy(mgr).
      For(&amp;api.ChaosPod{}).
      Complete()
   if err != nil {
      setupLog.Error(err, &quot;unable to create webhook&quot;)
      os.Exit(1)
   }

   setupLog.Info(&quot;starting manager&quot;)
   if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
      setupLog.Error(err, &quot;problem running manager&quot;)
      os.Exit(1)
   }
}
</code></pre>
<p>这个 <code>manager</code> 就是  <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/manager/manager.go#L52-L97" target="_blank"
   rel="noopener nofollow noreferrer" >controller-runtime\pkg\manager\manager.go</a> 下的 <code>Manager</code>， Manager 通过初始化 Caches 和 Clients 等共享依赖，并将它们提供给 Runnables。</p>
<pre><code class="language-go">type Manager interface {
	// 提供了与APIServer交互的方式，如incluster，indexer，cache等
	cluster.Cluster

    // Runnable 是任意可允许的cm中的组件，如 webhook，controller，Caches，在new中调用时，
    // 可以看到是传入的是一个controller，这里可以启动的是带有Start()方法的，通过调用Start()
    // 来启动组件
    Add(Runnable) error
    
    // 实现选举方法。当elected关闭，则选举为leader
	Elected() &lt;-chan struct{}

	// 这为一些列健康检查和指标的方法，和我们关注的没有太大关系
	AddMetricsExtraHandler(path string, handler http.Handler) error
	AddHealthzCheck(name string, check healthz.Checker) error
	AddReadyzCheck(name string, check healthz.Checker) error

	// Start将启动所有注册进来的控制器，直到ctx取消。如果有任意controller报错，则立即退出
    // 如果使用了 LeaderElection，则必须在此返回后立即退出二进制文件，
	Start(ctx context.Context) error

	// GetWebhookServer returns a webhook.Server
	GetWebhookServer() *webhook.Server

	// GetLogger returns this manager's logger.
	GetLogger() logr.Logger

	// GetControllerOptions returns controller global configuration options.
	GetControllerOptions() v1alpha1.ControllerConfigurationSpec
}
</code></pre>
<h3 id="controller-manager">controller-manager</h3>
<p><a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/manager/internal.go#L66-L173" target="_blank"
   rel="noopener nofollow noreferrer" >controllerManager</a> 则实现了这个manager的抽象</p>
<pre><code class="language-go">type controllerManager struct {
	sync.Mutex
	started bool

	stopProcedureEngaged *int64
	errChan              chan error
	runnables            *runnables
	
	cluster cluster.Cluster

	// recorderProvider 用于记录eventhandler source predictate
	recorderProvider *intrec.Provider

	// resourceLock forms the basis for leader election
	resourceLock resourcelock.Interface

	// 在退出时是否关闭选举租约
	leaderElectionReleaseOnCancel bool
	// 一些指标性的，暂时不需要关注
	metricsListener net.Listener
	metricsExtraHandlers map[string]http.Handler
	healthProbeListener net.Listener
	readinessEndpointName string
	livenessEndpointName string
	readyzHandler *healthz.Handler
	healthzHandler *healthz.Handler

	// 有关controller全局参数
	controllerOptions v1alpha1.ControllerConfigurationSpec

	logger logr.Logger

	// 用于关闭 LeaderElection.Run(...) 的信号
	leaderElectionStopped chan struct{}

    // 取消选举，在失去选举后，必须延迟到gracefulShutdown之后os.exit()
	leaderElectionCancel context.CancelFunc

	// leader取消选举
	elected chan struct{}

	port int
	host string
	certDir string
	webhookServer *webhook.Server
	webhookServerOnce sync.Once
	// 非leader节点强制leader的等待时间
	leaseDuration time.Duration
	// renewDeadline is the duration that the acting controlplane will retry
	// refreshing leadership before giving up.
	renewDeadline time.Duration
	// LeaderElector重新操作的时间
	retryPeriod time.Duration
	// gracefulShutdownTimeout 是在manager停止之前让runnables停止的持续时间。
	gracefulShutdownTimeout time.Duration

	// onStoppedLeading is callled when the leader election lease is lost.
	// It can be overridden for tests.
	onStoppedLeading func()

	shutdownCtx context.Context
	internalCtx    context.Context
	internalCancel context.CancelFunc
	internalProceduresStop chan struct{}
}
</code></pre>
<h3 id="workflow">workflow</h3>
<p>了解完ControllerManager之后，我们通过 example 来看看 ControllerManager 的workflow</p>
<pre><code class="language-go">func main() {
   ctrl.SetLogger(zap.New())
   // New一个manager
   mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{})
   if err != nil {
      setupLog.Error(err, &quot;unable to start manager&quot;)
      os.Exit(1)
   }

   // in a real controller, we'd create a new scheme for this
   err = api.AddToScheme(mgr.GetScheme())
   if err != nil {
      setupLog.Error(err, &quot;unable to add scheme&quot;)
      os.Exit(1)
   }

   err = ctrl.NewControllerManagedBy(mgr).
      For(&amp;api.ChaosPod{}).
      Owns(&amp;corev1.Pod{}).
      Complete(&amp;reconciler{
         Client: mgr.GetClient(),
         scheme: mgr.GetScheme(),
      })
   if err != nil {
      setupLog.Error(err, &quot;unable to create controller&quot;)
      os.Exit(1)
   }

   err = ctrl.NewWebhookManagedBy(mgr).
      For(&amp;api.ChaosPod{}).
      Complete()
   if err != nil {
      setupLog.Error(err, &quot;unable to create webhook&quot;)
      os.Exit(1)
   }

   setupLog.Info(&quot;starting manager&quot;)
   if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
      setupLog.Error(err, &quot;problem running manager&quot;)
      os.Exit(1)
   }
}
</code></pre>
<ul>
<li>通过  <code>manager.New()</code> 初始化一个manager，这里面会初始化一些列的manager的参数</li>
<li>通过 <code>ctrl.NewControllerManagedBy</code> 注册 controller 到manager中
<ul>
<li><code>ctrl.NewControllerManagedBy</code>  是 builder的一个别名，构建出一个builder类型的controller</li>
<li><code>builder</code> 中的 <code>ctrl</code> 就是 controller</li>
</ul>
</li>
<li>启动manager</li>
</ul>
<h3 id="builder">builder</h3>
<p>下面看来看下builder在构建时做了什么</p>
<pre><code class="language-go">// Builder builds a Controller.
type Builder struct {
	forInput         ForInput
	ownsInput        []OwnsInput
	watchesInput     []WatchesInput
	mgr              manager.Manager
	globalPredicates []predicate.Predicate
	ctrl             controller.Controller
	ctrlOptions      controller.Options
	name             string
}
</code></pre>
<p>我们看到 example 中是调用了 <code>For()</code> 动作，那么这个   <code>For()</code> 是什么呢？</p>
<p>通过注释，我们可以看到 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/builder/controller.go#L82-L94" target="_blank"
   rel="noopener nofollow noreferrer" >For()</a> 提供了 调解对象类型，ControllerManagedBy 通过  <em>reconciling object</em> 来相应对应<code>create/delete/update</code> 事件。调用 <code>For()</code> 相当于调用了 <code>Watches(&amp;source.Kind{Type: apiType}, &amp;handler.EnqueueRequestForObject{})</code> 。</p>
<pre><code class="language-go">func (blder *Builder) For(object client.Object, opts ...ForOption) *Builder {
	if blder.forInput.object != nil {
		blder.forInput.err = fmt.Errorf(&quot;For(...) should only be called once, could not assign multiple objects for reconciliation&quot;)
		return blder
	}
	input := ForInput{object: object}
	for _, opt := range opts {
		opt.ApplyToFor(&amp;input) //最终把我们要监听的对象每个 opts注册进去
	}

	blder.forInput = input
	return blder
}
</code></pre>
<p>接下来是调用的 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/builder/controller.go#L106-L114" target="_blank"
   rel="noopener nofollow noreferrer" >Owns()</a> ，<code>Owns()</code> 看起来和  <code>For()</code> 功能是类似的。只是说属于不同，是通过Owns方法设置的</p>
<pre><code class="language-go">func (blder *Builder) Owns(object client.Object, opts ...OwnsOption) *Builder {
	input := OwnsInput{object: object}
	for _, opt := range opts {
		opt.ApplyToOwns(&amp;input)
	}

	blder.ownsInput = append(blder.ownsInput, input)
	return blder
}
</code></pre>
<p>最后到了 Complete()，<code>Complete</code> 是完成这个controller的构建</p>
<pre><code class="language-go">// Complete builds the Application Controller.
func (blder *Builder) Complete(r reconcile.Reconciler) error {
	_, err := blder.Build(r)
	return err
}

// Build 创建控制器并返回
func (blder *Builder) Build(r reconcile.Reconciler) (controller.Controller, error) {
	if r == nil {
		return nil, fmt.Errorf(&quot;must provide a non-nil Reconciler&quot;)
	}
	if blder.mgr == nil {
		return nil, fmt.Errorf(&quot;must provide a non-nil Manager&quot;)
	}
	if blder.forInput.err != nil {
		return nil, blder.forInput.err
	}
	// Checking the reconcile type exist or not
	if blder.forInput.object == nil {
		return nil, fmt.Errorf(&quot;must provide an object for reconciliation&quot;)
	}

	// Set the ControllerManagedBy
	if err := blder.doController(r); err != nil {
		return nil, err
	}

	// Set the Watch
	if err := blder.doWatch(); err != nil {
		return nil, err
	}

	return blder.ctrl, nil
}
</code></pre>
<p>这里面可以看到，会完成 doController 和 doWatch</p>
<p>doController会初始化好这个controller并返回</p>
<pre><code class="language-go">func (blder *Builder) doController(r reconcile.Reconciler) error {
	globalOpts := blder.mgr.GetControllerOptions()

	ctrlOptions := blder.ctrlOptions
	if ctrlOptions.Reconciler == nil {
		ctrlOptions.Reconciler = r
	}

	// 通过检索GVK获得默认的名称
	gvk, err := getGvk(blder.forInput.object, blder.mgr.GetScheme())
	if err != nil {
		return err
	}

	// 设置并发，如果最大并发为0则找到一个
    // 追踪下去看似是对于没有设置时，例如会根据 app group中的 ReplicaSet设定
    // 就是在For()传递的一个类型的数量来确定并发的数量
	if ctrlOptions.MaxConcurrentReconciles == 0 {
		groupKind := gvk.GroupKind().String()

		if concurrency, ok := globalOpts.GroupKindConcurrency[groupKind]; ok &amp;&amp; concurrency &gt; 0 {
			ctrlOptions.MaxConcurrentReconciles = concurrency
		}
	}

	// Setup cache sync timeout.
	if ctrlOptions.CacheSyncTimeout == 0 &amp;&amp; globalOpts.CacheSyncTimeout != nil {
		ctrlOptions.CacheSyncTimeout = *globalOpts.CacheSyncTimeout
	}
	// 给controller一个name，如果没有初始化传递，则使用Kind做名称
	controllerName := blder.getControllerName(gvk)

	// Setup the logger.
	if ctrlOptions.LogConstructor == nil {
		log := blder.mgr.GetLogger().WithValues(
			&quot;controller&quot;, controllerName,
			&quot;controllerGroup&quot;, gvk.Group,
			&quot;controllerKind&quot;, gvk.Kind,
		)

		lowerCamelCaseKind := strings.ToLower(gvk.Kind[:1]) + gvk.Kind[1:]

		ctrlOptions.LogConstructor = func(req *reconcile.Request) logr.Logger {
			log := log
			if req != nil {
				log = log.WithValues(
					lowerCamelCaseKind, klog.KRef(req.Namespace, req.Name),
					&quot;namespace&quot;, req.Namespace, &quot;name&quot;, req.Name,
				)
			}
			return log
		}
	}

	// 这里就是构建一个新的控制器了，也就是前面说到的  manager.New()
	blder.ctrl, err = newController(controllerName, blder.mgr, ctrlOptions)
	return err
}
</code></pre>
<p><a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/manager/manager.go#L336-L436" target="_blank"
   rel="noopener nofollow noreferrer" >manager.New()</a></p>
<h3 id="start-manager">start Manager</h3>
<p>接下来是manager的启动，也就是对应的 <code>start()</code> 与 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/builder/controller.go#L220-L270" target="_blank"
   rel="noopener nofollow noreferrer" >doWatch()</a></p>
<p>通过下述代码我们可以看出来，对于 <code>doWatch()</code> 就是把 <code>compete()</code> 前的一些资源的事件函数都注入到controller 中</p>
<pre><code class="language-go">func (blder *Builder) doWatch() error {
	// 调解类型，这也也就是对于For的obj来说，我们需要的是什么结构的，如非结构化数据或metadata-only
    // metadata-only就是配置成一个GVK schema.GroupVersionKind
	typeForSrc, err := blder.project(blder.forInput.object, blder.forInput.objectProjection)
	if err != nil {
		return err
    }&amp;source.Kind{}
    // 一些准备工作，将对象封装为&amp;source.Kind{}
    // 
	src := &amp;source.Kind{Type: typeForSrc}
	hdler := &amp;handler.EnqueueRequestForObject{} // 就是包含obj的一个事件队列
	allPredicates := append(blder.globalPredicates, blder.forInput.predicates...)
	// 这里又到之前说过的controller watch了
    // 将一系列的准备动作注入到cache 如 source eventHandler predicate
    if err := blder.ctrl.Watch(src, hdler, allPredicates...); err != nil {
		return err
	}

	// 再重复 ownsInput 动作
	for _, own := range blder.ownsInput {
		typeForSrc, err := blder.project(own.object, own.objectProjection)
		if err != nil {
			return err
		}
		src := &amp;source.Kind{Type: typeForSrc}
		hdler := &amp;handler.EnqueueRequestForOwner{
			OwnerType:    blder.forInput.object,
			IsController: true,
		}
		allPredicates := append([]predicate.Predicate(nil), blder.globalPredicates...)
		allPredicates = append(allPredicates, own.predicates...)
		if err := blder.ctrl.Watch(src, hdler, allPredicates...); err != nil {
			return err
		}
	}

	// 在对 ownsInput 进行重复的操作
	for _, w := range blder.watchesInput {
		allPredicates := append([]predicate.Predicate(nil), blder.globalPredicates...)
		allPredicates = append(allPredicates, w.predicates...)

		// If the source of this watch is of type *source.Kind, project it.
		if srckind, ok := w.src.(*source.Kind); ok {
			typeForSrc, err := blder.project(srckind.Type, w.objectProjection)
			if err != nil {
				return err
			}
			srckind.Type = typeForSrc
		}

		if err := blder.ctrl.Watch(w.src, w.eventhandler, allPredicates...); err != nil {
			return err
		}
	}
	return nil
}
</code></pre>
<p>由于前两部 <code>builder</code> 的操作将 mgr 指针传入到 builder中，并且操作了 <code>complete()</code> ，也就是操作了 <code>build()</code> ,这代表了对 <code>controller</code> 完成了初始化，和事件注入（<code>watch</code>）的操作，所以 Start()，就是将controller启动</p>
<pre><code class="language-go">func (cm *controllerManager) Start(ctx context.Context) (err error) {
	cm.Lock()
	if cm.started {
		cm.Unlock()
		return errors.New(&quot;manager already started&quot;)
	}
	var ready bool
	defer func() {
		if !ready {
			cm.Unlock()
		}
	}()

	// Initialize the internal context.
	cm.internalCtx, cm.internalCancel = context.WithCancel(ctx)

	// 这个channel代表了controller的停止
	stopComplete := make(chan struct{})
	defer close(stopComplete)
	// This must be deferred after closing stopComplete, otherwise we deadlock.
	defer func() {
		stopErr := cm.engageStopProcedure(stopComplete)
		if stopErr != nil {
			if err != nil {
				err = kerrors.NewAggregate([]error{err, stopErr})
			} else {
				err = stopErr
			}
		}
	}()

	// Add the cluster runnable.
	if err := cm.add(cm.cluster); err != nil {
		return fmt.Errorf(&quot;failed to add cluster to runnables: %w&quot;, err)
	}
    // 指标类
	if cm.metricsListener != nil {
		cm.serveMetrics()
	}
	if cm.healthProbeListener != nil {
		cm.serveHealthProbes()
	}
	if err := cm.runnables.Webhooks.Start(cm.internalCtx); err != nil {
		if !errors.Is(err, wait.ErrWaitTimeout) {
			return err
		}
	}

	// 等待informer同步完成
	if err := cm.runnables.Caches.Start(cm.internalCtx); err != nil {
		if !errors.Is(err, wait.ErrWaitTimeout) {
			return err
		}
	}

	// 非选举模式，runnable将在cache同步完成后启动
	if err := cm.runnables.Others.Start(cm.internalCtx); err != nil {
		if !errors.Is(err, wait.ErrWaitTimeout) {
			return err
		}
	}

	// Start the leader election and all required runnables.
	{
		ctx, cancel := context.WithCancel(context.Background())
		cm.leaderElectionCancel = cancel
		go func() {
			if cm.resourceLock != nil {
				if err := cm.startLeaderElection(ctx); err != nil {
					cm.errChan &lt;- err
				}
			} else {
				// Treat not having leader election enabled the same as being elected.
				if err := cm.startLeaderElectionRunnables(); err != nil {
					cm.errChan &lt;- err
				}
				close(cm.elected)
			}
		}()
	}

	ready = true
	cm.Unlock()
	select {
	case &lt;-ctx.Done():
		// We are done
		return nil
	case err := &lt;-cm.errChan:
		// Error starting or running a runnable
		return err
	}
}
</code></pre>
<p>可以看到上面启动了4种类型的runnable，实际上就是对这runnable进行启动，例如 controller，cache等。</p>
<p>回顾一下，我们之前在使用<code>code-generator</code> 生成，并自定义controller时，我们也是通过启动 <code>informer.Start()</code> ，否则会报错。</p>
<p>最后可以通过一张关系图来表示，client-go与controller-manager之间的关系</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/controller-runtime.svg" alt="svg" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p>Reference</p>
<p><a href="https://vivilearns2code.github.io/k8s/2021/03/12/diving-into-controller-runtime.html#the-manager" target="_blank"
   rel="noopener nofollow noreferrer" >diving controller runtime</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>扩展Kubernetes API的另一种方式 - APIServer aggregation</title>
      <link>https://www.oomkill.com/2022/06/ch04-apiserver-aggregation/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch04-apiserver-aggregation/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<h3 id="what-is-kubernetes-aggregation">What is Kubernetes aggregation</h3>
<p><strong>Kubernetes apiserver aggregation</strong> <em>AA</em> 是Kubernetes提供的一种扩展API的方法，目前并没有GA</p>
<h3 id="difference-between--crd-and-aa">Difference between  CRD and AA</h3>
<p>众所周知，kubernetes扩展API的方法大概为三种：CRD、AA、手动扩展源码。根据<a href="https://www.youtube.com/c/cloudnativefdn" target="_blank"
   rel="noopener nofollow noreferrer" >CNCF分享</a>中Min Kim说的AA更关注于实践，而用户无需了解底层的原理，这里使用过 <code>kubebuilder</code>， <code>code-generator</code> 的用户是很能体会到这点。官方也给出了CRD与AA的区别</p>
<blockquote>
<h3 id="api-access-control">API Access Control</h3>
<h5 id="authentication">Authentication</h5>
<ul>
<li><strong>CR</strong>: All strategies supported. Configured by root apiserver.</li>
<li><strong>AA</strong>: Supporting all root apiserver&rsquo;s authenticating strategies but it has to be done via <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication" target="_blank"
   rel="noopener nofollow noreferrer" >authentication token review api</a> except for <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authenticating-proxy" target="_blank"
   rel="noopener nofollow noreferrer" >authentication proxy</a> which will cause an extra cost of network RTT.</li>
</ul>
<h5 id="authorization">Authorization</h5>
<ul>
<li><strong>CR</strong>: All strategies supported. Configured by root apiserver.</li>
<li><strong>AA</strong>: Delegating authorization requests to root apiserver via <a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/#checking-api-access" target="_blank"
   rel="noopener nofollow noreferrer" >SubjectAccessReview api</a>. Note that this approach will also cost a network RTT.</li>
</ul>
<h5 id="admission-control">Admission Control</h5>
<ul>
<li><strong>CR</strong>: You could extend via dynamic admission control webhook (which is costing network RTT).</li>
<li><strong>AA</strong>: While You can develop and customize your own admission controller which is dedicated to your AA. While You can&rsquo;t reuse root-apiserver&rsquo;s built-in admission controllers nomore.</li>
</ul>
<h3 id="api-schema">API Schema</h3>
<p>Note: CR&rsquo;s integration with OpenAPI schema is being enhanced in the future releases and it will have a stronger integration with OpenAPI mechanism.</p>
<h5 id="validating">Validating</h5>
<ul>
<li><strong>CR</strong>: (landed in 1.12) Defined via OpenAPIv3 Schema grammar. <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#validation" target="_blank"
   rel="noopener nofollow noreferrer" >more</a></li>
<li><strong>AA</strong>: You can customize any validating flow you want.</li>
</ul>
<h5 id="conversion">Conversion</h5>
<ul>
<li><strong>CR</strong>: (landed in 1.13) The CR conversioning (basically from storage version to requested version) could be done via conversioning webhook.</li>
<li><strong>AA</strong>: Develop any conversion you want.</li>
</ul>
<h5 id="subresource">SubResource</h5>
<ul>
<li><strong>CR</strong>: Currently only status and scale sub-resource supported.</li>
<li><strong>AA</strong>: You can customize any sub-resouce you want.</li>
</ul>
<h5 id="openapi-schema">OpenAPI Schema</h5>
<ul>
<li><strong>CR</strong>: (landed in 1.13) The corresponding CRD&rsquo;s OpenAPI schema will be automatically synced to root-apiserver&rsquo;s openapi doc api.</li>
<li><strong>AA</strong>: OpenAPI doc has to be manually generated by code-generating tools.</li>
</ul>
</blockquote>
<h2 id="authentication-1">Authentication</h2>
<p>要想很好的使用AA，就需要对kubernetes与 AA 之间认证机制进行有一定的了解，这里涉及到一些概念</p>
<ul>
<li>客户端证书认证</li>
<li>token认证</li>
<li>请求头认证</li>
</ul>
<p>在下面的说明中，所有出现的APIServer都是指Kubernetes集群组件APIServer也可以为 root APIServer；所有的AA都是指 extension apiserver，就是自行开发的 AA。</p>
<h3 id="客户端证书">客户端证书</h3>
<p>客户端证书就是CA签名的证书，由客户端指定CA证书，在客户端连接时进行身份验证，在Kubernetes APIserver也使用了相同的机制。</p>
<p>默认情况下，APIServer在启动时指定参数 <code>--client-ca-file</code> ，这时APIServer会创建一个名为 <code>extension-apiserver-authentication</code> ，命名空间为 <code>kube-system</code> 下的 configMap。</p>
<pre><code class="language-bash">$ kubectl get cm -A
NAMESPACE     NAME                                 DATA   AGE
kube-system   extension-apiserver-authentication   6      21h

kubectl get cm extension-apiserver-authentication -n kube-system -o yaml
</code></pre>
<p>由上面的命令可以看出这个configMap将被填充到客户端（AA Pod实例）中，使用此CA证书作为用于验证客户端身份的CA。这样客户端会读取这个configMap，与APIServer进行身份认证。</p>
<pre><code class="language-bash">I0622 14:24:00.509486       1 secure_serving.go:178] Serving securely on [::]:443
I0622 14:24:00.509556       1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
</code></pre>
<h3 id="token认证">token认证</h3>
<p>Token认证是指通过HTTP Header传入 <code>Authorization: Bearer $TOKEN</code> 的方式进行客户端认证，这也是Kubernetes集群内认证常用的方法。</p>
<p>在这种情况下，允许对APIServer进行认证也同样可以对AA进行认证。如果不想 AA 对同一集群进行身份验证，或AA在集群外部运行，可以将参数 <code>--authentication-kubeconfig</code> 以指定要使用的不同 Kubeconfig 认证。</p>
<p>下面实例是AA的启动参数</p>
<pre><code class="language-bash">./bin/apiserver -h|grep authentication-kubeconfig
      --authentication-kubeconfig string                        kubeconfig file pointing at the 'core' kubernetes server with enough righ
ts to create tokenreviews.authentication.k8s.io. This is optional. If empty, all token requests are considered to be anonymous and no cli
ent CA is looked up in the cluster.
</code></pre>
<h3 id="请求头认证">请求头认证</h3>
<p>RequestHeader 认证是指，APIServer对来自AA代理连接进行的身份认证。</p>
<p>默认情况下，AA 从 <code>extension-apiserver-authentication</code> 中提到的 ConfigMap 中 提取 requestheader 客户端 CA 证书与 CN。如果主 Kubernetes APIServer 配置了选项 <code>--requestheader-client-ca-file</code> ，则它会填充此内容。</p>
<p>跳过客户端认证 <code>--authentication-skip-lookup</code></p>
<h3 id="授权">授权</h3>
<p>默认情况下，AA  服务器会通过自动注入到 Kubernetes 集群上运行的 pod 的连接信息和凭据，来连接到主 Kubernetes API 服务器。</p>
<pre><code class="language-bash">E0622 11:20:12.375512       1 errors.go:77] Post &quot;https://192.168.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews&quot;: write tcp 192.168.0.36:39324-&gt;192.168.0.1:443: write: connection reset by peer
</code></pre>
<p>如果AA在集群外部部署，可以指定<code>--authorization-kubeconfig</code> 通过kubeconfig进行认证，这就类似于二进制部署中的信息。</p>
<p>默认情况下，Kubernetes 集群会启用RBAC，这就意味着AA 创建多个clusterrolebinding。</p>
<p>下面日志是 AA 对于集群中资源访问无权限的情况</p>
<pre><code class="language-bash">E0622 09:01:26.750320       1 reflector.go:178] pkg/mod/k8s.io/client-go@v0.18.10/tools/cache/reflector.go:125: Failed to list *v1.MutatingWebhookConfiguration: mutatingwebhookconfigurations.admissionregistration.k8s.io is forbidden: User &quot;system:serviceaccount:default:default&quot; cannot list resource &quot;mutatingwebhookconfigurations&quot; in API group &quot;admissionregistration.k8s.io&quot; at the cluster scope
E0622 09:01:29.357897       1 reflector.go:178] pkg/mod/k8s.io/client-go@v0.18.10/tools/cache/reflector.go:125: Failed to list *v1.Namespace: namespaces is forbidden: User &quot;system:serviceaccount:default:default&quot; cannot list resource &quot;namespaces&quot; in API group &quot;&quot; at the cluster scope
E0622 09:01:39.998496       1 reflector.go:178] pkg/mod/k8s.io/client-go@v0.18.10/tools/cache/reflector.go:125: Failed to list *v1.ValidatingWebhookConfiguration: validatingwebhookconfigurations.admissionregistration.k8s.io is forbidden: User &quot;system:serviceaccount:default:default&quot; cannot list resource &quot;validatingwebhookconfigurations&quot; in API group &quot;admissionregistration.k8s.io&quot; at the cluster scope
</code></pre>
<p>需要手动在namespace <code>kube-system</code> 中创建rolebindding到 role  <code>extension-apiserver-authentication-reader</code> 。这样就可以访问到configMap了。</p>
<h2 id="apiserver-builder">apiserver-builder</h2>
<p><code>apiserver-builder</code> 项目就是创建AA的工具，可以参考 <a href="https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/v1.18.0/docs/installing.md" target="_blank"
   rel="noopener nofollow noreferrer" >installing.md</a> 来安装</p>
<h3 id="初始化项目">初始化项目</h3>
<p>初始化命令</p>
<ul>
<li><code>&lt;your-domain&gt;</code> 这个是你的API资源的组，参考 <code>k8s.io/api</code>
<ul>
<li>如果组的名称是域名就设置为主域名，例如内置组
<ul>
<li><code>/apis/authentication.k8s.io</code></li>
<li><code>/apis/batch</code></li>
</ul>
</li>
</ul>
</li>
<li>生成的go mod 包名为你所在的目录的名称
<ul>
<li>例如，在firewalld目录下，go.mod 的名称为 firewalld</li>
</ul>
</li>
</ul>
<pre><code class="language-bash">apiserver-boot init repo --domain &lt;your-domain&gt;
</code></pre>
<p>例如</p>
<pre><code class="language-bash">apiserver-boot init repo --domain fedoraproject.org
</code></pre>
<blockquote>
<p>注：这里&ndash;domain设置为主域名就可以了，后面生成的group会按照格式 <group>+<domain></p>
</blockquote>
<pre><code>apiserver-boot must be run from the directory containing the go package to bootstrap. This must
 be under $GOPATH/src/&lt;package&gt;.
</code></pre>
<p>必须在 <code>$GOPATH/src</code> 下创建你的项目，我这里的为 <code>GOPATH=go/src</code> ，这时创建项目必须在目录 <code>go/src/src/{project}</code>  下创建</p>
<h3 id="创建一个gvk">创建一个GVK</h3>
<pre><code class="language-bash">apiserver-boot create group version resource \
	--group firewalld \
	--version v1 \
	--kind PortRule
</code></pre>
<p>在创建完成之后会生成 api-like的类型，我们只需要填充自己需要的就可以了</p>
<pre><code class="language-go">type PortRule struct {
	metav1.TypeMeta   `json:&quot;,inline&quot;`
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`

	Spec   PortRuleSpec   `json:&quot;spec,omitempty&quot;`
	Status PortRuleStatus `json:&quot;status,omitempty&quot;`
}

// PortRuleSpec defines the desired state of PortRule
type PortRuleSpec struct { // 这里内容都为空的，自己添加即可
	Name        string `json:&quot;name&quot;`
	Host        string `json:&quot;host&quot;`
	Port        int    `json:&quot;port&quot;`
	IsPremanent bool   `json:&quot;isPremanent,omitempty&quot;`
}

// PortRuleStatus defines the observed state of PortRule
type PortRuleStatus struct {
}
</code></pre>
<h3 id="生成代码">生成代码</h3>
<p><code>apiserver-boot</code> 没有专门用来生成代码的命令，可以执行任意生成命令即可，这里使用生成二进制执行文件命令，这个过程相当长。</p>
<pre><code class="language-bash">apiserver-boot build executables
</code></pre>
<p>如果编译错误可以使用 <code>--generate=false</code> 跳过生成，这样就可以节省大量时间。</p>
<h3 id="运行方式">运行方式</h3>
<p>运行方式无非三种，本地运行，集群内运行，集群外运行</p>
<h4 id="running_locally">running_locally</h4>
<p>本地运行需要有一个etcd服务，不用配置ca证书，这里使用docker运行</p>
<pre><code class="language-sh">docker run -d --name Etcd-server \
    --publish 2379:2379 \
    --publish 2380:2380 \
    --env ALLOW_NONE_AUTHENTICATION=yes \
    --env ETCD_ADVERTISE_CLIENT_URLS=http://etcd-server:2379 \
    bitnami/etcd:latest
</code></pre>
<p>然后执行命令，执行成功后会弹出对应的访问地址</p>
<pre><code class="language-bash">apiserver-boot build executables
apiserver-boot run local
</code></pre>
<h4 id="running_in_cluster">running_in_cluster</h4>
<h5 id="构建镜像">构建镜像</h5>
<p>需要先构建容器镜像，<code>apiserver-boot build container --image &lt;image&gt;</code>  这将生成代码，构建 apiserver 和controller二进制文件，然后构建容器映像。构建完成后还需要将对应的镜像push到仓库（可选）</p>
<pre><code class="language-bash">apiserver-boot build config \
	--name &lt;servicename&gt; \
	--namespace &lt;namespace to run in&gt; \
	--image &lt;image to run&gt;
</code></pre>
<blockquote>
<p>注，这个操作需要在支持Linux内核的环境下构建，wsl不具备内核功能故会报错，需要替换为wsl2，而工具是下载的，如果需要wsl1+Docker Desktop构建，需要自己修改</p>
</blockquote>
<h5 id="构建配置">构建配置</h5>
<pre><code>apiserver-boot build config \
	--name &lt;servicename&gt; \
	--namespace &lt;namespace to run in&gt; \
	--image &lt;image to run&gt;
</code></pre>
<p>构建配置的操作会执行以下几个步骤：</p>
<ul>
<li>在 <code>&lt;project/config/certificates</code> 目录下创建一个 CA证书</li>
<li>在目录 <code>&lt;project/config/*.yaml</code> 下生成kubernetes所需的资源清单。</li>
</ul>
<blockquote>
<p>注：</p>
<p>实际上这个清单并不能完美适配任何环境，需要手动修改一下配置</p>
<p>运行的Pod中包含apiserver与controller，如果使用kubebuilder创建的controller可以自行修改资源清单</p>
</blockquote>
<h5 id="修改apiserver的配置">修改apiserver的配置</h5>
<p>下面参数是有关于 AA 认证的参数</p>
<pre><code>--proxy-client-cert-file=/etc/kubernetes/pki/firewalld.crt \
--proxy-client-key-file=/etc/kubernetes/pki/firewalld.key \
--requestheader-allowed-names=kube-apiserver-kubelet-client,firewalld.default.svc,firewalld-certificate-authority \
--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt \
--requestheader-extra-headers-prefix=X-Remote-Extra- \
</code></pre>
<ul>
<li><code>--requestheader-username-headers</code>：用于存储用户名的标头</li>
<li><code>--requestheader-group-headers</code>：用于存储组的标题</li>
<li><code>--requestheader-extra-headers-prefix</code>：附加到所有额外标头的前缀</li>
<li><code>--proxy-client-key-file</code> ：私钥文件</li>
<li><code>--proxy-client-cert-file</code>：客户端证书文件</li>
<li><code>--requestheader-client-ca-file</code>：签署客户端证书文件的 CA 的证书</li>
<li><code>--requestheader-allowed-names</code>：签名客户端证书中的CN)</li>
</ul>
<p>由以上信息得知，实际上 <code>apiserver-boot</code> 所生成的ca用不上，需要kubernetes自己的ca进行签署，这里简单提供两个命令，使用kubernetes集群证书进行颁发证书。这里kubernetes集群证书使用<a href="https://github.com/CylonChau/kubernetes-generator" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes-generator</a> 生产的。这里根据这个ca再次生成用于 AA 认证的证书。</p>
<pre><code>openssl req -new \
    -key firewalld.key \
    -subj &quot;/CN=firewalld.default.svc&quot; \
    -config &lt;(cat /etc/pki/tls/openssl.cnf &lt;(printf &quot;[aa]\nsubjectAltName=DNS:firewalld, DNS:firewalld.default.svc, DNS:firewalld-certificate-authority, DNS:kubernetes.default.svc&quot;)) \
    -out firewalld.csr

openssl ca \
	-in firewalld.csr \
	-cert front-proxy-ca.crt \
	-keyfile front-proxy-ca.key \
	-out firewalld.crt \
	-days 3650 \
	-extensions aa \
	-extfile &lt;(cat /etc/pki/tls/openssl.cnf  &lt;(printf &quot;[aa]\nsubjectAltName=DNS:firewalld, DNS:firewalld.default.svc, DNS:firewalld-certificate-authority, DNS:kubernetes.default.svc&quot;))
</code></pre>
<p>完成后重新生成所需的yaml资源清单即可，通过资源清单来测试下扩展的API</p>
<pre><code class="language-yaml">apiVersion: firewalld.fedoraproject.org/v1
kind: PortRule
metadata:
  name: portrule-example
spec:
  name: &quot;nginx&quot;
  host: &quot;10.0.0.3&quot;
  port: 80

</code></pre>
<pre><code class="language-bash">$ kubectl apply -f http.yaml 
portrule.firewalld.fedoraproject.org/portrule-example created
$ kubectl get portrule
NAME               CREATED AT
portrule-example   2022-06-22T15:12:59Z
</code></pre>
<p>更详细的说明建议阅读下Reference，都是官方提供的详细说明文档</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/" target="_blank"
   rel="noopener nofollow noreferrer" >aggregation layer</a></p>
<p><a href="https://github.com/kubernetes-sigs/apiserver-builder-alpha/tree/master/docs" target="_blank"
   rel="noopener nofollow noreferrer" >apiserver-builder doc</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes代码生成器 - code-generator</title>
      <link>https://www.oomkill.com/2022/06/ch14-code-generator/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch14-code-generator/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>Kubernetes中提供了多种自定义控制器的方式：</p>
<ul>
<li><a href="https://github.com/kubernetes/code-generator" target="_blank"
   rel="noopener nofollow noreferrer" >code-generator</a></li>
<li><a href="https://github.com/kubernetes-sigs/kubebuilder" target="_blank"
   rel="noopener nofollow noreferrer" >kubebuilder</a></li>
<li><a href="https://github.com/operator-framework/operator-sdk" target="_blank"
   rel="noopener nofollow noreferrer" >Operator</a></li>
</ul>
<p>Controller 作为CRD的核心，这里将解释如何使用 <code>code-generator</code> 来创建自定义的控制器，作为文章的案例，将完成一个 Firewalld Port 规则的控制器作为描述，通过 Kubernetes 规则来生成对应节点上的 iptables规则。</p>
<h3 id="prerequisites">Prerequisites</h3>
<h3 id="crd">CRD</h3>
<pre><code class="language-yaml">apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: ports.firewalld.fedoraproject.org
spec:
  group: firewalld.fedoraproject.org
  scope: Namespaced
  names:
    plural: ports
    singular: port
    kind: PortRule
    shortNames: 
    - fp
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              name:
                type: string
              port:
                type: integer
              host:
                type: string
              isPermanent:
                type: boolean

</code></pre>
<h3 id="code-generator">code-generator</h3>
<p>需要预先下载 <code>code-generator</code> 。因为这个工具不是必需要求的。</p>
<blockquote>
<p>注意，下载完成后需要将代码库的的分支更改为你目前使用的版本，版本的选择与client-go类似，如果使用master分支，会与当前的 Kubernetes 集群不兼容。</p>
</blockquote>
<pre><code class="language-bash">git clone https://github.com/kubernetes/code-generator
cd code-generator; git checkout {version}  # eg. v0.18.0
</code></pre>
<h3 id="编写代码模板">编写代码模板</h3>
<p>要想使用 <code>code-generator</code> 生成控制器，必须准备三个文件 <code>doc.go</code> , <code>register.go</code> , <code>types.go</code> 。</p>
<ul>
<li><code>doc.go</code> 中声明了这个包全局内，要使用生成器的tag</li>
<li><code>register.go</code> 类似于kubernetes API，是将声明的类型注册到schema中</li>
<li><code>type.go</code> 是需要具体声明对象类型</li>
</ul>
<h3 id="code-generator-tag说明">code-generator Tag说明</h3>
<p>在使用 <code>code-generator</code> 时，就需要对 <code>code-generator</code> 的tag进行了解。<code>code-generator</code> 的tag是根据几个固定格式进行定义的，tag是 <code>+k8s:</code> + <code>conversion</code> 的组合，在仓库中 <code>cmd</code> 中的 <code>*-gen*</code> 文件夹就代表了 <em>conversion</em> 的替换位置。</p>
<ul>
<li>对于 <code>client-gen</code>的tag 参数可以在 <a href="https://github.com/kubernetes/code-generator/blob/master/cmd/client-gen/generators/util/tags.go" target="_blank"
   rel="noopener nofollow noreferrer" >code-generator\cmd\client-gen\generators\util\tags.go</a></li>
<li>对于其他类型的使用方法，例如 <a href="https://github.com/kubernetes/code-generator/blob/master/cmd/deepcopy-gen/main.go" target="_blank"
   rel="noopener nofollow noreferrer" >deepcopy-gen</a> ,可以在包 main.go中看注释说明
<ul>
<li>+k8s:openapi-gen=true：启用一个生成器</li>
</ul>
</li>
</ul>
<blockquote>
<p>注：最终准备完成的文件（ <code>doc.go</code> , <code>register.go</code> , <code>types.go</code>）应该为：<code>apis/example.com/v1</code> 这种类型的</p>
<p>需要遵循的是，将这些文件放在 <code>&lt;version&gt;</code> 目录中，例如 <code>v1</code> 。这里 <code>v1</code>, <code>v1alpha1</code>, 根据自己需求定义。</p>
</blockquote>
<h2 id="开始填写文件内容">开始填写文件内容</h2>
<h4 id="typego">type.go</h4>
<pre><code class="language-go">package v1

import (
	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
)

// +genclient
// +genclient:noStatus
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
type Port struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// Standard object metadata.
	// +optional
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

	// Specification of the desired behavior of the Deployment.
	// +optional
	Spec PortSpec `json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;`
}

// +k8s:deepcopy-gen=false
type PortSpec struct {
	Name        string `json:&quot;name&quot;`
	Host        string `json:&quot;host&quot;`
	Port        int    `json:&quot;port&quot;`
	IsPermanent bool   `json:&quot;isPermanent&quot;`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
type PortList struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// +optional
	metav1.ListMeta `json:&quot;metadata,omitempty&quot;`

	Items []Port `json:&quot;items&quot;`
}

</code></pre>
<h4 id="docgo">doc.go</h4>
<pre><code class="language-go">// +k8s:deepcopy-gen=package
// +k8s:protobuf-gen=package
// +k8s:openapi-gen=true

// +groupName=firewalld.fedoraproject.org

package v1 // import &quot;k8s.io/api/firewalld/v1&quot;
</code></pre>
<h4 id="registergo">register.go</h4>
<p>这里是从 <a href="https://github.com/kubernetes/kubernetes/tree/master/staging/src/k8s.io/api" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/api</a> 里任意一个复制的，例如 <a href="https://github.com/kubernetes/kubernetes/blob/609db7ed0b1f2839e414c17d29fe4d76edc994bd/staging/src/k8s.io/api/core/v1/register.go#L1" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/api/core/v1/register.go</a></p>
<pre><code class="language-go">package v1

import (
	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	&quot;k8s.io/apimachinery/pkg/runtime&quot;
	&quot;k8s.io/apimachinery/pkg/runtime/schema&quot;
)

// GroupName is the group name use in this package
const GroupName = &quot;firewalld.fedoraproject.org&quot;

// SchemeGroupVersion is group version used to register these objects
var SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: &quot;v1&quot;}

// Resource takes an unqualified resource and returns a Group qualified GroupResource
func Resource(resource string) schema.GroupResource {
	return SchemeGroupVersion.WithResource(resource).GroupResource()
}

var (
	// TODO: move SchemeBuilder with zz_generated.deepcopy.go to k8s.io/api.
	// localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes.
	SchemeBuilder      = runtime.NewSchemeBuilder(addKnownTypes)
	localSchemeBuilder = &amp;SchemeBuilder
	AddToScheme        = localSchemeBuilder.AddToScheme
)

// Adds the list of known types to the given scheme.
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&amp;Port{},
		&amp;PortList{},
	)
	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
	return nil
}
</code></pre>
<h4 id="生成所需文件">生成所需文件</h4>
<p>使用 <code>code-generator</code> 时，实际上就是使用这个库中的脚本 <a href="https://github.com/kubernetes/code-generator/blob/master/generate-groups.sh" target="_blank"
   rel="noopener nofollow noreferrer" >generate-groups.sh</a> ，该脚本又四个参数</p>
<ul>
<li>第一个参数：使用那些生成器，就是 <em>*.gen</em>，用逗号分割，all表示使用全部</li>
<li>第二个参数：client（client-go中informer, lister等）生成的文件存放到哪里</li>
<li>第三个参数：api（api结构，<code>k8s.io/api/</code>） 生成的文件存放到哪里，可以和定义的文件为一个目录</li>
<li>第四个参数：定义group:version</li>
<li>-output-base：输出包存放的根目录</li>
<li>-go-header-file：生成文件的头注释信息，这个是必要参数，除非生成失败</li>
</ul>
<blockquote>
<p>注：对于参数二，三，与-output-base，指定的路径，这里可以使用相对路径也可以使用go.mod中的定义的包名，对于使用相对路径而言，生成的文件中的import也将会为 &ldquo;../../&rdquo; 的格式</p>
</blockquote>
<p>一个完整的示例</p>
<pre><code class="language-bash">../code-generator/generate-groups.sh all \
	../code-controller/client \
	../code-controller/apis  \
	firewalld:v1 \
	--output-base ../code-controller/ \
	--go-header-file ../code-generator/hack/boilerplate.go.txt
</code></pre>
<blockquote>
<p>Reference</p>
<p><a href="https://insujang.github.io/2020-02-13/programming-kubernetes-crd/" target="_blank"
   rel="noopener nofollow noreferrer" >CRD Programming</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>手写一个kubernetes controller</title>
      <link>https://www.oomkill.com/2022/06/ch12-controller/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch12-controller/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>根据Kuberneter文档对<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md" target="_blank"
   rel="noopener nofollow noreferrer" >Controller</a>的描述，Controller在kubernetes中是负责协调的组件，根据设计模式可知，controller会不断的你的对象（如Pod）从当前状态与期望状态同步的一个过程。当然Controller会监听你的实际状态与期望状态。</p>
<h2 id="writing-controllers">Writing Controllers</h2>
<pre><code class="language-go">package main

import (
	&quot;flag&quot;
	&quot;fmt&quot;
	&quot;os&quot;
	&quot;time&quot;

	v1 &quot;k8s.io/api/core/v1&quot;
	&quot;k8s.io/apimachinery/pkg/fields&quot;
	utilruntime &quot;k8s.io/apimachinery/pkg/util/runtime&quot;
	&quot;k8s.io/apimachinery/pkg/util/wait&quot;
	&quot;k8s.io/client-go/kubernetes&quot;
	&quot;k8s.io/client-go/rest&quot;
	&quot;k8s.io/client-go/tools/cache&quot;
	&quot;k8s.io/client-go/tools/clientcmd&quot;
	&quot;k8s.io/client-go/util/homedir&quot;
	&quot;k8s.io/client-go/util/workqueue&quot;
	&quot;k8s.io/klog&quot;
)

type Controller struct {
	lister     cache.Indexer
	controller cache.Controller
	queue      workqueue.RateLimitingInterface
}

func NewController(lister cache.Indexer, controller cache.Controller, queue workqueue.RateLimitingInterface) *Controller {
	return &amp;Controller{
		lister:     lister,
		controller: controller,
		queue:      queue,
	}
}

func (c *Controller) processItem() bool {
	item, quit := c.queue.Get()
	if quit {
		return false
	}
	defer c.queue.Done(item)
	fmt.Println(item)
	err := c.processWrapper(item.(string))
	if err != nil {
		c.handleError(item.(string))
	}
	return true
}

func (c *Controller) handleError(key string) {

	if c.queue.NumRequeues(key) &lt; 3 {
		c.queue.AddRateLimited(key)
		return
	}
	c.queue.Forget(key)
	klog.Infof(&quot;Drop Object %s in queue&quot;, key)
}

func (c *Controller) processWrapper(key string) error {
	item, exists, err := c.lister.GetByKey(key)
	if err != nil {
		klog.Error(err)
		return err
	}
	if !exists {
		klog.Info(fmt.Sprintf(&quot;item %v not exists in cache.\n&quot;, item))
	} else {
		fmt.Println(item.(*v1.Pod).GetName())
	}
	return err
}

func (c *Controller) Run(threadiness int, stopCh chan struct{}) {
	defer utilruntime.HandleCrash()
	defer c.queue.ShutDown()
	klog.Infof(&quot;Starting custom controller&quot;)

	go c.controller.Run(stopCh)

	if !cache.WaitForCacheSync(stopCh, c.controller.HasSynced) {
		utilruntime.HandleError(fmt.Errorf(&quot;sync failed.&quot;))
		return
	}

	for i := 0; i &lt; threadiness; i++ {
		go wait.Until(func() {
			for c.processItem() {
			}
		}, time.Second, stopCh)
	}
	&lt;-stopCh
	klog.Info(&quot;Stopping custom controller&quot;)
}

func main() {
	var (
		k8sconfig  *string //使用kubeconfig配置文件进行集群权限认证
		restConfig *rest.Config
		err        error
	)
	if home := homedir.HomeDir(); home != &quot;&quot; {
		k8sconfig = flag.String(&quot;kubeconfig&quot;, fmt.Sprintf(&quot;%s/.kube/config&quot;, home), &quot;kubernetes auth config&quot;)
	}
	k8sconfig = k8sconfig
	flag.Parse()
	if _, err := os.Stat(*k8sconfig); err != nil {
		panic(err)
	}

	if restConfig, err = rest.InClusterConfig(); err != nil {
		// 这里是从masterUrl 或者 kubeconfig传入集群的信息，两者选一
		restConfig, err = clientcmd.BuildConfigFromFlags(&quot;&quot;, *k8sconfig)
		if err != nil {
			panic(err)
		}
	}
	restset, err := kubernetes.NewForConfig(restConfig)
	lister := cache.NewListWatchFromClient(restset.CoreV1().RESTClient(), &quot;pods&quot;, &quot;default&quot;, fields.Everything())
	queue := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())
	indexer, controller := cache.NewIndexerInformer(lister, &amp;v1.Pod{}, 0, cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			fmt.Println(&quot;add &quot;, obj.(*v1.Pod).GetName())
			key, err := cache.MetaNamespaceKeyFunc(obj)
			if err == nil {
				queue.Add(key)
			}

		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			fmt.Println(&quot;update&quot;, newObj.(*v1.Pod).GetName())
			if newObj.(*v1.Pod).Status.Conditions[0].Status == &quot;True&quot; {
				fmt.Println(&quot;update: the Initialized Status&quot;, newObj.(*v1.Pod).Status.Conditions[0].Status)
			} else {
				fmt.Println(&quot;update: the Initialized Status &quot;, newObj.(*v1.Pod).Status.Conditions[0].Status)
				fmt.Println(&quot;update: the Initialized Reason &quot;, newObj.(*v1.Pod).Status.Conditions[0].Reason)
			}

			if len(newObj.(*v1.Pod).Status.Conditions) &gt; 1 {
				if newObj.(*v1.Pod).Status.Conditions[1].Status == &quot;True&quot; {
					fmt.Println(&quot;update: the Ready Status&quot;, newObj.(*v1.Pod).Status.Conditions[1].Status)
				} else {
					fmt.Println(&quot;update: the Ready Status &quot;, newObj.(*v1.Pod).Status.Conditions[1].Status)
					fmt.Println(&quot;update: the Ready Reason &quot;, newObj.(*v1.Pod).Status.Conditions[1].Reason)
				}

				if newObj.(*v1.Pod).Status.Conditions[2].Status == &quot;True&quot; {
					fmt.Println(&quot;update: the PodCondition Status&quot;, newObj.(*v1.Pod).Status.Conditions[2].Status)
				} else {
					fmt.Println(&quot;update: the PodCondition Status &quot;, newObj.(*v1.Pod).Status.Conditions[2].Status)
					fmt.Println(&quot;update: the PodCondition Reason &quot;, newObj.(*v1.Pod).Status.Conditions[2].Reason)
				}

				if newObj.(*v1.Pod).Status.Conditions[3].Status == &quot;True&quot; {
					fmt.Println(&quot;update: the PodScheduled Status&quot;, newObj.(*v1.Pod).Status.Conditions[3].Status)
				} else {
					fmt.Println(&quot;update: the PodScheduled Status &quot;, newObj.(*v1.Pod).Status.Conditions[3].Status)
					fmt.Println(&quot;update: the PodScheduled Reason &quot;, newObj.(*v1.Pod).Status.Conditions[3].Reason)
				}
			}

		},
		DeleteFunc: func(obj interface{}) {
			fmt.Println(&quot;delete &quot;, obj.(*v1.Pod).GetName(), &quot;Status &quot;, obj.(*v1.Pod).Status.Phase)
			// 上面是事件函数的处理，下面是对workqueue的操作
			key, err := cache.MetaNamespaceKeyFunc(obj)
			if err == nil {
				queue.Add(key)
			}
		},
	}, cache.Indexers{})

	c := NewController(indexer, controller, queue)
	stopCh := make(chan struct{})
	stopCh1 := make(chan struct{})
	c.Run(1, stopCh)
	defer close(stopCh)
	&lt;-stopCh1
}

</code></pre>
<p>通过日志可以看出，Pod create后的步骤大概为4步：</p>
<ul>
<li>Initialized：初始化好后状态为Pending</li>
<li>PodScheduled：然后调度</li>
<li>PodCondition</li>
<li>Ready</li>
</ul>
<pre><code>add  netbox
default/netbox
netbox
update netbox status Pending to Pending
update: the Initialized Status True
update netbox status Pending to Pending
update: the Initialized Status True
update: the Ready Status  False
update: the Ready Reason  ContainersNotReady
update: the PodCondition Status  False
update: the PodCondition Reason  ContainersNotReady
update: the PodScheduled Status True


update netbox status Pending to Running
update: the Initialized Status True
update: the Ready Status True
update: the PodCondition Status True
update: the PodScheduled Status True
</code></pre>
<p>大致上与 <code>kubectl describe pod</code> 看到的内容页相似</p>
<pre><code>default-scheduler  Successfully assigned default/netbox to master-machine
  Normal  Pulling    85s   kubelet            Pulling image &quot;cylonchau/netbox&quot;
  Normal  Pulled     30s   kubelet            Successfully pulled image &quot;cylonchau/netbox&quot;
  Normal  Created    30s   kubelet            Created container netbox
  Normal  Started    30s   kubelet            Started container netbox
</code></pre>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md" target="_blank"
   rel="noopener nofollow noreferrer" >controllers.md</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用CRD扩展Kubernetes API</title>
      <link>https://www.oomkill.com/2022/06/ch13-crd/</link>
      <pubDate>Sun, 19 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch13-crd/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>Kubernetes的主节点或控制面板当中主要有三个组件，其中apiserver是整个系统的数据库，借助于Cluster Store（etcd）服务，来实现所有的包括用户所期望状态的定义，以及集群上资源当前状态的实时记录等。</p>
<p>etcd是分布式通用的K/V系统 <code>KV Store</code> ，可存储用户所定义的任何由KV Store所支持的可持久化的数据。它不仅仅被apiserver所使用，如flannel、calico二者也需要以etcd来保存当前应用程序对应的存储数据。 任何一个分布式应用程序几乎都会用到一个高可用的存储系统。</p>
<p>apiserver将etcd所提供的存储接口做了高度抽象，使用户通过apiserver来完成数据存取时，只能使用apiserver中所内建支持的数据范式。在某种情况之下，我们所期望管理的资源或存储对象在现有的Kubernetes资源无法满足需求时。</p>
<p>Operator本身是建构在StatefulSet以及本身的基本Kubernetes资源之上，由开发者自定义的更高级的、更抽象的自定义资源类型。他可借助于底层的Pod、Service功能，再次抽象出新资源类型。更重要的是，整个集群本身可抽象成一个单一资源。</p>
<p>为了实现更高级的资源管理，需要利用已有的基础资源类型，做一个更高级的抽象，来定义成更能符合用户所需要的、可单一管理的资源类型，而无需去分别管理每一个资源。</p>
<p>在Kubernetes之上自定义资源一般被称为扩展Kubernetes所支持的资源类型，</p>
<ul>
<li>自定义资源类型 CRD Custom Resource Definition</li>
<li>自定义apiserver</li>
<li>修改APIServer源代码，改动内部的资源类型定义</li>
</ul>
<p>CRD是kubernetes内建的资源类型，从而使得用户可以定义的不是具体的资源，而是资源类型，也是扩展Kubernetes最简单的方式。</p>
<h2 id="intorduction-crd">Intorduction CRD</h2>
<h3 id="什么是crd">什么是CRD</h3>
<p>在 Kubernetes API 中，resources 是存储 API 对象集合的endpoint。例如，内置 Pod resource 包含 Pod 对象的集合。当我们想扩展API，原生的Kubernetes就不能满足我们的需求了，这时 <strong>CRD</strong>  (<code>CustomResourceDefinition</code>) 就出现了。在 Kubernetes 中创建了 CRD 后，就可以像使用任何其他原生 Kubernetes 对象一样使用它，从而利用 Kubernetes 的所有功能、如安全性、API 服务、RBAC 等。</p>
<p>Kubernetes 1.7 之后增加了对 CRD 自定义资源二次开发能力来扩展 Kubernetes API，通过 CRD 我们可以向 Kubernetes API 中增加新资源类型，而不需要修改 Kubernetes 源码来创建自定义的 API server，该功能大大提高了 Kubernetes 的扩展能力。</p>
<h2 id="创建-crd">创建 CRD</h2>
<p><strong>前提条件</strong>： Kubernetes 服务器版本必须不低于版本 1.16</p>
<p>再创建新的 <code>CustomResourceDefinition</code>（CRD）时，Kubernetes API 服务器会为指定的每一个版本生成一个 <code>RESTful</code> 的资源路径。（即定义一个Restful API）。CRD 可以是<code>namespace</code>作用域的，也可以是<code>cluster</code>作用域的，取决于 CRD 的 <code>scope</code> 字段设置。和其他现有的内置对象一样，删除一个<code>namespace</code>时，该<code>namespace</code>下的所有定制对象也会被删除。CustomResourceDefinition 本身是不受名字空间限制的，对所有名字空间可用。</p>
<p>例如，编写一个firewall port 规则：</p>
<pre><code class="language-yaml"># 1.16版本后固定格式
apiVersion: apiextensions.k8s.io/v1
# 类型crd
kind: CustomResourceDefinition
metadata:
  # 必须为name=spec.names.plural + spec.group
  name: ports.firewalld.fedoraproject.org
spec:
  # api中的group
  # /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;
  group: firewalld.fedoraproject.org
  # 此crd作用于 可选Namespaced|Cluster
  scope: Namespaced
  names:
    # 名字的复数形式，用于api
    plural: ports
    # 名称的单数形式。用于命令行
    singular: port
    # 种类，资源清单类型
    kind: PortRule
    # 名字简写，类似允许 CLI 上较短的字符串匹配的资源
    shortNames: 
    - fp
  versions:
  # 定义版本的类型
  - name: v1
  # 通过 served 标志来启用或禁止
    served: true
  # 其中一个且只有一个版本必需被标记为存储版本
    storage: true
  # 自定义资源的默认认证的模式
    schema:
  # 使用的版本
      openAPIV3Schema:
        # 定义一个参数为对象类型
        type: object
        # 这个参数的类型
        properties:
        # 参数属性spec
          spec:
            # spec属性的类型为对象
            type: object
            # 对象属性
            properties:
              # spec属性name
              name:
              # 类型为string
                type: string
              port:
                type: integer
              isPermanent:
                type: boolean
</code></pre>
<p>需要注意的是v1.16版本以后已经 <code>GA</code>了，使用的是<code>v1</code>版本，之前都是<code>vlbeta1</code>，定义规范有部分变化，所以要注意版本变化。</p>
<p>这个地方的定义和我们定义普通的资源对象比较类似，我们说我们可以随意定义一个自定义的资源对象，但是在创建资源的时候，肯定不是任由我们随意去编写<code>YAML</code>文件的，当我们把上面的<code>CRD</code>文件提交给Kubernetes之后，Kubernetes会对我们提交的声明文件进行校验，从定义可以看出CRD是基于 <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.1.0.md" target="_blank"
   rel="noopener nofollow noreferrer" >OpenAPIv3 schem</a> 进行规范的。当然这种校验只是<strong>对于字段的类型进行校验</strong>，比较初级，如果想要更加复杂的校验，这个时候就需要通过Kubernetes的admission webhook来实现了。关于校验的更多用法，可以前往官方文档查看。</p>
<p>创建一个crd类型资源</p>
<pre><code class="language-yaml">apiVersion: &quot;firewalld.fedoraproject.org/v1&quot;
kind: PortRule
metadata:
  name: http-port
spec:
  name: &quot;nginx&quot;
  port: 80
  isPermanent: false
</code></pre>
<p>查看创建的crd</p>
<pre><code>$ kubectl get t
NAME                                CREATED AT
firewallds.port.fedoraproject.org   2022-06-19T09:27:09Z
</code></pre>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank"
   rel="noopener nofollow noreferrer" >CRD</a></p>
<p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#customresourcedefinition-v1-apiextensions-k8s-io" target="_blank"
   rel="noopener nofollow noreferrer" >CRD Definition</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析client-go架构 - queue</title>
      <link>https://www.oomkill.com/2022/06/ch09-queue/</link>
      <pubDate>Fri, 17 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch09-queue/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="通用队列">通用队列</h2>
<p>在kubernetes中，使用go的channel无法满足kubernetes的应用场景，如延迟、限速等；在kubernetes中存在三种队列通用队列 <code>common queue</code> ，延迟队列 <code>delaying queue</code>，和限速队列 <code>rate limiters queue</code></p>
<h3 id="inferface">Inferface</h3>
<p>Interface作为所有队列的一个抽象定义</p>
<pre><code class="language-go">type Interface interface {
	Add(item interface{})
	Len() int
	Get() (item interface{}, shutdown bool)
	Done(item interface{})
	ShutDown()
	ShuttingDown() bool
}
</code></pre>
<h3 id="implementation">Implementation</h3>
<pre><code class="language-go">type Type struct { // 一个work queue
	queue []t // queue用slice做存储
	dirty set // 脏位，定义了需要处理的元素，类似于操作系统，表示已修改但为写入
	processing set // 当前正在处理的元素集合
	cond *sync.Cond
	shuttingDown bool
	metrics queueMetrics
	unfinishedWorkUpdatePeriod time.Duration
	clock                      clock.Clock
}
type empty struct{}
type t interface{} // t queue中的元素
type set map[t]empty // dirty 和 processing中的元素
</code></pre>
<p>可以看到其中核心属性就是 <code>queue</code> , <code>dirty</code> , <code>processing</code></p>
<h2 id="延迟队列">延迟队列</h2>
<p>在研究优先级队列前，需要对 <code> Heap</code> 有一定的了解，因为delay queue使用了 <code>heap</code> 做延迟队列</p>
<h3 id="heap">Heap</h3>
<p><code>Heap</code> 是基于树属性的特殊数据结构；heap是一种完全二叉树类型，具有两种类型：</p>
<ul>
<li>如：B 是 A 的子节点，则  $key(A) \geq key(B)$ 。这就意味着具有最大Key的元素始终位于根节点，这类Heap称为最大堆 <strong>MaxHeap</strong>。</li>
<li>父节点的值小于或等于其左右子节点的值叫做 <strong>MinHeap</strong></li>
</ul>
<p>二叉堆的存储规则：</p>
<ul>
<li>每个节点包含的元素大于或等于该节点子节点的元素。</li>
<li>树是完全二叉树。</li>
</ul>
<p>那么下列图片中，那个是堆</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/f.10.1.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>heap的实现</p>
<h4 id="实例向左边添加一个值为42的元素的过程">实例：向左边添加一个值为42的元素的过程</h4>
<p><strong>步骤一</strong>：将新元素放入堆中的第一个可用位置。这将使结构保持为完整的二叉树，但它可能不再是堆，因为新元素可能具有比其父元素更大的值。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/reheap1.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><strong>步骤二</strong>：如果新元素的值大于父元素，将新元素与父元素交换，直到达到新元素到根，或者新元素大于等于其父元素的值时将停止</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/reheap2.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>这种过程被称为 <strong>向上调整</strong> （<code>reheapification upward</code>）</p>
<h4 id="实例移除根">实例：移除根</h4>
<p><strong>步骤一</strong>：将根元素复制到用于返回值的变量中，将最深层的最后一个元素复制到根，然后将最后一个节点从树中取出。该元素称为 <code>out-of-place</code> 。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/reheap3.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><strong>步骤二</strong>：而将异位元素与其最大值的子元素交换，并返回在步骤1中保存的值。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/reheap4.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>这个过程被称为<strong>向下调整</strong> （<code>reheapification downward</code>）</p>
<h3 id="优先级队列">优先级队列</h3>
<p>优先级队列的行为：</p>
<ul>
<li>元素被放置在队列中，然后被取出。</li>
<li>优先级队列中的每个元素都有一个关联的数字，称为优先级。</li>
<li>当元素离开优先级队列时，最高优先级的元素最先离开。</li>
</ul>
<p>如何实现的：</p>
<ul>
<li>
<p>在优先级队列中，heap的每个节点都包含一个元素以及元素的优先级，并且维护树以便它遵循使用元素的优先级来比较节点的堆存储规则：</p>
<ul>
<li>每个节点包含的元素的优先级大于或等于该节点子元素的优先级。</li>
<li>树是完全二叉树。</li>
</ul>
</li>
<li>
<p>实现的代码：<a href="https://pkg.go.dev/container/heap#example__priorityQueue" target="_blank"
   rel="noopener nofollow noreferrer" >golang priorityQueue</a></p>
</li>
</ul>
<blockquote>
<p>Reference</p>
<p><a href="https://www.cpp.edu/~ftang/courses/CS241/notes/heap.htm" target="_blank"
   rel="noopener nofollow noreferrer" >heap</a></p>
</blockquote>
<h3 id="client-go-的延迟队列">Client-go 的延迟队列</h3>
<p>在Kubernetes中对 <code>delaying queue</code> 的设计非常精美，通过使用 <code>heap</code> 实现的延迟队列，加上kubernetes中的通过队列，完成了延迟队列的功能。</p>
<pre><code class="language-go">// 注释中给了一个hot-loop热循环，通过这个loop实现了delaying
type DelayingInterface interface {
	Interface // 继承了workqueue的功能
	AddAfter(item interface{}, duration time.Duration) // 在time后将内容添加到工作队列中
}
</code></pre>
<p>具体实现了 <code>DelayingInterface</code> 的实例</p>
<pre><code class="language-go">type delayingType struct {
	Interface // 通用的queue 
	clock clock.Clock // 对比的时间 ，包含一些定时器的功能
    	type Clock interface {
            PassiveClock
            		type PassiveClock interface {
                        Now() time.Time
                        Since(time.Time) time.Duration
                    }
            After(time.Duration) &lt;-chan time.Time
            NewTimer(time.Duration) Timer
            Sleep(time.Duration)
            NewTicker(time.Duration) Ticker
        }
	stopCh chan struct{} // 停止loop
	stopOnce sync.Once // 保证退出只会触发一次
	heartbeat clock.Ticker // 一个定时器，保证了loop的最大空事件等待时间
	waitingForAddCh chan *waitFor // 普通的chan，用来接收数据插入到延迟队列中
	metrics retryMetrics // 重试的指数
}
</code></pre>
<p>那么延迟队列的整个数据结构如下图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220527215559879.png" alt="image-20220527215559879" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>而上面部分也说到了，这个延迟队列的核心就是一个优先级队列，而优先级队列又需要满足：</p>
<ul>
<li>优先级队列中的每个元素都有一个关联的数字，称为优先级。</li>
<li>当元素离开优先级队列时，最高优先级的元素最先离开。</li>
</ul>
<p>而 <code>waitFor</code> 就是这个优先级队列的数据结构</p>
<pre><code class="language-go">type waitFor struct {
	data    t // 数据
	readyAt time.Time // 加入工作队列的时间
	index int // 优先级队列中的索引
}
</code></pre>
<p>而 <code>waitForPriorityQueue</code> 是对 <code>container/heap/heap.go.Inferface</code> 的实现，其数据结构就是使最小 <code>readyAt</code> 位于Root 的一个 <code>MinHeap</code></p>
<pre><code class="language-go">type Interface interface {
	sort.Interface
	Push(x interface{}) // add x as element Len()
	Pop() interface{}   // remove and return element Len() - 1.
}
</code></pre>
<p>而这个的实现是 <code>waitForPriorityQueue</code></p>
<pre><code class="language-go">type waitForPriorityQueue []*waitFor

func (pq waitForPriorityQueue) Len() int {
	return len(pq)
}
// 这个也是最重要的一个，就是哪个属性是排序的关键，也是heap.down和heap.up中使用的
func (pq waitForPriorityQueue) Less(i, j int) bool {
	return pq[i].readyAt.Before(pq[j].readyAt)
}
func (pq waitForPriorityQueue) Swap(i, j int) {
	pq[i], pq[j] = pq[j], pq[i]
	pq[i].index = i
	pq[j].index = j
}
// push 和pop 必须使用heap.push 和heap.pop
func (pq *waitForPriorityQueue) Push(x interface{}) {
	n := len(*pq)
	item := x.(*waitFor)
	item.index = n
	*pq = append(*pq, item)
}


func (pq *waitForPriorityQueue) Pop() interface{} {
	n := len(*pq)
	item := (*pq)[n-1]
	item.index = -1
	*pq = (*pq)[0:(n - 1)]
	return item
}

// Peek returns the item at the beginning of the queue, without removing the
// item or otherwise mutating the queue. It is safe to call directly.
func (pq waitForPriorityQueue) Peek() interface{} {
	return pq[0]
}
</code></pre>
<p>而整个延迟队列的核心就是 <code>waitingLoop</code>，作为了延迟队列的主要逻辑，检查 <code>waitingForAddCh</code> 有没有要延迟的内容，取出延迟的内容放置到 <code>Heap</code> 中；以及保证最大的阻塞周期</p>
<pre><code class="language-go">func (q *delayingType) waitingLoop() {
	defer utilruntime.HandleCrash()
	never := make(&lt;-chan time.Time) // 作为占位符
	var nextReadyAtTimer clock.Timer // 最近一个任务要执行的定时器
	waitingForQueue := &amp;waitForPriorityQueue{} // 优先级队列，heap
	heap.Init(waitingForQueue)
	waitingEntryByData := map[t]*waitFor{} // 检查是否反复添加

	for {
		if q.Interface.ShuttingDown() {
			return
		}

		now := q.clock.Now()
		for waitingForQueue.Len() &gt; 0 {
			entry := waitingForQueue.Peek().(*waitFor)
			if entry.readyAt.After(now) {
				break // 时间没到则不处理
			}

			entry = heap.Pop(waitingForQueue).(*waitFor) // 从优先级队列中取出一个
			q.Add(entry.data) // 添加到延迟队列中
			delete(waitingEntryByData, entry.data) // 删除map表中的数据
		}

		// 如果存在数据则设置最近一个内容要执行的定时器
		nextReadyAt := never
		if waitingForQueue.Len() &gt; 0 {
			if nextReadyAtTimer != nil {
				nextReadyAtTimer.Stop()
			}
			entry := waitingForQueue.Peek().(*waitFor) // 窥视[0]和值
			nextReadyAtTimer = q.clock.NewTimer(entry.readyAt.Sub(now)) // 创建一个定时器
			nextReadyAt = nextReadyAtTimer.C()
		}

		select {
		case &lt;-q.stopCh: // 退出
			return
		case &lt;-q.heartbeat.C(): // 多久没有任何动作时重新一次循环
		case &lt;-nextReadyAt: // 如果有元素时间到了，则继续执行循环，处理上面添加的操作
		case waitEntry := &lt;-q.waitingForAddCh:
			if waitEntry.readyAt.After(q.clock.Now()) { // 时间没到，是用readyAt和now对比time.Now
				// 添加到延迟队列中，有两个 waitingEntryByData waitingForQueue
				insert(waitingForQueue, waitingEntryByData, waitEntry)
			} else {
				q.Add(waitEntry.data)
			}

			drained := false // 保证可以取完q.waitingForAddCh // addafter
			for !drained {
				select {
                // 这里是一个有buffer的队列，需要保障这个队列读完
				case waitEntry := &lt;-q.waitingForAddCh: 
					if waitEntry.readyAt.After(q.clock.Now()) {
						insert(waitingForQueue, waitingEntryByData, waitEntry)
					} else {
						q.Add(waitEntry.data)
					}
				default: // 保证可以退出，但限制于上一个分支的0~n的读取
				// 如果上一个分支阻塞，则为没有数据就是取尽了，走到这个分支
				// 如果上个分支不阻塞则读取到上个分支阻塞为止，代表阻塞，则走default退出
					drained = true
				}
			}
		}
	}
}
</code></pre>
<h2 id="限速队列">限速队列</h2>
<p>限速队列 <code>RateLimiting</code> 是在优先级队列是在延迟队列的基础上进行扩展的一个队列</p>
<pre><code class="language-go">type RateLimitingInterface interface {
	DelayingInterface // 继承延迟队列
	// 在限速器准备完成后（即合规后）添加条目到队列中
	AddRateLimited(item interface{})
	// drop掉条目，无论成功或失败
	Forget(item interface{})
	// 被重新放入队列中的次数
	NumRequeues(item interface{}) int
}
</code></pre>
<p>可以看到一个限速队列的抽象对应只要满足了 <code>AddRateLimited()</code> , <code>Forget()</code> , <code>NumRequeues()</code> 的延迟队列都是限速队列。看了解规则之后，需要对具体的实现进行分析。</p>
<pre><code class="language-go">type rateLimitingType struct {
	DelayingInterface
	rateLimiter RateLimiter
}

func (q *rateLimitingType) AddRateLimited(item interface{}) {
	q.DelayingInterface.AddAfter(item, q.rateLimiter.When(item))
}

func (q *rateLimitingType) NumRequeues(item interface{}) int {
	return q.rateLimiter.NumRequeues(item)
}

func (q *rateLimitingType) Forget(item interface{}) {
	q.rateLimiter.Forget(item)
}
</code></pre>
<p><code>rateLimitingType</code> 则是对抽象规范 <code>RateLimitingInterface</code> 的实现，可以看出是在延迟队列的基础上增加了一个限速器 <code>RateLimiter</code></p>
<pre><code class="language-go">type RateLimiter interface {
	// when决定等待多长时间
	When(item interface{}) time.Duration
	// drop掉item
	// or for success, we'll stop tracking it
	Forget(item interface{})
	// 重新加入队列中的次数
	NumRequeues(item interface{}) int
}
</code></pre>
<p>抽象限速器的实现，有 <code>BucketRateLimiter</code> , <code>ItemBucketRateLimiter</code> , <code>ItemExponentialFailureRateLimiter</code> , <code>ItemFastSlowRateLimiter</code> ,  <code>MaxOfRateLimiter</code> ，下面对这些限速器进行分析</p>
<h3 id="bucketratelimiter">BucketRateLimiter</h3>
<p><code>BucketRateLimiter</code> 是实现 <code>rate.Limiter</code> 与 抽象 <code>RateLimiter</code> 的一个令牌桶，初始化时通过 <code>workqueue.DefaultControllerRateLimiter()</code> 进行初始化。</p>
<pre><code class="language-go">func DefaultControllerRateLimiter() RateLimiter {
	return NewMaxOfRateLimiter(
		NewItemExponentialFailureRateLimiter(5*time.Millisecond, 1000*time.Second),
		// 10 qps, 100 bucket size.  This is only for retry speed and its only the overall factor (not per item)
		&amp;BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(10), 100)},
	)
}
</code></pre>
<p><a href="https://www.cnblogs.com/Cylon/p/16379709.html" target="_blank"
   rel="noopener nofollow noreferrer" >更多关于令牌桶算法可以参考这里</a></p>
<h3 id="itembucketratelimiter">ItemBucketRateLimiter</h3>
<p><code>ItemBucketRateLimiter</code> 是作为列表存储每个令牌桶的实现，每个key都是单独的限速器</p>
<pre><code class="language-go">type ItemBucketRateLimiter struct {
	r     rate.Limit
	burst int

	limitersLock sync.Mutex
	limiters     map[interface{}]*rate.Limiter
}

func NewItemBucketRateLimiter(r rate.Limit, burst int) *ItemBucketRateLimiter {
	return &amp;ItemBucketRateLimiter{
		r:        r,
		burst:    burst,
		limiters: make(map[interface{}]*rate.Limiter),
	}
}
</code></pre>
<h3 id="itemexponentialfailureratelimiter">ItemExponentialFailureRateLimiter</h3>
<p>如名所知 <code>ItemExponentialFailureRateLimiter</code> 限速器是一个错误指数限速器，根据错误的次数，将指数用于delay的时长，指数的计算公式为：$baseDelay\times2^{<num-failures>}$。 可以看出When绝定了流量整形的delay时间，根据错误次数为指数进行延长重试时间</p>
<pre><code class="language-go">type ItemExponentialFailureRateLimiter struct {
	failuresLock sync.Mutex
	failures     map[interface{}]int // 失败的次数

	baseDelay time.Duration // 延迟基数
	maxDelay  time.Duration // 最大延迟
}

func (r *ItemExponentialFailureRateLimiter) When(item interface{}) time.Duration {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	exp := r.failures[item]
	r.failures[item] = r.failures[item] + 1

	// The backoff is capped such that 'calculated' value never overflows.
	backoff := float64(r.baseDelay.Nanoseconds()) * math.Pow(2, float64(exp))
	if backoff &gt; math.MaxInt64 {
		return r.maxDelay
	}

	calculated := time.Duration(backoff)
	if calculated &gt; r.maxDelay {
		return r.maxDelay
	}

	return calculated
}

func (r *ItemExponentialFailureRateLimiter) NumRequeues(item interface{}) int {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	return r.failures[item]
}

func (r *ItemExponentialFailureRateLimiter) Forget(item interface{}) {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	delete(r.failures, item)
}
</code></pre>
<h3 id="itemfastslowratelimiter">ItemFastSlowRateLimiter</h3>
<p><code>ItemFastSlowRateLimiter </code> ，限速器先快速重试一定次数，然后慢速重试</p>
<pre><code class="language-go">type ItemFastSlowRateLimiter struct {
	failuresLock sync.Mutex
	failures     map[interface{}]int

	maxFastAttempts int // 最大尝试次数
	fastDelay       time.Duration // 快的速度
	slowDelay       time.Duration // 慢的速度
}


func NewItemFastSlowRateLimiter(fastDelay, slowDelay time.Duration, maxFastAttempts int) RateLimiter {
	return &amp;ItemFastSlowRateLimiter{
		failures:        map[interface{}]int{},
		fastDelay:       fastDelay,
		slowDelay:       slowDelay,
		maxFastAttempts: maxFastAttempts,
	}
}

func (r *ItemFastSlowRateLimiter) When(item interface{}) time.Duration {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	r.failures[item] = r.failures[item] + 1
	// 当错误次数没超过快速的阈值使用快速，否则使用慢速
	if r.failures[item] &lt;= r.maxFastAttempts {
		return r.fastDelay
	}

	return r.slowDelay
}

func (r *ItemFastSlowRateLimiter) NumRequeues(item interface{}) int {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	return r.failures[item]
}

func (r *ItemFastSlowRateLimiter) Forget(item interface{}) {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	delete(r.failures, item)
}
</code></pre>
<h3 id="maxofratelimiter">MaxOfRateLimiter</h3>
<p><code>MaxOfRateLimiter</code> 是返回限速器列表中，延迟最大的那个限速器</p>
<pre><code class="language-go">type MaxOfRateLimiter struct {
	limiters []RateLimiter
}

func (r *MaxOfRateLimiter) When(item interface{}) time.Duration {
	ret := time.Duration(0)
	for _, limiter := range r.limiters {
		curr := limiter.When(item)
		if curr &gt; ret {
			ret = curr
		}
	}

	return ret
}

func NewMaxOfRateLimiter(limiters ...RateLimiter) RateLimiter {
	return &amp;MaxOfRateLimiter{limiters: limiters}
}

func (r *MaxOfRateLimiter) NumRequeues(item interface{}) int {
	ret := 0
    // 找到列表內所有的NumRequeues（失败的次数），以最多次的为主。 
	for _, limiter := range r.limiters {
		curr := limiter.NumRequeues(item)
		if curr &gt; ret {
			ret = curr
		}
	}

	return ret
}

func (r *MaxOfRateLimiter) Forget(item interface{}) {
	for _, limiter := range r.limiters {
		limiter.Forget(item)
	}
}
</code></pre>
<h3 id="如何使用kubernetes的限速器">如何使用Kubernetes的限速器</h3>
<p>基于流量管制的限速队列实例，可以大量突发，但是需要进行整形，添加操作会根据 <code>When()</code> 中设计的需要等待的时间进行添加。根据不同的队列实现不同方式的延迟</p>
<pre><code class="language-go">package main

import (
	&quot;fmt&quot;
	&quot;log&quot;
	&quot;strconv&quot;
	&quot;time&quot;

	&quot;k8s.io/client-go/util/workqueue&quot;
)

func main() {
	stopCh := make(chan string)
	timeLayout := &quot;2006-01-02:15:04:05.0000&quot;
	limiter := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())
	length := 20 // 一共请求20次
	chs := make([]chan string, length)
	for i := 0; i &lt; length; i++ {
		chs[i] = make(chan string, 1)
		go func(taskId string, ch chan string) {
			item := &quot;Task-&quot; + taskId + time.Now().Format(timeLayout)
			log.Println(item + &quot; Added.&quot;)
            limiter.AddRateLimited(item) // 添加会根据When() 延迟添加到工作队列中

		}(strconv.FormatInt(int64(i), 10), chs[i])

		go func() {
			for {
				key, quit := limiter.Get()
				if quit {
					return
				}
				log.Println(fmt.Sprintf(&quot;%s process done&quot;, key))
				defer limiter.Done(key)

			}
		}()
	}
	&lt;-stopCh
}
</code></pre>
<p>因为默认的限速器不支持初始化 QPS，修改源码内的为 $BT(1, 5)$ ，执行结果可以看出，大突发流量时，超过桶内token数时，会根据token生成的速度进行放行。</p>
<p>图中，任务的添加是突发性的，日志打印的是同时添加，但是在添加前输出的日志，消费端可以看到实际是被延迟了。配置的是每秒一个token，实际上放行流量也是每秒一个token。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220617173106990.png" alt="image-20220617173106990" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析client-go架构 - 什么是informer</title>
      <link>https://www.oomkill.com/2022/05/ch08-informer/</link>
      <pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/05/ch08-informer/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>之前了解了client-go中的架构设计，也就是 <code>tools/cache</code> 下面的一些概念，那么下面将对informer进行分析</p>
<h2 id="controller">Controller</h2>
<p>在client-go informer架构中存在一个 <code>controller</code> ，这个不是 Kubernetes 中的Controller组件；而是在 <code>tools/cache</code> 中的一个概念，<code>controller</code> 位于 informer 之下，Reflector 之上。<a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/controller.go#L90-L115" target="_blank"
   rel="noopener nofollow noreferrer" >code</a></p>
<h3 id="config">Config</h3>
<p>从严格意义上来讲，<code>controller</code> 是作为一个  <code>sharedInformer</code> 使用，通过接受一个 <code>Config</code> ，而 <code>Reflector</code> 则作为 <code>controller</code> 的 slot。<code>Config</code> 则包含了这个 <code>controller</code> 里所有的设置。</p>
<pre><code class="language-go">type Config struct {
	Queue // DeltaFIFO
	ListerWatcher // 用于list watch的
	Process ProcessFunc // 定义如何从DeltaFIFO中弹出数据后处理的操作
	ObjectType runtime.Object // Controller处理的对象数据，实际上就是kubernetes中的资源
	FullResyncPeriod time.Duration // 全量同步的周期
	ShouldResync ShouldResyncFunc // Reflector通过该标记来确定是否应该重新同步
	RetryOnError bool
}
</code></pre>
<h3 id="controller-1">controller</h3>
<p>然后 <code>controller</code>  又为 <code>reflertor</code> 的上层</p>
<pre><code class="language-go">type controller struct {
	config         Config
	reflector      *Reflector 
	reflectorMutex sync.RWMutex
	clock          clock.Clock
}

type Controller interface {
	// controller 主要做两件事，
    // 1. 构建并运行 Reflector,将listerwacther中的泵压到queue（Delta fifo）中
    // 2. Queue用Pop()弹出数据，具体的操作是Process
    // 直到 stopCh 不阻塞，这两个协程将退出
	Run(stopCh &lt;-chan struct{})
	HasSynced() bool // 这个实际上是从store中继承的，标记这个controller已经
	LastSyncResourceVersion() string
}
</code></pre>
<p><code>controller</code> 中的方法，仅有一个 <code>Run()</code> 和 <code>New()</code>；这意味着，<code>controller</code> 只是一个抽象的概念，作为 <code>Reflector</code>,  <code>Delta FIFO</code> 整合的工作流</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220523224050974.png" alt="image-20220523224050974" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>而 <code>controller</code> 则是 <code>SharedInformer</code> 了。</p>
<h3 id="queue">Queue</h3>
<p>这里的 <code>queue</code> 可以理解为是一个具有 <code>Pop()</code> 功能的 <code>Indexer</code> ;而 <code>Pop()</code> 的功能则是 <code>controller</code> 中的一部分；也就是说 <code>queue</code> 是一个扩展的 <code>Store</code> ， <code>Store</code> 是不具备弹出功能的。</p>
<pre><code class="language-go">type Queue interface {
	Store
	// Pop会阻塞等待，直到有内容弹出，删除对应的值并处理计数器
	Pop(PopProcessFunc) (interface{}, error)

	// AddIfNotPresent puts the given accumulator into the Queue (in
	// association with the accumulator's key) if and only if that key
	// is not already associated with a non-empty accumulator.
	AddIfNotPresent(interface{}) error

	// HasSynced returns true if the first batch of keys have all been
	// popped.  The first batch of keys are those of the first Replace
	// operation if that happened before any Add, Update, or Delete;
	// otherwise the first batch is empty.
	HasSynced() bool
	Close() // 关闭queue
}
</code></pre>
<p>而弹出的操作是通过 controller 中的 <code>processLoop()</code> 进行的，最终走到Delta FIFO中进行处理。</p>
<p>通过忙等待去读取要弹出的数据，然后在弹出前 通过<code>PopProcessFunc</code> 进行处理</p>
<pre><code class="language-go">func (c *controller) processLoop() {
	for {
		obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process))
		if err != nil {
			if err == ErrFIFOClosed {
				return
			}
			if c.config.RetryOnError {
				// This is the safe way to re-enqueue.
				c.config.Queue.AddIfNotPresent(obj)
			}
		}
	}
}
</code></pre>
<p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/delta_fifo.go#L515" target="_blank"
   rel="noopener nofollow noreferrer" >DeltaFIFO.Pop()</a></p>
<pre><code class="language-go">func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) {
	f.lock.Lock()
	defer f.lock.Unlock()
	for {
		for len(f.queue) == 0 {
			// When the queue is empty, invocation of Pop() is blocked until new item is enqueued.
			// When Close() is called, the f.closed is set and the condition is broadcasted.
			// Which causes this loop to continue and return from the Pop().
			if f.IsClosed() {
				return nil, ErrFIFOClosed
			}

			f.cond.Wait()
		}
		id := f.queue[0]
		f.queue = f.queue[1:]
		if f.initialPopulationCount &gt; 0 {
			f.initialPopulationCount--
		}
		item, ok := f.items[id]
		if !ok {
			// Item may have been deleted subsequently.
			continue
		}
		delete(f.items, id)
		err := process(item) // 进行处理
		if e, ok := err.(ErrRequeue); ok {
			f.addIfNotPresent(id, item) // 如果失败，再重新加入到队列中
			err = e.Err 
		}
		// Don't need to copyDeltas here, because we're transferring
		// ownership to the caller.
		return item, err
	}
}
</code></pre>
<h2 id="informer">Informer</h2>
<p>通过对 <code>Reflector</code>, <code>Store</code>, <code>Queue</code>, <code>ListerWatcher</code>、<code>ProcessFunc</code>, 等的概念，发现由 <code>controller</code> 所包装的起的功能并不能完成通过对API的动作监听，并通过动作来处理本地缓存的一个能力；这个情况下诞生了 <code>informer</code> 严格意义上来讲是  <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/controller.go#L317" target="_blank"
   rel="noopener nofollow noreferrer" >sharedInformer</a></p>
<pre><code class="language-go">func newInformer(
	lw ListerWatcher,
	objType runtime.Object,
	resyncPeriod time.Duration,
	h ResourceEventHandler,
	clientState Store,
) Controller {
	// This will hold incoming changes. Note how we pass clientState in as a
	// KeyLister, that way resync operations will result in the correct set
	// of update/delete deltas.
	fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{
		KnownObjects:          clientState,
		EmitDeltaTypeReplaced: true,
	})

	cfg := &amp;Config{
		Queue:            fifo,
		ListerWatcher:    lw,
		ObjectType:       objType,
		FullResyncPeriod: resyncPeriod,
		RetryOnError:     false,

		Process: func(obj interface{}) error {
			// from oldest to newest
			for _, d := range obj.(Deltas) {
				switch d.Type {
				case Sync, Replaced, Added, Updated:
					if old, exists, err := clientState.Get(d.Object); err == nil &amp;&amp; exists {
						if err := clientState.Update(d.Object); err != nil {
							return err
						}
						h.OnUpdate(old, d.Object)
					} else {
						if err := clientState.Add(d.Object); err != nil {
							return err
						}
						h.OnAdd(d.Object)
					}
				case Deleted:
					if err := clientState.Delete(d.Object); err != nil {
						return err
					}
					h.OnDelete(d.Object)
				}
			}
			return nil
		},
	}
	return New(cfg)
}
</code></pre>
<p>newInformer是位于 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/controller.go#L317" target="_blank"
   rel="noopener nofollow noreferrer" >tools/cache/controller.go</a> 下，可以看出，这里面并没有informer的概念，这里通过注释可以看到，newInformer实际上是一个提供了存储和事件通知的informer。他关联的 <code>queue</code> 则是 <code>Delta FIFO</code>，并包含了 <code>ProcessFunc</code>, <code>Store</code> 等 controller的概念。最终对外的方法为 <code>NewInformer()</code></p>
<pre><code class="language-go">func NewInformer(
	lw ListerWatcher,
	objType runtime.Object,
	resyncPeriod time.Duration,
	h ResourceEventHandler,
) (Store, Controller) {
	// This will hold the client state, as we know it.
	clientState := NewStore(DeletionHandlingMetaNamespaceKeyFunc)

	return clientState, newInformer(lw, objType, resyncPeriod, h, clientState)
}

type ResourceEventHandler interface {
	OnAdd(obj interface{})
	OnUpdate(oldObj, newObj interface{})
	OnDelete(obj interface{})
}
</code></pre>
<p>可以看到  <code>NewInformer()</code> 就是一个带有 Store功能的controller，通过这些可以假定出，<strong>Informer</strong> 就是<code>controller</code> ，将queue中相关操作分发给不同事件处理的功能</p>
<h2 id="sharedindexinformer">SharedIndexInformer</h2>
<p><code>shareInformer</code> 为客户端提供了与apiserver一致的数据对象本地缓存，并支持多事件处理程序的<strong>informer</strong>，而 <code>shareIndexInformer </code> 则是对<code>shareInformer</code>  的扩展</p>
<pre><code class="language-go">type SharedInformer interface {
	// AddEventHandler adds an event handler to the shared informer using the shared informer's resync
	// period.  Events to a single handler are delivered sequentially, but there is no coordination
	// between different handlers.
	AddEventHandler(handler ResourceEventHandler)
	// AddEventHandlerWithResyncPeriod adds an event handler to the
	// shared informer with the requested resync period; zero means
	// this handler does not care about resyncs.  The resync operation
	// consists of delivering to the handler an update notification
	// for every object in the informer's local cache; it does not add
	// any interactions with the authoritative storage.  Some
	// informers do no resyncs at all, not even for handlers added
	// with a non-zero resyncPeriod.  For an informer that does
	// resyncs, and for each handler that requests resyncs, that
	// informer develops a nominal resync period that is no shorter
	// than the requested period but may be longer.  The actual time
	// between any two resyncs may be longer than the nominal period
	// because the implementation takes time to do work and there may
	// be competing load and scheduling noise.
	AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration)
	// GetStore returns the informer's local cache as a Store.
	GetStore() Store
	// GetController is deprecated, it does nothing useful
	GetController() Controller
	// Run starts and runs the shared informer, returning after it stops.
	// The informer will be stopped when stopCh is closed.
	Run(stopCh &lt;-chan struct{})
	// HasSynced returns true if the shared informer's store has been
	// informed by at least one full LIST of the authoritative state
	// of the informer's object collection.  This is unrelated to &quot;resync&quot;.
	HasSynced() bool
	// LastSyncResourceVersion is the resource version observed when last synced with the underlying
	// store. The value returned is not synchronized with access to the underlying store and is not
	// thread-safe.
	LastSyncResourceVersion() string
}
</code></pre>
<p><code>SharedIndexInformer</code> 是对SharedInformer的实现，可以从结构中看出，<code>SharedIndexInformer</code> 大致具有如下功能：</p>
<ul>
<li>索引本地缓存</li>
<li>controller，通过list watch拉取API并推入 <code>Deltal FIFO</code></li>
<li>事件的处理</li>
</ul>
<pre><code class="language-go">type sharedIndexInformer struct {
	indexer    Indexer // 具有索引的本地缓存
	controller Controller // controller

	processor             *sharedProcessor // 事件处理函数集合
	cacheMutationDetector MutationDetector

	listerWatcher ListerWatcher
	objectType runtime.Object
	resyncCheckPeriod time.Duration
	defaultEventHandlerResyncPeriod time.Duration
	clock clock.Clock
	started, stopped bool
	startedLock      sync.Mutex
	blockDeltas sync.Mutex
}
</code></pre>
<p>而在 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L397-L444" target="_blank"
   rel="noopener nofollow noreferrer" >tools/cache/share_informer.go</a> 可以看到 shareIndexInformer 的运行过程</p>
<pre><code class="language-go">func (s *sharedIndexInformer) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash()

	fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{
		KnownObjects:          s.indexer,
		EmitDeltaTypeReplaced: true,
	})

	cfg := &amp;Config{
		Queue:            fifo,
		ListerWatcher:    s.listerWatcher,
		ObjectType:       s.objectType,
		FullResyncPeriod: s.resyncCheckPeriod,
		RetryOnError:     false,
		ShouldResync:     s.processor.shouldResync,

		Process: s.HandleDeltas, // process 弹出时操作的流程
	}

	func() {
		s.startedLock.Lock()
		defer s.startedLock.Unlock()

		s.controller = New(cfg)
		s.controller.(*controller).clock = s.clock
		s.started = true
	}()

	// Separate stop channel because Processor should be stopped strictly after controller
	processorStopCh := make(chan struct{})
	var wg wait.Group
	defer wg.Wait()              // Wait for Processor to stop
	defer close(processorStopCh) // Tell Processor to stop
	wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run)
	wg.StartWithChannel(processorStopCh, s.processor.run) // 启动事件处理函数

	defer func() {
		s.startedLock.Lock()
		defer s.startedLock.Unlock()
		s.stopped = true // Don't want any new listeners
	}()
    s.controller.Run(stopCh) // 启动controller，controller会启动Reflector和fifo的Pop()
}
</code></pre>
<p>而在操作Delta FIFO中可以看到，做具体操作时，会将动作分发至对应的事件处理函数中，这个是informer初始化时对事件操作的函数</p>
<pre><code class="language-go">func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error {
	s.blockDeltas.Lock()
	defer s.blockDeltas.Unlock()


	for _, d := range obj.(Deltas) {
		switch d.Type {
		case Sync, Replaced, Added, Updated:
			s.cacheMutationDetector.AddObject(d.Object)
			if old, exists, err := s.indexer.Get(d.Object); err == nil &amp;&amp; exists {
				if err := s.indexer.Update(d.Object); err != nil {
					return err
				}

				isSync := false
				switch {
				case d.Type == Sync:
					isSync = true
				case d.Type == Replaced:
					if accessor, err := meta.Accessor(d.Object); err == nil {
						if oldAccessor, err := meta.Accessor(old); err == nil {
							isSync = accessor.GetResourceVersion() == oldAccessor.GetResourceVersion()
						}
					}
				}
                // 事件的分发
				s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync)
			} else {
				if err := s.indexer.Add(d.Object); err != nil {
					return err
				}
                // 事件的分发
				s.processor.distribute(addNotification{newObj: d.Object}, false)
			}
		case Deleted:
			if err := s.indexer.Delete(d.Object); err != nil {
				return err
			}
			s.processor.distribute(deleteNotification{oldObj: d.Object}, false)
		}
	}
	return nil
}
</code></pre>
<h3 id="事件处理函数-processor">事件处理函数 processor</h3>
<p>启动informer时也会启动注册进来的事件处理函数；<code>processor</code> 就是这个事件处理函数。</p>
<p><code>run()</code> 函数会启动两个 listener，j监听事件处理业务函数 <code>listener.run</code> 和 事件的处理</p>
<pre><code class="language-go">wg.StartWithChannel(processorStopCh, s.processor.run)

func (p *sharedProcessor) run(stopCh &lt;-chan struct{}) {
	func() {
		p.listenersLock.RLock()
		defer p.listenersLock.RUnlock()
		for _, listener := range p.listeners {
			p.wg.Start(listener.run) 
			p.wg.Start(listener.pop)
		}
		p.listenersStarted = true
	}()
	&lt;-stopCh
	p.listenersLock.RLock()
	defer p.listenersLock.RUnlock()
	for _, listener := range p.listeners {
		close(listener.addCh) // Tell .pop() to stop. .pop() will tell .run() to stop
	}
	p.wg.Wait() // Wait for all .pop() and .run() to stop
}
</code></pre>
<p>可以看出，就是拿到的事件，根据注册的到informer的事件函数进行处理</p>
<pre><code class="language-go">func (p *processorListener) run() {
	stopCh := make(chan struct{})
	wait.Until(func() {
		for next := range p.nextCh { // 消费事件
			switch notification := next.(type) {
			case updateNotification:
				p.handler.OnUpdate(notification.oldObj, notification.newObj)
			case addNotification:
				p.handler.OnAdd(notification.newObj)
			case deleteNotification:
				p.handler.OnDelete(notification.oldObj)
			default:
				utilruntime.HandleError(fmt.Errorf(&quot;unrecognized notification: %T&quot;, next))
			}
		}
		// the only way to get here is if the p.nextCh is empty and closed
		close(stopCh)
	}, 1*time.Second, stopCh)
}
</code></pre>
<h3 id="informer中的事件的设计">informer中的事件的设计</h3>
<p>了解了informer如何处理事件，就需要学习下，informer的事件系统设计 <code>prossorListener</code></p>
<h4 id="事件的添加">事件的添加</h4>
<p>当在handleDelta时，会分发具体的事件</p>
<pre><code class="language-go">// 事件的分发
s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync)
</code></pre>
<p>此时，事件泵 <code>Pop()</code> 会根据接收到的事件进行处理</p>
<pre><code class="language-go">// run() 时会启动一个事件泵
p.wg.Start(listener.pop)

func (p *processorListener) pop() {
	defer utilruntime.HandleCrash()
	defer close(p.nextCh) 

	var nextCh chan&lt;- interface{}
	var notification interface{}
	for {
		select {
        case nextCh &lt;- notification: // 这里实际上是一个阻塞的等待
            // 单向channel 可能不会走到这步骤
			var ok bool
            // deltahandle 中 distribute 会将事件添加到addCh待处理事件中
            // 处理完事件会再次拿到一个事件
			notification, ok = p.pendingNotifications.ReadOne()
			if !ok { // Nothing to pop
				nextCh = nil // Disable this select case
			}
        // 处理 分发过来的事件 addCh
		case notificationToAdd, ok := &lt;-p.addCh: // distribute分发的事件
			if !ok {
				return
			}
            // 这里代表第一次，没有任何事件时，或者上面步骤完成读取
			if notification == nil { // 就会走这里
				notification = notificationToAdd 
				nextCh = p.nextCh 
			} else { 
                // notification否则代表没有处理完，将数据再次添加到待处理中
				p.pendingNotifications.WriteOne(notificationToAdd)
			}
		}
	}
}
</code></pre>
<p>该消息事件的流程图为</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220525213837136.png" alt="image-20220525213837136" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>通过一个简单实例来学习client-go中的消息通知机制</p>
<pre><code class="language-go">package main

import (
	&quot;fmt&quot;
	&quot;time&quot;

	&quot;k8s.io/utils/buffer&quot;
)

var nextCh1 = make(chan interface{})
var addCh = make(chan interface{})
var stopper = make(chan struct{})
var notification interface{}
var pendding = *buffer.NewRingGrowing(2)

func main() {
	// pop
	go func() {
		var nextCh chan&lt;- interface{}
		var notification interface{}
		//var n int
		for {
			fmt.Println(&quot;busy wait&quot;)
			fmt.Println(&quot;entry select&quot;, notification)
			select {
			// 初始时，一个未初始化的channel，nil，形成一个阻塞（单channel下是死锁）
			case nextCh &lt;- notification:
				fmt.Println(&quot;entry nextCh&quot;, notification)
				var ok bool
				// 读不到数据代表已处理完，置空锁
				notification, ok = pendding.ReadOne()
				if !ok {
					fmt.Println(&quot;unactive nextch&quot;)
					nextCh = nil
				}
			// 事件的分发，监听，初始时也是一个阻塞
			case notificationToAdd, ok := &lt;-addCh:
				fmt.Println(notificationToAdd, notification)
				if !ok {
					return
				}
				// 线程安全
				// 当消息为空时，没有被处理
				// 锁为空，就分发数据
				if notification == nil {
					fmt.Println(&quot;frist notification nil&quot;)
					notification = notificationToAdd
					nextCh = nextCh1 // 这步骤等于初始化了局部的nextCh，会触发上面的流程
				} else {
					// 在第三次时，会走到这里，数据进入环
					fmt.Println(&quot;into ring&quot;, notificationToAdd)
					pendding.WriteOne(notificationToAdd)
				}
			}
		}
	}()
	// producer
	go func() {
		i := 0
		for {
			i++
			if i%5 == 0 {
				addCh &lt;- fmt.Sprintf(&quot;thread 2 inner -- %d&quot;, i)
				time.Sleep(time.Millisecond * 9000)
			} else {
				addCh &lt;- fmt.Sprintf(&quot;thread 2 outer -- %d&quot;, i)
				time.Sleep(time.Millisecond * 500)
			}
		}
	}()
	// subsriber
	go func() {
		for {
			for next := range nextCh1 {
				time.Sleep(time.Millisecond * 300)
				fmt.Println(&quot;consumer&quot;, next)
			}
		}
	}()
	&lt;-stopper
}
</code></pre>
<p>总结，这里的机制类似于线程安全，进入临界区的一些算法，临界区就是 <code>nextCh</code>，<code>notification</code> 就是保证了至少有一个进程可以进入临界区（要么分发事件，要么生产事件）；<code>nextCh</code> 和 <code>nextCh1</code> 一个是局部管道一个是全局的，管道未初始化代表了死锁（阻塞）；当有消息要处理时，会将局部管道 <code>nextCh</code> 赋值给 全局 <code>nextCh1</code> 此时相当于解除了分发的步骤（对管道赋值，触发分发操作）；<code>ringbuffer</code> 实际上是提供了一个对 <code>notification</code> 加锁的操作，在没有处理的消息时，需要保障 <code>notification</code> 为空，同时也关闭了流程 <code>nextCh</code> 的写入。这里主要是考虑对golang中channel的用法</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes组件核心 - client-go</title>
      <link>https://www.oomkill.com/2022/05/ch06-client-go/</link>
      <pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/05/ch06-client-go/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="prepare">Prepare</h2>
<h3 id="introduction">Introduction</h3>
<p>从2016年8月起，Kubernetes官方提取了与Kubernetes相关的核心源代码，形成了一个独立的项目，即<code>client-go</code>，作为官方提供的go客户端。Kubernetes的部分代码也是基于这个项目的。</p>
<p><code>client-go</code> 是kubernetes中广义的客户端基础库，在Kubernetes各个组件中或多或少都有使用其功能。。也就是说，<code>client-go</code>可以在kubernetes集群中添加、删除和查询资源对象（包括deployment、service、pod、ns等）。</p>
<p>在了解client-go前，还需要掌握一些概念</p>
<ul>
<li>在客户端验证 API</li>
<li>使用证书和使用令牌，来验证客户端</li>
<li>kubernetes集群的访问模式</li>
</ul>
<h3 id="使用证书和令牌来验证客户端">使用证书和令牌来验证客户端</h3>
<p>在访问apiserver时，会对访问者进行鉴权，因为是https请求，在请求时是需要ca的，也可以使用 -k 使用insecure模式</p>
<pre><code>$ curl --cacert /etc/kubernetes/pki/ca.crt https://10.0.0.4:6443/version
\{
  &quot;major&quot;: &quot;1&quot;,
  &quot;minor&quot;: &quot;18+&quot;,
  &quot;gitVersion&quot;: &quot;v1.18.20-dirty&quot;,
  &quot;gitCommit&quot;: &quot;1f3e19b7beb1cc0110255668c4238ed63dadb7ad&quot;,
  &quot;gitTreeState&quot;: &quot;dirty&quot;,
  &quot;buildDate&quot;: &quot;2022-05-17T12:45:14Z&quot;,
  &quot;goVersion&quot;: &quot;go1.16.15&quot;,
  &quot;compiler&quot;: &quot;gc&quot;,
  &quot;platform&quot;: &quot;linux/amd64&quot;
}

$ curl -k https://10.0.0.4:6443/api/v1/namespace/default/pods/netbox
{
  &quot;kind&quot;: &quot;Status&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {
    
  },
  &quot;status&quot;: &quot;Failure&quot;,
  &quot;message&quot;: &quot;namespace \&quot;default\&quot; is forbidden: User \&quot;system:anonymous\&quot; cannot get resource \&quot;namespace/pods\&quot; in API group \&quot;\&quot; at the cluster scope&quot;,
  &quot;reason&quot;: &quot;Forbidden&quot;,
  &quot;details&quot;: {
    &quot;name&quot;: &quot;default&quot;,
    &quot;kind&quot;: &quot;namespace&quot;
  },
  &quot;code&quot;: 403
}
</code></pre>
<p>从错误中可以看出，该请求已通过身份验证，用户是  <code>system:anonymous</code>，但该用户未授权列出对应的资源。而上述请求只是忽略 curl 的https请求需要做的验证，而Kubernetes也有对应验证的机制，这个时候需要提供额外的身份信息来获得所需的访问权限。<a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes支持多种身份认证机制</a>，ssl证书也是其中一种。</p>
<blockquote>
<p>注：在Kubernetes中没有表示用户的资源。即kubernetes集群中，无法添加和创建。但由集群提供的有效证书的用户都视为允许的用户。Kubernetes从证书中的使用者CN和使用者可选名称中获得<strong>用户</strong>；然后，RBAC 判断用户是否有权限操作资源。从 Kubernetes1.4 开始，支持<strong>用户组</strong>，即证书中的O</p>
</blockquote>
<p>可以使用 curl 的 <code>--cert</code> 和 <code>--key</code> 指定用户的证书</p>
<pre><code>curl --cacert /etc/kubernetes/pki/ca.crt  \
	--cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \
	--key /etc/kubernetes/pki/apiserver-ubelet-client.key \
	https://10.0.0.4:6443/api/v1/namespaces/default/pods/netbox
</code></pre>
<h3 id="使用serviceaccount验证客户端身份">使用serviceaccount验证客户端身份</h3>
<p>使用一个serviceaccount JWT，获取一个SA的方式如下</p>
<pre><code>kubectl get secrets \
$(kubectl get serviceaccounts/default -o jsonpath='{.secrets[0].name}')  -o jsonpath='{.data.token}' \
| base64 --decode

JWT=$(kubectl get secrets \
$(kubectl get serviceaccounts/default -o jsonpath='{.secrets[0].name}')  -o jsonpath='{.data.token}' \
| base64 --decode)
</code></pre>
<p>使用secret来访问API</p>
<pre><code>curl --cacert /etc/kubernetes/pki/ca.crt \
	--header &quot;Authorization: Bearer $JWT&quot; \
	https://10.0.0.4:6443/apis/apps/v1/namespaces/default/deployments
</code></pre>
<h3 id="pod内部调用kubernetes-api">Pod内部调用Kubernetes API</h3>
<p>kubernete会将Kubernetes API地址通过环境变量提供给 Pod，可以通过命令看到</p>
<pre><code>$ env|grep -i kuber
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT=tcp://192.168.0.1:443
KUBERNETES_PORT_443_TCP_ADDR=192.168.0.1
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP=tcp://192.168.0.1:443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_HOST=192.168.0.1
</code></pre>
<p>并且还会在将 Kubernetes CA和SA等信息放置在目录 <code>/var/run/secrets/kubernetes.io/serviceaccount/</code>，通过这些就可以从Pod内部访问API</p>
<pre><code>cd /var/run/secrets/kubernetes.io/serviceaccount/

curl --cacert ca.crt --header &quot;Authorization: Bearer $(cat token)&quot; https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces/default/pods/netbox
</code></pre>
<blockquote>
<p>Reference</p>
<p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes API Reference Docs</a></p>
</blockquote>
<h2 id="client-go">client-go</h2>
<h3 id="关于client-go的模块">关于client-go的模块</h3>
<h4 id="k8sioapi">k8s.io/api</h4>
<p>与Pods、ConfigMaps、Secrets和其他Kubernetes 对象所对应的数据结构都在，<a href="https://github.com/kubernetes/api" target="_blank"
   rel="noopener nofollow noreferrer" ><code>k8s.io/api</code></a>，此包几乎没有算法，仅仅是数据机构，该模块有多达上千个用于描述Kubernetes中资源API的结构；通常被client，server，controller等其他的组件使用。</p>
<h4 id="k8sioapimachinery">k8s.io/apimachinery</h4>
<p>根据该库的<a href="https://github.com/kubernetes/apimachinery/tree/3d7c63b4de4fdee1917284129969901d4777facc#purpose" target="_blank"
   rel="noopener nofollow noreferrer" >描述文件</a>可知，这个库是Server和Client中使用的Kubernetes API共享依赖库，也是kubernetes中更低一级的通用的数据结构。在我们构建自定义资源时，不需要为自定义结构创建属性，如 <code>Kind</code>, <code>apiVersion</code>，<code>name</code>&hellip;，这些都是库 <code>apimachinery</code> 所提供的功能。</p>
<p>如，在包 <code>k8s.io/apimachinery/pkg/apis/meta </code> 定义了两个结构 <code>TypeMeta</code> 和 <code>ObjectMeta</code>；将这这两个结构嵌入自定义的结构中，可以以通用的方式兼容对象，如Kubernetes中的资源 <code>Deplyment</code> 也是这么完成的</p>
<center>通过图来了解Kubernetes的资源如何实现的</center>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220523211835602.png" alt="image-20220523211835602" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>如在 <code>k8s.io/apimachinery/pkg/runtime/interfaces.go</code> 中定义了 interface，这个类为在schema中注册的API都需要实现这个结构</p>
<pre><code class="language-go">type Object interface {
	GetObjectKind() schema.ObjectKind
	DeepCopyObject() Object
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220519162127427.png" alt="image-20220519162127427" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="非结构化数据">非结构化数据</h4>
<p>非结构化数据 <code>Unstructured </code> 是指在kubernete中允许将没有注册为Kubernetes API的对象，作为Json对象的方式进行操作，如，<a href="https://github.com/iximiuz/client-go-examples/blob/5b220c4572d65ea8bf0ad68e369e015902e7521c/crud-dynamic-simple/main.go#L36" target="_blank"
   rel="noopener nofollow noreferrer" >使用非结构化 Kubernetes 对象</a></p>
<pre><code class="language-go">desired := &amp;unstructured.Unstructured{
    Object: map[string]interface{}{
        &quot;apiVersion&quot;: &quot;v1&quot;,
        &quot;kind&quot;:       &quot;ConfigMap&quot;,
        &quot;metadata&quot;: map[string]interface{}{
            &quot;namespace&quot;:    namespace,
            &quot;generateName&quot;: &quot;crud-dynamic-simple-&quot;,
        },
        &quot;data&quot;: map[string]interface{}{
            &quot;foo&quot;: &quot;bar&quot;,
        },
    },
}
</code></pre>
<h4 id="非结构化数据的转换">非结构化数据的转换</h4>
<p>在 <code>k8s.io/apimachinery/pkg/runtime.UnstructuredConverter</code> 中，也提供了将非结构化数据转换为Kubernetes API注册过的结构，参考如何将<a href="https://github.com/iximiuz/client-go-examples/tree/main/convert-unstructured-typed" target="_blank"
   rel="noopener nofollow noreferrer" >非结构化对象转换为Kubernetes Object</a>。</p>
<blockquote>
<p>Reference</p>
<p><a href="https://iximiuz.com/en/posts/kubernetes-api-go-types-and-common-machinery/" target="_blank"
   rel="noopener nofollow noreferrer" >go types</a></p>
</blockquote>
<h3 id="install-client-go">install client-go</h3>
<p><strong>如何选择 <code>client-go</code> 的版本</strong></p>
<p>​	对于不同的kubernetes版本使用标签 <code>v0.x.y</code> 来表示对应的客户端版本。具体对应参考 <a href="https://github.com/kubernetes/client-go#compatibility-matrix" target="_blank"
   rel="noopener nofollow noreferrer" >client-go</a> 。</p>
<p>​	例如使用的kubernetes版本为 <code>v1.18.20</code> 则使用对应的标签 <code>v0.x.y</code> 来替换符合当前版本的客户端库。例如：</p>
<pre><code>go get k8s.io/client-go@v0.18.10
</code></pre>
<p>官网中给出了<code>client-go</code>的兼容性矩阵，可以很明了的看出如何选择适用于自己kubernetes版本的对应的client-go</p>
<ul>
<li><code>✓</code> 表示 该版本的 <code>client-go</code> 与对应的 kubernetes版本功能完全一致</li>
<li><code>+</code> <code>client-go</code> 具有 kubernetes apiserver中不具备的功能。</li>
<li><code>-</code>  Kubernetes apiserver 具有<code>client-go</code> 无法使用的功。</li>
</ul>
<p>一般情况下，除了对应的版本号完全一致外，其他都存在 功能的<code>+-</code>。</p>
<h3 id="client-go-目录介绍">client-go 目录介绍</h3>
<p>client-go的每一个目录都是一个go package</p>
<ul>
<li><code>kubernetes</code> 包含与Kubernetes API所通信的客户端集</li>
<li><code>discovery</code>  用于发现kube-apiserver所支持的api</li>
<li><code>dynamic</code>  包含了一个动态客户端，该客户端能够对kube-apiserver任意的API进行操作。</li>
<li><code>transport</code>  提供了用于设置认证和启动链接的功能</li>
<li><code>tools/cache</code>: 一些 low-level controller与一些数据结构如fifo，reflector等</li>
</ul>
<h3 id="structure-of-client-go">structure of client-go</h3>
<ul>
<li>
<p><code>RestClient</code>：是最基础的基础架构，其作用是将是使用了http包进行封装成RESTClient。位于<code>rest</code> 目录，RESTClient封装了资源URL的通用格式，例如<code>Get()</code>、<code>Put()</code>、<code>Post()</code> <code>Delete()</code>。是与Kubernetes API的访问行为提供的基于RESTful方法进行交互基础架构。</p>
<ul>
<li>同时支持Json 与 protobuf</li>
<li>支持所有的原生资源和CRD</li>
</ul>
</li>
<li>
<p><code>ClientSet</code>：Clientset基于RestClient进行封装对 Resource 与 version 管理集合；<a href="https://pkg.go.dev/k8s.io/client-go@v0.24.0/kubernetes#NewForConfig" target="_blank"
   rel="noopener nofollow noreferrer" >如何创建</a></p>
</li>
<li>
<p><code>DiscoverySet</code>：RestClient进行封装，可动态发现 kube-apiserver 所支持的 GVR（Group Version Resource）；<a href="https://pkg.go.dev/k8s.io/client-go@v0.24.0/discovery#NewDiscoveryClient" target="_blank"
   rel="noopener nofollow noreferrer" >如何创建</a>，这种类型是一种非映射至clientset的客户端</p>
</li>
<li>
<p><code>DynamicClient</code>：基于RestClient，包含动态的客户端，可以对Kubernetes所支持的 API对象进行操作，包括CRD；<a href="https://pkg.go.dev/k8s.io/client-go@v0.24.0/dynamic#NewForConfig" target="_blank"
   rel="noopener nofollow noreferrer" >如何创建</a></p>
</li>
<li>
<p>仅支持json</p>
</li>
<li>
<p><code>fakeClient</code>， <code>client-go</code> 实现的mock对象，主要用于单元测试。</p>
</li>
</ul>
<p>以上client-go所提供的客户端，仅可使用kubeconfig进行连接。</p>
<h3 id="什么是clientset">什么是clientset</h3>
<p>clientset代表了kubernetes中所有的资源类型，这里不包含CRD的资源，如：</p>
<ul>
<li><code>core</code></li>
<li><code>extensions</code></li>
<li><code>batch</code></li>
<li>&hellip;</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220518185246688.png" alt="image-20220518185246688" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="client-go使用">client-go使用</h3>
<p><strong>DynamicClient客户端</strong></p>
<ul>
<li>
<p>与 ClientSet 的区别是，可以对任意 Kubernetes 资源进行 RESTful 操作。同样提供管理的方法</p>
</li>
<li>
<p>最大的不同，ClientSet 需要预先实现每种 Resource 和 Version 的操作，内部的数据都是结构化数据（已知数据结构）；DynamicClient 内部实现了 Unstructured，用于处理非结构化的数据（无法提前预知的数据结构），这是其可以处理 CRD 自定义资源的关键。</p>
</li>
</ul>
<p><strong>dynamicClient 实现流程</strong></p>
<ul>
<li>
<p>通过 NewForConfig 实例化 conf 为 DynamicInterface客户端</p>
</li>
<li>
<p><code>DynamicInterface </code>客户端中，实现了一个<code>Resource</code> 方法即为实现了<code>Interface</code>接口</p>
</li>
<li>
<p><code>dynamicClient</code> 实现了非结构化数据类型与rest client，可以通过其方法将<code>Resource</code> 由rest从apiserver中获得api对象，<code>runtime.DeafultUnstructuredConverter.FromUnstructrued</code> 转为对应的类型。</p>
</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220522223023430.png" alt="image-20220522223023430" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<hr>
<p>注意：<code>GVR </code>中资源类型 resource为复数。<code>kind:Pod</code> 即为 <code>Pods</code></p>
<pre><code class="language-go">package main

import (
	&quot;context&quot;
	&quot;flag&quot;
	&quot;fmt&quot;
	&quot;os&quot;

	v1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	&quot;k8s.io/apimachinery/pkg/runtime/schema&quot;
	&quot;k8s.io/client-go/dynamic&quot;
	&quot;k8s.io/client-go/kubernetes&quot;
	&quot;k8s.io/client-go/rest&quot;
	&quot;k8s.io/client-go/tools/clientcmd&quot;
	&quot;k8s.io/client-go/util/homedir&quot;
)

func main() {
	var (
		k8sconfig  *string //使用kubeconfig配置文件进行集群权限认证
		restConfig *rest.Config
		err        error
	)

	if home := homedir.HomeDir(); home != &quot;&quot; {
		k8sconfig = flag.String(&quot;kubeconfig&quot;, fmt.Sprintf(&quot;%s/.kube/config&quot;, home), &quot;kubernetes auth config&quot;)
	}
	k8sconfig = k8sconfig
	flag.Parse()
	if _, err := os.Stat(*k8sconfig); err != nil {
		panic(err)
	}

	if restConfig, err = rest.InClusterConfig(); err != nil {
		// 这里是从masterUrl 或者 kubeconfig传入集群的信息，两者选一
		restConfig, err = clientcmd.BuildConfigFromFlags(&quot;&quot;, *k8sconfig)
		if err != nil {
			panic(err)
		}
	}
	// 创建客户端类型
	// NewForConfig creates a new dynamic client or returns an error.
	// dynamic.NewForConfig(restConfig)
	// NewForConfig creates a new Clientset for the given config
	// kubernetes.NewForConfig(restConfig)
	// NewDiscoveryClientForConfig creates a new DiscoveryClient for the given config.
	//clientset, err := discovery.NewDiscoveryClientForConfig(restConfig)
	dynamicset, err := dynamic.NewForConfig(restConfig)

	// 这里遵循的是 kubernetes Rest API，如Pod是
	// /api/v1/namespaces/{namespace}/pods
	// /apis/apps/v1/namespaces/{namespace}/deployments
	// 遵循GVR格式填写
	podList, err := dynamicset.Resource(schema.GroupVersionResource{
		Group:    &quot;&quot;,
		Version:  &quot;v1&quot;,
		Resource: &quot;pods&quot;,
	}).Namespace(&quot;default&quot;).List(context.TODO(), v1.ListOptions{})
	if err != nil {
		panic(err)
	}

	daemonsetList, err := dynamicset.Resource(schema.GroupVersionResource{
		Group:    &quot;apps&quot;,
		Version:  &quot;v1&quot;,
		Resource: &quot;daemonsets&quot;,
	}).Namespace(&quot;kube-system&quot;).List(context.TODO(), v1.ListOptions{})

	if err != nil {
		panic(err)
	}

	for _, row := range podList.Items {
		fmt.Println(row.GetName())
	}

	for _, row := range daemonsetList.Items {
		fmt.Println(row.GetName())
	}

	// clientset mode

	clientset, err := kubernetes.NewForConfig(restConfig)
	podIns, err := clientset.CoreV1().Pods(&quot;default&quot;).List(context.TODO(), v1.ListOptions{})
	for _, row := range podIns.Items {
		fmt.Println(row.GetName())
	}
}
</code></pre>
<blockquote>
<p>Extension</p>
<p><a href="http://yuezhizizhang.github.io/kubernetes/kubectl/client-go/2020/05/13/kubectl-client-go-part-2.html" target="_blank"
   rel="noopener nofollow noreferrer" >一些client-go使用</a></p>
</blockquote>
<h2 id="informer">Informer</h2>
<p>informer是client-go提供的 <strong>Listwatcher</strong> 接口，主要作为 Controller构成的组件，在Kubernetes中， Controller的一个重要作用是观察对象的期望状态 <code>spec</code> 和实际状态 <code>statue</code> 。<strong>为了观察对象的状态，Controller需要向 Apiserver发送请求</strong>；但是通常情况下，频繁向Apiserver发出请求的会增加etcd的压力，为了解决这类问题，<code>client-go</code> 一个缓存，通过缓存，控制器可以不必发出大量请求，并且只关心对象的事件。也就是 informer。</p>
<p>从本质上来讲，informer是使用kubernetes API观察其变化，来维护状态的缓存，称为 <code>indexer</code>；并通过对应事件函数通知客户端信息的变化，informer为一系列组件，通过这些组件来实现的这些功能。</p>
<ul>
<li>Reflector：与 apiserver交互的组件</li>
<li>Delta FIFO：一个特殊的队列，Reflector将状态的变化存储在里面</li>
<li>indexer：本地存储，与etcd保持一致，减轻API Server与etcd的压力</li>
<li>Processor：监听处理器，通过将监听到的事件发送给对应的监听函数</li>
<li>Controller：从队列中对整个数据的编排处理的过程</li>
</ul>
<h3 id="informer的工作模式">informer的工作模式</h3>
<p>首先通过<code>List</code>从Kubernetes API中获取资源所有对象并同时缓存，然后通过<code>Watch</code>机制监控资源。这样，通过informer与缓存，就可以直接和informer交互，而不用每次都和Kubernetes API交互。</p>
<p>另外，<code>informer</code> 还提供了事件的处理机制，以便 Controller 或其他应用程序根据回调钩子函数等处理特定的业务逻辑。因为<code>Informer</code>可以通过<code>List/Watch</code>机制监控所有资源的所有事件，只要在<code>Informer</code>中添加<code>ResourceEventHandler</code>实例的回调函数，如：<code>onadd(obj interface {})</code>, <code>onupdate (oldobj, newobj interface {})</code>和<code>OnDelete( obj interface {})</code> 可以实现处理资源的创建、更新和删除。 在Kubernetes中，各种控制器都使用了Informer。</p>
<h3 id="分析informer的流程">分析informer的流程</h3>
<p>通过代码 <a href="https://github.com/kubernetes/client-go/blob/master/informers/apps/v1/deployment.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/client-go/informers/apps/v1/deployment.go</a> 可以看出，在每个控制器下，都实现了一个 <code>Informer</code> 和 <code>Lister</code> ，Lister就是indexer；</p>
<pre><code class="language-go">type SharedInformer interface {
    // 添加一个事件处理函数，使用informer默认的resync period
	AddEventHandler(handler ResourceEventHandler)
    // 将事件处理函数注册到 share informer，将resyncPeriod作为参数传入
	AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration)
	// 从本地缓存获取的信息作为infomer的返回
	GetStore() Store
	// 已弃用
	GetController() Controller
	// 运行一个informer，当stopCh停止时，informer也被关闭
	Run(stopCh &lt;-chan struct{})
	// HasSynced returns true if the shared informer's store has been
	// informed by at least one full LIST of the authoritative state
	// of the informer's object collection.  This is unrelated to &quot;resync&quot;.
	HasSynced() bool
	// LastSyncResourceVersion is the resource version observed when last synced with the underlying store. The value returned is not synchronized with access to the underlying store and is not thread-safe.
	LastSyncResourceVersion() string
}
</code></pre>
<p>而 Shared Informer 对所有的API组提供一个shared informer</p>
<pre><code class="language-go">// SharedInformerFactory provides shared informers for resources in all known
// API group versions.
type SharedInformerFactory interface {
	internalinterfaces.SharedInformerFactory
	ForResource(resource schema.GroupVersionResource) (GenericInformer, error)
	WaitForCacheSync(stopCh &lt;-chan struct{}) map[reflect.Type]bool

	Admissionregistration() admissionregistration.Interface
	Apps() apps.Interface
	Auditregistration() auditregistration.Interface
	Autoscaling() autoscaling.Interface
	Batch() batch.Interface
	Certificates() certificates.Interface
	Coordination() coordination.Interface
	Core() core.Interface
	Discovery() discovery.Interface
	Events() events.Interface
	Extensions() extensions.Interface
	Flowcontrol() flowcontrol.Interface
	Networking() networking.Interface
	Node() node.Interface
	Policy() policy.Interface
	Rbac() rbac.Interface
	Scheduling() scheduling.Interface
	Settings() settings.Interface
	Storage() storage.Interface
}
</code></pre>
<p>可以看到在 <a href="https://github.com/kubernetes/kubernetes/blob/v1.9.0/staging/src/k8s.io/client-go/informers/apps/v1/deployment.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/client-go/informers/apps/v1/deployment.go</a> 实现了这个interface</p>
<pre><code class="language-go">type DeploymentInformer interface {
   Informer() cache.SharedIndexInformer
   Lister() v1.DeploymentLister
}
</code></pre>
<p>而在对应的 deployment controller中会调用这个<code>Informer</code> 实现对状态的监听；``</p>
<pre><code class="language-go">// NewDeploymentController creates a new DeploymentController.
//  appsinformers.DeploymentInformer就是client-go 中的 /apps/v1/deployment实现的informer
func NewDeploymentController(dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) {
	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartLogging(klog.Infof)
	eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl{Interface: client.CoreV1().Events(&quot;&quot;)})

	if client != nil &amp;&amp; client.CoreV1().RESTClient().GetRateLimiter() != nil {
		if err := ratelimiter.RegisterMetricAndTrackRateLimiterUsage(&quot;deployment_controller&quot;, client.CoreV1().RESTClient().GetRateLimiter()); err != nil {
			return nil, err
		}
	}
	dc := &amp;DeploymentController{
		client:        client,
		eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: &quot;deployment-controller&quot;}),
		queue:         workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;deployment&quot;),
	}
	dc.rsControl = controller.RealRSControl{
		KubeClient: client,
		Recorder:   dc.eventRecorder,
	}

	dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    dc.addDeployment,
		UpdateFunc: dc.updateDeployment,
		// This will enter the sync loop and no-op, because the deployment has been deleted from the store.
		DeleteFunc: dc.deleteDeployment,
	})
	rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    dc.addReplicaSet,
		UpdateFunc: dc.updateReplicaSet,
		DeleteFunc: dc.deleteReplicaSet,
	})
	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		DeleteFunc: dc.deletePod,
	})

	dc.syncHandler = dc.syncDeployment
	dc.enqueueDeployment = dc.enqueue

	dc.dLister = dInformer.Lister()
	dc.rsLister = rsInformer.Lister()
	dc.podLister = podInformer.Lister()
	dc.dListerSynced = dInformer.Informer().HasSynced
	dc.rsListerSynced = rsInformer.Informer().HasSynced
	dc.podListerSynced = podInformer.Informer().HasSynced
	return dc, nil
}
</code></pre>
<h2 id="reflector">Reflector</h2>
<p>reflector是client-go中负责监听 Kubernetes API 的组件，也是整个机制中的生产者，负责将 watch到的数据将其放入 <code>watchHandler</code> 中的delta FIFO队列中。也就是吧etcd的数据反射为 delta fifo的数据</p>
<p>在代码 <a href="https://github.com/kubernetes/kubernetes/blob/v1.9.0/staging/src/k8s.io/client-go/tools/cache/reflector.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/client-go/tools/cache/reflector.go</a> 中定义了 Reflector 对象</p>
<pre><code class="language-go">type Reflector struct {
    // reflector的名称，默认为一个 file:line的格式
	name string
    // 期待的类型名称，这里只做展示用，
    // 如果提供，是一个expectedGVK字符串类型，否则是expectedType字符串类型
	expectedTypeName string
    // 期待放置在存储中的类型，如果是一个非格式化数据，那么其 APIVersion与Kind也必须为正确的格式
	expectedType reflect.Type
    // GVK 存储中的对象，是GVK格式
	expectedGVK *schema.GroupVersionKind
	// 同步数据的存储
	store Store
	// 这个是reflector的一个核心，提供了 List和Watch功能
	listerWatcher ListerWatcher

	// backoff manages backoff of ListWatch
	backoffManager wait.BackoffManager

	resyncPeriod time.Duration
	
	ShouldResync func() bool
	// clock allows tests to manipulate time
	clock clock.Clock
	
	paginatedResult bool
	// 最后资源的版本号
	lastSyncResourceVersion string
    // 当 lastSyncResourceVersion 过期或者版本太大，这个值将为 true
	isLastSyncResourceVersionUnavailable bool
    // 读写锁，对lastSyncResourceVersion的读写操作的保护
	lastSyncResourceVersionMutex sync.RWMutex
	// WatchListPageSize is the requested chunk size of initial and resync watch lists.
	// scalability problems.
    // 是初始化时，或者重新同步时的块大小。如果没有设置，将为任意的旧数据
    // 因为是提供了分页功能，RV=0则为默认的页面大小
    // 
	WatchListPageSize int64
}
</code></pre>
<p>而 方法 <code>NewReflector()</code> 给用户提供了一个初始化 Reflector的接口</p>
<p>在 cotroller.go 中会初始化一个 relector</p>
<pre><code class="language-go">func (c *controller) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash()
	go func() {
		&lt;-stopCh
		c.config.Queue.Close()
	}()
	r := NewReflector(
		c.config.ListerWatcher,
		c.config.ObjectType,
		c.config.Queue,
		c.config.FullResyncPeriod,
	)
</code></pre>
<p>Reflector下有三个可对用户提供的方法，<code>Run()</code>, <code>ListAndWatch()</code> , <code>LastSyncResourceVersion()</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220519222729807.png" alt="image-20220519222729807" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><code>Run()</code> 是对Reflector的运行，也就是对 <code>ListAndWatch()</code> ；</p>
<pre><code class="language-go">func (r *Reflector) Run(stopCh &lt;-chan struct{}) {
	klog.V(2).Infof(&quot;Starting reflector %s (%s) from %s&quot;, r.expectedTypeName, r.resyncPeriod, r.name)
	wait.BackoffUntil(func() {
		if err := r.ListAndWatch(stopCh); err != nil {
			utilruntime.HandleError(err)
		}
	}, r.backoffManager, true, stopCh)
	klog.V(2).Infof(&quot;Stopping reflector %s (%s) from %s&quot;, r.expectedTypeName, r.resyncPeriod, r.name)
}
</code></pre>
<p>而 <code>ListAndWatch()</code> 则是实际上真实的对Reflector业务的执行</p>
<pre><code class="language-go">// 前面一些都是对信息的初始化与日志输出
func (r *Reflector) ListAndWatch(stopCh &lt;-chan struct{}) error {
	klog.V(3).Infof(&quot;Listing and watching %v from %s&quot;, r.expectedTypeName, r.name)
	var resourceVersion string

	options := metav1.ListOptions{ResourceVersion: r.relistResourceVersion()}
	// 分页功能
	if err := func() error {
		initTrace := trace.New(&quot;Reflector ListAndWatch&quot;, trace.Field{&quot;name&quot;, r.name})
		defer initTrace.LogIfLong(10 * time.Second)
		var list runtime.Object
		var paginatedResult bool
		var err error
		listCh := make(chan struct{}, 1)
		panicCh := make(chan interface{}, 1)
		go func() {
			....
	// 清理和重新同步的一些
	resyncerrc := make(chan error, 1)
	cancelCh := make(chan struct{})
	defer close(cancelCh)
	go func() {
		...
	}()

	for {
		// give the stopCh a chance to stop the loop, even in case of continue statements further down on errors
		select {
		case &lt;-stopCh:
			return nil
		default:
		}

		timeoutSeconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0))
		options = metav1.ListOptions{
			ResourceVersion: resourceVersion,
			// 为了避免watch的挂起设置一个超时
            // 仅在工作窗口期，处理任何时间
			TimeoutSeconds: &amp;timeoutSeconds,
			// To reduce load on kube-apiserver on watch restarts, you may enable watch bookmarks.
			// Reflector doesn't assume bookmarks are returned at all (if the server do not support
			// watch bookmarks, it will ignore this field).
			AllowWatchBookmarks: true,
		}

		start := r.clock.Now()
        // 开始监听
		w, err := r.listerWatcher.Watch(options)
		if err != nil {
			switch {
			case isExpiredError(err):
				// 没有设置 LastSyncResourceVersionExpired 也就是过期，会保持与返回数据相同的
				// 首次会先将RV列出
				klog.V(4).Infof(&quot;%s: watch of %v closed with: %v&quot;, r.name, r.expectedTypeName, err)
			case err == io.EOF:
				// 通常为watch关闭
			case err == io.ErrUnexpectedEOF:
				klog.V(1).Infof(&quot;%s: Watch for %v closed with unexpected EOF: %v&quot;, r.name, r.expectedTypeName, err)
			default:
				utilruntime.HandleError(fmt.Errorf(&quot;%s: Failed to watch %v: %v&quot;, r.name, r.expectedTypeName, err))
			}
			// 如果出现 connection refuse，通常与apisserver通讯失败，这个时候会重新发送请求
			if utilnet.IsConnectionRefused(err) {
				time.Sleep(time.Second)
				continue
			}
			return nil
		}

		if err := r.watchHandler(start, w, &amp;resourceVersion, resyncerrc, stopCh); err != nil {
			if err != errorStopRequested {
				switch {
				case isExpiredError(err):
					// 同上步骤的功能
					klog.V(4).Infof(&quot;%s: watch of %v closed with: %v&quot;, r.name, r.expectedTypeName, err)
				default:
					klog.Warningf(&quot;%s: watch of %v ended with: %v&quot;, r.name, r.expectedTypeName, err)
				}
			}
			return nil
		}
	}
}
</code></pre>
<p>那么在实现时，如 deploymentinformer,会实现 Listfunc和 watchfunc，这其实就是clientset中的操作方法，也是就list与watch</p>
<pre><code class="language-go">func NewFilteredDeploymentInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
	return cache.NewSharedIndexInformer(
		&amp;cache.ListWatch{
			ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
				if tweakListOptions != nil {
					tweakListOptions(&amp;options)
				}
				return client.AppsV1().Deployments(namespace).List(context.TODO(), options)
			},
			WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
				if tweakListOptions != nil {
					tweakListOptions(&amp;options)
				}
				return client.AppsV1().Deployments(namespace).Watch(context.TODO(), options)
			},
		},
		&amp;appsv1.Deployment{},
		resyncPeriod,
		indexers,
	)
}
</code></pre>
<p><code>tools/cache/controller.go</code> 是存储controller的配置及实现。</p>
<pre><code class="language-go">type Config struct {
	Queue // 对象的队列，必须为DeltaFIFO
	ListerWatcher // 这里能够监视并列出对象的一些信息，这个对象接受process函数的弹出
	// Something that can process a popped Deltas.
	Process ProcessFunc // 处理Delta的弹出
    // 对象类型，这个controller期待的处理类型，其apiServer与kind必须正确，即，GVR必须正确
	ObjectType runtime.Object
        // FullResyncPeriod是每次重新同步的时间间隔
	FullResyncPeriod time.Duration
        // type ShouldResyncFunc func() bool
    // 返回值nil或true，则表示reflector继续同步
	ShouldResync ShouldResyncFunc
    RetryOnError bool // 标志位，true时，在process()返回错误时重新排列对象
	// Called whenever the ListAndWatch drops the connection with an error.
    // 断开连接是出现错误调用这个函数处理
	WatchErrorHandler WatchErrorHandler
	// WatchListPageSize is the requested chunk size of initial and relist watch lists.
	WatchListPageSize int64
}
</code></pre>
<p>实现这个接口</p>
<pre><code class="language-go">type controller struct {
	config         Config
	reflector      *Reflector
	reflectorMutex sync.RWMutex
	clock          clock.Clock
}
</code></pre>
<p><code>New()</code> 为给定controller 配置的设置，即为上面的config struct，用来初始化controller对象</p>
<p><code>NewInformer()</code> ：返回一个store（保存数据的最终接口）和一个用于store的controller，同时提供事件的通知(crud)等</p>
<p><code>NewIndexerInformer()</code>：返回一个索引与一个用于索引填充的控制器</p>
<p>控制器的run()的功能实现</p>
<pre><code class="language-go">func (c *controller) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash() // 延迟销毁
	go func() {  // 信号处理，用于线程管理
		&lt;-stopCh
		c.config.Queue.Close()
	}() 
	r := NewReflector(  // 初始化Reflector
		c.config.ListerWatcher, // ls
		c.config.ObjectType,
		c.config.Queue,
		c.config.FullResyncPeriod,
	)
	r.ShouldResync = c.config.ShouldResync // 配置是否应该继续同步
	r.WatchListPageSize = c.config.WatchListPageSize
	r.clock = c.clock
	if c.config.WatchErrorHandler != nil { // 断开连接错误处理
		r.watchErrorHandler = c.config.WatchErrorHandler
	}

	c.reflectorMutex.Lock()
	c.reflector = r
	c.reflectorMutex.Unlock()

	var wg wait.Group

	wg.StartWithChannel(stopCh, r.Run) // 这里是真正的运行。
    // processLoop() 是DeltaFIFO的消费者方法
	wait.Until(c.processLoop, time.Second, stopCh) // 消费队列的数据
	wg.Wait()
}
</code></pre>
<h3 id="总结">总结</h3>
<p>在controller的初始化时就初始化了Reflector， controller.Run里面Reflector是结构体初始化时的Reflector，主要作用是watch指定的资源，并且将变化同步到本地的<code>store</code>中。</p>
<p>Reflector接着执行ListAndWatch函数，ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。首先会将资源版本号设置为0，<code>list()</code>可能会导致本地的缓存相对于etcd里面的内容存在延迟，<code>Reflector</code>会通过<code>watch</code>的方法将延迟的部分补充上，使得本地缓存数据与etcd的数据保持一致。</p>
<p><code>controller.Run</code>函数还会调用processLoop函数，processLoop通过调用HandleDeltas，再调用distribute，processorListener.add最终将不同更新类型的对象加入<code>processorListener</code>的channel中，供processorListener.Run使用。</p>
<h2 id="delta-fifo">Delta FIFO</h2>
<p>通过下图可以看出，<code>Delta FIFO</code>  是位于Reflector中的一个FIFO队列，那么 <code>Delta FIFO</code> 究竟是什么，让我们来进一步深剖。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1iI8uFsPRBY5m_g_WW4huMQ.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图源于：https://miro.medium.com/max/700/1*iI8uFsPRBY5m_g_WW4huMQ.png</center>
<p>在代码中的注释可以看到一些信息，根据信息可以总结出</p>
<ul>
<li>Delta FIFO 是一个生产者-消费者的队列，生产者是 <code>Reflector</code>，消费者是 <code>Pop()</code></li>
<li>与传统的FIFO有两点不同
<ul>
<li>Delta FIFO</li>
</ul>
</li>
</ul>
<p>Delta FIFO也是实现了 Queue以及一些其他 interface 的类，</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220520174110756.png" alt="image-20220520174110756" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<pre><code class="language-go">type DeltaFIFO struct {
	lock sync.RWMutex  // 一个读写锁，保证线程安全
	cond sync.Cond
	items map[string]Deltas // 存放的类型是一个key[string] =》 value[Delta] 类型的数据
	queue []string  // 用于存储item的key，是一个fifo
	populated bool // populated 是用来标记首次被加入的数据是否被变动
    initialPopulationCount int // 首次调用 replace() 的数量
	keyFunc KeyFunc
	knownObjects KeyListerGetter // 这里为indexer
	closed     bool       // 代表已关闭
	closedLock sync.Mutex
    emitDeltaTypeReplaced bool // 表示事件的类型，true为 replace(), false 为 sync()
}
</code></pre>
<p>那么delta的类型是，也就是说通常情况下，Delta为一个 <code>string[runtime.object]</code> 的对象</p>
<pre><code class="language-go">type Delta struct {
	Type   DeltaType // 这就是一个string
	Object interface{} // 之前API部分有了解到，API的类型大致为两类，runtime.Object和非结构化数据
}
</code></pre>
<p><a href="k8s.io/">apimachinery/pkg/runtime/interfaces.go</a></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220520182830431.png" alt="image-20220520182830431" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>那么此时，已经明白了Delta FIFO的结构，为一个Delta的队列，整个结构如下</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220520191654098.png" alt="image-20220520191654098" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="第一步创建一个delta-fifo">第一步创建一个Delta FIFO</h3>
<p>现在版本中，对创建Delta FIFO是通过函数 <code>NewDeltaFIFOWithOptions()</code></p>
<pre><code class="language-go">func NewDeltaFIFOWithOptions(opts DeltaFIFOOptions) *DeltaFIFO {
	if opts.KeyFunction == nil {
		opts.KeyFunction = MetaNamespaceKeyFunc // 默认的计算key的方法
	}
	f := &amp;DeltaFIFO{
		items:        map[string]Deltas{},
		queue:        []string{},
		keyFunc:      opts.KeyFunction,
		knownObjects: opts.KnownObjects,

		emitDeltaTypeReplaced: opts.EmitDeltaTypeReplaced,
	}
	f.cond.L = &amp;f.lock
	return f
}
</code></pre>
<h3 id="queueactionlockeddelta-fifo添加操作">queueActionLocked，Delta FIFO添加操作</h3>
<p>这里说下之前说道的，在追加时的操作 <code>queueActionLocked</code> ，如add update delete实际上走的都是这里</p>
<pre><code class="language-go">func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error {
	id, err := f.KeyOf(obj) // 计算key
	if err != nil {
		return KeyError{obj, err}
	}
	// 把新数据添加到DeltaFIFO中，Detal就是 动作为key，对象为值
    // item是DeltaFIFO中维护的一个 map[string]Deltas
	newDeltas := append(f.items[id], Delta{actionType, obj})
	newDeltas = dedupDeltas(newDeltas) // 去重，去重我们前面讨论过了

	if len(newDeltas) &gt; 0 {
		if _, exists := f.items[id]; !exists {
			f.queue = append(f.queue, id)
		} // 不存在则添加
		f.items[id] = newDeltas
		f.cond.Broadcast()
	} else {
		delete(f.items, id) // 这里走不到，因为添加更新等操作用newDelta是1
        // 源码中也说要忽略这里
	}
	return nil
}
</code></pre>
<p>在FIFO继承的Stroe的方法中，如，Add, Update等都是需要去重的，去重的操作是通过对比最后一个和倒数第二个值</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220520222611473.png" alt="image-20220520222611473" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<pre><code class="language-go">func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error {
	id, err := f.KeyOf(obj)
	if err != nil {
		return KeyError{obj, err}
	}

	newDeltas := append(f.items[id], Delta{actionType, obj})
	newDeltas = dedupDeltas(newDeltas)
...
</code></pre>
<p>在函数 <code>dedupDeltas()</code> 中实现的这个</p>
<pre><code class="language-go">// re-listing and watching can deliver the same update multiple times in any
order. This will combine the most recent two deltas if they are the same.
func dedupDeltas(deltas Deltas) Deltas {
	n := len(deltas)
	if n &lt; 2 {
		return deltas
	}
    a := &amp;deltas[n-1] // 如 [1,2,3,4] a=4
	b := &amp;deltas[n-2] // b=3,这里两个值其实为事件
	if out := isDup(a, b); out != nil {
		d := append(Deltas{}, deltas[:n-2]...)
		return append(d, *out)
	}
	return deltas
}
</code></pre>
<p>如果b对象的类型是 <code>DeletedFinalStateUnknown</code> 也会认为是一个旧对象被删除，这里在去重时也只是对删除的操作进行去重。</p>
<pre><code class="language-go">// tools/cache/delta_fifo.go
func isDup(a, b *Delta) *Delta {
	if out := isDeletionDup(a, b); out != nil {
		return out
	}
	// TODO: Detect other duplicate situations? Are there any?
	return nil
}
// keep the one with the most information if both are deletions.
func isDeletionDup(a, b *Delta) *Delta {
	if b.Type != Deleted || a.Type != Deleted {
		return nil
	}
	// Do more sophisticated checks, or is this sufficient?
	if _, ok := b.Object.(DeletedFinalStateUnknown); ok {
		return a
	}
	return b
}
</code></pre>
<p><strong>为什么需要去重？什么情况下需合并</strong></p>
<p>代码中开发者给我们留了一个TODO</p>
<blockquote>
<p>TODO: is there anything other than deletions that need deduping?</p>
</blockquote>
<ul>
<li>取决于Detal FIFO 生产-消费延迟
<ul>
<li>当在一个资源的创建时，其状态会频繁的更新，如 Creating，Runinng等，这个时候会出现大量写入FIFO中的数据，但是在消费端可能之前的并未消费完。</li>
<li>在上面那种情况下，以及Kubernetes 声明式 API 的设计，其实多余的根本不关注，只需要最后一个动作如Running，这种情况下，多个内容可以合并为一个步骤</li>
</ul>
</li>
<li>然而在代码中，去重仅仅是在Delete状态生效，显然这不可用；那么结合这些得到：
<ul>
<li>在一个工作时间窗口内，如果对于删除操作来说发生多次，与发生一次实际上没什么区别，可以去重</li>
<li>但在更新于新增操作时，实际上在对于声明式 API 的设计个人感觉是完全可以做到去重操作。
<ul>
<li>同一个时间窗口内多次操作，如更新，实际上Kubernetes应该只关注最终状态而不是命令式？</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="compute-key">Compute Key</h3>
<p>上面大概对一些Detal FIFO的逻辑进行了分析，那么对于Detal FIFO如何去计算，也就是说 <code>MetaNamespaceKeyFunc</code> ，这个是默认的KeyFunc，作用是计算Detals中的唯一key。</p>
<pre><code class="language-go">func MetaNamespaceKeyFunc(obj interface{}) (string, error) {
	if key, ok := obj.(ExplicitKey); ok {  // 显示声明的则为这个值
		return string(key), nil
	}
	meta, err := meta.Accessor(obj) // 那么使用Accessor,每一个资源都会实现这个Accessor
	if err != nil {
		return &quot;&quot;, fmt.Errorf(&quot;object has no meta: %v&quot;, err)
	}
	if len(meta.GetNamespace()) &gt; 0 {
		return meta.GetNamespace() + &quot;/&quot; + meta.GetName(), nil
	}
	return meta.GetName(), nil
}
</code></pre>
<p><code>ObjectMetaAccessor</code> 每个Kubernetes资源都会实现这个对象，如Deployment</p>
<pre><code class="language-go">// accessor interface
type ObjectMetaAccessor interface {
	GetObjectMeta() Object
}

// 会被ObjectMeta所实现
func (obj *ObjectMeta) GetObjectMeta() Object { return obj }
// 而每一个资源都会继承这个 ObjectMeta，如 ClusterRole

type ClusterRole struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;protobuf:&quot;bytes,1,opt,name=metadata&quot;`
</code></pre>
<p>那么这个Deltas的key则为集群类型的是资源本身的名字，namespace范围的则为 <code>meta.GetNamespace() + &quot;/&quot; + meta.GetName()</code>，可以在上面代码中看到，这样就可以给Detal生成了一个唯一的key</p>
<h3 id="keyof用于计算对象的key">keyof，用于计算对象的key</h3>
<pre><code class="language-go">func (f *DeltaFIFO) KeyOf(obj interface{}) (string, error) {
	if d, ok := obj.(Deltas); ok {
		if len(d) == 0 { // 长度为0的时候是一个初始的类型
			return &quot;&quot;, KeyError{obj, ErrZeroLengthDeltasObject}
		}
		obj = d.Newest().Object // 用最新的一个对象，如果为空则是nil
	}
	if d, ok := obj.(DeletedFinalStateUnknown); ok {  
		return d.Key, nil // 到了这里，之前提到过，是一个过期的值将会被删除
	}
	return f.keyFunc(obj) // 调用具体的key计算函数
}
</code></pre>
<h2 id="indexer">Indexer</h2>
<p>indexer 在整个 client-go 架构中提供了一个具有线程安全的数据存储的对象存储功能；对于Indexer这里会分析下对应的架构及使用方法。</p>
<p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/index.go" target="_blank"
   rel="noopener nofollow noreferrer" >client-go/tools/cache/index.go</a> 中可以看到 indexer是一个实现了<code>Store</code> 的一个interface</p>
<pre><code class="language-go">type Indexer interface {
    // 继承了store，拥有store的所有方法
	Store
	// 返回indexname的obj的交集
	Index(indexName string, obj interface{}) ([]interface{}, error)
	// 通过对 indexName，indexedValue与之相匹配的集合
	IndexKeys(indexName, indexedValue string) ([]string, error)
    // 给定一个indexName 返回所有的indexed
	ListIndexFuncValues(indexName string) []string
	// 通过indexname，返回与indexedvalue相关的 obj
	ByIndex(indexName, indexedValue string) ([]interface{}, error)
	// 返回所有的indexer
	GetIndexers() Indexers
	AddIndexers(newIndexers Indexers) error
}
</code></pre>
<p>实际上对他的实现是一个 cache，cache是一个KeyFunc与ThreadSafeStore实现的indexer，有名称可知具有线程安全的功能</p>
<pre><code class="language-go">type cache struct {
	cacheStorage ThreadSafeStore
	keyFunc KeyFunc
}
</code></pre>
<p>既然index继承了Store那么，也就是 <code>ThreadSafeStore</code> 必然实现了Store，这是一个基础保证</p>
<pre><code class="language-go">type ThreadSafeStore interface {
	Add(key string, obj interface{})
	Update(key string, obj interface{})
	Delete(key string)
	Get(key string) (item interface{}, exists bool)
	List() []interface{}
	ListKeys() []string
	Replace(map[string]interface{}, string)
	Index(indexName string, obj interface{}) ([]interface{}, error)
	IndexKeys(indexName, indexKey string) ([]string, error)
	ListIndexFuncValues(name string) []string
	ByIndex(indexName, indexKey string) ([]interface{}, error)
	GetIndexers() Indexers
	AddIndexers(newIndexers Indexers) error
	Resync() error // Resync is a no-op and is deprecated
}
// KeyFunc是一个生成key的函数，给一个对象，返回一个key值
type KeyFunc func(obj interface{}) (string, error)
</code></pre>
<p>那么这个indexer structure可以通过图来很直观的看出来</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220522003635661.png" alt="image-20220522003635661" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="cache的结构">cache的结构</h3>
<p>cache中会出现三种数据结构，也可以成为三种名词，为 <code>Index </code>, <code>Indexers</code> , <code>Indices</code></p>
<pre><code class="language-go">type Index map[string]sets.String
type Indexers map[string]IndexFunc
type Indices map[string]Index
</code></pre>
<p>可以看出：</p>
<ul>
<li><code>Index</code> 映射到对象，<code>sets.String</code> 也是在API中定义的数据类型 <code>[string]Struct{}</code>，</li>
<li><code>Indexers</code> 是这个 <code>Index</code> 的 <code>IndexFunc</code> , 是一个如何计算Index的keyname的函数</li>
<li><code>Indices</code> 通过Index 名词拿到对应的对象</li>
</ul>
<p>这个名词的概念如下，通过图来了解会更加清晰</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220522172028443.png" alt="image-20220522172028443" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="从创建开始">从创建开始</h3>
<p>创建一个cache有两种方式，一种是指定indexer，一种是默认indexer</p>
<pre><code class="language-go">// NewStore returns a Store implemented simply with a map and a lock.
func NewStore(keyFunc KeyFunc) Store {
	return &amp;cache{
		cacheStorage: NewThreadSafeStore(Indexers{}, Indices{}),
		keyFunc:      keyFunc,
	}
}

// NewIndexer returns an Indexer implemented simply with a map and a lock.
func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer {
	return &amp;cache{
		cacheStorage: NewThreadSafeStore(indexers, Indices{}),
		keyFunc:      keyFunc,
	}
}
</code></pre>
<h3 id="更新操作">更新操作</h3>
<p>在indexer中的更新操作（诸如 <code>add</code> , <code>update</code> ），实际上操作的是 <code>updateIndices</code>， 通过在代码可以看出</p>
<p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/thread_safe_store.go" target="_blank"
   rel="noopener nofollow noreferrer" >tools/cache/thread_safe_store.go</a> 的 77行起，那么就来看下 <code>updateIndices()</code> 具体做了什么</p>
<pre><code class="language-go">func (c *threadSafeMap) updateIndices(oldObj interface{}, newObj interface{}, key string) {
	// 在操作时，如果有旧对象，需要先删除
	if oldObj != nil {
		c.deleteFromIndices(oldObj, key)
	}
    // 先对整个indexer遍历，拿到index name与 index function
	for name, indexFunc := range c.indexers {
        // 通过index function，计算出对象的indexed name
		indexValues, err := indexFunc(newObj)
		if err != nil {
			panic(fmt.Errorf(&quot;unable to calculate an index entry for key %q on index %q: %v&quot;, key, name, err))
		}
        // 接下来通过遍历的index name 拿到这个index的对象
		index := c.indices[name]
		if index == nil { // 确认这个index是否存在，
            index = Index{} // 如果不存在将一个Index{}初始化
			c.indices[name] = index
		}
		// 通过计算出的indexed name来拿到对应的 set of object
		for _, indexValue := range indexValues {
			set := index[indexValue]
			if set == nil {
                // 如果这个set不存在，则初始化这个set
				set = sets.String{}
				index[indexValue] = set
			}
			set.Insert(key) // 然后将key插入set中
		}
	}
}
</code></pre>
<p>那么通过上面可以了解到了 <code>updateIndices</code> 的逻辑，那么通过对更新函数分析来看看他具体做了什么？这里是add函数，通过一段代码模拟操作来熟悉结构</p>
<pre><code class="language-go">testIndexer := &quot;testIndexer&quot;
testIndex := &quot;testIndex&quot;

indexers := cache.Indexers{
    testIndexer: func(obj interface{}) (strings []string, e error) {
        indexes := []string{testIndex} // index的名词
        return indexes, nil
    },
}

indices := cache.Indices{}
store := cache.NewThreadSafeStore(indexers, indices)

fmt.Printf(&quot;%#v\n&quot;, store.GetIndexers())

store.Add(&quot;retain&quot;, &quot;pod--1&quot;)
store.Add(&quot;delete&quot;, &quot;pod--2&quot;)
store.Update(&quot;retain&quot;, &quot;pod-3&quot;)
//lists := store.Update(&quot;retain&quot;, &quot;pod-3&quot;)
lists := store.List()
for _, item := range lists {
    fmt.Println(item)
}
</code></pre>
<p>这里是对add操作以及对<code>updateIndices()</code> 进行操作</p>
<pre><code class="language-go">// threadSafe.go
func (c *threadSafeMap) Add(key string, obj interface{}) {
	c.lock.Lock()
	defer c.lock.Unlock()
	oldObject := c.items[key] // 这个item就是存储object的地方, 为空
	c.items[key] = obj // 这里已经添加了新的值
	c.updateIndices(oldObject, obj, key) // 转至updateIndices
}

// updateIndices
func (c *threadSafeMap) updateIndices(oldObj interface{}, newObj interface{}, key string) {
	// 就当是新创建的，这里是空的忽略
	if oldObj != nil {
		c.deleteFromIndices(oldObj, key)
	}
    // 这个时候拿到的就是 name=testKey function=testIndexer
	for name, indexFunc := range c.indexers {
        // 通过testIndexer对testKey计算出的结果是 []string{testIndexer}
		indexValues, err := indexFunc(newObj)
		if err != nil {
			panic(fmt.Errorf(&quot;unable to calculate an index entry for key %q on index %q: %v&quot;, key, name, err))
		}
		index := c.indices[name] 
		if index == nil { 
            index = Index{} 
            // 因为假设为空了，故到这里c.indices[testIndexer]= Index{}
			c.indices[name] = index 
		}
		for _, indexValue := range indexValues {
            // indexValue=testIndexer
            // set := c.index[name] = c.indices[testIndexer]Index{}
			set := index[indexValue]
			if set == nil {
				set = sets.String{}
				index[indexValue] = set
			}
			set.Insert(key) // 到这里就为set=indices[testIndexer]Index{}
		}
	}
}
</code></pre>
<p>总结一下，到这里，可以很明显的看出来，indexer中的三个概念是什么了，前面如果没有看明白话</p>
<ul>
<li><code>Index</code>：通过indexer计算出key的名称，值为对应obj的一个集合，可以理解为索引的数据结构
<ul>
<li>比如说 <code>Pod:{&quot;nginx-pod1&quot;: v1.Pod{Name:Nginx}}</code></li>
</ul>
</li>
<li><code>Indexers</code> ：这个很简单，就是，对于Index中如何计算每个key的名称；可以理解为分词器，索引的过程</li>
<li><code>Indices</code> 通过Index 名词拿到对应的对象，是Index的集合；是将原始数据Item做了一个索引，可以理解为做索引的具体字段
<ul>
<li>比如说 <code>Indices[&quot;Pod&quot;]{&quot;nginx-pod1&quot;: v1.Pod{Name:Nginx}, &quot;nginx-pod2&quot;: v1.Pod{Name:Nginx}}</code></li>
</ul>
</li>
<li><code>Items</code>：实际上存储的在Indices中的<code>set.String{key:value}</code> ，中的 <code>key=value</code>
<ul>
<li>例如：<code>Item:{&quot;nginx-pod1&quot;: v1.Pod{Name:Nginx}, &quot;coredns-depoyment&quot;: App.Deployment{Name:coredns}}</code></li>
</ul>
</li>
</ul>
<h3 id="删除操作">删除操作</h3>
<p>对于删除操作，在最新版本中是使用了 <code>updateIndices</code> 就是 add update delete全都是相同的方法操作，对于旧版包含1.19- 是单独的一个操作</p>
<pre><code>// v1.2+
func (c *threadSafeMap) Delete(key string) {
	c.lock.Lock()
	defer c.lock.Unlock()
	if obj, exists := c.items[key]; exists {
		c.updateIndices(obj, nil, key)
		delete(c.items, key)
	}
}
// v1.19-
func (c *threadSafeMap) Delete(key string) {
	c.lock.Lock()
	defer c.lock.Unlock()
	if obj, exists := c.items[key]; exists {
		c.deleteFromIndices(obj, key)
		delete(c.items, key)
	}
}
</code></pre>
<h3 id="indexer使用">indexer使用</h3>
<p>上面了解了indexer概念，可以通过写代码来尝试使用一些indexer</p>
<pre><code class="language-go">package main

import (
	&quot;fmt&quot;

	appsV1 &quot;k8s.io/api/apps/v1&quot;
	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	&quot;k8s.io/client-go/tools/cache&quot;
)

func main() {

	indexers := cache.Indexers{
		&quot;getDeplyment&quot;: func(obj interface{}) (strings []string, e error) {
			d, ok := obj.(*appsV1.Deployment)
			if !ok {
				return []string{}, nil
			}
			return []string{d.Name}, nil
		},
		&quot;getDaemonset&quot;: func(obj interface{}) (strings []string, e error) {
			d, ok := obj.(*appsV1.DaemonSet)
			if !ok {
				return []string{}, nil
			}
			return []string{d.Name}, nil
		},
	}

	// 第一个参数是计算set内的key的名称 就是map[string]sets.String的这个strings的名称/namespace/resorcename
	// 第二个参数是计算index即外部的key的名称
	indexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, indexers)

	deployment := &amp;appsV1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      &quot;nginx-deplyment&quot;,
			Namespace: &quot;test&quot;,
		},
	}

	daemonset := &amp;appsV1.DaemonSet{
		ObjectMeta: metav1.ObjectMeta{
			Name:      &quot;firewall-daemonset&quot;,
			Namespace: &quot;test&quot;,
		},
	}

	daemonset2 := &amp;appsV1.DaemonSet{
		ObjectMeta: metav1.ObjectMeta{
			Name:      &quot;etcd-daemonset&quot;,
			Namespace: &quot;default&quot;,
		},
	}

	indexer.Add(deployment)
	indexer.Add(daemonset)
	indexer.Add(daemonset2)

	// 第一个参数是索引器
	// 第二个参数是所引起做索引的字段
	lists, _ := indexer.ByIndex(&quot;getDaemonset&quot;, &quot;etcd-daemonset&quot;)
	for _, item := range lists {
		switch item.(type) {
		case *appsV1.Deployment:
			fmt.Println(item.(*appsV1.Deployment).Name)
		case *appsV1.DaemonSet:
			fmt.Println(item.(*appsV1.DaemonSet).Name)
		}
	}
}
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>如何通过源码编译Kubernetes</title>
      <link>https://www.oomkill.com/2022/05/ch11-code-compile/</link>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/05/ch11-code-compile/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="本地构建">本地构建</h2>
<h3 id="选择要构建的版本">选择要构建的版本</h3>
<pre><code>git checkout tags/v1.19.5
</code></pre>
<h3 id="将依赖包复制到对应路径下">将依赖包复制到对应路径下</h3>
<pre><code>cp staging/src/k8s.io vendor/
</code></pre>
<h3 id="调整makefile">调整makefile</h3>
<p>在windows上编译的克隆下可能文件编码变了，需要手动修改下文件编码。比如说出现 <code>\r not found</code> 类似关键词时</p>
<p>这里转换编码使用了 dos2unix，需要提前安装下</p>
<pre><code>apt install dos2unix
</code></pre>
<p>转换原因是因为对于bash 脚本执行识别不了windows的换行</p>
<pre><code>find . -name '*.sh' -exec dos2unix {} \;
</code></pre>
<p>然后将 <code>build/root/</code> 的文件复制到项目根目录</p>
<pre><code>cp build/root/Makefile* ./
</code></pre>
<h3 id="编译">编译</h3>
<p>查看帮助 <code>make help</code></p>
<p>编译 <code>make all WHAT=cmd/kube-apiserver GOFLAGS=-v</code></p>
<p><code>WHAT=cmd/kube-apiserver</code> 为仅编译单一组件，<code>all</code> 为所有的组件</p>
<p>还可以增加其他的一些环境变量 <code>KUBE_BUILD_PLATFORMS=</code> 如编译的平台</p>
<p>更多的可以 <code>make help</code> 查看帮助</p>
<h3 id="编译中问题">编译中问题</h3>
<p><strong>Makefile:93: recipe for target &lsquo;all&rsquo; failed</strong></p>
<pre><code>!!! [0515 21:32:52] Call tree:
!!! [0515 21:32:52]  1: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:717 kube::golang::build_some_binaries(...)
!!! [0515 21:32:52]  2: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:861 kube::golang::build_binaries_for_platform(...)
!!! [0515 21:32:52]  3: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0515 21:32:52] Call tree:
!!! [0515 21:32:52]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0515 21:32:52] Call tree:
!!! [0515 21:32:52]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
Makefile:93: recipe for target 'all' failed
</code></pre>
<p>这里看报错根本不知道发生什么问题，使用 <code>strace</code> 追送了下，很明显看到是没有gcc</p>
<p>cgo: exec gcc: exec: &ldquo;gcc&rdquo;: executable file not found in $PATH</p>
<pre><code>rt_sigprocmask(SIG_BLOCK, [HUP INT QUIT TERM XCPU XFSZ], NULL, 8) = 0
clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7fbf45410a10) = 17890
rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
wait4(-1, +++ [0515 21:34:40] Building go targets for linux/amd64:
    cmd/kubelet
k8s.io/kubernetes/vendor/github.com/opencontainers/runc/libcontainer/system
k8s.io/kubernetes/vendor/github.com/mindprince/gonvml
# k8s.io/kubernetes/vendor/github.com/opencontainers/runc/libcontainer/system
cgo: exec gcc: exec: &quot;gcc&quot;: executable file not found in $PATH
# k8s.io/kubernetes/vendor/github.com/mindprince/gonvml
cgo: exec gcc: exec: &quot;gcc&quot;: executable file not found in $PATH
!!! [0515 21:34:42] Call tree:
!!! [0515 21:34:42]  1: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:717 kube::golang::build_some_binaries(...)
!!! [0515 21:34:42]  2: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:861 kube::golang::build_binaries_for_platform(...)
!!! [0515 21:34:42]  3: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0515 21:34:42] Call tree:
!!! [0515 21:34:42]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0515 21:34:42] Call tree:
!!! [0515 21:34:42]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
[{WIFEXITED(s) &amp;&amp; WEXITSTATUS(s) == 1}], 0, NULL) = 17890
--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=17890, si_uid=0, si_status=1, si_utime=0, si_stime=0} ---
rt_sigreturn({mask=[]})                 = 17890
openat(AT_FDCWD, &quot;/usr/share/locale/C.UTF-8/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale/C.utf8/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale/C/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale-langpack/C.UTF-8/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale-langpack/C.utf8/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale-langpack/C/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
fstat(1, {st_mode=S_IFCHR|0640, st_rdev=makedev(4, 1), ...}) = 0
ioctl(1, TCGETS, {B38400 opost isig icanon echo ...}) = 0
write(1, &quot;Makefile:93: recipe for target '&quot;..., 44Makefile:93: recipe for target 'all' failed
) = 44
write(2, &quot;make: *** [all] Error 1\n&quot;, 24make: *** [all] Error 1
) = 24
rt_sigprocmask(SIG_BLOCK, [HUP INT QUIT TERM XCPU XFSZ], NULL, 8) = 0
rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
chdir(&quot;/mnt/d/src/go_work/src/kubernetes&quot;) = 0
close(1)                                = 0
exit_group(2)                           = ?
+++ exited with 2 +++
</code></pre>
<p><strong>修改后编译问题可以明显看出是哪里</strong></p>
<p>如尝试增加一种资源类型后编译，这种类型的错误可以根据报错提示进行修改</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20220516172727798-791337963.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<pre><code>+++ [0515 21:47:59] Building go targets for linux/amd64:
    cmd/kube-apiserver
k8s.io/kubernetes/vendor/k8s.io/api/apps/v1
# k8s.io/kubernetes/vendor/k8s.io/api/apps/v1
vendor/k8s.io/api/apps/v1/register.go:48:3: cannot use &amp;StateDeploy{} (type *StateDeploy) as type runtime.Object in argument to scheme.Ad
dKnownTypes:
        *StateDeploy does not implement runtime.Object (missing DeepCopyObject method)
!!! [0515 21:48:01] Call tree:
!!! [0515 21:48:01]  1: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:706 k
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解kubernetes API</title>
      <link>https://www.oomkill.com/2022/05/ch02-kubernetes-api/</link>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/05/ch02-kubernetes-api/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="apiserver">APIServer</h2>
<p>在kubernetes架构概念层面上，Kubernetes由一些具有不同角色的服务节点组成。而master的控制平面由 <code>Apiserver</code>  <code>Controller-manager</code> 和 <code>Scheduler</code> 组成。</p>
<p><code>Apiserver</code> 从概念上理解可以分为 <code>api</code> 和 <code>object</code> 的集合，<code>api</code> 可以理解为，处理读写请求来修改相应 <code>object</code> 的组件；而 <code>object</code> 可以表示为 kubernetes 对象，如 <code>Pod</code>， <code>Deployment</code> 等 。</p>
<h2 id="基于声明式的api">基于声明式的API</h2>
<p>在命令式 API 中，会直接发送要执行的命令，例如：<em>运行</em>、<em>停止</em> 等命令。在声明式API 中，将声明希望系统执行的操作，系统将不断将自身状态朝希望状态改变。</p>
<h3 id="为什么使用声明式">为什么使用声明式</h3>
<p>在分布式系统中，任何组件随时都可能发生故障，当组件故障恢复时，需要明白自己需要做什么。在使用命令式时，出现故障的组件可能在异常时错过调用，并且在恢复时需要其他外部组件进行干预。而声明式仅需要在恢复时确定当前状态以确定他需要做什么。</p>
<h3 id="external-apis">External APIs</h3>
<p>在kubernetes中，控制平面是透明的，及没有internal APIs。这就意味着Kubernetes组件间使用相同的API交互。这里通过一个例子来说明外部APIs与声明式的关系。</p>
<p>例如，创建一个Pod对象，<code>Scheduler</code> 会监听 API来完成创建，创建完成后，调度程序不会命令被分配节点启动Pod。而在kubelet端，发现pod具有与自己相同的一些信息时，会监听pod状态。如改变kubelet则修改状态，如果删除掉Pod（对象资源不存在与API中），那么kubelet则将终止他。</p>
<h3 id="为什么不使用internal-api">为什么不使用Internal API</h3>
<p>使用External API可以使kubernetes组件都使用相同的API，使得kubernetes具有可扩展性和可组合性。对于kubernetes中任何默认组件，如不足满足需求时，都可以更换为使用相同API的组件。</p>
<p>另外，外部API还可轻松的使用公共API来扩展kubernetes的功能</p>
<h2 id="api资源">API资源</h2>
<p>从广义上讲，kubernetes对象可以用任何数据结构来表示，如：资源实例、配置（审计策略）或持久化实体（Pod）；在使用中，常见到的就是对应YAML的资源清单。转换出来就是RESTful地址，那么应该怎么理解这个呢？即，对资源的动作（操作）如图所示。但如果需要了解Kubernetes API需要掌握一些概念才可继续。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220513221304830.png" alt="image-20220513221304830" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="group">Group</h3>
<p>出于对kubernetes扩展性的原因，将资源类型分为了API组进行独立管理，可以通过 <code>kubectl api-resources</code>查看。在代码部分为 <code>vendor/k8s.io/api</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220513215038910.png" alt="image-20220513215038910" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>也可以通过 <code>kubectl xxx -v 6</code> 来查看 <code>kubectl</code> 命令进行了那些API调用</p>
<pre><code>$ kubectl get pods -v 6
I0513 21:54:33.250752   38661 round_trippers.go:444] GET http://localhost:8080/api?timeout=32s 200 OK in 1 milliseconds
I0513 21:54:33.293831   38661 round_trippers.go:444] GET http://localhost:8080/apis?timeout=32s 200 OK in 0 milliseconds
I0513 21:54:33.299741   38661 round_trippers.go:444] GET http://localhost:8080/apis/discovery.k8s.io/v1beta1?timeout=32s 200 OK in 3 milliseconds
I0513 21:54:33.301097   38661 round_trippers.go:444] GET http://localhost:8080/apis/autoscaling/v2beta1?timeout=32s 200 OK in 4 milliseconds
I0513 21:54:33.301128   38661 round_trippers.go:444] GET http://localhost:8080/apis/authorization.k8s.io/v1beta1?timeout=32s 200 OK in 3 milliseconds
I0513 21:54:33.301222   38661 round_trippers.go:444] GET http://localhost:8080/apis/rbac.authorization.k8s.io/v1beta1?timeout=32s 200 OK in 1 milliseconds
I0513 21:54:33.301238   38661 round_trippers.go:444] GET http://localhost:8080/apis/authentication.k8s.io/v1beta1?timeout=32s 200 OK in 4 milliseconds
I0513 21:54:33.301280   38661 round_trippers.go:444] GET http://localhost:8080/apis/certificates.k8s.io/v1beta1?timeout=32s 200 OK in 4 milliseconds
....
No resources found in default namespace.
</code></pre>
<h3 id="kind">Kind</h3>
<p>在<code>kubectl api-resources</code> 中可以看到，有Kind字段，大部分人通常会盲目的 <code>kubectl apply</code> ,这导致了，很多人以为 <code>kind</code> 实际上为资源名称，<code>Pod</code> ，<code>Deployment</code> 等。</p>
<p>根据 <a href="https://github.com/kubernetes/community/blob/7f3f3205448a8acfdff4f1ddad81364709ae9b71/contributors/devel/sig-architecture/api-conventions.md#types-kinds" target="_blank"
   rel="noopener nofollow noreferrer" >api-conventions.md</a> 的说明，<strong>Kind</strong> 是对象模式，包含三种类型：</p>
<ul>
<li>Object，代表系统中持久化数据资源，如，<code>Service</code>, <code>Namespace</code>, <code>Pod</code>等</li>
<li>List，是一个或多个资源的集合，通常以List结尾，如 <code>DeploymentList</code></li>
<li>对Object的操作和和非持久化实体，如，当发生错误时会返回“status”类型，并不会持久化该数据。</li>
</ul>
<h3 id="object">Object</h3>
<p>对象是Kubernetes中持久化的实体，也就是保存在etcd中的数据；如：<code>Replicaset</code> , <code>Configmap</code> 等。这个对象代表的了集群期望状态和实际状态。</p>
<blockquote>
<p>例如：创建了Pod，kubernetes集群会调整状态，直到相应的容器在运行</p>
</blockquote>
<p>Kubernetes资源又代表了对象，对象必须定义一些<a href="https://github.com/kubernetes/community/blob/7f3f3205448a8acfdff4f1ddad81364709ae9b71/contributors/devel/sig-architecture/api-conventions.md#resources" target="_blank"
   rel="noopener nofollow noreferrer" >字段</a>：</p>
<ul>
<li>所有对象必须具有以下字段：
<ul>
<li>Kind</li>
<li>apiVersion</li>
</ul>
</li>
<li>metadata</li>
<li>spec：期望的状态</li>
<li>status：实际的状态</li>
</ul>
<h3 id="api-link">API Link</h3>
<p>前面讲到的 <code>kubectl api-resources</code> 展示的列表不能完整称为API资源，而是已知类型的kubernetes对象，要对展示这个API对象，需要了解其完整的周期。以 <code>kubectl get --raw /</code> 可以递归查询每个路径。</p>
<pre><code>kubectl get --raw /
{
  &quot;paths&quot;: [
    &quot;/api&quot;,
    &quot;/api/v1&quot;,
    &quot;/apis&quot;,
    &quot;/apis/&quot;,
    &quot;/apis/admissionregistration.k8s.io&quot;,
    &quot;/apis/admissionregistration.k8s.io/v1&quot;,
    &quot;/apis/admissionregistration.k8s.io/v1beta1&quot;,
    &quot;/apis/apiextensions.k8s.io&quot;,
    &quot;/apis/apiextensions.k8s.io/v1&quot;,
...
</code></pre>
<p>对于一个Pod来说，其查询路径就为 <code>/api/v1/namespaces/kube-system/pods/coredns-f9bbb4898-7zkbf</code></p>
<pre><code>kubectl get --raw /api/v1/namespaces/kube-system/pods/coredns-f9bbb4898-7zkbf|jq
	
kind: Pod
apiVersion: v1
metadata: {}
spec:{}
status: {}
</code></pre>
<p>但有一些资源对象也并非这种结构，如 <code>configMap</code> ，因其只是存储的数据，所以没有 <code>spec</code> 和 <code>status</code></p>
<pre><code>kubectl get --raw /api/v1/namespaces/kube-system/configmaps/coredns|jq

kind
apiVersion
metadata
data
</code></pre>
<h3 id="api组成">API组成</h3>
<p>一个API的组成为 一个 API 组<code>Group</code> , 一个版本 <code>Version</code> , 和一个资源 <code>Resource</code> ; 简称为 <code>GVR</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/API-server-gvr.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>转换为实际的http路径为：</p>
<ul>
<li><code>/api/{version}/namespaces/{namespace_name}/resourcesPlural/{actual_resources_name}</code></li>
<li><code>/api/v1/namespaces/default/pods/pods123</code></li>
</ul>
<p>而GVR中的R代表的是RESTful中的资源，转换为Kubernetes中资源应为 <code>Kind</code>，简称为 <code>GVK</code>，K在URI中表示在：</p>
<ul>
<li><code>/apis/{GROUP}/{VERSION}/namespaces/{namespace}/{KIND}</code></li>
</ul>
<h2 id="请求和处理">请求和处理</h2>
<p>这里讨论API请求和处理，API的一些数据结构位于 <code>k8s.io/api</code> ，并处理集群内部与外部的请求，而Apiserver 位于 <code>k8s.io/apiserver/pkg/server</code> 提供了http服务。</p>
<p>那么，当 HTTP 请求到达 Kubernetes API 时，实际上会发生什么？</p>
<ul>
<li>首先HTTP请求在 <code>DefaultBuildHandlerChain</code> （可以参考<code>k8s.io/apiserver/pkg/server/config.go</code>）中注册filter chain，过滤器允许并将相应的信息附加至 <code>ctx.RequestInfo</code>; 如身份验证的相应</li>
<li><code>k8s.io/apiserver/pkg/server/mux</code> 将其分配到对应的应用</li>
<li><code>k8s.io/apiserver/pkg/server/routes</code> 定义了REST与对应应用相关联</li>
<li><code>k8s.io/apiserver/pkg/endpoints/groupversion.go.InstallREST()</code> 接收上下文，从存储中传递请求的对象。</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220516171908228.png" alt="image-20220516171908228" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p>Reference</p>
<p><a href="https://iximiuz.com/en/posts/kubernetes-api-structure-and-terminology/" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes API 基础——资源、种类和对象</a></p>
<p><a href="https://networktechstudy.com/home/kubernetes-the-one-api" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes - 一个 API 统治</a></p>
<p><a href="https://flugel.it/infrastructure-as-code/building-kubernetes-operators-part-2/" target="_blank"
   rel="noopener nofollow noreferrer" >Design and implementation</a></p>
<p><a href="https://www.codeproject.com/Articles/5252640/Extending-the-Kubernetes-API" target="_blank"
   rel="noopener nofollow noreferrer" >Extending-the-Kubernetes-API</a></p>
<p><a href="https://cloud.redhat.com/blog/kubernetes-deep-dive-api-server-part-1" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes 深入探讨</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>理解kubernetes listwatch机制原理</title>
      <link>https://www.oomkill.com/2021/12/ch05-listwatch-mechanism/</link>
      <pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/12/ch05-listwatch-mechanism/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">overview</h2>
<p>kubernetes的设计里面大致上分为3部分：</p>
<ul>
<li>API驱动型的特点 (<code>API-driven</code>)</li>
<li>控制循环（<code>control loops</code>）与 条件触发 （<code>Level Trigger</code>）</li>
<li>API的可延伸性</li>
</ul>
<p>而正因为这些设计特性，才使得kubernetes工作非常稳定。</p>
<h2 id="什么是level-trigger与-edge-trigger">什么是Level Trigger与 Edge trigger</h2>
<p>看到网上有资料是这么解释两个属于的：</p>
<ul>
<li>
<p><strong>条件触发(level-trigger，也被称为水平触发)LT指</strong>： 只要满足条件，就触发一个事件(只要有数据没有被获取，就不断通知)。</p>
</li>
<li>
<p><strong>边缘触发(edge-trigger)ET</strong>: 每当状态变化时，触发一个事件。</p>
</li>
</ul>
<p>通过查询了一些资料，实际上也不明白这些究竟属于哪门科学中的理论，但是具体解释起来看的很明白。</p>
<p><strong>LEVEL TRIGGERING</strong>：当电流有两个级别，<code>VH</code> 和 <code>VL</code>。代表了两个触发事件的级别。如果将<code>VH</code> 设置为LED在正时钟。当电压为VH时，LED可以在该时间线任何时刻点亮。这称为<strong>LEVEL TRIGGERING</strong>，每当遇到<code>VH</code> 时间线就会触发事件。事件是在时间内的任何时刻开始，直到满足条件。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212232954599.png" alt="image-20211212232954599" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><strong>Edge TRIGGERING</strong>:</p>
<p>如图所示，会看到上升线与下降线，当事件在上升/下降边缘触发时（两个状态的交点），称为边缘触发（<strong>Edge TRIGGERING</strong>:）。</p>
<p>如果需要打开LED灯，则当时钟从<code>VL</code>转换到<code>VH</code>时才会亮起，而不是一家处在对应的时钟线上，仅仅是在过渡时亮起。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212233004721.png" alt="image-20211212233004721" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="为什么kubernetes使用level-trigger而不使用edge-trigger">为什么kubernetes使用Level Trigger而不使用Edge trigger</h2>
<p>如图所述，两种不同的设计模式，随着时间形状进行相应，当系统在由高转低，或由低转高时，系统处在关闭或者不可控的异常状态下，应如何触发对应的事件呢。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212233012534.png" alt="image-20211212233012534" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>换一种方式来来解释，比如说通过 加法运算，如下，i=3，当给I+4作为一个操作触发事件。</p>
<pre><code># let i=3
# let i+=4
# let i
# echo $i
7
</code></pre>
<p>当为<code>Edge trigger</code>时操作的情况下，将看到 <code>i+4</code> ,而在 <code>level trigger</code> 时看到的是 <code>i=7</code>。这里将会从``i+4` 一直到下一个信号的触发。</p>
<h3 id="信号的干扰">信号的干扰</h3>
<p>通常情况下，两者是没有区别的，但在大规模分布式网络环境中，有很多因素的影响下，任何都是不可靠的，在这种情况下会改变了我们对事件信号的感知。</p>
<p>如图所示，图为<code>Level Trigger</code>与<code>Edge trigger</code> 的信号发生模拟，在理想情况下，两者间并没有什么不同。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212233021583.png" alt="image-20211212233021583" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="一次中断场景">一次中断场景</h4>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212232713291.png" alt="image-20211212232713291" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>由图可知，<code>Edge trigger</code>当在恰当的时间点发生信号中断，会对整个流产生很大的影响，甚至改变了整个状态，对于较少的干扰并不会对有更好的结果，而单次的中断，使<code>Edge trigger</code>错过了从高到低的变化，而 <code>level trigger</code> 基本上保证了整个信号量的所有改变状态。</p>
<h4 id="两次中断的场景下">两次中断的场景下</h4>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212232747769.png" alt="image-20211212232747769" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>由图可看到，信号的上升和下降中如果存在了中断，<code>Edge trigger</code> 丢失了上升的信号，但最终状态是正确的。</p>
<p>在信号状态的两次变化时发生了两次中断，<code>Level Trigger</code>与<code>Edge trigger</code> 之间的区别很明显，<code>Edge trigger</code> 的信号错过了第一次上升，而<code>Level Trigger</code> 保持了最后观察到的状态，知道拿到了其他状态，这种模式保证了得到的信号基本的正确性，但是发生延迟到中断恢复后。</p>
<h4 id="通过运算来表示两种模式的变化情况">通过运算来表示两种模式的变化情况</h4>
<p>完整的信号</p>
<pre><code class="language-bash"># let i=2

# let i+1
# let i-=1
# let i+1

# echo $i
3
</code></pre>
<p><strong>Edge trigger</strong></p>
<pre><code class="language-bash"># let i=2

# let i+1  
(# let i-=1) miss this
# let i+1

# echo $i
4
</code></pre>
<h2 id="如何使理想状态和实际状态一样呢">如何使理想状态和实际状态一样呢？</h2>
<p>在Kubernetes中，不仅仅是观察对象的一个信号，还观察了其他两个信号，集群的期待状态与实际状态，期望的状态是用户期望集群所处的状态，如我运行了2个实例（pod）。在最理想的场景下，集群的实际状态与期待状态是相同的，但这个过程会受到任意的外界因素干扰被影响下，实际状态与理想状态发生偏差。</p>
<p>Kubernetes必须接受实际状态，并将其与所需状态调和。不断地这样做，采取两种状态，确定其之间的差异，并纠正其不断的更改，以使实际状态达到理想状态。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212232804746.png" alt="image-20211212232804746" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>如图所示，在一个<code>Edge trigger</code> 中，最终的结果很可能会与理想中的结果发生偏差。</p>
<p>当初始实例为1时，并希望扩展为5个副本，然后再向下缩容到2个副本，则<code>Edge trigger</code>环境下将看到以下状态：系统的实际状态不能立即对这些命令作出反应。正如图所述，当只有3个副本在运行时，它可能会终止3个副本。这就给我们留下了0个副本，而不是所需的2个副本。</p>
<pre><code># let replicas=1
# let replicas += 4 # 此时副本数为5，但是这个过程需要时间而不是立即完成至理想状态
# let replicas -= 3 # 当未完成时又接到信号的变化，此时副本数为3，减去3，很可能实际状态为0，与理想状态2发生了偏差
</code></pre>
<p>而使用<code>Level Trigger</code>时，会总是比较完整的期望状态和实际状态，直到实际状态与期望状态相同。这大大减少了状态同步间（错误）的产生。</p>
<h2 id="summary">summary</h2>
<p>每一种触发器的产生一定有其道理，<code>Edge trigger</code>本身并不是很差，只是应用场景的不同，而使用的模式也不同，比如nginx的高性能就是使用了<code>Edge trigger</code>模型，如nginx使用了 <code>Level trigger</code>在大并发下，当发生了变更信号等待返回时，发生大量客户端连接在侦听队列，而<code>Edge trigger</code>模型则不会出现这种情况。</p>
<p>综上所述，kubernetes在设计时，各个组件需要感知数据的最终理想状态，无需担心错过数据变化的过程。而设计kubernentes系统消息通知机制（或数据实时通知机制），也应满足以下要求：</p>
<ul>
<li>
<p>实时性（即数据变化时，相关组件感觉越快越好）。消息必须是实时的。在<code>list/watch</code>机制下，每当apiserver资源有状态变化事件时，都会及时将事件推送到客户端，以保证消息的实时性。</p>
</li>
<li>
<p>消息序列：消息的顺序也很重要。在并发场景下，客户端可能会在短时间内收到同一资源的多个事件。对于关注最终一致性的kubernetes来说，它需要知道哪个是最新的事件，并保证资源的最终状态与最新事件所表达的一致。kubernetes在每个资源事件中都携带一个<code>resourceVersion</code>标签，这个标签是递增的。因此，客户端在并发处理同一资源的事件时，可以比较<code>resourceVersion</code>，以确保最终状态与最新事件的预期状态一致。</p>
</li>
<li>
<p>消息的可靠性，保证消息不丢失或者有可靠的重新获取的机制（比如 <code>kubelet</code>和 <code>kube-apisever</code>之间的网络波动（<code>network flashover</code> ）需要保证kubelet在网络恢复后可以接收到网络故障时产生的消息）。</p>
</li>
</ul>
<p>正是因为Kubernetes使用了 <code>Level trigger</code>才让集群更加可靠。</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://levelup.gitconnected.com/nginx-event-driven-architecture-demonstrated-in-code-51bf0061cad9" target="_blank"
   rel="noopener nofollow noreferrer" >nginx-event-driven-architecture</a></p>
<p><a href="https://www.quora.com/What-is-meant-by-edge-triggering-and-level-triggering" target="_blank"
   rel="noopener nofollow noreferrer" >What-is-meant-by-edge-triggering-and-level-triggering</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>理解kubernetes schema</title>
      <link>https://www.oomkill.com/2021/11/ch03-kubernetes-schema/</link>
      <pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/11/ch03-kubernetes-schema/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="什么是schema">什么是schema</h2>
<p>schema一词起源于希腊语中的<code>form</code>或<code>figure</code>，但具体应该如何定义<code>schema</code>取决于应用环境的上下文。<code>schema</code>有不同的类型，其含义与数据科学、教育、营销和SEO以及心理学等领域密切相关。</p>
<blockquote>
<p>在维基百科中将schema解释为，<strong>图式</strong>，在心里学中主要描述一种思维或行为类型，用来组织资讯的类别，以及资讯之间的关系。它也可以被描述为先入为主思想的心理结构，表示世界某些观点的框架，或是用于组织和感知新资讯的系统。</p>
</blockquote>
<p>但在计算机科学中，从很多地方都可以看到 <em>schema</em> 这个名词，例如 database，openldap，programing language等的。这里可以简单的吧<em>schema</em> 理解为 <strong>元数据集合</strong> （metadata component）<strong>数据模型</strong>，主要包含元素及属性的声明，与其他数据结构组成。</p>
<h3 id="数据库中的schema">数据库中的schema</h3>
<p>在数据库中，<code>schema</code> 就像一个骨架结构，代表整个数据库的逻辑视图。它设计了应用于特定数据库中数据的所有约束。当在数据建模时，就会产生一个schema。在谈到关系数据库]和面向对象数据库时经常使用schema。有时也指将结构或文本的描述。</p>
<p>数据库中schema描述数据的形状以及它与其他模型、表和库之间的关系。在这种情况下，数据库条目是schema的一个实例，包含schema中描述的所有属性。</p>
<p>数据库schema通常分为两类：定义数据文件实际存储方式的**物理数据库schema <strong>；和</strong>逻辑数据库schema **，它描述了应用于存储数据的所有逻辑约束，包括完整性、表和视图。常见包括</p>
<ul>
<li>星型模式（star schema）</li>
<li>雪花模式（snowflake schema）</li>
<li>事实星座模型（fact constellation schema 或 galaxy schema）</li>
</ul>
<p>星型模式是类似于一个简单的数据仓库图，包括一对多的事实表和维度表。它使用非规范化数据。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/dm-star_schema-f_mobile.png" alt="星型模式" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>雪花模式是更为复杂的一种流行的数据库模式，在该模式下，维度表是规范化的，可以节省存储空间并最大限度地减少数据冗余。</p>
<p>事实星座模式远比星型模式和雪花模式复杂得多。它拥有多个共享多个维度表的事实表。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/dm-snowflake_schema-f_mobile.png" alt="snowflake schema" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="kubernetes中的schema">Kubernetes中的schema</h2>
<p>通过上面的阐述，大概上可以明白 schema究竟是什么东西了，在Kubernetes中也有schema的概念，通过对kubernetes中资源（GVK）的规范定义、相互关系间的映射等，schema即k8s资源对象元数据。</p>
<p>而kubernetes中资源对象即  <code>Group</code> <code>Version</code> <code>Kind</code> 这些被定义在 <code>staging/src/k8s.io/api/type.go</code>中，即平时所操作的yaml文件，例如</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment  
metadata:
  name:  ngx
  namespace: default
spec:
  selector:  
    matchLabels:
      app: ngx
  template:  
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: ngx-schema
        image: nginx
        ports:
        - containerPort: 80
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211117001624039.png" alt="image-20211117001624039" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>而对应的的即为<code>TypeMeta</code> 、<code>ObjectMeta</code> 和 <code>DeploymentSpec</code>,</p>
<p><code>TypeMeta</code> 为 <code>kind</code> 与 <code>apiserver</code></p>
<p><code>ObjectMeta</code> 为  <code>Name</code> 、<code>Namespace</code>  <code>CreationTimestamp</code>等段。</p>
<p><code>DeploymentSpec</code> 则对应了 yaml 中的 spec。</p>
<p>而整个yaml组成了 一个 k8s的资源对象。</p>
<pre><code class="language-go">type Deployment struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// Standard object metadata.
	// +optional
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

	// Specification of the desired behavior of the Deployment.
	// +optional
	Spec DeploymentSpec `json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;`

	// Most recently observed status of the Deployment.
	// +optional
	Status DeploymentStatus `json:&quot;status,omitempty&quot; protobuf:&quot;bytes,3,opt,name=status&quot;`
}
</code></pre>
<p><code>register.go</code> 则是将对应的资源类型注册到schema中的类</p>
<pre><code class="language-go">var (
	// TODO: move SchemeBuilder with zz_generated.deepcopy.go to k8s.io/api.
	// localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes.
	SchemeBuilder      = runtime.NewSchemeBuilder(addKnownTypes)
	localSchemeBuilder = &amp;SchemeBuilder
	AddToScheme        = localSchemeBuilder.AddToScheme
)

// Adds the list of known types to the given scheme.
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&amp;Deployment{},
		&amp;DeploymentList{},
		&amp;StatefulSet{},
		&amp;StatefulSetList{},
		&amp;DaemonSet{},
		&amp;DaemonSetList{},
		&amp;ReplicaSet{},
		&amp;ReplicaSetList{},
		&amp;ControllerRevision{},
		&amp;ControllerRevisionList{},
	)
	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
	return nil
}
</code></pre>
<p>而 <code>apimachinery</code> 包则是 schema的实现，通过看其内容可以发下，kubernetes中 schema就是 <strong>GVK</strong> 的属性约束 与 <strong>GVR</strong> 之间的映射。</p>
<h2 id="通过示例了解schema">通过示例了解schema</h2>
<p>例如在 <code>apps/v1/deployment</code> 这个资源，在代码中表示 <code>k8s.io/api/apps/v1/types.go</code> ，如果需要对其资源进行扩展那么需要怎么做？如，建立一个 <code>StateDeplyment</code> 资源</p>
<pre><code>type Deployment struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// Standard object metadata.
	// +optional
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

</code></pre>
<p>如上述代码所示，Deployment 中的 <code>metav1.TypeMeta</code> 和 <code>metav1.ObjectMeta</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220517172901775.png" alt="image-20220517172901775" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>那么我们复制一个 Deployment 为 StateDeployment，注意，因为 Deployment的两个属性， <code>metav1.TypeMeta</code> 和 <code>metav1.ObjectMeta</code> 分别实现了不同的方法，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220517173136077.png" alt="image-20220517173136077" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>所以在实现方法时，需要实现 <code>DeepCopyinfo</code> ， <code>DeepCopy</code>  和继承接口 <code>Object</code> 的 <code>DeepCopyObject</code> 方法</p>
<pre><code class="language-go">// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *StateDeployment) DeepCopyInto(out *StateDeployment) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&amp;out.ObjectMeta)
	in.Spec.DeepCopyInto(&amp;out.Spec)
	in.Status.DeepCopyInto(&amp;out.Status)
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StateDeployment.
func (in *StateDeployment) DeepCopy() *StateDeployment {
	if in == nil {
		return nil
	}
	out := new(StateDeployment)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *StateDeployment) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}
</code></pre>
<p>那么扩展一个资源的整个流为：</p>
<ul>
<li>资源类型在：<code>k8s.io/api/{Group}/types.go</code></li>
<li>资料类型的实现接口 <code>k8s.io/apimachinery/pkg/runtime/interfaces.go.Object</code></li>
<li>其中是基于 <code>Deployment</code> 的类型，<code>metav1.TypeMeta</code> 和 <code>metav1.ObjectMeta</code></li>
<li><code>metav1.TypeMeta</code> 实现了 <code>GetObjectKind()</code> ；<code>metav1.ObjectMeta</code> 实现了<code>DeepCopyinfo=()</code> ， <code>DeepCopy()</code> ，还需要实现 <code>DeepCopyObject()</code></li>
<li>最后注册资源到schema中 <code>k8s.io/api/apps/v1/register.go</code></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>k8s开发环境准备 - 如何配置开发环境</title>
      <link>https://www.oomkill.com/2021/11/ch01-k8s-perpare/</link>
      <pubDate>Fri, 19 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/11/ch01-k8s-perpare/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="下载源码">下载源码</h2>
<p>根据kubernetes github 方式可以</p>
<pre><code>mkdir -p $GOPATH/src/k8s.io
cd $GOPATH/src/k8s.io
git clone https://github.com/kubernetes/kubernetes
cd kubernetes
make
</code></pre>
<p>如果有需要可以切换到对应的版本进行学习或者修改，一般kubernetes版本为对应tag</p>
<pre><code>git fetch origin [远程tag名]
git checkout  [远程tag名]
git branch
</code></pre>
<h2 id="配置goland">配置goland</h2>
<p>kubernetes本身是支持 go mod 的，但源码这里提供了所有的依赖在 <code>staging/src/k8s.io/</code> 目录下，可以将此目录内的文件复制到 <code>vendor</code>下。</p>
<pre><code class="language-bash">cp -a staging/src/k8s.io/* vendor/k8s.io/
</code></pre>
<p>对于 <code>k8s.io/kubernetes/pkg/</code> 发红的（找不到依赖的），可以将手动创建一个目录在 <code>vendor/k8s.io/</code> 将克隆下来的根目录 <code>pkg</code> 复制到刚才的目录下。</p>
<p>goland中，此时不推荐使用go mod模式了，这里goland一定要配置GOPATH的模式。对应的GOPATH加入 <code>{project}/vender</code>即可。 这里可以添加到 goland中 <code>project GOPATH</code>里。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211116222531919.png" alt="image-20211116222531919" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>calico网络策略</title>
      <link>https://www.oomkill.com/2021/02/calico-network-policy/</link>
      <pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/02/calico-network-policy/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="什么是网络策略">什么是网络策略</h2>
<p>在Kubernetes平台中，要实现零信任网络的安全架构，Calico与istio是在Kubernetes集群中构建零信任网络必不可少的组件。</p>
<p>而建立和维护整个集群中的“零信任网络”中，网络策略的功能在操作上大致可以总结为<strong>使用资源配置模板来管理控制平面数据流</strong>。说白了讲网络策略就是用来控制Pod间流量的规则。</p>
<h2 id="在calico中如何编写网络策略">在Calico中如何编写网络策略</h2>
<p>要使用网络策略就需要先了解Calico功能**：NetworkPolicy<strong>和</strong>GlobalNetworkPolicy**。</p>
<p><code>NetworkPolicy</code>资源，简称<code>np</code>；是命名空间级别资源。规则应用于与标签选择器匹配的endpoint的集合。</p>
<p><code>GlobalNetworkPolicy</code>资源，简称 <code>gnp</code>/<code>gnps</code>与<code>NetworkPolicy</code>功能一样，是整个集群级别的资源。</p>
<p><code>GlobalNetworkPolicy</code> 与 <code>NetworkPolicy</code>资源的管理也与calico的部署方式有关，使用etcd作为存储时，资源的管理只能使用 <code>calicoctl</code>进行管理</p>
<h3 id="networkpolicy与globalnetworkpolicy的构成">NetworkPolicy与GlobalNetworkPolicy的构成</h3>
<pre><code class="language-yaml">apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-tcp-90
spec:
  selector: app == 'envoy' # 应用此策略的endpoint
  types: # 应用策略的流量方向
    - Ingress 
    - Egress
  ingress: # 入口的流量规则
    - action: Allow # 流量的行为
      protocol: ICMP # 流量的协议
      notProtocol: TCP # 匹配流量协议不为 值 的流量 
      source: # 流量的来源 src与dst的匹配关系为 与，所有的都生效即生效
        nets: # 有效的来源IP
        selector: # 标签选择器
        namespaceSelector: # 名称空间选择器
        ports: # 端口
        - 80 # 单独端口
        - 6040:6050	# 端口范围
      destination: # 流量的目标
  egress: # 出口的流量规则
    - action: Allow
  serviceAccountSelector: # 使用与此规则的serviceAccount
</code></pre>
<h3 id="networkpolicy使用">NetworkPolicy使用</h3>
<p>实例：允许6379流量可以被 <code>role=frontend</code>的pod访问</p>
<pre><code class="language-yaml">apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-tcp-6379
  namespace: production
spec:
  selector: role == 'database'
  types:
  - Ingress
  - Egress
  ingress:
  - action: Allow
    metadata:
      annotations:
        from: frontend
        to: database
    protocol: TCP
    source:
      selector: role == 'frontend'
    destination:
      ports:
      - 6379
  egress:
  - action: Allow
</code></pre>
<p>实例：禁止ICMP流量</p>
<pre><code class="language-yaml">apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-tcp-90
spec:
  selector: app == 'netbox'
  types:
    - Ingress
    - Egress
  ingress:
    - action: Deny
      protocol: ICMP
  egress:
    - action: Deny
      protocol: ICMP
</code></pre>
<p>实例：禁止访问指定服务</p>
<pre><code class="language-yaml">apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-tcp-90
spec:
  selector: app == 'netbox'
  types:
    - Ingress
    - Egress
  ingress:
    - action: Allow
  egress:
    - action: Deny
      destination:
        selector: app == 'envoy'
</code></pre>
<h2 id="globalnetworkpolicy">GlobalNetworkPolicy</h2>
<p>GlobalNetworkPolicy与NetworkPolicy使用方法基本一致，只是作用域的不同，并且可以应用很多高级的网络策略：</p>
<ul>
<li><a href="https://docs.projectcalico.org/security/host-forwarded-traffic" target="_blank"
   rel="noopener nofollow noreferrer" >转发流量</a></li>
<li><a href="https://docs.projectcalico.org/security/defend-dos-attack" target="_blank"
   rel="noopener nofollow noreferrer" >防御DoS</a></li>
<li>&hellip;.</li>
</ul>
<p><strong>GlobalNetworkPolicy</strong> 中提供了一个preDNAT的功能，是kube-proxy对Node port的端口和IP的流量DNAT到所对应的Pod中的时候，为了既允许正常的ingress流量，又拒绝其他的ingress流量，这个时候必须要在DNAT前生效，这种情况需要使用<strong>preDNAT</strong>。</p>
<p><strong>preDNAT</strong> 适用的条件是，流量仅为ingress并且在DNAT之前。</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://docs.projectcalico.org/reference/resources/networkpolicy#spec" target="_blank"
   rel="noopener nofollow noreferrer" >NetworkPolicy.spec</a></p>
<p><a href="https://docs.projectcalico.org/reference/resources/networkpolicy#rule" target="_blank"
   rel="noopener nofollow noreferrer" >NetworkPolicy.spec.ingress|egress</a></p>
<p><a href="https://docs.projectcalico.org/reference/resources/networkpolicy#entityrule" target="_blank"
   rel="noopener nofollow noreferrer" >NetworkPolicy.spec.ingress.src|dst</a></p>
<p><a href="https://docs.projectcalico.org/reference/resources/globalnetworkpolicy" target="_blank"
   rel="noopener nofollow noreferrer" >globalnetworkpolicy</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>基于混合云模式的calico部署</title>
      <link>https://www.oomkill.com/2021/02/calico-deploy-on-hybrid-cloud/</link>
      <pubDate>Mon, 15 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/02/calico-deploy-on-hybrid-cloud/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="开始前准备">开始前准备</h2>
<p>确定calico数据存储</p>
<p>Calico同时支持kubernetes api和etcd数据存储。官方给出的建议是在本地部署中使用<strong>K8S API</strong>，仅支持Kubernetes模式。而官方给出的etcd则是混合部署（Calico作为Kubernetes和OpenStack的网络插件运行）的最佳数据存储。</p>
<p>使用etcd作为calico数据存储的好处：</p>
<ul>
<li>允许多平台混用calico，如Kubernetes OpenStack上运行Calico</li>
<li>Kubernetes资源与Calico资源分离</li>
<li>一个Calico群集，该群集不仅仅包含一个Kubernetes群集，如可与多个kubernetes集群互通。</li>
</ul>
<p>坏处：</p>
<ul>
<li>安装步骤繁琐</li>
<li>无法使用Kubernetes RBAC对calico资源的控制</li>
<li>无法使用Kubernetes资源对calico进行管理</li>
</ul>
<h3 id="下载calico部署清单">下载calico部署清单</h3>
<pre><code>curl https://docs.projectcalico.org/manifests/calico-etcd.yaml -o calico.yaml
</code></pre>
<h3 id="修改pod-cidr">修改Pod CIDR</h3>
<p>　Calico默认的Pod CIDR使用的是<code>192.168.0.0/16</code>，这里一般使用与controller-manager中的<code>--cluster-cidr</code> 保持一,取消资源清单内的 <code>CALICO_IPV4POOL_CIDR</code>变量的注释，并将其设置为与所选Pod CIDR相同的值。</p>
<h3 id="calico的ip分配范围">calico的IP分配范围</h3>
<p>　Calico IPAM从<code>ipPool</code>分配IP地址。修改Pod的默认IP范围则修改清单<code>calico.yaml</code>中的<code>CALICO_IPV4POOL_CIDR</code></p>
<h3 id="配置calico的-ip-in-ip">配置Calico的 <code>IP in IP</code></h3>
<p>默认情况下，Calico中的IPIP已经禁用，这里使用的v3.17.2 低版本默认会使用IPIP</p>
<p>要开启IPIP mode则需要修改配置清单内的 <code>CALICO_IPV4POOL_IPIP</code> 环境变量改为 <code>always</code></p>
<h3 id="修改secret">修改secret</h3>
<pre><code class="language-yaml">  # Populate the following with etcd TLS configuration if desired, but leave blank if
  # not using TLS for etcd.
  # The keys below should be uncommented and the values populated with the base64
  # encoded contents of each file that would be associated with the TLS data.
  # Example command for encoding a file contents: cat &lt;file&gt; | base64 -w 0
  # etcd的ca
etcd-ca: # 填写上面命令编码后的值
# etcd客户端key
etcd-key: # 填写上面命令编码后的值
# etcd客户端访问证书
etcd-cert: # 填写上面命令编码后的值
</code></pre>
<h3 id="修改configmap">修改configMap</h3>
<pre><code class="language-yaml">  etcd_endpoints: &quot;https://10.0.0.6:2379&quot;
  # If you're using TLS enabled etcd uncomment the following.
  # You must also populate the Secret below with these files.
  etcd_ca: &quot;/calico-secrets/etcd-ca&quot;
  etcd_cert: &quot;/calico-secrets/etcd-cert&quot;
  etcd_key: &quot;/calico-secrets/etcd-key&quot;
</code></pre>
<h2 id="开始安装">开始安装</h2>
<pre><code>kubectl apply -f calico.yaml
</code></pre>
<h2 id="安装出错">安装出错</h2>
<p><code>/calico-secrets/etcd-cert: permission denied</code></p>
<pre><code>2021-02-08 02:15:10.485 [INFO][1] main.go 88: Loaded configuration from environment config=&amp;config.Config{LogLevel:&quot;info&quot;, WorkloadEndpointWorkers:1, ProfileWorkers:1, PolicyWorkers:1, NodeWorkers:1, Kubeconfig:&quot;&quot;, DatastoreType:&quot;etcdv3&quot;}
2021-02-08 02:15:10.485 [FATAL][1] main.go 101: Failed to start error=failed to build Calico client: could not initialize etcdv3 client: open /calico-secrets/etcd-cert: permission denied
</code></pre>
<p>找到资源清单内的对应容器（<code>calico-kube-controllers</code>）的配置。在卷装载中设置440将解决此问题</p>
<pre><code class="language-yaml">volumes:
# Mount in the etcd TLS secrets with mode 400.
# See https://kubernetes.io/docs/concepts/configuration/secret/
- name: etcd-certs
  secret:
  secretName: calico-etcd-secrets
  defaultMode: 0400 # 改为0440
</code></pre>
<h2 id="修改calicoctl的数据源">修改calicoctl的数据源</h2>
<p>　使用单独的etcd作为calico数据存储还需要修改calicoctl数据存储访问配置</p>
<p>　<code>calicoctl</code> 在默认情况下，查找配置文件的路径为<code>/etc/calico/calicoctl.cfg</code>上。可以使用<code>--config</code>覆盖此选项默认配置（使用中测试不成功，官方给出有这个方法）。</p>
<p>　如果<code>calicoctl</code>无法获得配置文件，将检查环境变量。</p>
<pre><code class="language-yaml">apiVersion: projectcalico.org/v3
kind: CalicoAPIConfig
metadata:
spec:
  datastoreType: etcdv3
  etcdEndpoints: &quot;https://10.0.0.6:2379&quot;
  etcdCACert: |
    # 这里填写etcd ca证书文件的内容，无需转码base64
  etcdCert: |
    # 这里填写etcd client证书文件的内容，无需转码base64
  etcdKey: |
    # 这里填写etcd client秘钥文件的内容，无需转码base64
</code></pre>
<blockquote>
<p>reference：</p>
<p><a href="https://github.com/kubernetes/website/issues/25587" target="_blank"
   rel="noopener nofollow noreferrer" >Secret permission denied</a></p>
<p><a href="https://docs.projectcalico.org/getting-started/clis/calicoctl/configure/overview" target="_blank"
   rel="noopener nofollow noreferrer" >configuration calicoctl</a></p>
<p><a href="https://docs.projectcalico.org/reference/installation/api" target="_blank"
   rel="noopener nofollow noreferrer" >calico installation</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>基于Kubernetes的PaaS平台提供dashboard支持的一种方案</title>
      <link>https://www.oomkill.com/2021/01/pass-base-dashboard-k8s/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/01/pass-base-dashboard-k8s/</guid>
      <description></description>
      <content:encoded><![CDATA[<p><strong>本文转自博客</strong>： <a href="">我的小米粥分你一半</a></p>
<blockquote>
<p>我一直在负责维护的PaaS平台引入了Kubernetes作为底层支持, 可以借助Kubernetes的生态做更多的事情, 这篇博客主要介绍如何为用户提供dashboard功能, 以及一些可以扩展的想法. 希望读者有一定的kubernetes使用经验, 并且了解rbac的功能。</p>
</blockquote>
<h2 id="dashboard功能">Dashboard功能</h2>
<p>Kubernetes原生提供了Web界面, 也就是Dashboard, 具体的参考可以见<a href="https://kubernetes.io/zh/docs/tasks/access-application-cluster/web-ui-dashboard/" target="_blank"
   rel="noopener nofollow noreferrer" >官方文档</a>:</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220516174111629.png" alt="image-20220516174111629" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>​	安装完成后, 我们一般是通过token来使用的, 不同的token有着不同的权限.</p>
<p>​	<img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220516174123726.png" alt="image-20220516174123726" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>上面所说的<code>token</code>是<code>Bearer Token</code>, 除了在界面上输入之外, 你可以这么来用, 通过添加header即可.</p>
<pre><code>curl -H &quot;Authorization: Bearer ${TOKEN}&quot; https://{dashboard}/api/myresource
</code></pre>
<h2 id="paas平台使用dashboard简要讨论">PaaS平台使用Dashboard简要讨论</h2>
<h3 id="需求分析">需求分析</h3>
<p>Dashboard本身的功能是十分强大的, 但是给所有人admin权限显然是不现实的. 对于一个普通用户来讲, PaaS平台的将他的应用(代码)部署好并运行, 他所需要关注的就只有属于他自己的项目, 平台也需要做好权限控制, 避免一个用户操作了另一个用户的应用.</p>
<h3 id="权限系统设计">权限系统设计</h3>
<p>基于以上的需求讨论, 平台需要做的操作就是为每个用户创建属于自己的权限提供, 并限制可以访问到的资源. 考虑这样的情况:</p>
<p>我们有一个用户A, 他拥有自己的一个应用群组(G), 群组中部署了一系列应用程序(a1, a2…). 在Kubernetes中, 这样的群组概念我们将其映射为namespace, <code>群组(G) &lt;=&gt; 用户空间(NS)</code>, 我们需要控制的权限控制策略就变成了用户A在用户空间NS的权限控制.</p>
<h3 id="token分发策略">token分发策略</h3>
<p>拥有了权限控制后, 所需要打就是将token分发给用户, 当然这是一种极度不安全的做法, Kubernetes中的token创建之后一般是不会改变的, 分发这样的token会有很大的安全风险, 有两个方面:</p>
<pre><code>1. 用户A将token保存了下来, 那么他就能不经过平台登录Dashboard, 这样不利于审计工作,
2. token一旦泄露, PaaS平台很难做到反应(因为token脱离了平台的控制, 无法判断究竟是什么时候发生了泄露, 也无法马上吊销这个token), 安全风险比较高.
</code></pre>
<p>因此, 最好的做法就是不把token交给用户, 用户每次想要登录dashboard, 从平台进行跳转, 跳转时携带安全信息, 在dashboard登录时, 由平台自己的程序请求token, 避免经手用户.</p>
<hr>
<blockquote>
<p>如果到这里, 你没有理解上面的内容, 建议回去再看一次需求, 如果还是理解不了, 就不要往下看了, 下面只是介绍具体的实现方案.</p>
</blockquote>
<hr>
<h2 id="kubernetes权限限制">Kubernetes权限限制</h2>
<p>Kubernetes本身有着比较复杂的权限控制系统, 设计时没必要纠结过多, 按照可以给用户和不能给用户的权限进行区分就好了. 我直接贴一下我的权限控制策略吧, 并不一定适合每个人, 只是可以做个参考.</p>
<pre><code>---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: xxx:xxx-group-yyy
  namespace: xxx-group-yyy
rules:
  # 可以查看当前NS下面的service, pod, logs, events
  - apiGroups: [&quot;&quot;]
    resources: [&quot;services&quot;, &quot;pods&quot;, &quot;pods/log&quot;, &quot;events&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]

  # 可以使用exec命令进入容器
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods/exec&quot;]
    verbs: [&quot;create&quot;, &quot;list&quot;, &quot;watch&quot;]

  # 可以查看deployments和replicasets
  - apiGroups: [&quot;extensions&quot;, &quot;apps&quot;]
    resources: [&quot;deployments&quot;, &quot;replicasets&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]

  # 可以查看job, cronjob以及ingress
  - apiGroups: [&quot;batch&quot;, &quot;extensions&quot;]
    resources: [&quot;cronjobs&quot;, &quot;jobs&quot;, &quot;ingresses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
</code></pre>
<p>正如上面的注释一样, 尽可能只给用户只读的权限, 也许你已经发现了, 甚至不需要给用户namespace的查看权限, 这也是为了安全, 避免用户得知其他人的namespace.</p>
<h2 id="分发token以及安全性保证">分发Token以及安全性保证</h2>
<p>这是本篇博客的核心内容: 如何使得用户可以无感知的登录到dashboard(对用户隐藏token).</p>
<p>该方案用到的方法是: 添加一层访问控制的网关, 用于处理token获取的操作, 具体的流程图如下.</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220516174135751.png" alt="image-20220516174135751" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>需要注意的有几点:</p>
<ol>
<li>PaaS给出的<code>secret_code</code>是有时效性的, 不允许用户一直用同一个<code>secret_code</code>进行访问</li>
<li>网关与PaaS平台间的通信应该加密, 网关必须是PaaS平台可信的</li>
<li>网关不应该长期保存<code>token</code></li>
<li>网关的访问最好添加OpenID校验, 确保网关可以精确定位到每个用户的每次访问</li>
</ol>
<h3 id="体验优化">体验优化</h3>
<ol>
<li>首先, 第2步到第3步, <code>secret_code</code>获取之后, 可以以302重定向的方式跳转至网关入口</li>
<li>网关可以临时性的保存<code>secret_code</code>与<code>token</code>的映射关系, 既能够提升用户体验, 也能有效减缓PaaS平台的压力</li>
<li>dashbaord的webshell功能是基于websocket支持的, 所以请确保你的网关可以通过websocket请求, 否则终端连接后几分钟就断了, websocket可以持续几个小时那么久</li>
<li>跳转到网关时, 可以携带更多的信息, 比如携带某个pod的id, 网关就可以直接跳转到对应的pod, 用户打开webshell就很方便了</li>
</ol>
<p>网关的实现我不做过多的说明了, 只有一点建议, <code>secret_code</code>在跳转到网关后, 马上进行校验. 由于<code>dashboard</code>的前端路由实现问题, <code>secret_code</code>最好在校验后加密放置到cookie中, 实现方面的问题其他可以发邮件与我讨论.</p>
<h2 id="总结">总结</h2>
<p>这篇博客主要介绍了一种允许普通用户使用dashboard的功能. 在实现策略上, 利用了kubernetes的权限限制, token隐藏的方案, 该方案目前我已经加入到了我负责的PaaS平台中, 稳定性方面可以满足工作需求, 安全性正如博客中介绍, 大家可以自行斟酌.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>calico network cni网络方案</title>
      <link>https://www.oomkill.com/2021/01/calico-network-cni/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/01/calico-network-cni/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>Calico针对容器、虚拟机的开源网络和网络安全解决方案。是纯三层的数据中心网络方案。</p>
<p>Calico在每一个计算节点利用Linux Kernel实现了一个高效的虚拟路由器<code>vRouter</code>来负责数据转发，而每个<code>vRouter</code>通过BGP协议负责把自己上运行的workload的路由信息向整个Calico网络内传播。（小规模部署可以直接互联 <code>BGP full mesh</code>，大规模下可通过指定的<code>BGP route reflector</code>来完成）。 这样保证最终所有的<code>workload</code>之间的数据流量都是通过IP路由的方式完成互联的。Calico节点组网可以直接利用数据中心的网络结构（无论是L2或者L3），不需要额外的<code>NAT</code>，隧道或者<code>Overlay Network</code>。</p>
<p>Calico还基于<code>iptables</code>还提供了丰富而灵活的网络<code>Policy</code>，保证通过各个节点上的<code>ACLs</code>来提供Workload的多租户隔离、安全组以及其他可达性限制等功能。</p>
<h3 id="calico组件">calico组件</h3>
<p>在Kubernetes平台之上<code>calico/node</code>容器会通过DaemonSet部署到每个节点，并运行三个守护程序：</p>
<ul>
<li>Felix：用于管理路由规则，负责状态上报。</li>
<li>BIRD：BGP的客户端，用于将Felix的路由信息加载到内核中，同时负责路由信息在集群中的分发。</li>
<li>confd：用于监视Calico存储（etcd）中的配置变更并更新<code>BIRD</code>的配置文件。</li>
</ul>
<p>calicoctl使用问题</p>
<pre><code>Failed to create Calico API client: invalid configuration: no configuration has been provided
</code></pre>
<p>默认情况下，<code>calicoctl</code> 将使用位于的默认<code>KUBECONFIG</code>从 Kubernetes APIServer 读取<code>$(HOME)/.kube/config</code> 。</p>
<p>如果默认的 <code>KUBECONFIG</code> 不存在，或者想从指定的存储访问信息，则需要单独配置。</p>
<pre><code class="language-bash">export DATASTORE_TYPE=kubernetes
export DATASTORE_TYPE=etcdv3
export KUBECONFIG=~/.kube/config
</code></pre>
<p><a href="https://docs.projectcalico.org/getting-started/clis/calicoctl/configure/" target="_blank"
   rel="noopener nofollow noreferrer" >reference for</a></p>
<h2 id="calico-安装配置">calico 安装配置</h2>
<p>开始前准备</p>
<p>确定calico数据存储</p>
<p>Calico同时支持kubernetes api和etcd数据存储。官方给出的建议是在本地部署中使用<strong>K8S API</strong>，仅支持Kubernetes模式。而官方给出的etcd则是混合部署（Calico作为Kubernetes和OpenStack的网络插件运行）的最佳数据存储。</p>
<p>使用kubernetes api作为数据存储的安装</p>
<pre><code>curl https://docs.projectcalico.org/manifests/calico.yaml -O
kubectl apply -f calico.yaml
</code></pre>
<p>修改Pod CIDR</p>
<p>Calico默认的Pod CIDR使用的是<code>192.168.0.0/16</code>，这里一般使用与controller-manager中的<code>--cluster-cidr</code> 保持一,取消资源清单内的 <code>CALICO_IPV4POOL_CIDR</code>变量的注释，并将其设置为与所选Pod CIDR相同的值。</p>
<p>calico的IP分配范围</p>
<p>Calico IPAM从<code>ipPool</code>分配IP地址。修改Pod的默认IP范围则修改清单<code>calico.yaml</code>中的 <code>CALICO_IPV4POOL_CIDR</code></p>
<p>配置Calico的 <code>IP in IP</code></p>
<p>默认情况下，Calico中的IPIP已经禁用，这里使用的v3.17.2 低版本默认会使用IPIP</p>
<p>要开启IPIP mode则需要修改配置清单内的 <code>CALICO_IPV4POOL_IPIP</code> 环境变量改为 <code>always</code></p>
<p>修改secret</p>
<pre><code class="language-yaml">  # Populate the following with etcd TLS configuration if desired, but leave blank if
  # not using TLS for etcd.
  # The keys below should be uncommented and the values populated with the base64
  # encoded contents of each file that would be associated with the TLS data.
  # Example command for encoding a file contents: cat &lt;file&gt; | base64 -w 0
  # etcd的ca
etcd-ca: # 填写上面命令编码后的值
# etcd客户端key
etcd-key: # 填写上面命令编码后的值
# etcd客户端访问证书
etcd-cert: # 填写上面命令编码后的值
</code></pre>
<p>修改configMap</p>
<pre><code class="language-yaml">  etcd_endpoints: &quot;https://10.0.0.6:2379&quot;
  # If you're using TLS enabled etcd uncomment the following.
  # You must also populate the Secret below with these files.
  etcd_ca: &quot;/calico-secrets/etcd-ca&quot;
  etcd_cert: &quot;/calico-secrets/etcd-cert&quot;
  etcd_key: &quot;/calico-secrets/etcd-key&quot;
</code></pre>
<p>在卷装载中设置440将解决此问题</p>
<p><code>/calico-secrets/etcd-cert: permission denied</code></p>
<pre><code>2021-02-08 02:15:10.485 [INFO][1] main.go 88: Loaded configuration from environment config=&amp;config.Config{LogLevel:&quot;info&quot;, WorkloadEndpointWorkers:1, ProfileWorkers:1, PolicyWorkers:1, NodeWorkers:1, Kubeconfig:&quot;&quot;, DatastoreType:&quot;etcdv3&quot;}
2021-02-08 02:15:10.485 [FATAL][1] main.go 101: Failed to start error=failed to build Calico client: could not initialize etcdv3 client: open /calico-secrets/etcd-cert: permission denied
</code></pre>
<p>找到资源清单内的对应容器（<code>calico-kube-controllers</code>）的配置。</p>
<pre><code class="language-yaml">volumes:
# Mount in the etcd TLS secrets with mode 400.
# See https://kubernetes.io/docs/concepts/configuration/secret/
- name: etcd-certs
  secret:
  secretName: calico-etcd-secrets
  defaultMode: 0400 # 改为0440
</code></pre>
<p>使用单独的etcd作为calico数据存储还需要修改calicoctl数据存储访问配置</p>
<p><code>calicoctl</code> 在默认情况下，查找配置文件的路径为<code>/etc/calico/calicoctl.cfg</code>上。可以使用<code>--config</code>覆盖此选项默认配置。</p>
<p>如果<code>calicoctl</code>无法获得配置文件，将检查环境变量。</p>
<pre><code class="language-yaml">apiVersion: projectcalico.org/v3
kind: CalicoAPIConfig
metadata:
spec:
  datastoreType: etcdv3
  etcdEndpoints: &quot;https://10.0.0.6:2379&quot;
  etcdCACert: |
    # 这里填写etcd ca证书文件的内容，无需转码base64
  etcdCert: |
    # 这里填写etcd client证书文件的内容，无需转码base64
  etcdKey: |
    # 这里填写etcd client秘钥文件的内容，无需转码base64
</code></pre>
<blockquote>
<p>reference：</p>
<p><a href="https://github.com/kubernetes/website/issues/25587" target="_blank"
   rel="noopener nofollow noreferrer" >Secret permission denied</a></p>
<p><a href="https://docs.projectcalico.org/getting-started/clis/calicoctl/configure/overview" target="_blank"
   rel="noopener nofollow noreferrer" >configuration calicoctl</a></p>
</blockquote>
<pre><code>eb  7 21:25:13 master01 etcd: recognized environment variable ETCD_NAME, but unused: shadowed by corresponding flag
Feb  7 21:25:13 master01 etcd: unrecognized environment variable ETCD_SERVER_NAME=hk.etcd
Feb  7 21:25:13 master01 etcd: recognized environment variable ETCD_DATA_DIR, but unused: shadowed by corresponding flag
Feb  7 21:25:13 master01 etcd: recognized environment variable ETCD_LISTEN_CLIENT_URLS, but unused: shadowed by corresponding flag
Feb  7 21:25:13 master01 etcd: etcd Version: 3.3.11
Feb  7 21:25:13 master01 etcd: Git SHA: 2cf9e51
Feb  7 21:25:13 master01 etcd: Go Version: go1.10.3
Feb  7 21:25:13 master01 etcd: Go OS/Arch: linux/amd64
Feb  7 21:25:13 master01 etcd: setting maximum number of CPUs to 2, total number of available CPUs is 2
Feb  7 21:25:13 master01 etcd: the server is already initialized as member before, starting as etcd member...
Feb  7 21:25:13 master01 etcd: peerTLS: cert = /etc/etcd/pki/peer.crt, key = /etc/etcd/pki/peer.key, ca = , trusted-ca = /etc/etcd/pki/ca.crt,
 client-cert-auth = true, crl-file =
Feb  7 21:25:13 master01 etcd: listening for peers on https://10.0.0.5:2380
Feb  7 21:25:13 master01 etcd: listening for client requests on 10.0.0.5:2379
Feb  7 21:25:13 master01 etcd: panic: freepages: failed to get all reachable pages (page 3471766746605708656: out of bounds: 1633)
</code></pre>
<p>集群节点损坏</p>
<pre><code>panic: freepages: failed to get all reachable pages (page 3471766746605708656: out of bounds: 1633)
</code></pre>
<p>这是k8s不支持当前calico版本的原因, calico版本与k8s版本支持关系可到calico官网查看:</p>
<pre><code class="language-bash">error: unable to recognize &quot;calico.yaml&quot;: no matches for kind &quot;PodDisruptionBudget&quot; in version &quot;policy/v1&quot;
</code></pre>
<p>配置SW</p>
<pre><code>system-view
sysname SW1
vlan batch 10 20 30

interface GigabitEthernet0/0/1
port link-type trunk
port trunk allow-pass vlan 10 20 30

interface GigabitEthernet0/0/2
port link-type trunk
port trunk allow-pass vlan 10 20 30

interface GigabitEthernet0/0/3
port link-type trunk
port trunk allow-pass vlan 10 20 30
</code></pre>
<p>配置路由器间的ospf</p>
<pre><code>interface l0
ip address 1.1.1.1 32
quit
ospf router-id 1.1.1.1
area 0
network 1.1.1.1 0.0.0.0
network 10.0.0.253 0.0.0.0
dis this

interface l0
ip address 2.2.2.2 32
quit
ospf router-id 2.2.2.2
area 0
network 2.2.2.2 0.0.0.0
network 10.0.0.254 0.0.0.0
dis this
</code></pre>
<p>配置两个k8s节点与路由器之间的bgp</p>
<pre><code>system-view
sysname R1

interface GigabitEthernet0/0/0
ip address 10.0.0.253 24
dis this
quit

bgp 64512
router-id 10.0.0.253
peer 10.0.0.5 as-number 64512
peer 10.0.0.5 reflect-client
dis ip interface brief



system-view
sysname R2

interface GigabitEthernet0/0/0
ip address 10.0.0.254 24
dis this
quit

bgp 63400
router-id 10.0.0.254
peer 10.0.0.6 as-number 63400
peer 10.0.0.6 reflect-client
dis ip interface brief

bgp 64512
router-id 10.0.0.253 
peer 2.2.2.2 as-number 63400


bgp 63400
router-id 10.0.0.254
peer 1.1.1.1 as-number 64512
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>网络隧道技术</title>
      <link>https://www.oomkill.com/2021/01/network-tunnel-technology/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/01/network-tunnel-technology/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="隧道技术概要">隧道技术概要</h2>
<p>隧道技术（<em><strong>Tunneling</strong></em>）是网络基础设置在网络之间传递数据的方式，使用隧道技术传递可以是不同协议的数据包，隧道协议将这些其他协议的数据包重新封装在新的包头中发送。被封装的数据包在隧道的两个端点之间通过网络进行路由，<strong>被封装数据包在网络上传递时所经历的逻辑路径称为隧道</strong>。</p>
<p>简单来说，<font style="background:#f8070d;" size=3>隧道技术是一类网络协议</font>，是将一个数据包封装在另一个数据包中进行传输的技术；**使用隧道的原因是在不兼容的网络上传输数据，或在不安全网络上提供一个安全路径。**通过网络隧道技术，可以使隧道两端的网络组成一个更大的内部网络。（把不支持的协议数据包打包成支持的协议数据包之后进行传输）。</p>
<h2 id="隧道协议">隧道协议</h2>
<p>要创建隧道，隧道的客户机和服务器双方必须使用相同的隧道技术，隧道协议有二层隧道协议与三层隧道协议两类。</p>
<p>二层隧道协议对应OSI模型中数据链路层，使用 <strong>帧</strong> 作为数据交换单位，PPTP、L2TP、L2F都属于二层隧道协议。是将数据封装在点对点协议的帧中通过互联网络发送。</p>
<p>三层隧道协议对应OSI模型中网络层，使用 <strong>包</strong> 作为数据交换单位，GRE、IPSec 都属于三层隧道协议。都是数据包封装在附加的IP包头中通过IP网络传送。</p>
<p>在例如VxLAN，工作在传输层和网络层之间。具体来说，将运行在用户数据报协议 (UDP) 和网络数据报协议 (IP) 之间，以便在网络中建立安全的通信通道。</p>
<h2 id="网络隧道技术应用">网络隧道技术应用</h2>
<h3 id="隧道在linux-中应用">隧道在Linux 中应用</h3>
<p>IP隧道是指一种可在两网络间进行通信的通道。在该通道里，会先封装其他网络协议的数据包，之后再传输信息。</p>
<p>Linux原生共支持5种IPIP隧道：</p>
<ul>
<li>ipip: 普通的IPIP隧道，就是在报文的基础上再封装成一个IPv4报文</li>
<li>gre: 通用路由封装（Generic Routing Encapsulation），定义了在任意一种网络层协议上封装其他任意一种网络层协议的机制，所以对于IPv4和IPv6都适用</li>
<li>sit: sit模式主要用于IPv4报文封装IPv6报文，即IPv6 over IPv4</li>
<li>isatap: 站内自动隧道寻址协议（Intra-Site Automatic Tunnel Addressing Protocol），类似于sit也是用于IPv6的隧道封装</li>
<li>vti: 即虚拟隧道接口（Virtual Tunnel Interface），是一种IPsec隧道技术</li>
</ul>
<p>像IPVS/LVS中的 <code>Virtual Server via IP Tunneling</code>，就是使用了IPIP隧道</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1b499670.png" alt="1b499670" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="ssh隧道技术">SSH隧道技术</h3>
<p>SSH提供了一个重要功能，称为转发 <font color=#f8070d size=3><code>forwarding</code></font> 或者称为隧道传输<font color=#f8070d size=3><code>tunneling</code></font>，它可以通过加密频道将明文流量导入隧道中，在创建SSH隧道时， SSH客户端要设置并转交一个特定本地端口号到远程机器上；一旦SSH隧道创建，用户可以连到指定的本地端口号以访问网络服务。本地端口号不用与远地端口号一样。</p>
<p>SSH隧道主要使用场景一般为 <strong>规避防火墙</strong>、<strong>加密网络流量</strong></p>
<p><strong>规避防火墙</strong>，SSH隧道可以使一个被防火墙阻挡的协议可被包在另一个没被防火墙阻挡的协议里，这技巧可用来逃避防火墙政策。而这种操作符合“数据包封装在另一个数据包中进行传输的技术”，故称为SSH隧道技术。</p>
<h3 id="ssh隧道类型">SSH隧道类型</h3>
<p>在ssh连接的基础上，指定 <code>ssh client</code> 或 <code>ssh server</code> 的某个端口作为源地址，所有发至该端口的数据包都会透过ssh连接被转发出去；至于转发的目标地址，目标地址既可以指定，也可以不指定，如果指定了目标地址，称为定向转发，如果不指定目标地址则称为动态转发：</p>
<p><strong>定向转发</strong></p>
<p>定向转发把数据包转发到指定的目标地址。目标地址不限定是ssh client 或 ssh server，既可以是二者之一，也可以是二者以外的其他机器。</p>
<p><strong>动态转发</strong></p>
<p>动态转发不指定目标地址，数据包转发的目的地是动态决定的。</p>
<h4 id="本地端口转发">本地端口转发</h4>
<p>本地转发中的本地是指将本地的某个端口(1024-65535)通过SSH隧道转发至其他主机的套接字，这样当我们的程序连接本地的这个端口时，其实间接连上了其他主机的某个端口，当我们发数据包到这个端口时数据包就自动转发到了那个远程端口上了</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20200730182255288.png" alt="image-20200730182255288" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="远程端口转发">远程端口转发</h4>
<p>远程转发和本地很相似，原理也差不多，但是不同的是，本地转发是在本地主机指定的一个端口，而远程转发是由SSH服务器经由SSH客户端转发，连接至目标服务器上。本质一样，区别在于需要转发的端口是在远程主机上还是在本地主机上</p>
<p>现在SSH就可以把连接从（39.104.112.253:80）转发到（10.0.0.10:85）。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20200730182352895.png" alt="image-20200730182352895" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="动态端口转发">动态端口转发</h4>
<p>定向转发（包括本地转发和远程转发）的局限性是必须指定某个目标地址，如果需要借助一台中间服务器访问很多目标地址，一个一个地定向转发显然不是好办法，这时就要用的是ssh动态端口转发，它相当于建立一个SOCKS服务器。各种应用经由SSH客户端转发，经过SSH服务器，到达目标服务器，不固定端口。</p>
<h3 id="ssh隧道的本质">SSH隧道的本质</h3>
<p><strong>SSH隧道可以被认为是一种应用层隧道</strong>，与其他隧道类型（如IPIP, VxLAN）不同的是，SSH隧道是基于SSH协议的一种应用，而IPIP, VxLAN这种，则是基于IP协议，UDP协议的一种封包机制。</p>
<p>SSH（<em><strong>Secure Shell</strong></em>）是一种网络协议，支持远程登录和其他安全网络服务的加密通信。SSH隧道属于SSH协议中的一种应用场景，用于在SSH加密连接上建立通信隧道。SSH隧道允许用户通过加密终端 (SSH客户端) 和远程服务之间的连接，在不暴露底层网络协议的信息（例如IP地址、端口号等）的情况下，传输数据。</p>
<p>SSH隧道工作方式如下：</p>
<ul>
<li>首先，在本地主机和目标服务器之间建立SSH连接，SSH连接是一条安全加密的连接管道，连接过程中对数据进行加密传输。</li>
<li>连接建立后，通过SSH隧道在本地主机和目标服务器之间建立一个TCP连接，并将本地主机上的数据通过SSH隧道加密传输到目标服务器，目标服务器接收数据，解密后将数据传输到最终目的地。</li>
<li>同样，当接收数据时，目标服务器会将数据加密再通过SSH隧道传输回本地主机。</li>
</ul>
<p>由于SSH隧道在SSH连接上建立通信隧道，因此可以将其视为应用层隧道。应用层隧道是在应用层协议上建立的隧道，用于将应用程序传输的数据加密传输到目标地址。SSH隧道给用户提供了一种安全的数据通信方法，在安全性上比普通TCP/IP连接更具有优势。</p>
<p>SSH隧道也可以成为一种代理模式，常用于越过不可访问的网络时使用</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1%20s8z9sil7EnwspH77kdrzjg.webp" alt="1 s8z9sil7EnwspH77kdrzjg" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：SSH隧道应用图解</center>
<center><em>Source：</em>https://infosecwriteups.com/bypass-the-firewall-with-ssh-tunnelling-711fa78ea97f</center><br>
<h3 id="其他隧道协议">其他隧道协议</h3>
<p>对于隧道，上面也提到了，隧道就是网络数据包封包一种协议，那就是说很多常见的协议其实都是隧道技术</p>
<ul>
<li>工作与数据链路层的隧道技术：
<ol>
<li>PPP隧道协议（Point-to-Point Protocol）：PPP隧道协议是一种在两个点之间建立可靠连接的协议，它能够在一条串行线路上同时传输多种网络层协议。PPP隧道协议通过在两个点之间建立隧道，将其他协议的数据封装起来进行传输。</li>
<li>L2TP协议（Layer 2 Tunnel Protocol）：L2TP协议是一种在不安全的公共网络上传输数据的加密协议，常用于建立VPN（Virtual Private Network）隧道。L2TP协议将PPP协议属性和L2TP控制消息封装在IP（Internet Protocol）数据报中。</li>
<li>PPTP协议（Point-to-Point Tunneling Protocol）：PPTP协议是一种在不安全的公共网络上传输数据的加密协议，也常用于建立VPN隧道。PPTP协议通过在数据包中添加PPTP头和PPP协议数据负载来传输数据。</li>
<li>GRE协议（Generic Routing Encapsulation）：GRE协议是一种通用路由封装协议，它可以将其他协议的数据封装在IP数据报中进行传输。GRE协议主要用于连接不同类型的网络，通常用于建立VPN隧道。</li>
</ol>
</li>
<li>工作与网络层的隧道协议：
<ol>
<li>负载均衡协议 (LBP) 是一种在网络层以上实现的协议，用于在二层 (链路层) 上实现数据包的转发。LBP 可以将数据包转发到多个服务器上，从而实现负载均衡。LBP 可以用于实现网站负载均衡、存储集群等功能。</li>
<li>协议映射协议 (PMP) 是一种在网络层以下实现的协议，用于在网络层以上实现数据包的映射。PMP  可以将一个数据包映射到另一个数据包中，从而实现数据包的转发。PMP 可以用于实现虚拟专用网络 (Virtual Private  Network,VPN) 和防火墙等功能。</li>
<li>虚拟隧道协议 (Virtual Tunneling Protocol,VTP) 是一种在网络层以下实现的协议，用于在网络中创建和管理隧道。VTP 可以将一个网络中的多个子网互联，使得数据包可以在这些子网之间传输。VTP 可以用于实现数据包的路由、负载均衡和安全性等方面。</li>
</ol>
</li>
<li>工作与应用层的隧道技术：
<ol>
<li>HTTP隧道：HTTP隧道通过HTTP连接创建隧道，将其他协议的数据封装在HTTP报文中，传输到目标地址。HTTP隧道通常用于访问受限制的服务器，如防火墙后的服务器。</li>
<li>SSL/TLS隧道：SSL/TLS隧道也是基于加密传输的应用层隧道。通过SSL/TLS加密传输，将通信数据封装在加密连接中，传输到目标服务器。SSL/TLS隧道通常用于保护Web应用程序中传输的机密数据。</li>
<li>SOCKS代理隧道：SOCKS代理隧道是一种应用层代理协议，用于将流量转发到目标地址并代理转发返回数据。SOCKS代理隧道通常用于隐藏客户端的真实IP地址和身份。</li>
<li>DNS隧道：DNS隧道是通过将数据封装在DNS请求或响应中来传输数据的应用层隧道。DNS隧道通常被用于绕过安全防护措施或访问受限制的服务器。</li>
</ol>
</li>
</ul>
<h2 id="ccp常提到的非法信道中的信道和隧道是一样的吗">CCP常提到的“非法信道”中的“信道”和“隧道”是一样的吗？</h2>
<p>首先，“信道”和“隧道” 是两种不同的概念，就和“男人”和“女人”一样，同属于人但完全不同，常见的表示形式如下：</p>
<ol>
<li>意义不同：<strong>信道</strong>是指物理媒介或虚拟路径，用于数据的传输，例如网络电缆或无线信道。<strong>隧道</strong>则是一种逻辑隧道，通过在底层通信协议的基础上创建加密通道来传输数据。</li>
<li>位置不同：<strong>信道</strong>通常是指在通信的物理媒介上的传输路径，而<strong>隧道</strong>则是在信道之上的OSI模型层协上创建加密通道的逻辑概念。</li>
<li>传输方式不同：<strong>信道</strong>是直接用于传输数据的物理媒介，信号通过信道进行传输；<strong>隧道</strong>则是在传输数据时，将数据封装成新的协议格式，通过信道进行加密传输。</li>
<li>使用场景不同：<strong>信道</strong>常用于介质访问控制、传输层控制、传输介质选择等方面，例如在局域网中使用以太网电缆传送数据。<strong>隧道</strong>则通常用于保障企业内部网络安全、建立虚拟专用网络、跨越防火墙等隧道服务需求。</li>
<li>技术特点不同：<strong>信道</strong>是一种物理层或数据链路层技术，而<strong>隧道</strong>是一种应用层或数据链路层技术。</li>
</ol>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes包管理 - Helm</title>
      <link>https://www.oomkill.com/2019/11/helm/</link>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/11/helm/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="什么是-helm">什么是 Helm</h2>
<p>Helm 是一个用于管理 Kubernetes 应用程序的包管理工具。它允许您定义、安装和升级 Kubernetes 应用程序，以简化应用程序部署和管理的过程。</p>
<p>在 Kubernetes 中，应用程序被打包为一个或多个称为 &ldquo;Charts&rdquo; 的 Helm 资源。一个 Chart 是一个预定义的目录结构，包含了用于部署应用程序的 Kubernetes 资源清单模板。Chart 可以包含 Deployment、Service、ConfigMap、Ingress 等 Kubernetes 资源的定义。</p>
<p>使用 Helm，您可以将应用程序打包为一个 Chart，并使用 Helm 客户端来安装和管理 Chart。这使得应用程序的部署过程更加简单、可重复和可扩展。您可以根据需要部署多个实例，轻松地进行升级和回滚操作，并使用 Helm 提供的值覆盖机制来自定义每个实例的配置。</p>
<p>最重要的是，Helm 支持使用 Helm 仓库来共享和发布 Charts。Helm 仓库是一个集中存储 Charts 的地方，供用户从中搜索和安装 Charts。Helm 仓库可以是公共的，也可以是私有的，您可以自己搭建私有仓库来管理自己的 Charts。</p>
<h3 id="helm-所作的事情">Helm 所作的事情</h3>
<p>Helm 管理名为 <code>chart</code> 的Kubernetes包的工具。故 Helm 可以做以下的事情：</p>
<ul>
<li>创建一个新的 chart</li>
<li>将 chart 打包成归档 (tgz) 文件</li>
<li>与存储 chart 的仓库进行交互</li>
<li>在现有的 Kubernetes 集群中安装和卸载 chart</li>
<li>管理与Helm一起安装的 chart 的发布周期</li>
</ul>
<h3 id="helm中的术语">Helm中的术语</h3>
<ul>
<li><em>chart</em>：类似于rpm包，deb包，包含Kubernetes资源所需要的必要信息。</li>
<li><em>repo</em>：chart仓库，类似于yum的仓库，chart仓库是一个简单的HTTP服务。</li>
<li><em>values</em>：提供了自定义信息用来覆盖模板中的默认值。</li>
<li><em>release</em> ：chart安装后的版本记录。</li>
</ul>
<h3 id="helm-与-yaml-资源清单比有什么优势">Helm 与 YAML 资源清单比有什么优势？</h3>
<ol>
<li><strong>模板化和参数化</strong>: Helm 使用 Go 的模板引擎来创建 Kubernetes 资源清单。这使得您可以在 Chart 中使用模板来定义资源配置的部分内容，例如标签、名称、端口等。同时，Helm 还支持使用参数化的值，允许您根据不同的环境或需求来自定义 Chart 的配置。这样一来，您可以根据需要生成不同的 Kubernetes 资源清单，而无需手动编辑每个清单文件。</li>
<li><strong>可重用性</strong>: Helm 提供了一种将应用程序打包为 Chart 的方式，可以将 Chart 存储在 Helm 仓库中进行共享和重用。这样，您可以使用其他人创建的 Charts 来快速部署常见的应用程序，避免从头开始编写和管理 Kubernetes 资源清单。同时，您也可以将自己的应用程序打包为 Chart，方便自己和团队在不同环境中部署和管理。</li>
<li><strong>版本管理和升级</strong>: 使用 Helm，您可以对已安装的 Chart 进行版本管理和升级。当应用程序的配置或代码发生变化时，您可以通过升级 Chart 来自动应用这些更改，而无需手动修改和重新部署 Kubernetes 资源清单。Helm 还提供了回滚功能，允许您在升级出现问题时快速回退到之前的版本。</li>
<li><strong>依赖管理</strong>: Helm 允许您在 Chart 中定义和管理依赖关系。这意味着您可以在部署应用程序时自动解析和安装它所依赖的其他 Charts。这样，您可以轻松地管理应用程序所需的其他资源，减少手动处理依赖关系的工作。</li>
<li><strong>部署的一致性和标准化</strong>: Helm 提供了一种标准的部署方式，使得不同团队或开发者之间可以使用相同的工具和流程来管理应用程序的部署。这样可以确保在不同环境中的一致性，并降低由于不同部署方式导致的错误和配置差异。</li>
<li><strong>可管理的 Charts</strong>: Helm Charts 是可管理的，您可以在 Chart 中定义预先配置的模板、默认值、钩子和配置验证。这使得管理应用程序的配置和部署过程更加灵活和可控。</li>
<li><strong>社区支持和生态系统</strong>: Helm 是一个活跃的开源项目，拥有庞大的用户社区和丰富的生态系统。这意味着您可以轻松地找到文档、示例、教程和问题解答，并从社区中获取支持和贡献。</li>
<li><strong>可扩展性和插件支持</strong>: Helm 提供了插件机制，允许您扩展 Helm 的功能。您可以使用插件来添加自定义的命令、功能和工作流程，以满足特定需求或自动化常见的任务。</li>
<li><strong>可视化界面和用户友好性</strong>: Helm 可以与各种第三方工具和平台集成，提供可视化界面和用户友好的操作方式。这使得非技术人员或不熟悉命令行的开发人员也能够方便地部署和管理应用程序。</li>
</ol>
<h2 id="安装helm">安装helm</h2>
<p>Helm 安装主要官方提供了几种安装方式</p>
<ul>
<li>二进制版本安装：利用预编译好的二进制包直接解压使用</li>
<li>使用脚本安装：Helm 提供了安装脚本，可以直接拉去最新版进行安装在本地</li>
<li>各操作系统上的包管理工具进行安装</li>
</ul>
<h3 id="添加源">添加源</h3>
<pre><code class="language-bash">helm repo add stable http://mirror.azure.cn/kubernetes/charts/
helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/
</code></pre>
<h2 id="helm-使用例子-supa-href22asup">Helm 使用例子 <sup><a href="#2">[2]</a></sup></h2>
<p><strong>命令行参数</strong></p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>helm list</td>
<td>查看发布</td>
</tr>
<tr>
<td>helm remove</td>
<td>删除</td>
</tr>
<tr>
<td>helm repo add xxx url</td>
<td>添加仓库</td>
</tr>
<tr>
<td>helm upgrade</td>
<td>更新</td>
</tr>
<tr>
<td>helm rollback</td>
<td>回滚</td>
</tr>
<tr>
<td>&ndash;generate-name</td>
<td>为部署的应用生成一个随即名</td>
</tr>
<tr>
<td>&ndash;namespace</td>
<td>部署在哪个名称空间</td>
</tr>
<tr>
<td>&ndash;set</td>
<td>覆盖 chart 中的默认 values 值</td>
</tr>
<tr>
<td>inspect</td>
<td>查看存在哪些 values 值</td>
</tr>
<tr>
<td>show</td>
<td>查看你要查看的内容，例如 chart, values等</td>
</tr>
</tbody>
</table>
<h3 id="使用本地-chart-包安装">使用本地 chart 包安装</h3>
<pre><code class="language-bash">$ helm install --generate-name ./

# 或者

$ helm install --generate-name ./charts/grafana-6.56.6.tgz
</code></pre>
<h3 id="查看-chart-中的-values">查看 chart 中的 values</h3>
<p>查看 chart 中的 values，可以查看 tar 归档的 chart</p>
<pre><code class="language-bash">$ helm show values ./charts/grafana-6.56.6.tgz

global:
  # To help compatibility with other charts which use global.imagePullSecrets.
  # Allow either an array of {name: pullSecret} maps (k8s-style), or an array of strings (more common helm-style).
  # Can be tempalted.
  # global:
  #   imagePullSecrets:
  #   - name: pullSecret1
  #   - name: pullSecret2
  # or
  # global:
  #   imagePullSecrets:
  #   - pullSecret1
  #   - pullSecret2
  imagePullSecrets: []

rbac:
  create: true
  ## Use an existing ClusterRole/Role (depending on rbac.namespaced false/true)
  # useExistingRole: name-of-some-(cluster)role
  pspEnabled: false
  pspUseAppArmor: false
  namespaced: false
  extraRoleRules: []
  # - apiGroups: []
  #   resources: []
  #   verbs: []
  extraClusterRoleRules: []
  # - apiGroups: []
  #   resources: []
  #   verbs: []
serviceAccount:
  create: true
  name:
  nameTest:
  ## ServiceAccount labels.
  labels: {}

....
</code></pre>
<p>也可以查看解压后的</p>
<pre><code class="language-bash">$ helm show values ./

# Default values for kube-prometheus-stack.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

## Provide a name in place of kube-prometheus-stack for `app:` labels
##
nameOverride: &quot;&quot;

## Override the deployment namespace
##
namespaceOverride: &quot;&quot;

## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6
##
kubeTargetVersionOverride: &quot;&quot;

## Allow kubeVersion to be overridden while creating the ingress
##
kubeVersionOverride: &quot;&quot;

## Provide a name to substitute for the full names of resources
##
fullnameOverride: &quot;&quot;

## Labels to apply to all resources
##
commonLabels: {}
# scmhash: abc123
# myLabel: aakkmd

## Create default rules for monitoring the cluster
##
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
:

....
</code></pre>
<h3 id="查看-chart-包">查看 chart 包</h3>
<pre><code class="language-bash">$ helm show chart .
annotations:
  artifacthub.io/license: Apache-2.0
  artifacthub.io/links: |
    - name: Chart Source
      url: https://github.com/prometheus-community/helm-charts
    - name: Upstream Project
      url: https://github.com/prometheus-operator/kube-prometheus
  artifacthub.io/operator: &quot;true&quot;
apiVersion: v2
appVersion: v0.65.1
dependencies:
- condition: kubeStateMetrics.enabled
  name: kube-state-metrics
  repository: https://prometheus-community.github.io/helm-charts
  version: 5.6.*
- condition: nodeExporter.enabled
  name: prometheus-node-exporter
  repository: https://prometheus-community.github.io/helm-charts
  version: 4.16.*
- condition: grafana.enabled
  name: grafana
  repository: https://grafana.github.io/helm-charts
  version: 6.56.*
description: kube-prometheus-stack collects Kubernetes manifests, Grafana dashboards,
  and Prometheus rules combined with documentation and scripts to provide easy to
  operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus
  Operator.
home: https://github.com/prometheus-operator/kube-prometheus
icon: https://raw.githubusercontent.com/prometheus/prometheus.github.io/master/assets/prometheus_logo-cb55bb5c346.png
keywords:
- operator
- prometheus
- kube-prometheus
kubeVersion: '&gt;=1.16.0-0'
maintainers:
- email: andrew@quadcorps.co.uk
  name: andrewgkew
- email: gianrubio@gmail.com
  name: gianrubio
- email: github.gkarthiks@gmail.com
  name: gkarthiks
- email: kube-prometheus-stack@sisti.pt
  name: GMartinez-Sisti
- email: scott@r6by.com
  name: scottrigby
- email: miroslav.hadzhiev@gmail.com
  name: Xtigyro
- email: quentin.bisson@gmail.com
  name: QuentinBisson
name: kube-prometheus-stack
sources:
- https://github.com/prometheus-community/helm-charts
- https://github.com/prometheus-operator/kube-prometheus
type: application
version: 46.5.0
</code></pre>
<p>show 查看chart值，可查看在线和离线</p>
<pre><code>$ helm show values stable/jenkins | url | file  # 查看chart包values
</code></pre>
<h3 id="覆盖-chart-默认values值">覆盖 chart 默认values值</h3>
<p>使用 <strong>&ndash;set</strong> 可以覆盖 chart 的默认值，可以指定多个</p>
<pre><code>$ helm install --generate-name ./ \
	--set alertmanager.service.type=NodePort \
	--set prometheus.service.type=NodePort
</code></pre>
<h3 id="更新一个应用">更新一个应用</h3>
<p>可以依据一个已经存在的 Chart 来更新已经部署过的应用</p>
<pre><code class="language-bash">$ helm upgrade chart-1685626375 ./ \
	--set alertmanager.service.type=NodePort \
	--set prometheus.service.type=NodePort
</code></pre>
<p>查看对应 service 配置已经更改</p>
<pre><code class="language-bash">$  kubectl get svc
NAME                                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
chart-1685626375-kube-prom-alertmanager     NodePort    10.96.187.159    &lt;none&gt;        9093:30903/TCP               103m
chart-1685626375-kube-prom-operator         ClusterIP   10.98.6.186      &lt;none&gt;        443/TCP                      103m
chart-1685626375-kube-prom-prometheus       NodePort    10.98.68.1       &lt;none&gt;        9090:30090/TCP               103m
</code></pre>
<h3 id="从仓库下载一个-chart">从仓库下载一个 Chart</h3>
<pre><code class="language-bash">$ helm fetch stable/minio
</code></pre>
<h2 id="chart">Chart</h2>
<p>Charts 是创建在特定目录下面的文件集合，然后可以将它们打包到一个版本化的存档中来部署。接下来我们就来看看使用 Helm 构建 charts 的一些基本方法。</p>
<p><code>Chart.yaml</code>文件是chart必需的。包含了以下字段：</p>
<p><a href="https://github.com/helm/helm/issues/5907" target="_blank"
   rel="noopener nofollow noreferrer" >apiVersion 版本的说明</a></p>
<pre><code class="language-yaml">apiVersion: chart API 版本 （必需）
name: chart名称 （必需）
version: 语义化2 版本（必需）
kubeVersion: 兼容Kubernetes版本的语义化版本（可选）
description: 一句话对这个项目的描述（可选）
type: chart类型 （可选）
keywords:
  - 关于项目的一组关键字（可选）
home: 项目home页面的URL （可选）
sources:
  - 项目源码的URL列表（可选）
dependencies: # chart 必要条件列表 （可选）
  - name: chart名称 (nginx)
    version: chart版本 (&quot;1.2.3&quot;)
    repository: 仓库URL (&quot;https://example.com/charts&quot;) 或别名 (&quot;@repo-name&quot;)
    condition: （可选） 解析为布尔值的yaml路径，用于启用/禁用chart (e.g. subchart1.enabled )
    tags: # （可选）
      - 用于一次启用/禁用 一组chart的tag
    enabled: （可选） 决定是否加载chart的布尔值
    import-values: # （可选）
      - ImportValue 保存源值到导入父键的映射。每项可以是字符串或者一对子/父列表项
    alias: （可选） chart中使用的别名。当你要多次添加相同的chart时会很有用
maintainers: # （可选）
  - name: 维护者名字 （每个维护者都需要）
    email: 维护者邮箱 （每个维护者可选）
    url: 维护者URL （每个维护者可选）
icon: 用做icon的SVG或PNG图片URL （可选）
appVersion: 包含的应用版本（可选）。不需要是语义化的
deprecated: 不被推荐的chart （可选，布尔值）
annotations:
  example: 按名称输入的批注列表 （可选）.
</code></pre>
<p><code>--dry-run</code> 模拟安装</p>
<p><code>--debug</code> 详细的输出</p>
<p><code>--generate-name</code>：生成随机实例名</p>
<pre><code>helm install --generate-name --dry-run --debug --set favoriteDrink=7up ./testchart
</code></pre>
<p>go template</p>
<p><code>{{ .Values.favoriteDrink }}</code> 读取变量值</p>
<p><code>quote</code> go template 函数，<a href="https://helm.sh/docs/chart_template_guide/functions_and_pipelines/" target="_blank"
   rel="noopener nofollow noreferrer" >引用字符串</a> ，给变量值加<code>&quot;&quot;</code></p>
<p><code>pipeline</code> 可将多个功能连接在一起</p>
<p>默认值 <code>chart.yaml</code>  <code>gender: {{ .values.gender|default &quot;zhangsan&quot; }}</code> values.yaml 中不能加default函数</p>
<p>流程控制</p>
<p><a href="https://helm.sh/docs/chart_template_guide/control_structures/" target="_blank"
   rel="noopener nofollow noreferrer" >if else</a></p>
<pre><code>{{ if PIPELINE }}
  # Do something
{{ else if OTHER PIPELINE }}
  # Do something else
{{ else }}
  # Default case
{{ end }}
</code></pre>
<p>如果 pipeline的值被判定为如下的值则为false：</p>
<ul>
<li>a boolean false</li>
<li>a numeric zero</li>
<li>an empty string</li>
<li>a <code>nil</code> (empty or null)</li>
<li>an empty collection (<code>map</code>, <code>slice</code>, <code>tuple</code>, <code>dict</code>, <code>array</code>)</li>
</ul>
<p>使用 <code>-</code> 摆脱新行<code>{{- if eq .Values.favorite.drink &quot;coffee&quot; }}</code> <code>-</code> 在前面表示删除前面的空行，在后面表示删除后面的空行，<code>{{-</code> 之间没有空格</p>
<p><code>{{ indent 2 &quot;mug:true&quot; }}</code> 缩进，缩进的是文字内容不是yaml</p>
<p><code>{{with .Values.xxx}} {{end}}</code> 提升作用于，<code>release: {{ $.Release.Name }}</code> 执行时将变量映射到根域，可以在作用域中使用</p>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://learnk8s.io/kubernetes-custom-authentication" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Implementing a custom Kubernetes authentication method</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://helm.sh/zh/docs/helm/helm/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Helm 命令行</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#users-in-kubernetes" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Users in Kubernetes</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#bootstrap-tokens" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>bootstrap tokens</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Webhook Token Authentication</strong></em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes应用 - Traefik Ingress Controller</title>
      <link>https://www.oomkill.com/2019/10/traefik-ingresscontroller/</link>
      <pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/10/traefik-ingresscontroller/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="kubernetes-ingress">Kubernetes Ingress</h2>
<p>Kubernetes Ingress是路由规则的集合，这些规则控制外部用户如何访问Kubernetes集群中运行的服务。</p>
<p>在Kubernetes中，有三种方式可以使内部Pod公开访问。</p>
<ul>
<li>NodePort：使用Kubernetes Pod的<code>NodePort</code>，将Pod内应用程序公开到每个节点上的端口上。</li>
<li>Service LoadBalancer：使用Kubernetes Service，改功能会创建一个外部负载均衡器，使流量转向集群中的Kubernetes Pod。</li>
<li>Ingress Controller：</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/d4c3ff03d767b1495101096ae3cdd76cd6246b99-661x418.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>Node Port是在Kubernetes集群中每个节点（Node）上开放端口，Kubernetes直接将流量转向集群中Pod。Kubernetes集群中使用NodePort，则需要编辑防火墙规则，但是NodePort是范围在Kubernetes集群中默认设置的范围为 30000–32767，最终导致流量端口暴露在非标准端口之上。</p>
<p>LoadBalancer一般应用于云厂商提供的Kubernetes服务，如果自行在机器上部署Kubernetes集群，则需要自行配置LoadBalancer的实现，</p>
<p>Kubernetes Ingress，为Kubernetes中的抽象概念，实现为第三方代理实现，这种三方实现集合统称为Ingress Controller。Ingress Controller负责引入外部流量并将流量处理并转向对应的服务。</p>
<h2 id="kubernetes-ingresscontroller功能实现">Kubernetes IngressController功能实现</h2>
<p>上面只是说道，在Kubernetes集群中，如何将外部流量引入到Kubernetes集群服务中。</p>
<h3 id="负载均衡">负载均衡</h3>
<p>无论在Kubernetes集群中，无论采用什么方式进行流量引入，都需要在外部负载均衡完成，而后负载均衡将流量引入Kubernetes集群入口或内部中，</p>
<p>通常情况下，NodePort方式管理繁琐，一般不用于生产环境。</p>
<h3 id="服务的ingress选择">服务的Ingress选择</h3>
<p>Kubernetes Ingress是选择正确的方法来管理引入外部流量到服务内部。一般选择也是具有多样性的。</p>
<ul>
<li>Nginx Ingress Controller，Kubernetes默认推荐的Ingress，弊端①最终配置加载依赖<code>config reload</code>，②定制化开发较难，配置基本来源于config file。</li>
<li>Envoy &amp; traefik api网关，支持tcp/udp/grpc/ws等多协议，支持流量控制，可观测性，多配置提供者。</li>
<li>云厂商提供的Ingress。AWS ALB，GCP GLBG/GCE，Azure AGIC</li>
</ul>
<h2 id="traefik介绍">Traefik介绍</h2>
<p>traefik-现代反向代理，也可称为现代边缘路由；traefik原声兼容主流集群，Kubernetes，Docker，AWS等。官方的定位traefik是一个让开发人员将时间花费在系统研发与部署功能上，而非配置和维护。并且traefik官方也提供自己的服务网格解决方案</p>
<p>作为一个 modern edge router ，traefik拥有与envoy相似的特性</p>
<ul>
<li>基于go语言研发，目的是为了简化开发人员的配置和维护</li>
<li>tcp/udp支持</li>
<li>http L7支持</li>
<li>GRPC支持</li>
<li>服务发现和动态配置</li>
<li>front/ edge prory支持</li>
<li>可观测性</li>
<li>流量管理</li>
<li>&hellip;</li>
</ul>
<h3 id="traefik-术语">traefik 术语</h3>
<p>要了解trafik，首先需要先了解一下 有关trafik中的一些术语。</p>
<ul>
<li>EntryPoints 入口点，是可以被下游客户端连接的命名网络位置，类似于envoy 的listener和nginx的listen</li>
<li>services 服务，负载均衡，上游主机接收来自traefik的连接和请求并返回响应。 类似于nginx upstream envoy的clusters</li>
<li>Providers 提供者，提供配置文件的后端，如file，kubernetes，consul，redis，etcd等，可使traefik自动更新</li>
<li>routers 路由器，承上启下，分析请求，将下游主机的请求处理转入到services</li>
<li>middlewares: 中间件，在将下游主机的请求转入到services时进行的流量调整</li>
</ul>
<h2 id="在kubernetes中使用traefik网关作为ingress">在Kubernetes中使用traefik网关作为Ingress</h2>
<p>Traefik于2019年9月发布2.0 GA版，增加了很多新特性，包括IngressRoute Kubernetes CRD，TCP，最新版增加UDP等。</p>
<h3 id="安装traefik">安装traefik</h3>
<p>Traefik 支持两种方式创建路由规则，一是Traefik 自定义 <em>Kubernetes CRD</em> ，还有一种是 <em>Kubernetes Ingress</em>。这里使用 Kubernetes CRD</p>
<p>官方完整的部署清单资源见附录3 <sup><a href="#3">[3]</a></sup></p>
<h3 id="创建traefik使用的kubernetes-crd-资源">创建traefik使用的Kubernetes CRD 资源</h3>
<p>traefik官网提供了创建时所需要的 <a href="https://doc.traefik.io/traefik/reference/dynamic-configuration/kubernetes-crd/" target="_blank"
   rel="noopener nofollow noreferrer" >yaml</a>文件，这里仅需要使用官网提供的Definitions与RBAC即可。</p>
<p>在官网提供的yaml文件缺少 ServiceAccount，需要自行创建。</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: default
  name: traefik-ingress-controller
</code></pre>
<h3 id="创建控制器">创建控制器</h3>
<p>官方文件中，暂未找到所需运行traefik的控制器，需要自己创建一个。</p>
<p>traefik配置一般分为静态和动态配置，此处的静态是指，大部分时间内，不改变的配置（如nginx.conf），动态配置指，经常情况下改变的配置（可以理解为 nginx中 virtual host的每个独立配置文件）。</p>
<p>traefik提供配置的提供者也有很多种，此处使用命令行方式设置不长改变静态配置，也可以使用配置文件方式进行配置提供。</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: traefik
spec:
  selector:
    app: traefik-ingress
  type: NodePort
  ports:
    - name: web
      port: 80
      targetPort: 80
    - name: ssl
      port: 443
      targetPort: 443
    - name: dashboard
      port: 8080
      targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: traefik-ingress-controller-deployment
  labels:
    app: traefik-ingress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: traefik-ingress
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      name: traefik-ingress
      labels:
        app: traefik-ingress
    spec:
      serviceAccountName: traefik-ingress-controller
      volumes:
        - name: ssl
          hostPath:
            path: /etc/kubernetes/pki
            type: DirectoryOrCreate
      containers:
        - image: traefik:v2.3.3
          name: traefik-ingress-lb
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: ssl
            mountPath: /usr/local/pki
          ports:
            - name: web
              containerPort: 80
              hostPort: 1880
            - name: ssl
              containerPort: 443
              hostPort: 18443
          securityContext:
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
          args:
            - --entrypoints.web.address=:80
            - --entrypoints.ssl.address=:443
            - --providers.kubernetescrd
            - --providers.kubernetesingress
            - --api=true
            - --ping=true
            - --api.dashboard=true
            - --serverstransport.insecureskipverify=true
            - --serverstransport.rootcas=/usr/local/pki/ca.crt
            - --accesslog
          livenessProbe:
            httpGet:
              path: /ping
              port: 8080
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /ping
              port: 8080
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
</code></pre>
<p>配置文件方式，仅需参考官网，把&ndash;agrs中的参数转换为配置文件并引入到pod后。替换启动参数</p>
<pre><code class="language-yaml">args:
- --configfile=/config/traefik.yaml
</code></pre>
<h2 id="使用crd配置traefik的流量管理">使用CRD配置Traefik的流量管理</h2>
<p>官网提供的基于Kubernetes CRD方式配置不是很多，可以参考动态配置中Kubernetes CRD Resources小结。</p>
<h3 id="基于traefik-dashboard方式配置增加http-router">基于traefik dashboard方式配置增加HTTP router</h3>
<p>前面使用官方提供CRD文件注册了Kubernetes CRD资源，所以traefik 中的资源类型，可作为kubernetes中资源使用。如 <code>kubectl get Middleware </code></p>
<p>dashboard是traefik自己提供的服务所需的services为自己，traefik的entryPoints，当开启<code>--api.dashboard=true</code> 会增加一个8080端口作为traefik dashboard使用。而  <code>api@internal</code> 是traefik自己提供的services资源。</p>
<pre><code class="language-yaml">apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: traefik-dashboard-route
spec:
  entryPoints:
    - traefik
  routes:
    - match: Host(`10.0.0.5`)
      kind: Rule
      services:
        - name: api@internal
          port: 8080
          kind: TraefikService
</code></pre>
<h3 id="为kubernetes-dashboard增加http路由">为Kubernetes dashboard增加HTTP路由</h3>
<p>基于kubernetes crd作为提供者运行的traefik中，后端service可以是traefik的services也可以是kubernetes资源中的service。</p>
<p>基于kubernetes sevices</p>
<pre><code class="language-yaml">apiVersion: traefik.containo.us/v1alpha1
kind: IngressRoute
metadata:
  name: kubernetes-dashboard-route
  namespace: kubernetes-dashboard
  annotations:
    traefik.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
spec:
  entryPoints:
    - ssl
  tls:
    secretName: k8s-ca

  routes:
    - match: PathPrefix(`/ui`)
      kind: Rule
      middlewares:
        - name: strip-ui
      services:
        - name: kubernetes-dashboard # kubernetes中的service对应的名称
          kind: Service # kubernetes中的service
          port: 443
          namespace: kubernetes-dashboard # kubernetes中的service对应的名称空间。
---
apiVersion: traefik.containo.us/v1alpha1
kind: Middleware
metadata:
  name: strip-ui
  namespace: kubernetes-dashboard
spec:
  stripPrefix:
    prefixes:
      - &quot;/ui&quot;
      - &quot;/ui/&quot;
</code></pre>
<p>此处使用了TLS的，开启了TLS，默认traefik是进行双向认证的，而kubernetes的dashboard的证书的ca并不知道，在访问时会出现<code>Internal Server Error</code>，目前没有找到有效的双向认证方法，普遍使用的方法都是调过认证<code>--serverstransport.insecureskipverify=true</code></p>
<h3 id="负载均衡-1">负载均衡</h3>
<p>负载均衡使用到的是traefik的services部分。</p>
<pre><code class="language-yaml">apiVersion: traefik.containo.us/v1alpha1
kind: TraefikService
metadata:
  name: LoadBalancer
spec:
  weighted:
    services:
    - name: prod-v1.2
      port: 443
      weight: 1
      kind: Service
    - name: prod-v1
      port: 443
      weight: 2
      kind: Service
---
</code></pre>
<h3 id="流量镜像">流量镜像</h3>
<pre><code class="language-yaml">apiVersion: traefik.containo.us/v1alpha1
kind: TraefikService
metadata:
  name: mirror1
spec:
  mirroring:
    name: s1
    port: 80
    mirrors:
      - name: s3
        percent: 20
        port: 80
      - name: mirror2
        kind: TraefikService
        percent: 20
</code></pre>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://stackoverflow.com/questions/49412376/internal-server-error-with-traefik-https-backend-on-port-443" target="_blank"
   rel="noopener nofollow noreferrer" ><strong>Internal Server Error with Traefik HTTPS backend on port 443</strong></a></p>
<p><sup id="2">[2]</sup> <a href="https://github.com/traefik/traefik/issues/6821" target="_blank"
   rel="noopener nofollow noreferrer" ><strong>IngressRoute subset not found for kube-system/kubernetes-dashboard #6821</strong></a></p>
<p><sup id="3">[3]</sup> <a href="https://doc.traefik.io/traefik/user-guides/crd-acme/#traefik-crd-lets-encrypt" target="_blank"
   rel="noopener nofollow noreferrer" ><strong>Traefik &amp; CRD &amp; Let&rsquo;s Encrypt</strong></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用二进制文件构建k8s集群</title>
      <link>https://www.oomkill.com/2019/01/kubernetes-install-with-binary-files/</link>
      <pubDate>Sun, 20 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/01/kubernetes-install-with-binary-files/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="kubernetes集群的构成">Kubernetes集群的构成</h2>
<h3 id="master-node-control-plane">Master Node (Control plane)</h3>
<p>Master 是整个 Kubernetes 集群构成的基础，它负责整个集群的管理，例如处理集群的状态；组件包含 API Server, Controller manager, Scheduller, Etcd</p>
<h4 id="api-server">API server</h4>
<p>API 服务器是 Master 的统一前端入口，负责集群内其他组件的 协调 与 通信。该组件用于定义集群的状态。可以通过命令行, HTTP API, 第三方托管平台（dashboard, Rancker, Kuboard等）与 Kubernetes API 进行交互。</p>
<h4 id="scheduler">Scheduler</h4>
<p>调度程序 Scheduler 负责根据可用资源来决定如何去部署容器，部署到哪里？确保所有 Pod（容器组）都分配给某一组节点。</p>
<h4 id="controller-manager">Controller Manager</h4>
<p>Controller manager，又分为Controller 和 Manager，Controller的组要作用是用于协调各种控制器(Deployment, Daemonset&hellip;)，这些控制器可确保在节点发生故障时采取适当的措施。而 Manager 则管理的众多Controller；更一般地说，CM 负责随时将集群的当前状态调整到所需状态（Kubernetes设计基石）。</p>
<h4 id="etcd">etcd</h4>
<p>etcd 是控制平面内的一个组件，他提供了 Kubernetes 资源的存储，并为集群内组件提供了 Watch 的功能，这将意味着，etcd 在 kubernetes 集群中作为存储与分布式协调的功能。</p>
<h3 id="worker-nodes">Worker nodes</h3>
<p>每个集群中至少需要存在一个工作节点，但是通常会有大量的节点；而工作节点包括的组件不限于 Kubelet, Kube-proxy, CNI Plugin。</p>
<h4 id="kubelet">Kubelet</h4>
<p>kubelet是工作节点中管理运行时的组件，负责整个Pod （容器组）进程的生命周期</p>
<h4 id="kube-proxy">Kube-proxy</h4>
<p>Kube-proxy 为整个集群内提供了 service 的功能，如果这个组件无法正常工作，那么整个集群内的网络通信将不能正常，因为 service 是作为集群内服务的访问入口，包含 Kubernetes API service。</p>
<h4 id="cni">CNI</h4>
<p>CNI (<em><strong>Container Network Interface</strong></em>) 是 Kubernetes 集群中提供跨节点通信的一个组件，通常来说，它是一个独立于容器运行时（Docker等）的网络插件规范，旨在实现容器之间和容器与宿主机之间的网络连接。</p>
<p>一个 CNI 基本的功能就是去管理网络设备的生命周期，例如，生成网卡，添加IP地址，注销网卡，而构成这些网络功能的则有操作系统来提供的，例如网络隧道(VxLAN)，而还存在一种情况就是三层网络，这时CNI会作为一个路由器来分发路由。除此之外，CNI还提供了更多的功能，例如数据包加密，网络策略，服务网格，网络加速，IPAM等功能</p>
<h2 id="通过二进制安装kubernetes-cluster">通过二进制安装Kubernetes cluster</h2>
<h3 id="硬件配置推荐">硬件配置推荐</h3>
<p>在选择节点数量时，需要考虑集群的用途，通常情况下建议至少使用 <strong>3 个节点</strong> (APIServer)，对于控制平面其他组件来说，通常最少为两个即可，因为HA架构中工作节点总是需要一个即可</p>
<p>要运行 apiserver 和 etcd，您需要一台具有 <strong>2 个内核和 2GB RAM 的</strong> 机器，用于中小型集群，更大规模集群可能需要更多的核心。工作节点必须有足够的资源来托管您的应用程序，并且可以有不同的配置。</p>
<h3 id="etcd-硬件配置推荐">etcd 硬件配置推荐</h3>
<p>Kuberentes 中工作节点需要启动一个或多个 etcd 实例，通常官方推荐运行奇数个 etcd 实例，因为一个 3 实例时有两个活跃就具备了集群的 quorum ，同理五个实例时存在3个实例活跃即可，7=&gt;4 等，通常集群规模不大于9，否则存在同步时的延迟。</p>
<h3 id="集群的部署模式">集群的部署模式</h3>
<ul>
<li>单独托管的 etcd 集群</li>
<li>master节点同台主机上托管 etcd 集群</li>
<li>通过 kubeadm 生成的 etcd 集群</li>
</ul>
<p>在这里我们使用二进制方式最小化部署，即 单节点的 etcd 集群，与同时为 control plane 与 worker 一体托管在单台物理/虚拟机之上的 Kubernetes 集群</p>
<blockquote>
<p>题外话：kubeadm 和 二进制究竟有什么区别？</p>
<p>实际上 kubeadm 和 二进制本质上并没有什么区别，如果非要说存在区别的话，那么就是其 Procss Manager 不同，</p>
<ul>
<li>一个是由 systemd/system v 或其他操作系统维护的1 id的进程进行维护；</li>
<li>kubeadm 部署的 kubelet 将 由 <code>kubelet</code> 进程自己来监控，当 <code>pod </code>崩溃时重启该 <code>pod</code>，<code>kubelete</code> 也无法对他们进行健康检查。静态 pod 始终绑定在某一个 <code>kubelet</code>，并且始终运行在同一个节点上，可以通过在 master 节点上查看 <code>/etc/kubernetes/manifests</code> 目录</li>
</ul>
</blockquote>
<h2 id="配置证书">配置证书</h2>
<p>通常情况下，大家都知道安装 kubernetes 集群需要证书，但是不知道为什么需要证书，这里需要了解 Kubernetes 认证机制，也就是“用户”在kubernetes集群中被称为什么</p>
<p>Kubernetes 中用户被分为几类：</p>
<ul>
<li>X.509 证书</li>
<li>静态 Token 文件</li>
<li>Bootstrap Tokens</li>
<li>Service Account Tokens</li>
</ul>
<p>kubernetes 使用了客户端证书方式进行认证，这是作为外部用户的一种方式，这些证书会被默认生成对应的内部用户，所以需要准备证书文件，在本文中一切准备文件都是由脚本生成，不做基础的配置讲解。</p>
<p>对于 X.509 证书，需要注意有几点</p>
<ul>
<li>kubernetes 中，对于证书的用户，CN就是用户名，O 就是组织。</li>
<li>对于证书认证来说，客户端证书，也就是说客户端必须在可信任列表中，也就是 subject_name 中允许的</li>
<li>对于 Kubernetes 组件来说，通常情况下需要包含 Kubernetes API service 的所有名称（短域名+完整域名）
<ul>
<li>kubernetes</li>
<li>kubernetes.default</li>
<li>kubernetes.default.svc</li>
<li>kubernetes.default.svc.cluster</li>
<li>kubernetes.default.svc.cluster.local</li>
</ul>
</li>
</ul>
<p>如果你希望使用IP，而不是域名，也可以对证书中sub_name增加对应域名</p>
<p>对于组件间认证，目前 Kubernetes 中基本上组件间认证都有 kubeconfig 完成，而 kubelet，kube-controller-manager 这两个组件，由于提供的功能不同，可能存在其他的认证方式；例如 kubelet 所作的事情是 “监听Pod资源进行部署” 那么这个时候与 APIServer 通讯使用了 kubeconfig，在例如 kubelet 要被 APIServer 签发时，此时认证是使用的 bootstrap token 进行的，而这个kubeconfig 就是 签发后生成的内容，本质上来说，kubelet 没有客户端证书，只有token，而例如 kube-scheduler 是有客户端证书，但是需要生产 kubeconfig 文件</p>
<h2 id="签发kubelet">签发kubelet</h2>
<p>给 kubelet 签发证书主要由两部分组成，一种是 kube-controller-manager 自动签发，一种是 <code>kubectl certifcate approve</code> 手动签发，这里就有必要知道 kubelet 的认证的流程：</p>
<ul>
<li>kubelet启动时会找 kubeconfig ，如果没有进入下一部</li>
<li>没有 kubeconfig，会使用 bootstraps 进行认证，此时会被颁发客户端证书</li>
<li>在通过后，kubelet 会根据签发证书 生成 &ndash;kubeconfig 指定的 kubeconfig 文件</li>
<li>下次后，kubelet 会根据这个 kubeconfig 同 Kubernetes API 进行 验证</li>
</ul>
<p>了解了 kubelet 的签发过程，就明白在二进制部署时，为什么需要做一个 clusterrolebinding ？</p>
<p>因为Kubernetes API 为 kubelet 的 bootstraps-token 使用的是用户 <code>system:node-bootstrapper</code> ，所以要想让这个用户可以访问 API 资源，那么就需要为这个用户绑定上集群角色（用户/组），这就需要执行一个命令</p>
<pre><code class="language-bash">$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
</code></pre>
<p>此时再去查看证书颁发，这时因为有权限访问 API 资源了， 所以获得了证书的被签发</p>
<pre><code class="language-bash">$ kubectl get  csr
NAME                                                   AGE     REQUESTOR             CONDITION
node-csr-a-MREQ1IybB0U5M8RP5FasSjckQOZiCoCYlf8ipDwx8   5m11s   system:bootstrapper   Pending
</code></pre>
<h2 id="kube-dns-的部署">kube-dns 的部署</h2>
<p>coredns部署</p>
<pre><code class="language-sh">$ mkdir coredns &amp;&amp; cd coredns
$ wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
$ wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.sh
$ bash deploy.sh -i 10.96.0.10 -r &quot;10.96.0.0/12&quot; -s -t coredns.yaml.sed |kubectl apply -f - 
</code></pre>
<h2 id="开启ipvs">开启IPVS</h2>
<p>内核开启IPVS功能否则降级</p>
<pre><code class="language-sh">cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt; EOF
#!/bin/bash
ipvs_mods_dir=&quot;/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs&quot;
for i in \$(ls \$ipvs_mods_dir | grep -o &quot;^[^.]*&quot;); do
    /sbin/modinfo -F filename \$i &gt;/dev/null 
    if [ \$? -eq 0 ]; then
    /sbin/modprobe -- \$i 
    fi 
done
EOF
</code></pre>
<p>or</p>
<pre><code class="language-yaml">    cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF
    #!/bin/bash
    ipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4&quot;
    for kernel_module in \${ipvs_modules}; do
        /sbin/modinfo -F filename \${kernel_module} &gt; /dev/null 2&gt;&amp;1
        if [ $? -eq 0 ]; then
            /sbin/modprobe \${kernel_module}
        fi
    done
    EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs
</code></pre>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="apiserver报错-没有kubeappserver用户">apiserver报错 没有kubeappserver用户</h3>
<pre><code>Failed at step USER spawning  No such process
</code></pre>
<pre><code class="language-log">Mar  2 04:54:56 node02 systemd: controller-manager.service: main process exited, code=exited, status=1/FAILURE
Mar  2 04:54:56 node02 kube-controller-manager: Get http://127.0.0.1:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication: net/http: HTTP/1.x transport connection broken: malformed HTTP response &quot;\x15\x03\x01\x00\x02\x02&quot;
Mar  2 04:54:56 node02 systemd: Unit controller-manager.service entered failed state.
</code></pre>
<p><a href="https://blog.csdn.net/kevin_loving/article/details/81326470?utm_source=blogxgwz9" target="_blank"
   rel="noopener nofollow noreferrer" >Unable to connect to the server: net/http: HTTP/1.x transport connection broken: malformed HTTP resp - kevin_loving的博客 - CSDN博客</a></p>
<p>可能是启动单元文件有问题，手动启动后正常</p>
<pre><code>Mar  2 09:44:51 node02 kube-controller-manager: W0302 09:44:51.672404   39926 client_config.go:554] error creating inClusterConfig, falling 
back to default config: unable to load in-cluster configuration, KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined
</code></pre>
<h3 id="node-xxxx-not-found">node &ldquo;xxxx&rdquo; not found</h3>
<p>如下面问题，可能原因为 kubelet 正式还未签发</p>
<pre><code>Nov 14 16:31:03 master01 kubelet: E1114 16:31:03.677280    8587 kubelet.go:2270] node &quot;master01&quot; not found

380   19810 kubelet.go:2292] node &quot;master-machine&quot; not found
May 12 23:45:11 master-machine kubelet: E0512 23:45:11.415099   19810 kubelet.go:2292] node &quot;master-machine&quot; not found
May 12 23:45:11 master-machine kubelet: E0512 23:45:11.460097   19810 certificate_manager.go:434] Failed while requesting a signed certificate from the master: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &quot;system:anonymous&quot; cannot create resource &quot;certificatesigningrequests&quot; in API group &quot;certificates.k8s.io&quot; at the cluster scope
</code></pre>
<h3 id="user-systemanonymous-cannot-list-resource-xxx">User &ldquo;system:anonymous&rdquo; cannot list resource xxx</h3>
<p>原因为：kubelet 正式还未签发</p>
<pre><code>k8s.io/client-go/informers/factory.go:135: Failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User &quot;system:anonymous&quot; cannot list resource &quot;csidrivers&quot; in API group &quot;storage.k8s.io&quot; at the cluster scope
</code></pre>
<blockquote>
<p>问题：<code>&quot;master01&quot; is forbidden: User &quot;system:anonymous&quot; </code></p>
<p>原因：用户未授权</p>
</blockquote>
<p>需要注意的是，这里使用的是 bootstrap 证书进行授权，所以绑定的用户必须为 bootstrap 证书授权的用户或组。</p>
<pre><code class="language-bash">$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
</code></pre>
<blockquote>
<p>The kube-apiserver has several requirements to enable TLS bootstrapping: Authenticating the bootstrapping kubelet to the <code>system:bootstrappers</code> group <sup><a href="#2">[2]</a></sup></p>
</blockquote>
<pre><code class="language-yaml">root@debian-template:~# kubectl get clusterrole system:node-bootstrapper -oyaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;
  creationTimestamp: &quot;2024-11-23T04:26:57Z&quot;
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:node-bootstrapper
  resourceVersion: &quot;54&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Anode-bootstrapper
  uid: 6f39c3aa-e46d-413f-8834-35c8832c328a
rules:
- apiGroups:
  - certificates.k8s.io
  resources:
  - certificatesigningrequests
  verbs:
  - create
  - get
  - list
  - watch

</code></pre>
<p>报错如下</p>
<pre><code>master01 kubelet: E1114 17:16:33.581116    8559 kubelet.go:2270] node &quot;master01&quot; not found

failed to ensure node lease exists, will retry in 6.4s, error: leases.coordination.k8s.io &quot;master01&quot; is forbidden: User &quot;system:anonymous&quot; cannot get resource &quot;leases&quot; in API group &quot;coordination.k8s.io&quot; in the namespace &quot;kube-node-lease&quot;
</code></pre>
<h3 id="容器内-dial-tcp-109601443-connect-connection-timed-out">容器内 dial tcp 10.96.0.1:443: connect: connection timed out</h3>
<blockquote>
<p>问题1： flannela访问 <code>dial tcp 10.96.0.1:443: connect: connection timed out</code></p>
<p>问题2：<code>open /run/flannel/subnet.env: no such file or directory</code></p>
<p>问题3： <code>Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</code></p>
<p>解决：检查kube-proxy组件</p>
</blockquote>
<pre><code>Nov 14 17:30:04 master01 kubelet: E1114 17:30:04.097261    1160 kubelet.go:2190] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
</code></pre>
<pre><code>$ kubectl logs kube-flannel-ds-dvsv7 -n kube-system
....
E1114 23:22:20.984238       1 main.go:243] Failed to create SubnetManager: error retrieving pod spec for 'kube-system/kube-flannel-ds-dvsv7': Get &quot;https://10.96.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-dvsv7&quot;: dial tcp 10.96.0.1:443: connect: connection timed out
$ 
</code></pre>
<h3 id="kubectl-get-csr-显示no-resources-found的解决记录">kubectl get csr 显示No Resources Found的解决记录</h3>
<blockquote>
<p>问题：kubectl get csr 显示No Resources Found的解决记录</p>
<p>解决：<font size=2>需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求</font></p>
</blockquote>
<h3 id="授予kube-apiserver对kubelet-api的访问权限">授予KUBE-APISERVER对KUBELET API的访问权限</h3>
<p>kubelet启动启动时，<code>--kubeletconfig</code> 使用参数对应的文件是否存在 如果不存在<code>--bootstrap-kubeconfig</code>指定的kubeconfig文件向kube-apiserver发送CSR请求</p>
<p>kube-apiserver 收到 CSR 请求后，对其中的 token 进行认证，认证通过后将请求的用户设置为<code>system:bootstrap:&lt;Token ID&gt;</code>，组设置为 ，<code>system:bootstrappers</code>此操作称为<code>Bootstrap Token Auth</code>。</p>
<p>默认这个用户和组没有创建CSR的权限，kubelet没有启动，错误日志如下：</p>
<pre><code>Unable to register node &quot;master-machine&quot; with API server: nodes is forbidden: User &quot;system:anonymous&quot; cannot create resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope
</code></pre>
<p>解决方法是：创建一个集群角色绑定，绑定一个组：<code>system:bootstrapper</code> 和一个clusterrole <code>system:node-bootstrapper</code></p>
<pre><code>$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
</code></pre>
<p>测试授权</p>
<pre><code class="language-bash">kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers

system:serviceaccount:default:default

kubectl create clusterrolebinding firewalld-default --clusterrole=system:aggregate-to-admin --user=system:serviceaccount:default:default

system:aggregate-to-admin
</code></pre>
<h3 id="kube-flannel-cant-get-cidr-although-podcidr-available-on-node">kube flannel cant get cidr although podcidr available on node</h3>
<pre><code>E0729 06:56:09.253632       1 main.go:330] Error registering network: failed to acquire lease: node &quot;master-machine&quot; pod cidr not assigned
I0729 06:56:09.253682       1 main.go:447] Stopping shutdownHandler...
W0729 06:56:09.253809       1 reflector.go:436] github.com/flannel-io/flannel/subnet/kube/kube.go:403: watch of *v1.Node ended with: an error on the server (&quot;unable to decode an event from the watch stream: context canceled&quot;) has prevented the request from succeeding
</code></pre>
<p>参考：<a href="https://stackoverflow.com/questions/50833616/kube-flannel-cant-get-cidr-although-podcidr-available-on-node" target="_blank"
   rel="noopener nofollow noreferrer" >kube flannel cant get cidr although podcidr available on node</a> 原因为 CIDR无法分配</p>
<h3 id="network-plugin-is-not-ready-cni-config-uninitialized">network plugin is not ready: cni config uninitialized</h3>
<pre><code class="language-bash">Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 
</code></pre>
<p>没有安装网络插件，安装任意网络插件后恢复。</p>
<h3 id="mvcc-required-revision-has-been-compacted">mvcc: required revision has been compacted</h3>
<pre><code class="language-log">kube-apiserver: W1120 17:18:20.199950   70454 watcher.go:207] watch chan error: etcdserver: mvcc: required revision has been compacted
</code></pre>
<p>这里是etcd返回的错误，被apiserver视为警告，在注释中有这么一句话</p>
<blockquote>
<p>If the context is &ldquo;context.Background/TODO&rdquo;, returned &ldquo;WatchChan&rdquo; will not be closed and block until event is triggered, except when server returns a non-recoverable error (e.g. ErrCompacted).</p>
<p>如果这个上下文返回WatchChan将在下次事件被触发前不会被关闭或阻塞，除非服务器返回一个ErrCompacted （不可恢复）</p>
</blockquote>
<p>对于etcd 对 Revision 有如下说明</p>
<p>etcd对每个kv的revision 都会保留一个压缩周期的值，例如每5分钟收集一次最新revision，当压缩周期达到时，将从历史记录中后去最后一个修订版本，例如为100，此时会压缩，压缩成功会重置计数器，并以最新的revision和新的历史记录进行开始，压缩失败将在5分钟后重试<code>--auto-compaction-retention=10</code> 是配置压缩周期的  每多少个小时键值存储运行定期压缩。</p>
<p>如果最新的revision已被修订，etcd返回一个 <code>ErrCompacted</code>  表示已修订，此时表示为不可恢复状态</p>
<p>返回错误时表示这个watch被关闭，为了优雅的关闭chan，kubernetes会对这个watch错误进行返回，而 <code>ErrCompacted</code>  本质上不算错误</p>
<pre><code class="language-go">wch := wc.watcher.client.Watch(wc.ctx, wc.key, opts...)
for wres := range wch {
    if wres.Err() != nil {
        err := wres.Err()
        // If there is an error on server (e.g. compaction), the channel will return it before closed.
        logWatchChannelErr(err)
        wc.sendError(err)
        return
    }
    ...
</code></pre>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://etcd.io/docs/v3.5/op-guide/maintenance/#auto-compaction" target="_blank"
   rel="noopener nofollow noreferrer" >Maintenance</a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/#kube-apiserver-configuration" target="_blank"
   rel="noopener nofollow noreferrer" >kubelet-tls-bootstrapping</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>etcd二进制安装与配置</title>
      <link>https://www.oomkill.com/2018/11/etcd-install-bin/</link>
      <pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/11/etcd-install-bin/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="概述">概述</h2>
<p>etcd 是兼具一致性和高可用性的键值数据库，为云原生架构中重要的基础组件，由<code>CNCF </code>孵化托管。etcd 在微服务和 Kubernates 集群中不仅可以作为服务注册与发现，还可以作为 key-value 存储的中间件。</p>
<h3 id="先决条件">先决条件</h3>
<ul>
<li>运行的 etcd 集群个数成员为奇数。</li>
<li>etcd 是一个 leader-based 分布式系统。确保主节点定期向所有从节点发送心跳，以保持集群稳定。</li>
<li>保持稳定的 etcd 集群对 Kubernetes 集群的稳定性至关重要。因此，请在专用机器或隔离环境上运行 etcd 集群，以满足所需资源需求]。</li>
<li>确保不发生资源不足。<br>集群的性能和稳定性对网络和磁盘 IO 非常敏感。任何资源匮乏都会导致心跳超时，从而导致集群的不稳定。不稳定的情况表明没有选出任何主节点。在这种情况下，集群不能对其当前状态进行任何更改，这意味着不能调度新的 pod。</li>
</ul>
<h2 id="相关术语">相关术语</h2>
<ul>
<li><code>Raft</code>：etcd所采用的保证分布式系统强一致性的算法。</li>
<li><code>Node</code>：节点 ，Raft状态机的一个实例，具有唯一标识。</li>
<li><code>Member</code>：  成员，一个etcd实例。承载一个Node，且可为客户端请求提供服务。</li>
<li><code>Cluster</code>：集群，由多个Member构成可以协同工作的etcd集群。</li>
<li><code>Peer</code>：同伴，<code>Cluster</code>中其他成员。</li>
<li><code>Proposal </code>：提议，一个需要完成 raft 协议的请求(例如写请求，配置修改请求)。</li>
<li><code>Client</code>： 向etcd集群发送HTTP请求的客户端。</li>
<li><code>WAL</code>：预写式日志，etcd用于持久化存储的日志格式。</li>
<li><code>snapshot</code>：etcd防止WAL文件过多而设置的快照，存储etcd数据状态。</li>
<li><code>Proxy</code>：etcd的一种模式，为etcd集群提供反向代理服务。</li>
<li><code>Leader</code>：Raft算法中通过竞选而产生的处理所有数据提交的节点。</li>
<li><code>Follower</code>：竞选失败的节点作为Raft中的从属节点，为算法提供强一致性保证。</li>
<li><code>Candidate</code>：当Follower超过一定时间接收不到Leader的心跳时转变为Candidate开始竞选。</li>
<li><code>Term</code>：某个节点成为Leader到下一次竞选时间，称为Ubuntu一个Term。</li>
<li><code>Index</code>：数据项编号。Raft中通过Term和Index来定位数据。</li>
</ul>
<h2 id="etcd-部署">ETCD 部署</h2>
<h3 id="源码安装">源码安装</h3>
<p>基于master分支构建etcd</p>
<pre><code class="language-bash">git clone https://github.com/etcd-io/etcd.git
cd etcd
./build # 如脚本格式为dos的，需要将其格式修改为unix，否则报错。
</code></pre>
<p>启动命令</p>
<p><code>--listen-client-urls</code> 于 <code>--listen-peer-urls</code> 不能为域名</p>
<p><code>--listen-client-urls</code> 于 <code>--advertise-client-urls</code></p>
<pre><code class="language-sh"> ./etcd --name=etcd \
 --data-dir=/var/lib/etcd/ \
 --listen-client-urls=https://10.0.0.1:2379 \
 --listen-peer-urls=https://10.0.0.1:2380 \
 --advertise-client-urls=https://hketcd:2379 \
 --initial-advertise-peer-urls=https://hketcd:2380 \
 --cert-file=&quot;/etc/etcd/pki/server.crt&quot; \
 --key-file=&quot;/etc/etcd/pki/server.key&quot; \
 --client-cert-auth=true \
 --trusted-ca-file=&quot;/etc/etcd/pki/ca.crt&quot; \
 --auto-tls=false \
 --peer-cert-file=&quot;/etc/etcd/pki/peer.crt&quot; \
 --peer-key-file=&quot;/etc/etcd/pki/peer.key&quot; \
 --peer-client-cert-auth=true \
 --peer-trusted-ca-file=&quot;/etc/etcd/pki/ca.crt&quot; \
 --peer-auto-tls=false
</code></pre>
<h3 id="其他方式">其他方式</h3>
<ul>
<li>CentOS 可以使用 <code>yum install etcd -y</code></li>
<li>Ubuntu 可以预构建的<a href="https://github.com/etcd-io/etcd/releases/" target="_blank"
   rel="noopener nofollow noreferrer" >二进制文件</a></li>
</ul>
<h3 id="安装报错">安装报错</h3>
<blockquote>
<p>certificate: x509: certificate specifies an incompatible key usage</p>
<p>原因：此处证书用于<code>serverAuth</code> 与<code>clientAuth</code>，缺少<code>clientAuth</code>导致</p>
<p>解决：<code>extendedKeyUsage=serverAuth， clientAuth</code></p>
</blockquote>
<pre><code class="language-bash">WARNING: 2020/11/12 14:11:42 grpc: addrConn.createTransport failed to connect to {0.0.0.0:2379  &lt;nil&gt; 0 &lt;nil&gt;}. Err: connection error: desc = &quot;transport: authentication handshake failed: remote error: tls: bad certificate&quot;. Reconnecting...
{&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2020-11-12T14:11:46.415+0800&quot;,&quot;caller&quot;:&quot;embed/config_logging.go:198&quot;,&quot;msg&quot;:&quot;rejected connection&quot;,&quot;remote-addr&quot;:&quot;127.0.0.1:52597&quot;,&quot;server-name&quot;:&quot;&quot;,&quot;error&quot;:&quot;tls: failed to verify client certificate: x509: certificate specifies an incompatible key usage&quot;}
</code></pre>
<blockquote>
<p>原因：证书使用的者不对。</p>
<p>解决：查看<code>subjectAltName</code> 是否与请求地址一致。</p>
</blockquote>
<pre><code class="language-sh">error &quot;tls: failed to verify client's certificate: x509: certificate specifies an incompatible key usage&quot;, ServerName &quot;&quot;
</code></pre>
<blockquote>
<p>原因：<code>ETCD_LISTEN_PEER_URLS</code> 与 <code>ETCD_LISTEN_CLIENT_URLS</code> 不能用域名</p>
</blockquote>
<pre><code>error verifying flags, expected IP in URL for binding (https://hketcd:2380). See 'etcd --help'
</code></pre>
<blockquote>
<p>error #0: x509: certificate has expired or is not yet valid</p>
<p>原因：证书还未生效</p>
<p>解决：因服务器时间不对导致，校对时间后正常</p>
</blockquote>
<pre><code>etcd: rejected connection from &quot;x.x.x.x:1126&quot; (error &quot;tls: failed to verify client's certificate: x509: certificate signed by unknown authority (possibly because of \&quot;crypto/rsa: verification error\&quot; while trying to verify candidate authority certificate \&quot;etcd-ca\&quot;)&quot;, ServerName &quot;&quot;)
</code></pre>
<h2 id="配置文件详解">配置文件详解</h2>
<p><a href="https://etcd.io/docs/v3.3.13/op-guide/configuration/" target="_blank"
   rel="noopener nofollow noreferrer" >config</a></p>
<h2 id="etcdctl-使用">etcdctl 使用</h2>
<pre><code class="language-sh">etcdctl --key-file=/etc/etcd/pki/client.key \
--cert-file=/etc/etcd/pki/client.crt \
--ca-file=/etc/etcd/pki/ca.crt  \
--endpoint=&quot;https://node01.k8s.test:2379&quot; \
cluster-health

member 288506ee270a7733 is healthy: got healthy result from https://node03.k8s.test:2379
member 863156df9b1575d1 is healthy: got healthy result from https://node02.k8s.test:2379
member ff386de9dc0b3c40 is healthy: got healthy result from https://node01.k8s.test:2379
</code></pre>
<p>v3 版本客户端使用</p>
<pre><code class="language-sh">export ETCDCTL_API=3

etcdctl --key=/etc/etcd/pki/client.key \
--cert=/etc/etcd/pki/client.crt \
--cacert=/etc/etcd/pki/ca.crt \
--endpoints=&quot;https://master.k8s:2379&quot; \
endpoint health


etcdctl \
--key=/etc/etcd/pki/client.key \
--cert=/etc/etcd/pki/client.crt \
--cacert=/etc/etcd/pki/ca.crt \
--endpoints=&quot;https://master.k8stx.com:2379&quot; \
endpoint status
</code></pre>
<h3 id="日志独立">日志独立</h3>
<p>etcd日志默认输出到 <code>/var/log/message</code> 如果想独立日志为一个文件，可以使用rsyslogd过滤器功能，使etcd的日志输出到单独的文件内。</p>
<ol>
<li><strong>新建<code>/etc/rsyslog.d/xx.conf</code>文件</strong>。</li>
<li><strong>在新建文件内写入内容如下</strong></li>
</ol>
<pre><code>if $programname == 'etcd' then /var/log/etcd.log
# 停止往其他文件内写入，如果不加此句，会继续往/var/log/message写入。
if $programname == 'etcd' then stop  
</code></pre>
<p>也可以</p>
<pre><code>if ($programname == 'etcd') then {
   action(type=&quot;omfile&quot; file=&quot;/var/log/etcd.log&quot;)
   stop
}
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes存储卷</title>
      <link>https://www.oomkill.com/2018/09/k8s-volumes/</link>
      <pubDate>Sun, 30 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/09/k8s-volumes/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>在Kubernetes之上，在节点级提供一个存储卷的方式来持久存储数据的逻辑，这种只具备一定程度上的持久性。为了实现更强大的持久性，应该使用脱离节点而存在的共享存储设备。 为此Kubernetes提供了不同类型的存储卷。</p>
<p>大多数和数据存储服务相关的应用，和有状态应用几乎都是需要持久存储数据的。容器本身是有生命周期的，为了使容器终结后可以将其删除，或者编排至其他节点上去运行。意味着数据不能存储在容器本地。一旦Pod故障就会触发重构。如果将数据放置在Pod自有的容器内名称空间中，数据随着Pod终结而结束。为了突破Pod生命周期的限制，需要将数据放置在Pod自有文件系统之外的地方。</p>
<p>存储卷</p>
<p>对Kubernetes来讲，存储卷不属于容器，而属于Pod。因此，在Kubernetes中同一个Pod内的多个容器可共享访问同一组存储卷。</p>
<p>Pod底部有一个基础容器， ==<code>pause</code>==，但是不会启动。pause是基础架构容器。创建Pod时pause时Pod的根，所有Pod，包括网络命名空间等分配都是分配给pause的。在Pod中运行的容器是pause的网络名称空间的。容器在挂载存储卷时，实际上是复制pause的存储卷。</p>
<p>因此为了真的实现持久性，存储卷应为宿主机挂载的外部存储设备的存储卷。如果需要实现跨节点持久，一般而言需要使用脱离节点本地的网络存储设备（ceph、glusterfs、nfs）来实现。节点如果需要使用此种存储的话，需要可以驱动相应存储设备才可以（在节点级可以访问相应网络存储设备）。</p>
<h4 id="k8s之上可使用的存储卷">k8s之上可使用的存储卷</h4>
<p>Kubernetes支持的存储卷类型</p>
<ul>
<li><strong>empryDir</strong>：只在节点本地使用的，用于做临时目录，或当缓存使用。一旦Pod删除，存储卷一并被删除。empryDir背后关联的宿主机目录可以使宿主机的内存。</li>
<li><strong>hostPath</strong>：使宿主机目录与容器建立关联关系。</li>
<li><strong>网络存储</strong>
<ul>
<li>传统的SAN（iSCSI，FC）NAS（常见用法协议 NFS,cifs,http）设备所构建的网络存储设备。</li>
<li>分布式存储（分机系统或块级别），glusterfs，ceph(rbd ceph的块接口存储)，cephfs等。</li>
<li>云存储：EBS（弹性块存储）亚马逊 ,Azure Disk 微软。此模型只适用于Kubernetes集群托管在其公有云之上的场景。</li>
</ul>
</li>
</ul>
<p>使用<code>kubectl explain pod.spec.volumes</code>查看Kubernetes所支持的存储类型。</p>
<h4 id="emptydir">emptyDir</h4>
<blockquote>
<p><strong>语法</strong></p>
</blockquote>
<ul>
<li><strong><code>emptyDir</code></strong>
<ul>
<li><code>medium</code> 媒介类型 empty string （disk 默认） or memory</li>
<li><code>sizeLimit</code> 空间上限</li>
</ul>
</li>
</ul>
<hr>
<p>定义完存储卷之后，需要在<code>container</code>当中使用<code>volumeMounts</code>指明挂载哪个或哪些个存储卷</p>
<hr>
<pre><code class="language-yaml">- container
    - mountPath 挂载路径
    - name 挂载那个卷
    - readOnly 是否只读挂载
    - subPath 是否挂载子路径之下
</code></pre>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-nginx
  namespace: default
spec:
  containers:
  - name: busybox
    image: busybox
    imagePullPolicy: IfNotPresent
    ports:
    - name: http
      containerPort: 80
    command: [&quot;tail&quot;]
    volumeMounts:  # 指明挂载哪一个存储卷
    - name: html
      mountPath: /data/web/html # 指明挂载到容器的哪个路径下
  volumes:
  - name: html
    emptyDir: {} # 表示空映射，都使用默认值，大小不限制，使用磁盘空间，而不是不定义
</code></pre>
<p>在Kubernetes中 $()是变量引用</p>
<h4 id="gitrepo">gitRepo</h4>
<p>将git仓库当做存储卷来使用，其实并不是Pod将git仓库当存储卷来使用。只不过是在Pod创建时，会自动连接到git仓库之上（此链连接依赖于宿主机上有git命令来完成），由宿主机驱动，将git仓库中的内容clone到本地来，并且将其作为存储卷挂载至Pod之上。</p>
<ol>
<li>
<p>==gitRepo是建立在emptyDir之上==。所不同的在于，将所指定仓库的内容clone下来并放至空目录中。因此主容器将此目录当做服务于用户的数据来源。需要注意的是，在此处做的修改是不会同步到git仓库中去的。</p>
</li>
<li>
<p>如果git仓库在Pod运行过程中内容发生改变，Pod之内的存储卷内容是不会随之改变的。</p>
</li>
</ol>
<blockquote>
<p><strong>gitRepo 卷示例</strong>：</p>
</blockquote>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: my-git
  namespace: default
spec:
  containers:
  - name: git
    image: nginx
    imagePullPolicy: IfNotPresent
    ports:
    - name: http
      containerPort: 80
    volumeMounts:
    - name: git-volumes
      mountPath: /data/web/html
  volumes:
  - name: git-volumes
    gitRepo:
      repository: &quot;https://github.com/potester/test-k8s.git&quot;
      revision: &quot;master&quot;
</code></pre>
<p>查看Pod内容器挂载的目录</p>
<pre><code class="language-sh">$ kubectl exec -it my-git -- ls -l /data/web/html/test-k8s/
total 16
nginx.svc
redis.svc
svc-redis
test.yaml
</code></pre>
<h4 id="hostpath">hostPath</h4>
<p>hostPath  将Pod所在宿主机之上、脱离Pod中容器名称空间之外的宿主机的文件系统的某一目录与Pod建立关联关系。在Pod被删除时，此存储卷是不会被删除的。</p>
<p>hostPath在一定程度上拥有持久的特性，但这种持久只是节点级的持久，在被跨节点调度时，这些数据还是会丢失的。</p>
<p><a href="https://kubernetes.io/docs/concepts/storage/volumes/#hostpath" target="_blank"
   rel="noopener nofollow noreferrer" >Volumes - Kubernetes</a></p>
<p><strong>type</strong></p>
<ul>
<li><code>DirectoryOrCreate</code> 挂载路径在宿主机上是已存在的目录，如目录不存在则创建此目录。</li>
<li><code>Directory</code> 挂载路径在宿主机上必须已存在的目录。</li>
<li><code>FileOrCreate</code> 文件或创建新的空文件。</li>
<li><code>File</code> 必须存在此文件，将其挂载至容器中。</li>
<li><code>Socket</code> 必须是socket类型的文件。</li>
<li><code>CharDevice</code> 必须是一个字符类型的设备文件。</li>
<li><code>BlockDevice</code> 块类型的设备文件。</li>
</ul>
<blockquote>
<p><strong>hostPath 卷示例</strong>：</p>
</blockquote>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: pod-hostPath
  namespace: default
spec:
  containers:
  - name: myapp
    image: nginx:1.8
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html/ # 挂载路径
  volumes: # 定义存储卷
  - name: html
    hostPath:
      path: /data/html
      type: DirectoryOrCreate
</code></pre>
<h4 id="nfs对于nfs存储卷来讲">NFS，对于nfs存储卷来讲</h4>
<ul>
<li>path -required-  nfs服务器导出路径，</li>
<li>readOnly 只读 true or false 默认false</li>
<li>server [required] 服务器地址</li>
</ul>
<pre><code class="language-yaml">volumes:
- name: html
  nfs:
    path: /data/volumes/
    server: nfs01.test.com
</code></pre>
<h3 id="pvc使用逻辑">PVC使用逻辑</h3>
<p><img loading="lazy" src="../../../images/%E5%AD%98%E5%82%A8%E5%8D%B7/7807979d.png" alt="7807979d.png" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>在Pod中只需定义存储卷，定义时只需指定使用存储卷大小，这个存储卷叫PVC类型的存储卷。PVC存储卷必须与当前名称空间中的PVC建立直接绑定关系，而PVC必须与PV建立绑定关系，而PV是某个真实存储设备上的存储空间。所以PV和PVC是kubernetes系统之上的抽象的标准资源。PV和PVC之间的关系，在PVC不被调用时是空载的。</p>
<p>对于PV类型的资源的使用</p>
<p><img loading="lazy" src="../../../images/%E5%AD%98%E5%82%A8%E5%8D%B7/0dbd2bc7.png" alt="0dbd2bc7.png" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="pvc语法">PVC语法</h4>
<p>PVC是标准K8S资源，也有自己所属的属组。</p>
<p><code>kubectl explain pvc</code></p>
<pre><code class="language-yaml">apiVersion: v1
kind: pvc
metadata:
  name: k8s-pvc
  namespaces: default
spec:
  accessModes # 访问模型
  resources # 资源限制，至少多少G
  selector # 标签选择器，必须要选择哪个PV建立关联关系
  storageClassName # 存储类名称
  volumeMode # 后端存储卷模式。
  volumeName # 存储卷名称，指后端PersistentVolume
</code></pre>
<blockquote>
<p><strong>PVC选择的模式</strong></p>
</blockquote>
<ul>
<li>使用存储卷名称，一对一绑定了（精确选择），</li>
<li>选择器选定</li>
<li>如不指定名称，会从大量符合条件的PV选一个。</li>
<li>类型限制，volumeMode，那一类型的PV可以被当前claim所使用。</li>
</ul>
<h4 id="在pod中使用当前名称空间已经存在的pvc">在Pod中使用当前名称空间已经存在的PVC</h4>
<p>exportfs -arv
<code>kubectl explain pods.spec.volumes.persistentVolumeClaim</code>，PV和PVC的关联是一对一的，一旦被使用（状态为banding）</p>
<pre><code class="language-yaml">volmes
persistentVolumeClaim
  claimName # pvc名称
  readOnly # 要不要只读
</code></pre>
<p>定义PV时一定不要加名称空间，PV是集群级别的，不属于名称空间，但==PVC是属于名称空间级别==的。PVC并不属于节点(node)，PVC是标准的Kubernetes资源，它存储在etcd当中。只有Pod才需要运行在节点之上，所有其他资源基本都是保存在集群状态存储（apiserver的存储）etcd当中。</p>
<p>在Kubernetes新版本中，只要PV还被PVC绑定，就不支持删除。</p>
<p>定义accessMode时需要注意存储设备，有些存储设备不支持多路读写与多路只读，只支持单路读写。</p>
<ul>
<li><strong>accessMode</strong> []string   accessMode可定义多个参数
<ul>
<li><code>ReadWriteOnce</code> 单路读写，可简写为RWO</li>
<li><code>ReadOnlyMany</code>  多路只读，ROX</li>
<li><code>ReadWriteMany</code> 多路读写操作 RWX</li>
</ul>
</li>
<li><strong>capacity</strong>  用来指定存储空间的大小，需要使用资源访问模型来定义。
<ul>
<li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes#capacity" target="_blank"
   rel="noopener nofollow noreferrer" >capacity</a></li>
<li><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/resources.md" target="_blank"
   rel="noopener nofollow noreferrer" >resources.md</a></li>
</ul>
</li>
<li><strong>persistentVolumeReclaimPolicy</strong> 回收策略  <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain" target="_blank"
   rel="noopener nofollow noreferrer" >Persistent Volumes</a>
<ul>
<li><code>Retain</code> 保留。</li>
<li><code>Recycle</code> 回收，将数据删除，并且把PV置为空闲状态可以让其他设备绑定。</li>
<li><code>delete</code> 删除PV</li>
</ul>
</li>
</ul>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv01
  labels:
    name: pv01
    type: ssd
spec:
  nfs:
    path: /data/pv01
    server: 192.168.1.2
  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadWriteOnce&quot;]
  capacity:
    storage: 200Mi
</code></pre>
<p>PVC</p>
<ul>
<li><strong>spec</strong>
<ul>
<li><code>accessModes</code> PVC也需要定义accessModes，此accessModes模式要求一定是PV的accessModes的子集才可以被匹配到。</li>
<li><code>resources</code> 如指定，PV一定要满足（大于、等于）PVC此值才能被使用。</li>
<li><code>requests</code> []map此处是与PV不一样之处，需要要求有多大空间</li>
</ul>
</li>
</ul>
<blockquote>
<p>PVC的使用</p>
</blockquote>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv01
  labels:
    name: pv-test1
spec:
  accessModes: 
  - ReadWriteOnce
  capacity: 
    storage: 2Gi
  nfs:
    path: /data/v1
    server: 10.0.0.11
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv02
  labels:
    name: pv-test2
spec:
  accessModes:
  - ReadWriteOnce
  capacity: 
    storage: 2Gi
  nfs:
    path: /data/v2
    server: 10.0.0.11
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv03
  labels:
    name: pv-test3
spec:
  accessModes:
  - ReadWriteOnce
  capacity: 
    storage: 2Gi
  nfs:
    path: /data/v3
    server: 10.0.0.11
</code></pre>
<p>查看PV和PVC</p>
<pre><code class="language-sh">$ kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM          STORAGECLASS   REASON   AGE
pv01   2Gi        RWO            Retain           Bound       default/pvc1                           5h45m
pv02   2Gi        RWO            Retain           Available                                          5h45m
pv03   2Gi        RWO            Retain           Available                                          5h45m

$ kubectl get pvc
NAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc1   Bound    pv01     2Gi        RWO                           5h11m
</code></pre>
<h3 id="storageclass">StorageClass</h3>
<p>在PVC申请时，未必就有现成的PV能正好符合PVC在申请中指定的条件，为此Kubernetes设计了一种工作逻辑，能够让PVC在申请PV时不针对某个PV进行。可以针对某个存储类（Kubernetes之上的标准资源之一）<code>StorageClass</code>，借助此资源层，来完成资源分配的。</p>
<p><code>StorageClass</code>可以理解为，事先把众多的存储设备当中所提供好的现有可用存储空间（尚未做成PV的存储空间）进行分类（根据综合服务质量、IO性能等）。定义好存储类之后，当PVC再去申请PV时，不针对某个PV直接进行，而是针对存储了进行。必须让存储设备支持Restful风格的请求创建接口，用户可以通过Restful风格的请求1.在磁盘上划分刚好符合PVC大小的分区。2 编辑<code>/etc/export</code>文件，将分区挂载至本地某个目录上。3. 动态创建PV去绑定之前动态导出的空间。</p>
<p><img loading="lazy" src="../images/%E5%AD%98%E5%82%A8%E5%8D%B7/a8fee656.png" alt="a8fee656.png" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>nfs 动态</p>
<p><a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client#how-to-deploy-nfs-client-to-your-cluster" target="_blank"
   rel="noopener nofollow noreferrer" >external-storage/nfs-client at master · kubernetes-incubator/external-storage · GitHub</a></p>
<p><a href="https://blog.51cto.com/wks97/2173503" target="_blank"
   rel="noopener nofollow noreferrer" >Docker(二十九)k8s 创建动态存储，基于nfs 的storageclass-洒脱，是云谈风轻的态度-51CTO博客</a></p>
<p><a href="https://www.lijiaocn.com/%E6%8A%80%E5%B7%A7/2018/08/30/confd-prometheus-dynamic-config.html" target="_blank"
   rel="noopener nofollow noreferrer" >通过consul、confd，动态为prometheus添加监控目标和告警规则</a></p>
<p><a href="https://www.lijiaocn.com/%E9%A1%B9%E7%9B%AE/2018/09/19/kubernetes-clsuter-monitor.html" target="_blank"
   rel="noopener nofollow noreferrer" >使用Prometheus建设Kubernetes的监控告警系统</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - configMap</title>
      <link>https://www.oomkill.com/2018/09/k8s-cm/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/09/k8s-cm/</guid>
      <description></description>
      <content:encoded><![CDATA[<p><code>secret</code>、<code>configMap</code>特殊类型的存储卷，多数情况下不是为Pod提供存储空间来用的，而是给管理员或用户提供了从集群外部向Pod内部应用注入配置信息的方式。</p>
<p>工作实现</p>
<h3 id="configmap">configMap</h3>
<p>在集群内部存在一个名称空间，在名称空间当中拥有一个可正常运行的Pod，当镜像启动时使用的配置文件在做镜像之前就确定了，并且做完镜像就不能修改了。除非在做镜像时使用entryPoint脚本去接受用户启动容器时传入环境变量进来，将环境变量的数据替换到配置文件中去，从而使应用程序在启动之前就能获得一个新的配置文件而后得到新的配置。当需要修改配置文件时是很麻烦的。而配置中心只需将集中的配置文件修改，并通知给相应进程，让其重载配置文件。而Kubernetes的应用也存在此类问题，当配置文件修改后就需要更新整个镜像。因此无需将配置信息写死在镜像中。而是引入一个新的资源，这个资源甚至是整个Kubernetes集群上的一等公民（标准的K8S资源）。这个资源被叫做configMap</p>
<p>configMap当中存放的配置信息，随后启动每一个Pod时，Pod可以共享使用同一个configMap资源，这个资源对象可以当存储卷来使用，也可以从中基于环境变量方式从中获取到一些数据传递给环境变量，注入到容器中去使用。 因此configMap扮演了Kubernetes中的配置中心的功能。但是configMap是明文存储数据的。因此和configMap拥有同样功能的标准资源<code>secret</code>就诞生了。与configMap所不同之处在于，secret中存放的数据是用过编码机制进行存放的。</p>
<p><strong>核心作用</strong>：让配置信息从镜像中解耦，从而增强应用的可移植性与复用性。使一个镜像文件可以为应用程序运行不同配置的环境而工作。简单来讲，一个configMap就是一系列配置数据的集合。这些数据可以注入到Pod对象中的容器所使用。</p>
<p>在configMap中，所有的配置信息都保存为<code>key value</code>格式。V只是代表了一段配置信息，可能是一个配置参数，或整个配置文件信息都是没有问题的。</p>
<blockquote>
<p>配置容器化应用的方式</p>
</blockquote>
<ul>
<li>自定义命令行参数  <code>args []</code></li>
<li>把配置文件直接陪进镜像；</li>
<li>环境变量
<ul>
<li>Cloud Native的应用程序一般可直接通过环境变量加载配置</li>
<li>通过<code>entrypoint</code>脚本来预处理变量为配置文件中的配置信息。</li>
</ul>
</li>
<li>存储卷</li>
</ul>
<blockquote>
<p>配置文件注入方式：</p>
</blockquote>
<ul>
<li>将configMap做存储卷</li>
<li>使用env</li>
</ul>
<p>docker config</p>
<hr>
<ul>
<li><strong>contioners</strong>
<ul>
<li><strong>env</strong>
<ul>
<li><code>name</code> 变量名</li>
<li><code>value</code> 变量值</li>
<li><code>valueFrom</code> 数据不是一个字符串，而是引用另外一个对象将其传递给这个变量。
<ul>
<li><code>configMapKeyRef</code> configMap中的某个键</li>
<li><code>fieldRef</code> 某个字段。此资源可以是Pod自身的字段。如<code>metadata.labels</code> <code>status.hostIP</code> <code>status.podIP</code></li>
<li><code>resourceFieldRef</code> 资源需求和资源限制。</li>
<li><code>secreKeyRef</code> 引用secre</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>configMap无需复杂描述，因此没有spec字段</p>
<pre><code>apiVersion
kind
data
binaryData 一般情况下data与binaryData只使用其中一种。
</code></pre>
<p>创建简单的configMap还可以使用  <code>kubectl create configMap</code>来创建。如需要长期使用，可以定义为配置清单文件。</p>
<pre><code>kubectl create configmap nginx-config \
--from-literal=nginx_port=80 \
--from-literal=servername=test.com
</code></pre>
<p>使用文件创建</p>
<pre><code>kubectl create configmap nginx-conf --from-file=www.conf
</code></pre>
<p>将configMap注入到Pod中。</p>
<p>方式1：使用环境变量方式注入</p>
<pre><code class="language-yaml">env:
- name: NGINX_SERVER_PORT
  valueFrom:
    configMapKeyRef:
      name: nginx-config
      key: nginx_port
      optional # 如为true表示必须拥有此key
- name: NGINX_SERVER_NAME
  valueFrom:
    configMapKeyRef:
      name: nginx-config
      key: server_name
</code></pre>
<p>当使用环境变量方式注入时，只在系统启动时有效，如使用存储卷方式定义的，是可以实时更新的。</p>
<p>方式2：</p>
<pre><code class="language-yaml">containers:
- name: nginx-configMap
  volumeMounts:
  - name: nginxconfig
    mountPaht: /etc/nginx/con.d/
    readOnly: true # 不需要容器去修改他的内容
volumes:
- name: nginxconfig
  configMap: # volume类型
    name: nginx-conf
</code></pre>
<p>k8s  节点为了运行Pod，而获取镜像，镜像如果托管在必须认证才能获取的私有仓库上时。node节点上的kubelet需能自动完成认证。</p>
<p>docker-registry docker私有仓库认证信息使用
generic
tls 证书私钥</p>
<p>spec</p>
<ul>
<li>imagePullSecrets Pod在创建时，如果要连到私有仓库需要做认证，此处的secret包含了让kubelet去连接私有仓库的账号和密码。此账号密码是secret对象提供的， 必须是专用对象。</li>
</ul>
<p>创建方法</p>
<pre><code>kubectl create secret type(docker-registry|generic|tls) Name 
</code></pre>
<p>查询</p>
<pre><code>kubectl get secret passwd -o yaml
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/427f4e23.png" alt="427f4e23.png" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>env方式载入secret</p>
<pre><code class="language-yaml">env:
- name: MYSQL_ROOT_PASSWD
  valueFrom:
    secretKeyRef:
      name: root-pwd
      key: passwd
      optional # 如为true表示必须拥有此key
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - Dashboard</title>
      <link>https://www.oomkill.com/2018/09/k8s-dashboard/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/09/k8s-dashboard/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>基于web的UI前端，认证是由Kubernetes完成的。登陆dashboard的密码是k8s的账号和密码，和dashboard自身没有关系。dashboard自身不做认证。</p>
<pre><code>kubectl patch svc kubernetes-dashboard -p'{&quot;spec&quot;:{&quot;type&quot;:&quot;NodePort&quot;}}'-n kube-system
</code></pre>
<p>如使用域名访问，CN一定要与域名保持一致。</p>
<pre><code>(umask 077; openssl genrsa -out dashboard.key 2048)
openssl req -new -key dashboard.key -out dashboard.csr -subj &quot;/O=test/CN=dashboard&quot;
openssl req -in dashboard.csr -noout -text
openssl x509 -req -in dashboard.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out dashboard.crt -days 3650
</code></pre>
<p><a href="https://docs.oracle.com/cd/E24191_01/common/tutorials/authz_cert_attributes.html" target="_blank"
   rel="noopener nofollow noreferrer" >Certificate Attributes</a></p>
<p>要想穿透集群边界，从集群外访问集群内部某一服务或Pod上的容器的应用，有两种方式 nodePort、NodeBlanc 或ingress</p>
<pre><code>kubectl create secret generic \
dashboard-cert \
-n kube-system \
--from-file=dashboard.crt=./dashboard.crt \
--from-file=dashboard.key=./dashboard.key
</code></pre>
<p>dashboard运行在Pod中时，当用户通过浏览器来进行登陆时，所提供的认证证书必须时serviceaccount，</p>
<pre><code>kubectl create serviceaccount dashboard-admin -n kube-system
</code></pre>
<p>通过rolebindding吧对应的dashboard-admin和集群管理员建立起绑定关系，否则无法透过rbac的权限检查。</p>
<p>指明serviceaccount时，必须指明是哪个名称空间的的哪个账号，格式：<font color="#f8070d" size=3><code>namespace:serviceaccount</code></font></p>
<pre><code>kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
</code></pre>
<p>使serviceaccount用户，能够有权限访问整个集群级别的资源。</p>
<p>查询dashboard-admin的secret，这个是自动生成的。</p>
<pre><code>kubectl get secret $(kubectl get secret -n kube-system|grep dashboard-admin-token|awk '{print $1}') -n kube-system -o jsonpath={.data.token}|base64 -d
</code></pre>
<p>kubectl config set-credentials 命令，用户的认证方式，既可以使用证书方式，也可以使用token认证。</p>
<p>在apply之前先将证书做成secret，apply操作会将其加载成为apply操作对外提供https服务时使用的证书。此操作作为serviceaccount认证是没有关系的，只不过是被dashboard用来做https证书的。如果不提供证书，dashboard会自动生成新的证书。</p>
<h2 id="kubernetes-dashboard延长自动超时注销">kubernetes dashboard延长自动超时注销</h2>
<p>方法1：部署清单时，修改yaml文件，添加 <code>container.Args</code> 增加 <code>--token-ttl=43200</code> 其中43200是设置自动超时的秒数。也可以设置 <code>token-ttl=0</code> 以完全禁用超时。</p>
<p>方法2：操作已经部署的配置，<code>kubectl edit deployment -n kube-system kubernetes-dashboard</code>，新增上面参数到 <code>args</code> 中</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - ingress</title>
      <link>https://www.oomkill.com/2018/09/k8s-ingress/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/09/k8s-ingress/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>IngressController比较独特，它与DaemonSet、Deployment、Repliacaset不同，DaemonSet、Deployment等控制器是作为ControllerManager的子组件存在的。Ingress Controller是独立运行的一组Pod资源，通常是拥有七层代理、调度能力的应用程序。</p>
<p>通常在使用IngressController时有三种选择Nginx、Traefik、Envoy。</p>
<p>IngressController nginx运行在Pod中，其配置文件是在Pod中。后端代理的Pod随时会发生变动，IngressController需要watch API当中的后端Pod资源的改变。IngressController自身无法识别目前符合自己关联的（条件的）被代理的Pod资源有哪些，IngressController需借助service来实现。</p>
<p>因此要想定义一个对应的调度功能，还需要创建service，此service通过label selector关联至每一个upstream服务器组，通过此service资源关联至后端的Pod。此service不会被当做被代理时的中间节点，它仅仅是为Pod做分类的。此service关联的Pod，就将其写入upstream中。</p>
<p>在Kubernetes中有一种特殊资源叫做Ingress，当Pod发生改变时，其servcie对应的资源也会发生改变， 依赖于IngressResource将变化结果反应至配置文件中。</p>
<p>Ingress定义期望IngressController如何创建前段代理资源（虚拟主机、Url路由映射），同时定义后端池（upstream）。upstream中的列表数量，是通过service获得。</p>
<p>Ingress可以通过编辑注入到IngressController中，并保存为配置文件，且Ingress发现service选定的后端Pod资源发生改变，此改变会及时反映至Ingress中，Ingress将其注入到前端调度器Pod中，并触发Pod中的container主进程（nginx）重载配置文件。</p>
<p>要想使用Ingress功能，需要有service对某些后端资源进行分类，而后Ingress通过分类识别出Pod的数量和IP地址信息，并将反映结果生成配置信息注入到upstream中。</p>
<p>IngressController根据自身需求方式来定义前端，而后根据servcie收集到的后端Pod IP定义成upstream server，将这些信息反映在Ingress server当中，由Ingress动态注入到IngressController当中。</p>
<p>Ingress也是标准的Kubernetes资源，定义Ingress时同样类似于Pod方式来定义。使用<code>kubectl explain Ingress</code>查看帮助。</p>
<ul>
<li>spec</li>
<li>rules 规则，对象列表
<ul>
<li>host 主机调度 虚拟主机而非url映射</li>
<li>http
<ul>
<li>paths 路径调度
<ul>
<li>backend</li>
<li>path</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>backend 定义被调度的后端主机，靠service定义，找到后端相关联的Pod资源。
<ul>
<li>serviceName 后端servcie名称，即用来关联Pod资源的service。</li>
<li>servicePort</li>
</ul>
</li>
</ul>
<p>IngressController部署</p>
<p>namespace.yaml 创建名称空间
configmap.yaml 为nginx从外部注入配置的
rbac.yaml 定义集群角色、授权。必要时让IngressController拥有访问他本身到达不了的名称空间的权限</p>
<p>ingress.yaml</p>
<pre><code class="language-yaml">apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-myapp
  namespace: defualt # 与deployment和要发布的service处在同一名称空间内
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
spec:
  rules:
  - host: test.baidu.com # 这个是外部访问域名，service映射到主机节点地址上
    http:
      paths:
      - pach: 
        backend:
          serviceName: myapp 
          servicePort: 80
</code></pre>
<p>证书是不能直接贴入ingress中的，需要将其转为特殊格式 secret， secret是标准的Kubernetes对象，c可以直接注入到Pod中被Pod所引用。</p>
<p><a href="https://www.cnblogs.com/ericnie/p/6965091.html" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes的负载均衡问题(Nginx Ingress) - ericnie - 博客园</a></p>
<p><a href="https://kubernetes.github.io/ingress-nginx/deploy/#verify-installation" target="_blank"
   rel="noopener nofollow noreferrer" >Installation Guide - NGINX Ingress Controller</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - Kubenetes Deployment</title>
      <link>https://www.oomkill.com/2018/09/kubenetes-deployment/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/09/kubenetes-deployment/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>Deployment当中借助于ReplicaSet进行更新的策略反映在Deployment的对象定义所需字段可使用<font color="#f8070d" size=3><code>kubectl explain deploy</code></font>，Deployment属于extension群组。在1.10版本中它被移至到apps群组。他与ReplicaSet相比增加了几个字段。</p>
<p>stratgy 重要字段，定义更新策略，它支持两种策略 重建式更新 <font color="#f8070d" size=3><code>Recreate</code></font>与滚动更新<font color="#f8070d" size=3><code>RollingUpdate</code></font>，如果type为RollingUpdate，那么RollingUpdate的策略还可以使用RollingUpdate来定义，如果type为Recreate，那么RollingUpdate字段无效。 默认值为<font color="#f8070d" size=3><code>RollingUpdate</code></font></p>
<p>stratgy.RollingUpdate控制RollingUpdate更新力度</p>
<ul>
<li>maxSurge 对应的更新过程当中，最多能超出目标副本数几个。有两种取值方式，为直接指定数量和百分比。在使用百分比时，在计算数据时如果不足1会补位1个。</li>
<li>maxUnavailable 最多有几个副本不可用。</li>
</ul>
<p>revisionHistoryLimit 滚动更新后，在历史当中最多保留几个历史版本，默认10。</p>
<p>    在使用Deployment创建Pod时，Deployment会自动创建ReplicaSet，而且Deployment名称是使用Pod模板的hash值，此值是固定的。</p>
<p>    Deployment在实现更新应用时，可以通过编辑配置文件来实现，使用kubectl apply -f更改每次变化。每次的变化通过吧变化同步至apiserver中，apiserver发现其状态与etcd不同，从而改变etcd值来实现修改其期望状态，来实现现有状态去逼近期望状态。</p>
<p>kubectl explain deploy</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deploy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: deploy
      release: canary
  template:
    metadata:
      labels:
        app: deploy
        release: canary
    spec:
      containers:
      - name: my-deploy
        image: node01:5000/busybox:v1
        ports:
        - name: http
          containerPort: 80
        command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;/bin/httpd -f -h /tmp&quot;]
</code></pre>
<blockquote>
<p><strong>使用<font color="#f8070d" size=3><code>kubectl apply</code></font> 声明式更新、创建资源对象。</strong></p>
</blockquote>
<p>将上述资源配置清单的replicaSet数量改为3个后，可以看到数量增加为3，而对应的hash值没变化。</p>
<pre><code class="language-sh">$ kubectl apply -f deploy.yaml 
deployment.apps/app-deploy configured
$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
app-deploy-5b8db6bc7d-bkldv   1/1     Running   0          4s
app-deploy-5b8db6bc7d-r2pcv   1/1     Running   0          5h5m
app-deploy-5b8db6bc7d-wgbbg   1/1     Running   0          5h5m
</code></pre>
<p>    由于Deployment是构建在ReplicaSet之上，对Pod做扩展、缩容是很方便的。处理动态修改资源配置清单外，还可以使用<font color="#f8070d" size=3><code>kubectl patch</code></font>（打补丁）进行操作。</p>
<p>    patch操作是对对象的JSON内容进行打补丁，-p选项值为JSON格式，其建值需以引号引起。</p>
<blockquote>
<p><strong>语法</strong></p>
</blockquote>
<pre><code>kubectl patch
</code></pre>
<p>-p 提供补丁</p>
<pre><code>$ kubectl patch deployment app-deploy -p '{&quot;spec&quot;:{&quot;replicas&quot;:4}}'
deployment.extensions/app-deploy patched
$ kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
app-deploy-58d74fd87f-555jm   1/1     Running   0          15m
app-deploy-58d74fd87f-kkwz9   1/1     Running   0          15m
app-deploy-58d74fd87f-vzthx   1/1     Running   0          15m
app-deploy-58d74fd87f-zwdn5   1/1     Running   0          4s
</code></pre>
<pre><code>kubectl patch deployment app-deploy -p '{&quot;spec&quot;: {&quot;strategy&quot;: {&quot;rollingUpdate&quot;: {&quot;maxSurge&quot;:1,&quot;maxUnavailable&quot;:0 }}}}'
</code></pre>
<blockquote>
<p><strong>更新版本</strong></p>
</blockquote>
<p>    在更新完成后使用 kubectl get rs 查看可看到有两个 rs版本，所不同的是，镜像版本不同和可用的数量为0，但这个对应的模板会保留，随时等待回滚。</p>
<pre><code>$ kubectl get rs -o wide
NAME                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                   SELECTOR
app-deploy-58d74fd87f   3         3         3       76s     my-deploy    node01:5000/busybox:v2   app=deploy,pod-template-hash=58d74fd87f,release=canary
app-deploy-5b8db6bc7d   0         0         0       5h10m   my-deploy    node01:5000/busybox:v1   app=deploy,pod-template-hash=5b8db6bc7d,release=canary
</code></pre>
<p>更新版本可以使用可使用 <font color="#f8070d" size=3><code>kubectl set image</code></font>进行更新</p>
<p>语法</p>
<pre><code>kubectl set image [-f filename | type name] container=version
</code></pre>
<pre><code>kubectl set image deployment app-deploy my-deploy=node01:5000/busybox:v4 &amp;&amp; kubectl rollout pause deployment app-deploy
</code></pre>
<p>此时可以看到rs保留多个版本</p>
<pre><code>$ kubectl get rs -o wide
NAME                    DESIRED   CURRENT   READY   AGE    CONTAINERS   IMAGES                   SELECTOR
app-deploy-58d74fd87f   0         0         0       34m    my-deploy    node01:5000/busybox:v2   app=deploy,pod-template-hash=58d74fd87f,release=canary
app-deploy-7fbc8b6df    3         3         3       25m    my-deploy    node01:5000/busybox:v4   app=deploy,pod-template-hash=7fbc8b6df,release=canary
</code></pre>
<p>使用 kubectl rollout pause可以暂停更新。</p>
<pre><code>kubectl rollout pause type typename
</code></pre>
<p>使用resume可恢复暂停操作</p>
<pre><code>kubectl rollout resume type typename
</code></pre>
<h3 id="deployment版本滚动的历史保留">Deployment版本滚动的历史保留</h3>
<p>可使用 <font color="#f8070d" size=3><code>kubectl rollout history</code></font> 查看滚动历史</p>
<pre><code>$ kubectl rollout history deployment app-deploy
deployment.extensions/app-deploy 
REVISION  CHANGE-CAUSE
1         [none]
2         [none]
</code></pre>
<h4 id="版本回滚">版本回滚</h4>
<p>使用 <font color="#f8070d" size=3><code>kubectl rollout undo</code></font>默认是回滚至上一个版本</p>
<blockquote>
<p><strong>语法</strong></p>
</blockquote>
<pre><code>kubectl rollout undo type typename 
</code></pre>
<p><font color="#f8070d" size=3><code>--to-revision=n</code></font> 回滚至指定版本</p>
<pre><code>kubectl rollout undo deployment app-deploy --to-revision=1
</code></pre>
<p>回滚后查询当前rs版本</p>
<pre><code>$ kubectl get rs -o wide
NAME                    DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES                   SELECTOR
app-deploy-58d74fd87f   3         3         3       108m    my-deploy    node01:5000/busybox:v2   app=deploy,pod-template-hash=58d74fd87f,release=canary
app-deploy-7fbc8b6df    0         0         0       99m     my-deploy    node01:5000/busybox:v4   app=deploy,pod-template-hash=7fbc8b6df,release=canary
</code></pre>
<p>Deployment回滚演示</p>
<p><img loading="lazy" src="https://note.youdao.com/yws/public/resource/db979da305f5f19c7dbef51e4b9f32bf/xmlnote/WEBRESOURCE5695ba7b8d4db604cc2ac74a5a8f8eb6/732" alt="b1fc4e91.gif" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - Kubernetes Pod控制器</title>
      <link>https://www.oomkill.com/2018/09/kubernetes-pod-controller/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/09/kubernetes-pod-controller/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="kubernetes资源清单">Kubernetes资源清单</h2>
<table>
<thead>
<tr>
<th>类别</th>
<th>名称</th>
</tr>
</thead>
<tbody>
<tr>
<td>工作负载型资源（workload）</td>
<td>运行应用程序，对外提供服务：Pod、ReplicaSet、Deployment、StatefulSet、DaemonSet、Job、Cronjob （ReplicationController在v1.11版本被废弃）</td>
</tr>
<tr>
<td>服务发现及负载均衡</td>
<td>service、Ingress</td>
</tr>
<tr>
<td>配置与存储</td>
<td>Volume、CSI（容器存储接口</td>
</tr>
<tr>
<td>特殊类型存储卷</td>
<td>ConfigMap（当配置中心来使用的资源类型）、Secret（保存敏感数据）、DownwardAPI（把外部环境中的信息输出给容器）</td>
</tr>
<tr>
<td>集群级资源</td>
<td>Namespace、Node、Role、ClusterRole、RoleBinding（角色绑定）、ClusterRoleBinding（集群角色绑定）</td>
</tr>
<tr>
<td>元数据型资源</td>
<td>HPA、PodTemplate（Pod模板，用于让控制器创建Pod时使用的模板）、LimitRange（用来定义硬件资源限制的）</td>
</tr>
</tbody>
</table>
<h2 id="kubernetes配置清单使用说明">Kubernetes配置清单使用说明</h2>
<p>在Kubernetes中创建资源时，除了命令式创建方式，还可以使用yaml格式的文件来创建符合我们预期期望的pod，这样的yaml文件我们一般称为<font color="#f8070d" size=2>资源清单</font>。资源清单由很多属性或字段所组成。</p>
<p>以yawl格式输出pod的详细信息。</p>
<h3 id="资源清单格式">资源清单格式</h3>
<pre><code class="language-yaml">kubectl get pod clients -o yaml
</code></pre>
<h3 id="pod资源清单常用字段讲解">Pod资源清单常用字段讲解</h3>
<p>在创建资源时，apiserver仅接收JSON格式的资源定义。在使用<font color="#f8070d" size=3><code>kubectl run</code></font>命令时，自动将给定内容转换成JSON格式。yaml格式提供配置清单，apiserver可自动将其转为JSON格式，（yaml可无损转为json），而后再提交。使用资源配置请清单可带来<font color="#f8070d" size=2>复用效果</font>。</p>
<p>Pod资源配置清单由五个一级字段组成，通过<font color="#f8070d" size=3><code>kubectl create -f yamlfile</code></font>就可以创建一个Pod</p>
<ul>
<li>
<p><strong><font color="#f8070d" size=3>apiVersion</font></strong>: 说明对应的对象属于Kubernetes的哪一个API群组名称和版本。给定apiVersion时由两部分组成<font color="#f8070d" size=3><code>group/version</code></font>，group如果省略表示core（核心组）之意。使用<font color="#f8070d" size=3><code>kubectl api-versions</code></font>获得当前系统所支持的apiserver版本。alpha 内测版、beta 公测版、stable 稳定版</p>
</li>
<li>
<p><strong><font color="#f8070d" size=3>kind</font></strong>: 资源类别，用来指明哪种资源用来初始化成资源对象时使用。</p>
</li>
<li>
<p><strong><font color="#f8070d" size=3>metadata</font></strong>: 元数据，内部嵌套很多2级、3级字段。主要提供以下几个字段。</p>
<ul>
<li>
<p><strong><font color="#0215cd" size=3>name</font></strong>，在同一类别当中name必须是唯一的。</p>
</li>
<li>
<p><strong><font color="#0215cd" size=3>namespace</font></strong> 对应的对象属于哪个名称空间，name受限于namespace，不同的namespace中name可以重名。</p>
</li>
<li>
<p><strong><font color="#0215cd" size=3>lables</font></strong> key-value数据，对于key名称及value，最多为63个字符，value，可为空。填写时只能使用<font color="#f8070d" size=2><code>字母</code></font>、<font color="#f8070d" size=2>数字</font>、<font color="#f8070d" size=3><code>_</code></font>、<font color="#f8070d" size=3><code>-</code></font>、<font color="#f8070d" size=3><code>.</code></font>，只能以字母或数字开头及结尾。</p>
</li>
<li>
<p><strong><font color="#0215cd" size=3>annotations</font></strong> 资源注解。与label不同的地方在于，它不能用于挑选资源对象，仅用于为对象提供“元数据”。对键值长度没有要求。在构建大型镜像时通常会用其标记对应的资源对象的元数据</p>
</li>
</ul>
</li>
<li>
<p><strong><font color="#f8070d" size=3>spec</font></strong>: specification，定义接下来创建的资源对象应该满足的规范（<font color="#f8070d" size=2>期望的状态 disired state</font>）。spec是用户定义的。不同的资源类型，其所需要嵌套的字段各不相同。如果某一字段属性标记为required表示为必选字段，剩余的都为可选字段，系统会赋予其默认值。如果某一字段标记为Cannot be updated，则表示为对象一旦创建后不能改变字段值。可使用<font color="#f8070d" size=3><code>kubectl explain pods.spec</code></font>查看详情。</p>
<ul>
<li>
<p><strong><font color="#0215cd" size=3>containers</font></strong> [required]object list</p>
<ul>
<li>
<p><strong><font color="#ffc104" size=3>name</font></strong>  [string] 定义容器名称</p>
</li>
<li>
<p><strong><font color="#ffc104" size=3>image</font></strong> [string] 启动Pod内嵌容器时所使用的镜像。可是顶级、私有、第三方仓库镜像。</p>
</li>
<li>
<p><strong><font color="#ffc104" size=3>imagePulLPolicy</font></strong> [string] 镜像获取的策略，可选参数Always（总是从仓库下载，无论本地有无此镜像）、Never（从不下载，无论本地有无此镜像）、IfNotPresent（本地存在则使用，不存在则从仓库拉去镜像）。如果tag设置为latest，默认值则为<font color="#f8070d" size=3><code>Always</code></font>，非latest标签，默认值都为<font color="#f8070d" size=3><code>IfNotPresent</code></font>。</p>
</li>
</ul>
<ul>
<li><strong><font color="#ffc104" size=3>ports</font></strong> []object 定义容器内要暴露的端口时，可以暴露多个端口，每个端口应该由多个属性来定义（端口名称、端口号、协议）。<font color="#f8070d" size=2>注意暴露端口仅仅是提供额外信息的，并不能限制系统是否真能暴露</font>。</li>
</ul>
<ul>
<li>
<p><strong><font color="#ffc104" size=3>command</font></strong> Entrypoint array，运行的程序</p>
</li>
<li>
<p><strong><font color="#ffc104" size=3>args</font></strong> 向Entrypoint传递参数，官方对command和args的对比说明<a href="https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/" target="_blank"
   rel="noopener nofollow noreferrer" >Define a Command and Arguments for a Container - Kubernetes</a></p>
</li>
</ul>
</li>
</ul>
<ul>
<li>
<p><strong><font color="#0215cd" size=3>nodeSelector</font></strong> map[string]string 节点标签选择器，确定Pod只运行在哪个或哪类节点上。</p>
</li>
<li>
<p><strong><font color="#0215cd" size=3>livenessProbe</font></strong> [object] 存活性验证</p>
<ul>
<li><strong><font color="#ffc104" size=3>exec</font></strong> 执行容器中存在的用户自定义命令。
<ul>
<li>command []string 运行命令来探测是否执行成功。</li>
</ul>
</li>
</ul>
<ul>
<li><strong><font color="#ffc104" size=3>httpGet</font></strong></li>
<li><strong><font color="#ffc104" size=3>tcpSocket</font></strong></li>
<li><strong><font color="#ffc104" size=3>failureThreshold</font></strong> 确定失败的探测的失败次数，默认值3，最小值1。</li>
<li><strong><font color="#ffc104" size=3>periodSeconds</font></strong> 周期间隔时长。默认10秒。</li>
<li><strong><font color="#ffc104" size=3>timeoutSeconds</font></strong> 超时时长，默认1秒。</li>
<li><strong><font color="#ffc104" size=3>initialDelaySeconds</font></strong> [integer] 初始化延迟探测时间，默认容器启动时立刻探测。</li>
</ul>
</li>
<li>
<p><strong><font color="#0215cd" size=3>readinessProbe</font></strong> 就绪性探测</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong><font color="#f8070d" size=3>status</font></strong>: 资源的当前状态 <font color="#f8070d" size=2>current state</font>。Kubernetes用于确保每一个资源定义完后，让其当前状态无限向目标状态转移，从而满足用户期望。从此角度来看，status是只读的，有Kubernets集群自行维护。</li>
</ul>
<p>Kubernetes内嵌格式说明</p>
<h3 id="获取pod资源的配置清单帮助">获取pod资源的配置清单帮助</h3>
<blockquote>
<p><strong>语法格式</strong></p>
</blockquote>
<pre><code class="language-sh">kubectl explain pod.lev1.lev2...
</code></pre>
<blockquote>
<p><strong>示例</strong></p>
</blockquote>
<pre><code class="language-sh">$ kubectl explain pod.spec
KIND:     Pod
VERSION:  v1

RESOURCE: spec {Object}

DESCRIPTION:
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status

     PodSpec is a description of a pod.

FIELDS:
   activeDeadlineSeconds	{integer}
     Optional duration in seconds the pod may be active on the node relative to
     StartTime before the system will actively try to mark it failed and kill
     associated containers. Value must be a positive integer.

   affinity	{Object}
     If specified, the pod's scheduling constraints

   automountServiceAccountToken	{boolean}
     AutomountServiceAccountToken indicates whether a service account token
     should be automatically mounted.

   containers	{[]Object} -required-
     List of containers belonging to the pod. Containers cannot currently be
     added or removed. There must be at least one container in a Pod. Cannot be
     updated.
     
   hostname	{string}
     Specifies the hostname of the Pod If not specified, the pod's hostname will
     be set to a system-defined value.

   imagePullSecrets	{[]Object}
    
    ....
    ....
    ....    

   volumes	{[]Object}
     List of volumes that can be mounted by containers belonging to the pod.
     More info: https://kubernetes.io/docs/concepts/storage/volumes

</code></pre>
<h2 id="资源清单定义">资源清单定义</h2>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  namespace: default
  labels:
    app: myapp-redis
    tier: frontend
spec:
  containers:
  - name: redis-app
    image: redis
    imagePullPolicy: IfNotPresent
    ports:
    - name: redis
      containerPort: 6379
  - name: busybox
    image: busybox
    command:
    - &quot;/bin/sh&quot;
    - &quot;-c&quot;
    - &quot;sleep 3600&quot;
</code></pre>
<blockquote>
<p><strong>从yaml文件加载创建资源</strong></p>
</blockquote>
<pre><code class="language-sh">kubectl create -f pod.yaml 
</code></pre>
<blockquote>
<p><strong>从yaml文件加载删除资源</strong></p>
</blockquote>
<pre><code class="language-sh">kubectl delete -f pod.yaml 
</code></pre>
<h2 id="标签选择器的使用">标签选择器的使用</h2>
<p>Label是Kubernetes中极具特色的功能之一，是附加在对象之上的<font color="#f8070d" size=2>键值对</font>，每一个资源可存在多个标签，每一个标签都是一组键值对。每一个标签都可以被标签选择器进行匹配度检查，从而完成资源挑选。Label既可以在对象创建时指定，可以在资源创建啊之后使用命令来管理（添加、修改、删除）。</p>
<p>Label可基于简单且直接的标准将Pod多个较小的分组。而service也需要识别标签对其识别并管控，或关联到的资源。最资源设定标签后，还可以使用标签来查看、删除等对其执行相应管理操作。</p>
<hr>
<p><strong><font color="#0215cd" size=2><font color="#f8070d" size=2>⚠</font> 注意：在定义标签时，资源标签其标签名称key及value的值必须小于等于63个字符，value，可为空。填写时只能使用<font color="#f8070d" size=2><code>字母</code></font>、<font color="#f8070d" size=2>数字</font>、<font color="#f8070d" size=3><code>_</code></font>、<font color="#f8070d" size=3><code>-</code></font>、<font color="#f8070d" size=3><code>.</code></font>，只能以字母或数字开头及结尾。
</font></strong></p>
<hr>
<p>在定义键名时也可以使用键前缀(DNS域名)，加前缀总长度不能超过253个字符。</p>
<h3 id="对标签进行过滤">对标签进行过滤</h3>
<p><code>kubectl get pods</code>常用参数说明</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>-l</td>
<td>大S</td>
</tr>
<tr>
<td>-L，label-columns=[]</td>
<td>接受以逗号分隔的标签列表，这些标签将作为列显示。名字区分大小写。</td>
</tr>
<tr>
<td>&ndash;show-labels</td>
<td>在最后一列打印标签。</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>&ndash;show-labels 显示Pod标签</strong></p>
</blockquote>
<pre><code class="language-sh">$ kubectl get pods --show-labels
NAME                     READY   STATUS    RESTARTS   AGE   LABELS
nginx-7cdbd8cdc9-8blzc   1/1     Running   0          39s   pod-template-hash=7cdbd8cdc9,run=nginx
test-pod                 2/2     Running   0          17h   app=myapp-redis,tier=frontend
</code></pre>
<blockquote>
<p><strong>-L 获取显示指定类别的资源对象时，对每个资源对象显示其标签值。</strong></p>
</blockquote>
<pre><code class="language-sh">$ kubectl get pods --show-labels -L=run,app
NAME                     READY   STATUS    RESTARTS   AGE   RUN     APP           LABELS
nginx-7cdbd8cdc9-8blzc   1/1     Running   0          11m   nginx                 pod-template-hash=7cdbd8cdc9,run=nginx
test-pod                 2/2     Running   0          17h           myapp-redis   app=myapp-redis,tier=frontend
</code></pre>
<blockquote>
<p><strong>-l 标签过滤</strong></p>
</blockquote>
<pre><code class="language-sh">$ kubectl get pods --show-labels
NAME                     READY   STATUS    RESTARTS   AGE   LABELS
nginx-7cdbd8cdc9-8blzc   1/1     Running   0          14m   pod-template-hash=7cdbd8cdc9,run=nginx
test-pod                 2/2     Running   0          17h   app=myapp-redis,tier=frontend

$ kubectl get pods --show-labels -l app
NAME       READY   STATUS    RESTARTS   AGE   LABELS
test-pod   2/2     Running   0          17h   app=myapp-redis,tier=frontend
</code></pre>
<h3 id="资源对象打标签">资源对象打标签</h3>
<blockquote>
<p><strong>kubectl label语法</strong></p>
</blockquote>
<pre><code class="language-sh">kubectl label -f filename|typename key1=value1 ... keyn valuen
</code></pre>
<p>对已有Pod添加标签</p>
<pre><code class="language-sh">$ kubectl label pod nginx-7cdbd8cdc9-8blzc app=nginx-demo
pod/nginx-7cdbd8cdc9-8blzc labeled
</code></pre>
<p>修改已有Label值得Pod，需要使用 <font color="#f8070d" size=3><code>--overwrite</code></font></p>
<pre><code class="language-sh">$ kubectl label pod nginx-7cdbd8cdc9-8blzc app=nginx-demo
error: 'app' already has a value (nginx-demo), and --overwrite is false

$ kubectl label pod nginx-7cdbd8cdc9-8blzc app=nginx-demo1 --overwrite
pod/nginx-7cdbd8cdc9-8blzc labeled
</code></pre>
<h3 id="使用复杂格式标签选择器">使用复杂格式标签选择器</h3>
<p>标签选择器</p>
<p>Kubernetes支持的标签选择器有两类，第一类是<font color="#f8070d" size=2>基于等值关系</font>的标签选择器，另一类是<font color="#f8070d" size=2>基于集合关系</font>的标签选择器。</p>
<blockquote>
<p><strong>基于等值关系的选择器</strong></p>
</blockquote>
<p>基于等值关系的标签选择器的操作费无非就是<font color="#f8070d" size=2>等值关系</font>判断的的符号，如：<font color="#f8070d" size=3><code>=</code></font>  <font color="#f807	0d" size=3><code>==</code></font>  <font color="#f8070d" size=3><code>!=</code></font></p>
<pre><code class="language-sh">$ kubectl get pods --show-labels -l app=nginx-demo1
NAME                     READY   STATUS    RESTARTS   AGE   LABELS
nginx-7cdbd8cdc9-8blzc   1/1     Running   0          6h    app=nginx-demo1,pod-template-hash=7cdbd8cdc9,run=nginx

$ kubectl get pods --show-labels -l app,tier
NAME       READY   STATUS    RESTARTS   AGE   LABELS
test-pod   2/2     Running   5          23h   app=myapp-redis,tier=frontend

$ kubectl get pods --show-labels -l app=myapp-redis,tier=frontend
NAME       READY   STATUS    RESTARTS   AGE   LABELS
test-pod   2/2     Running   5          23h   app=myapp-redis,tier=frontend
</code></pre>
<blockquote>
<p><strong>基于集合关系的标签选择器。于集合关系的标签选择器。</strong></p>
</blockquote>
<p>基于集合关系就是如下几种类型来进行判断</p>
<ul>
<li>key in(value1,value2..,valueN)</li>
<li>key notin(value1,value2,..valueN) 不具有此键也表示符合条件。</li>
<li>key</li>
<li>!key 不存在此键的资源</li>
</ul>
<pre><code class="language-sh">$ kubectl get pods --show-labels -l &quot;app in (nginx-demo1,redus)&quot;
NAME                     READY   STATUS    RESTARTS   AGE     LABELS
nginx-7cdbd8cdc9-8blzc   1/1     Running   0          6h15m   app=nginx-demo1,pod-template-hash=7cdbd8cdc9,run=nginx
$ kubectl get pods --show-labels -l &quot;app notin (nginx-demo1,redus)&quot;
NAME       READY   STATUS    RESTARTS   AGE   LABELS
test-pod   2/2     Running   6          23h   app=myapp-redis,tier=frontend

</code></pre>
<p>可以使用标签的不止是Pod，各种对象都可以打标签，包括Node。当节点有标签后，在添加资源时，就可以让资源对节点有倾向性。</p>
<pre><code class="language-sh">$ kubectl get nodes --show-labels
NAME              STATUS   ROLES    AGE    VERSION   LABELS
node02.k8s.test   Ready    [none]   3d1h   v1.13.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node02.k8s.test
node03.k8s.test   Ready    [none]   3d1h   v1.13.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node03.k8s.test
</code></pre>
<h2 id="pod的生命周期">Pod的生命周期</h2>
<p>常见的Pod状态：</p>
<p>Pending 挂起，请求创建Pod时，发现条件不满足，调度尚未完成
running
Failed
Success
Unkown，未知状态。Pod的状态是Apiserver与运行Pod的节点上的Kubelet通信获取状态信息的。当Node节点上Kubelet进程发生故障。Apiserver无法获取Pod信息。</p>
<p>Pod的创建过程：</p>
<p>用户创建Pod时，将请求提交给Apiserver，Apiserver将创建请求的目标状态保存在etcd中，而后apiserver请求scheduler进行调度（负责挑选出合适的节点来运行Pod）。并将调度结果保存至etcd的Pod资源信息中。随后目标节点kubelet通过apiserver的状态变化，拿到用户所提交的创建清单。根据清单在当前节点上创建并运行Pod，并将当前结果状态发送给apiserver，由apiserver将此状态信息存至etcd中。</p>
<p>restartPolicy: 重启策略
Always 总是重启, OnFailure 只有其状态为错误时重启，正常终止时不重启, Never 从不重启. Default to Always.</p>
<p>容器的重启策略</p>
<p>Pod在被调度至某一节点之上时，只要此节点存在，Pod不会被重新调度，只会重启。除非Pod删除或Pod存在节点故障才会被重新调度。</p>
<p>Pod的终止过程</p>
<p>在kubenetes集群中，Pod代表在Kubernetes集群节点上运行的程序或进程，是向用户提供服务的主要单位。当在提交删除一个Pod时，不会直接kill删除的，而是向Pod内的每一个容器发送TEAM终止信号，使Pod中容器正常终止。终止默认有30秒宽限期。宽限期结束，依然无法终止，会重新发送kill信号，强行进行终止。</p>
<h2 id="kubernetes中的探测方式">kubernetes中的探测方式</h2>
<p>所谓的容器探测无非就是，在容器中设置一些探针或传感器来获取相应的数据。作为其存活与否、就绪与否的标准。目前来讲Kubernetes所支持的存货性探测方式和就绪行探测方式都是一样的。</p>
<p>Kubernetes中的探针类型有三种<font color="#f8070d" size=3><code>ExecAction</code></font>，TCP套接字探针 <font color="#f8070d" size=3><code>TCPSocketAction</code></font> ，<font color="#f8070d" size=3><code>HTTPGetAction</code></font>，使用<font color="#f8070d" size=3><code>kubectl explain pod.containers.xxx</code></font>获取帮助信息。</p>
<h3 id="livenessprobe实例">livenessProbe实例</h3>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata: 
  name: liveness-pod
  namespace: default
spec:
  containers:
  - name: liveness-container
    image: busybox
    imagePullPolicy: IfNotPresent
    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;touch /tmp/healthy; sleep 20; rm -fr /tmp/healthy; sleep 3600&quot;]
    livenessProbe:
      exec:
        command: [&quot;test&quot;,&quot;-e&quot;,&quot;/tmp/healthy&quot;]
      initialDelaySeconds: 3
      periodSeconds: 2
      successThreshold: 1
</code></pre>
<p>在创建后使用describe查看错误</p>
<pre><code>$ kubectl describe pod liveness-pod
Name:               liveness-pod
Namespace:          default
Priority:           0
PriorityClassName:  [none]
Node:               node03.k8s.test/10.0.0.17
Start Time:         Thu, 10 Jan 2019 18:36:27 +0800
Labels:             [none]
Annotations:        [none]
Status:             Running
IP:                 10.244.1.9
Containers:
  liveness-container:
    Container ID:  docker://10062f9026968e664fbb128ddb33a14e30efc848e914fe12d769bab9180ab21a
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:7964ad52e396a6e045c39b5a44438424ac52e12e4d5a25d94895f2058cb863a0
    Port:          [none]
    Host Port:     [none]
    Command:
      /bin/sh
      -c
      touch /tmp/healthy; sleep 20; rm -fr /tmp/healthy; sleep 3600
    State:          Running
      Started:      Thu, 10 Jan 2019 18:39:17 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Thu, 10 Jan 2019 18:38:22 +0800
      Finished:     Thu, 10 Jan 2019 18:39:17 +0800
    Ready:          True
    Restart Count:  3
    Liveness:       exec [test -e /tmp/healthy] delay=3s timeout=1s period=2s #success=1 #failure=3
    Environment:    [none]
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ml2gd (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-ml2gd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-ml2gd
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  [none]
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
</code></pre>
<p>可以看到根据定义的检测规则，Pod在不停的重启。</p>
<pre><code>$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
liveness-pod             1/1     Running   8          17m
</code></pre>
<h3 id="readinessprobe就绪性探测实例">readinessProbe，就绪性探测实例</h3>
<blockquote>
<p><strong>编写yaml文件，使用HTTPAction探针进行就绪性探测</strong></p>
</blockquote>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: http-pod
  namespace: default
spec:
  containers:
  - name: http-container
    image: httpd
    imagePullPolicy: IfNotPresent
    ports:
    - name: http
      containerPort: 80
    readinessProbe:
      httpGet:
        port: http
        path: /index.html
      initialDelaySeconds: 2
      periodSeconds: 3
</code></pre>
<blockquote>
<p><strong>使用资源配置清单创建Pod，并查看其状态</strong></p>
</blockquote>
<pre><code>$ kubectl create -f http.yaml 
pod/http-pod created


$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
http-pod                 1/1     Running   0          6s
</code></pre>
<p>查看此Pod的就绪性。</p>
<pre><code>$ kubectl describe pod http-pod
Name:               http-pod
Namespace:          default
Priority:           0
PriorityClassName:  [none]
Node:               node02.k8s.test/10.0.0.16
Start Time:         Fri, 11 Jan 2019 11:34:36 +0800
Labels:             [none]
Annotations:        [none]
Status:             Running
IP:                 10.244.0.18
Containers:
  http-container:
    Container ID:   docker://562525c9498153ed0285d6fdaa03b822efca470194341f12e5f510ae9e93f570
    Image:          httpd
    Image ID:       docker-pullable://httpd@sha256:a613d8f1dbb35b18cdf5a756d2ea0e621aee1c25a6321b4a05e6414fdd3c1ac1
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 11 Jan 2019 11:34:38 +0800
    Ready:          True
    Restart Count:  0
    Readiness:      http-get http://:http/index.html delay=2s timeout=1s period=3s #success=1 #failure=3
    Environment:    [none]
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ml2gd (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-ml2gd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-ml2gd
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  [none]
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                      Message
  ----    ------     ----  ----                      -------
  Normal  Scheduled  11s   default-scheduler         Successfully assigned default/http-pod to node02.k8s.test
  Normal  Pulled     9s    kubelet, node02.k8s.test  Container image &quot;httpd&quot; already present on machine
  Normal  Created    9s    kubelet, node02.k8s.test  Created container
  Normal  Started    9s    kubelet, node02.k8s.test  Started container
</code></pre>
<p>手动接入Pod内，将探测的文件删除。此时查看Pod的就绪性如下。提示404</p>
<pre><code class="language-sh">kubectl exec -it http-pod -- /bin/bash
</code></pre>
<p>此时，Pod就绪的容器量为0个，也就是说，容器中httpd进程正常，但是web页面不存在。</p>
<pre><code>$ kubectl get pods 
NAME                     READY   STATUS    RESTARTS   AGE
http-pod                 0/1     Running   0          3h39m
</code></pre>
<pre><code>$ kubectl describe pod http-pod
Name:               http-pod
Namespace:          default
Priority:           0
PriorityClassName:  [none]
Node:               node02.k8s.test/10.0.0.16
Start Time:         Fri, 11 Jan 2019 11:34:36 +0800
Labels:             [none]
Annotations:        [none]
Status:             Running
IP:                 10.244.0.18
Containers:
  http-container:
    Container ID:   docker://562525c9498153ed0285d6fdaa03b822efca470194341f12e5f510ae9e93f570
    Image:          httpd
    Image ID:       docker-pullable://httpd@sha256:a613d8f1dbb35b18cdf5a756d2ea0e621aee1c25a6321b4a05e6414fdd3c1ac1
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 11 Jan 2019 11:34:38 +0800
    Ready:          False
    Restart Count:  0
    Readiness:      http-get http://:http/index.html delay=2s timeout=1s period=3s #success=1 #failure=3
    Environment:    [none]
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ml2gd (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-ml2gd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-ml2gd
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  [none]
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                From                      Message
  ----     ------     ----               ----                      -------
  Normal   Scheduled  3h39m              default-scheduler         Successfully assigned default/http-pod to node02.k8s.test
  Normal   Pulled     3h39m              kubelet, node02.k8s.test  Container image &quot;httpd&quot; already present on machine
  Normal   Created    3h39m              kubelet, node02.k8s.test  Created container
  Normal   Started    3h39m              kubelet, node02.k8s.test  Started container
  Warning  Unhealthy  0s (x10 over 27s)  kubelet, node02.k8s.test  Readiness probe failed: HTTP probe failed with statuscode: 404
</code></pre>
<p>lifecycle 生命周期，用来定义启动后和终止前钩子</p>
<p>lifecycle</p>
<p>postStart Pod在创建启动之后立即执行的操作。如果执行失败，容器会终止，被重启，重启与否取决于重启策略。</p>
<p>preSTop Pod在终止之前立即被执行的命令。命令执行完毕后，Pod才会被终止。</p>
<hr>
<p><strong><font color="#0215cd" size=2> <font color="#f8070d" size=2>⚠</font> 注意：注意两个command的执行顺序<font color="#f8070d" size=3><code>containers.command</code></font>是定义容器的command，<font color="#f8070d" size=3><code>containers.postStart.exec.command</code></font>是定义容器启动后初始化操作
</font></strong></p>
<hr>
<p>许多资源支持内嵌字段定义其使用的标签选择器： 
matchLabels：直接给定key value service只支持此类
matchExpressions：基于给定的表达式定义使用的标签选择器<code>{key:&quot;KEY&quot;,operator:&quot;OPERATOR&quot;,values:[VAL1,VAL2,...]</code>。</p>
<p>操作符：
In，Notin：values必须为非空列表
Exists，NotExists：values必须为空列表。</p>
<p>Pod控制器都是内嵌Pod模板，</p>
<p>Pod控制器去管理Pod中间层，并确保每一个Pod资源始终处于定义、或所期望的目标状态。当Pod状态出现故障，首先尝试重启容器，</p>
<p>Pod控制器有多种类型</p>
<ul>
<li>ReplicaSet：ReplicaSet被称为新一代的ReplicationController，它的核心作用在于代用户创建指定数量的副本，并确保Pod副本一直处于满足用户期望数量的状态，多退少补。还支持自动扩缩容机制。</li>
</ul>
<p>主要有三个组件组成</p>
<p>用户期望Pod副本数量
标签选择器，以便选定由自己管理控制的Pod副本。
Pod资源模板  通过标签选择器选到的标签副本数量低于指定数量，会使用Pod资源模板完成Pod资源的新建。</p>
<p>帮助用户管理无状态的Pod资源，并确保精确反应用户所定义的目标数量，但是Kubernetes不建议直接使用ReplicaSet</p>
<p>Deployment</p>
<p>Deployment工作于ReplicaSet之上，一个Deployment可以管理多个rs，但是存活的（），通常保留历史版本中的10个。Deployment通过控制ReplicaSet来控制Pod。Deployment除了支持ReplicaSet所支持的功能，还支持滚动更新、回滚等机制，而且还提供了声明式配置的功能。是用来管理无状态应用的目前最佳的Pod控制器。</p>
<ul>
<li>Deployment能提供滚动式自定义、自控制的更新。</li>
<li>Deployment在更新时可控制更新节奏和更新逻辑。</li>
</ul>
<p>声明式配置在创建资源时，可以基于声明逻辑来定义，所有更新的资源可以随时重新进行声明，只要资源支持动态运行时修改，就可以随时改变在apiserver上定义的目标期望状态。</p>
<ul>
<li>Pod副本数量是有可能大于节点数的，并且数量本身彼此间没有任何精确对应关系。</li>
</ul>
<p>DaemonSet</p>
<p>用于确保集群的每一个节点或指定条件的节点上只运行一个特定的Pod副本，通常用于实现系统级的后台任务。</p>
<p>Job 只能执行一次性的作业
CronJob 周期性运行作业，每次运行都有正常退出的时间 不需要持续后台运行。如果前一次任务没有完成，下一次时间点又到了，CronJob还需要处理此类问题</p>
<p>区别</p>
<p>Job和CronJob与DaemonSet和Deployment显著区别就在于，Job和CronJob不需要持续后台运行。</p>
<p>Deployment只能用于管控无状态应用。常用于只关注群体，而不必关注个体的场景。</p>
<p>StatefulSet能够实现管理有状态应用，而且每一个应用，每一个Pod副本都是被单独管理的。他拥有自己的独有标识和独有的数据集，一旦节点发生故障，在加进来之前需要进行初始化操作。</p>
<p>StatefulSet提供了封装控制器，将需要人为手动做的操作、复杂的执行逻辑，定义成脚本，放置在StatefulSet Pod模板的定义当中。每次节点故障，通过脚本可自动恢复状态。</p>
<p>ReplicaSet的使用</p>
<p>ReplicaSet定义方式可以使用<font color="#f8070d" size=3><code>kubectl explain rs</code></font>查看帮助，ReplicaSet使用的apps组中的v1，而不在是core组。内嵌字段与Pod类似，而spec中定义时最核心的只有3个 relicas副本数量、selector 标签选择器 templates Pod模板，对于templates而言，其内部就是Pod模板。</p>
<p>使用ReplicaSet创建Pod</p>
<p>labels中的标签，必须符合selector中的选择标准。否则创建的Pod是无用的，创建Pod都不够relicas定义的数量，它将会永久创建下去。</p>
<p>在Pod模板中起的Pod名称是没有用的，它会自动以控制器的名称后跟一串随机串，来作为Pod名称来创建。</p>
<blockquote>
<p><strong>当Pod控制器数量超出用户期望数量，会随机删除其中一个Pod</strong></p>
</blockquote>
<pre><code>$ kubectl get pods --show-labels
NAME                     READY   STATUS        RESTARTS   AGE    LABELS
rs-5xw7l                 1/1     Terminating   0          40m    app=myapp,release=canary,test=1a
rs-6zx6b                 1/1     Running       0          40m    app=myapp,release=canary,test=1a
test-pod                 2/2     Running       0          46m    app=myapp,release=canary

$ kubectl label pods test-pod release=canary
pod/test-pod labeled

$ kubectl get pods --show-labels
NAME                     READY   STATUS        RESTARTS   AGE    LABELS
rs-5xw7l                 1/1     Terminating   0          40m    app=myapp,release=canary,test=1a
rs-6zx6b                 1/1     Running       0          40m    app=myapp,release=canary,test=1a
test-pod                 2/2     Running       0          46m    app=myapp,release=canary

$ kubectl get pods --show-labels
NAME                     READY   STATUS    RESTARTS   AGE    LABELS
rs-6zx6b                 1/1     Running   0          42m    app=myapp,release=canary,test=1a
test-pod                 2/2     Running   0          47m    app=myapp,release=canary
</code></pre>
<p>ReplicaSet动态规模的扩容、缩容</p>
<p>修改了控制器后，Pod资源并不会随之更改，因为Pod资源足额就不会被重建，只有重建的Pod资源的版本才是新版本的。</p>
<h2 id="pod-preset">Pod Preset</h2>
<p><code>Pod Preset</code> 是一种 API 资源，在 Pod 创建时，用户可以用它将额外的将运行时需求信息注入 Pod内。 使用标签选择算符来指定 Pod Preset 所适用的 Pod。</p>
<h3 id="在集群中启动pod-preset">在集群中启动Pod Preset</h3>
<p>在集群中使用 Pod Preset，必须确保以下几点：</p>
<ul>
<li>需要确保你使用的是<code>kubernetes 1.8</code>版本以上</li>
<li>已启用 API 类型 <code>settings.k8s.io/v1alpha1/podpreset</code></li>
<li>已启用准入控制器 <code>PodPreset</code></li>
</ul>
<p>apiserver添加参数 <code>--enable-admission-plugins</code>  <code>--runtime-config </code></p>
<pre><code class="language-bash">--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset \
--runtime-config=settings.k8s.io/v1alpha1=true
</code></pre>
<p>创建一个PodPreset</p>
<pre><code class="language-yaml">apiVersion: settings.k8s.io/v1alpha1
kind: PodPreset
metadata:
  name: time-preset
  namespace: default
spec:
  selector:
    matchLabels:
  volumeMounts:
    - mountPath: /etc/localtime
      name: time
  volumes:
    - name: time
      hostPath:
        path: /etc/localtime
  env:
    - name: ENVOY_END
      value: envoy-1.15
</code></pre>
<h3 id="特定pod禁用pod-preset">特定Pod禁用Pod Preset</h3>
<p>在 Pod 的 <code>.spec</code> 中添加形如 <code>podpreset.admission.kubernetes.io/exclude: &quot;true&quot;</code> 的注解</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: podpreset-deply
  labels:
    app: podpreset-deply

spec:
  replicas: 1
  selector:
    matchLabels:
      app: podpreset-deply
  template:
    metadata:
      name: podpreset-deply
      labels:
        app: podpreset-deply
      annotations:
        podpreset.admission.kubernetes.io/exclude: &quot;true&quot;
    spec:
      containers:
        - name: envoy-end
          image: sealloong/envoy-end
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
</code></pre>
<blockquote>
<p>Reference</p>
<p><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.7/html/developer_guide/dev-guide-pod-presets#sample-pod-spec-exclude-preset" target="_blank"
   rel="noopener nofollow noreferrer" >openshift</a></p>
<p><a href="https://v1-18.docs.kubernetes.io/zh/docs/reference/command-line-tools-reference/kube-apiserver/" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes-pod preset</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - kubernetes调度</title>
      <link>https://www.oomkill.com/2018/09/kubernetes-schedule/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/09/kubernetes-schedule/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p><code>kube-scheduler</code> 是kubernetes控制平面的核心组件，其默认行为是将 pod 分配给节点，同时平衡Pod与Node间的资源利用率。通俗来讲就是 <code>kube-scheduler</code> 在运行在控制平面，并将工作负载分配给 Kubernetes 集群。</p>
<p>本文将深入 Kubernetes 调度的使用，包含：”一般调度”，”亲和度“，“污点与容忍的调度驱逐”。最后会分析下 <strong>Scheduler Performance Tuning</strong>，即微调scheduler的参数来适应集群。</p>
<h2 id="简单的调度">简单的调度</h2>
<h3 id="nodename-supa-href11asup">NodeName <sup><a href="#1">[1]</a></sup></h3>
<p>最简单的调度可以指定一个 <strong>NodeName</strong> 字段，使Pod可以运行在对应的节点上。如下列资源清单所示</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: netpod
spec:
  containers:
  - name: netbox
    image: cylonchau/netbox
  nodeName: node01  
</code></pre>
<p>通过上面的资源清单Pod最终会在 node01上运行。这种情况下也会存在很多的弊端，如资源节点不足，未知的nodename都会影响到Pod的正常工作，通常情况下，这种方式是不推荐的。</p>
<pre><code class="language-bash">$ kubectl describe pods netpod 
Name:         netpod
Namespace:    default

	...

QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason   Age   From     Message
  ----    ------   ----  ----     -------
  Normal  Pulling  86s   kubelet  Pulling image &quot;cylonchau/netbox&quot;
  Normal  Pulled   17s   kubelet  Successfully pulled image &quot;cylonchau/netbox&quot;
  Normal  Created  17s   kubelet  Created container netbox
  Normal  Started  17s   kubelet  Started container netbox



$ kubectl get pods netpod  -o wide
NAME     READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
netpod   1/1     Running   0          48m   192.168.0.3   node01   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>通过上面的输出可以确定，通过 <code>NodeName</code> 方式是不经过 <em>scheduler</em> 调度的</p>
<h3 id="nodeselector--supa-href22asup">nodeSelector  <sup><a href="#2">[2]</a></sup></h3>
<p><code>label</code> 是 kubernetes中一个很重要的概念，通常情况下，每一个工作节点都被赋予多组 <em>label</em> ,可以通过命令查看对应的 <em>label</em> 。</p>
<pre><code class="language-bash">$ kubectl get node node01 --show-labels
NAME     STATUS   ROLES    AGE   VERSION    LABELS
node01   Ready    &lt;none&gt;   15h   v1.18.20   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux
</code></pre>
<p>而 <code>nodeSelector</code> 就是根据这些 <em>label</em> ，来选择具有特定一个或多个标签的节点。例如，如果需要在一组特定的节点上运行pod，可以设置在 “<em>PodSpec</em>” 中定义<code>nodeSelector</code> 为一组键值对：</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: netpod-nodeselector
spec:
  selector:
    matchLabels:
      app: netpod
  replicas: 2 
  template:
    metadata:
      labels:
        app: netpod
    spec:
      containers:
      - name: netbox
        image: cylonchau/netbox
      nodeSelector: 
        beta.kubernetes.io/os: linux
</code></pre>
<p>对于上面的pod来讲，Kubernetes Scheduler 会找到带有 <code>beta.kubernetes.io/os: linux</code>标签的节点。对于更多kubernetes内置的标签，可以参考  <sup><a href="#3">[3]</a></sup></p>
<p>对于标签选择器来说，最终会分布在具有标签的节点上</p>
<pre><code class="language-bash">kubectl describe pod netpod-nodeselector-69fdb567d8-lcnv6 
Name:         netpod-nodeselector-69fdb567d8-lcnv6
Namespace:    default

	...

QoS Class:       BestEffort
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  8m18s  default-scheduler  Successfully assigned default/netpod-nodeselector-69fdb567d8-lcnv6 to node01
  Normal  Pulling    8m17s  kubelet            Pulling image &quot;cylonchau/netbox&quot;
  Normal  Pulled     7m25s  kubelet            Successfully pulled image &quot;cylonchau/netbox&quot;
  Normal  Created    7m25s  kubelet            Created container netbox
  Normal  Started    7m25s  kubelet            Started container netbox
</code></pre>
<h2 id="节点亲和性-supa-href44asup">节点亲和性 <sup><a href="#4">[4]</a></sup></h2>
<p>对于使用了调度功能的系统来说，亲和度 （<code>Affinity</code>）是个很常见的概念，通常亲和度发生在并行（<code>parallel </code>）环境中；在这种环境下，亲和度提供了在一个节点上运行pod可能比在其他节点上运行更有效，而计算亲和度通常由多种条件组成。一般情况下，亲和度分为“<strong>软亲和</strong>与<strong>硬亲和</strong></p>
<ul>
<li>软亲和，<strong>Soft Affinity</strong>，是调度器尽可能将任务保持在同一个节点上。这只是一种尝试；如果不可行，则将进程迁移到另一个节点</li>
<li>硬亲和，<strong>Hard affinity</strong>，硬亲和度是强行将任务绑定到指定的节点上</li>
</ul>
<p>而在kubernetes中也支持亲和度的概念，而亲和度是与 <em>nodeSelector</em> 配合形成的一个算法。其中硬亲和被定义为<code>requiredDuringSchedulingIgnoredDuringExecution</code>；软亲和被定义为 <code>preferredDuringSchedulingIgnoredDuringExecution</code></p>
<ul>
<li>硬亲和性（<code>requiredDuringSchedulingIgnoredDuringExecution</code>）：必须满足条件，否则调度程序无法调度 Pod。</li>
<li>软亲和性 （<code>preferredDuringSchedulingIgnoredDuringExecution</code>）：<em>scheduler</em> 将查找符合条件的节点。如果没有满足要求的节点将忽略这条规则，<em>scheduler</em> 将仍会调度 Pod。</li>
</ul>
<h3 id="node-affinity">Node Affinity</h3>
<h4 id="node-affinity参数说明">Node Affinity参数说明</h4>
<p>调度程序会更倾向于将 pod 调度到满足该字段指定的亲和性表达式的节点，但它可能会选择违反一个或多个表达式的节点。最优选的节点是权重总和最大的节点，即对于满足所有调度要求（资源请求、requiredDuringScheduling 亲和表达式等）的每个节点，通过迭代该字段的元素来计算总和如果节点匹配相应的matchExpressions，则将“权重”添加到总和中；具有最高和的节点是最优选的。</p>
<p>如果在调度时不满足该字段指定的亲和性要求，则不会将 Pod 调度到该节点上。如果在 pod 执行期间的某个时间点不再满足此字段指定的亲和性要求（例如，由于更新），系统可能会或可能不会尝试最终将 pod 从其节点中逐出。</p>
<p>affinity 范围应用于 <code>Pod.Spec</code> 下，参数如下：</p>
<ul>
<li><strong><code>nodeAffinity</code></strong>：node亲和度相关根配置
<ul>
<li><strong><code>preferredDuringSchedulingIgnoredDuringExecution</code></strong>：软亲和
<ul>
<li><strong><code>preference</code></strong> (<em>required</em>)：选择器
<ul>
<li><strong><code>matchExpressions</code></strong>：匹配表达式，标签可以指定部分
<ul>
<li><strong><code>key</code></strong> (<em>&lt;string&gt; -required-</em>)：</li>
<li><strong><code>operator</code></strong> (<em>&lt;string&gt; -required-</em>)：# 与一组 key-values的运算方式。
<ul>
<li>In, NotIn, Exists, DoesNotExist, Gt, Lt。</li>
</ul>
</li>
<li><strong><code>values</code></strong> (<em>&lt;[]string&gt;</em>)：</li>
</ul>
</li>
<li><strong><code>matchFields</code></strong>：  匹配字段
<ul>
<li><strong><code>key</code></strong> (<em>&lt;string&gt; -required-</em>)：</li>
<li><strong><code>operator</code></strong> (<em>&lt;string&gt; -required-</em>)：# 与一组 key-values的运算方式。
<ul>
<li>In, NotIn, Exists, DoesNotExist, Gt, Lt。</li>
</ul>
</li>
<li><strong><code>values</code></strong> (<em>&lt;[]string&gt;</em>)：</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>weight</code></strong> (<em>required</em>)：范围为 1-100，具有最高和的节点是最优选的</li>
</ul>
</li>
<li><strong><code>requiredDuringSchedulingIgnoredDuringExecution</code></strong>：硬亲和
<ul>
<li><strong><code>nodeSelectorTerms</code></strong>
<ul>
<li><strong><code>matchExpressions</code></strong>：
<ul>
<li><strong><code>key</code></strong> (<em>&lt;string&gt; -required-</em>)：</li>
<li><strong><code>operator</code></strong> (<em>&lt;string&gt; -required-</em>)：# 与一组 key-values的运算方式。
<ul>
<li>In, NotIn, Exists, DoesNotExist, Gt, Lt。</li>
</ul>
</li>
<li><strong><code>values</code></strong> (<em>&lt;[]string&gt;</em>)：</li>
</ul>
</li>
<li><strong><code>matchFields</code></strong>：
<ul>
<li><strong><code>key</code></strong> (<em>&lt;string&gt; -required-</em>)：</li>
<li><strong><code>operator</code></strong> (<em>&lt;string&gt; -required-</em>)：一组 key-values 的运算方式。
<ul>
<li>In, NotIn, Exists, DoesNotExist, Gt, Lt。</li>
</ul>
</li>
<li><strong><code>values</code></strong> (<em>&lt;[]string&gt;</em>)：</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>Notes: matchFields使用的是资源清单的字段（kubectl get node -o yaml），而matchExpressions匹配的是标签</p>
</blockquote>
<h4 id="node-affinity示例">Node Affinity示例</h4>
<p>上面的介绍了解到了Kubernetes中相对与 <code>nodeSelector</code>可以更好表达复杂的调度需求：<strong>节点亲和性</strong>，使用PodSpec中的字段 <code>.spec.affinity.nodeAffinity</code>  指定相关 <em>affinity</em> 配置。</p>
<pre><code class="language-YAML">apiVersion: apps/v1
kind: Deployment
metadata:
  name: netpod-nodeselector
spec:
  selector:
    matchLabels:
      app: netpod
  replicas: 2 
  template:
    metadata:
      labels:
        app: netpod
    spec:
      containers:
      - name: netbox
        image: cylonchau/netbox
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: app
                operator: In
                values:
                - test
</code></pre>
<p>上面的清单表明，当节点存在 <code>app: test</code> 标签时，会调度到对应的Node上，如果没有节点匹配这些条件也不要紧，会根据普通匹配进行调度。</p>
<p>当硬策略和软策略同时存在时的情况，根据设置的不同，硬策略优先级会高于软策略，哪怕软策略权重设置为100</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: netpod-nodeselector
spec:
  selector:
    matchLabels:
      app: netpod
  replicas: 2 
  template:
    metadata:
      labels:
        app: netpod
    spec:
      containers:
      - name: netbox
        image: cylonchau/netbox
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: app
                operator: In 
                values:
                - test
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - antarctica-east1
                - antarctica-west1
</code></pre>
<p>下面是报错信息</p>
<pre><code class="language-bash">Warning  FailedScheduling  4s (x3 over 24s)  default-scheduler  0/2 nodes are available: 2 node(s) didn't match node selector.
</code></pre>
<h2 id="pod亲和性-supa-href44asup">Pod亲和性 <sup><a href="#4">[4]</a></sup></h2>
<p>pod亲和性和反亲和性是指根据节点上已运行的Pod的标签而不是Node标签来限制Pod可以在哪些节点上调度。例如：<em>X</em> 满足一个或多个运行 <em>Y</em> 的条件，这个时候 Pod满足在X中运行。其中 <em>X</em> 为拓扑域，<em>Y</em> 则是规则。</p>
<blockquote>
<p>Notes：官方文档中不推荐pod亲和度在超过百个节点的集群中使用该功能 <sup><a href="#5">[5]</a></sup></p>
</blockquote>
<h3 id="pod亲和性配置">Pod亲和性配置</h3>
<p>Pod亲和性和反亲和性与Node亲和性类似，affinity 范围应用于 <code>Pod.Spec.podAffinity</code>  下，这里不做重复复述，可以参考Node亲和性参数说明部分。</p>
<p>topologyKey，==不允许是空值==，该值将影响Pod部署的位置，影响范围为，与亲和性条件匹配的对应的节点中的什么拓扑，topologyKey的拓扑域由label标签决定。</p>
<p>除了 <code>topologyKey</code> 之外，还有标签选择器 <code>labelSelector</code>  与 名称空间 <code>namespaces</code> 可以作为同级的替代选项</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: netpod-podaffinity
spec:
  selector:
    matchLabels:
      app: podaffinity
  replicas: 1 
  template:
    metadata:
      labels:
        app: podaffinity
    spec:
      containers:
      - name: podaffinity
        image: cylonchau/netbox
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - netpod
            topologyKey: zone
</code></pre>
<p>如果没有Pod匹配到规则，则pending状态</p>
<pre><code class="language-bash">Warning  FailedScheduling  59s (x2 over 59s)  default-scheduler  0/2 nodes are available: 2 node(s) didn't match pod affinity rules, 2 node(s) didn't match pod affinity/anti-affinity.
</code></pre>
<h3 id="pod-anti-affinity">Pod Anti-Affinity</h3>
<p>在某些场景下，部分节点不应该有很多资源，即某些节点不想被调度。例如监控运行节点由于其性质，不希望该节点上有很多资源，或者因节点配置的不同，配置较低节点不希望调度很多资源；在这种情况下，如果将符合预期之外的Pod调度过来会降低其托管业务的性能。这种情况下就需要 <em><strong>反亲和度</strong></em>（<code>Anti-Affinity</code>）来使Pod远离这组节点</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: netpod-podaffinity
spec:
  selector:
    matchLabels:
      app: podaffinity
  replicas: 1 
  template:
    metadata:
      labels:
        app: podaffinity
    spec:
      containers:
      - name: podaffinity
        image: cylonchau/netbox
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - netpod
              topologyKey: zone
</code></pre>
<h2 id="taints-and-tolerations-supa-href66asup">Taints And Tolerations <sup><a href="#6">[6]</a></sup></h2>
<h3 id="taints">Taints</h3>
<p>亲和度和反亲和度虽然可以阻止Pod在特定节点上运行，但还存在一个问题，就是亲和度和反亲和度需要声明运行的节点或者是不想运行的节点，如果忘记声明，还是会被调度到对应的Node上。Kubernetes还提供了一种驱逐Pod的方法，就是污点（<code>Taints</code>）与容忍（<code>Tolerations</code>）。</p>
<p>创建一个污点</p>
<pre><code class="language-bash">kubectl taint nodes node1 key1=value1:NoSchedule

$ kubectl taint nodes mon01 role=monitoring:NoSchedule
</code></pre>
<p>删除一个污点，</p>
<pre><code class="language-bash">kubectl taint nodes node1 key1=value1:NoSchedule-
</code></pre>
<p>除了 <code>NoSchedule</code> ，还有 <code>PreferNoSchedule</code> 与 NoExecute</p>
<ul>
<li><code>PreferNoSchedule</code> ：类似于软亲和性的属性，尽量去避免污点，但不是强制的。</li>
<li><code>NoExecute</code> 表示，当Pod还没在节点上运行时，并且存在至少一个污点时生效，此时Pod不会被调度到该节点；当Pod已经运行在节点上时，并且存在至少一个污点时生效，Pod将会从节点上被驱逐。</li>
</ul>
<h3 id="tolerations">Tolerations</h3>
<p>当Node有污点时，在调度时会自动被排除。当调度在受污染的节点上执行Predicate部分时将失败，而容忍度则是使 pod 具有对该节点上的污点进行容忍，即拥有容忍度的Pod可以调度到有污点的节点之上。</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: netpod-podaffinity
spec:
  selector:
    matchLabels:
      app: podaffinity
  replicas: 1 
  template:
    metadata:
      labels:
        app: podaffinity
    spec:
      containers:
      - name: podaffinity
        image: cylonchau/netbox
      tolerations:
      - key: &quot;role&quot;
        operator: &quot;Equal&quot;
        value: &quot;monitoring&quot;
        effect: &quot;NoSchedule&quot;
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - netpod
              topologyKey: zone
</code></pre>
<p>容忍度中存在一个特殊字段 <code>TolerationSeconds</code> ，表示容忍的时间，默认不会设置，即永远容忍污点。设置成0或者负数表示理解驱逐。==仅在污点为 <code>NoExecute</code> 时生效==</p>
<p><code>operator</code> 属性有两个值 <code>Exists</code> 和 <code>Equal</code></p>
<ul>
<li>
<p>如果 operator 为 <code>Exists</code>，则无需 value 属性，因为判断的是有污点的情况下。</p>
</li>
<li>
<p>如果 operator 为 <code>Equal</code>，则表示 key 与 value 之间的关系是 $key=value$</p>
</li>
<li>
<p>空 key，并且operator为 <code>Exists</code>，将匹配到所有，即容忍所有污点</p>
</li>
<li>
<p>空 <code>effect</code> 匹配所有  <code>effect</code> ，即匹配所有污点；这种情况下加上条件的话，可以容忍所有类型的污点</p>
</li>
</ul>
<h2 id="驱逐-supa-href77asup">驱逐 <sup><a href="#7">[7]</a></sup></h2>
<p>当污点设置为 <code>NoExecute</code>这种情况下会驱逐Pod，驱逐条件又如下几个：</p>
<ul>
<li>不容忍污点的 pod 会立即被驱逐</li>
<li>容忍污点但未配置 <code>tolerationSeconds</code> 属性的会保持不变，即该节点与Pod保持绑定</li>
<li>容忍指定污点的 pod 并且配置了<code>tolerationSeconds</code> 属性，节点与Pod绑定状态仅在配置的时间内。</li>
</ul>
<p>Kubernetes内置了一些污点，此时 <em>Controller</em> 会自动污染节点：</p>
<ul>
<li>
<p><code>node.kubernetes.io/not-ready</code>: Node故障。对应 NodeCondition 的<code>Ready</code> =  <code>False</code>。</p>
</li>
<li>
<p><code>node.kubernetes.io/unreachable</code>：Node控制器无法访问节点。对应 NodeCondition <code>Ready</code>= <code>Unknown</code>。</p>
</li>
<li>
<p><code>node.kubernetes.io/memory-pressure</code>：Node内存压力。</p>
</li>
<li>
<p><code>node.kubernetes.io/disk-pressure</code>：Node磁盘压力。</p>
</li>
<li>
<p><code>node.kubernetes.io/pid-pressure</code>：Node有PID压力。</p>
</li>
<li>
<p><code>node.kubernetes.io/network-unavailable</code>：Node网络不可用。</p>
</li>
<li>
<p><code>node.kubernetes.io/unschedulable</code>：Node不可调度。</p>
</li>
</ul>
<blockquote>
<p>Notes：Kubernetes  <code>node.kubernetes.io/not-ready</code> 属性和 <code>node.kubernetes.io/unreachable</code> 属性添加容差时效 <code>tolerationSeconds=300</code>。即在检测到其中问题后，Pod 将保持绑定5分钟。</p>
</blockquote>
<h2 id="优先级和抢占">优先级和抢占</h2>
<p>kubernetes中也为Pod提供了优先级的机制，有了优先级机制就可以在并行系统中提供抢占机制，有了抢占机制后，当还未调度时，高优先级Pod会比低优先级Pod先被调度，在资源不足时，低优先级Pod可以被高优先级Pod驱逐。</p>
<p>优先级功能由 <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass" target="_blank"
   rel="noopener nofollow noreferrer" >PriorityClasses</a> 提供。<code>PriorityClasses</code> 是作为集群级别资源而不是命名空间级别资源，只是用来声明优先级级别。</p>
<p><code>value</code> 作为优先级级别，数字越大优先级级别越高。而 <code>name</code> 是这个优先级的名称，与其他资源name值相似，值的内容需要符合DNS域名约束。</p>
<p><code>globalDefault</code> 是集群内默认的优先级级别，仅只有一个 <code>PriorityClass</code> 可以设置为 <code>true</code></p>
<pre><code class="language-yaml">apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: &quot;This priority class should be used for XYZ service pods only.&quot;
</code></pre>
<blockquote>
<p>Notes:</p>
<ul>
<li>如果集群内不存在任何 <code>PriorityClass</code> ，则存在的Pod的优先级都为0</li>
<li>当对集群设置了 <code>globalDefault=true</code> 后，不会改变已经存在的 Pod 的优先级。仅对于 <code>PriorityClass</code>   <code>globalDefault=true</code> 后创建的 Pod。</li>
<li>如果删除了 <code>PriorityClass</code> ，存在还是使用的这个 <code>PriorityClass</code> 的Pod保持不变，新创建的Pod无法使用这个 <code>PriorityClass</code> 。</li>
</ul>
</blockquote>
<h3 id="非抢占">非抢占</h3>
<p>当 <code>preemptionPolicy: Never</code> 时，Pod不会抢占其他Pod，但不可调度时，会一直在调度队列中等待调度，直到满足要求才会被调度。==非抢占式pod仍可能被其他高优先级的pod抢占==</p>
<blockquote>
<p>Notes：preemptionPolicy在Kubernetes v1.24 [stable]</p>
</blockquote>
<p><code>preemptionPolicy</code> 是作为非抢占的配置，默认参数为 <code>PreemptLowerPriority</code>；表示了允许高优先级Pod抢占低优先级Pod。如果 <code>preemptionPolicy: Never</code>，代表Pod是非抢占式的。</p>
<p>下列是一个非抢占式的配置样例</p>
<pre><code class="language-yaml">apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority-nonpreempting
value: 1000000
preemptionPolicy: Never
globalDefault: false
description: &quot;This priority class will not cause other pods to be preempted.&quot;
</code></pre>
<p>当配置了优先级后，优先级准入控制器会使用 <code>priorityClassName</code> 中配置的对应的 <code>PriorityClass</code> 的 value值来填充当前Pod的优先级，如果没有找到对应抢占策略，则拒绝。</p>
<p>下面是在Pod中配置优先级的示例</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority
</code></pre>
<h3 id="抢占">抢占</h3>
<p>当创建Pod之后，Pod会进入队列并等待被调度。<em>scheduler</em> 从队列中选择一个 Pod 并尝试将其调度到一个节点上。如果没有找到满足 Pod 的所有指定要求的 Node，则为 <code>Pending</code> 的 Pod 触发抢占。当Pod在找合适的节点时，即试图抢占一个节点，会在这个Node中删除一个或多个优先级低于当前Pod的Pod，使当前Pod能够被调度到对应的Node上。当低优先级Pod被驱逐后，当前Pod可以被调度到该Node，这个过程被称为抢占 <code>preemption</code>。</p>
<p>而提供可驱逐资源的Node成为被提名Node（<code>nominated Node </code>），在当Pod抢占到一个Node时，其 <code>nominatedNodeName</code> 会被标注为这个 Node的名称，当然标注后也不一定，一定是被抢占到这个Node之上，例如，当前Pod在等待驱逐低优先级Pod的过程中，有其他节点变成可用节点 <em>FN</em> 时，这个时候Pod会被抢占到这个节点。</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BC%9A%E8%A2%AB%E8%B0%83%E5%BA%A6%E5%88%B0%E7%89%B9%E5%AE%9A%E8%8A%82%E7%82%B9%E4%B8%8A%E7%9A%84-pod" target="_blank"
   rel="noopener nofollow noreferrer" >创建一个会被调度到特定节点上的 Pod</a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector" target="_blank"
   rel="noopener nofollow noreferrer" >nodeSelector</a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/reference/labels-annotations-taints/" target="_blank"
   rel="noopener nofollow noreferrer" >labels annotations taints</a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity" target="_blank"
   rel="noopener nofollow noreferrer" >affinity and anti-affinity</a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity" target="_blank"
   rel="noopener nofollow noreferrer" >inter pod affinity and anti affinity</a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank"
   rel="noopener nofollow noreferrer" >taint and toleration</a></p>
<p><sup id="7">[7]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions" target="_blank"
   rel="noopener nofollow noreferrer" >evictions</a></p>
<p><sup id="8">[8]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/" target="_blank"
   rel="noopener nofollow noreferrer" >pod priority preemption</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - Service</title>
      <link>https://www.oomkill.com/2018/09/kubernetes-service/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/09/kubernetes-service/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>在Kubernetes集群中，Pod是有生命周期的，为了能够给对应的客户端提供一个固定访问端点，因此在客户端与服务端（Pod之间）添加了一个固定中间层，这个中间层被称之为Service。Service的工作严重依赖于在Kubernetes集群之上，部署的附件Kubernetes DNS服务。较新版本使用的coreDNS，1.11之前使用的KubeDNS。</p>
<ul>
<li>service的名称解析是强依赖于DNS附件的。因此在部署完Kubernetes后，需要部署CoreDNS或KubeDNS。</li>
<li>Kubernetes要想向客户端提供网络功能，依赖于第三方方案，在较新版本中，可通过CNI容器网络插件标准接口，来接入任何遵循插件标准的第三方方案。</li>
</ul>
<p>Service从一定程度上来说，在每个节点之上都工作有一个组件Kube-proxy，Kube-proxy将始终监视apiserver当中，有关service资源的变动状态。此过程是通过Kubernetes中固有的请求方法watch来实现的。一旦有service资源的内容发生变动，kube-proxy都将其转换为当前节点之上的能够实现service资源调度至特定Pod之上的规则。</p>
<h3 id="service实现方式">service实现方式</h3>
<p>在Kubernetes中service的实现方式有三种模型。</p>
<ul>
<li><strong>userspace</strong> 用户空间，可以理解为，用户的请求。 1.1之前包括1.1使用此模型。</li>
</ul>
<p>用户的请求到达当前节点的内核空间的iptables规则（service规则），由service转发至本地监听的某个套接字上的用户空间的kube-proxy，kube-proxy在处理完再转发给service，最终代理至service相关联的各个Pod，实现调度。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025004009491.png" alt="image-20221025004009491" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<ul>
<li><strong>iptables</strong> 1.10-</li>
</ul>
<p>客户端IP请求时，直接请求serviceIP，IP为本地内核空间中的service规则所截取，并直接调度至相关Pod。service工作在内核空间，由iptables直接调度。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025004018408.png" alt="image-20221025004018408" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<ul>
<li><strong>ipvs</strong>  1.11默认使用，如IPVS没有激活，默认降级为iptables</li>
</ul>
<p>客户端请求到达内核空间后，直接由ipvs规则直接调度至Pod网络地址范围内的相关Pod资源。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025004051034.png" alt="image-20221025004051034" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="使用清单创建service资源">使用清单创建service资源</h3>
<p>SVC中的kubernetes service是集群中各Pod需要与Kubernetes集群apiserver联系时需要通过此svc地址联系。这个地址是集群内的apiserver服务地址。</p>
<h4 id="service类型">service类型</h4>
<ul>
<li>
<p><strong>ClusterIP</strong> 默认值，表示分配集群IP地址，仅用于集群内通信。自动分配地址，如需固定，需要指定相应地址，在创建后无法修改。当使用ClusterIP时，只有两个端口有用，<code>port</code>与<code>targetPort</code></p>
</li>
<li>
<p><strong>NodePort</strong>  接入集群外部流量，默认分配的端口是30000~32767</p>
</li>
<li>
<p><strong>LoadBalancer</strong>  表示将Kubernetes部署在虚拟机上，虚拟机是工作在云环境中，云环境支持lbaas（负载均衡及服务的一键调用）。</p>
</li>
<li>
<p><strong>ExternaName</strong> 表示将集群外部服务引用到集群内部中来，在集群内部直接使用。</p>
</li>
</ul>
<pre><code class="language-yaml">spec:
  ports: # 将哪个端口与后端容器端口建立关联关系。
  - port # service对外提供服务的端口
    name 指明port的名称
    targetPort # 容器的端口
    nodePort # 只有类型为NodePort时，才有必要用节点端口，否则此选项是无用的。
    protocol 协议，默认TCP
  seletcor 关联到哪些Pod资源上
    app: redis
    run: redis
  clusterIP: # clusterIP可以动态分贝可以不配置
  type: ClusterIP
</code></pre>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: default
spec:
  selector: 
    run: redis
  clusterIP: 10.96.100.0
  type: ClusterIP
  ports: 
  - port: 6379
    targetPort: 6379
</code></pre>
<p>service到Pod是有一个中间层的，service会先到endpoints资源(==标准的Kubernetes对象==)， 地址加端口，而后由endpoints关联至后端Pod。</p>
<pre><code class="language-sh">$ kubectl describe svc redis
Name:              redis
Namespace:         default
Labels:            &lt;none&gt;
Annotations:       kubectl.kubernetes.io/last-applied-configuration:
                     {&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;redis&quot;,&quot;namespace&quot;:&quot;default&quot;},&quot;spec&quot;:{&quot;clusterIP&quot;:&quot;10.96.100.0&quot;,...
Selector:          run=redis
Type:              ClusterIP
IP:                10.96.100.0
Port:              &lt;unset&gt;  6379/TCP
TargetPort:        6379/TCP
Endpoints:         10.244.0.5:6379,10.244.1.4:6379
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>
<p>service创建完成后，只要kubernetes集群中的DNS是存在的，就可以直接解析其服务名每一个service创建完后，都会在集群DNS中动态添加一个资源记录（不止一个）。</p>
<p>资源记录的默认格式为，==<code>SVC_NAME.NS_NAME.DOMAIN.LTD.</code>==，==<code>DOMAIN.LTD</code>== 默认是 ==<code>svc.cluster.local</code>==。故redis-svc的资源记录为<code>redis.default.svc.cluster.local.</code></p>
<h4 id="nodeport-svc">nodePort SVC</h4>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: redis1
  namespace: default
spec:
  selector: 
    run: redis
  clusterIP: 10.96.100.1
  type: NodePort
  ports: 
  - port: 6380
    targetPort: 6379
    nodePort: 30001
</code></pre>
<h4 id="externalname">ExternalName</h4>
<p>在本地局域网环境中，但是在Kubernetes集群之外，或者在互联网之上的服务，我们==期望此服务让集群内的服务可访问到==，集群内部使用的都是私网地址，就算可以将请求路由出去，离开本地网络到外部，外部的相应报文也无法回到Kubernetes集群内网中。此时无法正常通信。</p>
<p>ExternalName用于实现，在集群中创建service， 此service端点不是本地Pod，而是service关联至外部服务上。当集群内部客户端区访问service时，由service通过层级转换，请求到外部的服务，外部报文响应给NodeIP，再由NodeIP转交至service，再由service转发至Pod，从而使Pod可访问集群外部服务。</p>
<pre><code class="language-sh">$ kubectl explain svc.spec.externalName
KIND:     Service
VERSION:  v1

FIELD:    externalName &lt;string&gt;
</code></pre>
<p>Service在实现负载均衡时，还支持<code>sessionAffinity</code>会话粘性，默认情况下基于源IP做粘性的，<code>ClientIP</code>、<code>None</code>(默认)。</p>
<p>sessionAffinity在service内部实现session保持。支持两种模式 <code>ClusterIP</code> <code>None</code>。设置<code>ClusterIP</code>为，将同一个客户端IP调度到同一个后端Pod。</p>
<h4 id="无头service-headless">无头service (headless)</h4>
<p>在访问service时，解析的应为service名称，每个service有其响应的service名称，解析至其ClusterIP，由service调度（dnat）至后端Pod，因此名称解析结果只会有一个ClusterIP。</p>
<p>所谓headless service，即在解析Service IP时，==无Service ClusterIP，此时，解析服务名时会解析至后端Pod IP之上==，IP数量取决于Pod的数量。这种service被称为<code>headless service</code>。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025004104717.png" alt="image-20221025004104717" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>设置方式</p>
<pre><code class="language-yaml">ClusterIP: none
</code></pre>
<pre><code class="language-sh">$ kubectl get svc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP          15h
nginx        ClusterIP   None          &lt;none&gt;        80/TCP           5s
</code></pre>
<pre><code class="language-sh">$ dig -t A nginx.default.svc.cluster.local. @10.96.0.10

; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-74.el7_6.1 &lt;&lt;&gt;&gt; -t A nginx.default.svc.cluster.local. @10.96.0.10
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 59245
;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;nginx.default.svc.cluster.local. IN    A

;; ANSWER SECTION:
nginx.default.svc.cluster.local. 5 IN   A       10.244.0.6
nginx.default.svc.cluster.local. 5 IN   A       10.244.1.5

;; Query time: 1 msec
;; SERVER: 10.96.0.10#53(10.96.0.10)
;; WHEN: 一 7月 08 13:24:47 CST 2019
;; MSG SIZE  rcvd: 154


$ kubectl get pods -o wide -l run=nginx
NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
nginx-7db9fccd9b-h72bb   1/1     Running   0          14m   10.244.1.5   node01   &lt;none&gt;           &lt;none&gt;
nginx-7db9fccd9b-mrv95   1/1     Running   0          14m   10.244.0.6   node02   &lt;none&gt;           &lt;none&gt;
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - serviceaccount</title>
      <link>https://www.oomkill.com/2018/09/kubernetes-serviceaccount/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/09/kubernetes-serviceaccount/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>在整个Kubernetes集群来讲 apiserver是访问控制的唯一入口。如通过service或ingress暴露之后，是可以不通过apiserver接入的，只需要通过节点的nodePort或者ingress controller daemonset共享宿主机节点网络名称空间监听的宿主机网络地址（节点地址），直接接入。</p>
<p>当请求到达APIServer时，会经历几个阶段，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025003822017.png" alt="image-20221025003822017" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图</center>
<center><em>Source：</em>https://kubevious.io/blog/post/securing-kubernetes-using-pod-security-policy-admission-controller</center>
<p><strong>任何用户（sa与人类用户）在通过任何方式试图操作API资源时，必须要经历下列的操作</strong>：</p>
<ul>
<li><strong>Authentication</strong>，这个步骤在建立TLS连接后，验证包含，证书、密码，Token；可以指定多种认证，依次尝试每一个，直到其中一个认证成功。如果认证失败，此时客户端收到的是401。</li>
<li><strong>Authorization</strong>，此步骤是在完成 <em>Authentication</em> 后确定了来源用户，此时用户的请求动作必须被授权。如bob用户对pod资源有 <code>get</code> , <code>list</code> 权限操作。如果</li>
<li><strong>Admission Control</strong>：此步骤为图3，与 <em>Authorization</em> 不同的时，这里只要有任意准入控制器拒绝，则拒绝；多个准入控制器会按顺序执行</li>
</ul>
<p>Refer to <a href="https://kubernetes.io/docs/concepts/security/controlling-access/" target="_blank"
   rel="noopener nofollow noreferrer" >controlling access</a></p>
<h3 id="认证">认证</h3>
<p>Kubernetes是高度模块化设计的，因此其认证授权与准入控制是各自都通过插件的方式，可由用户自定义选择经由什么样的插件来完成何种控制逻辑。如<strong>对称秘钥认证方式</strong>、<strong>令牌认证</strong>。由于Kubernetes提供的是resetful方式的接口，其服务都是通过HTTP协议提供的，因此认证信息只能经由HTTP协议的认证首部进行传递，此认证首部通常被称作认证令牌(token)。</p>
<p><strong>ssl认证</strong>，对于Kubernetes访问来讲，ssl证书能让客户端去确认服务器的身份，（要求服务端发送服务端证书，确认证书是否为认可的CA签署的。）在Kubernetes通信过程当中，重要的是服务器还需认证客户端的身份，因此==Kubectl也应有一个证书，并且此证书为server端所认可的CA所签署的证书==。并且客户端身份也要与证书当中标识的身份保持一致。双方需互相做双向证书认证。认证之后双方基于SSL会话实现加密通讯。</p>
<hr>
<p>注：kubernetes认证无需执行串行检查，用户经过任何一个认证插件通过后，即表示认证通过，无需再经由其他插件进行检查。</p>
<hr>
<h3 id="授权">授权</h3>
<p>kubernetes的授权也支持多种授权插件来完成用户的权限检查，kubernetes 1.6之后开始支持基于RBAC的认证。除此只外还有基于节点的认证、webhook基于http回调机制，通过web的rest服务来实现认证的检查机制。最重要的是RBAC的授权检查机制。基于角色的访问控制，通常只有许可授权，没有拒绝授权。默认都是拒绝。</p>
<p>在默认情况下，使用kubeadm部署Kubernetes集群是强制启用了RBAC认证的。</p>
<h3 id="准入控制">准入控制</h3>
<p>一般而言，准入控制本身只是用来定义对应授权检查完成之后的后续其他安全检查操作的。</p>
<h3 id="用户账号">用户账号</h3>
<p>一般而言用户账号大体上应具有以下信息</p>
<ul>
<li>
<p><strong>user</strong> 用户，一般而言由<code>username</code>与<code>userid</code>组成。</p>
</li>
<li>
<p><strong>group</strong> 用户组</p>
</li>
<li>
<p><strong>extra</strong> 用来提供额外信息</p>
</li>
<li>
<p><strong>API资源</strong> k8sapiserver是分组的，向哪个组，哪个版本的哪个api资源对象发出请求必须进行标识，所有的请求资源通过url path进行标识的。如 <strong><code>/apis/apps/v1/</code></strong>，所有名称空间级别的资源在访问时一般都需指名namespaces关键词，并给出namespaces名称来获取 <strong><code>/apis/apps/v1/namespaces/default/</code></strong>  <strong><code>/apis/apps/v1/namespaces/default/nginx</code></strong> 。</p>
<p>一个完整意义上的url 对象引用url格式 ==<code>/apis/&lt;GROUPS&gt;/&lt;VERSION&gt;/namespaces/&lt;NameSpace_name&gt;/&lt;Kind&gt;/[/object_id]</code>==</p>
</li>
</ul>
<pre><code class="language-bash">$ kubectl api-versions
admissionregistration.k8s.io/v1beta1
...
</code></pre>
<p>Kubernetes中，所有的api都取决于一个根 <code>/apis</code></p>
<pre><code>$ curl -k --cert /etc/k8s/pki/apiserver-kubelet-client.crt --key /etc/k8s/pki/apiserver-kubelet-client.key  https://localhost:6443/apis/apps/v1/namespaces/typay/deployments/nginx-ingress-controller
{
  &quot;kind&quot;: &quot;Deployment&quot;,
  &quot;apiVersion&quot;: &quot;apps/v1&quot;,
  &quot;metadata&quot;: {
    &quot;name&quot;: &quot;nginx-ingress-controller&quot;,
    &quot;namespace&quot;: &quot;houtu&quot;,
    &quot;selfLink&quot;: &quot;/apis/apps/v1/namespaces/houtu/deployments/nginx-ingress-controller&quot;,
    &quot;uid&quot;: &quot;dc8cbca7-fcab-49c5-a4f4-b44858bbf603&quot;,
    &quot;resourceVersion&quot;: &quot;225525&quot;,
    &quot;generation&quot;: 4,
    &quot;creationTimestamp&quot;: &quot;2019-11-21T12:47:33Z&quot;,
    &quot;labels&quot;: {
      &quot;k8s-app&quot;: &quot;nginx-ingress-controller&quot;
    },
    &quot;annotations&quot;: {
      &quot;deployment.kubernetes.io/revision&quot;: &quot;4&quot;,
      &quot;kubectl.kubernetes.io/last-applied-configuration&quot;: &quot;{\&quot;apiVersion\&quot;:\&quot;extensions/v1beta1\&quot;,\&quot;kind\&quot;:\&quot;Deployment\&quot;,\&quot;metadata\&quot;:{\&quot;annotations\&quot;:{},\&quot;labels\&quot;:{\&quot;k8s-app\&quot;:\&quot;nginx-ingress-controller\&quot;},\&quot;name\&quot;:\&quot;nginx-ingress-controller\&quot;,\&quot;namespace\&quot;:\&quot;houtu\&quot;},\&quot;spec\&quot;:{\&quot;replicas\&quot;:1,\&quot;template\&quot;:{\&quot;metadata\&quot;:{\&quot;labels\&quot;:{\&quot;k8s-app\&quot;:\&quot;nginx-ingress-controller\&quot;}},\&quot;spec\&quot;:{\&quot;containers\&quot;:[{\&quot;args\&quot;:[\&quot;/nginx-ingress-controller\&quot;,\&quot;--default-backend-service=houtu/push-front\&quot;],\&quot;env\&quot;:[{\&quot;name\&quot;:\&quot;POD_NAME\&quot;,\&quot;valueFrom\&quot;:{\&quot;fieldRef\&quot;:{\&quot;fieldPath\&quot;:\&quot;metadata.name\&quot;}}},{\&quot;name\&quot;:\&quot;POD_NAMESPACE\&quot;,\&quot;valueFrom\&quot;:{\&quot;fieldRef\&quot;:{\&quot;fieldPath\&quot;:\&quot;metadata.namespace\&quot;}}}],\&quot;image\&quot;:\&quot;quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1\&quot;,\&quot;livenessProbe\&quot;:{\&quot;httpGet\&quot;:{\&quot;path\&quot;:\&quot;/healthz\&quot;,\&quot;port\&quot;:10254,\&quot;scheme\&quot;:\&quot;HTTP\&quot;},\&quot;initialDelaySeconds\&quot;:10,\&quot;timeoutSeconds\&quot;:1},\&quot;name\&quot;:\&quot;nginx-ingress-controller\&quot;,\&quot;ports\&quot;:[{\&quot;containerPort\&quot;:80,\&quot;hostPort\&quot;:80},{\&quot;containerPort\&quot;:443,\&quot;hostPort\&quot;:443}],\&quot;readinessProbe\&quot;:{\&quot;httpGet\&quot;:{\&quot;path\&quot;:\&quot;/healthz\&quot;,\&quot;port\&quot;:10254,\&quot;scheme\&quot;:\&quot;HTTP\&quot;}}}],\&quot;hostNetwork\&quot;:true,\&quot;serviceAccountName\&quot;:\&quot;nginx-ingress-serviceaccount\&quot;,\&quot;terminationGracePeriodSeconds\&quot;:60}}}}\n&quot;
    }
  },
  &quot;spec&quot;: {
    &quot;replicas&quot;: 1,
    &quot;selector&quot;: {
      &quot;matchLabels&quot;: {
        &quot;k8s-app&quot;: &quot;nginx-ingress-controller&quot;
      }
    },
    &quot;template&quot;: {
      &quot;metadata&quot;: {
        &quot;creationTimestamp&quot;: null,
        &quot;labels&quot;: {
          &quot;k8s-app&quot;: &quot;nginx-ingress-controller&quot;
        }
      },
      &quot;spec&quot;: {
        &quot;containers&quot;: [
          {
            &quot;name&quot;: &quot;nginx-ingress-controller&quot;,
            &quot;image&quot;: &quot;quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1&quot;,
            &quot;args&quot;: [
              &quot;/nginx-ingress-controller&quot;,
              &quot;--default-backend-service=houtu/push-front&quot;
            ],
            &quot;ports&quot;: [
              {
                &quot;hostPort&quot;: 80,
                &quot;containerPort&quot;: 80,
                &quot;protocol&quot;: &quot;TCP&quot;
              },
              {
                &quot;hostPort&quot;: 443,
                &quot;containerPort&quot;: 443,
                &quot;protocol&quot;: &quot;TCP&quot;
              }
            ],
            &quot;env&quot;: [
              {
                &quot;name&quot;: &quot;POD_NAME&quot;,
                &quot;valueFrom&quot;: {
                  &quot;fieldRef&quot;: {
                    &quot;apiVersion&quot;: &quot;v1&quot;,
                    &quot;fieldPath&quot;: &quot;metadata.name&quot;
                  }
                }
              },
              {
                &quot;name&quot;: &quot;POD_NAMESPACE&quot;,
                &quot;valueFrom&quot;: {
                  &quot;fieldRef&quot;: {
                    &quot;apiVersion&quot;: &quot;v1&quot;,
                    &quot;fieldPath&quot;: &quot;metadata.namespace&quot;
                  }
                }
              }
            ],
            &quot;resources&quot;: {
              
            },
            &quot;livenessProbe&quot;: {
              &quot;httpGet&quot;: {
                &quot;path&quot;: &quot;/healthz&quot;,
                &quot;port&quot;: 10254,
                &quot;scheme&quot;: &quot;HTTP&quot;
              },
              &quot;initialDelaySeconds&quot;: 10,
              &quot;timeoutSeconds&quot;: 1,
              &quot;periodSeconds&quot;: 10,
              &quot;successThreshold&quot;: 1,
              &quot;failureThreshold&quot;: 3
            },
            &quot;readinessProbe&quot;: {
              &quot;httpGet&quot;: {
                &quot;path&quot;: &quot;/healthz&quot;,
                &quot;port&quot;: 10254,
                &quot;scheme&quot;: &quot;HTTP&quot;
              },
              &quot;timeoutSeconds&quot;: 1,
              &quot;periodSeconds&quot;: 10,
              &quot;successThreshold&quot;: 1,
              &quot;failureThreshold&quot;: 3
            },
            &quot;terminationMessagePath&quot;: &quot;/dev/termination-log&quot;,
            &quot;terminationMessagePolicy&quot;: &quot;File&quot;,
            &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot;
          }
        ],
        &quot;restartPolicy&quot;: &quot;Always&quot;,
        &quot;terminationGracePeriodSeconds&quot;: 60,
        &quot;dnsPolicy&quot;: &quot;ClusterFirst&quot;,
        &quot;serviceAccountName&quot;: &quot;nginx-ingress-serviceaccount&quot;,
        &quot;serviceAccount&quot;: &quot;nginx-ingress-serviceaccount&quot;,
        &quot;hostNetwork&quot;: true,
        &quot;securityContext&quot;: {
          
        },
        &quot;schedulerName&quot;: &quot;default-scheduler&quot;
      }
    },
    &quot;strategy&quot;: {
      &quot;type&quot;: &quot;RollingUpdate&quot;,
      &quot;rollingUpdate&quot;: {
        &quot;maxUnavailable&quot;: 1,
        &quot;maxSurge&quot;: 1
      }
    },
    &quot;revisionHistoryLimit&quot;: 2147483647,
    &quot;progressDeadlineSeconds&quot;: 2147483647
  },
  &quot;status&quot;: {
    &quot;observedGeneration&quot;: 4,
    &quot;replicas&quot;: 1,
    &quot;updatedReplicas&quot;: 1,
    &quot;unavailableReplicas&quot;: 1,
    &quot;conditions&quot;: [
      {
        &quot;type&quot;: &quot;Available&quot;,
        &quot;status&quot;: &quot;True&quot;,
        &quot;lastUpdateTime&quot;: &quot;2019-11-21T12:47:33Z&quot;,
        &quot;lastTransitionTime&quot;: &quot;2019-11-21T12:47:33Z&quot;,
        &quot;reason&quot;: &quot;MinimumReplicasAvailable&quot;,
        &quot;message&quot;: &quot;Deployment has minimum availability.&quot;
      }
    ]
  }
}
</code></pre>
<pre><code class="language-bash">$ curl -k --cert /etc/k8s/pki/apiserver-kubelet-client.crt --key /etc/k8s/pki/apiserver-kubelet-client.key  https://localhost:6443/api/v1/namespaces/houtu
{
  &quot;kind&quot;: &quot;Namespace&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {
    &quot;name&quot;: &quot;houtu&quot;,
    &quot;selfLink&quot;: &quot;/api/v1/namespaces/houtu&quot;,
    &quot;uid&quot;: &quot;da736612-d112-4e38-8546-0f2b9169b92f&quot;,
    &quot;resourceVersion&quot;: &quot;201510&quot;,
    &quot;creationTimestamp&quot;: &quot;2019-11-21T08:33:38Z&quot;
  },
  &quot;spec&quot;: {
    &quot;finalizers&quot;: [
      &quot;kubernetes&quot;
    ]
  },
  &quot;status&quot;: {
    &quot;phase&quot;: &quot;Active&quot;
  }
}
</code></pre>
<p>删除操作</p>
<pre><code>$ curl X DELETE -k --cert /etc/k8s/pki/apiserver-kubelet-client.crt --key /etc/k8s/pki/apiserver-kubelet-client.key  https://localhost:6443/apis/apps/v1/namespaces/default/deployments/nginx-test/
{
  &quot;kind&quot;: &quot;Status&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {
    
  },
  &quot;status&quot;: &quot;Success&quot;,
  &quot;details&quot;: {
    &quot;name&quot;: &quot;nginx-test&quot;,
    &quot;group&quot;: &quot;apps&quot;,
    &quot;kind&quot;: &quot;deployments&quot;,
    &quot;uid&quot;: &quot;12c5184c-82bc-4f7d-8ec8-38a2439983cf&quot;
  }
}

$ kubectl get deploy
No resources found.
</code></pre>
<p>在Kubernetes之上，来自于那些地方的客户端需要和apiserver打交道</p>
<ul>
<li>集群外部客户端，通过apiserver对外通信的监听地址</li>
<li>集群之上的客户端，apiserver拥有一个在集群内工作地址，``kubectl get svc` 查看，kubernetes是将apiserver以service方式引入到集群内部，从而使得Pod直接请求集群上的apiserver的服务了。</li>
</ul>
<p>Pod在请求apiserver上的服务是通过10.96.0.1来进行的。但是apiserver请求是需要做认证的。首先apiserver将自己证书传递给客户端，客户端去校验服务端(apiserver)身份。 服务器发给每个Pod客户端的时候，证书所标明的身份的地址为10.96.0.1，所以在apiserver上手动创建证书，必须要确保证书持有者名称能够解析到两条IP记录才可以。</p>
<pre><code>$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   47h

$ kubectl describe svc kubernetes
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       &lt;none&gt;
Selector:          &lt;none&gt;
Type:              ClusterIP
IP:                10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.31.71.50:6443
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>
<p>类型分类</p>
<ul>
<li>对象类 deployment、namespace都属于对象类。</li>
<li>同一类型下的所有对象的集合在reset风格API下叫集合，在Kubernetes中被称之为列表（list）。</li>
<li>xx</li>
</ul>
<p>Kubernetes api账户有两类，真实的账户（人用的账户）userAccount与==Pod客户端serviceAccount==（Pod连接apiserver使用的账户）。</p>
<p>每个Pod无论你定义与否，都会挂载一个存储卷，这就是Podservice认证时的认证信息，通过secret定义存储卷的方式关联到Pod上，从而使Pod内运行的应用，通次secret保存的认证信息，来连接apiserver并完成认证。</p>
<p>在每一个名称空间当中都存在一个默认的secret，<code>default-token-xxx</code>，这是让当前名称空间当中所有的Pod资源试图去连接apiserver时，隐藏的、预制的一个认证信息。所以所有的Pod都能直接连接apiserver。此secret所包含的认证信息仅仅是获取当前Pod自身的属性。</p>
<h3 id="给pod增加自定义服务账号">给Pod增加自定义服务账号。</h3>
<p>serviceAccount也属于标准的Kubernetes资源，可以自行创建serviceAccount，由自定义Pod使用serviceAccountName去加载自定义serviceAccount。serviceAccount是一个可以使用命令行创建的简单资源对象，可以使用<code>kubectl create serviceaccount</code>，创建也可以使用资源清单进行创建。</p>
<p>语法</p>
<pre><code>kubectl create serviceaccount {Name} -o yaml --dry-run
</code></pre>
<p><strong>serveraccount本身不具备权限，可以使用rbac对serviceaccount授予权限</strong></p>
<p>创建完之后会自动生成一个token信息，用于让sa连接至当前系统认证的信息。注：认证不代表权限，可以登录、认证到Kubernetes但是做不了其余事情。所有的的操作权限靠授权实现的。</p>
<p>在创建Pod中使用自定义的sa</p>
<pre><code class="language-yaml">spec:
  containers:
  serviceAccountName: admin
</code></pre>
<pre><code>Volumes:
  default-token-4pj85:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-4pj85
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  &lt;none&gt;
</code></pre>
<p>在定义好secret后，在定义Pod时，使用 <code>imagePullSecrets</code> 指明用哪个secret对象，secret对象中包含了认证私有regsi的账号和密码。</p>
<p>在Pod当中也可以不使用imagePullSecrets来告诉Pod如何去下载镜像文件。而可以直接使用serviceAccountName。serviceAccountName相当于指定一个sa账号，而sa账号是可以附带认证到私有regsiry的secret信息的。Pod通过sa的<code>Image pull secrets</code>也能完成资源镜像下载时的认证。这样就不会在Pod资源清单中泄露出去secret使用的什么信息。</p>
<p>使用kubectl describe sa admin</p>
<blockquote>
<p><strong>Pod获取私有镜像时的两种认证方式</strong></p>
</blockquote>
<ul>
<li>在Pod上直接使用<code>imagePullSecrets</code>字段指定认证使用的secret对象。</li>
<li>在Pod自定义serviceAccount，在serviceAccount附加此Pod获取镜像认证时使用的secret对象。</li>
</ul>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://kubernetes.io/docs/concepts/security/controlling-access/" target="_blank"
   rel="noopener nofollow noreferrer" >controlling access</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes概念 - RBAC</title>
      <link>https://www.oomkill.com/2018/08/kubernetes-rbac/</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/08/kubernetes-rbac/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="kubernetes-api-object">Kubernetes API Object</h2>
<p>在Kubernetes线群中，Kubernetes对象是持久化的实体（最终存入etcd 中的数据），集群中通过这些实体来表示整个集群的状态。前面通过<code>kubectl</code>来提交的资源清单文件，将我们的YAML文件转换成集群中的一个API对象的，然后创建的对应的资源对象。</p>
<p>Kubernetes API是一个以<code>JSON</code>为主要序列化方式的<code>HTTP</code>服务，除此之外支持<code>Protocol Buffers</code>序列化方式（主要用干集群内年件间的通信）。为了api的可扩展性，Kubemetes在不同的API路径（<code>/api/v1</code>或<code>/apis/batch</code>）下面支持了多个API版本，不同的API版本就味不同级别稳定性和支持。</p>
<ul>
<li>Alpha ：例如<code>v1Alpha</code>：默认情况下是禁用的，可以随时删除对功能的支持。</li>
<li>Beta：例如 <code>v2beta1</code> 默认是启用的，表示代码已经经过了很好的测试，但是对象的语义可能会在施后的版本中以不兼咨的方式更改</li>
<li>Stable：例如：<code>v1</code> 表示已经是稳定版本，也会出现在后续的很多版本中。</li>
</ul>
<p>在Kubernetes集群中，一个API对象在Etcd 里的完整资源路径，是由：<code>group</code> （API组）、 <code>version</code> （API版本） 和 <code>Resource</code> API资源类型）三个部分组成。通过这种的结构，整个Kubernetes 中所有API对象，就可以用如下的树形结构表示出来：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/kube-api-1.png" alt="kube-api-1" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="kubernetes-api-object的使用">Kubernetes API Object的使用</h3>
<p>API对象组成查看：<code>kubectl get --raw /</code></p>
<p>通常，<code>KubernetesAPI</code>支持通过标准HTTP <code>P0ST</code>、<code>PUT</code>、<code>DELETE</code> 和 <code>GET</code> 在指定PATH路径上创建、更新、删除和检索操作，并使用JSON作为默认的数据交互格式。</p>
<p>如要创建一个Deployment对象，那YAML文件的声明就需：</p>
<pre><code class="language-yaml">apiVersion: apps/v1 # 
kind: Deployment
</code></pre>
<p><code>Deployment</code>就是这个API对象的资源类型（Resource），<code>apps</code>就是它的组（Group），<code>v1</code>就是它的版本（Version）。API Group、Version 和资源满唯一定义了一个HTTP路径，然后在<code>kube-apiserver </code> 对这个url进行了监听，然后把对应的请求传递给了对应的控制器进行处理。</p>
<p><a href="https://k8s.mybatis.io" target="_blank"
   rel="noopener nofollow noreferrer" >API对象参考文档</a></p>
<h2 id="授权插件分类">授权插件分类</h2>
<ul>
<li>
<p>Node 由节点来认证。</p>
</li>
<li>
<p>ABAC 基于属性的访问控制，RBAC之前的授权控制的插件算法</p>
</li>
<li>
<p>RBAC Role-based Access Control。</p>
</li>
<li>
<p>Webhook 基于http的回调机制来实现访问控制。</p>
</li>
</ul>
<h2 id="rbac">RBAC</h2>
<p>基于角色的访问控制可以理解为，角色（role）反而是授权的机制，完成了权限的授予、分配等。角色是指一个组织或者任务工作中的位置，通常代表一种权利、资格、责任等。在基于角色的访问控制中还有一种术语叫做 ==许可==（permission）。</p>
<p>简单来讲就如同上图描述，使用户去扮演这个角色，而角色拥有这个权限，所以用户拥有这个角色的权限。所以授权不授予用户而授予角色。</p>
<p><code>RBAC</code> 使用 <code>rbac.authorization.k8s.io</code>API组来驱动鉴权操作，允许管理员通过 Kubernetes API 动态配置策略。</p>
<p>在 1.8 版本中，RBAC 模式是稳定的并通过 rbac.authorization.k8s.io/v1 API 提供支持。</p>
<h3 id="启用rbac">启用RBAC</h3>
<p>要使用用RBAC，需要在启动kube-apiserver时添加<code>--authorization-mode=RBAC</code> 参数。</p>
<h3 id="api概述">API概述</h3>
<ul>
<li>==<code>/apis/[group]/[version]/namespaces/[namespaces_name]/[kind][/object_id]</code>==</li>
</ul>
<p>在Kubernetes当中的RBAC在实现授权时，无非就是定义标准的角色，在角色上绑定权限。使用户扮演角色。将这些概念体现为：</p>
<ul>
<li>
<p><strong>role</strong> 标准的Kubernetes资源</p>
<ul>
<li>operations  允许那些对象..</li>
<li>objects  执行那些操作..</li>
</ul>
</li>
<li>
<p><strong>rolebinding</strong> 角色绑定</p>
<ul>
<li>
<p>user 将那个用户(user account OR service account)&hellip;</p>
</li>
<li>
<p>role 绑定在那个角色上&hellip;</p>
</li>
</ul>
</li>
<li>
<p><strong>Rule</strong>：规则是一组属于不同<code>API Group</code>资源上的一组操作的集合</p>
</li>
<li>
<p><strong>Group</strong>：用来关联多个账户，集群中有一些默认建的组，比如cluster-admin</p>
</li>
<li>
<p><strong>Subject</strong>：主题，对应集群中尝试操作的对象，集群中定义了3种类型的主题资源：</p>
<ul>
<li>user account：用户，Kubernetes真正意义上User，而是使用证书的CN与O，加上kubeconfig上下文实现用户与组，这个用户是由外部独立服务进行管理的，对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的API来进行管理。</li>
<li>service account：通过<code>KubernetesAPI</code>来管理的一些用户帐号，和namespace进行关联的，适用于集群内部运行的应用程序，需要通过API来完成权限认证，所以在集群内部进行权限操作。</li>
</ul>
</li>
</ul>
<blockquote>
<p>在Kubern1etes之上资源分属于两种级别</p>
</blockquote>
<ul>
<li>
<p><strong>cluster</strong></p>
</li>
<li>
<p><strong>namespaces</strong></p>
</li>
</ul>
<p>所以role和rolebinding是在名称空间级别，授予此名称空间范围内的许可权限的。除了role和rolebinding之外，集群还有另外两个组件：</p>
<ul>
<li>
<p><strong>cluster role</strong>  集群角色。</p>
</li>
<li>
<p><strong>cluster rolebinding</strong> 集群角色绑定。</p>
</li>
</ul>
<p>cluster role当中定义的权限是相对于多个名称空间共有，如果使用rolebinding绑定，这个权限被限制为 <code>用户仅能获取rolebinding所属名称空间上的所有权限</code>。</p>
<h3 id="使用-kubeconfig-文件组织集群访问">使用 kubeconfig 文件组织集群访问</h3>
<p>​	在使用kubectl命令时是有使用配置文件的，配置文件是kubectl连接服务器认证文件。<code>kubectl config</code> 是专门用来管理kubectl的配置文件的。所有连接apiserver的客户端在认证时，如果基于配置文件来保存客户端的认证信息就应该将其配置配置为配置文件。</p>
<p>​	kubernetes组件除了apiserver都可以被称之问apiserver客户端。每个组件为了能够连接正确的集群。apiserver需提供正确的私钥、证书等认证时使用的信息需要将这些信息保存为一个配置文件，此配置文件被叫做<code>kubeconfig</code>。</p>
<p>​	<code>kubeconfig </code>文件可以用来组织有关集群、用户、命名空间和身份认证机制的信息。<code>kubectl</code> 命令行工具使用 kubeconfig 文件来与集群的 API 服务器进行通信。</p>
<p>​	默认情况下 <code>kubeconfig</code> 在 <code>$HOME/.kube</code> 目录下查找名为 <code>config</code> 的文件。可以设置 环境变量<code>KUBECONFIG</code> 或者设置 <code>kubectl --kubeconfig</code> 来选择指定的kubeconfig</p>
<h4 id="context">context</h4>
<p>​	kubeconfig 的 <em>context</em> ，定义对访问参数进行分组。每个context都有三个参数：<code>cluster</code>、<code>namespace </code>和 <code>user</code>。默认情况下，<code>kubectl</code> 命令行工具使用 <code>namespace</code> 参数设置的值与集群进行通信。默认为defaul。</p>
<ul>
<li><code>kubectl config get-contexts</code> 查看拥有上下文</li>
<li><code>kubectl config use-context</code> 选择上下文</li>
<li><code>kubectl config set-credentials</code> 设置一个用户项</li>
</ul>
<p>reference</p>
<p><a href="http://kubernetes.kansea.com/docs/user-guide/kubectl/kubectl_config_set-credentials/" target="_blank"
   rel="noopener nofollow noreferrer" >set-credentials</a></p>
<p><a href="https://kubernetes.io/zh/docs/concepts/configuration/organize-cluster-access-kubeconfig/" target="_blank"
   rel="noopener nofollow noreferrer" >kubeconfig</a></p>
<h3 id="角色的创建与管理">角色的创建与管理</h3>
<p>​	Kubernetes的访问权限主要可以梳理为几个步骤</p>
<ul>
<li>
<p>创建用户：使用CA签发用户认证证书作为用户名称。</p>
</li>
<li>
<p>创建权限组：创建role或clusterrole确定操作权限。</p>
</li>
<li>
<p>绑定用户和权限组：创建rolebinding或clusterrolebinding将权限绑定在用户上。</p>
</li>
<li>
<p>使用用户访问验证：切换kubeconfig。</p>
<p><code>role</code> <code>cluster role</code> <code>rolebinding</code> <code>cluster rolebinding</code>都是标准的Kubernetes资源，可以通过<code>kubectl explain</code>查看，或<code>kubectl create role</code> 创建</p>
<p><strong>role在限制资源范围时有三种方式</strong>：</p>
<ul>
<li>resources 资源类别，允许对这些类所有资源支持授权 操作。</li>
<li>resource Names 资源名称，表示对此类别当中，某个或某些特定资源执行操作。</li>
<li>Non-Resource URLs 非资源url，是一些不能定义为对象的资源，在Kubernetes中通常表示对某些资源所执行的一种操作，或某种特殊操作。</li>
</ul>
</li>
</ul>
<pre><code class="language-bash">ROLENAME=default-admin
NS=default
kubectl create clusterrole ${ROLENAME} \
--verb=get,list \
--resource=pods,deployments \
-o yaml \
-n default \
--dry-run=client 
</code></pre>
<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-list
  namespace: kube-system
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - pods
  - deployments
  verbs:
  - get
  - list
- apiGroups: # 表示对哪些api群组内的资源做操作。
  - apps
</code></pre>
<p>将权限与角色绑定</p>
<pre><code class="language-bash">USERNAME=scott
ROLENAME=default-admin
NS=default
kubectl create clusterrolebinding ${ROLENAME} \
--clusterrole=${ROLENAME} \
--group=${ROLENAME} \
-n ${NS} \
-o yaml \
--dry-run=client
</code></pre>
<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: podlist:testrbac
  namespace: kube-system
roleRef: # 引用那个role
  apiGroup: rbac.authorization.k8s.io # 那个api之内的
  kind: Role # 哪一类
  name: pod-list # role名称，为了避免引用的是cluster role必须使用 此方式来定义
subjects: # 动作的执行主题
# 对于user group是 rbac.auth... 对于serviceaccount是&quot;&quot;
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: testrbac # user并不是单独存在的用户资源
</code></pre>
<p>创建用户</p>
<pre><code>USERNAME=cylon
openssl genrsa -out ${USERNAME}.key 2048
o=default-admin

openssl req -new \
-key ${USERNAME}.key \
-out ${USERNAME}.csr \
-subj &quot;/CN=${USERNAME}/O=${o}&quot; \
-days 3650

openssl x509 -req \
-in ${USERNAME}.csr \
-CA ca.crt \
-CAkey ca.key \
-out ${USERNAME}.crt \
-days 360
</code></pre>
<p>配置kubeconfig</p>
<pre><code class="language-bash">USERNAME=cylon
kubectl config set-credentials ${USERNAME} \
--client-certificate=/etc/kubernetes/pki/${USERNAME}.crt \
--client-key=/etc/kubernetes/pki/${USERNAME}.key \
--embed-certs=true  \
--kubeconfig=${USERNAME} # 输出到指定的配置文件，若不指定写入KUBECONFIG环境变量指定的路径
</code></pre>
<p>设置上下文</p>
<pre><code class="language-bash">USERNAME=cylon
kubectl config set-context ${USERNAME}@kubernetes \
--cluster=kubernetes \
--user=${USERNAME}
--kubeconfig=${USERNAME}
</code></pre>
<p>设置集群</p>
<pre><code>kubectl config set-cluster k8s \
--certificate-authority=/etc/kubernetes/pki/ca.crt \
--embed-certs=true  \
--server=https://127.0.0.1:6443 \
--kubeconfig=${USERNAME}
</code></pre>
<p>在RBAC上进行授权时，允许我们存在<strong>3类</strong>组件 useraccount group serviceaccount</p>
<ul>
<li>user 授权绑定时，<strong><code>cluster rolebinding</code></strong> 或 <strong><code>rolebinding</code></strong> 都可以绑定在user上，也可以绑定在group上，还可以绑定在service account上。</li>
</ul>
<p>绑定在用户上，表示只授权一个用户扮演相关角色。</p>
<p>绑定在在一个组上表示授权组内所有用户都在一个角色。所以想一次授权多个用户在一个名称空间中拥有一个权限可以定义为组。授权时做组授权。</p>
<p>如果任何一个Pod在启动时以serviceaccount name作为使用的serviceAccount，Pod中的应用程序就拥有了它所授予的权限。</p>
<p>创建Pod时可以给Pod指明一个属性。serviceAccount</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
