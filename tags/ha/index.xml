<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>HA on Cylon&#39;s Collection</title>
    <link>https://www.oomkill.com/tags/ha/</link>
    <description>Recent content in HA on Cylon&#39;s Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 31 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://www.oomkill.com/tags/ha/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>haproxy 中 http 代理的连接模式</title>
      <link>https://www.oomkill.com/2023/01/haproxy-http-connection-mode/</link>
      <pubDate>Tue, 31 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/01/haproxy-http-connection-mode/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>haproxy作为一个『代理软件』如果当工作与 HTTP 模式下，所有经由haproxy的的连接的请求和响应都取决于 <strong>frondend</strong> 中配置的 『http_connection_mode』 即 haproxy 中 frontend 与 backend 的组合，而haproxy 支持 3 种连接模式：</p>
<ul>
<li>KAL <em><strong>keep alive</strong></em>: frontend 中配置为 <code>http-keep-alive</code> ; 这是默认模式，这也是http中的keepalive 表示所有请求和响应都得到处理，连接保持打开状态，但在响应和新请求之间处于空闲状态。</li>
<li>SCL <em><strong>server close</strong></em> : frontend 中配置为 <code>http-server-close</code> ; 接收到响应结束后，面向服务器的连接关闭，但面向客户端的连接保持打开状态</li>
<li>CLO <em><strong>close</strong></em>: frontend 中配置为 <code>httpclose</code> ；连接在响应结束后关闭，并在两个方向上附加 &ldquo;Connection: close&rdquo; 。</li>
</ul>
<p>下列矩阵表示的是通过 frondend 与 backend 之间两端的代理模式，这个模式是对称的</p>
<pre><code>     			| KAL | SCL | CLO
            ----+-----+-----+----
            KAL | KAL | SCL | CLO
            ----+-----+-----+----
   mode     SCL | SCL | SCL | CLO
            ----+-----+-----+----
            CLO | CLO | CLO | CLO
</code></pre>
<h2 id="对于http选项的说明">对于http选项的说明</h2>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>forwardfor</td>
<td>这个选项同时存在于backend 与 frontend端，但backend中的优先级超过frontend 如果同时设置了这个参数，那么 backend段的子参数将优先与 frontend 一端</td>
</tr>
<tr>
<td>httpchk</td>
<td>启用http协议检查来检测server的健康状态，默认情况下状态检查是仅建立一个tcp连接</td>
</tr>
<tr>
<td>httpclose</td>
<td>这个选项代表了haproxy 对于http协议持久连接方便的配置</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Reference：<a href="https://www.haproxy.org/download/2.6/doc/configuration.txt" target="_blank"
   rel="noopener nofollow noreferrer" >configuration.txt</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>haproxy v1 与 haproxy v2</title>
      <link>https://www.oomkill.com/2022/12/haproxy2/</link>
      <pubDate>Wed, 14 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/12/haproxy2/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="haproxy1-vs-haproxy2">haproxy1 VS haproxy2</h2>
<p>haproxy2由 2019-06-16 被发布，对于与haproxy1版本来说，haproxy 2.0 增加了对云原生的支持，这使得haproxy 2.0 更适用于云原生环境，对比于 haproxy1.0 在2001年发布来，到 1.9.16 在 <a href="http://www.haproxy.org/download/1.9/src/CHANGELOG" target="_blank"
   rel="noopener nofollow noreferrer" >2020/07/31</a> 最后一次更新也代表haproxy1.0的结束维护</p>
<h2 id="为什么选择haproxy20">为什么选择haproxy2.0</h2>
<p>haproxy2.0的核心功能就是<strong>集成了云原生架构的支持</strong>。包含L7重试, Prometheus metrics, 流量镜像 (<em><strong>traffic shadowing</strong></em>), 多语言可扩展性,  gRPC 。haproxy2.0 还增加 基于haproxy2.0 的 Kubernetes Ingress Controller 和强大的 HAProxy Data Plane API，这提供了用于配置和管理 HAProxy 的 REST API</p>
<h2 id="安装haproxy20">安装haproxy2.0</h2>
<p>对于 Ubuntu/Debian 来说，社区版haproxy提供了更友好的安装方式，用户直接添加对应仓库可以直接安装最新版本的haproxy <a href="https://haproxy.debian.net/" target="_blank"
   rel="noopener nofollow noreferrer" >Debian/Ubuntu HAProxy packages</a></p>
<p>对于 CentOS/Fedora 来说，只有Fedora 仓库提供了较为新版的haproxy，通常来在这类平台的Linux都是通过编译安装haproxy</p>
<ol>
<li>
<p>下载haproxy2.6源码 [ <a href="http://www.haproxy.org/" target="_blank"
   rel="noopener nofollow noreferrer" >haproxy下载</a> ]</p>
</li>
<li>
<p>安装依赖包</p>
<pre><code class="language-bash">yum install gcc pcre-devel openssl-devel tar make -y

</code></pre>
</li>
<li>
<p>编译程序</p>
<pre><code class="language-bash">tar xf haproxy-2.6.7.tar.gz &amp;&amp; cd haproxy-2.6.7/

# 查看编译参数
# 直接使用make可以查看编译参数，这是makefile中配置的
make

# 编译参数
make TARGET=/app/haproxy USE_ZLIB=1 USE_OPENSSL=1 USE_PCRE=1
make install
</code></pre>
</li>
<li>
<p>默认安装的路径在 <code>/usr/local/</code> 下</p>
</li>
</ol>
<p>官方提供的一份 haproxy2.0 配置文件 <a href="https://gist.github.com/haproxytechblog/dc5c3b5e2801d36b79e00f07b2309c14" target="_blank"
   rel="noopener nofollow noreferrer" >HAProxy 2.0 configuration</a></p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://upcloud.com/resources/tutorials/haproxy-load-balancer-centos" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>How to install HAProxy load balancer on CentOS</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://www.haproxy.com/blog/haproxy-2-0-and-beyond/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>HAProxy 2.0 and Beyond</strong></em></a></p>
</blockquote>
<h2 id="troubeshooting">Troubeshooting</h2>
<h3 id="the-configuration-file-is-not-declared-in-the-haproxy_cfgfiles-environment-variable-cannot-start">The configuration file is not declared in the HAPROXY_CFGFILES environment variable, cannot start.</h3>
<pre><code class="language-bash">$ haproxy -f haproxy.cfg
[NOTICE]   (3143) : New program 'api' (3144) forked
[NOTICE]   (3143) : New worker (3145) forked
[NOTICE]   (3143) : Loading success.
time=&quot;2022-12-15T18:43:44+08:00&quot; level=fatal msg=&quot;The configuration file is not declared in the HAPROXY_CFGFILES environment variable, cannot start.&quot;
[NOTICE]   (3143) : haproxy version is 2.6.7-c55bfdb
[NOTICE]   (3143) : path to executable is /usr/local/sbin/haproxy
[ALERT]    (3143) : Current program 'api' (3144) exited with code 1 (Exit)
[ALERT]    (3143) : exit-on-failure: killing every processes with SIGTERM
[ALERT]    (3143) : Current worker (3145) exited with code 143 (Terminated)
[WARNING]  (3143) : All workers exited. Exiting... (1)
</code></pre>
<p>原因：指定的配置文件必须带有路径 <code>haproxy -f haproxy.cfg</code> 这种是错误的，<code>-f</code> 参数属性为</p>
<ul>
<li>如果为目录，则是这个目录下所有的 <code>.cfg</code> 结尾的文件</li>
<li>如果是目录，<code>./&lt;filename&gt;</code> 与 <code>filename</code> 都提示这个报错，必须绝对路径</li>
</ul>
<h3 id="no-users-configured">no users configured</h3>
<pre><code class="language-bash"> haproxy -f /root/haproxy.cfg 
[NOTICE]   (3193) : New program 'api' (3194) forked
[NOTICE]   (3193) : New worker (3195) forked
[NOTICE]   (3193) : Loading success.
time=&quot;2022-12-15T18:45:49+08:00&quot; level=fatal msg=&quot;Error initiating users: no users configured in /root/haproxy.cfg, error: section missing&quot;
[NOTICE]   (3193) : haproxy version is 2.6.7-c55bfdb
</code></pre>
<p>原因：data plane api 程序必须有运行的用户和用户组在配置文件中，官方手册中给出的配置不全 <sup><a href="#1">[1]</a></sup> ，对于data plane api部分配置可以参考 <sup><a href="#2">[2]</a></sup></p>
<h3 id="set-gid-operation-not-permitted">set gid: operation not permitted</h3>
<pre><code class="language-bash"># haproxy -f /root/haproxy.cfg 
[NOTICE]   (3701) : haproxy version is 2.6.7-c55bfdb
[NOTICE]   (3701) : path to executable is /usr/local/sbin/haproxy
[WARNING]  (3701) : config : missing timeouts for frontend 'myfrontend'.
   | While not properly invalid, you will certainly encounter various problems
   | with such a configuration. To fix this, please ensure that all following
   | timeouts are set to a non-zero value: 'client', 'connect', 'server'.
[WARNING]  (3701) : config : missing timeouts for backend 'web_servers'.
   | While not properly invalid, you will certainly encounter various problems
   | with such a configuration. To fix this, please ensure that all following
   | timeouts are set to a non-zero value: 'client', 'connect', 'server'.
[NOTICE]   (3701) : New program 'api' (3702) forked
[NOTICE]   (3701) : New worker (3703) forked
[NOTICE]   (3701) : Loading success.
set gid: operation not permitted
[NOTICE]   (3701) : haproxy version is 2.6.7-c55bfdb
[NOTICE]   (3701) : path to executable is /usr/local/sbin/haproxy
[ALERT]    (3701) : Current program 'api' (3702) exited with code 1 (Exit)
[ALERT]    (3701) : exit-on-failure: killing every processes with SIGTERM
[ALERT]    (3701) : Current worker (3703) exited with code 143 (Terminated)
[WARNING]  (3701) : All workers exited. Exiting... (1)
</code></pre>
<h2 id="reference-1">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://www.haproxy.com/documentation/hapee/latest/api/data-plane-api/installation/haproxy-community/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>HAProxy Community</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://github.com/haproxytech/dataplaneapi/tree/master/configuration/examples" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>configuration examples</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://ubuntu.com/server/docs/service-sssd-ldap" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>SSSD and LDAP</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_authentication_and_authorization_in_rhel/assembly_migrating-authentication-from-nslcd-to-sssd_restricting-domains-for-pam-services-using-sssd#doc-wrapper" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Chapter 10. Migrating authentication from nslcd to SSSD</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://www.linuxquestions.org/questions/linux-desktop-74/openldap-client-2-4-23-tls-negotiation-failure-903809/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>OpenLDAP Client 2.4.23: TLS negotiation failure</strong></em></a></p>
<p><sup id="6">[6]</sup> <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_authentication_and_authorization_in_rhel/assembly_migrating-authentication-from-nslcd-to-sssd_restricting-domains-for-pam-services-using-sssd#doc-wrapper" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Chapter 10. Migrating authentication from nslcd to SSSD</strong></em></a></p>
<p><sup id="7">[7]</sup> <a href="https://www.ibm.com/docs/en/cloud-paks/cp-management/2.2.x?topic=SSFC4F_2.2.0/Infra_mgmt/auth/ldap.htm#configure-sssd" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Configure SSSD</strong></em></a></p>
<p><sup id="8">[8]</sup> <a href="https://kifarunix.com/configure-openldap-sssd-client-on-centos-6-7/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Configure OpenLDAP SSSD client on CentOS 6/7</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>详解haproxy</title>
      <link>https://www.oomkill.com/2017/11/haproxy/</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2017/11/haproxy/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="haproxy-介绍">haproxy 介绍</h2>
<p>haproxy是一个开源的、高性能的基于TCP和HTTP应用代理的高可用的、负载均衡服务软件，它支持双机热备、高可用、负载均衡、虚拟主机、基于TCP和HTTP的应用代理、图形界面查看信息等功能。其配置简单、维护方便，而且拥有很好的对服务器节点的健康检查功能(相当于keepalived健康检查)，当其代理的后端服务器出现故障时，haproxy会自动的将该故障服务器摘除，当故障的服务器恢复后，haproxy还会自动将该服务器自动加入进来提供服务。</p>
<h3 id="lvsnginx对比">LVS/NGINX对比</h3>
<p>haproxy 特别适用于那些高负载、访问量很大，但又需要会话保持及七层应用代理的业务应用。haproxy运行在今天的普通的服务器硬件上，几乎不需要进行任何的优化就可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单、轻松、安全的整合到各种己有的网站架构中，同时，haproxy的代理模式，可以使得所有应用服务器不会暴露到公共网络上，即后面的节点服务器不需要公网IP地址。</p>
<p>从1.3版本起，haproxy软件引入了frontend,backend的功能，frontend (ad规则匹配)可以让运维管理人员根据任意HTTP请求头内容做规则匹配，然后把请求定向到相关的backend(这个是事先定义好的多个server pools，等待前端把请求转过来的服务器组)。通过frontend和backend，我们可以很容易的买现haproxy的各种7层应用代理功能。</p>
<h3 id="haproxy代理模式">haproxy代理模式</h3>
<p>haproxy支持两种主要代理模式：</p>
<p>1、基于4层的tcp应用代理(例如:可用于邮件服务、内部协议通信服务器、MySQL、HTTPS服务等)。</p>
<p>2、基于7层的http代理。在4层tcp代理模式下，haproxy仅在客户端和服务器之间进行流量转发。但是在7层http代理模式下，haproxy会分析应用层协议，并且能通过允许、拒绝、交换、增加、修改或者删除请求(request)或者回应(response)里指定内容来控制协议。</p>
<p>官方网站:www.haproxy.org</p>
<h2 id="haproxy-解决方案拓扑图">haproxy 解决方案拓扑图</h2>
<h3 id="haproxy-l4负载均衡应用架构拓扑">haproxy L4负载均衡应用架构拓扑</h3>
<p>haproxy软件的四层tcp应用代理非常优秀，且配置非常简单、方便，比LVS和Nginx的配置要简单很多，首先，配置haproxy不需要在RS端做任何特殊配置 (只要对应服务开启就OK)就可以实现应用代理，其次，haproxy的配置语法和增加虚拟主机功能等也比lvs/nginx简单。并且和商业版的NS (Netscaler)、F5, A10等负载均衡硬件的使用方法和在架构中的位置一模一样。下面是haproxy的Layer4层应用代理的拓扑结构图:</p>
<hr>
<p>说明:由于haproxy软件采用的是类NAT模式(本质不同)的应用代理，数据包来去都会经过haproxy，因此，在流量特别大的情况下(门户级别的流量)，其效率和性能不如LVS的DR模式负载均衡。</p>
<hr>
<p>在一般的中小型公司，建议采用haproxy做负载均衡，而不要使用LVS或Nginx。为什么强调中小型公司呢?换句话说，千万PV级别以下直接使用haproxy做负载均衡，会让我们负责维护的运维管理人员配置简单、快速、维护方便，出问题好排查。</p>
<h3 id="haproxy-l7负载均衡应用架构拓扑">haproxy L7负载均衡应用架构拓扑</h3>
<p>haproxy软件的最大优势在于其7层的根据URL请求头应用过滤的功能以及sesson会话功能，在门户网站的高并发生产架构中，haproxy软件一般用在4层LVS负载均衡软件的下一层，或者像haproxy官方推荐的也可以挂在硬件负载均衡althon, NS, F5, A10下使用，其表现非常好。从2009年起taobao，京东商城的业务也大面积使用了haproxy作为7层CACHE应用代理。</p>
<h2 id="安装haproxy">安装haproxy</h2>
<p>模拟真实环境</p>
<p>搭建合适的模拟环境是一个人学习能力的重要体现。例如：人类第一次上太空也没有真正的环境，但是想去太空就是要自己动手去搭建逼真的模拟环境。实验多了就是经验，自然就有解除生产环境的机会了。</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>接口</th>
<th>IP</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td><font style="background:#659bfd;" size=3>MASTER</font></td>
<td>eth0</td>
<td>10.0.0.7</td>
<td>外网管理IP用于WAN数据转发</td>
</tr>
<tr>
<td></td>
<td>eth1</td>
<td>172.16.1.7</td>
<td>内网管理IP，用于LAN数据转发</td>
</tr>
<tr>
<td></td>
<td>eth2</td>
<td>10.0.10.7</td>
<td>用于服务器间心跳连接（直连）</td>
</tr>
<tr>
<td>VIP</td>
<td></td>
<td>10.0.0.17</td>
<td>用于提供应用程序A挂载服务</td>
</tr>
<tr>
<td><font style="background:#fee904;" size=3> BACKUP</font></td>
<td>eth0</td>
<td>10.0.0.8</td>
<td>外网管理IP，用于WAN数据转发</td>
</tr>
<tr>
<td></td>
<td>eth1</td>
<td>172.16.1.8</td>
<td>内网管理IP，用于LAN数据转发</td>
</tr>
<tr>
<td></td>
<td>eth2</td>
<td>10.0.10.8</td>
<td>用于服务器间心跳连接（直连）</td>
</tr>
<tr>
<td>VIP</td>
<td></td>
<td>10.0.0.8</td>
<td>用于提供应用程序B挂载服务</td>
</tr>
</tbody>
</table>
<h3 id="下载安装haproxy">下载安装haproxy</h3>
<p>下载地址：http://www.haproxy.org/download/</p>
<p>文档地址：http://www.haproxy.org/download/1.7/doc/configuration.txt</p>
<h3 id="编译haproxy">编译haproxy</h3>
<pre><code class="language-bash">make TARGET=linux2628 ARCH=x86_64  # &lt;==64位编译配置
make TARGET=linux2628 ARCH=i386    # &lt;==32位编译配置
make PREFIX=/app/haproxy-1.7.5 install 
ln -s /app/haproxy-1.7.5/ /app/haproxy
</code></pre>
<h3 id="配置内核转发功能">配置内核转发功能</h3>
<pre><code class="language-bash">net.ipv4_forward=1  # &lt;==基于NAT模式的负载均衡器都需要打开系统转发功能
sysctl -p
</code></pre>
<h3 id="haproxy启动命令">haproxy启动命令</h3>
<p>直接运行命令查看帮助</p>
<pre><code class="language-bash">$ /app/haproxy/sbin/haproxy
HA-Proxy version 1.7.5 2017/04/03
Copyright 2000-2017 Willy Tarreau &lt;willy@haproxy.org&gt;

Usage : haproxy [-f &lt;cfgfile|cfgdir&gt;]* [ -vdVD ] [ -n &lt;maxconn&gt; ] [ -N &lt;maxpconn&gt; ]
        [ -p &lt;pidfile&gt; ] [ -m &lt;max megs&gt; ] [ -C &lt;dir&gt; ] [-- &lt;cfgfile&gt;*]
        -v displays version ; -vv shows known build options.
        -d enters debug mode ; -db only disables background mode.
        -dM[&lt;byte&gt;] poisons memory with &lt;byte&gt; (defaults to 0x50)
...
</code></pre>
<p>命令选项</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>-D</td>
<td>以后台守护进程启动服务</td>
</tr>
<tr>
<td>-f</td>
<td>指定配置文件</td>
</tr>
<tr>
<td>-c</td>
<td>检查配置文件语法</td>
</tr>
<tr>
<td>-n</td>
<td>设置最大连接数，一般在配置文件中指定</td>
</tr>
<tr>
<td>-q</td>
<td>启动时不显示警告</td>
</tr>
<tr>
<td>-m</td>
<td>限制使用的内存量</td>
</tr>
<tr>
<td>-p</td>
<td>将pid写入文件</td>
</tr>
<tr>
<td>-sf</td>
<td>平滑重启</td>
</tr>
<tr>
<td>-st</td>
<td>强制重启</td>
</tr>
</tbody>
</table>
<p>注意 <code>-sf</code> 和 <code>-st</code> 重启时需要指定配置文件</p>
<h3 id="haproxy服务脚本">haproxy服务脚本</h3>
<p>在下载解压目录中有默认的启动脚本</p>
<pre><code class="language-bash">$ ll /root/tools/haproxy-1.7.5/examples/haproxy.init
/root/tools/haproxy-1.7.5/examples/haproxy.init
</code></pre>
<p>自定义启动脚本</p>
<pre><code class="language-bash">#!/bin/sh
BASE=/app/haproxy
COMM=$BASE/sbin/haproxy
PIDFILE=$BASE/var/run/haproxy.pid
CONF_FILE=$BASE/conf/haproxy.conf1
case &quot;$1&quot; in
	'start'|'START')
		if [ ! -f $PIDFILE ];then
				$COMM -f $CONF_FILE -D
		else
				echo 'haproxy has been started'
		fi
		;;
	'status'|'STATUS')
		if [ ! -f $PIDFILE ];then
			echo 'haproxy is not running'
			exit 1
		fi
		for pid in $(cat $PIDFILE);do
			kill -0 $pid
			RETVAL=$?
			if [ $RETVAL == 0 ];then
				echo 'process '$pid ' not running'
			fi
		done
		echo 'haproxy is running'
		;;
	'restart'|'RESTART')
		$COMM -f $CONF_FILE -sf $(cat $PIDFILE)
		;;
	'stop'|'STOP')
		kill $(cat $PIDFILE)
		rm -f $PIDFILE
		;;
	'check'|'CHECK')
		$COMM -f $CONF_FILE -c
		;;
	*)
		echo &quot;USAGE $0 start|stop|restart|status|check|&quot;
		exit 1
		;;
esac
</code></pre>
<h2 id="haproxy配置文件">haproxy配置文件</h2>
<p>haproxy 的默认配置文件在下载解压目录下</p>
<pre><code class="language-bash">/root/tools/haproxy-1.7.5/examples
</code></pre>
<p>aproxy的配置文件可以分为5部分</p>
<ul>
<li>
<p>global：全局配置参数段，主要用来控制haproxy启动前的进程及系统相关设置。</p>
</li>
<li>
<p>default：配置一些默认参数，如果frontend，backend，listen等段未设置则使用default段配置。</p>
</li>
<li>
<p>listen：</p>
</li>
<li>
<p>frontend：用来匹配接受客户所请求的域名，uri等，并针对不同配置，做不同的请求处理。</p>
</li>
<li>
<p>backend：定义后端服务器集群，以及对后端 服务器的一切权重、队列、连接数等选项的设置。</p>
</li>
</ul>
<p>配置文件示例注释说明</p>
<pre><code class="language-conf">global #&lt;==全局配置, 用于设定义全局参数, 属于进程级的配置, 通常与操作系统配置有关
	chroot  /app/haproxy/var/chroot #&lt;==运行路径
	daemon #&lt;==以守护方式运行haproxy
	user    haproxy #&lt;==运行haproxy用户/组, 或者使用关键字uid/gid
    group   haproxy
log		127.0.0.1 local0 debug 
# 全局日志配置指定127.0.0.1:514的syslog服务中local0日志设备。
# 与记录日志的模式[err warning info debug]
    pidfile /app/haproxy/var/run/haproxy.pid
    
maxconn	2000 
#设置每haproxy进程的最大并发连接数, 其等同于命令行选项“-n”; 
# “ulimit -n”自动计算的结果参照此参数设定.
    
    nbproc  1 #&lt;==启动的haproxy进程数量, 只能用于守护进程模式。应该设置为cup核数
    
# ulimit-n 655350  
# 设置每进程所能够打开的最大文件描述符数目。
# 默认情况其会自动进行计算, 因此不推荐修改此选项.    
    
  defaults #&lt;==默认配置

    mode http  #&lt;==默认的模式【tcp:4层； http:7层； health:只返回OK】
    
    log global #&lt;==继承全局的日志定义输出
      
    #option httplog #&lt;==日志类别, httplog
   
    # 如果后端服务器需要记录客户端真实ip, 需要在HTTP请求中添加”X-Forwarded-For”字段;
# 但haproxy自身的健康检测机制访问后端服务器时, 不应将记录访问日志。
# 可用except来排除127.0.0.0，即haproxy本身.
    #option forwardfor except 127.0.0.0/8
option forwardfor

option httpclose 
# 开启http协议中服务器端关闭功能每个请求完毕后主动关闭http通道。
# 使得支持长连接，使得会话可以被重用，使得每一个日志记录都会被记录.
 
    option dontlognull #&lt;==如果产生了一个空连接，那这个空连接的日志将不会记录.
    
    option redispatch	#&lt;==当与后端服务器的会话失败(服务器故障或其他原因)时, 把会话重新分发到其他健康的服务器上; 当故障服务器恢复时, 会话又被定向到已恢复的服务器上;
    
	retries 3  #&lt;==在判定会话失败时的尝试连接的次数
      
    option abortonclose #&lt;==当haproxy负载很高时, 自动结束掉当前队列处理比较久的连接.
   
    timeout http-request 10s #&lt;==默认http请求超时时间
	
    timeout queue 1m	#&lt;==默认队列超时时间, 后端服务器在高负载时, 会将haproxy发来的请求放进一个队列中.
    
    timeout connect 5s	#&lt;==haproxy与后端服务器连接超时时间.
    
    timeout client 1m	#&lt;==客户端与haproxy连接后, 数据传输完毕, 不再有数据传输, 即非活动连接的超时时间.
    
    timeout server 1m	#&lt;==haproxy与后端服务器非活动连接的超时时间.
    
    timeout http-keep-alive 10s #&lt;==默认新的http请求连接建立的超时时间，时间较短时可以尽快释放出资源，节约资源.
    
    timeout check 10s	#&lt;==心跳检测超时时间
      
    maxconn 2000	#&lt;==最大并发连接数
      
    #设置默认的负载均衡方式
    #balance source 
    #balnace leastconn

  listen admin_status 
  # 统计页面配置, frontend和backend的组合体。
  # 监控组的名称可按需自定义
    
    mode http #&lt;==监控运行模式
      
    bind 0.0.0.0:80	#&lt;==统计页面访问端口
      
    maxconn 10  #&lt;==统计页面默认最大连接数
      
    option httplog  #&lt;==#http日志格式
      
    stats enable   #&lt;==开启web统计
      
    stats hide-version 	#&lt;==隐藏统计页面上的haproxy版本信息
      
    stats refresh 30s	#&lt;==监控页面自动刷新时间
      
    stats uri /admin?status #&lt;==统计页面访问url
    
	stats auth admin:111	#&lt;==监控页面的用户和密码:admin, 可设置多个用户名
    
    stats realm hellow world #&lt;==统计页面密码框提示文本
    
    stats admin if TRUE	#&lt;==手工启动/禁用后端服务器, 可通过web管理节点
    
	#设置haproxy错误页面
    #errorfile 400 /usr/local/haproxy/errorfiles/400.http
    #errorfile 403 /usr/local/haproxy/errorfiles/403.http
    #errorfile 408 /usr/local/haproxy/errorfiles/408.http
    #errorfile 500 /usr/local/haproxy/errorfiles/500.http
    #errorfile 502 /usr/local/haproxy/errorfiles/502.http
    #errorfile 503 /usr/local/haproxy/errorfiles/503.http
    #errorfile 504 /usr/local/haproxy/errorfiles/504.http
	
	option forwardfor #&lt;==将用户的IP转发给监控的IP
	option httpchk HEAD /check.html HTTP/1.0 #&lt;==http的健康检查
	
  frontend  WEB_SITE  #&lt;==vip
	bind    *:80
	mode    http
	log     global
	option  httplog    
	option  httpclose  &lt;== http7层代理专用
	default_backend WWW
  backend WWW #&lt;==real server
	option forwardfor header X-REAL-IP
	option httpchk HEAD / HTTP/1.0  &lt;==检查real server是否存活的方式，[get post head]
	server web1 10.0.0.3:80 check inter 2000 rise 30 fall 15  
	server web2 www.test.com check inter 2000 rise 30 fall 15  
# inter为检查间隔 rise为连续30次检查成功则认为有效的。
# fall为连续15次检查失败则认为宕机
</code></pre>
<h2 id="haproxy日志配置">haproxy日志配置</h2>
<p><strong><font color="#f8070d" size=3>CentOS 5.X</font></strong></p>
<p>编辑/etc/syslog.conf增加如下配置</p>
<pre><code class="language-bash">local0.* /app/haproxy/logs/haproxy.log
</code></pre>
<p><strong><font color="#f8070d" size=3>CentOS 6 &amp; 7</font></strong></p>
<pre><code class="language-bash">local0.* -/app/haproxy/logs/haproxy.log 
# 将local0设备的日志定向到haproxy.log因为haproxy使用的local0
</code></pre>
<hr>
<p><font color="#0215cd" size=3> 注：使用了local0设备需要将 local0 在 <code>/var/log/message</code> 里制空，否则会记录双份</font></p>
<hr>
<pre><code class="language-bash">*.info;mail.none;authpriv.none;cron.none;local0.none;    /var/log/messages
</code></pre>
<p>修改/etc/sysconfig/syslog</p>
<pre><code class="language-bash">#-r enables logging from remote machines
# -x disables DNS lookups on messages recieved with -r
SYSLOGD_OPTIONS=&quot;-m 0 -r -c 2&quot;
</code></pre>
<p>重启后生效</p>
<pre><code class="language-bash">/etc/init.d/syslog restart  # &lt;== CentOS 5
/etc/init.d/rsyslog restart # &lt;== CentOS 6
systemctl restart rsyslog   # &lt;== CentOS 7
</code></pre>
<p><a href="http://www.cnblogs.com/aaa103439/p/3537163.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.cnblogs.com/aaa103439/p/3537163.html</a></p>
<p><a href="http://www.cnblogs.com/MacoLee/p/5853413.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.cnblogs.com/MacoLee/p/5853413.html</a></p>
<h2 id="基于权重的轮训round-robin">基于权重的轮训round robin</h2>
<pre><code class="language-bash">server web1 10.0.0.2 check  weight 3
server web1 10.0.0.3 check  weight 1
for n in {0..20};do curl 10.0.02;sleep 1 done
</code></pre>
<p>leastconn&ndash;&gt; 类似于 lvs中 的 wlc</p>
<p>不过这里只考虑活动连接数，即选择活动连接数少的。另外，最好在长连接会话中使用，如sql,ldap</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221213234451180.gif" alt="image-20221213234451180" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="负载模式实验">负载模式实验</h2>
<h3 id="环境准备">环境准备</h3>
<table>
<thead>
<tr>
<th>IP</th>
<th>地位</th>
</tr>
</thead>
<tbody>
<tr>
<td>10.0.0.2</td>
<td>haproxy</td>
</tr>
<tr>
<td>10.0.0.1</td>
<td>real server 1</td>
</tr>
<tr>
<td>10.0.0.3</td>
<td>real server 2</td>
</tr>
</tbody>
</table>
<h3 id="tcp负载模式配置">TCP负载模式配置</h3>
<pre><code class="language-bash">frontend  WEB_SITE  #&lt;==vip
  bind *:80
  mode    tcp
  log     global
  default_backend WWW
backend WWW #&lt;==real server
  server web1 10.0.0.3:52113 check inter 2000 rise 3 fall 5
  server web2 10.0.0.1:52113 check inter 2000 rise 3 fall 5
</code></pre>
<hr>
<p><font color="#0215cd" size=3> 注意：listen可以看做是frontend与bankend的集合，顾如果使用tcp模式代理的话，不要开启web监控</font></p>
<hr>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-tcp_2.gif" alt="image-20221213234451180" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="tcp代理所出现的问题">TCP代理所出现的问题</h3>
<h4 id="tcp模式开启web监控页面出现负载准问题">tcp模式开启web监控页面出现负载准问题</h4>
<p>开启web监控页面的haproxy日志</p>
<pre><code class="language-bash">May 19 22:41:02 127.0.0.1 haproxy[2579]: Connect from 192.168.2.1:50820 to 10.0.0.2:80 (WEB_SITE/TCP)
May 19 22:41:02 127.0.0.1 haproxy[2579]: Connect from 192.168.2.1:50820 to 10.0.0.2:80 (WEB_SITE/TCP)
May 19 22:41:02 127.0.0.1 haproxy[2578]: 192.168.2.1:50821 [19/May/2017:22:41:02.405] admin_status admin_status/&lt;STATS&gt; 0/0/0/0/1 200 16975 - - LR-- 0/0/0/0/0 0/0 &quot;GET /admin?status HTTP/1.1&quot;
May 19 22:41:02 127.0.0.1 haproxy[2579]: Connect from 192.168.2.1:50822 to 10.0.0.2:80 (WEB_SITE/TCP)
May 19 22:41:02 127.0.0.1 haproxy[2579]: Connect from 192.168.2.1:50822 to 10.0.0.2:80 (WEB_SITE/TCP)
May 19 22:41:02 127.0.0.1 haproxy[2579]: 192.168.2.1:50823 [19/May/2017:22:41:02.816] admin_status admin_status/&lt;STATS&gt; 0/0/0/0/1 200 16996 - - LR-- 0/0/0/0/0 0/0 &quot;GET /admin?status HTTP/1.1&quot;
</code></pre>
<h4 id="代理ssh负载出现如下问题">代理ssh负载出现如下问题</h4>
<pre><code class="language-bash">@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
The fingerprint for the RSA key sent by the remote host is
11:f2:f1:05:1e:80:0a:bf:9f:09:20:3f:02:e1:12:b8.
Please contact your system administrator.
Add correct host key in /root/.ssh/known_hosts to get rid of this message.
Offending RSA key in /root/.ssh/known_hosts:1
RSA host key for [10.0.0.2]:80 has changed and you have requested strict checking.
Host key verification failed.
</code></pre>
<p>出现此问题是因为，登陆ssh时使用的是vip，而真正的server是两个，当登陆第一台server时，将指纹保存到vip，当轮训到第二台时，因为已经保存过其他server的指纹了，会提示指纹改变。所以这个不算是问题</p>
<h3 id="l7代理实验">L7代理实验</h3>
<pre><code class="language-conf">frontend  WEB_SITE  #&lt;==vip
  bind *:80
  mode    http
  option  httplog
  option  httpclose
  log     global
  default_backend WWW
backend WWW #&lt;==real server
  server web1 10.0.0.3:52113 check inter 2000 rise 3 fall 5
  server web2 10.0.0.1:52113 check inter 2000 rise 3 fall 5
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-http.gif" alt="image-20221213234451180" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="acl">ACL</h2>
<p>ACL名称必须由 大小写字母、数字、”-“（破折号）、“_”（下划线）、“.”（点）和 ”:“（冒号）。 ACL名称区分大小写，这意味着 “my_acl” 和 “My_Acl” 是两个不同的ACL。ACL的数量没有强制限制。未使用的不影响性能，他们只是消耗少量的内存。</p>
<hr>
<p>注：在http代理模式中uri的请求会被分配到real server的实体路径中</p>
<hr>
<pre><code class="language-bash">acl &lt;aclname&gt; &lt;criterion&gt; [flags] [operator] &lt;value&gt; ... 
</code></pre>
<p>参数说明：</p>
<ul>
<li>
<p>&lt;aclname&gt; &lt;==ACL名称；</p>
</li>
<li>
<p>&lt;criterion&gt;：测试标准，即对什么信息发起测试；测试方式可以由 [flags] 指定的标志进行调整；而有些测试标准也可以需要为其在之前指定一个操作符 [operator]；</p>
</li>
</ul>
<p>ACL的flag：</p>
<ul>
<li>-i：在匹配所有后续模式时忽略大小写。</li>
<li>-f：从文件加载模式。</li>
<li>-m：使用特定的模式匹配方法</li>
<li>-n：禁止DNS解析</li>
<li>-M：像地图文件一样加载-f指向的文件。</li>
<li>&ndash; ：强制结束标志。 当字符串看起来像其中一个标志时很有用。</li>
<li>-u：强制ACL的唯一ID</li>
</ul>
<p>&lt;value&gt;：acl测试条件支持的值有以下四类：</p>
<ol>
<li>整数或整数范围：如 1024:65535 表示从 1024~65535；仅支持使用正整数(如果出现类似小数的标识，其为通常为版本测试)，且支持使用的操作符有5个，分别为eq、ge、gt、le和lt；</li>
<li>字符串：支持使用 “-i” 以忽略字符大小写，支持使用 “\” 进行转义；如果在模式首部出现了-i，可以在其之前使用“–”标志位；</li>
<li>正则表达式：其机制类同字符串匹配；</li>
<li>IP地址及网络地址</li>
</ol>
<p>常用的测试标准(criteria)</p>
<pre><code class="language-bash">be_sess_rate(backend) &lt;integer&gt;
</code></pre>
<p>用于测试指定的backend上会话创建的速率(即每秒创建的会话数)是否满足指定的条件；常用于在指定backend上的会话速率过高时将用户请求转发至另外的backend，或用于阻止攻击行为。</p>
<p>例如：</p>
<pre><code class="language-bash">acl being_scanned be_sess_rate gt 50  # &lt;==此方案是定义在backend里的
redirect location /error_pages/denied.html if being_scanned
</code></pre>
<p>sd</p>
<pre><code class="language-bash">fe_sess_rate(frontend) &lt;integer&gt;
</code></pre>
<p>用于测试指定的frontend(或当前frontend)上的会话创建速率是否满足指定的条件；</p>
<p>常用于为frontend指定一个合理的会话创建速率的上限以防止服务被滥用。例如下面的例子限定入站邮件速率不能大于50封/秒，所有在此指定范围之外的请求都将被延时50毫秒。</p>
<pre><code class="language-conf">frontend mail
    bind :25
    mode tcp
    maxconn 500
    acl too_fast fe_sess_rate ge 50
    tcp-request inspect-delay 50ms
    tcp-request content accept if ! too_fast
    tcp-request content accept if WAIT_END
</code></pre>
<p>hdr &lt;string&gt;</p>
<p>用于测试请求报文中的所有首部或指定首部是否满足指定的条件；指定首部时，其名称不区分大小写，且在括号 “()” 中不能有任何多余的空白字符。测试服务器端的响应报文时可以使用 <code>shdr()</code>。例如下面的例子用于测试首部Connection的值是否为close。</p>
<pre><code class="language-conf">acl url_bao hdr(Host) -i www.baidu.com
</code></pre>
<p>method &lt;string&gt;</p>
<p>测试HTTP请求报文中使用的方法。</p>
<pre><code class="language-conf">front test
  use_backend front
  acl me method get
  default_backend back
backend front
  server web01 10.0.0.1:8080 check port 8080 inter 5000 fall 5
backend back
  server w1 10.0.0.3:8080 check port 8080 inter 5000 fall 5
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221214000225672.png" alt="image-20221214000225672" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>path_beg &lt;string&gt;</p>
<p>用于测试请求的URL是否以指定的模式开头。</p>
<p>下面的例子用于测试URL是否以/static、/images、/javascript或/stylesheets头。</p>
<pre><code class="language-conf">acl url_static path_beg -i /static /images /javascript /stylesheets
</code></pre>
<p>path_end &lt;string&gt;</p>
<p>用于测试请求的URL是否以指定的模式结尾。</p>
<p>例如，下面的例子用户测试URL是否以jpg、gif、png、css或js结尾</p>
<pre><code class="language-conf">acl url_static path_end -i .jpg .gif .png .css .js
</code></pre>
<p>hdr_beg &lt;string&gt;</p>
<p>用于测试请求报文的指定首部的开头部分是否符合指定的模式。</p>
<p>例如，下面的例子用记测试请求是否为提供静态内容的主机img、video、download或ftp。</p>
<pre><code class="language-conf">acl host_static hdr_beg(host) -i img. video. download. ftp.
</code></pre>
<p>请求uri中包含static</p>
<pre><code class="language-conf">acl timetask_req url_dir -i timetask
</code></pre>
<p>请求头长度</p>
<pre><code class="language-conf">acl cl hdr_cnt(Content-length) eq 0
</code></pre>
<h2 id="web-stats">web stats</h2>
<p>添加一个 <strong>frontend</strong></p>
<pre><code class="language-bash">frontend stats
	# 必填参数，默认无法访问
    mode http   
    
    # 统计页面访问端口
    bind 0.0.0.0:1080
    
    # 统计页面默认最大连接数
    maxconn 10
    
    # http日志格式
    option httplog
    
    # 开启web统计
    stats enable
    # 隐藏统计页面上的haproxy版本信息
    stats hide-version
    
    # 监控页面自动刷新时间
    stats refresh 30s
    
    # 统计页面访问url
    stats uri /admin?status
    
    # 监控页面的用户和密码:admin, 可设置多个用户名
    stats auth admin:111
    
    # 统计页面密码框提示文本,某些浏览器不适合中文
    stats realm mCloud\ Haproxy
    
    # 手工启动/禁用后端服务器, 可通过web管理节点
    stats admin if TRUE
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/wps8A90.tmp.jpg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221214000411609.png" alt="image-20221214000411609" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="socat动态操作haproxy">socat动态操作haproxy</h2>
<p>socat是一个多功能的网络工具软件，名字来由是”Socket CAT”，功能与netcat类似，可以看做netcat的加强版。</p>
<p><strong>配置haproxy</strong></p>
<pre><code class="language-bash">stats socket /app/haproxy/var/run/haproxy.sock mode 600 level admin
stats timeout 2m
</code></pre>
<p><strong>安装socat</strong></p>
<pre><code class="language-bash">yum install socat -y
</code></pre>
<p><strong>远程操作haproxy</strong></p>
<p>socat帮助</p>
<pre><code class="language-bash">echo help|socat stdio /app/haproxy/var/run/haproxy.sock
</code></pre>
<p>上线测试摘取集群节点</p>
<pre><code class="language-bash">echo disable server back/w1 |socat stdio /app/haproxy/var/run/haproxy.sock
echo enable server back/w1 |socat stdio /app/haproxy/var/run/haproxy.sock
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-socat.gif" alt="image-20221213234451180" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Keepalived 高可用集群应用实践</title>
      <link>https://www.oomkill.com/2017/02/keepalived/</link>
      <pubDate>Wed, 15 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2017/02/keepalived/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="keepalived介绍">Keepalived介绍</h2>
<p>Keepalived软件起初是专为LVS负载均衡软件设计的，用来管理并监控LVS集群系统中各个服务节点的状态，后来又加入了可以实现高可用的VRRP功能。因此，Keepalived除了能够管理LVS软件外，还可以作为其他服务（例如：Nginx、Haproxy、MySQL等）的高可用解决方案软件。</p>
<p>Keepalived软件主要是通过VRRP协议实现高可用功能的。VRRP是Virtual Router Redundancy Protocol（虚拟路由器冗余协议）的缩写，VRRP出现的目的就是为了解决静态路由单点故障问题的，它能够保证当个别节点宕机时，整个网络可以不间断地运行。所以，Keepalived一方面具有配置管理LVS的功能，同时还具有对LVS下面节点进行健康检查的功能，另一方面也可实现系统网络服务的高可用功能。</p>
<h3 id="keepalived服务的三个重要功能">Keepalived服务的三个重要功能</h3>
<h4 id="管理lvs负载均衡软件">管理LVS负载均衡软件</h4>
<p>早期的LVS软件，需要通过命令行或脚本实现管理，并且没有针对LVS节点的健康检查功能。为了解决LVS的这些使用不便的问题，Keepalived就诞生了，可以说，Keepalived软件起初是专为解决LVS的问题而诞生的。因此，Keepalived和LVS的感情很深，它们的关系如同夫妻一样，可以紧密地结合，愉快地工作。Keepalived可以通过读取自身的配置文件，实现通过更底层的接口直接管理LVS的配置以及控制服务的启动、停止等功能，这使得LVS的应用更加简单方便了。LVS和Keepalived的组合应用不是本章的内容范围。</p>
<h4 id="实现对lvs集群节点健康检查功能healthcheck">实现对LVS集群节点健康检查功能（healthcheck）</h4>
<p>前文已讲过，Keepalived可以通过在自身的keepalived.conf文件里配置LVS的节点IP和相关参数实现对LVS的直接管理；除此之外，当LVS集群中的某一个甚至是几个节点服务器同时发生故障无法提供服务时，Keepalived服务会自动将失效的节点服务器从LVS的正常转发队列中清除出去，并将请求调度到别的正常节点服务器上，从而保证最终用户的访问不受影响；当故障的节点服务器被修复以后，Keepalived服务又会自动地把它们加入到正常转发队列中，对客户提供服务。</p>
<h3 id="作为系统网络服务的高可用功能failover">作为系统网络服务的高可用功能（failover）</h3>
<p>Keepalived可以实现任意两台主机之间，例如Master和Backup主机之间的故障转移和自动切换，这个主机可以是普通的不能停机的业务服务器，也可以是LVS负载均衡、Nginx反向代理这样的服务器。</p>
<p>Keepalived高可用功能实现的简单原理为，两台主机同时安装好Keepalived软件并启动服务，开始正常工作时，由角色为Master的主机获得所有资源并对用户提供服务，角色为Backup的主机作为Master主机的热备；当角色为Master的主机失效或出现故障时，角色为Backup的主机将自动接管Master主机的所有工作，包括接管VIP资源及相应资源服务；而当角色为Master的主机故障修复后，又会自动接管回它原来处理的工作，角色为Backup的主机则同时释放Master主机失效时它接管的工作，此时，两台主机将恢复到最初启动时各自的原始角色及工作状态。</p>
<hr>
<p><strong><font color="#0215cd" size=2>说明：Keepalived的高可用功能是本章的重点，后面除了讲解Keepalived高可用的功能外，还会讲解Keepalived配合Nginx反向代理负载均衡的高可用的实战案例。
</font></strong></p>
<hr>
<h3 id="keepalived高可用故障切换转移原理">Keepalived高可用故障切换转移原理</h3>
<p>Keepalived高可用服务对之间的故障切换转移，是通过VRRP（Virtual Router Redundancy Protocol，虚拟路由器冗余协议）来实现的。</p>
<p>在Keepalived服务正常工作时，主Master节点会不断地向备节点发送（多播的方式）心跳消息，用以告诉备Backup节点自己还活着，当主Master节点发生故障时，就无法发送心跳消息，备节点也就因此无法继续检测到来自主Master节点的心跳了，于是调用自身的接管程序，接管主Master节点的IP资源及服务。而当主Master节点恢复时，备Backup节点又会释放主节点故障时自身接管的IP资源及服务，恢复到原来的备用角色。</p>
<h4 id="什么是vrrp">什么是VRRP</h4>
<p>VRRP，全称Virtual Router Redundancy Protocol，中文名为虚拟路由冗余协议，VRRP的出现就是为了解决静态路由的单点故障问题，VRRP是通过一种竞选机制来将路由的任务交给某台VRRP路由器的。</p>
<p>VRRP早期是用来解决交换机、路由器等设备单点故障的，下面是交换、路由的Master和Backup切换原理描述，同样适用于Keepalived的工作原理。</p>
<p>在一组VRRP路由器集群中，有多台物理VRRP路由器，但是这多台物理的机器并不是同时工作的，而是由一台称为Master的机器负责路由工作，其他的机器都是Backup。Master角色并非一成不变的，VRRP会让每个VRRP路由参与竞选，最终获胜的就是Master。获胜的Master有一些特权，比如拥有虚拟路由器的IP地址等，拥有系统资源的Master负责转发发送给网关地址的包和响应ARP请求。</p>
<p>VRRP通过竞选机制来实现虚拟路由器的功能，所有的协议报文都是通过IP多播（Multicast）包（默认的多播地址224.0.0.18）形式发送的。虚拟路由器由VRID（范围0-255）和一组IP地址组成，对外表现为一个周知的MAC地址：00-00-5E-00-01-{VRID}。所以，在一个虚拟路由器中，不管谁是Master，对外都是相同的MAC和IP（称之为VIP）。客户端主机并不需要因Master的改变而修改自己的路由配置。对它们来说，这种切换是透明的。</p>
<p>在一组虚拟路由器中，只有作为Master的VRRP路由器会一直发送VRRP广播包（VRRP Advertisement messages），此时Backup不会抢占Master。当Master不可用时，Backup就收不到来自Master的广播包了，此时多台Backup中优先级最高的路由器会抢占为Master。这种抢占是非常快速的（可能只有1秒甚至更少），以保证服务的连续性。出于安全性考虑，VRRP数据包使用了加密协议进行了加密。</p>
<p>如果你在面试时，要你解答Keepalived的工作原理，建议用自己的话回答如下内容，以下为对面试官的表述：</p>
<p>Keepalived高可用对之间是通过VRRP通信的：</p>
<ol>
<li>VRRP，全称Virtual Router Redundancy Protocol，中文名为虚拟路由冗余协议，VRRP的出现是为了解决静态路由的单点故障。</li>
<li>VRRP是通过一种竞选协议机制来将路由任务交给某台VRRP路由器的。</li>
<li>VRRP用IP多播的方式（默认多播地址（224.0.0.18））实现高可用对之间通信。</li>
<li>工作时主节点发包，备节点接包，当备节点接收不到主节点发的数据包的时候，就启动接管程序接管主节点的资源。备节点可以有多个，通过优先级竞选，但一般Keepalived系统运维工作中都是一对。</li>
<li>VRRP使用了加密协议加密数据，但Keepalived官方目前还是推荐用明文的方式配置认证类型和密码。</li>
</ol>
<h3 id="keepalived服务的工作原理">Keepalived服务的工作原理</h3>
<p>介绍完了VRRP，接下来我再介绍一下Keepalived服务的工作原理：</p>
<p>Keepalived高可用对之间是通过VRRP进行通信的，VRRP是通过竞选机制来确定主备的，主的优先级高于备，因此，工作时主会优先获得所有的资源，备节点处于等待状态，当主挂了的时候，备节点就会接管主节点的资源，然后顶替主节点对外提供服务。
在Keepalived服务对之间，只有作为主的服务器会一直发送VRRP广播包，告诉备它还活着，此时备不会抢占主，当主不可用时，即备监听不到主发送的广播包时，就会启动相关服务接管资源，保证业务的连续性。接管速度最快可以小于1秒。</p>
<h2 id="keepalived高可用服务搭建">Keepalived高可用服务搭建</h2>
<h3 id="安装keepalived">安装Keepalived</h3>
<p>通过官方地址获取Keepalived源码软件包编译安装</p>
<pre><code class="language-sh">./configure \
--prefix=/app/keepalived-\
--mandir=/usr/local/share/man
make &amp;&amp; make install
</code></pre>
<p>复制命令到/usr/sbin下</p>
<pre><code class="language-sh">ln -s /app/keepalived-1.3.5/sbin/keepalived /usr/sbin/
</code></pre>
<p>keepalived默认会读取/etc/keepalived/keepalived.conf配置文件</p>
<pre><code class="language-sh">mkdir /etc/keepalived &amp;&amp; \
cp /app/keepalived-1.3.5/etc/keepalived/keepalived.conf /etc/keepalived/
</code></pre>
<p>复制sysconfig文件到/etc/sysconfig下</p>
<pre><code class="language-sh">cp /app/keepalived-1.3.5/etc/sysconfig/keepalived /etc/sysconfig/
</code></pre>
<p>复制启动脚本到/etc/init.d下</p>
<pre><code class="language-sh">cp ./keepalived/etc/init.d/keepalived /etc/init.d/
chmod 700 /etc/init.d/keepalived
</code></pre>
<h3 id="编译错误">编译错误</h3>
<pre><code class="language-sh">configure: error: libnfnetlink headers missing
yum install -y libnfnetlink-devel
</code></pre>
<h3 id="启动错误">启动错误</h3>
<p>在centos7中，执行上面的步骤安装完毕后，/etc/init.d/start启动keepalived服务会报如下错误.这是因为centos7 编译安装keepalived会自动生成/usr/lib/systemd/system/keepalived.service单元文件。在单元文件中的启动命令调用的脚本没有执行权限，并且我们复制的启动脚本是复制到/etc/init.d目录下导致的</p>
<pre><code class="language-sh">$ systemctl status keepalived
● keepalived.service - LVS and VRRP High Availability Monitor
   Loaded: loaded (/usr/lib/systemd/system/keepalived.service; disabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since 五 2017-04-07 21:22:25 CST; 5s ago
  Process: 24459 ExecStart=/app/keepalived-1.3.5/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=203/EXEC)

4月 07 21:22:25 mb7 systemd[1]: Starting LVS and VRRP High Availability Monitor...
4月 07 21:22:25 mb7 systemd[1]: keepalived.service: control process exited, code=exited status=203
4月 07 21:22:25 mb7 systemd[1]: Failed to start LVS and VRRP High Availability Monitor.
4月 07 21:22:25 mb7 systemd[1]: Unit keepalived.service entered failed state.
4月 07 21:22:25 mb7 systemd[1]: keepalived.service failed.
Warning: keepalived.service changed on disk. Run 'systemctl daemon-reload' to reload units.
</code></pre>
<p>解决方法：不使用/etc/init.d/keepalived脚本启动了，修改单元文件 删掉单元文件</p>
<h2 id="keepalived配置文件说明">keepalived配置文件说明</h2>
<p>这里的具备高可用功能的keepalived.conf配置文件包含了两个重要区块.</p>
<h3 id="全局定义global-definitions部分">全局定义(Global Definitions)部分</h3>
<p>这部分主要用来设置Keepalived的故障通知机制和Router ID标识。示例代码如下：</p>
<pre><code class="language-sh">! Configuration File for keepalived
global_defs {
   notification_email { #←定义服务故障报警的Email地址。作用是当服务发生切换或RS节点等有故障时，发报警邮件。
   #←这几行是可选配置，notification_email指定在keepalived发生事件时，需要发送的Email地址，可以有多个，每行一个.
      171575158@qq.com
   }
   # 发送邮件的发送人，即发件人地址，可选.
   notification_email_from Alexandre.Cassen@firewall.loc 
   
   # 发送邮件的smtp服务器，如果本机开启了sendmail或postfix，就可以使用上面默认配置实现邮件发送，可选.
   smtp_server 192.168.200.1
   
    # 连接smtp的超时时间，可选.
   smtp_connect_timeout 30
   
   # Keepalived服务器的路由标识（router_id）。在一个局域网内，router_id应该是唯一的.
   router_id LVS_01 
}
</code></pre>
<hr>
<p><strong><font color="#f8070d" size=2>注意：第4~11行所有和邮件报警相关的参数均可以不配，在实际工作中会将监控的任务交给更加擅长监控报警的Nagios或Zabbix软件。</font></strong></p>
<hr>
<h3 id="vrrp实例定义区块vrrp-instances部分">VRRP实例定义区块(VRRP instance(s)部分</h3>
<pre><code class="language-sh"># 定义一个vrrp_instance实例,名字是VI_1,每个vrrp_instance实例可以认为是Keepalived服务的一个实例或者作为一个业务服务，
# 在Keepalived服务配置中,这样的vrrp_instance实例可以有多个.注意,存在于主节点中的vrrp_instance实例在备节点中也要存在，
# 这样才能实现故障切换接管.
vrrp_instance VI_1 {
    
    # state MASTER表示当前实例VI_1的角色状态,当前角色为MASTER,这个状态只能有MASTER和BACKUP两种状态,并且需要大写这些字符。
    # 其中MASTER为正式工作的状态,BACKUP为备用的状态.当MASTER所在的服务器故障或失效时，
    # BACKUP所在的服务器会接管故障的MASTER继续提供服务.
    state MASTER
    
    # interface为网络通信接口.为对外提供服务的网络接口,如eth0、eth1。当前主流的服务器都有2~4个网络接口。
    interface eth0 
    
    # 虚拟路由ID标识,这个标识最好是一个数字,并且要在一个keepalived.conf配置中是唯一的。
    # 但是MASTER和BACKUP配置中相同实例的virtual_router_id又必须是一致的,否则将出现脑裂问题.
    virtual_router_id 51 
    
    # 优先级,其后面的数值也是一个数字,数字越大,表示实例优先级越高.在同一个vrrp_instance实例里，
    # MASTER的优先级配置要高于BACKUP的.若MASTER的priority值为150,那么BACKUP的priority必须小于150，
    # 一般建议间隔50以上为佳,例如：设置BACKUP的priority为100或更小的数值。
    priority 150 
    
    # 同步通知间隔.MASTER与BACKUP之间通信检查的时间间隔,单位为秒,默认为1。
    advert_int 1
    
    # 权限认证配置.包含认证类型（auth_type）和认证密码（auth_pass）；
    # 认证类型有PASS(Simple Passwd(suggested))、AH(IPSEC(not recommended))两种。
    # 官方推荐使用的类型为PASS.验证密码为明文方式，最好长度不要超过8个字符，建议用4位的数字，
    # 同一vrrp实例的MASTER与BACKUP使用相同的密码才能正常通信。
    
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    
    # 虚拟IP地址.可以配置多个IP地址,每个地址占一行,配置时最好明确指定子网掩码以及虚拟IP绑定的网络接口。
    # 否则,子网掩码默认是32位,绑定的接口和前面的interface参数配置的一致。
    # 注意,这里的虚拟IP就是在工作中需要和域名绑定的IP,即和配置的高可用服务监听的IP要保持一致.
    virtual_ipaddress { 
 		    192.168.2.120
    }
}
</code></pre>
<h2 id="keepalived高可用服务单实例">Keepalived高可用服务单实例</h2>
<p>当没有配置高可用服务时，如果服务器宕机了怎么解决呢？无非就是找一个新服务器，配好域名解析的那个原IP，然后搭好相应的网络服务罢了，只不过手工去实现这个过程会比较漫长，相比而言，自动化切换效率更高，效果更好，而且还可以有更多的功能，例如：发送ARP广播，触发执行相关脚本动作等。</p>
<p>实际上也可以将高可用对的两台机器应用服务同时开启，但是只让有VIP一端的服务器提供服务，若主的服务器宕机，VIP会自动漂移到备用服务器上，此时用户的请求直接发送到备用服务器上，而无需临时启动对应服务（事先开启应用服务）。</p>
<h3 id="实战配置keepalived主服务器lb01-master">实战配置Keepalived主服务器lb01 master</h3>
<p>首先，配置lb01 MASTER的keepalived.conf配置文件，操作步骤如下：</p>
<p>删掉已有的所有默认配置：</p>
<pre><code class="language-sh">vim /etc/keepalived/keepalived.conf
</code></pre>
<pre><code class="language-sh">! Configuration File for keepalived

global_defs {
   notification_email {
		  171575158@qq.com
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS_01   #&lt;==id为lb01，不同的keepalived.conf此ID要唯一
   vrrp_skip_check_adv_addr
   vrrp_strict
   vrrp_garp_interval 0
   vrrp_gna_interval 0
}
vrrp_instance VI_1 {  #←实例名字为VI_1,相同实例的备节点名字要和这个相同
    state MASTER	 #←状态为MASTER,备节点状态需要为BACKUP
    interface eth0   #←通信接口为eth0，此参数备节点设置和主节点相同
    virtual_router_id 51 #←实例ID为51，keepalived.conf里唯一
    priority 150	 #←优先级为150，备节点的优先级必须比此数字低
    advert_int 1	 #←通信检查间隔时间1秒

    authentication {	#←PASS认证类型，此参数备节点设置和主节点相同
        auth_type PASS
        auth_pass 1111
    }
    
    #←虚拟IP，即VIP为192.168.2.120,子网掩码为24位，绑定接口为eth0，别名为eth0:1，此参数备节点设置和主节点相同
    virtual_ipaddress {
        192.168.2.120 
    }
}
</code></pre>
<p>配置完毕后，启动Keepalived服务，然后检查配置结果，查看是否有虚拟IP 192.168.2.120。</p>
<pre><code class="language-sh">$ ip addr|grep 192.168.2.120
    inet 192.168.2.120/24 scope global secondary eth0
</code></pre>
<h3 id="配置keepalived备服务器lb02-backup">配置Keepalived备服务器lb02 backup</h3>
<pre><code class="language-sh">! Configuration File for keepalived

global_defs {
   notification_email {
		  171575158@qq.com
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS_02   
   vrrp_skip_check_adv_addr
   vrrp_strict
   vrrp_garp_interval 0
   vrrp_gna_interval 0
}

vrrp_instance VI_1 {  
    state BACKUP
    interface eth0   
    virtual_router_id 51 
    priority 100	 
    advert_int 1	 
    
    authentication {	
        auth_type PASS
        auth_pass 1111
    }
    
    virtual_ipaddress {
        192.168.2.120 
    }
}
</code></pre>
<p>配置完毕后重启服务，检查配置结果，查看是否有虚拟IP 192.168.2.120。</p>
<pre><code class="language-sh">$ ip addr|grep 192.168.2.120
#←没有返回任何结果就对了,因为lb02为BACKUP,当主节点活着的时候,它不会接管VIP 192.168.2.120
</code></pre>
<p>出现无任何结果的现象，表示lb02的Keepalived服务单实例配置成功。如果有192.168.2.120的IP，则表示Keepalived工作不正常，说明高可用裂脑了，裂脑是两台服务器争抢同一资源导致的，同一个IP地址同一时刻应该只能出现一台服务器。</p>
<p>出现上述两台服务器争抢同一IP资源问题，先考虑排查两个地方：</p>
<ul>
<li>主备两台服务器之间是否通信正常，如果不正常是否有iptables防火墙阻挡？</li>
<li>主备两台服务器对应的keepalived.conf配置文件是否有错误？例如，是否同一实例的virtual_router_id配置不一致。</li>
</ul>
<h3 id="高可用主备服务器切换">高可用主备服务器切换</h3>
<p>停掉主服务器上的Keealived服务或关闭主服务器</p>
<pre><code class="language-sh">$ ip addr|grep 192.168.2.120
    inet 192.168.2.120/24 scope global secondary eth0
$ /etc/init.d/keepalived stop
Stopping keepalived (via systemctl):          [  确定  ]
$ ip addr|grep 192.168.2.120    #←查看VIP消失了
$
</code></pre>
<p>此时查看BACKUP备服务器，看是否会有VIP 出现</p>
<pre><code class="language-sh">$ ip addr|grep 192.168.2.120
    inet 192.168.2.120/24 scope global secondary eth0
</code></pre>
<p>可以看到备节点lb02已经接管绑定了VIP，这期间备节点还会发送ARP广播，让所有的客户端更新本地的ARP表，以便客户端访问新接管VIP服务的节点。</p>
<p>此时如果再启动主服务器的Keealived服务，主服务器就会接管回VIP 10.0.0.12，启动后可以观察下主备的IP漂移情况，备服务器是否释放了IP？主服务器是否又接管了IP？</p>
<pre><code class="language-sh">$ /etc/init.d/keepalived start
Starting keepalived (via systemctl):              [  确定  ]
$ ip addr|grep 192.168.2.120
    inet 192.168.2.120/24 scope global secondary eth0
$ ip addr|grep 192.168.2.120	  #←备节点上的VIP则被释放了
</code></pre>
<h3 id="检查nginx--keepalived工作">检查nginx + keepalived工作</h3>
<p>当主节点工作是，web页面如下：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/7cc2732f.png" alt="7cc2732f" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /> <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/96c56887.png" alt="96c56887" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>此时模拟主节点宕机后查看nginx是否正常工作</p>
<img src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/7d862050.png" alt="7d862050"  />
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/b01b07d8.png" alt="b01b07d8" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /> <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/ee56a4c8.png" alt="ee56a4c8" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="多实例">多实例</h3>
<p>keepalive01.conf</p>
<pre><code class="language-conf">! Configuration File for keepalived

global_defs {
   notification_email {
	171575158@qq.com
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS_02
}

vrrp_instance VI_1 {
    state BACKUP
    interface eth0
    virtual_router_id 51
    priority 140
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        10.0.0.10/24
    }
}

vrrp_instance VI_2 {
    state BACKUP
    interface eth0
    virtual_router_id 52
    priority 141
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        10.0.0.11/24
    }
}

</code></pre>
<p>keepalived04.conf</p>
<pre><code class="language-conf">! Configuration File for keepalived

global_defs {
   notification_email {
				171575158@qq.com
	 }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS_01
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 150
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        10.0.0.10/24
		}
}

vrrp_instance VI_2 {
    state MASTER
    interface eth0
    virtual_router_id 52
    priority 151
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
				10.0.0.11/24
		}
}

</code></pre>
<pre><code class="language-sh">$ ip addr|grep 10.0.0
    inet 10.0.0.10/24 scope global eth0
$ ip addr|grep 10.0.0
inet 10.0.0.11/24 scope global eth0
$ /etc/init.d/keepalived stop
Stopping keepalived (via systemctl):                       [  确定  ]
$ ip addr|grep 10.0.0
$
$ ip addr|grep 10.0.0
    inet 10.0.0.10/24 scope global eth0
    inet 10.0.0.11/24 scope global secondary eth0
</code></pre>
<h2 id="keepalived高可用服务器的-裂脑-问题">Keepalived高可用服务器的 “裂脑” 问题</h2>
<h3 id="什么是裂脑">什么是裂脑</h3>
<p>由于某些原因，导致两台高可用服务器对在指定时间内，无法检测到对方的心跳消息，各自取得资源及服务的所有权，而此时的两台高可用服务器对都还活着并在正常运行，这样就会导致同一个IP或服务在两端同时存在而发生冲突，最严重的是两台主机占用同一个VIP地址，当用户写入数据时可能会分别写入到两端，这可能会导致服务器两端的数据不一致或造成数据丢失，这种情况就被称为裂脑。</p>
<h3 id="导致裂脑发生的原因">导致裂脑发生的原因</h3>
<p>一般来说，裂脑的发生，有以下几种原因：</p>
<ul>
<li>高可用服务器对之间心跳线链路发生故障，导致无法正常通信。</li>
<li>心跳线坏了（包括断了，老化）。</li>
<li>网卡及相关驱动坏了，IP配置及冲突问题（网卡直连）。</li>
<li>心跳线间连接的设备故障（网卡及交换机）。</li>
<li>仲裁的机器出问题（采用仲裁的方案）。</li>
<li>高可用服务器上开启了iptables防火墙阻挡了心跳消息传输。</li>
<li>高可用服务器上心跳网卡地址等信息配置不正确，导致发送心跳失败。</li>
<li>其他服务配置不当等原因，如心跳方式不同，心跳广播冲突、软件Bug等。</li>
</ul>
<hr>
<p><font color="#0215cd" size=2> 提示：Keepalived配置里同一VRRP实例如果virtual_router_id两端参数配置不一致，也会导致裂脑问题发生。</font></p>
<hr>
<h3 id="解决裂脑的常见方案">解决裂脑的常见方案</h3>
<p>在实际生产环境中，我们可以从以下几个方面来防止裂脑问题的发生：</p>
<p>同时使用串行电缆和以太网电缆连接，同时用两条心跳线路，这样一条线路坏了，另一个还是好的，依然能传送心跳消息。</p>
<p>当检测到裂脑时强行关闭一个心跳节点（这个功能需特殊设备支持，如Stonith、fence）。相当于备节点接收不到心跳消息，通过单独的线路发送关机命令关闭主节点的电源。</p>
<p>做好对裂脑的监控报警（如邮件及手机短信等或值班），在问题发生时人为第一时间介入仲裁，降低损失。例如，百度的监控报警短信就有上行和下行的区别。报警信息发送到管理员手机上，管理员可以通过手机回复对应数字或简单的字符串操作返回给服务器，让服务器根据指令自动处理相应故障，这样解决故障的时间更短。</p>
<p>当然，在实施高可用方案时，要根据业务实际需求确定是否能容忍这样的损失。对于一般的网站常规业务，这个损失是可容忍的。</p>
<h3 id="解决keepalived裂脑的常见方案">解决Keepalived裂脑的常见方案</h3>
<p>作为互联网应用服务器的高可用，特别是前端Web负载均衡器的高可用，裂脑的问题对普通业务的影响是可以忍受的，如果是数据库或者存储的业务，一般出现裂脑问题就非常严重了。因此，可以通过增加冗余心跳线路来避免裂脑问题的发生，同时加强对系统的监控，以便裂脑发生时人为快速介入解决问题。</p>
<ul>
<li>如果开启防火墙，一定要让心跳消息通过，一般通过允许IP段的形式解决。</li>
<li>可以拉一条以太网网线或者串口线作为主被节点心跳线路的冗余。</li>
<li>开发监测程序通过监控软件（例如Nagios）监测裂脑。
下面是生产场景检测裂脑故障的一些思路：</li>
</ul>
<ol>
<li>简单判断的思想：只要备节点出现VIP就报警，这个报警有两种情况，一是主机宕机了备机接管了；二是主机没宕，裂脑了。不管属于哪个情况，都进行报警，然后由人工查看判断及解决。</li>
<li>比较严谨的判断：备节点出现对应VIP，并且主节点及对应服务（如果能远程连接主节点看是否有VIP就更好了）还活着，就说明发生裂脑了。</li>
</ol>
<h3 id="检测裂脑脚本">检测裂脑脚本</h3>
<pre><code class="language-sh">#!/bin/sh
vip=192.168.2.120
ip=192.168.2.24

function check()
{
  ping -c 2 -W 3 $ip &amp;&gt;/dev/null
  if [ $? -eq 0 -a `ip addr|grep $vip|wc -l` -eq 1 ]
  then
    echo 'fail'
  else
    echo ok
  fi
}
while true
do
  check
done
</code></pre>
<h2 id="解决服务监听的网卡上不存在ip地址问题">解决服务监听的网卡上不存在IP地址问题</h2>
<p>如果配置使用<font color="#f8070d" size=3><code>listen 10.0.0.12：80;</code></font>的方式指定IP监听服务，而本地的网卡上没有10.0.0.12这个IP，Nginx就会报错：</p>
<pre><code class="language-sh">root@lb01 ~]# /app/nginx/sbin/nginx
nginx [emerg] bind(to 10.0.0.12 80 failed 99 Cannot assign requested address)
</code></pre>
<p>如果要实施双主即主备同时跑不同的业务，配置文件里指定了IP监听，备节点则会因为网卡实际不存在VIP也报错。</p>
<p>出现上面问题的原因就是在物理网卡上没有与配置文件里监听的IP相对应的IP，解决办法是在<font color="#f8070d" size=3><code>/etc/sysctl.conf</code></font>中加入如下内核参数配置：</p>
<pre><code class="language-sh">net.ipv4.ip_nonlocal_bind = 1
</code></pre>
<p>也可通过如下命令快速追加：</p>
<pre><code class="language-sh">echo 'net.ipv4.ip_nonlocal_bind = 1' &gt;&gt; /etc/sysctl.conf
</code></pre>
<hr>
<p>注：net.ipv4.ip_nonlocal_bind = 1 #此项表示启动nginx而忽略配置中监听的VIP是否存在，它同样适合Haproxy.</p>
<hr>
<p>最后执行sysctl-p使上述修改生效。</p>
<h2 id="解决高可用服务只针对物理服务器的问题">解决高可用服务只针对物理服务器的问题</h2>
<p>默认情况下Keepalived软件仅仅在对方机器宕机或Keepalived停掉的时候才会接管业务。但在实际工作中，有业务服务停止而Keepalived服务还在工作的情况，这就会导致用户访问的VIP无法找到对应的服务，那么，如何解决业务服务宕机可以将IP漂移到备节点使之接管提供服务？</p>
<p>第一个方法：可以写守护进程脚本来处理。当Nginx业务有问题时，就停掉本地的Keepalived服务，实现IP漂移到对端继续提供服务。实际工作中部署及开发的示例脚本如下：</p>
<pre><code class="language-sh">#!/bin/sh
while true
do
	if [ `ps -ef|grep nginx|grep -v grep|wc -l` -lt 2 ]; then
		/etc/init.d/keepalived stop &amp;&gt;/dev/null
	fi
	sleep 15
done
</code></pre>
<p>后台运行脚本后停止nginx服务，查看进程状态，发现脚本会自动执行命令停止keepalived服务.</p>
<pre><code class="language-sh">$ ps -ef|grep keep
root       3735      1  	0 00:48 ?       00:00:00 keepalived -D
root       3738   3735  0 00:48 ?      	00:00:00 keepalived -D
root       3782   3686  0 00:48 pts/2  	00:00:00 /bin/sh /etc/init.d/keepalived stop
root       3791   3782  	0 00:48 pts/2  	00:00:00 /bin/systemctl stop keepalived.service
root       3797      1  	0 00:48 ?       00:00:00 /bin/sh /etc/rc.d/init.d/keepalived stop
</code></pre>
<p>此时查看备节点状态，可以看出，备节点已经接管服务.</p>
<pre><code class="language-sh">$ ip addr|grep 192.168.2.120
$ ip addr|grep 192.168.2.120
    inet 192.168.2.120/24 scope global secondary eth0
</code></pre>
<h2 id="配置指定文件接收keepalived服务日志">配置指定文件接收Keepalived服务日志</h2>
<p>默认情况下Keepalived服务日志会输出到系统日志/var/log/messages，和其他日志信息混合在一起，很不方便，可以将其调整成由独立的文件记录Keepalived服务日志。操作步骤如下：</p>
<p><strong>1. 编辑配置文件/etc/sysconfig/keepalived</strong></p>
<pre><code class="language-sh">KEEPALIVED_OPTIONS=&quot;-D&quot;
KEEPALIVED_OPTIONS=&quot;-D-d-S 0&quot;
</code></pre>
<p>说明：可以查看/etc/sysconfig/keepalived里注释获得上述参数的说明</p>
<pre><code class="language-sh"> --vrrp               	-P    #←只运行VRRP子系统.
 --check              	-C    #←只运行健康检查子系统.
 --dont-release-vrrp  	-V    #←不要在守护程序停止时删除VRRP VIP和VROUTE.
 --dont-release-ipvs  	-I    #←不要在守护程序停止时删除IPVS拓扑.
 --dump-conf          	-d    #←导出备份配置数据.
 --log-detail         	-D    #←详细日志.
 --log-facility       	-S    #←设置本地的syslog设备，编号0-7(default=LOG_DAEMON)0表示指定为local0设备
</code></pre>
<p><strong>2. 修改rsyslog的配置文件/etc/rsyslog.conf</strong></p>
<pre><code class="language-sh">74 local0.*	/var/log/keepalived.log

</code></pre>
<p>上述配置表示来自local0设备的所有日志信息都记录到/var/log/keepalived.log文件。</p>
<p>然后在约第42行如下信息的第一列结尾加入“；local0.none”：</p>
<pre><code class="language-sh"> 52 # Log anything (except mail) of level info or higher.
 53 # Don't log private authentication messages!
 54 *.info;mail.none;authpriv.none;cron.none;local0.none     /var/log/messages
</code></pre>
<p>上述配置表示来自local0设备的所有日志信息不再记录于/var/log/messages里。</p>
<p><strong>3. 配置完成后，重启rsyslog服务</strong></p>
<pre><code class="language-sh">systemctl restart rsyslog
</code></pre>
<p>测试Keepalived日志记录结果。在重启Keepalived服务后，就会把日志信息输出到rsyslog定义的/var/log/keepalived.log文件</p>
<pre><code class="language-sh">$ head -3 /var/log/keepalived.log
Apr  9 18:58:54 lb_02 Keepalived[32024]: Starting Keepalived v(03/19,2017), git commit v1.3.5-6-g6fa32f2
Apr  9 18:58:54 lb_02 Keepalived[32024]: Unable to resolve default script username 'keepalived_script' - ignoring
Apr  9 18:58:54 lb_02 Keepalived[32024]: Opening file '/etc/keepalived/keepalived.conf'.
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>LVS &amp; keepalived 集群架构</title>
      <link>https://www.oomkill.com/2017/01/lvs-and-keepalived/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2017/01/lvs-and-keepalived/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="lvs概述">LVS概述</h2>
<p>负载均衡(Load Balance)集群提供了一种廉价、有效、透明的方法，来扩展网络设备和服务器的负载、带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。</p>
<blockquote>
<p><strong>搭建负载均衡服务的需求</strong></p>
</blockquote>
<ol>
<li>把单台计算机无法承受的大规模的并发访问或数据流量分担到多台节点设备上分别处理，减少用户等待响应的时间，提升用户体验.</li>
<li>单个重负载的运算分担到多台节点设备上做并发处理，每个节点设备处理结束后，将结果汇总，返回给用户，系统处理能力得到大幅度提高。</li>
<li>7*24小时服务保证，任意一个或多个有限后面节点设备宕机，要求不能影响业务。</li>
</ol>
<p>在负载均衡集群中，所有计算机节点都应该提供相同的服务。集群负载均衡器所截获所有对该服务的入站请求。然后将这些请求尽可能的平均分配在所有集群节点上。</p>
<h3 id="lvs-linux-virtual-server介绍">LVS (Linux Virtual Server)介绍</h3>
<p>LVS是Linux Virtual Server的简写，意即Linux虚拟服务器，是一个虚拟的服务器集群系统，可在UNIX、Linux平台下实现负载均衡集群功能。该项目在1998年5月由章文嵩博士组织成立，是中国国内最早出现的自由软件项目之一</p>
<p>LVS项目介绍 <a href="http://www.linuxvirtualserver.org/zh/lvs1.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.linuxvirtualserver.org/zh/lvs1.html</a></p>
<p>LVS集群的体系结构 <a href="http://www.linuxvirtualserver.org/zh/lvs2.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.linuxvirtualserver.org/zh/lvs2.html</a></p>
<p>LVS集群中的IP负载均衡技术 <a href="http://www.linuxvirtualserver.org/zh/lvs3.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.linuxvirtualserver.org/zh/lvs3.html</a></p>
<p>LVS集群的负载调度 <a href="http://www.linuxvirtualserver.org/zh/lvs4.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.linuxvirtualserver.org/zh/lvs4.html</a></p>
<h3 id="ipvslvs发展史">IPVS（LVS）发展史</h3>
<p>早在2.2内核时，IPVS就已经以内核补丁的形式出现。</p>
<p>从2.4.23版本开始，IPVS软件就是合并到Linux内核的常用版本的内核补丁的集合。</p>
<p>从2.4.24以后IPVS已经成为Linux官方标准内核的一部分。</p>
<h3 id="ipvs软件工作层次图">IPVS软件工作层次图</h3>
<img src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/4af27131.png" alt="4af27131" style="zoom:80%;" />
<p>从上图可以看出，LVS负载均衡调度技术是在Linux内核中实现的，因此，被称之为Linux虚拟服务器（Linux virtual Server）。我们使用该软件配置LVS时候，不能直接配置内核中的ipvs，而需要使用ipvs的管理工具ipsadm进行管理.</p>
<blockquote>
<p><strong>LVS技术点小结</strong>：</p>
</blockquote>
<ul>
<li>真正实现调度的工具是IPVS， 工作在Linux内核层面</li>
<li>LVS自导IPVS管理工具是ipvsadm</li>
<li>keepalived实现管理IPVS及负载均衡器的高可用。</li>
<li>RedHat工具Piranha WEB管理实现调度的工具IPVS。</li>
</ul>
<h3 id="lvs体系结构与工作原理简单描述">LVS体系结构与工作原理简单描述</h3>
<p>LVS集群负载均衡器接受服务的所有入站客户端计算机请求，并根据调度算法决定那个集群几点应该处理回复请求。负载均衡器简称(LB)有时也被成为LVS Director简称Director</p>
<p>LVS虚拟服务器的体系结构如下图所示，一组服务器通过告诉的局域网或者地理分布的广域网互相连接，在他们的前端有一个负载调度器（Load Balancer）。负载调度器能无缝地将网络请求调度到真实服务器上，从而使得服务器集群的结构对客户是透明的，客户访问集群系统提供的网络服务就像访问一台高性能、高可用的服务器一样。客户程序不收服务器集群的影响不需作任何修改。胸的伸缩性通过在服务集群中透明的加入和删除一各节点来达到，通过检测节点或服务进程故障和正确地重置系统达到高可用性。由于我们的负载调度技术是在Linux内核中实现的，我们称之为Linux虚拟服务器（Linux Virtual Server）。</p>
<img src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/d8e6dbb7.png" alt="d8e6dbb7" style="zoom:80%;" />
<h3 id="lvs基本工作过程图">LVS基本工作过程图</h3>
<blockquote>
<p>**LVS基本工作过程图1：带颜色的小方块代表不同的客户端请求</p>
</blockquote>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/9b7099eb.png" alt="9b7099eb" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p><strong>LVS基本工作过程图2</strong>：</p>
</blockquote>
<p>不同的客户端请求小方块经过负载均衡器，通过指定的分配策略被分发到后面的机器上</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/0d677548.png" alt="0d677548" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p><strong>LVS基本工作过程图3</strong>：</p>
</blockquote>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1a42b889.png" alt="1a42b889" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p><strong>LVS基本工作过程图4</strong>：</p>
</blockquote>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/001c6c47.png" alt="001c6c47" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="lvs相关术语命名约定">LVS相关术语命名约定</h3>
<table>
<thead>
<tr>
<th>名称</th>
<th>缩写</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>虚拟IP地址(Virtual IP Address)</td>
<td>VIP  </td>
<td>VIP为Direcort用于向客户端计算机提供IP地址.如www.baidu.com域名就要解析到VIP上提供服务</td>
</tr>
<tr>
<td>真实IP地址(Real Server IP Address)</td>
<td>RIP</td>
<td>在集群下面节点上使用的IP地址，物理IP地址</td>
</tr>
<tr>
<td>Director的IP地址(Director IP Address)</td>
<td>DIP</td>
<td>Director用于连接内外网络的IP地址，物理网卡上的IP地址，是负载均衡器上的IP</td>
</tr>
<tr>
<td>客户端主机IP地址(Client IP Address)</td>
<td>CIP</td>
<td>客户端用户计算机请求集群服务器的IP地址，该地址用作发送给集群的请求的源IP地址</td>
</tr>
</tbody>
</table>
<p>LVS集群内部的节点称为真实服务器(Real Server)，也叫做集群节点。请求集群服务的计算机称为客户端计算机。</p>
<p>与计算机通常在网上交换数据包的方式相同，客户端计算机、Director和真实服务器使用IP地址彼此进行通信。</p>
<h3 id="lvs集群的4种工作模式介绍与原理讲解">LVS集群的4种工作模式介绍与原理讲解</h3>
<p>IP虚拟服务器软件IPVS</p>
<p>在调度器的实现技术中，IP负载金恒技术是效率最高的。在已有的IP负载均衡技术中有通过网络地址转换(Network Address Translation)将一组服务器构成一个高性能的、高可用的虚拟服务器，我们称之为VS/NAT技术(Virtual Server via Network Address Translation)，大多数商业化的IP负载均衡调度器产品都是使用NAT方法，如Cisco的LocalDirector、F5、Netscaler的Big/IP和Alteon的ACEDirector。</p>
<p>在分析VS/NAT的缺点和网络服务的非对称性的基础上，我们提出通过IP隧道实现虚拟服务器方法VS/TUN(Virtual Server via IP Tunneling ) 和通过直接路由实现虚拟服务器的方法VS/DR(Virtual Server via Direct Routing )，它们可以极大地提高系统的伸缩性。所以，IPVS软件实现了这三种IP负载均衡技术，淘宝开源的模式FULLNAT。</p>
<h4 id="nat模式网络地址转换收费站模式">NAT模式==&gt;网络地址转换&lt;==收费站模式</h4>
<blockquote>
<p><strong>Virtual Server via Network Address Translation (VS/NAT)</strong></p>
</blockquote>
<p>通过网络地址转换，调度器LB重写请求报文的目标地址，根据预设的调度算法，将请求分派给后端的真实服务器；真实服务器的响应报文处理之后，返回时必须要通过调度器，经过调度器时报文的源地址被重写，再返回给客户，完成整个负载调度过程。</p>
<hr>
<p><strong><font color="#0215cd" size=2> <font color="#f8070d" size=2>⚠</font> 提示：VS/NAT模式，很类似公路上的收费站，来去都要经过LB负载均衡器，通过修改目的地址，端口或源端口。</font></strong></p>
<hr>
<p><strong>原理描述</strong></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/556081af.png" alt="556081af" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>客户端通过Virtual IP Address（虚拟服务的IP地址）访问网络服务时，请求的报文到达调度器LB时，调度器根据连接调度算法从一组真实服务器中选出一台服务器，将报文的目标地址VIP改写成选的服务器的地址RIP1，请求报文的目标端口改写成选定服务器的相应端口（RS）提供服务端口，最后将修改后的报文发送给选出服务器RS1。同时，调度器LB在连接的Hash表中记录这个连接，当这个连接的下一个报文到达时，从连接的Hash表中可以得到原选定服务器的地址和端口，进行同样的改写操作，并将报文传给原选定的服务器RS1。当来自真实服务器RS1的相应报文返回调度器时，调度器将返回报文的源地址和源端口改为VIP和相应端口，然后调度器再把报文发给请求用户。in DNAT out SNAT。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/46a03c4c.png" alt="46a03c4c" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p><strong>NAT模式小结</strong></p>
</blockquote>
<ol>
<li>
<p>NAT技术将请求的报文（通过DNAT方式改写）和响应的报文（通过SNAT方式改写），通过调度器地址重写然后在转发给内部的服务器，报文返回时在改写成原来的用户请求的地址。</p>
</li>
<li>
<p>只需要在调度器LB上配置WAN公网IP即可，调度器也要有私有LAN IP和内部RS节点通信。</p>
</li>
<li>
<p>每台内部RS节点的网关地址，必须要配成调度器LB的私有LAN内物理网卡地址 (LD1P)，这样才能确保数据报文返回时仍然经过调度器LB。</p>
</li>
<li>
<p>由于清求与响应的数据报文都经过调度器LB.因此，网站访问早大时调度器LB有较大瓶颈，一般要求最多10-20台节点。</p>
</li>
<li>
<p>NAT模式支持对IP及端口的转换，即用户请求10.0.0.1:80,可以通过调度器转换到RS节点的10.0.0.2:8080 (DR和TUN模式不具备的）。-</p>
</li>
<li>
<p>所有NAT内部RS节点只需配置私有LAN IP即可。</p>
</li>
<li>
<p>由于数据包来回都需要经过调度器，因此，要开启内核转发<font color="#f8070d" size=3><code>net.ipv4.ip_forward=1</code></font>,当然也包括iptables防火枪的forward功能（DR和TUX模式不需要）。</p>
</li>
</ol>
<h4 id="dr模式-直连路由模式">DR模式-直连路由模式</h4>
<p><strong>Virtual Server via Direct Routing (VS/DR)</strong></p>
<p>VS/DR模式是通过改写请求报文的目标MAC地址，将请求发给真实服务器的，而真实服务器将相应后的处理结果直接返回给客户端用户。同VS/TUN技术一样，VS/DR技术可极大的提高集群系统的伸缩性。而且，这种DR模式没有IP隧道的开销，对集群中的真实服务器也没有必须支持IP隧道协议的要求，但是要求调度器LB与真实服务器RS都有一块物理网卡连在同一物理网段上，即必须在同一个局域网环境。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/4d7565ce.png" alt="4d7565ce" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>在LVS-DR配置中，Director将所有入站请求转发给集群内部节点，但集群内部的节点直接将他们的回复发给客户端计算机（没有通过Director回来）如图所示：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/c895a20d.png" alt="c895a20d" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><strong><font color="#0215cd" size=2> <font color="#f8070d" size=2>⚠</font> 特别提示：(VS/DR)模式是互联网使用的最多的一种模式。
</font></strong></p>
<hr>
<p>VS/DR模式的工作流程如下图所示：它的连接调度和管理与VS/NAT和VS/TUN中的一样，它的报文转发方法和前两种又有不同，DR模式将报文直接路由给目标服务器，在VS/DR模式中，调度器根据各个真实服务器的负载情况，连接数多少等，动态地选择一台服务器，不修改目的IP地址和目的端口，也不封装IP报文，而是将请求的数据帧的MAC地址改为选出服务器的MAC地址，然后再将修改后的数据帧在与服务器组的局域网上发送。因为请求的数据帧的MAC地址是选出的真实服务器，所以真实服务器肯定可以收到这个改写了目标MAC地址的数据帧，从中可以获得该请求的IP报文。当真实服务器发现 报文的目标地址VIP是在本地的网络设备上，真实服务器处理这个报文，然后根据路由表 将响应报文直接返回给客户。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/31be91fa.png" alt="31be91fa" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>在VS/DR中，根据缺省的TCP/IP协议栈处理，请求报文的目标地址为VIP，响应报文的源地址肯定也为VIP，所以响应报文不需要作任何修改，可以直接返回给客户，客户认为得到正常的服务，而不会知道是哪一台服务器处理的。</p>
<p>VS/DR负载调度器跟VS/TUN—样只处于从客户到服务器的半连接中，按照半连接的TCP有限状态机进行状态迁移。</p>
<blockquote>
<p><strong>原理图：IN更改目的MAC/OUT null</strong></p>
</blockquote>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/cf02c772.png" alt="cf02c772" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p><strong>DR模式小结</strong>：</p>
</blockquote>
<ol>
<li>通过在调度器LB上修改数据包的目的MAC地址实现转发。注意，源IP地址仍然是 CIP，目的IP地址仍然是VIP。</li>
<li>请求的报文经过调度器，而RS响应处理后的报文无需经过调度器LB，因此，并发访问量大时使用效率很高（和NAT模式比）。</li>
<li>因DR模式是通过MAC地址的改写机制实现的转发，因此，所有RS节点和调度器LB 只能在一个局域网LAN中（小缺点）。</li>
<li>需要注意RS节点的VIP的绑定（lo:vip/32，lo1:vip/32)和ARP抑制问题。</li>
<li>强调下：RS节点的默认网关不需要是调度器LB的DIP而直接是IDC机房分配的上级路由器的IP (这是RS带有外网IP地址的情况），理论讲：只要RS可以出网即可，不是必须要配置外网IP。</li>
<li>由于DR模式的调度器仅进行了目的MAC地址的改写，因此，调度器LB无法改变请求的报文的目的端口（和NAT要区别）。</li>
<li>当前，调度器LB支持几乎所有的UNIX, LINUX系统，但目前不支持WINDOWS系统。真实服务器RS节点可以是WINDOWS系统。</li>
<li>总的来说DR模式效率很高，但是配置也较麻烦，因此，访问量不是特别大的公司可以用haproxy/nginx取代。这符合运维的原则：简单、易用、高效。日1000-2000W PV或并发请求小于1W以下都可以考虑用haproxy/nginx（LVS NAT）模式</li>
<li>直接对外的业务访问，例如web服务做RS节点，RS最好用公网IP地址。如果不直接对外的业务，例如：MySQL，存储系统RS节点，最好只用内部IP地址。</li>
</ol>
<h3 id="virtual-server-via-ip-tunneling-vstun">Virtual Server via IP Tunneling (VS/TUN)</h3>
<p>采用NAT技术时，由于请求和响应的报文都必须经过调度器地址重写，当客户请求越来越多时，调度器的处理能力将成为瓶颈。为了解决这个问题，调度器把请求的报文通过IP隧道（相当于ipip或ipsec ）转发至真实服务器，而真实服务器将响应处理后直接返回给客户端用户，这样调度器就只处理请求的入站报文。由于一般网络服务应答数据比请求报文大很多，采用VS/TUN技术后，集群系统的最大吞吐量可以提高10倍。</p>
<p>它的连接调度和管理与VS/NAT中的一样，只是它的报文转发方法不同。调度器根据各个服务器的负载情况，连接数多少，动态地选择一台服务器，将原请求的报文封装在另一个IP报文中，再将封装后的IP报文转发给选出的真实服务器；真实服务器收到报文后，先将收到的报文解封获得原来目标地址为VIP地址的报文，服务器发现VIP地址被配置在本地的IP隧道设备上(此处要人为配置)，所以就处理这个请求，然后根据路由表将响应报文直接返回给客户。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1b499670.png" alt="1b499670" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>TUN模式</p>
<ol>
<li>负载均衡器通过把请求的报文通过IP隧道(ipip隧道)的方式(请求的报文不经过原目的地址的改写(包括MAC)，而是直接封装成另外的IP报文)，转发至真实服务器，而真实服务器将响应处理后直接返回给客户端用户。</li>
<li>由于真实服务器将响应处理后的报文直接返回给客户端用户，因此。最好RS有一个外网IP地址，这样效率才会更高。理论上：只要能出网即可，无需外网IP地址。</li>
<li>由于调度器LB只处理入站请求的报文。因此，此集群系统的吞吐量可以提高10倍以上，但隧道模式也会带来一定的系统开销。TUN模式适合LAN/WAN.</li>
<li>TUN模式的LAN环境转发不如DR模式效率高，而且还要考虑系统对IP隧道的支持问题。</li>
<li>所有的RS服务器都要绑定VIP，抑制ARP，配置复杂.</li>
<li>LAN环境一般多采用DR模式，WAN环境可以用TUN模式，但是当前在，WAN环境下，请求转发更多的被haproxy/nginx/DNS调度等代理取代。因此，TUN模式在国内公司实际应用的已经很少。跨机房应用要么拉光纤成局域网，要么DNS调度，底层数据还得同步.</li>
<li>直接对外的访问业务，例如:web服务做RS节点，最好用公网IP地址。不直接对外的业务，例如:MySQL,存储系统RS节点，最好用内部IP地址。</li>
</ol>
<h4 id="fullnat模式-淘宝网最新开源的">FULLNAT模式 淘宝网最新开源的</h4>
<blockquote>
<p><strong>背景</strong></p>
</blockquote>
<p>LVS当前应用主要采用DR和NAT模式，但这2种模式要求RealServer和LVS在同 一个vlan中，导致部署成本过髙：TUNNEL模式虽然可以跨vlan，但RealServer上需要部署ipip隧道模块等，网络拓扑上需要连通外网，较复杂，不易运维。
为了解决上述问题，我们在LVS上添加了一种新的转发模式：FULLNAT，该模式和NAT模式的区别是：Packet IN时，除了做DNAT,还做SNAT(用户ip-&gt;内网ip),从而实现LVS-RealServer之间可以跨vlan通讯，RealServer只需要连接到内网;</p>
<blockquote>
<p><strong>目标</strong></p>
</blockquote>
<p>FULLNAT将作为一种新工作镆式（同DR/NAT/TUNNEL),实现如下功能：</p>
<ol>
<li>Packet IN时，目标IP更换为realserver ip，源IP更换为内网local IP；</li>
<li>Packet OUT时，目标IP更换为client IP 注：Local IP为一组内网ip地址;性能要求，和NAT比，正常转发性能下降&lt;10%。</li>
</ol>
<h3 id="arp协议">ARP协议</h3>
<h4 id="什么是arp协议">什么是ARP协议</h4>
<p>ARP协议，全称“Address Resolution Protocol”，中文名称是地址解析协议，使用ARP协议可实现通过IP地址获得对应主机的的物理地址（MAC）。</p>
<p>在TCP/IP的网络环境下，每个互联网的主机都会被分配一个32位的IP地址，这种互联网地址是在网际范围标识主机的一种逻辑地址。为了让报文在物理网路上传输，还补习要知道对方目的主机的物理地址才行。这样就存在把IP地址变换成物理地址的地址转换问题。</p>
<p>在以太网环境，为了正确地向目的主机传送报文，必须把目的主机的32为IP地址转换成为目的主机48位以太网地址(MAC),这个就需要在互联层有一个服务或功能将IP地址转换为相应的物理地址(MAC)，这个服务就是ARP协议。</p>
<p>所谓的地址解析&quot;地址解析&quot;，就是主机在发送帧之前将目标IP地址转换成目标MAC地址的过程。ARP协议的基本功能就是通过目标设备的IP地址，查询目标设备的MAC地址，以保证主机间互相通信的顺利进行.</p>
<p>ARP协议和DNS有相像之处。不同点是：DNS实在域名和IP之间解析，另外ARP协议不需要配置服务，而DNS要配置服务才行。</p>
<h4 id="arp缓存表">ARP缓存表</h4>
<p>在每台安装有TCP/IP协议的电脑里都会有一个ARP缓存表（windows命令提示符里输入<font color="#f8070d" size=3><code>arp -a</code></font>即可）， 表里的IP地址与MAC地址是一一对应的。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/a4e30825.png" alt="a4e30825" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<pre><code>C:\Users\CM&gt;arp -a
接口: 192.168.1.103 --- 0x3
  Internet 地址           物理地址                类型
  192.168.1.1             3c-46-d8-5d-53-87       动态
  192.168.1.255           ff-ff-ff-ff-ff-ff       静态
  224.0.0.22              01-00-5e-00-00-16       静态
  224.0.0.251             01-00-5e-00-00-fb       静态
  224.0.0.252             01-00-5e-00-00-fc       静态
  239.11.20.1             01-00-5e-0b-14-01       静态
  239.255.255.250         01-00-5e-7f-ff-fa       静态
</code></pre>
<p>arp常用命令</p>
<p>arp -a 查看所有记录</p>
<p>arp -d 清除</p>
<p>arp -s ip mac  绑定IP和MAC</p>
<h4 id="arp缓存是把双刃剑">ARP缓存是把双刃剑</h4>
<p>主机有了arp缓存表，可以加快arp的解析速度，减少局域网内广播风暴。
正是有了arp缓存表，给恶意黑客带来了攻击服务器主机的风险，这个就是arp欺骗攻击
切换路由器，负载均衡器等设备时，可能会导致短时网络中断.</p>
<h4 id="为什么要使用arp协议">为什么要使用ARP协议</h4>
<p>OSI模型把网络工作分为7层，彼此不直接打交道，只通过接口(layer interface)。IP地址工作在第三层，MAC地址工作在第二层。当协议在发送数据包时，需要先封装第三层IP地址，第二层MAC地址的报头，但协议只知道目的的节点的IP地址，不知道目的节点的MAC地址，又不能跨第二、三层，所以得用ARP协议服务，来帮助获取到目的节点的MAC地址.</p>
<p>osi7层模型协议 包封装解封装详解 <a href="http://www.tudou.com/programs/view/sP9JY_KranA" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.tudou.com/programs/view/sP9JY_KranA</a></p>
<p>tcp三次握手四次断开原理过程详解 <a href="http://www.tudou.com/programs/view/XjHCDedZQa8" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.tudou.com/programs/view/XjHCDedZQa8</a></p>
<h4 id="arp在生产环境产生的问题及解决办法">ARP在生产环境产生的问题及解决办法</h4>
<p>ARP病毒，ARP欺骗
高可用服务器对之间切换时要考虑ARP缓存问题
路由器等设备无缝迁移时要考虑ARP缓存的问题，例如：更换办公室的路由器.</p>
<blockquote>
<p><strong>ARP欺骗原理</strong></p>
</blockquote>
<p>ARP攻击就是通过伪造IP地址和MAC地址对实现ARP欺骗的，如果一台主机中了ARP病毒，那么它就能够在网络中产生大量的ARP通信量（它会以很快的频率进行广播），以至于使网络阻塞，攻击者只要持续不断的发出伪造ARP响应包就能更改局域网中目标主机ARP缓存中的IP-MAC条目，造成网络中断或中间人攻击。</p>
<p>ARP攻击主要是存在于局域网网络中，局域网中若有一个人感染ARP木马，则感染该ARP木马的系统将会试图通过“ARP欺骗”手段截获所在网络内其他计算机的通信故障。</p>
<blockquote>
<p><strong>服务器切换ARP问题</strong></p>
</blockquote>
<p>当网络中一台提供服务的机器宕机后，当在其他运行正常的机器添加宕机的机器的IP时，会因为客户端的ARP table cache的地址解析还是宕机的机器的MAC地址。从而导致，即使在其他运行正常的机器添加宕机的机器的IP，也会发生客户依然无法访问的情况。</p>
<p>解决方法是：当宕机时，IP地址迁移到其他机器上时，需要通过arping命令来通知所有网络内机器清除其本地的ARP table cache，从而使得客户机访问时重新广播获取MAC地址。几乎所有的高可用软件都会考虑这个问题。</p>
<p>ARP广播而进行新的地址解析。</p>
<p>linux下具体命令：</p>
<pre><code class="language-sh">arping -I eth0 -c 3 -s 10.0.0.162 10.0.0.253
arping -U -I eth0 10.0.0.162
</code></pre>
<h2 id="lvs的调度算法">LVS的调度算法</h2>
<p>LVS的调度算法决定了如何在集群节点之间分布工作负荷。</p>
<p>当Director调度器收到来自客户端计算机访问它的VIP上的集群服务的入站请求时，Director调度器必须决定哪个集群节点应该处理请求。Director调度器可用于做出该决定的调度方法分成两个基本类别:</p>
<p>固定调度方法：rr wrr dh sh</p>
<p>动态调度算法：wlc lc lblc lblcr SED NQ(后两种官方站点没提到，编译LVS, make过程可以看到召10种调度算法见如下表格：</p>
<table>
<thead>
<tr>
<th>算法</th>
<th>说明</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>rr</td>
<td>轮循调度(Round-Robin)，它将请求依次分配不同的RS节点，也就是在RS节点中均摊请求。这种算法简单，但是只适合于RS节点处理性能相差不大的情况。</td>
<td></td>
</tr>
<tr>
<td>wrr</td>
<td>加权轮循调度(Weighted Round-Robin)，它将依据不同RS节点的权值分配任务。权值较高的RS将优先获得任务，并且分配到的连接数将比权值较低的RS节点更多。相同权值的RS得到相同数目的连接数。</td>
<td></td>
</tr>
<tr>
<td>dh</td>
<td>目的地址哈希调度(Destination Hashing)以目的地址为关键字查找一个静态hash表来获得需要的RS。</td>
<td></td>
</tr>
<tr>
<td>sh</td>
<td>源地址哈希调度(Source Hashing)以源地址为关键字查找一个静态hash表来获得需要的RS。</td>
<td></td>
</tr>
<tr>
<td>wlc</td>
<td>加权最小连接数调度(Weighted Least-Connection) 假设各台RS的权值依次为Wi(I=1..n)，当前的TCP连接数依次为Ti ( I= 1..n )，依次选取Ti/Wi为最小的RS作为     下一个分配的RS。</td>
<td></td>
</tr>
<tr>
<td>lc</td>
<td>最小连接数调度压(Least-Connection)，IPVS表存储了所有的活动的连接。把新的连接请求发送到当前连接数最小的RS。</td>
<td></td>
</tr>
<tr>
<td>lblc</td>
<td>基于地址的最小连接数调度(Locality-Based Least-Connection)，将来自同一目的地址的请求分配给同一台RS节点，如果这台服务器尚未满负荷，分配给连接数最小的RS，并以它为下一次分配的首先考虑。</td>
<td></td>
</tr>
<tr>
<td>lblcr</td>
<td>基于地址带重复最小连接数调度(Locality-Based Least-Connection with Replication)对于某一目的地址，对应有一个RS子集。对此地址请求，为它分配子集中连接数最小RS;如果子集中所有服务器均已满负荷，则从集群中选择一个连接数较小服务器，将它加入到此子集并分配连接;若一定时间内，未被做任何修改，则将子集中负载最大的节点从子集删除。</td>
<td></td>
</tr>
<tr>
<td>SED</td>
<td>最短的期望的延迟(Shortest Expected Delay Scheduling SED) (SED)<br>基于wlc算法。这个必须举例来说了，<br>ABC三台机器分别权重123，连接数也分别是123。那么如果使用WLC算法的话一个新请求进入时它可能会分给ABC中的任意一个。使用sed算法后会进行这样一个运算。<br>A(1+1)/1,<br>B(1+2)/2,<br>C(1+3)/3,<br>根据运算结果，把连接交给C</td>
<td></td>
</tr>
<tr>
<td>NQ</td>
<td>最少队列调度(Never Queue Scheduling NQ) (NQ)<br>无需队列。如果有台realserver的连接数=0就直接分配过去，不需要在进行sed运算</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="lvs的调度算法的生产环境选型">LVS的调度算法的生产环境选型</h3>
<p>(1) 一般的网络服务，如HTTP、Mail、MySQL等，常用的LVS调度算法为：</p>
<ul>
<li>基本轮叫调度rr算法</li>
<li>加权最小连接调度wlc</li>
<li>加权轮训调度wrr算法</li>
</ul>
<p>(2) 基于局部性的最少链接LBLC和带复制的基于局部性最少链接LBLCR主要适用于Web Cache和Db Cache集群，但是我们很少这样用。</p>
<p>(3) 源地址散列调度SH和目标地址散列调度DH可以结合使用在防火墙集群中，它们可以保证整个系统的唯一出入口。</p>
<p>(4) 最短预期延时调度SED和不排队调度NQ主要是对处理时间相对比较长的网络服务。
实际使用中，这些算法的适用范围不限于这些。我们最好参考内核中的连接调度算法的实现原理，根据具体的业务需求合理的选型。</p>
<h2 id="安装lvs">安装LVS</h2>
<p>下载地址：http://www.linuxvirtualserver.org/software/kernel-2.6/ipvsadm-1.26.tar.gz</p>
<p>安装前准备</p>
<pre><code class="language-sh"># ipvs为lvs调度器，工作在内核层，先查看是否安装
lsmod|grep ip_vs 

# 以uname -r结果为准工作中如果做安装虚拟化可能有多个内核，lvs是基于内核的
ln -s /usr/src/kernels/`uname -r`/ /usr/src/linux 
</code></pre>
<p>安装依赖包</p>
<pre><code class="language-sh">yum install libnl-devel popt-devel popt-static  #&lt;==centos7未发现问题
make &amp;&amp; make install
/sbin/ipvsadm ||  modprobe ip_vs

$ lsmod|grep ip_vs
ip_vs                   136798  0
nf_conntrack            105702  1 ip_vs
libcrc32c              12644  2 xfs,ip_vs
</code></pre>
<blockquote>
<p><strong>VS小结</strong></p>
</blockquote>
<p>1、Centos5.X安装lvs，使用1.2.4版本。不要用1.2.6。</p>
<p>2、Centos6.4安装lvs，使用1.2.6版本。并且需要先安装yum install libnl* popt*-y。</p>
<p>3、安装lvs后，要执行ipvsadm把ip_vs模块加载到内核。</p>
<h2 id="手动配置lvs负载均衡服务">手动配置LVS负载均衡服务</h2>
<h3 id="手工添加lvs转发">手工添加LVS转发</h3>
<p>用户访问www.lb.com然后被DNS解析到vip 10.0.0.10，这个步骤是在DNS里配置的。
如果是自建DNS lb域的DNS记录设置如下</p>
<pre><code class="language-sh">www IN A 10.0.0.10
</code></pre>
<p>如果未自建dns，需要在购买DNS域名商提供的DNS管理界面增加类似上面的DNS记录一条。这里的IP地址一定外网地址，才能正式使用，我们假设192.168.1.0/24段为外网段。修改结果类似下图(必须做真正环境才能做下面修改)</p>
<h3 id="配置lvs虚拟ip-vip">配置LVS虚拟IP (VIP)</h3>
<pre><code class="language-sh">ifconfig eth0:0 10.0.0.10/24 up     
route  add  -host  10.0.0.10  dev  eth0   #←添加主机路由，也可不加此行
</code></pre>
<p>因虚拟网卡网段和VIP不在一个网段，需要设置路由，windows设置路由方法如下：</p>
<pre><code class="language-sh">route -p add 10.0.0.0/24 192.168.2.23
route print
</code></pre>
<p>到这里说明VIP</p>
<pre><code class="language-sh">C:\Users\CM&gt;ping 10.0.0.10

正在 Ping 10.0.0.10 具有 32 字节的数据:
来自 10.0.0.10 的回复: 字节=32 时间&lt;1ms TTL=64
来自 10.0.0.10 的回复: 字节=32 时间&lt;1ms TTL=64
来自 10.0.0.10 的回复: 字节=32 时间&lt;1ms TTL=64
</code></pre>
<hr>
<p><strong><font color="#0215cd" size=2> <font color="#f8070d" size=2>⚠</font> 提示:到这里说明VIP地址己经配好，并可以使用了。</font></strong></p>
<hr>
<h3 id="手工执行配置添加lvs服务并增加两台rs">手工执行配置添加LVS服务并增加两台RS</h3>
<pre><code class="language-sh">ipvsadm -C
ipvsadm --set 30 5 60
ipvsadm -A -t 10.0.0.10:80 -s wrr
ipvsadm -A -t 10.0.0.10:80 -s wrr -p 20
ipvsadm -a -t 10.0.0.10:80 -r 192.168.2.82:80 -g -w 1
ipvsadm -a -t 10.0.0.10:80 -r 192.168.2.21 -g -w 1
ipvsadm -D -t 10.0.0.10:80 -s wrr
ipvsadm -d -t 10.0.0.10:80 -r 192.168.2.21
</code></pre>
<blockquote>
<p><strong>相关参数说明</strong></p>
</blockquote>
<pre><code class="language-sh">#&lt;==清除内核虚拟服务器表中的所有记录
Either long or short options are allowed.
  --add-service       -A        add virtual service with options
  --edit-service      -E        edit virtual service with options
  --delete-service    -D        delete virtual service
  --clear             -C        clear the whole table
  --restore           -R        restore rules from stdin
  --save              -S        save rules to stdout
  --add-server        -a        add real server with options
  --edit-server       -e        edit real server with options
  --delete-server     -d        delete real server
  --list              -L|-l     list the table
  --set tcp tcpfin udp            set connection timeout values
  --tcp-service  -t service-address   service-address is host[:port]
  --udp-service  -u service-address   service-address is host[:port]
  --scheduler    -s scheduler         one of rr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq,
                                      the default scheduler is wlc.
  --persistent    -p [timeout]          persistent service
  --netmask       -M netmask            persistent granularity mask
  --real-server   -r server-address     server-address is host (and port)
  --gatewaying    -g                    gatewaying (direct routing) (default)
  --ipip          -i                    ipip encapsulation (tunneling)
  --masquerading  -m                    masquerading (NAT)
  --weight        -w weight             capacity of real server
  --mcast-interface interface           multicast interface for connection sync
  --connection    -c                    output of current IPVS connections
  --timeout                               output of timeout (tcp tcpfin udp)
  --stats                                 output of statistics information
</code></pre>
<p>此时，在浏览器访问10.0.0.10结果是无法访问的：因为根据LVS原理，RS在接收到包后发现不是自己IP后就丢弃了。  <a href="http://blog.csdn.net/raintungli/article/details/39051435" target="_blank"
   rel="noopener nofollow noreferrer" >IPVS(也叫LVS)的源码分析之persistent参数 - 沧海一粟 - CSDN博客</a></p>
<h3 id="手工在rs端绑定">手工在RS端绑定</h3>
<pre><code class="language-sh">ifconfig lo:0 10.0.0.10/32 up    #&lt;==注意，子网掩码特殊

</code></pre>
<p>每个集群节点上的环回接口 (lo) 设备上被绑定VIP地址(其广播地址是其本身，子网掩码是255.255.255.255，采取可变长掩码方式把网段划分成只含一个主机地址的目的避免ip地址冲突)允许LVS-DR集群中的集群节点接受发向该VIP地址的数据包，这会有一个非常严重的问题发生，集群内部的真实服务器将尝试回复来自正在请求VIP客户端ARP广播，这样所有的真实服务器都将声称自己拥有该VIP地址，这时客户端将有可能接发送请求数据包到某台真实服务器上，从而破坏了DR集群的负载均衡策略。因此，必须要抑制所有真实服务器响应目标地址为VIP的ARP广播，而把客户端ARP广播的响应交给负载均衡调度器。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/06e08a6d.png" alt="06e08a6d" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>抑制ARP响应方法如下</p>
<pre><code class="language-sh">echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore
echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce    
echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore
echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce
</code></pre>
<h4 id="arp抑制技术参数说明">arp抑制技术参数说明</h4>
<p>中文说明:，</p>
<blockquote>
<p><strong>arp_ignore- INTEGER</strong></p>
</blockquote>
<p>定义对目标地址为本地IP的ARP询问不同的应答模式。</p>
<p>0 (默认值)：回应任何网络接口上对任何本地IP地址的arp查询请求。</p>
<p>1：只回答目标IP地址是来访问网络接口本地地址的ARP查询请求</p>
<p>2：只回答目标IP地址是来访网络接口本地地址的ARP查询请求，且来访IP必须在该网络接口的子网段内。</p>
<p>3：不回应该网络界面的arp请求，而只对设置的唯一和连接地址做出回应。，</p>
<p>4-7：保留未使用。‘</p>
<p>8：不回应所有(本地地址)的arp查询。</p>
<blockquote>
<p><strong>arp_announce一INTEGER</strong></p>
</blockquote>
<p>对网络接口上，本地IP地址的发出的，ARP回应，作出相应级别的限制：确定不同程度的限制，宣布对来自本地源lP地址发出Ail，请求的接口</p>
<p>0 (默认)在任意网络接口(eth0,eth1, lo)上的任何本地地址</p>
<p>1：尽量避免不在该网络接口子网段的本地地址做出arp回应.当发起ARP请求的源IP地址是被设置应该经由路由达到此网络接口的时候很有用。此时会检查来访IP是否为所有接口上的子网段内ip之一。如果该来访IP不属于各个网络接口上的子网段内，那么将采用级别2的方式来进行处理。</p>
<p>2：一对查询目标使用最适当的本地地址，在此模式下将忽略这个IP数据包的源地址并尝试能与该地址通信的本地地址，首要是选择所有的网络接口的子网中外出访问子网中包含该目标IP地址的本地地址。如果没有合适的地址被发现，将选择当前的发送网络接口或其他的有可能接受到该ARP回应的网络接来进发送。限制了使用本地VIP地址作为优先的网络接口。</p>
<h4 id="检查手工添加lvs转发成果">检查手工添加LVS转发成果</h4>
<p>测试LVS服务的转发：</p>
<p>首先在客户端浏览器访问RS  http://192.168.1.6 及 RS http://192.168.1.7 确认是否RS端正常然后访问DR的VIP http://10.0.0.10， 如果经过多次测试能分别出现RS1, RS2的 不同结果内容就是表示配置成功。</p>
<hr>
<p><strong><font color="#0215cd" size=2> <font color="#f8070d" size=2>⚠</font> 提示:负载均衡的算法倾向于一个客户端IP定向到一个后端服务器，以保持会话连贯性，如果用两三台机器去测试也许就不一样。</font></strong></p>
<hr>
<h4 id="在访问的同时可以用命令查看状态信息">在访问的同时可以用命令查看状态信息</h4>
<pre><code class="language-sh">$ ipvsadm -L -n --stats
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes
  -&gt; RemoteAddress:Port
TCP  10.0.0.10:80                       88      674        0    88293        0
  -&gt; 192.168.2.21:80                  44      355        0    47851        0
  -&gt; 192.168.2.82:80                  44      319        0    40442        0
</code></pre>
<h3 id="使用脚本配置lvs负载均衡服务">使用脚本配置lvs负载均衡服务</h3>
<pre><code class="language-sh">#!/bin/sh
VIP='10.0.0.10'
. /etc/init.d/functions
Port=80

RIP=(
  192.168.2.21
  192.168.2.82
)

clean_all(){
    ipvsadm -C
    return $?
}

set_virtual_server(){
    ipvsadm --set 30 5 60
    ipvsadm -A -t $VIP:$Port -s wrr
    return $?
}

start(){
    clean_all
    [ $? -eq 0 ] &amp;&amp; set_virtual_server || return 1
    for n in ${RIP[@]}
    do
        ipvsadm -a -t $VIP:$Port -r $n:$Port -g -w 1
    done
}
stop(){
    clean_all
}

usage(){
  echo 'USAGE:'$0 '{start|stop|restart}'
}

case &quot;$1&quot; in
    start|START)
    start
    ;;
    stop|STOP)
    stop
    ;;
    restart|RESTART)
    stop
    start
    ;;
    *)
    usage
esac
</code></pre>
<pre><code class="language-sh">#!/bin/sh
. /etc/init.d/functions

master_ip=192.168.2.23
VIP='10.0.0.10'
Port=80
RIP=(
  192.168.2.21
  192.168.2.82
)

set_virtual_server(){
  ipvsadm --set 30 5 60
  ipvsadm -A -t $VIP:$Port -s wrr
  return $?
}

check_master(){ 
  /usr/bin/ping -c 2 $master_ip &amp;&gt;/dev/null
  return $?
}

clean_all(){
  ipvsadm -C
}

start(){
  clean_all &amp;&amp; /sbin/ifconfig eth0:0 $VIP/24 up
  [ $? -eq 0 ] &amp;&amp; set_virtual_server || return 10
  for n in ${RIP[@]}
  do
    /sbin/ipvsadm -a -t $VIP:$Port -r $n:$Port -g -w 1
  done
}

stop(){
  clean_all
  /sbin/ifconfig eth0:0 $VIP/24 down
}

check_isnode(){
  num=`ipvsadm -L -n|grep 192|wc -l`
  if [ $num -lt ${#RIP[@]} ];then
    return 0
  fi
  return 11
}

check_isup(){
  num=`ifconfig eth0:0|grep $VIP|wc -l`
  if [ $num -eq 1 ];then
    return 0
  fi
  return 12
}

while true
do
  check_master
  if [ $? -ne 0 ];then
    check_isnode
    if [ $? -eq 0 ];then
      start
    fi
  else
    check_isup
    if [ $? -eq 0 ];then 
      stop
    fi
  fi
  sleep 3
done
</code></pre>
<h2 id="常见的lvs负载均衡高可用解决方案">常见的LVS负载均衡高可用解决方案</h2>
<p>(1) 通过开发上面的脚本来解决，如果负载均衡器硬件坏了。几分钟或秒级别内在其它备机上完成新的部署，如果做的细的，还可以写脚本来做调度器之间的切换和接管功能。早起的方法，还是比较笨重的，目前已经不推荐使用。</p>
<p>(2) heartbeart+lvs+ldirectord脚本配置方案，这个方案同学们自己可以去搜索，这个方案中heartbeat负责VIP的切换以及资源的启动停止，ldirectord负责RS节点的健康检查，用于比较复杂，不易控制，属于早期的解决方案，现在已经很少使用了。</p>
<blockquote>
<p><strong>heartbeat and ldirectord方案资料</strong>:</p>
</blockquote>
<p><a href="http://www.linuxvirtualserver.org/docs/ha/heartbeat_ldirectord.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.linuxvirtualserver.org/docs/ha/heartbeat_ldirectord.html</a></p>
<p><a href="http://www.linuxvirtualserver.org/docs/ha/ultramonkey.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.linuxvirtualserver.org/docs/ha/ultramonkey.html</a></p>
<p>(3) 通过Redhat提供的工具piranha来配置LVS
Piranha是REDHAT提供的一个基于Web的LVS配置软件，可以省去手工配置LVS的繁琐工作，同时，也可单独提供。cluster功能，例如，可以通过Piranh。激活Director Server的后备主机，也就是配置Director Server的双机热备功能。</p>
<p>(4) keepalived+LVS方案，当前最优方案，因为这个方案符合简单、易用、高效的运维原则。
The Keepalived Solution <a href="http://www.linuxvirtualserver.org/docs/ha/keepalived" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.linuxvirtualserver.org/docs/ha/keepalived</a></p>
<p>(5) 其他</p>
<p><a href="http://bbs.linuxtone.org/thread-1402-1-1.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://bbs.linuxtone.org/thread-1402-1-1.html</a></p>
<p>LVS Documentation</p>
<p><a href="http://www.linuxvirtualserver.org/zh/index.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.linuxvirtualserver.org/zh/index.html</a></p>
<p><a href="http://www.linuxvirtualserver.org/Documents.html#performance" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.linuxvirtualserver.org/Documents.html#performance</a></p>
<p><a href="http://zh.linuxvirtualserver.org/node/2230" target="_blank"
   rel="noopener nofollow noreferrer" >http://zh.linuxvirtualserver.org/node/2230</a></p>
<h2 id="lvs集群分发请求rs不均衡生产环境实战解决">LVS集群分发请求RS不均衡生产环境实战解决</h2>
<p>生产环境中 <code>ipvsadm -L -n</code> 发现两台RS的负载不均衡，一台有很多请求，一台没有请求，并且没有请求的那台RS经测试服务正常，lo:VIP也有。但是就是没有请求。</p>
<pre><code class="language-sh">TCP 172.168.1.50:3307 wrr persistent 10
  一&gt;172.168.1.51:3307          Route   1      0         0
  一&gt;172.168.1.52:3307          Route   1      8         12758
</code></pre>
<blockquote>
<p><strong>问题原因</strong>：</p>
</blockquote>
<p>persistent 10的原因，persistent会话保持，当clientA访问网站的时候，LVS把请求分发给了52,那么以后clientA再点击的其他操作其他请求，也会发送给52这台机器。</p>
<blockquote>
<p><strong>解决办法</strong>:</p>
</blockquote>
<p>到keepalived中注释掉persistent 10然后/etc/init.d/keepalived reload，然后可以看到以后负载均衡两边都请求都均衡了。</p>
<p><font style="background:#ffc104;" size=2>其它导致负载不均的原因可能有：</font></p>
<ol>
<li>LVS自身的会话保持参数设置((-p 300, persistent 300)。优化:大公司尽量用cookie替代session</li>
<li>LVS调度算法设置，例如：rr, wrr, wld,lc算法。</li>
<li>后端RS节点的会话保持参数，例如:apache的keealive参数。</li>
<li>访问量较少的情况不均衡的现象更加明显。</li>
<li>用户发送的请求时间长短，和请求资源多少大小。</li>
</ol>
<p>实现会话保持的方案：</p>
<p><a href="http://oldboy.blog.5lcto.com/2561410/1331316" target="_blank"
   rel="noopener nofollow noreferrer" >http://oldboy.blog.5lcto.com/2561410/1331316</a></p>
<p><a href="http://oldboy.blog.51cto.com/2561410/1323468" target="_blank"
   rel="noopener nofollow noreferrer" >http://oldboy.blog.51cto.com/2561410/1323468</a></p>
<h2 id="lvs故障排错理论及实战讲讲解">LVS故障排错理论及实战讲讲解</h2>
<p>排查的大的思路就是，要熟悉LVS的工作原理过程，然后根据原理过程来排查。</p>
<ol>
<li>调度器上LVS调度规则及IP的正确性。</li>
<li>RS节点上VIP绑定和arp抑制的检查。</li>
</ol>
<p>生产处理思路：</p>
<ol>
<li>对绑定的vip做实时监控，出问题报警或者自动处理后报警。</li>
<li>把绑定的vip做成配置文件，例如:vi /etc/sysconfig/network-scripts/lo:0</li>
</ol>
<p>ARP抑制的配置思路：</p>
<ol>
<li>如果是单个VIP，那么可以用stop传参设置0。</li>
<li>如果RS端有多个、VIP绑定，此时，即使是停止VIP绑定也一定不要置0。</li>
</ol>
<pre><code class="language-sh">if [ ${#VIP[@]} ];then
echo &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore
echo &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce
echo &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore
echo &quot;0&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce
if
</code></pre>
<ol start="3">
<li>
<p>RS节点上自身提供月够的检查(DR不能端口转换)</p>
</li>
<li>
<p>辅助排除工具有tcpdump, ping等。</p>
</li>
<li>
<p>负载均衡和反向代理集群的三角形排查理论。</p>
</li>
</ol>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/960fc67a.png" alt="960fc67a" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="keepalivedlvs负载均衡配置">keepalived+lvs负载均衡配置</h2>
<pre><code class="language-conf">virtual_server 10.0.0.10 80 { 
  delay_loop 3 #&lt;== 健康检查时间，单位是秒  
  lb_algo wrr #&lt;== 负载调度的算法为wlc
  lb_kind DR  #&lt;==LVS实现负载的机制，NAT/DR/TUN/FULLNAT  
  nat_mask 255.255.255.0  
  persistence_timeout 20 #&lt;==会话保持 -p的功能
  protocol TCP #&lt;==lvs4层负载均衡 tcp udp
# ipvsadm -A -t 10.0.0.10:80 -s wrr -p 20

  real_server 192.168.1.5 80 {
    weight 1 #&lt;==配置节点权值，数字越大权重越高  
    TCP_CHECK { #&lt;==健康检查
      connect_timeout 10 #&lt;==超时时间
      nb_get_retry 3 #&lt;==延迟重试次数
      delay_before_retry 3 #&lt;==重试次数
      connect_port 80 #&lt;==检查端口
    }
  }
}
# ipvsadm -a -t 10.0.0.10:80 -r 192.168.1.5 -g -w 1
</code></pre>
<p>查看负载结果</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/b403805d.png" alt="b403805d" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/188bf29c.png" alt="188bf29c" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>当master宕机后，可看到近1分钟时间进行切换</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/36ac7562.png" alt="36ac7562" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>LVS负载均衡代码平滑上线发布思路</p>
<p>发布代码：</p>
<p>开发人员本地测试-一&gt;办公室内部测试（开发个人，测试人员）一（配置管理员）&ndash;&gt;IDC机房测试环境（测试人员）&ndash;&gt;正是服务器&ndash;&gt;运维上线&ndash;&gt;100台</p>
<p>一台一台下，测试完ok挂上去，此时，机器上代码不一致，用户体验就不同，此时需要下线一半，测试，测试完再下线另一半。</p>
<p>下线方法：</p>
<p>准备两套配置文件，一套配置文件含有前两台配置文件的配置，一套配置文件含有后两台配置文件的配置。最后用完整的配置文件替换。</p>
<p>方法二：用ipvsadm来控制节点的增加和删除。keepalived不重启节点就不会改变</p>
<p>生产场景测试步骤</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>heartbeat权威指南</title>
      <link>https://www.oomkill.com/2016/11/heartbeat/</link>
      <pubDate>Fri, 25 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2016/11/heartbeat/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="heartbeat介绍">heartbeat介绍</h2>
<p>Heartbeat一款开源提供高可用(Highly-Available)服务的软件，通过heartbeat，可以将资源（IP及程序服务等资源）从一台已经故障的计算机快速转移到另一台正常运转的机器上继续提供服务，一般称之为高可用服务。在实际生产应用场景中，heartbeat的功能和另一个高可用开源软件keepalived有很多相同之处，但在生产中，对应实际的业务应用也是有区别的，例如:keepalived主要是控制IP的漂移，配置、应用简单，而heartbeat则不但可以控制IP漂移，更搜长对资源服务的控制，配置、应用比较复杂</p>
<h2 id="heartbeat工作原理">heartbeat工作原理</h2>
<p>通过修改heartbeat软件的配置文件，可以指定哪一台Heartbeat服务器作为主服务器，则另一台将自动成为热备服务器.然后在热备服务器上配里Heartbeat守护程序来监听来自主服务器的心跳消息。如果热备服务器在指定时间内未监听到来自主服务器的心跳，就会启动故障转移程序，并取得主服务器上的相关资源服务的所有权，接替主服务器继续不间断的提供服务，从而达到资源及服务高可用性的目的。</p>
<p>以上描述的是heartbeat主备的模式，heartbeat还支持主主棋式，即两台服务器互为主备，这时它们之间会相互发送报文来告诉对方自己当前的状态，如果在指定的时间内未受到对方发送的心跳报文，那么，一方就会认为对方失效或者宕机了，这时每个运行正常的主机就会启动自身的资源接管模块来接管运行在对方主机上的资源或者服务，继续为用户提供服务。一般情况下，可以较好的实现一台主机故障后，企业业务仍能够不间断的持续运行。</p>
<p>注意：所谓的业务不间断，再故障转移期间也是需要切换时间的(例如:停止数据库及存储服务等)，heartbeat的主备高可用的切换时间一般是在5-20秒左右(服务器宕机的切换比人工切换要快)。</p>
<p>另外，和keepalived高可用软件一样，heartbeat高可用是操作系统级别的，不是服务(软件)级别的，可以通过简单的脚本控制.实现软件级别的高可用。</p>
<p><strong>高可用服务器切换的常见条件场景</strong>：</p>
<ol>
<li>主服务器物理宕机(硬件损坏，操作系统故障)。</li>
<li>Heartbeat服务软件本身故障。</li>
<li>两台主备服务器之间心跳连接故障。</li>
</ol>
<p><font style="background:#fee904;" size=3>服务故障不会导致切换.可以通过服务宕机把heartbeat服务停掉。</font></p>
<p>3 heartbeat心跳连接
经过前面的叙述，要部署heartbeat服务，至少需要两台主机来完成。那么，要实现高可用服务，这两台主机之间是如何做到互相通信和互相监侧的呢？</p>
<p><strong>下面是两台heartbeat主机之间通信的一些常用的可行方法</strong>：</p>
<ul>
<li>利用串行电缆，即所谓的串口线连接两台服务器(可选)。</li>
<li>一根以太网电缆两网卡直连(可选)。</li>
<li>以太网电缆，通过交换机等网络设备连接(次选)。</li>
</ul>
<h3 id="如何为高可用服务器端选择心跳通信方案">如何为高可用服务器端选择心跳通信方案？</h3>
<ol>
<li>
<p>串口线信号不会和以太网网络交集，也不需要单独配置丐地址等信息，因此传输稳定不容易出现问题，使用串口线的缺点是两个服务器对之间的距离距离不能太远，串口线对应服务端的设备为/dev/ttys0。串口线形状如下图所示：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221214230935860.png" alt="image-20221214230935860" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
</li>
<li>
<p>使用以太网网线(无需特殊交叉线了)直连网卡的方式，配置也比较简单，只需对这两块直连网线的网卡配好独立的IP段地址能够互相通信即可，普通的网线就可以了（推荐）</p>
</li>
<li>
<p>使用联网以太网网线和网卡作为心跳线是次选的方案，因为这个链路里增加了交换机设备这样的故障点，且这个线路不是专用心跳线路，容易受以太网其他数据传输的影响，导致心跳报文发送延迟或者无法送达问题。</p>
</li>
</ol>
<p><strong>选择方案小结</strong>：</p>
<ol>
<li>和数据相关的业务，要求较高，可以串口和网线直连的方式并用。</li>
<li>Web业务，可以网线直连的方式或局域网通信方式也可。</li>
</ol>
<h2 id="heartbeat软件未来发展说明">Heartbeat软件未来发展说明</h2>
<p>有关heartbeat分3个分支的说明</p>
<p>自2.1.4版本后，Linux-HA将Heartbeat分包成三个不同的子项目，并且提供了一个cluster-glue的组件，专用于Local ResourceManager 的管理。即heartbeat + cluster-glue + resouce-agent 三部分。</p>
<h3 id="heartbeat">Heartbeat</h3>
<p>hearbeat本身是整个集群的基础（cluster messaging layer），负责维护集群各节点的信息以及它们之前通信。</p>
<h3 id="cluster-glue">Cluster Glue</h3>
<p>相当于一个中间层，负责调度，可以将heartbeat和crm（pacemaker）联系起来，包括两个摸块:本地资漂管理(Local Resource Manager)LRM和STONITH。</p>
<h3 id="resource-agents">Resource Agents</h3>
<p>资源代理层，各种的资源的ocf脚本，这些脚本将被LRM调用从而实现各种资源启动、停止、监控等等。</p>
<p>Pacfrmaker资料</p>
<p>pacemaker介绍：http://baike.baidu.com/view/8635511.htm</p>
<p>从头开始搭建其群在Fedora上面创建主/主和主/备集群
<a href="http://www.clusterlabs.org/doc/zh-CN/Pacemaker/1.1/html-single/Clusters_from_Scratch/index.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.clusterlabs.org/doc/zh-CN/Pacemaker/1.1/html-single/Clusters_from_Scratch/index.html</a></p>
<p>参考文档：http://www.2cto.com/os/201511/448872.html</p>
<h2 id="裂脑">裂脑</h2>
<h3 id="什么是裂脑">什么是裂脑</h3>
<p>由于某些原因，导致两台高可用服务器对之间在指定时间内，无法互相检侧到对方心跳而各自启动故障转移功能，取得了资源及服务的所有权，而此时的两台高可用服务器对都还活着并在正常运行，这样就会导致同一个IP或服务在两端同时启动而发生冲突的严重问题，最严重的是两台主机占用同一个VIP地址，当用户写入数据时可能会分别写入到两端，这样可能会导致服务器两端的数据不一致或造成数据丢失，这种情况就被称为裂脑，也有人称其为分区集群或大脑垂直分割，英文为split brain。</p>
<h3 id="导致裂脑发生的多种原因">导致裂脑发生的多种原因</h3>
<p>一般来说，裂脑的发生，有以下几个原因。
高可用服务器对之间心跳线链路故障。导致无法正常通信。</p>
<ol>
<li>心跳线坏了(包括断了，老化)。</li>
<li>网卡及相关驱动坏了，IP配置及冲突问题(网卡直连)。</li>
<li>心跳线间连接的设备故障(网卡及交换机)。</li>
<li>仲裁的机器出问题(仲裁的方案)。
<ul>
<li>高可用服务器对上开启了如iptables防火墙阻挡了心跳的传输。</li>
<li>高可用服务器对上心跳网卡地址等信息配置不正确，导致发送心跳失败。</li>
<li>其它服务配置不当等原因，如心跳方式不同，心跳广播冲突、软件BUG等。</li>
</ul>
</li>
</ol>
<hr>
<p>提示:另外的高可用软件keepalived配置里如果virtual router_id参数，两端配置不一致，也会导致裂脑问题发生。</p>
<hr>
<h3 id="防止裂脑发生的8种秘籍">防止裂脑发生的8种秘籍</h3>
<p>发生裂脑时，对业务的影响是极其严重的，有时甚至是致命的。如:两台高可用服务器对之间发生裂脑，导致互相争用同一IP资源，就如同我们在局域网内常见的IP地址冲突一样，两个机器就会有一个或者两个都不正常，影响用户正常访问服务器。如果是应用在<font style="background:#fee904;" size=3>数据库或者存储服务</font>这种极重要的高可用上，那就可能会导致用户发布的数据<font style="background:#fee904;" size=3>间断的写在两台不同服务器上</font>，最终数据恢复极困难或难以恢复(当然，有NAS等公共存储的硬件也许会好一些）。</p>
<p>实际生产环境中，我们可以从以下几个方面来防止裂脑问题的发生。</p>
<ol>
<li>
<p>同时使用串行电缆和以太网电缆连接，同时用两条心跳线路，这样一条线路坏了，另一个还是好的，依然能传送心跳消息。(网卡设备和网线设备)。</p>
</li>
<li>
<p>当检测到裂脑时强行关闭一个心跳节点。（这个功能需要特殊设备支持，如stonith、fence）相当于程序上北街店发现心跳线故障，发送关机命令到主节点。</p>
</li>
<li>
<p>做好对裂脑的监控报警（如邮件及手机短信等，值班），在问题发生时人为第一时间介入仲裁，降低损失。百度的报警监控有上行和下行。和人工交互的过程。当然，在实施高可用方案时，要根据业务需求确定是否能容忍这样的损失。对于一般的网站常规业务，这个损失是可控的。</p>
</li>
<li>
<p>启用磁盘锁.正在服务一方锁住共享磁盘，“裂脑”发生时让对方完全“抢不走”共享磁盘资源。但使用锁磁盘也会有一个不小的问题，如果占用共享盘的一方<font style="background:#fee904;" size=3>不主动&quot;解锁&quot;</font>，另一方就永远得不到共享磁盘.现实中假如服务节点突然死机或崩演，就不可能执行解锁命令。后备节点也就接管不了共享资源和应用服务。于是有人在HA中设计了“智能”锁。即，<font style="background:#fee904;" size=2>正在服务的一方只在发现心跳线全部断开（察觉觉不到对端）时才启用磁盘锁。平时不上锁。此功能适合共享场景。</font></p>
</li>
<li>
<p>报警报在服务器接管之前，给人员你处理留够时间。1分钟内报警了，但是服务器此时没有接管，而是5分钟接管。接管的时间较长，数据不会丢，导致用户无法写数据。</p>
</li>
<li>
<p>报警后不自动服务器接管，而是由人为人员控制管理。</p>
</li>
<li>
<p>增加仲裁机制，确定谁该获得资派。这又有几个参考的思路：</p>
<ul>
<li>加一个仲截机制。例如设且参考IP(如网关IP).当心跳线完全断开时，2个节点都各自Ping一下参考IP，不通则表明断点就出在本端。不仅心跳线、还有对外服务的本地网络链路断了，这样就主动放弃竞争.让能够Ping通参考IP的一端去接管服务。Ping不通参考IP的一方可以自我重启，以彻底释放有可能还占用着的那些共享资源(heanbeat也有此功能)。</li>
<li>通过第三方软件仲裁谁该获得资源，这个在阿里的集团有类似的软件应用。</li>
</ul>
</li>
</ol>
<p><strong>小结:如何开发程序判断裂脑</strong>：</p>
<ol>
<li>简单判断，只要备节点出现VIP就是报普(a.主机宕机了，各机接管了。b.主机没宕，裂脑了)，不管哪个情况，人工查看。</li>
<li>严谨判断，备机出现VIP，并且主机及服务还活着，裂脑了（依赖报警）。</li>
</ol>
<h3 id="fence设备和仲裁机制">fence设备和仲裁机制</h3>
<p>先说下我以前做项目的环境，基本都是RHEL/CENTOS (以下简称RHEL),用的是RHCS集群套件，这个集群套件其实只是很多个软件整合在一起，在其他linux发行版里也有，只是比较零散，在RHEL中RHCS集群套件被做成了一个group，可以通过yum group install来安装集群套件，当然一个个rpm包安装也没问题。这个集群套件里还包含了一个LB的软件，就是LVS. Heartbeat和RHCS其实是差不多的东西，都是靠心跳来检测健康状态的，所以下面说的内容在Heartbeat应该也是通用的。</p>
<p>先说fence，fence只是HA集群环境下的术语，在硬件领域，fence设备其实就是一个只能电源管理设备(IPMI)，也叫做Intelligent PowerManagement Interface，如果你去和服务器代理商说fence，他们一定不知道是什么东西（原厂可能知道），你得和他们说是队们一定不知道是什么东西(原厂可能知道)，你得和他们说是智能电源管理设备或远程管理卡，他们就理解了，老师在视频里说这是一个特殊的插线板，这是fence设备的一种，叫做外部fence，还有一种叫内部fence.是插在服务器里的，不管是内部还是外部fence，这些设备都是带有以太网口的，用来在HA切换触发时通过网络重启提供资源服务的服务器。</p>
<p>至于外部fence设备，我只用过APC(著名的UPS电源生产商）的PowerSwitch, 这是一个带以太网口的电源插座，征每个插口都对应一个ID号，用来在命令中指定对哪一个ID号上的电源进行切断或者重启，为什么我当时会接触到外部fence设备呢，是因为当时做了一个医院的挂号系统，用的是IBM的小机装的RHEL5.x，因为小机上没有支持的内部fence，只有使用外部fence来实现了，至于为什么在小机上装RHEL这种2B方案（AIX也有成熟的HA解决方案)，得牵涉到商务上的忽悠，给客户返点各种营销上的潜规则，我才得以在实战中接触到这些东西。</p>
<p>在RHCS下有仲裁机制是一个叫做仲裁盘的东西，他是通过额外的存储来实现的，比如SAN，通过mkgdisk命令来制作的一个特殊块设备，这个设备做什么用呢?默认情况下双节点的HA架构，主从服务器的投票数都是1，双方使用的ping网关的方式来将自己的存活状态写入仲裁盘内，一旦节点心跳发生问题并且仲裁盘没有收到节点的存活信息，则启动fence设备来重启/关闭故障节点。这种方式可以有效的防止裂脑情况。缺点就是判断时间会不普通的HA时间长，这而要配合业务的需求，当时我们用RHCS+ORALCE+SAN存储来实现全国中信银行的帐务集中系统，数据必须保证完全一致，我们在实际测试中从故障到切换到备机接管能够提供服务的时间在2分钟以内。当然随着数据库增大，可能在启动Oracle数据库实例的时候会有所增加。以上就是我对fence设备和仲裁机制的个人补充，欢迎老师和大家拍砖。</p>
<h3 id="stonith">stonith</h3>
<h4 id="介绍">介绍</h4>
<p>stonith是“shoot the other node in the head”的首字母简写，它是Heartbeat软件包的一个组件，它允许使用一个远程或“智能的”连接到健康服务器的电源设备自动重启失效服务器的电源，stonith设备可以关闭电源并响应软件命令，运行Heartbeat的服务器可以通过串口线或网线向stonith设备发送命令，它控制高可用服务器对中其他服务器的电力供应，换句话说，主服务器可以复位备用服务器的电源，备用服务器也可以复位主服务器的电源。</p>
<p>注意:尽管理论上连接到远程或“智能的”循环电源系统的电力设备的数量是没有限制的，但大多数stonith实现只使用服务器，因为双服务器stonith配置是最简单的，最容易理解，它能够长时间运行且不系统的可靠性和高可用性。</p>
<h4 id="stonith事件触发工作步骤">Stonith事件触发工作步骤</h4>
<ol>
<li><strong>当备用服务器听不到心跳时Stontih事件开始。</strong></li>
</ol>
<p>注意:这并不一定意味着主服务器没有发送心跳，心跳可能有多种原因而没有抵达备用服务器，这就是为什么建议至少需要两条物理路径传输心跳以避免出现假象的原因了。
2. <strong>备用服务器发出一个Stonith复位命令到Stonith设备。</strong></p>
<ol start="3">
<li><strong>Stonith设备关闭主服务器的电力供应。</strong></li>
</ol>
<p>一经切断主服务器的电源，它就不能再访问集群资源，也不能再为客户端提供资源，保证客户端计算机不能访问主服务器上的资源，排除可能发生的头脑分裂状态。</p>
<p><strong>4. 然后备用服务器获得主服务器的资源，Heartbeat用start参数运行资源脚本，并执行arp欺骗广播以便客户端计算机发送它们的请求到它的网络接口上。</strong>
详情可参考Heartbeat高可用Stonith配置201503。</p>
<h3 id="heartbeat消息类型">Heartbeat消息类型</h3>
<p>Heartbeat高可用软件在工作过程中，一般来说，有三种消息类型，具体为：心跳消息、集群转换消息、重传请求</p>
<h4 id="心跳消息">心跳消息</h4>
<p>心跳消息为约150字节的数据包，可能为单播、广播或多播的方式，控制心跳频率及出现故障要等待多久进行故障转换。</p>
<h4 id="集群转换消息">集群转换消息</h4>
<p>ip-request和ip-request-respy</p>
<p>当主服务器恢复在线状态时，通过ip-request 消息要求备机释放主服务器失败时备服务器取得的资源，然后备份服务器关闭释放主服务器失败时取得的资源及服务。</p>
<p>备服务器释放主服务器失败时取得的资源及服务后，就会通过ip-request-resp消息通知主服务器它不在拥有该资源及服务，主服务器收到来自备节点的ip-request-resp消息通知后，启动失败时释放的资源及服务，并开始提供正常的访问服务。</p>
<p>提示:以上心跳控制消息都使用UDP协议发送到/etc/ha.d/ha.cf文件指定的任意端口，或指定的多播地址，如果使用多播默认端口为694</p>
<h4 id="跳实现方式及查看心跳消息">跳实现方式及查看心跳消息</h4>
<p>参考：http://blog.chinaunix.net/uid-7921481-id-1617030.html</p>
<h2 id="heartbeat-ip地址接管和故障转移">Heartbeat IP地址接管和故障转移</h2>
<p>Heartbeat是通过IP地址接管和ARP广播进行故陈转移的。</p>
<p>ARP广播:在主服务器故阵时，备用节点接管资源后.会立即强制更新所有客户端本地的ARP表(即清除客户端本地缓存的失败取务器的VIP地址和mac地址的解析记录)。确保客户端和新的主服务器对话。</p>
<h3 id="vipip别名辅助ip">VIP/IP别名/辅助IP</h3>
<h4 id="real-ip">real IP</h4>
<p>真实IP，又彼称为管理IP，一般是配置在物理网卡上的实际IP，这可以看作你本人的姓名，如:张三。在负载均衡及高可用环境中，管理IP是不对外提供用户访问服务的，而仅作为管理服务器用，如SSH可以通过这个管理IP连接服务器。</p>
<h4 id="vip">VIP</h4>
<p>虚拟IP即VIP，实际上救是heartbeat<font style="background:#fee904;" size=3>临时绑定在物理网卡上的别名IP</font> (heartbeat3以上也采用了辅助IP)。如eth0:x，x为0-255的任惫数字.你可以在一块网卡上绑定多个别名.这个VIP可以看作是你上网的QQ网名、呢称、外号等。<font style="background:#fee904;" size=3>在实际生产环境中，需要把DNS配置中把网站域名地址解析到这个VIP地址。由这个VIP对用户提供服务。</font></p>
<p>这样做的好处就是当提供服务的服务器宕机以后，在接管的服务器上直接会自动配置上同样的VIP提供服务。如果是使用管理IP的话，来回迁移就难以做到，而且，迁移走了。我们就只能去机房连接服务器了。VIP的实质就是确保两台服务器各有一个管理IP不动，就是随时可以连上机器，然后，增加绑定其他的VIP，这样就算VIP转移走了，也不至于服务器本身连不上，因为还有管理IP呢。</p>
<p>Linux系统给网卡配置VIP的方法常见的有两种，即别名和辅助IP###</p>
<h4 id="别名ipalias-ip">别名IP（alias ip）</h4>
<p>ip alias 是由 Linux 系统的 ifconfig 命令来创建和维护的，别名IP就是在网卡设备上绑定的第二个及以上的IP，例如：
<strong>手工配置别名VIP的方法</strong></p>
<pre><code class="language-bash">ifconfig eth0:1 192.168.1.1 netmask 255.255.255.0 up
ifconfig eth0:1 192.168.1.1/24 up #&lt;==up 关键字可省略默认为up
ifconfig eth0:1 192.168.1.1/24 down
</code></pre>
<p><strong>让别名IP永久生效</strong>
写入到网卡配置文件可以让别名IP永久生效，名字可以为ifcfg-eth0:x，x为0-255的任意数字，IP等内容格式和ifcfg-eth0一致，或者将命令写入/etc/rc.local</p>
<hr>
<p><font color="#0215cd" size=3>注意：别名IP在centos 7中被遗弃，用辅助IP替代。</font></p>
<hr>
<h4 id="辅助ipsecondary-ip-address">辅助IP（secondary ip address）</h4>
<p>辅助IP则是由Linux系统的ip命令创建和维护的，<code>ip addr add</code>创建的辅助IP，不能通过<code>ifconfig</code>查看，但是通过<code>ifconfig</code>创建的别名IP却可以在ip addr show 命令查看。</p>
<pre><code class="language-bash">ip addr add 192.168.3.3/24 dev eth0
ip addr del 192.168.3.3/24 dev eth0
</code></pre>
<p>heartbeat3 版本起，不在使用别名，而是使用辅助IP提供服务，而 <code>keepalived</code> 软件一直都是使用的辅助IP技术。</p>
<h2 id="部署heartbeat">部署heartbeat</h2>
<h3 id="heartbeat服务主机资源规划">heartbeat服务主机资源规划</h3>
<table>
<thead>
<tr>
<th>名称</th>
<th>接口</th>
<th>IP</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MATER</strong></td>
<td>eth0</td>
<td>192.168.2.22</td>
<td>外网管理IP，用于WAN数据转发。</td>
</tr>
<tr>
<td></td>
<td>eth1</td>
<td>10.0.0.1</td>
<td>内网管理IP，用于LAN数据转发。</td>
</tr>
<tr>
<td></td>
<td>eth2</td>
<td>10.1.0.1</td>
<td>用于服务器心跳连接（直连）。</td>
</tr>
<tr>
<td>VIP</td>
<td></td>
<td>172.168.1.1</td>
<td>用于提供应用程序A挂载服务。</td>
</tr>
<tr>
<td>BACKUP</td>
<td>eth0</td>
<td>192.168.2.82</td>
<td>外网管理IP，用于WAN数据转发。</td>
</tr>
<tr>
<td></td>
<td>eth1</td>
<td>10.0.0.3</td>
<td>内网管理IP，用于LAN数据转发。</td>
</tr>
<tr>
<td></td>
<td>eth2</td>
<td>10.1.0.3</td>
<td>用于服务器心跳连接（直连）。</td>
</tr>
<tr>
<td>VIP</td>
<td></td>
<td>10.1.0.3</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="安装heartbeat">安装heartbeat</h3>
<p><a href="http://blog.csdn.net/celeste7777/article/details/47808519" target="_blank"
   rel="noopener nofollow noreferrer" >http://blog.csdn.net/celeste7777/article/details/47808519</a>
<a href="http://www.cnblogs.com/zhanjindong/p/3618055.html#anzhuang" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.cnblogs.com/zhanjindong/p/3618055.html#anzhuang</a>
<a href="http://wangzhijian.blog.51cto.com/6427016/1708694?utm_source=tuicool&amp;utm_medium=referral" target="_blank"
   rel="noopener nofollow noreferrer" >http://wangzhijian.blog.51cto.com/6427016/1708694?utm_source=tuicool&utm_medium=referral</a></p>
<pre><code class="language-bash">rpm -ivh https://mirrors.aliyun.com/epel/epel-release-latest-6.noarch.rpm
</code></pre>
<hr>
<p><font color="#0215cd" size=3>注：centos 7 epel源内没有heartbeat。</font></p>
<hr>
<h4 id="yum安装heartbeat">yum安装heartbeat</h4>
<pre><code class="language-bash">yum install heartbeat -y
</code></pre>
<h4 id="编译安装heartbeat">编译安装heartbeat</h4>
<p>heartbeat3.x版本把安装包分成了4个部分，分别是：Cluster Glue、Resource Agents、heartbeat和pacemaker，所以要分别安装。</p>
<p>安装依赖</p>
<pre><code class="language-bash">yum install gcc \
gcc-c++ \
autoconf \
automake \
libtool \
glib2-devel \
libxml2-devel \
bzip2 bzip2-devel \
e2fsprogs-devel \
libxslt-devel \
libtool-ltdl-devel \
asciidoc -y
</code></pre>
<p><em><strong>创建用户</strong></em></p>
<pre><code class="language-bash">useradd hab -s /sbin/nologin -M
</code></pre>
<p><em><strong>安装Cluster Glue</strong></em></p>
<pre><code class="language-bash">./autogen

./configure \
--prefix=/app/heartbeat-3.0.6 \
--with-daemon-user=hab \
--with-daemon-group=hab \
--enable-fatal-warnings=no LIBS='/lib64/libuuid.so.1' #&lt;==指定uuid库文件
</code></pre>
<p><em><strong>编译Resource Agents</strong></em></p>
<pre><code class="language-bash">./autogen

./configure \
--prefix=/app/heartbeat-3.0.6 \
--with-daemon-user=hab \
--with-daemon-group=hab \
--enable-fatal-warnings=no LIBS='/lib64/libuuid.so.1' #&lt;==指定uuid库文件
</code></pre>
<p><em><strong>编译heartbeat</strong></em></p>
<pre><code class="language-bash">export CFLAGS=&quot;$CFLAGS -I/app/heartbeat-3.0.6/include -L/app/heartbeat-3.0.6/lib&quot;

./configure \
--prefix=/app/heartbeat-3.0.6 \
--with-daemon-user=hab \
--with-daemon-group=hab \
--enable-fatal-warnings=no LIBS='/lib64/libuuid.so.1'
</code></pre>
<h3 id="heartbeat说明">heartbeat说明</h3>
<pre><code class="language-sh">/etc/init.d/heartbeat #&lt;== 启动脚本
/etc/ha.d #&lt;==配置文件目录
/etc/ha.d/resource.d #&lt;==控制资源的脚本，被HA调用。也可以放到/etc/init.d下
</code></pre>
<hr>
<p><font color="#0215cd" size=3>提示:把脚本放到上面两个路径其中任意一个下面，然后在heartbeat的haresourc配置文件中配置脚本名称就能调用到该脚本，进而控制资源和服务的启动和关闭。</font></p>
<hr>
<h4 id="heartbeat配置文件">heartbeat配置文件</h4>
<p>heartbeat的默认配置文件目录为/etc/ha.d。heartbeat常用的配置文件有三个，分别为ha.c，authkey，haresource，如果你细看，可以发现名字信息就如其实际功能。</p>
<table>
<thead>
<tr>
<th>配置名称</th>
<th>作用</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>ha.cf</td>
<td>heartbeat参数配置文件</td>
<td>在这里配置heartbeat的一些基本参数</td>
</tr>
<tr>
<td>authkey</td>
<td>heartbeat认证文件</td>
<td>高可用服务器对之间根据对端authkey，对对端进行认证。</td>
</tr>
<tr>
<td>haresource</td>
<td>heartbeat资源配置文件</td>
<td>如配置启动IP资源及脚本程序，服务等，调用/etc/ha.d/resource.d下面配置</td>
</tr>
</tbody>
</table>
<h4 id="配置服务器心跳连接">配置服务器心跳连接</h4>
<p>eth2 10.0.0.1和eth2 10.1.0.3两块网卡之间是通过普通网线直连连接的，即不通过交换机，直接将两块网卡通过网线连在一起，用于做心跳检测或传输数据等。</p>
<p>高可用服务器对上的Heartbeat软件会利用这条心跳找来性查对端的权器足否存活，进而决定是
否做故障漂移，资源切换，来保证业务的连续性。</p>
<p>如条件允许，以上连接可同时使用，来加大保险系数防止裂肺问砚发生。在我的生产环境中，常使用前两者之一或者结合使用。本文的环节为一根以太网网线两网卡直连，也是近几年，在生产环境中选用的。选用原因:简单、容易部署、效果也不错。做一件事情有多种选择.往往到及后娜泛性价比方面的考虑。如:部署简单，维护方便，效果不是最好的，但也无不错的。这样就好了。并不是做什么都是最好的。实际工作中往往最好的是最不现实的。</p>
<h3 id="配置hacf文件">配置ha.cf文件</h3>
<p>heartbeat配置文件模板路径在</p>
<pre><code class="language-bash">/usr/share/doc/heartbeat-3.0.4/
</code></pre>
<p>ha.cf文件详细说明</p>
<pre><code class="language-bash">debugfile /var/log/ha-debug #&lt;== heartbeat的调试日志存放位置
logfile /var/log/ha-log #&lt;== heartbeat的日志存放位置
logfacility local0 #&lt;== 在syslog服务中配置通过local0设备接受日志
keepalive 2 #&lt;== 指定心跳间隔时间为2秒（即每两秒中在eth2上发一次广播）
deadtime 30 #&lt;== 指定若备用节点在30秒内没有收到主节点的心跳信号，则立即接管主节点的服务资源
warntime 10 #&lt;== 指定心跳延迟的时间为10秒。当10秒种内备份节点不能接收到主节点的心跳信号时，就会往日志中写入一个警告日志，但此时不会切换服务。
initdead 120 #&lt;== 指定在HEARTBEAT首次运行后，需要等待120秒才启动主服务器的任何资源。该选项用于解决这种情况产生的时间间隔。取值至少为deadtime的两倍。单机启动时会遇到vip绑定很慢，为正常现象。该值设置的长的原因
bcast eth2 #&lt;== 指明心跳使用以太网广播方式在eth2接口上进行广播。如使用两个实际网络来传送心跳，则 bcast eth1 eth2
mcast eth2 225.0.0.1 694 1 0 #&lt;== 设置广播通信使用的端口，694为默认使用的端口号
auto_failback on #&lt;== 用来定义当主节点恢复后，是否将服务自动切回.
node data-1-1 #&lt;== 主节点主机名，通过命令 uname -n查看
node data-1-3 #&lt;== 备用节点主机名，可以通过命令 uname-n 查看
crm no	#&lt;== 是否开启cluster resource manager（集群资源管理）功能
</code></pre>
<hr>
<p>还可以查/usr/share/doc/heanbeat-3.0.4/下的ha.cf.来了解更详细的参数信息</p>
<hr>
<h4 id="配置authkey文件">配置authkey文件</h4>
<p>软件提供的authkey默认文件并不是很复杂
<a href="http://wangzhijian.blog.51cto.com/6427016/1708694" target="_blank"
   rel="noopener nofollow noreferrer" >http://wangzhijian.blog.51cto.com/6427016/1708694</a></p>
<pre><code class="language-bash">Authentication file.  Must be mode 600 #&lt;==此处提到了authkey文件权限必须为600
Available methods: crc sha1, md5.  Crc doesn't need/want a key. #&lt;==可以设置的认证方法
sha1 is believed to be the &quot;best&quot;, md5 next best.
crc adds no security, except from packet corruption
</code></pre>
<pre><code class="language-bash">$ echo 111|sha1sum
63bea2e3b0c7cd2d1f98bc5b7a9951eafcfead0f  -

cat &gt;authkeys &lt;&lt;EOF 
auth 1
1 sha1 63bea2e3b0c7cd2d1f98bc5b7a9951eafcfead0f
EOF
</code></pre>
<h4 id="配置haresource文件">配置haresource文件</h4>
<p>编辑配置heartbeat资源文件<code>/etc/ha.d/haresources</code>
生产环境的配置如下：</p>
<pre><code class="language-bash">ha-b IPaddr::10.1.0.2/24/eth0
</code></pre>
<p><strong>配置haresource说明</strong>
<font style="background:#fee904;" size=2>ha-b为主机名</font>，表示初始状态会在ha-b绑定IP 10.0.0.17，<font style="background:#fee904;" size=2>IPaddr为heartbeat配置lP的默认脚本</font>，其后的lP等都是脚本的参数。10.0.0.17/24/eth0为集群对外服务的VIP，初始启动在ha-b上，<font style="background:#fee904;" size=2>24为子网掩码</font>，<font style="background:#fee904;" size=2>eth0为ip绑定的实际物理网卡</font>，为heartbeat提供对外服务的通信接口。</p>
<pre><code>ha-b drbddis:data Filesystem::/dev/drbd0::/data::ext3 rsdata IPaddr:10.0.0.1/24/eth0
</code></pre>
<ol>
<li>设置<font style="background:#02fa3c;" size=2>drbb，drbddisk::data</font></li>
<li>挂载/data到/dev/drbdO <font style="background:#02fa3c;" size=2>Filesystem::/dev/drbd0::/data/ext3</font></li>
<li>NFS/MFS服务配置 <font style="background:#02fa3c;" size=2>rsdata</font> (不要开机启动)</li>
<li>启动 VIP IPaddr::10.0.0.3/24/eth0</li>
</ol>
<hr>
<p>以上设置休验最好</p>
<hr>
<p>一旦heartbeat无法控制资源的启动，heartbeat会采取极端的措施.例如重启系统.来释放没法管理的资源。因此，<font style="background:#659bfd;" size=2>被管理的资源必须不能开机启动</font>。可以写脚本来张制控制资源的处理，来防止heartbeat重启。</p>
<h3 id="安装错误">安装错误</h3>
<pre><code class="language-bash">Oct 26 10:07:18 node1 heartbeat: [2063]: ERROR: Illegal directive [ucast] in /usr/local/heartbeat/etc/ha.d//ha.cf
Oct 26 10:07:18 node1 heartbeat: [2063]: ERROR: Illegal directive [ping] in /usr/local/heartbeat/etc/ha.d//ha.cf
</code></pre>
<p>解决方法：建立plugin软链接:</p>
<pre><code class="language-bash">ln -svf /app/heartbeat/lib64/heartbeat/plugins/* /app/heartbeat/lib/heartbeat/plugins/
</code></pre>
<h2 id="heartbeat高可用实战">heartbeat高可用实战</h2>
<h3 id="有关heartbeat调用资源的生产场景应用">有关heartbeat调用资源的生产场景应用</h3>
<p>在实际工作中有两种常见方法实现高可用问题：</p>
<ol>
<li>heartbeat可以仅控制vip资源的漂移，不负责服务资源的启动及停止，本节的httpd服务就可以这样做。适合web服务</li>
<li>heartbeat即控制vip资源的漂移，同时又控制服务资源启动及停止，本节的httpd服务例子既是ip和服务要切换都切换. ←适合数据服务(数据库和存储)只能一端写。</li>
</ol>
<p>VIP正常，httpd服务宕了.这个时候不会做高可用切换.写个简单的脚本定时或守护进程判断httpd服务，如果有问题，则停止heartbeat，主动使其上的业务到另一台。</p>
<p>两端服务能同时起，那最好不要交给heartbeat，对于某些服务，不能两端同时起，heartbeat可以控制服务启动。</p>
<h3 id="ha高可用httpd案例结论">ha高可用httpd案例结论</h3>
<ol>
<li>日志很重要。不管是heartbeat，所有服务的日志都很重要。有问题时多查看相关日志。</li>
<li>httpd的高可用还可以是两边都处于启动状态，即httpd不需要交给ha启动，而是默认状态就先启动运行。</li>
<li>这个httpd高可用性配置在生产环境中用的很少，但它确是生产环境需求的一个初级模型。</li>
</ol>
<p>如:heanbeat+drbd+mysql实现数据库高可用性配置，heanbeat+active/active+nfs/mfs实现存储高可用性配置。</p>
<h3 id="heartbeat和keepalived应用场景区别">heartbeat和keepalived应用场景区别</h3>
<ol>
<li>对于一般的web, db、负载均衡(nginx,haproxy )等等heartbeat和keepalived都可以实现。</li>
<li>lvs负载均衡最好和keepalived结合，虽然heartbeat也可以调用带有ipvsadm命令的脚本来启动和停止lvs负载均衡，但是heartbeat本身并没有对下面节点rs的健康检查功能，heartbeat的这个缺陷可以通过ldircetord插聆来弥补，所以，当你搜索heartbeat+lvs+ldirectord可以有lvs的别解决方案)。</li>
<li>需要要数据同步(配合drbd )的高可用业务最好用heartbeat，例如:mysgl双主多从，NFS/MFS
存储，他们的特点是需要数据同步，这样的业务最好用heartbeat.因为hearbeat自带了drbd的脚本，可以利用强大的drbd同步软件配合实现同步。如果你解决了数据同步可以不用drbd,例如:共享存储或者inotify+rsync (sersync+rsync)，那么就可以考虑keepalived。</li>
</ol>
<p>运维人员对哪个更热悉就用哪个，其实，就是你要能控制维护你部署的服务。目前，总体网友们更倾向于使用leepalived软件的多一些。</p>
<h3 id="heartbeat服务生产环境下维护要点">heartbeat服务生产环境下维护要点</h3>
<p>修改配置文件要点：在我们每天的实战运维工作中，当有新项目上线或者VIP更改需求时，可能会进行添加修改服务VIP的操作，那么，下面我们就以heartbeat+haproxy/nginx高可用负载均衡为例给大家讲解下生产环境下的维护方法。</p>
<p>所有配置放到SVN，更改后提交SVN，对比。推送到正式环境!</p>
<p>常见的情况就是修改配置文件，我们知道配置文件有3个：<code>ha.cf</code> <code>authkey</code> <code>haresourece</code>。在修改配置前执行<code>/etc/init.d/heartbeat stop</code>或<code>/app/heartbeat/share/heartbeat/hb_staudby</code>(编译安装)，<code>/usr/lib/heartbeat/hb_staudby</code>(此命令最好)</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
