<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>kubernetes develop on Cylon&#39;s Collection</title>
    <link>https://www.oomkill.com/tags/kubernetes-develop/</link>
    <description>Recent content in kubernetes develop on Cylon&#39;s Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 30 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.oomkill.com/tags/kubernetes-develop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深入理解Kubernetes - 基于OOMKill的QoS的设计</title>
      <link>https://www.oomkill.com/2024/01/ch30-oomkill/</link>
      <pubDate>Tue, 30 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/01/ch30-oomkill/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>阅读完本文，您当了解</p>
<ul>
<li>Linux oom kill</li>
<li>Kubernetes oom 算法</li>
<li>Kubernetes QoS</li>
</ul>
<blockquote>
<p>本文只是个人理解，如果有大佬觉得不是这样的可以留言一起讨论，参考源码版本为 1.18.20，与高版本相差不大</p>
</blockquote>
<h2 id="什么是oom-kill">什么是OOM Kill</h2>
<p>当你的Linux机器内存不足时，内核会调用Out of Memory (OOM) killer来释放一些内存。这经常在运行许多内存密集型进程的服务器上遇到。</p>
<h3 id="oom-killer是如何选择要杀死的进程的">OOM Killer是如何选择要杀死的进程的？</h3>
<p>Linux内核为每个运行的进程分配一个分数，称为 <code>oom_score</code>，==显示在内存紧张时终止该进程的可能性有多大==。该 Score 与进程使用的内存量成比例。 Score 是进程使用内存的百分比乘以10。因此，最大分数是 $100% \times 10 = 1000$。此外，如果一个进程以特权用户身份运行，那么与普通用户进程相比，它的 <code>oom_score</code> 会稍低。</p>
<p>在主发行版内核会将 <code>/proc/sys/vm/overcommit_memory</code> 的默认值设置为零，这意味着进程可以请求比系统中当前可用的内存更多的内存。这是基于以下启发式完成的：分配的内存不会立即使用，并且进程在其生命周期内也不会使用它们分配的所有内存。如果没有过度使用，系统将无法充分利用其内存，从而浪费一些内存。过量使用内存允许系统以更有效的方式使用内存，但存在 OOM 情况的风险。占用内存的程序会耗尽系统内存，使整个系统陷入瘫痪。当内存太低时，这可能会导致这样的情况：即使是单个页面也无法分配给用户进程，从而允许管理员终止适当的任务，或者内核执行重要操作，例如释放内存。在这种情况下，OOM Killer 就会介入，并将该进程识别为牺牲品，以保证系统其余部分的利益。</p>
<p>用户和系统管理员经常询问控制 OOM Killer 行为的方法。为了方便控制，引入了 <code>/proc/&lt;pid&gt;/oom_adj</code> 来防止系统中的重要进程被杀死，并定义进程被杀死的顺序。 oom_adj 的可能值范围为 -17 到 +15。Score 越高，相关进程就越有可能被 OOM-killer Kill。如果 <code>oom_adj</code> 设置为 -17，则 OOM Killer 不会  Kill 该进程。</p>
<p>oom_score 分数为 1 ~ 1000，值越低，程序被杀死的机会就越小。</p>
<ul>
<li>oom_score 0 表示该进程未使用任何可用内存。</li>
<li>oom_score  1000 表示该进程正在使用 100% 的可用内存，大于1000，也取1000。</li>
</ul>
<h3 id="谁是糟糕的进程">谁是糟糕的进程？</h3>
<p>在内存不足的情况下选择要被终止的进程是基于其  <em>oom_score</em> 。糟糕进程 Score 被记录在 <code>/proc/&lt;pid&gt;/oom_score</code> 文件中。该值是基于系统损失的最小工作量、回收的大量内存、不终止任何消耗大量内存的无辜进程以及终止的进程数量最小化（如果可能限制在一个）等因素来确定的。糟糕程度得分是使用进程的原始内存大小、其 CPU 时间（utime + stime）、运行时间（uptime - 启动时间）以及其 <code>oom_adj</code> 值计算的。进程使用的内存越多，得分越高。进程在系统中存在的时间越长，得分越小。</p>
<h3 id="列出所有正在运行的进程的oom-score">列出所有正在运行的进程的OOM Score</h3>
<pre><code class="language-bash">printf 'PID\tOOM Score\tOOM Adj\tCommand\n'
while read -r pid comm; do [ -f /proc/$pid/oom_score ] &amp;&amp; [ $(cat /proc/$pid/oom_score) != 0 ] &amp;&amp; printf '%d\t%d\t\t%d\t%s\n' &quot;$pid&quot; &quot;$(cat /proc/$pid/oom_score)&quot; &quot;$(cat /proc/$pid/oom_score_adj)&quot; &quot;$comm&quot;; done &lt; &lt;(ps -e -o pid= -o comm=) | sort -k 2nr
</code></pre>
<h3 id="如何检查进程是否已被-oom-终止">如何检查进程是否已被 OOM 终止</h3>
<p>最简单的方法是查看<code>grep</code>系统日志。在 Ubuntu 中：<code>grep -i kill /var/log/syslog</code>。如果进程已被终止，您可能会得到类似的结果</p>
<pre><code class="language-bash">my_process invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0
</code></pre>
<h2 id="kubernetes的qos是如何设计的">Kubernetes的QoS是如何设计的</h2>
<p>Kubernetes 中 Pod 存在一个 “服务质量等级” (<em>QoS</em>)，它保证了Kubernetes 在 Node 资源不足时使用 QoS 类来就驱逐 Pod 作出决定。这个 QoS 就是基于 OOM Kill Score 和 Adj 来设计的。</p>
<p>对于用户来讲，Kubernetes Pod 的 QoS 有三类，这些设置是被自动设置的，除此之外还有两种单独的等级：“Worker 组件”，总共 Pod QoS 的级别有5种</p>
<ul>
<li>Kubelet</li>
<li>KubeProxy</li>
<li>Guaranteed</li>
<li>Besteffort</li>
<li>Burstable</li>
</ul>
<p>这些在 <a href="pkg/kubelet/qos/policy.go">pkg/kubelet/qos/policy.go</a> 中可以看到，其中 Burstable 属于一个动态的级别。</p>
<pre><code class="language-go">const (
	// KubeletOOMScoreAdj is the OOM score adjustment for Kubelet
	KubeletOOMScoreAdj int = -999
	// KubeProxyOOMScoreAdj is the OOM score adjustment for kube-proxy
	KubeProxyOOMScoreAdj  int = -999
	guaranteedOOMScoreAdj int = -998
	besteffortOOMScoreAdj int = 1000
)
</code></pre>
<p>其中最重要的分数就是 Burstable，这保证了驱逐的优先级，他的算法为：$1000 - \frac{1000 \times Request}{memoryCapacity}$ ，Request 为 Deployment 这类清单中配置的 <em>Memory Request</em> 的部分，<em>memoryCapacity</em> 则为 Node 的内存数量。</p>
<p>例如 Node 为 64G，Pod Request 值配置了 2G，那么最终 <code>oom_score_adj</code> 的值为 $1000 - \frac{1000 \times Request}{memoryCapacity} = 1000 - \frac{1000\times2}{64} = 968$</p>
<p>这部分可以在下面代码中看到，其中算出的值将被写入 /proc/{pid}/oom_score_adj 文件内</p>
<pre><code class="language-go">func GetContainerOOMScoreAdjust(pod *v1.Pod, container *v1.Container, memoryCapacity int64) int {
	if types.IsNodeCriticalPod(pod) {
		// Only node critical pod should be the last to get killed.
		return guaranteedOOMScoreAdj
	}

	switch v1qos.GetPodQOS(pod) {
	case v1.PodQOSGuaranteed:
		// Guaranteed containers should be the last to get killed.
		return guaranteedOOMScoreAdj
	case v1.PodQOSBestEffort:
		return besteffortOOMScoreAdj
	}

	// Burstable containers are a middle tier, between Guaranteed and Best-Effort. Ideally,
	// we want to protect Burstable containers that consume less memory than requested.
	// The formula below is a heuristic. A container requesting for 10% of a system's
	// memory will have an OOM score adjust of 900. If a process in container Y
	// uses over 10% of memory, its OOM score will be 1000. The idea is that containers
	// which use more than their request will have an OOM score of 1000 and will be prime
	// targets for OOM kills.
	// Note that this is a heuristic, it won't work if a container has many small processes.
	memoryRequest := container.Resources.Requests.Memory().Value()
	if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) {
		if cs, ok := podutil.GetContainerStatus(pod.Status.ContainerStatuses, container.Name); ok {
			memoryRequest = cs.AllocatedResources.Memory().Value()
		}
	}
	oomScoreAdjust := 1000 - (1000*memoryRequest)/memoryCapacity
	// A guaranteed pod using 100% of memory can have an OOM score of 10. Ensure
	// that burstable pods have a higher OOM score adjustment.
	if int(oomScoreAdjust) &lt; (1000 + guaranteedOOMScoreAdj) {
		return (1000 + guaranteedOOMScoreAdj)
	}
	// Give burstable pods a higher chance of survival over besteffort pods.
	if int(oomScoreAdjust) == besteffortOOMScoreAdj {
		return int(oomScoreAdjust - 1)
	}
	return int(oomScoreAdjust)
}
</code></pre>
<p>到此可以了解到 Pod QoS 级别为</p>
<ul>
<li>
<p>Kubelet = KubeProxy = -999</p>
</li>
<li>
<p>Guaranteed = -998</p>
</li>
<li>
<p>1000(<em>Besteffort</em>) &gt; Burstable &gt; -998 (<em>Guaranteed</em>)</p>
</li>
<li>
<p>Besteffort = 1000</p>
</li>
</ul>
<p>那么在当 Node 节点内存不足时，发生驱逐的条件就会根据  <code>oom_score_adj</code> 完成，但当 Pod 中程序使用内存达到了 Limits 限制，此时的OOM Killed和上面阐述的无关。</p>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://lwn.net/Articles/317814/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Taming the OOM killer</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://unix.stackexchange.com/questions/153585/how-does-the-oom-killer-decide-which-process-to-kill-first" target="_blank"
   rel="noopener nofollow noreferrer" ><em>How does the OOM killer decide which process to kill first?</em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>client-go - Pod使用in-cluster方式访问集群</title>
      <link>https://www.oomkill.com/2023/11/ch07-in-cluster-pod/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/11/ch07-in-cluster-pod/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>在我们基于 Kubernetes 编写云原生 GoLang 代码时，通常在本地调试时，使用 kubeconfig 文件，以构建基于 clientSet 的客户端。而在将代码作为容器部署到集群时，则会使用集群 (in-cluster) 内的配置。</p>
<p>clientcmd 模块用于通过传递本地 kubeconfig 文件构建 clientSet。因此，在容器内使用相同模块构建 clientSet 将需要维护容器进程可访问的 kubeconfig 文件，并设置具有访问 Kubernetes 资源权限的 serviceaccount token。</p>
<p>下面是一个基于 kubeconfig 访问集群的代码模式</p>
<pre><code class="language-go">var (
    k8sconfig  *string //使用kubeconfig配置文件进行集群权限认证
    restConfig *rest.Config
    err        error
)
if home := homedir.HomeDir(); home != &quot;&quot; {
    k8sconfig = flag.String(&quot;kubeconfig&quot;, fmt.Sprintf(&quot;./admin.conf&quot;), &quot;kubernetes auth config&quot;)
}

flag.Parse()
if _, err := os.Stat(*k8sconfig); err != nil {
    panic(err)
}
clientset,err := kubernetes.NewConfig(k8sconfig)
if err != nil {
    panic(err)
}
</code></pre>
<p>这样做可能导致 serviceaccount token 本身被潜在地暴露出去。如果任何用户能够执行到使用 kubeconfig 与集群通信的容器，那么就可以获取该 token，并可以伪装成服务账号从集群外部与 kube-apiserver 进行通信。</p>
<p>为了避免这种情况，我们在 client-go 模块中使用了 rest 包。这将帮助我们从集群内部与集群通信，前提是使用适当的服务账号运行。但这需要对代码进行重写，以适应从集群外部构建 client-set 的方式。</p>
<p>下面代码时使用 in-cluster 方式进行通讯的模式</p>
<pre><code class="language-go">	var (
		k8sconfig  *string //使用kubeconfig配置文件进行集群权限认证
		restConfig *rest.Config
		err        error
	)
	if home := homedir.HomeDir(); home != &quot;&quot; {
		k8sconfig = flag.String(&quot;kubeconfig&quot;, fmt.Sprintf(&quot;./admin.conf&quot;), &quot;kubernetes auth config&quot;)
	}
	k8sconfig = k8sconfig
	flag.Parse()
	if _, err := os.Stat(*k8sconfig); err != nil {
		panic(err)
	}

	if restConfig, err = rest.InClusterConfig(); err != nil {
		// 这里是从masterUrl 或者 kubeconfig传入集群的信息，两者选一
        // 先从 in-cluster 方式获取，如果不能获取，再执行这里
		restConfig, err = clientcmd.BuildConfigFromFlags(&quot;&quot;, *k8sconfig)
		if err != nil {
			panic(err)
		}
	}
	restset, err := kubernetes.NewForConfig(restConfig)
</code></pre>
<p>除了这些之外，还需要创建对应的 serviceaccount 来让 Pod 在 in-cluster 有权限获取到自己要的资源，下面是一个完整的 deployment 创建这些资源的清单</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: infra
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-proxier-secret-reader
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;pods&quot;]
    verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pod-proxier-rolebinding
subjects:
  - kind: ServiceAccount
    name: pod-proxier-secret-sa
    namespace: infra
roleRef:
  kind: ClusterRole
  name: pod-proxier-secret-reader
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ServiceAccount
metadata:
  namespace: infra
  name: pod-proxier-secret-sa
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pod-proxier
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pod-proxier
  template:
    metadata:
      labels:
        app: pod-proxier
    spec:
      serviceAccount: pod-proxier-secret-sa # 使用上面定义的 sa 进行in-cluster 访问
      containers:
        - name: container-1
          image: haproxytech/haproxy-debian:2.6
          ports:
            - containerPort: 80
          hostPort: 8080  # 添加 hostPort 字段
        - name: container-2
          image: container-2-image:tag
          ports:
            - containerPort: 8080
          hostPort: 8081  # 添加 hostPort 字段
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes集群中的IP伪装 - ip-masq-agent</title>
      <link>https://www.oomkill.com/2023/10/ch24-ip-masq/</link>
      <pubDate>Sat, 28 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/10/ch24-ip-masq/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>“IP 伪装” 通常应用于云环境中，例如 GKE, AWS, CCE 等云厂商都有使用 “IP伪装” 技术，本文将围绕 “IP伪装” 技术本身，以及这项技术在 Kubernetes 集群中的实现应用 <em>ip-masq-agent</em> 的源码分析，以及 ”IP伪装“ 能为 Kubernetes 带来什么作用，这三个方向阐述。</p>
<h2 id="什么是ip伪装">什么是IP伪装？</h2>
<p>IP 伪装 (<em>IP Masquerade</em>)  是 Linux 中的一个网络功能，一对多 (1 to Many) 的网络地址转换 (NAT) 的功能 。</p>
<p>IP 伪装允许一组计算机通过 “伪装” 网关无形地访问互联网。对于互联网上的其他计算机，出站流量将看起来来自于 IP MASQ 服务器本身。互联网上任何希望发回数据包（作为答复）的主机必须将该数据包发送到网关 （IP MASQ 服务器本身）。记住，网关（IP MASQ 服务器本身）是互联网上唯一可见的主机。网关重写目标地址，用被伪装的机器的 IP 地址替换自己的地址，并将该数据包转发到本地网络进行传递。</p>
<p>除了增加的功能之外，IP Masquerade 为创建一个高度安全的网络环境提供了基础。通过良好构建的防火墙，突破经过良好配置的伪装系统和内部局域网的安全性应该会相当困难。</p>
<p>IP Masquerade 从 Linux 1.3.x 开始支持，目前基本所有 Linux 发行版都带有 IP 伪装的功能</p>
<h3 id="什么情况下不需要ip伪装">什么情况下不需要IP伪装</h3>
<ul>
<li>已经连接到互联网的独立主机</li>
<li>为其他主机分配了多个公共地址</li>
</ul>
<h2 id="ip伪装在kubernetes集群中的应用">IP伪装在Kubernetes集群中的应用</h2>
<p>IP 伪装通常应用在大规模 Kubernetes 集群中，主要用于解决 “地址冲突” 的问题，例如在 GCP 中，通常是一种 IP 可路由的网络模型，例如分配给 Pod service 的 ClusterIP 只能在 Kubernetes 集群内部可用，而分配 IP CIDR 又是一种不可控的情况，假设，我们为 k8s 分配的 IP CIDR 段如下表所示：</p>
<table>
<thead>
<tr>
<th>角色</th>
<th>IP CIDR</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kubernetes Nodes</td>
<td>10.0.0.0/16</td>
</tr>
<tr>
<td>Kubernetes Services</td>
<td>10.1.0.0/16</td>
</tr>
<tr>
<td>Kubernetes Pods</td>
<td>192.168.0.0/24</td>
</tr>
<tr>
<td>其他不可控业务网段</td>
<td>192.168.0.0/24</td>
</tr>
</tbody>
</table>
<p>通过上表可以看出，通常管理员在管理  Kubernetes 集群会配置三个网段，此时的配置，如果 Pod 需要与其他节点网络进行通讯（如我需要连接数据库），那么可能会出现 ”IP 重叠“ 的现象，尤其是在公有云环境中，<font color="#f8070d" size=3>用户在配置 Kubernetes 集群网络时不知道数据中心所保留的 CIDR 是什么</font>，在这种情况下就很容易产生 ”IP 重叠“ 的现象，为了解决这个问题，Kubernetes 提出了一种使用 “IP伪装” 技术来解决这个问题。</p>
<blockquote>
<p>在不使用 IP Masquerade 的情况下， Kubernetes 集群管理员如果在规划集群 CIDR 时，必须要了解了解整个组织中已预留/未使用的 CIDR 规划。</p>
</blockquote>
<h2 id="ip-masquerade-agent">IP Masquerade Agent</h2>
<p>IP伪装在 kubernetes 中的应用是名为 <em>ip-masq-agent</em> 的项目， <em>ip-masq-agent</em> 是用于配置 iptables 规则，以便在将流量发送到集群节点的 IP 和集群 IP 范围之外的目标时处理伪装节点或 Pod 的 IP 地址。这本质上隐藏了集群节点 IP 地址后面的 Pod IP 地址。在某些环境中，去往&quot;外部&quot;地址的流量必须从已知的机器地址发出。 例如，在 GCP 中，任何到互联网的流量都必须来自 VM 的 IP。 使用容器时，如 GKE，从 Pod IP 发出的流量将被拒绝出站。 为了避免这种情况，我们必须将 Pod IP 隐藏在 VM 自己的 IP 地址后面 - 通常称为&quot;伪装&quot;。 默认情况下，代理配置为将 RFC 1918指定的三个私有 IP 范围视为非伪装 CIDR。 这些范围是 <code>10.0.0.0/8</code>、<code>172.16.0.0/12</code> 和 <code>192.168.0.0/16</code>。 默认情况下，代理还将链路本地地址（169.254.0.0/16）视为非伪装 CIDR。 代理程序配置为每隔 60 秒从 <strong>/etc/config/ip-masq-agent</strong> 重新加载其配置， 这也是可修改的。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/ip-masq.png" alt="masq/non-masq example" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：ip-masq-agent工作原理</center>
<center><em>Source：</em>https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/ip-masq-agent/</center><br>
<p>默认情况下，CIDR <em>10.0.0.0/8</em>，<em>172.16.0.0/12</em>, <em>192.168.0.0/16</em> 范围内的流量不会被伪装。 任何其他 CIDR 流量将被伪装。 Pod 访问本地目的地的例子，可以是其节点 (Node) 的 IP 地址，另一节点 (Node) 的地址或集群的 IP 地址 (ClusterIP) 范围内的一个 IP 地址。 默认情况下，任何其他流量都将伪装。以下条目展示了 ip-masq-agent 的默认使用的规则：</p>
<pre><code class="language-bash">$ iptables -t nat -L IP-MASQ-AGENT
target     prot opt source               destination
RETURN     all  --  anywhere             169.254.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             10.0.0.0/8           /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             172.16.0.0/12        /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
RETURN     all  --  anywhere             192.168.0.0/16       /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL
</code></pre>
<h2 id="部署-ip-masq-agent">部署 ip-masq-agent</h2>
<p>ip-masq-agent 的部署可以直接使用官方提供的资源清单 <sup><a href="#1">[1]</a></sup></p>
<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml
</code></pre>
<p>清除 ip-masq-agent</p>
<pre><code class="language-yaml">kubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml
</code></pre>
<p>部署后需要同时将对应的节点标签应用于集群中希望代理运行的任何节点</p>
<pre><code class="language-bash">kubectl label nodes my-node node.kubernetes.io/masq-agent-ds-ready=true
</code></pre>
<p>配置好之后，需要创建配置，以对不伪装的地址增加白名单</p>
<pre><code class="language-bash">nonMasqueradeCIDRs:
  - 10.0.0.0/8
resyncInterval: 60s
</code></pre>
<h2 id="ip-masq-agent-深入解析">ip-masq-agent 深入解析</h2>
<p>ip-masq-agent 的代码很少，只有400多行，但是作用却很大，直接可以解决管理员集群网络规划与大拓扑网络的网络冲突问题，下面就分析他的原理，以及如何完成集群 IP 伪装功能</p>
<h3 id="ip-masq-agent源码的分析">ip-masq-agent源码的分析</h3>
<p>ip-masq-agent 只有这一个文件 cmd/ip-masq-agent/ip-masq-agent.go，包含了整个的业务逻辑</p>
<p>首先在 <a href="https://github.com/kubernetes-sigs/ip-masq-agent/blob/master/cmd/ip-masq-agent/ip-masq-agent.go#L139" target="_blank"
   rel="noopener nofollow noreferrer" >main()</a> 启动时，定义了这个链的名称，之后调用 Run()</p>
<pre><code class="language-go">masqChain = utiliptables.Chain(*masqChainFlag)

..

m.Run()
</code></pre>
<p>在 <a href="https://github.com/kubernetes-sigs/ip-masq-agent/blob/1a093b8aad7c372c535c1fb104377728c1e54a7a/cmd/ip-masq-agent/ip-masq-agent.go#L153-L175" target="_blank"
   rel="noopener nofollow noreferrer" >Run()</a> 中，只是做了周期性同步</p>
<pre><code class="language-go">func (m *MasqDaemon) Run() {
	// Periodically resync to reconfigure or heal from any rule decay
	for {
		func() {
			defer time.Sleep(time.Duration(m.config.ResyncInterval))
			// resync config
			if err := m.osSyncConfig(); err != nil {
				glog.Errorf(&quot;error syncing configuration: %v&quot;, err)
				return
			}
			// resync rules
			if err := m.syncMasqRules(); err != nil {
				glog.Errorf(&quot;error syncing masquerade rules: %v&quot;, err)
				return
			}
			// resync ipv6 rules
			if err := m.syncMasqRulesIPv6(); err != nil {
				glog.Errorf(&quot;error syncing masquerade rules for ipv6: %v&quot;, err)
				return
			}
		}()
	}
}
</code></pre>
<p>重点就在 m.osSyncConfig() , 这里做的是同步实际的规则</p>
<pre><code class="language-go">func (m *MasqDaemon) syncMasqRules() error {
    // 指定的链是否存在，如果不存在则创建，masqChain全局变量 是 main() 中初始化的名称，默认为IP-MASQ-AGENT
	m.iptables.EnsureChain(utiliptables.TableNAT, masqChain)

	// ensure that any non-local in POSTROUTING jumps to masqChain
	if err := m.ensurePostroutingJump(); err != nil {
		return err
	}

	// build up lines to pass to iptables-restore
	lines := bytes.NewBuffer(nil)
	writeLine(lines, &quot;*nat&quot;)
	writeLine(lines, utiliptables.MakeChainLine(masqChain)) // effectively flushes masqChain atomically with rule restore

	// local-link cidr 不伪装（&quot;169.254.0.0/16&quot;） 固定值
	if !m.config.MasqLinkLocal {
		writeNonMasqRule(lines, linkLocalCIDR)
	}

	// 用户定义的不伪装的 CIDR 部分
	for _, cidr := range m.config.NonMasqueradeCIDRs {
		if !isIPv6CIDR(cidr) {
			writeNonMasqRule(lines, cidr)
		}
	}

	// masquerade all other traffic that is not bound for a --dst-type LOCAL destination
	writeMasqRule(lines)

	writeLine(lines, &quot;COMMIT&quot;)

	if err := m.iptables.RestoreAll(lines.Bytes(), utiliptables.NoFlushTables, utiliptables.NoRestoreCounters); err != nil {
		return err
	}
	return nil
}

</code></pre>
<p>看完同步规则后，了解到上面就是两个操作，”伪装“ 和 “不伪装” 的操作如下所示</p>
<p>不伪装部分实际上就是关键词 RETURN</p>
<pre><code class="language-go">func writeNonMasqRule(lines *bytes.Buffer, cidr string) {
	writeRule(lines, utiliptables.Append, masqChain, nonMasqRuleComment, &quot;-d&quot;, cidr, &quot;-j&quot;, &quot;RETURN&quot;)
}
</code></pre>
<p>伪装部分实际上就是关键词 MASQUERADE</p>
<pre><code class="language-go">func writeMasqRule(lines *bytes.Buffer) {
	writeRule(lines, utiliptables.Append, masqChain, masqRuleComment, &quot;-j&quot;, &quot;MASQUERADE&quot;, &quot;--random-fully&quot;)
}
</code></pre>
<h3 id="伪装网络包的分析">伪装网络包的分析</h3>
<p>创建一个 ip-masq-agent 的配置文件</p>
<pre><code class="language-bash">tee &gt; config &lt;&lt;EOF
nonMasqueradeCIDRs:
  - 10.244.0.0/16
  - 192.0.0.0/8
resyncInterval: 60s
EOF
</code></pre>
<p>创建 configmap</p>
<pre><code class="language-yaml">kubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system
</code></pre>
<p>验证规则是否生效</p>
<pre><code class="language-bash">$ iptables -t nat -L IP-MASQ-AGENT
Chain IP-MASQ-AGENT (1 references)
target     prot opt source               destination         
RETURN     all  --  anywhere             link-local/16        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
RETURN     all  --  anywhere             10.244.0.0/16        /* ip-masq-agent: local traffic is not subject to MASQUERADE */
RETURN     all  --  anywhere             192.0.0.0/8          /* ip-masq-agent: local traffic is not subject to MASQUERADE */
MASQUERADE  all  --  anywhere             anywhere             /* ip-masq-agent: outbound traffic is subject to MASQUERADE (must be last in chain) */
</code></pre>
<p>抓包查看包是否被伪装</p>
<pre><code class="language-bash">$ cpid=`docker inspect --format '{{.State.Pid}}' 6b0a92ca4327`
$ nsenter -t $cpid -n ifconfig eth0|grep inet
        inet 10.244.196.132  netmask 255.255.255.255  broadcast 10.244.196.132
$ nsenter -t $cpid -n ping 10.0.0.2
$ tcpdump -i any icmp and host 10.0.0.2 -w icap.cap
</code></pre>
<p>通过导出的 wireshark 包，可以很清楚的看到，去往 10.0.0.2 的已经被伪装了</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20231105222224315.png" alt="image-20231105222224315" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes集群节点IP伪装抓包</center>
<h2 id="reference"><strong>Reference</strong></h2>
<p><sup id="1">[1]</sup> <a href="https://github.com/kubernetes-sigs/ip-masq-agent/blob/master/ip-masq-agent.yaml" target="_blank"
   rel="noopener nofollow noreferrer" >ip-masq-agent.yaml</a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/ip-masq-agent/" target="_blank"
   rel="noopener nofollow noreferrer" >IP Masquerade Agent 用户指南</a></p>
<p><sup id="3">[3]</sup> <a href="https://medium.com/google-cloud/ip-address-management-strategy-a-crucial-aspect-of-running-gke-f063fe90cfbd" target="_blank"
   rel="noopener nofollow noreferrer" >IP address management strategy — a crucial aspect of running GKE</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>kube-proxy参数ClusterCIDR做什么</title>
      <link>https://www.oomkill.com/2023/09/ch26-kube-proxy-clustercidr/</link>
      <pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/09/ch26-kube-proxy-clustercidr/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>我们可以看到，<em>kube-proxy</em> 有一个 <em>&ndash;cluster-cidr</em> 的参数，我们就来解开这个参数究竟有没有用</p>
<pre><code class="language-bash">$ kube-proxy -h|grep cidr
      --cluster-cidr string                          The CIDR range of pods in the cluster. When configured, traffic sent to a Service cluster IP from outside this range will be masqueraded and traffic sent from pods to an external LoadBalancer IP will be directed to the respective cluster IP instead
</code></pre>
<p>可以看到，参数说明是说，如果配置，那么从外部发往 Service Cluster IP 的流量将被伪装，从 Pod 发往外部 LB 将被直接发往对应的 cluster IP。但实际上做了什么并不知道，那么就从源码解决这个问题。</p>
<p>首先我们知道，参数是作为 kube-proxy server 的参数，位于 cmd/kube-proxy 下，而对应的逻辑则位于 pkg/kube-proxy 下，参数很明显，就是 clusterCIDR，那么我们就寻找这个参数的调用即可。</p>
<p>在 API <em>KubeProxyConfiguration</em> 中我们找到的对应的 <em>ClusterCIDR</em> ，在这里的注释又变为 ”用于桥接集群外部流量“。这里涉及到关于 <em>kube-proxy</em> 的两个模式 “LocalMode” 和 “ProxyMode“。</p>
<ul>
<li><em><strong>LocalMode</strong></em>：表示是来自节点本地流量的模式，包含 ClusterCIDR, NodeCIDR</li>
<li><em><strong>ProxyMode</strong></em>：就是 kube-proxy 最常用的模式，包含 iptables, IPVS, user namespace, kernelspace</li>
</ul>
<p>而参数 <em>&ndash;cluster-cidr</em> 是作为选择使用的 “本地网络检测器” (Local Network Detector)，这里起到的作用就是 “将集群外部的流量伪装成 service VIP” ，从代码中我们可以看到 Detector 将决定了你使用的是什么网络，无论是 <em>LocalMode</em> 还是 <em>ProxyMode</em>。</p>
<p>在代码 <a href="cmd/kube-proxy/app/server_others.go">cmd/kube-proxy/app/server_others.go</a> 中可以看到是如何选择的 <em>LocalMode</em> 方式，可以看出在存在三种模式：</p>
<ul>
<li>没有配置 <em>&ndash;cluster-cidr</em> 则会返回一个 <em>NoOpLocalDetector</em>；</li>
<li>在配置了 <em>&ndash;cluster-cidr</em> ，则将会使用 CIDR 的本地模式；</li>
<li>如果  <em>&ndash;cluster-cidr</em> 没有配置，但配置了 LocalModeNodeCIDR，则会设置为 CNI 为该 Node 配置的 POD CIDR 的地址 (使用参数 <em>&ndash;proxy-mode</em> 指定的模式，如果为空，那么会检测对应操作系统默认 Linux 为 iptables，如果内核开启 IPVS 那么则使用 IPVS，windows 默认为 kernelspace)</li>
</ul>
<pre><code class="language-go">func getLocalDetector(mode proxyconfigapi.LocalMode, config *proxyconfigapi.KubeProxyConfiguration, ipt utiliptables.Interface, nodeInfo *v1.Node) (proxyutiliptables.LocalTrafficDetector, error) {
	switch mode {
	case proxyconfigapi.LocalModeClusterCIDR:
		if len(strings.TrimSpace(config.ClusterCIDR)) == 0 {
			klog.Warning(&quot;detect-local-mode set to ClusterCIDR, but no cluster CIDR defined&quot;)
			break
		}
		return proxyutiliptables.NewDetectLocalByCIDR(config.ClusterCIDR, ipt)
	case proxyconfigapi.LocalModeNodeCIDR:
		if len(strings.TrimSpace(nodeInfo.Spec.PodCIDR)) == 0 {
			klog.Warning(&quot;detect-local-mode set to NodeCIDR, but no PodCIDR defined at node&quot;)
			break
		}
		return proxyutiliptables.NewDetectLocalByCIDR(nodeInfo.Spec.PodCIDR, ipt)
	}
	klog.V(0).Info(&quot;detect-local-mode: &quot;, string(mode), &quot; , defaulting to no-op detect-local&quot;)
	return proxyutiliptables.NewNoOpLocalDetector(), nil
}
</code></pre>
<p>这里我们以 IPVS 为例，如果开启了 localDetector 在 这个 <em>ipvs proxier</em> 中做了什么? 在代码 <a href="pkg/proxy/ipvs/proxier.go">pkg/proxy/ipvs/proxier.go</a> 可以看到</p>
<pre><code class="language-go">	if !proxier.ipsetList[kubeClusterIPSet].isEmpty() {
		args = append(args[:0],
			&quot;-A&quot;, string(kubeServicesChain),
			&quot;-m&quot;, &quot;comment&quot;, &quot;--comment&quot;, proxier.ipsetList[kubeClusterIPSet].getComment(),
			&quot;-m&quot;, &quot;set&quot;, &quot;--match-set&quot;, proxier.ipsetList[kubeClusterIPSet].Name,
		)
		if proxier.masqueradeAll {
			writeLine(proxier.natRules, append(args, &quot;dst,dst&quot;, &quot;-j&quot;, string(KubeMarkMasqChain))...)
		} else if proxier.localDetector.IsImplemented() {
			// This masquerades off-cluster traffic to a service VIP.  The idea
			// is that you can establish a static route for your Service range,
			// routing to any node, and that node will bridge into the Service
			// for you.  Since that might bounce off-node, we masquerade here.
			// If/when we support &quot;Local&quot; policy for VIPs, we should update this.
			writeLine(proxier.natRules, proxier.localDetector.JumpIfNotLocal(append(args, &quot;dst,dst&quot;), string(KubeMarkMasqChain))...)
		} else {
			// Masquerade all OUTPUT traffic coming from a service ip.
			// The kube dummy interface has all service VIPs assigned which
			// results in the service VIP being picked as the source IP to reach
			// a VIP. This leads to a connection from VIP:&lt;random port&gt; to
			// VIP:&lt;service port&gt;.
			// Always masquerading OUTPUT (node-originating) traffic with a VIP
			// source ip and service port destination fixes the outgoing connections.
			writeLine(proxier.natRules, append(args, &quot;src,dst&quot;, &quot;-j&quot;, string(KubeMarkMasqChain))...)
		}
	}
</code></pre>
<p>可以看到“不管使用了什么模式，都会更新一条 iptables 规则” 这就代表了使用了什么模式，而这个则被称之为 <em>LocalTrafficDetector</em>，也就是本地流量的检测，那我们看一下这个做了什么。</p>
<p>在使用 IPVS 的日志中，可以看到这样一条规则，这个是来自集群外部的 IP 去访问集群 CLUSTER IP (<em>KUBE-CLUSTER-IP</em>，即集群内所有 service IP) 时, 将非集群 IP 地址，转换为集群内的 IP 地址 (做源地址转换)</p>
<pre><code class="language-bash">[DetectLocalByCIDR (10.244.0.0/16)] Jump Not Local: [-A KUBE-SERVICES -m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot; -m set --match-set KUBE-CLUSTER-IP dst,dst ! -s 10.244.0.0/16 -j KUBE-MARK-MASQ]
</code></pre>
<p>而这个步骤分布在所有模式下 (iptables&amp;ipvs)，这里还是没说到两个概念 <em><strong>LocalMode</strong></em> 和 <em><strong>ProxyMode</strong></em>，实际上这两个模式的区别为：</p>
<ul>
<li><strong>LocalMode</strong>：集群 IP 伪装采用 <em>ClusterCIDR</em> 还是 <em>NodeCIDR</em>，<em>ClusterCIDR</em> 是使用集群 Pod IP 的地址段 (IP Range)，而 <em>LocalCIDR</em> 只仅仅使用被分配给该 kubernetes node 上的 Pod 做地址伪装</li>
<li><strong>ProxyMode</strong>：和 <em><strong>LocalMode</strong></em> 没有任何关系，是 <em>kube-proxy</em> 在运行时使用什么为集群 service 做代理，例如 iptables, ipvs ，而在这些模式下将采用什么 <em>LocalMode</em> 为集群外部地址作伪装，大概分为三种类型：
<ul>
<li>为来自集群外部地址 (<em>cluster-off</em>)：所有非 Pod 地址的请求执行跳转 (<em>KUBE-POSTROUTING</em>)</li>
<li>没有操作 ：在非 iptables/ipvs 模式下，不做伪装</li>
<li>masqueradeAll：为所有访问 cluster ip 的地址做伪装</li>
</ul>
</li>
</ul>
<h2 id="clustercidr-原理">ClusterCIDR 原理</h2>
<p><em>kube-proxy</em> 为 kube node 上生成一些 NAT 规则，如下所示</p>
<pre><code class="language-bash">-A KUBE-FIREWALL -j KUBE-MARK-DROP
-A KUBE-LOAD-BALANCER -j KUBE-MARK-MASQ
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-NODE-PORT -p tcp -m comment --comment &quot;Kubernetes nodeport TCP port for masquerade purpose&quot; -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ
-A KUBE-POSTROUTING -m comment --comment &quot;Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose&quot; -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE
-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN
-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0
-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -j MASQUERADE
-A KUBE-SERVICES ! -s 10.244.0.0/16 -m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot; -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ
-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT
-A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT
</code></pre>
<p>可以看到这里做了几个链，在 <em>KUBE-SERVICES</em> 链中指明了非来自 ClusterCIDR 的 IP 都做一个，并且访问的目的地址是 <em>KUBE-CLUSTER-IP</em> (ipset 里配置的地址) 那么将跳转到 <em>KUBE-MARK-MASQ</em> 链做一个 <code> --set-xmark 0x4000/0x4000</code> ，而在 <em>KUBE-POSTROUTING</em> 中对没有被标记 <code>0x4000/0x4000</code> 的操作不做处理</p>
<p>具体来说，<code>-A KUBE-NODE-PORT -p tcp -m comment --comment &quot;Kubernetes nodeport TCP port for masquerade purpose&quot; -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ</code> 做了如下操作：</p>
<ul>
<li><code>-A KUBE-SERVICES</code>：将这条规则附加到名为<code>KUBE-SERVICES</code>的iptables链。</li>
<li><code>! -s 10.244.0.0/16</code>：排除源IP地址为<code>10.244.0.0/16</code>的流量（即来自Kubernetes服务集群IP的流量）。</li>
<li><code>-m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot;</code>：添加一条注释，说明这个规则的用途。</li>
<li><code>-m set --match-set KUBE-CLUSTER-IP dst,dst</code>：使用IP集合<code>KUBE-CLUSTER-IP</code>来匹配目标IP地址和目标端口。</li>
<li><code>-j KUBE-MARK-MASQ</code>：如果流量匹配了前面的条件，将流量传递到名为<code>KUBE-MARK-MASQ</code>的目标。</li>
</ul>
<blockquote>
<p><code>iptables -j RETURN</code> 是用于iptables规则中的一个目标动作，它不是用于拒绝或接受数据包的动作，而是用于从当前规则链中返回（返回到调用链）的动作。</p>
<p>具体来说，当规则链中的数据包被标记为 <code>RETURN</code> 时，它们将不再受到当前链中后续规则的影响，而会立即返回到调用链，以便继续进行后续规则的处理。这通常用于某些高级设置，例如在自定义规则链中执行特定的操作后返回到主要的防火墙链。</p>
</blockquote>
<p>从代码中可以看到，对应执行 jump 的操作的链就是 <em>KUBE-MARK-MASQ</em></p>
<pre><code class="language-go">} else if proxier.localDetector.IsImplemented() {
			// This masquerades off-cluster traffic to a service VIP.  The idea
			// is that you can establish a static route for your Service range,
			// routing to any node, and that node will bridge into the Service
			// for you.  Since that might bounce off-node, we masquerade here.
			// If/when we support &quot;Local&quot; policy for VIPs, we should update this.
			writeLine(proxier.natRules, proxier.localDetector.JumpIfNotLocal(append(args, &quot;dst,dst&quot;), string(KubeMarkMasqChain))...)	

// KubeMarkMasqChain is the mark-for-masquerade chain
KubeMarkMasqChain utiliptables.Chain = &quot;KUBE-MARK-MASQ&quot;
    
// 具体拼接的就是 -j 链名的操作
func (d *detectLocalByCIDR) JumpIfNotLocal(args []string, toChain string) []string {
	line := append(args, &quot;!&quot;, &quot;-s&quot;, d.cidr, &quot;-j&quot;, toChain)
	klog.V(4).Info(&quot;[DetectLocalByCIDR (&quot;, d.cidr, &quot;)]&quot;, &quot; Jump Not Local: &quot;, line)
	return line
}
</code></pre>
<p>继续往下 <em>KUBE-POSTROUTING</em> 可以看到对应伪装是一个动态的源地址改造，而 <em>RETURN</em> 则不是被标记的请求</p>
<pre><code class="language-go">Chain KUBE-POSTROUTING (1 references)
target     prot opt source               destination         
MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0            /* Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose */ match-set KUBE-LOOP-BACK dst,dst,src
RETURN     all  --  0.0.0.0/0            0.0.0.0/0            mark match ! 0x4000/0x4000
MARK       all  --  0.0.0.0/0            0.0.0.0/0            MARK xor 0x4000
MASQUERADE  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */
</code></pre>
<p>这整体就是 ClusterCIDR 在 <em>kube-proxy</em> 中的应用，换句话说还需要关注一个 LocalCIDR</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解kubelet - VolumeManager源码解析</title>
      <link>https://www.oomkill.com/2023/08/ch29-volumemanager/</link>
      <pubDate>Sun, 20 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/08/ch29-volumemanager/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>阅读完本文，您当了解</p>
<ul>
<li>Kubernetes 卷</li>
<li>CephFS 在 kubernetes 中的挂载</li>
<li>Kubelet VolumeManager</li>
</ul>
<blockquote>
<p>本文只是个人理解，如果有大佬觉得不是这样的可以留言一起讨论，参考源码版本为 1.18.20，与高版本相差不大</p>
</blockquote>
<h2 id="volumemanager">VolumeManager</h2>
<p>VolumeManager VM 是在 kubelet 启动时被初始化的一个异步进程，主要是维护 “Pod&quot; 卷的两个状态，”desiredStateOfWorld“ 和 ”actualStateOfWorld“； 这两个状态用于将节点上的卷 “协调” 到所需的状态。</p>
<p>VM 实际上包含三个 “异步进程” (goroutine)，其中有一个 reconciler 就是用于协调与挂载的，下面就来阐述 VM 的挂载过程。</p>
<h3 id="vm中的重要组件">VM中的重要组件</h3>
<ul>
<li>actualStateOfWorld</li>
<li>mountedPod</li>
<li>desiredStateOfWorld</li>
<li>VolumeToMount</li>
<li>podToMount</li>
</ul>
<h3 id="vm的组成">VM的组成</h3>
<p>VM 的代码位于，由图可以看出，主要包含三个重要部分：</p>
<ul>
<li>reconciler：协调器</li>
<li>populator：填充器</li>
<li>cache：包含 ”desiredStateOfWorld“ 和 ”actualStateOfWorld“</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230820221712742.png" alt="image-20230820221712742" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：VM的目录组成</center>
<p>在代码结构上，<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/volume_manager.go#L216-L257" target="_blank"
   rel="noopener nofollow noreferrer" >volumeManager</a> 如下所示</p>
<pre><code class="language-go">// volumeManager implements the VolumeManager interface
type volumeManager struct {
    // DesiredStateOfWorldPopulator 用来与 API 服务器通信以获取 PV 和 PVC 对象的 API 客户端
	kubeClient clientset.Interface
    
    // VolumePluginMgr 是用于访问 VolumePlugin 插件的 VolumePlugin 管理器。它必须预初始化。
	volumePluginMgr *volume.VolumePluginMgr

    // desiredStateOfWorld 是一个数据结构，包含根据 VM 所需的状态：即应附加哪些卷以及 &quot;哪些pod” 正在引用这些卷。
    // 使用 kubelet pod manager 根据 world populator 的所需状态填充数据结构。
	desiredStateOfWorld cache.DesiredStateOfWorld

	// 与 desiredStateOfWorld 相似，是实际状态：即哪些卷被 attacted 到该 Node 以及 volume 被 mounted 到哪些 pod。
    // 成功完成 reconciler attach,detach, mount, 和 unmount 操作后，将填充数据结构。
	actualStateOfWorld cache.ActualStateOfWorld

	// operationExecutor 用于启动异步 attach,detach, mount, 和 unmount 操作。
	operationExecutor operationexecutor.OperationExecutor

	// reconciler reconciler 运行异步周期性循环，通过使用操作执行器触发 attach,detach, mount, 和 unmount操作
    // 来协调 desiredStateOfWorld 与 actualStateOfWorld。
	reconciler reconciler.Reconciler

    // desiredStateOfWorldPopulator 运行异步周期性循环以使用 kubelet Pod Manager 填充desiredStateOfWorld。
	desiredStateOfWorldPopulator populator.DesiredStateOfWorldPopulator

	// csiMigratedPluginManager keeps track of CSI migration status of plugins
	csiMigratedPluginManager csimigration.PluginManager

	// intreeToCSITranslator translates in-tree volume specs to CSI
	intreeToCSITranslator csimigration.InTreeToCSITranslator
}
</code></pre>
<h3 id="vm的初始化">VM的初始化</h3>
<ul>
<li>入口：“volumeManager”(vm) 的 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/kubelet.go#L1436" target="_blank"
   rel="noopener nofollow noreferrer" >初始化</a> 操作发生在 kubelet Run 时被作为一个异步进程启动。</li>
<li>VM 初始化：
<ul>
<li>如代码1所示，VM在初始化阶段创建了两个 cache 对象 “desiredStateOfWorld”（dsw）和“actualStateOfWorld”（asw）以及一个 “operationExecutor”，用于启动异步的线程操作 attach,detach, mount, 和 unmount</li>
<li>如代码2所示：VM在初始化阶段还创建了 “desiredStateOfWorldPopulator” (dswp) 与 “reconciler”
<ul>
<li>“reconciler” 通过使用上面的 “operationExecutor”  触发 attach,detach, mount 和 unmount来协调 dsw 与 asw</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>代码1</p>
<pre><code class="language-go">vm := &amp;volumeManager{
    kubeClient:          kubeClient,
    volumePluginMgr:     volumePluginMgr,
    desiredStateOfWorld: cache.NewDesiredStateOfWorld(volumePluginMgr),
    actualStateOfWorld:  cache.NewActualStateOfWorld(nodeName, volumePluginMgr),
    operationExecutor: operationexecutor.NewOperationExecutor(operationexecutor.NewOperationGenerator(
        kubeClient,
        volumePluginMgr,
        recorder,
        checkNodeCapabilitiesBeforeMount,
        blockVolumePathHandler)),
}
</code></pre>
<p>代码2：</p>
<pre><code class="language-go">vm.intreeToCSITranslator = intreeToCSITranslator
vm.csiMigratedPluginManager = csiMigratedPluginManager
vm.desiredStateOfWorldPopulator = populator.NewDesiredStateOfWorldPopulator(
    kubeClient,
    desiredStateOfWorldPopulatorLoopSleepPeriod,
    desiredStateOfWorldPopulatorGetPodStatusRetryDuration,
    podManager,
    podStatusProvider,
    vm.desiredStateOfWorld,
    vm.actualStateOfWorld,
    kubeContainerRuntime,
    keepTerminatedPodVolumes,
    csiMigratedPluginManager,
    intreeToCSITranslator)
vm.reconciler = reconciler.NewReconciler(
    kubeClient,
    controllerAttachDetachEnabled,
    reconcilerLoopSleepPeriod,
    waitForAttachTimeout,
    nodeName,
    vm.desiredStateOfWorld,
    vm.actualStateOfWorld,
    vm.desiredStateOfWorldPopulator.HasAddedPods,
    vm.operationExecutor,
    mounter,
    hostutil,
    volumePluginMgr,
    kubeletPodsDir)
</code></pre>
<h3 id="vm-的-run">VM 的 Run</h3>
<p>VM 是在 Kubelet 启动时作为异步线程启动，如代码1所示</p>
<p>如下面代码2所示，VM 在运行时会启动 <strong>三个</strong> 异步线程</p>
<ul>
<li>第一个调用是 第二个是调用 “dswp” 填充其的“ <a href="https://github.com/kubernetes/kubernetes/blob/v1.21.2/pkg/kubelet/volumemanager/volume_manager.go#L268" target="_blank"
   rel="noopener nofollow noreferrer" >Run</a> ”，这里主要做的操作是从 API 拿到 Pod 列表，根据对应条件来决定 attach,detach, mount, 和 unmount</li>
<li>第二个调用的是，reconciler 来协调应该安装的卷是否已安装以及应该卸载的卷是否已卸载。</li>
<li>第三个调用的是，volumePluginMgr，启用 CSI  informer</li>
</ul>
<p><a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/kubelet.go#L1435-L1436" target="_blank"
   rel="noopener nofollow noreferrer" >代码1</a></p>
<pre><code>// Start volume manager
go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop)
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/volume_manager.go#L259-L277" target="_blank"
   rel="noopener nofollow noreferrer" >代码2</a></p>
<pre><code class="language-go">func (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh &lt;-chan struct{}) {
	defer runtime.HandleCrash()

	go vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh)
	klog.V(2).Infof(&quot;The desired_state_of_world populator starts&quot;)

	klog.Infof(&quot;Starting Kubelet Volume Manager&quot;)
	go vm.reconciler.Run(stopCh)

	metrics.Register(vm.actualStateOfWorld, vm.desiredStateOfWorld, vm.volumePluginMgr)

	if vm.kubeClient != nil {
		// start informer for CSIDriver
		vm.volumePluginMgr.Run(stopCh)
	}

	&lt;-stopCh
	klog.Infof(&quot;Shutting down Kubelet Volume Manager&quot;)
}
</code></pre>
<h3 id="vm-的调用流程">VM 的调用流程</h3>
<h4 id="desiredstateofworldpopulator">desiredStateOfWorldPopulator</h4>
<p><code>DesiredStateOfWorldPopulator</code> 是一个周期 Loop，会定期循环遍历 Active Pod 列表，并确保每个 Pod 都处于所需状态（如果有卷，World state）。它还会验证 World cache 中处于所需状态的 pod 是否仍然存在，如果不存在，则会将其删除。</p>
<p>desiredStateOfWorldPopulator 结构包含两个方法，ReprocessPod 和 HasAddedPods；<strong><code>ReprocessPod</code></strong> 负责将 processedPods 中指定 pod 的值设置为false，强制重新处理它。这是在 Pod 更新时启用重新挂载卷所必需的。而 <strong><code>HasAddedPods</code></strong> 返回 填充器 是否已循环遍历 Active Pod 列表并将它们添加到 world cache 的所需状态。</p>
<p>在期待填充器 desiredStateOfWorldPopulator 启动时，会运行一个 populatorLoop，这里主要负责运行两个函数，</p>
<ul>
<li><code>findAndAddNewPods</code> 负责迭代所有 pod，如果它们不存在添加到  desired state of world (desiredStateOfWorld)</li>
<li><code>findAndRemoveDeletedPods</code> 负责迭代 <em><code>desiredStateOfWorld</code></em> 下的所有 Pod，如果它们不再存在则将其删除</li>
</ul>
<h4 id="reconciler">reconciler</h4>
<p>reconciler Run 的过程是通过一个 Loop 函数 <code>reconciliationLoopFunc</code> 完成的，正如下列 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/reconciler/reconciler.go#L128-L179" target="_blank"
   rel="noopener nofollow noreferrer" >代码</a> 所示</p>
<pre><code class="language-go">func (rc *reconciler) Run(stopCh &lt;-chan struct{}) {
	wait.Until(rc.reconciliationLoopFunc(), rc.loopSleepDuration, stopCh)
}

func (rc *reconciler) reconciliationLoopFunc() func() {
	return func() {
		rc.reconcile()

		// Sync the state with the reality once after all existing pods are added to the desired state from all sources.
		// Otherwise, the reconstruct process may clean up pods' volumes that are still in use because
		// desired state of world does not contain a complete list of pods.
		if rc.populatorHasAddedPods() &amp;&amp; !rc.StatesHasBeenSynced() {
			klog.Infof(&quot;Reconciler: start to sync state&quot;)
			rc.sync()
		}
	}
}

func (rc *reconciler) reconcile() {
   // Unmounts are triggered before mounts so that a volume that was
   // referenced by a pod that was deleted and is now referenced by another
   // pod is unmounted from the first pod before being mounted to the new
   // pod.
   // 卸载会在挂载之前触发，以便已删除的 Pod 引用的卷现在被另一个 Pod 引用，
   // 然后再挂载到新 Pod 之前从第一个 Pod 中卸载。
   rc.unmountVolumes()

   // Next we mount required volumes. This function could also trigger
   // attach if kubelet is responsible for attaching volumes.
   // If underlying PVC was resized while in-use then this function also handles volume
   // resizing.
   // 接下来我们安装所需的卷。如果 kubelet 负责附加卷，
   // 则此函数还可以触发附加。如果底层 PVC 在使用时调整了大小，则此函数还可以处理卷大小调整。
   rc.mountAttachVolumes()

   // Ensure devices that should be detached/unmounted are detached/unmounted.
   // 确保应 detached/unmounted 的设备已完成 detached/unmounted。
   rc.unmountDetachDevices()
}
</code></pre>
<p>Reconciler <strong>是挂载部分最重要的角色</strong>，用于协调应该安装的卷是否已安装以及应该卸载的卷是否已卸载；对应的，实际上执行的为三个函数：“unmountVolumes”、“mountAttachVolumes” 和 “unmountDetachDevices”。</p>
<h5 id="mountattachvolumes">mountAttachVolumes</h5>
<ol>
<li>
<p>首先，“<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/reconciler/reconciler.go#L202-L292" target="_blank"
   rel="noopener nofollow noreferrer" >mountAttachVolumes</a>” 会调用 “dsw” (desiredStateOfWorld) 的函数 “GetVolumesToMount” 来检索所有 “volumesToMount” 并迭代它们，这里主要是为了确保 “volumes” 应完成了 “attached/mounted”</p>
</li>
<li>
<p>接下来这个循环做的工作是，对于每个 Volume 和 Pod，都会检查该 Volume 或 Pod 是否存在于 “asw” 的 “attachedVolumes” 中。如果 Volume 不存在，则“asw”返回 “<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/kubelet/volumemanager/cache/actual_state_of_world.go#L662" target="_blank"
   rel="noopener nofollow noreferrer" >newVolumeNotAttachedError</a> ”，否则它检查指定的 pod 是否存在并根据状态返回结果。这里存在 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_executor.go#L400-L409" target="_blank"
   rel="noopener nofollow noreferrer" >三个状态</a>，返回也是根据这个状态返回。<strong>这里主要为了得到挂载路径和是否挂载</strong></p>
<ul>
<li>
<p>VolumeMounted：表示 Volume 已挂载到 pod 的本地路径中</p>
</li>
<li>
<p>VolumeMountUncertain：表示 Volume 可能会也可能不会安装在 Pod 的本地路径中</p>
</li>
<li>
<p>VolumeNotMounted：表示 Volume  还未挂载到 pod 的本地路径中</p>
</li>
</ul>
</li>
<li>
<p>当“ asw” 返回 “ <a href="https://github.com/kubernetes/kubernetes/blob/v1.21.2/pkg/kubelet/volumemanager/reconciler/reconciler.go#L207" target="_blank"
   rel="noopener nofollow noreferrer" >newVolumeNotAttachedError</a> ” 时，“reconciler” 会检查 “controllerAttachDetachEnabled” 是否启用，或 “<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_executor.go#L336" target="_blank"
   rel="noopener nofollow noreferrer" >volumeToMount</a>” 没有实现了对应插件，这里面如果其中任何一个为 true，“reconciler” 将调用 “operationExecutor” 来执行操作“ ，走到这里代表了 Volume 没有被 attach，或者没有实现 attacher，例如 cephfs 没有实现 attacher；或者是 kubelet 禁用了 attach  <sup><a href="#1">[1]</a></sup> （默认是开启状态），将进入 “ <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_executor.go#L909-L921" target="_blank"
   rel="noopener nofollow noreferrer" >VerifyControllerAttachedVolume</a> ”</p>
<ul>
<li>
<p>在此期间，“operationExecutor” 生成一个名为 “<a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_generator.go#L1278-L1352" target="_blank"
   rel="noopener nofollow noreferrer" >verifyControllerAttachedVolumeFunc</a>” 的函数来实际实现。在此函数中，如果 “volumeToMount” 的 “PluginIsAttachable” 为 <em>false</em>（没有实现），则假设其已经实现并标记 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_generator.go#L1288C17-L1301" target="_blank"
   rel="noopener nofollow noreferrer" >attached</a>，标记出错时进行重试（这是一个函数用于后面的调用，这里只是定义）</p>
</li>
<li>
<p>如果还没有将 Node attached 到 Volume 节点列表状态中，则返回错误进行重试（这是一个函数用于后面的调用，这里只是定义）</p>
</li>
<li>
<p>上面两个步骤是为了组装这个操作，返回的是操作的内容，包含执行的函数，完成的hook等，最后运行这个函数并返回</p>
</li>
</ul>
</li>
<li>
<p>这是步骤3的另外一个分支，即 kubelet 启用了 ADController，并且实现了对应的 attcher，那么将执行附加操作</p>
<ul>
<li>拼接对象</li>
<li>执行函数 ”AttachVolume“</li>
<li>AttachVolume 如上面步骤一样，拼接出最后的执行的动作，进行执行操作（将 node 附加到 volume 之上）</li>
</ul>
</li>
<li>
<p><a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_executor.go#L798-L958" target="_blank"
   rel="noopener nofollow noreferrer" >步骤5</a> 表示 3, 4 条件均不满足，也就是 Attached，目前状态为 ”未挂载“  或者 ”已挂载“，将执行这个步骤，未挂载的进行挂载，已挂载的进行 remount</p>
</li>
<li>
<p>在该分支中（也就是 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/util/operationexecutor/operation_generator.go#L489-L694" target="_blank"
   rel="noopener nofollow noreferrer" >步骤5</a> 执行的）执行的是名为 “<a href="https://github.com/kubernetes/kubernetes/blob/v1.21.2/pkg/volume/util/operationexecutor/operation_generator.go#L500" target="_blank"
   rel="noopener nofollow noreferrer" >GenerateMountVolumeFunc</a>“ 的函数，在此函数中，会获取 Plugin ，并通过 Plugin 创建出一个 volumeMounter，在通过 Plugin 获取一个 deviceMouter（能够挂载块设备的）；当然我们这里挂载的是 ”cephfs“ 所以没有 ”deviceMouter“ 这里不被执行。</p>
<ul>
<li>如果 ”deviceMounter“ 定义了，那么则执行这个 plugin 的 &ldquo;MountDevice&rdquo; 函数</li>
<li>如果没有定义，那么执行 volumeMounter 的 SetUp 进行挂载（因为不是块设备）</li>
</ul>
</li>
<li>
<p>执行 SetUp 函数，通常 <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/nfs/nfs.go#L240-L242" target="_blank"
   rel="noopener nofollow noreferrer" >NFS</a>, <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/cephfs/cephfs.go#L223-L225" target="_blank"
   rel="noopener nofollow noreferrer" >CephFS</a>, <a href="https://github.com/kubernetes/kubernetes/blob/1f3e19b7beb1cc0110255668c4238ed63dadb7ad/pkg/volume/local/local.go#L472-L474" target="_blank"
   rel="noopener nofollow noreferrer" >HostPath</a>，都实现了这个函数，那么就会通过这个函数挂载到 Node 对应的目录</p>
</li>
<li>
<p>最后通过 Overlay2 文件系统附加到容器里</p>
</li>
</ol>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>command-line-tools-reference kubelet</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://shuanglu1993.medium.com/what-happens-when-volumemanager-in-the-kubelet-starts-1fea623ac6ce" target="_blank"
   rel="noopener nofollow noreferrer" ><em>What happens when volumeManager in the kubelet starts?</em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析 - Kubernetes中的事件通知机制</title>
      <link>https://www.oomkill.com/2023/06/kubernetes-event/</link>
      <pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/06/kubernetes-event/</guid>
      <description>文章分析了Kuberentes事件记录及事件是如何调度的</description>
      <content:encoded><![CDATA[<p>在 Kubernetes 中 事件 ( <em>Event</em> ） 通常被大家认知为是展示集群中发生的情况，通常用作 Pod 的查看，例如为什么 CrashBackOff, 为什么 Pendding，而很少有人知道事件在 Kubernetes 整个系统中的设计是非常巧妙的，可以通过各组件间的传递，使得用户可以知道集群中的情况，文章中将一地揭开Kubernetes to神秘面纱。</p>
<h2 id="为什么需要事件">为什么需要事件</h2>
<p>Kubernetes 在设计时就是 “声明式”，而声明式的最大特点就是 “多组件的协同工作”，而在多组件协同工作时，势必需要传递一些事件，以告知用户任务的状态如何；而事件本身上是一种资源，在很早版本就以及被移入 api/v1 中。下面是 “事件” 资源的定义。</p>
<p>位于 <a href="vendor/k8s.io/api/core/v1/types.go">vendor/k8s.io/api/core/v1/types.go</a> ，因为 <code>vendor/k8s.io</code> 实际上是做了一个软连接，那么真实的实际上位于 <code>{kubernetes_repo}/staging/src/k8s.io/api/core/v1</code></p>
<pre><code class="language-go">type Event struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// 标准的元数据
	metav1.ObjectMeta `json:&quot;metadata&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

	// 事件涉及的对象
	InvolvedObject ObjectReference `json:&quot;involvedObject&quot; protobuf:&quot;bytes,2,opt,name=involvedObject&quot;`

	// 这里表示的是事件原因，通常为简短的的一种状态名称
	// TODO: provide exact specification for format.
	// +optional
	Reason string `json:&quot;reason,omitempty&quot; protobuf:&quot;bytes,3,opt,name=reason&quot;`

	// 以人类可读取的方式描述，类似于 tcmpdump -A
	// TODO: decide on maximum length.
	// +optional
	Message string `json:&quot;message,omitempty&quot; protobuf:&quot;bytes,4,opt,name=message&quot;`

	// 报告事件的组件，通常包含这个结构体包含 “组件+主机名” 的结构
	// +optional
	Source EventSource `json:&quot;source,omitempty&quot; protobuf:&quot;bytes,5,opt,name=source&quot;`

	// 首次上报事件的事件
	// +optional
	FirstTimestamp metav1.Time `json:&quot;firstTimestamp,omitempty&quot; protobuf:&quot;bytes,6,opt,name=firstTimestamp&quot;`

	// 最近一次记录事件的事件
	// +optional
	LastTimestamp metav1.Time `json:&quot;lastTimestamp,omitempty&quot; protobuf:&quot;bytes,7,opt,name=lastTimestamp&quot;`

	// 事件发生的次数
	// +optional
	Count int32 `json:&quot;count,omitempty&quot; protobuf:&quot;varint,8,opt,name=count&quot;`

    // 事件的类型(Normal, Warning)
	// +optional
	Type string `json:&quot;type,omitempty&quot; protobuf:&quot;bytes,9,opt,name=type&quot;`

	// 首次观察到事件的.
	// +optional
	EventTime metav1.MicroTime `json:&quot;eventTime,omitempty&quot; protobuf:&quot;bytes,10,opt,name=eventTime&quot;`

    // 事件相关的序列，如果事件为单例事件，那么则为nil
	// +optional
	Series *EventSeries `json:&quot;series,omitempty&quot; protobuf:&quot;bytes,11,opt,name=series&quot;`

	// 对事件对象采取的行动
	// +optional
	Action string `json:&quot;action,omitempty&quot; protobuf:&quot;bytes,12,opt,name=action&quot;`

	// Optional secondary object for more complex actions.
	// +optional
	Related *ObjectReference `json:&quot;related,omitempty&quot; protobuf:&quot;bytes,13,opt,name=related&quot;`

	// 发出事件的对应的控制器，也可以理解为组件，因为通常controller-manager 包含多个控制器
    // e.g. `kubernetes.io/kubelet`.
	// +optional
	ReportingController string `json:&quot;reportingComponent&quot; protobuf:&quot;bytes,14,opt,name=reportingComponent&quot;`

	// 控制器实例的ID, e.g. `kubelet-xyzf`.
	// +optional
	ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;`
}
</code></pre>
<h2 id="事件管理器">事件管理器</h2>
<p>通过上面知道了事件这个资源的设计，里面存在一个 ”发出事件的对应的控制器“ 那么必然是作为每一个组件的内置功能，也就是说这可以作为 client-go 中的一个组件。</p>
<p>代码 <a href="vendor/k8s.io/client-go/tools/events/interfaces.go">vendor/k8s.io/client-go/tools/events/interfaces.go</a> 中定义了一个事件管理器，这将定义了如何接收或发送事件到任何地方，例如事件接收器 (<em>EventSink</em>) 或 log</p>
<pre><code class="language-go">type EventBroadcaster interface {
    // 发送从指定的eventBroadcaster接收到的事件
	StartRecordingToSink(stopCh &lt;-chan struct{})

    // 返回一个 EventRecorder 并可以使用发送事件到 EventBroadcaster，并将事件源设置为给定的事件源。
	NewRecorder(scheme *runtime.Scheme, reportingController string) EventRecorder

    // StartEventWatcher 可以使在不使用 StartRecordingToSink 的情况下发送事件
    // 这使得可以通过自定义方式记录事件
    // NOTE: 在使用 eventHandler 接收到的事件时应先进行复制一份。
	// TODO: figure out if this can be removed.
	StartEventWatcher(eventHandler func(event runtime.Object)) func()

    // StartStructuredLogging 可以接收 EventBroadcaster 发送的结构化日志功能
    // 如果需要可以忽略返回值或使用于停止记录
	StartStructuredLogging(verbosity klog.Level) func()

    // 关闭广播
	Shutdown()
}
</code></pre>
<p>EventBroadcaster 的实现只有一个 eventBroadcasterImpl</p>
<pre><code>type eventBroadcasterImpl struct {
   *watch.Broadcaster
   mu            sync.Mutex
   eventCache    map[eventKey]*eventsv1.Event
   sleepDuration time.Duration
   sink          EventSink
}
</code></pre>
<p>这里面最重要的就是 sink，sink就是决定如何去存储事件的一个组件，他返回的是一组 client-go 的 REST 客户端。</p>
<h3 id="事件管理器的设计">事件管理器的设计</h3>
<h3 id="事件生产者">事件生产者</h3>
<p>事件生产者在事件管理器中是作为</p>
<h2 id="控制器">控制器</h2>
<p>service的资源创建很奇妙，继不属于 <code>controller-manager</code> 组件，也不属于 <code>kube-proxy</code> 组件，而是存在于 <code>apiserver</code> 中的一个被成为控制器的组件；而这个控制器又区别于准入控制器。更准确来说，准入控制器是位于kubeapiserver中的组件，而 <strong>控制器</strong> 则是存在于单独的一个包，这里包含了很多kubernetes集群的公共组件的功能，其中就有service。这也就是在操作kubernetes时 当 <code>controller-manager</code> 于  <code>kube-proxy</code> 未工作时，也可以准确的为service分配IP。</p>
<p>首先在构建出apiserver时，也就是代码 <a href="cmd/kube-apiserver/app/server.go">cmd/kube-apiserver/app/server.go</a></p>
<pre><code class="language-go">serviceIPRange, apiServerServiceIP, err := master.ServiceIPRange(s.PrimaryServiceClusterIPRange)
if err != nil {
    return nil, nil, nil, nil, err
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/pkg/master/services.go#L34-L54" target="_blank"
   rel="noopener nofollow noreferrer" >master.ServiceIPRange</a> 承接了为service分配IP的功能，这部分逻辑就很简单了</p>
<pre><code class="language-go">func ServiceIPRange(passedServiceClusterIPRange net.IPNet) (net.IPNet, net.IP, error) {
	serviceClusterIPRange := passedServiceClusterIPRange
	if passedServiceClusterIPRange.IP == nil {
		klog.Warningf(&quot;No CIDR for service cluster IPs specified. Default value which was %s is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.&quot;, kubeoptions.DefaultServiceIPCIDR.String())
		serviceClusterIPRange = kubeoptions.DefaultServiceIPCIDR
	}

	size := integer.Int64Min(utilnet.RangeSize(&amp;serviceClusterIPRange), 1&lt;&lt;16)
	if size &lt; 8 {
		return net.IPNet{}, net.IP{}, fmt.Errorf(&quot;the service cluster IP range must be at least %d IP addresses&quot;, 8)
	}

	// Select the first valid IP from ServiceClusterIPRange to use as the GenericAPIServer service IP.
	apiServerServiceIP, err := utilnet.GetIndexedIP(&amp;serviceClusterIPRange, 1)
	if err != nil {
		return net.IPNet{}, net.IP{}, err
	}
	klog.V(4).Infof(&quot;Setting service IP to %q (read-write).&quot;, apiServerServiceIP)

	return serviceClusterIPRange, apiServerServiceIP, nil
}
</code></pre>
<p>而后kube-apiserver为service分为两类</p>
<ul>
<li>apiserver 地址在集群内的service，在代码中表示为 <a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L351" target="_blank"
   rel="noopener nofollow noreferrer" >APIServerServiceIP</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L352" target="_blank"
   rel="noopener nofollow noreferrer" >Service</a>，<code>--service-cluster-ip-range</code> 配置指定的ip，通过『逗号』分割可以为两个</li>
</ul>
<p>有了对 service 更好的理解后，接下来开始本系列第二节<a href="https://cylonchau.github.io/kubernetes-without-service.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - kube-proxy软件架构分析</a></p>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>dual-stack service</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kube-proxy如何保证规则的一致性</title>
      <link>https://www.oomkill.com/2023/03/ch24-kube-proxy-performance/</link>
      <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/03/ch24-kube-proxy-performance/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
</blockquote>
<h2 id="前景">前景</h2>
<p>这里谈 <code>kube-proxy</code> 如何保证规则的一致性以及提升 <code>kube-proxy</code> 性能点的地方，这也是 kubernetes 使用稳定性的一部分。</p>
<h2 id="kube-proxy-如何做到的crud">kube-proxy 如何做到的CRUD</h2>
<p><code>kube-proxy</code>  实际上与其他内置 controller 架构是相同的，实际上也属于一个 controller ，但它属于一个 service, endpoints 的可读可写的控制器，node的读控制器。对于CRUD方面，kube-proxy，在设计上分为 增/改 两方面。正如下面代码所示 <a href="pkg/proxy/ipvs/proxier.go">pkg/proxy/ipvs/proxier.go</a></p>
<pre><code class="language-go">func (proxier *Proxier) OnServiceAdd(service *v1.Service) {
	proxier.OnServiceUpdate(nil, service)
}

// OnServiceUpdate is called whenever modification of an existing service object is observed.
func (proxier *Proxier) OnServiceUpdate(oldService, service *v1.Service) {
	if proxier.serviceChanges.Update(oldService, service) &amp;&amp; proxier.isInitialized() {
		proxier.Sync()
	}
}

// OnServiceDelete is called whenever deletion of an existing service object is observed.
func (proxier *Proxier) OnServiceDelete(service *v1.Service) {
	proxier.OnServiceUpdate(service, nil)
}
</code></pre>
<p>可以看到代码最终调用的都是 <code>OnServiceUpdate</code>，最终 调用的是 <code>proxier.Sync()</code>。对于 Sync()，这里会调用在 Proxier 初始化时注入的那个函数，而 Sync() 本质上是 一个异步有限的函数管理器，这里将实现两个方面，一是，定时去触发执行这个函数；二是满足规则去触发这个函数；而 Sync() 属于条件2</p>
<p>对于注入的函数，则是 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L490" target="_blank"
   rel="noopener nofollow noreferrer" >proxier.syncProxyRules</a> ，由下列代码可以看到</p>
<pre><code class="language-go">proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs)
</code></pre>
<p>这样就是说，kube-proxy 通过 syncProxyRules 实现了整个 service 与 endpoint 的增删改查</p>
<h2 id="性能提升点1">性能提升点1</h2>
<p>由上面有限的函数管理器 runner，可以作为性能提升点，而该runner初始化时提供了minSyncPeriod, syncPeriod 两个函数，这两个函数代表的意思为，minSyncPeriod是runner允许你在最小多少时间内可以调用，如果你的集群规模大，那么则可以适当配置该参数小写，因为service的频繁更改会被这个参数限制。</p>
<h2 id="如何通过一个函数做crud">如何通过一个函数做CRUD</h2>
<p>对于增改，存在三个资源，ClusterIP, NodePort, Ingress,当这些资源被触发时，会同步这个service与endpoint，如代码所示 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1398-L1406" target="_blank"
   rel="noopener nofollow noreferrer" >syncProxyRules</a></p>
<pre><code class="language-go">if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil {
    activeIPVSServices[serv.String()] = true
    activeBindAddrs[serv.Address.String()] = true
    if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil {
        klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
    }
} else {
    klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
}
</code></pre>
<p>由上面代码可知，所有的 service 与 endpoint 的更新，都会触发 <code>Sync()</code>，而 <code>Sync()</code> 执行的是 <code>syncProxyRules()</code> ，当service有变动时，就会通过 syncService/syncEndpoint 进行同步</p>
<p>而对于删除动作来说，kube-proxy 提供了 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1626" target="_blank"
   rel="noopener nofollow noreferrer" >cleanLegacyService</a> 函数在变动做完时，进行清理遗留的service规则，如下列代码所示。</p>
<pre><code class="language-go">proxier.cleanLegacyService(activeIPVSServices, currentIPVSServices, legacyBindAddrs)
</code></pre>
<p>并且通过两个数组来维护两个 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1091-L1094" target="_blank"
   rel="noopener nofollow noreferrer" >activeIPVSServices</a> , 与 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1091-L1094" target="_blank"
   rel="noopener nofollow noreferrer" >currentIPVSServices</a> 为主，来维护删除的数据</p>
<h2 id="crud实际实现">CRUD实际实现</h2>
<p>上面了解到了删除与添加的逻辑，下面分析这些是如何进行的</p>
<p>当添加被触发时，会触发 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1936-L1974" target="_blank"
   rel="noopener nofollow noreferrer" >proxier.syncService()</a> ，首先会进行本机上ipvs规则是否存在这个规则，存在则更改，而后返回一个 error, 这个 error 取决于是否更新 endpoints，如下列代码所示</p>
<pre><code class="language-go">func (proxier *Proxier) syncService(svcName string, vs *utilipvs.VirtualServer, bindAddr bool, bindedAddresses sets.String) error {
	appliedVirtualServer, _ := proxier.ipvs.GetVirtualServer(vs)
	if appliedVirtualServer == nil || !appliedVirtualServer.Equal(vs) {
		if appliedVirtualServer == nil {
			// IPVS service is not found, create a new service
			klog.V(3).Infof(&quot;Adding new service %q %s:%d/%s&quot;, svcName, vs.Address, vs.Port, vs.Protocol)
			if err := proxier.ipvs.AddVirtualServer(vs); err != nil {
				klog.Errorf(&quot;Failed to add IPVS service %q: %v&quot;, svcName, err)
				return err
			}
		} else {
			// IPVS service was changed, update the existing one
			// During updates, service VIP will not go down
			klog.V(3).Infof(&quot;IPVS service %s was changed&quot;, svcName)
			if err := proxier.ipvs.UpdateVirtualServer(vs); err != nil {
				klog.Errorf(&quot;Failed to update IPVS service, err:%v&quot;, err)
				return err
			}
		}
	}

	// bind service address to dummy interface
	if bindAddr {
		// always attempt to bind if bindedAddresses is nil,
		// otherwise check if it's already binded and return early
		if bindedAddresses != nil &amp;&amp; bindedAddresses.Has(vs.Address.String()) {
			return nil
		}

		klog.V(4).Infof(&quot;Bind addr %s&quot;, vs.Address.String())
		_, err := proxier.netlinkHandle.EnsureAddressBind(vs.Address.String(), DefaultDummyDevice)
		if err != nil {
			klog.Errorf(&quot;Failed to bind service address to dummy device %q: %v&quot;, svcName, err)
			return err
		}
	}

	return nil
}
</code></pre>
<p>接下来通过后会 触发 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1976-L2084" target="_blank"
   rel="noopener nofollow noreferrer" >proxier.syncEndpoint()</a> 这里传入了当前的 service, 这里是为了与 IPVS 概念相吻合，如IPVS 中存在 RealServers/VirtualServers，首先会拿到本机这个VirtualServer下的所有RealServer，而后进行添加，而后对比 传入的 Endpoints 列表 与 本机这个VirtualServer下的所有RealServer，不相等的则被删除；删除的动作是一个异步操作。由 gracefuldeleteManager 维护</p>
<pre><code class="language-go">func (proxier *Proxier) syncEndpoint(svcPortName proxy.ServicePortName, onlyNodeLocalEndpoints bool, vs *utilipvs.VirtualServer) error {
	appliedVirtualServer, err := proxier.ipvs.GetVirtualServer(vs)
	if err != nil || appliedVirtualServer == nil {
		klog.Errorf(&quot;Failed to get IPVS service, error: %v&quot;, err)
		return err
	}

	// curEndpoints represents IPVS destinations listed from current system.
	curEndpoints := sets.NewString()
	// newEndpoints represents Endpoints watched from API Server.
	newEndpoints := sets.NewString()

	curDests, err := proxier.ipvs.GetRealServers(appliedVirtualServer)
	if err != nil {
		klog.Errorf(&quot;Failed to list IPVS destinations, error: %v&quot;, err)
		return err
	}
	for _, des := range curDests {
		curEndpoints.Insert(des.String())
	}

	endpoints := proxier.endpointsMap[svcPortName]

	// Service Topology will not be enabled in the following cases:
	// 1. externalTrafficPolicy=Local (mutually exclusive with service topology).
	// 2. ServiceTopology is not enabled.
	// 3. EndpointSlice is not enabled (service topology depends on endpoint slice
	// to get topology information).
	if !onlyNodeLocalEndpoints &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.ServiceTopology) &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.EndpointSliceProxying) {
		endpoints = proxy.FilterTopologyEndpoint(proxier.nodeLabels, proxier.serviceMap[svcPortName].TopologyKeys(), endpoints)
	}

	for _, epInfo := range endpoints {
		if onlyNodeLocalEndpoints &amp;&amp; !epInfo.GetIsLocal() {
			continue
		}
		newEndpoints.Insert(epInfo.String())
	}

	// Create new endpoints
	for _, ep := range newEndpoints.List() {
		ip, port, err := net.SplitHostPort(ep)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint: %v, error: %v&quot;, ep, err)
			continue
		}
		portNum, err := strconv.Atoi(port)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint port %s, error: %v&quot;, port, err)
			continue
		}

		newDest := &amp;utilipvs.RealServer{
			Address: net.ParseIP(ip),
			Port:    uint16(portNum),
			Weight:  1,
		}

		if curEndpoints.Has(ep) {
			// check if newEndpoint is in gracefulDelete list, if true, delete this ep immediately
			uniqueRS := GetUniqueRSName(vs, newDest)
			if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
				continue
			}
			klog.V(5).Infof(&quot;new ep %q is in graceful delete list&quot;, uniqueRS)
			err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS)
			if err != nil {
				klog.Errorf(&quot;Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v&quot;, ep, err)
				continue
			}
		}
		err = proxier.ipvs.AddRealServer(appliedVirtualServer, newDest)
		if err != nil {
			klog.Errorf(&quot;Failed to add destination: %v, error: %v&quot;, newDest, err)
			continue
		}
	}
	// Delete old endpoints
	for _, ep := range curEndpoints.Difference(newEndpoints).UnsortedList() {
		// if curEndpoint is in gracefulDelete, skip
		uniqueRS := vs.String() + &quot;/&quot; + ep
		if proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
			continue
		}
		ip, port, err := net.SplitHostPort(ep)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint: %v, error: %v&quot;, ep, err)
			continue
		}
		portNum, err := strconv.Atoi(port)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint port %s, error: %v&quot;, port, err)
			continue
		}

		delDest := &amp;utilipvs.RealServer{
			Address: net.ParseIP(ip),
			Port:    uint16(portNum),
		}

		klog.V(5).Infof(&quot;Using graceful delete to delete: %v&quot;, uniqueRS)
		err = proxier.gracefuldeleteManager.GracefulDeleteRS(appliedVirtualServer, delDest)
		if err != nil {
			klog.Errorf(&quot;Failed to delete destination: %v, error: %v&quot;, uniqueRS, err)
			continue
		}
	}
	return nil
}
</code></pre>
<p>删除 service 将删除所有的 RealServer，这点上面提到过，kube-proxy 通过 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L2086-L2114" target="_blank"
   rel="noopener nofollow noreferrer" >cleanLegacyService</a> 进行删除，如下列代码所示</p>
<pre><code class="language-go">func (proxier *Proxier) cleanLegacyService(activeServices map[string]bool, currentServices map[string]*utilipvs.VirtualServer, legacyBindAddrs map[string]bool) {
	isIPv6 := utilnet.IsIPv6(proxier.nodeIP)
	for cs := range currentServices {
		svc := currentServices[cs]
		if proxier.isIPInExcludeCIDRs(svc.Address) {
			continue
		}
		if utilnet.IsIPv6(svc.Address) != isIPv6 {
			// Not our family
			continue
		}
		if _, ok := activeServices[cs]; !ok {
			klog.V(4).Infof(&quot;Delete service %s&quot;, svc.String())
			if err := proxier.ipvs.DeleteVirtualServer(svc); err != nil {
				klog.Errorf(&quot;Failed to delete service %s, error: %v&quot;, svc.String(), err)
			}
			addr := svc.Address.String()
			if _, ok := legacyBindAddrs[addr]; ok {
				klog.V(4).Infof(&quot;Unbinding address %s&quot;, addr)
				if err := proxier.netlinkHandle.UnbindAddress(addr, DefaultDummyDevice); err != nil {
					klog.Errorf(&quot;Failed to unbind service addr %s from dummy interface %s: %v&quot;, addr, DefaultDummyDevice, err)
				} else {
					// In case we delete a multi-port service, avoid trying to unbind multiple times
					delete(legacyBindAddrs, addr)
				}
			}
		}
	}
}
</code></pre>
<h2 id="性能提升点2">性能提升点2</h2>
<p>由上面的讲解可知，CRUD动作是每一个事件 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/proxy/ipvs/proxier.go#L1398-L1406" target="_blank"
   rel="noopener nofollow noreferrer" >syncProxyRules</a> 被触发时都会进行执行，而删除动作存在多组循环（如构建维护的两个列表；进行循环删除）即每一次 Endpoints 变动也会触发 大量的 Service 的循环，从而检测 是否由遗留的Service资源，而这个操作保留到kubernetes 1.26版本</p>
<p>假设你的集群节点是5000个，service资源是两万个，那么当你更新一个Service资源循环的次数，会至少循环多达数万次（Service, EndpointSpilt, currentServices, NodeIP, Ingress）其中无用的为currentServices，因为这个只有在删除Service本身才会有效（如果存在20000个service，其中currentServices在构建与对比的过程超过4万次）</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes service - EndpointSlices做了什么？</title>
      <link>https://www.oomkill.com/2023/02/ch18-endpointslices/</link>
      <pubDate>Sat, 25 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/02/ch18-endpointslices/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="endpoint">Endpoint</h2>
<p>Endpoints 就是 service 中后端的server，通常来说 endpoint 与 service是关联的，例如下面的一个endpoints 资源。</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Endpoints
metadata:
  name: nginx
subsets:
  - addresses:
      - ip: 172.17.0.2
      - ip: 172.17.0.3
    ports:
      - port: 80
        name: &quot;111&quot; # 多个端口需要用name
      - port: 88
        name: &quot;222&quot;
</code></pre>
<p>而 Endpoints 资源是由控制平面的  Endpoints controller 进行管理的，主要用于将外部server引入至集群内时使用的，例如Kube-apiserver 在集群外的地址，以及external service所需要创建的。</p>
<p>我们看到 Endpoints controller 代码中，在对 该 informer 监听的包含 service 与 Pod，位于 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/controller/endpoint/endpoints_controller.go#L79-L128" target="_blank"
   rel="noopener nofollow noreferrer" >NewEndpointController()</a></p>
<pre><code class="language-go">// NewEndpointController returns a new *EndpointController.
func NewEndpointController(podInformer coreinformers.PodInformer, serviceInformer coreinformers.ServiceInformer,
	endpointsInformer coreinformers.EndpointsInformer, client clientset.Interface, endpointUpdatesBatchPeriod time.Duration) *EndpointController {
	broadcaster := record.NewBroadcaster()
	broadcaster.StartStructuredLogging(0)
	broadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl{Interface: client.CoreV1().Events(&quot;&quot;)})
	recorder := broadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: &quot;endpoint-controller&quot;})

	if client != nil &amp;&amp; client.CoreV1().RESTClient().GetRateLimiter() != nil {
		ratelimiter.RegisterMetricAndTrackRateLimiterUsage(&quot;endpoint_controller&quot;, client.CoreV1().RESTClient().GetRateLimiter())
	}
	e := &amp;EndpointController{
		client:           client,
		queue:            workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;endpoint&quot;),
		workerLoopPeriod: time.Second,
	}

	serviceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: e.onServiceUpdate,
		UpdateFunc: func(old, cur interface{}) {
			e.onServiceUpdate(cur)
		},
		DeleteFunc: e.onServiceDelete,
	})
	e.serviceLister = serviceInformer.Lister()
	e.servicesSynced = serviceInformer.Informer().HasSynced

	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    e.addPod,
		UpdateFunc: e.updatePod,
		DeleteFunc: e.deletePod,
	})
	e.podLister = podInformer.Lister()
	e.podsSynced = podInformer.Informer().HasSynced

	endpointsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		DeleteFunc: e.onEndpointsDelete,
	})
	e.endpointsLister = endpointsInformer.Lister()
	e.endpointsSynced = endpointsInformer.Informer().HasSynced

    ....
    
}
</code></pre>
<h2 id="endpointslices-supa-href11asup">EndpointSlices <sup><a href="#1">[1]</a></sup></h2>
<p>EndpointSlices 是提供为集群内用于替换 Endpoints 资源的一种灵活并具有扩展性的一种资源，由控制平面的 EndpointSlices Controller 来创建和管理的，默认情况下 EndpointSlices Controller 创建和管理的EndpointSlices 资源将不大于100个Endpoints；可以通过 <code>kube-controller-manager</code> 的参数  <code>--max-endpoints-per-slice</code>  设置，该参数最大为1000 <sup><a href="#2">[2]</a></sup></p>
<p>通常情况下无需自行创建该资源，因为在创建 service 资源时 通常是通过 label 来匹配到对应的 backend server</p>
<p>下面是一个完整的 EndpointSlices 资源清单</p>
<pre><code class="language-yaml">addressType: IPv4
apiVersion: discovery.k8s.io/v1beta1 #注意版本 1.21后是 v1
endpoints:
- addresses:
  - 192.168.1.241
  conditions:
    ready: true
  targetRef:
    kind: Pod
    name: netbox-ff6dd9445-kxr4s
    namespace: default
    resourceVersion: &quot;1994535&quot;
  topology:
    kubernetes.io/hostname: master-machine
- addresses:
  - 192.168.1.242
  conditions:
    ready: true
  targetRef:
    kind: Pod
    name: netbox-ff6dd9445-566tj
    namespace: default
  topology:
    kubernetes.io/hostname: master-machine
kind: EndpointSlice
metadata:
  annotations:
    endpoints.kubernetes.io/last-change-trigger-time: &quot;2023-02-24T22:40:20+08:00&quot;
  labels:
    endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io
    kubernetes.io/service-name: netbox
  name: netbox-l489z
  namespace: default
ports:
- name: &quot;&quot;
  port: 80
  protocol: TCP
</code></pre>
<p>在代码中 <a href="https://github.com/kubernetes/kubernetes/blob/98d5dc5d36d34a7ee13368a7893dcb400ec4e566/pkg/apis/discovery/types.go#L29-L54" target="_blank"
   rel="noopener nofollow noreferrer" >EndpointSlices</a> 资源是这么呈现的，可以看到主要的就是包含一组 Endpoints 资源</p>
<pre><code class="language-go">type EndpointSlice struct {
	metav1.TypeMeta
	metav1.ObjectMeta
	AddressType AddressType
	Endpoints []Endpoint
	Ports []EndpointPort
}
</code></pre>
<h2 id="endpointslices-在-kube-proxy中的应用">EndpointSlices 在 kube-proxy中的应用</h2>
<p>Google 工程师 <em>Rob Scott</em> 在2020年一文 <sup><a href="#3">[3]</a></sup> 中提到了 EndpointSlices 的作用，从kubernetes 1.19 开始EndpointSlices 默认被开启，而开启后的kube-proxy将使用 EndpointSlices 读取集群内的service的 Endpoints，而这个最大的变化就是『拓扑感知路由』(<em><strong>Topology Aware Routing</strong></em>)</p>
<p><em>Rob Scott</em> 在文中提到 EndpointSlice API 是为了提升 Endpoints API 的限制，例如，etcd的存储大小，以及pod规模变动时最大产生的超过22TB数据的问题</p>
<p>而这些问题可以通过文中变化图来说明，开启功能后会将所有匹配到的 Endpoint，划分为多个EndpointSlices，而在大规模集群环境场景下，每次的变更只需要修改其中一个 EndpointSlices 即可，这将带给 kube-proxy 提供超Endpoint模式 10倍的性能</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/endpoint-slices.png" alt="端点切片" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes EndpointSlices </center>
<center><em>Source：</em>https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices</center><br>
<blockquote>
<p><em><strong>Notes</strong></em>：该文中没有提到的一点是：”EndpointSlices资源解决的是集群内的service节点问题，如你使用了endpoint类资源，那么不会触发到EndpointSlices的资源，这部分在 <code>kube-proxy</code> 源码中可以很清晰的看到</p>
</blockquote>
<p>下面的 kube-proxy 日志可以看到获取 server是通过 Endpoints 还是 EndpointSlices</p>
<pre><code class="language-log">endpointslicecache.go:322] Setting endpoints for &quot;default/netbox&quot; to [192.168.1.241:80 192.168.1.242:80]
10008 proxier.go:1057] Syncing ipvs Proxier rules
10008 iptables.go:343] running iptables-save [-t filter]
10008 iptables.go:343] running iptables-save [-t nat]
10008 ipset.go:173] Successfully add entry: 192.168.1.241,tcp:80,192.168.1.241 to ip set: KUBE-LOOP-BACK
</code></pre>
<h2 id="总结">总结</h2>
<ul>
<li>Endpoints 与 EndpointSlices 均是为service提供端点的</li>
<li>Service规模越大，那么Endpoints中的 Pod 数量越大，传输的 EndPoints 对象就越大。集群中 Pod 更改的频率越高，也意味着传输在网络中发生的频率就越高</li>
<li>Endpoints 对象在大规模集群场景下存在下列问题：
<ul>
<li>增加网络流量</li>
<li>超大规模的 service 理论上会无法存储 该 Endpoints</li>
<li>处理Endpoints资源的 worker 会消耗更多的计算资源</li>
<li>隐性增加对控制平面的影响，service的可扩展性将降低</li>
</ul>
</li>
<li>Endpointslices 解决了：
<ul>
<li>部分更新，更少的网络流量</li>
<li>Worker 处理 Endpoints 更新所需的资源更少</li>
<li>减少对控制平面的影响，提升的性能和 service 规模</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>EndpointSlices</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/#address-types" target="_blank"
   rel="noopener nofollow noreferrer" ><em>EndpointSlice API</em></a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/#address-types" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Scaling Kubernetes Networking With EndpointSlices</em></a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/#scalability-limitations-of-the-endpoints-api" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Scalability Limitations of the Endpoints API</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？</title>
      <link>https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/</link>
      <pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p>本文是关于Kubernetes service解析的第四章</p>
<ul>
<li><a href="https://cylonchau.github.io/kubernetes-service-controller.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - 你真的理解service吗?</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-endpointslices.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - EndpointSlices做了什么？</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-kube-proxy-code.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - kube-proxy架构分析</a></li>
<li>深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？</li>
</ul>
<p>所有关于Kubernetes service 部分代码上传至仓库 <a href="https://github.com/cylonchau/kube-haproxy" target="_blank"
   rel="noopener nofollow noreferrer" >github.com/cylonchau/kube-haproxy</a></p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>在前两部分中，学习了一些 service,于kube-proxy在设计架构，但存在扩展问题将引入了一些问题：</p>
<ul>
<li>为什么需要了解这部分内容呢？</li>
<li>与传统架构有什么区别呢？</li>
<li>于eBPF 的 cilium又有什么区别呢？</li>
<li>既然eBPF可以做到，那为什么要这部分内容呢？</li>
</ul>
<p>接下来的内容将围绕这四个问题展开来讲，而不是代码的讲解，代码可以看置顶</p>
<h2 id="ipvs与iptables在kubernetes中应用时的问题">IPVS与iptables在kubernetes中应用时的问题</h2>
<p>对于在使用了kubernetes用户以及了解 kube-proxy 架构后，知道当集群规模过大时，service必将增多，而一个service未必是一条iptables/ipvs规则，对于kubernetes这种分布式架构来说，集群规模越大，集群状态就越不可控，尤其时kube-proxy。</p>
<p>为什么单指kube-proxy呢？想想可以知道，pod的故障 或 node 的故障对于kubernetes集群来说却不是致命的，因为 这些资源集群中存在 避免方案，例如Pod的驱逐。而kube-proxy或iptables/IPVS问题将导致服务的不可控 『抖动』例如规则生成的快慢和Pod就绪的快慢不一致，部分节点不存在 service 此时服务必然抖动。</p>
<p>再例如 iptables/IPVS 排查的难度对于普通运维工程师或开发工程师的技术水平有很高的要求，网上随处可见分析该类问题的帖子：</p>
<ul>
<li>
<p><a href="https://www.bilibili.com/video/BV1yK411V7oa/?spm_id_from=333.337.search-card.all.click&amp;vd_source=80a7f916d4f5b3fd494735dbc609331f" target="_blank"
   rel="noopener nofollow noreferrer" >kube-proxy源码分析与问题定位</a></p>
</li>
<li>
<p><a href="http://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE/%E5%8A%A0%E9%A4%9001%20%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90%EF%BC%9A%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E6%B5%B7%E9%87%8FIPVS%E8%A7%84%E5%88%99%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%BD%91%E7%BB%9C%E5%BB%B6%E6%97%B6%E6%8A%96%E5%8A%A8%E9%97%AE%E9%A2%98%EF%BC%9F.md" target="_blank"
   rel="noopener nofollow noreferrer" >案例分析：怎么解决海量IPVS规则带来的网络延时抖动问题？</a></p>
</li>
<li>
<p><a href="https://imroc.cc/kubernetes/networking/faq/ipvs-conn-reuse-mode.html#ipvs-%e8%bf%9e%e6%8e%a5%e5%a4%8d%e7%94%a8%e5%bc%95%e5%8f%91%e7%9a%84%e7%b3%bb%e5%88%97%e9%97%ae%e9%a2%98" target="_blank"
   rel="noopener nofollow noreferrer" >ipvs 连接复用引发的系列问题</a></p>
</li>
<li>
<p><a href="https://www.diva-portal.org/smash/get/diva2:1610208/FULLTEXT01.pdf" target="_blank"
   rel="noopener nofollow noreferrer" >Investigating Causes of Jitter in Container Networking</a></p>
</li>
<li>
<p><a href="https://main.qcloudimg.com/raw/document/intl/product/pdf/457_37358_en.pdf" target="_blank"
   rel="noopener nofollow noreferrer" >ContainerNative network LoadBalancer IPVS jitter</a></p>
</li>
</ul>
<p>对于上述问题，相信遇到了很难定位处理，虽然现在已fixed，并有eBPF技术的加入减少了此类问题的发生，但是eBPF实际同理于IPVS 都是需要对Linux内核有一定了解后才可以，这也就是为什么需要了解这部分</p>
<h2 id="如果需要自定义proxier为什么会解决这个问题">如果需要自定义proxier为什么会解决这个问题</h2>
<p>这里就是放大到kubernetes意外的传统架构中，当直接部署于Linux系统上使用nginx等传统LB时就很少有人提到这些问题了，而这些问题存在一个关键字「Container」；而引发这个问题的则是 service。去除 service 的功能，传统架构于Kubernetes架构部署的应用则是相同的，只是区分了名称空间。</p>
<p>抓入关键的核心之后就做接下来的事情了，我称之为「shed kube-proxy, fetch service」；即把service提取到集群外部的LB之上，例如F5, nginx等。</p>
<p>这里会存在一个疑问：「这个不是ingress吗？」，这个问题会在下一章讲到 <a href="https://cylonchau.github.io/kubernetes-auditing.html" target="_blank"
   rel="noopener nofollow noreferrer" >proxier与ingress有什么区别?</a></p>
<h2 id="软件的设计">软件的设计</h2>
<p>既然拿到了核心问题就该定义软件工作模式，这里将软件架构设计为三种：</p>
<ul>
<li>only fetch：任然需要 kube-proxy 组件，通过定义 contoller 将流量引入，即不过service，这种场景不会破坏现有的集群架构，从而去除service的功能，如果需要service功能配置外部service即可</li>
<li>SK (similar kube-proxy)：通过效仿kube-proxy + ipvs架构，将LB于proxier部署在每个worker节点上，让浏览都走本地</li>
<li>replacement kube-proxy：完全取代kube-proxy 这于cilium类似了，但不同的是，<strong>proxier</strong> 可以于 <code>kube-controller-manager</code>；<code>kube-scheduler</code> 作为控制平面为集群提供 <code>service </code> 功能，而无需为所有worker节点都部署一个 <code>kube-proxy</code> 或 <code>cilium</code> 这种架构</li>
</ul>
<h2 id="最后一个问题">最后一个问题</h2>
<p>此时可以引入最后一个问题了：「既然eBPF可以做到，那为什么要这部分内容呢？」。</p>
<p>答：其一简单，每个运维人员无需额外知识都可以对 service 问题进行排错，简便了运维复杂度。另外这一部分其实是对于完整企业生态来讲，统一的流量转发平台是所必须的，有了这个就不需要单独的 service 功能了</p>
<h2 id="实践基于haproxy的proxier">实践：基于haproxy的proxier</h2>
<p>在扩展proxier需要对 kube-proxy 有一定的了解，并且，kube-proxy 在可扩展性来说做的也是相当不错的，我们只需要实现一个 proxier.go 就可以基本上完成了对 kube-proxy ；而 proxier.go 的核心方法只需要三个函数即可（==这里是根据iptables/ipvs的设计进行的，也可以整合为一个方法==）</p>
<p>除了这三个函数外，其他的函数全都是 kube-proxy 已经实现好的通用的，这里直接使用或者按照其他内置proxier的方法即可</p>
<h3 id="满足条件">满足条件</h3>
<ul>
<li>haproxy工作与proxier相同的节点，可以是集群内也可以是集群外，整个集群只需要一个</li>
<li>实现方法：syncProxyRules(), syncService(),  syncEndpoint()</li>
</ul>
<p>查看当前的service</p>
<pre><code class="language-bash">$ kubectl get endpointslices
NAME           ADDRESSTYPE   PORTS   ENDPOINTS                     AGE
kubernetes     IPv4          6443    10.0.0.4                      195d
netbox-l489z   IPv4          80      192.168.1.241,192.168.1.242   2d1h
</code></pre>
<p>查看service 配置</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: netbox
spec:
  clusterIP: 192.168.129.5
  ports:
  - port: 88
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
</code></pre>
<p>通过 proxier 生成了对应的 backend 与 frontend，这样就可以通过 haproxy 作为一个外部LB来跨过 service 与 IPVS/IPTables，通过这种情况下，我们可以将集群拉出一个平面至传统架构上，而又不影响集群的功能</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20230226234851398.png" alt="image-20230226234851398" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>在这种场景下需要注意的是：</p>
<ul>
<li>OF模式下，我们需要 kube-proxy 组件，而使用 kube-proxy 组件</li>
<li>所有模式下，haproxy worker和kubernetes nodes需处于一个网络平面</li>
<li>非OF模式下需要自行修改 <code>kube-apiserver</code> 源代码（主要是使kubernetes service分配机制）</li>
</ul>
<h2 id="proxier与ingress的区别">Proxier与Ingress的区别</h2>
<p>肯定有人会问，kubernetes提供了Ingress功能不是和这个一样吗？</p>
<p>答：对比一个LB是Proxier还是Ingress最好的区别就是“舍去kube-proxy”可以工作正常吗？</p>
<p>而kubernetes官方也提供说明，Ingress的后端是service，service的后端则是IPVS/IPTables，而IPVS的后端才是Pod；相对于Proxier LB，他的后端则直接是Pod，跨越了Service。</p>
<ul>
<li>Kubernetes Ingress 架构说明 <sup><a href="#1">[1]</a></sup></li>
<li>Traefik Ingress 架构说明 <sup><a href="#2">[2]</a></sup></li>
<li>APISIX Ingress 架构说明 <sup><a href="#3">[3]</a></sup></li>
</ul>
<p>而相对的本文的学习思路，haproxy官方提供了对应的解决方案 <sup><a href="#4">[4]</a></sup> ；而由此，可以灵活的为Kubernetes提供更多的LB方案</p>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#what-is-ingress" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Kubernetes Ingress 架构说明</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://traefik.io/solutions/kubernetes-ingress/#architecture" target="_blank"
   rel="noopener nofollow noreferrer" ><em>Traefik Ingress 架构说明</em></a></p>
<p><sup id="3">[3]</sup> <a href="https://github.com/apache/apisix-ingress-controller" target="_blank"
   rel="noopener nofollow noreferrer" ><em>APISIX Ingress 架构说明</em></a></p>
<p><sup id="4">[4]</sup> <a href="https://haproxy-ingress.github.io/docs/examples/external-haproxy/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>External haproxy</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes service - kube-proxy架构分析</title>
      <link>https://www.oomkill.com/2023/02/ch19-kube-proxy-code/</link>
      <pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/02/ch19-kube-proxy-code/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="前提概述">前提概述</h2>
<p>kubernetes集群中运行在每个Worker节点上的组件 <strong>kube-proxy</strong>，本文讲解的是如何快速的了解 kube-proxy 的软件架构，而不是流程的分析，专注于 proxy 层面的设计讲解，而不会贴大量的代码</p>
<h2 id="kube-proxy软件设计">kube-proxy软件设计</h2>
<p>kube-proxy 在设计上分为三个模块 server 于 <a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/kube-proxy" target="_blank"
   rel="noopener nofollow noreferrer" >proxy</a>：</p>
<ul>
<li><a href="https://github.com/kubernetes/kubernetes/tree/master/cmd/kube-proxy" target="_blank"
   rel="noopener nofollow noreferrer" >server</a>: 是一个常驻进程用于处理service的事件</li>
<li><a href="https://github.com/kubernetes/kubernetes/tree/v1.24.5/pkg/proxy" target="_blank"
   rel="noopener nofollow noreferrer" >proxy</a>: 是 kube-proxy 的工作核心，实际上的角色是一个 service controller，通过监听 node, service, endpoint 而生成规则</li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/v1.24.5/pkg/proxy/ipvs/proxier.go" target="_blank"
   rel="noopener nofollow noreferrer" >proxier</a>: 是实现service的组件，例如iptables, ipvs&hellip;.</li>
</ul>
<h2 id="如何快速读懂kube-proxy源码">如何快速读懂kube-proxy源码</h2>
<p>要想快速读懂 kube-proxy 源码就需要对 kube-proxy 设计有深刻的了解，例如需要看 kube-proxy 的实现，我们就可以看 proxy的部分，下列是 proxy 部分的目录结构</p>
<pre><code class="language-bash">$ tree -L 1
.
├── BUILD
├── OWNERS
├── apis
├── config
├── healthcheck
├── iptables
├── ipvs
├── metaproxier
├── metrics
├── userspace
├── util
├── winuserspace
├── winkernel
├── doc.go
├── endpoints.go
├── endpoints_test.go
├── endpointslicecache.go
├── endpointslicecache_test.go
├── service.go
├── service_test.go
├── topology.go
├── topology_test.go
└── types.go
</code></pre>
<ul>
<li>目录 ipvs, iptables, 就是所知的 kube-proxy 提供的两种 load balancer</li>
<li>目录 apis, 则是kube-proxy 配置文件资源类型的定义，<code>--config=/etc/kubernetes/kube-proxy-config.yaml</code> 所指定问题的shema</li>
<li>目录config: 定义了每种资源的handler需要实现什么</li>
<li>service.go, endpoints.go：是controller的实现</li>
<li>type.go: 是每个资源的interface定义，例如：
<ul>
<li>Provider: 规定了每个proxier需要实现什么</li>
<li>ServicePort: service 控制器需要实现什么</li>
<li>Endpoint: service 控制器需要实现什么</li>
</ul>
</li>
</ul>
<p>上述是整个 proxy 的一级结构</p>
<h2 id="service-controller">service controller</h2>
<p>service控制器换句话说，就是工作内容类似于kubernetes集群中的pod控制器那些，所作的工作就是监听对应资源做出相应事件处理，而这个处理被定义为 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/types.go#L30-L41" target="_blank"
   rel="noopener nofollow noreferrer" >handler</a></p>
<pre><code class="language-go">type Provider interface {
	config.EndpointsHandler
	config.EndpointSliceHandler
	config.ServiceHandler
	config.NodeHandler

	// Sync immediately synchronizes the Provider's current state to proxy rules.
	Sync()
	// SyncLoop runs periodic work.
	// This is expected to run as a goroutine or as the main loop of the app.
	// It does not return.
	SyncLoop()
}
</code></pre>
<p>由上面代码可以看到，每一个Provider 即 proxier （用于实现的LB的控制器）都需要包含对应资源的事件处理函数于 一个 <code>Sync()</code> 和 <code>SyncLoop()</code>，所以这里将总结为 controller 而不是用于这里给到的术语</p>
<p>同理，其他类型的 controller 则是相同与 service controller</p>
<h2 id="深入理解proxier">深入理解proxier</h2>
<p>这里将以 ipvs 为例，如图所示，这将是一个 proxier 的实现，而 proxier.go 则是真实的 proxier 实现</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20230212221556051.png" alt="image-20230212221556051" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图（详细）</center>
<p>而 ipvs 的 <a href="https://github.com/kubernetes/kubernetes/blob/v1.24.5/pkg/proxy/ipvs/proxier.go" target="_blank"
   rel="noopener nofollow noreferrer" >proxier</a> 则是如下定义的</p>
<pre><code class="language-go">type Proxier struct {
	// endpointsChanges and serviceChanges contains all changes to endpoints and
	// services that happened since last syncProxyRules call. For a single object,
	// changes are accumulated, i.e. previous is state from before all of them,
	// current is state after applying all of those.
	endpointsChanges *proxy.EndpointChangeTracker
	serviceChanges   *proxy.ServiceChangeTracker

	mu           sync.Mutex // protects the following fields
	serviceMap   proxy.ServiceMap
	endpointsMap proxy.EndpointsMap
	portsMap     map[utilproxy.LocalPort]utilproxy.Closeable
	nodeLabels   map[string]string
	// endpointsSynced, endpointSlicesSynced, and servicesSynced are set to true when
	// corresponding objects are synced after startup. This is used to avoid updating
	// ipvs rules with some partial data after kube-proxy restart.
	endpointsSynced      bool
	endpointSlicesSynced bool
	servicesSynced       bool
	initialized          int32
	syncRunner           *async.BoundedFrequencyRunner // governs calls to syncProxyRules

	// These are effectively const and do not need the mutex to be held.
	syncPeriod    time.Duration
	minSyncPeriod time.Duration
	// Values are CIDR's to exclude when cleaning up IPVS rules.
	excludeCIDRs []*net.IPNet
	// Set to true to set sysctls arp_ignore and arp_announce
	strictARP      bool
	iptables       utiliptables.Interface
	ipvs           utilipvs.Interface
	ipset          utilipset.Interface
	exec           utilexec.Interface
	masqueradeAll  bool
	masqueradeMark string
	localDetector  proxyutiliptables.LocalTrafficDetector
	hostname       string
	nodeIP         net.IP
	portMapper     utilproxy.PortOpener
	recorder       record.EventRecorder

	serviceHealthServer healthcheck.ServiceHealthServer
	healthzServer       healthcheck.ProxierHealthUpdater

	ipvsScheduler string
	// Added as a member to the struct to allow injection for testing.
	ipGetter IPGetter
	// The following buffers are used to reuse memory and avoid allocations
	// that are significantly impacting performance.
	iptablesData     *bytes.Buffer
	filterChainsData *bytes.Buffer
	natChains        *bytes.Buffer
	filterChains     *bytes.Buffer
	natRules         *bytes.Buffer
	filterRules      *bytes.Buffer
	// Added as a member to the struct to allow injection for testing.
	netlinkHandle NetLinkHandle
	// ipsetList is the list of ipsets that ipvs proxier used.
	ipsetList map[string]*IPSet
	// Values are as a parameter to select the interfaces which nodeport works.
	nodePortAddresses []string
	// networkInterfacer defines an interface for several net library functions.
	// Inject for test purpose.
	networkInterfacer     utilproxy.NetworkInterfacer
	gracefuldeleteManager *GracefulTerminationManager
}
</code></pre>
<p>再看这个结构体的 structure，发现他实现了上述提到的 Handler 和 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L843-L849" target="_blank"
   rel="noopener nofollow noreferrer" >Sync()</a></p>
<p>可以看到 Sync() 的实现是调用 runner.Run()</p>
<pre><code class="language-go">func (proxier *Proxier) Sync() {
	if proxier.healthzServer != nil {
		proxier.healthzServer.QueuedUpdate()
	}
	metrics.SyncProxyRulesLastQueuedTimestamp.SetToCurrentTime()
	proxier.syncRunner.Run()
}
</code></pre>
<p>而 handler 中的任意事件的触发则是调用 Sync()</p>
<pre><code class="language-go">// handler 不同的事件均指向 On{rs}Update() 函数
func (proxier *Proxier) OnEndpointsAdd(endpoints *v1.Endpoints) {
	proxier.OnEndpointsUpdate(nil, endpoints)
}
// OnEndpointsDelete is called whenever deletion of an existing endpoints object is observed.
func (proxier *Proxier) OnEndpointsDelete(endpoints *v1.Endpoints) {
	proxier.OnEndpointsUpdate(endpoints, nil)
}

...

// 而 update 调用的则是 Sync()
func (proxier *Proxier) OnEndpointsUpdate(oldEndpoints, endpoints *v1.Endpoints) {
	if proxier.endpointsChanges.Update(oldEndpoints, endpoints) &amp;&amp; proxier.isInitialized() {
		proxier.Sync()
	}
}
</code></pre>
<p>到了这里，明了了 runner 才是这个 proxier 的核心，被定义于 proxier 结构图的 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L232" target="_blank"
   rel="noopener nofollow noreferrer" >syncRunner</a> 在初始化时被注入了函数 syncProxyRules()</p>
<pre><code class="language-go">func NewProxier(ipt utiliptables.Interface,

...
	
    proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs)
	proxier.gracefuldeleteManager.Run()
	return proxier, nil
}
</code></pre>
<h2 id="syncproxyrules">syncProxyRules()</h2>
<p>而这个 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1004-L1621" target="_blank"
   rel="noopener nofollow noreferrer" >syncProxyRules()</a> 则是完成了整个 ipvs 以及 service 的生命周期</p>
<p>对于了解kubernetes架构的同学来说，kube-proxy 完成的功能就是 ipvs 的规则管理，那么换句话说就是干预 ipvs 规则的生命周期，也就是分析函数 syncProxyRules() 是如何干预这些规则的。</p>
<p>syncProxyRules() 将动作分为两部分，一是对 ipvs 资源的增改，二是对资源的销毁；引入完概念后，就开始进行分析吧。</p>
<p>600多行的代码看起来很困难，那就拆分成步骤进行分析</p>
<h3 id="step1-前期准备工作">step1 前期准备工作</h3>
<p>为什么这么叫第一部分呢？看下列代码就知道，做的工作和 ipvs rules 没多大关系</p>
<pre><code class="language-go">func (proxier *Proxier) syncProxyRules() {
    // 互斥锁
	proxier.mu.Lock()
	defer proxier.mu.Unlock()
	
	// don't sync rules till we've received services and endpoints
    // 在等待接收完信息前，不同步收到的 services和endpoints资源
	if !proxier.isInitialized() {
		klog.V(2).Info(&quot;Not syncing ipvs rules until Services and Endpoints have been received from master&quot;)
		return
	}

	// Keep track of how long syncs take.
    // 记录同步耗时
	start := time.Now()
	defer func() {
		metrics.SyncProxyRulesLatency.Observe(metrics.SinceInSeconds(start))
		klog.V(4).Infof(&quot;syncProxyRules took %v&quot;, time.Since(start))
	}()
	// 获取本地多个IP地址
	localAddrs, err := utilproxy.GetLocalAddrs()
	if err != nil {
		klog.Errorf(&quot;Failed to get local addresses during proxy sync: %v, assuming external IPs are not local&quot;, err)
	} else if len(localAddrs) == 0 {
		klog.Warning(&quot;No local addresses found, assuming all external IPs are not local&quot;)
	}

	localAddrSet := utilnet.IPSet{}
	localAddrSet.Insert(localAddrs...)

	// We assume that if this was called, we really want to sync them,
	// even if nothing changed in the meantime. In other words, callers are
	// responsible for detecting no-op changes and not calling this function.
    // 这两步骤正如注释所讲，如果在资源修改的前提下需要同步，也就是update操作包含了增改
	serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges)
	endpointUpdateResult := proxier.endpointsMap.Update(proxier.endpointsChanges)
	// 陈腐的UDP信息处理
	staleServices := serviceUpdateResult.UDPStaleClusterIP
    // merge stale services gathered from updateEndpointsMap
	for _, svcPortName := range endpointUpdateResult.StaleServiceNames {
		if svcInfo, ok := proxier.serviceMap[svcPortName]; ok &amp;&amp; svcInfo != nil &amp;&amp; conntrack.IsClearConntrackNeeded(svcInfo.Protocol()) {
			klog.V(2).Infof(&quot;Stale %s service %v -&gt; %s&quot;, strings.ToLower(string(svcInfo.Protocol())), svcPortName, svcInfo.ClusterIP().String())
			staleServices.Insert(svcInfo.ClusterIP().String())
			for _, extIP := range svcInfo.ExternalIPStrings() {
				staleServices.Insert(extIP)
			}
		}
	}
</code></pre>
<h3 id="step2构建ipvs规则">step2：构建IPVS规则</h3>
<p>首先会经历一些预处理的操作 这部分掠过了 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1042-L1140" target="_blank"
   rel="noopener nofollow noreferrer" >L1042-L1140</a></p>
<pre><code class="language-go">	klog.V(3).Infof(&quot;Syncing ipvs Proxier rules&quot;)

	// Begin install iptables

	// Reset all buffers used later.
	// This is to avoid memory reallocations and thus improve performance.
	proxier.natChains.Reset()
	proxier.natRules.Reset()
	proxier.filterChains.Reset()
	proxier.filterRules.Reset()

	// Write table headers.
	writeLine(proxier.filterChains, &quot;*filter&quot;)
	writeLine(proxier.natChains, &quot;*nat&quot;)

	proxier.createAndLinkeKubeChain()

	// make sure dummy interface exists in the system where ipvs Proxier will bind service address on it
	_, err = proxier.netlinkHandle.EnsureDummyDevice(DefaultDummyDevice)
	if err != nil {
		klog.Errorf(&quot;Failed to create dummy interface: %s, error: %v&quot;, DefaultDummyDevice, err)
		return
	}

	// make sure ip sets exists in the system.
	for _, set := range proxier.ipsetList {
		if err := ensureIPSet(set); err != nil {
			return
		}
		set.resetEntries()
	}

	// Accumulate the set of local ports that we will be holding open once this update is complete
	replacementPortsMap := map[utilproxy.LocalPort]utilproxy.Closeable{}
	// activeIPVSServices represents IPVS service successfully created in this round of sync
	activeIPVSServices := map[string]bool{}
	// currentIPVSServices represent IPVS services listed from the system
	currentIPVSServices := make(map[string]*utilipvs.VirtualServer)
	// activeBindAddrs represents ip address successfully bind to DefaultDummyDevice in this round of sync
	activeBindAddrs := map[string]bool{}

	bindedAddresses, err := proxier.ipGetter.BindedIPs()
	if err != nil {
		klog.Errorf(&quot;error listing addresses binded to dummy interface, error: %v&quot;, err)
	}
	// 检查是否是nodeport类型
	hasNodePort := false
	for _, svc := range proxier.serviceMap {
		svcInfo, ok := svc.(*serviceInfo)
		if ok &amp;&amp; svcInfo.NodePort() != 0 {
			hasNodePort = true
			break
		}
	}

	// Both nodeAddresses and nodeIPs can be reused for all nodePort services
	// and only need to be computed if we have at least one nodePort service.
	var (
		// List of node addresses to listen on if a nodePort is set.
		nodeAddresses []string
		// List of node IP addresses to be used as IPVS services if nodePort is set.
		nodeIPs []net.IP
	)

	if hasNodePort {
		nodeAddrSet, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer)
		if err != nil {
			klog.Errorf(&quot;Failed to get node ip address matching nodeport cidr: %v&quot;, err)
		} else {
			nodeAddresses = nodeAddrSet.List()
			for _, address := range nodeAddresses {
				if utilproxy.IsZeroCIDR(address) {
					nodeIPs, err = proxier.ipGetter.NodeIPs()
					if err != nil {
						klog.Errorf(&quot;Failed to list all node IPs from host, err: %v&quot;, err)
					}
					break
				}
				nodeIPs = append(nodeIPs, net.ParseIP(address))
			}
		}
	}
</code></pre>
<p>接下来是整个构建ipvs规则的关键部分，大概200-300行代码，通过循环 serviceMap 拿到每一个 service 的信息，然后在通过 循环 endpointsMap[svcName] 得到每个 service下所属的 endpoint ，然后构建 ipvs 的规则</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1141-L1542" target="_blank"
   rel="noopener nofollow noreferrer" >L1141-L1542</a> 这里也包含了 nodeport, clusterIP, ingress等不同的service类型</p>
<pre><code class="language-go">	// Build IPVS rules for each service.
	for svcName, svc := range proxier.serviceMap {
		svcInfo, ok := svc.(*serviceInfo)
		if !ok {
			klog.Errorf(&quot;Failed to cast serviceInfo %q&quot;, svcName.String())
			continue
		}
		isIPv6 := utilnet.IsIPv6(svcInfo.ClusterIP())
		protocol := strings.ToLower(string(svcInfo.Protocol()))
		// Precompute svcNameString; with many services the many calls
		// to ServicePortName.String() show up in CPU profiles.
		svcNameString := svcName.String()
		
        // 循环endpoint
		// Handle traffic that loops back to the originator with SNAT.
		for _, e := range proxier.endpointsMap[svcName] {
			ep, ok := e.(*proxy.BaseEndpointInfo)
			if !ok {
				klog.Errorf(&quot;Failed to cast BaseEndpointInfo %q&quot;, e.String())
				continue
			}
			if !ep.IsLocal {
				continue
			}
			epIP := ep.IP()
			epPort, err := ep.Port()
			// Error parsing this endpoint has been logged. Skip to next endpoint.
			if epIP == &quot;&quot; || err != nil {
				continue
			}
			entry := &amp;utilipset.Entry{
				IP:       epIP,
				Port:     epPort,
				Protocol: protocol,
				IP2:      epIP,
				SetType:  utilipset.HashIPPortIP,
			}
			if valid := proxier.ipsetList[kubeLoopBackIPSet].validateEntry(entry); !valid {
				klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoopBackIPSet].Name))
				continue
			}
			proxier.ipsetList[kubeLoopBackIPSet].activeEntries.Insert(entry.String())
		}

		// Capture the clusterIP.
		// ipset call
		entry := &amp;utilipset.Entry{
			IP:       svcInfo.ClusterIP().String(),
			Port:     svcInfo.Port(),
			Protocol: protocol,
			SetType:  utilipset.HashIPPort,
		}
		// add service Cluster IP:Port to kubeServiceAccess ip set for the purpose of solving hairpin.
		// proxier.kubeServiceAccessSet.activeEntries.Insert(entry.String())
		if valid := proxier.ipsetList[kubeClusterIPSet].validateEntry(entry); !valid {
			klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeClusterIPSet].Name))
			continue
		}
		proxier.ipsetList[kubeClusterIPSet].activeEntries.Insert(entry.String())
		// ipvs call
		serv := &amp;utilipvs.VirtualServer{
			Address:   svcInfo.ClusterIP(),
			Port:      uint16(svcInfo.Port()),
			Protocol:  string(svcInfo.Protocol()),
			Scheduler: proxier.ipvsScheduler,
		}
		// Set session affinity flag and timeout for IPVS service
		if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP {
			serv.Flags |= utilipvs.FlagPersistent
			serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds())
		}
		// We need to bind ClusterIP to dummy interface, so set `bindAddr` parameter to `true` in syncService()
		if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil {
			activeIPVSServices[serv.String()] = true
			activeBindAddrs[serv.Address.String()] = true
			// ExternalTrafficPolicy only works for NodePort and external LB traffic, does not affect ClusterIP
			// So we still need clusterIP rules in onlyNodeLocalEndpoints mode.
			if err := proxier.syncEndpoint(svcName, false, serv); err != nil {
				klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
			}
		} else {
			klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
		}

		// Capture externalIPs.
		for _, externalIP := range svcInfo.ExternalIPStrings() {
			// If the &quot;external&quot; IP happens to be an IP that is local to this
			// machine, hold the local port open so no other process can open it
			// (because the socket might open but it would never work).
			if (svcInfo.Protocol() != v1.ProtocolSCTP) &amp;&amp; localAddrSet.Has(net.ParseIP(externalIP)) {
				// We do not start listening on SCTP ports, according to our agreement in the SCTP support KEP
				lp := utilproxy.LocalPort{
					Description: &quot;externalIP for &quot; + svcNameString,
					IP:          externalIP,
					Port:        svcInfo.Port(),
					Protocol:    protocol,
				}
				if proxier.portsMap[lp] != nil {
					klog.V(4).Infof(&quot;Port %s was open before and is still needed&quot;, lp.String())
					replacementPortsMap[lp] = proxier.portsMap[lp]
				} else {
					socket, err := proxier.portMapper.OpenLocalPort(&amp;lp, isIPv6)
					if err != nil {
						msg := fmt.Sprintf(&quot;can't open %s, skipping this externalIP: %v&quot;, lp.String(), err)

						proxier.recorder.Eventf(
							&amp;v1.ObjectReference{
								Kind:      &quot;Node&quot;,
								Name:      proxier.hostname,
								UID:       types.UID(proxier.hostname),
								Namespace: &quot;&quot;,
							}, v1.EventTypeWarning, err.Error(), msg)
						klog.Error(msg)
						continue
					}
					replacementPortsMap[lp] = socket
				}
			} // We're holding the port, so it's OK to install IPVS rules.

			// ipset call
			entry := &amp;utilipset.Entry{
				IP:       externalIP,
				Port:     svcInfo.Port(),
				Protocol: protocol,
				SetType:  utilipset.HashIPPort,
			}

			if utilfeature.DefaultFeatureGate.Enabled(features.ExternalPolicyForExternalIP) &amp;&amp; svcInfo.OnlyNodeLocalEndpoints() {
				if valid := proxier.ipsetList[kubeExternalIPLocalSet].validateEntry(entry); !valid {
					klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeExternalIPLocalSet].Name))
					continue
				}
				proxier.ipsetList[kubeExternalIPLocalSet].activeEntries.Insert(entry.String())
			} else {
				// We have to SNAT packets to external IPs.
				if valid := proxier.ipsetList[kubeExternalIPSet].validateEntry(entry); !valid {
					klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeExternalIPSet].Name))
					continue
				}
				proxier.ipsetList[kubeExternalIPSet].activeEntries.Insert(entry.String())
			}

			// ipvs call
			serv := &amp;utilipvs.VirtualServer{
				Address:   net.ParseIP(externalIP),
				Port:      uint16(svcInfo.Port()),
				Protocol:  string(svcInfo.Protocol()),
				Scheduler: proxier.ipvsScheduler,
			}
			if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP {
				serv.Flags |= utilipvs.FlagPersistent
				serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds())
			}
			if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil {
				activeIPVSServices[serv.String()] = true
				activeBindAddrs[serv.Address.String()] = true

				onlyNodeLocalEndpoints := false
				if utilfeature.DefaultFeatureGate.Enabled(features.ExternalPolicyForExternalIP) {
					onlyNodeLocalEndpoints = svcInfo.OnlyNodeLocalEndpoints()
				}
				if err := proxier.syncEndpoint(svcName, onlyNodeLocalEndpoints, serv); err != nil {
					klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
				}
			} else {
				klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
			}
		}

		// Capture load-balancer ingress.
		for _, ingress := range svcInfo.LoadBalancerIPStrings() {
			if ingress != &quot;&quot; {
				// ipset call
				entry = &amp;utilipset.Entry{
					IP:       ingress,
					Port:     svcInfo.Port(),
					Protocol: protocol,
					SetType:  utilipset.HashIPPort,
				}
				// add service load balancer ingressIP:Port to kubeServiceAccess ip set for the purpose of solving hairpin.
				// proxier.kubeServiceAccessSet.activeEntries.Insert(entry.String())
				// If we are proxying globally, we need to masquerade in case we cross nodes.
				// If we are proxying only locally, we can retain the source IP.
				if valid := proxier.ipsetList[kubeLoadBalancerSet].validateEntry(entry); !valid {
					klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSet].Name))
					continue
				}
				proxier.ipsetList[kubeLoadBalancerSet].activeEntries.Insert(entry.String())
				// insert loadbalancer entry to lbIngressLocalSet if service externaltrafficpolicy=local
				if svcInfo.OnlyNodeLocalEndpoints() {
					if valid := proxier.ipsetList[kubeLoadBalancerLocalSet].validateEntry(entry); !valid {
						klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerLocalSet].Name))
						continue
					}
					proxier.ipsetList[kubeLoadBalancerLocalSet].activeEntries.Insert(entry.String())
				}
				if len(svcInfo.LoadBalancerSourceRanges()) != 0 {
					// The service firewall rules are created based on ServiceSpec.loadBalancerSourceRanges field.
					// This currently works for loadbalancers that preserves source ips.
					// For loadbalancers which direct traffic to service NodePort, the firewall rules will not apply.
					if valid := proxier.ipsetList[kubeLoadbalancerFWSet].validateEntry(entry); !valid {
						klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadbalancerFWSet].Name))
						continue
					}
					proxier.ipsetList[kubeLoadbalancerFWSet].activeEntries.Insert(entry.String())
					allowFromNode := false
					for _, src := range svcInfo.LoadBalancerSourceRanges() {
						// ipset call
						entry = &amp;utilipset.Entry{
							IP:       ingress,
							Port:     svcInfo.Port(),
							Protocol: protocol,
							Net:      src,
							SetType:  utilipset.HashIPPortNet,
						}
						// enumerate all white list source cidr
						if valid := proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].validateEntry(entry); !valid {
							klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].Name))
							continue
						}
						proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].activeEntries.Insert(entry.String())

						// ignore error because it has been validated
						_, cidr, _ := net.ParseCIDR(src)
						if cidr.Contains(proxier.nodeIP) {
							allowFromNode = true
						}
					}
					// generally, ip route rule was added to intercept request to loadbalancer vip from the
					// loadbalancer's backend hosts. In this case, request will not hit the loadbalancer but loop back directly.
					// Need to add the following rule to allow request on host.
					if allowFromNode {
						entry = &amp;utilipset.Entry{
							IP:       ingress,
							Port:     svcInfo.Port(),
							Protocol: protocol,
							IP2:      ingress,
							SetType:  utilipset.HashIPPortIP,
						}
						// enumerate all white list source ip
						if valid := proxier.ipsetList[kubeLoadBalancerSourceIPSet].validateEntry(entry); !valid {
							klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSourceIPSet].Name))
							continue
						}
						proxier.ipsetList[kubeLoadBalancerSourceIPSet].activeEntries.Insert(entry.String())
					}
				}

				// ipvs call
				serv := &amp;utilipvs.VirtualServer{
					Address:   net.ParseIP(ingress),
					Port:      uint16(svcInfo.Port()),
					Protocol:  string(svcInfo.Protocol()),
					Scheduler: proxier.ipvsScheduler,
				}
				if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP {
					serv.Flags |= utilipvs.FlagPersistent
					serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds())
				}
				if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil {
					activeIPVSServices[serv.String()] = true
					activeBindAddrs[serv.Address.String()] = true
					if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil {
						klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
					}
				} else {
					klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
				}
			}
		}

		if svcInfo.NodePort() != 0 {
			if len(nodeAddresses) == 0 || len(nodeIPs) == 0 {
				// Skip nodePort configuration since an error occurred when
				// computing nodeAddresses or nodeIPs.
				continue
			}

			var lps []utilproxy.LocalPort
			for _, address := range nodeAddresses {
				lp := utilproxy.LocalPort{
					Description: &quot;nodePort for &quot; + svcNameString,
					IP:          address,
					Port:        svcInfo.NodePort(),
					Protocol:    protocol,
				}
				if utilproxy.IsZeroCIDR(address) {
					// Empty IP address means all
					lp.IP = &quot;&quot;
					lps = append(lps, lp)
					// If we encounter a zero CIDR, then there is no point in processing the rest of the addresses.
					break
				}
				lps = append(lps, lp)
			}

			// For ports on node IPs, open the actual port and hold it.
			for _, lp := range lps {
				if proxier.portsMap[lp] != nil {
					klog.V(4).Infof(&quot;Port %s was open before and is still needed&quot;, lp.String())
					replacementPortsMap[lp] = proxier.portsMap[lp]
					// We do not start listening on SCTP ports, according to our agreement in the
					// SCTP support KEP
				} else if svcInfo.Protocol() != v1.ProtocolSCTP {
					socket, err := proxier.portMapper.OpenLocalPort(&amp;lp, isIPv6)
					if err != nil {
						klog.Errorf(&quot;can't open %s, skipping this nodePort: %v&quot;, lp.String(), err)
						continue
					}
					if lp.Protocol == &quot;udp&quot; {
						conntrack.ClearEntriesForPort(proxier.exec, lp.Port, isIPv6, v1.ProtocolUDP)
					}
					replacementPortsMap[lp] = socket
				} // We're holding the port, so it's OK to install ipvs rules.
			}

			// Nodeports need SNAT, unless they're local.
			// ipset call

			var (
				nodePortSet *IPSet
				entries     []*utilipset.Entry
			)

			switch protocol {
			case &quot;tcp&quot;:
				nodePortSet = proxier.ipsetList[kubeNodePortSetTCP]
				entries = []*utilipset.Entry{{
					// No need to provide ip info
					Port:     svcInfo.NodePort(),
					Protocol: protocol,
					SetType:  utilipset.BitmapPort,
				}}
			case &quot;udp&quot;:
				nodePortSet = proxier.ipsetList[kubeNodePortSetUDP]
				entries = []*utilipset.Entry{{
					// No need to provide ip info
					Port:     svcInfo.NodePort(),
					Protocol: protocol,
					SetType:  utilipset.BitmapPort,
				}}
			case &quot;sctp&quot;:
				nodePortSet = proxier.ipsetList[kubeNodePortSetSCTP]
				// Since hash ip:port is used for SCTP, all the nodeIPs to be used in the SCTP ipset entries.
				entries = []*utilipset.Entry{}
				for _, nodeIP := range nodeIPs {
					entries = append(entries, &amp;utilipset.Entry{
						IP:       nodeIP.String(),
						Port:     svcInfo.NodePort(),
						Protocol: protocol,
						SetType:  utilipset.HashIPPort,
					})
				}
			default:
				// It should never hit
				klog.Errorf(&quot;Unsupported protocol type: %s&quot;, protocol)
			}
			if nodePortSet != nil {
				entryInvalidErr := false
				for _, entry := range entries {
					if valid := nodePortSet.validateEntry(entry); !valid {
						klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, nodePortSet.Name))
						entryInvalidErr = true
						break
					}
					nodePortSet.activeEntries.Insert(entry.String())
				}
				if entryInvalidErr {
					continue
				}
			}

			// Add externaltrafficpolicy=local type nodeport entry
			if svcInfo.OnlyNodeLocalEndpoints() {
				var nodePortLocalSet *IPSet
				switch protocol {
				case &quot;tcp&quot;:
					nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetTCP]
				case &quot;udp&quot;:
					nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetUDP]
				case &quot;sctp&quot;:
					nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetSCTP]
				default:
					// It should never hit
					klog.Errorf(&quot;Unsupported protocol type: %s&quot;, protocol)
				}
				if nodePortLocalSet != nil {
					entryInvalidErr := false
					for _, entry := range entries {
						if valid := nodePortLocalSet.validateEntry(entry); !valid {
							klog.Errorf(&quot;%s&quot;, fmt.Sprintf(EntryInvalidErr, entry, nodePortLocalSet.Name))
							entryInvalidErr = true
							break
						}
						nodePortLocalSet.activeEntries.Insert(entry.String())
					}
					if entryInvalidErr {
						continue
					}
				}
			}

			// Build ipvs kernel routes for each node ip address
			for _, nodeIP := range nodeIPs {
				// ipvs call
				serv := &amp;utilipvs.VirtualServer{
					Address:   nodeIP,
					Port:      uint16(svcInfo.NodePort()),
					Protocol:  string(svcInfo.Protocol()),
					Scheduler: proxier.ipvsScheduler,
				}
				if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP {
					serv.Flags |= utilipvs.FlagPersistent
					serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds())
				}
				// There is no need to bind Node IP to dummy interface, so set parameter `bindAddr` to `false`.
				if err := proxier.syncService(svcNameString, serv, false, bindedAddresses); err == nil {
					activeIPVSServices[serv.String()] = true
					if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil {
						klog.Errorf(&quot;Failed to sync endpoint for service: %v, err: %v&quot;, serv, err)
					}
				} else {
					klog.Errorf(&quot;Failed to sync service: %v, err: %v&quot;, serv, err)
				}
			}
		}
	}
</code></pre>
<p>其中有两个非常重要的函数 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1215-L1229" target="_blank"
   rel="noopener nofollow noreferrer" >syncService()</a> 于 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1215-L1229" target="_blank"
   rel="noopener nofollow noreferrer" >syncEndpoint()</a>  这两个定义了同步的过程</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1921-L1959" target="_blank"
   rel="noopener nofollow noreferrer" >syncService()</a> 函数表示了增加或删除一个service，如果存在则修改，如果存在则添加</p>
<pre><code class="language-go">func (proxier *Proxier) syncService(svcName string, vs *utilipvs.VirtualServer, bindAddr bool, bindedAddresses sets.String) error {
	appliedVirtualServer, _ := proxier.ipvs.GetVirtualServer(vs)
	if appliedVirtualServer == nil || !appliedVirtualServer.Equal(vs) {
		if appliedVirtualServer == nil {
			// IPVS service is not found, create a new service
			klog.V(3).Infof(&quot;Adding new service %q %s:%d/%s&quot;, svcName, vs.Address, vs.Port, vs.Protocol)
			if err := proxier.ipvs.AddVirtualServer(vs); err != nil {
				klog.Errorf(&quot;Failed to add IPVS service %q: %v&quot;, svcName, err)
				return err
			}
		} else {
			// IPVS service was changed, update the existing one
			// During updates, service VIP will not go down
			klog.V(3).Infof(&quot;IPVS service %s was changed&quot;, svcName)
			if err := proxier.ipvs.UpdateVirtualServer(vs); err != nil {
				klog.Errorf(&quot;Failed to update IPVS service, err:%v&quot;, err)
				return err
			}
		}
	}

	// bind service address to dummy interface
	if bindAddr {
		// always attempt to bind if bindedAddresses is nil,
		// otherwise check if it's already binded and return early
		if bindedAddresses != nil &amp;&amp; bindedAddresses.Has(vs.Address.String()) {
			return nil
		}

		klog.V(4).Infof(&quot;Bind addr %s&quot;, vs.Address.String())
		_, err := proxier.netlinkHandle.EnsureAddressBind(vs.Address.String(), DefaultDummyDevice)
		if err != nil {
			klog.Errorf(&quot;Failed to bind service address to dummy device %q: %v&quot;, svcName, err)
			return err
		}
	}

	return nil
}
</code></pre>
<p>同理 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1961-L2090" target="_blank"
   rel="noopener nofollow noreferrer" >syncEndpoint()</a> 也是相同的操作</p>
<pre><code class="language-go">func (proxier *Proxier) syncEndpoint(svcPortName proxy.ServicePortName, onlyNodeLocalEndpoints bool, vs *utilipvs.VirtualServer) error {
	appliedVirtualServer, err := proxier.ipvs.GetVirtualServer(vs)
	if err != nil || appliedVirtualServer == nil {
		klog.Errorf(&quot;Failed to get IPVS service, error: %v&quot;, err)
		return err
	}

	// curEndpoints represents IPVS destinations listed from current system.
	curEndpoints := sets.NewString()
	// newEndpoints represents Endpoints watched from API Server.
	newEndpoints := sets.NewString()

	curDests, err := proxier.ipvs.GetRealServers(appliedVirtualServer)
	if err != nil {
		klog.Errorf(&quot;Failed to list IPVS destinations, error: %v&quot;, err)
		return err
	}
	for _, des := range curDests {
		curEndpoints.Insert(des.String())
	}

	endpoints := proxier.endpointsMap[svcPortName]

	// Service Topology will not be enabled in the following cases:
	// 1. externalTrafficPolicy=Local (mutually exclusive with service topology).
	// 2. ServiceTopology is not enabled.
	// 3. EndpointSlice is not enabled (service topology depends on endpoint slice
	// to get topology information).
	if !onlyNodeLocalEndpoints &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.ServiceTopology) &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.EndpointSliceProxying) {
		endpoints = proxy.FilterTopologyEndpoint(proxier.nodeLabels, proxier.serviceMap[svcPortName].TopologyKeys(), endpoints)
	}

	for _, epInfo := range endpoints {
		if onlyNodeLocalEndpoints &amp;&amp; !epInfo.GetIsLocal() {
			continue
		}
		newEndpoints.Insert(epInfo.String())
	}

	// Create new endpoints
	for _, ep := range newEndpoints.List() {
		ip, port, err := net.SplitHostPort(ep)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint: %v, error: %v&quot;, ep, err)
			continue
		}
		portNum, err := strconv.Atoi(port)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint port %s, error: %v&quot;, port, err)
			continue
		}

		newDest := &amp;utilipvs.RealServer{
			Address: net.ParseIP(ip),
			Port:    uint16(portNum),
			Weight:  1,
		}

		if curEndpoints.Has(ep) {
			// check if newEndpoint is in gracefulDelete list, if true, delete this ep immediately
			uniqueRS := GetUniqueRSName(vs, newDest)
			if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
				continue
			}
			klog.V(5).Infof(&quot;new ep %q is in graceful delete list&quot;, uniqueRS)
			err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS)
			if err != nil {
				klog.Errorf(&quot;Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v&quot;, ep, err)
				continue
			}
		}
		err = proxier.ipvs.AddRealServer(appliedVirtualServer, newDest)
		if err != nil {
			klog.Errorf(&quot;Failed to add destination: %v, error: %v&quot;, newDest, err)
			continue
		}
	}
	// Delete old endpoints
	for _, ep := range curEndpoints.Difference(newEndpoints).UnsortedList() {
		// if curEndpoint is in gracefulDelete, skip
		uniqueRS := vs.String() + &quot;/&quot; + ep
		if proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
			continue
		}
		ip, port, err := net.SplitHostPort(ep)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint: %v, error: %v&quot;, ep, err)
			continue
		}
		portNum, err := strconv.Atoi(port)
		if err != nil {
			klog.Errorf(&quot;Failed to parse endpoint port %s, error: %v&quot;, port, err)
			continue
		}

		delDest := &amp;utilipvs.RealServer{
			Address: net.ParseIP(ip),
			Port:    uint16(portNum),
		}

		klog.V(5).Infof(&quot;Using graceful delete to delete: %v&quot;, uniqueRS)
		err = proxier.gracefuldeleteManager.GracefulDeleteRS(appliedVirtualServer, delDest)
		if err != nil {
			klog.Errorf(&quot;Failed to delete destination: %v, error: %v&quot;, uniqueRS, err)
			continue
		}
	}
	return nil
}
</code></pre>
<p>接下来就是同步规则的步骤了，<a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L1544-L1621" target="_blank"
   rel="noopener nofollow noreferrer" >L1544-L1621</a></p>
<h3 id="step-3规则的删除">step 3：规则的删除</h3>
<p>粗略翻到这里可能有一个疑问？没有提到删除，删除时包含在 syncXX() 函数中的</p>
<p>例如在 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L2063-L2088" target="_blank"
   rel="noopener nofollow noreferrer" >syncEndpoint()</a> 中会看是否在 终止列表中，如果在跳过，如果不在加入</p>
<pre><code class="language-go">if curEndpoints.Has(ep) {
    // check if newEndpoint is in gracefulDelete list, if true, delete this ep immediately
    uniqueRS := GetUniqueRSName(vs, newDest)
    if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) {
        continue
    }
    klog.V(5).Infof(&quot;new ep %q is in graceful delete list&quot;, uniqueRS)
    err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS)
    if err != nil {
        klog.Errorf(&quot;Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v&quot;, ep, err)
        continue
    }
}
</code></pre>
<p>而 <a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L275" target="_blank"
   rel="noopener nofollow noreferrer" >gracefuldeleteManager</a> 是一个 一直运行的协程，在初始化 proxier 时被 Run()</p>
<pre><code class="language-go">// Run start a goroutine to try to delete rs in the graceful delete rsList with an interval 1 minute
func (m *GracefulTerminationManager) Run() {
	go wait.Until(m.tryDeleteRs, rsCheckDeleteInterval, wait.NeverStop)
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/e979822c185a14537054f15808a118d7fcce1d6e/pkg/proxy/ipvs/proxier.go#L508" target="_blank"
   rel="noopener nofollow noreferrer" >proxier.go</a></p>
<pre><code class="language-go">proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs)
proxier.gracefuldeleteManager.Run()
return proxier, nil
</code></pre>
<h2 id="总结">总结</h2>
<p>到这里已经清楚的掌握了 kube-proxy 的架构，接下来的会为扩展kubernetes中service架构，以及手撸一个 kube-proxy做准备；本系列第三部分：<a href="https://cylonchau.github.io/ch3.7-admission-webhook.html" target="_blank"
   rel="noopener nofollow noreferrer" >如何扩展现有的kube-proxy架构</a></p>
<p>文中的知识都是个人根据理解整理的，如有不对的地方欢迎指出，感谢各位大佬</p>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="5">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>dual-stack service</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes service - 你真的理解service吗？</title>
      <link>https://www.oomkill.com/2023/02/ch17-service-controller/</link>
      <pubDate>Mon, 06 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/02/ch17-service-controller/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p>本文是关于Kubernetes service解析的第一章</p>
<ul>
<li>深入理解Kubernetes service - 你真的理解service吗?</li>
<li><a href="https://cylonchau.github.io/kubernetes-endpointslices.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - EndpointSlices做了什么？</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-kube-proxy-code.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - kube-proxy架构分析</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-extend-kube-proxy.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - 如何扩展现有的kube-proxy架构</a></li>
</ul>
<p>所有关于Kubernetes service 部分代码上传至仓库 <a href="https://github.com/cylonchau/kube-haproxy" target="_blank"
   rel="noopener nofollow noreferrer" >github.com/cylonchau/kube-haproxy</a></p>
</blockquote>
<h2 id="前景">前景</h2>
<p>对于了解kubernetes架构时，已知的是 <code>service</code> 是kubernetes在设计时为了避免Pod在频繁创建和销毁时IP变更问题，从而给集群内服务（一组Pod）提供访问的一个入口。而Pod在这里的角色是 『后端』( <em><strong>backend</strong></em> ) ，而 service 的角色是 『前端』( <em><strong>frontend</strong></em> )。本文将阐述service的生命周期</p>
<h2 id="为什么需要了解这部分内容呢">为什么需要了解这部分内容呢</h2>
<p>对于 without kube-proxy来说，这部分是最重要的部分，因为service的生成不是kube-proxy来完成的，而这部分也就是service ip定义的核心。</p>
<h2 id="控制器">控制器</h2>
<p>service的资源创建很奇妙，继不属于 <code>controller-manager</code> 组件，也不属于 <code>kube-proxy</code> 组件，而是存在于 <code>apiserver</code> 中的一个被成为控制器的组件；而这个控制器又区别于准入控制器。更准确来说，准入控制器是位于kubeapiserver中的组件，而 <strong>控制器</strong> 则是存在于单独的一个包，这里包含了很多kubernetes集群的公共组件的功能，其中就有service。这也就是在操作kubernetes时 当 <code>controller-manager</code> 于  <code>kube-proxy</code> 未工作时，也可以准确的为service分配IP。</p>
<p>首先在构建出apiserver时，也就是代码 <a href="cmd/kube-apiserver/app/server.go">cmd/kube-apiserver/app/server.go</a></p>
<pre><code class="language-go">serviceIPRange, apiServerServiceIP, err := master.ServiceIPRange(s.PrimaryServiceClusterIPRange)
if err != nil {
    return nil, nil, nil, nil, err
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/pkg/master/services.go#L34-L54" target="_blank"
   rel="noopener nofollow noreferrer" >master.ServiceIPRange</a> 承接了为service分配IP的功能，这部分逻辑就很简单了</p>
<pre><code class="language-go">func ServiceIPRange(passedServiceClusterIPRange net.IPNet) (net.IPNet, net.IP, error) {
	serviceClusterIPRange := passedServiceClusterIPRange
	if passedServiceClusterIPRange.IP == nil {
		klog.Warningf(&quot;No CIDR for service cluster IPs specified. Default value which was %s is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.&quot;, kubeoptions.DefaultServiceIPCIDR.String())
		serviceClusterIPRange = kubeoptions.DefaultServiceIPCIDR
	}

	size := integer.Int64Min(utilnet.RangeSize(&amp;serviceClusterIPRange), 1&lt;&lt;16)
	if size &lt; 8 {
		return net.IPNet{}, net.IP{}, fmt.Errorf(&quot;the service cluster IP range must be at least %d IP addresses&quot;, 8)
	}

	// Select the first valid IP from ServiceClusterIPRange to use as the GenericAPIServer service IP.
	apiServerServiceIP, err := utilnet.GetIndexedIP(&amp;serviceClusterIPRange, 1)
	if err != nil {
		return net.IPNet{}, net.IP{}, err
	}
	klog.V(4).Infof(&quot;Setting service IP to %q (read-write).&quot;, apiServerServiceIP)

	return serviceClusterIPRange, apiServerServiceIP, nil
}
</code></pre>
<p>而后kube-apiserver为service分为两类</p>
<ul>
<li>apiserver 地址在集群内的service，在代码中表示为 <a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L351" target="_blank"
   rel="noopener nofollow noreferrer" >APIServerServiceIP</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L352" target="_blank"
   rel="noopener nofollow noreferrer" >Service</a>，<code>--service-cluster-ip-range</code> 配置指定的ip，通过『逗号』分割可以为两个</li>
</ul>
<p>有了对 service 更好的理解后，接下来开始本系列第二节<a href="https://cylonchau.github.io/kubernetes-without-service.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - kube-proxy软件架构分析</a></p>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>dual-stack service</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes 4A - Audit源码解析</title>
      <link>https://www.oomkill.com/2022/11/ch34-auditing/</link>
      <pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/11/ch34-auditing/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<blockquote>
<p>本文是关于Kubernetes 4A解析的第四章</p>
<ul>
<li><a href="https://cylonchau.github.io/kubernetes-authentication.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authentication源码解析</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-authorization.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authorization源码解析</a></li>
<li><a href="https://cylonchau.github.io/ch3.7-admission-webhook.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Admission Control源码解析</a></li>
<li>深入理解Kubernetes 4A - Audit源码解析</li>
</ul>
<p>所有关于Kubernetes 4A四部分代码上传至仓库 github.com/cylonchau/hello-k8s-4A</p>
</blockquote>
<p>审计是信息系统中非常重要的一部分，Kubernetes 1.11中也增加了审计 (<em><strong>Auditing</strong></em>) 功能，通过审计功能获得 deployment, ns,等资源操作的事件。</p>
<p><strong>objective</strong>：</p>
<ul>
<li>从设计角度了解Auditing在kubernets中是如何实现的</li>
<li>了解kubernetes auditing webhook</li>
<li>完成实验，通过webhook来收集审计日志</li>
</ul>
<p>如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大。</p>
<h2 id="kubernetes-auditing">Kubernetes Auditing</h2>
<p>根据Kubernetes官方描述审计在kubernetes中是有控制平面 <em>kube-apiserver</em> 中产生的一个事件，记录了集群中所操作的资源，审计围绕下列几个维度来记录事件的：</p>
<ul>
<li>发生了什么</li>
<li>发生的事件</li>
<li>谁触发的</li>
<li>发生动作的对象</li>
<li>在哪里检查到动作的</li>
<li>从哪触发的</li>
<li>处理行为是什么</li>
</ul>
<p>审计生命周期开始于组件 <em>kube-apiserver</em> 准入控制阶段，在每个阶段内都会产生审计事件并经过预处理后写入后端，目前后端包含webhook与日志文件。</p>
<blockquote>
<p>审计日志功能增加了 <em>kube-apiserver</em> 的内存消耗，因为会为每个请求存储了审计所需的上下文。内存的消耗取决于审计日志配置 <sup><a href="#1">[1]</a></sup>。</p>
</blockquote>
<h2 id="审计事件设计">审计事件设计</h2>
<p>审计的schema不同于资源API的设计，没有 <code>metav1.ObjectMeta</code> 属性，Event是一个事件的结构体，Policy是事件配置，属于kubernetes资源，在代码 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/apis/audit/types.go#L79-L148" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/apis/audit/types.go</a> 可以看到</p>
<pre><code class="language-go">type Event struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	Level Level `json:&quot;level&quot; protobuf:&quot;bytes,1,opt,name=level,casttype=Level&quot;
	AuditID types.UID `json:&quot;auditID&quot; protobuf:&quot;bytes,2,opt,name=auditID,casttype=k8s.io/apimachinery/pkg/types.UID&quot;`
	
	Stage Stage `json:&quot;stage&quot; protobuf:&quot;bytes,3,opt,name=stage,casttype=Stage&quot;`
	RequestURI string `json:&quot;requestURI&quot; protobuf:&quot;bytes,4,opt,name=requestURI&quot;`
	Verb string `json:&quot;verb&quot; protobuf:&quot;bytes,5,opt,name=verb&quot;`
	User authnv1.UserInfo `json:&quot;user&quot; protobuf:&quot;bytes,6,opt,name=user&quot;`
	ImpersonatedUser *authnv1.UserInfo `json:&quot;impersonatedUser,omitempty&quot; protobuf:&quot;bytes,7,opt,name=impersonatedUser&quot;`
	SourceIPs []string `json:&quot;sourceIPs,omitempty&quot; protobuf:&quot;bytes,8,rep,name=sourceIPs&quot;`
	UserAgent string `json:&quot;userAgent,omitempty&quot; protobuf:&quot;bytes,16,opt,name=userAgent&quot;`
	ObjectRef *ObjectReference `json:&quot;objectRef,omitempty&quot; protobuf:&quot;bytes,9,opt,name=objectRef&quot;`
	// +optional
	ResponseStatus *metav1.Status `json:&quot;responseStatus,omitempty&quot; protobuf:&quot;bytes,10,opt,name=responseStatus&quot;`

...
	
}
</code></pre>
<p>对于记录的认证事件来说，会根据请求阶段记录审计的阶段，主要分为下属集中情况，每个请求会记录其中一个验证阶段，如代码所示 <sup><a href="#1">[1]</a></sup></p>
<pre><code class="language-go">const (
    // 这个阶段是audit handler收到请求后立即生成事件的阶段，然后委托handler chain处理。
	StageRequestReceived Stage = &quot;RequestReceived&quot;
    // 这个阶段阶段仅对长时间运行的请求如 watch
	// 将在发送响应标头后，响应正文之前生成的阶段
	StageResponseStarted Stage = &quot;ResponseStarted&quot;
    // 这个阶段是发送相应体后的事件。
	StageResponseComplete Stage = &quot;ResponseComplete&quot;
	// 如果程序出现panic，则触发这个阶段
	StagePanic Stage = &quot;Panic&quot;
)
</code></pre>
<h2 id="审计工作流程">审计工作流程</h2>
<p>审计真正工作的地方在 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go#L42-L119" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/endpoints/filters/audit.go.WithAudit</a> 函数，下面对与官方文档说明与这个实际代码进行结合</p>
<pre><code class="language-go">func WithAudit(handler http.Handler, sink audit.Sink, policy audit.PolicyRuleEvaluator, longRunningCheck request.LongRunningRequestCheck) http.Handler {
    // sink是一个backend（webhook 或 日志），policy则是自定义的事件配置
    // 如果两者之一未配置，则不会使用审计功能
	if sink == nil || policy == nil {
		return handler
	}
	return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) {
        // 通过给定的配置与请求构建出一个事件 context
        // 这里可以看到
		auditContext, err := evaluatePolicyAndCreateAuditEvent(req, policy)
		if err != nil {
			utilruntime.HandleError(fmt.Errorf(&quot;failed to create audit event: %v&quot;, err))
			responsewriters.InternalError(w, req, errors.New(&quot;failed to create audit event&quot;))
			return
		}
		// 下面代码可以看出是对事件context进行构建，与拿到来自请求Context
		ev := auditContext.Event
		if ev == nil || req.Context() == nil {
			handler.ServeHTTP(w, req)
			return
		}

		req = req.WithContext(audit.WithAuditContext(req.Context(), auditContext))

		ctx := req.Context()
		omitStages := auditContext.RequestAuditConfig.OmitStages
		
        // 这里到StageRequestReceived阶段，如果是收到请求阶段则通过注入的后端进行处理
		ev.Stage = auditinternal.StageRequestReceived
		if processed := processAuditEvent(ctx, sink, ev, omitStages); !processed {
            audit.ApiserverAuditDroppedCounter.WithContext(ctx).Inc()
			responsewriters.InternalError(w, req, errors.New(&quot;failed to store audit event&quot;))
			return
		}

		// 拦截watch类长请求的状态码
		var longRunningSink audit.Sink
		if longRunningCheck != nil {
			ri, _ := request.RequestInfoFrom(ctx)
			if longRunningCheck(req, ri) {
				longRunningSink = sink
			}
		}
		respWriter := decorateResponseWriter(ctx, w, ev, longRunningSink, omitStages)

		// send audit event when we leave this func, either via a panic or cleanly. In the case of long
		// running requests, this will be the second audit event.
        // 在离开函数前会处理 ResponseStarted、ResponseComplete、Panic这三个阶段
		defer func() {
			if r := recover(); r != nil {
				defer panic(r)
                // 当前发生panic的请求
				ev.Stage = auditinternal.StagePanic
				ev.ResponseStatus = &amp;metav1.Status{
					Code:    http.StatusInternalServerError,
					Status:  metav1.StatusFailure,
					Reason:  metav1.StatusReasonInternalError,
					Message: fmt.Sprintf(&quot;APIServer panic'd: %v&quot;, r),
				}
				processAuditEvent(ctx, sink, ev, omitStages)
				return
			}

			// if no StageResponseStarted event was sent b/c neither a status code nor a body was sent, fake it here
			// But Audit-Id http header will only be sent when http.ResponseWriter.WriteHeader is called.
			fakedSuccessStatus := &amp;metav1.Status{
				Code:    http.StatusOK,
				Status:  metav1.StatusSuccess,
				Message: &quot;Connection closed early&quot;,
			}
			if ev.ResponseStatus == nil &amp;&amp; longRunningSink != nil {
				ev.ResponseStatus = fakedSuccessStatus
				ev.Stage = auditinternal.StageResponseStarted
				processAuditEvent(ctx, longRunningSink, ev, omitStages)
			}
			// ResponseStarted 在响应头发送后，响应体发送前的事件。watch会触发他
            
			ev.Stage = auditinternal.StageResponseComplete
			if ev.ResponseStatus == nil {
                // 没有相应状态 正是上面构造的fakedSuccessStatus
				ev.ResponseStatus = fakedSuccessStatus
			}
            // 将事件发送到后端
			processAuditEvent(ctx, sink, ev, omitStages)
		}()
		handler.ServeHTTP(respWriter, req)
	})
}
</code></pre>
<ul>
<li>
<p>在评估请求时，会调用 <code>GetAuthorizerAttributes(ctx)</code> ，这里通过授权记录然后来通过给定的审计配置来</p>
</li>
<li>
<p>当在将事件发送到后端时，使用 <code>processAuditEvent()</code> 函数，最终修改时间后会转交至后端函数，例如webhook，会请求后端配置的webhook url的客户端，最终被执行 <code>return sink.ProcessEvents(ev)</code></p>
<pre><code class="language-go">func (b *backend) processEvents(ev ...*auditinternal.Event) error {
	var list auditinternal.EventList
	for _, e := range ev {
		list.Items = append(list.Items, *e)
	}
	return b.w.WithExponentialBackoff(context.Background(), func() rest.Result {
		trace := utiltrace.New(&quot;Call Audit Events webhook&quot;,
			utiltrace.Field{&quot;name&quot;, b.name},
			utiltrace.Field{&quot;event-count&quot;, len(list.Items)})
		// Only log audit webhook traces that exceed a 25ms per object limit plus a 50ms request overhead allowance. The high per object limit used here is primarily to allow enough time for the serialization/deserialization of audit events, which contain nested request and response objects plus additional event fields.
		defer trace.LogIfLong(time.Duration(50+25*len(list.Items)) * time.Millisecond)
		return b.w.RestClient.Post().Body(&amp;list).Do(context.TODO())
	}).Error()
}
</code></pre>
</li>
</ul>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go#L125-L155" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/endpoints/filters/audit.go.evaluatePolicyAndCreateAuditEvent</a> 会评估请求的级别和规则，而 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/audit/policy/checker.go#L64-L84" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/audit/policy/checker.go</a></p>
<pre><code class="language-go">func (p *policyRuleEvaluator) EvaluatePolicyRule(attrs authorizer.Attributes) auditinternal.RequestAuditConfigWithLevel {
	for _, rule := range p.Rules {
        // 评估则是评估用户与用户组，verb，ns,非API资源 /metrics /healthz
		if ruleMatches(&amp;rule, attrs) {
            // 通过后，则将这条规则与配置返回
			return auditinternal.RequestAuditConfigWithLevel{
				Level: rule.Level,
				RequestAuditConfig: auditinternal.RequestAuditConfig{
					OmitStages:        rule.OmitStages,
					OmitManagedFields: isOmitManagedFields(&amp;rule, p.OmitManagedFields),
				},
			}
		}
	}
	// 如果条件都不满足，则构建一个
	return auditinternal.RequestAuditConfigWithLevel{
		Level: DefaultAuditLevel,
		RequestAuditConfig: auditinternal.RequestAuditConfig{
			OmitStages:        p.OmitStages,
			OmitManagedFields: p.OmitManagedFields,
		},
	}
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/audit/request.go#L48-L93" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/audit/request.go.NewEventFromRequest</a> 创建出审计事件对象被上面 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go#L125-L155" target="_blank"
   rel="noopener nofollow noreferrer" >evaluatePolicyAndCreateAuditEvent</a> 返回</p>
<pre><code class="language-go">// evaluatePolicyAndCreateAuditEvent is responsible for evaluating the audit
// policy configuration applicable to the request and create a new audit
// event that will be written to the API audit log.
// - error if anything bad happened
func evaluatePolicyAndCreateAuditEvent(req *http.Request, policy audit.PolicyRuleEvaluator) (*audit.AuditContext, error) {
	ctx := req.Context()

	attribs, err := GetAuthorizerAttributes(ctx)
	if err != nil {
		return nil, fmt.Errorf(&quot;failed to GetAuthorizerAttributes: %v&quot;, err)
	}

	ls := policy.EvaluatePolicyRule(attribs)
	audit.ObservePolicyLevel(ctx, ls.Level)
	if ls.Level == auditinternal.LevelNone {
		// Don't audit.
		return &amp;audit.AuditContext{
			RequestAuditConfig: ls.RequestAuditConfig,
		}, nil
	}

	requestReceivedTimestamp, ok := request.ReceivedTimestampFrom(ctx)
	if !ok {
		requestReceivedTimestamp = time.Now()
	}
	ev, err := audit.NewEventFromRequest(req, requestReceivedTimestamp, ls.Level, attribs)
	if err != nil {
		return nil, fmt.Errorf(&quot;failed to complete audit event from request: %v&quot;, err)
	}

	return &amp;audit.AuditContext{
		RequestAuditConfig: ls.RequestAuditConfig,
		Event:              ev,
	}, nil
}
</code></pre>
<p>到这里，已经清楚的了解到，Kubernetes审计工作与什么位置了，而对于Kubernetes准入给出的登录（<em><strong>Authentication</strong></em>），授权 (<em><strong>Authorization</strong></em>) 与 准入控制 (<em><strong>Admission control</strong></em>) 三个阶段来说，Audition 位于授权之后，正如下图所示，而这个真正的流程在kubernetes中有个属于叫 <em><strong>handler chain</strong></em> 整个链条中，准入与审计只是其中一部分。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221128001225515.png" alt="image-20221128001225515" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes 4A 的 handler chain</center><br>
<p>由图再结合代码可以看出，所有的客户端访问API都需要经过完整经由整个链条，而 Auditing 事件的构建是需要获取经由验证过的用户等资源构建出的事件，首次发生的为 <code>StageRequestReceived</code> ，这将在收到请求后执行，而由代码又可知，因为在最终结束掉整个请求时会执行 <code>WithAudit</code> 函数，这就为 <code>StageResponseComplete</code> 与 <code>StageResponseStarted</code> 这两个阶段被执行，而这个将发生在被注册的 handler 完成后，也就是  <em><strong>Admission control</strong></em> 后因为 AC 是在每个真实REST中被执行。TODO</p>
<h2 id="审计策略级别-supa-href22asup">审计策略级别 <sup><a href="#2">[2]</a></sup></h2>
<p>审计策略级别是控制审计记录将记录那些对象的数据内容，当事件被处理时，会按照配置的审计规则进行比较。而使用该功能需要 <em>kube-apiserver</em> 开启参数 <code>--audit-policy-file</code> 指定对应的配置，如果未指定则默认不记录任何事件，可供定义的级别有四个，被定义在 k8s.io/apiserver/pkg/apis/audit/v1/types.go 中</p>
<pre><code class="language-go">const (
	// LevelNone disables auditing
	LevelNone Level = &quot;None&quot;
	// LevelMetadata provides the basic level of auditing.
	LevelMetadata Level = &quot;Metadata&quot;
	// LevelRequest provides Metadata level of auditing, and additionally
	// logs the request object (does not apply for non-resource requests).
	LevelRequest Level = &quot;Request&quot;
	// LevelRequestResponse provides Request level of auditing, and additionally
	// logs the response object (does not apply for non-resource requests).
	LevelRequestResponse Level = &quot;RequestResponse&quot;
)
</code></pre>
<ul>
<li><strong>None</strong>： 不记录符合该规则的事件</li>
<li><strong>Metadata</strong>：只记录请求元数据（如User, timestamp, resources, verb），不记录请求和响应体。</li>
<li><strong>Request</strong>：记录事件元数据和请求体，不记录响应体。</li>
<li><strong>RequestResponse</strong>： 记录事件元数据，请求和响应体</li>
</ul>
<p>下面是Kubernetes官网给出的 Policy 的配置 <sup><a href="#2">[2]</a></sup></p>
<pre><code class="language-yaml">apiVersion: audit.k8s.io/v1 # This is required.
kind: Policy
# omitStages 代表忽略该阶段所有请求事件
# RequestReceived 这里配置的指在RequestReceived阶段忽略所有请求事件
omitStages:
  - &quot;RequestReceived&quot;
rules:
  # 记录将以RequestResponse级别的格式记录pod更改
  - level: RequestResponse
    resources:
    - group: &quot;&quot;
      # 这里资源的配置必须与RBAC配置的一致，pods将不支持pods/log这类子资源
      resources: [&quot;pods&quot;]
  # 如果需要配置子资源按照下列方式
  - level: Metadata
    resources:
    - group: &quot;&quot;
      resources: [&quot;pods/log&quot;, &quot;pods/status&quot;]

  # 不记录的资源为controller-leader的configmaps资源的请求
  - level: None
    resources:
    - group: &quot;&quot;
      resources: [&quot;configmaps&quot;]
      resourceNames: [&quot;controller-leader&quot;]

  # 不记录用户为 &quot;system:kube-proxy&quot; 发起的对 endpoints与services资源的watch请求事件
  - level: None
    users: [&quot;system:kube-proxy&quot;]
    verbs: [&quot;watch&quot;]
    resources:
    - group: &quot;&quot; # core API group
      resources: [&quot;endpoints&quot;, &quot;services&quot;]

  # 每个登录成功的用户，都会被追加一个用户组为 &quot;system:authenticated&quot;
  # 下述规则为不记录包含非资源类型的URL的已认证请求
  - level: None
    userGroups: [&quot;system:authenticated&quot;]
    nonResourceURLs:
    - &quot;/api*&quot; # Wildcard matching.
    - &quot;/version&quot;

  # 记录kube-system名称空间configmap更改事件的请求体与元数据
  - level: Request
    resources:
    - group: &quot;&quot; # core API group
      resources: [&quot;configmaps&quot;]
    # This rule only applies to resources in the &quot;kube-system&quot; namespace.
    # The empty string &quot;&quot; can be used to select non-namespaced resources.
    namespaces: [&quot;kube-system&quot;]

  # 事件将记录所有名称空间内对于configmap与secret资源改变的元数据
  - level: Metadata
    resources:
    - group: &quot;&quot; # core API group
      resources: [&quot;secrets&quot;, &quot;configmaps&quot;]

  # 记录对group为core与extensions下的资源类型请求的 请求体与元数据（request级别）
  - level: Request
    resources:
    - group: &quot;&quot; # core API group
    - group: &quot;extensions&quot; # Version of group should NOT be included.

  # 这种属于泛规则，会记录所有上述其他之外的所有类型请求的元数据
  # 类似于授权，小权限在前，* 最后
  - level: Metadata
    # Long-running requests like watches that fall under this rule will not
    # generate an audit event in RequestReceived.
    omitStages:
      - &quot;RequestReceived&quot;
</code></pre>
<h2 id="backend-supa-href33asup">Backend <sup><a href="#3">[3]</a></sup></h2>
<p>kubernetes目前为Auditing 提供了两个后端，日志方式与webhook方式，kubernetes审计事件会遵循 <code>audit.k8s.io</code> 结构写入到后端。</p>
<h3 id="日志模式配置">日志模式配置</h3>
<p>启用日志模式只需要配置几个参数 <sup><a href="#4">[4]</a></sup></p>
<ul>
<li><code>--audit-log-path</code> 写入审计事件的日志路径。这个是必须配置的否则默认输出到STDOUT</li>
<li><code>--audit-log-maxage</code>  审计日志文件保留的最大天数</li>
<li><code>--audit-log-maxbackup</code> 审计日志保留的的最大数量</li>
<li><code>--audit-log-maxsize</code> 审计日志文件最大大小（单位M）大于会切割</li>
</ul>
<p>例如配置</p>
<pre><code class="language-bash">--audit-policy-file=/etc/kubernetes/audit-policy.yaml \
--audit-log-path=/var/log/kubernetes/audit.log \
--audit-log-maxsize=20M
</code></pre>
<h3 id="webhook-supa-href55asup">webhook <sup><a href="#5">[5]</a></sup></h3>
<p>webhook是指审计事件将由 <em>kube-apiserver</em> 发送到webhook服务中记录，开启webhook只需要配置 <code>--audit-webhook-config-file</code> 与 <code>--audit-policy-file</code> 两个参数，而其他的则是对该模式的辅助</p>
<ul>
<li>
<p><code>--audit-webhook-config-file</code> ：webhook的配置文件，格式是kubeconfig类型，所有的信息不是kubernetes api配置，而是webhook相关信息</p>
</li>
<li>
<p><code>--audit-webhook-initial-backoff </code> ：第一次失败后重试事件，随后仍失败后将以指数方式退避重试</p>
</li>
<li>
<p><code>--audit-webhook-mode</code> ：发送至webhook的模式。 <em>batch</em>, <em>blocking</em>, <em>blocking-strict</em> 。</p>
</li>
</ul>
<p>例如配置</p>
<pre><code class="language-yaml">--audit-policy-file=/etc/kubernetes/audit-policy.yaml \
--audit-webhook-config-file=/etc/kubernetes/auth/audit-webhook.yaml \
--audit-webhook-mode=batch \
</code></pre>
<p>对于initialBackoff 的退避重试则如代码所示 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/server/options/audit.go#L584-L594" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/options/audit.go</a></p>
<pre><code class="language-go">func (o *AuditWebhookOptions) newUntruncatedBackend(customDial utilnet.DialFunc) (audit.Backend, error) {
	groupVersion, _ := schema.ParseGroupVersion(o.GroupVersionString)
	webhook, err := pluginwebhook.NewBackend(o.ConfigFile, groupVersion, webhook.DefaultRetryBackoffWithInitialDelay(o.InitialBackoff), customDial)
	if err != nil {
		return nil, fmt.Errorf(&quot;initializing audit webhook: %v&quot;, err)
	}
	webhook = o.BatchOptions.wrapBackend(webhook)
	return webhook, nil
}
</code></pre>
<p>在函数 <a href="https://github.com/kubernetes/kubernetes/blob/e72eea92396503521f1acaab60a2975c13ffc618/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go#L42-L49" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/util/webhook/webhook.go.DefaultRetryBackoffWithInitialDelay</a> 中看到 通过 wait.Backoff 进行的</p>
<pre><code class="language-go">return wait.Backoff{
    // 时间间隔，用于调用 Step 方法时返回的时间间隔
    Duration: initialBackoffDelay,  
   	
    // 用于计算下次的时间间隔，不能为负数
    // Factor 大于 0 时，Backoff 在计算下次的时间间隔时都会根据 
    // Duration * Factor，Factor * Duration 不能大于 Cap
    Factor:   1.5,
    
    // 抖动，Jitter &gt; 0 时，每次迭代的时间间隔都会额外加上 0 - Duration * Jitter 的随机时间,
    // 并且抖动出的时间不会设置为 Duration，而且不受 Caps 的限制
    Jitter:   0.2,
    
    // 进行指数回退(*Factor) 操作的次数
    // 当 Factor * Duration &gt; Cap 时 Steps 会被设置为 0, Duration 设置为 Cap
    // 也就是说后续的迭代时间间隔都会返回 Duration
    Steps:    5,
    
    // 还有一个cap（Cap time.Duration），是最大的时间间隔
}
</code></pre>
<h3 id="批处理">批处理</h3>
<p>日志后端与webhook后端都支持批处理模式，默认值为webhook默认开启batch，而log则被禁用</p>
<ul>
<li><code>--audit-log-mode/--audit-webhook-mode</code> ：参数通过将webhook替换为log则为对应的 batch 模式的参数，可以通过 <code>kube-apiserver --help|grep &quot;audit&quot;|grep batch</code> 查看
<ul>
<li><code>batch</code> 默认值，缓冲事件进行异步批量处理
<ul>
<li><code>--audit-webhook-batch-buffer-size</code>：批处理之前要缓冲的事件数。如果传入事件的溢出，则被丢弃。</li>
<li><code>--audit-webhook-batch-max-size</code>：定义每一批中的最大事件数</li>
<li><code>--audit-webhook-batch-max-wait</code>：批处理队列未满时等待的事件，到时强制写入一次</li>
<li><code>--audit-webhook-batch-throttle-qps</code>：定义每秒最大平均批次</li>
<li><code>--audit-webhook-batch-throttle-burst</code>：如果之前还没使用throttle-qps之前，发送的最大批数，通常情况下为第一次启动时生效的参数</li>
</ul>
</li>
<li><code>blocking</code> 阻止 apiserver 处理每个单独事件</li>
<li><code>blocking-strict</code>：与 <em>blocking</em> 相同，但当 <em>RequestReceived</em> 阶段的审计日志记录失败时，对 kube-apiserver 的整个请求都将失败</li>
</ul>
</li>
</ul>
<h2 id="参数调整">参数调整</h2>
<p>适当的调整参数与策略可以有效适应 <em>kuber-apiserver</em> 的负载，如在记录日志时应只记录所需的事件，而不是所有的事件，这样可以避免 APIServer不必要开销，例如：</p>
<ul>
<li>每个请求存在多个阶段，而审计时其实不关心响应等信息，可以只记录 <code>RequestReceived</code> 的 <code>metadata</code> 级别。</li>
<li>&ldquo;pods/log&rdquo;, &ldquo;pods/status&rdquo; 在记录时应该区分子资源类型，而不要直接写 <em><code>pods</code></em> 或 <em><code>pods/*</code></em></li>
<li>kubernetes系统组件内的事件如果没有特殊要求可以不记录</li>
<li>对于资源类型，如configmap的请求其实没必要记录</li>
<li>审计记录应严格按照外部用户记录，而不是所有请求</li>
</ul>
<p>如何适配APIServer的负载能力，正如官方给的示例一样，如果 <em>kube-apiserver</em> 每秒收到100个请求，而记录事件为 <code>ResponseStarted</code> 和<code>ResponseComplete</code> 阶段，此时会记录的条数约 200/s ，如果batch缓冲区为100，那么需要配置的参数至少2Qps/s。再假设后端处理能力为5秒，那么缓冲区需要配置的大小至少为5秒的事件，即1000条evnet，10个batch。正如下图所示：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/624219-20220802143739996-2062420228.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：审计批处理参数调优结构图</center>
<center><em>Source：</em>https://www.cnblogs.com/zhangmingcheng/p/16539514.html</center><br>
<p>而kube-apiserver提供了两个Prometheus指标可以用于监控审计子系统的状态</p>
<ul>
<li><code>apiserver_audit_event_total</code> 审计事件的总数</li>
<li><code>apiserver_audit_error_total</code> 由于错误而被丢弃的审计事件总数，例如panic类型事件</li>
</ul>
<h2 id="实验audit-webhook">实验：Audit Webhook</h2>
<p>编写一个webhook，用于处理接收到的日志，这里直接打印</p>
<pre><code class="language-go">func serveAudit(w http.ResponseWriter, r *http.Request) {
	b, err := ioutil.ReadAll(r.Body)
	if err != nil {
		httpError(w, err)
		return
	}

	var eventList audit.EventList
	err = json.Unmarshal(b, &amp;eventList)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	for _, event := range eventList.Items {

		// here is your logic

		fmt.Printf(&quot;审计ID %s: 用户&lt;%s&gt;, 请求对象&lt;%s&gt;, 操作&lt;%s&gt;, 请求阶段&lt;%s&gt;\n&quot;,
			event.AuditID,
			event.User.UID,
			event.RequestURI,
			event.Verb,
			event.Stage,
		)
	}
	w.WriteHeader(http.StatusOK)
}
</code></pre>
<p>当使用命令执行查看Pod的操作时，会看到webhook收到的下述审计日志</p>
<p>操作命令</p>
<pre><code class="language-bash">for n in `seq 1 100`; do kubectl get pod --user=admin; done
</code></pre>
<p>审计日志</p>
<pre><code class="language-log">审计ID c0313416-f950-4361-9823-7c4792b143fd: 用户&lt;admin&gt;, 请求对象&lt;/api/v1/namespaces/default/pods?limit=500&gt;, 操作&lt;list
&gt;, 请求阶段&lt;ResponseComplete&gt;
审计ID db2390c1-83cf-42e7-b589-70cd04003d0e: 用户&lt;admin&gt;, 请求对象&lt;/api/v1/namespaces/default/pods?limit=500&gt;, 操作&lt;list
&gt;, 请求阶段&lt;ResponseComplete&gt;
审计ID a8fc2ff9-d0c5-4263-901c-b5974fd58026: 用户&lt;admin&gt;, 请求对象&lt;/api/v1/namespaces/default/pods?limit=500&gt;, 操作&lt;list
&gt;, 请求阶段&lt;ResponseComplete&gt;
</code></pre>
<h2 id="总结">总结</h2>
<p>kubernetes通过插件的方式提供了提供了一个IT系统 4A模型为集群提供了安全保障，与传统的4A (<em><strong>Authentication</strong></em>, <em><strong>Authorization</strong></em>, <em><strong>Accounting</strong></em>, <em><strong>Auditing</strong></em>) 不同的是，对于 <em>Accounting</em> 与 <em>Authentication</em> 在kubernetes中设计来说 Kubernetes没有用户的实现而是一个抽象，这使得Kubernetes可以更灵活使用任意的用户系统完成登录（OID, X.509, webhook, proxy, SA&hellip;.），而对于授权来说，Kubernetes 通过多种授权模型(RBAC, ABAC, Node, Webhook)，为集群提供了灵活的权限；而不同的是，通过 <em><strong>Admission Control</strong></em> 可以为集群提供更多的安全策略，例如镜像策略，通过三方提供的控制器来自定义更多的安全策略，如OPA。而这种设计为Kubernetes集群提供了一种更灵活的安全。</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Auditing</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-policy" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Audit policy</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#audit-backends" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Audit backends</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#log-backend" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Log backend</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#webhook-backend" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Webhook backend</strong></em></a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#batching" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Event batching</strong></em></a></p>
<p><sup id="7">[7]</sup> <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/#parameter-tuning" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Parameter tuning</strong></em></a></p>
<p><sup id="8">[8]</sup> <a href="https://ldapwiki.com/wiki/Privilege%20Management%20Infrastructure" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Privilege Management Infrastructure</strong></em></a></p>
<p><sup id="9">[9]</sup> <a href="https://www.cnblogs.com/zhangmingcheng/p/16539514.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Kubernetes 审计（Auditing）功能详解</strong></em></a></p>
<p><sup id="10">[10]</sup> <a href="https://blog.tianfeiyu.com/2019/01/30/k8s-audit-webhook/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>kubernetes 审计日志功能</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes 4A - Authorization源码解析</title>
      <link>https://www.oomkill.com/2022/11/ch32-authorization/</link>
      <pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/11/ch32-authorization/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>在 Kubernetes 中，当一个访问请求通过了登录阶段（<em><strong>Authentication</strong></em>），必须还需要请求拥有该对象的访问权限，而授权部分也是Kubernetes API 访问控制中的第二个部分 <em><strong>Authorization</strong></em> .</p>
<p><em><strong>Authorization</strong></em> 在 Kubernetes中是以评估发起请求的用户，根据其身份特性评估这次请求是被 ”拒绝“ 还是 “允许”，同访问控制三部曲中其他两个插件 (<em><strong>Authentication</strong></em>, <em><strong>Adminssion Control</strong></em>) 一样，<em><strong>Authorization</strong></em> 也可以同时配置多个，当收到用户的请求时，会依次检查这个阶段配置的所有模块，如果任何一个模块对该请求授予权限（拒绝或允许），那么该阶段会直接返回，当所有模块都没有该用户所属的权限时，默认是拒绝，在Kubernetes中，被该插件拒绝的用户显示为HTTP 403。</p>
<p>如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大</p>
<p><strong>objective</strong>：</p>
<ul>
<li>了解kubernetes Authorization机制</li>
<li>了解授权系统的设计</li>
<li>完成实验，使用 OPA 作为 Kubernetes 外部用户，权限认证模型 <em>RBAC</em> 的替代品</li>
</ul>
<h2 id="kubernetes是如何对用户授权的">Kubernetes是如何对用户授权的</h2>
<p>kubernetes对用户授权需要遵守的shema必须拥有下列属性，代码位于<a href="https://github.com/kubernetes/kubernetes/blob/57eb5d631ccd615cd161b6da36afc759af004b93/pkg/apis/authorization/types.go#L27-L36" target="_blank"
   rel="noopener nofollow noreferrer" >pkg\apis\authorization\types.go</a></p>
<pre><code class="language-go">type SubjectAccessReview struct {
    // API必须实现的部分
	metav1.TypeMeta
	metav1.ObjectMeta
	// 请求需要遵守的属性
	Spec SubjectAccessReviewSpec
	// 请求被授权的状态
	Status SubjectAccessReviewStatus
}
</code></pre>
<p>这里可以看到数据模型是</p>
<pre><code class="language-go">type SubjectAccessReviewSpec struct {
	// ResourceAttributes describes information for a resource access request
	ResourceAttributes *ResourceAttributes
	// NonResourceAttributes describes information for a non-resource access request
	NonResourceAttributes *NonResourceAttributes

	// 请求的用户，必填
    // 如果只传递 User，而没有Group，那么权限必须与用户对应，例如rolebinding/clusterrolebing
    // 如果传递了User与Group，那么rolebinding/clusterrolebing权限最大为Group，最小为User
	User string
	// Groups是用户所属组，可以有多个
	Groups []string
	// Extra corresponds to the user.Info.GetExtra() method from the authenticator.  Since that is input to the authorizer
	// 这里通常对于验证和授权阶段，没有特别的需求
	Extra map[string]ExtraValue
	// UID 请求用户的UID，通常来说与User相同，Authentication中也是这么做的
	UID string
}
</code></pre>
<p>由此可得知，在授权部分，kubernetes要求请求必须存在</p>
<ul>
<li><strong>用户类属性</strong>：<strong>user</strong>，<strong>group</strong> ，<strong>extra</strong>  由 <em><strong>Authentication</strong></em> 提供的用户信息</li>
<li><strong>请求类属性</strong>：
<ul>
<li><strong>API资源</strong>： <code>curl $API_SERVER_URL/api/v1/namespaces</code></li>
<li><strong>请求路径</strong>： 非API资源格式的路径，<code>/api</code>，<code>/healthz</code></li>
<li><strong>verb</strong>：HTTP请求方法，GET，POST..</li>
</ul>
</li>
<li>资源类属性：
<ul>
<li>访问的资源的名称或ID，如Pod名</li>
<li>要访问的名称空间</li>
<li>资源所属组，Kubernetes资源有GVR组成</li>
</ul>
</li>
</ul>
<p>那么，<code>SubjectAccessReview.Spec</code> 为要审查的对象，<code>SubjectAccessReview.Status</code> 为审查结果，通常在每个请求到来时，入库前必定被审查</p>
<h2 id="kubernetes中的授权模式">Kubernetes中的授权模式</h2>
<p>知道授权的对象，就需要知道如何对该对象进行授权，Kubernetes authorizer 提供了下列授权模式</p>
<p><a href="pkg/kubeapiserver/authorizer/modes/modes.go">pkg/kubeapiserver/authorizer/modes/modes.go</a></p>
<pre><code class="language-go">const (
	// ModeAlwaysAllow is the mode to set all requests as authorized
	ModeAlwaysAllow string = &quot;AlwaysAllow&quot;
	// ModeAlwaysDeny is the mode to set no requests as authorized
	ModeAlwaysDeny string = &quot;AlwaysDeny&quot;
	// ModeABAC is the mode to use Attribute Based Access Control to authorize
	ModeABAC string = &quot;ABAC&quot;
	// ModeWebhook is the mode to make an external webhook call to authorize
	ModeWebhook string = &quot;Webhook&quot;
	// ModeRBAC is the mode to use Role Based Access Control to authorize
	ModeRBAC string = &quot;RBAC&quot;
	// ModeNode is an authorization mode that authorizes API requests made by kubelets.
	ModeNode string = &quot;Node&quot;
)
</code></pre>
<p>可以看出，大致遵循模式进行授权</p>
<ul>
<li>ModeABAC (<em><strong>Attribute-based access control</strong></em>)：是一种将属性分组，而后属性组分配给用户的模型，通常情况下这种模型很少使用</li>
<li>ModeRBAC (<em><strong>Role Based Access Control</strong></em>) ：是kubernetes主流的授权模型，是将用户分组，将属性分配给用户组的一种模型</li>
<li>ModeNode：对kubelet授权的方式</li>
<li>ModeWebhook：用户注入给Kubernetes 授权插件进行回调的一种授权模式</li>
</ul>
<h2 id="kubernetes-授权生命周期">Kubernetes 授权生命周期</h2>
<p>在启动 <code>kube-apiserver</code> 是都会初始化被注入一个 <code>Authorizer</code> 而这个被上面模式进行实现，如 <code>RBACAuthorizer</code> ,  <code>WebhookAuthorizer</code> <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/staging/src/k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go</a></p>
<pre><code class="language-go">type Authorizer interface {
	Authorize(ctx context.Context, a Attributes) (authorized Decision, reason string, err error)
}
</code></pre>
<p>在 Run 中会创建一个CreateServerChain，这里面可以看到对应注册进来的  <code>Authorizer </code>  <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/cmd/kube-apiserver/app/server.go#L155-L173" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io\kubernetes\cmd\kube-apiserver\app\server.go</a></p>
<pre><code class="language-go">// Run runs the specified APIServer.  This should never exit.
func Run(completeOptions completedServerRunOptions, stopCh &lt;-chan struct{}) error {
	// To help debugging, immediately log version
	klog.Infof(&quot;Version: %+v&quot;, version.Get())

	klog.InfoS(&quot;Golang settings&quot;, &quot;GOGC&quot;, os.Getenv(&quot;GOGC&quot;), &quot;GOMAXPROCS&quot;, os.Getenv(&quot;GOMAXPROCS&quot;), &quot;GOTRACEBACK&quot;, os.Getenv(&quot;GOTRACEBACK&quot;))

	server, err := CreateServerChain(completeOptions)
	if err != nil {
		return err
	}

	prepared, err := server.PrepareRun()
	if err != nil {
		return err
	}

	return prepared.Run(stopCh)
}
</code></pre>
<p>可以看到在创建这个 <code>Authorizer </code>  时会调用一个 <code>BuildAuthorizer</code> 构建这个 <code>Authorizer </code></p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/cmd/kube-apiserver/app/server.go#L448" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/kubernetes/cmd/kube-apiserver/app/server.go</a></p>
<pre><code class="language-go">func buildGenericConfig(
	s *options.ServerRunOptions,
	proxyTransport *http.Transport,
) (
	genericConfig *genericapiserver.Config,
	versionedInformers clientgoinformers.SharedInformerFactory,
	serviceResolver aggregatorapiserver.ServiceResolver,
	pluginInitializers []admission.PluginInitializer,
	admissionPostStartHook genericapiserver.PostStartHookFunc,
	storageFactory *serverstorage.DefaultStorageFactory,
	lastErr error,
) {
    
	...

	genericConfig.Authorization.Authorizer, genericConfig.RuleResolver, err = BuildAuthorizer(s, genericConfig.EgressSelector, versionedInformers)
	if err != nil {
		lastErr = fmt.Errorf(&quot;invalid authorization config: %v&quot;, err)
		return
	}
	...
}
</code></pre>
<p>在代码 <code>BuildAuthorizer</code> 中构建了这个 <code>Authorizer</code> 其中可以看到 s 为 <code>kube-apiserver</code> 对于授权阶段的参数，例如参数，使用哪些模式 <code>--authorization-mode</code>，使用的webhook的配置 <code>--authentication-token-webhook-config-file</code> 等，通过传入的参数来决定这些</p>
<pre><code class="language-go">// BuildAuthorizer constructs the authorizer
func BuildAuthorizer(s *options.ServerRunOptions, EgressSelector *egressselector.EgressSelector, versionedInformers clientgoinformers.SharedInformerFactory) (authorizer.Authorizer, authorizer.RuleResolver, error) {
   // 这里构建出  authorizer.Config
	authorizationConfig := s.Authorization.ToAuthorizationConfig(versionedInformers)

	if EgressSelector != nil {
		egressDialer, err := EgressSelector.Lookup(egressselector.ControlPlane.AsNetworkContext())
		if err != nil {
			return nil, nil, err
		}
		authorizationConfig.CustomDial = egressDialer
	}
	
    // 然后返回你开启的每一个webhook的模式的 authorizer
	return authorizationConfig.New()
}
</code></pre>
<p>而对应这部分的数据结构如下所示  <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/pkg/kubeapiserver/options/authorization.go#L34-L46" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/pkg/kubeapiserver/options/authorization.go</a></p>
<pre><code class="language-go">// BuiltInAuthorizationOptions contains all build-in authorization options for API Server
type BuiltInAuthorizationOptions struct {
	Modes                       []string
	PolicyFile                  string
	WebhookConfigFile           string
	WebhookVersion              string
	WebhookCacheAuthorizedTTL   time.Duration
	WebhookCacheUnauthorizedTTL time.Duration
	// WebhookRetryBackoff specifies the backoff parameters for the authorization webhook retry logic.
	// This allows us to configure the sleep time at each iteration and the maximum number of retries allowed
	// before we fail the webhook call in order to limit the fan out that ensues when the system is degraded.
	WebhookRetryBackoff *wait.Backoff
}
</code></pre>
<p>例如在客户端部分，如果需要授权，都会使用该操作，可以在代码 <a href="pkg/registry/authorization/subjectaccessreview/rest.go">k8s.io/pkg/registry/authorization/subjectaccessreview/rest.go</a>  中可以看到REST中会 authorizer.Authorize 去验证是否有权限操作</p>
<pre><code class="language-go">func (r *REST) Create(ctx context.Context, obj runtime.Object, createValidation rest.ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) {
	subjectAccessReview, ok := obj.(*authorizationapi.SubjectAccessReview)
	if !ok {
		return nil, apierrors.NewBadRequest(fmt.Sprintf(&quot;not a SubjectAccessReview: %#v&quot;, obj))
	}
	if errs := authorizationvalidation.ValidateSubjectAccessReview(subjectAccessReview); len(errs) &gt; 0 {
		return nil, apierrors.NewInvalid(authorizationapi.Kind(subjectAccessReview.Kind), &quot;&quot;, errs)
	}

	if createValidation != nil {
		if err := createValidation(ctx, obj.DeepCopyObject()); err != nil {
			return nil, err
		}
	}

	authorizationAttributes := authorizationutil.AuthorizationAttributesFrom(subjectAccessReview.Spec)
	decision, reason, evaluationErr := r.authorizer.Authorize(ctx, authorizationAttributes)

	subjectAccessReview.Status = authorizationapi.SubjectAccessReviewStatus{
		Allowed: (decision == authorizer.DecisionAllow),
		Denied:  (decision == authorizer.DecisionDeny),
		Reason:  reason,
	}
	if evaluationErr != nil {
		subjectAccessReview.Status.EvaluationError = evaluationErr.Error()
	}

	return subjectAccessReview, nil
}
</code></pre>
<p>authorizer.Authorize 会被实现在每一个该阶段的模式下，在 withAuthentication 构建了一个授权的 http.Handler 函数</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go#L45-L79" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/endpoints/filters/authorization.go</a></p>
<pre><code class="language-go">func WithAuthorization(handler http.Handler, a authorizer.Authorizer, s runtime.NegotiatedSerializer) http.Handler {
	if a == nil {
		klog.Warning(&quot;Authorization is disabled&quot;)
		return handler
	}
	return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) {
		ctx := req.Context()

		attributes, err := GetAuthorizerAttributes(ctx)
		if err != nil {
			responsewriters.InternalError(w, req, err)
			return
		}
        // 这里调用了authorizer.Authorizer传入的authorizer来进行鉴权
		authorized, reason, err := a.Authorize(ctx, attributes)
		// an authorizer like RBAC could encounter evaluation errors and still allow the request, so authorizer decision is checked before error here.
		if authorized == authorizer.DecisionAllow {
			audit.AddAuditAnnotations(ctx,
				decisionAnnotationKey, decisionAllow,
				reasonAnnotationKey, reason)
			handler.ServeHTTP(w, req)
			return
		}
		if err != nil {
			audit.AddAuditAnnotation(ctx, reasonAnnotationKey, reasonError)
			responsewriters.InternalError(w, req, err)
			return
		}

		klog.V(4).InfoS(&quot;Forbidden&quot;, &quot;URI&quot;, req.RequestURI, &quot;Reason&quot;, reason)
		audit.AddAuditAnnotations(ctx,
			decisionAnnotationKey, decisionForbid,
			reasonAnnotationKey, reason)
		responsewriters.Forbidden(ctx, attributes, w, req, reason, s)
	})
}
</code></pre>
<p>接下来在 createAggregatorConfig 调用了 BuildHandlerChainWithStorageVersionPrecondition 而又调用了</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/cmd/kube-apiserver/app/aggregator.go#L56-L78" target="_blank"
   rel="noopener nofollow noreferrer" >cmd/kube-apiserver/app/aggregator.go</a></p>
<pre><code class="language-go">func createAggregatorConfig(
	kubeAPIServerConfig genericapiserver.Config,
	commandOptions *options.ServerRunOptions,
	externalInformers kubeexternalinformers.SharedInformerFactory,
	serviceResolver aggregatorapiserver.ServiceResolver,
	proxyTransport *http.Transport,
	pluginInitializers []admission.PluginInitializer,
) (*aggregatorapiserver.Config, error) {
	// make a shallow copy to let us twiddle a few things
	// most of the config actually remains the same.  We only need to mess with a couple items related to the particulars of the aggregator
	genericConfig := kubeAPIServerConfig
	genericConfig.PostStartHooks = map[string]genericapiserver.PostStartHookConfigEntry{}
	genericConfig.RESTOptionsGetter = nil
	// prevent generic API server from installing the OpenAPI handler. Aggregator server
	// has its own customized OpenAPI handler.
	genericConfig.SkipOpenAPIInstallation = true

	if utilfeature.DefaultFeatureGate.Enabled(genericfeatures.StorageVersionAPI) &amp;&amp;
		utilfeature.DefaultFeatureGate.Enabled(genericfeatures.APIServerIdentity) {
		// Add StorageVersionPrecondition handler to aggregator-apiserver.
		// The handler will block write requests to built-in resources until the
		// target resources' storage versions are up-to-date.
		genericConfig.BuildHandlerChainFunc = genericapiserver.BuildHandlerChainWithStorageVersionPrecondition
	}
</code></pre>
<p>而 <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/pkg/server/config.go#L601-L655" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/config.go</a> 返回这个函数 BuildHandlerChainWithStorageVersionPrecondition</p>
<pre><code class="language-go">handlerChainBuilder := func(handler http.Handler) http.Handler {
    return c.BuildHandlerChainFunc(handler, c.Config)
}

apiServerHandler := NewAPIServerHandler(name, c.Serializer, handlerChainBuilder, delegationTarget.UnprotectedHandler())

s := &amp;GenericAPIServer{
		discoveryAddresses:         c.DiscoveryAddresses,
		LoopbackClientConfig:       c.LoopbackClientConfig,
		legacyAPIGroupPrefixes:     c.LegacyAPIGroupPrefixes,
		admissionControl:           c.AdmissionControl,
		Serializer:                 c.Serializer,
		AuditBackend:               c.AuditBackend,
		Authorizer:                 c.Authorization.Authorizer,
		delegationTarget:           delegationTarget,
		EquivalentResourceRegistry: c.EquivalentResourceRegistry,
		HandlerChainWaitGroup:      c.HandlerChainWaitGroup,
		Handler:                    apiServerHandler,

		listedPathProvider: apiServerHandler,
</code></pre>
<p>只要知道哪里调用了 handlerChainBuilder 就知道了鉴权步骤在哪里了，可以看到 handlerChainBuilder 被传入了 apiServerHandler，而后被作为参数返回给 <code>listedPathProvider: &amp;GenericAPIServer{}</code></p>
<p>listedPathProvider在 <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go#L282-L284" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/genericapiserver.go</a></p>
<pre><code class="language-go">func (s *GenericAPIServer) ListedPaths() []string {
	return s.listedPathProvider.ListedPaths()
}
</code></pre>
<p>ListedPaths() 又在代码  <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/pkg/server/routes/index.go#L38-L47" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/routes/index.go</a> 中被 构建成这个http服务</p>
<pre><code class="language-go">// ListedPaths returns the paths that should be shown under /
func (a *APIServerHandler) ListedPaths() []string {
	var handledPaths []string
	// Extract the paths handled using restful.WebService
	for _, ws := range a.GoRestfulContainer.RegisteredWebServices() {
		handledPaths = append(handledPaths, ws.RootPath())
	}
	handledPaths = append(handledPaths, a.NonGoRestfulMux.ListedPaths()...)
	sort.Strings(handledPaths)

	return handledPaths
}


// ServeHTTP serves the available paths.
func (i IndexLister) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	responsewriters.WriteRawJSON(i.StatusCode, metav1.RootPaths{Paths: i.PathProvider.ListedPaths()}, w)
}
</code></pre>
<p>至此，可以知道，每次请求时，我们在配置 <em>kube-apiserver</em> 配置的授权插件 <code>.authorizer.Authorize</code> ，而这个参数会被带至 <code>subjectAccessReview</code> 向下传递，其中 User,Group,Extra,UID 为 authentication 部分提供</p>
<h2 id="authorization-webhook">Authorization webhook</h2>
<p>Authorization webhook 位于 <a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go#L166-L247" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go</a>，是通过 <em>kube-apiserver</em> 注入进来的配置，就是上面讲到的如果提供了配置就会加入这种类型的 Authorization 插件来认证。当配置此类型的授权插件，Authorize 会被调用，通过向注入的 URL 发起 REST 请求进行授权，请求对象是 <code>v1beta1.SubjectAccessReview</code></p>
<p>下面是请求的实例</p>
<pre><code>{
  &quot;apiVersion&quot;: &quot;authorization.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;SubjectAccessReview&quot;,
  &quot;spec&quot;: {
	&quot;resourceAttributes&quot;: {
	  &quot;namespace&quot;: &quot;kittensandponies&quot;,
	  &quot;verb&quot;: &quot;GET&quot;,
	  &quot;group&quot;: &quot;group3&quot;,
	  &quot;resource&quot;: &quot;pods&quot;
	},
	&quot;user&quot;: &quot;jane&quot;,
	&quot;group&quot;: [
	  &quot;group1&quot;,
	  &quot;group2&quot;
	]
  }
}
</code></pre>
<p>webhook 返回的格式</p>
<pre><code class="language-json">// 如果允许这个用户访问则返回这个格式
{
  &quot;apiVersion&quot;: &quot;authorization.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;SubjectAccessReview&quot;,
  &quot;status&quot;: {
	&quot;allowed&quot;: true
  }
}

// 如果拒绝这个用户访问则返回这个格式
{
  &quot;apiVersion&quot;: &quot;authorization.k8s.io/v1beta1&quot;,
  &quot;kind&quot;: &quot;SubjectAccessReview&quot;,
  &quot;status&quot;: {
	&quot;allowed&quot;: false,
	&quot;reason&quot;: &quot;user does not have read access to the namespace&quot;
  }
}
</code></pre>
<p>对于webhook来讲，只要接受请求保持上面格式，而返回格式为下属格式，就可以很好的将Kubernetes 权限体系接入到三方系统中，例如 <em><strong>open policy agent</strong></em>。</p>
<p>同样 webhook 也提供了  <code>Authorize</code> 函数，如同上面一样会被注入到每个handler中被执行</p>
<pre><code class="language-go">func (w *WebhookAuthorizer) Authorize(ctx context.Context, attr authorizer.Attributes) (decision authorizer.Decision, reason string, err error) {
	r := &amp;authorizationv1.SubjectAccessReview{}
	if user := attr.GetUser(); user != nil {
		r.Spec = authorizationv1.SubjectAccessReviewSpec{
			User:   user.GetName(),
			UID:    user.GetUID(),
			Groups: user.GetGroups(),
			Extra:  convertToSARExtra(user.GetExtra()),
		}
	}

	if attr.IsResourceRequest() {
		r.Spec.ResourceAttributes = &amp;authorizationv1.ResourceAttributes{
			Namespace:   attr.GetNamespace(),
			Verb:        attr.GetVerb(),
			Group:       attr.GetAPIGroup(),
			Version:     attr.GetAPIVersion(),
			Resource:    attr.GetResource(),
			Subresource: attr.GetSubresource(),
			Name:        attr.GetName(),
		}
	} else {
		r.Spec.NonResourceAttributes = &amp;authorizationv1.NonResourceAttributes{
			Path: attr.GetPath(),
			Verb: attr.GetVerb(),
		}
	}
	key, err := json.Marshal(r.Spec)
	if err != nil {
		return w.decisionOnError, &quot;&quot;, err
	}
	if entry, ok := w.responseCache.Get(string(key)); ok {
		r.Status = entry.(authorizationv1.SubjectAccessReviewStatus)
	} else {
		var result *authorizationv1.SubjectAccessReview
		// WithExponentialBackoff will return SAR create error (sarErr) if any.
		if err := webhook.WithExponentialBackoff(ctx, w.retryBackoff, func() error {
			var sarErr error
			var statusCode int

			start := time.Now()
			result, statusCode, sarErr = w.subjectAccessReview.Create(ctx, r, metav1.CreateOptions{})
			latency := time.Since(start)

			if statusCode != 0 {
				w.metrics.RecordRequestTotal(ctx, strconv.Itoa(statusCode))
				w.metrics.RecordRequestLatency(ctx, strconv.Itoa(statusCode), latency.Seconds())
				return sarErr
			}

			if sarErr != nil {
				w.metrics.RecordRequestTotal(ctx, &quot;&lt;error&gt;&quot;)
				w.metrics.RecordRequestLatency(ctx, &quot;&lt;error&gt;&quot;, latency.Seconds())
			}

			return sarErr
		}, webhook.DefaultShouldRetry); err != nil {
			klog.Errorf(&quot;Failed to make webhook authorizer request: %v&quot;, err)
			return w.decisionOnError, &quot;&quot;, err
		}

		r.Status = result.Status
		if shouldCache(attr) {
			if r.Status.Allowed {
				w.responseCache.Add(string(key), r.Status, w.authorizedTTL)
			} else {
				w.responseCache.Add(string(key), r.Status, w.unauthorizedTTL)
			}
		}
	}
	switch {
	case r.Status.Denied &amp;&amp; r.Status.Allowed:
		return authorizer.DecisionDeny, r.Status.Reason, fmt.Errorf(&quot;webhook subject access review returned both allow and deny response&quot;)
	case r.Status.Denied:
		return authorizer.DecisionDeny, r.Status.Reason, nil
	case r.Status.Allowed:
		return authorizer.DecisionAllow, r.Status.Reason, nil
	default:
		return authorizer.DecisionNoOpinion, r.Status.Reason, nil
	}

}
</code></pre>
<p>执行 <code>webhook.Authorize()</code> 会执行 <code>w.subjectAccessReview.Create()</code> 在这里可以看到会发起一个POST请求将 <code>v1beta1.SubjectAccessReview</code> 传入给webhook</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/d818028a1851891cdf934a543bb7ff959ec23d50/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go#L317-L329" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go.Create</a></p>
<pre><code class="language-go">func (t *subjectAccessReviewV1Client) Create(ctx context.Context, subjectAccessReview *authorizationv1.SubjectAccessReview, opts metav1.CreateOptions) (result *authorizationv1.SubjectAccessReview, statusCode int, err error) {
	result = &amp;authorizationv1.SubjectAccessReview{}

	restResult := t.client.Post().
		Resource(&quot;subjectaccessreviews&quot;).
		VersionedParams(&amp;opts, scheme.ParameterCodec).
		Body(subjectAccessReview).
		Do(ctx)

	restResult.StatusCode(&amp;statusCode)
	err = restResult.Into(result)
	return
}
</code></pre>
<h2 id="实验基于opa的rbac模型">实验：基于OPA的RBAC模型</h2>
<p>通过上面阐述，大致了解到kubernetes认证框架中的用户的分类以及认证的策略由哪些，实验的目的也是为了阐述一个结果，就是使用OIDC/webhook 是比其他方式更好的保护，管理kubernetes集群。首先在安全上，假设网络环境是不安全的，那么任意node节点遗漏 bootstrap  token文件，就意味着拥有了集群中最高权限；其次在管理上，越大的团队，人数越多，不可能每个用户都提供单独的证书或者token，要知道传统教程中讲到token在kubernetes集群中是永久有效的，除非你删除了这个secret/sa；而Kubernetes提供的插件就很好的解决了这些问题。</p>
<h3 id="实验环境">实验环境</h3>
<ul>
<li>一个kubernetes集群</li>
<li>了解OPA相关技术</li>
</ul>
<p><strong>实验大致分为以下几个步骤</strong>：</p>
<ul>
<li>建立一个HTTP服务器用于返回给kubernetes Authorization服务</li>
<li>查询用户操作是否有权限</li>
</ul>
<h3 id="实验开始">实验开始</h3>
<h4 id="编写webhook-authorization">编写webhook Authorization</h4>
<p>这里做的就是接收 <code>subjectAccessReview</code> ，将授权结果赋予 <code>subjectAccessReview.Status.Allowed</code> ，true/false，然后返回 <code>subjectAccessReview</code>  即可</p>
<pre><code class="language-go">func serveAuthorization(w http.ResponseWriter, r *http.Request) {
	b, err := ioutil.ReadAll(r.Body)
	if err != nil {
		httpError(w, err)
		return
	}
	klog.V(4).Info(&quot;Receied: &quot;, string(b))

	var subjectAccessReview authoV1.SubjectAccessReview
	err = json.Unmarshal(b, &amp;subjectAccessReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	subjectAccessReview.Status.Allowed = rbac.RBACChek(&amp;subjectAccessReview)
	b, err = json.Marshal(subjectAccessReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	w.Write(b)
	klog.V(3).Info(&quot;Returning: &quot;, string(b))
}
</code></pre>
<h4 id="编写rego">编写rego</h4>
<p>这里简单配置了两个权限，<em>admin</em> 组拥有所有操作权限，不包含 <code>watch</code> ，而 <em>conf</em> 组只能 <em>list</em>，在访问控制三部曲中，已授权的会增加一个组，例如 <code>system:authenticated</code> 代表被 <em>Authentication</em> 授予通过的用户，所以 Groups 为一个数组格式，这里检查为两个数组的交集 &gt; 1，则肯定代表这个用户拥有该组的权限。</p>
<p>实验中 <em>rego</em> 部分可以在 <a href="https://play.openpolicyagent.org/p/JFdryx8eqW" target="_blank"
   rel="noopener nofollow noreferrer" >playground</a> 中测试</p>
<pre><code class="language-go">var module = `package k8s
import future.keywords.in

default allow = false
admin_verbs := {&quot;create&quot;, &quot;list&quot;, &quot;delete&quot;, &quot;update&quot;}
admin_groups := {&quot;admin&quot;}
conf_groups := {&quot;conf&quot;}
conf_verbs := {&quot;list&quot;}
allow  {
	groups := {v | v := input.spec.groups[_]}
	count(admin_groups &amp; groups) &gt; 0
	input.spec.resourceAttributes.verb in admin_verbs
}

allow  {
	groups := {v | v := input.spec.groups[_]}
	count(conf_groups &amp; groups) &gt; 0
	input.spec.resourceAttributes.verb in conf_verbs
}
`
</code></pre>
<p>下面编写 RBACChek 函数，由于go1.16提供了embed功能，就可以直接将 rego embed go中，最后<code>result.Allowed()</code>  如果 <em>input</em> 通过评估则为 <code>true</code> ，反之亦然</p>
<pre><code class="language-go">func RBACChek(req *authoV1.SubjectAccessReview) bool {
	fmt.Printf(&quot;\n%+v\n&quot;, req)
	query, err := rego.New(
        // query是要检查的模块，data是固定格式，这与playground中不一样，需要.allow
        // k8s是package
		rego.Query(&quot;data.k8s.allow&quot;),
		rego.Module(&quot;k8s.allow&quot;, module),
	).PrepareForEval(context.TODO())

	if err != nil {
		klog.V(4).Info(err)
		return false
	}
	result, err := query.Eval(context.TODO(), rego.EvalInput(req))

	if err != nil {
		klog.V(4).Info(&quot;evaluation error:&quot;, err)
		return false
	} else if len(result) == 0 {
		klog.V(4).Info(&quot;undefined result&quot;, err)
		return false
	}
	return result.Allowed()
}
</code></pre>
<h4 id="配置kube-apiserver">配置kube-apiserver</h4>
<p>Authorization webhook 与其他 webhook 一样，启用的方法也是修改 <em>kube-apiserver</em> 参数，并指定 <code>kubeconfig</code> 类型的配置文件，其中对于 Kubernetes 集群来说 <code>kubeconfig</code> 是 kubernetes 客户端访问的信息，而 webhook 这里的 <code>kubeconfig</code> 配置文件要填写的则是 webhook的信息，其中 user,cluster,contexts 属性均为 webhook的配置信息 <sup><a href="#1">[1]</a></sup>。</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Config
clusters:
- cluster:
    server: http://10.0.0.1:88/authorization
    insecure-skip-tls-verify: true
  name: authorizator
users:
- name: webhook-authorizator
current-context: webhook-authorizator@authorizator
contexts:
- context:
    cluster: authorizator
    user: webhook-authorizator
  name: webhook-authorizator@authorizator
</code></pre>
<p>修改 <em>kube-apiserver</em> 参数</p>
<pre><code class="language-yaml">--authorization-webhook-config-file=/etc/kubernetes/auth/authorization-webhook.conf \
# 1s 是为了在测试时减少等待的时间，否则缓存太长不会走webhook
--authorization-webhook-cache-authorized-ttl=1s \
--authorization-webhook-cache-unauthorized-ttl=1s \
# api版本建议还是指定下，因为v1与v1beta1的 subjectAccessReview 内容不同rego因为格式问题会为空从而false
# 代码中schema v1与v1beta1相同，测试时收到的请求的格式不一样，没找到原因 TODO
--authorization-webhook-version=v1 \
</code></pre>
<h3 id="验证结果">验证结果</h3>
<p>准备三个外部用户，admin,admin1,searchUser，admin,admin1 为 admin 组，拥有所有权限，searchUser 为 conf 组，仅能 list 操作</p>
<pre><code class="language-yaml">- name: admin
  user: 
    token: admin@111
- name: admin1
  user:
    token: admin1@111
- name: searchUser
  user:
    token: searchUser@111
</code></pre>
<p>测试用户 searchUser ，可以看到只能list操作</p>
<pre><code class="language-bash">$ kubectl get pod --user=searchUser
NAME                      READY   STATUS             RESTARTS   AGE
netbox-85865d5556-hfg6v   1/1     Running            0          96d
netbox-85865d5556-vlgr4   1/1     Running            0          96d
pod                       0/1     CrashLoopBackOff   95         22h

$ kubectl delete pod pod --user=searchUser
Error from server (Forbidden): pods &quot;pod&quot; is forbidden: User &quot;searchUser&quot; cannot delete resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>
<p>测试用户 admin，可以看出可以进行写与查看的操作</p>
<pre><code class="language-bash">$ kubectl delete pod pod --user=admin
pod &quot;pod&quot; deleted

$ kubectl get pod --user=admin
NAME                      READY   STATUS    RESTARTS   AGE
netbox-85865d5556-hfg6v   1/1     Running   0          96d
netbox-85865d5556-vlgr4   1/1     Running   0          96d
</code></pre>
<h2 id="总结">总结</h2>
<p>kubernetes 提供了 Authentication,Authorization,Adminsion Control,Audit 几种webhook，可以自行在Kubernetes之上实现一个4A的标准，Authorization部分提供了一个并行与，但脱离Kubernetes的授权系统，使得外部用户可以很灵活的被授权，而不是手动管理多个clusterrolebinding,rolebingding 之类的资源。</p>
<p>实验中使用了OPA，这里是将rego静态文件embed入go中，在正常情况下OPA给出的架构如下图所示，存在一个 <em><strong>OPA Service</strong></em>，来进行验证，而实验中是直接嵌入到go中，OPA本身也提供了 <em><strong>HTTP Service</strong></em>，可以直接编译运行为 HTTP服务  <sup><a href="#2">[2]</a></sup>。 TODO</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/image-20221124224618920.png" alt="image-20221124224618920" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：OPA 架构</center>
<center><em>Source：</em>https://www.openpolicyagent.org/docs/latest/</center><br>
<p>OPA本身提供了 Gatekeeper ，可以作为Kubernetes 资源使用，官方示例是作为为一个kubernetes准入网关，也提供了ingress浏览的验证 <sup><a href="#3">[3]</a></sup></p>
<blockquote>
<p>Notes：实验中还需要注意的一点则是，如果RBAC与webhook同时验证时，需要合理的规划权限，例如集群组件的账户，coreDNS，flannel等，也会被拒绝（在OPA设置的 <code>default allow = false</code> ）。</p>
</blockquote>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Webhook Mode</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://www.openpolicyagent.org/docs/latest/http-api-authorization/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>HTTP APIs</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://www.openpolicyagent.org/docs/latest/kubernetes-introduction/#what-is-opa-gatekeeper" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>What is OPA Gatekeeper?</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://blog.haohtml.com/archives/31514" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>用 Goalng 开发 OPA 策略</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://blog.wu-boy.com/2021/04/setup-rbac-role-based-access-control-using-open-policy-agent/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>初探 Open Policy Agent 實作 RBAC (Role-based access control) 權限控管</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes 4A - Authentication源码解析</title>
      <link>https://www.oomkill.com/2022/11/ch31-authentication/</link>
      <pubDate>Wed, 16 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/11/ch31-authentication/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<blockquote>
<p>本文是关于Kubernetes 4A解析的第一章</p>
<ul>
<li>深入理解Kubernetes 4A - Authentication源码解析</li>
<li><a href="https://cylonchau.github.io/kubernetes-authorization.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authorization源码解析</a></li>
<li><a href="https://cylonchau.github.io/ch3.7-admission-webhook.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Admission Control源码解析</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-auditing.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Audit源码解析</a></li>
</ul>
<p>所有关于Kubernetes 4A部分代码上传至仓库 github.com/cylonchau/hello-k8s-4A</p>
</blockquote>
<p>本章主要简单阐述kubernetes 认证相关原理，最后以实验来阐述kubernetes用户系统的思路</p>
<p><strong>objective</strong>：</p>
<ul>
<li>了解kubernetes 各种认证机制的原理</li>
<li>了解kubernetes 用户的概念</li>
<li>了解kubernetes authentication webhook</li>
<li>完成实验，如何将其他用户系统接入到kubernetes中的一个思路</li>
</ul>
<p>如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大。</p>
<h2 id="kubernetes-认证">Kubernetes 认证</h2>
<p>在Kubernetes apiserver对于认证部分所描述的，对于所有用户访问Kubernetes API（通过任何客户端，客户端库，<code>kubectl</code> 等）时都会经历 验证 (<em><strong>Authentication</strong></em>) , 授权 (<em><strong>Authorization</strong></em>), 和准入控制 (<em><strong>Admission control</strong></em>) 三个阶段来完成对 “用户” 进行授权，整个流程正如下图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/vhesGDFN3dLdzXwS7vzPdXkI3aglQYZgGhjc-Cx_boaV6URKFFoe8mFRZZUuJyGHywa_bOkeUlIkm-nJkCVMHPk9dr2dXFwNzAQJKzft2phsTcEDjdObjmugBcYtpdPLpLIYuIGzeFYvtsR2Lw.jpeg" alt="image-20221025003822017" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图</center>
<center><em>Source：</em>https://www.armosec.io/blog/kubernetes-admission-controller/</center><br>
<p>其中在大多数教程中，在对这三个阶段所做的工作大致上为：</p>
<ul>
<li>
<p><em><strong>Authentication</strong></em> 阶段所指用于确认请求访问Kubernetes API 用户是否为合法用户，拒绝为401</p>
</li>
<li>
<p><em><strong>Authorization</strong></em> 阶段所指的将是这个用户是否有对操作的资源的权限，拒绝为403</p>
</li>
<li>
<p><em><strong>Admission control</strong></em> 阶段所指控制对请求资源进行控制，通俗来说，就是一票否决权，即使前两个步骤完成</p>
</li>
</ul>
<p>到这里了解到了Kubernetes API实际上做的工作就是 “人类用户” 与 “kubernetes service account&quot; <sup><a href="#2">[2]</a></sup>；那么就引出了一个重要概念就是 “用户” 在Kubernetes中是什么，以及用户在认证中的也是本章节的中心。</p>
<p>在Kubernetes官方手册中给出了 ”用户“ 的概念，Kubernetes集群中存在的用户包括 ”普通用户“ 与 “service account” 但是 Kubernetes 没有普通用户的管理方式，只是将使用集群的证书CA签署的有效证书的用户都被视为合法用户 <sup><a href="#3">[3]</a></sup></p>
<p>那么对于使得Kubernetes集群有一个真正的用户系统，就可以根据上面给出的概念将Kubernetes用户分为 ”外部用户“ 与 ”内部用户“。如何理解外部与内部用户呢？实际上就是有Kubernetes管理的用户，即在kubernetes定义用户的数据模型这种为 “内部用户” ，正如 service account；反之，非Kubernetes托管的用户则为 ”外部用户“ 这中概念也更好的对kubernetes用户的阐述。</p>
<p>对于外部用户来说，实际上Kubernetes给出了多种用户概念 <sup><a href="#3">[3]</a></sup>，例如：</p>
<ul>
<li>拥有kubernetes集群证书的用户</li>
<li>拥有Kubernetes集群token的用户（<code>--token-auth-file</code> 指定的静态token）</li>
<li>用户来自外部用户系统，例如 <em>OpenID</em>，<em>LDAP</em>，<em>QQ connect</em>, <em>google identity platform</em> 等</li>
</ul>
<h2 id="向外部用户授权集群访问的示例">向外部用户授权集群访问的示例</h2>
<h3 id="场景1通过证书请求k8s">场景1：通过证书请求k8s</h3>
<p>该场景中kubernetes将使用证书中的cn作为用户，ou作为组，如果对应 <code>rolebinding/clusterrolebinding</code> 给予该用户权限，那么请求为合法</p>
<pre><code class="language-bash">$ curl https://hostname:6443/api/v1/pods \
	--cert ./client.pem \
	--key ./client-key.pem \
	--cacert ./ca.pem 
</code></pre>
<p>接下来浅析下在代码中做的事情</p>
<p>确认用户是 <em><strong>apiserver</strong></em> 在 <em><strong>Authentication</strong></em> 阶段 做的事情，而对应代码在 <a href="pkg/kubeapiserver/authenticator">pkg/kubeapiserver/authenticator</a> 下，整个文件就是构建了一系列的认证器，而x.509证书指是其中一个</p>
<pre><code class="language-go">// 创建一个认证器，返回请求或一个k8s认证机制的标准错误
func (config Config) New() (authenticator.Request, *spec.SecurityDefinitions, error) {
    
...

	// X509 methods
    // 可以看到这里就是将x509证书解析为user
	if config.ClientCAContentProvider != nil {
		certAuth := x509.NewDynamic(config.ClientCAContentProvider.VerifyOptions, x509.CommonNameUserConversion)
		authenticators = append(authenticators, certAuth)
	}
...
</code></pre>
<p>接下来看实现原理，NewDynamic函数位于代码 <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/staging/src/k8s.io/apiserver/pkg/authentication/request/x509/x509.go#L126-L130" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/authentication/request/x509/x509.go</a></p>
<p>通过代码可以看出，是通过一个验证函数与用户来解析为一个 <em>Authenticator</em></p>
<pre><code class="language-go">// NewDynamic returns a request.Authenticator that verifies client certificates using the provided
// VerifyOptionFunc (which may be dynamic), and converts valid certificate chains into user.Info using the provided UserConversion
func NewDynamic(verifyOptionsFn VerifyOptionFunc, user UserConversion) *Authenticator {
	return &amp;Authenticator{verifyOptionsFn, user}
}
</code></pre>
<p>验证函数为 CAContentProvider 的方法，而x509部分实现为 <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/staging/src/k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go#L253-L261" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go.VerifyOptions</a>；可以看出返回是一个 <code>x509.VerifyOptions</code> + 与认证的状态</p>
<pre><code class="language-go">// VerifyOptions provides verifyoptions compatible with authenticators
func (c *DynamicFileCAContent) VerifyOptions() (x509.VerifyOptions, bool) {
	uncastObj := c.caBundle.Load()
	if uncastObj == nil {
		return x509.VerifyOptions{}, false
	}

	return uncastObj.(*caBundleAndVerifier).verifyOptions, true
}
</code></pre>
<p>而用户的获取则位于  <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/staging/src/k8s.io/apiserver/pkg/authentication/request/x509/x509.go#L248-L258" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/authentication/request/x509/x509.go</a>；可以看出，用户正是拿的证书的CN，而组则是为证书的OU</p>
<pre><code class="language-go">// CommonNameUserConversion builds user info from a certificate chain using the subject's CommonName
var CommonNameUserConversion = UserConversionFunc(func(chain []*x509.Certificate) (*authenticator.Response, bool, error) {
	if len(chain[0].Subject.CommonName) == 0 {
		return nil, false, nil
	}
	return &amp;authenticator.Response{
		User: &amp;user.DefaultInfo{
			Name:   chain[0].Subject.CommonName,
			Groups: chain[0].Subject.Organization,
		},
	}, true, nil
})
</code></pre>
<p>由于授权不在本章范围内，直接忽略至入库阶段，入库阶段由 <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/pkg/controlplane/instance.go#L561" target="_blank"
   rel="noopener nofollow noreferrer" >RESTStorageProvider</a> 实现 这里，每一个Provider都提供了 <code>Authenticator</code> 这里包含了已经允许的请求，将会被对应的REST客户端写入到库中</p>
<pre><code class="language-go">type RESTStorageProvider struct {
	Authenticator authenticator.Request
	APIAudiences  authenticator.Audiences
}
// RESTStorageProvider is a factory type for REST storage.
type RESTStorageProvider interface {
	GroupName() string
	NewRESTStorage(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter) (genericapiserver.APIGroupInfo, error)
}
</code></pre>
<h3 id="场景2通过token">场景2：通过token</h3>
<p>该场景中，当 <em><strong>kube-apiserver</strong></em> 开启了 <code>--enable-bootstrap-token-auth</code> 时，就可以使用 Bootstrap Token 进行认证，通常如下列命令，在请求头中增加 <code>Authorization: Bearer &lt;token&gt;</code> 标识</p>
<pre><code class="language-bash">$ curl https://hostname:6443/api/v1/pods \
  --cacert ${CACERT} \
  --header &quot;Authorization: Bearer &lt;token&gt;&quot; \
</code></pre>
<p>接下来浅析下在代码中做的事情</p>
<p>可以看到，在代码 <a href="pkg/kubeapiserver/authenticator">pkg/kubeapiserver/authenticator.New()</a> 中当 <em><strong>kube-apiserver</strong></em> 指定了参数 <code>--token-auth-file=/etc/kubernetes/token.csv&quot;</code> 这种认证会被激活</p>
<pre><code class="language-go">if len(config.TokenAuthFile) &gt; 0 {
    tokenAuth, err := newAuthenticatorFromTokenFile(config.TokenAuthFile)
    if err != nil {
        return nil, nil, err
    }
    tokenAuthenticators = append(tokenAuthenticators, authenticator.WrapAudienceAgnosticToken(config.APIAudiences, tokenAuth))
}
</code></pre>
<p>此时打开 token.csv 查看下token长什么样</p>
<pre><code class="language-bash">$ cat /etc/kubernetes/token.csv
12ba4f.d82a57a4433b2359,&quot;system:bootstrapper&quot;,10001,&quot;system:bootstrappers&quot;
</code></pre>
<p>这里回到代码 <a href="https://github.com/kubernetes/kubernetes/blob/fdc77503e954d1ee641c0e350481f7528e8d068b/staging/src/k8s.io/apiserver/pkg/authentication/token/tokenfile/tokenfile.go#L45-L91" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/authentication/token/tokenfile/tokenfile.go.NewCSV</a> ，这里可以看出，就是读取 <code>--token-auth-file=</code> 参数指定的tokenfile，然后解析为用户，<code>record[1]</code> 作为用户名，<code>record[2]</code> 作为UID</p>
<pre><code class="language-go">// NewCSV returns a TokenAuthenticator, populated from a CSV file.
// The CSV file must contain records in the format &quot;token,username,useruid&quot;
func NewCSV(path string) (*TokenAuthenticator, error) {
	file, err := os.Open(path)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	recordNum := 0
	tokens := make(map[string]*user.DefaultInfo)
	reader := csv.NewReader(file)
	reader.FieldsPerRecord = -1
	for {
		record, err := reader.Read()
		if err == io.EOF {
			break
		}
		if err != nil {
			return nil, err
		}
		if len(record) &lt; 3 {
			return nil, fmt.Errorf(&quot;token file '%s' must have at least 3 columns (token, user name, user uid), found %d&quot;, path, len(record))
		}

		recordNum++
		if record[0] == &quot;&quot; {
			klog.Warningf(&quot;empty token has been found in token file '%s', record number '%d'&quot;, path, recordNum)
			continue
		}

		obj := &amp;user.DefaultInfo{
			Name: record[1],
			UID:  record[2],
		}
		if _, exist := tokens[record[0]]; exist {
			klog.Warningf(&quot;duplicate token has been found in token file '%s', record number '%d'&quot;, path, recordNum)
		}
		tokens[record[0]] = obj

		if len(record) &gt;= 4 {
			obj.Groups = strings.Split(record[3], &quot;,&quot;)
		}
	}

	return &amp;TokenAuthenticator{
		tokens: tokens,
	}, nil
}
</code></pre>
<p>而token file中配置的格式正是以逗号分隔的一组字符串，</p>
<pre><code class="language-go">type DefaultInfo struct {
	Name   string
	UID    string
	Groups []string
	Extra  map[string][]string
}
</code></pre>
<p>这种用户最常见的方式就是 <em><strong>kubelet</strong></em> 通常会以此类用户向控制平面进行身份认证，例如下列配置</p>
<pre><code class="language-bash">KUBELET_ARGS=&quot;--v=0 \
    --logtostderr=true \
    --config=/etc/kubernetes/kubelet-config.yaml \
    --kubeconfig=/etc/kubernetes/auth/kubelet.conf \
    --network-plugin=cni \
    --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \
    --bootstrap-kubeconfig=/etc/kubernetes/auth/bootstrap.conf&quot;
</code></pre>
<p><code>/etc/kubernetes/auth/bootstrap.conf</code> 内容，这里就用到了 <em><strong>kube-apiserver</strong></em> 配置的 <code>--token-auth-file=</code> 用户名，组必须为 <code>system:bootstrappers</code></p>
<pre><code class="language-yaml">apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: ......
    server: https://10.0.0.4:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:bootstrapper
  name: system:bootstrapper@kubernetes
current-context: system:bootstrapper@kubernetes
kind: Config
preferences: {}
users:
- name: system:bootstrapper
</code></pre>
<p>而通常在二进制部署时会出现的问题，例如下列错误</p>
<pre><code class="language-log">Unable to register node &quot;hostname&quot; with API server: nodes is forbidden: User &quot;system:anonymous&quot; cannot create resource &quot;nodes&quot; in API group &quot;&quot; at the cluster scope
</code></pre>
<p>而通常解决方法是执行下列命令，这里就是将 <em><strong>kubelet</strong></em> 与 <em><strong>kube-apiserver</strong></em> 通讯时的用户授权，因为kubernetes官方给出的条件是，用户组必须为 <code>system:bootstrappers</code>  <sup><a href="#4">[4]</a></sup></p>
<pre><code class="language-bash">$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
</code></pre>
<p>生成的clusterrolebinding 如下</p>
<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  creationTimestamp: &quot;2022-08-14T22:26:51Z&quot;
  managedFields:
  - apiVersion: rbac.authorization.k8s.io/v1
    fieldsType: FieldsV1
   ...
    time: &quot;2022-08-14T22:26:51Z&quot;
  name: kubelet-bootstrap
  resourceVersion: &quot;158&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubelet-bootstrap
  uid: b4d70f4f-4ae0-468f-86b7-55e9351e4719
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node-bootstrapper
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:bootstrappers
</code></pre>
<p>上述就是 bootstrap token，翻译后就是引导token，因为其做的工作就是将节点载入Kubernetes系统过程提供认证机制的用户。</p>
<blockquote>
<p>Notes：这种用户不存在与kubernetes内，可以算属于一个外部用户，但认证机制中存在并绑定了最高权限，也可以用来做其他访问时的认证</p>
</blockquote>
<h3 id="场景3serviceaccount">场景3：serviceaccount</h3>
<p>serviceaccount通常为API自动创建的，但在用户中，实际上认证存在两个方向，一个是 <code>--service-account-key-file</code> 这个参数可以指定多个，指定对应的证书文件公钥或私钥，用以办法sa的token</p>
<p>首先会根据指定的公钥或私钥文件生成token</p>
<pre><code class="language-go">if len(config.ServiceAccountKeyFiles) &gt; 0 {
    serviceAccountAuth, err := newLegacyServiceAccountAuthenticator(config.ServiceAccountKeyFiles, config.ServiceAccountLookup, config.APIAudiences, config.ServiceAccountTokenGetter)
    if err != nil {
        return nil, nil, err
    }
    tokenAuthenticators = append(tokenAuthenticators, serviceAccountAuth)
}
if len(config.ServiceAccountIssuers) &gt; 0 {
    serviceAccountAuth, err := newServiceAccountAuthenticator(config.ServiceAccountIssuers, config.ServiceAccountKeyFiles, config.APIAudiences, config.ServiceAccountTokenGetter)
    if err != nil {
        return nil, nil, err
    }
    tokenAuthenticators = append(tokenAuthenticators, serviceAccountAuth)
}
</code></pre>
<p>对于  <code>--service-account-key-file</code>  他生成的用户都是 “kubernetes/serviceaccount”  , 而对于 <code>--service-account-issuer</code> 只是对sa颁发者提供了一个称号标识是谁，而不是统一的 “kubernetes/serviceaccount” ，这里可以从代码中看到，两者是完全相同的，只是称号不同罢了</p>
<pre><code class="language-go">// newLegacyServiceAccountAuthenticator returns an authenticator.Token or an error
func newLegacyServiceAccountAuthenticator(keyfiles []string, lookup bool, apiAudiences authenticator.Audiences, serviceAccountGetter serviceaccount.ServiceAccountTokenGetter) (authenticator.Token, error) {
	allPublicKeys := []interface{}{}
	for _, keyfile := range keyfiles {
		publicKeys, err := keyutil.PublicKeysFromFile(keyfile)
		if err != nil {
			return nil, err
		}
		allPublicKeys = append(allPublicKeys, publicKeys...)
	}
// 唯一的区别 这里使用了常量 serviceaccount.LegacyIssuer
	tokenAuthenticator := serviceaccount.JWTTokenAuthenticator([]string{serviceaccount.LegacyIssuer}, allPublicKeys, apiAudiences, serviceaccount.NewLegacyValidator(lookup, serviceAccountGetter))
	return tokenAuthenticator, nil
}

// newServiceAccountAuthenticator returns an authenticator.Token or an error
func newServiceAccountAuthenticator(issuers []string, keyfiles []string, apiAudiences authenticator.Audiences, serviceAccountGetter serviceaccount.ServiceAccountTokenGetter) (authenticator.Token, error) {
	allPublicKeys := []interface{}{}
	for _, keyfile := range keyfiles {
		publicKeys, err := keyutil.PublicKeysFromFile(keyfile)
		if err != nil {
			return nil, err
		}
		allPublicKeys = append(allPublicKeys, publicKeys...)
	}
// 唯一的区别 这里根据kube-apiserver提供的称号指定名称
	tokenAuthenticator := serviceaccount.JWTTokenAuthenticator(issuers, allPublicKeys, apiAudiences, serviceaccount.NewValidator(serviceAccountGetter))
	return tokenAuthenticator, nil
}
</code></pre>
<p>最后根据ServiceAccounts，Secrets等值签发一个token，也就是通过下列命令获取的值</p>
<pre><code class="language-go">$ kubectl get secret multus-token-v6bfg -n kube-system -o jsonpath={&quot;.data.token&quot;}
</code></pre>
<h3 id="场景4openid">场景4：openid</h3>
<p>OpenID Connect是 OAuth2 风格，允许用户授权三方网站访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方网站或分享他们数据的所有内容，下面是一张kubernetes 使用 OID 认证的逻辑图</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/kube-login-oidc-ad4caf57f124e622897e0781fe1e3d6e1ecb5c6099776e6677ca800c4458f1de.jpg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes OID认证</center>
<center><em>Source：</em>https://developer.okta.com/blog/2021/11/08/k8s-api-server-oidc</center><br>
<h3 id="场景5webhook">场景5：webhook</h3>
<p>webhook是kubernetes提供自定义认证的其中一种，主要是用于认证 “<em><strong>不记名 token</strong></em>“ 的钩子，“<em><strong>不记名 token</strong></em>“ 将 由身份验证服务创建。当用户对kubernetes访问时，会触发准入控制，当对kubernetes集群注册了 authenticaion webhook时，将会使用该webhook提供的方式进行身份验证时，此时会为您生成一个 token 。</p>
<p>如代码 <a href="pkg/kubeapiserver/authenticator">pkg/kubeapiserver/authenticator.New()</a>  中所示 newWebhookTokenAuthenticator 会通过提供的config (<code>--authentication-token-webhook-config-file</code>) 来创建出一个 WebhookTokenAuthenticator</p>
<pre><code class="language-go">if len(config.WebhookTokenAuthnConfigFile) &gt; 0 {
    webhookTokenAuth, err := newWebhookTokenAuthenticator(config)
    if err != nil {
        return nil, nil, err
    }

    tokenAuthenticators = append(tokenAuthenticators, webhookTokenAuth)
}
</code></pre>
<p>下图是kubernetes 中 WebhookToken 验证的工作原理</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/25d075712ff343ce492a5db30733cd93.svg" alt="Webhook 令牌认证插件" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：kubernetes WebhookToken验证原理</center>
<center><em>Source：</em>https://learnk8s.io/kubernetes-custom-authentication</center><br>
<p>最后由token中的authHandler，循环所有的Handlers在运行 <code>AuthenticateToken</code> 去进行获取用户的信息</p>
<pre><code class="language-go">func (authHandler *unionAuthTokenHandler) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) {
   var errlist []error
   for _, currAuthRequestHandler := range authHandler.Handlers {
      info, ok, err := currAuthRequestHandler.AuthenticateToken(ctx, token)
      if err != nil {
         if authHandler.FailOnError {
            return info, ok, err
         }
         errlist = append(errlist, err)
         continue
      }

      if ok {
         return info, ok, err
      }
   }

   return nil, false, utilerrors.NewAggregate(errlist)
}
</code></pre>
<p>而webhook插件也实现了这个方法 <code>AuthenticateToken</code> ,这里会通过POST请求，调用注入的webhook，该请求携带一个JSON 格式的 <code>TokenReview</code> 对象，其中包含要验证的令牌</p>
<pre><code class="language-go">func (w *WebhookTokenAuthenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) {

    ....

		start := time.Now()
		result, statusCode, tokenReviewErr = w.tokenReview.Create(ctx, r, metav1.CreateOptions{})
		latency := time.Since(start)
...
}
</code></pre>
<p>webhook token认证服务要返回<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#userinfo-v1beta1-authentication-k8s-io" target="_blank"
   rel="noopener nofollow noreferrer" >用户的身份信息</a>，就是上面token部分提到的数据结构（webhook来决定接受还是拒绝该用户）</p>
<pre><code class="language-go">type DefaultInfo struct {
	Name   string
	UID    string
	Groups []string
	Extra  map[string][]string
}
</code></pre>
<h3 id="场景6代理认证">场景6：代理认证</h3>
<h2 id="实验基于ldap的身份认证">实验：基于LDAP的身份认证</h2>
<p>通过上面阐述，大致了解到kubernetes认证框架中的用户的分类以及认证的策略由哪些，实验的目的也是为了阐述一个结果，就是使用OIDC/webhook 是比其他方式更好的保护，管理kubernetes集群。首先在安全上，假设网络环境是不安全的，那么任意node节点遗漏 bootstrap token文件，就意味着拥有了集群中最高权限；其次在管理上，越大的团队，人数越多，不可能每个用户都提供单独的证书或者token，要知道传统教程中讲到token在kubernetes集群中是永久有效的，除非你删除了这个secret/sa；而Kubernetes提供的插件就很好的解决了这些问题。</p>
<h3 id="实验环境">实验环境</h3>
<ul>
<li>一个kubernetes集群</li>
<li>一个openldap服务，建议可以是集群外部的，因为webhook不像SSSD有缓存机制，并且集群不可用，那么认证不可用，当认证不可用时会导致集群不可用，这样事故影响的范围可以得到控制，也叫最小化半径</li>
<li>了解ldap相关技术，并了解go ldap客户端</li>
</ul>
<p><strong>实验大致分为以下几个步骤</strong>：</p>
<ul>
<li>建立一个HTTP服务器用于返回给kubernetes Authenticaion服务</li>
<li>查询ldap该用户是否合法
<ul>
<li>查询用户是否合法</li>
<li>查询用户所属组是否拥有权限</li>
</ul>
</li>
</ul>
<h3 id="实验开始">实验开始</h3>
<h4 id="初始化用户数据">初始化用户数据</h4>
<p>首先准备openldap初始化数据，创建三个 posixGroup 组，与5个用户 admin, admin1, admin11, searchUser, syncUser 密码均为111，组与用户关联使用的 <code>memberUid</code></p>
<pre><code class="language-bash">cat &lt;&lt; EOF | ldapdelete -r  -H ldap://10.0.0.3 -D &quot;cn=admin,dc=test,dc=com&quot; -w 111
dn: dc=test,dc=com
objectClass: top
objectClass: organizationalUnit
objectClass: extensibleObject
description: US Organization
ou: people

dn: ou=tvb,dc=test,dc=com
objectClass: organizationalUnit
description: Television Broadcasts Limited
ou: tvb

dn: cn=admin,ou=tvb,dc=test,dc=com
objectClass: posixGroup
gidNumber: 10000
cn: admin

dn: cn=conf,ou=tvb,dc=test,dc=com
objectClass: posixGroup
gidNumber: 10001
cn: conf

dn: cn=dir,ou=tvb,dc=test,dc=com
objectClass: posixGroup
gidNumber: 10002
cn: dir

dn: uid=syncUser,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
pwdAttribute: userPassword
uid: syncUser
cn: syncUser
uidNumber: 10006
gidNumber: 10002
homeDirectory: /home/syncUser
loginShell: /bin/bash
sn: syncUser
givenName: syncUser
memberOf: cn=confGroup,ou=tvb,dc=test,dc=com

dn: uid=searchUser,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
pwdAttribute: userPassword
uid: searchUser
cn: searchUser
uidNumber: 10005
gidNumber: 10001
homeDirectory: /home/searchUser
loginShell: /bin/bash
sn: searchUser
givenName: searchUser
memberOf: cn=dirGroup,ou=tvb,dc=test,dc=com

dn: uid=admin1,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
pwdAttribute: userPassword
uid: admin1
sn: admin1
cn: admin
uidNumber: 10010
gidNumber: 10000
homeDirectory: /home/admin
loginShell: /bin/bash
givenName: admin
memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com

dn: uid=admin11,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
sn: admin11
pwdAttribute: userPassword
uid: admin11
cn: admin11
uidNumber: 10011
gidNumber: 10000
homeDirectory: /home/admin
loginShell: /bin/bash
givenName: admin11
memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com

dn: uid=admin,ou=tvb,dc=test,dc=com
objectClass: inetOrgPerson
objectClass: organizationalPerson
objectClass: person
objectClass: posixAccount
objectClass: shadowAccount
objectClass: pwdPolicy
pwdAttribute: userPassword
uid: admin
cn: admin
uidNumber: 10009
gidNumber: 10000
homeDirectory: /home/admin
loginShell: /bin/bash
sn: admin
givenName: admin
memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com
EOF
</code></pre>
<p>接下来需要确定如何为认证成功的用户，上面讲到对于kubernetes中用户格式为 <code>v1.UserInfo</code> 的格式，即要获得用户，即用户组，假设需要查找的用户为，admin，那么在openldap中查询filter如下：</p>
<pre><code class="language-bash">&quot;(|(&amp;(objectClass=posixAccount)(uid=admin))(&amp;(objectClass=posixGroup)(memberUid=admin)))&quot;
</code></pre>
<p>上面语句意思是，找到 <code>objectClass=posixAccount</code> 并且 <code>uid=admin</code> 或者 <code>objectClass=posixGroup</code> 并且 <code>memberUid=admin</code> 的条目信息，这里使用 ”|“ 与 ”&amp;“ 是为了要拿到这两个结果。</p>
<h4 id="编写webhook查询用户部分">编写webhook查询用户部分</h4>
<p>这里由于openldap配置密码保存格式不是明文的，如果直接使用 ”=“ 来验证是查询不到内容的，故直接多用了一次登录来验证用户是否合法</p>
<pre><code class="language-go">func ldapSearch(username, password string) (*v1.UserInfo, error) {
	ldapconn, err := ldap.DialURL(ldapURL)
	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}
	defer ldapconn.Close()

	// Authenticate as LDAP admin user
	err = ldapconn.Bind(&quot;uid=searchUser,ou=tvb,dc=test,dc=com&quot;, &quot;111&quot;)
	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	// Execute LDAP Search request
	result, err := ldapconn.Search(ldap.NewSearchRequest(
		&quot;ou=tvb,dc=test,dc=com&quot;,
		ldap.ScopeWholeSubtree,
		ldap.NeverDerefAliases,
		0,
		0,
		false,
		fmt.Sprintf(&quot;(&amp;(objectClass=posixGroup)(memberUid=%s))&quot;, username), // Filter
		nil,
		nil,
	))

	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	userResult, err := ldapconn.Search(ldap.NewSearchRequest(
		&quot;ou=tvb,dc=test,dc=com&quot;,
		ldap.ScopeWholeSubtree,
		ldap.NeverDerefAliases,
		0,
		0,
		false,
		fmt.Sprintf(&quot;(&amp;(objectClass=posixAccount)(uid=%s))&quot;, username), // Filter
		nil,
		nil,
	))

	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	if len(result.Entries) == 0 {
		klog.V(3).Info(&quot;User does not exist&quot;)
		return nil, errors.New(&quot;User does not exist&quot;)
	} else {
		// 验证用户名密码是否正确
		if err := ldapconn.Bind(userResult.Entries[0].DN, password); err != nil {
			e := fmt.Sprintf(&quot;Failed to auth. %s\n&quot;, err)
			klog.V(3).Info(e)
			return nil, errors.New(e)
		} else {
			klog.V(3).Info(fmt.Sprintf(&quot;User %s Authenticated successfuly!&quot;, username))
		}
		// 拼接为kubernetes authentication 的用户格式
		user := new(v1.UserInfo)
		for _, v := range result.Entries {
			attrubute := v.GetAttributeValue(&quot;objectClass&quot;)
			if strings.Contains(attrubute, &quot;posixGroup&quot;) {
				user.Groups = append(user.Groups, v.GetAttributeValue(&quot;cn&quot;))
			}
		}

		u := userResult.Entries[0].GetAttributeValue(&quot;uid&quot;)
		user.UID = u
		user.Username = u
		return user, nil
	}
}
</code></pre>
<h4 id="编写http部分">编写HTTP部分</h4>
<p>这里有几个需要注意的部分，即用户或者理解为要认证的token的定义，此处使用了 ”username@password“ 格式作为用户的辨别，即登录kubernetes时需要直接输入 ”username@password“ 来作为登录的凭据。</p>
<p>第二个部分为返回值，返回给Kubernetes的格式必须为 <code>api/authentication/v1.TokenReview</code> 格式，<code>Status.Authenticated</code> 表示用户身份验证结果，如果该用户合法，则设置 <code>tokenReview.Status.Authenticated = true</code> 反之亦然。如果验证成功还需要 <code>Status.User</code> 这就是在<code>ldapSearch</code></p>
<pre><code class="language-go">func serve(w http.ResponseWriter, r *http.Request) {
	b, err := ioutil.ReadAll(r.Body)
	if err != nil {
		httpError(w, err)
		return
	}
	klog.V(4).Info(&quot;Receiving: %s\n&quot;, string(b))

	var tokenReview v1.TokenReview
	err = json.Unmarshal(b, &amp;tokenReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}

	// 提取用户名与密码
	s := strings.SplitN(tokenReview.Spec.Token, &quot;@&quot;, 2)
	if len(s) != 2 {
		klog.V(3).Info(fmt.Errorf(&quot;badly formatted token: %s&quot;, tokenReview.Spec.Token))
		httpError(w, fmt.Errorf(&quot;badly formatted token: %s&quot;, tokenReview.Spec.Token))
		return
	}
	username, password := s[0], s[1]
	// 查询ldap，验证用户是否合法
	userInfo, err := ldapSearch(username, password)
	if err != nil {
		// 这里不打印日志的原因是 ldapSearch 中打印过了
		return
	}

	// 设置返回的tokenReview
	if userInfo == nil {
		tokenReview.Status.Authenticated = false
	} else {
		tokenReview.Status.Authenticated = true
		tokenReview.Status.User = *userInfo
	}

	b, err = json.Marshal(tokenReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	w.Write(b)
	klog.V(3).Info(&quot;Returning: &quot;, string(b))
}

func httpError(w http.ResponseWriter, err error) {
	err = fmt.Errorf(&quot;Error: %v&quot;, err)
	w.WriteHeader(http.StatusInternalServerError) // 500
	fmt.Fprintln(w, err)
	klog.V(4).Info(&quot;httpcode 500: &quot;, err)
}
</code></pre>
<p>下面是完整的代码</p>
<pre><code class="language-go">package main

import (
	&quot;encoding/json&quot;
	&quot;errors&quot;
	&quot;flag&quot;
	&quot;fmt&quot;
	&quot;io/ioutil&quot;
	&quot;net/http&quot;
	&quot;strings&quot;

	&quot;github.com/go-ldap/ldap&quot;
	&quot;k8s.io/api/authentication/v1&quot;
	&quot;k8s.io/klog/v2&quot;
)

var ldapURL string

func main() {
	klog.InitFlags(nil)
	flag.Parse()
	http.HandleFunc(&quot;/authenticate&quot;, serve)
	klog.V(4).Info(&quot;Listening on port 443 waiting for requests...&quot;)
	klog.V(4).Info(http.ListenAndServe(&quot;:443&quot;, nil))
	ldapURL = &quot;ldap://10.0.0.10:389&quot;
	ldapSearch(&quot;admin&quot;, &quot;1111&quot;)
}

func serve(w http.ResponseWriter, r *http.Request) {
	b, err := ioutil.ReadAll(r.Body)
	if err != nil {
		httpError(w, err)
		return
	}
	klog.V(4).Info(&quot;Receiving: %s\n&quot;, string(b))

	var tokenReview v1.TokenReview
	err = json.Unmarshal(b, &amp;tokenReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}

	// 提取用户名与密码
	s := strings.SplitN(tokenReview.Spec.Token, &quot;@&quot;, 2)
	if len(s) != 2 {
		klog.V(3).Info(fmt.Errorf(&quot;badly formatted token: %s&quot;, tokenReview.Spec.Token))
		httpError(w, fmt.Errorf(&quot;badly formatted token: %s&quot;, tokenReview.Spec.Token))
		return
	}
	username, password := s[0], s[1]
	// 查询ldap，验证用户是否合法
	userInfo, err := ldapSearch(username, password)
	if err != nil {
		// 这里不打印日志的原因是 ldapSearch 中打印过了
		return
	}

	// 设置返回的tokenReview
	if userInfo == nil {
		tokenReview.Status.Authenticated = false
	} else {
		tokenReview.Status.Authenticated = true
		tokenReview.Status.User = *userInfo
	}

	b, err = json.Marshal(tokenReview)
	if err != nil {
		klog.V(3).Info(&quot;Json convert err: &quot;, err)
		httpError(w, err)
		return
	}
	w.Write(b)
	klog.V(3).Info(&quot;Returning: &quot;, string(b))
}

func httpError(w http.ResponseWriter, err error) {
	err = fmt.Errorf(&quot;Error: %v&quot;, err)
	w.WriteHeader(http.StatusInternalServerError) // 500
	fmt.Fprintln(w, err)
	klog.V(4).Info(&quot;httpcode 500: &quot;, err)
}

func ldapSearch(username, password string) (*v1.UserInfo, error) {

	ldapconn, err := ldap.DialURL(ldapURL)
	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}
	defer ldapconn.Close()

	// Authenticate as LDAP admin user
	err = ldapconn.Bind(&quot;cn=admin,dc=test,dc=com&quot;, &quot;111&quot;)
	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	// Execute LDAP Search request
	result, err := ldapconn.Search(ldap.NewSearchRequest(
		&quot;ou=tvb,dc=test,dc=com&quot;,
		ldap.ScopeWholeSubtree,
		ldap.NeverDerefAliases,
		0,
		0,
		false,
		fmt.Sprintf(&quot;(&amp;(objectClass=posixGroup)(memberUid=%s))&quot;, username), // Filter
		nil,
		nil,
	))

	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	userResult, err := ldapconn.Search(ldap.NewSearchRequest(
		&quot;ou=tvb,dc=test,dc=com&quot;,
		ldap.ScopeWholeSubtree,
		ldap.NeverDerefAliases,
		0,
		0,
		false,
		fmt.Sprintf(&quot;(&amp;(objectClass=posixAccount)(uid=%s))&quot;, username), // Filter
		nil,
		nil,
	))

	if err != nil {
		klog.V(3).Info(err)
		return nil, err
	}

	if len(result.Entries) == 0 {
		klog.V(3).Info(&quot;User does not exist&quot;)
		return nil, errors.New(&quot;User does not exist&quot;)
	} else {
		// 验证用户名密码是否正确
		if err := ldapconn.Bind(userResult.Entries[0].DN, password); err != nil {
			e := fmt.Sprintf(&quot;Failed to auth. %s\n&quot;, err)
			klog.V(3).Info(e)
			return nil, errors.New(e)
		} else {
			klog.V(3).Info(fmt.Sprintf(&quot;User %s Authenticated successfuly!&quot;, username))
		}
		// 拼接为kubernetes authentication 的用户格式
		user := new(v1.UserInfo)
		for _, v := range result.Entries {
			attrubute := v.GetAttributeValue(&quot;objectClass&quot;)
			if strings.Contains(attrubute, &quot;posixGroup&quot;) {
				user.Groups = append(user.Groups, v.GetAttributeValue(&quot;cn&quot;))
			}
		}

		u := userResult.Entries[0].GetAttributeValue(&quot;uid&quot;)
		user.UID = u
		user.Username = u
		return user, nil
	}
}
</code></pre>
<h3 id="部署webhook">部署webhook</h3>
<p>kubernetes官方手册中指出，启用webhook认证的标记是在 <em><strong>kube-apiserver</strong></em> 指定参数 <code>--authentication-token-webhook-config-file</code> 。而这个配置文件是一个 <em>kubeconfig</em> 类型的文件格式 <sup><a href="#5">[5]</a></sup></p>
<p>下列是部署在kubernetes集群外部的配置</p>
<p>创建一个给 <em>kube-apiserver</em> 使用的配置文件 <code>/etc/kubernetes/auth/authentication-webhook.conf</code></p>
<pre><code class="language-yaml">apiVersion: v1
kind: Config
clusters:
- cluster:
    server: http://10.0.0.1:88/authenticate
  name: authenticator
users:
- name: webhook-authenticator
current-context: webhook-authenticator@authenticator
contexts:
- context:
    cluster: authenticator
    user: webhook-authenticator
  name: webhook-authenticator@authenticator
</code></pre>
<p>修改 <em>kube-apiserver</em> 参数</p>
<pre><code class="language-bash"># 指向对应的配置文件
--authentication-token-webhook-config-file=/etc/kubernetes/auth/authentication-webhook.conf
# 这个是token缓存时间，指的是用户在访问API时验证通过后在一定时间内无需在请求webhook进行认证了
--authentication-token-webhook-cache-ttl=30m
# 版本指定为API使用哪个版本？authentication.k8s.io/v1或v1beta1
--authentication-token-webhook-version=v1
</code></pre>
<p>启动服务后，创建一个 kubeconfig 中的用户用于验证结果</p>
<pre><code class="language-conf">apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: 
    server: https://10.0.0.4:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: k8s-admin
  name: k8s-admin@kubernetes
current-context: k8s-admin@kubernetes
kind: Config
preferences: {}
users:
- name: admin
  user: 
    token: admin@111
</code></pre>
<h3 id="验证结果">验证结果</h3>
<p><strong>当密码不正确时，使用用户admin请求集群</strong></p>
<pre><code class="language-bash">$ kubectl get pods --user=admin
error: You must be logged in to the server (Unauthorized)
</code></pre>
<p><strong>当密码正确时，使用用户admin请求集群</strong></p>
<pre><code class="language-bash">$ kubectl get pods --user=admin
Error from server (Forbidden): pods is forbidden: User &quot;admin&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
</code></pre>
<p>可以看到admin用户是一个不存在与集群中的用户，并且提示没有权限操作对应资源，此时将admin用户与集群中的cluster-admin绑定，测试结果</p>
<pre><code class="language-bash">$ kubectl create clusterrolebinding admin \
	--clusterrole=cluster-admin \
	--group=admin
</code></pre>
<p>此时再尝试使用admin用户访问集群</p>
<pre><code class="language-bash">$ kubectl get pods --user=admin
NAME                      READY   STATUS    RESTARTS   AGE
netbox-85865d5556-hfg6v   1/1     Running   0          91d
netbox-85865d5556-vlgr4   1/1     Running   0          91d
</code></pre>
<h2 id="总结">总结</h2>
<p>kubernetes authentication 插件提供的功能可以注入一个认证系统，这样可以完美解决了kubernetes中用户的问题，而这些用户并不存在与kubernetes中，并且也无需为多个用户准备大量serviceaccount或者证书，也可以完成鉴权操作。首先返回值标准如下所示，如果kubernetes集群有对在其他用户系统中获得的 <code>Groups</code> 并建立了 <code>clusterrolebinding</code> 或 <code>rolebinding</code> 那么这个组的所有用户都将有这些权限。管理员只需要维护与公司用户系统中组同样多的 clusterrole 与 clusterrolebinding 即可</p>
<pre><code class="language-go">type DefaultInfo struct {
	Name   string
	UID    string
	Groups []string
	Extra  map[string][]string
}
</code></pre>
<p>对于如何将 kubernetes 与其他平台进行融合可以参考 <a href="https://cylonchau.github.io/kubernetes-dashborad-based.html" target="_blank"
   rel="noopener nofollow noreferrer" >文章</a></p>
<blockquote>
<p>Notes：Kubernetes原生就支持OID，完全不用自己开发webhook从而实现接入其他系统，这里展示的只是一个思路</p>
</blockquote>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://learnk8s.io/kubernetes-custom-authentication" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Implementing a custom Kubernetes authentication method</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/concepts/security/controlling-access/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Controlling Access to the Kubernetes API</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#users-in-kubernetes" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Users in Kubernetes</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#bootstrap-tokens" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>bootstrap tokens</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Webhook Token Authentication</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>基于Prometheus的Kubernetes网络调度器</title>
      <link>https://www.oomkill.com/2022/08/ch22-custom-scheduler/</link>
      <pubDate>Mon, 08 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/08/ch22-custom-scheduler/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>本文将深入讲解 如何扩展 Kubernetes scheduler 中各个扩展点如何使用，与扩展scheduler的原理，这些是作为扩展 <em>scheduler</em> 的所需的知识点。最后会完成一个实验，基于网络流量的调度器。</p>
<h2 id="kubernetes调度配置">kubernetes调度配置</h2>
<p>kubernetes集群中允许运行多个不同的 <em>scheduler</em>  ，也可以为Pod指定不同的调度器进行调度。在一般的Kubernetes调度教程中并没有提到这点，这也就是说，对于亲和性，污点等策略实际上并没有完全的使用kubernetes调度功能，在之前的文章中提到的一些调度插件，如基于端口占用的调度 <code>NodePorts</code> 等策略一般情况下是没有使用到的，本章节就是对这部分内容进行讲解，这也是作为扩展调度器的一个基础。</p>
<h3 id="scheduler-configuration-supa-href11asup">Scheduler Configuration <sup><a href="#1">[1]</a></sup></h3>
<p><em>kube-scheduler</em> 提供了配置文件的资源，作为给 <em>kube-scheduler</em> 的配置文件，启动时通过 <code>--onfig=</code> 来指定文件。目前各个kubernetes版本中使用的 <code>KubeSchedulerConfiguration</code> 为，</p>
<ul>
<li>1.21 之前版本使用 <code>v1beta1</code></li>
<li>1.22 版本使用 <code>v1beta2</code> ，但保留了 <code>v1beta1</code></li>
<li>1.23, 1.24, 1.25 版本使用 <code>v1beta3</code> ，但保留了  <code>v1beta2</code>，删除了 <code>v1beta1</code></li>
</ul>
<p>下面是一个简单的 <em>kubeSchedulerConfiguration</em> 示例，其中 <code>kubeconfig</code> 与启动参数 <code>--kubeconfig</code> 是相同的功效。而 <em>kubeSchedulerConfiguration</em> 与其他组件的配置文件类似，如 <em>kubeletConfiguration</em> 都是作为服务启动的配置文件。</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: /etc/srv/kubernetes/kube-scheduler/kubeconfig
</code></pre>
<blockquote>
<p>Notes: <code>--kubeconfig</code> 与 <code>--config</code> 是不可以同时指定的，指定了 <code>--config</code> 则其他参数自然失效 <sup><a href="#2">[2]</a></sup></p>
</blockquote>
<h3 id="kubeschedulerconfiguration使用">kubeSchedulerConfiguration使用</h3>
<p>通过配置文件，用户可以自定义多个调度器，以及配置每个阶段的扩展点。而插件就是通过这些扩展点来提供在整个调度上下文中的调度行为。</p>
<p>下面配置是对于配置扩展点的部分的一个示例，关于扩展点的讲解可以参考kubernetes官方文档调度上下文部分</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
profiles:
  - plugins:
      score:
        disabled:
        - name: PodTopologySpread
        enabled:
        - name: MyCustomPluginA
          weight: 2
        - name: MyCustomPluginB
          weight: 1
</code></pre>
<blockquote>
<p>Notes: 如果name=&quot;*&quot; 的话，这种情况下将禁用/启用对应扩展点的所有插件</p>
</blockquote>
<p>既然kubernetes提供了多调度器，那么对于配置文件来说自然支持多个配置文件，profile也是列表形式，只要指定多个配置列表即可，下面是多配置文件示例，其中，如果存在多个扩展点，也可以为每个调度器配置多个扩展点。</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta2
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: default-scheduler
  	plugins:
      preScore:
        disabled:
        - name: '*'
      score:
        disabled:
        - name: '*'
  - schedulerName: no-scoring-scheduler
    plugins:
      preScore:
        disabled:
        - name: '*'
      score:
        disabled:
        - name: '*'
</code></pre>
<h3 id="scheduler调度插件-supa-href33asup">scheduler调度插件 <sup><a href="#3">[3]</a></sup></h3>
<p><em>kube-scheduler</em> 默认提供了很多插件作为调度方法，默认不配置的情况下会启用这些插件，如：</p>
<ul>
<li><em><strong>ImageLocality</strong></em>：调度将更偏向于Node存在容器镜像的节点。扩展点：<code>score</code>.</li>
<li><em><strong>TaintToleration</strong></em>：实现污点与容忍度功能。扩展点：<code>filter</code>, <code>preScore</code>, <code>score</code>.</li>
<li><em><strong>NodeName</strong></em>：实现调度策略中最简单的调度方法 <code>NodeName</code> 的实现。扩展点：<code>filter</code>.</li>
<li><em><strong>NodePorts</strong></em>：调度将检查Node端口是否已占用。扩展点：<code>preFilter</code>, <code>filter</code>.</li>
<li><em><strong>NodeAffinity</strong></em>：提供节点亲和性相关功能。扩展点：<code>filter</code>, <code>score</code>.</li>
<li><em><strong>PodTopologySpread</strong></em>：实现Pod拓扑域的功能。扩展点：<code>preFilter</code>, <code>filter</code>, <code>preScore</code>, <code>score</code>.</li>
<li><em><strong>NodeResourcesFit</strong></em>：该插件将检查节点是否拥有 Pod 请求的所有资源。使用以下三种策略之一：<code>LeastAllocated</code> （默认）<code>MostAllocated</code> 和 <code>RequestedToCapacityRatio</code>。扩展点：<code>preFilter</code>, <code>filter</code>, <code>score</code>.</li>
<li><em><strong>VolumeBinding</strong></em>：检查节点是否有或是否可以绑定请求的 <a href="https://kubernetes.io/docs/concepts/storage/volumes/" target="_blank"
   rel="noopener nofollow noreferrer" >卷</a>. 扩展点：<code>preFilter</code>, <code>filter</code>, <code>reserve</code>, <code>preBind</code>, <code>score</code>.</li>
<li><em><strong>VolumeRestrictions</strong></em>：检查安装在节点中的卷是否满足特定于卷提供程序的限制。扩展点：<code>filter</code>.</li>
<li><em><strong>VolumeZone</strong></em>：检查请求的卷是否满足它们可能具有的任何区域要求。扩展点：<code>filter</code>.</li>
<li><em><strong>InterPodAffinity</strong></em>： 实现Pod 间的亲和性与反亲和性的功能。扩展点：<code>preFilter</code>, <code>filter</code>, <code>preScore</code>, <code>score</code>.</li>
<li><em><strong>PrioritySort</strong></em>：提供基于默认优先级的排序。扩展点：<code>queueSort</code>.</li>
</ul>
<p>对于更多配置文件使用案例可以参考官方给出的文档</p>
<h2 id="如何扩展kube-scheduler-supa-href44asup">如何扩展kube-scheduler <sup><a href="#4">[4]</a></sup></h2>
<p>当在第一次考虑编写调度程序时，通常会认为扩展 <em>kube-scheduler</em> 是一件非常困难的事情，其实这些事情 kubernetes 官方早就想到了，kubernetes为此在 1.15 版本引入了framework的概念，framework旨在使 <em>scheduler</em> 更具有扩展性。</p>
<p><em>framework</em> 通过重新定义 各扩展点，将其作为 <em>plugins</em> 来使用，并且支持用户注册 <code>out of tree</code> 的扩展，使其可以被注册到 <em>kube-scheduler</em> 中，下面将对这些步骤进行分析。</p>
<h3 id="定义入口">定义入口</h3>
<p><em>scheduler</em> 允许进行自定义，但是对于只需要引用对应的 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/cmd/kube-scheduler/app/server.go#L64-L117" target="_blank"
   rel="noopener nofollow noreferrer" >NewSchedulerCommand</a>，并且实现自己的 <em>plugins</em> 的逻辑即可。</p>
<pre><code class="language-go">import (
    scheduler &quot;k8s.io/kubernetes/cmd/kube-scheduler/app&quot;
)

func main() {
    command := scheduler.NewSchedulerCommand(
            scheduler.WithPlugin(&quot;example-plugin1&quot;, ExamplePlugin1),
            scheduler.WithPlugin(&quot;example-plugin2&quot;, ExamplePlugin2))
    if err := command.Execute(); err != nil {
        fmt.Fprintf(os.Stderr, &quot;%v\n&quot;, err)
        os.Exit(1)
    }
}
</code></pre>
<p>而 <strong>NewSchedulerCommand</strong> 允许注入 out of tree plugins，也就是注入外部的自定义 plugins，这种情况下就无需通过修改源码方式去定义一个调度器，而仅仅通过自行实现即可完成一个自定义调度器。</p>
<pre><code class="language-go">// WithPlugin 用于注入out of tree plugins 因此scheduler代码中没有其引用。
func WithPlugin(name string, factory runtime.PluginFactory) Option {
	return func(registry runtime.Registry) error {
		return registry.Register(name, factory)
	}
}
</code></pre>
<h3 id="插件实现">插件实现</h3>
<p>对于插件的实现仅仅需要实现对应的扩展点接口。下面通过内置插件进行分析</p>
<p>对于内置插件 <code>NodeAffinity</code> ,我们通过观察他的结构可以发现，实现插件就是实现对应的扩展点抽象 <em>interface</em> 即可。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220807212221684.png" alt="image-20220807212221684" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="定义插件结构体">定义插件结构体</h4>
<p>其中 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/framework/v1alpha1/interface.go#L495-L524" target="_blank"
   rel="noopener nofollow noreferrer" >framework.FrameworkHandle</a> 是提供了Kubernetes API与 <em>scheduler</em> 之间调用使用的，通过结构可以看出包含 lister，informer等等，这个参数也是必须要实现的。</p>
<pre><code class="language-go">type NodeAffinity struct {
	handle framework.FrameworkHandle
}
</code></pre>
<h4 id="实现对应的扩展点">实现对应的扩展点</h4>
<pre><code class="language-go">func (pl *NodeAffinity) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	nodeInfo, err := pl.handle.SnapshotSharedLister().NodeInfos().Get(nodeName)
	if err != nil {
		return 0, framework.NewStatus(framework.Error, fmt.Sprintf(&quot;getting node %q from Snapshot: %v&quot;, nodeName, err))
	}

	node := nodeInfo.Node()
	if node == nil {
		return 0, framework.NewStatus(framework.Error, fmt.Sprintf(&quot;getting node %q from Snapshot: %v&quot;, nodeName, err))
	}

	affinity := pod.Spec.Affinity

	var count int64
	// A nil element of PreferredDuringSchedulingIgnoredDuringExecution matches no objects.
	// An element of PreferredDuringSchedulingIgnoredDuringExecution that refers to an
	// empty PreferredSchedulingTerm matches all objects.
	if affinity != nil &amp;&amp; affinity.NodeAffinity != nil &amp;&amp; affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution != nil {
		// Match PreferredDuringSchedulingIgnoredDuringExecution term by term.
		for i := range affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution {
			preferredSchedulingTerm := &amp;affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution[i]
			if preferredSchedulingTerm.Weight == 0 {
				continue
			}

			// TODO: Avoid computing it for all nodes if this becomes a performance problem.
			nodeSelector, err := v1helper.NodeSelectorRequirementsAsSelector(preferredSchedulingTerm.Preference.MatchExpressions)
			if err != nil {
				return 0, framework.NewStatus(framework.Error, err.Error())
			}

			if nodeSelector.Matches(labels.Set(node.Labels)) {
				count += int64(preferredSchedulingTerm.Weight)
			}
		}
	}

	return count, nil
}
</code></pre>
<p>最后在通过实现一个 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/framework/plugins/nodeaffinity/node_affinity.go#L116-L118" target="_blank"
   rel="noopener nofollow noreferrer" >New</a> 函数来提供注册这个扩展的方法。通过这个 <em>New</em> 函数可以在 <code>main.go</code> 中将其作为 out of tree plugins 注入到 <em>scheduler</em> 中即可</p>
<pre><code class="language-go">// New initializes a new plugin and returns it.
func New(_ runtime.Object, h framework.FrameworkHandle) (framework.Plugin, error) {
	return &amp;NodeAffinity{handle: h}, nil
}
</code></pre>
<h2 id="实验基于网络流量的调度-supa-href77asup">实验：基于网络流量的调度 <sup><a href="#7">[7]</a></sup></h2>
<p>通过上面阅读了解到了如何扩展 <em>scheduler</em> 插件，下面实验将完成一个基于流量的调度，通常情况下，网络一个Node在一段时间内使用的网络流量也是作为生产环境中很常见的情况。例如在配置均衡的多个主机中，主机A作为业务拉单脚本运行，主机B作为计算服务运行。通常来说计算服务会使用更多的系统资源，而拉单需要更多的是网络流量，此时在调度时，默认调度器有限选择的是系统空闲资源多的节点，这种情况下如果有Pod被调度到该节点上，那么可能双方业务都会收到影响（前端代理觉得这个节点连接数少会被大量调度，而拉单脚本因为网络带宽的占用降低了效能）。</p>
<h3 id="实验环境">实验环境</h3>
<ul>
<li>一个kubernetes集群，至少保证有两个节点。</li>
<li>提供的kubernetes集群都需要安装prometheus node_exporter，可以是集群内部的，也可以是集群外部的，这里使用的是集群外部的。</li>
<li>对 <a href="https://prometheus.io/docs/prometheus/latest/querying/basics/" target="_blank"
   rel="noopener nofollow noreferrer" >promQL</a> 与 <a href="https://github.com/prometheus/client_golang" target="_blank"
   rel="noopener nofollow noreferrer" >client_golang</a> 有所了解</li>
</ul>
<p><strong>实验大致分为以下几个步骤</strong>：</p>
<ul>
<li>定义插件API
<ul>
<li>插件命名为 <code>NetworkTraffic</code></li>
</ul>
</li>
<li>定义扩展点
<ul>
<li>这里使用了 Score 扩展点，并且定义评分的算法</li>
</ul>
</li>
<li>定义分数获取途径（从prometheus指标中拿到对应的数据）</li>
<li>定义对自定义调度器的参数传入</li>
<li>将项目部署到集群中（集群内部署与集群外部署）</li>
<li>实验的结果验证</li>
</ul>
<p>实验将仿照内置插件 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/framework/plugins/nodeaffinity/node_affinity.go" target="_blank"
   rel="noopener nofollow noreferrer" >nodeaffinity</a> 完成代码编写，为什么选择这个插件，只是因为这个插件相对比较简单，并且与我们实验目的基本相同，其实其他插件也是同样的效果。</p>
<p>整个实验的代码上传至 github.com/CylonChau/customScheduler</p>
<h3 id="实验开始">实验开始</h3>
<h4 id="错误处理">错误处理</h4>
<p>在初始化项目时，<code>go mod tidy</code> 等操作时，会遇到大量下面的错误</p>
<pre><code class="language-bash">go: github.com/GoogleCloudPlatform/spark-on-k8s-operator@v0.0.0-20210307184338-1947244ce5f4 requires
        k8s.io/apiextensions-apiserver@v0.0.0: reading k8s.io/apiextensions-apiserver/go.mod at revision v0.0.0: unknown revision v0.0.0
</code></pre>
<p>kubernetes issue #79384 <sup><a href="#5">[5]</a></sup> 中有提到这个问题，粗略浏览下没有说明为什么会出现这个问题，在最下方有个大佬提供了一个脚本，出现上述问题无法解决时直接运行该脚本后正常。</p>
<pre><code class="language-bash">#!/bin/sh
set -euo pipefail

VERSION=${1#&quot;v&quot;}
if [ -z &quot;$VERSION&quot; ]; then
    echo &quot;Must specify version!&quot;
    exit 1
fi
MODS=($(
    curl -sS https://raw.githubusercontent.com/kubernetes/kubernetes/v${VERSION}/go.mod |
    sed -n 's|.*k8s.io/\(.*\) =&gt; ./staging/src/k8s.io/.*|k8s.io/\1|p'
))
for MOD in &quot;${MODS[@]}&quot;; do
    V=$(
        go mod download -json &quot;${MOD}@kubernetes-${VERSION}&quot; |
        sed -n 's|.*&quot;Version&quot;: &quot;\(.*\)&quot;.*|\1|p'
    )
    go mod edit &quot;-replace=${MOD}=${MOD}@${V}&quot;
done
go get &quot;k8s.io/kubernetes@v${VERSION}&quot;
</code></pre>
<h4 id="定义插件api">定义插件API</h4>
<p>通过上面内容描述了解到了定义插件只需要实现对应的扩展点抽象 <em>interface</em> ，那么可以初始化项目目录 <code>pkg/networtraffic/networktraffice.go</code>。</p>
<p>定义插件名称与变量</p>
<pre><code class="language-go">const Name = &quot;NetworkTraffic&quot;
var _ = framework.ScorePlugin(&amp;NetworkTraffic{})
</code></pre>
<p>定义插件的结构体</p>
<pre><code class="language-go">type NetworkTraffic struct {
    // 这个作为后面获取node网络流量使用
	prometheus *PrometheusHandle
	// FrameworkHandle 提供插件可以使用的数据和一些工具。
	// 它在插件初始化时传递给 plugin 工厂类。
	// plugin 必须存储和使用这个handle来调用framework函数。
	handle framework.FrameworkHandle
}
</code></pre>
<h4 id="定义扩展点">定义扩展点</h4>
<p>因为选用 Score 扩展点，需要定义对应的方法，来实现对应的抽象</p>
<pre><code class="language-go">func (n *NetworkTraffic) Score(ctx context.Context, state *framework.CycleState, p *corev1.Pod, nodeName string) (int64, *framework.Status) {
    // 通过promethes拿到一段时间的node的网络使用情况
	nodeBandwidth, err := n.prometheus.GetGauge(nodeName)
	if err != nil {
		return 0, framework.NewStatus(framework.Error, fmt.Sprintf(&quot;error getting node bandwidth measure: %s&quot;, err))
	}
	bandWidth := int64(nodeBandwidth.Value)
	klog.Infof(&quot;[NetworkTraffic] node '%s' bandwidth: %s&quot;, nodeName, bandWidth)
	return bandWidth, nil // 这里直接返回就行
}
</code></pre>
<p>接下来需要对结果归一化，这里就回到了调度框架中扩展点的执行问题上了，通过源码可以看出，Score 扩展点需要实现的并不只是这单一的方法。</p>
<pre><code class="language-go">// Run NormalizeScore method for each ScorePlugin in parallel.
parallelize.Until(ctx, len(f.scorePlugins), func(index int) {
    pl := f.scorePlugins[index]
    nodeScoreList := pluginToNodeScores[pl.Name()]
    if pl.ScoreExtensions() == nil {
        return
    }
    status := f.runScoreExtension(ctx, pl, state, pod, nodeScoreList)
    if !status.IsSuccess() {
        err := fmt.Errorf(&quot;normalize score plugin %q failed with error %v&quot;, pl.Name(), status.Message())
        errCh.SendErrorWithCancel(err, cancel)
        return
    }
})
</code></pre>
<p>通过上面代码了解到，实现 <code>Score </code> 就必须实现 <code>ScoreExtensions</code>，如果没有实现则直接返回。而根据 <code>nodeaffinity</code> 中示例发现这个方法仅仅返回的是这个扩展点对象本身，而具体的归一化也就是真正进行打分的操作在 <code>NormalizeScore</code> 中。</p>
<pre><code class="language-go">// NormalizeScore invoked after scoring all nodes.
func (pl *NodeAffinity) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
	return pluginhelper.DefaultNormalizeScore(framework.MaxNodeScore, false, scores)
}

// ScoreExtensions of the Score plugin.
func (pl *NodeAffinity) ScoreExtensions() framework.ScoreExtensions {
	return pl
}
</code></pre>
<p>而在 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/framework/runtime/framework.go#L692-L700" target="_blank"
   rel="noopener nofollow noreferrer" >framework</a> 中，真正执行的操作的方法也是 <code>NormalizeScore()</code></p>
<pre><code class="language-go">func (f *frameworkImpl) runScoreExtension(ctx context.Context, pl framework.ScorePlugin, state *framework.CycleState, pod *v1.Pod, nodeScoreList framework.NodeScoreList) *framework.Status {
	if !state.ShouldRecordPluginMetrics() {
		return pl.ScoreExtensions().NormalizeScore(ctx, state, pod, nodeScoreList)
	}
	startTime := time.Now()
	status := pl.ScoreExtensions().NormalizeScore(ctx, state, pod, nodeScoreList)
	f.metricsRecorder.observePluginDurationAsync(scoreExtensionNormalize, pl.Name(), status, metrics.SinceInSeconds(startTime))
	return status
}
</code></pre>
<p>下面来实现对应的方法</p>
<p>在 <em>NormalizeScore</em> 中需要实现具体的选择node的算法，因为对node打分结果的区间为 $[0,100]$ ，所以这里实现的算法公式将为 $最高分 - (当前带宽 / 最高最高带宽 * 100)$，这样就保证了，带宽占用越大的机器，分数越低。</p>
<p>例如，最高带宽为200000，而当前Node带宽为140000，那么这个Node分数为：$max - \frac{140000}{200000}\times 100 = 100 - (0.7\times100)=30$</p>
<pre><code class="language-go">// 如果返回framework.ScoreExtensions 就需要实现framework.ScoreExtensions
func (n *NetworkTraffic) ScoreExtensions() framework.ScoreExtensions {
	return n
}

// NormalizeScore与ScoreExtensions是固定格式
func (n *NetworkTraffic) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *corev1.Pod, scores framework.NodeScoreList) *framework.Status {
	var higherScore int64
	for _, node := range scores {
		if higherScore &lt; node.Score {
			higherScore = node.Score
		}
	}
	// 计算公式为，满分 - (当前带宽 / 最高最高带宽 * 100)
	// 公式的计算结果为，带宽占用越大的机器，分数越低
	for i, node := range scores {
		scores[i].Score = framework.MaxNodeScore - (node.Score * 100 / higherScore)
		klog.Infof(&quot;[NetworkTraffic] Nodes final score: %v&quot;, scores)
	}

	klog.Infof(&quot;[NetworkTraffic] Nodes final score: %v&quot;, scores)
	return nil
}
</code></pre>
<blockquote>
<p>Notes：在kubernetes中最大的node数支持5000个，岂不是在获取最大分数时循环就占用了大量的性能，其实不必担心。<em>scheduler</em> 提供了一个参数 <code>percentageOfNodesToScore</code>。这个参数决定了这里要循环的数量。更多的细节可以参考官方文档对这部分的说明 <sup><a href="#6">[6]</a></sup></p>
</blockquote>
<p><strong>配置插件名称</strong></p>
<p>为了使插件注册时候使用，还需要为其配置一个名称</p>
<pre><code class="language-go">// Name returns name of the plugin. It is used in logs, etc.
func (n *NetworkTraffic) Name() string {
	return Name
}
</code></pre>
<h4 id="定义prometheushandle">定义PrometheusHandle</h4>
<p>网络插件的扩展中还存在一个 <code>prometheusHandle</code>，这个就是操作prometheus-server拿去指标的动作。</p>
<p>首先需要定义一个 <em>PrometheusHandle</em> 的结构体</p>
<pre><code class="language-go">type PrometheusHandle struct {
	deviceName string // 网络接口名称
	timeRange  time.Duration // 抓取的时间段
	ip         string // prometheus server的连接地址
	client     v1.API // 操作prometheus的客户端
}
</code></pre>
<p>有了结构就需要查询的动作和指标，对于指标来说，这里使用了 <code>node_network_receive_bytes_total</code> 作为获取Node的网络流量的计算方式。由于环境是部署在集群之外的，没有node的主机名，通过 <code>promQL</code> 获取，整个语句如下：</p>
<pre><code class="language-bash">sum_over_time(node_network_receive_bytes_total{device=&quot;eth0&quot;}[1s]) * on(instance) group_left(nodename) (node_uname_info{nodename=&quot;node01&quot;})
</code></pre>
<p>整个 <em>Prometheus</em> 部分如下：</p>
<pre><code class="language-go">type PrometheusHandle struct {
	deviceName string
	timeRange  time.Duration
	ip         string
	client     v1.API
}

func NewProme(ip, deviceName string, timeRace time.Duration) *PrometheusHandle {
	client, err := api.NewClient(api.Config{Address: ip})
	if err != nil {
		klog.Fatalf(&quot;[NetworkTraffic] FatalError creating prometheus client: %s&quot;, err.Error())
	}
	return &amp;PrometheusHandle{
		deviceName: deviceName,
		ip:         ip,
		timeRange:  timeRace,
		client:     v1.NewAPI(client),
	}
}

func (p *PrometheusHandle) GetGauge(node string) (*model.Sample, error) {
	value, err := p.query(fmt.Sprintf(nodeMeasureQueryTemplate, node, p.deviceName, p.timeRange))
	fmt.Println(fmt.Sprintf(nodeMeasureQueryTemplate, p.deviceName, p.timeRange, node))
	if err != nil {
		return nil, fmt.Errorf(&quot;[NetworkTraffic] Error querying prometheus: %w&quot;, err)
	}

	nodeMeasure := value.(model.Vector)
	if len(nodeMeasure) != 1 {
		return nil, fmt.Errorf(&quot;[NetworkTraffic] Invalid response, expected 1 value, got %d&quot;, len(nodeMeasure))
	}
	return nodeMeasure[0], nil
}

func (p *PrometheusHandle) query(promQL string) (model.Value, error) {
    // 通过promQL查询并返回结果
	results, warnings, err := p.client.Query(context.Background(), promQL, time.Now())
	if len(warnings) &gt; 0 {
		klog.Warningf(&quot;[NetworkTraffic Plugin] Warnings: %v\n&quot;, warnings)
	}

	return results, err
}
</code></pre>
<h4 id="定义调度器传入的参数">定义调度器传入的参数</h4>
<p>因为需要指定 <em>prometheus</em> 的地址，网卡名称，和获取数据的大小，故整个结构体如下，另外，参数结构必须遵循<code>&lt;Plugin Name&gt;Args</code> 格式的名称。</p>
<pre><code class="language-go">type NetworkTrafficArgs struct {
	IP         string `json:&quot;ip&quot;`
	DeviceName string `json:&quot;deviceName&quot;`
	TimeRange  int    `json:&quot;timeRange&quot;`
}
</code></pre>
<p>为了使这个类型的数据作为 <code>KubeSchedulerConfiguration</code> 可以解析的结构，还需要做一步操作，就是在扩展APIServer时扩展对应的资源类型。在这里kubernetes中提供两种方法来扩展 <code>KubeSchedulerConfiguration</code> 的资源类型。</p>
<p>一种是旧版中提供了 <a href="https://github.com/kubernetes/kubernetes/blob/7a98bb2b7c9112935387825f2fce1b7d40b76236/pkg/scheduler/framework/plugins/nodelabel/node_label.go#L65-L80" target="_blank"
   rel="noopener nofollow noreferrer" >framework.DecodeInto</a> 函数可以做这个操作</p>
<pre><code class="language-go">func New(plArgs *runtime.Unknown, handle framework.FrameworkHandle) (framework.Plugin, error) {
	args := Args{}
	if err := framework.DecodeInto(plArgs, &amp;args); err != nil {
		return nil, err
	}
	...
}
</code></pre>
<p>另外一种方式是必须实现对应的深拷贝方法，例如 <a href="https://github.com/kubernetes/kubernetes/blob/e37e4ab4cc8dcda84f1344dda47a97bb1927d074/pkg/scheduler/apis/config/types_pluginargs.go#L37-L49" target="_blank"
   rel="noopener nofollow noreferrer" >NodeLabel</a> 中的</p>
<pre><code class="language-go">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// NodeLabelArgs holds arguments used to configure the NodeLabel plugin.
type NodeLabelArgs struct {
	metav1.TypeMeta

	// PresentLabels should be present for the node to be considered a fit for hosting the pod
	PresentLabels []string
	// AbsentLabels should be absent for the node to be considered a fit for hosting the pod
	AbsentLabels []string
	// Nodes that have labels in the list will get a higher score.
	PresentLabelsPreference []string
	// Nodes that don't have labels in the list will get a higher score.
	AbsentLabelsPreference []string
}
</code></pre>
<p>最后将其注册到register中，整个行为与扩展APIServer是类似的</p>
<pre><code class="language-go">// addKnownTypes registers known types to the given scheme
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&amp;KubeSchedulerConfiguration{},
		&amp;Policy{},
		&amp;InterPodAffinityArgs{},
		&amp;NodeLabelArgs{},
		&amp;NodeResourcesFitArgs{},
		&amp;PodTopologySpreadArgs{},
		&amp;RequestedToCapacityRatioArgs{},
		&amp;ServiceAffinityArgs{},
		&amp;VolumeBindingArgs{},
		&amp;NodeResourcesLeastAllocatedArgs{},
		&amp;NodeResourcesMostAllocatedArgs{},
	)
	scheme.AddKnownTypes(schema.GroupVersion{Group: &quot;&quot;, Version: runtime.APIVersionInternal}, &amp;Policy{})
	return nil
}
</code></pre>
<blockquote>
<p>Notes：对于生成深拷贝函数及其他文件，可以使用 kubernetes 代码库中的脚本 <a href="https://github.com/kubernetes/kubernetes/blob/v1.24.3/hack/update-codegen.sh" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/hack/update-codegen.sh</a></p>
</blockquote>
<p>这里为了方便使用了 <em>framework.DecodeInto</em> 的方式。</p>
<h4 id="项目部署">项目部署</h4>
<p>准备 scheduler 的 profile，可以看到，我们自定义的参数，就可以被识别为 <em>KubeSchedulerConfiguration</em> 的资源类型了。</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: /mnt/d/src/go_work/customScheduler/scheduler.conf
profiles:
- schedulerName: custom-scheduler
  plugins:
    score:
      enabled:
      - name: &quot;NetworkTraffic&quot;
      disabled:
      - name: &quot;*&quot;
  pluginConfig:
    - name: &quot;NetworkTraffic&quot;
      args:
        ip: &quot;http://10.0.0.4:9090&quot;
        deviceName: &quot;eth0&quot;
        timeRange: 60
</code></pre>
<p>如果需要部署到集群内部，可以打包成镜像</p>
<pre><code class="language-dockerfile">FROM golang:alpine AS builder
MAINTAINER cylon
WORKDIR /scheduler
COPY ./ /scheduler
ENV GOPROXY https://goproxy.cn,direct
RUN \
    sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories &amp;&amp; \
    apk add upx  &amp;&amp; \
    GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -ldflags &quot;-s -w&quot; -o scheduler main.go &amp;&amp; \
    upx -1 scheduler &amp;&amp; \
    chmod +x scheduler

FROM alpine AS runner
WORKDIR /go/scheduler
COPY --from=builder /scheduler/scheduler .
COPY --from=builder /scheduler/scheduler.yaml /etc/
VOLUME [&quot;./scheduler&quot;]
</code></pre>
<p>部署在集群内部所需的资源清单</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: scheduler-sa
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: scheduler
subjects:
  - kind: ServiceAccount
    name: scheduler-sa
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: custom-scheduler
  namespace: kube-system
  labels:
    component: custom-scheduler
spec:
  selector:
    matchLabels:
      component: custom-scheduler
  template:
    metadata:
      labels:
        component: custom-scheduler
    spec:
      serviceAccountName: scheduler-sa
      priorityClassName: system-cluster-critical
      containers:
        - name: scheduler
          image: cylonchau/custom-scheduler:v0.0.1
          imagePullPolicy: IfNotPresent
          command:
            - ./scheduler
            - --config=/etc/scheduler.yaml
            - --v=3
          livenessProbe:
            httpGet:
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
          readinessProbe:
            httpGet:
              path: /healthz
              port: 10251
</code></pre>
<p>启动自定义 <em>scheduler</em>，这里通过简单的二进制方式启动，所以需要一个kubeconfig做认证文件</p>
<pre><code class="language-bash">./main --logtostderr=true \
	--address=127.0.0.1 \
	--v=3 \
	--config=`pwd`/scheduler.yaml \
	--kubeconfig=`pwd`/scheduler.conf
</code></pre>
<p>启动后为了验证方便性，关闭了原来的 <em>kube-scheduler</em> 服务，因为原来的  <em>kube-scheduler</em> 已经作为HA中的master，所以不会使用自定义的 <em>scheduler</em> 导致pod pending。</p>
<h4 id="验证结果">验证结果</h4>
<p>准备一个需要部署的Pod，指定使用的调度器名称</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
      schedulerName: custom-scheduler
</code></pre>
<p>这里实验环境为2个节点的kubernetes集群，master与node01，因为master的服务比node01要多，这种情况下不管怎样，调度结果永远会被调度到node01上。</p>
<pre><code class="language-bash">$ kubectl get pods -o wide
NAME                                READY   STATUS    RESTARTS   AGE   IP             NODE     NOMINATED NODE   READINESS GATES
nginx-deployment-69f76b454c-lpwbl   1/1     Running   0          43s   192.168.0.17   node01   &lt;none&gt;           &lt;none&gt;
nginx-deployment-69f76b454c-vsb7k   1/1     Running   0          43s   192.168.0.16   node01   &lt;none&gt;           &lt;none&gt;
</code></pre>
<p>而调度器的日志如下</p>
<pre><code class="language-log">I0808 01:56:31.098189   27131 networktraffic.go:83] [NetworkTraffic] node 'node01' bandwidth: %!s(int64=12541068340)
I0808 01:56:31.098461   27131 networktraffic.go:70] [NetworkTraffic] Nodes final score: [{master-machine 0} {node01 12541068340}]
I0808 01:56:31.098651   27131 networktraffic.go:70] [NetworkTraffic] Nodes final score: [{master-machine 0} {node01 71}]
I0808 01:56:31.098911   27131 networktraffic.go:73] [NetworkTraffic] Nodes final score: [{master-machine 0} {node01 71}]
I0808 01:56:31.099275   27131 default_binder.go:51] Attempting to bind default/nginx-deployment-69f76b454c-vsb7k to node01
I0808 01:56:31.101414   27131 eventhandlers.go:225] add event for scheduled pod default/nginx-deployment-69f76b454c-lpwbl
I0808 01:56:31.101414   27131 eventhandlers.go:205] delete event for unscheduled pod default/nginx-deployment-69f76b454c-lpwbl
I0808 01:56:31.103604   27131 scheduler.go:609] &quot;Successfully bound pod to node&quot; pod=&quot;default/nginx-deployment-69f76b454c-lpwbl&quot; node=&quot;no
de01&quot; evaluatedNodes=2 feasibleNodes=2
I0808 01:56:31.104540   27131 scheduler.go:609] &quot;Successfully bound pod to node&quot; pod=&quot;default/nginx-deployment-69f76b454c-vsb7k&quot; node=&quot;no
de01&quot; evaluatedNodes=2 feasibleNodes=2
</code></pre>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/reference/scheduling/config/" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling config</a></p>
<p><sup id="2">[2]</sup> <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank"
   rel="noopener nofollow noreferrer" >kube-scheduler</a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/reference/scheduling/config/#scheduling-plugins" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling-plugins</a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md#custom-scheduler-plugins-out-of-tree" target="_blank"
   rel="noopener nofollow noreferrer" >custom scheduler plugins</a></p>
<p><sup id="5">[5]</sup> <a href="https://github.com/kubernetes/kubernetes/issues/79384" target="_blank"
   rel="noopener nofollow noreferrer" >ssues #79384</a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/scheduler-perf-tuning/" target="_blank"
   rel="noopener nofollow noreferrer" >scheduler perf tuning</a></p>
<p><sup id="7">[7]</sup> <a href="https://medium.com/@juliorenner123/k8s-creating-a-kube-scheduler-plugin-8a826c486a1" target="_blank"
   rel="noopener nofollow noreferrer" >creating a kube-scheduler plugin</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>如何理解kubernetes调度框架与插件？</title>
      <link>https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/</link>
      <pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="调度框架-supa-href11asup">调度框架 <sup><a href="#1">[1]</a></sup></h2>
<p>本文基于 <a href="https://github.com/kubernetes/kubernetes/tree/release-1.24/pkg/scheduler" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes 1.24</a> 进行分析</p>
<p>调度框架（<code>Scheduling Framework</code>）是Kubernetes 的调度器 <code>kube-scheduler</code> 设计的的可插拔架构，将插件（调度算法）嵌入到调度上下文的每个扩展点中，并编译为 <code>kube-scheduler</code></p>
<p>在 <code>kube-scheduler</code> 1.22 之后，在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/interface.go" target="_blank"
   rel="noopener nofollow noreferrer" >pkg/scheduler/framework/interface.go</a> 中定义了一个 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/interface.go#L295-L297" target="_blank"
   rel="noopener nofollow noreferrer" >Plugin</a> 的 <em>interface</em>，这个 <em>interface</em> 作为了所有插件的父级。而每个未调度的 Pod，Kubernetes 调度器会根据一组规则尝试在集群中寻找一个节点。</p>
<pre><code class="language-go">type Plugin interface {
	Name() string
}
</code></pre>
<p>下面会对每个算法是如何实现的进行分析</p>
<p>在初始化 <em>scheduler</em> 时，会创建一个 <code>profile</code>，profile是关于 <em>scheduler</em> 调度配置相关的定义</p>
<pre><code class="language-go">func New(client clientset.Interface,
...
	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)
	if err != nil {
		return nil, fmt.Errorf(&quot;initializing profiles: %v&quot;, err)
	}

	if len(profiles) == 0 {
		return nil, errors.New(&quot;at least one profile is required&quot;)
	}
....
}
</code></pre>
<p>关于 <code>profile</code> 的实现，则为 <code>KubeSchedulerProfile</code>，也是作为 yaml生成时传入的配置</p>
<pre><code class="language-go">// KubeSchedulerProfile 是一个 scheduling profile.
type KubeSchedulerProfile struct {
	// SchedulerName 是与此配置文件关联的调度程序的名称。
    // 如果 SchedulerName 与 pod “spec.schedulerName”匹配，则使用此配置文件调度 pod。
	SchedulerName string

	// Plugins指定应该启用或禁用的插件集。
    // 启用的插件是除了默认插件之外应该启用的插件。禁用插件应是禁用的任何默认插件。
    // 当没有为扩展点指定启用或禁用插件时，将使用该扩展点的默认插件（如果有）。
    // 如果指定了 QueueSort 插件，
    // 则必须为所有配置文件指定相同的 QueueSort Plugin 和 PluginConfig。
    // 这个Plugins展现的形式则是调度上下文中的所有扩展点(这是抽象)，实际中会表现为多个扩展点
	Plugins *Plugins

	// PluginConfig 是每个插件的一组可选的自定义插件参数。
    // 如果省略PluginConfig参数等同于使用该插件的默认配置。
	PluginConfig []PluginConfig
}
</code></pre>
<p>对于 <code>profile.NewMap</code> 就是根据给定的配置来构建这个framework，因为配置可能是存在多个的。而 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/registry.go#L70" target="_blank"
   rel="noopener nofollow noreferrer" >Registry</a> 则是所有可用插件的集合，内部构造则是 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/registry.go#L30" target="_blank"
   rel="noopener nofollow noreferrer" >PluginFactory</a> ,通过函数来构建出对应的 plugin</p>
<pre><code class="language-go">func NewMap(cfgs []config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory,
	stopCh &lt;-chan struct{}, opts ...frameworkruntime.Option) (Map, error) {
	m := make(Map)
	v := cfgValidator{m: m}

	for _, cfg := range cfgs {
		p, err := newProfile(cfg, r, recorderFact, stopCh, opts...)
		if err != nil {
			return nil, fmt.Errorf(&quot;creating profile for scheduler name %s: %v&quot;, cfg.SchedulerName, err)
		}
		if err := v.validate(cfg, p); err != nil {
			return nil, err
		}
		m[cfg.SchedulerName] = p
	}
	return m, nil
}

// newProfile 给的配置构建出一个profile
func newProfile(cfg config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory,
	stopCh &lt;-chan struct{}, opts ...frameworkruntime.Option) (framework.Framework, error) {
	recorder := recorderFact(cfg.SchedulerName)
	opts = append(opts, frameworkruntime.WithEventRecorder(recorder))
	return frameworkruntime.NewFramework(r, &amp;cfg, stopCh, opts...)
}

</code></pre>
<p>可以看到最终返回的是一个 <code>Framework</code> 。那么来看下这个 <code>Framework</code></p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/interface.go#L495-L566" target="_blank"
   rel="noopener nofollow noreferrer" >Framework</a> 是一个抽象，管理着调度过程中所使用的所有插件，并在调度上下文中适当的位置去运行对应的插件</p>
<pre><code class="language-go">type Framework interface {
	Handle
	// QueueSortFunc 返回对调度队列中的 Pod 进行排序的函数
    // 也就是less，在Sort打分阶段的打分函数
	QueueSortFunc() LessFunc
    
    // RunPreFilterPlugins 运行配置的一组PreFilter插件。
    // 如果这组插件中，任何一个插件失败，则返回 *Status 并设置为non-success。
    // 如果返回状态为non-success，则调度周期中止。
    // 它还返回一个 PreFilterResult，它可能会影响到要评估下游的节点。
    
	RunPreFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod) (*PreFilterResult, *Status)

    // RunPostFilterPlugins 运行配置的一组PostFilter插件。 
    // PostFilter 插件是通知性插件，在这种情况下应配置为先执行并返回 Unschedulable 状态，
    // 或者尝试更改集群状态以使 pod 在未来的调度周期中可能会被调度。
	RunPostFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, filteredNodeStatusMap NodeToStatusMap) (*PostFilterResult, *Status)

    // RunPreBindPlugins 运行配置的一组 PreBind 插件。
    // 如果任何一个插件返回错误，则返回 *Status 并且code设置为non-success。
    // 如果code为“Unschedulable”，则调度检查失败，
    // 则认为是内部错误。在任何一种情况下，Pod都不会被bound。
	RunPreBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

    // RunPostBindPlugins 运行配置的一组PostBind插件
	RunPostBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string)

    // RunReservePluginsReserve运行配置的一组Reserve插件的Reserve方法。
    // 如果在这组调用中的任何一个插件返回错误，则不会继续运行剩余调用的插件并返回错误。
    // 在这种情况下，pod将不能被调度。
	RunReservePluginsReserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

    // RunReservePluginsUnreserve运行配置的一组Reserve插件的Unreserve方法。
	RunReservePluginsUnreserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string)

    // RunPermitPlugins运行配置的一组Permit插件。
    // 如果这些插件中的任何一个返回“Success”或“Wait”之外的状态，则它不会继续运行其余插件并返回错误。
    // 否则，如果任何插件返回 “Wait”，则此函数将创建等待pod并将其添加到当前等待pod的map中，
    // 并使用“Wait” code返回状态。 Pod将在Permit插件返回的最短持续时间内保持等待pod。
	RunPermitPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

    // 如果pod是waiting pod，WaitOnPermit 将阻塞，直到等待的pod被允许或拒绝。
	WaitOnPermit(ctx context.Context, pod *v1.Pod) *Status

    // RunBindPlugins运行配置的一组bind插件。 Bind插件可以选择是否处理Pod。
    // 如果 Bind 插件选择跳过binding，它应该返回 code=5(&quot;skip&quot;)状态。
    // 否则，它应该返回“Error”或“Success”。
    // 如果没有插件处理绑定，则RunBindPlugins返回code=5(&quot;skip&quot;)的状态。
	RunBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status

	// 如果至少定义了一个filter插件，则HasFilterPlugins返回true
	HasFilterPlugins() bool

    // 如果至少定义了一个PostFilter插件，则HasPostFilterPlugins返回 true。
	HasPostFilterPlugins() bool

	// 如果至少定义了一个Score插件，则HasScorePlugins返回 true。
	HasScorePlugins() bool

    // ListPlugins将返回map。key为扩展点名称，value则是配置的插件列表。
	ListPlugins() *config.Plugins

    // ProfileName则是与profile name关联的framework
	ProfileName() string
}
</code></pre>
<p>而实现这个抽象的则是 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/framework.go#L73-L102" target="_blank"
   rel="noopener nofollow noreferrer" >frameworkImpl</a>；<code>frameworkImpl</code> 是初始化与运行 <em>scheduler plugins</em> 的组件，并在调度上下文中会运行这些扩展点</p>
<pre><code>type frameworkImpl struct {
   registry             Registry
   snapshotSharedLister framework.SharedLister
   waitingPods          *waitingPodsMap
   scorePluginWeight    map[string]int
   queueSortPlugins     []framework.QueueSortPlugin
   preFilterPlugins     []framework.PreFilterPlugin
   filterPlugins        []framework.FilterPlugin
   postFilterPlugins    []framework.PostFilterPlugin
   preScorePlugins      []framework.PreScorePlugin
   scorePlugins         []framework.ScorePlugin
   reservePlugins       []framework.ReservePlugin
   preBindPlugins       []framework.PreBindPlugin
   bindPlugins          []framework.BindPlugin
   postBindPlugins      []framework.PostBindPlugin
   permitPlugins        []framework.PermitPlugin

   clientSet       clientset.Interface
   kubeConfig      *restclient.Config
   eventRecorder   events.EventRecorder
   informerFactory informers.SharedInformerFactory

   metricsRecorder *metricsRecorder
   profileName     string

   extenders []framework.Extender
   framework.PodNominator

   parallelizer parallelize.Parallelizer
}
</code></pre>
<p>那么来看下 Registry ，<code>Registry </code> 是作为一个可用插件的集合。<code>framework</code> 使用 <code>registry</code> 来启用和对插件配置的初始化。在初始化框架之前，所有插件都必须在注册表中。表现形式就是一个 <code>map[]</code>；<em>key</em> 是插件的名称，value是 <code>PluginFactory</code> 。</p>
<pre><code class="language-go">type Registry map[string]PluginFactory
</code></pre>
<p>而在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/plugins/registry.go" target="_blank"
   rel="noopener nofollow noreferrer" >pkg\scheduler\framework\plugins\registry.go</a> 中会将所有的 <code>in-tree plugin</code> 注册进来。通过 <code>NewInTreeRegistry</code> 。后续如果还有插件要注册，可以通过 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/scheduler.go#L176-L180" target="_blank"
   rel="noopener nofollow noreferrer" >WithFrameworkOutOfTreeRegistry</a>  来注册其他的插件。</p>
<pre><code class="language-go">func NewInTreeRegistry() runtime.Registry {
	fts := plfeature.Features{
		EnableReadWriteOncePod:                       feature.DefaultFeatureGate.Enabled(features.ReadWriteOncePod),
		EnableVolumeCapacityPriority:                 feature.DefaultFeatureGate.Enabled(features.VolumeCapacityPriority),
		EnableMinDomainsInPodTopologySpread:          feature.DefaultFeatureGate.Enabled(features.MinDomainsInPodTopologySpread),
		EnableNodeInclusionPolicyInPodTopologySpread: feature.DefaultFeatureGate.Enabled(features.NodeInclusionPolicyInPodTopologySpread),
	}

	return runtime.Registry{
		selectorspread.Name:                  selectorspread.New,
		imagelocality.Name:                   imagelocality.New,
		tainttoleration.Name:                 tainttoleration.New,
		nodename.Name:                        nodename.New,
		nodeports.Name:                       nodeports.New,
		nodeaffinity.Name:                    nodeaffinity.New,
		podtopologyspread.Name:               runtime.FactoryAdapter(fts, podtopologyspread.New),
		nodeunschedulable.Name:               nodeunschedulable.New,
		noderesources.Name:                   runtime.FactoryAdapter(fts, noderesources.NewFit),
		noderesources.BalancedAllocationName: runtime.FactoryAdapter(fts, noderesources.NewBalancedAllocation),
		volumebinding.Name:                   runtime.FactoryAdapter(fts, volumebinding.New),
		volumerestrictions.Name:              runtime.FactoryAdapter(fts, volumerestrictions.New),
		volumezone.Name:                      volumezone.New,
		nodevolumelimits.CSIName:             runtime.FactoryAdapter(fts, nodevolumelimits.NewCSI),
		nodevolumelimits.EBSName:             runtime.FactoryAdapter(fts, nodevolumelimits.NewEBS),
		nodevolumelimits.GCEPDName:           runtime.FactoryAdapter(fts, nodevolumelimits.NewGCEPD),
		nodevolumelimits.AzureDiskName:       runtime.FactoryAdapter(fts, nodevolumelimits.NewAzureDisk),
		nodevolumelimits.CinderName:          runtime.FactoryAdapter(fts, nodevolumelimits.NewCinder),
		interpodaffinity.Name:                interpodaffinity.New,
		queuesort.Name:                       queuesort.New,
		defaultbinder.Name:                   defaultbinder.New,
		defaultpreemption.Name:               runtime.FactoryAdapter(fts, defaultpreemption.New),
	}
}
</code></pre>
<blockquote>
<p>这里插入一个题外话，关于 <em>in-tree plugin</em></p>
<p>在这里没有找到关于，<em>kube-scheduler</em> ，只是找到有关的概念，大概可以解释为，in-tree表示为随kubernetes官方提供的二进制构建的 <em>plugin</em> 则为 <code>in-tree</code>，而独立于kubernetes代码库之外的为 <code>out-of-tree</code> <sup><a href="#3">[3]</a></sup> 。这种情况下，可以理解为，AA则是 <code>out-of-tree</code> 而 <code>Pod</code>, <code>DeplymentSet</code> 等是 <code>in-tree</code>。</p>
</blockquote>
<p>接下来回到初始化 <em>scheduler</em> ，在初始化一个 <em>scheduler</em> 时，会通过<code>NewInTreeRegistry</code> 来初始化</p>
<pre><code class="language-go">func New(client clientset.Interface,
	....
	registry := frameworkplugins.NewInTreeRegistry()
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}
         
	...

	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)
	...
}
</code></pre>
<p>接下来在调度上下文 <code>scheduleOne</code> 中 <code>schedulePod</code> 时，会通过 <code>framework</code> 调用对应的插件来处理这个扩展点工作。具体的体现在，pkg\scheduler\schedule_one.go 中的预选阶段</p>
<pre><code class="language-go">func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New(&quot;Scheduling&quot;, utiltrace.Field{Key: &quot;namespace&quot;, Value: pod.Namespace}, utiltrace.Field{Key: &quot;name&quot;, Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)

	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
		return result, err
	}
	trace.Step(&quot;Snapshotting scheduler cache and node infos done&quot;)

	if sched.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}

	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step(&quot;Computing predicates done&quot;)

</code></pre>
<p>与其他扩展点部分，在调度上下文 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L66-L278" target="_blank"
   rel="noopener nofollow noreferrer" >scheduleOne</a> 中可以很好的看出，功能都是 <code>framework</code> 提供的。</p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {

    ...
    
	scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)

    ...
    
	// Run the Reserve method of reserve plugins.
	if sts := fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {
	}

    ...
    
	// Run &quot;permit&quot; plugins.
	runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
	
		// One of the plugins returned status different than success or wait.
		fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)

...
    
	// bind the pod to its host asynchronously (we can do this b/c of the assumption step above).
	go func() {
		...
		waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod)
		if !waitOnPermitStatus.IsSuccess() {
			...
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		}

		// Run &quot;prebind&quot; plugins.
		preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		
        ...
        
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
	
        ...

		...
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			
        ...

		// Run &quot;postbind&quot; plugins.
		fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)

	...
}
</code></pre>
<h2 id="插件-supa-href44asup">插件 <sup><a href="#4">[4]</a></sup></h2>
<p>插件（<code>Plugins</code>）（也可以算是调度策略）在 <code>kube-scheduler</code> 中的实现为 <code>framework plugin</code>，插件API的实现分为两个步骤**：register** 和 <strong>configured</strong>，然后都实现了其父方法 <code>Plugin</code>。然后可以通过配置（kube-scheduler <code>--config</code> 提供）启动或禁用插件；除了默认插件外，还可以实现自定义调度插件与默认插件进行绑定。</p>
<pre><code class="language-go">type Plugin interface {
    Name() string
}
// sort扩展点
type QueueSortPlugin interface {
    Plugin
    Less(*v1.pod, *v1.pod) bool
}
// PreFilter扩展点
type PreFilterPlugin interface {
    Plugin
    PreFilter(context.Context, *framework.CycleState, *v1.pod) error
}

</code></pre>
<h3 id="插件的载入过程">插件的载入过程</h3>
<p>在 <em>scheduler</em> 被启动时，会 <code>scheduler.New(cc.Client..</code> 这个时候会传入 <code>profiles</code>，整个的流如下：</p>
<ul>
<li><code>NewScheduler</code> ：<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/cmd/kube-scheduler/app/server.go#L327-L346" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/cmd/kube-scheduler/app/server.go</a></li>
<li><code>profile.NewMap</code>：<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/profile/profile.go#L48-L64" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/pkg/scheduler/scheduler.go</a>
<ul>
<li><code>newProfile</code>：<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/profile/profile.go#L37-L42" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes/pkg/scheduler/scheduler.go</a></li>
</ul>
</li>
<li><code>frameworkruntime.NewFramework</code>：<a href="kubernetes/pkg/scheduler/framework/runtime/framework.go">kubernetes/pkg/scheduler/framework/runtime/framework.go</a>
<ul>
<li><code>pluginsNeeded</code>：<a href="kubernetes/pkg/scheduler/framework/runtime/framework.go">kubernetes/pkg/scheduler/framework/runtime/framework.go</a></li>
</ul>
</li>
</ul>
<h4 id="newscheduler">NewScheduler</h4>
<p>我们了解如何 New 一个 <em>scheduler</em> 即为 <code>Setup</code> 中去配置这些参数，</p>
<pre><code class="language-go">func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) {

    ...
    
	// Create the scheduler.
	sched, err := scheduler.New(cc.Client,
		cc.InformerFactory,
		cc.DynInformerFactory,
		recorderFactory,
		ctx.Done(),
		scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion),
		scheduler.WithKubeConfig(cc.KubeConfig),
		scheduler.WithProfiles(cc.ComponentConfig.Profiles...),
		scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore),
		scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry),
		scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds),
		scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds),
		scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration),
		scheduler.WithExtenders(cc.ComponentConfig.Extenders...),
		scheduler.WithParallelism(cc.ComponentConfig.Parallelism),
		scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) {
			// Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging
			completedProfiles = append(completedProfiles, profile)
		}),
	)
    ...
}
</code></pre>
<h4 id="profilenewmap">profile.NewMap</h4>
<p>在 <code>scheduler.New</code> 中，会根据配置生成profile，而 <code>profile.NewMap</code> 会完成这一步</p>
<pre><code class="language-go">func New(client clientset.Interface,
	...
         
	clusterEventMap := make(map[framework.ClusterEvent]sets.String)

	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)

         ...
}
</code></pre>
<h4 id="newframework">NewFramework</h4>
<p><code>newProfile</code> 返回的则是一个创建好的 framework</p>
<pre><code class="language-go">func newProfile(cfg config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory,
	stopCh &lt;-chan struct{}, opts ...frameworkruntime.Option) (framework.Framework, error) {
	recorder := recorderFact(cfg.SchedulerName)
	opts = append(opts, frameworkruntime.WithEventRecorder(recorder))
	return frameworkruntime.NewFramework(r, &amp;cfg, stopCh, opts...)
}
</code></pre>
<p>最终会走到 <code>pluginsNeeded</code>，这里会根据配置中开启的插件而返回一个插件集，这个就是最终在每个扩展点中药执行的插件。</p>
<pre><code class="language-go">func (f *frameworkImpl) pluginsNeeded(plugins *config.Plugins) sets.String {
	pgSet := sets.String{}

	if plugins == nil {
		return pgSet
	}

	find := func(pgs *config.PluginSet) {
		for _, pg := range pgs.Enabled {
			pgSet.Insert(pg.Name)
		}
	}
	// 获取到所有的扩展点，找到为Enabled的插件加入到pgSet
	for _, e := range f.getExtensionPoints(plugins) {
		find(e.plugins)
	}
	// Parse MultiPoint separately since they are not returned by f.getExtensionPoints()
	find(&amp;plugins.MultiPoint)

	return pgSet
}
</code></pre>
<h3 id="插件的执行">插件的执行</h3>
<p>在对插件源码部分分析，会找几个典型的插件进行分析，而不会对全部的进行分析，因为总的来说是大同小异，分析的插件有 <code>NodePorts</code>，<code>NodeResourcesFit</code>，<code>podtopologyspread</code></p>
<h4 id="nodeports">NodePorts</h4>
<p>这里以一个简单的插件来分析；<code>NodePorts</code> 插件用于检查Pod请求的端口，在节点上是否为空闲端口。</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L30" target="_blank"
   rel="noopener nofollow noreferrer" >NodePorts</a> 实现了 <code>FilterPlugin</code> 和 <code>PreFilterPlugin</code></p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L77-L81" target="_blank"
   rel="noopener nofollow noreferrer" >PreFilter</a>  将会被 <code>framework</code> 中 <code>PreFilter</code> 扩展点被调用。</p>
<pre><code class="language-go">func (pl *NodePorts) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	s := getContainerPorts(pod) // 或得Pod得端口
    // 写入状态
	cycleState.Write(preFilterStateKey, preFilterState(s))
	return nil, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L113-L125" target="_blank"
   rel="noopener nofollow noreferrer" >Filter</a> 将会被 <code>framework</code> 中 <code>Filter</code> 扩展点被调用。</p>
<pre><code class="language-go">// Filter invoked at the filter extension point.
func (pl *NodePorts) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
   wantPorts, err := getPreFilterState(cycleState)
   if err != nil {
      return framework.AsStatus(err)
   }

   fits := fitsPorts(wantPorts, nodeInfo)
   if !fits {
      return framework.NewStatus(framework.Unschedulable, ErrReason)
   }

   return nil
}

func fitsPorts(wantPorts []*v1.ContainerPort, nodeInfo *framework.NodeInfo) bool {
	// 对比existingPorts 和 wantPorts是否冲突，冲突则调度失败
	existingPorts := nodeInfo.UsedPorts
	for _, cp := range wantPorts {
		if existingPorts.CheckConflict(cp.HostIP, string(cp.Protocol), cp.HostPort) {
			return false
		}
	}
	return true
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/nodeports/node_ports.go#L144-L146" target="_blank"
   rel="noopener nofollow noreferrer" >New</a> ，初始化新插件，在 <code>register</code> 中注册得</p>
<pre><code class="language-go">func New(_ runtime.Object, _ framework.Handle) (framework.Plugin, error) {
	return &amp;NodePorts{}, nil
}
</code></pre>
<p>在调用中，如果有任何一个插件返回错误，则跳过该扩展点注册得其他插件，返回失败。</p>
<pre><code class="language-go">func (f *frameworkImpl) RunFilterPlugins(
	ctx context.Context,
	state *framework.CycleState,
	pod *v1.Pod,
	nodeInfo *framework.NodeInfo,
) framework.PluginToStatus {
	statuses := make(framework.PluginToStatus)
	for _, pl := range f.filterPlugins {
		pluginStatus := f.runFilterPlugin(ctx, pl, state, pod, nodeInfo)
		if !pluginStatus.IsSuccess() {
			if !pluginStatus.IsUnschedulable() 
				errStatus := framework.AsStatus(fmt.Errorf(&quot;running %q filter plugin: %w&quot;, pl.Name(), pluginStatus.AsError())).WithFailedPlugin(pl.Name())
				return map[string]*framework.Status{pl.Name(): errStatus}
			}
			pluginStatus.SetFailedPlugin(pl.Name())
			statuses[pl.Name()] = pluginStatus
		}
	}

	return statuses
}
</code></pre>
<p>返回得状态是一个 Status 结构体，该结构体表示了插件运行的结果。由 <code>Code</code>、<code>reasons</code>、（可选）<code>err</code> 和 <code>failedPlugin</code> （失败的那个插件名）组成。当 <em>code</em> 不是 <code>Success</code> 时，应说明原因。而且，当 <em>code</em> 为 <code>Success</code> 时，其他所有字段都应为空。<code>nil</code> 状态也被视为成功。</p>
<pre><code class="language-go">type Status struct {
	code    Code
	reasons []string
	err     error
	// failedPlugin is an optional field that records the plugin name a Pod failed by.
	// It's set by the framework when code is Error, Unschedulable or UnschedulableAndUnresolvable.
	failedPlugin string
}
</code></pre>
<h4 id="noderesourcesfit--supa-href55asup">NodeResourcesFit  <sup><a href="#5">[5]</a></sup></h4>
<p><code>NodeResourcesFit</code> 扩展检查节点是否拥有 Pod 请求的所有资源。分数可以使用以下三种策略之一，扩展点为：<code>preFilter</code>， <code>filter</code>，<code>score</code></p>
<ul>
<li><code>LeastAllocated</code> （默认）</li>
<li><code>MostAllocated</code></li>
<li><code>RequestedToCapacityRatio</code></li>
</ul>
<h4 id="fit">Fit</h4>
<p><code>NodeResourcesFit  </code> PreFilter 可以看到调用得 <code>computePodResourceRequest</code></p>
<pre><code>// PreFilter invoked at the prefilter extension point.
func (f *Fit) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
   cycleState.Write(preFilterStateKey, computePodResourceRequest(pod))
   return nil, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/noderesources/fit.go#L133-L175" target="_blank"
   rel="noopener nofollow noreferrer" >computePodResourceRequest</a> 这里有一个注释，总体解释起来是这样得：<code>computePodResourceRequest</code> ，返回值（ <code>framework.Resource</code>）覆盖了每一个维度中资源的最大宽度。因为将按照 <code>init-containers</code> , <code>containers</code> 得顺序运行，会通过迭代方式收集每个维度中的最大值。计算时会对常规容器的资源向量求和，因为<code>containers</code> 运行会同时运行多个容器。计算示例为：</p>
<pre><code class="language-yaml">Pod:
  InitContainers
    IC1:
      CPU: 2
      Memory: 1G
    IC2:
      CPU: 2
      Memory: 3G
  Containers
    C1:
      CPU: 2
      Memory: 1G
    C2:
      CPU: 1
      Memory: 1G
</code></pre>
<p>在维度1中（<code>InitContainers</code>）所需资源最大值时，CPU=2, Memory=3G；而维度2（<code>Containers</code>）所需资源最大值为：CPU=2, Memory=1G；那么最终结果为 CPU=3, Memory=3G，因为在维度1，最大资源时Memory=3G；而维度2最大资源是CPU=1+2, Memory=1+1，取每个维度中最大资源最大宽度即为 CPU=3, Memory=3G。</p>
<p>下面则看下代码得实现</p>
<pre><code class="language-go">func computePodResourceRequest(pod *v1.Pod) *preFilterState {
	result := &amp;preFilterState{}
	for _, container := range pod.Spec.Containers {
		result.Add(container.Resources.Requests)
	}

	// 取最大得资源
	for _, container := range pod.Spec.InitContainers {
		result.SetMaxResource(container.Resources.Requests)
	}

	// 如果Overhead正在使用，需要将其计算到总资源中
	if pod.Spec.Overhead != nil {
		result.Add(pod.Spec.Overhead)
	}
	return result
}

// SetMaxResource 是比较ResourceList并为每个资源取最大值。
func (r *Resource) SetMaxResource(rl v1.ResourceList) {
	if r == nil {
		return
	}

	for rName, rQuantity := range rl {
		switch rName {
		case v1.ResourceMemory:
			r.Memory = max(r.Memory, rQuantity.Value())
		case v1.ResourceCPU:
			r.MilliCPU = max(r.MilliCPU, rQuantity.MilliValue())
		case v1.ResourceEphemeralStorage:
			if utilfeature.DefaultFeatureGate.Enabled(features.LocalStorageCapacityIsolation) {
				r.EphemeralStorage = max(r.EphemeralStorage, rQuantity.Value())
			}
		default:
			if schedutil.IsScalarResourceName(rName) {
				r.SetScalar(rName, max(r.ScalarResources[rName], rQuantity.Value()))
			}
		}
	}
}
</code></pre>
<h4 id="leastallocate">leastAllocate</h4>
<p>LeastAllocated 是 NodeResourcesFit 的打分策略 ，<code>LeastAllocated</code> 打分的标准是更偏向于请求资源较少的Node。将会先计算出Node上调度的pod请求的内存、CPU与其他资源的百分比，然后并根据请求的比例与容量的平均值的最小值进行优先级排序。</p>
<p>计算公式是这样的：$\frac{\frac{cpu((capacity-requested) \times MaxNodeScore \times cpuWeight)}{capacity} + \frac{memory((capacity-requested) \times MaxNodeScore \times memoryWeight}{capacity}) + &hellip;}{weightSum}$</p>
<p>下面来看下实现</p>
<pre><code class="language-go">func leastResourceScorer(resToWeightMap resourceToWeightMap) func(resourceToValueMap, resourceToValueMap) int64 {
	return func(requested, allocable resourceToValueMap) int64 {
		var nodeScore, weightSum int64
		for resource := range requested {
			weight := resToWeightMap[resource]
            //  计算出的资源分数乘weight
			resourceScore := leastRequestedScore(requested[resource], allocable[resource])
			nodeScore += resourceScore * weight
			weightSum += weight
		}
		if weightSum == 0 {
			return 0
		}
        // 最终除weightSum
		return nodeScore / weightSum
	}
}
</code></pre>
<p>leastRequestedScore 计算标准为<strong>未使用容量</strong>的计算范围为 <code>0~MaxNodeScore</code>，0 为最低优先级，<code>MaxNodeScore</code> 为最高优先级。未使用的资源越多，得分越高。</p>
<pre><code class="language-go">func leastRequestedScore(requested, capacity int64) int64 {
	if capacity == 0 {
		return 0
	}
	if requested &gt; capacity {
		return 0
	}
	// 容量 - 请求的 x 预期值（100）/ 容量
	return ((capacity - requested) * int64(framework.MaxNodeScore)) / capacity
}
</code></pre>
<h3 id="topology-supa-href66asup">Topology <sup><a href="#6">[6]</a></sup></h3>
<h4 id="concept">Concept</h4>
<p>在对 <code>podtopologyspread</code> 插件进行分析前，先需要掌握Pod拓扑的概念。</p>
<p>Pod拓扑（<code>Pod Topology</code>）是Kubernetes Pod调度机制，可以将Pod分布在集群中不同 <code>Zone</code> ，以及用户自定义的各种拓扑域 （<code>topology domains</code>）。当有了拓扑域后，用户可以更高效的利用集群资源。</p>
<p>如何来解释拓扑域，首先需要提及为什么需要拓扑域，在集群有3个节点，并且当Pod副本数为2时，又不希望两个Pod在同一个Node上运行。在随着扩大Pod的规模，副本数扩展到到15个时，这时候最理想的方式是每个Node运行5个Pod，在这种背景下，用户希望对集群中Zone的安排为相似的副本数量，并且在集群存在部分问题时可以更好的自愈（也是按照相似的副本数量均匀的分布在Node上）。在这种情况下Kubernetes 提供了Pod 拓扑约束来解决这个问题。</p>
<h4 id="定义一个topology">定义一个Topology</h4>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  # Configure a topology spread constraint
  topologySpreadConstraints:
    - maxSkew: &lt;integer&gt; # 
      minDomains: &lt;integer&gt; # optional; alpha since v1.24
      topologyKey: &lt;string&gt;
      whenUnsatisfiable: &lt;string&gt;
      labelSelector: &lt;object&gt;
</code></pre>
<p><strong>参数的描述</strong>：</p>
<ul>
<li><strong>maxSkew</strong>：Required，Pod分布不均的程度，并且数字必须大于零
<ul>
<li>当 <code>whenUnsatisfiable: DoNotSchedule</code>，则定义目标拓扑中匹配 pod 的数量与 <strong>全局最小值</strong>（<em>拓扑域中的标签选择器匹配的 pod 的最小数量</em> ）<code>maxSkew</code>之间的最大允许差异。例如有 3 个 <code>Zone</code>，分别具有 2、4 和 5 个匹配的 pod，则全局最小值为 2</li>
<li>当 <code>whenUnsatisfiable: ScheduleAnyway</code>，<em>scheduler</em> 会为减少倾斜的拓扑提供更高的优先级。</li>
</ul>
</li>
<li><strong>minDomains</strong>：optional，符合条件的域的最小数量。
<ul>
<li>如果不指定该选项 <code>minDomains</code>，则约束的行为 <code>minDomains: 1</code> 。</li>
<li><code>minDomains</code>必须大于 0。<code>minDomains</code>与 <code>whenUnsatisfiable</code> 一起时为<code>whenUnsatisfiable: DoNotSchedule</code>。</li>
</ul>
</li>
<li><strong>topologyKey</strong>：Node label的key，如果多个Node都使用了这个lable key那么 <em>scheduler</em> 将这些 Node 看作为相同的拓扑域。</li>
<li><strong>whenUnsatisfiable</strong>：当 Pod 不满足分布的约束时，怎么去处理
<ul>
<li><code>DoNotSchedule</code>（默认）不要调度。</li>
<li><code>ScheduleAnyway</code>仍然调度它，同时优先考虑最小化倾斜节点</li>
</ul>
</li>
<li><strong>labelSelector</strong>：查找匹配的 Pod label选择器的node进行技术，以计算Pod如何分布在拓扑域中</li>
</ul>
<h4 id="对于拓扑域的理解">对于拓扑域的理解</h4>
<p>对于拓扑域，官方是这么说明的，假设有一个带有以下lable的 4 节点集群：</p>
<pre><code class="language-bash">NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre>
<p>那么集群拓扑如图：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725223516451.png" alt="image-20220725223516451" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图1：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>假设一个 4 节点集群，其中 3个label被标记为<code>foo: bar</code>的 Pod 分别位于Node1、Node2 和 Node3：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725224602667.png" alt="image-20220725224602667" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图2：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>这种情况下，新部署一个Pod，并希望新Pod与现有Pod跨 <code>Zone</code>均匀分布，资源清单文件如下：</p>
<pre><code class="language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1
</code></pre>
<p>这个清单对于拓扑域来说，<code>topologyKey: zone</code> 表示对Pod均匀分布仅应用于已标记的节点（如 <code>foo: bar</code>），将会跳过没有标签的节点（如<code>zone: &lt;any value&gt;</code>）。如果 <em>scheduler</em> 找不到满足约束的方法，<code>whenUnsatisfiable: DoNotSchedule</code> 设置的策略则是 <em>scheduler</em> 对新部署的Pod保持 <code>Pendding</code></p>
<p>如果此时 <em>scheduler</em> 将新Pod 调度至 $Zone_A$，此时Pod分布在拓扑域间为 $[3,1]$ ，而 <code>maxSkew</code> 配置的值是1。此时倾斜值为 $Zone_A - Zone_B = 3-1=2$，不满足 <code>maxSkew=1</code>，故这个Pod只能被调度到 $Zone_B$。</p>
<p>此时Pod调度拓扑图为图3或图4</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725230358777.png" alt="image-20220725230358777" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图3：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725230515969.png" alt="image-20220725230515969" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图4：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>如果需要将Pod调度到 $Zone_A$ ,可以按照如下方式进行：</p>
<ul>
<li>修改 <code>maxSkew=2</code></li>
<li>修改 <code>topologyKey: node</code> 而不是 <code>Zone</code> ，这种模式下可以将 Pod 均匀分布在Node而不是Zone之间。</li>
<li>修改 <code>whenUnsatisfiable: DoNotSchedule</code> 为 <code>whenUnsatisfiable: ScheduleAnyway</code> 确保新的Pod始终可被调度</li>
</ul>
<p>下面再通过一个例子增强对拓扑域了解</p>
<p><strong>多拓扑约束</strong></p>
<p>设拥有一个 4 节点集群，其中 3 个现有 Pod 标记 <code>foo: bar </code>分别位于 <code>node1</code>、<code>node2</code> 和 <code>node3</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220725231905415.png" alt="image-20220725231905415" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图5：集群拓扑图</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</center>
<p>部署的资源清单如下：可以看出拓扑分布约束配置了多个</p>
<pre><code class="language-yaml">kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  - maxSkew: 1
    topologyKey: node
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: k8s.gcr.io/pause:3.1
</code></pre>
<p>在这种情况下，为了匹配第一个约束条件，新Pod 只能放置在 $Zone_B$ ；而就第二个约束条件，新Pod只能调度到 <code>node4</code>。在这种配置多约束条件下， <em>scheduler</em> 只考虑满足所有约束的值，因此唯一有效的是 <code>node4</code>。</p>
<h4 id="如何为集群设置一个默认拓扑域约束">如何为集群设置一个默认拓扑域约束</h4>
<p>默认情况下，拓扑域约束也作 <em>scheduler</em> 的为 <em>scheduler configurtion</em> 中的一部分参数，这也意味着，可以通过profile为整个集群级别指定一个默认的拓扑域调度约束，</p>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List
</code></pre>
<h4 id="默认约束策略">默认约束策略</h4>
<p>如果在没有配置集群级别的约束策略时，<em>kube-scheduler</em> 内部 <code>topologyspread</code> 插件提供了一个默认的拓扑约束策略，大致上如下列清单所示</p>
<pre><code>defaultConstraints:
  - maxSkew: 3
    topologyKey: &quot;kubernetes.io/hostname&quot;
    whenUnsatisfiable: ScheduleAnyway
  - maxSkew: 5
    topologyKey: &quot;topology.kubernetes.io/zone&quot;
    whenUnsatisfiable: ScheduleAnyway
</code></pre>
<p>上述清单中内容可以在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/plugin.go#L42-L53" target="_blank"
   rel="noopener nofollow noreferrer" >pkg\scheduler\framework\plugins\podtopologyspread\plugin.go</a></p>
<pre><code class="language-go">var systemDefaultConstraints = []v1.TopologySpreadConstraint{
	{
		TopologyKey:       v1.LabelHostname,
		WhenUnsatisfiable: v1.ScheduleAnyway,
		MaxSkew:           3,
	},
	{
		TopologyKey:       v1.LabelTopologyZone,
		WhenUnsatisfiable: v1.ScheduleAnyway,
		MaxSkew:           5,
	},
}
</code></pre>
<p>可以通过在配置文件中留空，来禁用默认配置</p>
<ul>
<li><code>defaultConstraints: []</code></li>
<li><code>defaultingType: List</code></li>
</ul>
<pre><code class="language-yaml">apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints: []
          defaultingType: List
</code></pre>
<h3 id="通过源码学习topology">通过源码学习Topology</h3>
<p><code>podtopologyspread</code> 实现了4种扩展点方法，包含 <code>filter</code> 和 <code>score</code></p>
<h4 id="prefilter">PreFilter</h4>
<p>可以看到 <code>PreFilter</code> 的核心为 <code>calPreFilterState</code></p>
<pre><code class="language-go">func (pl *PodTopologySpread) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	s, err := pl.calPreFilterState(ctx, pod)
	if err != nil {
		return nil, framework.AsStatus(err)
	}
	cycleState.Write(preFilterStateKey, s)
	return nil, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L225-L307" target="_blank"
   rel="noopener nofollow noreferrer" >calPreFilterState</a> 主要功能是用在计算如何在拓扑域中分布Pod，首先看段代码时，需要掌握下属几个概念</p>
<ul>
<li><a href="#preFilterState">preFilterState</a></li>
<li><a href="#criticalPaths">criticalPaths</a></li>
<li><a href="#update">update</a></li>
</ul>
<pre><code class="language-go">func (pl *PodTopologySpread) calPreFilterState(ctx context.Context, pod *v1.Pod) (*preFilterState, error) {
    // 获取Node
	allNodes, err := pl.sharedLister.NodeInfos().List()
	if err != nil {
		return nil, fmt.Errorf(&quot;listing NodeInfos: %w&quot;, err)
	}
	var constraints []topologySpreadConstraint
	if len(pod.Spec.TopologySpreadConstraints) &gt; 0 {
		// 这里会构建出TopologySpreadConstraints，因为约束是不确定的
		constraints, err = filterTopologySpreadConstraints(
			pod.Spec.TopologySpreadConstraints,
			v1.DoNotSchedule,
			pl.enableMinDomainsInPodTopologySpread,
			pl.enableNodeInclusionPolicyInPodTopologySpread,
		)
		if err != nil {
			return nil, fmt.Errorf(&quot;obtaining pod's hard topology spread constraints: %w&quot;, err)
		}
	} else {
        // buildDefaultConstraints使用&quot;.DefaultConstraints&quot;与pod匹配的
        // service、replication controllers、replica sets 
        // 和stateful sets的选择器为pod构建一个约束。
		constraints, err = pl.buildDefaultConstraints(pod, v1.DoNotSchedule)
		if err != nil {
			return nil, fmt.Errorf(&quot;setting default hard topology spread constraints: %w&quot;, err)
		}
	}
	if len(constraints) == 0 { // 如果是空的，则返回空preFilterState
		return &amp;preFilterState{}, nil
	}
    // 初始化一个 preFilterState 状态
	s := preFilterState{
		Constraints:          constraints,
		TpKeyToCriticalPaths: make(map[string]*criticalPaths, len(constraints)),
		TpPairToMatchNum:     make(map[topologyPair]int, sizeHeuristic(len(allNodes), constraints)),
	}
	// 根据node统计拓扑域数量
	tpCountsByNode := make([]map[topologyPair]int, len(allNodes))
	// 获取pod亲和度配置
	requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod)
	processNode := func(i int) {
		nodeInfo := allNodes[i]
		node := nodeInfo.Node()
		if node == nil {
			klog.ErrorS(nil, &quot;Node not found&quot;)
			return
		}
		// 通过spreading去过滤node以用作filters，错误解析以向后兼容
		if !pl.enableNodeInclusionPolicyInPodTopologySpread {
			if match, _ := requiredNodeAffinity.Match(node); !match {
				return
			}
		}

		// 确保node的lable 包含topologyKeys定义的值
		if !nodeLabelsMatchSpreadConstraints(node.Labels, constraints) {
			return
		}

		tpCounts := make(map[topologyPair]int, len(constraints))
		for _, c := range constraints { // 对应的约束列表
			if pl.enableNodeInclusionPolicyInPodTopologySpread &amp;&amp;
				!c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) {
				continue
			}
			// 构建出 topologyPair 以key value形式，
			// 通常情况下TopologyKey属于什么类型的拓扑
			//  node.Labels[c.TopologyKey] 则是属于这个拓扑中那个子域
			pair := topologyPair{key: c.TopologyKey, value: node.Labels[c.TopologyKey]}
			// 计算与标签选择器相匹配的pod有多少个
			count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace)
			tpCounts[pair] = count
		}
		tpCountsByNode[i] = tpCounts // 最终形成的拓扑结构
	}
	// 执行上面的定义的processNode，执行的数量就是node的数量
	pl.parallelizer.Until(ctx, len(allNodes), processNode)
	// 最后构建出 TpPairToMatchNum
	// 表示每个拓扑域中的每个子域各分布多少Pod，如图6所示
	for _, tpCounts := range tpCountsByNode {
		for tp, count := range tpCounts {
			s.TpPairToMatchNum[tp] += count
		}
	}
	if pl.enableMinDomainsInPodTopologySpread {
		// 根据状态进行构建 preFilterState
		s.TpKeyToDomainsNum = make(map[string]int, len(constraints))
		for tp := range s.TpPairToMatchNum {
			s.TpKeyToDomainsNum[tp.key]++
		}
	}

	// 计算最小匹配出的拓扑对
	for i := 0; i &lt; len(constraints); i++ {
		key := constraints[i].TopologyKey
		s.TpKeyToCriticalPaths[key] = newCriticalPaths()
	}
	for pair, num := range s.TpPairToMatchNum {
		s.TpKeyToCriticalPaths[pair.key].update(pair.value, num)
	}

	return &amp;s, nil // 返回的值则包含最小的分布
}
</code></pre>
<p class="preFilterState">preFilterState</p>
<pre><code class="language-go">// preFilterState 是在PreFilter处计算并在Filter处使用。
// 它结合了 “TpKeyToCriticalPaths” 和 “TpPairToMatchNum” 来表示：
//（1）在每个分布约束上匹配最少pod的criticalPaths。 
// (2) 在每个分布约束上匹配的pod的数量。
// “nil preFilterState” 则表示没有设置（在PreFilter阶段）；
// empty “preFilterState”对象则表示它是一个合法的状态，并在PreFilter阶段设置。

type preFilterState struct {
	Constraints []topologySpreadConstraint

    // 这里记录2条关键路径而不是所有关键路径。 
    // criticalPaths[0].MatchNum 始终保存最小匹配数。 
    // criticalPaths[1].MatchNum 总是大于或等于criticalPaths[0].MatchNum，但不能保证是第二个最小匹配数。
	TpKeyToCriticalPaths map[string]*criticalPaths
	
    // TpKeyToDomainsNum 以 “topologyKey” 作为key ，并以zone的数量作为值。
	TpKeyToDomainsNum map[string]int
	
    // TpPairToMatchNum 以 “topologyPair作为key” ，并以匹配到pod的数量作为value。
	TpPairToMatchNum map[topologyPair]int
}
</code></pre>
<p class="criticalPaths">criticalPaths</p>
<pre><code class="language-go">// [2]criticalPath能够工作的原因是基于当前抢占算法的实现，特别是以下两个事实
// 事实 1：只抢占同一节点上的Pod，而不是多个节点上的 Pod。
// 事实 2：每个节点在其抢占周期期间在“preFilterState”的单独副本上进行评估。如果我们计划转向更复杂的算法，例如“多个节点上的任意pod”时则需要重新考虑这种结构。
type criticalPaths [2]struct {
	// TopologyValue代表映射到拓扑键的拓扑值。
	TopologyValue string
	// MatchNum代表匹配到的pod数量
	MatchNum int
}
</code></pre>
<p>单元测试中的测试案例，具有两个约束条件的场景，通过表格来解析如下：</p>
<p>Node列表与标签如下表：</p>
<table>
<thead>
<tr>
<th>Node Name</th>
<th>🏷️Lable-zone</th>
<th>🏷️Lable-node</th>
</tr>
</thead>
<tbody>
<tr>
<td>node-a</td>
<td>zone1</td>
<td>node-a</td>
</tr>
<tr>
<td>node-b</td>
<td>zone1</td>
<td>node-b</td>
</tr>
<tr>
<td>node-x</td>
<td>zone2</td>
<td>node-x</td>
</tr>
<tr>
<td>node-y</td>
<td>zone2</td>
<td>node-y</td>
</tr>
</tbody>
</table>
<p>Pod列表与标签如下表：</p>
<table>
<thead>
<tr>
<th>Pod Name</th>
<th>Node</th>
<th>🏷️Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>p-a1</td>
<td>node-a</td>
<td>foo:</td>
</tr>
<tr>
<td>p-a2</td>
<td>node-a</td>
<td>foo:</td>
</tr>
<tr>
<td>p-b1</td>
<td>node-b</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y1</td>
<td>node-y</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y2</td>
<td>node-y</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y3</td>
<td>node-y</td>
<td>foo:</td>
</tr>
<tr>
<td>p-y4</td>
<td>node-y</td>
<td>foo:</td>
</tr>
</tbody>
</table>
<p>对应的拓扑约束</p>
<pre><code class="language-yaml">spec:
  topologySpreadConstraints:
  - MaxSkew: 1
	TopologyKey: zone
	labelSelector:
	  matchLabels:
	    foo: bar
	MinDomains: 1
	NodeAffinityPolicy: Honor
	NodeTaintsPolicy: Ignore
  - MaxSkew: 1
	TopologyKey: node
	labelSelector:
	  matchLabels:
	    foo: bar
	MinDomains: 1
	NodeAffinityPolicy: Honor
	NodeTaintsPolicy: Ignore
</code></pre>
<p>那么整个分布如下：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220726214255638.png" alt="image-20220726214255638" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图6：具有两个场景的分布图</center>
<p id="prefiltertesting">实现的测试代码如下</p>
<pre><code class="language-go">...
{
    name: &quot;normal case with two spreadConstraints&quot;,
    pod: st.MakePod().Name(&quot;p&quot;).Label(&quot;foo&quot;, &quot;&quot;).
    SpreadConstraint(1, &quot;zone&quot;, v1.DoNotSchedule, fooSelector, nil, nil, nil).
    SpreadConstraint(1, &quot;node&quot;, v1.DoNotSchedule, fooSelector, nil, nil, nil).
    Obj(),
    nodes: []*v1.Node{
        st.MakeNode().Name(&quot;node-a&quot;).Label(&quot;zone&quot;, &quot;zone1&quot;).Label(&quot;node&quot;, &quot;node-a&quot;).Obj(),
        st.MakeNode().Name(&quot;node-b&quot;).Label(&quot;zone&quot;, &quot;zone1&quot;).Label(&quot;node&quot;, &quot;node-b&quot;).Obj(),
        st.MakeNode().Name(&quot;node-x&quot;).Label(&quot;zone&quot;, &quot;zone2&quot;).Label(&quot;node&quot;, &quot;node-x&quot;).Obj(),
        st.MakeNode().Name(&quot;node-y&quot;).Label(&quot;zone&quot;, &quot;zone2&quot;).Label(&quot;node&quot;, &quot;node-y&quot;).Obj(),
    },
    existingPods: []*v1.Pod{
        st.MakePod().Name(&quot;p-a1&quot;).Node(&quot;node-a&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-a2&quot;).Node(&quot;node-a&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-b1&quot;).Node(&quot;node-b&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y1&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y2&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y3&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
        st.MakePod().Name(&quot;p-y4&quot;).Node(&quot;node-y&quot;).Label(&quot;foo&quot;, &quot;&quot;).Obj(),
    },
    want: &amp;preFilterState{
        Constraints: []topologySpreadConstraint{
            {
                MaxSkew:            1,
                TopologyKey:        &quot;zone&quot;,
                Selector:           mustConvertLabelSelectorAsSelector(t, fooSelector),
                MinDomains:         1,
                NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,
                NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore,
            },
            {
                MaxSkew:            1,
                TopologyKey:        &quot;node&quot;,
                Selector:           mustConvertLabelSelectorAsSelector(t, fooSelector),
                MinDomains:         1,
                NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,
                NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore,
            },
        },
        TpKeyToCriticalPaths: map[string]*criticalPaths{
            &quot;zone&quot;: {{&quot;zone1&quot;, 3}, {&quot;zone2&quot;, 4}},
            &quot;node&quot;: {{&quot;node-x&quot;, 0}, {&quot;node-b&quot;, 1}},
        },
        for pair, num := range s.TpPairToMatchNum {
		s.TpKeyToCriticalPaths[pair.key].update(pair.value, num)
	}
        TpPairToMatchNum: map[topologyPair]int{
            {key: &quot;zone&quot;, value: &quot;zone1&quot;}:  3,
            {key: &quot;zone&quot;, value: &quot;zone2&quot;}:  4,
            {key: &quot;node&quot;, value: &quot;node-a&quot;}: 2,
            {key: &quot;node&quot;, value: &quot;node-b&quot;}: 1,
            {key: &quot;node&quot;, value: &quot;node-x&quot;}: 0,
            {key: &quot;node&quot;, value: &quot;node-y&quot;}: 4,
        },
    },
}
...
</code></pre>
<p class="update">update</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L120-L148" target="_blank"
   rel="noopener nofollow noreferrer" >update</a> 函数实际上时用于计算 <a href="#criticalPaths">criticalPaths</a> 中的第一位始终保持为是一个最小Pod匹配值</p>
<pre><code class="language-go">func (p *criticalPaths) update(tpVal string, num int) {
	// first verify if `tpVal` exists or not
	i := -1
	if tpVal == p[0].TopologyValue {
		i = 0
	} else if tpVal == p[1].TopologyValue {
		i = 1
	}

	if i &gt;= 0 {
		// `tpVal` 表示已经存在
		p[i].MatchNum = num
		if p[0].MatchNum &gt; p[1].MatchNum {
			// swap paths[0] and paths[1]
			p[0], p[1] = p[1], p[0]
		}
	} else {
		// `tpVal` 表示不存在，如一个新初始化的值
        // num对应子域分布的pod
        // 说明第一个元素不是最小的，则作为交换
		if num &lt; p[0].MatchNum {
			// update paths[1] with paths[0]
			p[1] = p[0]
			// update paths[0]
			p[0].TopologyValue, p[0].MatchNum = tpVal, num
		} else if num &lt; p[1].MatchNum {
			// 如果小于 paths[1]，则更新它，永远保证元素0是最小，1是次小的
			p[1].TopologyValue, p[1].MatchNum = tpVal, num
		}
	}
}
</code></pre>
<p>综合来讲 <code>Prefilter</code> 主要做的工作是。循环所有的节点，先根据 <code>NodeAffinity</code> 或者 <code>NodeSelector</code> 进行过滤，然后根据约束中定义的 <code>topologyKeys</code> （拓扑划分的依据） 来选择节点。</p>
<p>接下来会计算出每个拓扑域下的拓扑对（可以理解为子域）匹配的 Pod 数量，存入 <code>TpPairToMatchNum</code> 中，最后就是要把所有约束中匹配的 Pod 数量最小（第二小）匹配出来的路径（代码是这么定义的，理解上可以看作是分布图）放入 <code>TpKeyToCriticalPaths</code> 中保存起来。整个 <code>preFilterState</code> 保存下来传递到后续的 <code>filter</code> 插件中使用。</p>
<h4 id="filter">Filter</h4>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L178" target="_blank"
   rel="noopener nofollow noreferrer" >preFilter</a> 中 最后的计算结果会保存在 <code>CycleState</code> 中</p>
<pre><code class="language-go">cycleState.Write(preFilterStateKey, s)
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/filtering.go#L310-L362" target="_blank"
   rel="noopener nofollow noreferrer" >Filter</a> 主要是从 <code>PreFilter</code> 处理的过程中拿到状态 <code>preFilterState</code>，然后看下每个拓扑约束中的 <code>MaxSkew</code> 是否合法，具体的计算公式为：$matchNum + selfMatchNum - minMatchNum$</p>
<ul>
<li><code>matchNum</code>：Prefilter 中计算出的对应的拓扑分布数量，可以在<a href="#prefiltertesting">Prefilter</a>中参考对应的内容
<ul>
<li><code>if tpCount, ok := s.TpPairToMatchNum[pair]; ok {</code></li>
</ul>
</li>
<li><code>selfMatchNum</code>：匹配到label的数量，匹配到则是1，否则为0</li>
<li><code> minMatchNum</code>：获的 <code>Prefilter </code> 中计算出来的最小匹配的值</li>
</ul>
<pre><code class="language-go">func (pl *PodTopologySpread) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	node := nodeInfo.Node()
	if node == nil {
		return framework.AsStatus(fmt.Errorf(&quot;node not found&quot;))
	}
	// 拿到 prefilter处理的s，即preFilterState
	s, err := getPreFilterState(cycleState)
	if err != nil {
		return framework.AsStatus(err)
	}

	// 一个 空类型的 preFilterState是合法的，这种情况下将容忍每一个被调度的 Pod
	if len(s.Constraints) == 0 {
		return nil
	}

	podLabelSet := labels.Set(pod.Labels) // 设置标签
	for _, c := range s.Constraints { // 因为拓扑约束允许多个所以
		tpKey := c.TopologyKey
		tpVal, ok := node.Labels[c.TopologyKey]
		if !ok {
			klog.V(5).InfoS(&quot;Node doesn't have required label&quot;, &quot;node&quot;, klog.KObj(node), &quot;label&quot;, tpKey)
			return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReasonNodeLabelNotMatch)
		}

		// 判断标准
		// 现有的匹配数量 + 子匹配（1|0） - 全局minimum &lt;= maxSkew
		minMatchNum, err := s.minMatchNum(tpKey, c.MinDomains, pl.enableMinDomainsInPodTopologySpread)
		if err != nil {
			klog.ErrorS(err, &quot;Internal error occurred while retrieving value precalculated in PreFilter&quot;, &quot;topologyKey&quot;, tpKey, &quot;paths&quot;, s.TpKeyToCriticalPaths)
			continue
		}

		selfMatchNum := 0
		if c.Selector.Matches(podLabelSet) {
			selfMatchNum = 1
		}

		pair := topologyPair{key: tpKey, value: tpVal}
		matchNum := 0
		if tpCount, ok := s.TpPairToMatchNum[pair]; ok {
			matchNum = tpCount
		}
		skew := matchNum + selfMatchNum - minMatchNum
		if skew &gt; int(c.MaxSkew) {
			klog.V(5).InfoS(&quot;Node failed spreadConstraint: matchNum + selfMatchNum - minMatchNum &gt; maxSkew&quot;, &quot;node&quot;, klog.KObj(node), &quot;topologyKey&quot;, tpKey, &quot;matchNum&quot;, matchNum, &quot;selfMatchNum&quot;, selfMatchNum, &quot;minMatchNum&quot;, minMatchNum, &quot;maxSkew&quot;, c.MaxSkew)
			return framework.NewStatus(framework.Unschedulable, ErrReasonConstraintsNotMatch)
		}
	}

	return nil
}
</code></pre>
<p>minMatchNum</p>
<pre><code class="language-go">// minMatchNum用于计算 倾斜的全局最小值，同时考虑 MinDomains。
func (s *preFilterState) minMatchNum(tpKey string, minDomains int32, enableMinDomainsInPodTopologySpread bool) (int, error) {
	paths, ok := s.TpKeyToCriticalPaths[tpKey]
	if !ok {
		return 0, fmt.Errorf(&quot;failed to retrieve path by topology key&quot;)
	}
	// 通常来说最小值是第一个
	minMatchNum := paths[0].MatchNum
	if !enableMinDomainsInPodTopologySpread { // 就是plugin的配置的 enableMinDomainsInPodTopologySpread
		return minMatchNum, nil
	}

	domainsNum, ok := s.TpKeyToDomainsNum[tpKey]
	if !ok {
		return 0, fmt.Errorf(&quot;failed to retrieve the number of domains by topology key&quot;)
	}

	if domainsNum &lt; int(minDomains) {
		// 当有匹配拓扑键的符合条件的域的数量小于 配置的&quot;minDomains&quot;(每个约束条件的这个配置) 时，
		//它将全局“minimum” 设置为0。
		// 因为minimum默认就为1，如果他小于1，就让他为0
		minMatchNum = 0
	}

	return minMatchNum, nil
}
</code></pre>
<h4 id="prescore">PreScore</h4>
<p>与 Filter 类似， <code>PreScore</code> 也是类似 <code>PreFilter</code> 的构成。 <code>initPreScoreState</code> 来完成过滤。</p>
<p>有了 <code>PreFilter</code> 基础后，对于 Score 来说大同小异</p>
<pre><code class="language-go">func (pl *PodTopologySpread) PreScore(
	ctx context.Context,
	cycleState *framework.CycleState,
	pod *v1.Pod,
	filteredNodes []*v1.Node,
) *framework.Status {
	allNodes, err := pl.sharedLister.NodeInfos().List()
	if err != nil {
		return framework.AsStatus(fmt.Errorf(&quot;getting all nodes: %w&quot;, err))
	}

	if len(filteredNodes) == 0 || len(allNodes) == 0 {
		// No nodes to score.
		return nil
	}

	state := &amp;preScoreState{
		IgnoredNodes:            sets.NewString(),
		TopologyPairToPodCounts: make(map[topologyPair]*int64),
	}
	// Only require that nodes have all the topology labels if using
	// non-system-default spreading rules. This allows nodes that don't have a
	// zone label to still have hostname spreading.
	// 如果使用非系统默认分布规则，则仅要求节点具有所有拓扑标签。
	// 这将允许没有zone标签的节点仍然具有hostname分布。
	requireAllTopologies := len(pod.Spec.TopologySpreadConstraints) &gt; 0 || !pl.systemDefaulted
	err = pl.initPreScoreState(state, pod, filteredNodes, requireAllTopologies)
	if err != nil {
		return framework.AsStatus(fmt.Errorf(&quot;calculating preScoreState: %w&quot;, err))
	}

	// return if incoming pod doesn't have soft topology spread Constraints.
	if len(state.Constraints) == 0 {
		cycleState.Write(preScoreStateKey, state)
		return nil
	}

	// Ignore parsing errors for backwards compatibility.
	requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod)
	processAllNode := func(i int) {
		nodeInfo := allNodes[i]
		node := nodeInfo.Node()
		if node == nil {
			return
		}

		if !pl.enableNodeInclusionPolicyInPodTopologySpread {
			// `node` should satisfy incoming pod's NodeSelector/NodeAffinity
			if match, _ := requiredNodeAffinity.Match(node); !match {
				return
			}
		}

		// All topologyKeys need to be present in `node`
		if requireAllTopologies &amp;&amp; !nodeLabelsMatchSpreadConstraints(node.Labels, state.Constraints) {
			return
		}

		for _, c := range state.Constraints {
			if pl.enableNodeInclusionPolicyInPodTopologySpread &amp;&amp;
				!c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) {
				continue
			}

			pair := topologyPair{key: c.TopologyKey, value: node.Labels[c.TopologyKey]}
			// If current topology pair is not associated with any candidate node,
			// continue to avoid unnecessary calculation.
			// Per-node counts are also skipped, as they are done during Score.
			tpCount := state.TopologyPairToPodCounts[pair]
			if tpCount == nil {
				continue
			}
			count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace)
			atomic.AddInt64(tpCount, int64(count))
		}
	}
	pl.parallelizer.Until(ctx, len(allNodes), processAllNode)
	// 保存状态给后面sorce调用
	cycleState.Write(preScoreStateKey, state)
	return nil
}
</code></pre>
<p>与Filter中Update使用的函数一样，这里也会到这一步，这里会构建出TopologySpreadConstraints，因为约束是不确定的</p>
<pre><code class="language-go">func filterTopologySpreadConstraints(constraints []v1.TopologySpreadConstraint, action v1.UnsatisfiableConstraintAction, enableMinDomainsInPodTopologySpread, enableNodeInclusionPolicyInPodTopologySpread bool) ([]topologySpreadConstraint, error) {
	var result []topologySpreadConstraint
	for _, c := range constraints {
		if c.WhenUnsatisfiable == action { // 始终调度时
			selector, err := metav1.LabelSelectorAsSelector(c.LabelSelector)
			if err != nil {
				return nil, err
			}
			tsc := topologySpreadConstraint{
				MaxSkew:            c.MaxSkew,
				TopologyKey:        c.TopologyKey,
				Selector:           selector,
				MinDomains:         1,                            // If MinDomains is nil, we treat MinDomains as 1.
				NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,  // If NodeAffinityPolicy is nil, we treat NodeAffinityPolicy as &quot;Honor&quot;.
				NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore, // If NodeTaintsPolicy is nil, we treat NodeTaintsPolicy as &quot;Ignore&quot;.
			}
			if enableMinDomainsInPodTopologySpread &amp;&amp; c.MinDomains != nil {
				tsc.MinDomains = *c.MinDomains
			}
			if enableNodeInclusionPolicyInPodTopologySpread {
				if c.NodeAffinityPolicy != nil {
					tsc.NodeAffinityPolicy = *c.NodeAffinityPolicy
				}
				if c.NodeTaintsPolicy != nil {
					tsc.NodeTaintsPolicy = *c.NodeTaintsPolicy
				}
			}
			result = append(result, tsc)
		}
	}
	return result, nil
}
</code></pre>
<h4 id="score">Score</h4>
<pre><code class="language-GO">// 在分数扩展点调用分数。该函数返回的“score”是 `nodeName` 上匹配的 pod 数量，稍后会进行归一化。
func (pl *PodTopologySpread) Score(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	nodeInfo, err := pl.sharedLister.NodeInfos().Get(nodeName)
	if err != nil {
		return 0, framework.AsStatus(fmt.Errorf(&quot;getting node %q from Snapshot: %w&quot;, nodeName, err))
	}

	node := nodeInfo.Node()
	s, err := getPreScoreState(cycleState)
	if err != nil {
		return 0, framework.AsStatus(err)
	}

	// Return if the node is not qualified.
	if s.IgnoredNodes.Has(node.Name) {
		return 0, nil
	}

	// 对于每个当前的 &lt;pair&gt;，当前节点获得 &lt;matchSum&gt; 的信用分。
	// 计算 &lt;matchSum&gt;总和 并将其作为该节点的分数返回。
	var score float64
	for i, c := range s.Constraints {
		if tpVal, ok := node.Labels[c.TopologyKey]; ok {
			var cnt int64
			if c.TopologyKey == v1.LabelHostname {
				cnt = int64(countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace))
			} else {
				pair := topologyPair{key: c.TopologyKey, value: tpVal}
				cnt = *s.TopologyPairToPodCounts[pair]
			}
			score += scoreForCount(cnt, c.MaxSkew, s.TopologyNormalizingWeight[i])
		}
	}
	return int64(math.Round(score)), nil
}
</code></pre>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/runtime/framework.go#L940-L952" target="_blank"
   rel="noopener nofollow noreferrer" >Framework</a> 中会运行 <code>ScoreExtension</code> ，即 <code>NormalizeScore</code></p>
<pre><code class="language-go">// Run NormalizeScore method for each ScorePlugin in parallel.
f.Parallelizer().Until(ctx, len(f.scorePlugins), func(index int) {
    pl := f.scorePlugins[index]
    nodeScoreList := pluginToNodeScores[pl.Name()]
    if pl.ScoreExtensions() == nil {
        return
    }
    status := f.runScoreExtension(ctx, pl, state, pod, nodeScoreList)
    if !status.IsSuccess() {
        err := fmt.Errorf(&quot;plugin %q failed with: %w&quot;, pl.Name(), status.AsError())
        errCh.SendErrorWithCancel(err, cancel)
        return
    }
})
if err := errCh.ReceiveError(); err != nil {
    return nil, framework.AsStatus(fmt.Errorf(&quot;running Normalize on Score plugins: %w&quot;, err))
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/framework/plugins/podtopologyspread/scoring.go#L216-L255" target="_blank"
   rel="noopener nofollow noreferrer" >NormalizeScore</a> 会为所有的node根据之前计算出的权重进行打分</p>
<pre><code class="language-go">func (pl *PodTopologySpread) NormalizeScore(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
	s, err := getPreScoreState(cycleState)
	if err != nil {
		return framework.AsStatus(err)
	}
	if s == nil {
		return nil
	}

	// 计算 &lt;minScore&gt; 和 &lt;maxScore&gt;
	var minScore int64 = math.MaxInt64
	var maxScore int64
	for i, score := range scores {
		// it's mandatory to check if &lt;score.Name&gt; is present in m.IgnoredNodes
		if s.IgnoredNodes.Has(score.Name) {
			scores[i].Score = invalidScore
			continue
		}
		if score.Score &lt; minScore {
			minScore = score.Score
		}
		if score.Score &gt; maxScore {
			maxScore = score.Score
		}
	}

	for i := range scores {
		if scores[i].Score == invalidScore {
			scores[i].Score = 0
			continue
		}
		if maxScore == 0 {
			scores[i].Score = framework.MaxNodeScore
			continue
		}
		s := scores[i].Score
		scores[i].Score = framework.MaxNodeScore * (maxScore + minScore - s) / maxScore
	}
	return nil
}
</code></pre>
<p>到此，对于pod拓扑插件功能大概可以明了了，</p>
<ul>
<li>Filter 部分（<code>PreFilter</code>，<code>Filter</code>）完成拓扑对(<code>Topology Pair</code>)划分</li>
<li>Score部分（<code>PreScore</code>, <code>Score</code> , <code>NormalizeScore</code> ）主要是对拓扑对（可以理解为拓扑结构划分）来选择一个最适合的pod的节点（即分数最优的节点）</li>
</ul>
<p>而在 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/plugins/podtopologyspread/scoring_test.go" target="_blank"
   rel="noopener nofollow noreferrer" >scoring_test.go</a> 给了很多用例，可以更深入的了解这部分算法</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling code hierarchy</a></p>
<p><sup id="2">[2]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_algorithm.md" target="_blank"
   rel="noopener nofollow noreferrer" >scheduler algorithm</a></p>
<p><sup id="3">[3]</sup> <a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#in-tree-vs-out-of-tree-volume-plugins" target="_blank"
   rel="noopener nofollow noreferrer" >in tree VS out of tree volume plugins</a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_framework_plugins.md" target="_blank"
   rel="noopener nofollow noreferrer" >scheduler_framework_plugins</a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/reference/scheduling/config/" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling config</a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/" target="_blank"
   rel="noopener nofollow noreferrer" >topology spread constraints</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kube-scheduler的调度上下文</title>
      <link>https://www.oomkill.com/2022/07/ch20-schedule-workflow/</link>
      <pubDate>Thu, 21 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/07/ch20-schedule-workflow/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="scheduler">Scheduler</h2>
<p><a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L64-L102" target="_blank"
   rel="noopener nofollow noreferrer" >Scheduler</a> 是整个 <code>kube-scheduler</code> 的一个 structure，提供了 <code>kube-scheduler</code> 运行所需的组件。</p>
<pre><code class="language-go">type Scheduler struct {
	// Cache是一个抽象，会缓存pod的信息，作为scheduler进行查找，操作是基于Pod进行增加
	Cache internalcache.Cache
	// Extenders 算是调度框架中提供的调度插件，会影响kubernetes中的调度策略
	Extenders []framework.Extender

	// NextPod 作为一个函数提供，会阻塞获取下一个ke'diao'du
	NextPod func() *framework.QueuedPodInfo

	// Error is called if there is an error. It is passed the pod in
	// question, and the error
	Error func(*framework.QueuedPodInfo, error)

	// SchedulePod 尝试将给出的pod调度到Node。
	SchedulePod func(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (ScheduleResult, error)

	// 关闭scheduler的信号
	StopEverything &lt;-chan struct{}

	// SchedulingQueue保存要调度的Pod
	SchedulingQueue internalqueue.SchedulingQueue

	// Profiles中是多个调度框架
	Profiles profile.Map
	client clientset.Interface
	nodeInfoSnapshot *internalcache.Snapshot
	percentageOfNodesToScore int32
	nextStartNodeIndex int
}
</code></pre>
<p>作为实际执行的两个核心，<code>SchedulingQueue</code> ，与 <code>scheduleOne</code> 将会分析到这两个</p>
<h2 id="schedulingqueue">SchedulingQueue</h2>
<p>在知道 <code>kube-scheduler</code> 初始化过程后，需要对 <code>kube-scheduler</code> 的整个 <em>structure</em> 和 <em>workflow</em> 进行分析</p>
<p>在 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L336-L340" target="_blank"
   rel="noopener nofollow noreferrer" >Run</a> 中，运行的是 一个 <code>SchedulingQueue</code> 与 一个 <code>scheduleOne</code> ，从结构上看是属于 <em>Scheduler</em></p>
<pre><code class="language-go">func (sched *Scheduler) Run(ctx context.Context) {
	sched.SchedulingQueue.Run()

	// We need to start scheduleOne loop in a dedicated goroutine,
	// because scheduleOne function hangs on getting the next item
	// from the SchedulingQueue.
	// If there are no new pods to schedule, it will be hanging there
	// and if done in this goroutine it will be blocking closing
	// SchedulingQueue, in effect causing a deadlock on shutdown.
	go wait.UntilWithContext(ctx, sched.scheduleOne, 0)

	&lt;-ctx.Done()
	sched.SchedulingQueue.Close()
}

</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L81-L110" target="_blank"
   rel="noopener nofollow noreferrer" >SchedulingQueue</a> 是一个队列的抽象，用于存储等待调度的Pod。该接口遵循类似于 cache.FIFO 和 cache.Heap 的模式。</p>
<pre><code class="language-go">type SchedulingQueue interface {
	framework.PodNominator
	Add(pod *v1.Pod) error
	// Activate moves the given pods to activeQ iff they're in unschedulablePods or backoffQ.
	// The passed-in pods are originally compiled from plugins that want to activate Pods,
	// by injecting the pods through a reserved CycleState struct (PodsToActivate).
	Activate(pods map[string]*v1.Pod)
	// 将不可调度的Pod重入到队列中
	AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error
	// SchedulingCycle returns the current number of scheduling cycle which is
	// cached by scheduling queue. Normally, incrementing this number whenever
	// a pod is popped (e.g. called Pop()) is enough.
	SchedulingCycle() int64
	// Pop会弹出一个pod，并从head优先级队列中删除
	Pop() (*framework.QueuedPodInfo, error)
	Update(oldPod, newPod *v1.Pod) error
	Delete(pod *v1.Pod) error
	MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck)
	AssignedPodAdded(pod *v1.Pod)
	AssignedPodUpdated(pod *v1.Pod)
	PendingPods() []*v1.Pod
	// Close closes the SchedulingQueue so that the goroutine which is
	// waiting to pop items can exit gracefully.
	Close()
	// Run starts the goroutines managing the queue.
	Run()
}
</code></pre>
<p>而 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L134-L175" target="_blank"
   rel="noopener nofollow noreferrer" >PriorityQueue</a> 是 <code>SchedulingQueue</code> 的实现，该部分的核心构成是两个子队列与一个数据结构，即 <code>activeQ</code>、<code>backoffQ</code> 和 <code>unschedulablePods</code></p>
<ul>
<li><code>activeQ</code>：是一个 <em>heap</em> 类型的优先级队列，是 <em>sheduler</em> 从中获得优先级最高的Pod进行调度</li>
<li><code>backoffQ</code>：也是一个 <em>heap</em> 类型的优先级队列，存放的是不可调度的Pod</li>
<li><code>unschedulablePods </code>：保存确定不可被调度的Pod</li>
</ul>
<pre><code class="language-GO">type SchedulingQueue interface {
	framework.PodNominator
	Add(pod *v1.Pod) error
	// Activate moves the given pods to activeQ iff they're in unschedulablePods or backoffQ.
	// The passed-in pods are originally compiled from plugins that want to activate Pods,
	// by injecting the pods through a reserved CycleState struct (PodsToActivate).
	Activate(pods map[string]*v1.Pod)
	// AddUnschedulableIfNotPresent adds an unschedulable pod back to scheduling queue.
	// The podSchedulingCycle represents the current scheduling cycle number which can be
	// returned by calling SchedulingCycle().
	AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error
	// SchedulingCycle returns the current number of scheduling cycle which is
	// cached by scheduling queue. Normally, incrementing this number whenever
	// a pod is popped (e.g. called Pop()) is enough.
	SchedulingCycle() int64
	// Pop removes the head of the queue and returns it. It blocks if the
	// queue is empty and waits until a new item is added to the queue.
	Pop() (*framework.QueuedPodInfo, error)
	Update(oldPod, newPod *v1.Pod) error
	Delete(pod *v1.Pod) error
	MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck)
	AssignedPodAdded(pod *v1.Pod)
	AssignedPodUpdated(pod *v1.Pod)
	PendingPods() []*v1.Pod
	// Close closes the SchedulingQueue so that the goroutine which is
	// waiting to pop items can exit gracefully.
	Close()
	// Run starts the goroutines managing the queue.
	Run()
}
</code></pre>
<p>在New <em>scheduler</em> 时可以看到会初始化这个queue</p>
<pre><code class="language-go">podQueue := internalqueue.NewSchedulingQueue(
    // 实现pod对比的一个函数即less
    profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
    informerFactory,
    internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
    internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
    internalqueue.WithPodNominator(nominator),
    internalqueue.WithClusterEventMap(clusterEventMap),
    internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
)
</code></pre>
<p>而 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L252-L289" target="_blank"
   rel="noopener nofollow noreferrer" >NewSchedulingQueue</a> 则是初始化这个 PriorityQueue</p>
<pre><code class="language-go">// NewSchedulingQueue initializes a priority queue as a new scheduling queue.
func NewSchedulingQueue(
	lessFn framework.LessFunc,
	informerFactory informers.SharedInformerFactory,
	opts ...Option) SchedulingQueue {
	return NewPriorityQueue(lessFn, informerFactory, opts...)
}

// NewPriorityQueue creates a PriorityQueue object.
func NewPriorityQueue(
	lessFn framework.LessFunc,
	informerFactory informers.SharedInformerFactory,
	opts ...Option,
) *PriorityQueue {
	options := defaultPriorityQueueOptions
	for _, opt := range opts {
		opt(&amp;options)
	}
	// 这个就是 less函数，作为打分的一部分
	comp := func(podInfo1, podInfo2 interface{}) bool {
		pInfo1 := podInfo1.(*framework.QueuedPodInfo)
		pInfo2 := podInfo2.(*framework.QueuedPodInfo)
		return lessFn(pInfo1, pInfo2)
	}

	if options.podNominator == nil {
		options.podNominator = NewPodNominator(informerFactory.Core().V1().Pods().Lister())
	}

	pq := &amp;PriorityQueue{
		PodNominator:                      options.podNominator,
		clock:                             options.clock,
		stop:                              make(chan struct{}),
		podInitialBackoffDuration:         options.podInitialBackoffDuration,
		podMaxBackoffDuration:             options.podMaxBackoffDuration,
		podMaxInUnschedulablePodsDuration: options.podMaxInUnschedulablePodsDuration,
		activeQ:                           heap.NewWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()),
		unschedulablePods:                 newUnschedulablePods(metrics.NewUnschedulablePodsRecorder()),
		moveRequestCycle:                  -1,
		clusterEventMap:                   options.clusterEventMap,
	}
	pq.cond.L = &amp;pq.lock
	pq.podBackoffQ = heap.NewWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder())
	pq.nsLister = informerFactory.Core().V1().Namespaces().Lister()

	return pq
}
</code></pre>
<p>了解了Queue的结构，就需要知道 入队列与出队列是在哪里操作的。在初始化时，需要注册一个 <code>addEventHandlerFuncs</code> 这个时候，会注入三个动作函数，也就是controller中的概念；而在AddFunc中可以看到会入队列。</p>
<p>注入是对 Pod 的<a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/eventhandlers.go#L302-L305" target="_blank"
   rel="noopener nofollow noreferrer" >informer</a>注入的，注入的函数 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/eventhandlers.go#L114-L120" target="_blank"
   rel="noopener nofollow noreferrer" >addPodToSchedulingQueue</a>  就是入栈</p>
<pre><code class="language-go">Handler: cache.ResourceEventHandlerFuncs{
    AddFunc:    sched.addPodToSchedulingQueue,
    UpdateFunc: sched.updatePodInSchedulingQueue,
    DeleteFunc: sched.deletePodFromSchedulingQueue,
},

func (sched *Scheduler) addPodToSchedulingQueue(obj interface{}) {
	pod := obj.(*v1.Pod)
	klog.V(3).InfoS(&quot;Add event for unscheduled pod&quot;, &quot;pod&quot;, klog.KObj(pod))
	if err := sched.SchedulingQueue.Add(pod); err != nil {
		utilruntime.HandleError(fmt.Errorf(&quot;unable to queue %T: %v&quot;, obj, err))
	}
}
</code></pre>
<p>而这个 <code>SchedulingQueue</code> 的实现就是 <code>PriorityQueue</code> ，而Add中则对 activeQ进行的操作</p>
<pre><code class="language-go">func (p *PriorityQueue) Add(pod *v1.Pod) error {
	p.lock.Lock()
	defer p.lock.Unlock()
    // 格式化入栈数据，包含podinfo，里会包含v1.Pod
    // 初始化的时间，创建的时间，以及不能被调度时的记录其plugin的名称
	pInfo := p.newQueuedPodInfo(pod)
    // 入栈
	if err := p.activeQ.Add(pInfo); err != nil {
		klog.ErrorS(err, &quot;Error adding pod to the active queue&quot;, &quot;pod&quot;, klog.KObj(pod))
		return err
	}
	if p.unschedulablePods.get(pod) != nil {
		klog.ErrorS(nil, &quot;Error: pod is already in the unschedulable queue&quot;, &quot;pod&quot;, klog.KObj(pod))
		p.unschedulablePods.delete(pod)
	}
	// Delete pod from backoffQ if it is backing off
	if err := p.podBackoffQ.Delete(pInfo); err == nil {
		klog.ErrorS(nil, &quot;Error: pod is already in the podBackoff queue&quot;, &quot;pod&quot;, klog.KObj(pod))
	}
	metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;active&quot;, PodAdd).Inc()
	p.PodNominator.AddNominatedPod(pInfo.PodInfo, nil)
	p.cond.Broadcast()

	return nil
}
</code></pre>
<p>在上面看 <em>scheduler</em> 结构时，可以看到有一个 nextPod的，nextPod就是从队列中弹出一个pod，这个在<em>scheduler</em> 时会传入 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L952-L965" target="_blank"
   rel="noopener nofollow noreferrer" >MakeNextPodFunc</a> 就是这个 nextpod</p>
<pre><code class="language-go">func MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo {
	return func() *framework.QueuedPodInfo {
		podInfo, err := queue.Pop()
		if err == nil {
			klog.V(4).InfoS(&quot;About to try and schedule pod&quot;, &quot;pod&quot;, klog.KObj(podInfo.Pod))
			for plugin := range podInfo.UnschedulablePlugins {
				metrics.UnschedulableReason(plugin, podInfo.Pod.Spec.SchedulerName).Dec()
			}
			return podInfo
		}
		klog.ErrorS(err, &quot;Error while retrieving next pod from scheduling queue&quot;)
		return nil
	}
}
</code></pre>
<p>而这个 <code>queue.Pop()</code> 对应的就是 <code>PriorityQueue</code> 的 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/internal/queue/scheduling_queue.go#L483-L503" target="_blank"
   rel="noopener nofollow noreferrer" >Pop()</a> ，在这里会将作为 activeQ 的消费端</p>
<pre><code class="language-go">func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
   p.lock.Lock()
   defer p.lock.Unlock()
   for p.activeQ.Len() == 0 {
      // When the queue is empty, invocation of Pop() is blocked until new item is enqueued.
      // When Close() is called, the p.closed is set and the condition is broadcast,
      // which causes this loop to continue and return from the Pop().
      if p.closed {
         return nil, fmt.Errorf(queueClosed)
      }
      p.cond.Wait()
   }
   obj, err := p.activeQ.Pop()
   if err != nil {
      return nil, err
   }
   pInfo := obj.(*framework.QueuedPodInfo)
   pInfo.Attempts++
   p.schedulingCycle++
   return pInfo, nil
}
</code></pre>
<p>在上面入口部分也看到了，scheduleOne 和 scheduler，scheduleOne 就是去消费一个Pod，他会调用 NextPod，NextPod就是在初始化传入的 <code>MakeNextPodFunc</code> ，至此回到对应的 Pop来做消费。</p>
<p>schedulerOne是为一个Pod做调度的流程。</p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	podInfo := sched.NextPod()
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
	fwk, err := sched.frameworkForPod(pod)
	if err != nil {
		// This shouldn't happen, because we only accept for scheduling the pods
		// which specify a scheduler name that matches one of the profiles.
		klog.ErrorS(err, &quot;Error occurred&quot;)
		return
	}
	if sched.skipPodSchedule(fwk, pod) {
		return
	}
...
</code></pre>
<h2 id="调度上下文">调度上下文</h2>
<p>当了解了scheduler结构后，下面分析下调度上下文的过程。看看扩展点是怎么工作的。这个时候又需要提到官网的调度上下文的图。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/scheduling-framework-extensions.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图1：Pod的调度上下文</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework</center>
<p>而 <em>scheduler</em> 对于调度上下文来就是这个 <code>scheduleOne </code> ，下面就是看这个调度上下文</p>
<h3 id="sort">Sort</h3>
<p><code>Sort</code> 插件提供了排序功能，用于对在调度队列中待处理 Pod 进行排序。一次只能启用一个队列排序。</p>
<p>在进入 <code>scheduleOne</code> 后，<code>NextPod</code> 从 <code>activeQ</code> 中队列中得到一个Pod，然后的 <code>frameworkForPod</code> 会做打分的动作就是调度上下文的第一个扩展点 <code>sort</code></p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	podInfo := sched.NextPod()
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
	fwk, err := sched.frameworkForPod(pod)
...
    
func (sched *Scheduler) frameworkForPod(pod *v1.Pod) (framework.Framework, error) {
    // 获取指定的profile
	fwk, ok := sched.Profiles[pod.Spec.SchedulerName]
	if !ok {
		return nil, fmt.Errorf(&quot;profile not found for scheduler name %q&quot;, pod.Spec.SchedulerName)
	}
	return fwk, nil
}
</code></pre>
<p>回顾，因为在New scheduler时会初始化这个 sort 函数</p>
<pre><code class="language-go">podQueue := internalqueue.NewSchedulingQueue(
    profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
    informerFactory,
    internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
    internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
    internalqueue.WithPodNominator(nominator),
    internalqueue.WithClusterEventMap(clusterEventMap),
    internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
)
</code></pre>
<h3 id="prefilter">preFilter</h3>
<p>preFilter作为第一个扩展点，是用于在过滤之前预处理或检查 Pod 或集群的相关信息。这里会终止调度</p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	podInfo := sched.NextPod()
	// pod could be nil when schedulerQueue is closed
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
	fwk, err := sched.frameworkForPod(pod)
	if err != nil {
		// This shouldn't happen, because we only accept for scheduling the pods
		// which specify a scheduler name that matches one of the profiles.
		klog.ErrorS(err, &quot;Error occurred&quot;)
		return
	}
	if sched.skipPodSchedule(fwk, pod) {
		return
	}

	klog.V(3).InfoS(&quot;Attempting to schedule pod&quot;, &quot;pod&quot;, klog.KObj(pod))

	// Synchronously attempt to find a fit for the pod.
	start := time.Now()
	state := framework.NewCycleState()
	state.SetRecordPluginMetrics(rand.Intn(100) &lt; pluginMetricsSamplePercent)
	// Initialize an empty podsToActivate struct, which will be filled up by plugins or stay empty.
	podsToActivate := framework.NewPodsToActivate()
	state.Write(framework.PodsToActivateKey, podsToActivate)

	schedulingCycleCtx, cancel := context.WithCancel(ctx)
	defer cancel()
    // 这里将进入prefilter
	scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L311-L360" target="_blank"
   rel="noopener nofollow noreferrer" >schedulePod</a> 尝试将给定的 pod 调度到节点列表中的节点之一。如果成功，它将返回节点的名称。</p>
<pre><code class="language-go">func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New(&quot;Scheduling&quot;, utiltrace.Field{Key: &quot;namespace&quot;, Value: pod.Namespace}, utiltrace.Field{Key: &quot;name&quot;, Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)
	// 用于将cache更新为当前内容
	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
		return result, err
	}
	trace.Step(&quot;Snapshotting scheduler cache and node infos done&quot;)

	if sched.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}
	// 找到一个合适的pod时，会执行扩展点
	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	
    ...
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L364-L426" target="_blank"
   rel="noopener nofollow noreferrer" >findNodesThatFitPod</a> 会执行对应的过滤插件来找到最适合的Node，包括备注，以及方法名都可以看到，这里运行的插件😁😁，后面会分析算法内容，只对workflow学习。</p>
<pre><code class="language-go">func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
	diagnosis := framework.Diagnosis{
		NodeToStatusMap:      make(framework.NodeToStatusMap),
		UnschedulablePlugins: sets.NewString(),
	}

	// Run &quot;prefilter&quot; plugins.
	preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod)
	allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List()
	if err != nil {
		return nil, diagnosis, err
	}
	if !s.IsSuccess() {
		if !s.IsUnschedulable() {
			return nil, diagnosis, s.AsError()
		}
		// All nodes will have the same status. Some non trivial refactoring is
		// needed to avoid this copy.
		for _, n := range allNodes {
			diagnosis.NodeToStatusMap[n.Node().Name] = s
		}
		// Status satisfying IsUnschedulable() gets injected into diagnosis.UnschedulablePlugins.
		if s.FailedPlugin() != &quot;&quot; {
			diagnosis.UnschedulablePlugins.Insert(s.FailedPlugin())
		}
		return nil, diagnosis, nil
	}

	// &quot;NominatedNodeName&quot; can potentially be set in a previous scheduling cycle as a result of preemption.
	// This node is likely the only candidate that will fit the pod, and hence we try it first before iterating over all nodes.
	if len(pod.Status.NominatedNodeName) &gt; 0 {
		feasibleNodes, err := sched.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis)
		if err != nil {
			klog.ErrorS(err, &quot;Evaluation failed on nominated node&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, pod.Status.NominatedNodeName)
		}
		// Nominated node passes all the filters, scheduler is good to assign this node to the pod.
		if len(feasibleNodes) != 0 {
			return feasibleNodes, diagnosis, nil
		}
	}

	nodes := allNodes
	if !preRes.AllNodes() {
		nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames))
		for n := range preRes.NodeNames {
			nInfo, err := sched.nodeInfoSnapshot.NodeInfos().Get(n)
			if err != nil {
				return nil, diagnosis, err
			}
			nodes = append(nodes, nInfo)
		}
	}
	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)
	if err != nil {
		return nil, diagnosis, err
	}

	feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
	if err != nil {
		return nil, diagnosis, err
	}
	return feasibleNodes, diagnosis, nil
}
</code></pre>
<h3 id="filter">filter</h3>
<p>filter插件相当于<em>调度上下文</em>中的 <code>Predicates</code>，用于排除不能运行 Pod 的节点。Filter 会按配置的顺序进行调用。如果有一个filter将节点标记位不可用，则将 Pod 标记为不可调度（即不会向下执行）。</p>
<p>对于代码中来讲，filter还是处于 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L364-L426" target="_blank"
   rel="noopener nofollow noreferrer" >findNodesThatFitPod</a> 函数中，<code>findNodesThatPassFilters</code> 就是获取到 FN，即可行节点，而这个过程就是 <em>filter</em> 扩展点</p>
<pre><code class="language-go">func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
	...
    
	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)
	if err != nil {
		return nil, diagnosis, err
	}

	feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
	if err != nil {
		return nil, diagnosis, err
	}
	return feasibleNodes, diagnosis, nil
}
</code></pre>
<h3 id="postfilter">Postfilter</h3>
<p>当没有为 pod 找到<em>FN</em>时，该插件会按照配置的顺序进行调用。如果任何<code>postFilter</code>插件将 Pod 标记为<em>schedulable</em>，则不会调用其余插件。即 <code>filter</code> 成功后不会进行这步骤，那我们来验证下这里把😊</p>
<p>还是在 scheduleOne 中，当我们运行的 SchedulePod 完成后（成功或失败），这时会返回一个err，而 <code>postfilter</code> 会根据这个 err进行选择执行或不执行，符合官方给出的说法。</p>
<pre><code class="language-go">scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
	if err != nil {
		// SchedulePod() may have failed because the pod would not fit on any host, so we try to
		// preempt, with the expectation that the next time the pod is tried for scheduling it
		// will fit due to the preemption. It is also possible that a different pod will schedule
		// into the resources that were preempted, but this is harmless.
		var nominatingInfo *framework.NominatingInfo
		if fitError, ok := err.(*framework.FitError); ok {
			if !fwk.HasPostFilterPlugins() {
				klog.V(3).InfoS(&quot;No PostFilter plugins are registered, so no preemption will be performed&quot;)
			} else {
				// Run PostFilter plugins to try to make the pod schedulable in a future scheduling cycle.
				result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap)
				if status.Code() == framework.Error {
					klog.ErrorS(nil, &quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, status)
				} else {
					fitError.Diagnosis.PostFilterMsg = status.Message()
					klog.V(5).InfoS(&quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, status)
				}
				if result != nil {
					nominatingInfo = result.NominatingInfo
				}
			}
			// Pod did not fit anywhere, so it is counted as a failure. If preemption
			// succeeds, the pod should get counted as a success the next time we try to
			// schedule it. (hopefully)
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else if err == ErrNoNodesAvailable {
			nominatingInfo = clearNominatedNode
			// No nodes available is counted as unschedulable rather than an error.
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else {
			nominatingInfo = clearNominatedNode
			klog.ErrorS(err, &quot;Error selecting node for pod&quot;, &quot;pod&quot;, klog.KObj(pod))
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
		}
		sched.handleSchedulingFailure(ctx, fwk, podInfo, err, v1.PodReasonUnschedulable, nominatingInfo)
		return
	}
</code></pre>
<h3 id="prescorescore">PreScore,Score</h3>
<p>可用于进行预Score工作，作为通知性的扩展点，会在在filter完之后直接会关联 preScore 插件进行继续工作，而不是返回，如果配置的这些插件有任何一个返回失败，则Pod将被拒绝。</p>
<pre><code class="language-go">
func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New(&quot;Scheduling&quot;, utiltrace.Field{Key: &quot;namespace&quot;, Value: pod.Namespace}, utiltrace.Field{Key: &quot;name&quot;, Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)

	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
		return result, err
	}
	trace.Step(&quot;Snapshotting scheduler cache and node infos done&quot;)

	if sched.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}

	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step(&quot;Computing predicates done&quot;)

	if len(feasibleNodes) == 0 {
		return result, &amp;framework.FitError{
			Pod:         pod,
			NumAllNodes: sched.nodeInfoSnapshot.NumNodes(),
			Diagnosis:   diagnosis,
		}
	}

	// When only one node after predicate, just use it.
	if len(feasibleNodes) == 1 {
		return ScheduleResult{
			SuggestedHost:  feasibleNodes[0].Name,
			EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap),
			FeasibleNodes:  1,
		}, nil
	}
	// 这里会完成prescore，score
	priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes)
	if err != nil {
		return result, err
	}

	host, err := selectHost(priorityList)
	trace.Step(&quot;Prioritizing done&quot;)

	return ScheduleResult{
		SuggestedHost:  host,
		EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap),
		FeasibleNodes:  len(feasibleNodes),
	}, err
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L605-L705" target="_blank"
   rel="noopener nofollow noreferrer" >priorityNodes</a> 会通过配置的插件给Node打分，并返回每个Node的分数，将每个插件打分结果计算总和获得Node的分数，最后获得节点的加权总分数。</p>
<pre><code class="language-go">func prioritizeNodes(
	ctx context.Context,
	extenders []framework.Extender,
	fwk framework.Framework,
	state *framework.CycleState,
	pod *v1.Pod,
	nodes []*v1.Node,
) (framework.NodeScoreList, error) {
	// If no priority configs are provided, then all nodes will have a score of one.
	// This is required to generate the priority list in the required format
	if len(extenders) == 0 &amp;&amp; !fwk.HasScorePlugins() {
		result := make(framework.NodeScoreList, 0, len(nodes))
		for i := range nodes {
			result = append(result, framework.NodeScore{
				Name:  nodes[i].Name,
				Score: 1,
			})
		}
		return result, nil
	}

	// Run PreScore plugins.
	preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes)
	if !preScoreStatus.IsSuccess() {
		return nil, preScoreStatus.AsError()
	}

	// Run the Score plugins.
	scoresMap, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes)
	if !scoreStatus.IsSuccess() {
		return nil, scoreStatus.AsError()
	}

	// Additional details logged at level 10 if enabled.
	klogV := klog.V(10)
	if klogV.Enabled() {
		for plugin, nodeScoreList := range scoresMap {
			for _, nodeScore := range nodeScoreList {
				klogV.InfoS(&quot;Plugin scored node for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;plugin&quot;, plugin, &quot;node&quot;, nodeScore.Name, &quot;score&quot;, nodeScore.Score)
			}
		}
	}

	// Summarize all scores.
	result := make(framework.NodeScoreList, 0, len(nodes))

	for i := range nodes {
		result = append(result, framework.NodeScore{Name: nodes[i].Name, Score: 0})
		for j := range scoresMap {
			result[i].Score += scoresMap[j][i].Score
		}
	}

	if len(extenders) != 0 &amp;&amp; nodes != nil {
		var mu sync.Mutex
		var wg sync.WaitGroup
		combinedScores := make(map[string]int64, len(nodes))
		for i := range extenders {
			if !extenders[i].IsInterested(pod) {
				continue
			}
			wg.Add(1)
			go func(extIndex int) {
				metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Inc()
				defer func() {
					metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Dec()
					wg.Done()
				}()
				prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes)
				if err != nil {
					// Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities
					klog.V(5).InfoS(&quot;Failed to run extender's priority function. No score given by this extender.&quot;, &quot;error&quot;, err, &quot;pod&quot;, klog.KObj(pod), &quot;extender&quot;, extenders[extIndex].Name())
					return
				}
				mu.Lock()
				for i := range *prioritizedList {
					host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score
					if klogV.Enabled() {
						klogV.InfoS(&quot;Extender scored node for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;extender&quot;, extenders[extIndex].Name(), &quot;node&quot;, host, &quot;score&quot;, score)
					}
					combinedScores[host] += score * weight
				}
				mu.Unlock()
			}(i)
		}
		// wait for all go routines to finish
		wg.Wait()
		for i := range result {
			// MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore,
			// therefore we need to scale the score returned by extenders to the score range used by the scheduler.
			result[i].Score += combinedScores[result[i].Name] * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority)
		}
	}

	if klogV.Enabled() {
		for i := range result {
			klogV.InfoS(&quot;Calculated node's final score for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, result[i].Name, &quot;score&quot;, result[i].Score)
		}
	}
	return result, nil
}
</code></pre>
<h3 id="reserve">Reserve</h3>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L153-L163" target="_blank"
   rel="noopener nofollow noreferrer" >Reserve</a> 因为绑定事件时异步发生的，该插件是为了避免Pod在绑定到节点前时，调度到新的Pod，使节点使用资源超过可用资源情况。如果后续阶段发生错误或失败，将触发 <code>UnReserve</code> 回滚（通知性扩展点）。这也是作为调度周期中最后一个状态，要么成功到 <code>postBind</code> ，要么失败触发 <code>UnReserve</code>。</p>
<pre><code class="language-go">// Run the Reserve method of reserve plugins.
if sts := fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() { // 当处理不成功时
    metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
    // 触发 un-reserve 来清理相关Pod的状态
    fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
    if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
        klog.ErrorS(forgetErr, &quot;Scheduler cache ForgetPod failed&quot;)
    }
    sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, sts.AsError(), SchedulerError, clearNominatedNode)
    return
}
</code></pre>
<h3 id="permit">permit</h3>
<p><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L165-L183" target="_blank"
   rel="noopener nofollow noreferrer" >Permit</a> 插件可以阻止或延迟 Pod 的绑定</p>
<pre><code class="language-go">	// Run &quot;permit&quot; plugins.
	runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
	if !runPermitStatus.IsWait() &amp;&amp; !runPermitStatus.IsSuccess() {
		var reason string
		if runPermitStatus.IsUnschedulable() {
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
			reason = v1.PodReasonUnschedulable
		} else {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
			reason = SchedulerError
		}
        // 只要其中一个插件返回的状态不是 success 或者 wait
		fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
        // 从cache中忘掉pod
		if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
			klog.ErrorS(forgetErr, &quot;Scheduler cache ForgetPod failed&quot;)
		}
		sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, runPermitStatus.AsError(), reason, clearNominatedNode)
		return
	}

</code></pre>
<h3 id="binding-cycle">Binding Cycle</h3>
<p>在选择好 <em>FN</em> 后则做一个假设绑定，并更新到cache中，接下来回去执行真正的bind操作，也就是 <code>binding cycle</code></p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	...
    ...
	// binding cycle 是一个异步的操作，这里表现就是go协程
	go func() {
		bindingCycleCtx, cancel := context.WithCancel(ctx)
		defer cancel()
		metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Inc()
		defer metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Dec()
		// 运行WaitOnPermit插件，如果失败则，unReserve回滚
		waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod)
		if !waitOnPermitStatus.IsSuccess() {
			var reason string
			if waitOnPermitStatus.IsUnschedulable() {
				metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
				reason = v1.PodReasonUnschedulable
			} else {
				metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
				reason = SchedulerError
			}
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
				klog.ErrorS(forgetErr, &quot;scheduler cache ForgetPod failed&quot;)
			} else {
				// &quot;Forget&quot;ing an assumed Pod in binding cycle should be treated as a PodDelete event,
				// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
				// TODO(#103853): de-duplicate the logic.
				// Avoid moving the assumed Pod itself as it's always Unschedulable.
				// It's intentional to &quot;defer&quot; this operation; otherwise MoveAllToActiveOrBackoffQueue() would
				// update `q.moveRequest` and thus move the assumed pod to backoffQ anyways.
				defer sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, func(pod *v1.Pod) bool {
					return assumedPod.UID != pod.UID
				})
			}
			sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, waitOnPermitStatus.AsError(), reason, clearNominatedNode)
			return
		}

	// 运行Prebind 插件
		preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		if !preBindStatus.IsSuccess() {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
			// trigger un-reserve plugins to clean up state associated with the reserved Pod
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil {
				klog.ErrorS(forgetErr, &quot;scheduler cache ForgetPod failed&quot;)
			} else {
				// &quot;Forget&quot;ing an assumed Pod in binding cycle should be treated as a PodDelete event,
				// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
				// TODO(#103853): de-duplicate the logic.
				sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, nil)
			}
			sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, preBindStatus.AsError(), SchedulerError, clearNominatedNode)
			return
		}
		// bind是真正的绑定操作
		err := sched.bind(bindingCycleCtx, fwk, assumedPod, scheduleResult.SuggestedHost, state)
		if err != nil {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
			// 如果失败了就触发 un-reserve plugins 
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			if err := sched.Cache.ForgetPod(assumedPod); err != nil {
				klog.ErrorS(err, &quot;scheduler cache ForgetPod failed&quot;)
			} else {
				// &quot;Forget&quot;ing an assumed Pod in binding cycle should be treated as a PodDelete event,
				// as the assumed Pod had occupied a certain amount of resources in scheduler cache.
				// TODO(#103853): de-duplicate the logic.
				sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, nil)
			}
			sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, fmt.Errorf(&quot;binding rejected: %w&quot;, err), SchedulerError, clearNominatedNode)
			return
		}
		// Calculating nodeResourceString can be heavy. Avoid it if klog verbosity is below 2.
		klog.V(2).InfoS(&quot;Successfully bound pod to node&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, scheduleResult.SuggestedHost, &quot;evaluatedNodes&quot;, scheduleResult.EvaluatedNodes, &quot;feasibleNodes&quot;, scheduleResult.FeasibleNodes)
		metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start))
		metrics.PodSchedulingAttempts.Observe(float64(podInfo.Attempts))
		metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(podInfo)).Observe(metrics.SinceInSeconds(podInfo.InitialAttemptTimestamp))

		// 运行 &quot;postbind&quot; 插件
        // 是通知性的扩展点，该插件在绑定 Pod 后调用，可用于清理相关资源（）。
		fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)

		// At the end of a successful binding cycle, move up Pods if needed.
		if len(podsToActivate.Map) != 0 {
			sched.SchedulingQueue.Activate(podsToActivate.Map)
			// Unlike the logic in scheduling cycle, we don't bother deleting the entries
			// as `podsToActivate.Map` is no longer consumed.
		}
	}()
}
</code></pre>
<h2 id="调度上下文中的失败流程">调度上下文中的失败流程</h2>
<p>上面说到的都是正常的请求，下面会对失败的请求是如何重试的进行分析，而 <em>scheduler</em> 中关于失败处理方面相关的属性会涉及到上面 <em>scheduler</em> 结构中的  <code>backoffQ</code> 与 <code>unschedulablePods </code></p>
<ul>
<li><code>backoffQ</code>：也是一个 <em>heap</em> 类型的优先级队列，存放的是不可调度的Pod</li>
<li><code>unschedulablePods </code>：保存确定不可被调度的Pod，一个map类型</li>
</ul>
<p>backoffQ 与  unschedulablePods 会在初始化 <em>scheduler</em> 时初始化，</p>
<pre><code class="language-go">func NewPriorityQueue(
	lessFn framework.LessFunc,
	informerFactory informers.SharedInformerFactory,
	opts ...Option,
) *PriorityQueue {
	options := defaultPriorityQueueOptions
	for _, opt := range opts {
		opt(&amp;options)
	}

	comp := func(podInfo1, podInfo2 interface{}) bool {
		pInfo1 := podInfo1.(*framework.QueuedPodInfo)
		pInfo2 := podInfo2.(*framework.QueuedPodInfo)
		return lessFn(pInfo1, pInfo2)
	}

	if options.podNominator == nil {
		options.podNominator = NewPodNominator(informerFactory.Core().V1().Pods().Lister())
	}

	pq := &amp;PriorityQueue{
		PodNominator:                      options.podNominator,
		clock:                             options.clock,
		stop:                              make(chan struct{}),
		podInitialBackoffDuration:         options.podInitialBackoffDuration,
		podMaxBackoffDuration:             options.podMaxBackoffDuration,
		podMaxInUnschedulablePodsDuration: options.podMaxInUnschedulablePodsDuration,
		activeQ:                           heap.NewWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()),
		unschedulablePods:                 newUnschedulablePods(metrics.NewUnschedulablePodsRecorder()),
		moveRequestCycle:                  -1,
		clusterEventMap:                   options.clusterEventMap,
	}
	pq.cond.L = &amp;pq.lock
    // 初始化backoffQ
    // NewWithRecorder作为一个可选的 metricRecorder 的 Heap 对象。
    // podInfoKeyFunc是一个函数，返回错误与字符串
    // pq.podsCompareBackoffCompleted 比较两个pod的回退时间，如果第一个在第二个之前为true，
    // 反之 false
	pq.podBackoffQ = heap.NewWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder())
	pq.nsLister = informerFactory.Core().V1().Namespaces().Lister()

	return pq
}
</code></pre>
<p>对于初始化 backoffQ 会产生的两个函数，<a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L757-L761" target="_blank"
   rel="noopener nofollow noreferrer" >getBackoffTime</a> 与 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L765-L775" target="_blank"
   rel="noopener nofollow noreferrer" >calculateBackoffDuration</a></p>
<pre><code class="language-go">// getBackoffTime returns the time that podInfo completes backoff
func (p *PriorityQueue) getBackoffTime(podInfo *framework.QueuedPodInfo) time.Time {
	duration := p.calculateBackoffDuration(podInfo)
	backoffTime := podInfo.Timestamp.Add(duration)
	return backoffTime
}

// calculateBackoffDuration is a helper function for calculating the backoffDuration
// based on the number of attempts the pod has made.
func (p *PriorityQueue) calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration {
	duration := p.podInitialBackoffDuration
	for i := 1; i &lt; podInfo.Attempts; i++ {
		// Use subtraction instead of addition or multiplication to avoid overflow.
		if duration &gt; p.podMaxBackoffDuration-duration {
			return p.podMaxBackoffDuration
		}
		duration += duration
	}
	return duration
}
</code></pre>
<p>对于整个故障错误会按照如下流程进行，在初始化 <em>scheduler</em> 会注册一个 Error 函数，这个函数用作对不可调度Pod进行处理，实际上被注册的函数是 MakeDefaultErrorFunc。这个函数将作为 Error 函数被调用。</p>
<pre><code class="language-go">sched := newScheduler(
    schedulerCache,
    extenders,
    internalqueue.MakeNextPodFunc(podQueue),
    MakeDefaultErrorFunc(client, podLister, podQueue, schedulerCache),
    stopEverything,
    podQueue,
    profiles,
    client,
    snapshot,
    options.percentageOfNodesToScore,
)
</code></pre>
<p>而在 调度周期中，也就是 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L66-L132" target="_blank"
   rel="noopener nofollow noreferrer" >scheduleOne</a> 可以看到，每个扩展点操作失败后都会调用 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/schedule_one.go#L812-L834" target="_blank"
   rel="noopener nofollow noreferrer" >handleSchedulingFailure</a> 而该函数，使用了注册的 <em>Error</em> 函数来处理Pod</p>
<pre><code class="language-go">func (sched *Scheduler) scheduleOne(ctx context.Context) {
	...
	defer cancel()
	scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod)
	if err != nil {

		var nominatingInfo *framework.NominatingInfo
		if fitError, ok := err.(*framework.FitError); ok {
			if !fwk.HasPostFilterPlugins() {
				klog.V(3).InfoS(&quot;No PostFilter plugins are registered, so no preemption will be performed&quot;)
			} else {
			
				result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap)
				if status.Code() == framework.Error {
					klog.ErrorS(nil, &quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, status)
				} else {
					fitError.Diagnosis.PostFilterMsg = status.Message()
					klog.V(5).InfoS(&quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, status)
				}
				if result != nil {
					nominatingInfo = result.NominatingInfo
				}
			}
	
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else if err == ErrNoNodesAvailable {
			nominatingInfo = clearNominatedNode
			// No nodes available is counted as unschedulable rather than an error.
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else {
			nominatingInfo = clearNominatedNode
			klog.ErrorS(err, &quot;Error selecting node for pod&quot;, &quot;pod&quot;, klog.KObj(pod))
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
		}
        // 处理不可调度Pod
		sched.handleSchedulingFailure(ctx, fwk, podInfo, err, v1.PodReasonUnschedulable, nominatingInfo)
		return
	}

</code></pre>
<p>来到了注册的 <em>Error</em> 函数 <code>MakeDefaultErrorFunc</code></p>
<pre><code class="language-go">func MakeDefaultErrorFunc(client clientset.Interface, podLister corelisters.PodLister, podQueue internalqueue.SchedulingQueue, schedulerCache internalcache.Cache) func(*framework.QueuedPodInfo, error) {
	return func(podInfo *framework.QueuedPodInfo, err error) {
		pod := podInfo.Pod
		if err == ErrNoNodesAvailable {
			klog.V(2).InfoS(&quot;Unable to schedule pod; no nodes are registered to the cluster; waiting&quot;, &quot;pod&quot;, klog.KObj(pod))
		} else if fitError, ok := err.(*framework.FitError); ok {
			// Inject UnschedulablePlugins to PodInfo, which will be used later for moving Pods between queues efficiently.
			podInfo.UnschedulablePlugins = fitError.Diagnosis.UnschedulablePlugins
			klog.V(2).InfoS(&quot;Unable to schedule pod; no fit; waiting&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, err)
		} else if apierrors.IsNotFound(err) {
			klog.V(2).InfoS(&quot;Unable to schedule pod, possibly due to node not found; waiting&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, err)
			if errStatus, ok := err.(apierrors.APIStatus); ok &amp;&amp; errStatus.Status().Details.Kind == &quot;node&quot; {
				nodeName := errStatus.Status().Details.Name
				// when node is not found, We do not remove the node right away. Trying again to get
				// the node and if the node is still not found, then remove it from the scheduler cache.
				_, err := client.CoreV1().Nodes().Get(context.TODO(), nodeName, metav1.GetOptions{})
				if err != nil &amp;&amp; apierrors.IsNotFound(err) {
					node := v1.Node{ObjectMeta: metav1.ObjectMeta{Name: nodeName}}
					if err := schedulerCache.RemoveNode(&amp;node); err != nil {
						klog.V(4).InfoS(&quot;Node is not found; failed to remove it from the cache&quot;, &quot;node&quot;, node.Name)
					}
				}
			}
		} else {
			klog.ErrorS(err, &quot;Error scheduling pod; retrying&quot;, &quot;pod&quot;, klog.KObj(pod))
		}

		// Check if the Pod exists in informer cache.
		cachedPod, err := podLister.Pods(pod.Namespace).Get(pod.Name)
		if err != nil {
			klog.InfoS(&quot;Pod doesn't exist in informer cache&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, err)
			return
		}

		// In the case of extender, the pod may have been bound successfully, but timed out returning its response to the scheduler.
		// It could result in the live version to carry .spec.nodeName, and that's inconsistent with the internal-queued version.
		if len(cachedPod.Spec.NodeName) != 0 {
			klog.InfoS(&quot;Pod has been assigned to node. Abort adding it back to queue.&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, cachedPod.Spec.NodeName)
			return
		}

		// As &lt;cachedPod&gt; is from SharedInformer, we need to do a DeepCopy() here.
		podInfo.PodInfo = framework.NewPodInfo(cachedPod.DeepCopy())
        // 添加到unschedulable队列中
		if err := podQueue.AddUnschedulableIfNotPresent(podInfo, podQueue.SchedulingCycle()); err != nil {
			klog.ErrorS(err, &quot;Error occurred&quot;)
		}
	}
}
</code></pre>
<p>下面来到 <code>AddUnschedulableIfNotPresent</code> ，这个也是操作 <code>backoffQ</code> 和 <code>unschedulablePods </code> 的真正的动作</p>
<p><code>AddUnschedulableIfNotPresent</code> 函数会吧无法调度的 pod 插入队列，除非它已经在队列中。通常情况下，<code>PriorityQueue</code> 将不可调度的 Pod 放在 <code>unschedulablePods</code> 中。但如果最近有 move request，则将 pod 放入 <code>podBackoffQ</code> 中。</p>
<pre><code class="language-go">func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error {
	p.lock.Lock()
	defer p.lock.Unlock()
	pod := pInfo.Pod
    // 如果已经存在则不添加
	if p.unschedulablePods.get(pod) != nil {
		return fmt.Errorf(&quot;Pod %v is already present in unschedulable queue&quot;, klog.KObj(pod))
	}
	// 检查是否在activeQ中
	if _, exists, _ := p.activeQ.Get(pInfo); exists {
		return fmt.Errorf(&quot;Pod %v is already present in the active queue&quot;, klog.KObj(pod))
	}
    // 检查是否在podBackoffQ中
	if _, exists, _ := p.podBackoffQ.Get(pInfo); exists {
		return fmt.Errorf(&quot;Pod %v is already present in the backoff queue&quot;, klog.KObj(pod))
	}

	// 在重新添加时，会刷新 Pod时间为最新操作的时间
	pInfo.Timestamp = p.clock.Now()

	for plugin := range pInfo.UnschedulablePlugins {
		metrics.UnschedulableReason(plugin, pInfo.Pod.Spec.SchedulerName).Inc()
	}
    // 如果接受到move request那么则放入BackoffQ
	if p.moveRequestCycle &gt;= podSchedulingCycle {
		if err := p.podBackoffQ.Add(pInfo); err != nil {
			return fmt.Errorf(&quot;error adding pod %v to the backoff queue: %v&quot;, pod.Name, err)
		}
		metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;backoff&quot;, ScheduleAttemptFailure).Inc()
	} else {
        // 否则将放入到 unschedulablePods
		p.unschedulablePods.addOrUpdate(pInfo)
		metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;unschedulable&quot;, ScheduleAttemptFailure).Inc()

	}

	p.PodNominator.AddNominatedPod(pInfo.PodInfo, nil)
	return nil
}
</code></pre>
<p>在启动 <em>scheduler</em> 时，会将这两个队列异步启用两个loop来操作队列。表现在 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L292-L295" target="_blank"
   rel="noopener nofollow noreferrer" >Run()</a></p>
<pre><code class="language-go">func (p *PriorityQueue) Run() {
	go wait.Until(p.flushBackoffQCompleted, 1.0*time.Second, p.stop)
	go wait.Until(p.flushUnschedulablePodsLeftover, 30*time.Second, p.stop)
}
</code></pre>
<p>可以看到 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L431-L458" target="_blank"
   rel="noopener nofollow noreferrer" >flushBackoffQCompleted</a> 作为 <code>BackoffQ</code> 实现；而 <a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/pkg/scheduler/internal/queue/scheduling_queue.go#L462-L478" target="_blank"
   rel="noopener nofollow noreferrer" >flushUnschedulablePodsLeftover</a> 作为 <code>UnschedulablePods</code> 实现。</p>
<p><code>flushBackoffQCompleted</code> 是用于将所有已完成回退的 pod 从 <code>backoffQ</code> 移到 <code>activeQ</code> 中</p>
<pre><code class="language-go">func (p *PriorityQueue) flushBackoffQCompleted() {
	p.lock.Lock()
	defer p.lock.Unlock()
	broadcast := false
	for { // 这就是heap实现的方法，窥视下，但不弹出
		rawPodInfo := p.podBackoffQ.Peek()
		if rawPodInfo == nil {
			break
		}
		pod := rawPodInfo.(*framework.QueuedPodInfo).Pod
		boTime := p.getBackoffTime(rawPodInfo.(*framework.QueuedPodInfo))
		if boTime.After(p.clock.Now()) {
			break
		}
		_, err := p.podBackoffQ.Pop() // 弹出一个
		if err != nil {
			klog.ErrorS(err, &quot;Unable to pop pod from backoff queue despite backoff completion&quot;, &quot;pod&quot;, klog.KObj(pod))
			break
		}
		p.activeQ.Add(rawPodInfo) // 放入到活动队列中
		metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;active&quot;, BackoffComplete).Inc()
		broadcast = true
	}

	if broadcast {
		p.cond.Broadcast()
	}
}
</code></pre>
<p><code>flushUnschedulablePodsLeftover </code> 函数用于将在 <code>unschedulablePods</code> 中的存放时间超过 <code>podMaxInUnschedulablePodsDuration</code> 值的 pod 移动到 <code>backoffQ</code> 或 <code>activeQ</code> 中。</p>
<p><code>podMaxInUnschedulablePodsDuration</code> 会根据配置传入，当没有传入，也就是使用了 <em><a href="https://github.com/kubernetes/kubernetes/blob/32c483ea6ee90a3a81f382563c91034470af8a4a/cmd/kube-scheduler/app/options/options.go#L77-L84" target="_blank"
   rel="noopener nofollow noreferrer" >Deprecated</a></em> 那么会为5分钟。</p>
<pre><code class="language-go">func NewOptions() *Options {
	o := &amp;Options{
		SecureServing:  apiserveroptions.NewSecureServingOptions().WithLoopback(),
		Authentication: apiserveroptions.NewDelegatingAuthenticationOptions(),
		Authorization:  apiserveroptions.NewDelegatingAuthorizationOptions(),
		Deprecated: &amp;DeprecatedOptions{
			PodMaxInUnschedulablePodsDuration: 5 * time.Minute,
		},
</code></pre>
<p>对于 <code>flushUnschedulablePodsLeftover </code> 就是做一个时间对比，然后添加到对应的队列中</p>
<pre><code class="language-go">func (p *PriorityQueue) flushUnschedulablePodsLeftover() {
	p.lock.Lock()
	defer p.lock.Unlock()

	var podsToMove []*framework.QueuedPodInfo
	currentTime := p.clock.Now()
	for _, pInfo := range p.unschedulablePods.podInfoMap {
		lastScheduleTime := pInfo.Timestamp
		if currentTime.Sub(lastScheduleTime) &gt; p.podMaxInUnschedulablePodsDuration {
			podsToMove = append(podsToMove, pInfo)
		}
	}

	if len(podsToMove) &gt; 0 {
		p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout)
	}
}
</code></pre>
<h2 id="总结调度上下文流程">总结调度上下文流程</h2>
<ul>
<li>在构建一个 <em>scheduler</em> 时经历如下步骤：
<ul>
<li>准备cache，informer，queue，错误处理函数等</li>
<li>添加事件函数，会监听资源（如Pod），当有变动则触发对应事件函数，这是入站 <code>activeQ</code></li>
</ul>
</li>
<li>构建完成后会 run，run时会run一个 <code>SchedulingQueue</code>，这个是作为不可调度队列
<ul>
<li><code>BackoffQ</code></li>
<li><code>UnschedulablePods</code></li>
<li>不可调度队列会根据注册时定期消费队列中Pod将其添加到 <code>activeQ</code> 中</li>
</ul>
</li>
<li>启动一个 <code>scheduleOne</code> 的loop，这个是调度上下文中所有的扩展点的执行，也是 <code>activeQ</code> 的消费端
<ul>
<li><code>scheduleOne</code> 获取 pod</li>
<li>执行各个扩展点，如果出错则 <em>Error</em> 函数 <code>MakeDefaultErrorFunc</code> 将其添加到不可调度队列中</li>
<li>回到不可调度队列中消费部分</li>
</ul>
</li>
</ul>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://www.sobyte.net/post/2022-02/kubernetes-scheduling-framework-and-extender/" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes scheduler extender</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes的决策组件 - kube-scheduler原理分析</title>
      <link>https://www.oomkill.com/2022/07/ch16-scheduler/</link>
      <pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/07/ch16-scheduler/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview-supa-href11asup">Overview <sup><a href="#1">[1]</a></sup></h2>
<p>kubernetes集群中的调度程序 <code>kube-scheduler</code> 会 <code>watch</code> 未分配节点的新创建的Pod，并未该Pod找到可运行的最佳（特定）节点。那么这些动作或者说这些原理是怎么实现的呢，让我们往下剖析下。</p>
<p>对于新创建的 pod 或其他未调度的 pod来讲，kube-scheduler 选择一个最佳节点供它们运行。但是，Pod 中的每个容器对资源的要求都不同，每个 Pod 也有不同的要求。因此，需要根据具体的调度要求对现有节点进行过滤。</p>
<p>在Kubernetes集群中，满足 Pod 调度要求的节点称为可行节点 （<code> feasible nodes</code> <em>FN</em>） 。如果没有合适的节点，则 pod 将保持未调度状态，直到调度程序能够放置它。也就是说，当我们创建Pod时，如果长期处于 <code>Pending</code> 状态，这个时候应该看你的集群调度器是否因为某些问题没有合适的节点了</p>
<p>调度器为 Pod 找到 FN 后，然后运行一组函数对 FN 进行评分，并在 FN 中找到得分最高的节点来运行 Pod。</p>
<p>调度策略在决策时需要考虑的因素包括个人和集体资源需求、硬件/软件/策略约束 （<code>constraints</code>）、亲和性 (<code>affinity</code>) 和反亲和性（ <code>anti-affinity</code> ）规范、数据局部性、工作负载间干扰等。</p>
<h3 id="如何为pod选择节点">如何为pod选择节点？</h3>
<p><code>kube-scheduler</code> 为pod选择节点会分位两部：</p>
<ul>
<li>过滤 (<code>Filtering</code>)</li>
<li>打分 (<code>Scoring</code>)</li>
</ul>
<p>过滤也被称为预选 （<code>Predicates</code>），该步骤会找到可调度的节点集，然后通过是否满足特定资源的请求，例如通过 <code>PodFitsResources</code> 过滤器检查候选节点是否有足够的资源来满足 Pod 资源的请求。这个步骤完成后会得到一个包含合适的节点的列表（通常为多个），如果列表为空，则Pod不可调度。</p>
<p>打分也被称为优选（<code>Priorities</code>），在该步骤中，会对上一个步骤的输出进行打分，Scheduer 通过打分的规则为每个通过 <code>Filtering</code> 步骤的节点计算出一个分数。</p>
<p>完成上述两个步骤之后，<code>kube-scheduler</code> 会将Pod分配给分数最高的 Node，如果存在多个相同分数的节点，会随机选择一个。</p>
<h2 id="kubernetes的调度策略">kubernetes的调度策略</h2>
<p>Kubernetes 1.21之前版本可以在代码 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.21/pkg/scheduler/algorithmprovider/registry.go" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes\pkg\scheduler\algorithmprovider\registry.go</a> 中看到对应的注册模式，在1.22 scheduler 更换了其路径，对于registry文件更换到了<a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/pkg/scheduler/framework/plugins/registry.go" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes\pkg\scheduler\framework\plugins\registry.go</a> ；对于kubernetes官方说法为，<em>调度策略是用于“预选” (<code>Predicates </code>)或 过滤（<code>filtering </code>） 和 用于 优选（<code>Priorities</code>）或 评分 (<code>scoring</code>)的</em></p>
<blockquote>
<p>注：kubernetes官方没有找到预选和优选的概念，而Predicates和filtering 是处于预选阶段的动词，而Priorities和scoring是优选阶段的动词。后面用PF和PS代替这个两个词。</p>
</blockquote>
<h3 id="为pod预选节点-supa-href22asup">为Pod预选节点 <sup><a href="#2">[2]</a></sup></h3>
<p>上面也提到了，<code>filtering</code> 的目的是为了排除（过滤）掉不满足 Pod 要求的节点。例如，某个节点上的闲置资源小于 Pod 所需资源，则该节点不会被考虑在内，即被过滤掉。在 <em>“Predicates”</em> 阶段实现的 <em>filtering</em> 策略，包括：</p>
<ul>
<li><code>NoDiskConflict</code> ：评估是否有合适Pod请求的卷</li>
<li><code>NoVolumeZoneConflict</code>：在给定zone限制情况下，评估Pod请求所需的卷在Node上是否可用</li>
<li><code>PodFitsResources</code>：检查空闲资源（CPU、内存）是否满足Pod请求</li>
<li><code>PodFitsHostPorts</code>：检查Pod所需端口在Node上是否被占用</li>
<li><code>HostName</code>： 过滤除去，<code>PodSpec</code> 中 <code>NodeName</code> 字段中指定的Node之外的所有Node。</li>
<li><code>MatchNodeSelector</code>：检查Node的 <em>label</em> 是否与 <em>Pod</em> 配置中 <code>nodeSelector</code>字段中指定的 <em>label</em> 匹配，并且从 Kubernetes v1.2 开始， 如果存在 <code>nodeAffinity</code> 也会匹配。</li>
<li><code>CheckNodeMemoryPressure</code>：检查是否可以在已出现内存压力情况节点上调度 Pod。</li>
<li><code>CheckNodeDiskPressure</code>：检查是否可以在报告磁盘压力情况的节点上调度 Pod</li>
</ul>
<p>具体对应得策略可以在 kubernetes\pkg\scheduler\framework\plugins\registry.go 看到</p>
<h3 id="对预选节点打分-supa-href22asup">对预选节点打分 <sup><a href="#2">[2]</a></sup></h3>
<p>通过上面步骤过滤过得列表则是适合托管的Pod，这个结果通常来说是一个列表，如何选择最优Node进行调度，则是接下来打分的步骤步骤。</p>
<p>例如：Kubernetes对剩余节点进行优先级排序，优先级由一组函数计算；优先级函数将为剩余节点给出从<code>0~10</code> 的分数，10 表示最优，0 表示最差。每个优先级函数由一个正数加权组成，每个Node的得分是通过将所有加权得分相加来计算的。设有两个优先级函数，<code>priorityFunc1</code> 和 <code>priorityFunc2</code> 加上权重因子 <code>weight1</code> 和<code>weight2</code>，那么这个Node的最终得分为：$finalScore = (w1 \times priorityFunc1) + (w2 \times priorityFunc2)$。计算完分数后，选择最高分数的Node做为Pod的宿主机，存在多个相同分数Node情况下会随机选择一个Node。</p>
<p>目前kubernetes提供了一些在打分 <em>Scoring</em> 阶段算法：</p>
<ul>
<li><code>LeastRequestedPriority</code>：Node的优先级基于Node的空闲部分$\frac{capacity\ -\  Node上所有存在的Pod\ -\ 正在调度的Pod请求}{capacity}$，通过计算具有最高分数的Node是FN</li>
<li><code>BalancedResourceAllocation</code> ：该算法会将 Pod 放在一个Node上，使得在Pod 部署后 CPU 和内存的使用率为平衡的</li>
<li><code>SelectorSpreadPriority</code>：通过最小化资源方式，将属于同一种服务、控制器或同一Node上的Replica的 Pod的数量来分布Pod。如果节点上存在Zone，则会调整优先级，以便 pod可以分布在Zone之上。</li>
<li><code>CalculateAntiAffinityPriority</code>：根据label来分布，按照相同service上相同label值的pod进行分配</li>
<li><code>ImageLocalityPriority</code> ：根据Node上镜像进行打分，Node上存在Pod请求所需的镜像优先级较高。</li>
</ul>
<h3 id="在代码中查看上述的代码">在代码中查看上述的代码</h3>
<p>以 <code>PodFitsHostPorts</code> 算法为例，因为是Node类算法，在<a href="https://github.com/kubernetes/kubernetes/tree/release-1.23/pkg/scheduler/framework/plugins/nodeports" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes\pkg\scheduler\framework\plugins\nodeports</a></p>
<h2 id="调度框架-supa-href33asup">调度框架 <sup><a href="#3">[3]</a></sup></h2>
<p>调度框架 (<code>scheduling framework</code> <em>SF</em> ) 是kubernetes为 scheduler设计的一个pluggable的架构。SF 将scheduler设计为 <em>Plugin</em> 式的 API，API将上一章中提到的一些列调度策略实现为 <code>Plugin</code>。</p>
<p>在 <em>SF</em> 中，定义了一些扩展点 （<code>extension points</code> <em>EP</em> ），而被实现为Plugin的调度程序将被注册在一个或多个 <em>EP</em> 中，换句话来说，在这些 <em>EP</em> 的执行过程中如果注册在多个 <em>EP</em> 中，将会在多个 <em>EP</em> 被调用。</p>
<p>每次调度都分为两个阶段，调度周期（<code>Scheduling Cycel</code>）与绑定周期（<code>Binding Cycle</code>）。</p>
<ul>
<li><em>SC</em> 表示为，为Pod选择一个节点；<em>SC</em> 是串行运行的。</li>
<li><em>BC</em> 表示为，将 <em>SC</em> 决策结果应用于集群中；<em>BC</em> 可以同时运行。</li>
</ul>
<p>调度周期与绑定周期结合一起，被称为<strong>调度上下文</strong> （<code>Scheduling Context</code>）,下图则是调度上下文的工作流</p>
<blockquote>
<p>注：如果决策结果为Pod的调度结果无可用节点，或存在内部错误，则中止 <em>SC</em> 或 <em>BC</em>。Pod将重入队列重试</p>
</blockquote>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/scheduling-framework-extensions.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图1：Pod的调度上下文</center>
<center><em>Source：</em>https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework</center>
<h3 id="扩展点-supa-href44asup">扩展点 <sup><a href="#4">[4]</a></sup></h3>
<p>扩展点（<code>Extension points</code>）是指在<em>调度上下文</em>中的每个可扩展API，通过图提现为<a href="#podsc">[图1]</a>。其中 <code>Filter</code> 相当于 <code>Predicate</code> 而 <code>Scoring</code> 相当于 <code>Priority</code>。</p>
<p>对于调度阶段会通过以下扩展点：</p>
<ul>
<li>
<p><code>Sort</code>：该插件提供了排序功能，用于对在调度队列中待处理 Pod 进行排序。一次只能启用一个队列排序。</p>
</li>
<li>
<p><code>preFilter</code>：该插件用于在过滤之前预处理或检查 Pod 或集群的相关信息。这里会终止调度</p>
</li>
<li>
<p><code>filter</code>：该插件相当于<em>调度上下文</em>中的 <code>Predicates</code>，用于排除不能运行 Pod 的节点。Filter 会按配置的顺序进行调用。如果有一个filter将节点标记位不可用，则将 Pod 标记为不可调度（即不会向下执行）。</p>
</li>
<li>
<p><code>postFilter</code>：当没有为 pod 找到<em>FN</em>时，该插件会按照配置的顺序进行调用。如果任何<code>postFilter</code>插件将 Pod 标记为<em>schedulable</em>，则不会调用其余插件。即 <code>filter</code> 成功后不会进行这步骤</p>
</li>
<li>
<p><code>preScore</code>：可用于进行预Score工作（通知性的扩展点）。</p>
</li>
<li>
<p><code>score</code>：该插件为每个通过 <code>filter</code> 阶段的Node提供打分服务。然后Scheduler将选择具有最高加权分数总和的Node。</p>
</li>
<li>
<p><code>reserve</code>：因为绑定事件时异步发生的，该插件是为了避免Pod在绑定到节点前时，调度到新的Pod，使节点使用资源超过可用资源情况。如果后续阶段发生错误或失败，将触发 <code>UnReserve</code> 回滚（通知性扩展点）。这也是作为调度周期中最后一个状态，要么成功到 <code>postBind</code> ，要么失败触发 <code>UnReserve</code>。</p>
</li>
<li>
<p><code>permit</code>：该插件可以阻止或延迟 Pod 的绑定，一般情况下这步骤会做三件事：</p>
<ul>
<li><code>appove</code> ：调度器继续绑定过程</li>
<li><code>Deny</code>：如果任何一个Premit拒绝了Pod与节点的绑定，那么将触发 <code>UnReserve</code> ，并重入队列</li>
<li><code>Wait</code>： 如果 Permit 插件返回 <code>Wait</code>，该 Pod 将保留在内部 <code>Wait</code> Pod 列表中，直到被 <code>Appove</code>。如果发生超时，<code>wait</code> 变为 <code>deny</code> ，将Pod放回至调度队列中，并触发 <code>Unreserve</code> 回滚 。</li>
</ul>
</li>
<li>
<p><code>preBind</code>：该插件用于在 bind Pod 之前执行所需的前置工作。如，<code>preBind</code> 可能会提供一个网络卷并将其挂载到目标节点上。如果在该步骤中的任意插件返回错误，则Pod 将被 <code>deny</code> 并放置到调度队列中。</p>
</li>
<li>
<p><code>bind</code>：在所有的 <code>preBind</code> 完成后，该插件将用于将Pod绑定到Node，并按顺序调用绑定该步骤的插件。如果有一个插件处理了这个事件，那么则忽略其余所有插件。</p>
</li>
<li>
<p><code>postBind</code>：该插件在绑定 Pod 后调用，可用于清理相关资源（通知性的扩展点）。</p>
</li>
<li>
<p><code>multiPoint</code>：这是一个仅配置字段，允许同时为所有适用的扩展点启用或禁用插件。</p>
</li>
</ul>
<h2 id="kube-scheduler工作流分析">kube-scheduler工作流分析</h2>
<p>对于 <code>kube-scheduler</code> 组件的分析，包含 <code>kube-scheduler</code> 启动流程，以及scheduler调度流程。这里会主要针对启动流程分析，后面算法及二次开发部分会切入调度分析。</p>
<p>对于我们部署时使用的 <code>kube-scheduler</code> 位于 <a href="https://github.com/kubernetes/kubernetes/tree/release-1.24/cmd/kube-scheduler" target="_blank"
   rel="noopener nofollow noreferrer" >cmd/kube-scheduler</a> ，在 <em>Alpha (1.16)</em> 版本提供了调度框架的模式，到 <em>Stable (1.19)</em> ，从代码结构上是相似的；直到1.22后改变了代码风格。</p>
<p>首先看到的是 <code>kube-scheduler</code> 的入口 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.24/cmd/kube-scheduler/scheduler.go" target="_blank"
   rel="noopener nofollow noreferrer" >cmd/kube-scheduler</a> ，这里主要作为两部分，构建参数与启动<code>server</code> ,这里严格来讲 <code>kube-scheduer</code> 是作为一个server，而调度框架等部分是另外的。</p>
<pre><code class="language-go">func main() {
	command := app.NewSchedulerCommand()
	code := cli.Run(command)
	os.Exit(code)
}
</code></pre>
<p><code>cli.Run</code> 提供了cobra构成的命令行cli，日志将输出为标准输出</p>
<pre><code class="language-go">// 这里是main中执行的Run
func Run(cmd *cobra.Command) int {
	if logsInitialized, err := run(cmd); err != nil {
		if !logsInitialized {
			fmt.Fprintf(os.Stderr, &quot;Error: %v\n&quot;, err)
		} else {
			klog.ErrorS(err, &quot;command failed&quot;)
		}
		return 1
	}
	return 0
}
// 这个run作为
func run(cmd *cobra.Command) (logsInitialized bool, err error) {
	rand.Seed(time.Now().UnixNano())
	defer logs.FlushLogs()

	cmd.SetGlobalNormalizationFunc(cliflag.WordSepNormalizeFunc)

	if !cmd.SilenceUsage {
		cmd.SilenceUsage = true
		cmd.SetFlagErrorFunc(func(c *cobra.Command, err error) error {
			// Re-enable usage printing.
			c.SilenceUsage = false
			return err
		})
	}

	// In all cases error printing is done below.
	cmd.SilenceErrors = true

	// This is idempotent.
	logs.AddFlags(cmd.PersistentFlags())

	// Inject logs.InitLogs after command line parsing into one of the
	// PersistentPre* functions.
	switch {
	case cmd.PersistentPreRun != nil:
		pre := cmd.PersistentPreRun
		cmd.PersistentPreRun = func(cmd *cobra.Command, args []string) {
			logs.InitLogs()
			logsInitialized = true
			pre(cmd, args)
		}
	case cmd.PersistentPreRunE != nil:
		pre := cmd.PersistentPreRunE
		cmd.PersistentPreRunE = func(cmd *cobra.Command, args []string) error {
			logs.InitLogs()
			logsInitialized = true
			return pre(cmd, args)
		}
	default:
		cmd.PersistentPreRun = func(cmd *cobra.Command, args []string) {
			logs.InitLogs()
			logsInitialized = true
		}
	}

	err = cmd.Execute()
	return
}
</code></pre>
<p>可以看到最终是调用 <code>command.Execute() </code> 执行，这个是执行本身构建的命令，而真正被执行的则是上面的 <code>app.NewSchedulerCommand()</code> ,那么来看看这个是什么</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L72-L114" target="_blank"
   rel="noopener nofollow noreferrer" >app.NewSchedulerCommand()</a> 构建了一个cobra.Commond对象， <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L117-L142" target="_blank"
   rel="noopener nofollow noreferrer" >runCommand()</a> 被封装在内，这个是作为启动scheduler的函数</p>
<pre><code class="language-go">func NewSchedulerCommand(registryOptions ...Option) *cobra.Command {
	opts := options.NewOptions()

	cmd := &amp;cobra.Command{
		Use: &quot;kube-scheduler&quot;,
		Long: `The Kubernetes scheduler is a control plane process which assigns
Pods to Nodes. The scheduler determines which Nodes are valid placements for
each Pod in the scheduling queue according to constraints and available
resources. The scheduler then ranks each valid Node and binds the Pod to a
suitable Node. Multiple different schedulers may be used within a cluster;
kube-scheduler is the reference implementation.
See [scheduling](https://kubernetes.io/docs/concepts/scheduling-eviction/)
for more information about scheduling and the kube-scheduler component.`,
		RunE: func(cmd *cobra.Command, args []string) error {
			return runCommand(cmd, opts, registryOptions...)
		},
		Args: func(cmd *cobra.Command, args []string) error {
			for _, arg := range args {
				if len(arg) &gt; 0 {
					return fmt.Errorf(&quot;%q does not take any arguments, got %q&quot;, cmd.CommandPath(), args)
				}
			}
			return nil
		},
	}

	nfs := opts.Flags
	verflag.AddFlags(nfs.FlagSet(&quot;global&quot;))
	globalflag.AddGlobalFlags(nfs.FlagSet(&quot;global&quot;), cmd.Name(), logs.SkipLoggingConfigurationFlags())
	fs := cmd.Flags()
	for _, f := range nfs.FlagSets {
		fs.AddFlagSet(f)
	}

	cols, _, _ := term.TerminalSize(cmd.OutOrStdout())
	cliflag.SetUsageAndHelpFunc(cmd, *nfs, cols)

	if err := cmd.MarkFlagFilename(&quot;config&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;json&quot;); err != nil {
		klog.ErrorS(err, &quot;Failed to mark flag filename&quot;)
	}

	return cmd
}
</code></pre>
<p>下面来看下 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L117-L142" target="_blank"
   rel="noopener nofollow noreferrer" >runCommand()</a> 在启动 <em>scheduler</em> 时提供了什么功能。</p>
<p>在新版中已经没有 <code>algorithmprovider</code> 的概念，所以在 <code>runCommand</code> 中做的也就是仅仅启动这个 <code>scheduler</code> ，而 scheduler 作为kubernetes组件，也是会watch等操作，自然少不了informer。其次作为和 <code>controller-manager</code> 相同的工作特性，<code>kube-scheduler</code> 也是 基于Leader选举的。</p>
<pre><code class="language-go">func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error {
	// To help debugging, immediately log version
	klog.InfoS(&quot;Starting Kubernetes Scheduler&quot;, &quot;version&quot;, version.Get())

	klog.InfoS(&quot;Golang settings&quot;, &quot;GOGC&quot;, os.Getenv(&quot;GOGC&quot;), &quot;GOMAXPROCS&quot;, os.Getenv(&quot;GOMAXPROCS&quot;), &quot;GOTRACEBACK&quot;, os.Getenv(&quot;GOTRACEBACK&quot;))

	// Configz registration.
	if cz, err := configz.New(&quot;componentconfig&quot;); err == nil {
		cz.Set(cc.ComponentConfig)
	} else {
		return fmt.Errorf(&quot;unable to register configz: %s&quot;, err)
	}

	// Start events processing pipeline.
	cc.EventBroadcaster.StartRecordingToSink(ctx.Done())
	defer cc.EventBroadcaster.Shutdown()

	// Setup healthz checks.
	var checks []healthz.HealthChecker
	if cc.ComponentConfig.LeaderElection.LeaderElect {
		checks = append(checks, cc.LeaderElection.WatchDog)
	}

	waitingForLeader := make(chan struct{})
	isLeader := func() bool {
		select {
		case _, ok := &lt;-waitingForLeader:
			// if channel is closed, we are leading
			return !ok
		default:
			// channel is open, we are waiting for a leader
			return false
		}
	}

	// Start up the healthz server.
	if cc.SecureServing != nil {
		handler := buildHandlerChain(newHealthzAndMetricsHandler(&amp;cc.ComponentConfig, cc.InformerFactory, isLeader, checks...), cc.Authentication.Authenticator, cc.Authorization.Authorizer)
		// TODO: handle stoppedCh and listenerStoppedCh returned by c.SecureServing.Serve
		if _, _, err := cc.SecureServing.Serve(handler, 0, ctx.Done()); err != nil {
			// fail early for secure handlers, removing the old error loop from above
			return fmt.Errorf(&quot;failed to start secure server: %v&quot;, err)
		}
	}

	// Start all informers.
	cc.InformerFactory.Start(ctx.Done())
	// DynInformerFactory can be nil in tests.
	if cc.DynInformerFactory != nil {
		cc.DynInformerFactory.Start(ctx.Done())
	}

	// Wait for all caches to sync before scheduling.
	cc.InformerFactory.WaitForCacheSync(ctx.Done())
	// DynInformerFactory can be nil in tests.
	if cc.DynInformerFactory != nil {
		cc.DynInformerFactory.WaitForCacheSync(ctx.Done())
	}

	// If leader election is enabled, runCommand via LeaderElector until done and exit.
	if cc.LeaderElection != nil {
		cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{
			OnStartedLeading: func(ctx context.Context) {
				close(waitingForLeader)
				sched.Run(ctx)
			},
			OnStoppedLeading: func() {
				select {
				case &lt;-ctx.Done():
					// We were asked to terminate. Exit 0.
					klog.InfoS(&quot;Requested to terminate, exiting&quot;)
					os.Exit(0)
				default:
					// We lost the lock.
					klog.ErrorS(nil, &quot;Leaderelection lost&quot;)
					klog.FlushAndExit(klog.ExitFlushTimeout, 1)
				}
			},
		}
		leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection)
		if err != nil {
			return fmt.Errorf(&quot;couldn't create leader elector: %v&quot;, err)
		}

		leaderElector.Run(ctx)

		return fmt.Errorf(&quot;lost lease&quot;)
	}

	// Leader election is disabled, so runCommand inline until done.
	close(waitingForLeader)
	sched.Run(ctx)
	return fmt.Errorf(&quot;finished without leader elect&quot;)
}
</code></pre>
<p>上面看到了 <code>runCommend</code> 是作为启动 <em>scheduler</em> 的工作，那么通过参数构建一个 <em>scheduler</em> 则是在 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L298-L355" target="_blank"
   rel="noopener nofollow noreferrer" >Setup</a> 中完成的。</p>
<pre><code class="language-go">// Setup creates a completed config and a scheduler based on the command args and options
func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) {
	if cfg, err := latest.Default(); err != nil {
		return nil, nil, err
	} else {
		opts.ComponentConfig = cfg
	}
	// 验证参数
	if errs := opts.Validate(); len(errs) &gt; 0 {
		return nil, nil, utilerrors.NewAggregate(errs)
	}
	// 构建一个config对象
	c, err := opts.Config()
	if err != nil {
		return nil, nil, err
	}

	// 返回一个config对象，包含了scheduler所需的配置，如informer，leader selection
	cc := c.Complete()

	outOfTreeRegistry := make(runtime.Registry)
	for _, option := range outOfTreeRegistryOptions {
		if err := option(outOfTreeRegistry); err != nil {
			return nil, nil, err
		}
	}

	recorderFactory := getRecorderFactory(&amp;cc)
	completedProfiles := make([]kubeschedulerconfig.KubeSchedulerProfile, 0)
	// 创建出来的scheduler
	sched, err := scheduler.New(cc.Client,
		cc.InformerFactory,
		cc.DynInformerFactory,
		recorderFactory,
		ctx.Done(),
		scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion),
		scheduler.WithKubeConfig(cc.KubeConfig),
		scheduler.WithProfiles(cc.ComponentConfig.Profiles...),
		scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore),
		scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry),
		scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds),
		scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds),
		scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration),
		scheduler.WithExtenders(cc.ComponentConfig.Extenders...),
		scheduler.WithParallelism(cc.ComponentConfig.Parallelism),
		scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) {
			// Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging
			completedProfiles = append(completedProfiles, profile)
		}),
	)
	if err != nil {
		return nil, nil, err
	}
	if err := options.LogOrWriteConfig(opts.WriteConfigTo, &amp;cc.ComponentConfig, completedProfiles); err != nil {
		return nil, nil, err
	}

	return &amp;cc, sched, nil
}
</code></pre>
<p>上面了解到了 <em>scheduler</em> 是如何被构建出来的，下面就看看 构建时参数是如何传递进来的，而对象 option就是对应需要的配置结构，而 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/options/options.go#L203-L243" target="_blank"
   rel="noopener nofollow noreferrer" >ApplyTo</a> 则是将启动时传入的参数转化为构建 <em>scheduler</em> 所需的配置。</p>
<blockquote>
<p>对于Deprecated flags可以参考官方对于kube-scheduler启动参数的说明 <sup><a href="#5">[5]</a></sup></p>
<p>对于如何编写一个scheduler config请参考 <a href="#6">[6]</a> 与 <a href="#7">[7]</a></p>
</blockquote>
<pre><code class="language-go">func (o *Options) ApplyTo(c *schedulerappconfig.Config) error {
	if len(o.ConfigFile) == 0 {
		// 在没有指定 --config时会找到 Deprecated flags:参数
        // 通过kube-scheduler --help可以看到这些被弃用的参数
		o.ApplyDeprecated()
		o.ApplyLeaderElectionTo(o.ComponentConfig)
		c.ComponentConfig = *o.ComponentConfig
	} else {
        // 这里就是指定了--config
		cfg, err := loadConfigFromFile(o.ConfigFile)
		if err != nil {
			return err
		}
		// 这里会将leader选举的参数附加到配置中
		o.ApplyLeaderElectionTo(cfg)

		if err := validation.ValidateKubeSchedulerConfiguration(cfg); err != nil {
			return err
		}

		c.ComponentConfig = *cfg
	}

	if err := o.SecureServing.ApplyTo(&amp;c.SecureServing, &amp;c.LoopbackClientConfig); err != nil {
		return err
	}
	if o.SecureServing != nil &amp;&amp; (o.SecureServing.BindPort != 0 || o.SecureServing.Listener != nil) {
		if err := o.Authentication.ApplyTo(&amp;c.Authentication, c.SecureServing, nil); err != nil {
			return err
		}
		if err := o.Authorization.ApplyTo(&amp;c.Authorization); err != nil {
			return err
		}
	}
	o.Metrics.Apply()

	// Apply value independently instead of using ApplyDeprecated() because it can't be configured via ComponentConfig.
	if o.Deprecated != nil {
		c.PodMaxInUnschedulablePodsDuration = o.Deprecated.PodMaxInUnschedulablePodsDuration
	}

	return nil
}
</code></pre>
<p><code>Setup</code> 后会new一个 <code>schedueler</code> , <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L234-L333" target="_blank"
   rel="noopener nofollow noreferrer" >New</a> 则是这个动作，在里面可以看出，会初始化一些informer与 Pod的list等操作。</p>
<pre><code class="language-go">func New(client clientset.Interface,
	informerFactory informers.SharedInformerFactory,
	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
	recorderFactory profile.RecorderFactory,
	stopCh &lt;-chan struct{},
	opts ...Option) (*Scheduler, error) {

	stopEverything := stopCh
	if stopEverything == nil {
		stopEverything = wait.NeverStop
	}

	options := defaultSchedulerOptions // 默认调度策略，如percentageOfNodesToScore
	for _, opt := range opts {
		opt(&amp;options) // opt 是传入的函数，会返回一个schedulerOptions即相应的一些配置
	}

	if options.applyDefaultProfile { // 这个是个bool类型，默认scheduler会到这里
        // Profile包含了调度器的名称与调度器在两个过程中使用的插件
		var versionedCfg v1beta3.KubeSchedulerConfiguration
		scheme.Scheme.Default(&amp;versionedCfg)
		cfg := schedulerapi.KubeSchedulerConfiguration{} // 初始化一个配置，这个是--config传入的类型。因为默认的调度策略会初始化
        // convert 会将in转为out即versionedCfg转换为cfg
		if err := scheme.Scheme.Convert(&amp;versionedCfg, &amp;cfg, nil); err != nil {
			return nil, err
		}
		options.profiles = cfg.Profiles
	}

	registry := frameworkplugins.NewInTreeRegistry() // 调度框架的注册
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}

	metrics.Register() // 指标类

	extenders, err := buildExtenders(options.extenders, options.profiles)
	if err != nil {
		return nil, fmt.Errorf(&quot;couldn't build extenders: %w&quot;, err)
	}

	podLister := informerFactory.Core().V1().Pods().Lister()
	nodeLister := informerFactory.Core().V1().Nodes().Lister()

	// The nominator will be passed all the way to framework instantiation.
	nominator := internalqueue.NewPodNominator(podLister)
	snapshot := internalcache.NewEmptySnapshot()
	clusterEventMap := make(map[framework.ClusterEvent]sets.String)

	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		frameworkruntime.WithSnapshotSharedLister(snapshot),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(clusterEventMap),
		frameworkruntime.WithParallelism(int(options.parallelism)),
		frameworkruntime.WithExtenders(extenders),
	)
	if err != nil {
		return nil, fmt.Errorf(&quot;initializing profiles: %v&quot;, err)
	}

	if len(profiles) == 0 {
		return nil, errors.New(&quot;at least one profile is required&quot;)
	}

	podQueue := internalqueue.NewSchedulingQueue(
		profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
		informerFactory,
		internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second),
		internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second),
		internalqueue.WithPodNominator(nominator),
		internalqueue.WithClusterEventMap(clusterEventMap),
		internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration),
	)

	schedulerCache := internalcache.New(durationToExpireAssumedPod, stopEverything)

	// Setup cache debugger.
	debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue)
	debugger.ListenForSignal(stopEverything)

	sched := newScheduler(
		schedulerCache,
		extenders,
		internalqueue.MakeNextPodFunc(podQueue),
		MakeDefaultErrorFunc(client, podLister, podQueue, schedulerCache),
		stopEverything,
		podQueue,
		profiles,
		client,
		snapshot,
		options.percentageOfNodesToScore,
	)
	// 这个就是controller中onAdd等那三个必须的事件函数
	addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap))

	return sched, nil
}
</code></pre>
<p>接下来会启动这个 <em>scheduler</em>， 在上面我们看到 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L72-L114" target="_blank"
   rel="noopener nofollow noreferrer" >NewSchedulerCommand</a> 构建了一个cobra.Commond对象， <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L117-L142" target="_blank"
   rel="noopener nofollow noreferrer" >runCommand()</a> 最终会返回个 Run，而这个Run就是启动这个 <em>sche</em> 的。</p>
<p>下面这个 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/pkg/scheduler/scheduler.go#L336-L340" target="_blank"
   rel="noopener nofollow noreferrer" >run</a> 是 <em>sche</em> 的运行，他运行并watch资源，直到上下文完成。</p>
<pre><code class="language-go">func (sched *Scheduler) Run(ctx context.Context) {
	sched.SchedulingQueue.Run()

	// We need to start scheduleOne loop in a dedicated goroutine,
	// because scheduleOne function hangs on getting the next item
	// from the SchedulingQueue.
	// If there are no new pods to schedule, it will be hanging there
	// and if done in this goroutine it will be blocking closing
	// SchedulingQueue, in effect causing a deadlock on shutdown.
	go wait.UntilWithContext(ctx, sched.scheduleOne, 0)

	&lt;-ctx.Done()
	sched.SchedulingQueue.Close()
}
</code></pre>
<p>而调用这个 <em>Run</em> 的部分则是作为server的 <em>kube-scheduler</em> 中的 <a href="https://github.com/kubernetes/kubernetes/blob/140c27533044e9e00f800d3ad0517540e3e4ecad/cmd/kube-scheduler/app/server.go#L145-L237" target="_blank"
   rel="noopener nofollow noreferrer" >run</a></p>
<pre><code class="language-go">// Run executes the scheduler based on the given configuration. It only returns on error or when context is done.
func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error {
	// To help debugging, immediately log version
	klog.InfoS(&quot;Starting Kubernetes Scheduler&quot;, &quot;version&quot;, version.Get())

	klog.InfoS(&quot;Golang settings&quot;, &quot;GOGC&quot;, os.Getenv(&quot;GOGC&quot;), &quot;GOMAXPROCS&quot;, os.Getenv(&quot;GOMAXPROCS&quot;), &quot;GOTRACEBACK&quot;, os.Getenv(&quot;GOTRACEBACK&quot;))

	// Configz registration.
	if cz, err := configz.New(&quot;componentconfig&quot;); err == nil {
		cz.Set(cc.ComponentConfig)
	} else {
		return fmt.Errorf(&quot;unable to register configz: %s&quot;, err)
	}

	// Start events processing pipeline.
	cc.EventBroadcaster.StartRecordingToSink(ctx.Done())
	defer cc.EventBroadcaster.Shutdown()

	// Setup healthz checks.
	var checks []healthz.HealthChecker
	if cc.ComponentConfig.LeaderElection.LeaderElect {
		checks = append(checks, cc.LeaderElection.WatchDog)
	}

	waitingForLeader := make(chan struct{})
	isLeader := func() bool {
		select {
		case _, ok := &lt;-waitingForLeader:
			// if channel is closed, we are leading
			return !ok
		default:
			// channel is open, we are waiting for a leader
			return false
		}
	}

	// Start up the healthz server.
	if cc.SecureServing != nil {
		handler := buildHandlerChain(newHealthzAndMetricsHandler(&amp;cc.ComponentConfig, cc.InformerFactory, isLeader, checks...), cc.Authentication.Authenticator, cc.Authorization.Authorizer)
		// TODO: handle stoppedCh and listenerStoppedCh returned by c.SecureServing.Serve
		if _, _, err := cc.SecureServing.Serve(handler, 0, ctx.Done()); err != nil {
			// fail early for secure handlers, removing the old error loop from above
			return fmt.Errorf(&quot;failed to start secure server: %v&quot;, err)
		}
	}

	// Start all informers.
	cc.InformerFactory.Start(ctx.Done())
	// DynInformerFactory can be nil in tests.
	if cc.DynInformerFactory != nil {
		cc.DynInformerFactory.Start(ctx.Done())
	}

	// Wait for all caches to sync before scheduling.
	cc.InformerFactory.WaitForCacheSync(ctx.Done())
	// DynInformerFactory can be nil in tests.
	if cc.DynInformerFactory != nil {
		cc.DynInformerFactory.WaitForCacheSync(ctx.Done())
	}

	// If leader election is enabled, runCommand via LeaderElector until done and exit.
	if cc.LeaderElection != nil {
		cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{
			OnStartedLeading: func(ctx context.Context) {
				close(waitingForLeader)
				sched.Run(ctx)
			},
			OnStoppedLeading: func() {
				select {
				case &lt;-ctx.Done():
					// We were asked to terminate. Exit 0.
					klog.InfoS(&quot;Requested to terminate, exiting&quot;)
					os.Exit(0)
				default:
					// We lost the lock.
					klog.ErrorS(nil, &quot;Leaderelection lost&quot;)
					klog.FlushAndExit(klog.ExitFlushTimeout, 1)
				}
			},
		}
		leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection)
		if err != nil {
			return fmt.Errorf(&quot;couldn't create leader elector: %v&quot;, err)
		}

		leaderElector.Run(ctx)

		return fmt.Errorf(&quot;lost lease&quot;)
	}

	// Leader election is disabled, so runCommand inline until done.
	close(waitingForLeader)
	sched.Run(ctx)
	return fmt.Errorf(&quot;finished without leader elect&quot;)
}
</code></pre>
<p>而上面的 <em>server.Run</em> 会被 <code>runCommand</code> 也就是在 <code>NewSchedulerCommand</code> 时被返回，在 <code>kube-scheduler</code> 的入口文件中被执行。</p>
<pre><code class="language-go">cc, sched, err := Setup(ctx, opts, registryOptions...)
if err != nil {
    return err
}

return Run(ctx, cc, sched)
</code></pre>
<p>至此，整个 <code>kube-scheduler</code> 启动流就分析完了，这个的流程可以用下图表示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220718174104043.png" alt="image-20220718174104043" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图2：scheduler server运行流程</center>
<blockquote>
<p>Reference</p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" target="_blank"
   rel="noopener nofollow noreferrer" >kube scheduler</a></p>
<p><sup id="2">[2]</sup> <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduler_algorithm.md#filtering-the-nodes" target="_blank"
   rel="noopener nofollow noreferrer" >Scheduler Algorithm in Kubernetes</a></p>
<p><sup id="3">[3]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/" target="_blank"
   rel="noopener nofollow noreferrer" >scheduling framework</a></p>
<p><sup id="4">[4]</sup> <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/#permit" target="_blank"
   rel="noopener nofollow noreferrer" >permit</a></p>
<p><sup id="5">[5]</sup> <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank"
   rel="noopener nofollow noreferrer" >kube-scheduler parmater</a></p>
<p><sup id="6">[6]</sup> <a href="https://kubernetes.io/docs/reference/config-api/kube-scheduler-config.v1beta3/" target="_blank"
   rel="noopener nofollow noreferrer" >kube-scheduler config.v1beta3/</a></p>
<p><sup id="7">[7]</sup> <a href="https://kubernetes.io/docs/reference/scheduling/config/" target="_blank"
   rel="noopener nofollow noreferrer" >kube-scheduler config</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解Kubernetes 4A - Admission Control源码解析</title>
      <link>https://www.oomkill.com/2022/07/ch33-admission-webhook/</link>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/07/ch33-admission-webhook/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p>本文是关于Kubernetes 4A解析的第三章</p>
<ul>
<li><a href="https://cylonchau.github.io/kubernetes-authentication.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authentication源码解析</a></li>
<li><a href="https://cylonchau.github.io/kubernetes-authorization.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Authorization源码解析</a></li>
<li>深入理解Kubernetes 4A - Admission Control源码解析</li>
<li><a href="https://cylonchau.github.io/kubernetes-auditing.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes 4A - Audit源码解析</a></li>
</ul>
<p>所有关于Kubernetes 4A部分代码上传至仓库 github.com/cylonchau/hello-k8s-4A</p>
</blockquote>
<p>如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大</p>
<h2 id="background">BACKGROUND</h2>
<p><strong>admission controllers的特点</strong>：</p>
<ul>
<li>可定制性：准入功能可针对不同的场景进行调整。</li>
<li>可预防性：审计则是为了检测问题，而准入控制器可以预防问题发生</li>
<li>可扩展性：在kubernetes自有的验证机制外，增加了另外的防线，弥补了RBAC仅能对资源提供安全保证。</li>
</ul>
<p>下图，显示了用户操作资源的流程，可以看出 <em>admission controllers</em> 作用是在通过身份验证资源持久化之前起到拦截作用。在准入控制器的加入会使kubernetes增加了更高级的安全功能。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/admission-controller-phases.png" alt="准入控制器阶段" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图</center>
<center><em>Source：</em>https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/</center>
<p>这里找到一个大佬博客画的图，通过两张图可以很清晰的了解到admission webhook流程，与官方给出的不一样的地方在于，这里清楚地定位了kubernetes admission webhook 处于准入控制中，RBAC之后，push 之前。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/vhesGDFN3dLdzXwS7vzPdXkI3aglQYZgGhjc-Cx_boaV6URKFFoe8mFRZZUuJyGHywa_bOkeUlIkm-nJkCVMHPk9dr2dXFwNzAQJKzft2phsTcEDjdObjmugBcYtpdPLpLIYuIGzeFYvtsR2Lw.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Kubernetes API 请求的请求处理步骤图（详细）</center>
<center><em>Source：</em>https://www.armosec.io/blog/kubernetes-admission-controller/</center>
<h3 id="两种控制器有什么区别">两种控制器有什么区别？</h3>
<p>根据官方提供的说法是</p>
<blockquote>
<p>Mutating controllers may modify related objects to the requests they admit; validating controllers may not</p>
</blockquote>
<p>从结构图中也可以看出，<code>validating </code> 是在持久化之前，而 <code>Mutating </code> 是在结构验证前，根据这些特性我们可以使用 <code>Mutating</code> 修改这个资源对象内容（如增加验证的信息），在 <code>validating</code> 中验证是否合法。</p>
<h3 id="composition-of-admission-controllers">composition of admission controllers</h3>
<p>kubernetes中的  <em>admission controllers</em> 由两部分组成：</p>
<ul>
<li>内置在APIServer中的准入控制器 <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do" target="_blank"
   rel="noopener nofollow noreferrer" >build-in li.st</a></li>
<li>特殊的控制器；也是内置在APIServer中，但提供一些自定义的功能
<ul>
<li>MutatingAdmission</li>
<li>ValidatingAdmission</li>
</ul>
</li>
</ul>
<p>Mutating 控制器可以修改他们处理的资源对象，Validating 控制器不会。当在任何一个阶段中的任何控制器拒绝这个了请求，则会立即拒绝整个请求，并将错误返回。</p>
<h3 id="admission-webhook">admission webhook</h3>
<p>由于准入控制器是内置在 <code>kube-apiserver</code> 中的，这种情况下就限制了admission controller的可扩展性。在这种背景下，kubernetes提供了一种可扩展的准入控制器 <code>extensible admission controllers</code>，这种行为叫做动态准入控制 <code>Dynamic Admission Control</code>，而提供这个功能的就是 <code>admission webhook</code> 。</p>
<p><code>admission webhook</code>  通俗来讲就是 HTTP 回调，通过定义一个http server，接收准入请求并处理。用户可以通过kubernetes提供的两种类型的 <code>admission webhook</code>，<em>validating admission webhook</em> 和 <em>mutating admission webhook</em>。来完成自定义的准入策略的处理。</p>
<p>webhook 就是</p>
<blockquote>
<p>注：从上面的流程图也可以看出，admission webhook 也是有顺序的。首先调用mutating webhook，然后会调用validating webhook。</p>
</blockquote>
<h2 id="如何使用准入控制器">如何使用准入控制器</h2>
<p><strong>使用条件</strong>：kubernetes v1.16 使用 <code>admissionregistration.k8s.io/v1</code> ；kubernetes v1.9 使用 <code>admissionregistration.k8s.io/v1beta1</code>。</p>
<p><strong>如何在集群中开启准入控制器?</strong> ：查看kube-apiserver 的启动参数 <code>--enable-admission-plugins</code> ；通过该参数来配置要启动的准入控制器，如 <code>--enable-admission-plugins=NodeRestriction</code> 多个准入控制器以 <code>,</code> 分割，顺序无关紧要。 反之使用 <code>--disable-admission-plugins</code> 参数可以关闭相应的准入控制器（Refer to <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/#options" target="_blank"
   rel="noopener nofollow noreferrer" >apiserver opts</a>）。</p>
<p>通过 <code>kubectl</code> 命令可以看到，当前kubernetes集群所支持准入控制器的版本</p>
<pre><code class="language-bash">$ kubectl api-versions | grep admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1
admissionregistration.k8s.io/v1beta1
</code></pre>
<h2 id="webhook工作原理">webhook工作原理</h2>
<p>通过上面的学习，已经了解到了两种webhook的工作原理如下所示：</p>
<blockquote>
<p>mutating webhook，会在持久化前拦截在 MutatingWebhookConfiguration 中定义的规则匹配的请求。MutatingAdmissionWebhook 通过向 mutating webhook 服务器发送准入请求来执行验证。</p>
<p>validaing webhook，会在持久化前拦截在 <code>ValidatingWebhookConfiguration</code> 中定义的规则匹配的请求。ValidatingAdmissionWebhook 通过将准入请求发送到 validating webhook server来执行验证。</p>
</blockquote>
<p>那么接下来将从源码中看这个在这个工作流程中，究竟做了些什么？</p>
<h3 id="资源类型">资源类型</h3>
<p>对于 1.9 版本之后，也就是 <code>v1</code> 版本 ，admission 被定义在 <a href="https://github.com/kubernetes/kubernetes/blob/v1.18.20/staging/src/k8s.io/api/admissionregistration/v1/types.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io\api\admissionregistration\v1\types.go</a> ，大同小异，因为本地只有1.18集群，所以以这个讲解。</p>
<p>对于 <code>Validating Webhook</code> 来讲实现主要都在webhook中</p>
<pre><code class="language-go">type ValidatingWebhookConfiguration struct {
    // 每个api必须包含下列的metadata，这个是kubernetes规范，可以在注释中的url看到相关文档
	metav1.TypeMeta `json:&quot;,inline&quot;`
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`
	// Webhooks在这里被表示为[]ValidatingWebhook，表示我们可以注册多个
	// +optional
	// +patchMergeKey=name
	// +patchStrategy=merge
	Webhooks []ValidatingWebhook `json:&quot;webhooks,omitempty&quot; patchStrategy:&quot;merge&quot; patchMergeKey:&quot;name&quot; protobuf:&quot;bytes,2,rep,name=Webhooks&quot;`
}
</code></pre>
<p>webhook，则是对这种类型的webhook提供的操作、资源等。对于这部分不做过多的注释了，因为这里本身为kubernetes API资源，官网有很详细的例子与说明。这里更多字段的意思的可以参考官方 <a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#webhook-configuration" target="_blank"
   rel="noopener nofollow noreferrer" >doc</a></p>
<pre><code class="language-go">type ValidatingWebhook struct {
	//  admission webhook的名词，Required
	Name string `json:&quot;name&quot; protobuf:&quot;bytes,1,opt,name=name&quot;`

	// ClientConfig 定义了与webhook通讯的方式 Required
	ClientConfig WebhookClientConfig `json:&quot;clientConfig&quot; protobuf:&quot;bytes,2,opt,name=clientConfig&quot;`

	// rule表示了webhook对于哪些资源及子资源的操作进行关注
	Rules []RuleWithOperations `json:&quot;rules,omitempty&quot; protobuf:&quot;bytes,3,rep,name=rules&quot;`

	// FailurePolicy 对于无法识别的value将如何处理，allowed/Ignore optional
	FailurePolicy *FailurePolicyType `json:&quot;failurePolicy,omitempty&quot; protobuf:&quot;bytes,4,opt,name=failurePolicy,casttype=FailurePolicyType&quot;`

	// matchPolicy 定义了如何使用“rules”列表来匹配传入的请求。
	MatchPolicy *MatchPolicyType `json:&quot;matchPolicy,omitempty&quot; protobuf:&quot;bytes,9,opt,name=matchPolicy,casttype=MatchPolicyType&quot;`
	NamespaceSelector *metav1.LabelSelector `json:&quot;namespaceSelector,omitempty&quot; protobuf:&quot;bytes,5,opt,name=namespaceSelector&quot;`
	SideEffects *SideEffectClass `json:&quot;sideEffects&quot; protobuf:&quot;bytes,6,opt,name=sideEffects,casttype=SideEffectClass&quot;`
	AdmissionReviewVersions []string `json:&quot;admissionReviewVersions&quot; protobuf:&quot;bytes,8,rep,name=admissionReviewVersions&quot;`
}
</code></pre>
<p>到这里了解了一个webhook资源的定义，那么这个如何使用呢？通过 <code>Find Usages</code> 找到一个 <a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/accessors.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/apiserver/pkg/admission/plugin/webhook/accessors.go</a> 在使用它。这里没有注释，但在结构上可以看出，包含客户端与一系列选择器组成</p>
<pre><code class="language-go">type mutatingWebhookAccessor struct {
	*v1.MutatingWebhook
	uid               string
	configurationName string

	initObjectSelector sync.Once
	objectSelector     labels.Selector
	objectSelectorErr  error

	initNamespaceSelector sync.Once
	namespaceSelector     labels.Selector
	namespaceSelectorErr  error

	initClient sync.Once
	client     *rest.RESTClient
	clientErr  error
}
</code></pre>
<p><code>accessor</code> 因为包含了整个webhookconfig定义的一些动作（这里个人这么觉得）。</p>
<p><code>accessor.go</code> 下面 有一个 <code>GetRESTClient</code> 方法 ，通过这里可以看出，这里做的就是使用根据 <code>accessor</code> 构造一个客户端。</p>
<pre><code class="language-go">func (m *mutatingWebhookAccessor) GetRESTClient(clientManager *webhookutil.ClientManager) (*rest.RESTClient, error) {
	m.initClient.Do(func() {
		m.client, m.clientErr = clientManager.HookClient(hookClientConfigForWebhook(m))
	})
	return m.client, m.clientErr
}
</code></pre>
<p>到这步骤已经没必要往下看了，因已经知道这里是请求webhook前的步骤了，下面就是何时请求了。</p>
<p><a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/dispatcher.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io\apiserver\pkg\admission\plugin\webhook\validating\dispatcher.go</a> 下面有两个方法，Dispatch去请求我们自己定义的webhook</p>
<pre><code class="language-go">func (d *validatingDispatcher) Dispatch(ctx context.Context, attr admission.Attributes, o admission.ObjectInterfaces, hooks []webhook.WebhookAccessor) error {
	var relevantHooks []*generic.WebhookInvocation
	// Construct all the versions we need to call our webhooks
	versionedAttrs := map[schema.GroupVersionKind]*generic.VersionedAttributes{}
	for _, hook := range hooks {
		invocation, statusError := d.plugin.ShouldCallHook(hook, attr, o)
		if statusError != nil {
			return statusError
		}
		if invocation == nil {
			continue
		}
		relevantHooks = append(relevantHooks, invocation)
		// If we already have this version, continue
		if _, ok := versionedAttrs[invocation.Kind]; ok {
			continue
		}
		versionedAttr, err := generic.NewVersionedAttributes(attr, invocation.Kind, o)
		if err != nil {
			return apierrors.NewInternalError(err)
		}
		versionedAttrs[invocation.Kind] = versionedAttr
	}

	if len(relevantHooks) == 0 {
		// no matching hooks
		return nil
	}

	// Check if the request has already timed out before spawning remote calls
	select {
	case &lt;-ctx.Done():
		// parent context is canceled or timed out, no point in continuing
		return apierrors.NewTimeoutError(&quot;request did not complete within requested timeout&quot;, 0)
	default:
	}

	wg := sync.WaitGroup{}
	errCh := make(chan error, len(relevantHooks))
	wg.Add(len(relevantHooks))
    // 循环所有相关的注册的hook
	for i := range relevantHooks {
		go func(invocation *generic.WebhookInvocation) {
			defer wg.Done()
            // invacation 中有一个 Accessor,Accessor注册了一个相关的webhookconfig
            // 也就是我们 kubectl -f 注册进来的那个webhook的相关配置
			hook, ok := invocation.Webhook.GetValidatingWebhook()
			if !ok {
				utilruntime.HandleError(fmt.Errorf(&quot;validating webhook dispatch requires v1.ValidatingWebhook, but got %T&quot;, hook))
				return
			}
			versionedAttr := versionedAttrs[invocation.Kind]
			t := time.Now()
            // 调用了callHook去请求我们自定义的webhook
			err := d.callHook(ctx, hook, invocation, versionedAttr)
			ignoreClientCallFailures := hook.FailurePolicy != nil &amp;&amp; *hook.FailurePolicy == v1.Ignore
			rejected := false
			if err != nil {
				switch err := err.(type) {
				case *webhookutil.ErrCallingWebhook:
					if !ignoreClientCallFailures {
						rejected = true
						admissionmetrics.Metrics.ObserveWebhookRejection(hook.Name, &quot;validating&quot;, string(versionedAttr.Attributes.GetOperation()), admissionmetrics.WebhookRejectionCallingWebhookError, 0)
					}
				case *webhookutil.ErrWebhookRejection:
					rejected = true
					admissionmetrics.Metrics.ObserveWebhookRejection(hook.Name, &quot;validating&quot;, string(versionedAttr.Attributes.GetOperation()), admissionmetrics.WebhookRejectionNoError, int(err.Status.ErrStatus.Code))
				default:
					rejected = true
					admissionmetrics.Metrics.ObserveWebhookRejection(hook.Name, &quot;validating&quot;, string(versionedAttr.Attributes.GetOperation()), admissionmetrics.WebhookRejectionAPIServerInternalError, 0)
				}
			}
			admissionmetrics.Metrics.ObserveWebhook(time.Since(t), rejected, versionedAttr.Attributes, &quot;validating&quot;, hook.Name)
			if err == nil {
				return
			}

			if callErr, ok := err.(*webhookutil.ErrCallingWebhook); ok {
				if ignoreClientCallFailures {
					klog.Warningf(&quot;Failed calling webhook, failing open %v: %v&quot;, hook.Name, callErr)
					utilruntime.HandleError(callErr)
					return
				}

				klog.Warningf(&quot;Failed calling webhook, failing closed %v: %v&quot;, hook.Name, err)
				errCh &lt;- apierrors.NewInternalError(err)
				return
			}

			if rejectionErr, ok := err.(*webhookutil.ErrWebhookRejection); ok {
				err = rejectionErr.Status
			}
			klog.Warningf(&quot;rejected by webhook %q: %#v&quot;, hook.Name, err)
			errCh &lt;- err
		}(relevantHooks[i])
	}
	wg.Wait()
	close(errCh)

	var errs []error
	for e := range errCh {
		errs = append(errs, e)
	}
	if len(errs) == 0 {
		return nil
	}
	if len(errs) &gt; 1 {
		for i := 1; i &lt; len(errs); i++ {
			// TODO: merge status errors; until then, just return the first one.
			utilruntime.HandleError(errs[i])
		}
	}
	return errs[0]
}

</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/6adee9d4fb7a0cf3eec148448792e2ab091e1720/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/dispatcher.go#L216-L301" target="_blank"
   rel="noopener nofollow noreferrer" >callHook</a> 可以理解为真正去请求我们自定义的webhook服务的动作</p>
<pre><code class="language-go">func (d *validatingDispatcher) callHook(ctx context.Context, h *v1.ValidatingWebhook, invocation *generic.WebhookInvocation, attr *generic.VersionedAttributes) error {
   if attr.Attributes.IsDryRun() {
      if h.SideEffects == nil {
         return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: fmt.Errorf(&quot;Webhook SideEffects is nil&quot;)}
      }
      if !(*h.SideEffects == v1.SideEffectClassNone || *h.SideEffects == v1.SideEffectClassNoneOnDryRun) {
         return webhookerrors.NewDryRunUnsupportedErr(h.Name)
      }
   }

   uid, request, response, err := webhookrequest.CreateAdmissionObjects(attr, invocation)
   if err != nil {
      return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err}
   }
   // 发生请求，可以看到，这里从上面的讲到的地方获取了一个客户端
   client, err := invocation.Webhook.GetRESTClient(d.cm)
   if err != nil {
      return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err}
   }
   trace := utiltrace.New(&quot;Call validating webhook&quot;,
      utiltrace.Field{&quot;configuration&quot;, invocation.Webhook.GetConfigurationName()},
      utiltrace.Field{&quot;webhook&quot;, h.Name},
      utiltrace.Field{&quot;resource&quot;, attr.GetResource()},
      utiltrace.Field{&quot;subresource&quot;, attr.GetSubresource()},
      utiltrace.Field{&quot;operation&quot;, attr.GetOperation()},
      utiltrace.Field{&quot;UID&quot;, uid})
   defer trace.LogIfLong(500 * time.Millisecond)

   // 这里设置超时，超时时长就是在yaml资源清单中设置的那个值
   if h.TimeoutSeconds != nil {
      var cancel context.CancelFunc
      ctx, cancel = context.WithTimeout(ctx, time.Duration(*h.TimeoutSeconds)*time.Second)
      defer cancel()
   }
   // 直接用post请求我们自己定义的webhook接口
   r := client.Post().Body(request)

   // if the context has a deadline, set it as a parameter to inform the backend
   if deadline, hasDeadline := ctx.Deadline(); hasDeadline {
      // compute the timeout
      if timeout := time.Until(deadline); timeout &gt; 0 {
         // if it's not an even number of seconds, round up to the nearest second
         if truncated := timeout.Truncate(time.Second); truncated != timeout {
            timeout = truncated + time.Second
         }
         // set the timeout
         r.Timeout(timeout)
      }
   }

   if err := r.Do(ctx).Into(response); err != nil {
      return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err}
   }
   trace.Step(&quot;Request completed&quot;)

   result, err := webhookrequest.VerifyAdmissionResponse(uid, false, response)
   if err != nil {
      return &amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err}
   }

   for k, v := range result.AuditAnnotations {
      key := h.Name + &quot;/&quot; + k
      if err := attr.Attributes.AddAnnotation(key, v); err != nil {
         klog.Warningf(&quot;Failed to set admission audit annotation %s to %s for validating webhook %s: %v&quot;, key, v, h.Name, err)
      }
   }
   if result.Allowed {
      return nil
   }
   return &amp;webhookutil.ErrWebhookRejection{Status: webhookerrors.ToStatusErr(h.Name, result.Result)}
}
</code></pre>
<p>走到这里基本上对 <code>admission webhook</code> 有了大致的了解，可以知道这个操作是由 apiserver 完成的。下面就实际操作下自定义一个webhook。</p>
<p>这里还有两个概念，就是请求参数 <a href="https://github.com/kubernetes/kubernetes/blob/6adee9d4fb7a0cf3eec148448792e2ab091e1720/staging/src/k8s.io/api/admission/v1/types.go#L40-L113" target="_blank"
   rel="noopener nofollow noreferrer" >AdmissionRequest</a> 和相应参数 <a href="https://github.com/kubernetes/kubernetes/blob/6adee9d4fb7a0cf3eec148448792e2ab091e1720/staging/src/k8s.io/api/admission/v1/types.go#L116-L150" target="_blank"
   rel="noopener nofollow noreferrer" >AdmissionResponse</a>，这些可以在 <code>callHook</code> 中看到，这两个参数被定义在 <a href="https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/admission/v1/types.go#L29-L37" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io\api\admission\v1\types.go</a> ；这两个参数也就是我们在自定义 webhook 时需要处理接收到的body的结构，以及我们响应内容数据结构。</p>
<h2 id="如何编写一个自定义的admission-webhook">如何编写一个自定义的admission webhook</h2>
<p>通过上面的学习了解到了，自定义的webhook就是做为kubernetes提供给用户两种admission controller来验证自定义业务的一个中间件 admission webhook。本质上他是一个HTTP Server，用户可以使用任何语言来完成这部分功能。当然，如果涉及到需要对kubernetes集群资源操作的话，还是建议使用kubernetes官方提供了SDK的编程语言来完成自定义的webhook。</p>
<p>那么完成一个自定义admission webhook需要两个步骤：</p>
<ul>
<li>将相关的webhook config注册给kubernetes，也就是让kubernetes知道你的webhook</li>
<li>准备一个http server来处理 apiserver发过来验证的信息</li>
</ul>
<blockquote>
<p>注：这里使用go net/http包，本身不区分方法处理HTTP的何种请求，如果用其他框架实现的，如django，需要指定对应方法需要为POST</p>
</blockquote>
<h3 id="向kubernetes注册webhook对象">向kubernetes注册webhook对象</h3>
<p>kubernetes提供的两种类型可自定义的准入控制器，和其他资源一样，可以利用资源清单，动态配置那些资源要被adminssion webhook处理。 kubernetes将这种形式抽象为两种资源：</p>
<ul>
<li>
<p>ValidatingWebhookConfiguration</p>
</li>
<li>
<p>MutatingWebhookConfiguration</p>
</li>
</ul>
<h4 id="validatingadmission">ValidatingAdmission</h4>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: &quot;pod-policy.example.com&quot;
webhooks:
- name: &quot;pod-policy.example.com&quot;
  rules:
  - apiGroups:   [&quot;&quot;] # 拦截资源的Group &quot;&quot; 表示 core。&quot;*&quot; 表示所有。
    apiVersions: [&quot;v1&quot;] # 拦截资源的版本
    operations:  [&quot;CREATE&quot;] # 什么请求下拦截
    resources:   [&quot;pods&quot;]  # 拦截什么资源
    scope:       &quot;Namespaced&quot; # 生效的范围，cluster还是namespace &quot;*&quot;表示没有范围限制。
  clientConfig: # 我们部署的webhook服务，
    service: # service是在cluster-in模式下
      namespace: &quot;example-namespace&quot;
      name: &quot;example-service&quot;
      port: 443 # 服务的端口
      path: &quot;/validate&quot; # path是对应用于验证的接口
    # caBundle是提供给 admission webhook CA证书  
    caBundle: &quot;Ci0tLS0tQk...&lt;base64-encoded PEM bundle containing the CA that signed the webhook's serving certificate&gt;...tLS0K&quot;
  admissionReviewVersions: [&quot;v1&quot;, &quot;v1beta1&quot;]
  sideEffects: None
  timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间
</code></pre>
<h4 id="mutatingadmission">MutatingAdmission</h4>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: &quot;valipod-policy.example.com&quot;
webhooks:
- name: &quot;valipod-policy.example.com&quot;
  rules:
    - apiGroups:   [&quot;apps&quot;] # 拦截资源的Group &quot;&quot; 表示 core。&quot;*&quot; 表示所有。
      apiVersions: [&quot;v1&quot;] # 拦截资源的版本
      operations:  [&quot;CREATE&quot;] # 什么请求下拦截
      resources:   [&quot;deployments&quot;]  # 拦截什么资源
      scope:       &quot;Namespaced&quot; # 生效的范围，cluster还是namespace &quot;*&quot;表示没有范围限制。
  clientConfig: # 我们部署的webhook服务，
    url: &quot;https://10.0.0.1:81/validate&quot; # 这里是外部模式
    #      service: # service是在cluster-in模式下
    #        namespace: &quot;default&quot;
    #        name: &quot;admission-webhook&quot;
    #        port: 81 # 服务的端口
    #        path: &quot;/mutate&quot; # path是对应用于验证的接口
    # caBundle是提供给 admission webhook CA证书
    caBundle: &quot;Ci0tLS0tQk...&lt;base64-encoded PEM bundle containing the CA that signed the webhook's serving certificate&gt;...tLS0K&quot;
  admissionReviewVersions: [&quot;v1&quot;]
  sideEffects: None
  timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间
</code></pre>
<blockquote>
<p>注：对于webhook，也可以引入外部的服务，并非必须部署到集群内部</p>
</blockquote>
<p>对于外部服务来讲，需要 <code>clientConfig</code> 中的 <code>service</code> , 更换为 <code>url</code> ; 通过 <code>url</code> 参数可以将一个外部的服务引入</p>
<pre><code class="language-yaml">apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
...
webhooks:
- name: my-webhook.example.com
  clientConfig:
    url: &quot;https://my-webhook.example.com:9443/my-webhook-path&quot;
  ...
</code></pre>
<blockquote>
<p>注：这里的url规则必须准守下列形式：</p>
<ul>
<li><code>scheme://host:port/path</code></li>
<li>使用了url 时，这里不应填写集群内的服务</li>
<li><code>scheme</code> 必须是 https，不能为http，这就意味着，引入外部时也需要</li>
<li>配置时使用了，<code>?xx=xx</code> 的参数也是不被允许的（官方说法是这样的，通过源码学习了解到因为会发送特定的请求体，所以无需管参数）</li>
</ul>
</blockquote>
<p>更多的配置可以参考kubernetes官方提供的 <a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#webhook-configuration" target="_blank"
   rel="noopener nofollow noreferrer" >doc</a></p>
<h3 id="准备一个webhook">准备一个webhook</h3>
<p>让我们编写我们的 webhook  server。将创建两个钩子，<code>/mutate</code> 与 <code>/validate</code>；</p>
<ul>
<li><code>/mutate</code> 将在创建deployment资源时，基于版本，给资源加上注释 <code>webhook.example.com/allow: true</code></li>
<li><code>/validate</code> 将对 <code>/mutate</code>  增加的 <code>allow:true</code> 那么则继续，否则拒绝。</li>
</ul>
<p>这里为了方便，全部写在一起了，实际上不符合软件的设计。在kubernetes代码库中也提供了一个<a href="https://github.com/kubernetes/kubernetes/blob/release-1.21/test/images/agnhost/webhook/main.go" target="_blank"
   rel="noopener nofollow noreferrer" >webhook server</a>，可以参考他这个webhook server来学习具体要做什么</p>
<pre><code class="language-go">package main

import (
	&quot;context&quot;
	&quot;crypto/tls&quot;
	&quot;encoding/json&quot;
	&quot;fmt&quot;
	&quot;io/ioutil&quot;
	&quot;net/http&quot;
	&quot;os&quot;
	&quot;os/signal&quot;
	&quot;strings&quot;
	&quot;syscall&quot;

	v1admission &quot;k8s.io/api/admission/v1&quot;
	&quot;k8s.io/apimachinery/pkg/runtime&quot;
	&quot;k8s.io/apimachinery/pkg/runtime/serializer&quot;

	appv1 &quot;k8s.io/api/apps/v1&quot;
	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	&quot;k8s.io/klog&quot;
)

type patch struct {
	Op    string            `json:&quot;op&quot;`
	Path  string            `json:&quot;path&quot;`
	Value map[string]string `json:&quot;value&quot;`
}

func serve(w http.ResponseWriter, r *http.Request) {

	var body []byte
	if data, err := ioutil.ReadAll(r.Body); err == nil {
		body = data
	}
	klog.Infof(fmt.Sprintf(&quot;receive request: %v....&quot;, string(body)[:130]))
	if len(body) == 0 {
		klog.Error(fmt.Sprintf(&quot;admission request body is empty&quot;))
		http.Error(w, fmt.Errorf(&quot;admission request body is empty&quot;).Error(), http.StatusBadRequest)
		return
	}
	var admission v1admission.AdmissionReview
	codefc := serializer.NewCodecFactory(runtime.NewScheme())
	decoder := codefc.UniversalDeserializer()
	_, _, err := decoder.Decode(body, nil, &amp;admission)

	if err != nil {
		msg := fmt.Sprintf(&quot;Request could not be decoded: %v&quot;, err)
		klog.Error(msg)
		http.Error(w, msg, http.StatusBadRequest)
		return
	}

	if admission.Request == nil {
		klog.Error(fmt.Sprintf(&quot;admission review can't be used: Request field is nil&quot;))
		http.Error(w, fmt.Errorf(&quot;admission review can't be used: Request field is nil&quot;).Error(), http.StatusBadRequest)
		return
	}

	switch strings.Split(r.RequestURI, &quot;?&quot;)[0] {
	case &quot;/mutate&quot;:
		req := admission.Request
		var admissionResp v1admission.AdmissionReview
		admissionResp.APIVersion = admission.APIVersion
		admissionResp.Kind = admission.Kind
		klog.Infof(&quot;AdmissionReview for Kind=%v, Namespace=%v Name=%v UID=%v Operation=%v&quot;,
			req.Kind.Kind, req.Namespace, req.Name, req.UID, req.Operation)
		switch req.Kind.Kind {
		case &quot;Deployment&quot;:
			var (
				respstr []byte
				err     error
				deploy  appv1.Deployment
			)
			if err = json.Unmarshal(req.Object.Raw, &amp;deploy); err != nil {
				respStructure := v1admission.AdmissionResponse{Result: &amp;metav1.Status{
					Message: fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err),
					Code:    http.StatusInternalServerError,
				}}
				klog.Error(fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err))
				if respstr, err = json.Marshal(respStructure); err != nil {
					klog.Error(fmt.Errorf(&quot;could not unmarshal resouces review response: %v&quot;, err))
					http.Error(w, fmt.Errorf(&quot;could not unmarshal resouces review response: %v&quot;, err).Error(), http.StatusInternalServerError)
					return
				}
				http.Error(w, string(respstr), http.StatusBadRequest)
				return
			}

			current_annotations := deploy.GetAnnotations()
			pl := []patch{}
			for k, v := range current_annotations {
				pl = append(pl, patch{
					Op:   &quot;add&quot;,
					Path: &quot;/metadata/annotations&quot;,
					Value: map[string]string{
						k: v,
					},
				})
			}
			pl = append(pl, patch{
				Op:   &quot;add&quot;,
				Path: &quot;/metadata/annotations&quot;,
				Value: map[string]string{
					deploy.Name + &quot;/Allow&quot;: &quot;true&quot;,
				},
			})

			annotationbyte, err := json.Marshal(pl)

			if err != nil {
				http.Error(w, err.Error(), http.StatusInternalServerError)
				return
			}
			respStructure := &amp;v1admission.AdmissionResponse{
				UID:     req.UID,
				Allowed: true,
				Patch:   annotationbyte,
				PatchType: func() *v1admission.PatchType {
					t := v1admission.PatchTypeJSONPatch
					return &amp;t
				}(),
				Result: &amp;metav1.Status{
					Message: fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err),
					Code:    http.StatusOK,
				},
			}
			admissionResp.Response = respStructure

			klog.Infof(&quot;sending response: %s....&quot;, admissionResp.Response.String()[:130])
			respByte, err := json.Marshal(admissionResp)
			if err != nil {
				klog.Errorf(&quot;Can't encode response messages: %v&quot;, err)
				http.Error(w, err.Error(), http.StatusInternalServerError)
			}
			klog.Infof(&quot;prepare to write response...&quot;)
			w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
			if _, err := w.Write(respByte); err != nil {
				klog.Errorf(&quot;Can't write response: %v&quot;, err)
				http.Error(w, fmt.Sprintf(&quot;could not write response: %v&quot;, err), http.StatusInternalServerError)
			}

		default:
			klog.Error(fmt.Sprintf(&quot;unsupport resouces review request type&quot;))
			http.Error(w, &quot;unsupport resouces review request type&quot;, http.StatusBadRequest)
		}

	case &quot;/validate&quot;:
		req := admission.Request
		var admissionResp v1admission.AdmissionReview
		admissionResp.APIVersion = admission.APIVersion
		admissionResp.Kind = admission.Kind
		klog.Infof(&quot;AdmissionReview for Kind=%v, Namespace=%v Name=%v UID=%v Operation=%v&quot;,
			req.Kind.Kind, req.Namespace, req.Name, req.UID, req.Operation)
		var (
			deploy  appv1.Deployment
			respstr []byte
		)
		switch req.Kind.Kind {
		case &quot;Deployment&quot;:
			if err = json.Unmarshal(req.Object.Raw, &amp;deploy); err != nil {
				respStructure := v1admission.AdmissionResponse{Result: &amp;metav1.Status{
					Message: fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err),
					Code:    http.StatusInternalServerError,
				}}
				klog.Error(fmt.Sprintf(&quot;could not unmarshal resouces review request: %v&quot;, err))
				if respstr, err = json.Marshal(respStructure); err != nil {
					klog.Error(fmt.Errorf(&quot;could not unmarshal resouces review response: %v&quot;, err))
					http.Error(w, fmt.Errorf(&quot;could not unmarshal resouces review response: %v&quot;, err).Error(), http.StatusInternalServerError)
					return
				}
				http.Error(w, string(respstr), http.StatusBadRequest)
				return
			}
		}
		al := deploy.GetAnnotations()
		respStructure := v1admission.AdmissionResponse{
			UID: req.UID,
		}
		if al[fmt.Sprintf(&quot;%s/Allow&quot;, deploy.Name)] == &quot;true&quot; {
			respStructure.Allowed = true
			respStructure.Result = &amp;metav1.Status{
				Code: http.StatusOK,
			}
		} else {
			respStructure.Allowed = false
			respStructure.Result = &amp;metav1.Status{
				Code: http.StatusForbidden,
				Reason: func() metav1.StatusReason {
					return metav1.StatusReasonForbidden
				}(),
				Message: fmt.Sprintf(&quot;the resource %s couldn't to allow entry.&quot;, deploy.Kind),
			}
		}

		admissionResp.Response = &amp;respStructure

		klog.Infof(&quot;sending response: %s....&quot;, admissionResp.Response.String()[:130])
		respByte, err := json.Marshal(admissionResp)
		if err != nil {
			klog.Errorf(&quot;Can't encode response messages: %v&quot;, err)
			http.Error(w, err.Error(), http.StatusInternalServerError)
		}
		klog.Infof(&quot;prepare to write response...&quot;)
		w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
		if _, err := w.Write(respByte); err != nil {
			klog.Errorf(&quot;Can't write response: %v&quot;, err)
			http.Error(w, fmt.Sprintf(&quot;could not write response: %v&quot;, err), http.StatusInternalServerError)
		}
	}
}

func main() {
	var (
		cert, key string
	)

	if cert = os.Getenv(&quot;TLS_CERT&quot;); len(cert) == 0 {
		cert = &quot;./tls/tls.crt&quot;
	}

	if key = os.Getenv(&quot;TLS_KEY&quot;); len(key) == 0 {
		key = &quot;./tls/tls.key&quot;
	}

	ca, err := tls.LoadX509KeyPair(cert, key)
	if err != nil {
		klog.Error(err.Error())
		return
	}

	server := &amp;http.Server{
		Addr: &quot;:81&quot;,
		TLSConfig: &amp;tls.Config{
			Certificates: []tls.Certificate{
				ca,
			},
		},
	}

	httpserver := http.NewServeMux()

	httpserver.HandleFunc(&quot;/validate&quot;, serve)
	httpserver.HandleFunc(&quot;/mutate&quot;, serve)
	httpserver.HandleFunc(&quot;/ping&quot;, func(w http.ResponseWriter, r *http.Request) {
		klog.Info(fmt.Sprintf(&quot;%s %s&quot;, r.RequestURI, &quot;pong&quot;))
		fmt.Fprint(w, &quot;pong&quot;)
	})
	server.Handler = httpserver

	go func() {
		if err := server.ListenAndServeTLS(&quot;&quot;, &quot;&quot;); err != nil {
			klog.Errorf(&quot;Failed to listen and serve webhook server: %v&quot;, err)
		}
	}()

	klog.Info(&quot;starting serve.&quot;)
	signalChan := make(chan os.Signal, 1)
	signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM)
	&lt;-signalChan

	klog.Infof(&quot;Got shut signal, shutting...&quot;)
	if err := server.Shutdown(context.Background()); err != nil {
		klog.Errorf(&quot;HTTP server Shutdown: %v&quot;, err)
	}
}
</code></pre>
<p>对应的Dockerfile</p>
<pre><code class="language-docker">FROM golang:alpine AS builder
MAINTAINER cylon
WORKDIR /admission
COPY ./ /admission
ENV GOPROXY https://goproxy.cn,direct
RUN \
    sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories &amp;&amp; \
    apk add upx  &amp;&amp; \
    GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -ldflags &quot;-s -w&quot; -o webhook main.go &amp;&amp; \
    upx -1 webhook &amp;&amp; \
    chmod +x webhook

FROM alpine AS runner
WORKDIR /go/admission
COPY --from=builder /admission/webhook .
VOLUME [&quot;/admission&quot;]
</code></pre>
<p>集群内部部署所需的资源清单</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: admission-webhook
  labels:
    app: admission-webhook
spec:
  ports:
    - port: 81
      targetPort: 81
  selector:
    app: simple-webhook
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: simple-webhook
  name: simple-webhook
spec:
  replicas: 1
  selector:
    matchLabels:
      app: simple-webhook
  template:
    metadata:
      labels:
        app: simple-webhook
    spec:
      containers:
        - image: cylonchau/simple-webhook:v0.0.2
          imagePullPolicy: IfNotPresent
          name: webhook
          command: [&quot;./webhook&quot;]
          env:
            - name: &quot;TLS_CERT&quot;
              value: &quot;./tls/tls.crt&quot;
            - name: &quot;TLS_KEY&quot;
              value: &quot;./tls/tls.key&quot;
            - name: NS_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
          ports:
            - containerPort: 81
          volumeMounts:
            - name: tlsdir
              mountPath: /go/admission/tls
              readOnly: true
      volumes:
        - name: tlsdir
          secret:
            secretName: webhook
---
apiVersion: admissionregistration.k8s.io/v1
kind: MutatingWebhookConfiguration
metadata:
  name: &quot;pod-policy.example.com&quot;
webhooks:
  - name: &quot;pod-policy.example.com&quot;
    rules:
      - apiGroups:   [&quot;apps&quot;] # 拦截资源的Group &quot;&quot; 表示 core。&quot;*&quot; 表示所有。
        apiVersions: [&quot;v1&quot;] # 拦截资源的版本
        operations:  [&quot;CREATE&quot;] # 什么请求下拦截
        resources:   [&quot;deployments&quot;]  # 拦截什么资源
        scope:       &quot;Namespaced&quot; # 生效的范围，cluster还是namespace &quot;*&quot;表示没有范围限制。
    clientConfig: # 我们部署的webhook服务，
      url: &quot;https://10.0.0.1:81/mutate&quot;
#      service: # service是在cluster-in模式下
#        namespace: &quot;default&quot;
#        name: &quot;admission-webhook&quot;
#        port: 81 # 服务的端口
#        path: &quot;/mutate&quot; # path是对应用于验证的接口
      # caBundle是提供给 admission webhook CA证书
      caBundle: Put you CA (base64 encode) in here
    admissionReviewVersions: [&quot;v1&quot;]
    sideEffects: None
    timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: &quot;valipod-policy.example.com&quot;
webhooks:
- name: &quot;valipod-policy.example.com&quot;
  rules:
    - apiGroups:   [&quot;apps&quot;] # 拦截资源的Group &quot;&quot; 表示 core。&quot;*&quot; 表示所有。
      apiVersions: [&quot;v1&quot;] # 拦截资源的版本
      operations:  [&quot;CREATE&quot;] # 什么请求下拦截
      resources:   [&quot;deployments&quot;]  # 拦截什么资源
      scope:       &quot;Namespaced&quot; # 生效的范围，cluster还是namespace &quot;*&quot;表示没有范围限制。
  clientConfig: # 我们部署的webhook服务，
    #      service: # service是在cluster-in模式下
    #        namespace: &quot;default&quot;
    #        name: &quot;admission-webhook&quot;
    #        port: 81 # 服务的端口
    #        path: &quot;/mutate&quot; # path是对应用于验证的接口
    # caBundle是提供给 admission webhook CA证书
    caBundle: Put you CA (base64 encode) in here
  admissionReviewVersions: [&quot;v1&quot;]
  sideEffects: None
  timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间
</code></pre>
<h4 id="这里需要主义的问题">这里需要主义的问题</h4>
<p><strong>证书问题</strong></p>
<p>如果需要 <code>cluster-in</code> ，那么则需要对对应webhookconfig资源配置 <code>service</code> ；如果使用的是外部部署，则需要配置对应访问地址，如：<em>&ldquo;https://xxxx:port/method&rdquo;</em></p>
<p>这两种方式的证书均需要对应的 <code>subjectAltName</code> ，<code>cluster-in</code> 模式 需要对应service名称，如，至少包含<code>serviceName.NS.svc</code> 这一个域名。</p>
<p>下面就是证书类问题的错误</p>
<pre><code>Failed calling webhook, failing closed pod-policy.example.com: failed calling webhook &quot;pod-policy.example.com&quot;: Post https://admission-webhook.default.svc:81/mutate?timeout=5s: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;admission-webhook-ca&quot;)
</code></pre>
<p><strong>相应信息问题</strong></p>
<p>上面我们了解到的APIServer是去发出 <code>v1admission.AdmissionReview</code> 也就是 Request 和 Response类型的，所以，为了更清晰的表示出问题所在，需要对响应格式中的 <code>Reason</code> 与 <code>Message</code>  配置，这也就是我们在客户端看到的报错信息。</p>
<pre><code class="language-go">&amp;metav1.Status{
    Code: http.StatusForbidden,
    Reason: func() metav1.StatusReason {
        return metav1.StatusReasonForbidden
    }(),
    Message: fmt.Sprintf(&quot;the resource %s couldn't to allow entry.&quot;, deploy.Kind),
}
</code></pre>
<p>通过上面的设置用户可以看到下列错误</p>
<pre><code class="language-bash">$ kubectl apply -f nginx.yaml 
Error from server (Forbidden): error when creating &quot;nginx.yaml&quot;: admission webhook &quot;valipod-policy.example.com&quot; denied the request: the resource Deployment couldn't to allow entry.
</code></pre>
<blockquote>
<p>注：必须的参数还包含，UID，allowed，这两个是必须的，上面阐述的只是对用户友好的提示信息</p>
</blockquote>
<p>下面的报错就是对相应格式设置错误</p>
<pre><code class="language-go">Error from server (InternalError): error when creating &quot;nginx.yaml&quot;: Internal error occurred: failed calling webhook &quot;pod-policy.example.com&quot;: the server rejected our request for an unknown reason
</code></pre>
<p><strong>相应信息版本问题</strong></p>
<p>相应信息也需要指定一个版本，这个与请求来的结构中拿即可</p>
<pre><code class="language-go">admissionResp.APIVersion = admission.APIVersion
admissionResp.Kind = admission.Kind
</code></pre>
<p>下面是没有为对应相应信息配置对应KV的值出现的报错</p>
<pre><code>Error from server (InternalError): error when creating &quot;nginx.yaml&quot;: Internal error occurred: failed calling webhook &quot;pod-policy.example.com&quot;: expected webhook response of admission.k8s.io/v1, Kind=AdmissionReview, got /, Kind=
</code></pre>
<p><strong>关于patch</strong></p>
<p>kubernetes中patch使用的是特定的规范，如 <code>jsonpatch</code></p>
<blockquote>
<p>kubernetes当前唯一支持的 <code>patchType</code> 是 <code>JSONPatch</code>。 有关更多详细信息，请参见 <a href="https://jsonpatch.com/" target="_blank"
   rel="noopener nofollow noreferrer" >JSON patch</a></p>
<p>对于 <code>jsonpatch</code> 是一个固定的类型，在go中必须定义其结构体</p>
<pre><code class="language-json">{
	&quot;op&quot;: &quot;add&quot;, // 做什么操作
	&quot;path&quot;: &quot;/spec/replicas&quot;, // 操作的路径
	&quot;value&quot;: 3 // 对应添加的key value
}
</code></pre>
</blockquote>
<p>下面就是字符串类型设置为布尔型产生的报错</p>
<pre><code class="language-bash">Error from server (InternalError): error when creating &quot;nginx.yaml&quot;: Internal error occurred: v1.Deployment.ObjectMeta: v1.ObjectMeta.Annotations: ReadString: expects &quot; or n, but found t, error found in #10 byte of ...|t/Allow&quot;:true},&quot;crea|..., bigger context ...|tadata&quot;:{&quot;annotations&quot;:{&quot;nginx-deployment/Allow&quot;:true},&quot;creationTimestamp&quot;:null,&quot;managedFields&quot;:[{&quot;m|..
</code></pre>
<h3 id="准备证书">准备证书</h3>
<p>Ubuntu</p>
<pre><code class="language-bash">touch ./demoCAindex.txt
touch ./demoCA/serial 
touch ./demoCA/crlnumber
echo 01 &gt; ./demoCA/serial
mkdir ./demoCA/newcerts

openssl genrsa -out cakey.pem 2048

openssl req -new \
	-x509 \
	-key cakey.pem \
	-out cacert.pem \
	-days 3650 \
	-subj &quot;/CN=admission webhook ca&quot;

openssl genrsa -out tls.key 2048

openssl req -new \
	-key tls.key \
	-subj &quot;/CN=admission webhook client&quot; \
	-reqexts webhook \
	-config &lt;(cat /etc/ssl/openssl.cnf \
	&lt;(printf &quot;[webhook]\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1,  IP:10.0.0.4&quot;)) \
	-out tls.csr

sed -i 's/= match/= optional/g' /etc/ssl/openssl.cnf

openssl ca \
	-in tls.csr \
	-cert cacert.pem \
	-keyfile cakey.pem \
	-out tls.crt \
	-days 300 \
	-extensions webhook \
	-extfile &lt;(cat /etc/ssl/openssl.cnf \
    &lt;(printf &quot;[webhook]\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1,  IP:10.0.0.4&quot;))
</code></pre>
<p>CentOS</p>
<pre><code class="language-bash">touch /etc/pki/CA/index.txt
touch /etc/pki/CA/serial # 下一个要颁发的编号 16进制
touch /etc/pki/CA/crlnumber
echo 01 &gt; /etc/pki/CA/serial

openssl req -new \
	-x509 \
	-key cakey.pem \
	-out cacert.pem \
	-days 3650 \
	-subj &quot;/CN=admission webhook ca&quot;

openssl genrsa -out tls.key 2048

openssl req -new \
	-key tls.key \
	-subj &quot;/CN=admission webhook client&quot; \
	-reqexts webhook \
	-config &lt;(cat /etc/pki/tls/openssl.cnf \
	&lt;(printf &quot;[webhook]\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1,  IP:10.0.0.4&quot;)) \
	-out tls.csr

sed -i 's/= match/= optional/g' /etc/ssl/openssl.cnf

openssl ca \
	-in tls.csr \
	-cert cacert.pem \
	-keyfile cakey.pem \
	-out tls.crt \
	-days 300 \
	-extensions webhook \
	-extfile &lt;(cat /etc/pki/tls/openssl.cnf \
    &lt;(printf &quot;[webhook]\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1,  IP:10.0.0.4&quot;))
</code></pre>
<h3 id="通过部署测试结果">通过部署测试结果</h3>
<p>可以看到我们自己注入的 annotation <code>nginx-deployment/Allow: true</code>，在该示例中，仅为演示过程，而不是真的策略，实际环境中可以根据情况进行定制自己的策略。</p>
<p>结果可以看出，当在 <code>mutating</code> 中不通过，即缺少对应的 annotation 标签 , 则 <code>validating</code> 会不允许准入</p>
<pre><code class="language-bash">$ kubectl describe deploy nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Mon, 11 Jul 2022 20:25:16 +0800
Labels:                 &lt;none&gt;
Annotations:            deployment.kubernetes.io/revision: 1
                        nginx-deployment/Allow: true
Selector:               app=nginx
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.14.2
</code></pre>
<blockquote>
<p>Reference</p>
<p><a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank"
   rel="noopener nofollow noreferrer" >extensible admission controllers</a></p>
<p><a href="https://developer.aliyun.com/article/703438" target="_blank"
   rel="noopener nofollow noreferrer" >K8S client-go Patch example</a></p>
<p><a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#response" target="_blank"
   rel="noopener nofollow noreferrer" >admission controllers response</a></p>
<p><a href="https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/" target="_blank"
   rel="noopener nofollow noreferrer" >a guide to kubernetes admission controllers</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>利用kubernetes中的leader选举机制自定义HA应用</title>
      <link>https://www.oomkill.com/2022/06/ch28-leader-election-eg/</link>
      <pubDate>Wed, 29 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch28-leader-election-eg/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="backgroud">Backgroud</h2>
<p>前一章中，对kubernetes的选举原理进行了深度剖析，下面就通过一个example来实现一个，利用kubernetes提供的选举机制完成的高可用应用。</p>
<p>对于此章需要提前对一些概念有所了解后才可以继续看下去</p>
<ul>
<li>leader election mechanism</li>
<li>RBCA</li>
<li>Pod runtime mechanism</li>
</ul>
<h2 id="implementation">Implementation</h2>
<h3 id="代码实现">代码实现</h3>
<p>如果仅仅是使用Kubernetes中的锁，实现的代码也只有几行而已。</p>
<pre><code class="language-go">package main

import (
	&quot;context&quot;
	&quot;flag&quot;
	&quot;fmt&quot;
	&quot;os&quot;
	&quot;os/signal&quot;
	&quot;syscall&quot;
	&quot;time&quot;

	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	clientset &quot;k8s.io/client-go/kubernetes&quot;
	&quot;k8s.io/client-go/rest&quot;
	&quot;k8s.io/client-go/tools/clientcmd&quot;
	&quot;k8s.io/client-go/tools/leaderelection&quot;
	&quot;k8s.io/client-go/tools/leaderelection/resourcelock&quot;
	&quot;k8s.io/klog/v2&quot;
)

func buildConfig(kubeconfig string) (*rest.Config, error) {
	if kubeconfig != &quot;&quot; {
		cfg, err := clientcmd.BuildConfigFromFlags(&quot;&quot;, kubeconfig)
		if err != nil {
			return nil, err
		}
		return cfg, nil
	}

	cfg, err := rest.InClusterConfig()
	if err != nil {
		return nil, err
	}
	return cfg, nil
}

func main() {
	klog.InitFlags(nil)

	var kubeconfig string
	var leaseLockName string
	var leaseLockNamespace string
	var id string
	// 初始化客户端的部分
	flag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;absolute path to the kubeconfig file&quot;)
	flag.StringVar(&amp;id, &quot;id&quot;, &quot;&quot;, &quot;the holder identity name&quot;)
	flag.StringVar(&amp;leaseLockName, &quot;lease-lock-name&quot;, &quot;&quot;, &quot;the lease lock resource name&quot;)
	flag.StringVar(&amp;leaseLockNamespace, &quot;lease-lock-namespace&quot;, &quot;&quot;, &quot;the lease lock resource namespace&quot;)
	flag.Parse()

	if leaseLockName == &quot;&quot; {
		klog.Fatal(&quot;unable to get lease lock resource name (missing lease-lock-name flag).&quot;)
	}
	if leaseLockNamespace == &quot;&quot; {
		klog.Fatal(&quot;unable to get lease lock resource namespace (missing lease-lock-namespace flag).&quot;)
	}
	config, err := buildConfig(kubeconfig)
	if err != nil {
		klog.Fatal(err)
	}
	client := clientset.NewForConfigOrDie(config)

	run := func(ctx context.Context) {
		// 实现的业务逻辑，这里仅仅为实验，就直接打印了
		klog.Info(&quot;Controller loop...&quot;)

		for {
			fmt.Println(&quot;I am leader, I was working.&quot;)
			time.Sleep(time.Second * 5)
		}
	}

	// use a Go context so we can tell the leaderelection code when we
	// want to step down
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// 监听系统中断
	ch := make(chan os.Signal, 1)
	signal.Notify(ch, os.Interrupt, syscall.SIGTERM)
	go func() {
		&lt;-ch
		klog.Info(&quot;Received termination, signaling shutdown&quot;)
		cancel()
	}()

	// 创建一个资源锁
	lock := &amp;resourcelock.LeaseLock{
		LeaseMeta: metav1.ObjectMeta{
			Name:      leaseLockName,
			Namespace: leaseLockNamespace,
		},
		Client: client.CoordinationV1(),
		LockConfig: resourcelock.ResourceLockConfig{
			Identity: id,
		},
	}

	// 开启一个选举的循环
	leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
		Lock:            lock,
		ReleaseOnCancel: true,
		LeaseDuration:   60 * time.Second,
		RenewDeadline:   15 * time.Second,
		RetryPeriod:     5 * time.Second,
		Callbacks: leaderelection.LeaderCallbacks{
			OnStartedLeading: func(ctx context.Context) {
				// 当选举为leader后所运行的业务逻辑
				run(ctx)
			},
			OnStoppedLeading: func() {
				// we can do cleanup here
				klog.Infof(&quot;leader lost: %s&quot;, id)
				os.Exit(0)
			},
			OnNewLeader: func(identity string) { // 申请一个选举时的动作
				if identity == id {
					return
				}
				klog.Infof(&quot;new leader elected: %s&quot;, identity)
			},
		},
	})
}
</code></pre>
<blockquote>
<p>注：这种lease锁只能在in-cluster模式下运行，如果需要类似二进制部署的程序，可以选择endpoint类型的资源锁。</p>
</blockquote>
<h3 id="生成镜像">生成镜像</h3>
<p>这里已经制作好了镜像并上传到dockerhub（<code>cylonchau/leaderelection:v0.0.2</code>）上了，如果只要学习运行原理，则忽略此步骤</p>
<pre><code class="language-docker">FROM golang:alpine AS builder
MAINTAINER cylon
WORKDIR /election
COPY . /election
ENV GOPROXY https://goproxy.cn,direct
RUN GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -o elector main.go

FROM alpine AS runner
WORKDIR /go/elector
COPY --from=builder /election/elector .
VOLUME [&quot;/election&quot;]
ENTRYPOINT [&quot;./elector&quot;]
</code></pre>
<h3 id="准备资源清单">准备资源清单</h3>
<p>默认情况下，Kubernetes运行的pod在请求Kubernetes集群内资源时，默认的账户是没有权限的，默认服务帐户无权访问协调 API，因此我们需要创建另一个serviceaccount并相应地设置 对应的RBAC权限绑定；在清单中配置上这个sa，此时所有的pod就会有协调锁的权限了</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-leaderelection
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: leaderelection
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: leaderelection
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: leaderelection
subjects:
  - kind: ServiceAccount
    name: sa-leaderelection
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: leaderelection
  name: leaderelection
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: leaderelection
  template:
    metadata:
      labels:
        app: leaderelection
    spec:
      containers:
        - image: cylonchau/leaderelection:v0.0.2
          imagePullPolicy: IfNotPresent
          command: [&quot;./elector&quot;]
          args:
          - &quot;-id=$(POD_NAME)&quot;
          - &quot;-lease-lock-name=test&quot;
          - &quot;-lease-lock-namespace=default&quot;
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          name: elector
      serviceAccountName: sa-leaderelection
</code></pre>
<h3 id="集群中运行">集群中运行</h3>
<p>执行完清单后，当pod启动后，可以看到会创建出一个 lease</p>
<pre><code class="language-bash">$ kubectl get lease
NAME   HOLDER                            AGE
test   leaderelection-5644c5f84f-frs5n   1s


$ kubectl describe lease
Name:         test
Namespace:    default
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  coordination.k8s.io/v1
Kind:         Lease
Metadata:
  Creation Timestamp:  2022-06-28T16:39:45Z
  Managed Fields:
    API Version:  coordination.k8s.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        f:acquireTime:
        f:holderIdentity:
        f:leaseDurationSeconds:
        f:leaseTransitions:
        f:renewTime:
    Manager:         elector
    Operation:       Update
    Time:            2022-06-28T16:39:45Z
  Resource Version:  131693
  Self Link:         /apis/coordination.k8s.io/v1/namespaces/default/leases/test
  UID:               bef2b164-a117-44bd-bad3-3e651c94c97b
Spec:
  Acquire Time:            2022-06-28T16:39:45.931873Z
  Holder Identity:         leaderelection-5644c5f84f-frs5n
  Lease Duration Seconds:  60
  Lease Transitions:       0
  Renew Time:              2022-06-28T16:39:55.963537Z
Events:                    &lt;none&gt;
</code></pre>
<p>通过其持有者的信息查看对应pod（因为程序中对holder Identity设置的是pod的名称），实际上是工作的pod。</p>
<p>如上实例所述，这是利用Kubernetes集群完成的leader选举的方案，虽然这不是最完美解决方案，但这是一种简单的方法，因为可以无需在集群上部署更多东西或者进行大量的代码工作就可以利用Kubernetes集群来完成一个高可用的HA应用。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析Kubernetes HA机制 - leader election</title>
      <link>https://www.oomkill.com/2022/06/ch27-leader-election/</link>
      <pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch27-leader-election/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>在 Kubernetes的 <code>kube-controller-manager</code> , <code>kube-scheduler</code>, 以及使用 <code>Operator</code> 的底层实现 <code>controller-rumtime</code> 都支持高可用系统中的leader选举，本文将以理解 <code>controller-rumtime</code> （底层的实现是 <code>client-go</code>） 中的leader选举以在kubernetes controller中是如何实现的。</p>
<h2 id="background">Background</h2>
<p>在运行 <code>kube-controller-manager</code> 时，是有一些参数提供给cm进行leader选举使用的，可以参考官方文档提供的 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/" target="_blank"
   rel="noopener nofollow noreferrer" >参数</a> 来了解相关参数。</p>
<pre><code class="language-bash">--leader-elect                               Default: true
--leader-elect-renew-deadline duration       Default: 10s
--leader-elect-resource-lock string          Default: &quot;leases&quot;
--leader-elect-resource-name string     	 Default: &quot;kube-controller-manager&quot;
--leader-elect-resource-namespace string     Default: &quot;kube-system&quot;
--leader-elect-retry-period duration         Default: 2s
...
</code></pre>
<p>本身以为这些组件的选举动作时通过etcd进行的，但是后面对 <code>controller-runtime</code> 学习时，发现并没有配置其相关的etcd相关参数，这就引起了对选举机制的好奇。怀着这种好奇心搜索了下有关于 kubernetes的选举，发现官网是这么介绍的，下面是对官方的说明进行一个通俗总结。<a href="https://kubernetes.io/blog/2016/01/simple-leader-election-with-kubernetes/" target="_blank"
   rel="noopener nofollow noreferrer" >simple leader election with kubernetes</a></p>
<blockquote>
<p>通过阅读文章得知，kubernetes API 提供了一中选举机制，只要运行在集群内的容器，都是可以实现选举功能的。</p>
<p>Kubernetes API通过提供了两个属性来完成选举动作的</p>
<ul>
<li>ResourceVersions：每个API对象唯一一个ResourceVersion</li>
<li>Annotations：每个API对象都可以对这些key进行注释</li>
</ul>
<p>注：这种选举会增加APIServer的压力。也就对etcd会产生影响</p>
</blockquote>
<p>那么有了这些信息之后，我们来看一下，在Kubernetes集群中，谁是cm的leader（我们提供的集群只有一个节点，所以本节点就是leader）</p>
<p>在Kubernetes中所有启用了leader选举的服务都会生成一个 <code>EndPoint</code> ，在这个 <code>EndPoint</code> 中会有上面提到的label（<em>Annotations</em>）来标识谁是leader。</p>
<pre><code class="language-bash">$ kubectl get ep -n kube-system
NAME                      ENDPOINTS   AGE
kube-controller-manager   &lt;none&gt;      3d4h
kube-dns                              3d4h
kube-scheduler            &lt;none&gt;      3d4h
</code></pre>
<p>这里以 <code>kube-controller-manager</code> 为例，来看下这个 <code>EndPoint</code> 有什么信息</p>
<pre><code class="language-bash">$ kubectl describe ep kube-controller-manager -n kube-system
Name:         kube-controller-manager
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  control-plane.alpha.kubernetes.io/leader:
                {&quot;holderIdentity&quot;:&quot;master-machine_06730140-a503-487d-850b-1fe1619f1fe1&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2022-06-27T15:30:46Z&quot;,&quot;re...
Subsets:
Events:
  Type    Reason          Age    From                     Message
  ----    ------          ----   ----                     -------
  Normal  LeaderElection  2d22h  kube-controller-manager  master-machine_76aabcb5-49ff-45ff-bd18-4afa61fbc5af became leader
  Normal  LeaderElection  9m     kube-controller-manager  master-machine_06730140-a503-487d-850b-1fe1619f1fe1 became leader
</code></pre>
<p>可以看出 <code>Annotations:  control-plane.alpha.kubernetes.io/leader:</code> 标出了哪个node是leader。</p>
<h2 id="election-in-controller-runtime">election in controller-runtime</h2>
<p><code>controller-runtime</code> 有关leader选举的部分在 <a href="https://github.com/kubernetes-sigs/controller-runtime/tree/master/pkg/leaderelection" target="_blank"
   rel="noopener nofollow noreferrer" >pkg/leaderelection</a> 下面，总共100行代码，我们来看下做了些什么？</p>
<p>可以看到，这里只提供了创建资源锁的一些选项</p>
<pre><code class="language-go">type Options struct {
	// 在manager启动时，决定是否进行选举
	LeaderElection bool
	// 使用那种资源锁 默认为租用 lease
	LeaderElectionResourceLock string
	// 选举发生的名称空间
	LeaderElectionNamespace string
	// 该属性将决定持有leader锁资源的名称
	LeaderElectionID string
}
</code></pre>
<p>通过 <code>NewResourceLock</code> 可以看到，这里是走的 <a href="https://github.com/kubernetes/client-go/tree/v0.24.0/tools/leaderelection" target="_blank"
   rel="noopener nofollow noreferrer" >client-go/tools/leaderelection</a>下面，而这个leaderelection也有一个 <a href="https://github.com/kubernetes/client-go/blob/v0.24.0/examples/leader-election/main.go" target="_blank"
   rel="noopener nofollow noreferrer" >example</a> 来学习如何使用它。</p>
<p>通过 example 可以看到，进入选举的入口是一个 RunOrDie() 的函数</p>
<pre><code class="language-go">// 这里使用了一个lease锁，注释中说愿意为集群中存在lease的监听较少
lock := &amp;resourcelock.LeaseLock{
    LeaseMeta: metav1.ObjectMeta{
        Name:      leaseLockName,
        Namespace: leaseLockNamespace,
    },
    Client: client.CoordinationV1(),
    LockConfig: resourcelock.ResourceLockConfig{
        Identity: id,
    },
}

// 开启选举循环
leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
    Lock: lock,
    // 这里必须保证拥有的租约在调用cancel()前终止，否则会仍有一个loop在运行
    ReleaseOnCancel: true,
    LeaseDuration:   60 * time.Second,
    RenewDeadline:   15 * time.Second,
    RetryPeriod:     5 * time.Second,
    Callbacks: leaderelection.LeaderCallbacks{
        OnStartedLeading: func(ctx context.Context) {
            // 这里填写你的代码，
            // usually put your code
            run(ctx)
        },
        OnStoppedLeading: func() {
            // 这里清理你的lease
            klog.Infof(&quot;leader lost: %s&quot;, id)
            os.Exit(0)
        },
        OnNewLeader: func(identity string) {
            // we're notified when new leader elected
            if identity == id {
                // I just got the lock
                return
            }
            klog.Infof(&quot;new leader elected: %s&quot;, identity)
        },
    },
})
</code></pre>
<p>到这里，我们了解了锁的概念和如何启动一个锁，下面看下，client-go都提供了那些锁。</p>
<p>在代码 <a href="tools/leaderelection/resourcelock/interface.go">tools/leaderelection/resourcelock/interface.go</a> 定义了一个锁抽象，interface提供了一个通用接口，用于锁定leader选举中使用的资源。</p>
<pre><code class="language-go">type Interface interface {
	// Get 返回选举记录
	Get(ctx context.Context) (*LeaderElectionRecord, []byte, error)

	// Create 创建一个LeaderElectionRecord
	Create(ctx context.Context, ler LeaderElectionRecord) error

	// Update will update and existing LeaderElectionRecord
	Update(ctx context.Context, ler LeaderElectionRecord) error

	// RecordEvent is used to record events
	RecordEvent(string)

	// Identity 返回锁的标识
	Identity() string

	// Describe is used to convert details on current resource lock into a string
	Describe() string
}
</code></pre>
<p>那么实现这个抽象接口的就是，实现的资源锁，我们可以看到，client-go提供了四种资源锁</p>
<ul>
<li>leaselock</li>
<li>configmaplock</li>
<li>multilock</li>
<li>endpointlock</li>
</ul>
<h3 id="leaselock">leaselock</h3>
<p>Lease是kubernetes控制平面中的通过ETCD来实现的一个Leases的资源，主要为了提供分布式租约的一种控制机制。相关对这个API的描述可以参考于：<a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/" target="_blank"
   rel="noopener nofollow noreferrer" >Lease</a> 。</p>
<p>在Kubernetes集群中，我们可以使用如下命令来查看对应的lease</p>
<pre><code class="language-bash">$ kubectl get leases -A
NAMESPACE         NAME                      HOLDER                                                AGE
kube-node-lease   master-machine            master-machine                                        3d19h
kube-system       kube-controller-manager   master-machine_06730140-a503-487d-850b-1fe1619f1fe1   3d19h
kube-system       kube-scheduler            master-machine_1724e2d9-c19c-48d7-ae47-ee4217b27073   3d19h

$ kubectl describe leases kube-controller-manager -n kube-system
Name:         kube-controller-manager
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  coordination.k8s.io/v1
Kind:         Lease
Metadata:
  Creation Timestamp:  2022-06-24T11:01:51Z
  Managed Fields:
    API Version:  coordination.k8s.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        f:acquireTime:
        f:holderIdentity:
        f:leaseDurationSeconds:
        f:leaseTransitions:
        f:renewTime:
    Manager:         kube-controller-manager
    Operation:       Update
    Time:            2022-06-24T11:01:51Z
  Resource Version:  56012
  Self Link:         /apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager
  UID:               851a32d2-25dc-49b6-a3f7-7a76f152f071
Spec:
  Acquire Time:            2022-06-27T15:30:46.000000Z
  Holder Identity:         master-machine_06730140-a503-487d-850b-1fe1619f1fe1
  Lease Duration Seconds:  15
  Lease Transitions:       2
  Renew Time:              2022-06-28T06:09:26.837773Z
Events:                    &lt;none&gt;
</code></pre>
<p>下面来看下leaselock的实现，leaselock会实现了作为资源锁的抽象</p>
<pre><code class="language-go">type LeaseLock struct {
	// LeaseMeta 就是类似于其他资源类型的属性，包含name ns 以及其他关于lease的属性
	LeaseMeta  metav1.ObjectMeta
	Client     coordinationv1client.LeasesGetter // Client 就是提供了informer中的功能
	// lockconfig包含上面通过 describe 看到的 Identity与recoder用于记录资源锁的更改
    LockConfig ResourceLockConfig
    // lease 就是 API中的Lease资源，可以参考下上面给出的这个API的使用
	lease      *coordinationv1.Lease
}
</code></pre>
<p>下面来看下leaselock实现了那些方法？</p>
<h4 id="get">Get</h4>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/resourcelock/leaselock.go#L41-L53" target="_blank"
   rel="noopener nofollow noreferrer" >Get</a> 是从spec中返回选举的记录</p>
<pre><code class="language-go">func (ll *LeaseLock) Get(ctx context.Context) (*LeaderElectionRecord, []byte, error) {
	var err error
	ll.lease, err = ll.Client.Leases(ll.LeaseMeta.Namespace).Get(ctx, ll.LeaseMeta.Name, metav1.GetOptions{})
	if err != nil {
		return nil, nil, err
	}
	record := LeaseSpecToLeaderElectionRecord(&amp;ll.lease.Spec)
	recordByte, err := json.Marshal(*record)
	if err != nil {
		return nil, nil, err
	}
	return record, recordByte, nil
}

// 可以看出是返回这个资源spec里面填充的值
func LeaseSpecToLeaderElectionRecord(spec *coordinationv1.LeaseSpec) *LeaderElectionRecord {
	var r LeaderElectionRecord
	if spec.HolderIdentity != nil {
		r.HolderIdentity = *spec.HolderIdentity
	}
	if spec.LeaseDurationSeconds != nil {
		r.LeaseDurationSeconds = int(*spec.LeaseDurationSeconds)
	}
	if spec.LeaseTransitions != nil {
		r.LeaderTransitions = int(*spec.LeaseTransitions)
	}
	if spec.AcquireTime != nil {
		r.AcquireTime = metav1.Time{spec.AcquireTime.Time}
	}
	if spec.RenewTime != nil {
		r.RenewTime = metav1.Time{spec.RenewTime.Time}
	}
	return &amp;r
}
</code></pre>
<h4 id="create">Create</h4>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/resourcelock/leaselock.go#L56-L66" target="_blank"
   rel="noopener nofollow noreferrer" >Create</a> 是在kubernetes集群中尝试去创建一个租约，可以看到，Client就是API提供的对应资源的REST客户端，结果会在Kubernetes集群中创建这个Lease</p>
<pre><code class="language-go">func (ll *LeaseLock) Create(ctx context.Context, ler LeaderElectionRecord) error {
	var err error
	ll.lease, err = ll.Client.Leases(ll.LeaseMeta.Namespace).Create(ctx, &amp;coordinationv1.Lease{
		ObjectMeta: metav1.ObjectMeta{
			Name:      ll.LeaseMeta.Name,
			Namespace: ll.LeaseMeta.Namespace,
		},
		Spec: LeaderElectionRecordToLeaseSpec(&amp;ler),
	}, metav1.CreateOptions{})
	return err
}
</code></pre>
<h4 id="update">Update</h4>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/resourcelock/leaselock.go#L69-L82" target="_blank"
   rel="noopener nofollow noreferrer" >Update</a> 是更新Lease的spec</p>
<pre><code class="language-go">func (ll *LeaseLock) Update(ctx context.Context, ler LeaderElectionRecord) error {
	if ll.lease == nil {
		return errors.New(&quot;lease not initialized, call get or create first&quot;)
	}
	ll.lease.Spec = LeaderElectionRecordToLeaseSpec(&amp;ler)

	lease, err := ll.Client.Leases(ll.LeaseMeta.Namespace).Update(ctx, ll.lease, metav1.UpdateOptions{})
	if err != nil {
		return err
	}

	ll.lease = lease
	return nil
}
</code></pre>
<h4 id="recordevent">RecordEvent</h4>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/resourcelock/leaselock.go#L85-L95" target="_blank"
   rel="noopener nofollow noreferrer" >RecordEvent</a> 是记录选举时出现的事件，这时候我们回到上部分 在kubernetes集群中查看 ep 的信息时可以看到的event中存在 <code>became leader</code> 的事件，这里就是将产生的这个event添加到 <code>meta-data</code> 中。</p>
<pre><code>func (ll *LeaseLock) RecordEvent(s string) {
   if ll.LockConfig.EventRecorder == nil {
      return
   }
   events := fmt.Sprintf(&quot;%v %v&quot;, ll.LockConfig.Identity, s)
   subject := &amp;coordinationv1.Lease{ObjectMeta: ll.lease.ObjectMeta}
   // Populate the type meta, so we don't have to get it from the schema
   subject.Kind = &quot;Lease&quot;
   subject.APIVersion = coordinationv1.SchemeGroupVersion.String()
   ll.LockConfig.EventRecorder.Eventf(subject, corev1.EventTypeNormal, &quot;LeaderElection&quot;, events)
}
</code></pre>
<p>到这里大致上了解了资源锁究竟是什么了，其他种类的资源锁也是相同的实现的方式，这里就不过多阐述了；下面的我们来看看选举的过程。</p>
<h3 id="election-workflow">election workflow</h3>
<p>选举的代码入口是在 <a href="https://github.com/kubernetes/client-go/blob/v0.24.0/tools/leaderelection/leaderelection.go" target="_blank"
   rel="noopener nofollow noreferrer" >leaderelection.go</a> ，这里会继续上面的 example 向下分析整个选举的过程。</p>
<p>前面我们看到了进入选举的入口是一个 <a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/examples/leader-election/main.go#L122" target="_blank"
   rel="noopener nofollow noreferrer" >RunOrDie()</a> 的函数，那么就继续从这里开始来了解。进入 RunOrDie，看到其实只有几行而已，大致上了解到了RunOrDie会使用提供的配置来启动选举的客户端，之后会阻塞，直到 ctx 退出，或停止持有leader的租约。</p>
<pre><code class="language-go">func RunOrDie(ctx context.Context, lec LeaderElectionConfig) {
	le, err := NewLeaderElector(lec)
	if err != nil {
		panic(err)
	}
	if lec.WatchDog != nil {
		lec.WatchDog.SetLeaderElection(le)
	}
	le.Run(ctx)
}
</code></pre>
<p>下面看下 <a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/leaderelection.go#L77-L110" target="_blank"
   rel="noopener nofollow noreferrer" >NewLeaderElector</a> 做了些什么？可以看到，LeaderElector是一个结构体，这里只是创建他，这个结构体提供了我们选举中所需要的一切（LeaderElector就是RunOrDie创建的选举客户端）。</p>
<pre><code class="language-go">func NewLeaderElector(lec LeaderElectionConfig) (*LeaderElector, error) {
	if lec.LeaseDuration &lt;= lec.RenewDeadline {
		return nil, fmt.Errorf(&quot;leaseDuration must be greater than renewDeadline&quot;)
	}
	if lec.RenewDeadline &lt;= time.Duration(JitterFactor*float64(lec.RetryPeriod)) {
		return nil, fmt.Errorf(&quot;renewDeadline must be greater than retryPeriod*JitterFactor&quot;)
	}
	if lec.LeaseDuration &lt; 1 {
		return nil, fmt.Errorf(&quot;leaseDuration must be greater than zero&quot;)
	}
	if lec.RenewDeadline &lt; 1 {
		return nil, fmt.Errorf(&quot;renewDeadline must be greater than zero&quot;)
	}
	if lec.RetryPeriod &lt; 1 {
		return nil, fmt.Errorf(&quot;retryPeriod must be greater than zero&quot;)
	}
	if lec.Callbacks.OnStartedLeading == nil {
		return nil, fmt.Errorf(&quot;OnStartedLeading callback must not be nil&quot;)
	}
	if lec.Callbacks.OnStoppedLeading == nil {
		return nil, fmt.Errorf(&quot;OnStoppedLeading callback must not be nil&quot;)
	}

	if lec.Lock == nil {
		return nil, fmt.Errorf(&quot;Lock must not be nil.&quot;)
	}
	le := LeaderElector{
		config:  lec,
		clock:   clock.RealClock{},
		metrics: globalMetricsFactory.newLeaderMetrics(),
	}
	le.metrics.leaderOff(le.config.Name)
	return &amp;le, nil
}
</code></pre>
<p><a href="https://github.com/kubernetes/client-go/blob/cab7ba1d4a523956b6395dcbe38620159ac43fef/tools/leaderelection/leaderelection.go#L177-L195" target="_blank"
   rel="noopener nofollow noreferrer" >LeaderElector</a> 是建立的选举客户端，</p>
<pre><code class="language-go">type LeaderElector struct {
	config LeaderElectionConfig // 这个的配置，包含一些时间参数，健康检查
	// recoder相关属性
	observedRecord    rl.LeaderElectionRecord
	observedRawRecord []byte
	observedTime      time.Time
	// used to implement OnNewLeader(), may lag slightly from the
	// value observedRecord.HolderIdentity if the transition has
	// not yet been reported.
	reportedLeader string
	// clock is wrapper around time to allow for less flaky testing
	clock clock.Clock
	// 锁定 observedRecord
	observedRecordLock sync.Mutex
	metrics leaderMetricsAdapter
}
</code></pre>
<p>可以看到 Run 实现的选举逻辑就是在初始化客户端时传入的 三个 callback</p>
<pre><code class="language-go">func (le *LeaderElector) Run(ctx context.Context) {
	defer runtime.HandleCrash()
	defer func() { // 退出时执行callbacke的OnStoppedLeading
		le.config.Callbacks.OnStoppedLeading()
	}()

	if !le.acquire(ctx) {
		return
	}
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()
	go le.config.Callbacks.OnStartedLeading(ctx) // 选举时，执行 OnStartedLeading
	le.renew(ctx)
}
</code></pre>
<p>在 Run 中调用了 acquire，这个是 通过一个loop去调用 tryAcquireOrRenew，直到ctx传递过来结束信号</p>
<pre><code class="language-go">func (le *LeaderElector) acquire(ctx context.Context) bool {
	ctx, cancel := context.WithCancel(ctx)
	defer cancel()
	succeeded := false
	desc := le.config.Lock.Describe()
	klog.Infof(&quot;attempting to acquire leader lease %v...&quot;, desc)
    // jitterUntil是执行定时的函数 func() 是定时任务的逻辑
    // RetryPeriod是周期间隔
    // JitterFactor 是重试系数，类似于延迟队列中的系数 （duration + maxFactor * duration）
    // sliding 逻辑是否计算在时间内
    // 上下文传递
	wait.JitterUntil(func() {
		succeeded = le.tryAcquireOrRenew(ctx)
		le.maybeReportTransition()
		if !succeeded {
			klog.V(4).Infof(&quot;failed to acquire lease %v&quot;, desc)
			return
		}
		le.config.Lock.RecordEvent(&quot;became leader&quot;)
		le.metrics.leaderOn(le.config.Name)
		klog.Infof(&quot;successfully acquired lease %v&quot;, desc)
		cancel()
	}, le.config.RetryPeriod, JitterFactor, true, ctx.Done())
	return succeeded
}
</code></pre>
<p>这里实际上选举动作在 tryAcquireOrRenew 中，下面来看下tryAcquireOrRenew；tryAcquireOrRenew 是尝试获得一个leader租约，如果已经获得到了，则更新租约；否则可以得到租约则为true，反之false</p>
<pre><code class="language-go">func (le *LeaderElector) tryAcquireOrRenew(ctx context.Context) bool {
	now := metav1.Now() // 时间
	leaderElectionRecord := rl.LeaderElectionRecord{ // 构建一个选举record
		HolderIdentity:       le.config.Lock.Identity(), // 选举人的身份特征，ep与主机名有关
		LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second), // 默认15s
		RenewTime:            now, // 重新获取时间
		AcquireTime:          now, // 获得时间
	}

	// 1. 从API获取或创建一个recode，如果可以拿到则已经有租约，反之创建新租约
	oldLeaderElectionRecord, oldLeaderElectionRawRecord, err := le.config.Lock.Get(ctx)
	if err != nil {
		if !errors.IsNotFound(err) {
			klog.Errorf(&quot;error retrieving resource lock %v: %v&quot;, le.config.Lock.Describe(), err)
			return false
		}
		// 创建租约的动作就是新建一个对应的resource，这个lock就是leaderelection提供的四种锁，
		// 看你在runOrDie中初始化传入了什么锁
		if err = le.config.Lock.Create(ctx, leaderElectionRecord); err != nil {
			klog.Errorf(&quot;error initially creating leader election record: %v&quot;, err)
			return false
		}
		// 到了这里就已经拿到或者创建了租约，然后记录其一些属性，LeaderElectionRecord
		le.setObservedRecord(&amp;leaderElectionRecord)

		return true
	}

	// 2. 获取记录检查身份和时间
	if !bytes.Equal(le.observedRawRecord, oldLeaderElectionRawRecord) {
		le.setObservedRecord(oldLeaderElectionRecord)

		le.observedRawRecord = oldLeaderElectionRawRecord
	}
	if len(oldLeaderElectionRecord.HolderIdentity) &gt; 0 &amp;&amp;
		le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp;
		!le.IsLeader() { // 不是leader，进行HolderIdentity比较，再加上时间，这个时候没有到竞选其，跳出
		klog.V(4).Infof(&quot;lock is held by %v and has not yet expired&quot;, oldLeaderElectionRecord.HolderIdentity)
		return false
	}

	// 3.我们将尝试更新。 在这里leaderElectionRecord设置为默认值。让我们在更新之前更正它。
	if le.IsLeader() { // 到这就说明是leader，修正他的时间
		leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime
		leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions
	} else { // LeaderTransitions 就是指leader调整（转变为其他）了几次，如果是，
		// 则为发生转变，保持原有值
		// 反之，则+1
		leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1
	}
	// 完事之后更新APIServer中的锁资源，也就是更新对应的资源的属性信息
	if err = le.config.Lock.Update(ctx, leaderElectionRecord); err != nil {
		klog.Errorf(&quot;Failed to update lock: %v&quot;, err)
		return false
	}
	// setObservedRecord 是通过一个新的record来更新这个锁中的record
	// 操作是安全的，会上锁保证临界区仅可以被一个线程/进程操作
	le.setObservedRecord(&amp;leaderElectionRecord)
	return true
}
</code></pre>
<h2 id="summary">summary</h2>
<p>到这里，已经完整知道利用kubernetes进行选举的流程都是什么了；下面简单回顾下，上述leader选举所有的步骤：</p>
<ul>
<li>首选创建的服务就是该服务的leader，锁可以为 <code>lease</code> , <code>endpoint</code> 等资源进行上锁</li>
<li>已经是leader的实例会不断续租，租约的默认值是15秒 （<code>leaseDuration</code>）；leader在租约满时更新租约时间（<code>renewTime</code>）。</li>
<li>其他的follower，会不断检查对应资源锁的存在，如果已经有leader，那么则检查 <code>renewTime</code>，如果超过了租用时间（），则表明leader存在问题需要重新启动选举，直到有follower提升为leader。</li>
<li>而为了避免资源被抢占，Kubernetes API使用了 <code>ResourceVersion</code> 来避免被重复修改（如果版本号与请求版本号不一致，则表示已经被修改了，那么APIServer将返回错误）</li>
</ul>
<blockquote>
<p>Reference</p>
<p><a href="https://juejin.cn/post/6844903709336420360" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes 并发控制与数据一致性的实现原理</a></p>
<p><a href="http://liubin.org/blog/2018/04/28/how-to-build-controller-manager-high-available/" target="_blank"
   rel="noopener nofollow noreferrer" >Controller manager 的高可用实现方式</a></p>
<p><a href="https://medium.com/michaelbi-22303/deep-dive-into-kubernetes-simple-leader-election-3712a8be3a99" target="_blank"
   rel="noopener nofollow noreferrer" >deep dive into kubernetes simple leader election</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析Kubernetes controller组件 - controller-runtime</title>
      <link>https://www.oomkill.com/2022/06/ch15-controller-runtime/</link>
      <pubDate>Mon, 27 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch15-controller-runtime/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p><a href="https://github.com/kubernetes-sigs/controller-runtime" target="_blank"
   rel="noopener nofollow noreferrer" >controller-runtime</a> 是 Kubernetes 社区提供可供快速搭建一套  实现了controller 功能的工具，无需自行实现Controller的功能了；在 <code>Kubebuilder</code>  与  <code>Operator SDK</code> 也是使用 <code>controller-runtime</code> 。本文将对 <code>controller-runtime</code> 的工作原理以及在不同场景下的使用方式进行简要的总结和介绍。</p>
<h2 id="controller-runtime-structure">controller-runtime structure</h2>
<p><code>controller-runtime</code> 主要组成是需要用户创建的 <code>Manager</code> 和 <code>Reconciler</code> 以及 <code>Controller Runtime</code> 自己启动的 <code>Cache</code> 和 <code>Controller </code>。</p>
<ul>
<li><strong>Manager</strong>：是用户在初始化时创建的，用于启动 <code>Controller Runtime</code> 组件</li>
<li><strong>Reconciler</strong>：是用户需要提供来处理自己的业务逻辑的组件（即在通过 <code>code-generator</code> 生成的api-like而实现的controller中的业务处理部分）。</li>
<li><strong>Cache</strong>：一个缓存，用来建立 <code>Informer</code> 到 <code>ApiServer </code>的连接来监听资源并将被监听的对象推送到queue中。</li>
<li><strong>Controller</strong>： 一方面向 Informer 注册 <code>eventHandler</code>，另一方面从队列中获取数据。controller 将从队列中获取数据并执行用户自定义的 <code>Reconciler</code> 功能。</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220625221632675.png" alt="image-20220625221632675" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：controller-runtime structure</center>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220625221548958.png" alt="image-20220625221548958" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：controller-runtime flowchart</center>
<p>由图可知，Controller会向 Informer 注册一些列eventHandler；然后Cache启动Informer（informer属于cache包中），与ApiServer建立监听；当Informer检测到资源变化时，将对象加入queue，Controller 将元素取出并在用户端执行 Reconciler。</p>
<h2 id="controller引入">Controller引入</h2>
<p>我们从 controller-rumtime项目的 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/master/examples/crd/main.go" target="_blank"
   rel="noopener nofollow noreferrer" >example</a> 进行引入看下，整个架构都是如何实现的。</p>
<p>可以看到 example 下的实际上实现了一个 <code>reconciler </code> 的结构体，实现了 <code>Reconciler</code> 抽象和 <code>Client</code> 结构体</p>
<pre><code class="language-go">type reconciler struct {
	client.Client
	scheme *runtime.Scheme
}
</code></pre>
<p>那么来看下 抽象的 Reconciler 是什么，可以看到就是抽象了 <code>Reconcile </code>方法，这个是具体处理的逻辑过程</p>
<pre><code class="language-go">type Reconciler interface {
	Reconcile(context.Context, Request) (Result, error)
}
</code></pre>
<p>下面在看下谁来实现了这个 Reconciler 抽象</p>
<pre><code class="language-go">type Controller interface {
	reconcile.Reconciler // 协调的具体步骤，通过ns/name\
    // 通过predicates来评估来源数据，并加入queue中（放入队列的是reconcile.Requests）
	Watch(src source.Source, eventhandler handler.EventHandler, predicates ...predicate.Predicate) error
    // 启动controller，类似于自定义的Run()
	Start(ctx context.Context) error
	GetLogger() logr.Logger
}
</code></pre>
<h3 id="controller-structure">controller structure</h3>
<p>在 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/internal/controller/controller.go#L42" target="_blank"
   rel="noopener nofollow noreferrer" >controller-runtime\pkg\internal\controller\controller.go</a> 中实现了这个 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/controller/controller.go#L66" target="_blank"
   rel="noopener nofollow noreferrer" >Controller</a></p>
<pre><code class="language-go">type Controller struct {
	Name string // controller的标识
    
	MaxConcurrentReconciles int // 并发运行Reconciler的数量，默认1
	// 实现了reconcile.Reconciler的调节器， 默认DefaultReconcileFunc
	Do reconcile.Reconciler
	// makeQueue会构建一个对应的队列，就是返回一个限速队列
	MakeQueue func() workqueue.RateLimitingInterface
	// MakeQueue创造出来的，在出入队列就是操作的这个
	Queue workqueue.RateLimitingInterface

	// 用于注入其他内容
    // 已弃用
	SetFields func(i interface{}) error

	mu sync.Mutex
	// 标识开始的状态
	Started bool
	// 在启动时传递的上下文，用于停止控制器
	ctx context.Context
	// 等待缓存同步的时间 默认2分钟
	CacheSyncTimeout time.Duration

	// 维护了eventHandler predicates，在控制器启动时启动
	startWatches []watchDescription

	// 日志构建器，输出入日志
	LogConstructor func(request *reconcile.Request) logr.Logger

	// RecoverPanic为是否对reconcile引起的panic恢复
	RecoverPanic bool
}
</code></pre>
<p>看完了controller的structure，接下来看看controller是如何使用的</p>
<h3 id="injection">injection</h3>
<p><a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/internal/controller/controller.go#L125-L152" target="_blank"
   rel="noopener nofollow noreferrer" >Controller.Watch</a> 实现了注入的动作，可以看到 <code>watch()</code> 通过参数将 对应的事件函数传入到内部</p>
<pre><code class="language-go">func (c *Controller) Watch(src source.Source, evthdler handler.EventHandler, prct ...predicate.Predicate) error {
	c.mu.Lock()
	defer c.mu.Unlock()

	// 使用SetFields来完成注入操作
	if err := c.SetFields(src); err != nil {
		return err
	}
	if err := c.SetFields(evthdler); err != nil {
		return err
	}
	for _, pr := range prct {
		if err := c.SetFields(pr); err != nil {
			return err
		}
	}

	// 如果Controller还未启动，那么将这些动作缓存到本地
	if !c.Started {
		c.startWatches = append(c.startWatches, watchDescription{src: src, handler: evthdler, predicates: prct})
		return nil
	}

	c.LogConstructor(nil).Info(&quot;Starting EventSource&quot;, &quot;source&quot;, src)
	return src.Start(c.ctx, evthdler, c.Queue, prct...)
}
</code></pre>
<p>启动操作实际上为informer注入事件函数</p>
<pre><code class="language-go">type Source interface {
	// start 是Controller 调用，用以向 Informer 注册 EventHandler， 将 reconcile.Requests（一个入队列的动作） 排入队列。
	Start(context.Context, handler.EventHandler, workqueue.RateLimitingInterface, ...predicate.Predicate) error
}

func (is *Informer) Start(ctx context.Context, handler handler.EventHandler, queue workqueue.RateLimitingInterface,
	prct ...predicate.Predicate) error {
	// Informer should have been specified by the user.
	if is.Informer == nil {
		return fmt.Errorf(&quot;must specify Informer.Informer&quot;)
	}

	is.Informer.AddEventHandler(internal.EventHandler{Queue: queue, EventHandler: handler, Predicates: prct})
	return nil
}
</code></pre>
<p>我们知道对于 eventHandler，实际上应该是一个 <code>onAdd</code>，<code>onUpdate</code> 这种类型的函数，queue则是workqueue，那么 <code>Predicates</code> 是什么呢？</p>
<p>通过追踪可以看到定义了 Predicate 抽象，可以看出Predicate 是Watch到的事件时什么类型的，当对于每个类型的事件，对应的函数就为 true，在 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/source/internal/eventsource.go#L87-L91" target="_blank"
   rel="noopener nofollow noreferrer" >eventHandler</a> 中，这些被用作，事件的过滤。</p>
<pre><code class="language-go">// Predicate filters events before enqueuing the keys.
type Predicate interface {
	// Create returns true if the Create event should be processed
	Create(event.CreateEvent) bool

	// Delete returns true if the Delete event should be processed
	Delete(event.DeleteEvent) bool

	// Update returns true if the Update event should be processed
	Update(event.UpdateEvent) bool

	// Generic returns true if the Generic event should be processed
	Generic(event.GenericEvent) bool
}
</code></pre>
<p>在对应的动作中，可以看到这里作为过滤操作</p>
<pre><code class="language-go">func (e EventHandler) OnAdd(obj interface{}) {
	c := event.CreateEvent{}

	// Pull Object out of the object
	if o, ok := obj.(client.Object); ok {
		c.Object = o
	} else {
		log.Error(nil, &quot;OnAdd missing Object&quot;,
			&quot;object&quot;, obj, &quot;type&quot;, fmt.Sprintf(&quot;%T&quot;, obj))
		return
	}

	for _, p := range e.Predicates {
		if !p.Create(c) {
			return
		}
	}

	// Invoke create handler
	e.EventHandler.Create(c, e.Queue)
}
</code></pre>
<p>上面就看到了，对应是 <code>EventHandler.Create</code> 进行添加的，那么这些动作具体是在做什么呢？</p>
<p>在代码 <a href="https://github.com/kubernetes-sigs/controller-runtime/tree/master/pkg/handler" target="_blank"
   rel="noopener nofollow noreferrer" >pkg/handler</a> ,可以看到这些操作，类似于create，这里将ns/name放入到队列中。</p>
<pre><code class="language-go">func (e *EnqueueRequestForObject) Create(evt event.CreateEvent, q workqueue.RateLimitingInterface) {
	if evt.Object == nil {
		enqueueLog.Error(nil, &quot;CreateEvent received with no metadata&quot;, &quot;event&quot;, evt)
		return
	}
	q.Add(reconcile.Request{NamespacedName: types.NamespacedName{
		Name:      evt.Object.GetName(),
		Namespace: evt.Object.GetNamespace(),
	}})
}
</code></pre>
<h3 id="unqueue">unqueue</h3>
<p>上面看到了，入队的动作实际上都是将 <code>ns/name</code> 加入到队列中，那么出队列时又做了些什么呢？</p>
<p>通过 <code>controller.Start()</code>  可以看到controller在启动后都做了些什么动作</p>
<pre><code class="language-go">func (c *Controller) Start(ctx context.Context) error {
	c.mu.Lock()
	if c.Started {
		return errors.New(&quot;controller was started more than once. This is likely to be caused by being added to a manager multiple times&quot;)
	}

	c.initMetrics()

	// Set the internal context.
	c.ctx = ctx

	c.Queue = c.MakeQueue() // 初始化queue
	go func() { // 退出时，让queue关闭
		&lt;-ctx.Done()
		c.Queue.ShutDown()
	}()

	wg := &amp;sync.WaitGroup{}
	err := func() error {
		defer c.mu.Unlock()
		defer utilruntime.HandleCrash()

		// 启动informer前，将之前准备好的 evnetHandle predictates source注册
		for _, watch := range c.startWatches {
			c.LogConstructor(nil).Info(&quot;Starting EventSource&quot;, &quot;source&quot;, fmt.Sprintf(&quot;%s&quot;, watch.src))
				// 上面我们看过了，start就是真正的注册动作
			if err := watch.src.Start(ctx, watch.handler, c.Queue, watch.predicates...); err != nil {
				return err
			}
		}

		// Start the SharedIndexInformer factories to begin populating the SharedIndexInformer caches
		c.LogConstructor(nil).Info(&quot;Starting Controller&quot;)
		 // startWatches上面我们也看到了，是evnetHandle predictates source被缓存到里面，
        // 这里是拿出来将其启动
		for _, watch := range c.startWatches {
			syncingSource, ok := watch.src.(source.SyncingSource)
			if !ok {
				continue
			}

			if err := func() error {
				// use a context with timeout for launching sources and syncing caches.
				sourceStartCtx, cancel := context.WithTimeout(ctx, c.CacheSyncTimeout)
				defer cancel()

				// WaitForSync waits for a definitive timeout, and returns if there
				// is an error or a timeout
				if err := syncingSource.WaitForSync(sourceStartCtx); err != nil {
					err := fmt.Errorf(&quot;failed to wait for %s caches to sync: %w&quot;, c.Name, err)
					c.LogConstructor(nil).Error(err, &quot;Could not wait for Cache to sync&quot;)
					return err
				}

				return nil
			}(); err != nil {
				return err
			}
		}

		// which won't be garbage collected if we hold a reference to it.
		c.startWatches = nil

		// Launch workers to process resources
		c.LogConstructor(nil).Info(&quot;Starting workers&quot;, &quot;worker count&quot;, c.MaxConcurrentReconciles)
		wg.Add(c.MaxConcurrentReconciles)
        // 启动controller消费端的线程
		for i := 0; i &lt; c.MaxConcurrentReconciles; i++ {
			go func() {
				defer wg.Done()
				for c.processNextWorkItem(ctx) {
				}
			}()
		}

		c.Started = true
		return nil
	}()
	if err != nil {
		return err
	}

	&lt;-ctx.Done() // 阻塞，直到上下文关闭
	c.LogConstructor(nil).Info(&quot;Shutdown signal received, waiting for all workers to finish&quot;)
	wg.Wait() // 等待所有线程都关闭
	c.LogConstructor(nil).Info(&quot;All workers finished&quot;)
	return nil
}
</code></pre>
<p>通过上面的分析，可以看到，每个消费的worker线程，实际上调用的是 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/internal/controller/controller.go#L255-L275" target="_blank"
   rel="noopener nofollow noreferrer" >processNextWorkItem</a> 下面就来看看他究竟做了些什么？</p>
<pre><code class="language-go">func (c *Controller) processNextWorkItem(ctx context.Context) bool {
	obj, shutdown := c.Queue.Get() // 从队列中拿取数据
	if shutdown {
		return false
	}

	defer c.Queue.Done(obj)
	// 下面应该是prometheus指标的一些东西
	ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(1)
	defer ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(-1)
	// 获得的对象通过reconcileHandler处理
	c.reconcileHandler(ctx, obj)
	return true
}
</code></pre>
<p>那么下面看看 reconcileHandler 做了些什么</p>
<pre><code class="language-go">func (c *Controller) reconcileHandler(ctx context.Context, obj interface{}) {
	// Update metrics after processing each item
	reconcileStartTS := time.Now()
	defer func() {
		c.updateMetrics(time.Since(reconcileStartTS))
	}()

	// 检查下取出的数据是否为reconcile.Request，在之前enqueue时了解到是插入的这个类型的值
	req, ok := obj.(reconcile.Request)
	if !ok {
		// 如果错了就忘记
		c.Queue.Forget(obj)
		c.LogConstructor(nil).Error(nil, &quot;Queue item was not a Request&quot;, &quot;type&quot;, fmt.Sprintf(&quot;%T&quot;, obj), &quot;value&quot;, obj)
		return
	}

	log := c.LogConstructor(&amp;req)

	log = log.WithValues(&quot;reconcileID&quot;, uuid.NewUUID())
	ctx = logf.IntoContext(ctx, log)

	// 这里调用了自己在实现controller实现的Reconcile的动作
	result, err := c.Reconcile(ctx, req)
	switch {
	case err != nil:
		c.Queue.AddRateLimited(req)
		ctrlmetrics.ReconcileErrors.WithLabelValues(c.Name).Inc()
		ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelError).Inc()
		log.Error(err, &quot;Reconciler error&quot;)
	case result.RequeueAfter &gt; 0:
		c.Queue.Forget(obj)
		c.Queue.AddAfter(req, result.RequeueAfter)
		ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelRequeueAfter).Inc()
	case result.Requeue:
		c.Queue.AddRateLimited(req)
		ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelRequeue).Inc()
	default:
		c.Queue.Forget(obj)
		ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelSuccess).Inc()
	}
}
</code></pre>
<p>通过对example中的 <em>Reconcile</em> 查找其使用，可以看到，调用他的就是上面我们说道的 <code>reconcileHandler</code> ，到这里我们就知道了，controller 的运行流为 <code>Controller.Start()</code> &gt; <code>Controller.processNextWorkItem</code> &gt; <code>Controller.reconcileHandler</code> &gt; <code>Controller.Reconcile</code> 最终到达了我们自定义的业务逻辑处理 Reconcile</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220626231657599.png" alt="image-20220626231657599" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="manager">Manager</h2>
<p>在上面学习 <code>controller-runtime</code> 时了解到，有一个 <code>Manager</code> 的组件，这个组件是做什么呢？我们来分析下。</p>
<p><code>Manager</code> 是用来创建与启动 <code>controller</code> 的（允许多个 <code>controller</code> 与 一个 <code>manager</code> 关联），Manager会启动分配给他的所有controller，以及其他可启动的对象。</p>
<p>在 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/examples/crd/main.go#L104-L145" target="_blank"
   rel="noopener nofollow noreferrer" >example</a> 看到，会初始化一个 <code>ctrl.NewManager</code></p>
<pre><code class="language-go">func main() {
   ctrl.SetLogger(zap.New())

   mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{})
   if err != nil {
      setupLog.Error(err, &quot;unable to start manager&quot;)
      os.Exit(1)
   }

   // in a real controller, we'd create a new scheme for this
   err = api.AddToScheme(mgr.GetScheme())
   if err != nil {
      setupLog.Error(err, &quot;unable to add scheme&quot;)
      os.Exit(1)
   }

   err = ctrl.NewControllerManagedBy(mgr).
      For(&amp;api.ChaosPod{}).
      Owns(&amp;corev1.Pod{}).
      Complete(&amp;reconciler{
         Client: mgr.GetClient(),
         scheme: mgr.GetScheme(),
      })
   if err != nil {
      setupLog.Error(err, &quot;unable to create controller&quot;)
      os.Exit(1)
   }

   err = ctrl.NewWebhookManagedBy(mgr).
      For(&amp;api.ChaosPod{}).
      Complete()
   if err != nil {
      setupLog.Error(err, &quot;unable to create webhook&quot;)
      os.Exit(1)
   }

   setupLog.Info(&quot;starting manager&quot;)
   if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
      setupLog.Error(err, &quot;problem running manager&quot;)
      os.Exit(1)
   }
}
</code></pre>
<p>这个 <code>manager</code> 就是  <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/manager/manager.go#L52-L97" target="_blank"
   rel="noopener nofollow noreferrer" >controller-runtime\pkg\manager\manager.go</a> 下的 <code>Manager</code>， Manager 通过初始化 Caches 和 Clients 等共享依赖，并将它们提供给 Runnables。</p>
<pre><code class="language-go">type Manager interface {
	// 提供了与APIServer交互的方式，如incluster，indexer，cache等
	cluster.Cluster

    // Runnable 是任意可允许的cm中的组件，如 webhook，controller，Caches，在new中调用时，
    // 可以看到是传入的是一个controller，这里可以启动的是带有Start()方法的，通过调用Start()
    // 来启动组件
    Add(Runnable) error
    
    // 实现选举方法。当elected关闭，则选举为leader
	Elected() &lt;-chan struct{}

	// 这为一些列健康检查和指标的方法，和我们关注的没有太大关系
	AddMetricsExtraHandler(path string, handler http.Handler) error
	AddHealthzCheck(name string, check healthz.Checker) error
	AddReadyzCheck(name string, check healthz.Checker) error

	// Start将启动所有注册进来的控制器，直到ctx取消。如果有任意controller报错，则立即退出
    // 如果使用了 LeaderElection，则必须在此返回后立即退出二进制文件，
	Start(ctx context.Context) error

	// GetWebhookServer returns a webhook.Server
	GetWebhookServer() *webhook.Server

	// GetLogger returns this manager's logger.
	GetLogger() logr.Logger

	// GetControllerOptions returns controller global configuration options.
	GetControllerOptions() v1alpha1.ControllerConfigurationSpec
}
</code></pre>
<h3 id="controller-manager">controller-manager</h3>
<p><a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/manager/internal.go#L66-L173" target="_blank"
   rel="noopener nofollow noreferrer" >controllerManager</a> 则实现了这个manager的抽象</p>
<pre><code class="language-go">type controllerManager struct {
	sync.Mutex
	started bool

	stopProcedureEngaged *int64
	errChan              chan error
	runnables            *runnables
	
	cluster cluster.Cluster

	// recorderProvider 用于记录eventhandler source predictate
	recorderProvider *intrec.Provider

	// resourceLock forms the basis for leader election
	resourceLock resourcelock.Interface

	// 在退出时是否关闭选举租约
	leaderElectionReleaseOnCancel bool
	// 一些指标性的，暂时不需要关注
	metricsListener net.Listener
	metricsExtraHandlers map[string]http.Handler
	healthProbeListener net.Listener
	readinessEndpointName string
	livenessEndpointName string
	readyzHandler *healthz.Handler
	healthzHandler *healthz.Handler

	// 有关controller全局参数
	controllerOptions v1alpha1.ControllerConfigurationSpec

	logger logr.Logger

	// 用于关闭 LeaderElection.Run(...) 的信号
	leaderElectionStopped chan struct{}

    // 取消选举，在失去选举后，必须延迟到gracefulShutdown之后os.exit()
	leaderElectionCancel context.CancelFunc

	// leader取消选举
	elected chan struct{}

	port int
	host string
	certDir string
	webhookServer *webhook.Server
	webhookServerOnce sync.Once
	// 非leader节点强制leader的等待时间
	leaseDuration time.Duration
	// renewDeadline is the duration that the acting controlplane will retry
	// refreshing leadership before giving up.
	renewDeadline time.Duration
	// LeaderElector重新操作的时间
	retryPeriod time.Duration
	// gracefulShutdownTimeout 是在manager停止之前让runnables停止的持续时间。
	gracefulShutdownTimeout time.Duration

	// onStoppedLeading is callled when the leader election lease is lost.
	// It can be overridden for tests.
	onStoppedLeading func()

	shutdownCtx context.Context
	internalCtx    context.Context
	internalCancel context.CancelFunc
	internalProceduresStop chan struct{}
}
</code></pre>
<h3 id="workflow">workflow</h3>
<p>了解完ControllerManager之后，我们通过 example 来看看 ControllerManager 的workflow</p>
<pre><code class="language-go">func main() {
   ctrl.SetLogger(zap.New())
   // New一个manager
   mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{})
   if err != nil {
      setupLog.Error(err, &quot;unable to start manager&quot;)
      os.Exit(1)
   }

   // in a real controller, we'd create a new scheme for this
   err = api.AddToScheme(mgr.GetScheme())
   if err != nil {
      setupLog.Error(err, &quot;unable to add scheme&quot;)
      os.Exit(1)
   }

   err = ctrl.NewControllerManagedBy(mgr).
      For(&amp;api.ChaosPod{}).
      Owns(&amp;corev1.Pod{}).
      Complete(&amp;reconciler{
         Client: mgr.GetClient(),
         scheme: mgr.GetScheme(),
      })
   if err != nil {
      setupLog.Error(err, &quot;unable to create controller&quot;)
      os.Exit(1)
   }

   err = ctrl.NewWebhookManagedBy(mgr).
      For(&amp;api.ChaosPod{}).
      Complete()
   if err != nil {
      setupLog.Error(err, &quot;unable to create webhook&quot;)
      os.Exit(1)
   }

   setupLog.Info(&quot;starting manager&quot;)
   if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
      setupLog.Error(err, &quot;problem running manager&quot;)
      os.Exit(1)
   }
}
</code></pre>
<ul>
<li>通过  <code>manager.New()</code> 初始化一个manager，这里面会初始化一些列的manager的参数</li>
<li>通过 <code>ctrl.NewControllerManagedBy</code> 注册 controller 到manager中
<ul>
<li><code>ctrl.NewControllerManagedBy</code>  是 builder的一个别名，构建出一个builder类型的controller</li>
<li><code>builder</code> 中的 <code>ctrl</code> 就是 controller</li>
</ul>
</li>
<li>启动manager</li>
</ul>
<h3 id="builder">builder</h3>
<p>下面看来看下builder在构建时做了什么</p>
<pre><code class="language-go">// Builder builds a Controller.
type Builder struct {
	forInput         ForInput
	ownsInput        []OwnsInput
	watchesInput     []WatchesInput
	mgr              manager.Manager
	globalPredicates []predicate.Predicate
	ctrl             controller.Controller
	ctrlOptions      controller.Options
	name             string
}
</code></pre>
<p>我们看到 example 中是调用了 <code>For()</code> 动作，那么这个   <code>For()</code> 是什么呢？</p>
<p>通过注释，我们可以看到 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/builder/controller.go#L82-L94" target="_blank"
   rel="noopener nofollow noreferrer" >For()</a> 提供了 调解对象类型，ControllerManagedBy 通过  <em>reconciling object</em> 来相应对应<code>create/delete/update</code> 事件。调用 <code>For()</code> 相当于调用了 <code>Watches(&amp;source.Kind{Type: apiType}, &amp;handler.EnqueueRequestForObject{})</code> 。</p>
<pre><code class="language-go">func (blder *Builder) For(object client.Object, opts ...ForOption) *Builder {
	if blder.forInput.object != nil {
		blder.forInput.err = fmt.Errorf(&quot;For(...) should only be called once, could not assign multiple objects for reconciliation&quot;)
		return blder
	}
	input := ForInput{object: object}
	for _, opt := range opts {
		opt.ApplyToFor(&amp;input) //最终把我们要监听的对象每个 opts注册进去
	}

	blder.forInput = input
	return blder
}
</code></pre>
<p>接下来是调用的 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/builder/controller.go#L106-L114" target="_blank"
   rel="noopener nofollow noreferrer" >Owns()</a> ，<code>Owns()</code> 看起来和  <code>For()</code> 功能是类似的。只是说属于不同，是通过Owns方法设置的</p>
<pre><code class="language-go">func (blder *Builder) Owns(object client.Object, opts ...OwnsOption) *Builder {
	input := OwnsInput{object: object}
	for _, opt := range opts {
		opt.ApplyToOwns(&amp;input)
	}

	blder.ownsInput = append(blder.ownsInput, input)
	return blder
}
</code></pre>
<p>最后到了 Complete()，<code>Complete</code> 是完成这个controller的构建</p>
<pre><code class="language-go">// Complete builds the Application Controller.
func (blder *Builder) Complete(r reconcile.Reconciler) error {
	_, err := blder.Build(r)
	return err
}

// Build 创建控制器并返回
func (blder *Builder) Build(r reconcile.Reconciler) (controller.Controller, error) {
	if r == nil {
		return nil, fmt.Errorf(&quot;must provide a non-nil Reconciler&quot;)
	}
	if blder.mgr == nil {
		return nil, fmt.Errorf(&quot;must provide a non-nil Manager&quot;)
	}
	if blder.forInput.err != nil {
		return nil, blder.forInput.err
	}
	// Checking the reconcile type exist or not
	if blder.forInput.object == nil {
		return nil, fmt.Errorf(&quot;must provide an object for reconciliation&quot;)
	}

	// Set the ControllerManagedBy
	if err := blder.doController(r); err != nil {
		return nil, err
	}

	// Set the Watch
	if err := blder.doWatch(); err != nil {
		return nil, err
	}

	return blder.ctrl, nil
}
</code></pre>
<p>这里面可以看到，会完成 doController 和 doWatch</p>
<p>doController会初始化好这个controller并返回</p>
<pre><code class="language-go">func (blder *Builder) doController(r reconcile.Reconciler) error {
	globalOpts := blder.mgr.GetControllerOptions()

	ctrlOptions := blder.ctrlOptions
	if ctrlOptions.Reconciler == nil {
		ctrlOptions.Reconciler = r
	}

	// 通过检索GVK获得默认的名称
	gvk, err := getGvk(blder.forInput.object, blder.mgr.GetScheme())
	if err != nil {
		return err
	}

	// 设置并发，如果最大并发为0则找到一个
    // 追踪下去看似是对于没有设置时，例如会根据 app group中的 ReplicaSet设定
    // 就是在For()传递的一个类型的数量来确定并发的数量
	if ctrlOptions.MaxConcurrentReconciles == 0 {
		groupKind := gvk.GroupKind().String()

		if concurrency, ok := globalOpts.GroupKindConcurrency[groupKind]; ok &amp;&amp; concurrency &gt; 0 {
			ctrlOptions.MaxConcurrentReconciles = concurrency
		}
	}

	// Setup cache sync timeout.
	if ctrlOptions.CacheSyncTimeout == 0 &amp;&amp; globalOpts.CacheSyncTimeout != nil {
		ctrlOptions.CacheSyncTimeout = *globalOpts.CacheSyncTimeout
	}
	// 给controller一个name，如果没有初始化传递，则使用Kind做名称
	controllerName := blder.getControllerName(gvk)

	// Setup the logger.
	if ctrlOptions.LogConstructor == nil {
		log := blder.mgr.GetLogger().WithValues(
			&quot;controller&quot;, controllerName,
			&quot;controllerGroup&quot;, gvk.Group,
			&quot;controllerKind&quot;, gvk.Kind,
		)

		lowerCamelCaseKind := strings.ToLower(gvk.Kind[:1]) + gvk.Kind[1:]

		ctrlOptions.LogConstructor = func(req *reconcile.Request) logr.Logger {
			log := log
			if req != nil {
				log = log.WithValues(
					lowerCamelCaseKind, klog.KRef(req.Namespace, req.Name),
					&quot;namespace&quot;, req.Namespace, &quot;name&quot;, req.Name,
				)
			}
			return log
		}
	}

	// 这里就是构建一个新的控制器了，也就是前面说到的  manager.New()
	blder.ctrl, err = newController(controllerName, blder.mgr, ctrlOptions)
	return err
}
</code></pre>
<p><a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/manager/manager.go#L336-L436" target="_blank"
   rel="noopener nofollow noreferrer" >manager.New()</a></p>
<h3 id="start-manager">start Manager</h3>
<p>接下来是manager的启动，也就是对应的 <code>start()</code> 与 <a href="https://github.com/kubernetes-sigs/controller-runtime/blob/ffd9ec8768b7d2e82aee6b0c76d481a5bdda4075/pkg/builder/controller.go#L220-L270" target="_blank"
   rel="noopener nofollow noreferrer" >doWatch()</a></p>
<p>通过下述代码我们可以看出来，对于 <code>doWatch()</code> 就是把 <code>compete()</code> 前的一些资源的事件函数都注入到controller 中</p>
<pre><code class="language-go">func (blder *Builder) doWatch() error {
	// 调解类型，这也也就是对于For的obj来说，我们需要的是什么结构的，如非结构化数据或metadata-only
    // metadata-only就是配置成一个GVK schema.GroupVersionKind
	typeForSrc, err := blder.project(blder.forInput.object, blder.forInput.objectProjection)
	if err != nil {
		return err
    }&amp;source.Kind{}
    // 一些准备工作，将对象封装为&amp;source.Kind{}
    // 
	src := &amp;source.Kind{Type: typeForSrc}
	hdler := &amp;handler.EnqueueRequestForObject{} // 就是包含obj的一个事件队列
	allPredicates := append(blder.globalPredicates, blder.forInput.predicates...)
	// 这里又到之前说过的controller watch了
    // 将一系列的准备动作注入到cache 如 source eventHandler predicate
    if err := blder.ctrl.Watch(src, hdler, allPredicates...); err != nil {
		return err
	}

	// 再重复 ownsInput 动作
	for _, own := range blder.ownsInput {
		typeForSrc, err := blder.project(own.object, own.objectProjection)
		if err != nil {
			return err
		}
		src := &amp;source.Kind{Type: typeForSrc}
		hdler := &amp;handler.EnqueueRequestForOwner{
			OwnerType:    blder.forInput.object,
			IsController: true,
		}
		allPredicates := append([]predicate.Predicate(nil), blder.globalPredicates...)
		allPredicates = append(allPredicates, own.predicates...)
		if err := blder.ctrl.Watch(src, hdler, allPredicates...); err != nil {
			return err
		}
	}

	// 在对 ownsInput 进行重复的操作
	for _, w := range blder.watchesInput {
		allPredicates := append([]predicate.Predicate(nil), blder.globalPredicates...)
		allPredicates = append(allPredicates, w.predicates...)

		// If the source of this watch is of type *source.Kind, project it.
		if srckind, ok := w.src.(*source.Kind); ok {
			typeForSrc, err := blder.project(srckind.Type, w.objectProjection)
			if err != nil {
				return err
			}
			srckind.Type = typeForSrc
		}

		if err := blder.ctrl.Watch(w.src, w.eventhandler, allPredicates...); err != nil {
			return err
		}
	}
	return nil
}
</code></pre>
<p>由于前两部 <code>builder</code> 的操作将 mgr 指针传入到 builder中，并且操作了 <code>complete()</code> ，也就是操作了 <code>build()</code> ,这代表了对 <code>controller</code> 完成了初始化，和事件注入（<code>watch</code>）的操作，所以 Start()，就是将controller启动</p>
<pre><code class="language-go">func (cm *controllerManager) Start(ctx context.Context) (err error) {
	cm.Lock()
	if cm.started {
		cm.Unlock()
		return errors.New(&quot;manager already started&quot;)
	}
	var ready bool
	defer func() {
		if !ready {
			cm.Unlock()
		}
	}()

	// Initialize the internal context.
	cm.internalCtx, cm.internalCancel = context.WithCancel(ctx)

	// 这个channel代表了controller的停止
	stopComplete := make(chan struct{})
	defer close(stopComplete)
	// This must be deferred after closing stopComplete, otherwise we deadlock.
	defer func() {
		stopErr := cm.engageStopProcedure(stopComplete)
		if stopErr != nil {
			if err != nil {
				err = kerrors.NewAggregate([]error{err, stopErr})
			} else {
				err = stopErr
			}
		}
	}()

	// Add the cluster runnable.
	if err := cm.add(cm.cluster); err != nil {
		return fmt.Errorf(&quot;failed to add cluster to runnables: %w&quot;, err)
	}
    // 指标类
	if cm.metricsListener != nil {
		cm.serveMetrics()
	}
	if cm.healthProbeListener != nil {
		cm.serveHealthProbes()
	}
	if err := cm.runnables.Webhooks.Start(cm.internalCtx); err != nil {
		if !errors.Is(err, wait.ErrWaitTimeout) {
			return err
		}
	}

	// 等待informer同步完成
	if err := cm.runnables.Caches.Start(cm.internalCtx); err != nil {
		if !errors.Is(err, wait.ErrWaitTimeout) {
			return err
		}
	}

	// 非选举模式，runnable将在cache同步完成后启动
	if err := cm.runnables.Others.Start(cm.internalCtx); err != nil {
		if !errors.Is(err, wait.ErrWaitTimeout) {
			return err
		}
	}

	// Start the leader election and all required runnables.
	{
		ctx, cancel := context.WithCancel(context.Background())
		cm.leaderElectionCancel = cancel
		go func() {
			if cm.resourceLock != nil {
				if err := cm.startLeaderElection(ctx); err != nil {
					cm.errChan &lt;- err
				}
			} else {
				// Treat not having leader election enabled the same as being elected.
				if err := cm.startLeaderElectionRunnables(); err != nil {
					cm.errChan &lt;- err
				}
				close(cm.elected)
			}
		}()
	}

	ready = true
	cm.Unlock()
	select {
	case &lt;-ctx.Done():
		// We are done
		return nil
	case err := &lt;-cm.errChan:
		// Error starting or running a runnable
		return err
	}
}
</code></pre>
<p>可以看到上面启动了4种类型的runnable，实际上就是对这runnable进行启动，例如 controller，cache等。</p>
<p>回顾一下，我们之前在使用<code>code-generator</code> 生成，并自定义controller时，我们也是通过启动 <code>informer.Start()</code> ，否则会报错。</p>
<p>最后可以通过一张关系图来表示，client-go与controller-manager之间的关系</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/controller-runtime.svg" alt="svg" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p>Reference</p>
<p><a href="https://vivilearns2code.github.io/k8s/2021/03/12/diving-into-controller-runtime.html#the-manager" target="_blank"
   rel="noopener nofollow noreferrer" >diving controller runtime</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>扩展Kubernetes API的另一种方式 - APIServer aggregation</title>
      <link>https://www.oomkill.com/2022/06/ch04-apiserver-aggregation/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch04-apiserver-aggregation/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<h3 id="what-is-kubernetes-aggregation">What is Kubernetes aggregation</h3>
<p><strong>Kubernetes apiserver aggregation</strong> <em>AA</em> 是Kubernetes提供的一种扩展API的方法，目前并没有GA</p>
<h3 id="difference-between--crd-and-aa">Difference between  CRD and AA</h3>
<p>众所周知，kubernetes扩展API的方法大概为三种：CRD、AA、手动扩展源码。根据<a href="https://www.youtube.com/c/cloudnativefdn" target="_blank"
   rel="noopener nofollow noreferrer" >CNCF分享</a>中Min Kim说的AA更关注于实践，而用户无需了解底层的原理，这里使用过 <code>kubebuilder</code>， <code>code-generator</code> 的用户是很能体会到这点。官方也给出了CRD与AA的区别</p>
<blockquote>
<h3 id="api-access-control">API Access Control</h3>
<h5 id="authentication">Authentication</h5>
<ul>
<li><strong>CR</strong>: All strategies supported. Configured by root apiserver.</li>
<li><strong>AA</strong>: Supporting all root apiserver&rsquo;s authenticating strategies but it has to be done via <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication" target="_blank"
   rel="noopener nofollow noreferrer" >authentication token review api</a> except for <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#authenticating-proxy" target="_blank"
   rel="noopener nofollow noreferrer" >authentication proxy</a> which will cause an extra cost of network RTT.</li>
</ul>
<h5 id="authorization">Authorization</h5>
<ul>
<li><strong>CR</strong>: All strategies supported. Configured by root apiserver.</li>
<li><strong>AA</strong>: Delegating authorization requests to root apiserver via <a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/#checking-api-access" target="_blank"
   rel="noopener nofollow noreferrer" >SubjectAccessReview api</a>. Note that this approach will also cost a network RTT.</li>
</ul>
<h5 id="admission-control">Admission Control</h5>
<ul>
<li><strong>CR</strong>: You could extend via dynamic admission control webhook (which is costing network RTT).</li>
<li><strong>AA</strong>: While You can develop and customize your own admission controller which is dedicated to your AA. While You can&rsquo;t reuse root-apiserver&rsquo;s built-in admission controllers nomore.</li>
</ul>
<h3 id="api-schema">API Schema</h3>
<p>Note: CR&rsquo;s integration with OpenAPI schema is being enhanced in the future releases and it will have a stronger integration with OpenAPI mechanism.</p>
<h5 id="validating">Validating</h5>
<ul>
<li><strong>CR</strong>: (landed in 1.12) Defined via OpenAPIv3 Schema grammar. <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#validation" target="_blank"
   rel="noopener nofollow noreferrer" >more</a></li>
<li><strong>AA</strong>: You can customize any validating flow you want.</li>
</ul>
<h5 id="conversion">Conversion</h5>
<ul>
<li><strong>CR</strong>: (landed in 1.13) The CR conversioning (basically from storage version to requested version) could be done via conversioning webhook.</li>
<li><strong>AA</strong>: Develop any conversion you want.</li>
</ul>
<h5 id="subresource">SubResource</h5>
<ul>
<li><strong>CR</strong>: Currently only status and scale sub-resource supported.</li>
<li><strong>AA</strong>: You can customize any sub-resouce you want.</li>
</ul>
<h5 id="openapi-schema">OpenAPI Schema</h5>
<ul>
<li><strong>CR</strong>: (landed in 1.13) The corresponding CRD&rsquo;s OpenAPI schema will be automatically synced to root-apiserver&rsquo;s openapi doc api.</li>
<li><strong>AA</strong>: OpenAPI doc has to be manually generated by code-generating tools.</li>
</ul>
</blockquote>
<h2 id="authentication-1">Authentication</h2>
<p>要想很好的使用AA，就需要对kubernetes与 AA 之间认证机制进行有一定的了解，这里涉及到一些概念</p>
<ul>
<li>客户端证书认证</li>
<li>token认证</li>
<li>请求头认证</li>
</ul>
<p>在下面的说明中，所有出现的APIServer都是指Kubernetes集群组件APIServer也可以为 root APIServer；所有的AA都是指 extension apiserver，就是自行开发的 AA。</p>
<h3 id="客户端证书">客户端证书</h3>
<p>客户端证书就是CA签名的证书，由客户端指定CA证书，在客户端连接时进行身份验证，在Kubernetes APIserver也使用了相同的机制。</p>
<p>默认情况下，APIServer在启动时指定参数 <code>--client-ca-file</code> ，这时APIServer会创建一个名为 <code>extension-apiserver-authentication</code> ，命名空间为 <code>kube-system</code> 下的 configMap。</p>
<pre><code class="language-bash">$ kubectl get cm -A
NAMESPACE     NAME                                 DATA   AGE
kube-system   extension-apiserver-authentication   6      21h

kubectl get cm extension-apiserver-authentication -n kube-system -o yaml
</code></pre>
<p>由上面的命令可以看出这个configMap将被填充到客户端（AA Pod实例）中，使用此CA证书作为用于验证客户端身份的CA。这样客户端会读取这个configMap，与APIServer进行身份认证。</p>
<pre><code class="language-bash">I0622 14:24:00.509486       1 secure_serving.go:178] Serving securely on [::]:443
I0622 14:24:00.509556       1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
</code></pre>
<h3 id="token认证">token认证</h3>
<p>Token认证是指通过HTTP Header传入 <code>Authorization: Bearer $TOKEN</code> 的方式进行客户端认证，这也是Kubernetes集群内认证常用的方法。</p>
<p>在这种情况下，允许对APIServer进行认证也同样可以对AA进行认证。如果不想 AA 对同一集群进行身份验证，或AA在集群外部运行，可以将参数 <code>--authentication-kubeconfig</code> 以指定要使用的不同 Kubeconfig 认证。</p>
<p>下面实例是AA的启动参数</p>
<pre><code class="language-bash">./bin/apiserver -h|grep authentication-kubeconfig
      --authentication-kubeconfig string                        kubeconfig file pointing at the 'core' kubernetes server with enough righ
ts to create tokenreviews.authentication.k8s.io. This is optional. If empty, all token requests are considered to be anonymous and no cli
ent CA is looked up in the cluster.
</code></pre>
<h3 id="请求头认证">请求头认证</h3>
<p>RequestHeader 认证是指，APIServer对来自AA代理连接进行的身份认证。</p>
<p>默认情况下，AA 从 <code>extension-apiserver-authentication</code> 中提到的 ConfigMap 中 提取 requestheader 客户端 CA 证书与 CN。如果主 Kubernetes APIServer 配置了选项 <code>--requestheader-client-ca-file</code> ，则它会填充此内容。</p>
<p>跳过客户端认证 <code>--authentication-skip-lookup</code></p>
<h3 id="授权">授权</h3>
<p>默认情况下，AA  服务器会通过自动注入到 Kubernetes 集群上运行的 pod 的连接信息和凭据，来连接到主 Kubernetes API 服务器。</p>
<pre><code class="language-bash">E0622 11:20:12.375512       1 errors.go:77] Post &quot;https://192.168.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews&quot;: write tcp 192.168.0.36:39324-&gt;192.168.0.1:443: write: connection reset by peer
</code></pre>
<p>如果AA在集群外部部署，可以指定<code>--authorization-kubeconfig</code> 通过kubeconfig进行认证，这就类似于二进制部署中的信息。</p>
<p>默认情况下，Kubernetes 集群会启用RBAC，这就意味着AA 创建多个clusterrolebinding。</p>
<p>下面日志是 AA 对于集群中资源访问无权限的情况</p>
<pre><code class="language-bash">E0622 09:01:26.750320       1 reflector.go:178] pkg/mod/k8s.io/client-go@v0.18.10/tools/cache/reflector.go:125: Failed to list *v1.MutatingWebhookConfiguration: mutatingwebhookconfigurations.admissionregistration.k8s.io is forbidden: User &quot;system:serviceaccount:default:default&quot; cannot list resource &quot;mutatingwebhookconfigurations&quot; in API group &quot;admissionregistration.k8s.io&quot; at the cluster scope
E0622 09:01:29.357897       1 reflector.go:178] pkg/mod/k8s.io/client-go@v0.18.10/tools/cache/reflector.go:125: Failed to list *v1.Namespace: namespaces is forbidden: User &quot;system:serviceaccount:default:default&quot; cannot list resource &quot;namespaces&quot; in API group &quot;&quot; at the cluster scope
E0622 09:01:39.998496       1 reflector.go:178] pkg/mod/k8s.io/client-go@v0.18.10/tools/cache/reflector.go:125: Failed to list *v1.ValidatingWebhookConfiguration: validatingwebhookconfigurations.admissionregistration.k8s.io is forbidden: User &quot;system:serviceaccount:default:default&quot; cannot list resource &quot;validatingwebhookconfigurations&quot; in API group &quot;admissionregistration.k8s.io&quot; at the cluster scope
</code></pre>
<p>需要手动在namespace <code>kube-system</code> 中创建rolebindding到 role  <code>extension-apiserver-authentication-reader</code> 。这样就可以访问到configMap了。</p>
<h2 id="apiserver-builder">apiserver-builder</h2>
<p><code>apiserver-builder</code> 项目就是创建AA的工具，可以参考 <a href="https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/v1.18.0/docs/installing.md" target="_blank"
   rel="noopener nofollow noreferrer" >installing.md</a> 来安装</p>
<h3 id="初始化项目">初始化项目</h3>
<p>初始化命令</p>
<ul>
<li><code>&lt;your-domain&gt;</code> 这个是你的API资源的组，参考 <code>k8s.io/api</code>
<ul>
<li>如果组的名称是域名就设置为主域名，例如内置组
<ul>
<li><code>/apis/authentication.k8s.io</code></li>
<li><code>/apis/batch</code></li>
</ul>
</li>
</ul>
</li>
<li>生成的go mod 包名为你所在的目录的名称
<ul>
<li>例如，在firewalld目录下，go.mod 的名称为 firewalld</li>
</ul>
</li>
</ul>
<pre><code class="language-bash">apiserver-boot init repo --domain &lt;your-domain&gt;
</code></pre>
<p>例如</p>
<pre><code class="language-bash">apiserver-boot init repo --domain fedoraproject.org
</code></pre>
<blockquote>
<p>注：这里&ndash;domain设置为主域名就可以了，后面生成的group会按照格式 <group>+<domain></p>
</blockquote>
<pre><code>apiserver-boot must be run from the directory containing the go package to bootstrap. This must
 be under $GOPATH/src/&lt;package&gt;.
</code></pre>
<p>必须在 <code>$GOPATH/src</code> 下创建你的项目，我这里的为 <code>GOPATH=go/src</code> ，这时创建项目必须在目录 <code>go/src/src/{project}</code>  下创建</p>
<h3 id="创建一个gvk">创建一个GVK</h3>
<pre><code class="language-bash">apiserver-boot create group version resource \
	--group firewalld \
	--version v1 \
	--kind PortRule
</code></pre>
<p>在创建完成之后会生成 api-like的类型，我们只需要填充自己需要的就可以了</p>
<pre><code class="language-go">type PortRule struct {
	metav1.TypeMeta   `json:&quot;,inline&quot;`
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`

	Spec   PortRuleSpec   `json:&quot;spec,omitempty&quot;`
	Status PortRuleStatus `json:&quot;status,omitempty&quot;`
}

// PortRuleSpec defines the desired state of PortRule
type PortRuleSpec struct { // 这里内容都为空的，自己添加即可
	Name        string `json:&quot;name&quot;`
	Host        string `json:&quot;host&quot;`
	Port        int    `json:&quot;port&quot;`
	IsPremanent bool   `json:&quot;isPremanent,omitempty&quot;`
}

// PortRuleStatus defines the observed state of PortRule
type PortRuleStatus struct {
}
</code></pre>
<h3 id="生成代码">生成代码</h3>
<p><code>apiserver-boot</code> 没有专门用来生成代码的命令，可以执行任意生成命令即可，这里使用生成二进制执行文件命令，这个过程相当长。</p>
<pre><code class="language-bash">apiserver-boot build executables
</code></pre>
<p>如果编译错误可以使用 <code>--generate=false</code> 跳过生成，这样就可以节省大量时间。</p>
<h3 id="运行方式">运行方式</h3>
<p>运行方式无非三种，本地运行，集群内运行，集群外运行</p>
<h4 id="running_locally">running_locally</h4>
<p>本地运行需要有一个etcd服务，不用配置ca证书，这里使用docker运行</p>
<pre><code class="language-sh">docker run -d --name Etcd-server \
    --publish 2379:2379 \
    --publish 2380:2380 \
    --env ALLOW_NONE_AUTHENTICATION=yes \
    --env ETCD_ADVERTISE_CLIENT_URLS=http://etcd-server:2379 \
    bitnami/etcd:latest
</code></pre>
<p>然后执行命令，执行成功后会弹出对应的访问地址</p>
<pre><code class="language-bash">apiserver-boot build executables
apiserver-boot run local
</code></pre>
<h4 id="running_in_cluster">running_in_cluster</h4>
<h5 id="构建镜像">构建镜像</h5>
<p>需要先构建容器镜像，<code>apiserver-boot build container --image &lt;image&gt;</code>  这将生成代码，构建 apiserver 和controller二进制文件，然后构建容器映像。构建完成后还需要将对应的镜像push到仓库（可选）</p>
<pre><code class="language-bash">apiserver-boot build config \
	--name &lt;servicename&gt; \
	--namespace &lt;namespace to run in&gt; \
	--image &lt;image to run&gt;
</code></pre>
<blockquote>
<p>注，这个操作需要在支持Linux内核的环境下构建，wsl不具备内核功能故会报错，需要替换为wsl2，而工具是下载的，如果需要wsl1+Docker Desktop构建，需要自己修改</p>
</blockquote>
<h5 id="构建配置">构建配置</h5>
<pre><code>apiserver-boot build config \
	--name &lt;servicename&gt; \
	--namespace &lt;namespace to run in&gt; \
	--image &lt;image to run&gt;
</code></pre>
<p>构建配置的操作会执行以下几个步骤：</p>
<ul>
<li>在 <code>&lt;project/config/certificates</code> 目录下创建一个 CA证书</li>
<li>在目录 <code>&lt;project/config/*.yaml</code> 下生成kubernetes所需的资源清单。</li>
</ul>
<blockquote>
<p>注：</p>
<p>实际上这个清单并不能完美适配任何环境，需要手动修改一下配置</p>
<p>运行的Pod中包含apiserver与controller，如果使用kubebuilder创建的controller可以自行修改资源清单</p>
</blockquote>
<h5 id="修改apiserver的配置">修改apiserver的配置</h5>
<p>下面参数是有关于 AA 认证的参数</p>
<pre><code>--proxy-client-cert-file=/etc/kubernetes/pki/firewalld.crt \
--proxy-client-key-file=/etc/kubernetes/pki/firewalld.key \
--requestheader-allowed-names=kube-apiserver-kubelet-client,firewalld.default.svc,firewalld-certificate-authority \
--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt \
--requestheader-extra-headers-prefix=X-Remote-Extra- \
</code></pre>
<ul>
<li><code>--requestheader-username-headers</code>：用于存储用户名的标头</li>
<li><code>--requestheader-group-headers</code>：用于存储组的标题</li>
<li><code>--requestheader-extra-headers-prefix</code>：附加到所有额外标头的前缀</li>
<li><code>--proxy-client-key-file</code> ：私钥文件</li>
<li><code>--proxy-client-cert-file</code>：客户端证书文件</li>
<li><code>--requestheader-client-ca-file</code>：签署客户端证书文件的 CA 的证书</li>
<li><code>--requestheader-allowed-names</code>：签名客户端证书中的CN)</li>
</ul>
<p>由以上信息得知，实际上 <code>apiserver-boot</code> 所生成的ca用不上，需要kubernetes自己的ca进行签署，这里简单提供两个命令，使用kubernetes集群证书进行颁发证书。这里kubernetes集群证书使用<a href="https://github.com/CylonChau/kubernetes-generator" target="_blank"
   rel="noopener nofollow noreferrer" >kubernetes-generator</a> 生产的。这里根据这个ca再次生成用于 AA 认证的证书。</p>
<pre><code>openssl req -new \
    -key firewalld.key \
    -subj &quot;/CN=firewalld.default.svc&quot; \
    -config &lt;(cat /etc/pki/tls/openssl.cnf &lt;(printf &quot;[aa]\nsubjectAltName=DNS:firewalld, DNS:firewalld.default.svc, DNS:firewalld-certificate-authority, DNS:kubernetes.default.svc&quot;)) \
    -out firewalld.csr

openssl ca \
	-in firewalld.csr \
	-cert front-proxy-ca.crt \
	-keyfile front-proxy-ca.key \
	-out firewalld.crt \
	-days 3650 \
	-extensions aa \
	-extfile &lt;(cat /etc/pki/tls/openssl.cnf  &lt;(printf &quot;[aa]\nsubjectAltName=DNS:firewalld, DNS:firewalld.default.svc, DNS:firewalld-certificate-authority, DNS:kubernetes.default.svc&quot;))
</code></pre>
<p>完成后重新生成所需的yaml资源清单即可，通过资源清单来测试下扩展的API</p>
<pre><code class="language-yaml">apiVersion: firewalld.fedoraproject.org/v1
kind: PortRule
metadata:
  name: portrule-example
spec:
  name: &quot;nginx&quot;
  host: &quot;10.0.0.3&quot;
  port: 80

</code></pre>
<pre><code class="language-bash">$ kubectl apply -f http.yaml 
portrule.firewalld.fedoraproject.org/portrule-example created
$ kubectl get portrule
NAME               CREATED AT
portrule-example   2022-06-22T15:12:59Z
</code></pre>
<p>更详细的说明建议阅读下Reference，都是官方提供的详细说明文档</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/" target="_blank"
   rel="noopener nofollow noreferrer" >aggregation layer</a></p>
<p><a href="https://github.com/kubernetes-sigs/apiserver-builder-alpha/tree/master/docs" target="_blank"
   rel="noopener nofollow noreferrer" >apiserver-builder doc</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>kubernetes代码生成器 - code-generator</title>
      <link>https://www.oomkill.com/2022/06/ch14-code-generator/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch14-code-generator/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>Kubernetes中提供了多种自定义控制器的方式：</p>
<ul>
<li><a href="https://github.com/kubernetes/code-generator" target="_blank"
   rel="noopener nofollow noreferrer" >code-generator</a></li>
<li><a href="https://github.com/kubernetes-sigs/kubebuilder" target="_blank"
   rel="noopener nofollow noreferrer" >kubebuilder</a></li>
<li><a href="https://github.com/operator-framework/operator-sdk" target="_blank"
   rel="noopener nofollow noreferrer" >Operator</a></li>
</ul>
<p>Controller 作为CRD的核心，这里将解释如何使用 <code>code-generator</code> 来创建自定义的控制器，作为文章的案例，将完成一个 Firewalld Port 规则的控制器作为描述，通过 Kubernetes 规则来生成对应节点上的 iptables规则。</p>
<h3 id="prerequisites">Prerequisites</h3>
<h3 id="crd">CRD</h3>
<pre><code class="language-yaml">apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: ports.firewalld.fedoraproject.org
spec:
  group: firewalld.fedoraproject.org
  scope: Namespaced
  names:
    plural: ports
    singular: port
    kind: PortRule
    shortNames: 
    - fp
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              name:
                type: string
              port:
                type: integer
              host:
                type: string
              isPermanent:
                type: boolean

</code></pre>
<h3 id="code-generator">code-generator</h3>
<p>需要预先下载 <code>code-generator</code> 。因为这个工具不是必需要求的。</p>
<blockquote>
<p>注意，下载完成后需要将代码库的的分支更改为你目前使用的版本，版本的选择与client-go类似，如果使用master分支，会与当前的 Kubernetes 集群不兼容。</p>
</blockquote>
<pre><code class="language-bash">git clone https://github.com/kubernetes/code-generator
cd code-generator; git checkout {version}  # eg. v0.18.0
</code></pre>
<h3 id="编写代码模板">编写代码模板</h3>
<p>要想使用 <code>code-generator</code> 生成控制器，必须准备三个文件 <code>doc.go</code> , <code>register.go</code> , <code>types.go</code> 。</p>
<ul>
<li><code>doc.go</code> 中声明了这个包全局内，要使用生成器的tag</li>
<li><code>register.go</code> 类似于kubernetes API，是将声明的类型注册到schema中</li>
<li><code>type.go</code> 是需要具体声明对象类型</li>
</ul>
<h3 id="code-generator-tag说明">code-generator Tag说明</h3>
<p>在使用 <code>code-generator</code> 时，就需要对 <code>code-generator</code> 的tag进行了解。<code>code-generator</code> 的tag是根据几个固定格式进行定义的，tag是 <code>+k8s:</code> + <code>conversion</code> 的组合，在仓库中 <code>cmd</code> 中的 <code>*-gen*</code> 文件夹就代表了 <em>conversion</em> 的替换位置。</p>
<ul>
<li>对于 <code>client-gen</code>的tag 参数可以在 <a href="https://github.com/kubernetes/code-generator/blob/master/cmd/client-gen/generators/util/tags.go" target="_blank"
   rel="noopener nofollow noreferrer" >code-generator\cmd\client-gen\generators\util\tags.go</a></li>
<li>对于其他类型的使用方法，例如 <a href="https://github.com/kubernetes/code-generator/blob/master/cmd/deepcopy-gen/main.go" target="_blank"
   rel="noopener nofollow noreferrer" >deepcopy-gen</a> ,可以在包 main.go中看注释说明
<ul>
<li>+k8s:openapi-gen=true：启用一个生成器</li>
</ul>
</li>
</ul>
<blockquote>
<p>注：最终准备完成的文件（ <code>doc.go</code> , <code>register.go</code> , <code>types.go</code>）应该为：<code>apis/example.com/v1</code> 这种类型的</p>
<p>需要遵循的是，将这些文件放在 <code>&lt;version&gt;</code> 目录中，例如 <code>v1</code> 。这里 <code>v1</code>, <code>v1alpha1</code>, 根据自己需求定义。</p>
</blockquote>
<h2 id="开始填写文件内容">开始填写文件内容</h2>
<h4 id="typego">type.go</h4>
<pre><code class="language-go">package v1

import (
	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
)

// +genclient
// +genclient:noStatus
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
type Port struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// Standard object metadata.
	// +optional
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

	// Specification of the desired behavior of the Deployment.
	// +optional
	Spec PortSpec `json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;`
}

// +k8s:deepcopy-gen=false
type PortSpec struct {
	Name        string `json:&quot;name&quot;`
	Host        string `json:&quot;host&quot;`
	Port        int    `json:&quot;port&quot;`
	IsPermanent bool   `json:&quot;isPermanent&quot;`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
type PortList struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// +optional
	metav1.ListMeta `json:&quot;metadata,omitempty&quot;`

	Items []Port `json:&quot;items&quot;`
}

</code></pre>
<h4 id="docgo">doc.go</h4>
<pre><code class="language-go">// +k8s:deepcopy-gen=package
// +k8s:protobuf-gen=package
// +k8s:openapi-gen=true

// +groupName=firewalld.fedoraproject.org

package v1 // import &quot;k8s.io/api/firewalld/v1&quot;
</code></pre>
<h4 id="registergo">register.go</h4>
<p>这里是从 <a href="https://github.com/kubernetes/kubernetes/tree/master/staging/src/k8s.io/api" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/api</a> 里任意一个复制的，例如 <a href="https://github.com/kubernetes/kubernetes/blob/609db7ed0b1f2839e414c17d29fe4d76edc994bd/staging/src/k8s.io/api/core/v1/register.go#L1" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/api/core/v1/register.go</a></p>
<pre><code class="language-go">package v1

import (
	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	&quot;k8s.io/apimachinery/pkg/runtime&quot;
	&quot;k8s.io/apimachinery/pkg/runtime/schema&quot;
)

// GroupName is the group name use in this package
const GroupName = &quot;firewalld.fedoraproject.org&quot;

// SchemeGroupVersion is group version used to register these objects
var SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: &quot;v1&quot;}

// Resource takes an unqualified resource and returns a Group qualified GroupResource
func Resource(resource string) schema.GroupResource {
	return SchemeGroupVersion.WithResource(resource).GroupResource()
}

var (
	// TODO: move SchemeBuilder with zz_generated.deepcopy.go to k8s.io/api.
	// localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes.
	SchemeBuilder      = runtime.NewSchemeBuilder(addKnownTypes)
	localSchemeBuilder = &amp;SchemeBuilder
	AddToScheme        = localSchemeBuilder.AddToScheme
)

// Adds the list of known types to the given scheme.
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&amp;Port{},
		&amp;PortList{},
	)
	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
	return nil
}
</code></pre>
<h4 id="生成所需文件">生成所需文件</h4>
<p>使用 <code>code-generator</code> 时，实际上就是使用这个库中的脚本 <a href="https://github.com/kubernetes/code-generator/blob/master/generate-groups.sh" target="_blank"
   rel="noopener nofollow noreferrer" >generate-groups.sh</a> ，该脚本又四个参数</p>
<ul>
<li>第一个参数：使用那些生成器，就是 <em>*.gen</em>，用逗号分割，all表示使用全部</li>
<li>第二个参数：client（client-go中informer, lister等）生成的文件存放到哪里</li>
<li>第三个参数：api（api结构，<code>k8s.io/api/</code>） 生成的文件存放到哪里，可以和定义的文件为一个目录</li>
<li>第四个参数：定义group:version</li>
<li>-output-base：输出包存放的根目录</li>
<li>-go-header-file：生成文件的头注释信息，这个是必要参数，除非生成失败</li>
</ul>
<blockquote>
<p>注：对于参数二，三，与-output-base，指定的路径，这里可以使用相对路径也可以使用go.mod中的定义的包名，对于使用相对路径而言，生成的文件中的import也将会为 &ldquo;../../&rdquo; 的格式</p>
</blockquote>
<p>一个完整的示例</p>
<pre><code class="language-bash">../code-generator/generate-groups.sh all \
	../code-controller/client \
	../code-controller/apis  \
	firewalld:v1 \
	--output-base ../code-controller/ \
	--go-header-file ../code-generator/hack/boilerplate.go.txt
</code></pre>
<blockquote>
<p>Reference</p>
<p><a href="https://insujang.github.io/2020-02-13/programming-kubernetes-crd/" target="_blank"
   rel="noopener nofollow noreferrer" >CRD Programming</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>手写一个kubernetes controller</title>
      <link>https://www.oomkill.com/2022/06/ch12-controller/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch12-controller/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>根据Kuberneter文档对<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md" target="_blank"
   rel="noopener nofollow noreferrer" >Controller</a>的描述，Controller在kubernetes中是负责协调的组件，根据设计模式可知，controller会不断的你的对象（如Pod）从当前状态与期望状态同步的一个过程。当然Controller会监听你的实际状态与期望状态。</p>
<h2 id="writing-controllers">Writing Controllers</h2>
<pre><code class="language-go">package main

import (
	&quot;flag&quot;
	&quot;fmt&quot;
	&quot;os&quot;
	&quot;time&quot;

	v1 &quot;k8s.io/api/core/v1&quot;
	&quot;k8s.io/apimachinery/pkg/fields&quot;
	utilruntime &quot;k8s.io/apimachinery/pkg/util/runtime&quot;
	&quot;k8s.io/apimachinery/pkg/util/wait&quot;
	&quot;k8s.io/client-go/kubernetes&quot;
	&quot;k8s.io/client-go/rest&quot;
	&quot;k8s.io/client-go/tools/cache&quot;
	&quot;k8s.io/client-go/tools/clientcmd&quot;
	&quot;k8s.io/client-go/util/homedir&quot;
	&quot;k8s.io/client-go/util/workqueue&quot;
	&quot;k8s.io/klog&quot;
)

type Controller struct {
	lister     cache.Indexer
	controller cache.Controller
	queue      workqueue.RateLimitingInterface
}

func NewController(lister cache.Indexer, controller cache.Controller, queue workqueue.RateLimitingInterface) *Controller {
	return &amp;Controller{
		lister:     lister,
		controller: controller,
		queue:      queue,
	}
}

func (c *Controller) processItem() bool {
	item, quit := c.queue.Get()
	if quit {
		return false
	}
	defer c.queue.Done(item)
	fmt.Println(item)
	err := c.processWrapper(item.(string))
	if err != nil {
		c.handleError(item.(string))
	}
	return true
}

func (c *Controller) handleError(key string) {

	if c.queue.NumRequeues(key) &lt; 3 {
		c.queue.AddRateLimited(key)
		return
	}
	c.queue.Forget(key)
	klog.Infof(&quot;Drop Object %s in queue&quot;, key)
}

func (c *Controller) processWrapper(key string) error {
	item, exists, err := c.lister.GetByKey(key)
	if err != nil {
		klog.Error(err)
		return err
	}
	if !exists {
		klog.Info(fmt.Sprintf(&quot;item %v not exists in cache.\n&quot;, item))
	} else {
		fmt.Println(item.(*v1.Pod).GetName())
	}
	return err
}

func (c *Controller) Run(threadiness int, stopCh chan struct{}) {
	defer utilruntime.HandleCrash()
	defer c.queue.ShutDown()
	klog.Infof(&quot;Starting custom controller&quot;)

	go c.controller.Run(stopCh)

	if !cache.WaitForCacheSync(stopCh, c.controller.HasSynced) {
		utilruntime.HandleError(fmt.Errorf(&quot;sync failed.&quot;))
		return
	}

	for i := 0; i &lt; threadiness; i++ {
		go wait.Until(func() {
			for c.processItem() {
			}
		}, time.Second, stopCh)
	}
	&lt;-stopCh
	klog.Info(&quot;Stopping custom controller&quot;)
}

func main() {
	var (
		k8sconfig  *string //使用kubeconfig配置文件进行集群权限认证
		restConfig *rest.Config
		err        error
	)
	if home := homedir.HomeDir(); home != &quot;&quot; {
		k8sconfig = flag.String(&quot;kubeconfig&quot;, fmt.Sprintf(&quot;%s/.kube/config&quot;, home), &quot;kubernetes auth config&quot;)
	}
	k8sconfig = k8sconfig
	flag.Parse()
	if _, err := os.Stat(*k8sconfig); err != nil {
		panic(err)
	}

	if restConfig, err = rest.InClusterConfig(); err != nil {
		// 这里是从masterUrl 或者 kubeconfig传入集群的信息，两者选一
		restConfig, err = clientcmd.BuildConfigFromFlags(&quot;&quot;, *k8sconfig)
		if err != nil {
			panic(err)
		}
	}
	restset, err := kubernetes.NewForConfig(restConfig)
	lister := cache.NewListWatchFromClient(restset.CoreV1().RESTClient(), &quot;pods&quot;, &quot;default&quot;, fields.Everything())
	queue := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())
	indexer, controller := cache.NewIndexerInformer(lister, &amp;v1.Pod{}, 0, cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			fmt.Println(&quot;add &quot;, obj.(*v1.Pod).GetName())
			key, err := cache.MetaNamespaceKeyFunc(obj)
			if err == nil {
				queue.Add(key)
			}

		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			fmt.Println(&quot;update&quot;, newObj.(*v1.Pod).GetName())
			if newObj.(*v1.Pod).Status.Conditions[0].Status == &quot;True&quot; {
				fmt.Println(&quot;update: the Initialized Status&quot;, newObj.(*v1.Pod).Status.Conditions[0].Status)
			} else {
				fmt.Println(&quot;update: the Initialized Status &quot;, newObj.(*v1.Pod).Status.Conditions[0].Status)
				fmt.Println(&quot;update: the Initialized Reason &quot;, newObj.(*v1.Pod).Status.Conditions[0].Reason)
			}

			if len(newObj.(*v1.Pod).Status.Conditions) &gt; 1 {
				if newObj.(*v1.Pod).Status.Conditions[1].Status == &quot;True&quot; {
					fmt.Println(&quot;update: the Ready Status&quot;, newObj.(*v1.Pod).Status.Conditions[1].Status)
				} else {
					fmt.Println(&quot;update: the Ready Status &quot;, newObj.(*v1.Pod).Status.Conditions[1].Status)
					fmt.Println(&quot;update: the Ready Reason &quot;, newObj.(*v1.Pod).Status.Conditions[1].Reason)
				}

				if newObj.(*v1.Pod).Status.Conditions[2].Status == &quot;True&quot; {
					fmt.Println(&quot;update: the PodCondition Status&quot;, newObj.(*v1.Pod).Status.Conditions[2].Status)
				} else {
					fmt.Println(&quot;update: the PodCondition Status &quot;, newObj.(*v1.Pod).Status.Conditions[2].Status)
					fmt.Println(&quot;update: the PodCondition Reason &quot;, newObj.(*v1.Pod).Status.Conditions[2].Reason)
				}

				if newObj.(*v1.Pod).Status.Conditions[3].Status == &quot;True&quot; {
					fmt.Println(&quot;update: the PodScheduled Status&quot;, newObj.(*v1.Pod).Status.Conditions[3].Status)
				} else {
					fmt.Println(&quot;update: the PodScheduled Status &quot;, newObj.(*v1.Pod).Status.Conditions[3].Status)
					fmt.Println(&quot;update: the PodScheduled Reason &quot;, newObj.(*v1.Pod).Status.Conditions[3].Reason)
				}
			}

		},
		DeleteFunc: func(obj interface{}) {
			fmt.Println(&quot;delete &quot;, obj.(*v1.Pod).GetName(), &quot;Status &quot;, obj.(*v1.Pod).Status.Phase)
			// 上面是事件函数的处理，下面是对workqueue的操作
			key, err := cache.MetaNamespaceKeyFunc(obj)
			if err == nil {
				queue.Add(key)
			}
		},
	}, cache.Indexers{})

	c := NewController(indexer, controller, queue)
	stopCh := make(chan struct{})
	stopCh1 := make(chan struct{})
	c.Run(1, stopCh)
	defer close(stopCh)
	&lt;-stopCh1
}

</code></pre>
<p>通过日志可以看出，Pod create后的步骤大概为4步：</p>
<ul>
<li>Initialized：初始化好后状态为Pending</li>
<li>PodScheduled：然后调度</li>
<li>PodCondition</li>
<li>Ready</li>
</ul>
<pre><code>add  netbox
default/netbox
netbox
update netbox status Pending to Pending
update: the Initialized Status True
update netbox status Pending to Pending
update: the Initialized Status True
update: the Ready Status  False
update: the Ready Reason  ContainersNotReady
update: the PodCondition Status  False
update: the PodCondition Reason  ContainersNotReady
update: the PodScheduled Status True


update netbox status Pending to Running
update: the Initialized Status True
update: the Ready Status True
update: the PodCondition Status True
update: the PodScheduled Status True
</code></pre>
<p>大致上与 <code>kubectl describe pod</code> 看到的内容页相似</p>
<pre><code>default-scheduler  Successfully assigned default/netbox to master-machine
  Normal  Pulling    85s   kubelet            Pulling image &quot;cylonchau/netbox&quot;
  Normal  Pulled     30s   kubelet            Successfully pulled image &quot;cylonchau/netbox&quot;
  Normal  Created    30s   kubelet            Created container netbox
  Normal  Started    30s   kubelet            Started container netbox
</code></pre>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/controllers.md" target="_blank"
   rel="noopener nofollow noreferrer" >controllers.md</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用CRD扩展Kubernetes API</title>
      <link>https://www.oomkill.com/2022/06/ch13-crd/</link>
      <pubDate>Sun, 19 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch13-crd/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>Kubernetes的主节点或控制面板当中主要有三个组件，其中apiserver是整个系统的数据库，借助于Cluster Store（etcd）服务，来实现所有的包括用户所期望状态的定义，以及集群上资源当前状态的实时记录等。</p>
<p>etcd是分布式通用的K/V系统 <code>KV Store</code> ，可存储用户所定义的任何由KV Store所支持的可持久化的数据。它不仅仅被apiserver所使用，如flannel、calico二者也需要以etcd来保存当前应用程序对应的存储数据。 任何一个分布式应用程序几乎都会用到一个高可用的存储系统。</p>
<p>apiserver将etcd所提供的存储接口做了高度抽象，使用户通过apiserver来完成数据存取时，只能使用apiserver中所内建支持的数据范式。在某种情况之下，我们所期望管理的资源或存储对象在现有的Kubernetes资源无法满足需求时。</p>
<p>Operator本身是建构在StatefulSet以及本身的基本Kubernetes资源之上，由开发者自定义的更高级的、更抽象的自定义资源类型。他可借助于底层的Pod、Service功能，再次抽象出新资源类型。更重要的是，整个集群本身可抽象成一个单一资源。</p>
<p>为了实现更高级的资源管理，需要利用已有的基础资源类型，做一个更高级的抽象，来定义成更能符合用户所需要的、可单一管理的资源类型，而无需去分别管理每一个资源。</p>
<p>在Kubernetes之上自定义资源一般被称为扩展Kubernetes所支持的资源类型，</p>
<ul>
<li>自定义资源类型 CRD Custom Resource Definition</li>
<li>自定义apiserver</li>
<li>修改APIServer源代码，改动内部的资源类型定义</li>
</ul>
<p>CRD是kubernetes内建的资源类型，从而使得用户可以定义的不是具体的资源，而是资源类型，也是扩展Kubernetes最简单的方式。</p>
<h2 id="intorduction-crd">Intorduction CRD</h2>
<h3 id="什么是crd">什么是CRD</h3>
<p>在 Kubernetes API 中，resources 是存储 API 对象集合的endpoint。例如，内置 Pod resource 包含 Pod 对象的集合。当我们想扩展API，原生的Kubernetes就不能满足我们的需求了，这时 <strong>CRD</strong>  (<code>CustomResourceDefinition</code>) 就出现了。在 Kubernetes 中创建了 CRD 后，就可以像使用任何其他原生 Kubernetes 对象一样使用它，从而利用 Kubernetes 的所有功能、如安全性、API 服务、RBAC 等。</p>
<p>Kubernetes 1.7 之后增加了对 CRD 自定义资源二次开发能力来扩展 Kubernetes API，通过 CRD 我们可以向 Kubernetes API 中增加新资源类型，而不需要修改 Kubernetes 源码来创建自定义的 API server，该功能大大提高了 Kubernetes 的扩展能力。</p>
<h2 id="创建-crd">创建 CRD</h2>
<p><strong>前提条件</strong>： Kubernetes 服务器版本必须不低于版本 1.16</p>
<p>再创建新的 <code>CustomResourceDefinition</code>（CRD）时，Kubernetes API 服务器会为指定的每一个版本生成一个 <code>RESTful</code> 的资源路径。（即定义一个Restful API）。CRD 可以是<code>namespace</code>作用域的，也可以是<code>cluster</code>作用域的，取决于 CRD 的 <code>scope</code> 字段设置。和其他现有的内置对象一样，删除一个<code>namespace</code>时，该<code>namespace</code>下的所有定制对象也会被删除。CustomResourceDefinition 本身是不受名字空间限制的，对所有名字空间可用。</p>
<p>例如，编写一个firewall port 规则：</p>
<pre><code class="language-yaml"># 1.16版本后固定格式
apiVersion: apiextensions.k8s.io/v1
# 类型crd
kind: CustomResourceDefinition
metadata:
  # 必须为name=spec.names.plural + spec.group
  name: ports.firewalld.fedoraproject.org
spec:
  # api中的group
  # /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;
  group: firewalld.fedoraproject.org
  # 此crd作用于 可选Namespaced|Cluster
  scope: Namespaced
  names:
    # 名字的复数形式，用于api
    plural: ports
    # 名称的单数形式。用于命令行
    singular: port
    # 种类，资源清单类型
    kind: PortRule
    # 名字简写，类似允许 CLI 上较短的字符串匹配的资源
    shortNames: 
    - fp
  versions:
  # 定义版本的类型
  - name: v1
  # 通过 served 标志来启用或禁止
    served: true
  # 其中一个且只有一个版本必需被标记为存储版本
    storage: true
  # 自定义资源的默认认证的模式
    schema:
  # 使用的版本
      openAPIV3Schema:
        # 定义一个参数为对象类型
        type: object
        # 这个参数的类型
        properties:
        # 参数属性spec
          spec:
            # spec属性的类型为对象
            type: object
            # 对象属性
            properties:
              # spec属性name
              name:
              # 类型为string
                type: string
              port:
                type: integer
              isPermanent:
                type: boolean
</code></pre>
<p>需要注意的是v1.16版本以后已经 <code>GA</code>了，使用的是<code>v1</code>版本，之前都是<code>vlbeta1</code>，定义规范有部分变化，所以要注意版本变化。</p>
<p>这个地方的定义和我们定义普通的资源对象比较类似，我们说我们可以随意定义一个自定义的资源对象，但是在创建资源的时候，肯定不是任由我们随意去编写<code>YAML</code>文件的，当我们把上面的<code>CRD</code>文件提交给Kubernetes之后，Kubernetes会对我们提交的声明文件进行校验，从定义可以看出CRD是基于 <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.1.0.md" target="_blank"
   rel="noopener nofollow noreferrer" >OpenAPIv3 schem</a> 进行规范的。当然这种校验只是<strong>对于字段的类型进行校验</strong>，比较初级，如果想要更加复杂的校验，这个时候就需要通过Kubernetes的admission webhook来实现了。关于校验的更多用法，可以前往官方文档查看。</p>
<p>创建一个crd类型资源</p>
<pre><code class="language-yaml">apiVersion: &quot;firewalld.fedoraproject.org/v1&quot;
kind: PortRule
metadata:
  name: http-port
spec:
  name: &quot;nginx&quot;
  port: 80
  isPermanent: false
</code></pre>
<p>查看创建的crd</p>
<pre><code>$ kubectl get t
NAME                                CREATED AT
firewallds.port.fedoraproject.org   2022-06-19T09:27:09Z
</code></pre>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/" target="_blank"
   rel="noopener nofollow noreferrer" >CRD</a></p>
<p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#customresourcedefinition-v1-apiextensions-k8s-io" target="_blank"
   rel="noopener nofollow noreferrer" >CRD Definition</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析client-go架构 - queue</title>
      <link>https://www.oomkill.com/2022/06/ch09-queue/</link>
      <pubDate>Fri, 17 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/ch09-queue/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="通用队列">通用队列</h2>
<p>在kubernetes中，使用go的channel无法满足kubernetes的应用场景，如延迟、限速等；在kubernetes中存在三种队列通用队列 <code>common queue</code> ，延迟队列 <code>delaying queue</code>，和限速队列 <code>rate limiters queue</code></p>
<h3 id="inferface">Inferface</h3>
<p>Interface作为所有队列的一个抽象定义</p>
<pre><code class="language-go">type Interface interface {
	Add(item interface{})
	Len() int
	Get() (item interface{}, shutdown bool)
	Done(item interface{})
	ShutDown()
	ShuttingDown() bool
}
</code></pre>
<h3 id="implementation">Implementation</h3>
<pre><code class="language-go">type Type struct { // 一个work queue
	queue []t // queue用slice做存储
	dirty set // 脏位，定义了需要处理的元素，类似于操作系统，表示已修改但为写入
	processing set // 当前正在处理的元素集合
	cond *sync.Cond
	shuttingDown bool
	metrics queueMetrics
	unfinishedWorkUpdatePeriod time.Duration
	clock                      clock.Clock
}
type empty struct{}
type t interface{} // t queue中的元素
type set map[t]empty // dirty 和 processing中的元素
</code></pre>
<p>可以看到其中核心属性就是 <code>queue</code> , <code>dirty</code> , <code>processing</code></p>
<h2 id="延迟队列">延迟队列</h2>
<p>在研究优先级队列前，需要对 <code> Heap</code> 有一定的了解，因为delay queue使用了 <code>heap</code> 做延迟队列</p>
<h3 id="heap">Heap</h3>
<p><code>Heap</code> 是基于树属性的特殊数据结构；heap是一种完全二叉树类型，具有两种类型：</p>
<ul>
<li>如：B 是 A 的子节点，则  $key(A) \geq key(B)$ 。这就意味着具有最大Key的元素始终位于根节点，这类Heap称为最大堆 <strong>MaxHeap</strong>。</li>
<li>父节点的值小于或等于其左右子节点的值叫做 <strong>MinHeap</strong></li>
</ul>
<p>二叉堆的存储规则：</p>
<ul>
<li>每个节点包含的元素大于或等于该节点子节点的元素。</li>
<li>树是完全二叉树。</li>
</ul>
<p>那么下列图片中，那个是堆</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/f.10.1.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>heap的实现</p>
<h4 id="实例向左边添加一个值为42的元素的过程">实例：向左边添加一个值为42的元素的过程</h4>
<p><strong>步骤一</strong>：将新元素放入堆中的第一个可用位置。这将使结构保持为完整的二叉树，但它可能不再是堆，因为新元素可能具有比其父元素更大的值。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/reheap1.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><strong>步骤二</strong>：如果新元素的值大于父元素，将新元素与父元素交换，直到达到新元素到根，或者新元素大于等于其父元素的值时将停止</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/reheap2.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>这种过程被称为 <strong>向上调整</strong> （<code>reheapification upward</code>）</p>
<h4 id="实例移除根">实例：移除根</h4>
<p><strong>步骤一</strong>：将根元素复制到用于返回值的变量中，将最深层的最后一个元素复制到根，然后将最后一个节点从树中取出。该元素称为 <code>out-of-place</code> 。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/reheap3.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><strong>步骤二</strong>：而将异位元素与其最大值的子元素交换，并返回在步骤1中保存的值。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/reheap4.jpeg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>这个过程被称为<strong>向下调整</strong> （<code>reheapification downward</code>）</p>
<h3 id="优先级队列">优先级队列</h3>
<p>优先级队列的行为：</p>
<ul>
<li>元素被放置在队列中，然后被取出。</li>
<li>优先级队列中的每个元素都有一个关联的数字，称为优先级。</li>
<li>当元素离开优先级队列时，最高优先级的元素最先离开。</li>
</ul>
<p>如何实现的：</p>
<ul>
<li>
<p>在优先级队列中，heap的每个节点都包含一个元素以及元素的优先级，并且维护树以便它遵循使用元素的优先级来比较节点的堆存储规则：</p>
<ul>
<li>每个节点包含的元素的优先级大于或等于该节点子元素的优先级。</li>
<li>树是完全二叉树。</li>
</ul>
</li>
<li>
<p>实现的代码：<a href="https://pkg.go.dev/container/heap#example__priorityQueue" target="_blank"
   rel="noopener nofollow noreferrer" >golang priorityQueue</a></p>
</li>
</ul>
<blockquote>
<p>Reference</p>
<p><a href="https://www.cpp.edu/~ftang/courses/CS241/notes/heap.htm" target="_blank"
   rel="noopener nofollow noreferrer" >heap</a></p>
</blockquote>
<h3 id="client-go-的延迟队列">Client-go 的延迟队列</h3>
<p>在Kubernetes中对 <code>delaying queue</code> 的设计非常精美，通过使用 <code>heap</code> 实现的延迟队列，加上kubernetes中的通过队列，完成了延迟队列的功能。</p>
<pre><code class="language-go">// 注释中给了一个hot-loop热循环，通过这个loop实现了delaying
type DelayingInterface interface {
	Interface // 继承了workqueue的功能
	AddAfter(item interface{}, duration time.Duration) // 在time后将内容添加到工作队列中
}
</code></pre>
<p>具体实现了 <code>DelayingInterface</code> 的实例</p>
<pre><code class="language-go">type delayingType struct {
	Interface // 通用的queue 
	clock clock.Clock // 对比的时间 ，包含一些定时器的功能
    	type Clock interface {
            PassiveClock
            		type PassiveClock interface {
                        Now() time.Time
                        Since(time.Time) time.Duration
                    }
            After(time.Duration) &lt;-chan time.Time
            NewTimer(time.Duration) Timer
            Sleep(time.Duration)
            NewTicker(time.Duration) Ticker
        }
	stopCh chan struct{} // 停止loop
	stopOnce sync.Once // 保证退出只会触发一次
	heartbeat clock.Ticker // 一个定时器，保证了loop的最大空事件等待时间
	waitingForAddCh chan *waitFor // 普通的chan，用来接收数据插入到延迟队列中
	metrics retryMetrics // 重试的指数
}
</code></pre>
<p>那么延迟队列的整个数据结构如下图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220527215559879.png" alt="image-20220527215559879" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>而上面部分也说到了，这个延迟队列的核心就是一个优先级队列，而优先级队列又需要满足：</p>
<ul>
<li>优先级队列中的每个元素都有一个关联的数字，称为优先级。</li>
<li>当元素离开优先级队列时，最高优先级的元素最先离开。</li>
</ul>
<p>而 <code>waitFor</code> 就是这个优先级队列的数据结构</p>
<pre><code class="language-go">type waitFor struct {
	data    t // 数据
	readyAt time.Time // 加入工作队列的时间
	index int // 优先级队列中的索引
}
</code></pre>
<p>而 <code>waitForPriorityQueue</code> 是对 <code>container/heap/heap.go.Inferface</code> 的实现，其数据结构就是使最小 <code>readyAt</code> 位于Root 的一个 <code>MinHeap</code></p>
<pre><code class="language-go">type Interface interface {
	sort.Interface
	Push(x interface{}) // add x as element Len()
	Pop() interface{}   // remove and return element Len() - 1.
}
</code></pre>
<p>而这个的实现是 <code>waitForPriorityQueue</code></p>
<pre><code class="language-go">type waitForPriorityQueue []*waitFor

func (pq waitForPriorityQueue) Len() int {
	return len(pq)
}
// 这个也是最重要的一个，就是哪个属性是排序的关键，也是heap.down和heap.up中使用的
func (pq waitForPriorityQueue) Less(i, j int) bool {
	return pq[i].readyAt.Before(pq[j].readyAt)
}
func (pq waitForPriorityQueue) Swap(i, j int) {
	pq[i], pq[j] = pq[j], pq[i]
	pq[i].index = i
	pq[j].index = j
}
// push 和pop 必须使用heap.push 和heap.pop
func (pq *waitForPriorityQueue) Push(x interface{}) {
	n := len(*pq)
	item := x.(*waitFor)
	item.index = n
	*pq = append(*pq, item)
}


func (pq *waitForPriorityQueue) Pop() interface{} {
	n := len(*pq)
	item := (*pq)[n-1]
	item.index = -1
	*pq = (*pq)[0:(n - 1)]
	return item
}

// Peek returns the item at the beginning of the queue, without removing the
// item or otherwise mutating the queue. It is safe to call directly.
func (pq waitForPriorityQueue) Peek() interface{} {
	return pq[0]
}
</code></pre>
<p>而整个延迟队列的核心就是 <code>waitingLoop</code>，作为了延迟队列的主要逻辑，检查 <code>waitingForAddCh</code> 有没有要延迟的内容，取出延迟的内容放置到 <code>Heap</code> 中；以及保证最大的阻塞周期</p>
<pre><code class="language-go">func (q *delayingType) waitingLoop() {
	defer utilruntime.HandleCrash()
	never := make(&lt;-chan time.Time) // 作为占位符
	var nextReadyAtTimer clock.Timer // 最近一个任务要执行的定时器
	waitingForQueue := &amp;waitForPriorityQueue{} // 优先级队列，heap
	heap.Init(waitingForQueue)
	waitingEntryByData := map[t]*waitFor{} // 检查是否反复添加

	for {
		if q.Interface.ShuttingDown() {
			return
		}

		now := q.clock.Now()
		for waitingForQueue.Len() &gt; 0 {
			entry := waitingForQueue.Peek().(*waitFor)
			if entry.readyAt.After(now) {
				break // 时间没到则不处理
			}

			entry = heap.Pop(waitingForQueue).(*waitFor) // 从优先级队列中取出一个
			q.Add(entry.data) // 添加到延迟队列中
			delete(waitingEntryByData, entry.data) // 删除map表中的数据
		}

		// 如果存在数据则设置最近一个内容要执行的定时器
		nextReadyAt := never
		if waitingForQueue.Len() &gt; 0 {
			if nextReadyAtTimer != nil {
				nextReadyAtTimer.Stop()
			}
			entry := waitingForQueue.Peek().(*waitFor) // 窥视[0]和值
			nextReadyAtTimer = q.clock.NewTimer(entry.readyAt.Sub(now)) // 创建一个定时器
			nextReadyAt = nextReadyAtTimer.C()
		}

		select {
		case &lt;-q.stopCh: // 退出
			return
		case &lt;-q.heartbeat.C(): // 多久没有任何动作时重新一次循环
		case &lt;-nextReadyAt: // 如果有元素时间到了，则继续执行循环，处理上面添加的操作
		case waitEntry := &lt;-q.waitingForAddCh:
			if waitEntry.readyAt.After(q.clock.Now()) { // 时间没到，是用readyAt和now对比time.Now
				// 添加到延迟队列中，有两个 waitingEntryByData waitingForQueue
				insert(waitingForQueue, waitingEntryByData, waitEntry)
			} else {
				q.Add(waitEntry.data)
			}

			drained := false // 保证可以取完q.waitingForAddCh // addafter
			for !drained {
				select {
                // 这里是一个有buffer的队列，需要保障这个队列读完
				case waitEntry := &lt;-q.waitingForAddCh: 
					if waitEntry.readyAt.After(q.clock.Now()) {
						insert(waitingForQueue, waitingEntryByData, waitEntry)
					} else {
						q.Add(waitEntry.data)
					}
				default: // 保证可以退出，但限制于上一个分支的0~n的读取
				// 如果上一个分支阻塞，则为没有数据就是取尽了，走到这个分支
				// 如果上个分支不阻塞则读取到上个分支阻塞为止，代表阻塞，则走default退出
					drained = true
				}
			}
		}
	}
}
</code></pre>
<h2 id="限速队列">限速队列</h2>
<p>限速队列 <code>RateLimiting</code> 是在优先级队列是在延迟队列的基础上进行扩展的一个队列</p>
<pre><code class="language-go">type RateLimitingInterface interface {
	DelayingInterface // 继承延迟队列
	// 在限速器准备完成后（即合规后）添加条目到队列中
	AddRateLimited(item interface{})
	// drop掉条目，无论成功或失败
	Forget(item interface{})
	// 被重新放入队列中的次数
	NumRequeues(item interface{}) int
}
</code></pre>
<p>可以看到一个限速队列的抽象对应只要满足了 <code>AddRateLimited()</code> , <code>Forget()</code> , <code>NumRequeues()</code> 的延迟队列都是限速队列。看了解规则之后，需要对具体的实现进行分析。</p>
<pre><code class="language-go">type rateLimitingType struct {
	DelayingInterface
	rateLimiter RateLimiter
}

func (q *rateLimitingType) AddRateLimited(item interface{}) {
	q.DelayingInterface.AddAfter(item, q.rateLimiter.When(item))
}

func (q *rateLimitingType) NumRequeues(item interface{}) int {
	return q.rateLimiter.NumRequeues(item)
}

func (q *rateLimitingType) Forget(item interface{}) {
	q.rateLimiter.Forget(item)
}
</code></pre>
<p><code>rateLimitingType</code> 则是对抽象规范 <code>RateLimitingInterface</code> 的实现，可以看出是在延迟队列的基础上增加了一个限速器 <code>RateLimiter</code></p>
<pre><code class="language-go">type RateLimiter interface {
	// when决定等待多长时间
	When(item interface{}) time.Duration
	// drop掉item
	// or for success, we'll stop tracking it
	Forget(item interface{})
	// 重新加入队列中的次数
	NumRequeues(item interface{}) int
}
</code></pre>
<p>抽象限速器的实现，有 <code>BucketRateLimiter</code> , <code>ItemBucketRateLimiter</code> , <code>ItemExponentialFailureRateLimiter</code> , <code>ItemFastSlowRateLimiter</code> ,  <code>MaxOfRateLimiter</code> ，下面对这些限速器进行分析</p>
<h3 id="bucketratelimiter">BucketRateLimiter</h3>
<p><code>BucketRateLimiter</code> 是实现 <code>rate.Limiter</code> 与 抽象 <code>RateLimiter</code> 的一个令牌桶，初始化时通过 <code>workqueue.DefaultControllerRateLimiter()</code> 进行初始化。</p>
<pre><code class="language-go">func DefaultControllerRateLimiter() RateLimiter {
	return NewMaxOfRateLimiter(
		NewItemExponentialFailureRateLimiter(5*time.Millisecond, 1000*time.Second),
		// 10 qps, 100 bucket size.  This is only for retry speed and its only the overall factor (not per item)
		&amp;BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(10), 100)},
	)
}
</code></pre>
<p><a href="https://www.cnblogs.com/Cylon/p/16379709.html" target="_blank"
   rel="noopener nofollow noreferrer" >更多关于令牌桶算法可以参考这里</a></p>
<h3 id="itembucketratelimiter">ItemBucketRateLimiter</h3>
<p><code>ItemBucketRateLimiter</code> 是作为列表存储每个令牌桶的实现，每个key都是单独的限速器</p>
<pre><code class="language-go">type ItemBucketRateLimiter struct {
	r     rate.Limit
	burst int

	limitersLock sync.Mutex
	limiters     map[interface{}]*rate.Limiter
}

func NewItemBucketRateLimiter(r rate.Limit, burst int) *ItemBucketRateLimiter {
	return &amp;ItemBucketRateLimiter{
		r:        r,
		burst:    burst,
		limiters: make(map[interface{}]*rate.Limiter),
	}
}
</code></pre>
<h3 id="itemexponentialfailureratelimiter">ItemExponentialFailureRateLimiter</h3>
<p>如名所知 <code>ItemExponentialFailureRateLimiter</code> 限速器是一个错误指数限速器，根据错误的次数，将指数用于delay的时长，指数的计算公式为：$baseDelay\times2^{<num-failures>}$。 可以看出When绝定了流量整形的delay时间，根据错误次数为指数进行延长重试时间</p>
<pre><code class="language-go">type ItemExponentialFailureRateLimiter struct {
	failuresLock sync.Mutex
	failures     map[interface{}]int // 失败的次数

	baseDelay time.Duration // 延迟基数
	maxDelay  time.Duration // 最大延迟
}

func (r *ItemExponentialFailureRateLimiter) When(item interface{}) time.Duration {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	exp := r.failures[item]
	r.failures[item] = r.failures[item] + 1

	// The backoff is capped such that 'calculated' value never overflows.
	backoff := float64(r.baseDelay.Nanoseconds()) * math.Pow(2, float64(exp))
	if backoff &gt; math.MaxInt64 {
		return r.maxDelay
	}

	calculated := time.Duration(backoff)
	if calculated &gt; r.maxDelay {
		return r.maxDelay
	}

	return calculated
}

func (r *ItemExponentialFailureRateLimiter) NumRequeues(item interface{}) int {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	return r.failures[item]
}

func (r *ItemExponentialFailureRateLimiter) Forget(item interface{}) {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	delete(r.failures, item)
}
</code></pre>
<h3 id="itemfastslowratelimiter">ItemFastSlowRateLimiter</h3>
<p><code>ItemFastSlowRateLimiter </code> ，限速器先快速重试一定次数，然后慢速重试</p>
<pre><code class="language-go">type ItemFastSlowRateLimiter struct {
	failuresLock sync.Mutex
	failures     map[interface{}]int

	maxFastAttempts int // 最大尝试次数
	fastDelay       time.Duration // 快的速度
	slowDelay       time.Duration // 慢的速度
}


func NewItemFastSlowRateLimiter(fastDelay, slowDelay time.Duration, maxFastAttempts int) RateLimiter {
	return &amp;ItemFastSlowRateLimiter{
		failures:        map[interface{}]int{},
		fastDelay:       fastDelay,
		slowDelay:       slowDelay,
		maxFastAttempts: maxFastAttempts,
	}
}

func (r *ItemFastSlowRateLimiter) When(item interface{}) time.Duration {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	r.failures[item] = r.failures[item] + 1
	// 当错误次数没超过快速的阈值使用快速，否则使用慢速
	if r.failures[item] &lt;= r.maxFastAttempts {
		return r.fastDelay
	}

	return r.slowDelay
}

func (r *ItemFastSlowRateLimiter) NumRequeues(item interface{}) int {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	return r.failures[item]
}

func (r *ItemFastSlowRateLimiter) Forget(item interface{}) {
	r.failuresLock.Lock()
	defer r.failuresLock.Unlock()

	delete(r.failures, item)
}
</code></pre>
<h3 id="maxofratelimiter">MaxOfRateLimiter</h3>
<p><code>MaxOfRateLimiter</code> 是返回限速器列表中，延迟最大的那个限速器</p>
<pre><code class="language-go">type MaxOfRateLimiter struct {
	limiters []RateLimiter
}

func (r *MaxOfRateLimiter) When(item interface{}) time.Duration {
	ret := time.Duration(0)
	for _, limiter := range r.limiters {
		curr := limiter.When(item)
		if curr &gt; ret {
			ret = curr
		}
	}

	return ret
}

func NewMaxOfRateLimiter(limiters ...RateLimiter) RateLimiter {
	return &amp;MaxOfRateLimiter{limiters: limiters}
}

func (r *MaxOfRateLimiter) NumRequeues(item interface{}) int {
	ret := 0
    // 找到列表內所有的NumRequeues（失败的次数），以最多次的为主。 
	for _, limiter := range r.limiters {
		curr := limiter.NumRequeues(item)
		if curr &gt; ret {
			ret = curr
		}
	}

	return ret
}

func (r *MaxOfRateLimiter) Forget(item interface{}) {
	for _, limiter := range r.limiters {
		limiter.Forget(item)
	}
}
</code></pre>
<h3 id="如何使用kubernetes的限速器">如何使用Kubernetes的限速器</h3>
<p>基于流量管制的限速队列实例，可以大量突发，但是需要进行整形，添加操作会根据 <code>When()</code> 中设计的需要等待的时间进行添加。根据不同的队列实现不同方式的延迟</p>
<pre><code class="language-go">package main

import (
	&quot;fmt&quot;
	&quot;log&quot;
	&quot;strconv&quot;
	&quot;time&quot;

	&quot;k8s.io/client-go/util/workqueue&quot;
)

func main() {
	stopCh := make(chan string)
	timeLayout := &quot;2006-01-02:15:04:05.0000&quot;
	limiter := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())
	length := 20 // 一共请求20次
	chs := make([]chan string, length)
	for i := 0; i &lt; length; i++ {
		chs[i] = make(chan string, 1)
		go func(taskId string, ch chan string) {
			item := &quot;Task-&quot; + taskId + time.Now().Format(timeLayout)
			log.Println(item + &quot; Added.&quot;)
            limiter.AddRateLimited(item) // 添加会根据When() 延迟添加到工作队列中

		}(strconv.FormatInt(int64(i), 10), chs[i])

		go func() {
			for {
				key, quit := limiter.Get()
				if quit {
					return
				}
				log.Println(fmt.Sprintf(&quot;%s process done&quot;, key))
				defer limiter.Done(key)

			}
		}()
	}
	&lt;-stopCh
}
</code></pre>
<p>因为默认的限速器不支持初始化 QPS，修改源码内的为 $BT(1, 5)$ ，执行结果可以看出，大突发流量时，超过桶内token数时，会根据token生成的速度进行放行。</p>
<p>图中，任务的添加是突发性的，日志打印的是同时添加，但是在添加前输出的日志，消费端可以看到实际是被延迟了。配置的是每秒一个token，实际上放行流量也是每秒一个token。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220617173106990.png" alt="image-20220617173106990" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>源码分析client-go架构 - 什么是informer</title>
      <link>https://www.oomkill.com/2022/05/ch08-informer/</link>
      <pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/05/ch08-informer/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>之前了解了client-go中的架构设计，也就是 <code>tools/cache</code> 下面的一些概念，那么下面将对informer进行分析</p>
<h2 id="controller">Controller</h2>
<p>在client-go informer架构中存在一个 <code>controller</code> ，这个不是 Kubernetes 中的Controller组件；而是在 <code>tools/cache</code> 中的一个概念，<code>controller</code> 位于 informer 之下，Reflector 之上。<a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/controller.go#L90-L115" target="_blank"
   rel="noopener nofollow noreferrer" >code</a></p>
<h3 id="config">Config</h3>
<p>从严格意义上来讲，<code>controller</code> 是作为一个  <code>sharedInformer</code> 使用，通过接受一个 <code>Config</code> ，而 <code>Reflector</code> 则作为 <code>controller</code> 的 slot。<code>Config</code> 则包含了这个 <code>controller</code> 里所有的设置。</p>
<pre><code class="language-go">type Config struct {
	Queue // DeltaFIFO
	ListerWatcher // 用于list watch的
	Process ProcessFunc // 定义如何从DeltaFIFO中弹出数据后处理的操作
	ObjectType runtime.Object // Controller处理的对象数据，实际上就是kubernetes中的资源
	FullResyncPeriod time.Duration // 全量同步的周期
	ShouldResync ShouldResyncFunc // Reflector通过该标记来确定是否应该重新同步
	RetryOnError bool
}
</code></pre>
<h3 id="controller-1">controller</h3>
<p>然后 <code>controller</code>  又为 <code>reflertor</code> 的上层</p>
<pre><code class="language-go">type controller struct {
	config         Config
	reflector      *Reflector 
	reflectorMutex sync.RWMutex
	clock          clock.Clock
}

type Controller interface {
	// controller 主要做两件事，
    // 1. 构建并运行 Reflector,将listerwacther中的泵压到queue（Delta fifo）中
    // 2. Queue用Pop()弹出数据，具体的操作是Process
    // 直到 stopCh 不阻塞，这两个协程将退出
	Run(stopCh &lt;-chan struct{})
	HasSynced() bool // 这个实际上是从store中继承的，标记这个controller已经
	LastSyncResourceVersion() string
}
</code></pre>
<p><code>controller</code> 中的方法，仅有一个 <code>Run()</code> 和 <code>New()</code>；这意味着，<code>controller</code> 只是一个抽象的概念，作为 <code>Reflector</code>,  <code>Delta FIFO</code> 整合的工作流</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220523224050974.png" alt="image-20220523224050974" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>而 <code>controller</code> 则是 <code>SharedInformer</code> 了。</p>
<h3 id="queue">Queue</h3>
<p>这里的 <code>queue</code> 可以理解为是一个具有 <code>Pop()</code> 功能的 <code>Indexer</code> ;而 <code>Pop()</code> 的功能则是 <code>controller</code> 中的一部分；也就是说 <code>queue</code> 是一个扩展的 <code>Store</code> ， <code>Store</code> 是不具备弹出功能的。</p>
<pre><code class="language-go">type Queue interface {
	Store
	// Pop会阻塞等待，直到有内容弹出，删除对应的值并处理计数器
	Pop(PopProcessFunc) (interface{}, error)

	// AddIfNotPresent puts the given accumulator into the Queue (in
	// association with the accumulator's key) if and only if that key
	// is not already associated with a non-empty accumulator.
	AddIfNotPresent(interface{}) error

	// HasSynced returns true if the first batch of keys have all been
	// popped.  The first batch of keys are those of the first Replace
	// operation if that happened before any Add, Update, or Delete;
	// otherwise the first batch is empty.
	HasSynced() bool
	Close() // 关闭queue
}
</code></pre>
<p>而弹出的操作是通过 controller 中的 <code>processLoop()</code> 进行的，最终走到Delta FIFO中进行处理。</p>
<p>通过忙等待去读取要弹出的数据，然后在弹出前 通过<code>PopProcessFunc</code> 进行处理</p>
<pre><code class="language-go">func (c *controller) processLoop() {
	for {
		obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process))
		if err != nil {
			if err == ErrFIFOClosed {
				return
			}
			if c.config.RetryOnError {
				// This is the safe way to re-enqueue.
				c.config.Queue.AddIfNotPresent(obj)
			}
		}
	}
}
</code></pre>
<p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/delta_fifo.go#L515" target="_blank"
   rel="noopener nofollow noreferrer" >DeltaFIFO.Pop()</a></p>
<pre><code class="language-go">func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) {
	f.lock.Lock()
	defer f.lock.Unlock()
	for {
		for len(f.queue) == 0 {
			// When the queue is empty, invocation of Pop() is blocked until new item is enqueued.
			// When Close() is called, the f.closed is set and the condition is broadcasted.
			// Which causes this loop to continue and return from the Pop().
			if f.IsClosed() {
				return nil, ErrFIFOClosed
			}

			f.cond.Wait()
		}
		id := f.queue[0]
		f.queue = f.queue[1:]
		if f.initialPopulationCount &gt; 0 {
			f.initialPopulationCount--
		}
		item, ok := f.items[id]
		if !ok {
			// Item may have been deleted subsequently.
			continue
		}
		delete(f.items, id)
		err := process(item) // 进行处理
		if e, ok := err.(ErrRequeue); ok {
			f.addIfNotPresent(id, item) // 如果失败，再重新加入到队列中
			err = e.Err 
		}
		// Don't need to copyDeltas here, because we're transferring
		// ownership to the caller.
		return item, err
	}
}
</code></pre>
<h2 id="informer">Informer</h2>
<p>通过对 <code>Reflector</code>, <code>Store</code>, <code>Queue</code>, <code>ListerWatcher</code>、<code>ProcessFunc</code>, 等的概念，发现由 <code>controller</code> 所包装的起的功能并不能完成通过对API的动作监听，并通过动作来处理本地缓存的一个能力；这个情况下诞生了 <code>informer</code> 严格意义上来讲是  <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/controller.go#L317" target="_blank"
   rel="noopener nofollow noreferrer" >sharedInformer</a></p>
<pre><code class="language-go">func newInformer(
	lw ListerWatcher,
	objType runtime.Object,
	resyncPeriod time.Duration,
	h ResourceEventHandler,
	clientState Store,
) Controller {
	// This will hold incoming changes. Note how we pass clientState in as a
	// KeyLister, that way resync operations will result in the correct set
	// of update/delete deltas.
	fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{
		KnownObjects:          clientState,
		EmitDeltaTypeReplaced: true,
	})

	cfg := &amp;Config{
		Queue:            fifo,
		ListerWatcher:    lw,
		ObjectType:       objType,
		FullResyncPeriod: resyncPeriod,
		RetryOnError:     false,

		Process: func(obj interface{}) error {
			// from oldest to newest
			for _, d := range obj.(Deltas) {
				switch d.Type {
				case Sync, Replaced, Added, Updated:
					if old, exists, err := clientState.Get(d.Object); err == nil &amp;&amp; exists {
						if err := clientState.Update(d.Object); err != nil {
							return err
						}
						h.OnUpdate(old, d.Object)
					} else {
						if err := clientState.Add(d.Object); err != nil {
							return err
						}
						h.OnAdd(d.Object)
					}
				case Deleted:
					if err := clientState.Delete(d.Object); err != nil {
						return err
					}
					h.OnDelete(d.Object)
				}
			}
			return nil
		},
	}
	return New(cfg)
}
</code></pre>
<p>newInformer是位于 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/controller.go#L317" target="_blank"
   rel="noopener nofollow noreferrer" >tools/cache/controller.go</a> 下，可以看出，这里面并没有informer的概念，这里通过注释可以看到，newInformer实际上是一个提供了存储和事件通知的informer。他关联的 <code>queue</code> 则是 <code>Delta FIFO</code>，并包含了 <code>ProcessFunc</code>, <code>Store</code> 等 controller的概念。最终对外的方法为 <code>NewInformer()</code></p>
<pre><code class="language-go">func NewInformer(
	lw ListerWatcher,
	objType runtime.Object,
	resyncPeriod time.Duration,
	h ResourceEventHandler,
) (Store, Controller) {
	// This will hold the client state, as we know it.
	clientState := NewStore(DeletionHandlingMetaNamespaceKeyFunc)

	return clientState, newInformer(lw, objType, resyncPeriod, h, clientState)
}

type ResourceEventHandler interface {
	OnAdd(obj interface{})
	OnUpdate(oldObj, newObj interface{})
	OnDelete(obj interface{})
}
</code></pre>
<p>可以看到  <code>NewInformer()</code> 就是一个带有 Store功能的controller，通过这些可以假定出，<strong>Informer</strong> 就是<code>controller</code> ，将queue中相关操作分发给不同事件处理的功能</p>
<h2 id="sharedindexinformer">SharedIndexInformer</h2>
<p><code>shareInformer</code> 为客户端提供了与apiserver一致的数据对象本地缓存，并支持多事件处理程序的<strong>informer</strong>，而 <code>shareIndexInformer </code> 则是对<code>shareInformer</code>  的扩展</p>
<pre><code class="language-go">type SharedInformer interface {
	// AddEventHandler adds an event handler to the shared informer using the shared informer's resync
	// period.  Events to a single handler are delivered sequentially, but there is no coordination
	// between different handlers.
	AddEventHandler(handler ResourceEventHandler)
	// AddEventHandlerWithResyncPeriod adds an event handler to the
	// shared informer with the requested resync period; zero means
	// this handler does not care about resyncs.  The resync operation
	// consists of delivering to the handler an update notification
	// for every object in the informer's local cache; it does not add
	// any interactions with the authoritative storage.  Some
	// informers do no resyncs at all, not even for handlers added
	// with a non-zero resyncPeriod.  For an informer that does
	// resyncs, and for each handler that requests resyncs, that
	// informer develops a nominal resync period that is no shorter
	// than the requested period but may be longer.  The actual time
	// between any two resyncs may be longer than the nominal period
	// because the implementation takes time to do work and there may
	// be competing load and scheduling noise.
	AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration)
	// GetStore returns the informer's local cache as a Store.
	GetStore() Store
	// GetController is deprecated, it does nothing useful
	GetController() Controller
	// Run starts and runs the shared informer, returning after it stops.
	// The informer will be stopped when stopCh is closed.
	Run(stopCh &lt;-chan struct{})
	// HasSynced returns true if the shared informer's store has been
	// informed by at least one full LIST of the authoritative state
	// of the informer's object collection.  This is unrelated to &quot;resync&quot;.
	HasSynced() bool
	// LastSyncResourceVersion is the resource version observed when last synced with the underlying
	// store. The value returned is not synchronized with access to the underlying store and is not
	// thread-safe.
	LastSyncResourceVersion() string
}
</code></pre>
<p><code>SharedIndexInformer</code> 是对SharedInformer的实现，可以从结构中看出，<code>SharedIndexInformer</code> 大致具有如下功能：</p>
<ul>
<li>索引本地缓存</li>
<li>controller，通过list watch拉取API并推入 <code>Deltal FIFO</code></li>
<li>事件的处理</li>
</ul>
<pre><code class="language-go">type sharedIndexInformer struct {
	indexer    Indexer // 具有索引的本地缓存
	controller Controller // controller

	processor             *sharedProcessor // 事件处理函数集合
	cacheMutationDetector MutationDetector

	listerWatcher ListerWatcher
	objectType runtime.Object
	resyncCheckPeriod time.Duration
	defaultEventHandlerResyncPeriod time.Duration
	clock clock.Clock
	started, stopped bool
	startedLock      sync.Mutex
	blockDeltas sync.Mutex
}
</code></pre>
<p>而在 <a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/shared_informer.go#L397-L444" target="_blank"
   rel="noopener nofollow noreferrer" >tools/cache/share_informer.go</a> 可以看到 shareIndexInformer 的运行过程</p>
<pre><code class="language-go">func (s *sharedIndexInformer) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash()

	fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{
		KnownObjects:          s.indexer,
		EmitDeltaTypeReplaced: true,
	})

	cfg := &amp;Config{
		Queue:            fifo,
		ListerWatcher:    s.listerWatcher,
		ObjectType:       s.objectType,
		FullResyncPeriod: s.resyncCheckPeriod,
		RetryOnError:     false,
		ShouldResync:     s.processor.shouldResync,

		Process: s.HandleDeltas, // process 弹出时操作的流程
	}

	func() {
		s.startedLock.Lock()
		defer s.startedLock.Unlock()

		s.controller = New(cfg)
		s.controller.(*controller).clock = s.clock
		s.started = true
	}()

	// Separate stop channel because Processor should be stopped strictly after controller
	processorStopCh := make(chan struct{})
	var wg wait.Group
	defer wg.Wait()              // Wait for Processor to stop
	defer close(processorStopCh) // Tell Processor to stop
	wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run)
	wg.StartWithChannel(processorStopCh, s.processor.run) // 启动事件处理函数

	defer func() {
		s.startedLock.Lock()
		defer s.startedLock.Unlock()
		s.stopped = true // Don't want any new listeners
	}()
    s.controller.Run(stopCh) // 启动controller，controller会启动Reflector和fifo的Pop()
}
</code></pre>
<p>而在操作Delta FIFO中可以看到，做具体操作时，会将动作分发至对应的事件处理函数中，这个是informer初始化时对事件操作的函数</p>
<pre><code class="language-go">func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error {
	s.blockDeltas.Lock()
	defer s.blockDeltas.Unlock()


	for _, d := range obj.(Deltas) {
		switch d.Type {
		case Sync, Replaced, Added, Updated:
			s.cacheMutationDetector.AddObject(d.Object)
			if old, exists, err := s.indexer.Get(d.Object); err == nil &amp;&amp; exists {
				if err := s.indexer.Update(d.Object); err != nil {
					return err
				}

				isSync := false
				switch {
				case d.Type == Sync:
					isSync = true
				case d.Type == Replaced:
					if accessor, err := meta.Accessor(d.Object); err == nil {
						if oldAccessor, err := meta.Accessor(old); err == nil {
							isSync = accessor.GetResourceVersion() == oldAccessor.GetResourceVersion()
						}
					}
				}
                // 事件的分发
				s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync)
			} else {
				if err := s.indexer.Add(d.Object); err != nil {
					return err
				}
                // 事件的分发
				s.processor.distribute(addNotification{newObj: d.Object}, false)
			}
		case Deleted:
			if err := s.indexer.Delete(d.Object); err != nil {
				return err
			}
			s.processor.distribute(deleteNotification{oldObj: d.Object}, false)
		}
	}
	return nil
}
</code></pre>
<h3 id="事件处理函数-processor">事件处理函数 processor</h3>
<p>启动informer时也会启动注册进来的事件处理函数；<code>processor</code> 就是这个事件处理函数。</p>
<p><code>run()</code> 函数会启动两个 listener，j监听事件处理业务函数 <code>listener.run</code> 和 事件的处理</p>
<pre><code class="language-go">wg.StartWithChannel(processorStopCh, s.processor.run)

func (p *sharedProcessor) run(stopCh &lt;-chan struct{}) {
	func() {
		p.listenersLock.RLock()
		defer p.listenersLock.RUnlock()
		for _, listener := range p.listeners {
			p.wg.Start(listener.run) 
			p.wg.Start(listener.pop)
		}
		p.listenersStarted = true
	}()
	&lt;-stopCh
	p.listenersLock.RLock()
	defer p.listenersLock.RUnlock()
	for _, listener := range p.listeners {
		close(listener.addCh) // Tell .pop() to stop. .pop() will tell .run() to stop
	}
	p.wg.Wait() // Wait for all .pop() and .run() to stop
}
</code></pre>
<p>可以看出，就是拿到的事件，根据注册的到informer的事件函数进行处理</p>
<pre><code class="language-go">func (p *processorListener) run() {
	stopCh := make(chan struct{})
	wait.Until(func() {
		for next := range p.nextCh { // 消费事件
			switch notification := next.(type) {
			case updateNotification:
				p.handler.OnUpdate(notification.oldObj, notification.newObj)
			case addNotification:
				p.handler.OnAdd(notification.newObj)
			case deleteNotification:
				p.handler.OnDelete(notification.oldObj)
			default:
				utilruntime.HandleError(fmt.Errorf(&quot;unrecognized notification: %T&quot;, next))
			}
		}
		// the only way to get here is if the p.nextCh is empty and closed
		close(stopCh)
	}, 1*time.Second, stopCh)
}
</code></pre>
<h3 id="informer中的事件的设计">informer中的事件的设计</h3>
<p>了解了informer如何处理事件，就需要学习下，informer的事件系统设计 <code>prossorListener</code></p>
<h4 id="事件的添加">事件的添加</h4>
<p>当在handleDelta时，会分发具体的事件</p>
<pre><code class="language-go">// 事件的分发
s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync)
</code></pre>
<p>此时，事件泵 <code>Pop()</code> 会根据接收到的事件进行处理</p>
<pre><code class="language-go">// run() 时会启动一个事件泵
p.wg.Start(listener.pop)

func (p *processorListener) pop() {
	defer utilruntime.HandleCrash()
	defer close(p.nextCh) 

	var nextCh chan&lt;- interface{}
	var notification interface{}
	for {
		select {
        case nextCh &lt;- notification: // 这里实际上是一个阻塞的等待
            // 单向channel 可能不会走到这步骤
			var ok bool
            // deltahandle 中 distribute 会将事件添加到addCh待处理事件中
            // 处理完事件会再次拿到一个事件
			notification, ok = p.pendingNotifications.ReadOne()
			if !ok { // Nothing to pop
				nextCh = nil // Disable this select case
			}
        // 处理 分发过来的事件 addCh
		case notificationToAdd, ok := &lt;-p.addCh: // distribute分发的事件
			if !ok {
				return
			}
            // 这里代表第一次，没有任何事件时，或者上面步骤完成读取
			if notification == nil { // 就会走这里
				notification = notificationToAdd 
				nextCh = p.nextCh 
			} else { 
                // notification否则代表没有处理完，将数据再次添加到待处理中
				p.pendingNotifications.WriteOne(notificationToAdd)
			}
		}
	}
}
</code></pre>
<p>该消息事件的流程图为</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220525213837136.png" alt="image-20220525213837136" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>通过一个简单实例来学习client-go中的消息通知机制</p>
<pre><code class="language-go">package main

import (
	&quot;fmt&quot;
	&quot;time&quot;

	&quot;k8s.io/utils/buffer&quot;
)

var nextCh1 = make(chan interface{})
var addCh = make(chan interface{})
var stopper = make(chan struct{})
var notification interface{}
var pendding = *buffer.NewRingGrowing(2)

func main() {
	// pop
	go func() {
		var nextCh chan&lt;- interface{}
		var notification interface{}
		//var n int
		for {
			fmt.Println(&quot;busy wait&quot;)
			fmt.Println(&quot;entry select&quot;, notification)
			select {
			// 初始时，一个未初始化的channel，nil，形成一个阻塞（单channel下是死锁）
			case nextCh &lt;- notification:
				fmt.Println(&quot;entry nextCh&quot;, notification)
				var ok bool
				// 读不到数据代表已处理完，置空锁
				notification, ok = pendding.ReadOne()
				if !ok {
					fmt.Println(&quot;unactive nextch&quot;)
					nextCh = nil
				}
			// 事件的分发，监听，初始时也是一个阻塞
			case notificationToAdd, ok := &lt;-addCh:
				fmt.Println(notificationToAdd, notification)
				if !ok {
					return
				}
				// 线程安全
				// 当消息为空时，没有被处理
				// 锁为空，就分发数据
				if notification == nil {
					fmt.Println(&quot;frist notification nil&quot;)
					notification = notificationToAdd
					nextCh = nextCh1 // 这步骤等于初始化了局部的nextCh，会触发上面的流程
				} else {
					// 在第三次时，会走到这里，数据进入环
					fmt.Println(&quot;into ring&quot;, notificationToAdd)
					pendding.WriteOne(notificationToAdd)
				}
			}
		}
	}()
	// producer
	go func() {
		i := 0
		for {
			i++
			if i%5 == 0 {
				addCh &lt;- fmt.Sprintf(&quot;thread 2 inner -- %d&quot;, i)
				time.Sleep(time.Millisecond * 9000)
			} else {
				addCh &lt;- fmt.Sprintf(&quot;thread 2 outer -- %d&quot;, i)
				time.Sleep(time.Millisecond * 500)
			}
		}
	}()
	// subsriber
	go func() {
		for {
			for next := range nextCh1 {
				time.Sleep(time.Millisecond * 300)
				fmt.Println(&quot;consumer&quot;, next)
			}
		}
	}()
	&lt;-stopper
}
</code></pre>
<p>总结，这里的机制类似于线程安全，进入临界区的一些算法，临界区就是 <code>nextCh</code>，<code>notification</code> 就是保证了至少有一个进程可以进入临界区（要么分发事件，要么生产事件）；<code>nextCh</code> 和 <code>nextCh1</code> 一个是局部管道一个是全局的，管道未初始化代表了死锁（阻塞）；当有消息要处理时，会将局部管道 <code>nextCh</code> 赋值给 全局 <code>nextCh1</code> 此时相当于解除了分发的步骤（对管道赋值，触发分发操作）；<code>ringbuffer</code> 实际上是提供了一个对 <code>notification</code> 加锁的操作，在没有处理的消息时，需要保障 <code>notification</code> 为空，同时也关闭了流程 <code>nextCh</code> 的写入。这里主要是考虑对golang中channel的用法</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes组件核心 - client-go</title>
      <link>https://www.oomkill.com/2022/05/ch06-client-go/</link>
      <pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/05/ch06-client-go/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="prepare">Prepare</h2>
<h3 id="introduction">Introduction</h3>
<p>从2016年8月起，Kubernetes官方提取了与Kubernetes相关的核心源代码，形成了一个独立的项目，即<code>client-go</code>，作为官方提供的go客户端。Kubernetes的部分代码也是基于这个项目的。</p>
<p><code>client-go</code> 是kubernetes中广义的客户端基础库，在Kubernetes各个组件中或多或少都有使用其功能。。也就是说，<code>client-go</code>可以在kubernetes集群中添加、删除和查询资源对象（包括deployment、service、pod、ns等）。</p>
<p>在了解client-go前，还需要掌握一些概念</p>
<ul>
<li>在客户端验证 API</li>
<li>使用证书和使用令牌，来验证客户端</li>
<li>kubernetes集群的访问模式</li>
</ul>
<h3 id="使用证书和令牌来验证客户端">使用证书和令牌来验证客户端</h3>
<p>在访问apiserver时，会对访问者进行鉴权，因为是https请求，在请求时是需要ca的，也可以使用 -k 使用insecure模式</p>
<pre><code>$ curl --cacert /etc/kubernetes/pki/ca.crt https://10.0.0.4:6443/version
\{
  &quot;major&quot;: &quot;1&quot;,
  &quot;minor&quot;: &quot;18+&quot;,
  &quot;gitVersion&quot;: &quot;v1.18.20-dirty&quot;,
  &quot;gitCommit&quot;: &quot;1f3e19b7beb1cc0110255668c4238ed63dadb7ad&quot;,
  &quot;gitTreeState&quot;: &quot;dirty&quot;,
  &quot;buildDate&quot;: &quot;2022-05-17T12:45:14Z&quot;,
  &quot;goVersion&quot;: &quot;go1.16.15&quot;,
  &quot;compiler&quot;: &quot;gc&quot;,
  &quot;platform&quot;: &quot;linux/amd64&quot;
}

$ curl -k https://10.0.0.4:6443/api/v1/namespace/default/pods/netbox
{
  &quot;kind&quot;: &quot;Status&quot;,
  &quot;apiVersion&quot;: &quot;v1&quot;,
  &quot;metadata&quot;: {
    
  },
  &quot;status&quot;: &quot;Failure&quot;,
  &quot;message&quot;: &quot;namespace \&quot;default\&quot; is forbidden: User \&quot;system:anonymous\&quot; cannot get resource \&quot;namespace/pods\&quot; in API group \&quot;\&quot; at the cluster scope&quot;,
  &quot;reason&quot;: &quot;Forbidden&quot;,
  &quot;details&quot;: {
    &quot;name&quot;: &quot;default&quot;,
    &quot;kind&quot;: &quot;namespace&quot;
  },
  &quot;code&quot;: 403
}
</code></pre>
<p>从错误中可以看出，该请求已通过身份验证，用户是  <code>system:anonymous</code>，但该用户未授权列出对应的资源。而上述请求只是忽略 curl 的https请求需要做的验证，而Kubernetes也有对应验证的机制，这个时候需要提供额外的身份信息来获得所需的访问权限。<a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes支持多种身份认证机制</a>，ssl证书也是其中一种。</p>
<blockquote>
<p>注：在Kubernetes中没有表示用户的资源。即kubernetes集群中，无法添加和创建。但由集群提供的有效证书的用户都视为允许的用户。Kubernetes从证书中的使用者CN和使用者可选名称中获得<strong>用户</strong>；然后，RBAC 判断用户是否有权限操作资源。从 Kubernetes1.4 开始，支持<strong>用户组</strong>，即证书中的O</p>
</blockquote>
<p>可以使用 curl 的 <code>--cert</code> 和 <code>--key</code> 指定用户的证书</p>
<pre><code>curl --cacert /etc/kubernetes/pki/ca.crt  \
	--cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \
	--key /etc/kubernetes/pki/apiserver-ubelet-client.key \
	https://10.0.0.4:6443/api/v1/namespaces/default/pods/netbox
</code></pre>
<h3 id="使用serviceaccount验证客户端身份">使用serviceaccount验证客户端身份</h3>
<p>使用一个serviceaccount JWT，获取一个SA的方式如下</p>
<pre><code>kubectl get secrets \
$(kubectl get serviceaccounts/default -o jsonpath='{.secrets[0].name}')  -o jsonpath='{.data.token}' \
| base64 --decode

JWT=$(kubectl get secrets \
$(kubectl get serviceaccounts/default -o jsonpath='{.secrets[0].name}')  -o jsonpath='{.data.token}' \
| base64 --decode)
</code></pre>
<p>使用secret来访问API</p>
<pre><code>curl --cacert /etc/kubernetes/pki/ca.crt \
	--header &quot;Authorization: Bearer $JWT&quot; \
	https://10.0.0.4:6443/apis/apps/v1/namespaces/default/deployments
</code></pre>
<h3 id="pod内部调用kubernetes-api">Pod内部调用Kubernetes API</h3>
<p>kubernete会将Kubernetes API地址通过环境变量提供给 Pod，可以通过命令看到</p>
<pre><code>$ env|grep -i kuber
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT=tcp://192.168.0.1:443
KUBERNETES_PORT_443_TCP_ADDR=192.168.0.1
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP=tcp://192.168.0.1:443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_HOST=192.168.0.1
</code></pre>
<p>并且还会在将 Kubernetes CA和SA等信息放置在目录 <code>/var/run/secrets/kubernetes.io/serviceaccount/</code>，通过这些就可以从Pod内部访问API</p>
<pre><code>cd /var/run/secrets/kubernetes.io/serviceaccount/

curl --cacert ca.crt --header &quot;Authorization: Bearer $(cat token)&quot; https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces/default/pods/netbox
</code></pre>
<blockquote>
<p>Reference</p>
<p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes API Reference Docs</a></p>
</blockquote>
<h2 id="client-go">client-go</h2>
<h3 id="关于client-go的模块">关于client-go的模块</h3>
<h4 id="k8sioapi">k8s.io/api</h4>
<p>与Pods、ConfigMaps、Secrets和其他Kubernetes 对象所对应的数据结构都在，<a href="https://github.com/kubernetes/api" target="_blank"
   rel="noopener nofollow noreferrer" ><code>k8s.io/api</code></a>，此包几乎没有算法，仅仅是数据机构，该模块有多达上千个用于描述Kubernetes中资源API的结构；通常被client，server，controller等其他的组件使用。</p>
<h4 id="k8sioapimachinery">k8s.io/apimachinery</h4>
<p>根据该库的<a href="https://github.com/kubernetes/apimachinery/tree/3d7c63b4de4fdee1917284129969901d4777facc#purpose" target="_blank"
   rel="noopener nofollow noreferrer" >描述文件</a>可知，这个库是Server和Client中使用的Kubernetes API共享依赖库，也是kubernetes中更低一级的通用的数据结构。在我们构建自定义资源时，不需要为自定义结构创建属性，如 <code>Kind</code>, <code>apiVersion</code>，<code>name</code>&hellip;，这些都是库 <code>apimachinery</code> 所提供的功能。</p>
<p>如，在包 <code>k8s.io/apimachinery/pkg/apis/meta </code> 定义了两个结构 <code>TypeMeta</code> 和 <code>ObjectMeta</code>；将这这两个结构嵌入自定义的结构中，可以以通用的方式兼容对象，如Kubernetes中的资源 <code>Deplyment</code> 也是这么完成的</p>
<center>通过图来了解Kubernetes的资源如何实现的</center>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220523211835602.png" alt="image-20220523211835602" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>如在 <code>k8s.io/apimachinery/pkg/runtime/interfaces.go</code> 中定义了 interface，这个类为在schema中注册的API都需要实现这个结构</p>
<pre><code class="language-go">type Object interface {
	GetObjectKind() schema.ObjectKind
	DeepCopyObject() Object
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220519162127427.png" alt="image-20220519162127427" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="非结构化数据">非结构化数据</h4>
<p>非结构化数据 <code>Unstructured </code> 是指在kubernete中允许将没有注册为Kubernetes API的对象，作为Json对象的方式进行操作，如，<a href="https://github.com/iximiuz/client-go-examples/blob/5b220c4572d65ea8bf0ad68e369e015902e7521c/crud-dynamic-simple/main.go#L36" target="_blank"
   rel="noopener nofollow noreferrer" >使用非结构化 Kubernetes 对象</a></p>
<pre><code class="language-go">desired := &amp;unstructured.Unstructured{
    Object: map[string]interface{}{
        &quot;apiVersion&quot;: &quot;v1&quot;,
        &quot;kind&quot;:       &quot;ConfigMap&quot;,
        &quot;metadata&quot;: map[string]interface{}{
            &quot;namespace&quot;:    namespace,
            &quot;generateName&quot;: &quot;crud-dynamic-simple-&quot;,
        },
        &quot;data&quot;: map[string]interface{}{
            &quot;foo&quot;: &quot;bar&quot;,
        },
    },
}
</code></pre>
<h4 id="非结构化数据的转换">非结构化数据的转换</h4>
<p>在 <code>k8s.io/apimachinery/pkg/runtime.UnstructuredConverter</code> 中，也提供了将非结构化数据转换为Kubernetes API注册过的结构，参考如何将<a href="https://github.com/iximiuz/client-go-examples/tree/main/convert-unstructured-typed" target="_blank"
   rel="noopener nofollow noreferrer" >非结构化对象转换为Kubernetes Object</a>。</p>
<blockquote>
<p>Reference</p>
<p><a href="https://iximiuz.com/en/posts/kubernetes-api-go-types-and-common-machinery/" target="_blank"
   rel="noopener nofollow noreferrer" >go types</a></p>
</blockquote>
<h3 id="install-client-go">install client-go</h3>
<p><strong>如何选择 <code>client-go</code> 的版本</strong></p>
<p>​	对于不同的kubernetes版本使用标签 <code>v0.x.y</code> 来表示对应的客户端版本。具体对应参考 <a href="https://github.com/kubernetes/client-go#compatibility-matrix" target="_blank"
   rel="noopener nofollow noreferrer" >client-go</a> 。</p>
<p>​	例如使用的kubernetes版本为 <code>v1.18.20</code> 则使用对应的标签 <code>v0.x.y</code> 来替换符合当前版本的客户端库。例如：</p>
<pre><code>go get k8s.io/client-go@v0.18.10
</code></pre>
<p>官网中给出了<code>client-go</code>的兼容性矩阵，可以很明了的看出如何选择适用于自己kubernetes版本的对应的client-go</p>
<ul>
<li><code>✓</code> 表示 该版本的 <code>client-go</code> 与对应的 kubernetes版本功能完全一致</li>
<li><code>+</code> <code>client-go</code> 具有 kubernetes apiserver中不具备的功能。</li>
<li><code>-</code>  Kubernetes apiserver 具有<code>client-go</code> 无法使用的功。</li>
</ul>
<p>一般情况下，除了对应的版本号完全一致外，其他都存在 功能的<code>+-</code>。</p>
<h3 id="client-go-目录介绍">client-go 目录介绍</h3>
<p>client-go的每一个目录都是一个go package</p>
<ul>
<li><code>kubernetes</code> 包含与Kubernetes API所通信的客户端集</li>
<li><code>discovery</code>  用于发现kube-apiserver所支持的api</li>
<li><code>dynamic</code>  包含了一个动态客户端，该客户端能够对kube-apiserver任意的API进行操作。</li>
<li><code>transport</code>  提供了用于设置认证和启动链接的功能</li>
<li><code>tools/cache</code>: 一些 low-level controller与一些数据结构如fifo，reflector等</li>
</ul>
<h3 id="structure-of-client-go">structure of client-go</h3>
<ul>
<li>
<p><code>RestClient</code>：是最基础的基础架构，其作用是将是使用了http包进行封装成RESTClient。位于<code>rest</code> 目录，RESTClient封装了资源URL的通用格式，例如<code>Get()</code>、<code>Put()</code>、<code>Post()</code> <code>Delete()</code>。是与Kubernetes API的访问行为提供的基于RESTful方法进行交互基础架构。</p>
<ul>
<li>同时支持Json 与 protobuf</li>
<li>支持所有的原生资源和CRD</li>
</ul>
</li>
<li>
<p><code>ClientSet</code>：Clientset基于RestClient进行封装对 Resource 与 version 管理集合；<a href="https://pkg.go.dev/k8s.io/client-go@v0.24.0/kubernetes#NewForConfig" target="_blank"
   rel="noopener nofollow noreferrer" >如何创建</a></p>
</li>
<li>
<p><code>DiscoverySet</code>：RestClient进行封装，可动态发现 kube-apiserver 所支持的 GVR（Group Version Resource）；<a href="https://pkg.go.dev/k8s.io/client-go@v0.24.0/discovery#NewDiscoveryClient" target="_blank"
   rel="noopener nofollow noreferrer" >如何创建</a>，这种类型是一种非映射至clientset的客户端</p>
</li>
<li>
<p><code>DynamicClient</code>：基于RestClient，包含动态的客户端，可以对Kubernetes所支持的 API对象进行操作，包括CRD；<a href="https://pkg.go.dev/k8s.io/client-go@v0.24.0/dynamic#NewForConfig" target="_blank"
   rel="noopener nofollow noreferrer" >如何创建</a></p>
</li>
<li>
<p>仅支持json</p>
</li>
<li>
<p><code>fakeClient</code>， <code>client-go</code> 实现的mock对象，主要用于单元测试。</p>
</li>
</ul>
<p>以上client-go所提供的客户端，仅可使用kubeconfig进行连接。</p>
<h3 id="什么是clientset">什么是clientset</h3>
<p>clientset代表了kubernetes中所有的资源类型，这里不包含CRD的资源，如：</p>
<ul>
<li><code>core</code></li>
<li><code>extensions</code></li>
<li><code>batch</code></li>
<li>&hellip;</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220518185246688.png" alt="image-20220518185246688" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="client-go使用">client-go使用</h3>
<p><strong>DynamicClient客户端</strong></p>
<ul>
<li>
<p>与 ClientSet 的区别是，可以对任意 Kubernetes 资源进行 RESTful 操作。同样提供管理的方法</p>
</li>
<li>
<p>最大的不同，ClientSet 需要预先实现每种 Resource 和 Version 的操作，内部的数据都是结构化数据（已知数据结构）；DynamicClient 内部实现了 Unstructured，用于处理非结构化的数据（无法提前预知的数据结构），这是其可以处理 CRD 自定义资源的关键。</p>
</li>
</ul>
<p><strong>dynamicClient 实现流程</strong></p>
<ul>
<li>
<p>通过 NewForConfig 实例化 conf 为 DynamicInterface客户端</p>
</li>
<li>
<p><code>DynamicInterface </code>客户端中，实现了一个<code>Resource</code> 方法即为实现了<code>Interface</code>接口</p>
</li>
<li>
<p><code>dynamicClient</code> 实现了非结构化数据类型与rest client，可以通过其方法将<code>Resource</code> 由rest从apiserver中获得api对象，<code>runtime.DeafultUnstructuredConverter.FromUnstructrued</code> 转为对应的类型。</p>
</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220522223023430.png" alt="image-20220522223023430" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<hr>
<p>注意：<code>GVR </code>中资源类型 resource为复数。<code>kind:Pod</code> 即为 <code>Pods</code></p>
<pre><code class="language-go">package main

import (
	&quot;context&quot;
	&quot;flag&quot;
	&quot;fmt&quot;
	&quot;os&quot;

	v1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	&quot;k8s.io/apimachinery/pkg/runtime/schema&quot;
	&quot;k8s.io/client-go/dynamic&quot;
	&quot;k8s.io/client-go/kubernetes&quot;
	&quot;k8s.io/client-go/rest&quot;
	&quot;k8s.io/client-go/tools/clientcmd&quot;
	&quot;k8s.io/client-go/util/homedir&quot;
)

func main() {
	var (
		k8sconfig  *string //使用kubeconfig配置文件进行集群权限认证
		restConfig *rest.Config
		err        error
	)

	if home := homedir.HomeDir(); home != &quot;&quot; {
		k8sconfig = flag.String(&quot;kubeconfig&quot;, fmt.Sprintf(&quot;%s/.kube/config&quot;, home), &quot;kubernetes auth config&quot;)
	}
	k8sconfig = k8sconfig
	flag.Parse()
	if _, err := os.Stat(*k8sconfig); err != nil {
		panic(err)
	}

	if restConfig, err = rest.InClusterConfig(); err != nil {
		// 这里是从masterUrl 或者 kubeconfig传入集群的信息，两者选一
		restConfig, err = clientcmd.BuildConfigFromFlags(&quot;&quot;, *k8sconfig)
		if err != nil {
			panic(err)
		}
	}
	// 创建客户端类型
	// NewForConfig creates a new dynamic client or returns an error.
	// dynamic.NewForConfig(restConfig)
	// NewForConfig creates a new Clientset for the given config
	// kubernetes.NewForConfig(restConfig)
	// NewDiscoveryClientForConfig creates a new DiscoveryClient for the given config.
	//clientset, err := discovery.NewDiscoveryClientForConfig(restConfig)
	dynamicset, err := dynamic.NewForConfig(restConfig)

	// 这里遵循的是 kubernetes Rest API，如Pod是
	// /api/v1/namespaces/{namespace}/pods
	// /apis/apps/v1/namespaces/{namespace}/deployments
	// 遵循GVR格式填写
	podList, err := dynamicset.Resource(schema.GroupVersionResource{
		Group:    &quot;&quot;,
		Version:  &quot;v1&quot;,
		Resource: &quot;pods&quot;,
	}).Namespace(&quot;default&quot;).List(context.TODO(), v1.ListOptions{})
	if err != nil {
		panic(err)
	}

	daemonsetList, err := dynamicset.Resource(schema.GroupVersionResource{
		Group:    &quot;apps&quot;,
		Version:  &quot;v1&quot;,
		Resource: &quot;daemonsets&quot;,
	}).Namespace(&quot;kube-system&quot;).List(context.TODO(), v1.ListOptions{})

	if err != nil {
		panic(err)
	}

	for _, row := range podList.Items {
		fmt.Println(row.GetName())
	}

	for _, row := range daemonsetList.Items {
		fmt.Println(row.GetName())
	}

	// clientset mode

	clientset, err := kubernetes.NewForConfig(restConfig)
	podIns, err := clientset.CoreV1().Pods(&quot;default&quot;).List(context.TODO(), v1.ListOptions{})
	for _, row := range podIns.Items {
		fmt.Println(row.GetName())
	}
}
</code></pre>
<blockquote>
<p>Extension</p>
<p><a href="http://yuezhizizhang.github.io/kubernetes/kubectl/client-go/2020/05/13/kubectl-client-go-part-2.html" target="_blank"
   rel="noopener nofollow noreferrer" >一些client-go使用</a></p>
</blockquote>
<h2 id="informer">Informer</h2>
<p>informer是client-go提供的 <strong>Listwatcher</strong> 接口，主要作为 Controller构成的组件，在Kubernetes中， Controller的一个重要作用是观察对象的期望状态 <code>spec</code> 和实际状态 <code>statue</code> 。<strong>为了观察对象的状态，Controller需要向 Apiserver发送请求</strong>；但是通常情况下，频繁向Apiserver发出请求的会增加etcd的压力，为了解决这类问题，<code>client-go</code> 一个缓存，通过缓存，控制器可以不必发出大量请求，并且只关心对象的事件。也就是 informer。</p>
<p>从本质上来讲，informer是使用kubernetes API观察其变化，来维护状态的缓存，称为 <code>indexer</code>；并通过对应事件函数通知客户端信息的变化，informer为一系列组件，通过这些组件来实现的这些功能。</p>
<ul>
<li>Reflector：与 apiserver交互的组件</li>
<li>Delta FIFO：一个特殊的队列，Reflector将状态的变化存储在里面</li>
<li>indexer：本地存储，与etcd保持一致，减轻API Server与etcd的压力</li>
<li>Processor：监听处理器，通过将监听到的事件发送给对应的监听函数</li>
<li>Controller：从队列中对整个数据的编排处理的过程</li>
</ul>
<h3 id="informer的工作模式">informer的工作模式</h3>
<p>首先通过<code>List</code>从Kubernetes API中获取资源所有对象并同时缓存，然后通过<code>Watch</code>机制监控资源。这样，通过informer与缓存，就可以直接和informer交互，而不用每次都和Kubernetes API交互。</p>
<p>另外，<code>informer</code> 还提供了事件的处理机制，以便 Controller 或其他应用程序根据回调钩子函数等处理特定的业务逻辑。因为<code>Informer</code>可以通过<code>List/Watch</code>机制监控所有资源的所有事件，只要在<code>Informer</code>中添加<code>ResourceEventHandler</code>实例的回调函数，如：<code>onadd(obj interface {})</code>, <code>onupdate (oldobj, newobj interface {})</code>和<code>OnDelete( obj interface {})</code> 可以实现处理资源的创建、更新和删除。 在Kubernetes中，各种控制器都使用了Informer。</p>
<h3 id="分析informer的流程">分析informer的流程</h3>
<p>通过代码 <a href="https://github.com/kubernetes/client-go/blob/master/informers/apps/v1/deployment.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/client-go/informers/apps/v1/deployment.go</a> 可以看出，在每个控制器下，都实现了一个 <code>Informer</code> 和 <code>Lister</code> ，Lister就是indexer；</p>
<pre><code class="language-go">type SharedInformer interface {
    // 添加一个事件处理函数，使用informer默认的resync period
	AddEventHandler(handler ResourceEventHandler)
    // 将事件处理函数注册到 share informer，将resyncPeriod作为参数传入
	AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration)
	// 从本地缓存获取的信息作为infomer的返回
	GetStore() Store
	// 已弃用
	GetController() Controller
	// 运行一个informer，当stopCh停止时，informer也被关闭
	Run(stopCh &lt;-chan struct{})
	// HasSynced returns true if the shared informer's store has been
	// informed by at least one full LIST of the authoritative state
	// of the informer's object collection.  This is unrelated to &quot;resync&quot;.
	HasSynced() bool
	// LastSyncResourceVersion is the resource version observed when last synced with the underlying store. The value returned is not synchronized with access to the underlying store and is not thread-safe.
	LastSyncResourceVersion() string
}
</code></pre>
<p>而 Shared Informer 对所有的API组提供一个shared informer</p>
<pre><code class="language-go">// SharedInformerFactory provides shared informers for resources in all known
// API group versions.
type SharedInformerFactory interface {
	internalinterfaces.SharedInformerFactory
	ForResource(resource schema.GroupVersionResource) (GenericInformer, error)
	WaitForCacheSync(stopCh &lt;-chan struct{}) map[reflect.Type]bool

	Admissionregistration() admissionregistration.Interface
	Apps() apps.Interface
	Auditregistration() auditregistration.Interface
	Autoscaling() autoscaling.Interface
	Batch() batch.Interface
	Certificates() certificates.Interface
	Coordination() coordination.Interface
	Core() core.Interface
	Discovery() discovery.Interface
	Events() events.Interface
	Extensions() extensions.Interface
	Flowcontrol() flowcontrol.Interface
	Networking() networking.Interface
	Node() node.Interface
	Policy() policy.Interface
	Rbac() rbac.Interface
	Scheduling() scheduling.Interface
	Settings() settings.Interface
	Storage() storage.Interface
}
</code></pre>
<p>可以看到在 <a href="https://github.com/kubernetes/kubernetes/blob/v1.9.0/staging/src/k8s.io/client-go/informers/apps/v1/deployment.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/client-go/informers/apps/v1/deployment.go</a> 实现了这个interface</p>
<pre><code class="language-go">type DeploymentInformer interface {
   Informer() cache.SharedIndexInformer
   Lister() v1.DeploymentLister
}
</code></pre>
<p>而在对应的 deployment controller中会调用这个<code>Informer</code> 实现对状态的监听；``</p>
<pre><code class="language-go">// NewDeploymentController creates a new DeploymentController.
//  appsinformers.DeploymentInformer就是client-go 中的 /apps/v1/deployment实现的informer
func NewDeploymentController(dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) {
	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartLogging(klog.Infof)
	eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl{Interface: client.CoreV1().Events(&quot;&quot;)})

	if client != nil &amp;&amp; client.CoreV1().RESTClient().GetRateLimiter() != nil {
		if err := ratelimiter.RegisterMetricAndTrackRateLimiterUsage(&quot;deployment_controller&quot;, client.CoreV1().RESTClient().GetRateLimiter()); err != nil {
			return nil, err
		}
	}
	dc := &amp;DeploymentController{
		client:        client,
		eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: &quot;deployment-controller&quot;}),
		queue:         workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;deployment&quot;),
	}
	dc.rsControl = controller.RealRSControl{
		KubeClient: client,
		Recorder:   dc.eventRecorder,
	}

	dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    dc.addDeployment,
		UpdateFunc: dc.updateDeployment,
		// This will enter the sync loop and no-op, because the deployment has been deleted from the store.
		DeleteFunc: dc.deleteDeployment,
	})
	rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc:    dc.addReplicaSet,
		UpdateFunc: dc.updateReplicaSet,
		DeleteFunc: dc.deleteReplicaSet,
	})
	podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		DeleteFunc: dc.deletePod,
	})

	dc.syncHandler = dc.syncDeployment
	dc.enqueueDeployment = dc.enqueue

	dc.dLister = dInformer.Lister()
	dc.rsLister = rsInformer.Lister()
	dc.podLister = podInformer.Lister()
	dc.dListerSynced = dInformer.Informer().HasSynced
	dc.rsListerSynced = rsInformer.Informer().HasSynced
	dc.podListerSynced = podInformer.Informer().HasSynced
	return dc, nil
}
</code></pre>
<h2 id="reflector">Reflector</h2>
<p>reflector是client-go中负责监听 Kubernetes API 的组件，也是整个机制中的生产者，负责将 watch到的数据将其放入 <code>watchHandler</code> 中的delta FIFO队列中。也就是吧etcd的数据反射为 delta fifo的数据</p>
<p>在代码 <a href="https://github.com/kubernetes/kubernetes/blob/v1.9.0/staging/src/k8s.io/client-go/tools/cache/reflector.go" target="_blank"
   rel="noopener nofollow noreferrer" >k8s.io/client-go/tools/cache/reflector.go</a> 中定义了 Reflector 对象</p>
<pre><code class="language-go">type Reflector struct {
    // reflector的名称，默认为一个 file:line的格式
	name string
    // 期待的类型名称，这里只做展示用，
    // 如果提供，是一个expectedGVK字符串类型，否则是expectedType字符串类型
	expectedTypeName string
    // 期待放置在存储中的类型，如果是一个非格式化数据，那么其 APIVersion与Kind也必须为正确的格式
	expectedType reflect.Type
    // GVK 存储中的对象，是GVK格式
	expectedGVK *schema.GroupVersionKind
	// 同步数据的存储
	store Store
	// 这个是reflector的一个核心，提供了 List和Watch功能
	listerWatcher ListerWatcher

	// backoff manages backoff of ListWatch
	backoffManager wait.BackoffManager

	resyncPeriod time.Duration
	
	ShouldResync func() bool
	// clock allows tests to manipulate time
	clock clock.Clock
	
	paginatedResult bool
	// 最后资源的版本号
	lastSyncResourceVersion string
    // 当 lastSyncResourceVersion 过期或者版本太大，这个值将为 true
	isLastSyncResourceVersionUnavailable bool
    // 读写锁，对lastSyncResourceVersion的读写操作的保护
	lastSyncResourceVersionMutex sync.RWMutex
	// WatchListPageSize is the requested chunk size of initial and resync watch lists.
	// scalability problems.
    // 是初始化时，或者重新同步时的块大小。如果没有设置，将为任意的旧数据
    // 因为是提供了分页功能，RV=0则为默认的页面大小
    // 
	WatchListPageSize int64
}
</code></pre>
<p>而 方法 <code>NewReflector()</code> 给用户提供了一个初始化 Reflector的接口</p>
<p>在 cotroller.go 中会初始化一个 relector</p>
<pre><code class="language-go">func (c *controller) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash()
	go func() {
		&lt;-stopCh
		c.config.Queue.Close()
	}()
	r := NewReflector(
		c.config.ListerWatcher,
		c.config.ObjectType,
		c.config.Queue,
		c.config.FullResyncPeriod,
	)
</code></pre>
<p>Reflector下有三个可对用户提供的方法，<code>Run()</code>, <code>ListAndWatch()</code> , <code>LastSyncResourceVersion()</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220519222729807.png" alt="image-20220519222729807" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><code>Run()</code> 是对Reflector的运行，也就是对 <code>ListAndWatch()</code> ；</p>
<pre><code class="language-go">func (r *Reflector) Run(stopCh &lt;-chan struct{}) {
	klog.V(2).Infof(&quot;Starting reflector %s (%s) from %s&quot;, r.expectedTypeName, r.resyncPeriod, r.name)
	wait.BackoffUntil(func() {
		if err := r.ListAndWatch(stopCh); err != nil {
			utilruntime.HandleError(err)
		}
	}, r.backoffManager, true, stopCh)
	klog.V(2).Infof(&quot;Stopping reflector %s (%s) from %s&quot;, r.expectedTypeName, r.resyncPeriod, r.name)
}
</code></pre>
<p>而 <code>ListAndWatch()</code> 则是实际上真实的对Reflector业务的执行</p>
<pre><code class="language-go">// 前面一些都是对信息的初始化与日志输出
func (r *Reflector) ListAndWatch(stopCh &lt;-chan struct{}) error {
	klog.V(3).Infof(&quot;Listing and watching %v from %s&quot;, r.expectedTypeName, r.name)
	var resourceVersion string

	options := metav1.ListOptions{ResourceVersion: r.relistResourceVersion()}
	// 分页功能
	if err := func() error {
		initTrace := trace.New(&quot;Reflector ListAndWatch&quot;, trace.Field{&quot;name&quot;, r.name})
		defer initTrace.LogIfLong(10 * time.Second)
		var list runtime.Object
		var paginatedResult bool
		var err error
		listCh := make(chan struct{}, 1)
		panicCh := make(chan interface{}, 1)
		go func() {
			....
	// 清理和重新同步的一些
	resyncerrc := make(chan error, 1)
	cancelCh := make(chan struct{})
	defer close(cancelCh)
	go func() {
		...
	}()

	for {
		// give the stopCh a chance to stop the loop, even in case of continue statements further down on errors
		select {
		case &lt;-stopCh:
			return nil
		default:
		}

		timeoutSeconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0))
		options = metav1.ListOptions{
			ResourceVersion: resourceVersion,
			// 为了避免watch的挂起设置一个超时
            // 仅在工作窗口期，处理任何时间
			TimeoutSeconds: &amp;timeoutSeconds,
			// To reduce load on kube-apiserver on watch restarts, you may enable watch bookmarks.
			// Reflector doesn't assume bookmarks are returned at all (if the server do not support
			// watch bookmarks, it will ignore this field).
			AllowWatchBookmarks: true,
		}

		start := r.clock.Now()
        // 开始监听
		w, err := r.listerWatcher.Watch(options)
		if err != nil {
			switch {
			case isExpiredError(err):
				// 没有设置 LastSyncResourceVersionExpired 也就是过期，会保持与返回数据相同的
				// 首次会先将RV列出
				klog.V(4).Infof(&quot;%s: watch of %v closed with: %v&quot;, r.name, r.expectedTypeName, err)
			case err == io.EOF:
				// 通常为watch关闭
			case err == io.ErrUnexpectedEOF:
				klog.V(1).Infof(&quot;%s: Watch for %v closed with unexpected EOF: %v&quot;, r.name, r.expectedTypeName, err)
			default:
				utilruntime.HandleError(fmt.Errorf(&quot;%s: Failed to watch %v: %v&quot;, r.name, r.expectedTypeName, err))
			}
			// 如果出现 connection refuse，通常与apisserver通讯失败，这个时候会重新发送请求
			if utilnet.IsConnectionRefused(err) {
				time.Sleep(time.Second)
				continue
			}
			return nil
		}

		if err := r.watchHandler(start, w, &amp;resourceVersion, resyncerrc, stopCh); err != nil {
			if err != errorStopRequested {
				switch {
				case isExpiredError(err):
					// 同上步骤的功能
					klog.V(4).Infof(&quot;%s: watch of %v closed with: %v&quot;, r.name, r.expectedTypeName, err)
				default:
					klog.Warningf(&quot;%s: watch of %v ended with: %v&quot;, r.name, r.expectedTypeName, err)
				}
			}
			return nil
		}
	}
}
</code></pre>
<p>那么在实现时，如 deploymentinformer,会实现 Listfunc和 watchfunc，这其实就是clientset中的操作方法，也是就list与watch</p>
<pre><code class="language-go">func NewFilteredDeploymentInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {
	return cache.NewSharedIndexInformer(
		&amp;cache.ListWatch{
			ListFunc: func(options metav1.ListOptions) (runtime.Object, error) {
				if tweakListOptions != nil {
					tweakListOptions(&amp;options)
				}
				return client.AppsV1().Deployments(namespace).List(context.TODO(), options)
			},
			WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {
				if tweakListOptions != nil {
					tweakListOptions(&amp;options)
				}
				return client.AppsV1().Deployments(namespace).Watch(context.TODO(), options)
			},
		},
		&amp;appsv1.Deployment{},
		resyncPeriod,
		indexers,
	)
}
</code></pre>
<p><code>tools/cache/controller.go</code> 是存储controller的配置及实现。</p>
<pre><code class="language-go">type Config struct {
	Queue // 对象的队列，必须为DeltaFIFO
	ListerWatcher // 这里能够监视并列出对象的一些信息，这个对象接受process函数的弹出
	// Something that can process a popped Deltas.
	Process ProcessFunc // 处理Delta的弹出
    // 对象类型，这个controller期待的处理类型，其apiServer与kind必须正确，即，GVR必须正确
	ObjectType runtime.Object
        // FullResyncPeriod是每次重新同步的时间间隔
	FullResyncPeriod time.Duration
        // type ShouldResyncFunc func() bool
    // 返回值nil或true，则表示reflector继续同步
	ShouldResync ShouldResyncFunc
    RetryOnError bool // 标志位，true时，在process()返回错误时重新排列对象
	// Called whenever the ListAndWatch drops the connection with an error.
    // 断开连接是出现错误调用这个函数处理
	WatchErrorHandler WatchErrorHandler
	// WatchListPageSize is the requested chunk size of initial and relist watch lists.
	WatchListPageSize int64
}
</code></pre>
<p>实现这个接口</p>
<pre><code class="language-go">type controller struct {
	config         Config
	reflector      *Reflector
	reflectorMutex sync.RWMutex
	clock          clock.Clock
}
</code></pre>
<p><code>New()</code> 为给定controller 配置的设置，即为上面的config struct，用来初始化controller对象</p>
<p><code>NewInformer()</code> ：返回一个store（保存数据的最终接口）和一个用于store的controller，同时提供事件的通知(crud)等</p>
<p><code>NewIndexerInformer()</code>：返回一个索引与一个用于索引填充的控制器</p>
<p>控制器的run()的功能实现</p>
<pre><code class="language-go">func (c *controller) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash() // 延迟销毁
	go func() {  // 信号处理，用于线程管理
		&lt;-stopCh
		c.config.Queue.Close()
	}() 
	r := NewReflector(  // 初始化Reflector
		c.config.ListerWatcher, // ls
		c.config.ObjectType,
		c.config.Queue,
		c.config.FullResyncPeriod,
	)
	r.ShouldResync = c.config.ShouldResync // 配置是否应该继续同步
	r.WatchListPageSize = c.config.WatchListPageSize
	r.clock = c.clock
	if c.config.WatchErrorHandler != nil { // 断开连接错误处理
		r.watchErrorHandler = c.config.WatchErrorHandler
	}

	c.reflectorMutex.Lock()
	c.reflector = r
	c.reflectorMutex.Unlock()

	var wg wait.Group

	wg.StartWithChannel(stopCh, r.Run) // 这里是真正的运行。
    // processLoop() 是DeltaFIFO的消费者方法
	wait.Until(c.processLoop, time.Second, stopCh) // 消费队列的数据
	wg.Wait()
}
</code></pre>
<h3 id="总结">总结</h3>
<p>在controller的初始化时就初始化了Reflector， controller.Run里面Reflector是结构体初始化时的Reflector，主要作用是watch指定的资源，并且将变化同步到本地的<code>store</code>中。</p>
<p>Reflector接着执行ListAndWatch函数，ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。首先会将资源版本号设置为0，<code>list()</code>可能会导致本地的缓存相对于etcd里面的内容存在延迟，<code>Reflector</code>会通过<code>watch</code>的方法将延迟的部分补充上，使得本地缓存数据与etcd的数据保持一致。</p>
<p><code>controller.Run</code>函数还会调用processLoop函数，processLoop通过调用HandleDeltas，再调用distribute，processorListener.add最终将不同更新类型的对象加入<code>processorListener</code>的channel中，供processorListener.Run使用。</p>
<h2 id="delta-fifo">Delta FIFO</h2>
<p>通过下图可以看出，<code>Delta FIFO</code>  是位于Reflector中的一个FIFO队列，那么 <code>Delta FIFO</code> 究竟是什么，让我们来进一步深剖。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1iI8uFsPRBY5m_g_WW4huMQ.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图源于：https://miro.medium.com/max/700/1*iI8uFsPRBY5m_g_WW4huMQ.png</center>
<p>在代码中的注释可以看到一些信息，根据信息可以总结出</p>
<ul>
<li>Delta FIFO 是一个生产者-消费者的队列，生产者是 <code>Reflector</code>，消费者是 <code>Pop()</code></li>
<li>与传统的FIFO有两点不同
<ul>
<li>Delta FIFO</li>
</ul>
</li>
</ul>
<p>Delta FIFO也是实现了 Queue以及一些其他 interface 的类，</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220520174110756.png" alt="image-20220520174110756" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<pre><code class="language-go">type DeltaFIFO struct {
	lock sync.RWMutex  // 一个读写锁，保证线程安全
	cond sync.Cond
	items map[string]Deltas // 存放的类型是一个key[string] =》 value[Delta] 类型的数据
	queue []string  // 用于存储item的key，是一个fifo
	populated bool // populated 是用来标记首次被加入的数据是否被变动
    initialPopulationCount int // 首次调用 replace() 的数量
	keyFunc KeyFunc
	knownObjects KeyListerGetter // 这里为indexer
	closed     bool       // 代表已关闭
	closedLock sync.Mutex
    emitDeltaTypeReplaced bool // 表示事件的类型，true为 replace(), false 为 sync()
}
</code></pre>
<p>那么delta的类型是，也就是说通常情况下，Delta为一个 <code>string[runtime.object]</code> 的对象</p>
<pre><code class="language-go">type Delta struct {
	Type   DeltaType // 这就是一个string
	Object interface{} // 之前API部分有了解到，API的类型大致为两类，runtime.Object和非结构化数据
}
</code></pre>
<p><a href="k8s.io/">apimachinery/pkg/runtime/interfaces.go</a></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220520182830431.png" alt="image-20220520182830431" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>那么此时，已经明白了Delta FIFO的结构，为一个Delta的队列，整个结构如下</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220520191654098.png" alt="image-20220520191654098" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="第一步创建一个delta-fifo">第一步创建一个Delta FIFO</h3>
<p>现在版本中，对创建Delta FIFO是通过函数 <code>NewDeltaFIFOWithOptions()</code></p>
<pre><code class="language-go">func NewDeltaFIFOWithOptions(opts DeltaFIFOOptions) *DeltaFIFO {
	if opts.KeyFunction == nil {
		opts.KeyFunction = MetaNamespaceKeyFunc // 默认的计算key的方法
	}
	f := &amp;DeltaFIFO{
		items:        map[string]Deltas{},
		queue:        []string{},
		keyFunc:      opts.KeyFunction,
		knownObjects: opts.KnownObjects,

		emitDeltaTypeReplaced: opts.EmitDeltaTypeReplaced,
	}
	f.cond.L = &amp;f.lock
	return f
}
</code></pre>
<h3 id="queueactionlockeddelta-fifo添加操作">queueActionLocked，Delta FIFO添加操作</h3>
<p>这里说下之前说道的，在追加时的操作 <code>queueActionLocked</code> ，如add update delete实际上走的都是这里</p>
<pre><code class="language-go">func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error {
	id, err := f.KeyOf(obj) // 计算key
	if err != nil {
		return KeyError{obj, err}
	}
	// 把新数据添加到DeltaFIFO中，Detal就是 动作为key，对象为值
    // item是DeltaFIFO中维护的一个 map[string]Deltas
	newDeltas := append(f.items[id], Delta{actionType, obj})
	newDeltas = dedupDeltas(newDeltas) // 去重，去重我们前面讨论过了

	if len(newDeltas) &gt; 0 {
		if _, exists := f.items[id]; !exists {
			f.queue = append(f.queue, id)
		} // 不存在则添加
		f.items[id] = newDeltas
		f.cond.Broadcast()
	} else {
		delete(f.items, id) // 这里走不到，因为添加更新等操作用newDelta是1
        // 源码中也说要忽略这里
	}
	return nil
}
</code></pre>
<p>在FIFO继承的Stroe的方法中，如，Add, Update等都是需要去重的，去重的操作是通过对比最后一个和倒数第二个值</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220520222611473.png" alt="image-20220520222611473" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<pre><code class="language-go">func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error {
	id, err := f.KeyOf(obj)
	if err != nil {
		return KeyError{obj, err}
	}

	newDeltas := append(f.items[id], Delta{actionType, obj})
	newDeltas = dedupDeltas(newDeltas)
...
</code></pre>
<p>在函数 <code>dedupDeltas()</code> 中实现的这个</p>
<pre><code class="language-go">// re-listing and watching can deliver the same update multiple times in any
order. This will combine the most recent two deltas if they are the same.
func dedupDeltas(deltas Deltas) Deltas {
	n := len(deltas)
	if n &lt; 2 {
		return deltas
	}
    a := &amp;deltas[n-1] // 如 [1,2,3,4] a=4
	b := &amp;deltas[n-2] // b=3,这里两个值其实为事件
	if out := isDup(a, b); out != nil {
		d := append(Deltas{}, deltas[:n-2]...)
		return append(d, *out)
	}
	return deltas
}
</code></pre>
<p>如果b对象的类型是 <code>DeletedFinalStateUnknown</code> 也会认为是一个旧对象被删除，这里在去重时也只是对删除的操作进行去重。</p>
<pre><code class="language-go">// tools/cache/delta_fifo.go
func isDup(a, b *Delta) *Delta {
	if out := isDeletionDup(a, b); out != nil {
		return out
	}
	// TODO: Detect other duplicate situations? Are there any?
	return nil
}
// keep the one with the most information if both are deletions.
func isDeletionDup(a, b *Delta) *Delta {
	if b.Type != Deleted || a.Type != Deleted {
		return nil
	}
	// Do more sophisticated checks, or is this sufficient?
	if _, ok := b.Object.(DeletedFinalStateUnknown); ok {
		return a
	}
	return b
}
</code></pre>
<p><strong>为什么需要去重？什么情况下需合并</strong></p>
<p>代码中开发者给我们留了一个TODO</p>
<blockquote>
<p>TODO: is there anything other than deletions that need deduping?</p>
</blockquote>
<ul>
<li>取决于Detal FIFO 生产-消费延迟
<ul>
<li>当在一个资源的创建时，其状态会频繁的更新，如 Creating，Runinng等，这个时候会出现大量写入FIFO中的数据，但是在消费端可能之前的并未消费完。</li>
<li>在上面那种情况下，以及Kubernetes 声明式 API 的设计，其实多余的根本不关注，只需要最后一个动作如Running，这种情况下，多个内容可以合并为一个步骤</li>
</ul>
</li>
<li>然而在代码中，去重仅仅是在Delete状态生效，显然这不可用；那么结合这些得到：
<ul>
<li>在一个工作时间窗口内，如果对于删除操作来说发生多次，与发生一次实际上没什么区别，可以去重</li>
<li>但在更新于新增操作时，实际上在对于声明式 API 的设计个人感觉是完全可以做到去重操作。
<ul>
<li>同一个时间窗口内多次操作，如更新，实际上Kubernetes应该只关注最终状态而不是命令式？</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="compute-key">Compute Key</h3>
<p>上面大概对一些Detal FIFO的逻辑进行了分析，那么对于Detal FIFO如何去计算，也就是说 <code>MetaNamespaceKeyFunc</code> ，这个是默认的KeyFunc，作用是计算Detals中的唯一key。</p>
<pre><code class="language-go">func MetaNamespaceKeyFunc(obj interface{}) (string, error) {
	if key, ok := obj.(ExplicitKey); ok {  // 显示声明的则为这个值
		return string(key), nil
	}
	meta, err := meta.Accessor(obj) // 那么使用Accessor,每一个资源都会实现这个Accessor
	if err != nil {
		return &quot;&quot;, fmt.Errorf(&quot;object has no meta: %v&quot;, err)
	}
	if len(meta.GetNamespace()) &gt; 0 {
		return meta.GetNamespace() + &quot;/&quot; + meta.GetName(), nil
	}
	return meta.GetName(), nil
}
</code></pre>
<p><code>ObjectMetaAccessor</code> 每个Kubernetes资源都会实现这个对象，如Deployment</p>
<pre><code class="language-go">// accessor interface
type ObjectMetaAccessor interface {
	GetObjectMeta() Object
}

// 会被ObjectMeta所实现
func (obj *ObjectMeta) GetObjectMeta() Object { return obj }
// 而每一个资源都会继承这个 ObjectMeta，如 ClusterRole

type ClusterRole struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;protobuf:&quot;bytes,1,opt,name=metadata&quot;`
</code></pre>
<p>那么这个Deltas的key则为集群类型的是资源本身的名字，namespace范围的则为 <code>meta.GetNamespace() + &quot;/&quot; + meta.GetName()</code>，可以在上面代码中看到，这样就可以给Detal生成了一个唯一的key</p>
<h3 id="keyof用于计算对象的key">keyof，用于计算对象的key</h3>
<pre><code class="language-go">func (f *DeltaFIFO) KeyOf(obj interface{}) (string, error) {
	if d, ok := obj.(Deltas); ok {
		if len(d) == 0 { // 长度为0的时候是一个初始的类型
			return &quot;&quot;, KeyError{obj, ErrZeroLengthDeltasObject}
		}
		obj = d.Newest().Object // 用最新的一个对象，如果为空则是nil
	}
	if d, ok := obj.(DeletedFinalStateUnknown); ok {  
		return d.Key, nil // 到了这里，之前提到过，是一个过期的值将会被删除
	}
	return f.keyFunc(obj) // 调用具体的key计算函数
}
</code></pre>
<h2 id="indexer">Indexer</h2>
<p>indexer 在整个 client-go 架构中提供了一个具有线程安全的数据存储的对象存储功能；对于Indexer这里会分析下对应的架构及使用方法。</p>
<p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/index.go" target="_blank"
   rel="noopener nofollow noreferrer" >client-go/tools/cache/index.go</a> 中可以看到 indexer是一个实现了<code>Store</code> 的一个interface</p>
<pre><code class="language-go">type Indexer interface {
    // 继承了store，拥有store的所有方法
	Store
	// 返回indexname的obj的交集
	Index(indexName string, obj interface{}) ([]interface{}, error)
	// 通过对 indexName，indexedValue与之相匹配的集合
	IndexKeys(indexName, indexedValue string) ([]string, error)
    // 给定一个indexName 返回所有的indexed
	ListIndexFuncValues(indexName string) []string
	// 通过indexname，返回与indexedvalue相关的 obj
	ByIndex(indexName, indexedValue string) ([]interface{}, error)
	// 返回所有的indexer
	GetIndexers() Indexers
	AddIndexers(newIndexers Indexers) error
}
</code></pre>
<p>实际上对他的实现是一个 cache，cache是一个KeyFunc与ThreadSafeStore实现的indexer，有名称可知具有线程安全的功能</p>
<pre><code class="language-go">type cache struct {
	cacheStorage ThreadSafeStore
	keyFunc KeyFunc
}
</code></pre>
<p>既然index继承了Store那么，也就是 <code>ThreadSafeStore</code> 必然实现了Store，这是一个基础保证</p>
<pre><code class="language-go">type ThreadSafeStore interface {
	Add(key string, obj interface{})
	Update(key string, obj interface{})
	Delete(key string)
	Get(key string) (item interface{}, exists bool)
	List() []interface{}
	ListKeys() []string
	Replace(map[string]interface{}, string)
	Index(indexName string, obj interface{}) ([]interface{}, error)
	IndexKeys(indexName, indexKey string) ([]string, error)
	ListIndexFuncValues(name string) []string
	ByIndex(indexName, indexKey string) ([]interface{}, error)
	GetIndexers() Indexers
	AddIndexers(newIndexers Indexers) error
	Resync() error // Resync is a no-op and is deprecated
}
// KeyFunc是一个生成key的函数，给一个对象，返回一个key值
type KeyFunc func(obj interface{}) (string, error)
</code></pre>
<p>那么这个indexer structure可以通过图来很直观的看出来</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220522003635661.png" alt="image-20220522003635661" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="cache的结构">cache的结构</h3>
<p>cache中会出现三种数据结构，也可以成为三种名词，为 <code>Index </code>, <code>Indexers</code> , <code>Indices</code></p>
<pre><code class="language-go">type Index map[string]sets.String
type Indexers map[string]IndexFunc
type Indices map[string]Index
</code></pre>
<p>可以看出：</p>
<ul>
<li><code>Index</code> 映射到对象，<code>sets.String</code> 也是在API中定义的数据类型 <code>[string]Struct{}</code>，</li>
<li><code>Indexers</code> 是这个 <code>Index</code> 的 <code>IndexFunc</code> , 是一个如何计算Index的keyname的函数</li>
<li><code>Indices</code> 通过Index 名词拿到对应的对象</li>
</ul>
<p>这个名词的概念如下，通过图来了解会更加清晰</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220522172028443.png" alt="image-20220522172028443" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="从创建开始">从创建开始</h3>
<p>创建一个cache有两种方式，一种是指定indexer，一种是默认indexer</p>
<pre><code class="language-go">// NewStore returns a Store implemented simply with a map and a lock.
func NewStore(keyFunc KeyFunc) Store {
	return &amp;cache{
		cacheStorage: NewThreadSafeStore(Indexers{}, Indices{}),
		keyFunc:      keyFunc,
	}
}

// NewIndexer returns an Indexer implemented simply with a map and a lock.
func NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer {
	return &amp;cache{
		cacheStorage: NewThreadSafeStore(indexers, Indices{}),
		keyFunc:      keyFunc,
	}
}
</code></pre>
<h3 id="更新操作">更新操作</h3>
<p>在indexer中的更新操作（诸如 <code>add</code> , <code>update</code> ），实际上操作的是 <code>updateIndices</code>， 通过在代码可以看出</p>
<p><a href="https://github.com/kubernetes/client-go/blob/master/tools/cache/thread_safe_store.go" target="_blank"
   rel="noopener nofollow noreferrer" >tools/cache/thread_safe_store.go</a> 的 77行起，那么就来看下 <code>updateIndices()</code> 具体做了什么</p>
<pre><code class="language-go">func (c *threadSafeMap) updateIndices(oldObj interface{}, newObj interface{}, key string) {
	// 在操作时，如果有旧对象，需要先删除
	if oldObj != nil {
		c.deleteFromIndices(oldObj, key)
	}
    // 先对整个indexer遍历，拿到index name与 index function
	for name, indexFunc := range c.indexers {
        // 通过index function，计算出对象的indexed name
		indexValues, err := indexFunc(newObj)
		if err != nil {
			panic(fmt.Errorf(&quot;unable to calculate an index entry for key %q on index %q: %v&quot;, key, name, err))
		}
        // 接下来通过遍历的index name 拿到这个index的对象
		index := c.indices[name]
		if index == nil { // 确认这个index是否存在，
            index = Index{} // 如果不存在将一个Index{}初始化
			c.indices[name] = index
		}
		// 通过计算出的indexed name来拿到对应的 set of object
		for _, indexValue := range indexValues {
			set := index[indexValue]
			if set == nil {
                // 如果这个set不存在，则初始化这个set
				set = sets.String{}
				index[indexValue] = set
			}
			set.Insert(key) // 然后将key插入set中
		}
	}
}
</code></pre>
<p>那么通过上面可以了解到了 <code>updateIndices</code> 的逻辑，那么通过对更新函数分析来看看他具体做了什么？这里是add函数，通过一段代码模拟操作来熟悉结构</p>
<pre><code class="language-go">testIndexer := &quot;testIndexer&quot;
testIndex := &quot;testIndex&quot;

indexers := cache.Indexers{
    testIndexer: func(obj interface{}) (strings []string, e error) {
        indexes := []string{testIndex} // index的名词
        return indexes, nil
    },
}

indices := cache.Indices{}
store := cache.NewThreadSafeStore(indexers, indices)

fmt.Printf(&quot;%#v\n&quot;, store.GetIndexers())

store.Add(&quot;retain&quot;, &quot;pod--1&quot;)
store.Add(&quot;delete&quot;, &quot;pod--2&quot;)
store.Update(&quot;retain&quot;, &quot;pod-3&quot;)
//lists := store.Update(&quot;retain&quot;, &quot;pod-3&quot;)
lists := store.List()
for _, item := range lists {
    fmt.Println(item)
}
</code></pre>
<p>这里是对add操作以及对<code>updateIndices()</code> 进行操作</p>
<pre><code class="language-go">// threadSafe.go
func (c *threadSafeMap) Add(key string, obj interface{}) {
	c.lock.Lock()
	defer c.lock.Unlock()
	oldObject := c.items[key] // 这个item就是存储object的地方, 为空
	c.items[key] = obj // 这里已经添加了新的值
	c.updateIndices(oldObject, obj, key) // 转至updateIndices
}

// updateIndices
func (c *threadSafeMap) updateIndices(oldObj interface{}, newObj interface{}, key string) {
	// 就当是新创建的，这里是空的忽略
	if oldObj != nil {
		c.deleteFromIndices(oldObj, key)
	}
    // 这个时候拿到的就是 name=testKey function=testIndexer
	for name, indexFunc := range c.indexers {
        // 通过testIndexer对testKey计算出的结果是 []string{testIndexer}
		indexValues, err := indexFunc(newObj)
		if err != nil {
			panic(fmt.Errorf(&quot;unable to calculate an index entry for key %q on index %q: %v&quot;, key, name, err))
		}
		index := c.indices[name] 
		if index == nil { 
            index = Index{} 
            // 因为假设为空了，故到这里c.indices[testIndexer]= Index{}
			c.indices[name] = index 
		}
		for _, indexValue := range indexValues {
            // indexValue=testIndexer
            // set := c.index[name] = c.indices[testIndexer]Index{}
			set := index[indexValue]
			if set == nil {
				set = sets.String{}
				index[indexValue] = set
			}
			set.Insert(key) // 到这里就为set=indices[testIndexer]Index{}
		}
	}
}
</code></pre>
<p>总结一下，到这里，可以很明显的看出来，indexer中的三个概念是什么了，前面如果没有看明白话</p>
<ul>
<li><code>Index</code>：通过indexer计算出key的名称，值为对应obj的一个集合，可以理解为索引的数据结构
<ul>
<li>比如说 <code>Pod:{&quot;nginx-pod1&quot;: v1.Pod{Name:Nginx}}</code></li>
</ul>
</li>
<li><code>Indexers</code> ：这个很简单，就是，对于Index中如何计算每个key的名称；可以理解为分词器，索引的过程</li>
<li><code>Indices</code> 通过Index 名词拿到对应的对象，是Index的集合；是将原始数据Item做了一个索引，可以理解为做索引的具体字段
<ul>
<li>比如说 <code>Indices[&quot;Pod&quot;]{&quot;nginx-pod1&quot;: v1.Pod{Name:Nginx}, &quot;nginx-pod2&quot;: v1.Pod{Name:Nginx}}</code></li>
</ul>
</li>
<li><code>Items</code>：实际上存储的在Indices中的<code>set.String{key:value}</code> ，中的 <code>key=value</code>
<ul>
<li>例如：<code>Item:{&quot;nginx-pod1&quot;: v1.Pod{Name:Nginx}, &quot;coredns-depoyment&quot;: App.Deployment{Name:coredns}}</code></li>
</ul>
</li>
</ul>
<h3 id="删除操作">删除操作</h3>
<p>对于删除操作，在最新版本中是使用了 <code>updateIndices</code> 就是 add update delete全都是相同的方法操作，对于旧版包含1.19- 是单独的一个操作</p>
<pre><code>// v1.2+
func (c *threadSafeMap) Delete(key string) {
	c.lock.Lock()
	defer c.lock.Unlock()
	if obj, exists := c.items[key]; exists {
		c.updateIndices(obj, nil, key)
		delete(c.items, key)
	}
}
// v1.19-
func (c *threadSafeMap) Delete(key string) {
	c.lock.Lock()
	defer c.lock.Unlock()
	if obj, exists := c.items[key]; exists {
		c.deleteFromIndices(obj, key)
		delete(c.items, key)
	}
}
</code></pre>
<h3 id="indexer使用">indexer使用</h3>
<p>上面了解了indexer概念，可以通过写代码来尝试使用一些indexer</p>
<pre><code class="language-go">package main

import (
	&quot;fmt&quot;

	appsV1 &quot;k8s.io/api/apps/v1&quot;
	metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
	&quot;k8s.io/client-go/tools/cache&quot;
)

func main() {

	indexers := cache.Indexers{
		&quot;getDeplyment&quot;: func(obj interface{}) (strings []string, e error) {
			d, ok := obj.(*appsV1.Deployment)
			if !ok {
				return []string{}, nil
			}
			return []string{d.Name}, nil
		},
		&quot;getDaemonset&quot;: func(obj interface{}) (strings []string, e error) {
			d, ok := obj.(*appsV1.DaemonSet)
			if !ok {
				return []string{}, nil
			}
			return []string{d.Name}, nil
		},
	}

	// 第一个参数是计算set内的key的名称 就是map[string]sets.String的这个strings的名称/namespace/resorcename
	// 第二个参数是计算index即外部的key的名称
	indexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, indexers)

	deployment := &amp;appsV1.Deployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      &quot;nginx-deplyment&quot;,
			Namespace: &quot;test&quot;,
		},
	}

	daemonset := &amp;appsV1.DaemonSet{
		ObjectMeta: metav1.ObjectMeta{
			Name:      &quot;firewall-daemonset&quot;,
			Namespace: &quot;test&quot;,
		},
	}

	daemonset2 := &amp;appsV1.DaemonSet{
		ObjectMeta: metav1.ObjectMeta{
			Name:      &quot;etcd-daemonset&quot;,
			Namespace: &quot;default&quot;,
		},
	}

	indexer.Add(deployment)
	indexer.Add(daemonset)
	indexer.Add(daemonset2)

	// 第一个参数是索引器
	// 第二个参数是所引起做索引的字段
	lists, _ := indexer.ByIndex(&quot;getDaemonset&quot;, &quot;etcd-daemonset&quot;)
	for _, item := range lists {
		switch item.(type) {
		case *appsV1.Deployment:
			fmt.Println(item.(*appsV1.Deployment).Name)
		case *appsV1.DaemonSet:
			fmt.Println(item.(*appsV1.DaemonSet).Name)
		}
	}
}
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>如何通过源码编译Kubernetes</title>
      <link>https://www.oomkill.com/2022/05/ch11-code-compile/</link>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/05/ch11-code-compile/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="本地构建">本地构建</h2>
<h3 id="选择要构建的版本">选择要构建的版本</h3>
<pre><code>git checkout tags/v1.19.5
</code></pre>
<h3 id="将依赖包复制到对应路径下">将依赖包复制到对应路径下</h3>
<pre><code>cp staging/src/k8s.io vendor/
</code></pre>
<h3 id="调整makefile">调整makefile</h3>
<p>在windows上编译的克隆下可能文件编码变了，需要手动修改下文件编码。比如说出现 <code>\r not found</code> 类似关键词时</p>
<p>这里转换编码使用了 dos2unix，需要提前安装下</p>
<pre><code>apt install dos2unix
</code></pre>
<p>转换原因是因为对于bash 脚本执行识别不了windows的换行</p>
<pre><code>find . -name '*.sh' -exec dos2unix {} \;
</code></pre>
<p>然后将 <code>build/root/</code> 的文件复制到项目根目录</p>
<pre><code>cp build/root/Makefile* ./
</code></pre>
<h3 id="编译">编译</h3>
<p>查看帮助 <code>make help</code></p>
<p>编译 <code>make all WHAT=cmd/kube-apiserver GOFLAGS=-v</code></p>
<p><code>WHAT=cmd/kube-apiserver</code> 为仅编译单一组件，<code>all</code> 为所有的组件</p>
<p>还可以增加其他的一些环境变量 <code>KUBE_BUILD_PLATFORMS=</code> 如编译的平台</p>
<p>更多的可以 <code>make help</code> 查看帮助</p>
<h3 id="编译中问题">编译中问题</h3>
<p><strong>Makefile:93: recipe for target &lsquo;all&rsquo; failed</strong></p>
<pre><code>!!! [0515 21:32:52] Call tree:
!!! [0515 21:32:52]  1: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:717 kube::golang::build_some_binaries(...)
!!! [0515 21:32:52]  2: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:861 kube::golang::build_binaries_for_platform(...)
!!! [0515 21:32:52]  3: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0515 21:32:52] Call tree:
!!! [0515 21:32:52]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0515 21:32:52] Call tree:
!!! [0515 21:32:52]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
Makefile:93: recipe for target 'all' failed
</code></pre>
<p>这里看报错根本不知道发生什么问题，使用 <code>strace</code> 追送了下，很明显看到是没有gcc</p>
<p>cgo: exec gcc: exec: &ldquo;gcc&rdquo;: executable file not found in $PATH</p>
<pre><code>rt_sigprocmask(SIG_BLOCK, [HUP INT QUIT TERM XCPU XFSZ], NULL, 8) = 0
clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7fbf45410a10) = 17890
rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
wait4(-1, +++ [0515 21:34:40] Building go targets for linux/amd64:
    cmd/kubelet
k8s.io/kubernetes/vendor/github.com/opencontainers/runc/libcontainer/system
k8s.io/kubernetes/vendor/github.com/mindprince/gonvml
# k8s.io/kubernetes/vendor/github.com/opencontainers/runc/libcontainer/system
cgo: exec gcc: exec: &quot;gcc&quot;: executable file not found in $PATH
# k8s.io/kubernetes/vendor/github.com/mindprince/gonvml
cgo: exec gcc: exec: &quot;gcc&quot;: executable file not found in $PATH
!!! [0515 21:34:42] Call tree:
!!! [0515 21:34:42]  1: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:717 kube::golang::build_some_binaries(...)
!!! [0515 21:34:42]  2: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:861 kube::golang::build_binaries_for_platform(...)
!!! [0515 21:34:42]  3: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0515 21:34:42] Call tree:
!!! [0515 21:34:42]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
!!! [0515 21:34:42] Call tree:
!!! [0515 21:34:42]  1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...)
[{WIFEXITED(s) &amp;&amp; WEXITSTATUS(s) == 1}], 0, NULL) = 17890
--- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=17890, si_uid=0, si_status=1, si_utime=0, si_stime=0} ---
rt_sigreturn({mask=[]})                 = 17890
openat(AT_FDCWD, &quot;/usr/share/locale/C.UTF-8/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale/C.utf8/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale/C/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale-langpack/C.UTF-8/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale-langpack/C.utf8/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
openat(AT_FDCWD, &quot;/usr/share/locale-langpack/C/LC_MESSAGES/make.mo&quot;, O_RDONLY) = -1 ENOENT (No such file or directory)
fstat(1, {st_mode=S_IFCHR|0640, st_rdev=makedev(4, 1), ...}) = 0
ioctl(1, TCGETS, {B38400 opost isig icanon echo ...}) = 0
write(1, &quot;Makefile:93: recipe for target '&quot;..., 44Makefile:93: recipe for target 'all' failed
) = 44
write(2, &quot;make: *** [all] Error 1\n&quot;, 24make: *** [all] Error 1
) = 24
rt_sigprocmask(SIG_BLOCK, [HUP INT QUIT TERM XCPU XFSZ], NULL, 8) = 0
rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
chdir(&quot;/mnt/d/src/go_work/src/kubernetes&quot;) = 0
close(1)                                = 0
exit_group(2)                           = ?
+++ exited with 2 +++
</code></pre>
<p><strong>修改后编译问题可以明显看出是哪里</strong></p>
<p>如尝试增加一种资源类型后编译，这种类型的错误可以根据报错提示进行修改</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20220516172727798-791337963.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<pre><code>+++ [0515 21:47:59] Building go targets for linux/amd64:
    cmd/kube-apiserver
k8s.io/kubernetes/vendor/k8s.io/api/apps/v1
# k8s.io/kubernetes/vendor/k8s.io/api/apps/v1
vendor/k8s.io/api/apps/v1/register.go:48:3: cannot use &amp;StateDeploy{} (type *StateDeploy) as type runtime.Object in argument to scheme.Ad
dKnownTypes:
        *StateDeploy does not implement runtime.Object (missing DeepCopyObject method)
!!! [0515 21:48:01] Call tree:
!!! [0515 21:48:01]  1: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:706 k
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>深入理解kubernetes API</title>
      <link>https://www.oomkill.com/2022/05/ch02-kubernetes-api/</link>
      <pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/05/ch02-kubernetes-api/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="apiserver">APIServer</h2>
<p>在kubernetes架构概念层面上，Kubernetes由一些具有不同角色的服务节点组成。而master的控制平面由 <code>Apiserver</code>  <code>Controller-manager</code> 和 <code>Scheduler</code> 组成。</p>
<p><code>Apiserver</code> 从概念上理解可以分为 <code>api</code> 和 <code>object</code> 的集合，<code>api</code> 可以理解为，处理读写请求来修改相应 <code>object</code> 的组件；而 <code>object</code> 可以表示为 kubernetes 对象，如 <code>Pod</code>， <code>Deployment</code> 等 。</p>
<h2 id="基于声明式的api">基于声明式的API</h2>
<p>在命令式 API 中，会直接发送要执行的命令，例如：<em>运行</em>、<em>停止</em> 等命令。在声明式API 中，将声明希望系统执行的操作，系统将不断将自身状态朝希望状态改变。</p>
<h3 id="为什么使用声明式">为什么使用声明式</h3>
<p>在分布式系统中，任何组件随时都可能发生故障，当组件故障恢复时，需要明白自己需要做什么。在使用命令式时，出现故障的组件可能在异常时错过调用，并且在恢复时需要其他外部组件进行干预。而声明式仅需要在恢复时确定当前状态以确定他需要做什么。</p>
<h3 id="external-apis">External APIs</h3>
<p>在kubernetes中，控制平面是透明的，及没有internal APIs。这就意味着Kubernetes组件间使用相同的API交互。这里通过一个例子来说明外部APIs与声明式的关系。</p>
<p>例如，创建一个Pod对象，<code>Scheduler</code> 会监听 API来完成创建，创建完成后，调度程序不会命令被分配节点启动Pod。而在kubelet端，发现pod具有与自己相同的一些信息时，会监听pod状态。如改变kubelet则修改状态，如果删除掉Pod（对象资源不存在与API中），那么kubelet则将终止他。</p>
<h3 id="为什么不使用internal-api">为什么不使用Internal API</h3>
<p>使用External API可以使kubernetes组件都使用相同的API，使得kubernetes具有可扩展性和可组合性。对于kubernetes中任何默认组件，如不足满足需求时，都可以更换为使用相同API的组件。</p>
<p>另外，外部API还可轻松的使用公共API来扩展kubernetes的功能</p>
<h2 id="api资源">API资源</h2>
<p>从广义上讲，kubernetes对象可以用任何数据结构来表示，如：资源实例、配置（审计策略）或持久化实体（Pod）；在使用中，常见到的就是对应YAML的资源清单。转换出来就是RESTful地址，那么应该怎么理解这个呢？即，对资源的动作（操作）如图所示。但如果需要了解Kubernetes API需要掌握一些概念才可继续。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220513221304830.png" alt="image-20220513221304830" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="group">Group</h3>
<p>出于对kubernetes扩展性的原因，将资源类型分为了API组进行独立管理，可以通过 <code>kubectl api-resources</code>查看。在代码部分为 <code>vendor/k8s.io/api</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220513215038910.png" alt="image-20220513215038910" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>也可以通过 <code>kubectl xxx -v 6</code> 来查看 <code>kubectl</code> 命令进行了那些API调用</p>
<pre><code>$ kubectl get pods -v 6
I0513 21:54:33.250752   38661 round_trippers.go:444] GET http://localhost:8080/api?timeout=32s 200 OK in 1 milliseconds
I0513 21:54:33.293831   38661 round_trippers.go:444] GET http://localhost:8080/apis?timeout=32s 200 OK in 0 milliseconds
I0513 21:54:33.299741   38661 round_trippers.go:444] GET http://localhost:8080/apis/discovery.k8s.io/v1beta1?timeout=32s 200 OK in 3 milliseconds
I0513 21:54:33.301097   38661 round_trippers.go:444] GET http://localhost:8080/apis/autoscaling/v2beta1?timeout=32s 200 OK in 4 milliseconds
I0513 21:54:33.301128   38661 round_trippers.go:444] GET http://localhost:8080/apis/authorization.k8s.io/v1beta1?timeout=32s 200 OK in 3 milliseconds
I0513 21:54:33.301222   38661 round_trippers.go:444] GET http://localhost:8080/apis/rbac.authorization.k8s.io/v1beta1?timeout=32s 200 OK in 1 milliseconds
I0513 21:54:33.301238   38661 round_trippers.go:444] GET http://localhost:8080/apis/authentication.k8s.io/v1beta1?timeout=32s 200 OK in 4 milliseconds
I0513 21:54:33.301280   38661 round_trippers.go:444] GET http://localhost:8080/apis/certificates.k8s.io/v1beta1?timeout=32s 200 OK in 4 milliseconds
....
No resources found in default namespace.
</code></pre>
<h3 id="kind">Kind</h3>
<p>在<code>kubectl api-resources</code> 中可以看到，有Kind字段，大部分人通常会盲目的 <code>kubectl apply</code> ,这导致了，很多人以为 <code>kind</code> 实际上为资源名称，<code>Pod</code> ，<code>Deployment</code> 等。</p>
<p>根据 <a href="https://github.com/kubernetes/community/blob/7f3f3205448a8acfdff4f1ddad81364709ae9b71/contributors/devel/sig-architecture/api-conventions.md#types-kinds" target="_blank"
   rel="noopener nofollow noreferrer" >api-conventions.md</a> 的说明，<strong>Kind</strong> 是对象模式，包含三种类型：</p>
<ul>
<li>Object，代表系统中持久化数据资源，如，<code>Service</code>, <code>Namespace</code>, <code>Pod</code>等</li>
<li>List，是一个或多个资源的集合，通常以List结尾，如 <code>DeploymentList</code></li>
<li>对Object的操作和和非持久化实体，如，当发生错误时会返回“status”类型，并不会持久化该数据。</li>
</ul>
<h3 id="object">Object</h3>
<p>对象是Kubernetes中持久化的实体，也就是保存在etcd中的数据；如：<code>Replicaset</code> , <code>Configmap</code> 等。这个对象代表的了集群期望状态和实际状态。</p>
<blockquote>
<p>例如：创建了Pod，kubernetes集群会调整状态，直到相应的容器在运行</p>
</blockquote>
<p>Kubernetes资源又代表了对象，对象必须定义一些<a href="https://github.com/kubernetes/community/blob/7f3f3205448a8acfdff4f1ddad81364709ae9b71/contributors/devel/sig-architecture/api-conventions.md#resources" target="_blank"
   rel="noopener nofollow noreferrer" >字段</a>：</p>
<ul>
<li>所有对象必须具有以下字段：
<ul>
<li>Kind</li>
<li>apiVersion</li>
</ul>
</li>
<li>metadata</li>
<li>spec：期望的状态</li>
<li>status：实际的状态</li>
</ul>
<h3 id="api-link">API Link</h3>
<p>前面讲到的 <code>kubectl api-resources</code> 展示的列表不能完整称为API资源，而是已知类型的kubernetes对象，要对展示这个API对象，需要了解其完整的周期。以 <code>kubectl get --raw /</code> 可以递归查询每个路径。</p>
<pre><code>kubectl get --raw /
{
  &quot;paths&quot;: [
    &quot;/api&quot;,
    &quot;/api/v1&quot;,
    &quot;/apis&quot;,
    &quot;/apis/&quot;,
    &quot;/apis/admissionregistration.k8s.io&quot;,
    &quot;/apis/admissionregistration.k8s.io/v1&quot;,
    &quot;/apis/admissionregistration.k8s.io/v1beta1&quot;,
    &quot;/apis/apiextensions.k8s.io&quot;,
    &quot;/apis/apiextensions.k8s.io/v1&quot;,
...
</code></pre>
<p>对于一个Pod来说，其查询路径就为 <code>/api/v1/namespaces/kube-system/pods/coredns-f9bbb4898-7zkbf</code></p>
<pre><code>kubectl get --raw /api/v1/namespaces/kube-system/pods/coredns-f9bbb4898-7zkbf|jq
	
kind: Pod
apiVersion: v1
metadata: {}
spec:{}
status: {}
</code></pre>
<p>但有一些资源对象也并非这种结构，如 <code>configMap</code> ，因其只是存储的数据，所以没有 <code>spec</code> 和 <code>status</code></p>
<pre><code>kubectl get --raw /api/v1/namespaces/kube-system/configmaps/coredns|jq

kind
apiVersion
metadata
data
</code></pre>
<h3 id="api组成">API组成</h3>
<p>一个API的组成为 一个 API 组<code>Group</code> , 一个版本 <code>Version</code> , 和一个资源 <code>Resource</code> ; 简称为 <code>GVR</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/API-server-gvr.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>转换为实际的http路径为：</p>
<ul>
<li><code>/api/{version}/namespaces/{namespace_name}/resourcesPlural/{actual_resources_name}</code></li>
<li><code>/api/v1/namespaces/default/pods/pods123</code></li>
</ul>
<p>而GVR中的R代表的是RESTful中的资源，转换为Kubernetes中资源应为 <code>Kind</code>，简称为 <code>GVK</code>，K在URI中表示在：</p>
<ul>
<li><code>/apis/{GROUP}/{VERSION}/namespaces/{namespace}/{KIND}</code></li>
</ul>
<h2 id="请求和处理">请求和处理</h2>
<p>这里讨论API请求和处理，API的一些数据结构位于 <code>k8s.io/api</code> ，并处理集群内部与外部的请求，而Apiserver 位于 <code>k8s.io/apiserver/pkg/server</code> 提供了http服务。</p>
<p>那么，当 HTTP 请求到达 Kubernetes API 时，实际上会发生什么？</p>
<ul>
<li>首先HTTP请求在 <code>DefaultBuildHandlerChain</code> （可以参考<code>k8s.io/apiserver/pkg/server/config.go</code>）中注册filter chain，过滤器允许并将相应的信息附加至 <code>ctx.RequestInfo</code>; 如身份验证的相应</li>
<li><code>k8s.io/apiserver/pkg/server/mux</code> 将其分配到对应的应用</li>
<li><code>k8s.io/apiserver/pkg/server/routes</code> 定义了REST与对应应用相关联</li>
<li><code>k8s.io/apiserver/pkg/endpoints/groupversion.go.InstallREST()</code> 接收上下文，从存储中传递请求的对象。</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220516171908228.png" alt="image-20220516171908228" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p>Reference</p>
<p><a href="https://iximiuz.com/en/posts/kubernetes-api-structure-and-terminology/" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes API 基础——资源、种类和对象</a></p>
<p><a href="https://networktechstudy.com/home/kubernetes-the-one-api" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes - 一个 API 统治</a></p>
<p><a href="https://flugel.it/infrastructure-as-code/building-kubernetes-operators-part-2/" target="_blank"
   rel="noopener nofollow noreferrer" >Design and implementation</a></p>
<p><a href="https://www.codeproject.com/Articles/5252640/Extending-the-Kubernetes-API" target="_blank"
   rel="noopener nofollow noreferrer" >Extending-the-Kubernetes-API</a></p>
<p><a href="https://cloud.redhat.com/blog/kubernetes-deep-dive-api-server-part-1" target="_blank"
   rel="noopener nofollow noreferrer" >Kubernetes 深入探讨</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>理解kubernetes listwatch机制原理</title>
      <link>https://www.oomkill.com/2021/12/ch05-listwatch-mechanism/</link>
      <pubDate>Sun, 12 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/12/ch05-listwatch-mechanism/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">overview</h2>
<p>kubernetes的设计里面大致上分为3部分：</p>
<ul>
<li>API驱动型的特点 (<code>API-driven</code>)</li>
<li>控制循环（<code>control loops</code>）与 条件触发 （<code>Level Trigger</code>）</li>
<li>API的可延伸性</li>
</ul>
<p>而正因为这些设计特性，才使得kubernetes工作非常稳定。</p>
<h2 id="什么是level-trigger与-edge-trigger">什么是Level Trigger与 Edge trigger</h2>
<p>看到网上有资料是这么解释两个属于的：</p>
<ul>
<li>
<p><strong>条件触发(level-trigger，也被称为水平触发)LT指</strong>： 只要满足条件，就触发一个事件(只要有数据没有被获取，就不断通知)。</p>
</li>
<li>
<p><strong>边缘触发(edge-trigger)ET</strong>: 每当状态变化时，触发一个事件。</p>
</li>
</ul>
<p>通过查询了一些资料，实际上也不明白这些究竟属于哪门科学中的理论，但是具体解释起来看的很明白。</p>
<p><strong>LEVEL TRIGGERING</strong>：当电流有两个级别，<code>VH</code> 和 <code>VL</code>。代表了两个触发事件的级别。如果将<code>VH</code> 设置为LED在正时钟。当电压为VH时，LED可以在该时间线任何时刻点亮。这称为<strong>LEVEL TRIGGERING</strong>，每当遇到<code>VH</code> 时间线就会触发事件。事件是在时间内的任何时刻开始，直到满足条件。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212232954599.png" alt="image-20211212232954599" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><strong>Edge TRIGGERING</strong>:</p>
<p>如图所示，会看到上升线与下降线，当事件在上升/下降边缘触发时（两个状态的交点），称为边缘触发（<strong>Edge TRIGGERING</strong>:）。</p>
<p>如果需要打开LED灯，则当时钟从<code>VL</code>转换到<code>VH</code>时才会亮起，而不是一家处在对应的时钟线上，仅仅是在过渡时亮起。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212233004721.png" alt="image-20211212233004721" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="为什么kubernetes使用level-trigger而不使用edge-trigger">为什么kubernetes使用Level Trigger而不使用Edge trigger</h2>
<p>如图所述，两种不同的设计模式，随着时间形状进行相应，当系统在由高转低，或由低转高时，系统处在关闭或者不可控的异常状态下，应如何触发对应的事件呢。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212233012534.png" alt="image-20211212233012534" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>换一种方式来来解释，比如说通过 加法运算，如下，i=3，当给I+4作为一个操作触发事件。</p>
<pre><code># let i=3
# let i+=4
# let i
# echo $i
7
</code></pre>
<p>当为<code>Edge trigger</code>时操作的情况下，将看到 <code>i+4</code> ,而在 <code>level trigger</code> 时看到的是 <code>i=7</code>。这里将会从``i+4` 一直到下一个信号的触发。</p>
<h3 id="信号的干扰">信号的干扰</h3>
<p>通常情况下，两者是没有区别的，但在大规模分布式网络环境中，有很多因素的影响下，任何都是不可靠的，在这种情况下会改变了我们对事件信号的感知。</p>
<p>如图所示，图为<code>Level Trigger</code>与<code>Edge trigger</code> 的信号发生模拟，在理想情况下，两者间并没有什么不同。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212233021583.png" alt="image-20211212233021583" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="一次中断场景">一次中断场景</h4>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212232713291.png" alt="image-20211212232713291" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>由图可知，<code>Edge trigger</code>当在恰当的时间点发生信号中断，会对整个流产生很大的影响，甚至改变了整个状态，对于较少的干扰并不会对有更好的结果，而单次的中断，使<code>Edge trigger</code>错过了从高到低的变化，而 <code>level trigger</code> 基本上保证了整个信号量的所有改变状态。</p>
<h4 id="两次中断的场景下">两次中断的场景下</h4>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212232747769.png" alt="image-20211212232747769" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>由图可看到，信号的上升和下降中如果存在了中断，<code>Edge trigger</code> 丢失了上升的信号，但最终状态是正确的。</p>
<p>在信号状态的两次变化时发生了两次中断，<code>Level Trigger</code>与<code>Edge trigger</code> 之间的区别很明显，<code>Edge trigger</code> 的信号错过了第一次上升，而<code>Level Trigger</code> 保持了最后观察到的状态，知道拿到了其他状态，这种模式保证了得到的信号基本的正确性，但是发生延迟到中断恢复后。</p>
<h4 id="通过运算来表示两种模式的变化情况">通过运算来表示两种模式的变化情况</h4>
<p>完整的信号</p>
<pre><code class="language-bash"># let i=2

# let i+1
# let i-=1
# let i+1

# echo $i
3
</code></pre>
<p><strong>Edge trigger</strong></p>
<pre><code class="language-bash"># let i=2

# let i+1  
(# let i-=1) miss this
# let i+1

# echo $i
4
</code></pre>
<h2 id="如何使理想状态和实际状态一样呢">如何使理想状态和实际状态一样呢？</h2>
<p>在Kubernetes中，不仅仅是观察对象的一个信号，还观察了其他两个信号，集群的期待状态与实际状态，期望的状态是用户期望集群所处的状态，如我运行了2个实例（pod）。在最理想的场景下，集群的实际状态与期待状态是相同的，但这个过程会受到任意的外界因素干扰被影响下，实际状态与理想状态发生偏差。</p>
<p>Kubernetes必须接受实际状态，并将其与所需状态调和。不断地这样做，采取两种状态，确定其之间的差异，并纠正其不断的更改，以使实际状态达到理想状态。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211212232804746.png" alt="image-20211212232804746" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>如图所示，在一个<code>Edge trigger</code> 中，最终的结果很可能会与理想中的结果发生偏差。</p>
<p>当初始实例为1时，并希望扩展为5个副本，然后再向下缩容到2个副本，则<code>Edge trigger</code>环境下将看到以下状态：系统的实际状态不能立即对这些命令作出反应。正如图所述，当只有3个副本在运行时，它可能会终止3个副本。这就给我们留下了0个副本，而不是所需的2个副本。</p>
<pre><code># let replicas=1
# let replicas += 4 # 此时副本数为5，但是这个过程需要时间而不是立即完成至理想状态
# let replicas -= 3 # 当未完成时又接到信号的变化，此时副本数为3，减去3，很可能实际状态为0，与理想状态2发生了偏差
</code></pre>
<p>而使用<code>Level Trigger</code>时，会总是比较完整的期望状态和实际状态，直到实际状态与期望状态相同。这大大减少了状态同步间（错误）的产生。</p>
<h2 id="summary">summary</h2>
<p>每一种触发器的产生一定有其道理，<code>Edge trigger</code>本身并不是很差，只是应用场景的不同，而使用的模式也不同，比如nginx的高性能就是使用了<code>Edge trigger</code>模型，如nginx使用了 <code>Level trigger</code>在大并发下，当发生了变更信号等待返回时，发生大量客户端连接在侦听队列，而<code>Edge trigger</code>模型则不会出现这种情况。</p>
<p>综上所述，kubernetes在设计时，各个组件需要感知数据的最终理想状态，无需担心错过数据变化的过程。而设计kubernentes系统消息通知机制（或数据实时通知机制），也应满足以下要求：</p>
<ul>
<li>
<p>实时性（即数据变化时，相关组件感觉越快越好）。消息必须是实时的。在<code>list/watch</code>机制下，每当apiserver资源有状态变化事件时，都会及时将事件推送到客户端，以保证消息的实时性。</p>
</li>
<li>
<p>消息序列：消息的顺序也很重要。在并发场景下，客户端可能会在短时间内收到同一资源的多个事件。对于关注最终一致性的kubernetes来说，它需要知道哪个是最新的事件，并保证资源的最终状态与最新事件所表达的一致。kubernetes在每个资源事件中都携带一个<code>resourceVersion</code>标签，这个标签是递增的。因此，客户端在并发处理同一资源的事件时，可以比较<code>resourceVersion</code>，以确保最终状态与最新事件的预期状态一致。</p>
</li>
<li>
<p>消息的可靠性，保证消息不丢失或者有可靠的重新获取的机制（比如 <code>kubelet</code>和 <code>kube-apisever</code>之间的网络波动（<code>network flashover</code> ）需要保证kubelet在网络恢复后可以接收到网络故障时产生的消息）。</p>
</li>
</ul>
<p>正是因为Kubernetes使用了 <code>Level trigger</code>才让集群更加可靠。</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://levelup.gitconnected.com/nginx-event-driven-architecture-demonstrated-in-code-51bf0061cad9" target="_blank"
   rel="noopener nofollow noreferrer" >nginx-event-driven-architecture</a></p>
<p><a href="https://www.quora.com/What-is-meant-by-edge-triggering-and-level-triggering" target="_blank"
   rel="noopener nofollow noreferrer" >What-is-meant-by-edge-triggering-and-level-triggering</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>理解kubernetes schema</title>
      <link>https://www.oomkill.com/2021/11/ch03-kubernetes-schema/</link>
      <pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/11/ch03-kubernetes-schema/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="什么是schema">什么是schema</h2>
<p>schema一词起源于希腊语中的<code>form</code>或<code>figure</code>，但具体应该如何定义<code>schema</code>取决于应用环境的上下文。<code>schema</code>有不同的类型，其含义与数据科学、教育、营销和SEO以及心理学等领域密切相关。</p>
<blockquote>
<p>在维基百科中将schema解释为，<strong>图式</strong>，在心里学中主要描述一种思维或行为类型，用来组织资讯的类别，以及资讯之间的关系。它也可以被描述为先入为主思想的心理结构，表示世界某些观点的框架，或是用于组织和感知新资讯的系统。</p>
</blockquote>
<p>但在计算机科学中，从很多地方都可以看到 <em>schema</em> 这个名词，例如 database，openldap，programing language等的。这里可以简单的吧<em>schema</em> 理解为 <strong>元数据集合</strong> （metadata component）<strong>数据模型</strong>，主要包含元素及属性的声明，与其他数据结构组成。</p>
<h3 id="数据库中的schema">数据库中的schema</h3>
<p>在数据库中，<code>schema</code> 就像一个骨架结构，代表整个数据库的逻辑视图。它设计了应用于特定数据库中数据的所有约束。当在数据建模时，就会产生一个schema。在谈到关系数据库]和面向对象数据库时经常使用schema。有时也指将结构或文本的描述。</p>
<p>数据库中schema描述数据的形状以及它与其他模型、表和库之间的关系。在这种情况下，数据库条目是schema的一个实例，包含schema中描述的所有属性。</p>
<p>数据库schema通常分为两类：定义数据文件实际存储方式的**物理数据库schema <strong>；和</strong>逻辑数据库schema **，它描述了应用于存储数据的所有逻辑约束，包括完整性、表和视图。常见包括</p>
<ul>
<li>星型模式（star schema）</li>
<li>雪花模式（snowflake schema）</li>
<li>事实星座模型（fact constellation schema 或 galaxy schema）</li>
</ul>
<p>星型模式是类似于一个简单的数据仓库图，包括一对多的事实表和维度表。它使用非规范化数据。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/dm-star_schema-f_mobile.png" alt="星型模式" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>雪花模式是更为复杂的一种流行的数据库模式，在该模式下，维度表是规范化的，可以节省存储空间并最大限度地减少数据冗余。</p>
<p>事实星座模式远比星型模式和雪花模式复杂得多。它拥有多个共享多个维度表的事实表。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/dm-snowflake_schema-f_mobile.png" alt="snowflake schema" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="kubernetes中的schema">Kubernetes中的schema</h2>
<p>通过上面的阐述，大概上可以明白 schema究竟是什么东西了，在Kubernetes中也有schema的概念，通过对kubernetes中资源（GVK）的规范定义、相互关系间的映射等，schema即k8s资源对象元数据。</p>
<p>而kubernetes中资源对象即  <code>Group</code> <code>Version</code> <code>Kind</code> 这些被定义在 <code>staging/src/k8s.io/api/type.go</code>中，即平时所操作的yaml文件，例如</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment  
metadata:
  name:  ngx
  namespace: default
spec:
  selector:  
    matchLabels:
      app: ngx
  template:  
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: ngx-schema
        image: nginx
        ports:
        - containerPort: 80
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211117001624039.png" alt="image-20211117001624039" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>而对应的的即为<code>TypeMeta</code> 、<code>ObjectMeta</code> 和 <code>DeploymentSpec</code>,</p>
<p><code>TypeMeta</code> 为 <code>kind</code> 与 <code>apiserver</code></p>
<p><code>ObjectMeta</code> 为  <code>Name</code> 、<code>Namespace</code>  <code>CreationTimestamp</code>等段。</p>
<p><code>DeploymentSpec</code> 则对应了 yaml 中的 spec。</p>
<p>而整个yaml组成了 一个 k8s的资源对象。</p>
<pre><code class="language-go">type Deployment struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// Standard object metadata.
	// +optional
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

	// Specification of the desired behavior of the Deployment.
	// +optional
	Spec DeploymentSpec `json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;`

	// Most recently observed status of the Deployment.
	// +optional
	Status DeploymentStatus `json:&quot;status,omitempty&quot; protobuf:&quot;bytes,3,opt,name=status&quot;`
}
</code></pre>
<p><code>register.go</code> 则是将对应的资源类型注册到schema中的类</p>
<pre><code class="language-go">var (
	// TODO: move SchemeBuilder with zz_generated.deepcopy.go to k8s.io/api.
	// localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes.
	SchemeBuilder      = runtime.NewSchemeBuilder(addKnownTypes)
	localSchemeBuilder = &amp;SchemeBuilder
	AddToScheme        = localSchemeBuilder.AddToScheme
)

// Adds the list of known types to the given scheme.
func addKnownTypes(scheme *runtime.Scheme) error {
	scheme.AddKnownTypes(SchemeGroupVersion,
		&amp;Deployment{},
		&amp;DeploymentList{},
		&amp;StatefulSet{},
		&amp;StatefulSetList{},
		&amp;DaemonSet{},
		&amp;DaemonSetList{},
		&amp;ReplicaSet{},
		&amp;ReplicaSetList{},
		&amp;ControllerRevision{},
		&amp;ControllerRevisionList{},
	)
	metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
	return nil
}
</code></pre>
<p>而 <code>apimachinery</code> 包则是 schema的实现，通过看其内容可以发下，kubernetes中 schema就是 <strong>GVK</strong> 的属性约束 与 <strong>GVR</strong> 之间的映射。</p>
<h2 id="通过示例了解schema">通过示例了解schema</h2>
<p>例如在 <code>apps/v1/deployment</code> 这个资源，在代码中表示 <code>k8s.io/api/apps/v1/types.go</code> ，如果需要对其资源进行扩展那么需要怎么做？如，建立一个 <code>StateDeplyment</code> 资源</p>
<pre><code>type Deployment struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// Standard object metadata.
	// +optional
	metav1.ObjectMeta `json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

</code></pre>
<p>如上述代码所示，Deployment 中的 <code>metav1.TypeMeta</code> 和 <code>metav1.ObjectMeta</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220517172901775.png" alt="image-20220517172901775" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>那么我们复制一个 Deployment 为 StateDeployment，注意，因为 Deployment的两个属性， <code>metav1.TypeMeta</code> 和 <code>metav1.ObjectMeta</code> 分别实现了不同的方法，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220517173136077.png" alt="image-20220517173136077" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>所以在实现方法时，需要实现 <code>DeepCopyinfo</code> ， <code>DeepCopy</code>  和继承接口 <code>Object</code> 的 <code>DeepCopyObject</code> 方法</p>
<pre><code class="language-go">// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *StateDeployment) DeepCopyInto(out *StateDeployment) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&amp;out.ObjectMeta)
	in.Spec.DeepCopyInto(&amp;out.Spec)
	in.Status.DeepCopyInto(&amp;out.Status)
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StateDeployment.
func (in *StateDeployment) DeepCopy() *StateDeployment {
	if in == nil {
		return nil
	}
	out := new(StateDeployment)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *StateDeployment) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}
</code></pre>
<p>那么扩展一个资源的整个流为：</p>
<ul>
<li>资源类型在：<code>k8s.io/api/{Group}/types.go</code></li>
<li>资料类型的实现接口 <code>k8s.io/apimachinery/pkg/runtime/interfaces.go.Object</code></li>
<li>其中是基于 <code>Deployment</code> 的类型，<code>metav1.TypeMeta</code> 和 <code>metav1.ObjectMeta</code></li>
<li><code>metav1.TypeMeta</code> 实现了 <code>GetObjectKind()</code> ；<code>metav1.ObjectMeta</code> 实现了<code>DeepCopyinfo=()</code> ， <code>DeepCopy()</code> ，还需要实现 <code>DeepCopyObject()</code></li>
<li>最后注册资源到schema中 <code>k8s.io/api/apps/v1/register.go</code></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>k8s开发环境准备 - 如何配置开发环境</title>
      <link>https://www.oomkill.com/2021/11/ch01-k8s-perpare/</link>
      <pubDate>Fri, 19 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/11/ch01-k8s-perpare/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="下载源码">下载源码</h2>
<p>根据kubernetes github 方式可以</p>
<pre><code>mkdir -p $GOPATH/src/k8s.io
cd $GOPATH/src/k8s.io
git clone https://github.com/kubernetes/kubernetes
cd kubernetes
make
</code></pre>
<p>如果有需要可以切换到对应的版本进行学习或者修改，一般kubernetes版本为对应tag</p>
<pre><code>git fetch origin [远程tag名]
git checkout  [远程tag名]
git branch
</code></pre>
<h2 id="配置goland">配置goland</h2>
<p>kubernetes本身是支持 go mod 的，但源码这里提供了所有的依赖在 <code>staging/src/k8s.io/</code> 目录下，可以将此目录内的文件复制到 <code>vendor</code>下。</p>
<pre><code class="language-bash">cp -a staging/src/k8s.io/* vendor/k8s.io/
</code></pre>
<p>对于 <code>k8s.io/kubernetes/pkg/</code> 发红的（找不到依赖的），可以将手动创建一个目录在 <code>vendor/k8s.io/</code> 将克隆下来的根目录 <code>pkg</code> 复制到刚才的目录下。</p>
<p>goland中，此时不推荐使用go mod模式了，这里goland一定要配置GOPATH的模式。对应的GOPATH加入 <code>{project}/vender</code>即可。 这里可以添加到 goland中 <code>project GOPATH</code>里。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20211116222531919.png" alt="image-20211116222531919" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
