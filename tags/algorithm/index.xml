<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>algorithm on Cylon&#39;s Collection</title>
    <link>https://www.oomkill.com/tags/algorithm/</link>
    <description>Recent content in algorithm on Cylon&#39;s Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 28 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.oomkill.com/tags/algorithm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>漏桶算法与令牌桶算法</title>
      <link>https://www.oomkill.com/2022/08/ch10-token-bucket-algorithm/</link>
      <pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/08/ch10-token-bucket-algorithm/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="principle-of-token-bucket">Principle of token bucket</h2>
<p>随着互联网的发展，在处理流量的方法也不仅仅为 first-come，first-served，而在共享网络中实现流量管理的基本机制就是排队。而公平算法则是实现在优先级队列中基于哪些策略来排队的 “<strong>公平队列</strong>” 。<code>Token Bucket</code> 则是为公平排队提供了替代方案。Fair Queue 与 Token Bucket的区别主要在，对于Fair Queue来讲，如果请求者目前空闲，Queue会将该请求者的带宽分配给其他请求者；而 Token Bucket 则是分配给请求者的带宽是带宽的上限。</p>
<p><strong>通过例子了解算法原理</strong></p>
<p>假设出站带宽是 4个数据包/ms，此时有一个需求为，为一个特定的发送端 <strong>A</strong> 来分配 1个数据包/ms的带宽。此时可以使用公平排队的方法分给发送 <strong>A</strong> 25%的带宽。</p>
<p>此时存在的问题是我们希望可以灵活地允许 <strong>A</strong>  的数据包以无规则的时间间隔发送。例如假设 <strong>A</strong>  在每个数据包发送后等待1毫秒后再开始下一个数据包的发送。</p>
<ul>
<li>sence1：此时假设  <strong>A</strong>  以 1ms 的间隔去发送数据包，而由于某种原因导致应该在 t=6 到达的数据包却在 t=6.5 到达。随后的数据包在 t=7 准时到达，在这种情况下是否应该保留到t=7.5？</li>
<li>sence2：或者是否允许在 t=6.5 发送一个迟到的数据包，在 t=7 发送下一个数据包，此时理论上平均速率仍然还是 1 个数据包/ms？</li>
</ul>
<p>显然sence2是合理的，这个场景的解决方法就是<strong>令牌桶</strong>算法，规定 <strong>A</strong> 的配额，允许指定平均速率和突发容量。当数据包不符合令牌桶规范，那么就认为其不合理，此时会做出一下相应：</p>
<ul>
<li>delay，直到桶准备好</li>
<li>drop</li>
<li>mark，标记为不合规的数据包</li>
</ul>
<p>delay 被称为 <strong>整形</strong> <code>shaping</code> , <strong>shaping</strong> 是指在某个时间间隔内发送超过 <strong>Bc</strong>（Committed Burst）的大小，<strong>Bc</strong> 在这里指桶的尺寸。由于数据流量是突发性的，当在一段时间内不活动后，再次激活后的在一个间隔内发送的数量大于 <strong>Bc <strong>，那么额外的流量被称为</strong>Be</strong> （burst excess）。</p>
<p>将流量丢弃或标记超额流量，保持在一个流量速率限制称为 “<strong>管制</strong>” <code>policing</code>。</p>
<h3 id="definition">Definition</h3>
<p>令牌桶的定义是指，有一个桶，以稳定的速度填充令牌；桶中的任何一个溢出都会被丢弃。当要发送一个数据包，需要能够从桶中取出一个令牌；如果桶是空的那么此时数据包是不合规的数据包，必须进行 <code>delay</code> , <code>drop</code> , <code>mark</code> 操作。如果桶是满的，则会发送与桶容量相对应的突发（短时间内的高带宽传输），这是桶是空的。</p>
<p>令牌桶的规范：$TB(r,B_{max})$</p>
<ul>
<li>$r$ ：r个token每秒的令牌填充率，表示桶填充令牌的速率</li>
<li>$B$ ：桶容量，$B_{mac} &gt; 0$</li>
</ul>
<p>那么公式则表示，桶以指定的速率填充令牌，最大为 $B_{max}$ 。这就说明了为了使大小为 S 的数据包合规，桶内必须至少有 S 个令牌，即 $B \ge S$，否则数据包不合规，在发送时，桶为 $B=B-S$</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/fb2fa6c0c5aea327f72d2e67ed19c801.jpg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="examples">Examples</h3>
<p>场景1：假设令牌桶规范为 $TB(\frac{1}{3}\ packet/ms, 4\ packet)$，桶最初是满的，数据包在以下时间到达 <code>[0, 0, 0, 2, 3, 6, 9, 12]</code></p>
<p>在处理完所有 <code>T=0</code> 的数据包后，桶中还剩 1 个令牌。到第四个数据包 <code>T=2</code> 到达时，桶内已经有1个令牌 + $\frac{2}{3}$ 个令牌；当发送完第四个数据包时，桶内令牌数为  $\frac{2}{3}$ 。到 <code>T=3</code> 数据包时，桶内令牌为1，满足发送第 5 个数据包。万松完成后桶是空的，在后面 6 9 12时，都满足3/ms 一个数据包，都可以发送成功</p>
<p>场景2：另外一个实例，在同样的令牌桶规范下 $TB(\frac{1}{3}, 4)$，数据包到达时间为 <code>[0, 0, 0, 0, 12, 12, 12, 12, 24, 24, 24, 24]</code> ，可以看到在这个场景下，数据到达为3个突发，每个突发4个数据包，此时每次发送完成后桶被清空，当再次填满时需要12ms，此时另外一组突发达。故这组数据是合规的。、</p>
<p>场景3：在同样的令牌桶规范下 $TB(\frac{1}{3}, 4)$，数据包到达时间为 <code>[0, 1, 2, 3, 4, 5]</code> , 这组数据是不合规的</p>
<p>用表格形式表示如下：</p>
<table>
<thead>
<tr>
<th>数据包到达时间</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr>
<td>发送前桶内令牌</td>
<td>4</td>
<td>3 $\frac{1}{3}$</td>
<td>2 $\frac{2}{3}$</td>
<td>2</td>
<td>1 $\frac{1}{3}$</td>
<td>$\frac{2}{3}$</td>
</tr>
<tr>
<td>发送后桶内令牌</td>
<td>3</td>
<td>2 $\frac{1}{3}$</td>
<td>1 $\frac{2}{3}$</td>
<td>1</td>
<td>$\frac{1}{3}$</td>
<td>$\frac{2}{3}$</td>
</tr>
</tbody>
</table>
<p>如果一个数据包在桶中没有足够的令牌来发送它时到达，可以进行整形或管制，整形使数据包等到足够的令牌积累。管制会丢弃数据包。或者发送方可以立即发送数据包，但将其标记为不合规。</p>
<h2 id="principle-of-leaky-bucket">Principle of leaky bucket</h2>
<p><strong>漏桶</strong> （leaky bucket）是一种临时存储可变数量的请求并将它们组织成设定速率输出的数据包的方法。漏桶的概念与令牌桶比起是相反的，漏桶可以理解为是一个具有恒定服务时间的队列。</p>
<p>由下图可以看出，漏桶的概念是一个底部有孔的桶。无论水进入桶的速度是多少，它都会以恒定的速度通过孔从桶中泄漏出来。如果桶中没有水，则流速为零，如果桶已满，则多余的水溢出并丢失。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/leaky-bucket.png" alt="漏桶算法" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>和令牌桶一样，漏桶用于流量整形和流量管制</p>
<h2 id="difference-between-token-and-leaky">Difference between Token and Leaky</h2>
<table>
<thead>
<tr>
<th>Leaky</th>
<th>Token</th>
</tr>
</thead>
<tbody>
<tr>
<td>桶中存放的是所有到达的数据包，必须入桶</td>
<td>桶中存放的是定期生成的令牌</td>
</tr>
<tr>
<td>桶以恒定速率泄漏</td>
<td>桶有最大容量 $B_{max}$</td>
</tr>
<tr>
<td>突发流量入桶转换为恒定流量发送</td>
<td>发送数据包需要小号对应的token</td>
</tr>
</tbody>
</table>
<p><strong>token较leaky的优势</strong>：</p>
<ul>
<li>在令牌桶中，如果桶已满，处理的方式有 shaping和policing两种模型三种方式（延迟、丢弃、标记），而漏桶中的流量仅为shaping。
<ul>
<li>通俗来说，就是令牌桶已满，丢弃的是令牌，漏桶中丢弃的则是数据包</li>
</ul>
</li>
<li>令牌桶可以更快的速率发送大突发流量，而漏桶仅是恒定速率</li>
</ul>
<h2 id="implementation-with-go">Implementation with go</h2>
<h3 id="token">Token</h3>
<p>在golang中，内置的 <code>rate</code> 包实现了一个令牌桶算法，通过 <code>rate.NewLimiter(r,B)</code> 进行构造。与公式$TB(r,B_{max})$ 意思相同。</p>
<pre><code class="language-go">type Limiter struct {
	limit Limit // 向桶中放置令牌的速率
	burst int // 桶的容量
	mu     sync.Mutex
	tokens float64 // 可用令牌容量
	last time.Time // 上次放入token的时间
	lastEvent time.Time
}
</code></pre>
<p>Limiter中带有三种方法， <code>Allow</code>、<code>Reserve</code>、<code>Wait</code> 分别表示Token Bucket中的 <code>shaping</code> 和 <code>policing</code>：</p>
<ul>
<li>Allow：丢弃超过速率的事件，类似 <code>drop</code></li>
<li>Wait：等待，直到获取到令牌或者取消或deadline/timeout</li>
<li>Reserve：等待或减速，不丢弃事件，类似于 <code>delay</code></li>
</ul>
<h3 id="reservereserven">Reserve/ReserveN</h3>
<ul>
<li><code>Reserve()</code>  返回了 <code>ReserveN(time.Now(), 1)</code></li>
<li><code>ReserveN()</code> 无论如何都会返回一个 Reservation，指定了调用者在 n 个事件发生之前必须等待多长时间。</li>
<li>Reservation 是一个令牌桶事件信息</li>
<li>Reservation 中的 <code>Delay()</code> 方法返回了需要等待的时间，如果时间为0则不需要等待</li>
<li>Reservation 中的 <code>Cancel()</code> 将取消等待</li>
</ul>
<p>wait/waitN</p>
<h3 id="allowallown">Allow/AllowN</h3>
<ul>
<li>在获取不到令牌是丢弃对应的事件</li>
<li>返回的是一个 <code>reserveN()</code> 拿到token是合规的，并消耗掉token</li>
</ul>
<p>AllowN 为截止到某一时刻，当前桶内桶中数目是否至少为 n 个，满足则返回 true，同时从桶中消费 n 个 token。反之不消费 Token，false。</p>
<pre><code class="language-go">func (lim *Limiter) AllowN(now time.Time, n int) bool {
	return lim.reserveN(now, n, 0).ok // 由于仅需要一个合规否，顾合规的通过，不合规的丢弃
}
</code></pre>
<p><code>reserveN()</code> 是三个行为的核心，AllowN中指定的为 <strong>0</strong> ，因为 <code>maxFutureReserve</code> 是最大的等待时间，AllowN给定的是0，即如果突发大的情况下丢弃额外的 <strong>Bc</strong>。</p>
<pre><code class="language-go">func (lim *Limiter) reserveN(now time.Time, n int, maxFutureReserve time.Duration) Reservation {
	lim.mu.Lock()

	if lim.limit == Inf {
		lim.mu.Unlock()
		return Reservation{
			ok:        true,
			lim:       lim,
			tokens:    n,
			timeToAct: now,
		}
	}
	// 这里拿到的是now，上次更新token时间和桶内token数量
	now, last, tokens := lim.advance(now)
	// 计算剩余的token
	tokens -= float64(n)

	// Calculate the wait duration
	var waitDuration time.Duration
	if tokens &lt; 0 {
		waitDuration = lim.limit.durationFromTokens(-tokens)
	}

	// 确定是否合规，n是token
    // token 的数量要小于桶的容量，并且 等待时间小于最大等待时间
	ok := n &lt;= lim.burst &amp;&amp; waitDuration &lt;= maxFutureReserve

	// Prepare reservation
	r := Reservation{
		ok:    ok,
		lim:   lim,
		limit: lim.limit,
	}
	if ok {
		r.tokens = n
		r.timeToAct = now.Add(waitDuration)
	}

	// Update state
	if ok {
		lim.last = now
		lim.tokens = tokens
		lim.lastEvent = r.timeToAct
	} else {
		lim.last = last
	}

	lim.mu.Unlock()
	return r
}
</code></pre>
<p>在reserveN中调用了一个 <code>advance()</code> 函数，</p>
<pre><code class="language-go">func (lim *Limiter) advance(now time.Time) (newNow time.Time, newLast time.Time, newTokens float64) {
   last := lim.last
   if now.Before(last) { // 计算上次放入token是否在传入now之前
      last = now
   }

   // 当 last 很旧时，避免在下面进行 delta 溢出。
   // maxElapsed 计算装满需要多少时间
   maxElapsed := lim.limit.durationFromTokens(float64(lim.burst) - lim.tokens)
   elapsed := now.Sub(last) // 上次装入到现在的时差
   if elapsed &gt; maxElapsed { // 上次如果放入token时间超长，就让他与装满时间相等
      elapsed = maxElapsed // 即，让桶为满的
   }

   // 装桶的动作，下面函数表示，elapsed时间内可以生成多少个token
   delta := lim.limit.tokensFromDuration(elapsed)
   tokens := lim.tokens + delta // 当前的token
   if burst := float64(lim.burst); tokens &gt; burst {
      tokens = burst // 这里表示token溢出，让他装满就好
   }

   return now, last, tokens
}
</code></pre>
<h3 id="waitwaitn">wait/waitN</h3>
<ul>
<li>桶内令牌可以&gt;N时，返回，在获取不到令牌是阻塞，等待context取消或者超时</li>
<li>返回的是一个 <code>reserveN()</code> 拿到token是合规的，并消耗掉token</li>
</ul>
<pre><code class="language-go">func (lim *Limiter) WaitN(ctx context.Context, n int) (err error) {
	if n &gt; lim.burst &amp;&amp; lim.limit != Inf {
		return fmt.Errorf(&quot;rate: Wait(n=%d) exceeds limiter's burst %d&quot;, n, lim.burst)
	}
	// 外部已取消
	select {
	case &lt;-ctx.Done():
		return ctx.Err()
	default:
	}
	// Determine wait limit
	now := time.Now()
	waitLimit := InfDuration
	if deadline, ok := ctx.Deadline(); ok {
		waitLimit = deadline.Sub(now)
	}
	// 三个方法的核心，这里给定了deatline
	r := lim.reserveN(now, n, waitLimit)
	if !r.ok {
		return fmt.Errorf(&quot;rate: Wait(n=%d) would exceed context deadline&quot;, n)
	}
	// Wait if necessary
	delay := r.DelayFrom(now)
	if delay == 0 {
		return nil
	}
	t := time.NewTimer(delay)
	defer t.Stop()
	select {
	case &lt;-t.C:
		// We can proceed.
		return nil
	case &lt;-ctx.Done():
		// Context was canceled before we could proceed.  Cancel the
		// reservation, which may permit other events to proceed sooner.
		r.Cancel()
		return ctx.Err()
	}
}
</code></pre>
<h3 id="dynamic-adjustment">Dynamic Adjustment</h3>
<p>在 <code>rate.limiter</code> 中，支持调整速率和桶大小，这样就可以根据现有环境和条件，来动态的改变 Token生成速率和桶容量</p>
<ul>
<li><code>SetLimit(Limit)</code>  更改生成 Token 的速率</li>
<li><code>SetBurst(int)</code>  改变桶容量</li>
</ul>
<h3 id="example">Example</h3>
<h4 id="一个流量整形的场景">一个流量整形的场景</h4>
<pre><code class="language-go">package main

import (
	&quot;log&quot;
	&quot;strconv&quot;
	&quot;time&quot;

	&quot;golang.org/x/time/rate&quot;
)

func main() {
	timeLayout := &quot;2006-01-02:15:04:05.0000&quot;
	limiter := rate.NewLimiter(1, 5) // BT(1,5)
	log.Println(&quot;bucket current capacity: &quot; + strconv.Itoa(limiter.Burst()))
	length := 20 // 一共请求20次
	chs := make([]chan string, length)
	for i := 0; i &lt; length; i++ {
		chs[i] = make(chan string, 1)
		go func(taskId string, ch chan string, r *rate.Limiter) {
			err := limiter.Allow()
			if !err {
				ch &lt;- &quot;Task-&quot; + taskId + &quot; unallow &quot; + time.Now().Format(timeLayout)
			}

			time.Sleep(time.Duration(5) * time.Millisecond)
			ch &lt;- &quot;Task-&quot; + taskId + &quot; run success  &quot; + time.Now().Format(timeLayout)
			return

		}(strconv.FormatInt(int64(i), 10), chs[i], limiter)
	}
	for _, ch := range chs {
		log.Println(&quot;task start at &quot; + &lt;-ch)
	}
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220615200407126.png" alt="image-20220615200407126" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>通过执行结果可以看出，在突发为20的情况下，allow仅允许了获得token的事件执行，，这种场景下实现了流量整形的特性。</p>
<h4 id="一个流量管制的场景">一个流量管制的场景</h4>
<pre><code class="language-go">package main

import (
	&quot;context&quot;
	&quot;log&quot;
	&quot;strconv&quot;
	&quot;time&quot;

	&quot;golang.org/x/time/rate&quot;
)

func main() {
	timeLayout := &quot;2006-01-02:15:04:05.0000&quot;
	limiter := rate.NewLimiter(1, 5) // BT(1,5)
	log.Println(&quot;bucket current capacity: &quot; + strconv.Itoa(limiter.Burst()))
	length := 20 // 一共请求20次
	chs := make([]chan string, length)
	for i := 0; i &lt; length; i++ {
		chs[i] = make(chan string, 1)
		go func(taskId string, ch chan string, r *rate.Limiter) {
			err := limiter.Wait(context.TODO())
			if err != nil {
				ch &lt;- &quot;Task-&quot; + taskId + &quot; unallow &quot; + time.Now().Format(timeLayout)
			}
			ch &lt;- &quot;Task-&quot; + taskId + &quot; run success  &quot; + time.Now().Format(timeLayout)
			return

		}(strconv.FormatInt(int64(i), 10), chs[i], limiter)
	}
	for _, ch := range chs {
		log.Println(&quot;task start at &quot; + &lt;-ch)
	}
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220615201137581.png" alt="image-20220615201137581" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>结果可以看出，在大突发的情况下，在拿到token的任务会立即执行，没有拿到token的会等待拿到token后继续执行，这种场景下实现了流量管制的特性</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="http://intronetworks.cs.luc.edu/current/html/tokenbucket.html" target="_blank"
   rel="noopener nofollow noreferrer" >tokenbucket</a></li>
<li><a href="https://content.cisco.com/chapter.sjs?uri=/searchable/chapter/content/en/us/td/docs/routers/ncs4000/software/qos/configuration_guide/b-qos-cg/b-qos-cg_chapter_0111.html.xml" target="_blank"
   rel="noopener nofollow noreferrer" >QoS Policing</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>KNN算法</title>
      <link>https://www.oomkill.com/2022/06/knn/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/knn/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>K近邻值算法 <strong>KNN (K — Nearest Neighbors)</strong> 是一种机器学习中的分类算法；K-NN是一种<strong>非参数</strong>的<strong>惰性学习算法</strong>。非参数意味着没有对基础数据分布的假设，即模型结构是从数据集确定的。</p>
<p>它被称为<strong>惰性算法</strong>的原因是，因为它**不需要任何训练数据点来生成模型。**所有训练数据都用于测试阶段，这使得训练更快，测试阶段更慢且成本更高。</p>
<h2 id="如何工作">如何工作</h2>
<p>KNN 算法是通过计算新对象与训练数据集中所有对象之间的距离，对新实例进行分类或回归预测。然后选择训练数据集中距离最小的 K 个示例，并通过平均结果进行预测。</p>
<p>如图所示：一个未分类的数据（红色）和所有其他已分类的数据（黄色和紫色），每个数据都属于一个类别。因此，计算未分类数据与所有其他数据的距离，以了解哪些距离最小，因此当K= 3 （或K= 6 ）最接近的数据并检查出现最多的类，如下图所示，与新数据最接近的数据是在第一个圆圈内（圆圈内）的数据，在这个圆圈内还有 3 个其他数据（已经用黄色分类），我们将检查其中的主要类别，会被归类为紫色，因为有2个紫色球，1个黄色球。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221021234452066.png" alt="image-20221021234452066" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="knn算法要执行的步骤">KNN算法要执行的步骤</h3>
<ul>
<li>将数据分为训练数据和测试数据</li>
<li>选择一个值 K</li>
<li>确定要使用的距离算法</li>
<li>从需要分类的测试数据中选择一个样本，计算到它的 n 个训练样本的距离。</li>
<li>对获得的距离进行排序并取 k最近的数据样本。</li>
<li>根据 k 个邻居的多数票将测试类分配给该类。</li>
</ul>
<h3 id="影响knn算法性能的因素">影响KNN算法性能的因素</h3>
<ul>
<li>
<p>用于确定最近邻居的<strong>距离</strong>的算法</p>
</li>
<li>
<p>用于从 K 近邻派生分类的决策规则</p>
</li>
<li>
<p>用于对新示例进行分类的邻居<strong>数</strong></p>
</li>
</ul>
<h2 id="如何计算距离">如何计算距离</h2>
<p>测量距离是KNN算法的核心，总结了问题域中两个对象之间的相对差异。比较常见的是，这两个对象是描述主题（例如人、汽车或房屋）或事件（例如购买、索赔或诊断）的数据行。</p>
<h3 id="汉明距离">汉明距离</h3>
<p>汉明距离（<code>Hamming Distance</code>）计算两个二进制向量之间的距离，也简称为二进制串 <code>binary strings</code> 或位串 <code>bitstrings </code>；换句话说，汉明距离是将一个字符串更改为另一个字符串所需的最小替换次数，或将一个字符串转换为另一个字符串的最小错误数。</p>
<p>示例：如一列具有类别 “红色”、“绿色” 和 “蓝色”，您可以将每个示例独热编码为一个位串，每列一个位。</p>
<blockquote>
<p>注：独热编码 one-hot encoding：将分类数据，转换成二进制向量表示，这个二进制向量用来表示一种特殊的bit（二进制位）组合，该字节里，仅容许单一bit为1，其他bit都必须为0</p>
<p>如：</p>
<table>
<thead>
<tr>
<th>apple</th>
<th>banana</th>
<th>pineapple</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>100 表示苹果，100就是苹果的二进制向量
010 表示香蕉，010就是香蕉的二进制向量</p>
</blockquote>
<pre><code>red = [1, 0, 0]
green = [0, 1, 0]
blue = [0, 0, 1]
</code></pre>
<p>而red和green之间的距离就是<strong>两个等长bitstrings之间bit差</strong>（对应符号不同的位置）的总和或平均数，这就是汉明距离</p>
<ul>
<li>$Hamming Distance d(a, b)\ =\ sum(xi\ !=\ yi\ for\ xi,\ yi\ in\ zip(x, y))$</li>
</ul>
<p>上述的实现为：</p>
<pre><code class="language-python">def hammingDistance(a, b):
    if len(a) != len(b):
        raise ValueError(&quot;Undefined for sequences of unequal length.&quot;)
    return sum(abs(e1 - e2) for e1, e2 in zip(a, b))

row1 = [0, 0, 0, 0, 0, 1]
row2 = [0, 0, 0, 0, 1, 0]

dist = hammingDistance(row1, row2)
print(dist)
</code></pre>
<p>可以看到字符串之间有两个差异，或者 6 个位位置中有 2 个不同，平均 (2/6) 约为 1/3 或 0.333。</p>
<pre><code class="language-python">
from scipy.spatial.distance import hamming
# define data
row1 = [0, 0, 0, 0, 0, 1]
row2 = [0, 0, 0, 0, 1, 0]

# calculate distance
dist = hamming(row1, row2)
print(dist)
</code></pre>
<h3 id="欧几里得距离">欧几里得距离</h3>
<p>欧几里得距离（<code>Euclidean distance</code>） 是计算两个点之间的距离。在计算具体的数值（例如浮点数或整数）的两行数据之间的距离时，您最有可能使用欧几里得距离。</p>
<p>欧几里得距离计算公式为两个向量之间的平方差之和的平方根。</p>
<p>$EuclideanDistance=\sqrt[]{\sum(a-b)^2}$</p>
<p>如果要执行数千或数百万次距离计算，通常会去除平方根运算以加快计算速度。修改后的结果分数将具有相同的相对比例，并且仍然可以在机器学习算法中有效地用于查找最相似的示例。</p>
<p>$EuclideanDistance = sum\ for\ i\ to\ N\ (v1[i]\ –\ v2[i])^2$</p>
<pre><code class="language-python"># calculating euclidean distance between vectors
from math import sqrt
from scipy.spatial.distance import euclidean

# calculate euclidean distance
def euclidean_distance(a, b):
	return sqrt(sum((e1-e2)**2 for e1, e2 in zip(a,b)))
 
# define data
row1 = [10, 20, 15, 10, 5]
row2 = [12, 24, 18, 8, 7]
# calculate distance
dist = euclidean_distance(row1, row2)
print(dist)
print(euclidean(row1, row2))
</code></pre>
<h3 id="曼哈顿距离">曼哈顿距离</h3>
<p>曼哈顿距离（ <code>Manhattan distance</code> ）又被称作出租车几何学 <code>Taxicab geometry</code>；用于计算两个向量之间的距离。</p>
<p>对于描述网格上的对象（如棋盘或城市街区）的向量可能更有用。出租车在城市街区之间采取的最短路径（网格上的坐标）。</p>
<blockquote>
<p>粗略地说，欧几里得几何是中学常用的平面几何和立体几何 <a href="https://www.britannica.com/science/Euclidean-geometry/Plane-geometry" target="_blank"
   rel="noopener nofollow noreferrer" >Plane geometry</a></p>
</blockquote>
<p>曼哈顿距离可以理解为：欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221021234505760.png" alt="image-20221021234505760" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>图中： 红、蓝与黄线分别表示所有曼哈顿距离都拥有一样长度（12），绿线表示欧几里得距离 $6×\sqrt2 ≈ 8.48$</p>
<p>对于整数特征空间中的两个向量，应该计算曼哈顿距离而不是欧几里得距离</p>
<p>曼哈顿距离在二维平面的计算公式是，在X轴的亮点</p>
<p>$Manhattandistance\ d(x,y)=\left|x_{1}-x_{2}\right|+\left|y_{1}-y_{2}\right|$</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221021234535889.png" alt="image-20221021234535889" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>如果所示，描述格子和格子之间的距离可以用曼哈顿距离，如国王移动到右下角的距离是？</p>
<p>$King=|6-8|+|6-1| = 7$</p>
<p>两个向量间的距离可以表示为 $MD\ =\ Σ|Ai – Bi|$</p>
<p>python中的公式可以表示为 ：<code>sum(abs(val1-val2) for val1, val2 in zip(a,b))</code></p>
<pre><code class="language-python">from scipy.spatial.distance import cityblock
# calculate manhattan distance
def manhattan_distance(a, b):
	return sum(abs(e1-e2) for e1, e2 in zip(a,b))
 
# define data
row1 = [10, 20, 15, 10, 5]
row2 = [12, 24, 18, 8, 7]
# calculate distance
dist = manhattan_distance(row1, row2)
print(dist)
print(cityblock(row1, row2))
</code></pre>
<h3 id="闵可夫斯基距离">闵可夫斯基距离</h3>
<p>闵可夫斯基距离（<code>Minkowski distance</code>）并不是一种距离而是对是<strong>欧几里得距离</strong>和<strong>曼哈顿距离</strong>的概括，用来计算两个向量之间的距离。</p>
<p>闵可夫斯基增并添加了一个参数，称为“<strong>阶数</strong>”或 <code>p</code>：$d(x,y) = (\sum(|x-y|)^p)^\frac{1}{p}$</p>
<p>在python中的公式：</p>
<pre><code class="language-python">(sum for i to N (abs(v1[i] – v2[i]))^p)^(1/p)
</code></pre>
<p><code>p</code> 是一个有序的参数，当 $p=1$ 时，计算的是曼哈顿距离。当 $p=2$ 时，计算的是欧几里得距离。</p>
<p>在实现使用距离度量的机器学习算法时，通常会使用闵可夫斯基距离，因为可以通过调整参数“ <em>p</em> ”控制用于向量的距离度量算法的类型。</p>
<pre><code class="language-python"># calculating minkowski distance between vectors
from scipy.spatial import minkowski_distance
 
# calculate minkowski distance
def minkowski_distance(a, b, p):
	return sum(abs(e1-e2)**p for e1, e2 in zip(a,b))**(1/p)
 
# define data
row1 = [10, 20, 15, 10, 5]
row2 = [12, 24, 18, 8, 7]

# 手动实现的算法用来使用闵可夫斯基计算距离
dist = minkowski_distance(row1, row2, 1)
# 1为曼哈顿
print(dist)
# 1为欧几里得
dist = minkowski_distance(row1, row2, 2)
print(dist)

# 使用包 scipy.spatial来计算
print(minkowski_distance(row1, row2, 1))
print(minkowski_distance(row1, row2, 2))
</code></pre>
<h2 id="knn算法实现">KNN算法实现</h2>
<h3 id="prerequisite">Prerequisite</h3>
<p>首先会用示例来实现KNN算法的每个步骤，并加以分析，然后将所有步骤关联在在一起，形成一个适用于真实数据集的实现。</p>
<p>KNN在实现起来主要有三个步骤：</p>
<ul>
<li>计算距离（这里选择欧几里得距离）</li>
<li>获得临近邻居</li>
<li>做出预测</li>
</ul>
<p>这三个步骤是KNN算法用以解决分类和回归预测建模问题的基础知识</p>
<h3 id="计算距离">计算距离</h3>
<p>第一步计算数据集中两行之间的距离。在数据集中的数据行主要由数字组成，计算两行或数字向量之间的距离的一种简单方法是画一条直线。这在 2D 或 3D 平面中都是很好地选择，并且可以很好地扩展到更高的维度。</p>
<p>这里使用的是比较流行的计算距离的算法，<strong>欧几里得距离</strong>来计算两个向量之间的直线距离。欧几里得距离的公式是，两个向量的平方差的平方根，$Euclidean\ Distance=\sqrt[]{\sum(a-b)^2}$ ；在python中可以表示为：<code>sqrt(sum i to N (x1 – x2)^2)</code> ；其中 <code>x1</code> 是第一行数据，<code>x2</code> 是第二行数据，<code>i</code>  表示特定列的索引，因为可能需要对所有行进行计算。</p>
<p>在欧几里得距离中，值越小，两条记录就越相似； 0 表示两条记录之间没有差异。</p>
<p>那么使用python实现一个计算欧几里得距离的算法</p>
<pre><code class="language-python">def euclidean_distance(row1, row2):
	distance = 0.0
	for i in range(len(row1)-1):
		distance += (row1[i] - row2[i])**2
	return sqrt(distance)
</code></pre>
<p>准备一部分测试数据，来对测试距离算法</p>
<pre><code class="language-python">X1				X2					Y
2.7810836		2.550537003			0
1.465489372		2.362125076			0
3.396561688		4.400293529			0
1.38807019		1.850220317			0
3.06407232		3.005305973			0
7.627531214		2.759262235			1
5.332441248		2.088626775			1
6.922596716		1.77106367			1
8.675418651		-0.242068655		1
7.673756466		3.508563011			1
</code></pre>
<p>那么来测试这些数据，需要做到的是第一行与所有行之间的距离，对于第一行与自己的距离应该为<strong>0</strong></p>
<pre><code class="language-python">from math import sqrt
 
# 欧几里得距离，计算两个向量间距离的算法
def euclidean_distance(row1, row2):
	distance = 0.0
	for i in range(len(row1)-1):
		distance += (row1[i] - row2[i])**2 # 平方差
	return sqrt(distance) # 平方根
 
# 测试数据集
dataset = [
    [2.7810836,2.550537003,0],
	[1.465489372,2.362125076,0],
	[3.396561688,4.400293529,0],
	[1.38807019,1.850220317,0],
	[3.06407232,3.005305973,0],
	[7.627531214,2.759262235,1],
	[5.332441248,2.088626775,1],
	[6.922596716,1.77106367,1],
	[8.675418651,-0.242068655,1],
	[7.673756466,3.508563011,1]
]
row0 = dataset[0]
for row in dataset:
	distance = euclidean_distance(row0, row)
	print(distance)
    
# 0.0
# 1.3290173915275787
# 1.9494646655653247
# 1.5591439385540549
# 0.5356280721938492
# 4.850940186986411
# 2.592833759950511
# 4.214227042632867
# 6.522409988228337
# 4.985585382449795
</code></pre>
<h3 id="获取最近邻居">获取最近邻居</h3>
<p>数据集中新数据的邻居是k个最接近的实例（行），这个实例由距离定义。现在诞生的问题：<strong>如何找到最近的邻居？以及怎么找到最近的邻居？</strong></p>
<ul>
<li>
<p>为了在数据集中找到 K 的邻居，首先必须计算数据集中每条记录与新数据之间的距离。</p>
</li>
<li>
<p>有了距离之后，必须按照 <strong>K</strong> 的距离对训练集中的所有实例排序。然后选择前 <strong>k</strong> 个作为最近的邻居。</p>
</li>
</ul>
<p>这里实现起来是通过将数据集中每条记录的距离作为一个元组来跟踪，通过对元组列表进行排序（距离降序），然后检索最近邻居。下面是一个实现这些步骤的函数</p>
<pre><code class="language-python"># 找到最近的邻居
def get_neighbors(train, test_row, num_neighbors):
    &quot;&quot;&quot;
    计算训练集train中所有元素到test_row的距离
    :param train: list, 数据集，可以是训练集
    :param test_row: list, 新的实例，也就是K
    :param num_neighbors:int，需要多少个邻居
    :return: None
    &quot;&quot;&quot;
    distances = list()
    for train_row in train:
        # 计算出每一行的距离，把他添加到元组中
        dist = euclidean_distance(test_row, train_row)
        distances.append((train_row, dist))
    distances.sort(key=lambda knn: knn[1]) # 根据元素哪个字段进行排序
    neighbors = list()
    for i in range(num_neighbors):
        neighbors.append(distances[i][0])
    return neighbors
</code></pre>
<p>下面是完整的示例</p>
<pre><code class="language-python">from math import sqrt
 
# 欧几里得距离，计算两个向量间距离的算法
def euclidean_distance(row1, row2):
	distance = 0.0
	for i in range(len(row1)-1):
		distance += (row1[i] - row2[i])**2 # 平方差
	return sqrt(distance) # 平方根
 
# 找到最近的邻居
def get_neighbors(train, test_row, num_neighbors):
    &quot;&quot;&quot;
    计算训练集train中所有元素到test_row的距离
    :param train: list, 数据集，可以是训练集
    :param test_row: list, 新的实例，也就是K
    :param num_neighbors:int，需要多少个邻居
    :return: None
    &quot;&quot;&quot;
    distances = list()
    for train_row in train:
        # 计算出每一行的距离，把他添加到元组中
        dist = euclidean_distance(test_row, train_row)
        distances.append((train_row, dist))
    distances.sort(key=lambda knn: knn[1]) # 根据元素哪个字段进行排序
    neighbors = list()
    for i in range(num_neighbors):
        neighbors.append(distances[i][0])
    return neighbors

# 测试数据集
dataset = [
    [2.7810836,2.550537003,0],
	[1.465489372,2.362125076,0],
	[3.396561688,4.400293529,0],
	[1.38807019,1.850220317,0],
	[3.06407232,3.005305973,0],
	[7.627531214,2.759262235,1],
	[5.332441248,2.088626775,1],
	[6.922596716,1.77106367,1],
	[8.675418651,-0.242068655,1],
	[7.673756466,3.508563011,1]
]

neighbors = get_neighbors(dataset, dataset[0], 3)
for neighbor in neighbors:
	print(neighbor)

# [2.7810836, 2.550537003, 0]
# [3.06407232, 3.005305973, 0]
# [1.465489372, 2.362125076, 0]
</code></pre>
<p>可以看到，运行后会将数据集中最相似的 3 条记录按相似度顺序打印。和预测的一样，第一个记录与其本身最相似，并且位于列表的顶部。</p>
<h3 id="预测结果">预测结果</h3>
<p>预测结果在这里指定是，通过分类拿到了最近的邻居的实例，对邻居进行分类，找到邻居中最大类别的一类，作为预测值。这里使用的是对邻居值执行 <code>max()</code> 来实现这一点，下面是实现方式</p>
<pre><code class="language-python"># 预测值
def predict_classification(train, test_row, num_neighbors):
    &quot;&quot;&quot;
    计算训练集train中所有元素到test_row的距离
    :param train: list, 数据集，可以是训练集
    :param test_row: list, 新的实例，也就是K
    :param num_neighbors:int，需要多少个邻居
    :return: None
    &quot;&quot;&quot;
    neighbors = get_neighbors(train, test_row, num_neighbors)
    output_values = [row[-1] for row in neighbors] # 拿到所属类的真实类别
    prediction = max(set(output_values), key=output_values.count)  #算出邻居类别最大的数量
    return prediction
</code></pre>
<p>下面是完整的示例</p>
<pre><code class="language-python">from math import sqrt
 
# 欧几里得距离，计算两个向量间距离的算法
def euclidean_distance(row1, row2):
    distance = 0.0
    for i in range(len(row1)-1):
        distance += (row1[i] - row2[i])**2 # 平方差
    return sqrt(distance) # 平方根
 
# 找到最近的邻居
def get_neighbors(train, test_row, num_neighbors):
    &quot;&quot;&quot;
    计算训练集train中所有元素到test_row的距离
    :param train: list, 数据集，可以是训练集
    :param test_row: list, 新的实例，也就是K
    :param num_neighbors:int，需要多少个邻居
    :return: None
    &quot;&quot;&quot;
    distances = list()
    for train_row in train:
        # 计算出每一行的距离，把他添加到元组中
        dist = euclidean_distance(test_row, train_row)
        distances.append((train_row, dist))
    distances.sort(key=lambda knn: knn[1]) # 根据元素哪个字段进行排序
    neighbors = list()
    for i in range(num_neighbors):
        neighbors.append(distances[i][0])
    return neighbors

# 预测值
def predict_classification(train, test_row, num_neighbors):
    &quot;&quot;&quot;
    计算训练集train中所有元素到test_row的距离
    :param train: list, 数据集，可以是训练集
    :param test_row: list, 新的实例，也就是K
    :param num_neighbors:int，需要多少个邻居
    :return: None
    &quot;&quot;&quot;
    neighbors = get_neighbors(train, test_row, num_neighbors)
    output_values = [row[-1] for row in neighbors] # 拿到所属类的真实类别
    prediction = max(set(output_values), key=output_values.count)  #算出邻居类别最大的数量
    return prediction

# 测试数据集
dataset = [
    [2.7810836,2.550537003,0],
	[1.465489372,2.362125076,0],
	[3.396561688,4.400293529,0],
	[1.38807019,1.850220317,0],
	[3.06407232,3.005305973,0],
	[7.627531214,2.759262235,1],
	[5.332441248,2.088626775,1],
	[6.922596716,1.77106367,1],
	[8.675418651,-0.242068655,1],
	[7.673756466,3.508563011,1]
]

for n in range(len(dataset)):
    prediction = predict_classification(dataset, dataset[n], 5)
    print('Expected %d, Got %d.' % (dataset[n][-1], prediction))

# Expected 0, Got 0.
# Expected 0, Got 0.
# Expected 0, Got 0.
# Expected 0, Got 0.
# Expected 0, Got 0.
# Expected 1, Got 1.
# Expected 1, Got 1.
# Expected 1, Got 1.
# Expected 1, Got 1.
# Expected 1, Got 1.
</code></pre>
<p>运行结果打印了预期分类与从数据集中 3 个相进邻居预测结果是一直的。</p>
<h2 id="鸢尾花种实例">鸢尾花种实例</h2>
<p>这里使用的是 <a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv" target="_blank"
   rel="noopener nofollow noreferrer" >Iris Flower Species</a> 数据集。</p>
<p>鸢尾花数据集是根据鸢尾花的测量值预测花卉种类。这是一个多类分类问题。每个类的观察数量是平衡的。有 150 个观测值，有 4 个输入变量和 1 个输出变量。变量名称如下：</p>
<ul>
<li>萼片长度以厘米为单位。</li>
<li>萼片宽度以厘米为单位。</li>
<li>花瓣长度以厘米为单位。</li>
<li>花瓣宽度以厘米为单位。</li>
<li>真实类型</li>
</ul>
<p>更多的关于数据集的说明可以参考：<a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names" target="_blank"
   rel="noopener nofollow noreferrer" >Iris-databases数据集的说明</a></p>
<h3 id="prerequisite-1">Prerequisite</h3>
<p>实验的步骤大概分为如下：</p>
<ul>
<li>加载数据集并将数据转换为可用于均值和标准差计算的数字。将属性转为float，将类别转换为int。</li>
<li>使 5折的<strong>K</strong>折较差验证（<strong>K-Fold CV</strong>）评估该算法。</li>
</ul>
<h3 id="start">Start</h3>
<pre><code class="language-python">from random import seed
from random import randrange
from csv import reader
from math import sqrt

# 加载CSV
def load_csv(filename):
    dataset = list()
    with open(filename, 'r') as file:
        csv_reader = reader(file)
        for row in csv_reader:
            if not row:
                continue
            dataset.append(row)
    return dataset

# 转换所有的值为float方便运算
def str_column_to_float(dataset, column):
    for row in dataset:
        row[column] = float(row[column].strip())

# 转换所有的类型为int
def str_column_to_int(dataset, column):
    class_values = [row[column] for row in dataset]
    unique = set(class_values)
    lookup = dict()
    for i, value in enumerate(unique):
        lookup[value] = i
    for row in dataset:
        row[column] = lookup[row[column]]
    return lookup



# # k-folds CV函数进行划分
def cross_validation_split(dataset, n_folds):
    dataset_split = list()
    dataset_copy = list(dataset)
    # 平均分成n_folds折数
    fold_size = int(len(dataset) / n_folds)
    for _ in range(n_folds):
        fold = list()
        while len(fold) &lt; fold_size:
            index = randrange(len(dataset_copy))
            fold.append(dataset_copy.pop(index))
        dataset_split.append(fold)
    return dataset_split

# 计算精确度
def accuracy_metric(actual, predicted):
    correct = 0
    for i in range(len(actual)):
        if actual[i] == predicted[i]:
            correct += 1
    return correct / float(len(actual)) * 100.0

# 评估算法
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
    &quot;&quot;&quot;
    评估算法，计算算法的精确度
    :param dataset: list, 数据集
    :param algorithm: function, 算法名
    :param n_folds: int，折数
    :param args: 用于algorithm的参数
    :return: None
    &quot;&quot;&quot;
    folds = cross_validation_split(dataset, n_folds) # 分成5折
    scores = list()
    for fold in folds:
        train_set = list(folds)
        train_set.remove(fold) # 训练集不包含本身
        train_set = sum(train_set, [])
        test_set = list() # 测试集
        for row in fold:
            row_copy = list(row)
            test_set.append(row_copy)
            row_copy[-1] = None
        predicted = algorithm(train_set, test_set, *args)
        actual = [row[-1] for row in fold]
        accuracy = accuracy_metric(actual, predicted)
        scores.append(accuracy)
    return scores

# 欧几里得距离，计算两个向量间距离的算法
def euclidean_distance(row1, row2):
    distance = 0.0
    for i in range(len(row1)-1):
        distance += (row1[i] - row2[i])**2
    return sqrt(distance)

# 确定最邻近的邻居
def get_neighbors(train, test_row, num_neighbors):
    &quot;&quot;&quot;
    计算训练集train中所有元素到test_row的距离
    :param train: list, 数据集，可以是训练集
    :param test_row: list, 新的实例，也就是K
    :param num_neighbors:int，需要多少个邻居
    :return: None
    &quot;&quot;&quot;
    distances = list()
    for train_row in train:
        dist = euclidean_distance(test_row, train_row)
        distances.append((train_row, dist))
    distances.sort(key=lambda tup: tup[1])
    neighbors = list()
    for i in range(num_neighbors):
        neighbors.append(distances[i][0])
    return neighbors

# 与临近值进行比较并预测
def predict_classification(train, test_row, num_neighbors):
    &quot;&quot;&quot;
    计算训练集train中所有元素到test_row的距离
    :param train: list, 数据集，可以是训练集
    :param test_row: list, 新的实例，也就是K
    :param num_neighbors:int，需要多少个邻居
    :return: None
    &quot;&quot;&quot;
    neighbors = get_neighbors(train, test_row, num_neighbors)
    output_values = [row[-1] for row in neighbors]
    prediction = max(set(output_values), key=output_values.count)
    return prediction

# kNN Algorithm
def k_nearest_neighbors(train, test, num_neighbors):
    predictions = list()
    for row in test:
        output = predict_classification(train, row, num_neighbors)
        predictions.append(output)
    return(predictions)

# 使用KNN算法计算鸢尾花数据集
seed(1)
filename = 'iris.csv'
dataset = load_csv(filename)
for i in range(len(dataset[0])-1):
    str_column_to_float(dataset, i)
# 转换类型为int
str_column_to_int(dataset, len(dataset[0])-1)
# 评估算法
n_folds = 5 # 5折
num_neighbors = 5 #取5个邻居
scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)
print('Scores: %s' % scores)
print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))

# Scores: [96.66666666666667, 96.66666666666667, 100.0, 90.0, 100.0]
# Mean Accuracy: 96.667%
</code></pre>
<p>上述是对整个数据集的预测百分比，也可以对对应的类的信息进行输出</p>
<p>首先在类别转换函数 <code>str_column_to_int</code> 中增加打印方法</p>
<pre><code class="language-python">for i, value in enumerate(unique):
    lookup[value] = i
    print('[%s] =&gt; %d' % (value, i))
</code></pre>
<p>然后在定义一个新的实例，这个实例是用于预测的信息 <code>row = [5.7,2.9,4.2,1.3]</code> ; 然后修改需要预测的数据，进行预测</p>
<pre><code class="language-python"># 原来的整个数据集打分不需要了
# scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)
# print('Scores: %s' % scores)
# print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))

# 定义一个新数据
row = [5.7,2.9,4.2,1.3]

label = predict_classification(dataset, row, num_neighbors)
print('Data=%s, Predicted: %s' % (row, label))

# Data=[5.7, 2.9, 4.2, 1.3], Predicted: 1
</code></pre>
<p>通过预测，可以看出预测结果属于第 1 类，就知道该花为 <code>Iris-setosa</code> 。</p>
<blockquote>
<p>Reference</p>
<p><a href="https://machinelearningmastery.com/distance-measures-for-machine-learning/" target="_blank"
   rel="noopener nofollow noreferrer" >distance measures</a></p>
<p><a href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/" target="_blank"
   rel="noopener nofollow noreferrer" >k nearest neighbors implement</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>决策边界算法</title>
      <link>https://www.oomkill.com/2022/06/decision-boundary/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/decision-boundary/</guid>
      <description></description>
      <content:encoded><![CDATA[<p><strong>决策边界</strong> (<code>decision boundary</code>)</p>
<p>支持向量机获取这些数据点并输出最能分离标签的超平面。这条线是<strong>决策边界</strong></p>
<p>决策平面 （ <code>decision surface</code> ），是将空间划分为不同的区域。位于决策平面一侧的数据被定义为与位于另一侧的数据属于不同的类别。决策面可以作为学习过程的结果创建或修改，它们经常用于机器学习、模式识别和分类系统。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/Using-Eq-18-as-decision-surface-for-classifying-with-two-overlapped-data-classes.png" alt="Using Eq. (18) as decision surface for classifying, with two overlapped data classes" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>环境空间 ( <code>Ambient Space</code>)，围绕数学对象即对象本身的空间，如一维 <code>Line</code> ，可以独立研究，这种情况下L则是L；再例如将L作为二维空间 $R^2$ 的对象进行研究，这种情况下 <strong>L</strong> 的环境空间是 $R^2$。</p>
<p>超平面（<code>Hyperplane</code>）是一个子空间， N维空间的超平面是其具有维数的平面的子集。就其性质而言，它将空间分成两个半空间，其维度比其<strong>环境空间</strong>的维度小 <strong>1</strong>。如果空间是三维的，那么它的超平面就是二维维平面，而如果空间是 2 维的，那么它的超平面就是一维线。支持向量机 (SVM) 通过找到使两个类之间的边距最大化的超平面来执行分类。</p>
<p>法向量 （<code>Normal</code>） 是垂直于该平面、另一个向量的 90° 角倾斜</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/220px-Normal_vectors2.svg.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="什么是支持向量">什么是支持向量</h2>
<p>支持向量 （<code>Support vectors</code>），靠近决策平面（超平面）的数据点。</p>
<p>如图所示，从一维平面来看，哪个是分离的超平面？</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220530222414946.png" alt="image-20220530222414946" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>一般而言，会有很多种解决方法（超平面），支持向量机就是如何找到最佳方法的解决方案。</p>
<p>转置运算</p>
<p>矩阵的转置是原始矩阵的翻转版本，可以通过转换矩阵的行和列来转置矩阵。我们用 $A^T$ 表示矩阵 A 的转置。例如，$$A=\left[
\begin{matrix}
1 &amp; 2 &amp; 3 \
4 &amp; 5 &amp; 6 \
\end{matrix}
\right]$$ ；那么 A 的转置就为 $$A=\left[
\begin{matrix}
1 &amp; 4 \
2 &amp; 5 \
3 &amp; 6 \
\end{matrix}
\right]$$ ；</p>
<p>我们可以将向量的转置作为特例。由于 n 维向量 x 由 n×1 列矩阵表示：$$x=\left[
\begin{matrix}
x_1 \
x_2 \
x_3 \
&hellip;.  \
x_n \
\end{matrix}
\right]$$ ；那么 <strong>x</strong> 的转置（$x^T$）是一个 $1\times n$ 行矩阵 $$x^T=\left[
\begin{matrix}
x_1 &amp; x_2 &amp; x_3 &amp; &hellip; &amp; x_n \
\end{matrix}
\right]$$ 。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/200px-Matrix_transpose-16539874514953.gif" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>权重向量</p>
<p>$wx+b=0$  w：权重向量 x n维向量  $x_i=[1,2,3&hellip;n]$ $w_i=[1,2,3&hellip;n]$ 每个输入的值都乘以一个“权重” $w_i$。权重是表示计算输出时每个输入的重要性的值</p>
<p>权重决定了输入对输出的影响程度。 $Y=\sum(Weight \times input)+bias$ ；如果输入为 $[x_1,x_2\ &hellip; ,x_n]$ 权重是：$[w_1,w_2\ \ ,w_n]$</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/assets%252F-LvBP1svpACTB1R1x_U4%252F-LvI8vNq_N7u3RWVAPLk%252F-LvJSdcFXzoI-WW0L3w5%252Fimage.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>通过场景来理解</p>
<p>假设预估汽车的价格，汽车的价格取决于制造年份和行驶里程数。让我们假设汽车的年份越高，汽车价格越高。随后，汽车开得越多，汽车就越便宜。</p>
<p>这个例子应该可以帮助您了解汽车价格与制造年份之间存在正相关关系，而汽车价格与其行驶里程之间存在负关关系。因此，我们希望看到代表年份的特征的权重为正，代表里程的特征的权重为负。公式为：$car = (w_1x\ ear+w_2x\ miles)$</p>
<p>偏差 <strong>bais</strong> 是一个常数 <code>const</code> ，偏差用于将影响函数的结果向正或负方向移动。<code>bias</code> 会被被添加到 <strong>input</strong> 和 <strong>weight</strong> 的乘积中。偏差用于抵消结果。$x_1w_1+x_2w_2&hellip;x_nw_n+bias$</p>
<p>通过场景来理解</p>
<p>假设希望在输入为 0 时返回 2。由于权重和输入的乘积之和为 0，您将如何确保返回 2？<strong>此时可以添加2的bias</strong>。如果不包含偏差，只是对 <strong>input</strong> 和 <strong>weight</strong>  执行矩阵乘法。这将很容易导致过度拟合数据集。</p>
<blockquote>
<p><strong>过度拟合</strong>（overfitting）是指机器学习算模型在训练集上的误差和测试集上的误差之间差异过大。造成过度拟合的原因可能有多种．最常见的就是模型容量过高，模型过于复杂，换句话说是模型假设所包含的参数数量过多．如此一来，算法会将训练集中所包含的没有普遍性的一些特征也学习进来，结果降低了模型的泛化能力．</p>
</blockquote>
<p><a href="https://machine-learning.paperspace.com/wiki/weights-and-biases" target="_blank"
   rel="noopener nofollow noreferrer" >https://machine-learning.paperspace.com/wiki/weights-and-biases</a></p>
<p>范数</p>
<p>向量的范数（<strong>norm</strong>）是它的长度 ，x的范数表示为 $\parallel x \parallel$；常用的范数为 P 范数，其中 P 是大于等于1的任何数，向量 <strong>x</strong>的 p 范数表示为 $\parallel x \parallel_p$ ；通常情况下向量 x 的 p 范数的计算公式为： $\parallel x \parallel_p = (x_1^p+x_2^p+x_3^p+ \ &hellip;\ x_n^p)^{1/p}$ ；公式可以简写为：$\parallel x \parallel_p =  (\sum_{i=1}^n\ x_i^p)^{1/p}$</p>
<p>曼哈顿距离</p>
<p>曼哈顿距离也被称为1-范数 <code>1-norm</code>，因为它测量的中两点之间的距离。假设：向量a，我们必须计算 1-范式 $\vec{a} = [2,3]$  ，在图像中表示（红色线部分表示向量a的1-范式）</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1PU0J-FJWvTj37huxQjWy8g.png" alt="1-norm" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>通过公式来计算1-范式，可以将p替换为1，$\parallel a \parallel_1 = (x_1 + x_2) = (2+3)^1=5$</p>
<p>欧几里得范数</p>
<p>欧几里得范数又被称作2-范数 <code>2-norm</code> ，是范数中最常用的范数，欧几里得范数返回的是两点之间最短的距离，因此 $\vec{a}$ 的2-范式为 $\parallel x \parallel_2 = (2^2+3^2)^{\frac{1}{2}} = (4+9)^{\frac{1}{2}} = \sqrt{13}$ ；用图像表示为（红线部分表示2-范数，这是 $\vec{a}$ 表示的点到点之间的最低按距离）</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1tFNCvthlEe_ajinO4ngtEg.png" alt="2-norm" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>无穷范数</p>
<p>无穷范数 <code>Infinity-norm</code> 是返回给定向量中的最大绝对值；$\vec{a}$ 的无穷范式为  $\parallel a \parallel_\infty = 3$ （公式求得是上述图中 $[2,3]$ 这个实例）。</p>
<p>例如，如果我们必须找到一个向量的无穷范数，比如 $\vec{b}$  ，$\vec{b} = [4,3,-1]$ ；那么  $\parallel b \parallel_\infty = 6$ （这里最大是4，但是返回的是一个绝对值所以是<strong>6</strong>）</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://medium.com/linear-algebra/part-18-norms-30a8b3739bb" target="_blank"
   rel="noopener nofollow noreferrer" >norm</a></p>
<p><a href="https://machinelearningmastery.com/vector-norms-machine-learning/" target="_blank"
   rel="noopener nofollow noreferrer" >vector norms</a></p>
</blockquote>
<p><strong>拉格朗日乘子法</strong></p>
<p><strong>拉格朗日乘子法</strong> <code>Lagrange multiplier</code>，是一种寻找受<a href="https://en.wikipedia.org/wiki/Constraint_%28mathematics%29" target="_blank"
   rel="noopener nofollow noreferrer" >等式约束</a>的<a href="https://en.wikipedia.org/wiki/Function_%28mathematics%29" target="_blank"
   rel="noopener nofollow noreferrer" >函数的局部</a><a href="https://en.wikipedia.org/wiki/Maxima_and_minima" target="_blank"
   rel="noopener nofollow noreferrer" >最大值和最小值</a>的策略（即，必须满足一个或多个<a href="https://en.wikipedia.org/wiki/Equation" target="_blank"
   rel="noopener nofollow noreferrer" >方程</a>必须完全满足所选<a href="https://en.wikipedia.org/wiki/Variable_%28mathematics%29" target="_blank"
   rel="noopener nofollow noreferrer" >变量</a>值的条件）</p>
<p>设置超平面为 $wx+b=0$ ，其中 $w=[1,2,\ ..,\ n]$ ，w是 $n \times 1$ 维，n特征值的个数，x 训练的示例，b是bias，一个二维的超平面的特征为：$x=[x_1,x_2]$ ，$w=[w_1,w_2]$ ，b看做 wegiht $w_0$ ，</p>
<p>那么这个超平面的方程就为：$$
f(n)
\begin{cases}
w_1x_1+w_2x_2+w0 = 0\ \  超平面(决策边界)方程 \
w_1x_1+w_2x_2+w0 &gt; 0\ \ 超平面(决策边界)上部分 \
w_1x_1+w_2x_2+w0 &lt; 0\ \ 超平面(决策边界)下部分 \
\end{cases}
$$ ，那么在对公式进行分解，增加参数 <code>y</code> ，代表了对向量的分类，也就是说超平面两边的向量，这样公式为：$$
f(n)
\begin{cases}
w_1x_1+w_2x_2+w0 \ge 1\ \  当 y_i = +1 \
w_1x_1+w_2x_2+w0 \le 1\ \   当 y_i = -1 \
\end{cases}
$$</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>决策树</title>
      <link>https://www.oomkill.com/2022/06/decision-tree/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/decision-tree/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="熵和基尼指数">熵和基尼指数</h2>
<h3 id="信息增益">信息增益</h3>
<p>信息增益 <code>information gain</code> 是用于训练决策树的指标。具体来说，是指这些指标衡量<strong>拆分的质量</strong>。通俗来说是通过根据随机变量的给定值拆分数据集来衡量熵。</p>
<p>通过描述一个事件是否&quot;惊讶&quot;，通常低概率事件更令人惊讶，因此具有更大的信息量。而具有相同可能性的事件的概率分布更&quot;惊讶&quot;并且具有更大的熵。</p>
<p><strong>定义</strong>：熵 <strong>entropy</strong>是一组例子中<strong>杂质</strong>、<strong>无序</strong>或<strong>不确定性</strong>的度量。熵控制决策树如何决定<strong>拆分</strong>数据。它实际上影响了决策树如何绘制边界。</p>
<h3 id="熵">熵</h3>
<p>熵的计算公式为：$E=-\sum^i_{i=1}(p_i\times\log_2(p_i))$ ；$P_i$ 是类别 $i$ 的概率。我们来举一个例子来更好地理解熵及其计算。假设有一个由三种颜色组成的数据集，红色、紫色和黄色。如果我们的集合中有一个红色、三个紫色和四个黄色的观测值，我们的方程变为：$E=-(p_r \times \log_2(p_r) + p_p \times \log_2(p_p) + p_y \times \log_2(p_y)$</p>
<p>其中 $p_r$ 、$p_p$ 和 $p_y$ 分别是选择红色、紫色和黄色的概率。假设 $p_r=\frac{1}{8}$，$p_p=\frac{3}{8}$ ，$p_y=\frac{4}{8}$ 现在等式变为变为：</p>
<ul>
<li>$E=-(\frac{1}{8} \times \log_2(\frac{1}{8}) + \frac{3}{8} \times \log_2(\frac{3}{8}) + \frac{4}{8} \times \log_2(\frac{4}{8}))$</li>
<li>$0.125 \times log_2(0.125) + 0.375 \times log_2(0.375) + 0.5 \times log_2(0.375)$</li>
<li>$0.125 \times -3 + 0.375 \times -1.415 + 0.5 \times -1 = -0.375+-0.425 +-0.5 = 1.41$</li>
</ul>
<p>==当所有观测值都属于同一类时会发生什么？== 在这种情况下，熵将始终为零。$E=-(1log_21)=0$ ；这种情况下的数据集没有杂质，这就意味着没有数据集没有意义。又如果有两类数据集，一半是黄色，一半是紫色，那么熵为1，推导过程是：$E=−(\ (0.5\log_2(0.5))+(0.5\times \log_2(0.5))\ ) = 1$</p>
<h3 id="基尼指数">基尼指数</h3>
<p>基尼指数 <code>Gini index</code> 和熵 <code>entropy </code> 是计算信息增益的标准。决策树算法使用信息增益来拆分节点。</p>
<p>基尼指数计算特定变量在随机选择时被错误分类的概率程度以及基尼系数的变化。它适用于分类变量，提供“成功”或“失败”的结果，因此仅进行二元拆分（二叉树结构）。基尼指数在 0 和 1 之间变化，其中，1 表示元素在各个类别中的随机分布。基尼指数为 0.5 表示元素在某些类别中分布均匀。：</p>
<ul>
<li>0 表示为所有元素都与某个类相关联，或只存在一个类。</li>
<li>1 表示所有元素随机分布在各个类中，并且0.5 表示元素均匀分布到某些类中</li>
</ul>
<p>基尼指数公式：$1− \sum_n^{i=1}(p_i)^2$ ； $P_i$ 为分类到特定类别的概率。在构建决策树时，更愿意选择具有最小基尼指数的属性作为根节点。</p>
<p>通过实例了解公式</p>
<table>
<thead>
<tr>
<th><strong>Past Trend</strong></th>
<th><strong>Open Interest</strong></th>
<th><strong>Trading Volume</strong></th>
<th><strong>Return</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive</td>
<td>Low</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Negative</td>
<td>High</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Positive</td>
<td>High</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Negative</td>
<td>Low</td>
<td>High</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Negative</td>
<td>High</td>
<td>High</td>
<td>Down</td>
</tr>
<tr>
<td>Negative</td>
<td>Low</td>
<td>High</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>High</td>
<td>High</td>
<td>Up</td>
</tr>
</tbody>
</table>
<p>计算基尼指数</p>
<p>已知条件</p>
<ul>
<li>
<p>$P(Past\ Trend=Positive) = \frac{6}{10}$</p>
</li>
<li>
<p>$P(Past\ Trend=Negative) = \frac{4}{10}$</p>
</li>
</ul>
<p>过去趋势基尼指数计算</p>
<p>如果过去趋势为正面，回报为上涨，概率为：$P(Past\ Trend=Positive\ &amp;\ Return=Up) = \frac{4}{6}$</p>
<p>如果过去趋势为正面，回报为下降，概率为：$P(Past\ Trend=Positive\ &amp;\ Return=Down) = \frac{2}{6}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Past\ Trend) = 1-(\frac{4}{6}^2+\frac{2}{6}^2) = 0.45$</li>
</ul>
<p>如果过去趋势为负面，回报为上涨，概率为：$P(Past\ Trend=Negative\ &amp;\ Return=Up) = 0$</p>
<p>如果过去趋势为负面，回报为下降，概率为：$P(Past\ Trend=Negative\ &amp;\ Return=Down) = \frac{4}{4}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Past\ Trend=Negative) = 1-(0^2+\frac{4}{4}^2) = 1-(0+1)=0$</li>
</ul>
<p>那么过去交易量的的基尼指数加权 = $\frac{6}{10} \times 0.45 + \frac{4}{10}\times 0 = 0.27$</p>
<p>未平仓量基尼指数计算</p>
<p>已知条件</p>
<ul>
<li>$P(Open\ Interest=High): \frac{4}{10}$</li>
<li>$P(Open\ Interest=Low): \frac{6}{10}$</li>
</ul>
<p>如果未平仓量为 <code>high</code> 并且回报为上涨，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Up)=\frac{2}{4}$</p>
<p>如果未平仓量为 <code>high</code> 并且回报为下降，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Down)=\frac{2}{4}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Open\ Interest=High) = 1-(\frac{2}{4}^2+\frac{2}{4}^2) = 0.5$</li>
</ul>
<p>如果未平仓量为 <code>low</code> 并且回报为上涨，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Up)=\frac{2}{6}$</p>
<p>如果未平仓量为 <code>low</code> 并且回报为下降，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Down)=\frac{4}{6}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Open\ Interest=Low) = 1-(\frac{2}{6}^2+\frac{4}{6}^2) = 0.45$</li>
</ul>
<p>那么未平仓量基尼指数加权 = $\frac{4}{10} \times 0.5 + \frac{6}{10}\times 0.45 = 0.47$</p>
<p>计算交易量基尼指数</p>
<p>已知条件</p>
<ul>
<li>$P(Trading\ Volume=High): \frac{7}{10}$</li>
<li>$P(Trading\ Volume=Low): \frac{3}{10}$</li>
</ul>
<p>如果交易量为 <code>high</code> 并且回报为上涨，概率为：$P(Trading\ Volume=High\ &amp;\ Return\ = Up)=\frac{4}{7}$</p>
<p>如果交易量为 <code>high</code> 并且回报为下降，概率为：$P(Trading\ Volume = High\ &amp;\ Return\ = Down)=\frac{3}{7}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Trading\ Volume=High) = 1-(\frac{4}{7}^2+\frac{3}{7}^2) = 0.49$</li>
</ul>
<p>如果交易量为 <code>low</code> 并且回报为上涨，概率为：$P(Trading\ Volume = Low\ &amp;\ Return\ = Up)=0$</p>
<p>如果交易量为 <code>low</code> 并且回报为下降，概率为：$P(Trading\ Volume = Low\ &amp;\ Return\ = Down)=\frac{3}{3}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Trading\ Volume=Low) = 1-(0^2+1^2) = 0$</li>
</ul>
<p>那么交易量基尼指数加权 = $\frac{7}{10} \times 0.49 + \frac{3}{10}\times 0 = 0.34$</p>
<p>最终计算出的基尼指数列表如下，在表中可以观察到“<strong>Past Trend</strong>”的基尼指数最低，因此它将被选为决策树的根节点。</p>
<table>
<thead>
<tr>
<th><strong>Attributes</strong></th>
<th><strong>Gini Index</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Past Trend</td>
<td>0.27</td>
</tr>
<tr>
<td>Open Interest</td>
<td>0.47</td>
</tr>
<tr>
<td>Trading Volume</td>
<td>0.34</td>
</tr>
</tbody>
</table>
<p>这里将重复的过程来确定决策树的子节点或分支。将通过计算”<strong>Past Trend</strong>“的“<strong>Positive</strong>”分支的基尼指数如下：</p>
<table>
<thead>
<tr>
<th><strong>Past Trend</strong></th>
<th><strong>Open Interest</strong></th>
<th><strong>Trading Volume</strong></th>
<th><strong>Return</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive</td>
<td>Low</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Positive</td>
<td>High</td>
<td>High</td>
<td>Up</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>Low</td>
<td>Low</td>
<td>Down</td>
</tr>
<tr>
<td>Positive</td>
<td>High</td>
<td>High</td>
<td>Up</td>
</tr>
</tbody>
</table>
<p>针对过去正面趋势计算未平仓量的基尼指数</p>
<p>已知条件</p>
<ul>
<li>$P(Open\ Interest=High): \frac{2}{6}$</li>
<li>$P(Open\ Interest=Low): \frac{4}{6}$</li>
</ul>
<p>如果未平仓量为 <code>high</code> 并且回报为上涨，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Up)=\frac{2}{2}$</p>
<p>如果未平仓量为 <code>high</code> 并且回报为下降，概率为：$P(Open\ Interest = High\ &amp;\ Return\ = Down)=0$</p>
<ul>
<li>那么这个基尼指数为：$gini(Open\ Interest=High) = 1-(\frac{2}{2}^2+0^2) = 0$</li>
</ul>
<p>如果未平仓量为 <code>low</code> 并且回报为上涨，概率为：$P(Open\ Interest = Low\ &amp;\ Return\ = Up)=\frac{2}{4}$</p>
<p>如果未平仓量为 <code>low</code> 并且回报为下降，概率为：$P(Open\ Interest = Low\ &amp;\ Return\ = Down)=\frac{2}{4}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Open\ Interest=Low) = 1-(\frac{2}{4}^2+\frac{2}{4}^2) = 0.5$</li>
</ul>
<p>那么未平仓量基尼指数加权 = $\frac{2}{6} \times 0 + \frac{4}{6}\times 0.5 = 0.33$</p>
<p>计算交易量基尼指数</p>
<p>已知条件</p>
<ul>
<li>$P(Trading\ Volume=High): \frac{4}{6}$</li>
<li>$P(Trading\ Volume=Low): \frac{2}{6}$</li>
</ul>
<p>如果交易量为 <code>high</code> 并且回报为上涨，概率为：$P(Trading\ Volume=High\ &amp;\ Return\ = Up)=\frac{4}{4}$</p>
<p>如果交易量为 <code>high</code> 并且回报为下降，概率为：$P(Trading\ Volume = High\ &amp;\ Return\ = Down)=0$</p>
<ul>
<li>那么这个基尼指数为：$gini(Trading\ Volume=High) = 1-(\frac{4}{4}^2+0^2) = 0$</li>
</ul>
<p>如果交易量为 <code>low</code> 并且回报为上涨，概率为：$P(Trading\ Volume = Low\ &amp;\ Return\ = Up)=0$</p>
<p>如果交易量为 <code>low</code> 并且回报为下降，概率为：$P(Trading\ Volume = Low\ &amp;\ Return\ = Down)=\frac{2}{2}$</p>
<ul>
<li>那么这个基尼指数为：$gini(Trading\ Volume=Low) = 1-(0^2+\frac{2}{2}^2) = 0$</li>
</ul>
<p>那么交易量基尼指数加权 = $\frac{4}{6} \times 0 + \frac{2}{6}\times 0 = 0$</p>
<p>最终计算出的基尼指数列表如下，这里将使用“<strong>Trading Volume</strong>”进一步拆分节点，因为它具有最小的基尼指数。</p>
<table>
<thead>
<tr>
<th><strong>Attributes/Features</strong></th>
<th><strong>Gini Index</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Open Interest</td>
<td>0.33</td>
</tr>
<tr>
<td>Trading Volume</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>最终的模型就如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220602000050768.png" alt="image-20220602000050768" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="计算信息增益示例">计算信息增益示例</h4>
<p>我们可以根据属于一类数据的概率分布来考虑数据集的熵，例如，在二进制分类数据集的情况下为两个类。计算样本的熵如 $Entropy = -(P_0 \times log(P_0) + P_1 \times log(P_1)$ 。</p>
<p>两类的样本拆分为 <code>50/50</code> 的数据集将具最大熵（最惊讶），而拆分为 <code>10/90</code> 的不平衡数据集将具有较小的熵。可以通过在 Python 中计算这个不平衡数据集的熵的例子来证明这一点。</p>
<pre><code class="language-python">from math import log2
# 概率
class0 = 10/100
class1 = 90/100
# entropy formula
entropy = -(class0 * log2(class0) + class1 * log2(class1))
# print the result
print('entropy: %.3f bits' % entropy)
</code></pre>
<p>运行示例，可以看到用于二分类的数据集的熵小于 1 。也就是说，对来自数据集中的任意示例类进行编码所需的信息不到1。通过这种方式，熵可以用作数据集纯度的计算，例如类别分布的平衡程度。</p>
<p>熵为 0 位表示数据集包含一个类；1或更大位的熵表示平衡数据集的最大熵（取决于类别的数量），介于两者之间的值表示这些极端之间的水平。</p>
<h3 id="计算信息增益示例-1">计算信息增益示例</h3>
<p>要求：定义一个函数来根据属于 0 类和 1 类的样本的比率来计算一组样本的熵。</p>
<p>假设有一个20 个示例的数据集，13 个为0 类，7 个为1 类。我们可以计算该数据集的熵，它的熵小于 1 位。</p>
<pre><code class="language-python">from math import log2
# calculate the entropy for the split in the dataset
def entropy(class0, class1):
	return -(class0 * log2(class0) + class1 * log2(class1))

# split of the main dataset
class0 = 13 / 20
class1 = 7 / 20
# calculate entropy before the change
s_entropy = entropy(class0, class1)
print('Dataset Entropy: %.3f bits' % s_entropy)

# Dataset Entropy: 0.934 bits
</code></pre>
<p>假设按照 value1 分割数据集，有一组 8 个样本的数据集，7 个为第 0 类，1 个用于第 1 类。然后我们可以计算这组样本的熵。</p>
<pre><code class="language-python">from math import log2
# calculate the entropy for the split in the dataset
def entropy(class0, class1):
	return -(class0 * log2(class0) + class1 * log2(class1))

# split of the main dataset
s1_class0 = 7 / 8
s1_class1 = 1 / 8
# calculate entropy before the change
s_entropy = entropy(s1_class0, s1_class1)
print('Dataset Entropy: %.3f bits' % s_entropy)

# Dataset Entropy: 0.544 bits
</code></pre>
<p>假设现在按 value2 分割数据集；一组 12 个样本数据集，每组 6 个。我们希望这个组的熵为 1。</p>
<pre><code class="language-python">from math import log2
# calculate the entropy for the split in the dataset
def entropy(class0, class1):
	return -(class0 * log2(class0) + class1 * log2(class1))

# split of the main dataset
s1_class0 = 6 / 12
s1_class1 = 6 / 12
# calculate entropy before the change
s_entropy = entropy(s1_class0, s1_class1)
print('Dataset Entropy: %.3f bits' % s_entropy)

# Dataset Entropy: 1.000 bits
</code></pre>
<p>最后，可以根据为变量的每个值创建的组和计算的熵来计算该变量的信息增益。例如：</p>
<p>第一个变量从数据集中产生一组 8 个样本，第二组在数据集中有12 个样本。在这种情况下，信息增益计算：</p>
<ul>
<li>$Entropy(Dataset) – (\frac{(Count(Group1)}{Count(Dataset)} \times Entropy(Group1) + \frac{Count(Group2)}{Count(Dataset)} \times Entropy(Group2)))$</li>
</ul>
<p>这里是因为在每个子节点重复这个分裂过程直到空叶节点。这意味着每个节点的样本都属于同一类。但是，这种情况下会导致具有许多节点使非常<strong>深的树</strong>，这很容易导致过度拟合。因此，我们通常希望通过设置树的最大深度来修剪树。IG就是我们想确定给定训练特征向的量集中的<strong>哪个属性最有用</strong>，那么上面的公式推理就为：</p>
<ul>
<li>$IG(D_p) = I(D_p) − \frac{N_{left}}{N_p}I(D_{left})−\frac{N_{right}}{N_p}I(D_{right})$
<ul>
<li>$IG(D_P)$：数据集的信息增益</li>
<li>$I(D)$：叶子的熵或基尼指数</li>
<li>$\frac{N}{N_P}$ ：页数据集占总数据集的比例</li>
</ul>
</li>
</ul>
<p>我们将使用它来决定<strong>决策树</strong> 节点中<strong>属性的顺序</strong>。该行为在python中表示为：</p>
<pre><code class="language-python">from math import log2
 
# calculate the entropy for the split in the dataset
def entropy(class0, class1):
	return -(class0 * log2(class0) + class1 * log2(class1))
 
# split of the main dataset
class0 = 13 / 20
class1 = 7 / 20
# calculate entropy before the change
s_entropy = entropy(class0, class1)
print('Dataset Entropy: %.3f bits' % s_entropy)
 
# split 1 (split via value1)
s1_class0 = 7 / 8
s1_class1 = 1 / 8
# calculate the entropy of the first group
s1_entropy = entropy(s1_class0, s1_class1)
print('Group1 Entropy: %.3f bits' % s1_entropy)
 
# split 2  (split via value2)
s2_class0 = 6 / 12
s2_class1 = 6 / 12
# calculate the entropy of the second group
s2_entropy = entropy(s2_class0, s2_class1)
print('Group2 Entropy: %.3f bits' % s2_entropy)
 
# calculate the information gain
gain = s_entropy - (8/20 * s1_entropy + 12/20 * s2_entropy)
print('Information Gain: %.3f bits' % gain)


# Dataset Entropy: 0.934 bits
# Group1 Entropy: 0.544 bits
# Group2 Entropy: 1.000 bits
# Information Gain: 0.117 bits
</code></pre>
<p>通过实例，就可以很清楚的明白了，信息增益的概念：<strong>信息熵-条件熵</strong>，换句话来说就是==信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度==。</p>
<h2 id="python计算决策树实例">python计算决策树实例</h2>
<h3 id="基于基尼指数的决策树">基于基尼指数的决策树</h3>
<p>钞票数据集涉及根据从照片中采取的一系列措施来预测给定钞票是否是真实的。数据是取自真钞和伪钞样样本的图像中提取的。对于数字化，使用了通常用于印刷检查的工业相机，从图像中提取特征。</p>
<p>该数据集包含 1372 行和 5 个数值变量。这是一个二元分类的问题。</p>
<h4 id="基尼指数-1">基尼指数</h4>
<p>假设有两组数据，每组有 2 行。第一组的行都属于 0 类，第二组的行都属于 1 类，所以这是一个完美的拆分。</p>
<p>首先需要计算每个组中类的比例。</p>
<pre><code class="language-python">proportion = count(class_value) / count(rows)
</code></pre>
<p>这个比例是</p>
<pre><code class="language-python">group_1_class_0 = 2 / 2 = 1
group_1_class_1 = 0 / 2 = 0
group_2_class_0 = 0 / 2 = 0
group_2_class_1 = 2 / 2 = 1
</code></pre>
<p>为每个子节点计算 Gini index</p>
<pre><code class="language-python">gini_index = sum(proportion * (1.0 - proportion))
gini_index = 1.0 - sum(proportion * proportion)
</code></pre>
<p>然后对每组的基尼指数按组的大小加权，例如当前正在分组的所有样本。我们可以将此权重添加到组的基尼指数计算中，如下所示：</p>
<pre><code class="language-python">gini_index = (1.0 - sum(proportion * proportion)) * (group_size/total_samples)
</code></pre>
<p>在该案例中，每个组的基尼指数为：</p>
<pre><code class="language-python">Gini(group_1) = (1 - (1*1 + 0*0)) * 2/4
Gini(group_1) = 0.0 * 0.5 
Gini(group_1) = 0.0 # 分类1的基尼指数
Gini(group_2) = (1 - (0*0 + 1*1)) * 2/4
Gini(group_2) = 0.0 * 0.5 
Gini(group_2) = 0.0 # 分类2的基尼指数
</code></pre>
<p>然后在分割点的每个子节点上添加分数，以给出分割点的最终 Gini 分数，该分数可以与其他候选分割点进行比较。如该分割点的基尼系数为 $0.0 + 0.0$ 或完美的基尼系数 0.0。</p>
<p>编写一个 <code>gini_index()</code> 的函数，用于计算组列表和已知类值列表的基尼指数。</p>
<pre><code class="language-python">def gini_index(groups, classes):
    print(&quot;------------&quot;)
    # 计算所有样本的分割点，计算样本的总长度
    n_instances = float(sum([len(group) for group in groups]))
    # 计算每个组的总基尼指数
    gini = 0.0
    for group in groups:
        size = float(len(group))
        if size == 0: # avoid divide by zero
            continue
        score = 0.0
        # score the group based on the score for each class
        for class_val in classes:
            # row[-1] 代表每个样本的最后一个值，是否存在分类 class_val
            p = [row[-1] for row in group]
            p1 = p.count(class_val) / size
            score += p1 * p1
        # 按照对应的样本分割点，加权重
        gini += (1.0 - score) * (size / n_instances)
    return gini

print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))
print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))
</code></pre>
<p>运行该示例会打印两组的Gini index，最差情况的为 0.5，最少情况的指数为 0.0。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220602215655808.png" alt="image-20220602215655808" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="拆分">拆分</h4>
<h5 id="数据拆分">数据拆分</h5>
<p>拆分是由数据集中的一个属性和一个值组成。可以将其总结为要拆分的属性的索引和拆分该属性上的行的值。这只是索引数据行的有用简写。</p>
<p>创建拆分涉及三个部分，我们已经看过的第一个部分是计算基尼分数。剩下的两部分是：</p>
<ul>
<li>拆分数据集。</li>
<li>评估所有拆分。</li>
</ul>
<p>拆分数据是给定数据集索引和拆分值，将数据集拆分为两个行列表形成一个分类。具体是拆分数据集涉及遍历每一行，检查属性值是否低于或高于拆分值，并将其分别分配给左组或右组。当存在两个组时，可以按照基尼指数进行评估</p>
<p>编写一个**test_split()**函数，它实现了拆分。</p>
<pre><code class="language-python">def test_split(index, value, dataset):
	left, right = list(), list()
	for row in dataset:
		if row[index] &lt; value:
			left.append(row)
		else:
			right.append(row)
	return left, right
</code></pre>
<h5 id="评估拆分的数据">评估拆分的数据</h5>
<p>给定一个数据集，必须检查每个属性上的每个值作为候选拆分，评估拆分的成本并找到我们可以进行的最佳拆分。一旦找到最佳值，就可以将其用作决策树中的节点。</p>
<p>这里使用 <code>dict</code> 作为决策树中的节点，因为这样可以按名称存储数据。选择最佳基尼指数并将其用作树的新节点。</p>
<p>每组数据都是其小数据集，其中仅包含通过拆分过程分配给左组或右组的那些行。可以想象我们如何在构建决策树时递归地再次拆分每个组。</p>
<pre><code class="language-python">def get_split(dataset):
	class_values = list(set(row[-1] for row in dataset))
	b_index, b_value, b_score, b_groups = 999, 999, 999, None
	for index in range(len(dataset[0])-1):
		for row in dataset:
			groups = test_split(index, row[index], dataset)
			gini = gini_index(groups, class_values)
			if gini &lt; b_score:
				b_index, b_value, b_score, b_groups = index, row[index], gini, groups
	return {'index':b_index, 'value':b_value, 'groups':b_groups}
</code></pre>
<p>之后准备一些测试数据集进行测试，其中 $Y$ 是测试集的分类</p>
<pre><code>X1				X2				Y
2.771244718		1.784783929		0
1.728571309		1.169761413		0
3.678319846		2.81281357		0
3.961043357		2.61995032		0
2.999208922		2.209014212		0
7.497545867		3.162953546		1
9.00220326		3.339047188		1
7.444542326		0.476683375		1
10.12493903		3.234550982		1
6.642287351		3.319983761		1
</code></pre>
<p>将上述代码整合为一起，运行该代码后会打印所有基尼指数，基尼指数为 0.0 或完美分割。</p>
<pre><code class="language-python"># Split a dataset based on an attribute and an attribute value
def test_split(index, value, dataset):
    left, right = list(), list()
    for row in dataset:
        if row[index] &lt; value:
            left.append(row)
        else:
            right.append(row)
    return left, right

# Calculate the Gini index for a split dataset
def gini_index(groups, classes):
    # 计算两组数据集的总数每个种类的列表数量和
    n_instances = float(sum([len(group) for group in groups]))
    # 计算每组的基尼值
    gini = 0.0
    for group in groups:
        size = float(len(group))
        # avoid divide by zero
        if size == 0:
            continue
        score = 0.0
        # score the group based on the score for each class
        for class_val in classes:
            # 拿出数据集中每行的类型，拆开是为了更好的了解结构
            p = [row[-1] for row in group]
            # print(&quot;%f / %f = %f&quot; % (p.count(class_val), size, p.count(class_val) / size ))
            # 这里计算的是当前的分类在总数据集中占比
            p1 = p.count(class_val) / size
            score += p1 * p1 # gini index formula = 1 - sum(p_i^2)
        # 计算总的基尼指数，权重：当前分组占总数据集中的数量
        gini += (1.0 - score) * (size / n_instances)
    return gini

# Select the best split point for a dataset
def get_split(dataset):
    class_values = list(set(row[-1] for row in dataset))
    b_index, b_value, b_score, b_groups = 999, 999, 999, None
    for index in range(len(dataset[0])-1): # 最后分类不计算
        for row in dataset:
            # 根据每个值分类计算出最优基尼值，这个值就作为决策树的节点
            groups = test_split(index, row[index], dataset)
            gini = gini_index(groups, class_values)
            print('X%d &lt; %.3f Gini=%.3f' % ((index+1), row[index], gini))
            if gini &lt; b_score:
                b_index, b_value, b_score, b_groups = index, row[index], gini, groups
    return {'index':b_index, 'value':b_value, 'groups':b_groups}

dataset = [
    [2.771244718,1.784783929,0],
    [1.728571309,1.169761413,0],
    [3.678319846,2.81281357,0],
    [3.961043357,2.61995032,0],
    [2.999208922,2.209014212,0],
    [7.497545867,3.162953546,1],
    [9.00220326,3.339047188,1],
    [7.444542326,0.476683375,1],
    [10.12493903,3.234550982,1],
    [6.642287351,3.319983761,1]
]

split = get_split(dataset)
print('Split: [X%d &lt; %.3f]' % ((split['index']+1), split['value']))
</code></pre>
<p>通过执行结果可以看出，<code>X1 &lt; 6.642 Gini=0.000 </code>基尼指数为 0.0 为完美分割。</p>
<h4 id="如何构建树">如何构建树</h4>
<p>构建树主要分为 3 个部分</p>
<ul>
<li>终端节点 <code>Terminal Nodes</code> 零度节点称为终端节点或叶节点</li>
<li>递归拆分</li>
<li>建造一棵树</li>
</ul>
<h5 id="终端节点">终端节点</h5>
<p>需要决定何时停止种植树，这里可以使用节点在训练数据集中负责的<strong>深度</strong>和<strong>行数</strong>来做到。</p>
<ul>
<li><strong>树的最大深度</strong>：从树的根节点开始的最大节点数。一旦达到树的最大深度，停止拆分新节点。</li>
<li><strong>最小节点</strong>：对一个节点的要训练的最小值。一旦达到或低于此最小值，则停止拆分和添加新节点。</li>
</ul>
<p>这两种方法将是构建树的过程时用户的指定参数。当在给定点停止增长时，该节点称为终端节点，用于进行最终预测。</p>
<p>编写一个函数<strong>to_terminal()</strong>，这个函数将为一组行选择一类。它返回行列表中最常见的输出值。</p>
<pre><code class="language-python">def to_terminal(group):
	outcomes = [row[-1] for row in group]
	return max(set(outcomes), key=outcomes.count)
</code></pre>
<h5 id="递归拆分">递归拆分</h5>
<p>构建决策树会在为每个节点创建的组上一遍又一遍地调用 <code>get_split()</code> 函数。</p>
<p>添加到现有节点的新节点称为子节点。一个节点可能有零个子节点（一个终端节点）、一个子节点或两个子节点，这里将在给定节点的字典表示中将子节点称为左和右。当一旦创建出一个节点，则通过再次调用相同的函数来递归地从拆分的每组数据以创建子节点。</p>
<p>下面需要实现这个过程（递归函数）。函数接受一个节点作为参数，以及节点中的最大深度、最小模式数、节点的当前深度。</p>
<p>调用的过程分步为。设置，传入根节点和深度1：</p>
<ul>
<li>首先，将拆分后的两组数据提取出来使用，当处理过这些组时，节点不再需要访问这些数据。</li>
<li>接下来，我们检查左或右两组是否为空，如果是，则使用我们拥有的记录创建一个终端节点。</li>
<li>不为空的情况下，检查是否达到了最大深度，如果是，则创建一个终端节点。</li>
<li>然后我们处理左子节点，如果行组太小，则创建一个终端节点，否则以深度优先的方式创建并添加左节点，直到在该分支上到达树的底部。最后再以相同的方式处理右侧。</li>
</ul>
<pre><code class="language-python"># 创建子拆分或者终端节点
def split(node, max_depth, min_size, depth):
    &quot;&quot;&quot;
    :param node: {},分割好的的{'index':b_index, 'value':b_value, 'groups':b_groups}
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :param depth:int， 当前深度
    :return: None
    &quot;&quot;&quot;
    left, right = node['groups']
    del(node['groups'])
    # 检查两边的分割问题
    if not left or not right:
        node['left'] = node['right'] = to_terminal(left + right)
        return
    # 检查最大的深度
    if depth &gt;= max_depth:
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    # 处理左分支，数量要小于最小模式数为terminal node
    if len(left) &lt;= min_size:
        node['left'] = to_terminal(left)
    else:
        node['left'] = get_split(left)
        split(node['left'], max_depth, min_size, depth+1) # 否则递归
    # 处理左右支，数量要小于最小模式数为terminal node
    if len(right) &lt;= min_size:
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_split(right)
        split(node['right'], max_depth, min_size, depth+1)
</code></pre>
<h5 id="创建树">创建树</h5>
<p>构建一个树就是一个上面的步骤的合并，通过**split()**函数打分并确定树的根节点，然后通过递归来构建出整个树；下面代码是实现此过程的函数 <strong>build_tree()</strong>。</p>
<pre><code class="language-python"># Build a decision tree
def build_tree(train, max_depth, min_size):
    &quot;&quot;&quot;
    :param train: list, 数据集，可以是训练集
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :return: None
    &quot;&quot;&quot;
    root = get_split(train) # 对整个数据集进行打分
    split(root, max_depth, min_size, 1)
    return root
</code></pre>
<h5 id="整合">整合</h5>
<p>将全部代码整合为一个</p>
<pre><code class="language-python"># Split a dataset based on an attribute and an attribute value
def test_split(index, value, dataset):
    left, right = list(), list()
    for row in dataset:
        if row[index] &lt; value:
            left.append(row)
        else:
            right.append(row)
    return left, right

# Calculate the Gini index for a split dataset
def gini_index(groups, classes):
    # 计算两组数据集的总数每个种类的列表数量和
    n_instances = float(sum([len(group) for group in groups]))
    # 计算每组的基尼值
    gini = 0.0
    for group in groups:
        size = float(len(group))
        # avoid divide by zero
        if size == 0:
            continue
        score = 0.0
        # score the group based on the score for each class
        for class_val in classes:
            # 拿出数据集中每行的类型，拆开是为了更好的了解结构
            p = [row[-1] for row in group]
            # print(&quot;%f / %f = %f&quot; % (p.count(class_val), size, p.count(class_val) / size ))
            # 这里计算的是当前的分类在总数据集中占比
            p1 = p.count(class_val) / size
            score += p1 * p1 # gini index formula = 1 - sum(p_i^2)
        # 计算总的基尼指数，权重：当前分组占总数据集中的数量
        gini += (1.0 - score) * (size / n_instances)
    return gini

# Select the best split point for a dataset
def get_split(dataset):
    class_values = list(set(row[-1] for row in dataset))
    b_index, b_value, b_score, b_groups = 999, 999, 999, None
    for index in range(len(dataset[0])-1): # 最后分类不计算
        for row in dataset:
            # 根据每个值分类计算出最优基尼值，这个值就作为决策树的节点
            groups = test_split(index, row[index], dataset)
            gini = gini_index(groups, class_values)
            # print('X%d &lt; %.3f Gini=%.3f' % ((index+1), row[index], gini))
            if gini &lt; b_score: # 拿到最小的gini index那列
                b_index, b_value, b_score, b_groups = index, row[index], gini, groups
    return {'index':b_index, 'value':b_value, 'groups':b_groups}

# 创建子拆分或者终端节点
def split(node, max_depth, min_size, depth):
    &quot;&quot;&quot;
    :param node: {},分割好的的{'index':b_index, 'value':b_value, 'groups':b_groups}
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :param depth:int， 当前深度
    :return: None
    &quot;&quot;&quot;
    left, right = node['groups']
    del(node['groups'])
    # 检查两边的分割问题
    if not left or not right:
        node['left'] = node['right'] = to_terminal(left + right)
        return
    # 检查最大的深度
    if depth &gt;= max_depth:
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    # 处理左分支，数量要小于最小模式数为terminal node
    if len(left) &lt;= min_size:
        node['left'] = to_terminal(left)
    else:
        node['left'] = get_split(left)
        split(node['left'], max_depth, min_size, depth+1) # 否则递归
    # 处理左右支，数量要小于最小模式数为terminal node
    if len(right) &lt;= min_size:
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_split(right)
        split(node['right'], max_depth, min_size, depth+1)

# Build a decision tree
def build_tree(train, max_depth, min_size):
    &quot;&quot;&quot;
    :param train: list, 数据集，可以是训练集
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :return: None
    &quot;&quot;&quot;
    root = get_split(train) # 对整个数据集进行打分
    split(root, max_depth, min_size, 1)
    return root

# 打印树
def print_tree(node, depth=0):
    if isinstance(node, dict):
        print('%s[X%d &lt; %.3f]' % ( (depth*' ', (node['index']+1), node['value']) ))
        print_tree(node['left'], depth+1) # 递归打印左右
        print_tree(node['right'], depth+1)
    else:
        print('%s[%s]' % ((depth*' ', node))) # 不是对象就是terminal node

# 创建一个terminal node
def to_terminal(group):
    outcomes = [row[-1] for row in group]
    return max(set(outcomes), key=outcomes.count)

dataset = [
    [2.771244718,1.784783929,0],
    [1.728571309,1.169761413,0],
    [3.678319846,2.81281357,0],
    [3.961043357,2.61995032,0],
    [2.999208922,2.209014212,0],
    [7.497545867,3.162953546,1],
    [9.00220326,3.339047188,1],
    [7.444542326,0.476683375,1],
    [10.12493903,3.234550982,1],
    [6.642287351,3.319983761,1]
]

if __name__=='__main__':
    tree = build_tree(dataset, 4, 2)
    print_tree(tree)
</code></pre>
<p>可以看到打印结果是一个类似二叉树的</p>
<pre><code>[X1 &lt; 6.642]
 [X1 &lt; 2.771]
  [0]
  [X1 &lt; 2.771]
   [0]
   [0]
 [X1 &lt; 7.498]
  [X1 &lt; 7.445]
   [1]
   [1]
  [X1 &lt; 7.498]
   [1]
   [1]
</code></pre>
<h5 id="预测">预测</h5>
<p>预测是预测数据是该向右还是向左，是作为对数据进行导航的方式。这里可以使用递归来实现，其中使用左侧或右侧子节点再次调用相同的预测，具体取决于拆分如何影响提供的数据。</p>
<p>我们必须检查子节点是否是要作为预测返回的终端值，或者它是否是包含要考虑的树的另一个级别的字典节点。</p>
<p>下面是实现此过程的函数 <strong>predict()</strong>。</p>
<pre><code class="language-python"># Make a prediction with a decision tree
def predict(node, row):
	if row[node['index']] &lt; node['value']:
		if isinstance(node['left'], dict):
			return predict(node['left'], row)
		else:
			return node['left']
	else:
		if isinstance(node['right'], dict):
			return predict(node['right'], row)
		else:
			return node['right']
</code></pre>
<p>下面是一个使用硬编码决策树的示例，该决策树具有一个最好地分割数据的节点（决策树桩，这个就是gini index的最优质值）。通过对上面的测试数据集例来对每一行进行预测。</p>
<pre><code class="language-python">def predict(node, row):
    # 如果gini index与对应属性的值小于则向左，
	if row[node['index']] &lt; node['value']:
		if isinstance(node['left'], dict):
			return predict(node['left'], row) # 递归处理完整个树
		else:
			return node['left']
	else: # 否则的话，则为右
		if isinstance(node['right'], dict):
			return predict(node['right'], row)
		else:
			return node['right']
 
dataset = [[2.771244718,1.784783929,0],
	[1.728571309,1.169761413,0],
	[3.678319846,2.81281357,0],
	[3.961043357,2.61995032,0],
	[2.999208922,2.209014212,0],
	[7.497545867,3.162953546,1],
	[9.00220326,3.339047188,1],
	[7.444542326,0.476683375,1],
	[10.12493903,3.234550982,1],
	[6.642287351,3.319983761,1]]
 
#  这是之前用于计算出最优的gini index
stump = {'index': 0, 'right': 1, 'value': 6.642287351, 'left': 0}
for row in dataset:
	prediction = predict(stump, row)
	print('Expected=%d, Got=%d' % (row[-1], prediction))
</code></pre>
<p>通过观察可以看出预测结果和实际结果一样</p>
<pre><code class="language-python">Expected=0, Got=0
Expected=0, Got=0
Expected=0, Got=0
Expected=0, Got=0
Expected=0, Got=0
Expected=1, Got=1
Expected=1, Got=1
Expected=1, Got=1
Expected=1, Got=1
Expected=1, Got=1
</code></pre>
<h4 id="套用真实数据集来测试">套用真实数据集来测试</h4>
<p>这里将使用 <code>CART</code> 算法对<a href="https://archive.ics.uci.edu/ml/datasets/banknote&#43;authentication" target="_blank"
   rel="noopener nofollow noreferrer" >银行钞票数据集</a>进行预测。大概的流程为：</p>
<ul>
<li>加载数据集并转换格式。</li>
<li>编写拆分算法与准确度计算算法；这里使用 5折的k折交叉验证（<code>k-fold cross validation</code>）用于评估算法</li>
<li>编写 CART 算法，从训练数据集，创建树，对测试数据集进行预测操作</li>
</ul>
<h5 id="什么是-k折交叉验证">什么是 K折交叉验证</h5>
<p>K折较差验证（<strong>K-Fold CV</strong>）是将给定的数据集分成<strong>K</strong>个部分，其中每个折叠在某时用作测试集。以 5 折（K=5）为例。这种情况下，数据集被分成5份。在第一次迭代中，第一份用于测试模型，其余用于训练模型。在第二次迭代中，第 2 份用作测试集，其余用作训练集。重复这个过程，直到 5 个折叠中的每个折叠都被用作测试集。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1IjKy-Zc9zVOHFzMw2GXaQw.png" alt="K-Fold CV" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>下面来开始编写函数，函数的整个过程为</p>
<ul>
<li><code>evaluate_algorithm()</code> 作为最外层调用
<ul>
<li>使用五折交叉进行评估 <code>cross_validation_split()</code></li>
<li>使用决策树算法作为算法根据 <code>decision_tree()</code></li>
<li>构建树：<code>build_tree()</code>
<ul>
<li>拿到最优基尼指数作为叶子 <code>get_split()</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="language-python">from random import seed
from random import randrange
from csv import reader
 
# 加载csv文件
def load_csv(filename):
	file = open(filename, &quot;rt&quot;)
	lines = reader(file)
	dataset = list(lines)
	return dataset
 
# 将所有字段转换为float类型便于计算
def str_column_to_float(dataset, column):
	for row in dataset:
		row[column] = float(row[column].strip())
 
# k-folds CV函数
def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
    # 平均分位折数n_folds
	fold_size = int(len(dataset) / n_folds)
	for i in range(n_folds):
		fold = list()
		while len(fold) &lt; fold_size:
			index = randrange(len(dataset_copy)) # 随机
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split
 
# 计算精确度
def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0
 
# Evaluate an algorithm using a cross validation split
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
	folds = cross_validation_split(dataset, n_folds)
	scores = list()
	for fold in folds:
		train_set = list(folds)
		train_set.remove(fold)
		train_set = sum(train_set, [])
		test_set = list()
		for row in fold:
			row_copy = list(row)
			test_set.append(row_copy)
			row_copy[-1] = None
		predicted = algorithm(train_set, test_set, *args)
		actual = [row[-1] for row in fold]
		accuracy = accuracy_metric(actual, predicted)
		scores.append(accuracy)
	return scores
 
# 根据基尼指数划分value是应该在树的哪边？
def test_split(index, value, dataset):
	left, right = list(), list()
	for row in dataset:
		if row[index] &lt; value:
			left.append(row)
		else:
			right.append(row)
	return left, right
 
# 基尼指数打分
def gini_index(groups, classes):
    # 计算数据集中的多组数据的总个数
    n_instances = float(sum([len(group) for group in groups]))
    # 计算每组中的最优基尼指数
    gini = 0.0
    for group in groups:
        size = float(len(group))
        if size == 0:
            continue
        score = 0.0
        # 总基尼指数
        for class_val in classes:
            # 拿出数据集中每行的类型，拆开是为了更好的了解结构

            # 计算的是当前的分类在总数据集中占比
            p = [row[-1] for row in group]
            p1 = p.count(class_val) / size
            score += p1 * p1
        # 计算总的基尼指数，并根据相应大小增加权重。权重：当前分组占总数据集中的数量
        gini += (1.0 - score) * (size / n_instances)
    return gini
 
# 从数据集中获得基尼指数最佳的值
def get_split(dataset):
	class_values = list(set(row[-1] for row in dataset))
	b_index, b_value, b_score, b_groups = 999, 999, 999, None
	for index in range(len(dataset[0])-1):
		for row in dataset:
			groups = test_split(index, row[index], dataset)
			gini = gini_index(groups, class_values)
			if gini &lt; b_score:
				b_index, b_value, b_score, b_groups = index, row[index], gini, groups
	return {'index':b_index, 'value':b_value, 'groups':b_groups}
 
# 创建终端节点
def to_terminal(group):
	outcomes = [row[-1] for row in group]
	return max(set(outcomes), key=outcomes.count)
 
# 创建子节点，为终端节点或子节点
def split(node, max_depth, min_size, depth):
    &quot;&quot;&quot;
    :param node: {},分割好的的{'index':b_index, 'value':b_value, 'groups':b_groups}
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :param depth:int， 当前深度
    :return: None
    &quot;&quot;&quot;
    left, right = node['groups']
    del(node['groups'])
    # check for a no split
    if not left or not right:
        node['left'] = node['right'] = to_terminal(left + right)
        return
    # check for max depth
    if depth &gt;= max_depth:
        node['left'], node['right'] = to_terminal(left), to_terminal(right)
        return
    # process left child
    if len(left) &lt;= min_size:
        node['left'] = to_terminal(left)
    else:
        node['left'] = get_split(left)
        split(node['left'], max_depth, min_size, depth+1)
    # process right child
    if len(right) &lt;= min_size:
        node['right'] = to_terminal(right)
    else:
        node['right'] = get_split(right)
        split(node['right'], max_depth, min_size, depth+1)
 
# 构建树
def build_tree(train, max_depth, min_size):
    &quot;&quot;&quot;
    :param train: list, 数据集，可以是训练集
    :param max_depth: int, 最大深度
    :param min_size:int，最小模式数
    :ret
    &quot;&quot;&quot;
    root = get_split(train)
    split(root, max_depth, min_size, 1)
    return root

# 打印树
def print_tree(node, depth=0):
    if isinstance(node, dict):
        print('%s[X%d &lt; %.3f]' % ( (depth*' ', (node['index']+1), node['value']) ))
        print_tree(node['left'], depth+1) # 递归打印左右
        print_tree(node['right'], depth+1)
    else:
        print('%s[%s]' % ((depth*' ', node))) # 不是对象就是terminal node
 
# 预测，预测方式为当前基尼指数与最优基尼指数相比较，然后放入树两侧
def predict(node, row):
    &quot;&quot;&quot;
    :param node: {} 叶子值
    :param row: {}, 需要预测值
    :ret
    &quot;&quot;&quot;
    if row[node['index']] &lt; node['value']:
        if isinstance(node['left'], dict):
            return predict(node['left'], row)
        else:
            return node['left']
    else:
        if isinstance(node['right'], dict):
            return predict(node['right'], row)
        else:
            return node['right']
 

def decision_tree(train, test, max_depth, min_size):
	tree = build_tree(train, max_depth, min_size)
	predictions = list()
	for row in test:
		prediction = predict(tree, row)
		predictions.append(prediction)
	return(predictions)


# Test CART on Bank Note dataset
seed(1)
# 加载数据
filename = 'data_banknote_authentication.csv'
dataset = load_csv(filename)
# 转换格式
for i in range(len(dataset[0])):
	str_column_to_float(dataset, i)
# 评估算法
n_folds = 5
max_depth = 5
min_size = 10
scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)
print('Scores: %s' % scores)
print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))
</code></pre>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://www.bogotobogo.com/python/scikit-learn/scikt_machine_learning_Decision_Tree_Learning_Informatioin_Gain_IG_Impurity_Entropy_Gini_Classification_Error.php" target="_blank"
   rel="noopener nofollow noreferrer" >Informatioin Gain</a></p>
<p><a href="https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/" target="_blank"
   rel="noopener nofollow noreferrer" >implement decision tree algorithm</a></p>
<p><a href="https://machinelearningmastery.com/information-gain-and-mutual-information/" target="_blank"
   rel="noopener nofollow noreferrer" >inplement information gain</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>逻辑回归</title>
      <link>https://www.oomkill.com/2022/06/logistic-regression/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/logistic-regression/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>逻辑回归通常用于分类算法，例如预测某事是 <code>true</code> 还是 <code>false</code>（二元分类）。例如，对电子邮件进行分类，该算法将使用电子邮件中的单词作为特征，并据此预测电子邮件是否为垃圾邮件。用数学来讲就是指，假设因变量是 Y，而自变量集是 X，那么逻辑回归将预测因变量 $P(Y=1)$ 作为自变量集 X 的函数。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1UgYbimgPXf6XXxMy2yqRLw.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>逻辑回归性能在线性分类中是最好的，其核心为基于样本属于某个类别的概率。这里的概率必须是连续的并且在 <code>(0, 1)</code> 之间（有界）。它依赖于阈值函数来做出称为 <code>Sigmoid</code> 或 <code>Logistic</code> 函数决定的。</p>
<p>学好逻辑回归，需要了解逻辑回归的概念、优势比 (<strong>OR</strong>) 、Logit 函数、Sigmoid 函数、 Logistic 函数及交叉熵或Log Loss</p>
<h2 id="prerequisite">Prerequisite</h2>
<h3 id="odds-ratio">odds ratio</h3>
<h4 id="explain">explain</h4>
<p>odds ratio是预测变量的影响。优势比取决于预测变量是分类变量还是连续变量。</p>
<ul>
<li>连续预测变量：$OR &gt; 1$ 表示，随着预测变量的增加，事件发生的可能性增加。$OR &lt; 1$ 表示随着预测变量的增加，事件发生的可能性较小。</li>
<li>分类预测变量：事件发生在预测变量的 2 个不同级别的几率；如 A,B，$OR &gt; 1$ 表示事件在 A 级别的可能性更大。$OR&lt;1$ 表示事件更低的可能是在A。</li>
</ul>
<p>例如，假设 X 是受影响的概率，Y 是不受影响的概率，则 $OR= \frac{X}{Y}$ ，那么 $OR = \frac{P}{(1-P)}$ ，P是事件的概率。</p>
<p>让概率的范围为 <code>[0,1]</code>  ，假设 $P(success)=0.8$ ，$Q(failure) = 0.2$ ；$OR$ 则是 成功概率和失败概率的比值，如：$O(success)=\frac{P}{Q} = \frac{0.8}{0.2} = 4$ , $O(failure)=\frac{Q}{P} = \frac{0.2}{0.8} = 0.25$ 。</p>
<h4 id="odds和probability-的区别">odds和probability 的区别</h4>
<ul>
<li>
<p><strong>probability</strong> 表示在多次实验中，看到改事件的几率，位于 <code>[0,1]</code> 之间</p>
</li>
<li>
<p><strong>odds</strong> 表示 $\frac{(事件发生的概率)}{(事件不会发生的概率)}$ 的比率，位于 <code>[0,∞]</code></p>
</li>
</ul>
<p>例如赛马，一匹马跑 100 场比赛，赢了 80 场，那么获胜的概率是 $\frac{80}{100} = 0.80 = 80%$ ，获胜的几率是 $\frac{80}{20}=4:1$</p>
<p><strong>总结</strong>：probability 和 odds 之间的主要区别：</p>
<ul>
<li>“odds”用于描述是否有可能发生事件。相反，probability决定了事件发生的可能性，即事件发生的频率。</li>
<li>odds以比例表示，probability以百分比形式或小数表示。</li>
<li>odds通常从 <code>0 ~ ∞</code> ，其中0定义事件发生的可能性，<code>∞</code> 表示发生的可能性。相反，probability 介于 <code>0~1</code>之间。因此，probability越接近于0，不发生的可能性就越大，越接近于1，发生的可能性就越高。</li>
</ul>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_confidence_intervals/BS704_Confidence_Intervals10.html" target="_blank"
   rel="noopener nofollow noreferrer" >The Difference Between &ldquo;Probability&rdquo; and &ldquo;Odds&rdquo;</a></p>
</blockquote>
<h4 id="通过示例陈述公式">通过示例陈述公式</h4>
<p>假设一个体校的录取率中，10 个男生中有 7 个被录取，而10 个女生中有3个被录取。找出男生被录取的概率？</p>
<p>那么通过已知条件，设 P 为录取概率，Q则为未被录取的概率，那么</p>
<ul>
<li>男生被录取的概率为：
<ul>
<li>$P=\frac{7}{10} = 0.7$</li>
<li>$Q=1-0.7 = 0.3$</li>
</ul>
</li>
<li>女生被录取的概率为：
<ul>
<li>$P=\frac{3}{10}=0.3$</li>
<li>$Q=1-0.3=0.7$</li>
</ul>
</li>
<li>录取优势比：
<ul>
<li>$OR(boy)=\frac{0.7}{0.3}=2.33$</li>
<li>$OR(Gril) = \frac{0.3}{0.7}=0.42$</li>
</ul>
</li>
</ul>
<p>因此，一个男生被录取的几率为 $OR=\frac{2.33}{0.42}=5.44$</p>
<h3 id="logit-函数">Logit 函数</h3>
<p>logit函数是<code>Odd Ratio</code> 的对数 <strong>logarithm</strong> , 给出 <code>0~1</code> 范围内的输入，然后将它们转换为整个实数范围内的值。如：假设P，则 $\frac{P}{(1-P)}$ 为对应的OR；OR 的 logit 的公式为：$loggit(P) = log(odds) = log(\frac{P}{1-P})$.</p>
<p>以一辆汽车是否出售为例，1为出售，0为不出售，则等式 $P_i=B_0+B_1 * (Price_i) + \epsilon$</p>
<p>$ln(\frac{P}{1-P}) = \beta_0 + \beta_1X_1+\beta_2X_2&hellip; + \beta_nX_N$ ,对于简单的逻辑回归，有两个系数：</p>
<ul>
<li>$\beta_0$  截距 ：X 变量为 0 时的对数 odds ratio</li>
<li>$\beta_1$  斜率：odds ratio随X增加（或减少），1的变化</li>
</ul>
<p>例如：假设简单逻辑回归模型是 $Ln(odds) = -5.5 + 1.2*X$ ,那么 $\beta_0=-5.5$ ，$\beta_1 = 1.2$ ，意味着，X=0时，$odds\ ratio = 0$ ，X每增加一个单位 odds ration 增加 1.2（（X 增加2个单位odds ratio增加 2.4&hellip;.）</p>
<p><strong>求解</strong></p>
<p>通过上面的公式实际上不明白这些具体是什么，就可以通过求P来找到<strong>有结果的概率</strong>与<strong>截距</strong> $β_0$ 之间的关系，已知 $n=log_ab$ , $ a^n=b$ ，那么一个简单的逻辑回归公式为 $log(\frac{P}{1-P}) = \beta_0+\beta1X$ ，对这个公式进行推导：</p>
<ul>
<li>$\frac{P}{1-P} = e^{\beta_0+e^\beta1*X}$</li>
<li>$P = e^{\beta_0+e^\beta1<em>X} - Pe^{\beta_0+e^\beta1</em>X}$</li>
<li>$P(1+e^{\beta_0+e^\beta1<em>X}) = e^{\beta_0+e^\beta1</em>X}$</li>
<li>$P=\frac{e^{\beta_0+e^\beta1<em>X}}{1+e^{\beta_0+e^\beta1</em>X}}$</li>
</ul>
<p>当 $X=0$ ,则 $\beta_1*X$ 没意义，公式为：$P = frac{e^{β_0}}{(1+e^{β_0})}$ ，其中e是一个常数，python为 <code>math.e</code></p>
<p>如果单纯不算概率，只看截距符号，那么满足：</p>
<ul>
<li>如果截距为<strong>负号</strong>：则产生结果的概率将 &lt; 0.5。</li>
<li>如果截距为<strong>正号</strong>：那么产生结果的概率将 &gt; 0.5。</li>
<li>如果截距<strong>等于 0</strong>：那么得到结果的概率正好是 0.5。</li>
</ul>
<p>通过例子来说明这点：假设研究为抽烟对心脏健康的影响，下表显示了一个逻辑回归</p>
<table>
<thead>
<tr>
<th></th>
<th>Coefficient</th>
<th>Standard Error</th>
<th>p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intercept</td>
<td>-1.93</td>
<td>0.13</td>
<td>&lt; 0.001</td>
</tr>
<tr>
<td>Smoking</td>
<td>0.38</td>
<td>0.17</td>
<td>0.03</td>
</tr>
</tbody>
</table>
<p>由表可知，截距为 -1.93，假设smoking系数为0，那么概率带入公式为：$P=\frac{e^{\beta_0}}{1+e^{\beta_0}} = P=\frac{e^{-1.93}}{1+e^{-1.93}} = 0.126$<code>(math.e ** -1.93)/(1+math.e  ** -1.93)</code></p>
<p>如果 Smoking是一个连续变量（每年的吸烟量），在这种情况下，<code>Smoking=0</code>  意味着每年使用0公斤烟草的人即不抽烟的人群；那么这个结果就为，不抽烟的人群在未来10年内心脏有问题几率为 0.126。</p>
<p>再如果是吸烟者应该怎么计算，假设，每年吸烟量为3kg，那么公式为：$P = \frac{e^{β0 + β_1X}}{(1+e^{β0 + β_1X})}$ ，在这里 X=3，那么 $P=\frac{e^{\beta_1+\beta_2X}}{(1-e^{\beta_1+\beta_2X})} = \frac{e^{-1.93+0.38<em>3}}{(1-e^{-1.93+0.38</em>3})} = 0.31$ ；即得出，每年3KG烟草消耗量10年后有心脏问题的概率是 31%</p>
<blockquote>
<p><a href="https://quantifyinghealth.com/interpret-logistic-regression-intercept/" target="_blank"
   rel="noopener nofollow noreferrer" >interpret</a></p>
</blockquote>
<h3 id="sigmoid">sigmoid</h3>
<p>logit 函数的逆函数称Sigmoid 函数，sigmoid方程来源于 logit 为：$P=\frac{e^{log(odds)}}{(1-e^{log(odds)})} = \frac{1}{e^{-log(odds)+1}} = \frac{1}{1+e^{-z}}$ 。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/220px-Kernel_Machine.svg.png" alt="具有线性支持向量机决策边界的散点图（虚线）" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>在python中，<code>np.exp</code> 是求 是求 $e^{x}$ 的值的函数。正好可以用在sigmod函数中，那么sigmoid可以写为</p>
<pre><code class="language-python">def sigmoid(z):
    return 1 / (1 + np.exp(-z))
</code></pre>
<h3 id="交叉熵或对数损失">交叉熵或对数损失</h3>
<p>交叉熵 <strong>Cross-Entropy</strong>，通常用于量化两个概率分布之间的差异。用于逻辑回归，公式为：$H=\sum^{x=n}(P(x) \times log(q(x))$</p>
<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<p>最大似然估计，<code>Maximum Likelihood Estimation</code>  <strong>MLE</strong>，是概率估算的一种解决方案。MLE在其中寻找一组参数，这些参数将影响数据样本 <em>X</em>  的联合概率的最佳拟合。</p>
<p>首先，定义一个称为 $\theta$ <em>theta</em> 的参数，该参数定义概率密度函数的选择和该分布的参数。它可能是一个数值向量，其值平滑变化并映射到不同的概率分布及其参数。在最大似然估计中，我们希望在给定特定概率分布及其参数的最大化情况下从联合概率分布中观察数据的概率，形式上表示为：$P(X|\theta)$ ，在这种情况下，条件概率通常使用分号 <strong>;</strong> 而不是竖线 <strong>|</strong>  ，因为 $\theta$ 不是随机变量，而是未知参数。表达为 $P(X;\theta)$ ,或 $P(x_1,x_2,\ &hellip;\ x_n;\theta)$ 。</p>
<p>这样产生的条件概率被称为在给定模型参数 （$\theta$）的情况下观察变量 $X$ 的概率，并使用符号 <strong>L</strong> 来 表示似然函数。例如：$L(X;\theta)$。而<strong>最大似然估计</strong>的目标是找到使似然函数最大化的一组参数 ( $\theta$ )，例如产生最大似然值，如：$max(L(X;\theta))$</p>
<p>鉴于上述提到的变量 $X$ 是由n个样本组成，可以将其定义为在给定概率分布参数 $\theta$ 的情况下，变量 $X$ 的联合概率,如这里数据样本为 $x_1,x_2,\ &hellip;\ ,x_n$ 的联合概率，同时表示为 $L(x_1,x2,\ &hellip;\ ,x_n;\theta)$</p>
<p>大多数情况下，求解似然方程很复杂。会使用对数似然作为一种解决方案。由于对数函数是单调递增的，因此对数似然和似然中的最优参数是相同的。因此定义条件最大似然估计为：$log(P(x_i ; h))$。</p>
<p>用逻辑回归模型替换<em>h</em>，需要假设一个概率分布。在逻辑回归的情况下，假设数据样本为二项式概率分布，其中每个示例都是二项式的一个结果。伯努利分布只有一个参数：成功结果的概率 P，那么为：</p>
<ul>
<li>$P(y=1)=P$</li>
<li>$P(y=0)=1-P$</li>
</ul>
<p>那么这个平均值为：$P(y=1)*1+P(y=0)<em>0$，给出P的值公式可以转换为：$P</em>1+(1-p)*0$；这种公式看似没有意义，那么通过一个小例子来了解下</p>
<pre><code class="language-python"># 二项式似然函数

def likelihood(y, p):
	return p * y + (1 - p) * (1 - y)

# test for y=1
y, p = 1, 0.9
print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p)))
y, yhat = 1, 0.1
print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p)))
# test for y=0
y, yhat = 0, 0.1
print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p)))
y, yhat = 0, 0.9
print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p)))

# y=1.0, p=0.9, likelihood: 0.900
# y=1.0, p=0.9, likelihood: 0.900
# y=0.0, p=0.9, likelihood: 0.100
# y=0.0, p=0.9, likelihood: 0.100
</code></pre>
<p>运行示例会为每个案例打印类别y 和预测概率p，其中每个案例的概率是否接近；这里也可以使用对数更新似然函数，$log(p) * y + log(1 – p) * (1 – y)$；最后可以根据数据集中实例求最大似然和最小似然</p>
<ul>
<li>$\sum^{i=1}_n log(p_i) * y_i + log(1 – p_i) * (1 – y_i)$</li>
<li>最小似然使用反转函数，使负对数自然作为最小似然。上面的公式前加 <code>-</code></li>
</ul>
<p>对于计算二项式分布的对数似然相当于计算二项式分布[交叉熵，其中<code>P(class)</code>表示第 class 项概率，<code>q()</code> 表示概率分布，$-(log(q(class0)) \times P(class0) + log(q(class1)) * P(class1))$</p>
<h2 id="lr算法实例">LR算法实例</h2>
<p>在研究如何从数据中估计模型的参数之前，我们需要了解逻辑回归准确计算的内容。</p>
<p>模型的线性部分（输入的加权和）计算成功事件的log-odds。</p>
<p>odds ratio：$\beta_0+\beta_1 \times x_1 + \beta_2 \times x_2\ &hellip;\ \beta_n \times x_n$ 该模型估计了每个级别的输入变量的log-odds。</p>
<p>由上面信息了解到，几率 <code>probability</code> 是输赢的比率 如 <code>1:10</code> ；<code>probability</code> 可以转换为 <code>odds ratio</code> 即成功概率除以不成功概率：$or=\frac{P}{1-P}$ ；计算or的对数，被称为log-odds是一种度量单位：$log(\frac{P}{1-P})$，而所求的即为 log-odds的逆函数，而在python中 <code>log</code> 函数是对数，求log的逆方法即 <code>exp</code> 返回n的x次方就是log的逆函数。</p>
<p>到这里已经和逻辑回归模型很接近了，对数函数公式可以简化为，$P=\frac{e^{log(odds)}}{(1-e^{log(odds)})}$ ，以上阐述了如何从log-odds转化为odds，然后在到逻辑回归模型。下面通过Python 中的示例来具体计算 <code>probability</code> 、<code>odds</code> 和 <code>log-odds</code> 之间的转换。假设将成功概率定义为 80% 或 0.8，然后将其转换为odds，然后再次转换为概率。</p>
<pre><code class="language-python">from math import log
from math import exp

prob = 0.8
print('Probability %.1f' % prob)
# 将 probability 转换为 odds
odds = prob / (1 - prob)
print('Odds %.1f' % odds)
# 将 odds 转换为 log-odds
logodds = log(odds)
print('Log-Odds %.1f' % logodds)
# 转换 log-odds 为  probability
prob = 1 / (1 + exp(-logodds))
print('Probability %.1f' % prob)

# Probability 0.8
# Odds 4.0
# Log-Odds 1.4
# Probability 0.8
</code></pre>
<p>通过这个例子，可以看到odds被转换成大约 1.4 的log-odds，然后正确地转回 0.8 的成功概率。</p>
<h2 id="逻辑回归实现">逻辑回归实现</h2>
<p>首先将实现分为3个步骤：</p>
<ul>
<li>预测</li>
<li>评估系数</li>
<li>真实数据集预测</li>
</ul>
<h3 id="预测">预测</h3>
<p>编写一个预测函数，在评估随机梯度下降中的候选系数值时以及在模型最终确定测试数据或新数据进行预测时。</p>
<p>下面是预测**predict()**函数，它预测给定一组系数的行的输出值。第一个系数是截距，也称为偏差或 b0，它是独立的，不负责输入值。</p>
<pre><code class="language-python">def predict(row, coefficients):
	p = coefficients[0]
	for i in range(len(row)-1):
		yhat += coefficients[i + 1] * row[i]
	return 1.0 / (1.0 + exp(-p))
</code></pre>
<p>准备一些测试数据，Y代表真实的类别</p>
<pre><code class="language-python">X1					X2						Y
2.7810836 	2.550537003		0
1.465489372	2.362125076		0
3.396561688	4.400293529		0
1.38807019	1.850220317		0
3.06407232	3.005305973		0
7.627531214	2.759262235		1
5.332441248	2.088626775		1
6.922596716	1.77106367		1
8.675418651	-0.242068655	1
7.673756466	3.508563011		1
</code></pre>
<p>这里有两个输入值，和三个系数，系数是自定义的固定值，那么预测的公式就为</p>
<pre><code class="language-python"># 系数为
coef = [-0.406605464, 0.852573316, -1.104746259]
y = 1.0 / (1.0 + e^(-(b0 + b1 * X1 + b2 * X2)))
# 套入公式（sigma）
y = 1.0 / (1.0 + e^(-(-0.406605464 + 0.852573316 * X1 + -1.104746259 * X2)))
</code></pre>
<p>完整的代码</p>
<pre><code class="language-python"># Make a prediction
from math import exp

# Make a prediction with coefficients
def predict(row, coefficients):
	yhat = coefficients[0]
	for i in range(len(row)-1):
		yhat += coefficients[i + 1] * row[i]
	return 1.0 / (1.0 + exp(-yhat))

# test predictions
dataset = [[2.7810836,2.550537003,0],
	[1.465489372,2.362125076,0],
	[3.396561688,4.400293529,0],
	[1.38807019,1.850220317,0],
	[3.06407232,3.005305973,0],
	[7.627531214,2.759262235,1],
	[5.332441248,2.088626775,1],
	[6.922596716,1.77106367,1],
	[8.675418651,-0.242068655,1],
	[7.673756466,3.508563011,1]]
coef = [-0.406605464, 0.852573316, -1.104746259]
for row in dataset:
	yhat = predict(row, coef)
	print(&quot;Expected=%.3f, Predicted=%.3f [%d]&quot; % (row[-1], yhat, round(yhat)))
</code></pre>
<h3 id="估计系数">估计系数</h3>
<p>这里可以使用我随机梯度下降来估计训练数据的系数值。随机梯度下降需要两个参数：</p>
<ul>
<li><strong>学习率</strong> Learning rate：用于限制每个系数每次更新时的修正量。</li>
<li><strong>Epochs</strong>：更新系数时遍历训练数据的次数。</li>
</ul>
<p>在每个epoch更新训练数据中每一行的每个系数。系数会根据模型产生的错误进行更新，误差为预期输出与预测值之间的差异。错误会随着epoch增加而减少</p>
<p>将每个都加权，并且这些系数以一致的方式进行更新，用公式可以表示为</p>
<pre><code class="language-python">b1(t+1) = b1(t) + learning_rate * (y(t) - p(t)) * p(t) * (1 - p(t)) * x1(t)
</code></pre>
<p>那么整合一起为</p>
<pre><code class="language-python">from math import exp

# 预测函数
def predict(row, coefficients):
    p = coefficients[0]
    for i in range(len(row)-1):
        p += coefficients[i + 1] * row[i]
    return 1.0 / (1.0 + exp(-p))

def coefficients_sgd(train, l_rate, n_epoch):
    coef = [0.0 for i in range(len(train[0]))] # 初始一个系数，第一次为都为0
    for epoch in range(n_epoch):
        sum_error = 0
        for row in train:
            p = predict(row, coef)
            # 错误为预期值与实际值直接差异
            error = row[-1] - p
            sum_error += error**2
            # 截距没有输入变量x，这里为row[0]
            coef[0] = coef[0] + l_rate * error * p * (1.0 - p)
            for i in range(len(row)-1):
                # 其他系数更新
                coef[i + 1] = coef[i + 1] + l_rate * error * p * (1.0 - p) * row[i]
        print('&gt;epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))
    return coef

# Calculate coefficients
dataset = [
    [2.7810836,2.550537003,0],
    [1.465489372,2.362125076,0],
    [3.396561688,4.400293529,0],
    [1.38807019,1.850220317,0],
    [3.06407232,3.005305973,0],
    [7.627531214,2.759262235,1],
    [5.332441248,2.088626775,1],
    [6.922596716,1.77106367,1],
    [8.675418651,-0.242068655,1],
    [7.673756466,3.508563011,1]
]
l_rate = 0.3
n_epoch = 100
coef = coefficients_sgd(dataset, l_rate, n_epoch)
print(coef)

# &gt;epoch=92, lrate=0.300, error=0.024
# &gt;epoch=93, lrate=0.300, error=0.024
# &gt;epoch=94, lrate=0.300, error=0.024
# &gt;epoch=95, lrate=0.300, error=0.023
# &gt;epoch=96, lrate=0.300, error=0.023
# &gt;epoch=97, lrate=0.300, error=0.023
# &gt;epoch=98, lrate=0.300, error=0.023
# &gt;epoch=99, lrate=0.300, error=0.022
#[-0.8596443546618897, 1.5223825112460005, -2.218700210565016]
</code></pre>
<p>这里跟踪了跟踪每个epoch误差平方的总和，以便我们可以在每个epoch中打印出error，实例中使用 0.3 学习率并训练100 个 epoch，每个epoch会打印出其误差平方，最终会打印总系数集</p>
<h3 id="套用真实数据集">套用真实数据集</h3>
<p><a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv" target="_blank"
   rel="noopener nofollow noreferrer" >糖尿病数据集</a> 是根据基本的医疗信息，预测印第安人5年内患糖尿病的情况。这是一个二元分类，阴性0与阳性1直接的关系。采用了二项式分布，也可以采用其他分布，如高斯等。</p>
<pre><code class="language-python">from random import seed
from random import randrange
from csv import reader
from math import exp

# Load a CSV file
def load_csv(filename):
	dataset = list()
	with open(filename, 'r') as file:
		csv_reader = reader(file)
		for row in csv_reader:
			if not row:
				continue
			dataset.append(row)
	return dataset

# Convert string column to float
def str_column_to_float(dataset, column):
	for row in dataset:
		row[column] = float(row[column].strip())

# 找到最小和最大的
def dataset_minmax(dataset):
	minmax = list()
	for i in range(len(dataset[0])):
		col_values = [row[i] for row in dataset]
		value_min = min(col_values)
		value_max = max(col_values)
		minmax.append([value_min, value_max])
	return minmax

# 归一化
def normalize_dataset(dataset, minmax):
	for row in dataset:
		for i in range(len(row)):
			row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])
# k-folds CV实现
def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / n_folds)
	for i in range(n_folds):
		fold = list()
		while len(fold) &lt; fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split

# 计算准确度百分比
def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0

# 使用CV评估算法
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
	folds = cross_validation_split(dataset, n_folds)
	scores = list()
	for fold in folds:
		train_set = list(folds)
		train_set.remove(fold)
		train_set = sum(train_set, [])
		test_set = list()
		for row in fold:
			row_copy = list(row)
			test_set.append(row_copy)
			row_copy[-1] = None
		predicted = algorithm(train_set, test_set, *args)
		actual = [row[-1] for row in fold]
		accuracy = accuracy_metric(actual, predicted)
		scores.append(accuracy)
	return scores

# 使用系数进行预测
def predict(row, coefficients):
	yhat = coefficients[0]
	for i in range(len(row)-1):
		yhat += coefficients[i + 1] * row[i]
	return 1.0 / (1.0 + exp(-yhat))

# 系数生成
def coefficients_sgd(self, train, l_rate, n_epoch):
    &quot;&quot;&quot;
    生成系数
    :param train: list, 数据集，可以是训练集
    :param l_rate: float, 学习率
    :param n_epoch:int，epoch，这里代表进行多少次迭代
    :return: None
    &quot;&quot;&quot;
    coef = [0.0 for i in range(len(train[0]))] # 初始一个系数，第一次为都为0
    for epoch in range(n_epoch):
        sum_error = 0
        for row in train:
            p = self.predict(row, coef)
            # 错误为预期值与实际值直接差异
            error = row[-1] - p
            sum_error += error**2
            # 截距没有输入变量x，这里为row[0]
            coef[0] = coef[0] + l_rate * error * p * (1.0 - p)
            for i in range(len(row)-1):
                # 其他系数更新
                coef[i + 1] = coef[i + 1] + l_rate * error * p * (1.0 - p) * row[i]
                # print('&gt;epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))
            return coef

# 随机梯度下降的逻辑回归算法
def logistic_regression(self, train, test, l_rate, n_epoch):
    predictions = list()
    coef = self.coefficients_sgd(train, l_rate, n_epoch)
    for row in test:
        p = self.predict(row, coef)
        p = round(p)
        predictions.append(p)
    return(predictions)


seed(1)
# 数据预处理
filename = 'pima-indians-diabetes.csv'
dataset = load_csv(filename)
for i in range(len(dataset[0])):
	str_column_to_float(dataset, i)
# 做归一化
minmax = dataset_minmax(dataset)
normalize_dataset(dataset, minmax)
# evaluate algorithm
n_folds = 5
l_rate = 0.1
n_epoch = 100
scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch)
print('Scores: %s' % scores)
print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))

# 0.35294117647058826
# Scores: [73.8562091503268, 78.43137254901961, 81.69934640522875, 75.81699346405229, 75.81699346405229]
# Mean Accuracy: 77.124%
</code></pre>
<p>上述是对整个数据集的预测百分比，也可以对对应的类的信息进行输出</p>
<h2 id="reference-1">Reference</h2>
<blockquote>
<p><a href="https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1" target="_blank"
   rel="noopener nofollow noreferrer" >Maximum likelihood estimation</a></p>
<p><a href="https://vitalflux.com/logistic-regression-sigmoid-function-python-code/" target="_blank"
   rel="noopener nofollow noreferrer" >Sigmoid Function</a></p>
<p><a href="https://christophm.github.io/interpretable-ml-book/logistic.html" target="_blank"
   rel="noopener nofollow noreferrer" >logistic</a></p>
<p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#binary-logistic-regression" target="_blank"
   rel="noopener nofollow noreferrer" >binary logistic regression</a></p>
<p><a href="https://machinelearningmastery.com/?s=Logistic&#43;Regression&amp;post_type=post&amp;submit=Search" target="_blank"
   rel="noopener nofollow noreferrer" >LR implementation</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>朴素贝叶斯算法</title>
      <link>https://www.oomkill.com/2022/06/naive-bayes/</link>
      <pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/06/naive-bayes/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="什么是naive-bayes">什么是naive bayes</h2>
<p>朴素贝叶斯 <code>naive bayes</code>，是一种概率类的机器学习算法，主要用于解决分类问题</p>
<p><strong>为什么被称为朴素贝叶斯？</strong></p>
<p>为什么被称为朴素，难道仅仅是因为贝叶斯很天真吗？实际上是因为，朴素贝叶斯会假设数据属性之间具有很强的的独立性。即该模型中的所有属性彼此之间都是独立的，改变一个属性的值，不会直接影响或改变算法中其他的属性的值</p>
<h2 id="贝叶斯定理">贝叶斯定理</h2>
<p>了解朴素贝叶斯之前，需要掌握一些概念才可继续</p>
<ul>
<li><strong>条件概率</strong> <code>Conditional probability</code>：在另一个事件已经发生的情况下，另外一个时间发生的概率。如，==在多云天气，下雨的概率是多少？== 这是一个条件概率</li>
<li><strong>联合概率</strong> <code>Joint Probability</code>：计算两个或多个事件同时发生的可能性</li>
<li><strong>边界概率</strong> <code>Marginal Probability</code>：事件发生的概率，与另一个变量的结果无关</li>
<li><strong>比例</strong> <code>Proportionality</code></li>
<li><strong>贝叶斯定理</strong> <code>Bayes' Theorem</code>：概率的公式；贝叶斯定律是指根据可能与事件的先验概率描述了事件的后验概率</li>
</ul>
<h3 id="边界概率">边界概率</h3>
<p>边界概率是指事件发生的概率，可以认为是无条件概率。不以另一个事件为条件；用公式表示为 $P(X)$ 如：抽到的牌是红色的概率是 $P(red) = 0.5$ ；</p>
<h3 id="联合概率">联合概率</h3>
<p>联合概率是指两个事件在同一时间点发生的可能性，公式可以表示为 $P(A \cap B)$</p>
<p><strong>A</strong> 和 <strong>B</strong> 是两个不同的事件相同相交，$P(A \and B)$   $P(A,B)$ = <strong>A</strong> 和 <strong>B</strong> 的联合概率</p>
<p>概率用于处理事件或现象发生的可能性。它被量化为介于 0 和 1 之间的数字，其中 0 表示不可能发生的机会，1 表示事件的一定结果。</p>
<p>如，从一副牌中抽到一张红牌的概率是 $\frac{1}{2}$。这意味着抽到红色和抽到黑色的概率相同；因为一副牌中有52张牌，其中 26 张是红色的，26 张是黑色的，所以抽到一张红牌与抽到一张黑牌的概率是 50%。</p>
<p>而联合概率是对测量同时发生的两个事件，只能应用于可能同时发生多个情况。例如，从一副52张牌扑克中，拿起一张既是红色又是6的牌的联合概率是 $P(6\cap red) = \frac{2}{52} = \frac{1}{26}$ ；这个是怎么得到的呢？因为抽到红色的概率为50%，而一副牌中有两个红色6（红桃6，方片6），而<strong>6</strong>和<strong>红色</strong>是两个独立的概率，那么计算公式就为：$P(6 \cap red) = P(6) \times P(red) = \frac{4}{52} \times \frac{26}{52} = \frac{1}{26}$</p>
<p>在联合概率中 $ \cap $ 称为交集，是事件 <strong>A</strong> 与 事件 <strong>B</strong> 发生的概率的相交点，通过图来表示为：两个圆的相交点，即6和红色牌共同的部分</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220501235937229.png" alt="image-20220501235937229" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="条件概率">条件概率</h3>
<p>条件概率是指事件发生的可能性，基于先有事件的结果的发生乘后续事件概率来计算的，例如，申请大学的通过率为80%，宿舍仅提供给60%学生使用，那么这个人被大学录取并提供宿舍的概率是多少？</p>
<p>$P(accept\ and\ get\ dorm) = P(Accept|Dorm) = P(Accept) \times P(Dorm) = 0.8 \times 0.6 = 0.48$</p>
<p>条件概率将会考虑两个事件之间的关系，例如你被大学录取的概率， 以及为你提供宿舍的概率；<strong>条件概率中的关键点</strong>：</p>
<ul>
<li>另一个事件发生的情况下，这件事发生的几率</li>
<li>表示为，给定 <strong>B</strong> 的概率 <strong>A</strong> 发生的概率，用公式表示为：$P(A|B)$，其中 <strong>A</strong> 取决于 <strong>B</strong> 发生的概率</li>
</ul>
<h4 id="通过例子了解条件概率">通过例子了解条件概率</h4>
<p>上述大致上了解到了：<strong>条件概率取决于先前的结果</strong>，那么通过几个例子来熟悉条件概率</p>
<p><strong>例1</strong>：袋子里有红色，蓝色，绿色三颗玻璃球，每种被拿到的概率相等，那么摸到蓝色之后再摸到红色的条件概率是多少？</p>
<ul>
<li>这里需要先得到摸到蓝色的概率：$P(Blue) = \frac{1}{3}$ 因为只有三种可能性</li>
<li>现在还剩下两颗玻璃球 红色和蓝色，那么摸到红色的概率为：$P(Red) = \frac{1}{2}$ 因为只有两种可能性</li>
<li>那么已经摸到蓝色在摸到红色的概率为 $P(Red|Blue) = \frac{1}{3} \times \frac{1}{2} = \frac{1}{6}$</li>
</ul>
<p><strong>例2</strong>：色子摇出5的概率为 $\frac{1}{6}$ 那么在结果是奇数里摇出5 那么可能就是 $\frac{1}{3}$，而这个<strong>奇数</strong>就是另外的一个条件，因为只有3个奇数，其中一个是5，那么在奇数中，抛出5的概率就是 $\frac{1}{3}$。</p>
<p>通过上述信息可知，<strong>B</strong> 作为附带条件修饰 <strong>A</strong> 发生的概率，称为给定 <strong>B</strong> ，<strong>A</strong> 发生的条件，用$P(A|B)$ 表示。那么可以的出：</p>
<ul>
<li>给定A，B发生的概率为，A和B的发生概率排除掉A的概率，即</li>
<li>$P(B|A) = \frac{P(A \cap B)}{P(A)} $</li>
</ul>
<h4 id="联合概率和条件概率的区别">联合概率和条件概率的区别</h4>
<p><strong>条件概率</strong>是一个事件在另一个事件发生的情况下的概率：$P(X\ given\ Y)$  公式为： $P(X∣Y)$；即一个事件发生的概率取决于另一事件的发生；如：从一副牌中，假设你抽到一张红牌，那么抽到6的概率是 $\frac{1}{13}$；因为26张红牌中仅有两张为6，用公式表示：$P(6|red) = \frac{2}{26}$</p>
<p><strong>联合概率</strong>仅考虑两个事件发生的可能性，对比与条件概率可用于计算联合概率：$P(X \cap Y) = P(X|Y) \times P(Y)$</p>
<p>通过合并上述例子得到，同时抽到6和红色的概率为：$\frac{1}{26}$</p>
<h3 id="贝叶斯定理-1">贝叶斯定理</h3>
<p>贝叶斯定理是确定条件概率的数学公式。贝叶斯定理依赖于先验概率分布以计算后验概率。</p>
<h4 id="后验概率和先验概率">后验概率和先验概率</h4>
<ul>
<li>先验概率 <code>prior probability</code>：在收集新数据之前发送事件的概率</li>
<li>后验概率 <code>posterior probability</code>：得到新的数据来修正之前事件发生的概率；换句话说是<strong>后验概率是在事件 B 已经发生的情况下，事件 A 发生的概率</strong>。</li>
</ul>
<p>例，从一副52 张牌中抽取一张牌，那么这张牌是K的概率是 $\frac{4}{52}$ , 因为一副牌中有4张K；假设抽中的牌是一张人物牌，那么抽到是K的概率则是 $\frac{4}{12}$；因为一副牌中有12张人物牌。那么贝叶斯定理的公式为：</p>
<ul>
<li>$P(A|B) = \frac{P(A \cap B)}{P(B)}$，$P(B|A) = \frac{P(B \cap A)}{P(A)}$
<ul>
<li>$P(A \cap B)$，$P(B \cap A)$ A和B同时发生和B和A同时发生时相等的</li>
<li>$P(B \cap A) = P(B|A) \times P(A)$</li>
<li>$P(A \cap B) = P(A|B) \times P(B)$</li>
</ul>
</li>
<li>那么根据上面的公式，已知 $P(A \cap B) = P(B \cap A)$ 可推导出公式：
<ul>
<li>因为 $P(B \cap A) = P(A \cap B)$ ，那么 $P(B|A) \times P(A) = P(A|B) \times P(B)$</li>
<li>那么吧 $P(A)$ 放置等式右边即 $P(B|A) = \frac{P(A|B) \times P(B)}{P(A)}$</li>
</ul>
</li>
<li>那么最终  <code>Formula for Bayes</code> 为  $P(B|A) = \frac{P(A|B) \times P(B)}{P(A)}$</li>
</ul>
<p>其中：</p>
<ul>
<li>
<p>$P(A)$：<strong>A</strong> 的边界概率</p>
</li>
<li>
<p>$P(B)$：<strong>B</strong> 的边界概率</p>
</li>
<li>
<p>$P(A|B)$ ：条件概率，已知 <strong>B</strong>，<strong>A</strong> 的概率</p>
</li>
<li>
<p>$P(B|A)$ ：条件概率，已知 <strong>A</strong>，<strong>B</strong> 的概率</p>
</li>
<li>
<p>$P(B \cap A)$：联合概率 <strong>B</strong> 与 <strong>A</strong> 同时发生的概率</p>
</li>
</ul>
<p>一个简单的概率问题可能会问：茅台股价下跌的概率是多少？条件概率通过询问这个问题更进一步：鉴于A股平均指数下跌，茅台股价下跌的概率是多少？ 给定 B 已经发生的条件下 A 的概率可以表示为：</p>
<p>$P(Mao|AS) = \frac{P(Mao \cap AS)}{P(AS)}$</p>
<p>$P(Mao \cap AS)$ 是 A 和 B 同时发生的概率，与 A 发生的情况下 B 也发生的概率 乘 A 发生的概率相等表示为： $P(Mao) \times P(AS|Mao)$；这两个表达式相等，也就是贝叶斯定理，可以表示为：</p>
<ul>
<li>如果， $P(Mao \cap AS) = P(Mao) \times P(AS|Mao)$</li>
<li>那么， $P(Mao|AS) = \frac{P(Mao) \times P(AS|Mao)}{P(AS)}$</li>
</ul>
<p>$P(Mao)$ 和 $P(AS)$ 分别为茅台和A股的下跌概率，彼此间没有关系</p>
<p>一般情况下，都是以 <strong>x</strong> （输入） <strong>y</strong> （输出） 在函数中，假设 $x=AS$ , $y=MAO$ 那么替代到公式中就 $P(Y|X) = \frac{P(X|Y) \times P(Y)}{P(X)}$</p>
<h2 id="朴素贝叶斯算法">朴素贝叶斯算法</h2>
<p>朴素贝叶斯不是一个的算法，而是一组算法，所有这些算法都基于一个共同的原则，即每一对被分类的特征必须相互独立。朴素贝叶斯是一个基本的贝叶斯称呼，包含三种算法的集合：多项式 <code>Multinomial</code>、 伯努利 <code>Bernoulli</code>、高斯 <code>Gaussian</code>。</p>
<h3 id="伯努利">伯努利</h3>
<p>伯努利朴素贝叶斯，又叫做二项式，只接受二进制值，简而言之，在伯努利中必须计算每个值的二进制出现特征，即一个单词是与否出现在文档中。</p>
<p>通俗地来说，伯努利有两个互斥的结果：$$NB=\begin{cases}
P(X=1)\ = \ q\
P(X=0)\ = \ 1-q\
\end{cases}
$$ ，在伯努利中，可以有多个特征，但每个特征都假设为是二进制的变量，因此，需要将样本表示为二进制向量。</p>
<p>那么扩展出的公式为：$P(A|B) = \frac{P(B|A) \times P(A)}{P(A) \times P(B|A) + P(not A) \times P(B|not A)}$</p>
<p><strong>例子</strong>：假设COVID-19测试并不准确，有**95%**几率在感染时测试出阳性（敏感性），这就意味着如果有人并未感染的概率是相同的（特异性）；问：如果Jeovanna检测为阳性，那么他感染COVID-19的概率是多少？</p>
<blockquote>
<p>敏感性和特异性是医学用语；敏感性，病人测出阳性的比例，特异性，非病人测试阴性的比例</p>
</blockquote>
<p>一般情况下没有更多的信息来确定Jeovanna是否感染，如驻留场所，是否发烧，丧失味觉等。就需要更多的信息来计算Jeovanna感染率，比如预估Jeovanna感染率为1%，这1%就是先验概率。</p>
<p>此时有100000人的测试样本，预计1000人感染（先验1%），那么就是99000为感染，又因为测试具有 95% 的敏感性和 95% 的特异性，这代表了 1000的95% 和 99000的5% 是阳性。整理一个表格</p>
<table>
<thead>
<tr>
<th></th>
<th>Has COVID-19</th>
<th>Do not Has COVID-19</th>
<th>count</th>
</tr>
</thead>
<tbody>
<tr>
<td>阳性</td>
<td>950</td>
<td>4950</td>
<td>5900</td>
</tr>
<tr>
<td>阴性</td>
<td>50</td>
<td>94050</td>
<td>94100</td>
</tr>
</tbody>
</table>
<p>那么可以看出，阳性的人并感染COVID-19的概率是，$\frac{950}{5900} = 16%$ ；那么也就是Jeovanna有16%几率是感染 COVID-19。</p>
<p>此时将先验概率设置为16%，那么爱丽丝得COVID-19的可能性为：</p>
<blockquote>
<p>$P(B|A)$ ：在95%成功率情况下又获得了阳性</p>
<p>$P(A)$：阳性的检测成功率</p>
<p>已知，$P(B|A) = 0.16$ ，$P(A) = 0.95$</p>
<p>$P(A|B) = \frac{P(B|A) \times P(A)}{P(A) \times P(B|A) + P(not A) \times P(B|not A)}  = \frac{0.16\times0.95}{0.95\times 0.16 + (1-0.95)\times(1-0.16)}= \frac{0.152}{0.194} = 0.7835 = 78.35%$</p>
</blockquote>
<p>那么就可以得知，在阳性情况下感染COVID-19的情况下，再去检测会有78%几率阳性</p>
<h3 id="多项式">多项式</h3>
<p>多项式朴素贝叶斯是基于多项分布的朴素叶贝斯，用来处理文本，计算 $d$ 在 $c$ 中的概率计算如下：</p>
<p>$P(c|d) \ \propto P(c) \prod_{i=1}^n\ P(t_k|c)$</p>
<p>通俗来说就是二项式的一个变种，是计算多个不同的实例</p>
<p>$P(t_k|c)$ 是 $t_k$ 的 条件概率，发生在数据集 $c$ 中，$P(t_k|c)$ 解释为 $t_k$ 有多少证据表明 $c$ 是正确的；$P(c)$ 是先验条件 $t1..\ t2..\ t3..\ tn_d$ 中的标记 $d$，它们是我们用于分类的词汇表的一部分，$n_d$ 是 标记 $d$ 的数量。</p>
<p>例如：&ldquo;Peking and Taipei join the WTO&rdquo;，$&lt;Peking,\ Taipei,\ join,\ WTO&gt;$ ,那么 $n_d = 4$</p>
<p>那么可以简化为，</p>
<p>$P(c=x|d=c_k) = P(c^1=x^1..,\ c^2=x^2..,\ c^n=x^n|d=c_k) = \prod_{i=1}^n(c^i|d)x^i + (1-P(c^i|d))	(1-x^i)$</p>
<p>$\prod_{i=1}^n$ 连乘积，即从下标起乘到上标</p>
<h2 id="朴素贝叶斯实现">朴素贝叶斯实现</h2>
<p>首先将朴素贝叶斯为 5 个部分：</p>
<ul>
<li>分类</li>
<li>数据集汇总</li>
<li>按类别汇总数据</li>
<li>高斯密度函数</li>
<li>分类概率</li>
</ul>
<h3 id="分类">分类</h3>
<p>根据数据所属的类别来计算数据的概率，即所谓的基本率。</p>
<p>先创建一个字典对象，其中每个键都是类值，然后添加所有记录的列表作为字典中的值。</p>
<p>假设每行中的最后一列是类型。</p>
<pre><code class="language-python"># 按类拆分数据集，返回结构是一个词典
def separate_by_class(dataset):
	separated = dict()
	for i in range(len(dataset)):
		vector = dataset[i]
		class_value = vector[-1] # dataset最后一行是类别
		if (class_value not in separated):
			separated[class_value] = list()
		separated[class_value].append(vector)
	return separated
</code></pre>
<p>准备一些数据集</p>
<pre><code>X1				X2						Class
3.393533211		2.331273381				0
3.110073483		1.781539638				0
7.423436942		4.696522875				1
1.343808831		3.368360954				0
3.582294042		4.67917911				0
9.172168622		2.511101045				1
7.792783481		3.424088941				1
2.280362439		2.866990263				0
5.745051997		3.533989803				1
7.939820817		0.791637231				1
</code></pre>
<p>测试分类函数的功能</p>
<pre><code class="language-python">def separate_by_class(dataset):
	separated = dict()
	for i in range(len(dataset)):
		vector = dataset[i]
		class_value = vector[-1]
		if (class_value not in separated):
			separated[class_value] = list()
		separated[class_value].append(vector)
	return separated
 
# 测试数据集
dataset = [
    [3.393533211,2.331273381,0],
	[3.110073483,1.781539638,0],
	[1.343808831,3.368360954,0],
    [7.423436942,4.696522875,1],
	[3.582294042,4.67917911,0],
    [9.172168622,2.511101045,1],
	[7.792783481,3.424088941,1],
	[2.280362439,2.866990263,0],
	[5.745051997,3.533989803,1],
	[7.939820817,0.791637231,1]
]
separated = separate_by_class(dataset)
for label in separated:
	print(label)
	for row in separated[label]:
		print(row)
</code></pre>
<p>可以看到分类是成功的</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220503215102264.png" alt="image-20220503215102264" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="数据集汇总">数据集汇总</h3>
<p>现在需要对给出数据集的两个数据进行统计，如何对指定数据集做概率计算？需要以下几步</p>
<p>计算数据集两个数据的平均值和标准差</p>
<p>平均值为： $\frac{sum(x)}{n} \times count(x)$ ；其中 $x$ 为正在查找值的列表</p>
<p>mean函数用于计算平均值</p>
<pre><code class="language-python">def mean(numbers):
	return sum(numbers) / float(len(numbers))
</code></pre>
<p>样本标准差的计算方式为平均值的平均差。公式可以为 <code> sqrt((sum i to N (x_i – mean(x))^2) / N-1)</code></p>
<p>函数用来计算</p>
<pre><code class="language-python">from math import sqrt
 
# Calculate the standard deviation of a list of numbers
def stdev(numbers):
	avg = mean(numbers) # 平均值
	variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)
	return sqrt(variance)
</code></pre>
<p>还需要对每个数据的每一列计算平均值和标准偏差统计量。通过将每列的所有值收集到一个列表中并计算该列表的平均值和标准差。计算完成后，将统计信息收集到数据汇总的列表或元组中。然后，对数据集中的每一列重复此操作并返回统计元组列表。</p>
<p>下面是 数据汇总的函数 <code>summarise_dataset()</code>用来统计每列列表的平均值和标准差</p>
<pre><code class="language-python">from math import sqrt
 
# 计算平均数
def mean(numbers):
	return sum(numbers)/float(len(numbers))
 
# 计算标准差
def stdev(numbers): # 标准差
	avg = mean(numbers) # 计算平均值
	variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1) # 计算所有的平方
	return sqrt(variance)
 
# 数据汇总
def summarize_dataset(dataset):
    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]
    del(summaries[-1]) # 因为分类不需要所以。删除掉分类哪行
    return summaries
 
# Test summarizing a dataset
dataset = [
    [3.393533211,2.331273381,0],
	[3.110073483,1.781539638,0],
	[1.343808831,3.368360954,0],
	[3.582294042,4.67917911,0],
	[2.280362439,2.866990263,0],
	[7.423436942,4.696522875,1],
	[5.745051997,3.533989803,1],
	[9.172168622,2.511101045,1],
	[7.792783481,3.424088941,1],
	[7.939820817,0.791637231,1]]
summary = summarize_dataset(dataset)
print(summary)
</code></pre>
<p>这里使用的是<code>zip()</code> 函数，将每列作为提供的参数。使用 * 作为位置函数，运将数据集传递给 <code>zip()</code> ，这个运算会将每一行的分割为单独列表。然后<code>zip()</code> 遍历每一行的每个元素，返回一列作为数字列表。</p>
<p>然后计算每列中的平均数、标准差和行数。删掉不需要的列（第三列类别列的平均数，标准差和行数）</p>
<p>可以看到</p>
<pre><code>[
(5.178333386499999, 2.7665845055177263, 10), 
(2.9984683241, 1.218556343617447, 10)
]
</code></pre>
<h3 id="根据类别汇总数据">根据类别汇总数据</h3>
<p><code>separate_by_class()</code> 是将数据分成行，现在要编写一个 <code>summarise_dataset()</code>；是先计算每列的统计汇总信息，然后在按照子集分类（X1，X2）</p>
<pre><code class="language-python"># 按类拆分数据集
def summarize_by_class(dataset):
	separated = separate_by_class(dataset)
	summaries = dict()
	for class_value, rows in separated.items():
		summaries[class_value] = summarize_dataset(rows)
	return summaries
</code></pre>
<p>这是完整的代码</p>
<pre><code class="language-python">from math import sqrt
 
# 计算平均数
def mean(numbers):
	return sum(numbers)/float(len(numbers))
 
# 计算标准差
def stdev(numbers): # 标准差
	avg = mean(numbers) # 计算平均值
	variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1) # 计算所有的平方
	return sqrt(variance)
 
# 数据汇总
def summarize_dataset(dataset):
    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]
    del(summaries[-1]) # 因为分类不需要所以。删除掉分类哪行
    return summaries
 
# 按类进行数据汇总
def summarize_by_class(dataset):
	separated = separate_by_class(dataset)
	summaries = dict()
	for class_value, rows in separated.items():
		summaries[class_value] = summarize_dataset(rows)
	return summaries
 
# 测试数据集
dataset = [
    [3.393533211,2.331273381,0],
	[3.110073483,1.781539638,0],
	[1.343808831,3.368360954,0],
	[3.582294042,4.67917911,0],
	[2.280362439,2.866990263,0],
	[7.423436942,4.696522875,1],
	[5.745051997,3.533989803,1],
	[9.172168622,2.511101045,1],
	[7.792783481,3.424088941,1],
	[7.939820817,0.791637231,1]]
summary = summarize_by_class(dataset)
for label in summary:
	print(label)
	for row in summary[label]:
		print(row)
</code></pre>
<p>按照分类，对每列计算平均值和标准差</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220504174107364.png" alt="image-20220504174107364" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="高斯分布函数">高斯分布函数</h3>
<p>高斯分布 <code>Gaussian distribution</code> 可以用两个数字来概括，高斯分布是具有对称的钟形的分布，所以中心右侧是左侧的镜像，曲线下的面积代表概率，曲线总面积之和等于1。</p>
<p>高斯分布中的大多数连续数据值倾向于围绕均值聚集，值离均值越远，那么它发生的可能性就越小。接近但从未完全贴合x 轴。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/bell-curve.jpg" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>高斯分布由均值和标准差两个参数决定的，任何点 (x) 都可以通过公式 $z = \frac{x-mean}{standard\ deviation}$ 来计算</p>
<blockquote>
<p>Reference</p>
<p><a href="https://www.simplypsychology.org/normal-distribution.html" target="_blank"
   rel="noopener nofollow noreferrer" >normal distribution</a></p>
</blockquote>
<p>通过这一点，就可以知道就可以计算出给定的概率，公式为：</p>
<p>$P({x_i}|Y) = \frac{1}{\sqrt2\pi\sigma_y^2}exp(-(\frac{(x_i-\mu_y)^2}{2\sigma_y^2})$</p>
<p>其中，$\sigma$ 为标准差，$\mu$ 为平均值，那么转换为可读懂的公式为：</p>
<p>$f(x) = \frac{1}{\sqrt{(2 \times pi )}\times sigma} \times exp(-(\frac{(x-mean)^2}{(2 \times sigma)^2}))$</p>
<p>其中，<code>sigma</code>是  <code>x</code> 的标准差，<code>mean</code>  是 <code>x</code> 的平均值，PI是 就是 $\pi$ <code>math.pi</code> 的值。</p>
<p>那么在转换成python中的代码为：</p>
<pre><code class="language-python">f(x) = (1 / sqrt(2 * PI) * sigma) * exp(-((x-mean)^2 / (2 * sigma^2)))
</code></pre>
<p>那么用python实现一个函数，来实现高斯公式</p>
<pre><code class="language-python"># 计算高斯分布的函数，需要三个参数，x 平均值，标准差
def calculate_probability(x, mean, stdev):
	exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))
	return (1 / (sqrt(2 * pi) * stdev)) * exponent
</code></pre>
<p>这里通过函数测试三个数据，<code>(0,1,1)</code>， <code>(1,1,1)</code>，<code>(2,1,1)</code></p>
<pre><code class="language-python">from math import sqrt
from math import pi
from math import exp
 
# 计算高斯分布的函数，需要三个参数，x 平均值，标准差
def calculate_probability(x, mean, stdev):
	exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))
	return (1 / (sqrt(2 * pi) * stdev)) * exponent
 
print(calculate_probability(1.0, 1.0, 1.0))
print(calculate_probability(2.0, 1.0, 1.0))
print(calculate_probability(0.0, 1.0, 1.0))
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220504220553349.png" alt="image-20220504220553349" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>这里可以看到结果，<code>(1,1,1)</code> 的概率最可能（三个值中趋于钟形顶部）</p>
<h3 id="分类概率">分类概率</h3>
<p>到这里，可以尝试通过测试数据来统计新数据的概率，这里每个类别的概率都是单独计算的，这里将简化概率计算公式 $P(class|data) = P(data|class) \times P(class)$；这是一个常见的简化，这将意味着将结果为最大值的类的计算作为预测结果。因为通常对预测感兴趣，而不是概率</p>
<p>对于上述例子，有两个变量，这里以 <code>class=0</code> 的类别来说明，公式为：</p>
<p>$P(class=0|X1,X2) = P(X1|class=0) \times P(X2|class=0) \times P(class=0)$</p>
<p>编写一个聚合函数，将上述四个步骤汇总处理，</p>
<pre><code class="language-python"># Example of calculating class probabilities
from math import sqrt
from math import pi
from math import exp

# 拆分数据集
def separate_by_class(dataset):
    separated = dict()
    for i in range(len(dataset)):
        vector = dataset[i]
        class_value = vector[-1]
        if (class_value not in separated):
            separated[class_value] = list()
    
        separated[class_value].append(vector)

    print(separated)
    return separated

# 计算平均数
def mean(numbers):
	return sum(numbers)/float(len(numbers))

# 计算标准差
def stdev(numbers): 
	avg = mean(numbers)  # 计算平均值
	variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1) # 标准差
	return sqrt(variance)

# 数据汇总
def summarize_dataset(dataset):
	summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]
	del(summaries[-1])
	return summaries

# 按类进行数据汇总
def summarize_by_class(dataset):
	separated = separate_by_class(dataset)
	summaries = dict()
	for class_value, rows in separated.items():
		summaries[class_value] = summarize_dataset(rows)
	return summaries

# 计算高斯分布的函数，需要三个参数，x 平均值，标准差
def calculate_probability(x, mean, stdev):
	exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))
	return (1 / (sqrt(2 * pi) * stdev)) * exponent

# 计算每个分类的概率
def converge_probabilities(summaries, row):
    # 计算所有分类的个数
    total_rows = sum([summaries[label][0][2] for label in summaries])
    probabilities = dict()
    for class_value, class_summaries in summaries.items():
        # 计算分类的概率，如这个分类在总分类里概率多少
        probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)
        for i in range(len(class_summaries)):
            mean, stdev, _ = class_summaries[i]
            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)
    return probabilities

# 测试数据集
dataset = [
    [3.393533211,2.331273381,0],
	[3.110073483,1.781539638,0],
	[1.343808831,3.368360954,0],
	[3.582294042,4.67917911,0],
	[2.280362439,2.866990263,0],
	[7.423436942,4.696522875,1],
	[5.745051997,3.533989803,1],
	[9.172168622,2.511101045,1],
	[7.792783481,3.424088941,1],
	[7.939820817,0.791637231,1]]
summaries = summarize_by_class(dataset)
probabilities = converge_probabilities(summaries, dataset[0])
print(probabilities)
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220505233319199.png" alt="image-20220505233319199" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>由结果可以得知，<code>dataset[0]</code> <strong>X1</strong> 的概率（0.0503）要大于 <strong>X2</strong> 的概率（0.0001），所以可以正确的判断出 <code>dataset[0]</code> 属于 <strong>X1</strong> 分类</p>
<h2 id="鸢尾花iris分类">鸢尾花(Iris)分类</h2>
<p>鸢尾花分类，是模式识别中非常出名的一种数据库，需要先将数据下载：</p>
<ul>
<li>
<p><a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.names" target="_blank"
   rel="noopener nofollow noreferrer" >关于Iris-databases数据集的说明</a></p>
</li>
<li>
<p><a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv" target="_blank"
   rel="noopener nofollow noreferrer" >iris dataset</a></p>
</li>
</ul>
<h3 id="实现开始">实现开始</h3>
<p>实验是根据上述实验的步骤，将朴素贝叶斯算法应用在鸢尾花数据集中，鸢尾花数据集的实验也是需要相同的步骤，只不过对于数据集中的数据还需要一些其他的步骤，大致可分为以下几种操作：</p>
<ul>
<li>数据的预处理
<ul>
<li>从文件中读取数据</li>
<li>将数据类型转换为可用于上面实验的类型（<code>float</code>）</li>
<li>将真实分类转换为数字 <code>int</code></li>
</ul>
</li>
<li>分类</li>
<li>数据集汇总</li>
<li>按类别汇总数据</li>
<li>高斯密度函数</li>
<li>分类概率</li>
</ul>
<pre><code class="language-python">from csv import reader
from random import seed
from random import randrange
from math import sqrt
from math import exp
from math import pi
 
# 读取数据集
def load_csv(filename):
	dataset = list()
	with open(filename, 'r') as file:
		csv_reader = reader(file)
		for row in csv_reader:
			if not row:
				continue
			dataset.append(row)
	return dataset
 
# 将每行的数字转换为float
def str_column_to_float(dataset, column):
    
    for row in dataset:
        row[column] = float(row[column].strip())
 
# 将真实分类转换为数字，按照下标
def str_column_to_int(dataset, column):
    '''
    :param dataset: list, 数据集
    :param column: string，是为类型的列要传入
    :return: None
    '''
    # 通过循环拿到所有分类
    class_values = [row[column] for row in dataset]
    # 对分类型去重
    unique = set(class_values)
    
    lookup = dict()
    # 拿到分类值的key 下标
    for i, value in enumerate(unique):
        lookup[value] = i

    # 已对应的下标进行替换
    for row in dataset:
        row[column] = lookup[row[column]]
    return lookup
 
# 将数据的一部分作为训练数据
def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / n_folds)
	for _ in range(n_folds):
		fold = list()
		while len(fold) &lt; fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split
 
# 计算准确度
def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0
 
# 对算法数据进行评估
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
    &quot;&quot;&quot;
    :param dataset:list, 原始数据集
    :param algorithm:function，算法函数
    :param n_folds:int，取多少数据作为训练集
    :param args:options ，参数
    :return: None
    &quot;&quot;&quot;
    folds = cross_validation_split(dataset, n_folds)
    scores = list()
    for fold in folds:
        train_set = list(folds)
        train_set.remove(fold)
        # 合并成一个数组
        train_set = sum(train_set, [])
        
        test_set = list()
        for row in fold:
            row_copy = list(row)
            test_set.append(row_copy)
            row_copy[-1] = None # 将最后一个类型字段设置为None
        predicted = algorithm(train_set, test_set, *args)
        # 真实的类型
        actual = [row[-1] for row in fold]
        # 精确的分数，即这一组数据正确率
        accuracy = accuracy_metric(actual, predicted)
        scores.append(accuracy)
    print(scores)
    return scores
 
# 按照分类拆分
def separate_by_class(dataset):
	separated = dict()
	for i in range(len(dataset)):
		vector = dataset[i]
		class_value = vector[-1]
		if (class_value not in separated):
			separated[class_value] = list()
		separated[class_value].append(vector)
	return separated
 
# 计算这一系列的平均值
def mean(numbers):
	return sum(numbers)/float(len(numbers))
 
# 计算一系列数字的标准差
def stdev(numbers):
	avg = mean(numbers)
	variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)
	return sqrt(variance)
 
# 计算数据集中每列的平均值 标准差 长度
def summarize_dataset(dataset):
	summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]
	del(summaries[-1])
	return summaries
 
# 按照分类划分数据集
def summarize_by_class(dataset):
	separated = separate_by_class(dataset)
	summaries = dict()
	for class_value, rows in separated.items():
		summaries[class_value] = summarize_dataset(rows)
	return summaries
 
# 计算x的高斯概率
def calculate_probability(x, mean, stdev):
    &quot;&quot;&quot;
    :param x:float, 计算这个值的高斯概率
    :param mean:float，x的平均值
    :param stdev:float，x的标准差
    :return: None
    &quot;&quot;&quot;
    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))
    return (1 / (sqrt(2 * pi) * stdev)) * exponent
 
# 计算每行的概率
def converge_probabilities(summaries, row):
    # 计算所有分类的个数
	total_rows = sum([summaries[label][0][2] for label in summaries])
	probabilities = dict()
	for class_value, class_summaries in summaries.items():
        # 计算分类的概率，如这个分类在总分类里概率多少
        # 公式中的P(class)
		probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)
        # 通过公式  P(X1|class=0) * P(X2|class=0) * P(class=0) 计算高斯概率
		for i in range(len(class_summaries)):
			mean, stdev, _ = class_summaries[i]
			probabilities[class_value] *= calculate_probability(row[i], mean, stdev)
	return probabilities
 
# 通过计算出来的值，预测该花属于哪个品种，取高斯概率最大的值
def predict(summaries, row):
	probabilities = converge_probabilities(summaries, row)
	best_label, best_prob = None, -1
	for class_value, probability in probabilities.items():
		if best_label is None or probability &gt; best_prob:
			best_prob = probability
			best_label = class_value
	return best_label
 
# Naive Bayes Algorithm
def naive_bayes(train, test):
    # 训练数据按照类分类排序
    summarize = summarize_by_class(train)
    predictions = list()
    for row in test:
        output = predict(summarize, row)
        predictions.append(output)

    print(predictions)
    return(predictions)
 
# 测试
if __name__ == '__main__':
    seed(1)
    filename = 'iris.csv'
    dataset = load_csv(filename)
    # 转换数值为float
    for i in range(len(dataset[0])-1):
        str_column_to_float(dataset, i)
    # 将类型转换为数字
    str_column_to_int(dataset, len(dataset[0])-1)

    # 将数据分位测试数据和训练数据，folds为多少数据为训练数据
    n_folds = 5
    scores = evaluate_algorithm(dataset, naive_bayes, n_folds)
    print('Scores: %s' % scores)
    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220506190801092.png" alt="image-20220506190801092" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>可以看到运行结果，对鸢尾花数据集的预测正确率，平均为95.333%</p>
<p>现在对 <code>main</code> 部分进行修改，使用全部数据集作为训练，新增记录作为预测</p>
<pre><code class="language-python"># 按照整个数据集分类
model = summarize_by_class(dataset)
# 新加一行预测数据
row = [5.3,3.9,3.2,2.3]
# 根据训练集进行对数据预测
label = predict(model, row)

print('Data=%s, Predicted: %s' % (row, label))
</code></pre>
<p>完整修改过的代码如下：</p>
<pre><code class="language-python">from csv import reader
from random import seed
from random import randrange
from math import sqrt
from math import exp
from math import pi
 
# 读取数据集
def load_csv(filename):
	dataset = list()
	with open(filename, 'r') as file:
		csv_reader = reader(file)
		for row in csv_reader:
			if not row:
				continue
			dataset.append(row)
	return dataset
 
# 将每行的数字转换为float
def str_column_to_float(dataset, column):
    
    for row in dataset:
        row[column] = float(row[column].strip())
 
# 将真实分类转换为数字，按照下标
def str_column_to_int(dataset, column):
    '''
    :param dataset: list, 数据集
    :param column: string，是为类型的列要传入
    :return: None
    '''
    # 通过循环拿到所有分类
    class_values = [row[column] for row in dataset]
    # 对分类型去重
    unique = set(class_values)
    
    lookup = dict()
    # 拿到分类值的key 下标
    for i, value in enumerate(unique):
        lookup[value] = i

    # 增加一行，来显示下标和真实名称对应的数据
    print(lookup)

    # 已对应的下标进行替换
    for row in dataset:
        row[column] = lookup[row[column]]
    return lookup
 
# 将数据的一部分作为训练数据
def cross_validation_split(dataset, n_folds):
	dataset_split = list()
	dataset_copy = list(dataset)
	fold_size = int(len(dataset) / n_folds)
	for _ in range(n_folds):
		fold = list()
		while len(fold) &lt; fold_size:
			index = randrange(len(dataset_copy))
			fold.append(dataset_copy.pop(index))
		dataset_split.append(fold)
	return dataset_split
 
# 计算准确度
def accuracy_metric(actual, predicted):
	correct = 0
	for i in range(len(actual)):
		if actual[i] == predicted[i]:
			correct += 1
	return correct / float(len(actual)) * 100.0
 
# 对算法数据进行评估
def evaluate_algorithm(dataset, algorithm, n_folds, *args):
    &quot;&quot;&quot;
    :param dataset:list, 原始数据集
    :param algorithm:function，算法函数
    :param n_folds:int，取多少数据作为训练集
    :param args:options ，参数
    :return: None
    &quot;&quot;&quot;
    folds = cross_validation_split(dataset, n_folds)
    scores = list()
    for fold in folds:
        train_set = list(folds)
        train_set.remove(fold)
        # 合并成一个数组
        train_set = sum(train_set, [])
        
        test_set = list()
        for row in fold:
            row_copy = list(row)
            test_set.append(row_copy)
            row_copy[-1] = None # 将最后一个类型字段设置为None
        predicted = algorithm(train_set, test_set, *args)
        # 真实的类型
        actual = [row[-1] for row in fold]
        # 精确的分数，即这一组数据正确率
        accuracy = accuracy_metric(actual, predicted)
        scores.append(accuracy)
    print(scores)
    return scores
 
# 按照分类拆分
def separate_by_class(dataset):
    &quot;&quot;&quot;
    :param dataset:list, 按分类好的列表
    :return: dict, 每个分类的每列（属性）的平均值，标准差，个数
    &quot;&quot;&quot;
    separated = dict()
    for i in range(len(dataset)):
        vector = dataset[i]
        class_value = vector[-1]
        if (class_value not in separated):
            separated[class_value] = list()
        separated[class_value].append(vector)
    return separated
 
# 计算这一系列的平均值
def mean(numbers):
	return sum(numbers)/float(len(numbers))
 
# 计算一系列数字的标准差
def stdev(numbers):
	avg = mean(numbers)
	variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)
	return sqrt(variance)
 
# 计算数据集中每列的平均值 标准差 长度
def summarize_dataset(dataset):
	summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]
	del(summaries[-1])
	return summaries
 
# 按照分类划分数据集
def summarize_by_class(dataset):
    separated = separate_by_class(dataset)
    summaries = dict()
    for class_value, rows in separated.items():
        summaries[class_value] = summarize_dataset(rows)

    return summaries
 
# 计算x的高斯概率
def calculate_probability(x, mean, stdev):
    &quot;&quot;&quot;
    :param x:float, 计算这个值的高斯概率
    :param mean:float，x的平均值
    :param stdev:float，x的标准差
    :return: None
    &quot;&quot;&quot;
    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))
    return (1 / (sqrt(2 * pi) * stdev)) * exponent
 
# 计算每行的概率
def converge_probabilities(summaries, row):
    # 计算所有分类的个数
    total_rows = sum([summaries[label][0][2] for label in summaries])
    probabilities = dict()
    for class_value, class_summaries in summaries.items():
        # 计算分类的概率，如这个分类在总分类里概率多少
        # 公式中的P(class)
        probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)
        # 通过公式  P(X1|class=0) * P(X2|class=0) * P(class=0) 计算高斯概率
        for i in range(len(class_summaries)):
            mean, stdev, _ = class_summaries[i]
            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)
      
    return probabilities
 
# 通过计算出来的值，预测该花属于哪个品种，取高斯概率最大的值
def predict(summaries, row):
	probabilities = converge_probabilities(summaries, row)
	best_label, best_prob = None, -1
	for class_value, probability in probabilities.items():
		if best_label is None or probability &gt; best_prob:
			best_prob = probability
			best_label = class_value
	return best_label
 
# Naive Bayes Algorithm
def naive_bayes(train, test):
    # 训练数据按照类分类排序
    summarize = summarize_by_class(train)
    predictions = list()
    for row in test:
        output = predict(summarize, row)
        predictions.append(output)

    print(predictions)
    return(predictions)
 
# 测试
if __name__ == '__main__':
    seed(1)
    filename = 'iris.csv'
    dataset = load_csv(filename)
    # 转换数值为float
    for i in range(len(dataset[0])-1):
        str_column_to_float(dataset, i)
    # 将类型转换为数字
    str_column_to_int(dataset, len(dataset[0])-1)

    # 按照整个数据集分类
    model = summarize_by_class(dataset)
    # 新加一行预测数据
    row = [5.3,3.9,3.2,2.3]
    # 根据训练集进行对数据预测
    label = predict(model, row)
    print('Data=%s, Predicted: %s' % (row, label))
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220506193631974.png" alt="image-20220506193631974" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>可以看到对数据集 <code>[5.3,3.9,3.2,2.3]</code> 预测为 <code>versicolor</code>，那将属性修改为，<code>[2.3,0.9,0.2,1.3]</code> 预测结果为 <code>setosa</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220506193805325.png" alt="image-20220506193805325" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p>Reference</p>
<p><a href="https://iq.opengenus.org/gaussian-naive-bayes/" target="_blank"
   rel="noopener nofollow noreferrer" >gaussian naive bayes</a></p>
<p><a href="https://www.varsity.co.uk/science/21149" target="_blank"
   rel="noopener nofollow noreferrer" >Naive Bayes Example</a></p>
<p><a href="https://www.omnicalculator.com/statistics/bayes-theorem" target="_blank"
   rel="noopener nofollow noreferrer" >caculator naive bayes</a></p>
<p><a href="https://towardsdatascience.com/a-mathematical-explanation-of-naive-bayes-in-5-minutes-44adebcdb5f8" target="_blank"
   rel="noopener nofollow noreferrer" >五分钟了解朴素贝叶斯</a></p>
<p><a href="https://www.investopedia.com/terms/j/jointprobability.asp" target="_blank"
   rel="noopener nofollow noreferrer" >Joint Probability</a></p>
<p><a href="https://www.investopedia.com/terms/c/conditional_probability.asp" target="_blank"
   rel="noopener nofollow noreferrer" >Conditional Probability</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>常用加密算法学习总结之散列函数(hash function)</title>
      <link>https://www.oomkill.com/2020/11/hash-function/</link>
      <pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2020/11/hash-function/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>散列函数（Hash function）又称散列算法、哈希函数，散列函数把消息或数据压缩成摘要，使得数据量变小，将数据的格式固定下来。该函数将数据打乱混合，重新创建一个叫做散列值（hash values）的指纹。这种转化是一种压缩映射，也就是散列值的空间通常远小于输入值的空间，不同的输入可能会散列成相同的输出，二不可能从散列值来唯一的确定输入值。<strong>简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要函数</strong>。</p>
<h2 id="散列函数性质">散列函数性质</h2>
<p>通过使用单向散列函数，即便是确认几百MB大小的文件的完整性，也只要对比很短的散列值就可以了。那么，单向散列函数必须具备怎样的性质呢？我们来整理一下。</p>
<ul>
<li>
<p><strong>根据任意长度的消息计算出固定长度的散列值</strong></p>
</li>
<li>
<p><strong>能够快速计算出散列值</strong></p>
</li>
</ul>
<p>计算散列值所花费的时间短。尽管消息越长，计算散列值的时间也会越长，但如果不能在现实的时间内完成计算就没有意义了。</p>
<ul>
<li>
<p><strong>消息不同散列值也不同</strong></p>
</li>
<li>
<p><strong>难以发现碰撞的性质称为抗碰撞性（collisionresistance）</strong>。密码技术中所使用的单向散列函数，都需要具备抗碰撞性。强抗碰撞性，是指要找到散列值相同的两条不同的消息是非常困难的这一性质。在这里，散列值可以是任意值。密码技术中的单向散列函数必须具备强抗碰撞性。</p>
</li>
<li>
<p><strong>具备单向性</strong></p>
</li>
</ul>
<p>单向散列函数必须具备单向性（one-way）。单向性指的是无法通过散列值反算出消息的性质。根据消息计算散列值可以很容易，但这条单行路是无法反过来走的。</p>
<h2 id="散列函数的应用">散列函数的应用</h2>
<p>散列函数应用具有多样性</p>
<blockquote>
<p>安全加密：</p>
</blockquote>
<ul>
<li>保护资料，散列值可用于唯一地识别机密信息。这需要散列函数是抗碰撞(collision-resistant)的，意味着很难找到产生相同散列值的资料。如数字签名、消息认证码。</li>
</ul>
<blockquote>
<p>数据校验：</p>
</blockquote>
<ul>
<li>确保传递真实的信息：消息或数据的接受者确认消息是否被篡改的性质叫数据的真实性，也称为完整性。</li>
<li>错误校正：使用一个散列函数可以很直观的检测出数据在传输时发生的错误。</li>
</ul>
<blockquote>
<p>负载均衡：</p>
</blockquote>
<ul>
<li>通过hash算法，对客户端IP进行计算hash值，将取到值与服务器数量进行取模运算。</li>
</ul>
<blockquote>
<p>分布式存储：如一致性hash。</p>
</blockquote>
<h2 id="常用单项散列函数">常用单项散列函数</h2>
<h3 id="md4-md5">MD4 MD5</h3>
<p>MD5在1996年后被证实存在弱点，可以被加以破解，对于需要高度安全性的资料，专家一般建议改用其他算法，如SHA-2。2004年，证实MD5算法无法防止碰撞攻击，因此不适用于安全性认证，如SSL公开密钥认证或是数字签名等用途。</p>
<h3 id="sha-1-sha-2">SHA-1 SHA-2</h3>
<p>SHA-1：1995年发布，SHA-1在许多安全协议中广为使用，包括TLS、GnuPG、SSH、S/MIME和IPsec，是MD5的后继者。但SHA-1的安全性在2010年以后已经不被大多数的加密场景所接受。2017年荷兰密码学研究小组CWI和Google正式宣布攻破了SHA-1。</p>
<p>SHA-2：2001年发布，包括<code>SHA-224</code>、<code>SHA-256</code>、<code>SHA-384</code>、<code>SHA-512</code>、<code>SHA-512/224</code>、<code>SHA-512/256</code>。<strong>SHA-2目前没有出现明显的弱点</strong>。虽然至今尚未出现对SHA-2有效的攻击，但它的算法跟SHA-1基本上仍然相似。 比特币使用的sha-256进行的数字签名</p>
<table>
<thead>
<tr>
<th style="text-align:center">算法和变体</th>
<th style="text-align:center">输出散列值长度 （bits）</th>
<th style="text-align:center">中继散列值长度 （bits）</th>
<th style="text-align:center">资料区块长度 （bits）</th>
<th style="text-align:center">最大输入消息长度 （bits）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">MD5</td>
<td style="text-align:center">128</td>
<td style="text-align:center">128 (4 × 32)</td>
<td style="text-align:center">512</td>
<td style="text-align:center">无限</td>
</tr>
<tr>
<td style="text-align:center">SHA-0</td>
<td style="text-align:center">160</td>
<td style="text-align:center">160 (5 × 32)</td>
<td style="text-align:center">512</td>
<td style="text-align:center">264 − 1</td>
</tr>
<tr>
<td style="text-align:center">SHA-1</td>
<td style="text-align:center">160</td>
<td style="text-align:center">160 (5 × 32)</td>
<td style="text-align:center">512</td>
<td style="text-align:center">264 − 1</td>
</tr>
<tr>
<td style="text-align:center">SHA-2</td>
<td style="text-align:center"><em>SHA-224</em> <em>SHA-256</em></td>
<td style="text-align:center">224 256</td>
<td style="text-align:center">256 (8 × 32)</td>
<td style="text-align:center">512</td>
</tr>
<tr>
<td style="text-align:center"><em>SHA-384</em> <em>SHA-512</em> <em>SHA-512/224</em> <em>SHA-512/256</em></td>
<td style="text-align:center">384 512 224  256</td>
<td style="text-align:center">512 (8 × 64)</td>
<td style="text-align:center">1024</td>
<td style="text-align:center">2128 − 1</td>
</tr>
</tbody>
</table>
<h2 id="go语言中使用散列函数">Go语言中使用散列函数</h2>
<h3 id="go语言使用md5">Go语言使用MD5</h3>
<p>方式一：</p>
<pre><code class="language-go">md5.Sum(&quot;123&quot;)
</code></pre>
<p>方式2:</p>
<pre><code class="language-go">func getMD5_2(str []byte) string {
	// 1. 创建一个使用MD5校验的Hash对象`
	myHash := md5.New()
	// 2. 通过io操作将数据写入hash对象中
	io.WriteString(myHash, &quot;hello&quot;)
	//io.WriteString(myHash, &quot;, world&quot;)
	myHash.Write([]byte(&quot;, world&quot;))
	// 3. 计算结果
	result := myHash.Sum(nil)
	fmt.Println(result)
	// 4. 将结果转换为16进制格式字符串
	res := fmt.Sprintf(&quot;%x&quot;, result)
	fmt.Println(res)
	// --- 这是另外一种格式化切片的方式
	res = hex.EncodeToString(result)
	fmt.Println(res)

	return res
}
</code></pre>
<h3 id="go语言sha-1sha-2的使用">Go语言SHA-1、SHA-2的使用</h3>
<p>方法一：</p>
<pre><code class="language-go">sha512.Sum512()
sha256.Sum256()
</code></pre>
<p>方法二与md5的使用类似</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>常用加密算法学习总结之数字签名</title>
      <link>https://www.oomkill.com/2020/11/digital-signature/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2020/11/digital-signature/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>数字签名（Digital Signature），通俗来讲是基于非对称加密算法，用秘钥对内容进行散列值签名，在对内容与签名一起发送。</p>
<p><a href="http://www.youdzone.com/signature.html" target="_blank"
   rel="noopener nofollow noreferrer" >更详细的解说</a>
<a href="http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html" target="_blank"
   rel="noopener nofollow noreferrer" >更详细的解说 - 中文</a></p>
<h2 id="数字签名的生成个验证">数字签名的生成个验证</h2>
<blockquote>
<p><strong>签名</strong></p>
<p>⑴ 对数据进行散列值运算。
⑵ 签名：使用签名者的私钥对数据的散列值进行加密。
⑶ 数字签名数据：签名与原始数据。</p>
</blockquote>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/how-do-digital-signatures-and-digital-certificates-work-together-in-ssl.png" alt="how do digital signatures and digital certificates work together in ssl" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center class="podsc">图：数字签名</center>
<center><em>Source：</em>https://cheapsslsecurity.com/blog/digital-signature-vs-digital-certificate-the-difference-explained/</center>
<blockquote>
<p><strong>验证</strong>
⑴ 接收数据：原始数据&amp;数字签名。
⑵ 使用公钥进行解密得到散列值。
⑶ 将原始数据的散列值与解密后的散列值进行对比。</p>
</blockquote>
<h2 id="go语言中使用rsa进行数字签名">Go语言中使用RSA进行数字签名</h2>
<blockquote>
<p>⑴ pem解码：使用pem对私钥进行解码, 得到pem.Block结构体
⑵ 获得私钥：使用GO x509接口<code>pem.Block</code>据解析成私钥结构体
⑶ 计算hash值：对明文进行散列值计算
⑷ 使用秘钥对散列值签名</p>
</blockquote>
<pre><code class="language-go">package main

import (
	&quot;crypto&quot;
	&quot;crypto/rand&quot;
	&quot;crypto/rsa&quot;
	&quot;crypto/sha256&quot;
	&quot;crypto/x509&quot;
	&quot;encoding/pem&quot;
	&quot;fmt&quot;
)

var (
	private = `-----BEGIN 私钥-----
MIICXQIBAAKBgQDc73afIxqYOHg80puDIMYrqUAiTi8EiTVDEiO9YE3+VxRvN0sa
pe3zx1UdhgIn3iCPUzyI2vwNADId3LjuIjkdCcdB2fHrBTbcy6u0545HnY42F9aQ
7cAr168bHcqhQoKcna9i9nukO+w7So1J9C6Wr8J4e4923q7+T7z7bZeXywIDAQAB
AoGBAItX5KLdywoyo3MJCdgcNaCX8MEyOmlL+HHC4ROxx78gQN0cLJw0Bu33zHEA
ch+e8z4yKz3Nj6bLdtBqw6A9qXLBCfWfD/p9YKDZNFP/6+u9teUirOgiBSq7kXWy
mtBm0I3pz33EomCuSJzLj/Mj/fkKs+425jPFcZboJdZpCyBhAkEA8mtGUGYuAZwV
RKBDkf1bz5EyPBGV+9CyXa6pd6md61APY0j+qhb1w9ADfHKkAzfoilhpucznRhaz
kAheqMPAMwJBAOlQEx2Ytc8TxfFqhF8RPTODe2N0jBBvsvJ85k7vNiQ+hnmaAray
XS6pCbZdvmGHYKlz3MVGeis/UJKDdSzE0gkCQQCoZijkNPcEmz6S+5m00oFywXRa
EgVUdndRaMHEpIlVK7pkyBJQab60Fc42JxUUP0RExoI7VcHbCG4YQhgvuDvNAkBQ
CUolcwebe/sBcDrsqetGyqn/WjHaSZcnnDUdiu4VzOUwveaEafeRVCeiydHPfzNn
rflkK2MphtTLDhGaRAKRAkASKlhV8aTBzTty/V3XMQfFVIAdHCyEIGMdjDDSzPly
shZCn66IyIze8j5Q4ZLcRz6GPglHdrkBnyt4QFuGurpl
-----END 私钥-----`

	public = `-----BEGIN 公钥-----
MIGJAoGBANzvdp8jGpg4eDzSm4MgxiupQCJOLwSJNUMSI71gTf5XFG83Sxql7fPH
VR2GAifeII9TPIja/A0AMh3cuO4iOR0Jx0HZ8esFNtzLq7TnjkedjjYX1pDtwCvX
rxsdyqFCgpydr2L2e6Q77DtKjUn0Lpavwnh7j3berv5PvPttl5fLAgMBAAE=
-----END 公钥-----`
)

func digitalSign(privateKey, plainText string) (signText []byte, err error) {
	var (
		pemBlock, _   = pem.Decode([]byte(privateKey))
		privateStream *rsa.PrivateKey
		plainHash     = sha256.Sum256([]byte(plainText))
	)

	if privateStream, err = x509.ParsePKCS1PrivateKey(pemBlock.Bytes); err != nil {
		return
	}
	if signText, err = rsa.SignPKCS1v15(rand.Reader, privateStream, crypto.SHA256, plainHash[:]); err != nil {
		return
	}
	return
}

func digitalVerify(publicKeyByte, plainText string, signText []byte) (ok bool, err error) {
	var (
		pemBlock, _  = pem.Decode([]byte(publicKeyByte))
		publicStream *rsa.PublicKey
		plainHash    = sha256.Sum256([]byte(plainText))
	)

	if publicStream, err = x509.ParsePKCS1PublicKey(pemBlock.Bytes); err != nil {
		return
	}

	if err = rsa.VerifyPKCS1v15(publicStream, crypto.SHA256, plainHash[:], signText); err != nil {
		return
	}
	return true, nil
}

func main() {
	text, err := digitalSign(private, &quot;张三李四王五赵柳&quot;)
	ok, err := digitalVerify(public, &quot;张三李四王五赵柳&quot;, text)
	fmt.Println(ok)
	fmt.Println(err)
}
</code></pre>
<blockquote>
<p>总结
在Go语言API中公钥私钥的注释头尾也需要加上</p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>常用加密算法学习总结之非对称加密</title>
      <link>https://www.oomkill.com/2020/11/asymmetric/</link>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2020/11/asymmetric/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>公开密钥密码学（英语：Public-key cryptography）也称非对称式密码学（英语：Asymmetric cryptography）是密码学的一种演算法。常用的非对称加密算法有 <code>RSA</code> <code>DSA</code> <code>ECC</code> 等。<a href="https://zh.wikipedia.org/zh-hans/%E5%85%AC%E5%BC%80%E5%AF%86%E9%92%A5%E5%8A%A0%E5%AF%86" target="_blank"
   rel="noopener nofollow noreferrer" >公开密钥加密</a></p>
<p>非对称加密算法使用<strong>公钥</strong>、<strong>私钥</strong>来加解密。</p>
<ul>
<li>公钥与私钥是成对出现的。</li>
<li>多个用户（终端等）使用的密钥交公钥，只有一个用户（终端等）使用的秘钥叫私钥。</li>
<li>使用公钥加密的数据只有对应的私钥可以解密；使用私钥加密的数据只有对应的公钥可以解密。</li>
</ul>
<h2 id="非对称加密通信过程">非对称加密通信过程</h2>
<p>下面我们来看一看使用公钥密码的通信流程。假设Alice要给Bob发送一条消息，Alice是发送者，Bob是接收者，而这一次窃听者Eve依然能够窃所到他们之间的通信内容。 <a href="https://zh.wikipedia.org/zh-hans/%E5%85%AC%E5%BC%80%E5%AF%86%E9%92%A5%E5%8A%A0%E5%AF%86" target="_blank"
   rel="noopener nofollow noreferrer" >参考自维基百科</a></p>
<blockquote>
<p>⑴ Alice与bob事先互不认识，也没有可靠安全的沟通渠道，但Alice现在却要透过不安全的互联网向bob发送信息。
⑵ Alice撰写好原文，原文在未加密的状态下称之为明文 <code>plainText</code>。
⑶ bob使用密码学安全伪随机数生成器产生一对密钥，其中一个作为公钥 <code>publicKey</code>，另一个作为私钥 <code>privateKey</code>。
⑷ bob可以用任何方法传送公钥<code>publicKey</code> 给Alice，即使在中间被窃听到也没问题。
⑸ Alice用公钥<code>publicKey</code>把明文<code>plainText</code>进行加密，得到密文 <code>cipherText</code>
⑹ Alice可以用任何方法传输密文给bob，即使中间被窃听到密文也没问题。
⑺ bob收到密文，用私钥对密文进行解密，得到明文 <code>plainText</code>。
由于其他人没有私钥，所以无法得知明文；如果Alice，在没有得到bob私钥的情况下，她将重新得到原文。</p>
</blockquote>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201102205804406-1240772965.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" />
<img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201102204939055-1888285763.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="rsa">RSA</h2>
<p>RSA是一种非对称加密算法，是由罗纳德·李维斯特（Ron Rivest）、阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（Leonard Adleman）在1977年一起提出，并以三人姓氏开头字母拼在一起组成的。</p>
<blockquote>
<p>RSA公钥和密钥的获取：随机选择两个大的素数，<code>p</code> <code>q</code>  $N = p*q$
RSA加密过程：$cipherText = plainText ^ E  mod  N$，$(N,e)$为公钥，$(N,d)$为私钥。
RSA解密过程：$plainText = cipherText^ D  mod     N$</p>
</blockquote>
<h2 id="go语言中rsa的应用">Go语言中RSA的应用</h2>
<h3 id="在go语言中生成公钥与私钥">在Go语言中生成公钥与私钥</h3>
<h4 id="生成秘钥流程">生成秘钥流程</h4>
<blockquote>
<p>⑴ 使用<code>crypto/rsa</code>中的<code>GenerateKey(random io.Reader, bits int)</code>方法生成私钥（结构体）
⑵ 因为X509证书采用了<a href="https://wuziqingwzq.github.io/ca/2017/12/26/x509-knowledge-asn1.html" target="_blank"
   rel="noopener nofollow noreferrer" >ASN1</a>描述结构，需要通过Go语言API将的到的私钥（结构体），转换为<code>BER</code>编码规则的字符串。
⑶ 需要将ASN1 BER 规则转回为PEM数据编码。<code>pem.Encode(out io.Writer, b *Block)</code>
⑷ 将返回的数据保存</p>
</blockquote>
<h4 id="生成私钥">生成私钥</h4>
<pre><code class="language-go">func GeneratePrivateKey(keySize int) (privateKey bytes.Buffer, err error) {
	// 生成私钥
	var (
		privateKeyStruct *rsa.PrivateKey
		privateStream    []byte
	)
	privateKeyStruct, err = rsa.GenerateKey(rand.Reader, keySize)

	if err != nil {
		return
	}

	privateStream = x509.MarshalPKCS1PrivateKey(privateKeyStruct)

	privateBlock := pem.Block{Type: &quot;私钥&quot;, Bytes: privateStream}

	if err = pem.Encode(&amp;privateKey, &amp;privateBlock); err != nil {
		return
	}
	return
}
</code></pre>
<h4 id="通过私钥获取公钥">通过私钥获取公钥</h4>
<p>通过私钥获取公钥需要将私钥生成的步骤翻转</p>
<blockquote>
<p>⑴ 私钥[]byte解码为一个pemBlock <code>pem.Decode()</code>
⑵ pemBlock.Bytes是<code>BER</code>编码规则的字符串。将其转换为结构体 <code>x509.ParsePKCS1PrivateKey()</code>
⑶ 转换为的结构体的属性<code>PublicKey</code>为公钥结构体，需将其转换为<code>BER</code>编码规则的字符串。<code>x509.MarshalPKCS1PublicKey(&amp;PublicKey)</code>
⑷ 拼接公钥pemBlock，并需要将ASN1 BER规则字符串转回为PEM数据编码。<code>pem.Encode(out io.Writer, b *Block)</code></p>
</blockquote>
<pre><code class="language-go">func GetPublicKey(privateKey []byte) (publicKey bytes.Buffer, err error) {
	pemBlock, _ := pem.Decode(privateKey)

	privateStream, err := x509.ParsePKCS1PrivateKey(pemBlock.Bytes)
	if err != nil {
		return
	}
	publicStream := x509.MarshalPKCS1PublicKey(&amp;privateStream.PublicKey)
	privateBlock := pem.Block{Type: &quot;公钥&quot;, Bytes: publicStream}

	if err = pem.Encode(&amp;publicKey, &amp;privateBlock); err != nil {
		return
	}
	return
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201102235640888-2142103656.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="使用rsa密钥进行加解密">使用RSA密钥进行加解密</h3>
<p>RSA加/解密步骤</p>
<blockquote>
<p>⑴ 因为在生成公钥与私钥时，进行了pem编码，需要先对其（一般情况下加密都使用公钥）进行解码为pemBlock。<code>pem.Decode()</code>
⑵ pemBlock.Bytes是<code>BER</code>编码规则的字符串。将其转换为结构体 <code>x509.ParsePKCS1PublicKey(pemBlock.Bytes)</code>
⑶ 使用 <code>rsa.DecryptPKCS1v15</code> 或 <code>rsa.EncryptPKCS1v15</code> 进行加解密，如：<code>rsa.DecryptPKCS1v15(rand.Reader, public|private stream, []byte plain|cipher)</code>，返回值即为加/解密好的数据。</p>
</blockquote>
<pre><code class="language-go">func RSAEncrypt(publicKey []byte, plainText string) (cipherText []byte, err error) {
	pemBlock, _ := pem.Decode(publicKey)
	publicStream, err := x509.ParsePKCS1PublicKey(pemBlock.Bytes)
	if err != nil {
		return
	}

	if cipherText, err = rsa.EncryptPKCS1v15(rand.Reader, publicStream, []byte(plainText)); err != nil {
		return
	}
	return
}

func RSADecrypt(privateKey, cipherText []byte) (plainText []byte, err error) {
	pemBlock, _ := pem.Decode(privateKey)
	privateStream, err := x509.ParsePKCS1PrivateKey(pemBlock.Bytes)
	if err != nil {
		return
	}

	if plainText, err = rsa.DecryptPKCS1v15(rand.Reader, privateStream, []byte(cipherText)); err != nil {
		return
	}
	return
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201103160438176-867986897.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p>总结</p>
<ul>
<li>Go语言接口中，明文内容的长度不能大于秘钥本身。</li>
<li>RSA算法加解密速度慢，不推荐对较大数据加密。</li>
</ul>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>常用加密算法学习总结之对称加密</title>
      <link>https://www.oomkill.com/2020/10/symmetric/</link>
      <pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2020/10/symmetric/</guid>
      <description></description>
      <content:encoded><![CDATA[<p><strong>对称加密</strong>，又称为 <strong>共享密钥加密算法</strong>，是指加密和解密方使用相同密钥的加密算法。对称加密算法的优点在于加解密的高速度和使用长密钥时的难破解性。</p>
<h2 id="对称加密算法">对称加密算法</h2>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031172028652-1698637107.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="des">DES</h3>
<p>DES（Data Encryption Standard）：数据加密标准，速度较快，适用于加密大量数据的场合。1977年被美国联邦政府的国家标准局确定为联邦资料处理标准（FIPS）</p>
<h4 id="des的加密和解密">DES的加密和解密</h4>
<p>DES是一种将64bit（8Byte）的明文加密成64bit的密文的对称密码算法，==它的密钥长度是56比特==。<font color="red">从规格上来说，DES的密钥长度是64bit，但由于每隔7bit会设置一个用于==错误检查==的比特，因此实质上其密钥长度是56bit</font>。</p>
<p><font color="red">DES是以64bit的明文（比特序列）为一个单位来进行加密的</font>，<strong>这个64bit的单位称为分组</strong>。一般来说，以分组为单位进行处理的密码算法称为<strong>分组密码（blockcipher）</strong>，DES就是分组密码的一种。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031173939221-127508220.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>DES每次只能加密64比特的数据，如果要加密的明文比较长，就需要对DES加密进行迭代（反复），而迭代的具体方式就称为模式（mode）。</p>
<h3 id="3des">3DES</h3>
<p>3DES（Triple DES）：是三重数据加密算法（TDEA，Triple Data Encryption Algorithm）块密码的通称。是基于DES，对一块数据用三个不同的密钥进行三次加密，强度更高。</p>
<p>3DES是基于计算机的运算能力的增强，基于DES算法，增强秘钥进行多绪加密，而不是一种块密码算法。</p>
<h3 id="aes">AES</h3>
<p>AES（Advanced Encryption Standard）：高级加密标准，是美国联邦政府采用的一种区块加密标准。</p>
<h3 id="分组密码模式">分组密码模式</h3>
<p>**分组密码（<code>blockcipher</code>）**是每次只能处理特定长度的一块数据的一类密码算法，这里的一块&quot;就称为分组（block）。此外，一个分组的比特数就称为分组长度（<code>blocklength</code>）。</p>
<p>例如，<strong>DES和3DES的分组长度都是64比特</strong>。这些密码算法一次只能加密64比特的明文．并生成64比特的密文。</p>
<p><strong>AES的分组长度可以从128比特、192比特和256比特中进行选择。当选择128比特的分组长度时，AES一次可加密128比特的明文，并生成128比特的密文。</strong></p>
<p><strong>分组密码算法只能加密固定长度的分组，但是我们需要加密的明文长度可能会超过分组密码的分组长度，这时就需要对分组密码算法进行迭代，以便将一段很长的明文全部加密。而迭代的方法就称为分组密码的模式（mode）</strong>。</p>
<p>分组密码的模式有很多种类，分组密码的主要模式有以下5种：</p>
<h4 id="明文与密文分组">明文与密文分组</h4>
<ul>
<li>**明文分组: **是指分组密码算法中作为加密对象的明文。明文分组的长度与分组密码算法的分组长度是相等的。</li>
<li>**密文分组: **是指使用分组密码算法将明文分组加密之后所生成的密文。</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031180750458-1657314183.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="ecb模式electronic-code-book-mode电子密码本模式">ECB模式：Electronic Code Book mode（电子密码本模式）</h4>
<p>ECB是最简单的加密模式，<font color="red">明文消息被分成固定大小的块（分组），并且每个块被单独加密。</font>  每个块的加密和解密都是独立的，且使用相同的方法进行加密，所以可以进行并行计算，但是这种方法一旦有一个块被破解，使用相同的方法可以解密所有的明文数据，<font color="red">安全性比较差。  适用于数据较少的情形，加密前需要把明文数据填充到块大小的整倍数。</font></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031180917418-1828892139.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" />
<img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031180907816-1922224853.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p>使用ECB模式加密时，相同的明文分组会被转换为相同的密文分组，因此ECB模式也称为电子密码本模式<strong>当最后一个明文分组的内容小于分组长度时（如一个分组8bit），需要用一特定的数据进行填充（padding），让值一个分组长度等于分组长度</strong>。</p>
</blockquote>
<blockquote>
<p>ECB模式是所有模式中最简单的一种。ECB模式中，明文分组与密文分组是一一对应的关系，因此，如果明文中存在多个相同的明文分组，则这些明文分组最终都将被转换为相同的密文分组。这样一来，只要观察一下密文，就可以知道明文中存在怎样的重复组合，并可以以此为线索来破译密码，因此ECB模式是存在一定风险的。</p>
</blockquote>
<h4 id="cbc模式cipher-block-chaining-mode密码分组链接密码块-模式">CBC模式：Cipher Block Chaining mode（密码分组链接/密码块 模式）</h4>
<p>1976年，IBM发明了密码分组链接CBC。CBC<font color="red">模式中每一个分组要先和前一个分组加密后的数据进行XOR异或操作，然后再进行加密</font>。 这样每个密文块依赖该块之前的所有明文块，为了保持每条消息都具有唯一性，<font color="red">在<strong>第一个块</strong>进行加密之前需要用初始化向量 <code>IV</code> 进行异或操作</font>。  <font color="blue">CBC模式是一种最常用的加密模式，它主要缺点是加密是连续的，不能并行处理，并且与ECB一样消息块必须填充到块大小的整倍数。</font></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031194830815-323142789.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031194838945-4927688.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>**当加密第一个明文分组时，由于不存在 “前一个密文分组&quot;，因此<font color="red">需要事先准备一个长度为一个分组的比特序列来代替“前一个密文分组</font>&quot;，这个比特序列称为初始化向量（initialization vector）**通常缩写为 <code>IV</code>。一般来说，每次加密时都会随机产生一个不同的比特序列来作为初始化向量。</p>
<h4 id="cfb模式cipher-feedback-mode密文反馈模式">CFB模式：Cipher FeedBack mode（密文反馈模式）</h4>
<p>密文反馈模式 CFB；在CFB模式中，<font color="red">前一个分组的密文加密后和当前分组的明文XOR异或操作生成当前分组的密文</font>。所谓反馈，这里指的就是返回输入端的意思，即前一个密文分组会被送回到密码算法的输入端。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031200320644-9430185.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" />
<img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031200333802-435217010.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>在ECB和CBC中，明文分组都是通过密码算法进行加密的，然而，在CFB模式中，明文分组和密文分组之间并没有经过&quot;加密&quot;这一步骤，明文分和密文分组之间只有一个XOR。</p>
<h4 id="ofb模式output-feedback-mode输出反馈模式">OFB模式：Output FeedBack mode（输出反馈模式）</h4>
<p>输出反馈模式, OFB。在OFB模式中，上一个分组密码算法的输出是当前分组密码算法的输入（下图）</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031201815366-1110323402.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031201835300-606590872.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h4 id="ctr模式counter-mode计数器模式"><strong>CTR模式</strong>：CounTeR mode（计数器模式）</h4>
<p><font color="red">CTR是一种通过将逐次累加的计数器进行加密来生成密钥流的流密码</font>；即每个分组对应一个逐次累加的计数器，并通过对计数器进行加密来生成密钥流。也就是说，最终的密文分组是通过将计数器加密得到的比特序列，与明文分组进行XOR而得到的。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031202045165-1475648471.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031202056336-731754020.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><strong>CTR模式的特点</strong></p>
<blockquote>
<p>CTR模式的加密和解密使用了完全相同的结构，因此在程序实现上比较容易。这一特点和同为流密码的OFB模式是一样的。
CTR模式中可以以任意顺序对分组进行加密和解密，因此在加密和解密时需要用到的“计数器&quot;的值可以由nonce和分组序号直接计算出来。这一性质是OFB模式所不具备的。
CTR模式能够以任意顺序处理分组，就意味着能够实现并行计算。在支持并行计算的系统中，CTR模式的速度是非常快的。</p>
</blockquote>
<p>总结</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201031202458031-1831855010.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p>初始化向量 - IV</p>
<ul>
<li>ecb, ctr模式不需要初始化向量</li>
<li>cbc, ofc, cfb需要初始化向量</li>
</ul>
</blockquote>
<blockquote>
<p>最后一个明文分组的填充</p>
<ul>
<li>使用cbc, ecb需要填充
<ul>
<li>明文分组中进行了填充, 然后加密</li>
<li>解密密文得到明文, 需要把填充的字节删除</li>
</ul>
</li>
<li>使用 ofb, cfb, ctr不需要填充</li>
</ul>
</blockquote>
<h2 id="对称加密在go语言中的实现方式">对称加密在Go语言中的实现方式</h2>
<h3 id="cbc分组模式">CBC分组模式</h3>
<pre><code class="language-go">/*
 *  @brief DES加密函数，
 *  @param1 加密的明文
 *  @param2 秘钥
 *  @return，得到的密文
 */

func DesEncrypt(plainText, key string) ([]byte, error) {

	var (
		// 创建一个des加密的接口
		block, err = des.NewCipher([]byte(key))
		// 分组加密 需要对最后进行填充
		padText    = LastPadding([]byte(plainText), block.BlockSize())
		cipherText = make([]byte, len(padText))
	)

	if err != nil {
		return nil, err
	}
	// 创建使用cbc分组模式加密接口
	mode := cipher.NewCBCEncrypter(block, []byte(&quot;12345678&quot;))
	// 加密
	mode.CryptBlocks(cipherText, padText)
	return cipherText, nil
}

/*
 *  @brief DES解密函数，
 *  @param1 加密的明文
 *  @param2 秘钥
 *  @return，得到的密文
 */

func DesDecrypt(cipherText, key string) ([]byte, error) {

	var (
		// 创建一个des加密的接口
		block, err = des.NewCipher([]byte(key))
		// 创建使用cbc分组模式解密接口
		mode           = cipher.NewCBCDecrypter(block, []byte(&quot;12345678&quot;))
		byteCipherText = []byte(cipherText)
		// 明文存储变量
		plainText = make([]byte, len(byteCipherText))
	)

	if err != nil {
		return nil, err
	}
	// 解密，无返回值
	mode.CryptBlocks(plainText, byteCipherText)
	// 将填充的内容删除
	return LastUnPadding(plainText, des.BlockSize), nil
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201101190422825-4035893.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p>总结</p>
<ul>
<li>DES使用64bit钥对数据块进行加密</li>
<li>在Go语言中<code>iv</code>的长须需要与密钥对长度一致。</li>
<li>CBC使用的流密码算法</li>
<li>CBC需要对最后明文分组填充</li>
</ul>
</blockquote>
<h3 id="ofb分组模式">OFB分组模式</h3>
<pre><code class="language-go">func OFBEncrypt(plainText, key string) ([]byte, error) {
	var (
		// 创建一个des加密的接口

		block, err = des.NewCipher([]byte(key))
		// 分组加密 需要对最后进行填充

		cipherText = make([]byte, len(plainText))
	)

	if err != nil {
		return nil, err
	}
	// 创建使用cbc分组模式加密接口
	mode := cipher.NewOFB(block, []byte(&quot;12345678&quot;))
	//mode := cipher.NewCBCEncrypter(block, []byte(&quot;12345678&quot;))
	// 加密
	mode.XORKeyStream(cipherText, []byte(plainText))
	return cipherText, nil
}

func OFBDecrypt(cipherText, key string) ([]byte, error) {

	var (
		// 创建一个des加密的接口
		block, err = des.NewCipher([]byte(key))
		// 创建使用cbc分组模式解密接口
		mode           = cipher.NewOFB(block, []byte(&quot;12345678&quot;))
		byteCipherText = []byte(cipherText)
		// 明文存储变量
		plainText = make([]byte, len(byteCipherText))
	)

	if err != nil {
		return nil, err
	}
	// 解密，无返回值
	mode.XORKeyStream(plainText, byteCipherText)
	// 将填充的内容删除
	return LastUnPadding(plainText, des.BlockSize), nil
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201101214408337-854797848.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="填充方式">填充方式</h3>
<pre><code class="language-go">/*
 *  填充函数，如果最后一个分组字节数不够则填充，填充的字节数为缺少的字节数
 *  如果最后一个字节数正好的话，则新建一个分组
 */
func LastPadding(plainText []byte, blockSize int) []byte {

	var (
		// 获得明文的长度，以判断时候需要补充
		paddingLength = blockSize - len(plainText)%blockSize
		// 初始化填充的内容
		padText = bytes.Repeat([]byte{byte(paddingLength)}, paddingLength)
	)
	//将填充的内容追加到明文后
	return append(plainText, padText...)
}

/*
 *  删除填充函数，如果最后一个分组字节数不够则填充，填充的字节数为缺少的字节数
 *  如果最后一个字节数正好的话，则新建一个分组
 */
func LastUnPadding(plainText []byte, blockSize int) []byte {

	var (
		// 获得明文的长度，以判断时候需要补充
		paddingLength = len(plainText)
		// 获得尾部填充的字节数量
		lastChar = int(plainText[paddingLength-1])
	)
	return bytes.TrimFunc(plainText, func(r rune) bool {
		return r == rune(lastChar)
	})

}
</code></pre>
<blockquote>
<p>总结</p>
<ul>
<li>ofb不需要最后为明文分组填充</li>
<li>加密算法Go语言API已经提供，但算法的分组业务流程需要自己实现</li>
</ul>
</blockquote>
<h3 id="aes-1">AES</h3>
<pre><code class="language-go">func AESEncrypt(cipherText, key string) ([]byte, error) {

	var (
		// 创建一个AES加密的接口
		block, err     = aes.NewCipher([]byte(key))
		byteCipherText = []byte(cipherText)
		// 明文存储变量
		plainText = make([]byte, len(byteCipherText))
	)

	if err != nil {
		return nil, err
	}

	// 创建使用cbc分组模式解密接口
	mode := cipher.NewOFB(block, []byte(&quot;1234567812345678&quot;))

	// 解密，无返回值
	mode.XORKeyStream(plainText, byteCipherText)
	// 将填充的内容删除
	return LastUnPadding(plainText, aes.BlockSize), nil
}
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20201102002205863-975724083.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p><strong>总结</strong></p>
<ul>
<li>AES秘钥为 16,24,32 Byte 即 128,196,256 bit</li>
<li>在无需明文填充的分组模式下，<code>ofb</code> <code>cfb</code>  <code>ctr</code>，加密解密的业务逻辑处理是一样的。</li>
</ul>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>一致性hash在memcache中的应用</title>
      <link>https://www.oomkill.com/2016/09/consistent-hash/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2016/09/consistent-hash/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="memcache应用场景">Memcache应用场景</h2>
<h3 id="基本场景">基本场景</h3>
<p>比如有 N 台 cache 服务器（后面简称 cache），那么如何将一个对象 object 映射到 N 个 cache 上呢，你很可能会采用类似下面的通用方法计算 object 的 hash 值，然后均匀的映射到到N个cache; <font color="#f8070d" size=3><code>hash(object)%N</code></font></p>
<p>如下图：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235352996.png" alt="image-20221024235352996" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p><strong>这时，一切都运行正常，再考虑如下的两种情况</strong>：</p>
</blockquote>
<p>一个 cache服务器m down掉了（在实际应用中必须要考虑这种情况），这样所有映射到cache m的对象都会失效，怎么办，需要把cache m从cache 中移除，这时候 cache 是 $N-1$ 台，映射公式变成了 <font color="#f8070d" size=3><code>hash(object)%(N-1)</code></font> 。此时数据 $3%3-1=3%2=1$ 此时，3应该在S3上，但是由于S3down机导致到S1去取，这时会未命中。如下图</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235406831.png" alt="image-20221024235406831" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>由于访问加重，需要添加 cache ，这时候 cache 是 $N+1$ 台，映射公式变成了 <font color="#f8070d" size=3><code>hash(object)%(N+1)</code></font> 。1和2意味着突然之间几乎所有的 cache 都失效了。对于服务器而言，这是一场灾难，洪水般的访问都会直接冲向后台服务器。$\frac{N-1} { N\times (N-1)}$</p>
<p><strong>即</strong>：</p>
<p>有N台服务器，变为 $N-1$ 台，即每 $N \times (N-1)$个数中，求余相同的只有 <font color="#f8070d" size=3><code>N-1</code></font> 个。命中率为：$\frac{1}{3}$</p>
<p>再来考虑第三个问题，由于硬件能力越来越强，你可能想让后面添加的节点多做点活，显然上面的 hash 算法也做不到。</p>
<p>有什么方法可以改变这个状况呢，这就是 <font color="#f8070d" size=3>consistent hashing</font>&hellip;</p>
<p>但现在一致性hash算法在分布式系统中也得到了广泛应用，研究过memcached缓存数据库的人都知道，memcached服务器端本身不提供分布式cache的一致性，而是由客户端来提供，具体在计算一致性hash时采用如下步骤：</p>
<ol>
<li>
<p>首先求出memcached服务器（节点）的哈希值，并将其配置到 0～2<sup>32 </sup>的圆（continuum）上。</p>
</li>
<li>
<p>然后采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上。</p>
</li>
<li>
<p>然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过2<sup>32</sup>仍然找不到服务器，就会保存到第一台memcached服务器上。</p>
</li>
</ol>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235424248.png" alt="image-20221024235424248" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>从上图的状态中添加一台memcached服务器。余数分布式算法由于保存键的服务器会发生巨大变化而影响缓存的命中率，但Consistent Hashing中，只有在圆（continuum）上增加服务器的地点逆时针方向的第一台服务器上的键会受到影响，如下图所示：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235424248.png" alt="image-20221024235424248" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。</p>
<h2 id="consistent-hash原理">consistent hash原理</h2>
<h3 id="基本概念">基本概念</h3>
<p>一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为 <font color="#f8070d" size=3>0-2<sup>32</sup>-1</font>（即哈希值是一个32位无符号整形），整个哈希空间环如下：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235550927.png" alt="image-20221024235550927" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>整个空间按顺时针方向组织。0和2<sup>32</sup>-1在零点中方向重合。</p>
<p>下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用ip地址哈希后在环空间的位置如下：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235605237.png" alt="image-20221024235605237" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。</p>
<p>例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235630275.png" alt="image-20221024235630275" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。</p>
<p>下面分析一致性哈希算法的容错性和可扩展性。现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。如下图所示：</p>
<p>下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235642951.png" alt="image-20221024235642951" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。</p>
<p>综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。</p>
<p>另外，一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235702564.png" alt="image-20221024235702564" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性哈希算法引入了<font style="background:#ffc104;" size=2>虚拟节点机制</font>，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 <font color="#f8070d" size=3><code>Node A#1</code></font>  <font color="#f8070d" size=3><code>Node A#2</code></font>  <font color="#f8070d" size=3><code>Node A#3</code></font>  <font color="#f8070d" size=3><code>Node B#1</code></font>  <font color="#f8070d" size=3><code>Node B#2</code></font>  <font color="#f8070d" size=3><code>Node B#3</code></font> 的哈希值，于是形成六个虚拟节点：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235718095.png" alt="image-20221024235718095" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到<font color="#f8070d" size=3><code>Node A#1</code></font>  <font color="#f8070d" size=3><code>Node A#2</code></font>  <font color="#f8070d" size=3><code>Node A#3</code></font> 三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为**==32==**甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。</p>
<p>参考: <a href="http://www.cnblogs.com/haippy/archive/2011/12/10/2282943.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.cnblogs.com/haippy/archive/2011/12/10/2282943.html</a></p>
<h2 id="一致性hashconsistent-hash在php中使用">一致性hash（consistent hash）在PHP中使用</h2>
<pre><code class="language-php">&lt;?php 
	class ConsistentHash
	{
		public $nodes = array();
		
		public function __construct(){
			
		}
  
		public function generateHash($str){
			return sprintf('%u',crc32($str));
		}
		
		public function findNode(){
			
		}
  
		public function lookup($key){
			$tmp = $this-&gt;generateHash($key);
			$node = current($this-&gt;nodes);
			foreach($this-&gt;nodes as $key=&gt;$val){
				if( $tmp &lt;= $key ){
					$node = $val;
					break;
				}
			}
			return $node;
		}
		
		public function getNode(){
			var_dump($this-&gt;nodes);
		}
		
		public function addNode($node){
			$this-&gt;nodes[$this-&gt;generateHash($node)] = $node;
			ksort($this-&gt;nodes);
		}	
	}
	
	$hash = new ConsistentHash;
	$hash-&gt;addNode('192.168.2.80:11211');
	$hash-&gt;addNode('192.168.2.80:11212');
	$hash-&gt;addNode('192.168.2.80:11213');
	echo '&lt;hr&gt;&lt;br&gt;';
	$hash-&gt;getNode();
	echo '&lt;br&gt;';
	echo $hash-&gt;generateHash('zhangsan'),'&lt;br&gt;';
	echo $hash-&gt;lookup('zhangsan'),'&lt;br&gt;';
?&gt;
</code></pre>
<p>执行结果</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235759078.png" alt="image-20221024235759078" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235812368.png" alt="image-20221024235812368" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>这时可以看出，数据倾斜问题。</p>
<p>创建虚拟节点，解决数据倾斜问题</p>
<pre><code class="language-php">&lt;?php 
	class ConsistentHash{
		public $nodes = array();
		protected $num = 0;
		protected $priNode = array();
		
		public function __construct($nodeNum){
			$this-&gt;num = $nodeNum;
		}
		
    public function generateHash($str){
			return sprintf('%u',crc32($str));
		}
		
		public function selectNode($key){
			$tmp = $this-&gt;generateHash($key);
			$node = current($this-&gt;nodes);  # 选择最小的节点作为默认值
			foreach($this-&gt;priNode as $key=&gt;$val){
				if( $tmp &lt;= $key ){
					$node = $val;
					break;
				}
			}
			return $node;
		}
		
		public function getNode(){
			var_dump($this-&gt;nodes);
			var_dump($this-&gt;priNode);
		}
		
		public function addNode($node){	
			for($n=0;$n&lt;$this-&gt;num;$n++){
				$this-&gt;priNode[$this-&gt;generateHash($node.'_'.$n)] = $node;
			}
			$this-&gt;nodes[$this-&gt;generateHash($node)] = $node;
			ksort($this-&gt;priNode);
		}	
	}
	
	$hash = new ConsistentHash(32);
	$hash-&gt;addNode('192.168.2.80:11211');
	$hash-&gt;addNode('192.168.2.80:11212');
	$hash-&gt;addNode('192.168.2.80:11213');
	$hash-&gt;getNode();
	echo '&lt;br&gt;';
	echo $hash-&gt;generateHash('zhangsan'),'&lt;br&gt;';
	echo $hash-&gt;generateHash('lisi'),'&lt;br&gt;';
	echo $hash-&gt;lookup('zhangsan'),'&lt;br&gt;';
	echo $hash-&gt;lookup('lisi'),'&lt;br&gt;';
	echo $hash-&gt;generateHash('wangwu'),'&lt;br&gt;';
	echo $hash-&gt;lookup('wangwu'),'&lt;br&gt;';
?&gt;
</code></pre>
<p>此时自动分配的节点为</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235829164.png" alt="image-20221024235829164" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>可看出6E-8E存在11211上，大于。35.7E-35.9E存在11212上</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235847363.png" alt="image-20221024235847363" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235855512.png" alt="image-20221024235855512" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="一致性hash与取模命中率的对比实验">一致性hash与取模命中率的对比实验</h2>
<p><a href="../../../images%5cdring.rar">dring.rar</a></p>
<h3 id="实验目的">实验目的</h3>
<p>测试Memcached缓存服务器有N台变为N-台时，取模和consistent hasing算法的命中率</p>
<h3 id="实验原理">实验原理</h3>
<p>相同的硬件环境、操作系统、数据缓存环境，5个memcached节点，用两种分布式算法建立缓存，缓存命中率稳定后，减少1个节点，观察命中率的变化，知道命中率在次稳定。</p>
<h3 id="前端软件架构">前端软件架构</h3>
<pre><code class="language-sh">config.php 		#←配置memcached节点信息
hash.php 	 	#←分布式算法
init.php 		#←初始化数据
exec.php 		#←减少节点后请求数据
stat.php		#←统计平均命中率
index.html 		#←生成动态图表
</code></pre>
<h3 id="取模算法的实验">取模算法的实验</h3>
<p>当5台缓存服务器全部正常的情况下，此时的命中率统计图如下：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235917083.png" alt="image-20221024235917083" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<blockquote>
<p><strong>这是查看5台缓存服务器的查询与命中次数如下</strong>：</p>
</blockquote>
<pre><code class="language-sh">$ for n in {1..5};do printf &quot;stats\r\n&quot;|nc 127.0.0.1 1121$n|egrep 'get_hits|cmd_get'; done
STAT cmd_get 0
STAT get_hits 0
STAT cmd_get 0
STAT get_hits 0
STAT cmd_get 0
STAT get_hits 0
STAT cmd_get 0
STAT get_hits 0
STAT cmd_get 0
STAT get_hits 0
$ for n in {1..5};do printf &quot;stats\r\n&quot;|nc 127.0.0.1 1121$n|grep item; done
STAT curr_items 2044
STAT total_items 2044
STAT curr_items 1983
STAT total_items 1983
STAT curr_items 1993
STAT total_items 1993
STAT curr_items 2001
STAT total_items 2001
STAT curr_items 1979
STAT total_items 1979
</code></pre>
<p>这时断掉一台缓存服务器，此时的命中率从100%瞬间降至8%。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235932558.png" alt="image-20221024235932558" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>运行一段时间后，可见命中率保持20%左右，在预热完毕后，逐步上升。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221024235950143.png" alt="image-20221024235950143" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>此时查看5台缓存服务器的查询次数与命中次数，发现已经很均匀了。</p>
<pre><code class="language-sh">$ for n in {1..5};do printf &quot;stats\r\n&quot;|nc 127.0.0.1 1121$n|egrep 'get_hits|cmd_get'; done
STAT cmd_get 0
STAT get_hits 0
STAT cmd_get 8481
STAT get_hits 6465
STAT cmd_get 8482
STAT get_hits 6476
STAT cmd_get 8482
STAT get_hits 6504
STAT cmd_get 8483
STAT get_hits 6478
</code></pre>
<p>经过较长时间后，可以看到命中率已经很平稳了</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025000013472.png" alt="image-20221025000013472" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h3 id="一致性hash算法命中率实验">一致性hash算法命中率实验</h3>
<p>模拟出正常情况下，5台缓存服务器的命中率</p>
<pre><code class="language-sh">$ for n in {1..5};do printf &quot;stats\r\n&quot;|nc 127.0.0.1 1121$n|egrep 'get_hits|cmd_get'; done
STAT cmd_get 0
STAT get_hits 0
STAT cmd_get 0
STAT get_hits 0
STAT cmd_get 0
STAT get_hits 0
STAT cmd_get 0
STAT get_hits 0
STAT cmd_get 0
STAT get_hits 0

$ for n in {1..5};do printf &quot;stats\r\n&quot;|nc 127.0.0.1 1121$n|grep item; done
STAT curr_items 999
STAT total_items 999
STAT curr_items 1005
STAT total_items 1005
STAT curr_items 6005
STAT total_items 6005
STAT curr_items 998
STAT total_items 998
STAT curr_items 993
STAT total_items 993
</code></pre>
<p>此时断开1台服务器，可以见到命中率下降到73%就稳定了。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025000031883.png" alt="image-20221025000031883" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>观察一段时间后命中率逐步上升到95%</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025000050675.png" alt="image-20221025000050675" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="在实战中会存在的问题">在实战中会存在的问题</h2>
<p>缓存雪崩的现象</p>
<p>一般是由于某个节点生效，导致其他节点的缓存命中率下降，缓存中缺失的数据去数据可查询。短时间内造成数据库服务器崩溃。或，由于缓存周期性的输小，如：6小时失效一次，那么每6小时，将有一个请求““峰值”，严重情况下会导致数据库宕机。</p>
<p>建议解决方案：</p>
<ul>
<li>将缓存的生命周期设置为随机的时间短（如4-10）小时，这样缓存不同时失效，把工作分担到各个时间点上。</li>
<li>可在夜间缓慢建立一部分缓存</li>
<li>可建立多个缓存交叉使用，做好镜像，将多个缓存失效时间错开。</li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
