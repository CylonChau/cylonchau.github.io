<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>记录一次ceph集群故障处理记录 | Cylon's Collection</title><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NP3JNCPR" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><meta name=keywords content="cephfs"><meta name=description content="处理记录 Ceph版本：octopus
首先遇到問題是，业务端无法挂在 cephfs 查看内核日志发现是 bad authorize reply ，以为是 ceph keyring被替换了
text 1 2 3 4 5 6 7 8 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10."><meta name=author content="cylon"><link rel=canonical href=https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://www.oomkill.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.oomkill.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.oomkill.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.oomkill.com/favicon.ico><link rel=mask-icon href=https://www.oomkill.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/><noscript><style>#theme-toggle,#top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=/assets/css/pe.min.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/pe.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/v4-shims.min.css><script id=MathJax-script async src=https://cdn.staticfile.net/mathjax/3.2.2/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["\\$","\\$"]]}}</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-NP3JNCPR")</script><meta property="og:title" content="记录一次ceph集群故障处理记录"><meta property="og:description" content="处理记录 Ceph版本：octopus
首先遇到問題是，业务端无法挂在 cephfs 查看内核日志发现是 bad authorize reply ，以为是 ceph keyring被替换了
text 1 2 3 4 5 6 7 8 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10."><meta property="og:type" content="article"><meta property="og:url" content="https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-13T23:10:36+08:00"><meta property="og:site_name" content="Cylon's Collection"><meta name=twitter:card content="summary"><meta name=twitter:title content="记录一次ceph集群故障处理记录"><meta name=twitter:description content="处理记录 Ceph版本：octopus
首先遇到問題是，业务端无法挂在 cephfs 查看内核日志发现是 bad authorize reply ，以为是 ceph keyring被替换了
text 1 2 3 4 5 6 7 8 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.oomkill.com/posts/"},{"@type":"ListItem","position":2,"name":"记录一次ceph集群故障处理记录","item":"https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"记录一次ceph集群故障处理记录","name":"记录一次ceph集群故障处理记录","description":"处理记录 Ceph版本：octopus\n首先遇到問題是，业务端无法挂在 cephfs 查看内核日志发现是 bad authorize reply ，以为是 ceph keyring被替换了\ntext 1 2 3 4 5 6 7 8 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.","keywords":["cephfs"],"articleBody":"处理记录 Ceph版本：octopus\n首先遇到問題是，业务端无法挂在 cephfs 查看内核日志发现是 bad authorize reply ，以为是 ceph keyring被替换了\ntext 1 2 3 4 5 6 7 8 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply 在排查完 keyring 后，手动尝试挂载 cephfs 提示 Input/output error ，此时看出是集群问题了\nbash 1 2 $ mount -t ceph 10.80.20.100:6789:/tmp /tmp/ceph -o secret=AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ==,name=cephuser mount error 5 = Input/output error 因为一开始看到日志是 bad authorize reply 以为是认证错误，重新登录了一下发现是相同的提示，这时查看 ceph status 发现集群异常，除了下面报错外，还有一个 osd down 的状态。\ntext 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 $ ceph health detail HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data redundancy: 39560/118680 objects degraded (33.333%), 201 pgs degraded, 255 pgs undersized; 4 daemons have recently crashed [WRN] CEPHADM_HOST_CHECK_FAIL: 1 hosts fail cephadm check host localhost failed check: ['podman|docker (/bin/docker) is present', 'systemctl is present', 'lvcreate is present', \"No time sync service is running; checked for ['chrony.service', 'chronyd.service', 'systemd-timesyncd.service', 'ntpd.service', 'ntp.service']\", 'ERROR: No time synchronizetion is active'] [WRN] MDS_SLOW_METADATA_IO: 1 MDSs report slow metadata IOs mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked \u003e 30 secs, oldest blocked for 4830 secs [WRN] MDS_SLOW_REQUEST: 1 MDSs report slow requests mds.cephfs.localhost.mhlzaj(mds.0): 14 slow metadata IOs are blocked \u003e 30 secs [WRN] MON_CLOCK_SKEW: clock skew detected on mon.localhost, mon.localhost1 mon.localhost clock skew 29357.8s \u003e max 0.05s (latency 0.0132089s) mon.localhost1 clock skew 29357.8s \u003e max 0.05s (latency 0.0117421s) [WRN] PG_DEGRADED: Degraded data redundancy: 39501/118680 objects degraded (33.333%), 189 pgs degraded, 241 pgs undersized pg 1.0 is stuck undersized for 22m, current state active+undersized+degraded, last acting [1] pg 2.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.8 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 2.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 2.1b is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.9 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0] pg 3.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] pg 3.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [1] [WRN] RECENT_CRASH: 4 daemons have recently crashed osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz CEPHADM_HOST_CHECK_FAIL：一台或多台主机未通过基本 cephadm 主机检查，该检查验证 (1) 主机可访问并且可以在其中执行 cephadm，以及 (2) 主机满足基本先决条件，例如工作容器运行时（podman 或 docker）和工作时间同步。如果此测试失败，cephadm 将无法管理该主机上的服务。\nMDS_SLOW_METADATA_IO\nMDS_SLOW_REQUEST：N条慢请求被阻塞\nMON_CLOCK_SKEW：运行 ceph-mon 的主机上的时钟未很好同步。如果集群检测到时钟偏差大于 mon_clock_drift_allowed，则会引发此运行状况检查。\nPG_DEGRADED：一个或多个PG的健康状态受到了损害。一种常见的情况是，某个OSD发生故障或离线，导致PG进入降级状态。在这种情况下，数据副本的可用性会受到影响，并且Ceph集群的性能也可能下降。\n首先重启 chronyd 修复了时间同步的问题，因为机房内机器经常出现 chronyd 的服务导致异常，其次重启 osd 服务，让 ceph 做再平衡完成后剩下下面报错。\n并且现象有两个：\ncephfs no such file or director ceph orch 命令还是没有反应 text 1 2 3 4 5 6 7 8 9 10 $ ceph health detail HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data [WRN] FS_DEGRADED: 1 filesystem is degraded fs cephfs is degraded [WRN] MDS_SLOW_METADATA_IO: 1 MDSs slow metadata IOs mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked \u003e 30 secs, oldest blocked for 930 secs [WRN] RECENT_CRASH: 4 daemons have recently crashed osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz [WRN] SLOW_OPS: 2 slow ops, oldset one blocked for 474 sec, mon.localhost has slow ops 通过 search 了一下，查询到 orch 是 MGR 模块\nThe orchestrator is a MGR module, have you checked if the containers are up and running [1]\n此时操作登录对应ceph node，docker restart ceph-mgr的模块，并且重启 mds 模块\nbash 1 2 3 4 $ ceph health detail [WRN] RECENT_CRASH: 4 daemons have recently crashed osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz 此时集群恢复正常，cephfs 恢复\n总结 由于长期没有在处理 ceph 方向问题，对排查有以下生疏：\n无法挂载时没有及时查看 ceph 集群信息，而是盯着客户端方向日志查询了半天。 对 ceph 故障代码没有了解过，如果有了解，可以很明确的定位问题，而不用耽误2小时。 Reference [1] ceph orch status hangs forever\n[2] HEALTH CHECKS\n[3] CEPHFS HEALTH MESSAGES\n","wordCount":"1268","inLanguage":"zh","datePublished":"2024-02-13T00:00:00Z","dateModified":"2024-02-13T23:10:36+08:00","author":{"@type":"Person","name":"cylon"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/"},"publisher":{"@type":"Organization","name":"Cylon's Collection","logo":{"@type":"ImageObject","url":"https://www.oomkill.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.oomkill.com/><img src=https://www.oomkill.com/favicon.ico alt aria-label=logo height=20>Cylon's Collection</a><div class=logo-switches><button id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.oomkill.com/archives><span>归档</span></a></li><li><a href=https://www.oomkill.com/tags><span>标签</span></a></li><li><a href=https://www.oomkill.com/search><span>搜索</span></a></li><li><a href=https://www.oomkill.com/about accesskey=/><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">记录一次ceph集群故障处理记录</h1><div class=post-meta><span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2024-02-13</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg><span>1268 字</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><span>6 分钟</span></span>
<span class=pe-post-meta-item>&nbsp;·&nbsp;<svg t="1714036239378" fill="currentcolor" class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6659" width="256" height="256"><path d="M690 78.2c-18.6-18.8-49-19-67.8-.4s-19 49-.4 67.8l255.4 258.6c67.8 68.6 67.8 178.8.0 247.4L653.4 878.2c-18.6 18.8-18.4 49.2.4 67.8s49.2 18.4 67.8-.4l224-226.4c104.8-106 104.8-276.4.0-382.4L690 78.2zM485.4 101.4c-24-24-56.6-37.4-90.6-37.4H96C43 64 0 107 0 160v299c0 34 13.4 66.6 37.4 90.6l336 336c50 50 131 50 181 0l267-267c50-50 50-131 0-181l-336-336zM96 160h299c8.4.0 16.6 3.4 22.6 9.4l336 336c12.4 12.4 12.4 32.8.0 45.2l-267 267c-12.4 12.4-32.8 12.4-45.2.0l-336-336c-6-6-9.4-14.2-9.4-22.6V160zm192 128a64 64 0 10-128 0 64 64 0 10128 0z" p-id="6660"/></svg></span><ul class=pe-post-meta-item><a href=https://www.oomkill.com/tags/storage/>#Storage</a>
<a href=https://www.oomkill.com/tags/troubleshooting/>#Troubleshooting</a></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e5%a4%84%e7%90%86%e8%ae%b0%e5%bd%95 aria-label=处理记录>处理记录</a><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a><li><a href=#reference aria-label=Reference>Reference</a></li></div></details></div></aside><script src=/js/pe-toc.min.445eb1bfc5e85dd13b9519fcc2a806522e9629b6224a2974052789ba00ab78af.js integrity="sha256-RF6xv8XoXdE7lRn8wqgGUi6WKbYiSil0BSeJugCreK8="></script><div class=post-content><h2 id=处理记录>处理记录<a hidden class=anchor aria-hidden=true href=#处理记录>#</a></h2><p>Ceph版本：octopus</p><p>首先遇到問題是，业务端无法挂在 cephfs 查看内核日志发现是 <code>bad authorize reply</code> ，以为是 ceph keyring被替换了</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>text</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class=line><span class=cl>2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class=line><span class=cl>2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class=line><span class=cl>2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class=line><span class=cl>2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class=line><span class=cl>2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class=line><span class=cl>2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class=line><span class=cl>2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply</span></span></code></pre></td></tr></table></div></div></div></div><p>在排查完 keyring 后，手动尝试挂载 cephfs 提示 <code>Input/output error</code> ，此时看出是集群问题了</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ mount -t ceph 10.80.20.100:6789:/tmp /tmp/ceph -o <span class=nv>secret</span><span class=o>=</span><span class=nv>AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ</span><span class=o>==</span>,name<span class=o>=</span>cephuser
</span></span><span class=line><span class=cl>mount error <span class=nv>5</span> <span class=o>=</span> Input/output error</span></span></code></pre></td></tr></table></div></div></div></div><p>因为一开始看到日志是 <em>bad authorize reply</em> 以为是认证错误，重新登录了一下发现是相同的提示，这时查看 ceph status 发现集群异常，除了下面报错外，还有一个 osd down 的状态。</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>text</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>$ ceph health detail
</span></span><span class=line><span class=cl>HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data redundancy: 39560/118680 objects degraded (33.333%), 201 pgs degraded, 255 pgs undersized; 4 daemons have recently crashed
</span></span><span class=line><span class=cl>[WRN] CEPHADM_HOST_CHECK_FAIL: 1 hosts fail cephadm check
</span></span><span class=line><span class=cl>    host localhost failed check: [&#39;podman|docker (/bin/docker) is present&#39;, &#39;systemctl is present&#39;, &#39;lvcreate is present&#39;, &#34;No time sync service is running; checked for [&#39;chrony.service&#39;, &#39;chronyd.service&#39;, &#39;systemd-timesyncd.service&#39;, &#39;ntpd.service&#39;, &#39;ntp.service&#39;]&#34;, &#39;ERROR: No time synchronizetion is active&#39;]
</span></span><span class=line><span class=cl>[WRN] MDS_SLOW_METADATA_IO: 1 MDSs report slow metadata IOs
</span></span><span class=line><span class=cl>    mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked &gt; 30 secs, oldest blocked for 4830 secs
</span></span><span class=line><span class=cl>[WRN] MDS_SLOW_REQUEST: 1 MDSs report slow requests
</span></span><span class=line><span class=cl>    mds.cephfs.localhost.mhlzaj(mds.0): 14 slow metadata IOs are blocked &gt; 30 secs
</span></span><span class=line><span class=cl>[WRN] MON_CLOCK_SKEW: clock skew detected on mon.localhost, mon.localhost1
</span></span><span class=line><span class=cl>    mon.localhost clock skew 29357.8s &gt; max 0.05s (latency 0.0132089s)
</span></span><span class=line><span class=cl>    mon.localhost1 clock skew 29357.8s &gt; max 0.05s (latency 0.0117421s)
</span></span><span class=line><span class=cl>[WRN] PG_DEGRADED: Degraded data redundancy: 39501/118680 objects degraded (33.333%), 189 pgs degraded, 241 pgs undersized
</span></span><span class=line><span class=cl>    pg 1.0 is stuck undersized for 22m, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.8 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 2.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 2.1b is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.9 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class=line><span class=cl>    pg 3.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>    pg 3.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class=line><span class=cl>[WRN] RECENT_CRASH: 4 daemons have recently crashed
</span></span><span class=line><span class=cl>    osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span><span class=line><span class=cl>    client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz</span></span></code></pre></td></tr></table></div></div></div></div><ul><li><p>CEPHADM_HOST_CHECK_FAIL：一台或多台主机未通过基本 cephadm 主机检查，该检查验证 (1) 主机可访问并且可以在其中执行 cephadm，以及 (2) 主机满足基本先决条件，例如工作容器运行时（podman 或 docker）和工作时间同步。如果此测试失败，cephadm 将无法管理该主机上的服务。</p></li><li><p>MDS_SLOW_METADATA_IO</p></li><li><p>MDS_SLOW_REQUEST：N条慢请求被阻塞</p></li><li><p>MON_CLOCK_SKEW：运行 ceph-mon 的主机上的时钟未很好同步。如果集群检测到时钟偏差大于 mon_clock_drift_allowed，则会引发此运行状况检查。</p></li><li><p>PG_DEGRADED：一个或多个PG的健康状态受到了损害。一种常见的情况是，某个OSD发生故障或离线，导致PG进入降级状态。在这种情况下，数据副本的可用性会受到影响，并且Ceph集群的性能也可能下降。</p></li></ul><p>首先重启 chronyd 修复了时间同步的问题，因为机房内机器经常出现 chronyd 的服务导致异常，其次重启 osd 服务，让 ceph 做再平衡完成后剩下下面报错。</p><p>并且现象有两个：</p><ul><li>cephfs no such file or director</li><li>ceph orch 命令还是没有反应</li></ul><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>text</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>$ ceph health detail
</span></span><span class=line><span class=cl>HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data 
</span></span><span class=line><span class=cl>[WRN] FS_DEGRADED: 1 filesystem is degraded
</span></span><span class=line><span class=cl>    fs cephfs is degraded
</span></span><span class=line><span class=cl>[WRN] MDS_SLOW_METADATA_IO: 1 MDSs slow metadata IOs
</span></span><span class=line><span class=cl>     mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked &gt; 30 secs, oldest blocked for 930 secs
</span></span><span class=line><span class=cl>[WRN] RECENT_CRASH: 4 daemons have recently crashed
</span></span><span class=line><span class=cl>    osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span><span class=line><span class=cl>    client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span><span class=line><span class=cl>[WRN] SLOW_OPS: 2 slow ops, oldset one blocked for 474 sec, mon.localhost has slow ops</span></span></code></pre></td></tr></table></div></div></div></div><p>通过 search 了一下，查询到 orch 是 MGR 模块</p><blockquote><p>The orchestrator is a MGR module, have you checked if the containers are up and running <sup><a href=#1>[1]</a></sup></p></blockquote><p>此时操作登录对应ceph node，docker restart ceph-mgr的模块，并且重启 mds 模块</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ceph health detail
</span></span><span class=line><span class=cl><span class=o>[</span>WRN<span class=o>]</span> RECENT_CRASH: <span class=m>4</span> daemons have recently crashed
</span></span><span class=line><span class=cl>    osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span><span class=line><span class=cl>    client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz</span></span></code></pre></td></tr></table></div></div></div></div><p>此时集群恢复正常，cephfs 恢复</p><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>由于长期没有在处理 ceph 方向问题，对排查有以下生疏：</p><ul><li>无法挂载时没有及时查看 ceph 集群信息，而是盯着客户端方向日志查询了半天。</li><li>对 ceph 故障代码没有了解过，如果有了解，可以很明确的定位问题，而不用耽误2小时。</li></ul><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p><sup id=1>[1]</sup> <a href=https://lists.ceph.io/hyperkitty/list/ceph-users@ceph.io/thread/2OSO26WYFBS4HZ4LPHNMBZUQ6Y3GI6GG/ target=_blank rel="noopener nofollow noreferrer">ceph orch status hangs forever</a></p><p><sup id=2>[2]</sup> <a href=https://docs.ceph.com/en/quincy/rados/operations/health-checks/ target=_blank rel="noopener nofollow noreferrer">HEALTH CHECKS</a></p><p><sup id=3>[3]</sup> <a href="https://docs.ceph.com/en/quincy/cephfs/health-messages/?highlight=MDS_SLOW_METADATA_IO" target=_blank rel="noopener nofollow noreferrer">CEPHFS HEALTH MESSAGES</a></p></div><div class=pe-copyright><hr><blockquote><p>本文为原创内容，版权归作者所有。如需转载，请在文章中声明本文标题及链接。</p><p>文章标题：记录一次ceph集群故障处理记录</p><p>文章链接：<a href=https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/ target=_blank>https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/</a></p><p>许可协议：<a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></blockquote></div><div class=comments-separator></div><h3 class=relatedContentTitle>相关阅读</h3><ul class=relatedContent><li><a href=/2023/11/10-1-ceph-fscache/><span>当cephfs和fscache结合时在K8s环境下的全集群规模故障</span></a></li><li><a href=/2023/09/11-1-ceph-common-cmd/><span>ceph常用命令</span></a></li><li><a href=/2023/09/6-1-ceph-rebalance/><span>Ceph重新平衡 - Rebalance</span></a></li><li><a href=/2023/09/05-4-s3cmd-in-windows/><span>Ceph对象存储 - windows上安装s3cmd</span></a></li><li><a href=/2023/09/05-3-s3cmd/><span>Ceph对象存储 - 使用s3cmd管理对象存储</span></a></li></ul><div class=comments-separator></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.oomkill.com/tags/storage/>Storage</a></li><li><a href=https://www.oomkill.com/tags/troubleshooting/>Troubleshooting</a></li></ul><nav class=paginav><a class=prev href=https://www.oomkill.com/2024/02/k8s-jsonnet/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></polyline></svg>&nbsp;</span>
<span>k8s - jsonnet从入门到放弃</span>
</a><a class=next href=https://www.oomkill.com/2024/01/ch30-oomkill/><span class=title></span>
<span>深入理解Kubernetes - 基于OOMKill的QoS的设计&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span></a></nav></footer><div class=pe-comments-decoration><p class=pe-comments-title></p><p class=pe-comments-subtitle></p></div><div id=pe-comments></div><script src=/js/pe-go-comment.min.86a214102576ba5f9b7bdc29eed8d58dd56e34aef80b3c65c73ea9cc88443696.js integrity="sha256-hqIUECV2ul+be9wp7tjVjdVuNK74Czxlxz6pzIhENpY="></script><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="dark"?"dark":"light",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"cylonchau/cylonchau.github.io","data-repo-id":"R_kgDOIRlNSQ","data-category":"Announcements","data-category-id":"DIC_kwDOIRlNSc4CXy1U","data-mapping":"pathname","data-term":"posts/10-2-troubeshooting-crash","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":getStoredTheme(),"data-lang":"zh-TW","data-loading":"lazy",crossorigin:"anonymous",async:""},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#pe-comments").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://www.oomkill.com/>Cylon's Collection</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> on
<a href=https://pages.github.com/ rel=noopener target=_blank>GitHub Pages</a> & Theme
        <a href=https://github.com/tofuwine/PaperMod-PE rel=noopener target=_blank>PaperMod-PE</a></span></footer><div class=pe-right-sidebar><a href=javascript:void(0); id=theme-toggle-float class=pe-float-btn><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a><a href=#top class=pe-float-btn id=top-link><span id=pe-read-progress></span></a></div><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>