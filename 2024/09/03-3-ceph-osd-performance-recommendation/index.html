<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Ceph OSD内存优化与建议 | Cylon&#39;s Collection</title>
<meta name="keywords" content="ceph osd, troubleshooting">
<meta name="description" content="Ceph OSD内存优化与建议 - Cylon&#39;s Collection">
<meta name="author" content="cylon">
<link rel="canonical" href="https://www.oomkill.com/2024/09/03-3-ceph-osd-performance-recommendation/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.41a8706089174fae1769fc26da4d1d354fa88083db604a95688ff58852dd9006.css" integrity="sha256-QahwYIkXT64Xafwm2k0dNU&#43;ogIPbYEqVaI/1iFLdkAY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://www.oomkill.com/favicon.ico">
<link rel="apple-touch-icon" href="https://www.oomkill.com/apple-touch-icon.png">

<meta name="twitter:title" content="Ceph OSD内存优化与建议 | Cylon&#39;s Collection" />
<meta name="twitter:description" content="" />
<meta property="og:title" content="Ceph OSD内存优化与建议 | Cylon&#39;s Collection" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.oomkill.com/2024/09/03-3-ceph-osd-performance-recommendation/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2024-09-13T00:00:00&#43;00:00" />
  <meta property="article:modified_time" content="2024-09-13T23:10:36&#43;08:00" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://www.oomkill.com/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Ceph OSD内存优化与建议",
      "item": "https://www.oomkill.com/2024/09/03-3-ceph-osd-performance-recommendation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Ceph OSD内存优化与建议 | Cylon's Collection",
  "name": "Ceph OSD内存优化与建议",
  "description": "",
  "keywords": [
    "ceph osd", "troubleshooting"
  ],
  "wordCount" : "2482",
  "inLanguage": "zh",
  "datePublished": "2024-09-13T00:00:00Z",
  "dateModified": "2024-09-13T23:10:36+08:00",
  "author":{
    "@type": "Person",
    "name": "cylon"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.oomkill.com/2024/09/03-3-ceph-osd-performance-recommendation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Cylon's Collection",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.oomkill.com/favicon.ico"
    }
  }
}
</script><script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary-bg: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list-page {
                background: var(--theme);
            }

            .list-page:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list-page:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

</head>

<body class=" type-posts kind-page layout-" id="top"><script data-no-instant>
function switchTheme(theme) {
  switch (theme) {
    case 'light':
      document.body.classList.remove('dark');
      break;
    case 'dark':
      document.body.classList.add('dark');
      break;
    
    default:
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
      }
  }
}

function isDarkTheme() {
  return document.body.className.includes("dark");
}

function getPrefTheme() {
  return localStorage.getItem("pref-theme");
}

function setPrefTheme(theme) {
  switchTheme(theme)
  localStorage.setItem("pref-theme", theme);
}

const toggleThemeCallbacks = {}
toggleThemeCallbacks['main'] = (isDark) => {
  
  if (isDark) {
    setPrefTheme('light');
  } else {
    setPrefTheme('dark');
  }
}




window.addEventListener('toggle-theme', function() {
  
  const isDark = isDarkTheme()
  for (const key in toggleThemeCallbacks) {
    toggleThemeCallbacks[key](isDark)
  }
});


function toggleThemeListener() {
  
  window.dispatchEvent(new CustomEvent('toggle-theme'));
}

</script>
<script>
  
  (function() {
    const defaultTheme = 'auto';
    const prefTheme = getPrefTheme();
    const theme = prefTheme ? prefTheme : defaultTheme;

    switchTheme(theme);
  })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://www.oomkill.com" accesskey="h" title="Cylon&#39;s Collection (Alt + H)">Cylon&#39;s Collection</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://www.oomkill.com/archives/" title="归档"
                >归档
                </a>
            </li>
            <li>
                <a href="https://www.oomkill.com/tags/" title="标签"
                >标签
                </a>
            </li>
            <li>
                <a href="https://www.oomkill.com/search/" title="搜索 (Alt &#43; /)"data-no-instant accesskey=/
                >搜索
                </a>
            </li>
            <li>
                <a href="https://www.oomkill.com/about/" title="关于"
                >关于
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
  <header class="post-header"><h1 class="post-title">Ceph OSD内存优化与建议</h1>
    <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>2024-09-13</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6" style="user-select: text;"></line><line x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10" style="user-select: text;"></line></svg>
  <span>Edited on 2024-09-13</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon" style="user-select: text;"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z" style="user-select: text;"></path><line x1="7" y1="7" x2="7" y2="7" style="user-select: text;"></line></svg>
  <span class="post-tags"><a href="https://www.oomkill.com/tags/storage/">storage</a></span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"></path><circle cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>5 分钟</span></span>

      
      
    </div>
  </header> <div class="toc side right">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#osd%e7%9a%84%e5%86%85%e5%ad%98%e9%9c%80%e6%b1%82" aria-label="OSD的内存需求">OSD的内存需求</a><ul>
                        
                <li>
                    <a href="#ibm-storage-ceph" aria-label="IBM Storage Ceph">IBM Storage Ceph</a></li>
                <li>
                    <a href="#hardware-recommendations" aria-label="Hardware Recommendations">Hardware Recommendations</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%88%91%e4%bb%ac%e4%bd%bf%e7%94%a8ceph%e7%8e%af%e5%a2%83%e7%9a%84%e7%a4%ba%e4%be%8b" aria-label="我们使用Ceph环境的示例">我们使用Ceph环境的示例</a></li>
                <li>
                    <a href="#%e9%85%8d%e7%bd%ae%e7%9a%84%e4%b8%80%e4%b8%aa%e9%9c%80%e6%b1%82" aria-label="配置的一个需求">配置的一个需求</a><ul>
                        
                <li>
                    <a href="#%e7%bd%91%e4%b8%8a%e6%a1%88%e4%be%8b" aria-label="网上案例">网上案例</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%86%85%e5%ad%98%e6%9f%a5%e7%9c%8b" aria-label="内存查看">内存查看</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content">
    





<div class="copyrightTopBlock">
    <p>本文发布于<a href="https://www.oomkill.com/about" target="_blank">Cylon的收藏册</a>，转载请著名原文链接~</p>
    <div class="articleSuffix-bg"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 147.78 155.96"> <path d="M10.5,99.81a1.9,1.9,0,0,0-.53-.09,1.66,1.66,0,0,0-1.64,1.65A1.64,1.64,0,0,0,10,103a1.57,1.57,0,0,0,.87-.25l26.76,26.82.45-1.08L11.52,101.91A1.65,1.65,0,0,0,10.5,99.81Zm-.13,2a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.58.58,0,0,1,.57-.57h0a.57.57,0,0,1,.56.58A.55.55,0,0,1,10.37,101.77Z" style="fill:#c5c9e0"></path><path d="M56.15,117.58H39.06l0-.09a1.65,1.65,0,0,0-1.36-1H37.5a1.65,1.65,0,1,0,1.56,2.19H55.7L92.92,156h41.44v-1.08h-41Zm-18.25.94a.56.56,0,0,1-.79,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h0a.58.58,0,0,1,.57.58A.54.54,0,0,1,37.9,118.52Z" style="fill:#c5c9e0"></path><path d="M23.52,50.32a1.65,1.65,0,0,0,1.55-1.11H55.28l48-48.13h31.06V0H102.85l-48,48.13H25.07a1.64,1.64,0,0,0-2.09-1,1.64,1.64,0,0,0,.54,3.2Zm0-2.21a.57.57,0,0,1,0,1.13.57.57,0,1,1,0-1.13Z" style="fill:#c5c9e0"></path><polygon points="102.86 0 102.86 0 102.86 0 102.86 0" style="fill:#c5c9e0"></polygon><path d="M107.72,12.14h26.64V11.07H107.27L57.4,61H3.09a1.66,1.66,0,0,0-1.45-.86H1.52A1.65,1.65,0,1,0,2.81,63a1.59,1.59,0,0,0,.45-.87H57.85ZM2.05,62.23a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h.09a.57.57,0,0,1,.32,1Z" style="fill:#c5c9e0"></path><path d="M134.36,43.22V42.14h-22.3l-9.62,9.63a1.64,1.64,0,0,0-2.19.77,1.61,1.61,0,0,0-.17.71,1.65,1.65,0,1,0,3.29,0,1.61,1.61,0,0,0-.16-.72l9.3-9.32Zm-32.64,10.6a.57.57,0,0,1,0-1.13.57.57,0,0,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M147,52.3l-9,9H111.48a1.64,1.64,0,0,0-1.61-1.33h-.14a1.65,1.65,0,1,0,1.6,2.41h27.19l9.26-9.29L147,52.3Zm-37.15,9.85a.56.56,0,0,1-.56-.57h0a.56.56,0,0,1,.56-.56h0a.57.57,0,1,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M66.79,75.35l11,11.06h56.53V85.33H78.27l-11-11.06H49.49L37.12,86.67a1.64,1.64,0,0,0-2.09,1,1.61,1.61,0,0,0-.09.54,1.65,1.65,0,0,0,3.29,0,1.68,1.68,0,0,0-.26-.89l12-12ZM36.58,88.79a.57.57,0,1,1,.57-.56A.57.57,0,0,1,36.58,88.79Z" style="fill:#c5c9e0"></path><path d="M110.61,95.55,92.8,113.4a1.62,1.62,0,1,0,.77.76l17.49-17.53h23.31V95.55ZM92.49,115.28a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.57-.57h0a.58.58,0,0,1,.56.58A.55.55,0,0,1,92.49,115.28Z" style="fill:#c5c9e0"></path><path d="M97.89,122.3H76.62L64.2,109.85a1.65,1.65,0,0,0-.77-2.2,1.77,1.77,0,0,0-.72-.17h-.14a1.65,1.65,0,0,0,.15,3.29,1.58,1.58,0,0,0,.71-.17l12.74,12.77H98.34l17.48-17.52h18.54v-1.08h-19ZM63.12,109.53a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,1.14,0A.54.54,0,0,1,63.12,109.53Z" style="fill:#c5c9e0"></path> </svg> </div>
</div>
<br><p>本文记录了在使用 ceph 集群时遭遇到的内存问题，以及引用和参考一些资料用于对在 ceph 集群使用时的内存预估。</p>
<h2 id="osd的内存需求">OSD的内存需求<a hidden class="anchor" aria-hidden="true" href="#osd的内存需求">¶</a></h2>
<p>如何评估 Ceph OSD 所需的硬件也是对于集群选型，集群优化的一个必要条件，这里主要找到两个可靠的参考资料用于评估 OSD 内存配置大小</p>
<h3 id="ibm-storage-ceph">IBM Storage Ceph<a hidden class="anchor" aria-hidden="true" href="#ibm-storage-ceph">¶</a></h3>
<p>IBM  Storage  Ceph 提供了一个运行 Ceph 用于预估系统配置的一个最小推荐列表 <sup><a href="#1">[1]</a></sup>，个人感觉可以参考这些信息用于自己集群的优化。主要用于容器化的 Ceph 集群</p>
<table>
<thead>
<tr>
<th>Process</th>
<th>Criteria</th>
<th>Minimum Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>ceph-osd-container</strong></em></td>
<td>Processor</td>
<td>1x AMD64 or Intel 64 CPU CORE per OSD container</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>Minimum of 5 GB of RAM per OSD container</td>
</tr>
<tr>
<td></td>
<td>OS Disk</td>
<td>1x OS disk per host</td>
</tr>
<tr>
<td></td>
<td>OSD Storage</td>
<td>1x storage drive per OSD container. Cannot be shared with OS Disk.</td>
</tr>
<tr>
<td></td>
<td>block.db</td>
<td>Optional, but IBM recommended, 1x SSD or NVMe or Optane partition or lvm per daemon. Sizing is 4% of <code>block.data</code> for BlueStore for object, file, and mixed workloads and 1% of <code>block.data</code> for the BlueStore for Block Device, Openstack cinder, and Openstack cinder workloads.</td>
</tr>
<tr>
<td></td>
<td><code>block.wal</code></td>
<td>Optionally, 1x SSD or NVMe or Optane partition or logical volume per daemon. Use a small size, for example 10 GB, and only if it’s faster than the <code>block.db</code> device.</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>2x 10 GB Ethernet NICs</td>
</tr>
<tr>
<td><em><strong>ceph-mon-container</strong></em></td>
<td>Processor</td>
<td>1x AMD64 or Intel 64 CPU CORE per mon-container</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>3 GB per <code>mon-container</code></td>
</tr>
<tr>
<td></td>
<td>Disk Space</td>
<td>10 GB per <code>mon-container</code>, 50 GB Recommended</td>
</tr>
<tr>
<td></td>
<td>Monitor Disk</td>
<td>Optionally, 1x SSD disk for <code>Monitor rocksdb</code> data</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>2x 1 GB Ethernet NICs, 10 GB Recommended</td>
</tr>
<tr>
<td></td>
<td>Prometheus</td>
<td>20 GB to 50 GB under <code>/var/lib/ceph/</code> directory created as a separate file system to protect the contents under <code>/var/</code> directory.</td>
</tr>
<tr>
<td><em><strong>ceph-mgr-container</strong></em></td>
<td>Processor</td>
<td>1x AMD64 or Intel 64 CPU CORE per <code>mgr-container</code></td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>3 GB per <code>mgr-container</code></td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>2x 1 GB Ethernet NICs, 10 GB Recommended</td>
</tr>
<tr>
<td><em><strong>ceph-radosgw-container</strong></em></td>
<td>Processor</td>
<td>1x AMD64 or Intel 64 CPU CORE per radosgw-container</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>1 GB per daemon</td>
</tr>
<tr>
<td></td>
<td>Disk Space</td>
<td>5 GB per daemon</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>1x 1 GB Ethernet NICs</td>
</tr>
<tr>
<td><em><strong>ceph-mds-container</strong></em></td>
<td>Processor</td>
<td>1x AMD64 or Intel 64 CPU CORE per mds-container</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>3 GB per <code>mds-container</code>  This number is highly dependent on the configurable MDS cache size. The RAM requirement is typically twice as much as the amount set in the <code>mds_cache_memory_limit</code> configuration setting. Note also that this is the memory for your daemon, not the overall system memory.</td>
</tr>
<tr>
<td></td>
<td>Disk Space</td>
<td>2 GB per <code>mds-container</code>, plus considering any additional space required for possible debug logging, 20 GB is a good start.</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>2x 1 GB Ethernet NICs, 10 GB Recommended Note that this is the same network as the OSD containers. If you have a 10 GB network on your OSDs you should use the same on your MDS so that the MDS is not disadvantaged when it comes to latency.</td>
</tr>
</tbody>
</table>
<h3 id="hardware-recommendations">Hardware Recommendations<a hidden class="anchor" aria-hidden="true" href="#hardware-recommendations">¶</a></h3>
<p>Ceph 官方也提供了相应的硬件配置推荐，关键参数写的比较清晰，但实际的规模比较模棱两可，也是可以提供一些参考的，并且每个版本的 Ceph 所推荐的硬件也是不相同的。</p>
<p>下表是 Ceph nautilus 的推荐最小硬件 <sup><a href="#2">[2]</a></sup></p>
<table>
<thead>
<tr>
<th>Process</th>
<th>Criteria</th>
<th>Minimum Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>ceph-osd</strong></em></td>
<td>Processor</td>
<td>1x 64-bit AMD-64 1x 32-bit ARM dual-core or better</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>~1GB for 1TB of storage per daemon</td>
</tr>
<tr>
<td></td>
<td>Volume Storage</td>
<td>1x storage drive per daemon</td>
</tr>
<tr>
<td></td>
<td>Journal</td>
<td>1x SSD partition per daemon (optional)</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>2x 1GB Ethernet NICs</td>
</tr>
<tr>
<td><em><strong>ceph-mon</strong></em></td>
<td>Processor</td>
<td>1x 64-bit AMD-64 1x 32-bit ARM dual-core or better</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>1 GB per daemon</td>
</tr>
<tr>
<td></td>
<td>Disk Space</td>
<td>10 GB per daemon</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>2x 1GB Ethernet NICs</td>
</tr>
<tr>
<td><em><strong>ceph-mds</strong></em></td>
<td>Processor</td>
<td>1x 64-bit AMD-64 quad-core 1x 32-bit ARM quad-core</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>1 GB minimum per daemon</td>
</tr>
<tr>
<td></td>
<td>Disk Space</td>
<td>1 MB per daemon</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>2x 1GB Ethernet NICs</td>
</tr>
</tbody>
</table>
<p>下表是 reef 版本的官方推荐最小配置 <sup><a href="#3">[3]</a></sup></p>
<table>
<thead>
<tr>
<th>Process</th>
<th>Criteria</th>
<th>Bare Minimum and Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td><em><strong>ceph-osd</strong></em></td>
<td>Processor</td>
<td>1 core minimum, 2 recommended 1 core per 200-500 MB/s throughput 1 core per 1000-3000 IOPS  Results are before replication. Results may vary across CPU and drive models and Ceph configuration: (erasure coding, compression, etc) ARM processors specifically may require more cores for performance. SSD OSDs, especially NVMe, will benefit from additional cores per OSD. Actual performance depends on many factors including drives, net, and client throughput and latency. Benchmarking is highly recommended.</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>4GB+ per daemon (more is better) 2-4GB may function but may be slow Less than 2GB is not recommended</td>
</tr>
<tr>
<td></td>
<td>Storage Drives</td>
<td>1x storage drive per OSD</td>
</tr>
<tr>
<td></td>
<td>DB/WAL (optional)</td>
<td>1x SSD partion per HDD OSD 4-5x HDD OSDs per DB/WAL SATA SSD &lt;= 10 HDD OSDss per DB/WAL NVMe SSD</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>1x 1Gb/s (bonded 10+ Gb/s recommended)</td>
</tr>
<tr>
<td><em><strong>ceph-mon</strong></em></td>
<td>Processor</td>
<td>2 cores minimum</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>5GB+ per daemon (large / production clusters need more)</td>
</tr>
<tr>
<td></td>
<td>Storage</td>
<td>100 GB per daemon, SSD is recommended</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>1x 1Gb/s (10+ Gb/s recommended)</td>
</tr>
<tr>
<td><em><strong>ceph-mds</strong></em></td>
<td>Processor</td>
<td>2 cores minimum</td>
</tr>
<tr>
<td></td>
<td>RAM</td>
<td>2GB+ per daemon (more for production)</td>
</tr>
<tr>
<td></td>
<td>Disk Space</td>
<td>1 GB per daemon</td>
</tr>
<tr>
<td></td>
<td>Network</td>
<td>1x 1Gb/s (10+ Gb/s recommended)</td>
</tr>
</tbody>
</table>
<h2 id="我们使用ceph环境的示例">我们使用Ceph环境的示例<a hidden class="anchor" aria-hidden="true" href="#我们使用ceph环境的示例">¶</a></h2>
<p>用于 Openstack 环境的 Ceph OSD 使用内存记录，主要使用于RDB，机器配置为 1.8T, 900G 的混合硬盘，内存配置 512G， 可以看到 OSD 内存使用率在 0.3% 大概每个 OSD 使用内存量为 2GB。</p>
<pre><code class="language-bash">PID USER      PR  NI    VIRT    RES    SHR S  %CPU    %MEM     TIME+     COMMAND
225398 ceph   20   0 3849464   1.7g  22200 S   11.9    0.3    14501:14   ceph-osd    
224860 ceph   20   0 3612380   1.7g  22424 S   9.2     0.3    12697:04   ceph-osd
223902 ceph   20   0 3340844   1.7g  22172 S   8.6     0.3    21003:18   ceph-osd   
223440 ceph   20   0 3213884   1.7g  22288 S   5.9     0.3     8548:00   ceph-osd
224368 ceph   20   0 3292848   1.6g  22204 S   4.0     0.3     8655:56   ceph-osd     
222889 ceph   20   0 3231012   1.7g  22180 S   3.3     0.3     8190:03   ceph-osd
</code></pre>
<p>用于业务使用的 Ceph OSD，主要用于对象存储，机器配置为 8c/16G，硬盘是 700G 每块，可以看到每个 OSD 使用的内存大概为 1.8-2G，大概 OSD 的分布是每个节点最多三个 OSD。</p>
<p>Ceph node 01</p>
<pre><code class="language-bash"># ceph node 01
$ ps aux --sort=-%mem | head -10
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
ceph        1702  0.4 27.9 10128296 4550760 ?    Ssl  May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph
ceph        1721  0.6 12.8 3318456 2088704 ?     Ssl  May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph
ceph        1983  0.6 12.3 3358788 2012844 ?     Ssl  May03 1273:25 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph
ceph        1991  0.9 11.7 3451788 1912008 ?     Ssl  May03 1719:04 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph
ceph        1709  0.5  7.4 1646276 1212576 ?     Ssl  May03 1047:48 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
ceph       18979  1.0  4.5 1330064 742680 ?      Ssl  May03 1932:51 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
ceph      529617  3.7  4.4 1909588 721492 ?      Ssl  Jul15 3140:39 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
root         801  0.0  0.6 182536 98516 ?        Ss   May03 105:28 /usr/lib/systemd/systemd-journald
root        1704  0.0  0.3 701284 50132 ?        Ssl  May03  53:48 /usr/sbin/rsyslogd -n
</code></pre>
<p>Ceph node02</p>
<pre><code class="language-bash">$ ps aux --sort=-%mem | head -10
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
ceph       28650  1.4 12.8 3958988 2104296 ?     Ssl   2023 6214:07 /usr/bin/ceph-osd -f --cluster ceph --id 9 --setuser ceph --setgroup ceph
ceph      163854  1.4 12.7 3782156 2096396 ?     Ssl   2023 6092:28 /usr/bin/ceph-osd -f --cluster ceph --id 10 --setuser ceph --setgroup ceph
ceph     3801660  1.5 11.9 3389284 1959812 ?     Ssl  Jul10 1384:08 /usr/bin/ceph-osd -f --cluster ceph --id 11 --setuser ceph --setgroup ceph
root     3348820  0.1  0.1 510848 27732 ?        Sl   Jun27 171:24 /var/ossec/bin/wazuh-modulesd
root        1045  0.0  0.1 574296 21468 ?        Ssl   2023  85:44 /usr/bin/python2 -Es /usr/sbin/tuned -l -P
polkitd      670  0.0  0.0 612348 14992 ?        Ssl   2023  10:40 /usr/lib/polkit-1/polkitd --no-debug
</code></pre>
<p>Ceph node03</p>
<pre><code class="language-bash">$ ps aux --sort=-%mem | head -10
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
ceph     1942206  0.9 12.8 4214720 2092280 ?     Ssl   2023 7866:23 /usr/bin/ceph-osd -f --cluster ceph --id 7 --setuser ceph --setgroup ceph
ceph        2824  0.8 12.6 4274848 2051800 ?     Ssl   2022 7205:58 /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph
ceph     2802022  0.7 12.5 3831320 2047440 ?     Ssl   2023 4078:51 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph
ceph        1693  0.7  4.7 1439428 771228 ?      Ssl   2022 6767:46 /usr/bin/ceph-mon -f --cluster ceph --id node03 --setuser ceph --setgroup ceph
ceph     1058494  0.3  2.2 7492512 367288 ?      Ssl   2023 3388:44 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node03 --setuser ceph --setgroup ceph
ceph     1812870  2.6  0.8 970928 133116 ?       Ssl   Mar21 6749:43 /usr/bin/ceph-mgr -f --cluster ceph --id node03 --setuser ceph --setgroup ceph
root         778  0.0  0.1  76412 28084 ?        Ss    2022 113:06 /usr/lib/systemd/systemd-journald
ceph        1739  0.4  0.1 384760 28064 ?        Ssl   2022 4086:33 /usr/bin/ceph-mds -f --cluster ceph --id node03 --setuser ceph --setgroup ceph
</code></pre>
<p>Ceph node04，该节点上只有一个 OSD</p>
<pre><code class="language-bash">$ ps aux --sort=-%mem | head -10
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
ceph       83779  1.0 12.7 3911168 2087332 ?     Ssl  Jan23 3473:50 /usr/bin/ceph-osd -f --cluster ceph --id 12 --setuser ceph --setgroup ceph
root        6568  0.0  0.0 113020  7808 ?        Ss   Jan22   0:00 /usr/sbin/sshd -D
</code></pre>
<h2 id="配置的一个需求">配置的一个需求<a hidden class="anchor" aria-hidden="true" href="#配置的一个需求">¶</a></h2>
<p>osd 在运行在没有限制的情况下运行会消耗所有的可用内存，所以当数据节点配置不当，也会引起 oomkiller</p>
<blockquote>
<p>The OSDs are designed to consume all the available memory if they are  run without limits. So it is recommended to apply the resource limits,  and the OSDs will stay within the bounds you set. Typically 4GB is  sufficient per OSD. <sup><a href="#4">[4]</a></sup></p>
</blockquote>
<p>当 OSD 经历恢复时，它们的内存利用率会达到峰值。如果可用的 RAM 不足，OSD 性能会显着降低，守护进程甚至可能崩溃或被 Linux OOM Killer杀死。<sup><a href="#5">[5]</a></sup></p>
<p>使用 cephadm 部署的机器群可以通过下面命令查看内存使用情况</p>
<pre><code class="language-bash">ceph orch ps
</code></pre>
<p>通常只有两种类型的守护进程有内存限制：mon 和 osd，这些内存限制参数由如下配置进行控制的</p>
<pre><code class="language-bash">sudo ceph config get mon mon_memory_target  # in bytes
sudo ceph config get mon mon_memory_autotune
sudo ceph config get osd osd_memory_target  # in bytes
sudo ceph config get osd osd_memory_target_autotune
</code></pre>
<p>通过 orch ps 查看的内存限制是不同于 ceph osd 的目标值的，BlueStore 将 OSD 堆内存使用量保留在指定目标大小下，并使用 <code>osd_memory_target</code> 配置选项。</p>
<blockquote>
<p>选项 <code>osd_memory_target</code> 根据系统中可用的 RAM 来设置 OSD 内存。当 TCMalloc 配置为内存分配器，BlueStore 中的 <code>bluestore_cache_autotune</code> 选项设为 <code>true</code> 时，则使用此选项。</p>
</blockquote>
<p>查看现有集群 osd 的配置</p>
<pre><code class="language-bash"># 显示存储集群中的所有 OSD osd_memory_target
sudo ceph config get osd osd_memory_target
# 显示指定 OSD osd_memory_target
sudo ceph config get osd.0 osd_memory_target
</code></pre>
<p>配置集群 OSD <code>osd_memory_target</code></p>
<pre><code class="language-bash"># 为存储集群中的所有 OSD 设置 osd_memory_target
ceph config set osd osd_memory_target VALUE
# 为存储集群中的指定 OSD 设置 osd_memory_target，.id 是 OSD 的 ID 
ceph config set osd.id osd_memory_target VALUE
</code></pre>
<h3 id="网上案例">网上案例<a hidden class="anchor" aria-hidden="true" href="#网上案例">¶</a></h3>
<p>下面有两个网上搜到的案例，osd具有无限制的内存增长的案例</p>
<ul>
<li>osd(s) with unlimited ram growth <sup><a href="#6">[6]</a></sup></li>
<li>How to solve “the Out of Memory Killer issue that kills your OSDs due to bad entries in PG logs” <sup><a href="#7">[7]</a></sup></li>
</ul>
<h2 id="内存查看">内存查看<a hidden class="anchor" aria-hidden="true" href="#内存查看">¶</a></h2>
<p>使用统计命令，该命令的统计信息不需要运行探查器，也不会将堆分配信息转储到文件中。</p>
<pre><code class="language-bash">ceph tell osd.0 heap stats
</code></pre>
<p>使用内存池命令</p>
<pre><code class="language-bash">ceph daemon osd.NNN dump_mempools
</code></pre>
<p>使用 google-perftools，该命令会运行探针，来检测运行的命令</p>
<pre><code class="language-bash">google-pprof --text {path-to-daemon}  {log-path/filename}
# 例如
pprof --text /usr/bin/ceph-mon /var/log/ceph/mon.node1.profile.0001.heap
</code></pre>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">¶</a></h2>
<p><sup id="1">[1]</sup> <a href="https://www.ibm.com/docs/en/storage-ceph/7?topic=recommendations-minimum-hardware-considerations" target="_blank"
   rel="noopener nofollow noreferrer" >Minimum hardware considerations</a></p>
<p><sup id="2">[2]</sup> <a href="https://web.archive.org/web/20240914160430/https://docs.ceph.com/en/nautilus/start/hardware-recommendations/#minimum-hardware-recommendations" target="_blank"
   rel="noopener nofollow noreferrer" >minimum-hardware-recommendations nautilus</a></p>
<p><sup id="3">[3]</sup> <a href="https://web.archive.org/web/20240914160933/https://docs.ceph.com/en/reef/start/hardware-recommendations/#minimum-hardware-recommendations" target="_blank"
   rel="noopener nofollow noreferrer" >minimum-hardware-recommendations reef</a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/rook/rook/issues/12078" target="_blank"
   rel="noopener nofollow noreferrer" >Excessive OSD memory usage   #12078</a></p>
<p><sup id="5">[5]</sup> <a href="https://web.archive.org/web/20240915061444/https://www.cnblogs.com/varden/p/15949938.html" target="_blank"
   rel="noopener nofollow noreferrer" >Ceph OSD 故障排除之内存不足</a></p>
<p><sup id="6">[6]</sup> <a href="https://docs.clyso.com/blog/osds-with-unlimited-ram-growth/" target="_blank"
   rel="noopener nofollow noreferrer" >osd(s) with unlimited ram growth</a></p>
<p><sup id="7">[7]</sup> <a href="https://croit.io/blog/oom-killer-osds" target="_blank"
   rel="noopener nofollow noreferrer" >How to solve “the Out of Memory Killer issue that kills your OSDs due to bad entries in PG logs”</a></p>
<p><sup id="8">[8]</sup> <a href="https://docs.ceph.com/en/reef/rados/troubleshooting/memory-profiling/" target="_blank"
   rel="noopener nofollow noreferrer" >Memory Profiling</a></p>


    
    


<div class="copyrightBlock" >
    <div class="articleSuffix-bg"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 147.78 155.96"> <path d="M10.5,99.81a1.9,1.9,0,0,0-.53-.09,1.66,1.66,0,0,0-1.64,1.65A1.64,1.64,0,0,0,10,103a1.57,1.57,0,0,0,.87-.25l26.76,26.82.45-1.08L11.52,101.91A1.65,1.65,0,0,0,10.5,99.81Zm-.13,2a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.58.58,0,0,1,.57-.57h0a.57.57,0,0,1,.56.58A.55.55,0,0,1,10.37,101.77Z" style="fill:#c5c9e0"></path><path d="M56.15,117.58H39.06l0-.09a1.65,1.65,0,0,0-1.36-1H37.5a1.65,1.65,0,1,0,1.56,2.19H55.7L92.92,156h41.44v-1.08h-41Zm-18.25.94a.56.56,0,0,1-.79,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h0a.58.58,0,0,1,.57.58A.54.54,0,0,1,37.9,118.52Z" style="fill:#c5c9e0"></path><path d="M23.52,50.32a1.65,1.65,0,0,0,1.55-1.11H55.28l48-48.13h31.06V0H102.85l-48,48.13H25.07a1.64,1.64,0,0,0-2.09-1,1.64,1.64,0,0,0,.54,3.2Zm0-2.21a.57.57,0,0,1,0,1.13.57.57,0,1,1,0-1.13Z" style="fill:#c5c9e0"></path><polygon points="102.86 0 102.86 0 102.86 0 102.86 0" style="fill:#c5c9e0"></polygon><path d="M107.72,12.14h26.64V11.07H107.27L57.4,61H3.09a1.66,1.66,0,0,0-1.45-.86H1.52A1.65,1.65,0,1,0,2.81,63a1.59,1.59,0,0,0,.45-.87H57.85ZM2.05,62.23a.57.57,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.56-.57h.09a.57.57,0,0,1,.32,1Z" style="fill:#c5c9e0"></path><path d="M134.36,43.22V42.14h-22.3l-9.62,9.63a1.64,1.64,0,0,0-2.19.77,1.61,1.61,0,0,0-.17.71,1.65,1.65,0,1,0,3.29,0,1.61,1.61,0,0,0-.16-.72l9.3-9.32Zm-32.64,10.6a.57.57,0,0,1,0-1.13.57.57,0,0,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M147,52.3l-9,9H111.48a1.64,1.64,0,0,0-1.61-1.33h-.14a1.65,1.65,0,1,0,1.6,2.41h27.19l9.26-9.29L147,52.3Zm-37.15,9.85a.56.56,0,0,1-.56-.57h0a.56.56,0,0,1,.56-.56h0a.57.57,0,1,1,0,1.13Z" style="fill:#c5c9e0"></path><path d="M66.79,75.35l11,11.06h56.53V85.33H78.27l-11-11.06H49.49L37.12,86.67a1.64,1.64,0,0,0-2.09,1,1.61,1.61,0,0,0-.09.54,1.65,1.65,0,0,0,3.29,0,1.68,1.68,0,0,0-.26-.89l12-12ZM36.58,88.79a.57.57,0,1,1,.57-.56A.57.57,0,0,1,36.58,88.79Z" style="fill:#c5c9e0"></path><path d="M110.61,95.55,92.8,113.4a1.62,1.62,0,1,0,.77.76l17.49-17.53h23.31V95.55ZM92.49,115.28a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,.57-.57h0a.58.58,0,0,1,.56.58A.55.55,0,0,1,92.49,115.28Z" style="fill:#c5c9e0"></path><path d="M97.89,122.3H76.62L64.2,109.85a1.65,1.65,0,0,0-.77-2.2,1.77,1.77,0,0,0-.72-.17h-.14a1.65,1.65,0,0,0,.15,3.29,1.58,1.58,0,0,0,.71-.17l12.74,12.77H98.34l17.48-17.52h18.54v-1.08h-19ZM63.12,109.53a.56.56,0,0,1-.8,0,.58.58,0,0,1-.17-.41.57.57,0,0,1,1.14,0A.54.54,0,0,1,63.12,109.53Z" style="fill:#c5c9e0"></path> </svg> </div>
    <p>本文发布于<a href="https://www.oomkill.com/about" target="_blank">Cylon的收藏册</a>，转载请著名原文链接~</p>
    <p>链接：<a href="https://www.oomkill.com/2024/09/03-3-ceph-osd-performance-recommendation/" target="_blank">https://www.oomkill.com/2024/09/03-3-ceph-osd-performance-recommendation/</a></p>
    <p style="margin-bottom: 0px;">版权：本作品采用<a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">「署名-非商业性使用-相同方式共享 4.0 国际」</a> 许可协议进行许可。</p>
    </div>
</div>
  </div>

  <footer class="post-footer">
    
<nav class="paginav">
  <a class="prev" href="https://www.oomkill.com/2024/09/gorm-before-delete/">
    <span class="title"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select: text;"><line x1="19" y1="12" x2="5" y2="12" style="user-select: text;"></line><polyline points="12 19 5 12 12 5" style="user-select: text;"></polyline>
      </polyline></svg>&nbsp; </span>
    
    <span>Gorm - BeforeDelete无法获取正确条目</span>
  </a>
  <a class="next" href="https://www.oomkill.com/2024/09/05-5-failed-troubleshooting-for-rgw/" >
    <span class="title"> </span>
    
    <span>记录一次失败的radosgw问题排查记录&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select: text;"><line x1="5" y1="12" x2="19" y2="12" style="user-select: text;"></line><polyline points="12 5 19 12 12 19" style="user-select: text;"></polyline></svg></span>
  </a>
</nav>

  </footer>

  
  <div class="pagination__title">
    <span class="pagination__title-h"></span>
  </div>
  
  
  
  
    <div class="comments-separator"></div>
    

<h3 class="relatedContentTitle" >相关阅读</h3>
<ul class="relatedContent">
	
	<li><a href="/2024/09/05-5-failed-troubleshooting-for-rgw/"><span>记录一次失败的radosgw问题排查记录</span></a></li>
	
	<li><a href="/2020/09/alpine-trouble-q-and-a/"><span>使用alpine为基础镜像Q&amp;A</span></a></li>
	
	<li><a href="/2020/09/envoy-example-failed/"><span>envoy官方example运行失败问题处理</span></a></li>
	
	<li><a href="/2024/02/10-2-troubeshooting-crash/"><span>记录一次ceph集群故障处理记录</span></a></li>
	
	<li><a href="/2023/11/10-1-ceph-fscache/"><span>当cephfs和fscache结合时在K8s环境下的全集群规模故障</span></a></li>
	
</ul>

  

  
    
      <div class="comments-separator"></div>
<div class="comments">
    <script>
    function loadComment() {
        let theme = localStorage.getItem('pref-theme') === 'dark' ? 'dark' : 'light';
        let s = document.createElement('script');
        s.src = 'https://giscus.app/client.js';
        s.setAttribute('data-repo', 'cylonchau\/cylonchau.github.io');
        s.setAttribute('data-repo-id', 'R_kgDOIRlNSQ');
        s.setAttribute('data-category', 'Announcements');
        s.setAttribute('data-category-id', 'DIC_kwDOIRlNSc4CXy1U');
        s.setAttribute('data-mapping', 'title');
        s.setAttribute('data-reactions-enabled', '1');
        s.setAttribute('data-emit-metadata', '1');
        s.setAttribute('data-input-position', 'top');
        s.setAttribute('data-lang', 'zh-TW');
        s.setAttribute('data-theme', theme);
        s.setAttribute('crossorigin', 'anonymous');
        s.setAttribute('async', '');
        document.querySelector('div.comments').innerHTML = '';
        document.querySelector('div.comments').appendChild(s);
    }
    loadComment();
    </script>
</div>
</article>
    </main>
    
<footer class="footer">
  <p>
  Copyright
  <span>&copy; 2024 <a href="https://www.oomkill.com">Cylon&#39;s Collection</a></span></p>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> on github-page & Theme
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '1' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>

<script>
  document.addEventListener('scroll', function (e) {
      const readProgress = document.getElementById("read_progress");
      const scrollHeight = document.documentElement.scrollHeight;
      const clientHeight = document.documentElement.clientHeight;
      const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
      readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
  })
</script>

<script>
  var menu = document.getElementById('menu')
  if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
          localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
  }

  document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
          e.preventDefault();
          var id = this.getAttribute("href").substr(1);
          if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
              document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                  behavior: "smooth"
              });
          } else {
              document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
          }
          if (id === "top") {
              history.replaceState(null, null, " ");
          } else {
              history.pushState(null, null, `#${id}`);
          }
      });
  });
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
      if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
          mybutton.style.visibility = "visible";
          mybutton.style.opacity = "1";
      } else {
          mybutton.style.visibility = "hidden";
          mybutton.style.opacity = "0";
      }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'copy';

    function copyingDone() {
      copybutton.innerText = 'copied';
      setTimeout(() => {
        copybutton.innerText = 'copy';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

<script src="/js/instantclick.min.js" data-no-instant
></script>
<script data-no-instant>
  
  
  
  
  
  
  InstantClick.init();
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.6.0/mermaid.min.js" crossorigin="anonymous"></script>
<script>
    mermaid.init(undefined, '.language-mermaid');
</script>
</body>

</html>
