<!doctype html><html lang=zh dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ceph OSD内存优化与建议 | Cylon's Collection</title><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-NP3JNCPR" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><meta name=keywords content="ceph osd,troubleshooting"><meta name=description content="本文记录了在使用 ceph 集群时遭遇到的内存问题，以及引用和参考一些资料用于对在 ceph 集群使用时的内存预估。
OSD的内存需求 如何评估 Ceph OSD 所需的硬件也是对于集群选型，集群优化的一个必要条件，这里主要找到两个可靠的参考资料用于评估 OSD 内存配置大小
IBM Storage Ceph IBM Storage Ceph 提供了一个运行 Ceph 用于预估系统配置的一个最小推荐列表 [1]，个人感觉可以参考这些信息用于自己集群的优化。主要用于容器化的 Ceph 集群
Process Criteria Minimum Recommended ceph-osd-container Processor 1x AMD64 or Intel 64 CPU CORE per OSD container RAM Minimum of 5 GB of RAM per OSD container OS Disk 1x OS disk per host OSD Storage 1x storage drive per OSD container. Cannot be shared with OS Disk."><meta name=author content="cylon"><link rel=canonical href=http://localhost:1313/2024/09/03-3-ceph-osd-performance-recommendation/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/favicon.ico><link rel=mask-icon href=http://localhost:1313/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=http://localhost:1313/2024/09/03-3-ceph-osd-performance-recommendation/><noscript><style>#theme-toggle,#top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=/assets/css/pe.min.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/pe.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/v4-shims.min.css><script id=MathJax-script async src=https://cdn.staticfile.net/mathjax/3.2.2/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["\\$","\\$"]]}}</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-NP3JNCPR")</script><meta property="og:title" content="Ceph OSD内存优化与建议"><meta property="og:description" content="本文记录了在使用 ceph 集群时遭遇到的内存问题，以及引用和参考一些资料用于对在 ceph 集群使用时的内存预估。
OSD的内存需求 如何评估 Ceph OSD 所需的硬件也是对于集群选型，集群优化的一个必要条件，这里主要找到两个可靠的参考资料用于评估 OSD 内存配置大小
IBM Storage Ceph IBM Storage Ceph 提供了一个运行 Ceph 用于预估系统配置的一个最小推荐列表 [1]，个人感觉可以参考这些信息用于自己集群的优化。主要用于容器化的 Ceph 集群
Process Criteria Minimum Recommended ceph-osd-container Processor 1x AMD64 or Intel 64 CPU CORE per OSD container RAM Minimum of 5 GB of RAM per OSD container OS Disk 1x OS disk per host OSD Storage 1x storage drive per OSD container. Cannot be shared with OS Disk."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/2024/09/03-3-ceph-osd-performance-recommendation/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-13T23:10:36+08:00"><meta property="og:site_name" content="Cylon's Collection"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ceph OSD内存优化与建议"><meta name=twitter:description content="本文记录了在使用 ceph 集群时遭遇到的内存问题，以及引用和参考一些资料用于对在 ceph 集群使用时的内存预估。
OSD的内存需求 如何评估 Ceph OSD 所需的硬件也是对于集群选型，集群优化的一个必要条件，这里主要找到两个可靠的参考资料用于评估 OSD 内存配置大小
IBM Storage Ceph IBM Storage Ceph 提供了一个运行 Ceph 用于预估系统配置的一个最小推荐列表 [1]，个人感觉可以参考这些信息用于自己集群的优化。主要用于容器化的 Ceph 集群
Process Criteria Minimum Recommended ceph-osd-container Processor 1x AMD64 or Intel 64 CPU CORE per OSD container RAM Minimum of 5 GB of RAM per OSD container OS Disk 1x OS disk per host OSD Storage 1x storage drive per OSD container. Cannot be shared with OS Disk."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Ceph OSD内存优化与建议","item":"http://localhost:1313/2024/09/03-3-ceph-osd-performance-recommendation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ceph OSD内存优化与建议","name":"Ceph OSD内存优化与建议","description":"本文记录了在使用 ceph 集群时遭遇到的内存问题，以及引用和参考一些资料用于对在 ceph 集群使用时的内存预估。\nOSD的内存需求 如何评估 Ceph OSD 所需的硬件也是对于集群选型，集群优化的一个必要条件，这里主要找到两个可靠的参考资料用于评估 OSD 内存配置大小\nIBM Storage Ceph IBM Storage Ceph 提供了一个运行 Ceph 用于预估系统配置的一个最小推荐列表 [1]，个人感觉可以参考这些信息用于自己集群的优化。主要用于容器化的 Ceph 集群\nProcess Criteria Minimum Recommended ceph-osd-container Processor 1x AMD64 or Intel 64 CPU CORE per OSD container RAM Minimum of 5 GB of RAM per OSD container OS Disk 1x OS disk per host OSD Storage 1x storage drive per OSD container. Cannot be shared with OS Disk.","keywords":["ceph osd","troubleshooting"],"articleBody":"本文记录了在使用 ceph 集群时遭遇到的内存问题，以及引用和参考一些资料用于对在 ceph 集群使用时的内存预估。\nOSD的内存需求 如何评估 Ceph OSD 所需的硬件也是对于集群选型，集群优化的一个必要条件，这里主要找到两个可靠的参考资料用于评估 OSD 内存配置大小\nIBM Storage Ceph IBM Storage Ceph 提供了一个运行 Ceph 用于预估系统配置的一个最小推荐列表 [1]，个人感觉可以参考这些信息用于自己集群的优化。主要用于容器化的 Ceph 集群\nProcess Criteria Minimum Recommended ceph-osd-container Processor 1x AMD64 or Intel 64 CPU CORE per OSD container RAM Minimum of 5 GB of RAM per OSD container OS Disk 1x OS disk per host OSD Storage 1x storage drive per OSD container. Cannot be shared with OS Disk. block.db Optional, but IBM recommended, 1x SSD or NVMe or Optane partition or lvm per daemon. Sizing is 4% of block.data for BlueStore for object, file, and mixed workloads and 1% of block.data for the BlueStore for Block Device, Openstack cinder, and Openstack cinder workloads. block.wal Optionally, 1x SSD or NVMe or Optane partition or logical volume per daemon. Use a small size, for example 10 GB, and only if it’s faster than the block.db device. Network 2x 10 GB Ethernet NICs ceph-mon-container Processor 1x AMD64 or Intel 64 CPU CORE per mon-container RAM 3 GB per mon-container Disk Space 10 GB per mon-container, 50 GB Recommended Monitor Disk Optionally, 1x SSD disk for Monitor rocksdb data Network 2x 1 GB Ethernet NICs, 10 GB Recommended Prometheus 20 GB to 50 GB under /var/lib/ceph/ directory created as a separate file system to protect the contents under /var/ directory. ceph-mgr-container Processor 1x AMD64 or Intel 64 CPU CORE per mgr-container RAM 3 GB per mgr-container Network 2x 1 GB Ethernet NICs, 10 GB Recommended ceph-radosgw-container Processor 1x AMD64 or Intel 64 CPU CORE per radosgw-container RAM 1 GB per daemon Disk Space 5 GB per daemon Network 1x 1 GB Ethernet NICs ceph-mds-container Processor 1x AMD64 or Intel 64 CPU CORE per mds-container RAM 3 GB per mds-container This number is highly dependent on the configurable MDS cache size. The RAM requirement is typically twice as much as the amount set in the mds_cache_memory_limit configuration setting. Note also that this is the memory for your daemon, not the overall system memory. Disk Space 2 GB per mds-container, plus considering any additional space required for possible debug logging, 20 GB is a good start. Network 2x 1 GB Ethernet NICs, 10 GB Recommended Note that this is the same network as the OSD containers. If you have a 10 GB network on your OSDs you should use the same on your MDS so that the MDS is not disadvantaged when it comes to latency. Hardware Recommendations Ceph 官方也提供了相应的硬件配置推荐，关键参数写的比较清晰，但实际的规模比较模棱两可，也是可以提供一些参考的，并且每个版本的 Ceph 所推荐的硬件也是不相同的。\n下表是 Ceph nautilus 的推荐最小硬件 [2]\nProcess Criteria Minimum Recommended ceph-osd Processor 1x 64-bit AMD-64 1x 32-bit ARM dual-core or better RAM ~1GB for 1TB of storage per daemon Volume Storage 1x storage drive per daemon Journal 1x SSD partition per daemon (optional) Network 2x 1GB Ethernet NICs ceph-mon Processor 1x 64-bit AMD-64 1x 32-bit ARM dual-core or better RAM 1 GB per daemon Disk Space 10 GB per daemon Network 2x 1GB Ethernet NICs ceph-mds Processor 1x 64-bit AMD-64 quad-core 1x 32-bit ARM quad-core RAM 1 GB minimum per daemon Disk Space 1 MB per daemon Network 2x 1GB Ethernet NICs 下表是 reef 版本的官方推荐最小配置 [3]\nProcess Criteria Bare Minimum and Recommended ceph-osd Processor 1 core minimum, 2 recommended 1 core per 200-500 MB/s throughput 1 core per 1000-3000 IOPS Results are before replication. Results may vary across CPU and drive models and Ceph configuration: (erasure coding, compression, etc) ARM processors specifically may require more cores for performance. SSD OSDs, especially NVMe, will benefit from additional cores per OSD. Actual performance depends on many factors including drives, net, and client throughput and latency. Benchmarking is highly recommended. RAM 4GB+ per daemon (more is better) 2-4GB may function but may be slow Less than 2GB is not recommended Storage Drives 1x storage drive per OSD DB/WAL (optional) 1x SSD partion per HDD OSD 4-5x HDD OSDs per DB/WAL SATA SSD \u003c= 10 HDD OSDss per DB/WAL NVMe SSD Network 1x 1Gb/s (bonded 10+ Gb/s recommended) ceph-mon Processor 2 cores minimum RAM 5GB+ per daemon (large / production clusters need more) Storage 100 GB per daemon, SSD is recommended Network 1x 1Gb/s (10+ Gb/s recommended) ceph-mds Processor 2 cores minimum RAM 2GB+ per daemon (more for production) Disk Space 1 GB per daemon Network 1x 1Gb/s (10+ Gb/s recommended) 我们使用Ceph环境的示例 用于 Openstack 环境的 Ceph OSD 使用内存记录，主要使用于RDB，机器配置为 1.8T, 900G 的混合硬盘，内存配置 512G， 可以看到 OSD 内存使用率在 0.3% 大概每个 OSD 使用内存量为 2GB。\nbash 1 2 3 4 5 6 7 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 225398 ceph 20 0 3849464 1.7g 22200 S 11.9 0.3 14501:14 ceph-osd 224860 ceph 20 0 3612380 1.7g 22424 S 9.2 0.3 12697:04 ceph-osd 223902 ceph 20 0 3340844 1.7g 22172 S 8.6 0.3 21003:18 ceph-osd 223440 ceph 20 0 3213884 1.7g 22288 S 5.9 0.3 8548:00 ceph-osd 224368 ceph 20 0 3292848 1.6g 22204 S 4.0 0.3 8655:56 ceph-osd 222889 ceph 20 0 3231012 1.7g 22180 S 3.3 0.3 8190:03 ceph-osd 用于业务使用的 Ceph OSD，主要用于对象存储，机器配置为 8c/16G，硬盘是 700G 每块，可以看到每个 OSD 使用的内存大概为 1.8-2G，大概 OSD 的分布是每个节点最多三个 OSD。\nCeph node 01\nbash 1 2 3 4 5 6 7 8 9 10 11 12 # ceph node 01 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1702 0.4 27.9 10128296 4550760 ? Ssl May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph ceph 1721 0.6 12.8 3318456 2088704 ? Ssl May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph ceph 1983 0.6 12.3 3358788 2012844 ? Ssl May03 1273:25 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph ceph 1991 0.9 11.7 3451788 1912008 ? Ssl May03 1719:04 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph ceph 1709 0.5 7.4 1646276 1212576 ? Ssl May03 1047:48 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph ceph 18979 1.0 4.5 1330064 742680 ? Ssl May03 1932:51 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph ceph 529617 3.7 4.4 1909588 721492 ? Ssl Jul15 3140:39 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph root 801 0.0 0.6 182536 98516 ? Ss May03 105:28 /usr/lib/systemd/systemd-journald root 1704 0.0 0.3 701284 50132 ? Ssl May03 53:48 /usr/sbin/rsyslogd -n Ceph node02\nbash 1 2 3 4 5 6 7 8 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 28650 1.4 12.8 3958988 2104296 ? Ssl 2023 6214:07 /usr/bin/ceph-osd -f --cluster ceph --id 9 --setuser ceph --setgroup ceph ceph 163854 1.4 12.7 3782156 2096396 ? Ssl 2023 6092:28 /usr/bin/ceph-osd -f --cluster ceph --id 10 --setuser ceph --setgroup ceph ceph 3801660 1.5 11.9 3389284 1959812 ? Ssl Jul10 1384:08 /usr/bin/ceph-osd -f --cluster ceph --id 11 --setuser ceph --setgroup ceph root 3348820 0.1 0.1 510848 27732 ? Sl Jun27 171:24 /var/ossec/bin/wazuh-modulesd root 1045 0.0 0.1 574296 21468 ? Ssl 2023 85:44 /usr/bin/python2 -Es /usr/sbin/tuned -l -P polkitd 670 0.0 0.0 612348 14992 ? Ssl 2023 10:40 /usr/lib/polkit-1/polkitd --no-debug Ceph node03\nbash 1 2 3 4 5 6 7 8 9 10 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1942206 0.9 12.8 4214720 2092280 ? Ssl 2023 7866:23 /usr/bin/ceph-osd -f --cluster ceph --id 7 --setuser ceph --setgroup ceph ceph 2824 0.8 12.6 4274848 2051800 ? Ssl 2022 7205:58 /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph ceph 2802022 0.7 12.5 3831320 2047440 ? Ssl 2023 4078:51 /usr/bin/ceph-osd -f --cluster ceph --id 1 --setuser ceph --setgroup ceph ceph 1693 0.7 4.7 1439428 771228 ? Ssl 2022 6767:46 /usr/bin/ceph-mon -f --cluster ceph --id node03 --setuser ceph --setgroup ceph ceph 1058494 0.3 2.2 7492512 367288 ? Ssl 2023 3388:44 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node03 --setuser ceph --setgroup ceph ceph 1812870 2.6 0.8 970928 133116 ? Ssl Mar21 6749:43 /usr/bin/ceph-mgr -f --cluster ceph --id node03 --setuser ceph --setgroup ceph root 778 0.0 0.1 76412 28084 ? Ss 2022 113:06 /usr/lib/systemd/systemd-journald ceph 1739 0.4 0.1 384760 28064 ? Ssl 2022 4086:33 /usr/bin/ceph-mds -f --cluster ceph --id node03 --setuser ceph --setgroup ceph Ceph node04，该节点上只有一个 OSD\nbash 1 2 3 4 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 83779 1.0 12.7 3911168 2087332 ? Ssl Jan23 3473:50 /usr/bin/ceph-osd -f --cluster ceph --id 12 --setuser ceph --setgroup ceph root 6568 0.0 0.0 113020 7808 ? Ss Jan22 0:00 /usr/sbin/sshd -D 配置的一个需求 osd 在运行在没有限制的情况下运行会消耗所有的可用内存，所以当数据节点配置不当，也会引起 oomkiller\nThe OSDs are designed to consume all the available memory if they are run without limits. So it is recommended to apply the resource limits, and the OSDs will stay within the bounds you set. Typically 4GB is sufficient per OSD. [4]\n当 OSD 经历恢复时，它们的内存利用率会达到峰值。如果可用的 RAM 不足，OSD 性能会显着降低，守护进程甚至可能崩溃或被 Linux OOM Killer杀死。[5]\n使用 cephadm 部署的机器群可以通过下面命令查看内存使用情况\nbash 1 ceph orch ps 通常只有两种类型的守护进程有内存限制：mon 和 osd，这些内存限制参数由如下配置进行控制的\nbash 1 2 3 4 sudo ceph config get mon mon_memory_target # in bytes sudo ceph config get mon mon_memory_autotune sudo ceph config get osd osd_memory_target # in bytes sudo ceph config get osd osd_memory_target_autotune 通过 orch ps 查看的内存限制是不同于 ceph osd 的目标值的，BlueStore 将 OSD 堆内存使用量保留在指定目标大小下，并使用 osd_memory_target 配置选项。\n选项 osd_memory_target 根据系统中可用的 RAM 来设置 OSD 内存。当 TCMalloc 配置为内存分配器，BlueStore 中的 bluestore_cache_autotune 选项设为 true 时，则使用此选项。\n查看现有集群 osd 的配置\nbash 1 2 3 4 # 显示存储集群中的所有 OSD osd_memory_target sudo ceph config get osd osd_memory_target # 显示指定 OSD osd_memory_target sudo ceph config get osd.0 osd_memory_target 配置集群 OSD osd_memory_target\nbash 1 2 3 4 # 为存储集群中的所有 OSD 设置 osd_memory_target ceph config set osd osd_memory_target VALUE # 为存储集群中的指定 OSD 设置 osd_memory_target，.id 是 OSD 的 ID ceph config set osd.id osd_memory_target VALUE 网上案例 下面有两个网上搜到的案例，osd具有无限制的内存增长的案例\nosd(s) with unlimited ram growth [6] How to solve “the Out of Memory Killer issue that kills your OSDs due to bad entries in PG logs” [7] 内存查看 使用统计命令，该命令的统计信息不需要运行探查器，也不会将堆分配信息转储到文件中。\nbash 1 ceph tell osd.0 heap stats 使用内存池命令\nbash 1 ceph daemon osd.NNN dump_mempools 使用 google-perftools，该命令会运行探针，来检测运行的命令\nbash 1 2 3 google-pprof --text {path-to-daemon} {log-path/filename} # 例如 pprof --text /usr/bin/ceph-mon /var/log/ceph/mon.node1.profile.0001.heap Reference [1] Minimum hardware considerations\n[2] minimum-hardware-recommendations nautilus\n[3] minimum-hardware-recommendations reef\n[4] Excessive OSD memory usage #12078\n[5] Ceph OSD 故障排除之内存不足\n[6] osd(s) with unlimited ram growth\n[7] How to solve “the Out of Memory Killer issue that kills your OSDs due to bad entries in PG logs”\n[8] Memory Profiling\n","wordCount":"1697","inLanguage":"zh","datePublished":"2024-09-13T00:00:00Z","dateModified":"2024-09-13T23:10:36+08:00","author":{"@type":"Person","name":"cylon"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/2024/09/03-3-ceph-osd-performance-recommendation/"},"publisher":{"@type":"Organization","name":"Cylon's Collection","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/><img src=http://localhost:1313/favicon.ico alt aria-label=logo height=20>Cylon's Collection</a><div class=logo-switches><button id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives><span>归档</span></a></li><li><a href=http://localhost:1313/tags><span>标签</span></a></li><li><a href=http://localhost:1313/search><span>搜索</span></a></li><li><a href=http://localhost:1313/about accesskey=/><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Ceph OSD内存优化与建议</h1><div class=post-meta><span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2024-09-13</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg><span>1697 字</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><span>8 分钟</span></span>
<span class=pe-post-meta-item>&nbsp;·&nbsp;<svg t="1714036239378" fill="currentcolor" class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6659" width="256" height="256"><path d="M690 78.2c-18.6-18.8-49-19-67.8-.4s-19 49-.4 67.8l255.4 258.6c67.8 68.6 67.8 178.8.0 247.4L653.4 878.2c-18.6 18.8-18.4 49.2.4 67.8s49.2 18.4 67.8-.4l224-226.4c104.8-106 104.8-276.4.0-382.4L690 78.2zM485.4 101.4c-24-24-56.6-37.4-90.6-37.4H96C43 64 0 107 0 160v299c0 34 13.4 66.6 37.4 90.6l336 336c50 50 131 50 181 0l267-267c50-50 50-131 0-181l-336-336zM96 160h299c8.4.0 16.6 3.4 22.6 9.4l336 336c12.4 12.4 12.4 32.8.0 45.2l-267 267c-12.4 12.4-32.8 12.4-45.2.0l-336-336c-6-6-9.4-14.2-9.4-22.6V160zm192 128a64 64 0 10-128 0 64 64 0 10128 0z" p-id="6660"/></svg></span><ul class=pe-post-meta-item><a href=http://localhost:1313/tags/storage/>#Storage</a></ul></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary><span class=details>目录</span></summary><div class=inner><ul><li><a href=#osd%e7%9a%84%e5%86%85%e5%ad%98%e9%9c%80%e6%b1%82 aria-label=OSD的内存需求>OSD的内存需求</a><ul><li><a href=#ibm-storage-ceph aria-label="IBM Storage Ceph">IBM Storage Ceph</a><li><a href=#hardware-recommendations aria-label="Hardware Recommendations">Hardware Recommendations</a></ul><li><a href=#%e6%88%91%e4%bb%ac%e4%bd%bf%e7%94%a8ceph%e7%8e%af%e5%a2%83%e7%9a%84%e7%a4%ba%e4%be%8b aria-label=我们使用Ceph环境的示例>我们使用Ceph环境的示例</a><li><a href=#%e9%85%8d%e7%bd%ae%e7%9a%84%e4%b8%80%e4%b8%aa%e9%9c%80%e6%b1%82 aria-label=配置的一个需求>配置的一个需求</a><ul><li><a href=#%e7%bd%91%e4%b8%8a%e6%a1%88%e4%be%8b aria-label=网上案例>网上案例</a></ul><li><a href=#%e5%86%85%e5%ad%98%e6%9f%a5%e7%9c%8b aria-label=内存查看>内存查看</a><li><a href=#reference aria-label=Reference>Reference</a></li></div></details></div></aside><script src=/js/pe-toc.min.445eb1bfc5e85dd13b9519fcc2a806522e9629b6224a2974052789ba00ab78af.js integrity="sha256-RF6xv8XoXdE7lRn8wqgGUi6WKbYiSil0BSeJugCreK8="></script><div class=post-content><p>本文记录了在使用 ceph 集群时遭遇到的内存问题，以及引用和参考一些资料用于对在 ceph 集群使用时的内存预估。</p><h2 id=osd的内存需求>OSD的内存需求<a hidden class=anchor aria-hidden=true href=#osd的内存需求>#</a></h2><p>如何评估 Ceph OSD 所需的硬件也是对于集群选型，集群优化的一个必要条件，这里主要找到两个可靠的参考资料用于评估 OSD 内存配置大小</p><h3 id=ibm-storage-ceph>IBM Storage Ceph<a hidden class=anchor aria-hidden=true href=#ibm-storage-ceph>#</a></h3><p>IBM Storage Ceph 提供了一个运行 Ceph 用于预估系统配置的一个最小推荐列表 <sup><a href=#1>[1]</a></sup>，个人感觉可以参考这些信息用于自己集群的优化。主要用于容器化的 Ceph 集群</p><table><thead><tr><th>Process</th><th>Criteria</th><th>Minimum Recommended</th></tr></thead><tbody><tr><td><em><strong>ceph-osd-container</strong></em></td><td>Processor</td><td>1x AMD64 or Intel 64 CPU CORE per OSD container</td></tr><tr><td></td><td>RAM</td><td>Minimum of 5 GB of RAM per OSD container</td></tr><tr><td></td><td>OS Disk</td><td>1x OS disk per host</td></tr><tr><td></td><td>OSD Storage</td><td>1x storage drive per OSD container. Cannot be shared with OS Disk.</td></tr><tr><td></td><td>block.db</td><td>Optional, but IBM recommended, 1x SSD or NVMe or Optane partition or lvm per daemon. Sizing is 4% of <code>block.data</code> for BlueStore for object, file, and mixed workloads and 1% of <code>block.data</code> for the BlueStore for Block Device, Openstack cinder, and Openstack cinder workloads.</td></tr><tr><td></td><td><code>block.wal</code></td><td>Optionally, 1x SSD or NVMe or Optane partition or logical volume per daemon. Use a small size, for example 10 GB, and only if it’s faster than the <code>block.db</code> device.</td></tr><tr><td></td><td>Network</td><td>2x 10 GB Ethernet NICs</td></tr><tr><td><em><strong>ceph-mon-container</strong></em></td><td>Processor</td><td>1x AMD64 or Intel 64 CPU CORE per mon-container</td></tr><tr><td></td><td>RAM</td><td>3 GB per <code>mon-container</code></td></tr><tr><td></td><td>Disk Space</td><td>10 GB per <code>mon-container</code>, 50 GB Recommended</td></tr><tr><td></td><td>Monitor Disk</td><td>Optionally, 1x SSD disk for <code>Monitor rocksdb</code> data</td></tr><tr><td></td><td>Network</td><td>2x 1 GB Ethernet NICs, 10 GB Recommended</td></tr><tr><td></td><td>Prometheus</td><td>20 GB to 50 GB under <code>/var/lib/ceph/</code> directory created as a separate file system to protect the contents under <code>/var/</code> directory.</td></tr><tr><td><em><strong>ceph-mgr-container</strong></em></td><td>Processor</td><td>1x AMD64 or Intel 64 CPU CORE per <code>mgr-container</code></td></tr><tr><td></td><td>RAM</td><td>3 GB per <code>mgr-container</code></td></tr><tr><td></td><td>Network</td><td>2x 1 GB Ethernet NICs, 10 GB Recommended</td></tr><tr><td><em><strong>ceph-radosgw-container</strong></em></td><td>Processor</td><td>1x AMD64 or Intel 64 CPU CORE per radosgw-container</td></tr><tr><td></td><td>RAM</td><td>1 GB per daemon</td></tr><tr><td></td><td>Disk Space</td><td>5 GB per daemon</td></tr><tr><td></td><td>Network</td><td>1x 1 GB Ethernet NICs</td></tr><tr><td><em><strong>ceph-mds-container</strong></em></td><td>Processor</td><td>1x AMD64 or Intel 64 CPU CORE per mds-container</td></tr><tr><td></td><td>RAM</td><td>3 GB per <code>mds-container</code> This number is highly dependent on the configurable MDS cache size. The RAM requirement is typically twice as much as the amount set in the <code>mds_cache_memory_limit</code> configuration setting. Note also that this is the memory for your daemon, not the overall system memory.</td></tr><tr><td></td><td>Disk Space</td><td>2 GB per <code>mds-container</code>, plus considering any additional space required for possible debug logging, 20 GB is a good start.</td></tr><tr><td></td><td>Network</td><td>2x 1 GB Ethernet NICs, 10 GB Recommended Note that this is the same network as the OSD containers. If you have a 10 GB network on your OSDs you should use the same on your MDS so that the MDS is not disadvantaged when it comes to latency.</td></tr></tbody></table><h3 id=hardware-recommendations>Hardware Recommendations<a hidden class=anchor aria-hidden=true href=#hardware-recommendations>#</a></h3><p>Ceph 官方也提供了相应的硬件配置推荐，关键参数写的比较清晰，但实际的规模比较模棱两可，也是可以提供一些参考的，并且每个版本的 Ceph 所推荐的硬件也是不相同的。</p><p>下表是 Ceph nautilus 的推荐最小硬件 <sup><a href=#2>[2]</a></sup></p><table><thead><tr><th>Process</th><th>Criteria</th><th>Minimum Recommended</th></tr></thead><tbody><tr><td><em><strong>ceph-osd</strong></em></td><td>Processor</td><td>1x 64-bit AMD-64 1x 32-bit ARM dual-core or better</td></tr><tr><td></td><td>RAM</td><td>~1GB for 1TB of storage per daemon</td></tr><tr><td></td><td>Volume Storage</td><td>1x storage drive per daemon</td></tr><tr><td></td><td>Journal</td><td>1x SSD partition per daemon (optional)</td></tr><tr><td></td><td>Network</td><td>2x 1GB Ethernet NICs</td></tr><tr><td><em><strong>ceph-mon</strong></em></td><td>Processor</td><td>1x 64-bit AMD-64 1x 32-bit ARM dual-core or better</td></tr><tr><td></td><td>RAM</td><td>1 GB per daemon</td></tr><tr><td></td><td>Disk Space</td><td>10 GB per daemon</td></tr><tr><td></td><td>Network</td><td>2x 1GB Ethernet NICs</td></tr><tr><td><em><strong>ceph-mds</strong></em></td><td>Processor</td><td>1x 64-bit AMD-64 quad-core 1x 32-bit ARM quad-core</td></tr><tr><td></td><td>RAM</td><td>1 GB minimum per daemon</td></tr><tr><td></td><td>Disk Space</td><td>1 MB per daemon</td></tr><tr><td></td><td>Network</td><td>2x 1GB Ethernet NICs</td></tr></tbody></table><p>下表是 reef 版本的官方推荐最小配置 <sup><a href=#3>[3]</a></sup></p><table><thead><tr><th>Process</th><th>Criteria</th><th>Bare Minimum and Recommended</th></tr></thead><tbody><tr><td><em><strong>ceph-osd</strong></em></td><td>Processor</td><td>1 core minimum, 2 recommended 1 core per 200-500 MB/s throughput 1 core per 1000-3000 IOPS Results are before replication. Results may vary across CPU and drive models and Ceph configuration: (erasure coding, compression, etc) ARM processors specifically may require more cores for performance. SSD OSDs, especially NVMe, will benefit from additional cores per OSD. Actual performance depends on many factors including drives, net, and client throughput and latency. Benchmarking is highly recommended.</td></tr><tr><td></td><td>RAM</td><td>4GB+ per daemon (more is better) 2-4GB may function but may be slow Less than 2GB is not recommended</td></tr><tr><td></td><td>Storage Drives</td><td>1x storage drive per OSD</td></tr><tr><td></td><td>DB/WAL (optional)</td><td>1x SSD partion per HDD OSD 4-5x HDD OSDs per DB/WAL SATA SSD &lt;= 10 HDD OSDss per DB/WAL NVMe SSD</td></tr><tr><td></td><td>Network</td><td>1x 1Gb/s (bonded 10+ Gb/s recommended)</td></tr><tr><td><em><strong>ceph-mon</strong></em></td><td>Processor</td><td>2 cores minimum</td></tr><tr><td></td><td>RAM</td><td>5GB+ per daemon (large / production clusters need more)</td></tr><tr><td></td><td>Storage</td><td>100 GB per daemon, SSD is recommended</td></tr><tr><td></td><td>Network</td><td>1x 1Gb/s (10+ Gb/s recommended)</td></tr><tr><td><em><strong>ceph-mds</strong></em></td><td>Processor</td><td>2 cores minimum</td></tr><tr><td></td><td>RAM</td><td>2GB+ per daemon (more for production)</td></tr><tr><td></td><td>Disk Space</td><td>1 GB per daemon</td></tr><tr><td></td><td>Network</td><td>1x 1Gb/s (10+ Gb/s recommended)</td></tr></tbody></table><h2 id=我们使用ceph环境的示例>我们使用Ceph环境的示例<a hidden class=anchor aria-hidden=true href=#我们使用ceph环境的示例>#</a></h2><p>用于 Openstack 环境的 Ceph OSD 使用内存记录，主要使用于RDB，机器配置为 1.8T, 900G 的混合硬盘，内存配置 512G， 可以看到 OSD 内存使用率在 0.3% 大概每个 OSD 使用内存量为 2GB。</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>PID USER      PR  NI    VIRT    RES    SHR S  %CPU    %MEM     TIME+     COMMAND
</span></span><span class=line><span class=cl><span class=m>225398</span> ceph   <span class=m>20</span>   <span class=m>0</span> <span class=m>3849464</span>   1.7g  <span class=m>22200</span> S   11.9    0.3    14501:14   ceph-osd    
</span></span><span class=line><span class=cl><span class=m>224860</span> ceph   <span class=m>20</span>   <span class=m>0</span> <span class=m>3612380</span>   1.7g  <span class=m>22424</span> S   9.2     0.3    12697:04   ceph-osd
</span></span><span class=line><span class=cl><span class=m>223902</span> ceph   <span class=m>20</span>   <span class=m>0</span> <span class=m>3340844</span>   1.7g  <span class=m>22172</span> S   8.6     0.3    21003:18   ceph-osd   
</span></span><span class=line><span class=cl><span class=m>223440</span> ceph   <span class=m>20</span>   <span class=m>0</span> <span class=m>3213884</span>   1.7g  <span class=m>22288</span> S   5.9     0.3     8548:00   ceph-osd
</span></span><span class=line><span class=cl><span class=m>224368</span> ceph   <span class=m>20</span>   <span class=m>0</span> <span class=m>3292848</span>   1.6g  <span class=m>22204</span> S   4.0     0.3     8655:56   ceph-osd     
</span></span><span class=line><span class=cl><span class=m>222889</span> ceph   <span class=m>20</span>   <span class=m>0</span> <span class=m>3231012</span>   1.7g  <span class=m>22180</span> S   3.3     0.3     8190:03   ceph-osd</span></span></code></pre></td></tr></table></div></div></div></div><p>用于业务使用的 Ceph OSD，主要用于对象存储，机器配置为 8c/16G，硬盘是 700G 每块，可以看到每个 OSD 使用的内存大概为 1.8-2G，大概 OSD 的分布是每个节点最多三个 OSD。</p><p>Ceph node 01</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># ceph node 01</span>
</span></span><span class=line><span class=cl>$ ps aux --sort<span class=o>=</span>-%mem <span class=p>|</span> head -10
</span></span><span class=line><span class=cl>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
</span></span><span class=line><span class=cl>ceph        <span class=m>1702</span>  0.4 27.9 <span class=m>10128296</span> <span class=m>4550760</span> ?    Ssl  May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1721</span>  0.6 12.8 <span class=m>3318456</span> <span class=m>2088704</span> ?     Ssl  May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>6</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1983</span>  0.6 12.3 <span class=m>3358788</span> <span class=m>2012844</span> ?     Ssl  May03 1273:25 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>3</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1991</span>  0.9 11.7 <span class=m>3451788</span> <span class=m>1912008</span> ?     Ssl  May03 1719:04 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>2</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1709</span>  0.5  7.4 <span class=m>1646276</span> <span class=m>1212576</span> ?     Ssl  May03 1047:48 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph       <span class=m>18979</span>  1.0  4.5 <span class=m>1330064</span> <span class=m>742680</span> ?      Ssl  May03 1932:51 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph      <span class=m>529617</span>  3.7  4.4 <span class=m>1909588</span> <span class=m>721492</span> ?      Ssl  Jul15 3140:39 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>root         <span class=m>801</span>  0.0  0.6 <span class=m>182536</span> <span class=m>98516</span> ?        Ss   May03 105:28 /usr/lib/systemd/systemd-journald
</span></span><span class=line><span class=cl>root        <span class=m>1704</span>  0.0  0.3 <span class=m>701284</span> <span class=m>50132</span> ?        Ssl  May03  53:48 /usr/sbin/rsyslogd -n</span></span></code></pre></td></tr></table></div></div></div></div><p>Ceph node02</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ps aux --sort<span class=o>=</span>-%mem <span class=p>|</span> head -10
</span></span><span class=line><span class=cl>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
</span></span><span class=line><span class=cl>ceph       <span class=m>28650</span>  1.4 12.8 <span class=m>3958988</span> <span class=m>2104296</span> ?     Ssl   <span class=m>2023</span> 6214:07 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>9</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph      <span class=m>163854</span>  1.4 12.7 <span class=m>3782156</span> <span class=m>2096396</span> ?     Ssl   <span class=m>2023</span> 6092:28 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>10</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph     <span class=m>3801660</span>  1.5 11.9 <span class=m>3389284</span> <span class=m>1959812</span> ?     Ssl  Jul10 1384:08 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>11</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>root     <span class=m>3348820</span>  0.1  0.1 <span class=m>510848</span> <span class=m>27732</span> ?        Sl   Jun27 171:24 /var/ossec/bin/wazuh-modulesd
</span></span><span class=line><span class=cl>root        <span class=m>1045</span>  0.0  0.1 <span class=m>574296</span> <span class=m>21468</span> ?        Ssl   <span class=m>2023</span>  85:44 /usr/bin/python2 -Es /usr/sbin/tuned -l -P
</span></span><span class=line><span class=cl>polkitd      <span class=m>670</span>  0.0  0.0 <span class=m>612348</span> <span class=m>14992</span> ?        Ssl   <span class=m>2023</span>  10:40 /usr/lib/polkit-1/polkitd --no-debug</span></span></code></pre></td></tr></table></div></div></div></div><p>Ceph node03</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ps aux --sort<span class=o>=</span>-%mem <span class=p>|</span> head -10
</span></span><span class=line><span class=cl>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
</span></span><span class=line><span class=cl>ceph     <span class=m>1942206</span>  0.9 12.8 <span class=m>4214720</span> <span class=m>2092280</span> ?     Ssl   <span class=m>2023</span> 7866:23 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>7</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>2824</span>  0.8 12.6 <span class=m>4274848</span> <span class=m>2051800</span> ?     Ssl   <span class=m>2022</span> 7205:58 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>4</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph     <span class=m>2802022</span>  0.7 12.5 <span class=m>3831320</span> <span class=m>2047440</span> ?     Ssl   <span class=m>2023</span> 4078:51 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>1</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1693</span>  0.7  4.7 <span class=m>1439428</span> <span class=m>771228</span> ?      Ssl   <span class=m>2022</span> 6767:46 /usr/bin/ceph-mon -f --cluster ceph --id node03 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph     <span class=m>1058494</span>  0.3  2.2 <span class=m>7492512</span> <span class=m>367288</span> ?      Ssl   <span class=m>2023</span> 3388:44 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node03 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph     <span class=m>1812870</span>  2.6  0.8 <span class=m>970928</span> <span class=m>133116</span> ?       Ssl   Mar21 6749:43 /usr/bin/ceph-mgr -f --cluster ceph --id node03 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>root         <span class=m>778</span>  0.0  0.1  <span class=m>76412</span> <span class=m>28084</span> ?        Ss    <span class=m>2022</span> 113:06 /usr/lib/systemd/systemd-journald
</span></span><span class=line><span class=cl>ceph        <span class=m>1739</span>  0.4  0.1 <span class=m>384760</span> <span class=m>28064</span> ?        Ssl   <span class=m>2022</span> 4086:33 /usr/bin/ceph-mds -f --cluster ceph --id node03 --setuser ceph --setgroup ceph</span></span></code></pre></td></tr></table></div></div></div></div><p>Ceph node04，该节点上只有一个 OSD</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ps aux --sort<span class=o>=</span>-%mem <span class=p>|</span> head -10
</span></span><span class=line><span class=cl>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
</span></span><span class=line><span class=cl>ceph       <span class=m>83779</span>  1.0 12.7 <span class=m>3911168</span> <span class=m>2087332</span> ?     Ssl  Jan23 3473:50 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>12</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>root        <span class=m>6568</span>  0.0  0.0 <span class=m>113020</span>  <span class=m>7808</span> ?        Ss   Jan22   0:00 /usr/sbin/sshd -D</span></span></code></pre></td></tr></table></div></div></div></div><h2 id=配置的一个需求>配置的一个需求<a hidden class=anchor aria-hidden=true href=#配置的一个需求>#</a></h2><p>osd 在运行在没有限制的情况下运行会消耗所有的可用内存，所以当数据节点配置不当，也会引起 oomkiller</p><blockquote><p>The OSDs are designed to consume all the available memory if they are run without limits. So it is recommended to apply the resource limits, and the OSDs will stay within the bounds you set. Typically 4GB is sufficient per OSD. <sup><a href=#4>[4]</a></sup></p></blockquote><p>当 OSD 经历恢复时，它们的内存利用率会达到峰值。如果可用的 RAM 不足，OSD 性能会显着降低，守护进程甚至可能崩溃或被 Linux OOM Killer杀死。<sup><a href=#5>[5]</a></sup></p><p>使用 cephadm 部署的机器群可以通过下面命令查看内存使用情况</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph orch ps</span></span></code></pre></td></tr></table></div></div></div></div><p>通常只有两种类型的守护进程有内存限制：mon 和 osd，这些内存限制参数由如下配置进行控制的</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo ceph config get mon mon_memory_target  <span class=c1># in bytes</span>
</span></span><span class=line><span class=cl>sudo ceph config get mon mon_memory_autotune
</span></span><span class=line><span class=cl>sudo ceph config get osd osd_memory_target  <span class=c1># in bytes</span>
</span></span><span class=line><span class=cl>sudo ceph config get osd osd_memory_target_autotune</span></span></code></pre></td></tr></table></div></div></div></div><p>通过 orch ps 查看的内存限制是不同于 ceph osd 的目标值的，BlueStore 将 OSD 堆内存使用量保留在指定目标大小下，并使用 <code>osd_memory_target</code> 配置选项。</p><blockquote><p>选项 <code>osd_memory_target</code> 根据系统中可用的 RAM 来设置 OSD 内存。当 TCMalloc 配置为内存分配器，BlueStore 中的 <code>bluestore_cache_autotune</code> 选项设为 <code>true</code> 时，则使用此选项。</p></blockquote><p>查看现有集群 osd 的配置</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 显示存储集群中的所有 OSD osd_memory_target</span>
</span></span><span class=line><span class=cl>sudo ceph config get osd osd_memory_target
</span></span><span class=line><span class=cl><span class=c1># 显示指定 OSD osd_memory_target</span>
</span></span><span class=line><span class=cl>sudo ceph config get osd.0 osd_memory_target</span></span></code></pre></td></tr></table></div></div></div></div><p>配置集群 OSD <code>osd_memory_target</code></p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 为存储集群中的所有 OSD 设置 osd_memory_target</span>
</span></span><span class=line><span class=cl>ceph config <span class=nb>set</span> osd osd_memory_target VALUE
</span></span><span class=line><span class=cl><span class=c1># 为存储集群中的指定 OSD 设置 osd_memory_target，.id 是 OSD 的 ID </span>
</span></span><span class=line><span class=cl>ceph config <span class=nb>set</span> osd.id osd_memory_target VALUE</span></span></code></pre></td></tr></table></div></div></div></div><h3 id=网上案例>网上案例<a hidden class=anchor aria-hidden=true href=#网上案例>#</a></h3><p>下面有两个网上搜到的案例，osd具有无限制的内存增长的案例</p><ul><li>osd(s) with unlimited ram growth <sup><a href=#6>[6]</a></sup></li><li>How to solve “the Out of Memory Killer issue that kills your OSDs due to bad entries in PG logs” <sup><a href=#7>[7]</a></sup></li></ul><h2 id=内存查看>内存查看<a hidden class=anchor aria-hidden=true href=#内存查看>#</a></h2><p>使用统计命令，该命令的统计信息不需要运行探查器，也不会将堆分配信息转储到文件中。</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph tell osd.0 heap stats</span></span></code></pre></td></tr></table></div></div></div></div><p>使用内存池命令</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ceph daemon osd.NNN dump_mempools</span></span></code></pre></td></tr></table></div></div></div></div><p>使用 google-perftools，该命令会运行探针，来检测运行的命令</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>google-pprof --text <span class=o>{</span>path-to-daemon<span class=o>}</span>  <span class=o>{</span>log-path/filename<span class=o>}</span>
</span></span><span class=line><span class=cl><span class=c1># 例如</span>
</span></span><span class=line><span class=cl>pprof --text /usr/bin/ceph-mon /var/log/ceph/mon.node1.profile.0001.heap</span></span></code></pre></td></tr></table></div></div></div></div><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p><sup id=1>[1]</sup> <a href="https://www.ibm.com/docs/en/storage-ceph/7?topic=recommendations-minimum-hardware-considerations" target=_blank rel="noopener nofollow noreferrer">Minimum hardware considerations</a></p><p><sup id=2>[2]</sup> <a href=https://web.archive.org/web/20240914160430/https://docs.ceph.com/en/nautilus/start/hardware-recommendations/#minimum-hardware-recommendations target=_blank rel="noopener nofollow noreferrer">minimum-hardware-recommendations nautilus</a></p><p><sup id=3>[3]</sup> <a href=https://web.archive.org/web/20240914160933/https://docs.ceph.com/en/reef/start/hardware-recommendations/#minimum-hardware-recommendations target=_blank rel="noopener nofollow noreferrer">minimum-hardware-recommendations reef</a></p><p><sup id=4>[4]</sup> <a href=https://github.com/rook/rook/issues/12078 target=_blank rel="noopener nofollow noreferrer">Excessive OSD memory usage #12078</a></p><p><sup id=5>[5]</sup> <a href=https://web.archive.org/web/20240915061444/https://www.cnblogs.com/varden/p/15949938.html target=_blank rel="noopener nofollow noreferrer">Ceph OSD 故障排除之内存不足</a></p><p><sup id=6>[6]</sup> <a href=https://docs.clyso.com/blog/osds-with-unlimited-ram-growth/ target=_blank rel="noopener nofollow noreferrer">osd(s) with unlimited ram growth</a></p><p><sup id=7>[7]</sup> <a href=https://croit.io/blog/oom-killer-osds target=_blank rel="noopener nofollow noreferrer">How to solve “the Out of Memory Killer issue that kills your OSDs due to bad entries in PG logs”</a></p><p><sup id=8>[8]</sup> <a href=https://docs.ceph.com/en/reef/rados/troubleshooting/memory-profiling/ target=_blank rel="noopener nofollow noreferrer">Memory Profiling</a></p></div><div class=pe-copyright><hr><blockquote><p>本文为原创内容，版权归作者所有。如需转载，请在文章中声明本文标题及链接。</p><p>文章标题：Ceph OSD内存优化与建议</p><p>文章链接：<a href=http://localhost:1313/2024/09/03-3-ceph-osd-performance-recommendation/ target=_blank>http://localhost:1313/2024/09/03-3-ceph-osd-performance-recommendation/</a></p><p>许可协议：<a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></blockquote></div><div class=comments-separator></div><h3 class=relatedContentTitle>相关阅读</h3><ul class=relatedContent><li><a href=/2024/09/05-5-failed-troubleshooting-for-rgw/><span>记录一次失败的radosgw问题排查记录</span></a></li><li><a href=/2020/09/alpine-trouble-q-and-a/><span>使用alpine为基础镜像Q&amp;A</span></a></li><li><a href=/2020/09/envoy-example-failed/><span>envoy官方example运行失败问题处理</span></a></li><li><a href=/2024/02/10-2-troubeshooting-crash/><span>记录一次ceph集群故障处理记录</span></a></li><li><a href=/2023/11/10-1-ceph-fscache/><span>当cephfs和fscache结合时在K8s环境下的全集群规模故障</span></a></li></ul><div class=comments-separator></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/storage/>Storage</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/2024/09/nacos-deploy-with-gcp-eks/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></polyline></svg>&nbsp;</span>
<span>Kubernetes公有云集群中部署Nacos集群</span>
</a><a class=next href=http://localhost:1313/2024/09/05-5-failed-troubleshooting-for-rgw/><span class=title></span>
<span>记录一次失败的radosgw问题排查记录&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span></a></nav></footer><div class=pe-comments-decoration><p class=pe-comments-title></p><p class=pe-comments-subtitle></p></div><div id=pe-comments></div><script src=/js/pe-go-comment.min.86a214102576ba5f9b7bdc29eed8d58dd56e34aef80b3c65c73ea9cc88443696.js integrity="sha256-hqIUECV2ul+be9wp7tjVjdVuNK74Czxlxz6pzIhENpY="></script><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="dark"?"dark":"light",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"cylonchau/cylonchau.github.io","data-repo-id":"R_kgDOIRlNSQ","data-category":"Announcements","data-category-id":"DIC_kwDOIRlNSc4CXy1U","data-mapping":"pathname","data-term":"posts/03-3 ceph osd performance recommendation","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":getStoredTheme(),"data-lang":"zh-TW","data-loading":"lazy",crossorigin:"anonymous",async:""},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#pe-comments").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Cylon's Collection</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> on
<a href=https://pages.github.com/ rel=noopener target=_blank>GitHub Pages</a> & Theme
        <a href=https://github.com/tofuwine/PaperMod-PE rel=noopener target=_blank>PaperMod-PE</a></span></footer><div class=pe-right-sidebar><a href=javascript:void(0); id=theme-toggle-float class=pe-float-btn><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a><a href=#top class=pe-float-btn id=top-link><span id=pe-read-progress></span></a></div><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>