<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>记录一次失败的radosgw问题排查记录 | Cylon's Collection</title>
<meta name=keywords content="troubleshooting,object storage"><meta name=description content="记录一次因着急没有检查原因而直接下线 ceph 对象存储的的失败记录
操作流程 ceph 节点内存持续超过90%，因为本身有三个 OSD，检查内存使用情况发现 radosgw
bash 1 2 3 4 5 6 7 8 9 10 11 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1702 0.4 32.9 10128296 4550760 ? Ssl May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph ceph 1721 0.6 12.8 3318456 2088704 ? Ssl May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph ceph 1983 0."><meta name=author content="cylon"><link rel=canonical href=https://www.oomkill.com/2024/09/05-5-failed-troubleshooting-for-rgw/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://www.oomkill.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.oomkill.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.oomkill.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.oomkill.com/favicon.ico><link rel=mask-icon href=https://www.oomkill.com/favicon.ico><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://www.oomkill.com/2024/09/05-5-failed-troubleshooting-for-rgw/><noscript><style>#theme-toggle,#top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link crossorigin=anonymous href=/assets/css/pe.min.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/pe.min.js></script><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdn.staticfile.net/font-awesome/6.5.1/css/v4-shims.min.css><script id=MathJax-script async src=https://cdn.staticfile.net/mathjax/3.2.2/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["$$","$$"]],inlineMath:[["\\$","\\$"]]}}</script><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><meta name=referrer content="no-referrer-when-downgrade"><meta property="og:title" content="记录一次失败的radosgw问题排查记录"><meta property="og:description" content="记录一次因着急没有检查原因而直接下线 ceph 对象存储的的失败记录
操作流程 ceph 节点内存持续超过90%，因为本身有三个 OSD，检查内存使用情况发现 radosgw
bash 1 2 3 4 5 6 7 8 9 10 11 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1702 0.4 32.9 10128296 4550760 ? Ssl May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph ceph 1721 0.6 12.8 3318456 2088704 ? Ssl May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph ceph 1983 0."><meta property="og:type" content="article"><meta property="og:url" content="https://www.oomkill.com/2024/09/05-5-failed-troubleshooting-for-rgw/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-12T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-12T23:10:36+08:00"><meta property="og:site_name" content="Cylon's Collection"><meta name=twitter:card content="summary"><meta name=twitter:title content="记录一次失败的radosgw问题排查记录"><meta name=twitter:description content="记录一次因着急没有检查原因而直接下线 ceph 对象存储的的失败记录
操作流程 ceph 节点内存持续超过90%，因为本身有三个 OSD，检查内存使用情况发现 radosgw
bash 1 2 3 4 5 6 7 8 9 10 11 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1702 0.4 32.9 10128296 4550760 ? Ssl May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph ceph 1721 0.6 12.8 3318456 2088704 ? Ssl May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph ceph 1983 0."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.oomkill.com/posts/"},{"@type":"ListItem","position":2,"name":"记录一次失败的radosgw问题排查记录","item":"https://www.oomkill.com/2024/09/05-5-failed-troubleshooting-for-rgw/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"记录一次失败的radosgw问题排查记录","name":"记录一次失败的radosgw问题排查记录","description":"记录一次因着急没有检查原因而直接下线 ceph 对象存储的的失败记录\n操作流程 ceph 节点内存持续超过90%，因为本身有三个 OSD，检查内存使用情况发现 radosgw\nbash 1 2 3 4 5 6 7 8 9 10 11 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1702 0.4 32.9 10128296 4550760 ? Ssl May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph ceph 1721 0.6 12.8 3318456 2088704 ? Ssl May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph ceph 1983 0.","keywords":["troubleshooting","object storage"],"articleBody":"记录一次因着急没有检查原因而直接下线 ceph 对象存储的的失败记录\n操作流程 ceph 节点内存持续超过90%，因为本身有三个 OSD，检查内存使用情况发现 radosgw\nbash 1 2 3 4 5 6 7 8 9 10 11 $ ps aux --sort=-%mem | head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1702 0.4 32.9 10128296 4550760 ? Ssl May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph ceph 1721 0.6 12.8 3318456 2088704 ? Ssl May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph ceph 1983 0.6 12.3 3358788 2012844 ? Ssl May03 1273:25 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph ceph 1991 0.9 11.7 3451788 1912008 ? Ssl May03 1719:04 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph ceph 1709 0.5 7.4 1646276 1212576 ? Ssl May03 1047:48 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph ceph 18979 1.0 4.5 1330064 742680 ? Ssl May03 1932:51 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph ceph 529617 3.7 4.4 1909588 721492 ? Ssl Jul15 3140:39 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph root 801 0.0 0.6 182536 98516 ? Ss May03 105:28 /usr/lib/systemd/systemd-journald root 1704 0.0 0.3 701284 50132 ? Ssl May03 53:48 /usr/sbin/rsyslogd -n 因为这台节点包含3个 OSD, ceph-mon, ceph-mds 等全功能使用，所以最初的想法是 radosgw 转移到其他节点上，而不是分析为什么 radosgw 进程使用内存较高\n申请一个新节点部署 radosgw，部署时出现错误没有提示日志\nbash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $ ceph-deploy rgw create node06 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy rgw create node06 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] rgw : [('node06', 'rgw.node06')] eph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] subcommand : create [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] func : \u003cfunction rgw at 0x7fe8aa412050\u003e [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts node06:rgw.node06 Warning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts. [node06][DEBUG ] connection detected need for sudo Warning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts. We trust you have received the usual lecture from the local System Administrator. It usually boils down to these three things: #1) Respect the privacy of others. #2) Think before you type. #3) With great power comes great responsibility. sudo: no tty present and no askpass program specified [ceph_deploy.rgw][ERROR ] connecting to host: node06 resulted in errors: IOError cannot send (already closed?) [ceph_deploy][ERROR ] GenericError: Failed to create 1 RGWs 通过 journalctl -u 查看到如下错误 (新节点)\nbash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Sep 12 14:51:42 node06 sshd[30495]: error: Could not load host key: /etc/ssh/ssh_host_dsa_key Sep 12 14:51:42 node06 sshd[30495]: Accepted publickey for ceph from 192.168.20.88 port 37872 ssh2: RSA SHA256:XBYUcCiYBhdw+V32qwx6x0wex1EhaMiSHuz0gQVayTQ Sep 12 14:51:42 node06 systemd[1]: Created slice User Slice of ceph. Sep 12 14:51:42 node06 systemd-logind[743]: New session 97 of user ceph. Sep 12 14:51:42 node06 systemd[1]: Started Session 97 of user ceph. Sep 12 14:51:42 node06 sshd[30495]: pam_unix(sshd:session): session opened for user ceph by (uid=0) Sep 12 14:51:42 node06 sshd[30497]: Received disconnect from 192.168.20.88 port 37872:11: disconnected by user Sep 12 14:51:42 node06 sshd[30497]: Disconnected from 192.168.20.88 port 37872 Sep 12 14:51:42 node06 sshd[30495]: pam_unix(sshd:session): session closed for user ceph Sep 12 14:51:42 node06 systemd-logind[743]: Removed session 97. Sep 12 14:51:42 node06 systemd[1]: Removed slice User Slice of ceph. Sep 12 14:51:42 node06 sshd[30521]: error: Could not load host key: /etc/ssh/ssh_host_dsa_key Sep 12 14:51:42 node06 sshd[30521]: Accepted publickey for ceph from 192.168.20.88 port 37874 ssh2: RSA SHA256:XBYUcCiYBhdw+V32qwx6x0wex1EhaMiSHuz0gQVayTQ Sep 12 14:51:42 node06 systemd[1]: Created slice User Slice of ceph. Sep 12 14:51:42 node06 systemd-logind[743]: New session 98 of user ceph. Sep 12 14:51:42 node06 systemd[1]: Started Session 98 of user ceph. Sep 12 14:51:42 node06 sshd[30521]: pam_unix(sshd:session): session opened for user ceph by (uid=0) Sep 12 14:51:42 node06 sudo[30526]: pam_unix(sudo:auth): conversation failed Sep 12 14:51:42 node06 sudo[30526]: pam_unix(sudo:auth): auth could not identify password for [ceph] Sep 12 14:51:45 node06 sudo[30526]: ceph : user NOT in sudoers ; TTY=unknown ; PWD=/home/ceph ; USER=root ; COMMAND=/bin/python2 -c import sys;exec(eval(sys.stdin.readline())) Sep 12 14:51:45 node06 sshd[30525]: Received disconnect from 192.168.20.88 port 37874:11: disconnected by user Sep 12 14:51:45 node06 sshd[30525]: Disconnected from 192.168.20.88 port 37874 Sep 12 14:51:45 node06 sshd[30521]: pam_unix(sshd:session): session closed for user ceph Sep 12 14:51:45 node06 postfix/sendmail[30549]: fatal: parameter inet_interfaces: no local interface found for ::1 配置 sudo\nbash 1 2 #cat /etc/sudoers.d/ceph ceph ALL = (root) NOPASSWD:ALL 配置完成后部署 radosgw\nbash 1 2 3 4 5 6 # 拷贝配置文件 ceph-deploy --overwrite-conf config push node06 # 安装软件包 ceph-deploy install --no-adjust-repos --nogpgcheck node06 # new一个新 rgw 实例，ceph-deploy 只支持new ceph-deploy rgw create node06 完整的输出\nbash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 $ ceph-deploy --overwrite-conf config push node06 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy --overwrite-conf config push node06 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] overwrite_conf : True [ceph_deploy.cli][INFO ] subcommand : push [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] client : ['node06'] [ceph_deploy.cli][INFO ] func : \u003cfunction config at 0x7f3e96ec9c08\u003e [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.config][DEBUG ] Pushing config to node06 Warning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts. [node06][DEBUG ] connection detected need for sudo Warning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts. [node06][DEBUG ] connected to host: node06 [node06][DEBUG ] detect platform information from remote host [node06][DEBUG ] detect machine type [node06][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf $ ceph-deploy install --no-adjust-repos --nogpgcheck node06 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph/.cephdeploy.conf [ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy install --no-adjust-repos --nogpgcheck node06 [ceph_deploy.cli][INFO ] ceph-deploy options: [ceph_deploy.cli][INFO ] verbose : False [ceph_deploy.cli][INFO ] testing : None [ceph_deploy.cli][INFO ] cd_conf : [ceph_deploy.cli][INFO ] cluster : ceph [ceph_deploy.cli][INFO ] dev_commit : None [ceph_deploy.cli][INFO ] install_mds : False [ceph_deploy.cli][INFO ] stable : None [ceph_deploy.cli][INFO ] default_release : False [ceph_deploy.cli][INFO ] username : None [ceph_deploy.cli][INFO ] adjust_repos : False [ceph_deploy.cli][INFO ] func : \u003cfunction install at 0x7fc5041125f0\u003e [ceph_deploy.cli][INFO ] install_mgr : False [ceph_deploy.cli][INFO ] install_all : False [ceph_deploy.cli][INFO ] repo : False [ceph_deploy.cli][INFO ] host : ['node06'] [ceph_deploy.cli][INFO ] install_rgw : False [ceph_deploy.cli][INFO ] install_tests : False [ceph_deploy.cli][INFO ] repo_url : None [ceph_deploy.cli][INFO ] ceph_conf : None [ceph_deploy.cli][INFO ] install_osd : False [ceph_deploy.cli][INFO ] version_kind : stable [ceph_deploy.cli][INFO ] install_common : False [ceph_deploy.cli][INFO ] overwrite_conf : False [ceph_deploy.cli][INFO ] quiet : False [ceph_deploy.cli][INFO ] dev : master [ceph_deploy.cli][INFO ] nogpgcheck : True [ceph_deploy.cli][INFO ] local_mirror : None [ceph_deploy.cli][INFO ] release : None [ceph_deploy.cli][INFO ] install_mon : False [ceph_deploy.cli][INFO ] gpg_url : None [ceph_deploy.install][DEBUG ] Installing stable version mimic on cluster ceph hosts node06 [ceph_deploy.install][DEBUG ] Detecting platform for host node06 ... Warning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts. [node06][DEBUG ] connection detected need for sudo Warning: Permanently added '[node06]:55556,[192.168.20.88]:55556' (ECDSA) to the list of known hosts. [node06][DEBUG ] connected to host: node06 [node06][DEBUG ] detect platform information from remote host [node06][DEBUG ] detect machine type [ceph_deploy.install][INFO ] Distro info: CentOS Linux 7.9.2009 Core [node06][INFO ] installing Ceph on node06 [node06][INFO ] Running command: sudo yum clean all [node06][DEBUG ] Loaded plugins: fastestmirror [node06][DEBUG ] Cleaning repos: base centos-sclo-rh centos-sclo-sclo devops-Extra epel extras [node06][DEBUG ] : openresty remi-php72 remi-php73 remi-php74 remi-safe salt-latest [node06][DEBUG ] : tools-repo updates zabbix [node06][DEBUG ] Cleaning up list of fastest mirrors [node06][DEBUG ] Other repos take up 23 M of disk space (use --verbose for details) [node06][INFO ] Running command: sudo yum -y install ceph ceph-radosgw [node06][DEBUG ] Loaded plugins: fastestmirror [node06][DEBUG ] Determining fastest mirrors [node06][DEBUG ] No package ceph available. [node06][DEBUG ] Nothing to do [node06][INFO ] Running command: sudo ceph --version [node06][DEBUG ] ceph version 14.2.22 (ca74598065096e6fcbd8433c8779a2be0c889351) nautilus (stable) 查看节点是否上线\nbash 1 2 3 4 5 6 7 8 9 10 11 12 $ ceph -s cluster: id: baf87797-3ec1-4f2c-8126-bf0a44051b13 health: HEALTH_WARN 1 pools have many more objects per pg than average services: mon: 3 daemons, quorum node01,node02,node03 (age 2w) mgr: node01(active, since 8w), standbys: node02, node03 mds: kubefs:2 {0=node01=up:active,1=node02=up:active} 1 up:standby osd: 13 osds: 13 up (since 6d), 13 in (since 7M) rgw: 4 daemons active (node01, node02, node03, node06) 流量的请求时访问 radosgw 服务，这个时候新实例是没有引入流量的，需要修改负载均衡器增加新的节点进来，流量引入后需要确认旧服务已经不在处理业务请求后可以下线 确认请求，查看活跃连接\nbash 1 2 3 $ netstat -an|grep 7480 tcp 0 0 0.0.0.0:7480 0.0.0.0:* LISTEN tcp 0 0 192.168.20.84:7480 192.168.20.84:33152 ESTABLISHED 确认请求，查看服务日志\nbash 1 $ tail -f /var/log/ceph/ceph-client.rgw.node01.log 确认无误可以下线，ceph-deploy 部署的服务没有 cephadm ceph orch rgw delete xx 这类工具进行下线，直接通过 systemd 停止服务即可\nbash 1 2 3 4 $ systemctl -l|grep rados ceph-radosgw@rgw.node01.service loaded active running Ceph rados gateway system-ceph\\x2dradosgw.slice loaded active active system-ceph\\x2dradosgw.slice ceph-radosgw.target loaded active active ceph target allowing to start/stop all ceph-radosgw@.service instances at once 停止服务并检查内存状态\nbash 1 2 3 4 5 6 7 8 9 $ systemctl stop ceph-radosgw@rgw.node01.service #ps axu --sort=-%mem|head -10 USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1983 0.6 12.8 3358788 2084324 ? Ssl May03 1275:36 /usr/bin/ceph-osd -f --cluster ceph --id 3 --setuser ceph --setgroup ceph ceph 1991 0.9 12.5 3451788 2033560 ? Ssl May03 1722:12 /usr/bin/ceph-osd -f --cluster ceph --id 2 --setuser ceph --setgroup ceph ceph 1721 0.6 11.8 3318456 1920876 ? Ssl May03 1219:27 /usr/bin/ceph-osd -f --cluster ceph --id 6 --setuser ceph --setgroup ceph ceph 1709 0.5 7.4 1646276 1212516 ? Ssl May03 1050:21 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph ceph 18979 1.0 4.5 1330064 744972 ? Ssl May03 1937:16 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph ceph 529617 3.7 4.4 1914452 726436 ? Ssl Jul15 3153:14 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph 总结 本次操作没有分析为什么使用内存高，只是着急做了迁移，这样导致在事后无法确定问题的根本原因，后期遇到问题要先分析并保留证据，其次在做迁移之类动作。\n","wordCount":"1707","inLanguage":"zh","datePublished":"2024-09-12T00:00:00Z","dateModified":"2024-09-12T23:10:36+08:00","author":{"@type":"Person","name":"cylon"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.oomkill.com/2024/09/05-5-failed-troubleshooting-for-rgw/"},"publisher":{"@type":"Organization","name":"Cylon's Collection","logo":{"@type":"ImageObject","url":"https://www.oomkill.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.oomkill.com/><img src=https://www.oomkill.com/favicon.ico alt aria-label=logo height=20>Cylon's Collection</a><div class=logo-switches><button id=theme-toggle><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.oomkill.com/archives><span>归档</span></a></li><li><a href=https://www.oomkill.com/tags><span>标签</span></a></li><li><a href=https://www.oomkill.com/search><span>搜索</span></a></li><li><a href=https://www.oomkill.com/about accesskey=/><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">记录一次失败的radosgw问题排查记录</h1><div class=post-meta><span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar" style="user-select:text"><rect x="3" y="4" width="18" height="18" rx="2" ry="2" style="user-select:text"/><line x1="16" y1="2" x2="16" y2="6" style="user-select:text"/><line x1="8" y1="2" x2="8" y2="6" style="user-select:text"/><line x1="3" y1="10" x2="21" y2="10" style="user-select:text"/></svg><span>2024-09-12</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text" style="user-select:text"><path d="M14 2H6A2 2 0 004 4v16a2 2 0 002 2h12a2 2 0 002-2V8z" style="user-select:text"/><polyline points="14 2 14 8 20 8" style="user-select:text"/><line x1="16" y1="13" x2="8" y2="13" style="user-select:text"/><line x1="16" y1="17" x2="8" y2="17" style="user-select:text"/><polyline points="10 9 9 9 8 9" style="user-select:text"/></svg><span>1707 字</span></span>&nbsp;·&nbsp;<span class=pe-post-meta-item><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke="currentcolor" stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><span>9 分钟</span></span>
<span class=pe-post-meta-item>&nbsp;·&nbsp;<svg t="1714036239378" fill="currentcolor" class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" p-id="6659" width="256" height="256"><path d="M690 78.2c-18.6-18.8-49-19-67.8-.4s-19 49-.4 67.8l255.4 258.6c67.8 68.6 67.8 178.8.0 247.4L653.4 878.2c-18.6 18.8-18.4 49.2.4 67.8s49.2 18.4 67.8-.4l224-226.4c104.8-106 104.8-276.4.0-382.4L690 78.2zM485.4 101.4c-24-24-56.6-37.4-90.6-37.4H96C43 64 0 107 0 160v299c0 34 13.4 66.6 37.4 90.6l336 336c50 50 131 50 181 0l267-267c50-50 50-131 0-181l-336-336zM96 160h299c8.4.0 16.6 3.4 22.6 9.4l336 336c12.4 12.4 12.4 32.8.0 45.2l-267 267c-12.4 12.4-32.8 12.4-45.2.0l-336-336c-6-6-9.4-14.2-9.4-22.6V160zm192 128a64 64 0 10-128 0 64 64 0 10128 0z" p-id="6660"/></svg></span><ul class=pe-post-meta-item><a href=https://www.oomkill.com/tags/storage/>#Storage</a></ul>&nbsp;·&nbsp;<span id=busuanzi_container_page_pv>本文阅读量 <span id=busuanzi_value_page_pv></span> 次</span></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details><summary><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e6%93%8d%e4%bd%9c%e6%b5%81%e7%a8%8b aria-label=操作流程>操作流程</a><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></div></details></div></aside><script src=/js/pe-toc.min.445eb1bfc5e85dd13b9519fcc2a806522e9629b6224a2974052789ba00ab78af.js integrity="sha256-RF6xv8XoXdE7lRn8wqgGUi6WKbYiSil0BSeJugCreK8="></script><div class=post-content><p>记录一次因着急没有检查原因而直接下线 ceph 对象存储的的失败记录</p><h2 id=操作流程>操作流程<a hidden class=anchor aria-hidden=true href=#操作流程>#</a></h2><p>ceph 节点内存持续超过90%，因为本身有三个 OSD，检查内存使用情况发现 radosgw</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ps aux --sort<span class=o>=</span>-%mem <span class=p>|</span> head -10
</span></span><span class=line><span class=cl>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
</span></span><span class=line><span class=cl>ceph        <span class=m>1702</span>  0.4 32.9 <span class=m>10128296</span> <span class=m>4550760</span> ?    Ssl  May03 919:18 /usr/bin/radosgw -f --cluster ceph --name client.rgw.node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1721</span>  0.6 12.8 <span class=m>3318456</span> <span class=m>2088704</span> ?     Ssl  May03 1216:59 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>6</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1983</span>  0.6 12.3 <span class=m>3358788</span> <span class=m>2012844</span> ?     Ssl  May03 1273:25 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>3</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1991</span>  0.9 11.7 <span class=m>3451788</span> <span class=m>1912008</span> ?     Ssl  May03 1719:04 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>2</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1709</span>  0.5  7.4 <span class=m>1646276</span> <span class=m>1212576</span> ?     Ssl  May03 1047:48 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph       <span class=m>18979</span>  1.0  4.5 <span class=m>1330064</span> <span class=m>742680</span> ?      Ssl  May03 1932:51 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph      <span class=m>529617</span>  3.7  4.4 <span class=m>1909588</span> <span class=m>721492</span> ?      Ssl  Jul15 3140:39 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>root         <span class=m>801</span>  0.0  0.6 <span class=m>182536</span> <span class=m>98516</span> ?        Ss   May03 105:28 /usr/lib/systemd/systemd-journald
</span></span><span class=line><span class=cl>root        <span class=m>1704</span>  0.0  0.3 <span class=m>701284</span> <span class=m>50132</span> ?        Ssl  May03  53:48 /usr/sbin/rsyslogd -n</span></span></code></pre></td></tr></table></div></div></div></div><p>因为这台节点包含3个 OSD, ceph-mon, ceph-mds 等全功能使用，所以最初的想法是 radosgw 转移到其他节点上，而不是分析为什么 radosgw 进程使用内存较高</p><p>申请一个新节点部署 radosgw，部署时出现错误没有提示日志</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ceph-deploy rgw create node06
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.conf<span class=o>][</span>DEBUG <span class=o>]</span> found configuration file at: /home/ceph/.cephdeploy.conf
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span> Invoked <span class=o>(</span>2.0.1<span class=o>)</span>: /bin/ceph-deploy rgw create node06
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span> ceph-deploy options:
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  username                      : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  verbose                       : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  rgw                           : <span class=o>[(</span><span class=s1>&#39;node06&#39;</span>, <span class=s1>&#39;rgw.node06&#39;</span><span class=o>)]</span>
</span></span><span class=line><span class=cl>  eph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  overwrite_conf                : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  subcommand                    : create
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  quiet                         : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe8a9b583f8&gt;
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  cluster                       : ceph
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  func                          : &lt;<span class=k>function</span> rgw at 0x7fe8aa412050&gt;
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  ceph_conf                     : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  default_release               : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.rgw<span class=o>][</span>DEBUG <span class=o>]</span> Deploying rgw, cluster ceph hosts node06:rgw.node06
</span></span><span class=line><span class=cl>Warning: Permanently added <span class=s1>&#39;[node06]:55556,[192.168.20.88]:55556&#39;</span> <span class=o>(</span>ECDSA<span class=o>)</span> to the list of known hosts.
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> connection detected need <span class=k>for</span> sudo
</span></span><span class=line><span class=cl>Warning: Permanently added <span class=s1>&#39;[node06]:55556,[192.168.20.88]:55556&#39;</span> <span class=o>(</span>ECDSA<span class=o>)</span> to the list of known hosts.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>We trust you have received the usual lecture from the <span class=nb>local</span> System
</span></span><span class=line><span class=cl>Administrator. It usually boils down to these three things:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>#1) Respect the privacy of others.</span>
</span></span><span class=line><span class=cl>    <span class=c1>#2) Think before you type.</span>
</span></span><span class=line><span class=cl>    <span class=c1>#3) With great power comes great responsibility.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>sudo: no tty present and no askpass program specified
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.rgw<span class=o>][</span>ERROR <span class=o>]</span> connecting to host: node06 resulted in errors: IOError cannot send <span class=o>(</span>already closed?<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy<span class=o>][</span>ERROR <span class=o>]</span> GenericError: Failed to create <span class=m>1</span> RGWs</span></span></code></pre></td></tr></table></div></div></div></div><p>通过 journalctl -u 查看到如下错误 (新节点)</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sshd<span class=o>[</span>30495<span class=o>]</span>: error: Could not load host key: /etc/ssh/ssh_host_dsa_key
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sshd<span class=o>[</span>30495<span class=o>]</span>: Accepted publickey <span class=k>for</span> ceph from 192.168.20.88 port <span class=m>37872</span> ssh2: RSA SHA256:XBYUcCiYBhdw+V32qwx6x0wex1EhaMiSHuz0gQVayTQ
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 systemd<span class=o>[</span>1<span class=o>]</span>: Created slice User Slice of ceph.
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 systemd-logind<span class=o>[</span>743<span class=o>]</span>: New session <span class=m>97</span> of user ceph.
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 systemd<span class=o>[</span>1<span class=o>]</span>: Started Session <span class=m>97</span> of user ceph.
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sshd<span class=o>[</span>30495<span class=o>]</span>: pam_unix<span class=o>(</span>sshd:session<span class=o>)</span>: session opened <span class=k>for</span> user ceph by <span class=o>(</span><span class=nv>uid</span><span class=o>=</span>0<span class=o>)</span>
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sshd<span class=o>[</span>30497<span class=o>]</span>: Received disconnect from 192.168.20.88 port 37872:11: disconnected by user
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sshd<span class=o>[</span>30497<span class=o>]</span>: Disconnected from 192.168.20.88 port <span class=m>37872</span>
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sshd<span class=o>[</span>30495<span class=o>]</span>: pam_unix<span class=o>(</span>sshd:session<span class=o>)</span>: session closed <span class=k>for</span> user ceph
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 systemd-logind<span class=o>[</span>743<span class=o>]</span>: Removed session 97.
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 systemd<span class=o>[</span>1<span class=o>]</span>: Removed slice User Slice of ceph.
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sshd<span class=o>[</span>30521<span class=o>]</span>: error: Could not load host key: /etc/ssh/ssh_host_dsa_key
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sshd<span class=o>[</span>30521<span class=o>]</span>: Accepted publickey <span class=k>for</span> ceph from 192.168.20.88 port <span class=m>37874</span> ssh2: RSA SHA256:XBYUcCiYBhdw+V32qwx6x0wex1EhaMiSHuz0gQVayTQ
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 systemd<span class=o>[</span>1<span class=o>]</span>: Created slice User Slice of ceph.
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 systemd-logind<span class=o>[</span>743<span class=o>]</span>: New session <span class=m>98</span> of user ceph.
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 systemd<span class=o>[</span>1<span class=o>]</span>: Started Session <span class=m>98</span> of user ceph.
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sshd<span class=o>[</span>30521<span class=o>]</span>: pam_unix<span class=o>(</span>sshd:session<span class=o>)</span>: session opened <span class=k>for</span> user ceph by <span class=o>(</span><span class=nv>uid</span><span class=o>=</span>0<span class=o>)</span>
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sudo<span class=o>[</span>30526<span class=o>]</span>: pam_unix<span class=o>(</span>sudo:auth<span class=o>)</span>: conversation failed
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:42 node06 sudo<span class=o>[</span>30526<span class=o>]</span>: pam_unix<span class=o>(</span>sudo:auth<span class=o>)</span>: auth could not identify password <span class=k>for</span> <span class=o>[</span>ceph<span class=o>]</span>
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:45 node06 sudo<span class=o>[</span>30526<span class=o>]</span>:     ceph : user NOT in sudoers <span class=p>;</span> <span class=nv>TTY</span><span class=o>=</span>unknown <span class=p>;</span> <span class=nv>PWD</span><span class=o>=</span>/home/ceph <span class=p>;</span> <span class=nv>USER</span><span class=o>=</span>root <span class=p>;</span> <span class=nv>COMMAND</span><span class=o>=</span>/bin/python2 -c import sys<span class=p>;</span>exec<span class=o>(</span>eval<span class=o>(</span>sys.stdin.readline<span class=o>()))</span>
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:45 node06 sshd<span class=o>[</span>30525<span class=o>]</span>: Received disconnect from 192.168.20.88 port 37874:11: disconnected by user
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:45 node06 sshd<span class=o>[</span>30525<span class=o>]</span>: Disconnected from 192.168.20.88 port <span class=m>37874</span>
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:45 node06 sshd<span class=o>[</span>30521<span class=o>]</span>: pam_unix<span class=o>(</span>sshd:session<span class=o>)</span>: session closed <span class=k>for</span> user ceph
</span></span><span class=line><span class=cl>Sep <span class=m>12</span> 14:51:45 node06 postfix/sendmail<span class=o>[</span>30549<span class=o>]</span>: fatal: parameter inet_interfaces: no <span class=nb>local</span> interface found <span class=k>for</span> ::1</span></span></code></pre></td></tr></table></div></div></div></div><p>配置 sudo</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1>#cat /etc/sudoers.d/ceph   </span>
</span></span><span class=line><span class=cl>ceph <span class=nv>ALL</span> <span class=o>=</span> <span class=o>(</span>root<span class=o>)</span> NOPASSWD:ALL</span></span></code></pre></td></tr></table></div></div></div></div><p>配置完成后部署 radosgw</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 拷贝配置文件</span>
</span></span><span class=line><span class=cl>ceph-deploy --overwrite-conf config push node06
</span></span><span class=line><span class=cl><span class=c1># 安装软件包</span>
</span></span><span class=line><span class=cl>ceph-deploy install  --no-adjust-repos --nogpgcheck node06
</span></span><span class=line><span class=cl><span class=c1># new一个新 rgw 实例，ceph-deploy 只支持new</span>
</span></span><span class=line><span class=cl>ceph-deploy rgw create node06</span></span></code></pre></td></tr></table></div></div></div></div><p>完整的输出</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span><span class=lnt>77
</span><span class=lnt>78
</span><span class=lnt>79
</span><span class=lnt>80
</span><span class=lnt>81
</span><span class=lnt>82
</span><span class=lnt>83
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ceph-deploy --overwrite-conf config push node06
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.conf<span class=o>][</span>DEBUG <span class=o>]</span> found configuration file at: /home/ceph/.cephdeploy.conf
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span> Invoked <span class=o>(</span>2.0.1<span class=o>)</span>: /bin/ceph-deploy --overwrite-conf config push node06
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span> ceph-deploy options:
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  username                      : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  verbose                       : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  overwrite_conf                : True
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  subcommand                    : push
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  quiet                         : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3e96c9e8c0&gt;
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  cluster                       : ceph
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  client                        : <span class=o>[</span><span class=s1>&#39;node06&#39;</span><span class=o>]</span>
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  func                          : &lt;<span class=k>function</span> config at 0x7f3e96ec9c08&gt;
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  ceph_conf                     : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  default_release               : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.config<span class=o>][</span>DEBUG <span class=o>]</span> Pushing config to node06
</span></span><span class=line><span class=cl>Warning: Permanently added <span class=s1>&#39;[node06]:55556,[192.168.20.88]:55556&#39;</span> <span class=o>(</span>ECDSA<span class=o>)</span> to the list of known hosts.
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> connection detected need <span class=k>for</span> sudo
</span></span><span class=line><span class=cl>Warning: Permanently added <span class=s1>&#39;[node06]:55556,[192.168.20.88]:55556&#39;</span> <span class=o>(</span>ECDSA<span class=o>)</span> to the list of known hosts.
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> connected to host: node06 
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> detect platform information from remote host
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> detect machine <span class=nb>type</span>
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> write cluster configuration to /etc/ceph/<span class=o>{</span>cluster<span class=o>}</span>.conf
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>$ ceph-deploy install  --no-adjust-repos --nogpgcheck node06
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.conf<span class=o>][</span>DEBUG <span class=o>]</span> found configuration file at: /home/ceph/.cephdeploy.conf
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span> Invoked <span class=o>(</span>2.0.1<span class=o>)</span>: /bin/ceph-deploy install --no-adjust-repos --nogpgcheck node06
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span> ceph-deploy options:
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  verbose                       : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  testing                       : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fc503ac4758&gt;
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  cluster                       : ceph
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  dev_commit                    : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  install_mds                   : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  stable                        : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  default_release               : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  username                      : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  adjust_repos                  : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  func                          : &lt;<span class=k>function</span> install at 0x7fc5041125f0&gt;
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  install_mgr                   : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  install_all                   : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  repo                          : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  host                          : <span class=o>[</span><span class=s1>&#39;node06&#39;</span><span class=o>]</span>
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  install_rgw                   : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  install_tests                 : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  repo_url                      : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  ceph_conf                     : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  install_osd                   : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  version_kind                  : stable
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  install_common                : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  overwrite_conf                : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  quiet                         : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  dev                           : master
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  nogpgcheck                    : True
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  local_mirror                  : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  release                       : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  install_mon                   : False
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.cli<span class=o>][</span>INFO  <span class=o>]</span>  gpg_url                       : None
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.install<span class=o>][</span>DEBUG <span class=o>]</span> Installing stable version mimic on cluster ceph hosts node06
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.install<span class=o>][</span>DEBUG <span class=o>]</span> Detecting platform <span class=k>for</span> host node06 ...
</span></span><span class=line><span class=cl>Warning: Permanently added <span class=s1>&#39;[node06]:55556,[192.168.20.88]:55556&#39;</span> <span class=o>(</span>ECDSA<span class=o>)</span> to the list of known hosts.
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> connection detected need <span class=k>for</span> sudo
</span></span><span class=line><span class=cl>Warning: Permanently added <span class=s1>&#39;[node06]:55556,[192.168.20.88]:55556&#39;</span> <span class=o>(</span>ECDSA<span class=o>)</span> to the list of known hosts.
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> connected to host: node06 
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> detect platform information from remote host
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> detect machine <span class=nb>type</span>
</span></span><span class=line><span class=cl><span class=o>[</span>ceph_deploy.install<span class=o>][</span>INFO  <span class=o>]</span> Distro info: CentOS Linux 7.9.2009 Core
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>INFO  <span class=o>]</span> installing Ceph on node06
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>INFO  <span class=o>]</span> Running command: sudo yum clean all
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> Loaded plugins: fastestmirror
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> Cleaning repos: base centos-sclo-rh centos-sclo-sclo devops-Extra epel extras
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span>               : openresty remi-php72 remi-php73 remi-php74 remi-safe salt-latest
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span>               : tools-repo updates zabbix
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> Cleaning up list of fastest mirrors
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> Other repos take up <span class=m>23</span> M of disk space <span class=o>(</span>use --verbose <span class=k>for</span> details<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>INFO  <span class=o>]</span> Running command: sudo yum -y install ceph ceph-radosgw
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> Loaded plugins: fastestmirror
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> Determining fastest mirrors
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> No package ceph available.
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> Nothing to <span class=k>do</span>
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>INFO  <span class=o>]</span> Running command: sudo ceph --version
</span></span><span class=line><span class=cl><span class=o>[</span>node06<span class=o>][</span>DEBUG <span class=o>]</span> ceph version 14.2.22 <span class=o>(</span>ca74598065096e6fcbd8433c8779a2be0c889351<span class=o>)</span> nautilus <span class=o>(</span>stable<span class=o>)</span></span></span></code></pre></td></tr></table></div></div></div></div><p>查看节点是否上线</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ ceph -s
</span></span><span class=line><span class=cl>  cluster:
</span></span><span class=line><span class=cl>    id:     baf87797-3ec1-4f2c-8126-bf0a44051b13
</span></span><span class=line><span class=cl>    health: HEALTH_WARN
</span></span><span class=line><span class=cl>            <span class=m>1</span> pools have many more objects per pg than average
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>  services:
</span></span><span class=line><span class=cl>    mon: <span class=m>3</span> daemons, quorum node01,node02,node03 <span class=o>(</span>age 2w<span class=o>)</span>
</span></span><span class=line><span class=cl>    mgr: node01<span class=o>(</span>active, since 8w<span class=o>)</span>, standbys: node02, node03
</span></span><span class=line><span class=cl>    mds: kubefs:2 <span class=o>{</span><span class=nv>0</span><span class=o>=</span><span class=nv>node01</span><span class=o>=</span>up:active,1<span class=o>=</span><span class=nv>node02</span><span class=o>=</span>up:active<span class=o>}</span> <span class=m>1</span> up:standby
</span></span><span class=line><span class=cl>    osd: <span class=m>13</span> osds: <span class=m>13</span> up <span class=o>(</span>since 6d<span class=o>)</span>, <span class=m>13</span> in <span class=o>(</span>since 7M<span class=o>)</span>
</span></span><span class=line><span class=cl>    rgw: <span class=m>4</span> daemons active <span class=o>(</span>node01, node02, node03, node06<span class=o>)</span></span></span></code></pre></td></tr></table></div></div></div></div><p>流量的请求时访问 radosgw 服务，这个时候新实例是没有引入流量的，需要修改负载均衡器增加新的节点进来，流量引入后需要确认旧服务已经不在处理业务请求后可以下线
确认请求，查看活跃连接</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ netstat -an<span class=p>|</span>grep <span class=m>7480</span>
</span></span><span class=line><span class=cl>tcp        <span class=m>0</span>      <span class=m>0</span> 0.0.0.0:7480            0.0.0.0:*               LISTEN     
</span></span><span class=line><span class=cl>tcp        <span class=m>0</span>      <span class=m>0</span> 192.168.20.84:7480      192.168.20.84:33152      ESTABLISHED</span></span></code></pre></td></tr></table></div></div></div></div><p>确认请求，查看服务日志</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ tail -f /var/log/ceph/ceph-client.rgw.node01.log</span></span></code></pre></td></tr></table></div></div></div></div><p>确认无误可以下线，ceph-deploy 部署的服务没有 cephadm ceph orch rgw delete xx 这类工具进行下线，直接通过 systemd 停止服务即可</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ systemctl -l<span class=p>|</span>grep rados
</span></span><span class=line><span class=cl>  ceph-radosgw@rgw.node01.service                                                     loaded active     running      Ceph rados gateway
</span></span><span class=line><span class=cl>  system-ceph<span class=se>\x</span>2dradosgw.slice                                                                loaded active     active       system-ceph<span class=se>\x</span>2dradosgw.slice
</span></span><span class=line><span class=cl>  ceph-radosgw.target                                                                         loaded active     active       ceph target allowing to start/stop all ceph-radosgw@.service instances at once</span></span></code></pre></td></tr></table></div></div></div></div><p>停止服务并检查内存状态</p><div class="pe-code-block-wrap pe-code-details open scrollable"><div class="pe-code-block-header pe-code-details-summary"><div class=pe-code-block-header-left><i class="arrow fas fa-chevron-right fa-fw pe-code-details-icon" aria-hidden=true></i>
<span>bash</span></div><div class=pe-code-block-header-center><span></span></div><div class=pe-code-block-header-right><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i>
<button class=pe-code-copy-button><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="pe-icon"><path fill="currentcolor" fill-rule="evenodd" d="M7 5a3 3 0 013-3h9a3 3 0 013 3v9a3 3 0 01-3 3h-2v2a3 3 0 01-3 3H5a3 3 0 01-3-3v-9a3 3 0 013-3h2zm2 2h5a3 3 0 013 3v5h2a1 1 0 001-1V5a1 1 0 00-1-1h-9A1 1 0 009 5zM5 9a1 1 0 00-1 1v9a1 1 0 001 1h9a1 1 0 001-1v-9a1 1 0 00-1-1z" clip-rule="evenodd"/></svg></button></div></div><div class="pe-code-details-content scrollable"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ systemctl stop ceph-radosgw@rgw.node01.service  
</span></span><span class=line><span class=cl><span class=c1>#ps axu --sort=-%mem|head -10</span>
</span></span><span class=line><span class=cl>USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
</span></span><span class=line><span class=cl>ceph        <span class=m>1983</span>  0.6 12.8 <span class=m>3358788</span> <span class=m>2084324</span> ?     Ssl  May03 1275:36 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>3</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1991</span>  0.9 12.5 <span class=m>3451788</span> <span class=m>2033560</span> ?     Ssl  May03 1722:12 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>2</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1721</span>  0.6 11.8 <span class=m>3318456</span> <span class=m>1920876</span> ?     Ssl  May03 1219:27 /usr/bin/ceph-osd -f --cluster ceph --id <span class=m>6</span> --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph        <span class=m>1709</span>  0.5  7.4 <span class=m>1646276</span> <span class=m>1212516</span> ?     Ssl  May03 1050:21 /usr/bin/ceph-mds -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph       <span class=m>18979</span>  1.0  4.5 <span class=m>1330064</span> <span class=m>744972</span> ?      Ssl  May03 1937:16 /usr/bin/ceph-mon -f --cluster ceph --id node01 --setuser ceph --setgroup ceph
</span></span><span class=line><span class=cl>ceph      <span class=m>529617</span>  3.7  4.4 <span class=m>1914452</span> <span class=m>726436</span> ?      Ssl  Jul15 3153:14 /usr/bin/ceph-mgr -f --cluster ceph --id node01 --setuser ceph --setgroup ceph</span></span></code></pre></td></tr></table></div></div></div></div><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>本次操作没有分析为什么使用内存高，只是着急做了迁移，这样导致在事后无法确定问题的根本原因，后期遇到问题要先分析并保留证据，其次在做迁移之类动作。</p></div><div class=pe-copyright><hr><blockquote><p>本文为原创内容，版权归作者所有。如需转载，请在文章中声明本文标题及链接。</p><p>文章标题：记录一次失败的radosgw问题排查记录</p><p>文章链接：<a href=https://www.oomkill.com/2024/09/05-5-failed-troubleshooting-for-rgw/ target=_blank>https://www.oomkill.com/2024/09/05-5-failed-troubleshooting-for-rgw/</a></p><p>许可协议：<a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></p></blockquote></div><div class=comments-separator></div><h3 class=relatedContentTitle>相关阅读</h3><ul class=relatedContent><li><a href=/2023/08/acquaintance-stroage/><span>存储概念 - 存储类型对比</span></a></li><li><a href=/2020/09/alpine-trouble-q-and-a/><span>使用alpine为基础镜像Q&amp;A</span></a></li><li><a href=/2020/09/envoy-example-failed/><span>envoy官方example运行失败问题处理</span></a></li><li><a href=/2024/02/10-2-troubeshooting-crash/><span>记录一次ceph集群故障处理记录</span></a></li><li><a href=/2023/11/10-1-ceph-fscache/><span>当cephfs和fscache结合时在K8s环境下的全集群规模故障</span></a></li></ul><div class=comments-separator></div><footer class=post-footer><ul class=post-tags><li><a href=https://www.oomkill.com/tags/storage/>Storage</a></li></ul><nav class=paginav><a class=prev href=https://www.oomkill.com/2024/09/03-3-ceph-osd-performance-recommendation/><span class=title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left" style="user-select:text"><line x1="19" y1="12" x2="5" y2="12" style="user-select:text"/><polyline points="12 19 5 12 12 5" style="user-select:text"/></polyline></svg>&nbsp;</span>
<span>Ceph OSD内存优化与建议</span>
</a><a class=next href=https://www.oomkill.com/2024/09/gke-invalid-pod-limits/><span class=title></span>
<span>GKE强制升级后JAVA Pod无法识别limit限制&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right" style="user-select:text"><line x1="5" y1="12" x2="19" y2="12" style="user-select:text"/><polyline points="12 5 19 12 12 19" style="user-select:text"/></svg></span></a></nav></footer><div class=pe-comments-decoration><p class=pe-comments-title></p><p class=pe-comments-subtitle></p></div><div id=pe-comments></div><script src=/js/pe-go-comment.min.86a214102576ba5f9b7bdc29eed8d58dd56e34aef80b3c65c73ea9cc88443696.js integrity="sha256-hqIUECV2ul+be9wp7tjVjdVuNK74Czxlxz6pzIhENpY="></script><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="dark"?"dark":"light",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"cylonchau/cylonchau.github.io","data-repo-id":"R_kgDOIRlNSQ","data-category":"Announcements","data-category-id":"DIC_kwDOIRlNSc4CXy1U","data-mapping":"pathname","data-term":"posts/05-5 failed troubleshooting for rgw","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":getStoredTheme(),"data-lang":"zh-TW","data-loading":"lazy",crossorigin:"anonymous",async:""},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#pe-comments").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://www.oomkill.com/>Cylon's Collection</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> on
<a href=https://pages.github.com/ rel=noopener target=_blank>GitHub Pages</a> & Theme
        <a href=https://github.com/tofuwine/PaperMod-PE rel=noopener target=_blank>PaperMod-PE</a></span><div class=busuanzi-footer style="font-family:helvetica neue,Helvetica,Arial,sans-serif;text-align:center;padding:4px 0;color:#999"><span id=busuanzi_container_site_pv style=margin-right:8px;font-size:.85em>本站总访问量<span id=busuanzi_value_site_pv style=font-weight:500>0</span>次
</span><span id=busuanzi_container_site_uv style=font-size:.85em>本站访客数<span id=busuanzi_value_site_uv style=font-weight:500>0</span>人次</span></div></footer><div class=pe-right-sidebar><a href=javascript:void(0); id=theme-toggle-float class=pe-float-btn><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</a><a href=#top class=pe-float-btn id=top-link><span id=pe-read-progress></span></a></div><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>