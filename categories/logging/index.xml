<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>logging on Cylon&#39;s Collection</title>
    <link>https://www.oomkill.com/categories/logging/</link>
    <description>Recent content in logging on Cylon&#39;s Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 02 Nov 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://www.oomkill.com/categories/logging/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ELK收集java日志</title>
      <link>https://www.oomkill.com/2018/11/elk-collect-java/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/11/elk-collect-java/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>Java日志多由多行行组成，初始行之后的每一行以空格开头，如下例所示：在这种情况下，需要在将事件数据output之前处理多行事件。</p>
<pre><code class="language-log">[ERROR] - 2018-09-21 04:19:57.685 (SocketTransfer.java:73) - Socket交互遇到错误
   Exception in thread &quot;main&quot; java.lang.NullPointerException
   at com.example.myproject.Book.getTitle(Book.java:16)
   at com.example.myproject.Author.getBookTitles(Author.java:25)
   at com.example.myproject.Bootstrap.main(Bootstrap.java:14)
</code></pre>
<p>配置multiline参数如下：<a href="https://www.elastic.co/guide/en/logstash/6.4/plugins-codecs-multiline.html" target="_blank"
   rel="noopener nofollow noreferrer" >Multiline codec plugin</a></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>pattern</td>
<td>指定正则表达式。与指定正则表达式匹配的行被视为前一行的延续或新多行事件的开始。</td>
</tr>
<tr>
<td>what</td>
<td>previous或next。该previous值指定与pattern选项中的值匹配的行是上一行的一部分。该next值指定与pattern选项中的值匹配的行是以下行的一部分</td>
</tr>
<tr>
<td>negate</td>
<td>true或false（默认为false）。如果true，与pattern模式不匹配的消息将构成多行过滤器的匹配what并将应用。</td>
</tr>
</tbody>
</table>
<pre><code class="language-sh">input {
		file {
				path =&gt; &quot;/root/apache-tomcat-8.5.34/logs/catalina.out&quot;
				type =&gt; &quot;sk&quot;
				start_position =&gt; &quot;beginning&quot;
				codec =&gt; multiline {
						pattern =&gt; &quot;^\[&quot;
						negate =&gt; &quot;true&quot;
						what =&gt; &quot;previous&quot;
				}
		}
}
</code></pre>
<p>如上列日志格式为</p>
<pre><code class="language-sh">[日志级别] - 日期 java类 - 错误报错
</code></pre>
<p>一般情况下，我们不需要将对其grok，因grok会大量影响性能。如上列格式需要对每一列进行grok</p>
<pre><code class="language-sh"># 日期 
TESTTIME %{YEAR}\-%{MONTHNUM}\-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND}
</code></pre>
<p>根据以上格式写出如下的正则</p>
<pre><code class="language-logstash">filter {
  grok {
    match =&gt; {
      'message' =&gt; &quot;\[\s?%{LOGLEVEL:lev}\] - %{TESTTIME:date}\s+\((?&lt;\class&gt;.*\.java\:\d+)\)\s\-\s(?&lt;\msg&gt;.*)&quot;
    }
  
  }
  
  mutate {  
    remove_field =&gt; &quot;message&quot;
  }
}
</code></pre>
<p>匹配到的日志格式</p>
<pre><code class="language-sh">{
    &quot;@timestamp&quot; =&gt; &quot;2018-10-05T13:16:27.263Z&quot;,
      &quot;@version&quot; =&gt; &quot;1&quot;,
          &quot;path&quot; =&gt; &quot;/root/apache-tomcat-8.5.34/logs/catalina.out&quot;,
          &quot;host&quot; =&gt; &quot;node02&quot;,
          &quot;type&quot; =&gt; &quot;sk&quot;,
           &quot;lev&quot; =&gt; &quot;DEBUG&quot;,
          &quot;date&quot; =&gt; &quot;2018-10-04 12:48:56.142&quot;,
         &quot;class&quot; =&gt; &quot;ObjectDaoImpl.java:219&quot;,
           &quot;msg&quot; =&gt; &quot;queryPage with hql:select t.fplxdm,t.fpdm,t.fphm,t.fpcbh,f.ipdz,f.dkh,f.jqbh from ( select '026' as fplxdm,fpdm,fphm,kprq,fpcbh,jqbh from pj_zzspdz_fpmx where kprq&gt;'20180801000000' and jqbh='499099915361' and qmbz='N') t,dj_fwqxx f where t.jqbh=f.jqbh order by t.kprq&quot;
}
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>ELK收集syslog</title>
      <link>https://www.oomkill.com/2018/11/collect-syslog/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/11/collect-syslog/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>Logstash通过网络将syslog消息作为事件读取。使用用 UDPSocket, TCPServer 和</p>
<p>LogStash::Filters::Grok 来实现</p>
<h3 id="配置示例">配置示例</h3>
<pre><code class="language-sh">input {                                            
    syslog{
        port =&gt; &quot;514&quot;
    }
}
</code></pre>
<p>配置客户端，在修改linux主机<code>/etc/rsyslog.conf</code></p>
<pre><code class="language-sh">*.* @@host:port
</code></pre>
<h2 id="配置说明">配置说明</h2>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>*</td>
<td>类型</td>
</tr>
<tr>
<td>*</td>
<td>级别</td>
</tr>
<tr>
<td>@</td>
<td>udp</td>
</tr>
<tr>
<td>@@</td>
<td>tcp</td>
</tr>
<tr>
<td>host</td>
<td>可为主机名或ip地址</td>
</tr>
</tbody>
</table>
<hr>
<blockquote>
<p><strong><font color="#0215cd" size=2>注：收集到的数据，本身就以及是rsyslog格式了，无需再进行grok</font></strong></p>
</blockquote>
<hr>
<pre><code class="language-sh">{
           &quot;message&quot; =&gt; &quot;(root) CMD (/bin/echo 1111 &gt;&gt;/root/1.txt)\n&quot;,
          &quot;@version&quot; =&gt; &quot;1&quot;,
        &quot;@timestamp&quot; =&gt; &quot;2018-10-05T12:13:01.000Z&quot;,
              &quot;host&quot; =&gt; &quot;10.0.0.16&quot;,
          &quot;priority&quot; =&gt; 78,
         &quot;timestamp&quot; =&gt; &quot;Oct  5 20:13:01&quot;,
         &quot;logsource&quot; =&gt; &quot;node02&quot;,
           &quot;program&quot; =&gt; &quot;CROND&quot;,
               &quot;pid&quot; =&gt; &quot;92923&quot;,
          &quot;severity&quot; =&gt; 6,
          &quot;facility&quot; =&gt; 9,
    &quot;facility_label&quot; =&gt; &quot;clock&quot;,
    &quot;severity_label&quot; =&gt; &quot;Informational&quot;
}
</code></pre>
<p>收集到的数据需要对时间进行格式化</p>
<p>官方date插件说明：<a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html#plugins-filters-date-timezone" target="_blank"
   rel="noopener nofollow noreferrer" >Date filter plugin</a></p>
<pre><code class="language-sh">filter {
		date {
				match =&gt; [&quot;timestamp&quot; , &quot;MMM  dd HH:mm:ss&quot;]
				target =&gt; &quot;timestamp&quot;
				&quot;timezone&quot; =&gt; &quot;Asia/Shanghai&quot;
		}
}
</code></pre>
<p>格式化完后的时间如下示例</p>
<pre><code class="language-sh">{
           &quot;message&quot; =&gt; &quot;(root) CMD (/bin/echo 1111 &gt;&gt;/root/1.txt)\n&quot;,
          &quot;@version&quot; =&gt; &quot;1&quot;,
        &quot;@timestamp&quot; =&gt; &quot;2018-10-05T12:09:01.000Z&quot;,
              &quot;host&quot; =&gt; &quot;10.0.0.16&quot;,
          &quot;priority&quot; =&gt; 78,
         &quot;timestamp&quot; =&gt; &quot;2018-10-05T12:09:01.000Z&quot;,
         &quot;logsource&quot; =&gt; &quot;node02&quot;,
           &quot;program&quot; =&gt; &quot;CROND&quot;,
               &quot;pid&quot; =&gt; &quot;92825&quot;,
          &quot;severity&quot; =&gt; 6,
          &quot;facility&quot; =&gt; 9,
    &quot;facility_label&quot; =&gt; &quot;clock&quot;,
    &quot;severity_label&quot; =&gt; &quot;Informational&quot;
}
</code></pre>
<hr>
<p><strong><font color="#0215cd" size=2>说明：此处格式化完后时间与当前时区不符，相差8小时。这里不影响，在kibana中显示的为当前时区</font></strong></p>
<hr>
<p><img loading="lazy" src="../../images/%e6%94%b6%e9%9b%86syslog/dbefd2aa.png" alt="image" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Logstash使用</title>
      <link>https://www.oomkill.com/2018/11/logstash/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/11/logstash/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="1-logstash概述">1 logstash概述</h2>
<p>logstash是基于Jruby语言研发的server/agent结构的日志收集工具，可以在任何一个能产生日志的服务器上部署其agent。agent能同时监控多个产生日志的服务，并把每个服务所产生的日志信息收集并发送给logstash server端。由于收集到的日志信息是分散的，logstash有专门的节点用来将所有的节点收集到的日志信息按时间序列合并在一起形成一个序列，基于这一个序列请求写入并存储在elasticsearch集群中。</p>
<p>kibana是基于node.js开发的elasticsearch的图形用户接口。它可以将用户的搜索语句发送给ES，有ES完成搜索，并且将结果返回。kibana可以将返回的数据，用非常直观的方式（画图）来做趋势展示的。</p>
<p>logstash基于多种数据获取机制，TCP/UDP协议、文件、syslog、windows EventLogs及STDIN等。同时也支持多种数据输出机制。获取到数据后，它支持对数据执行过滤、修改等操作。</p>
<p>logstash基于JRuby语言研发，需要运行在JVM虚拟机之上。工作于agent/server模型。</p>
<h3 id="11-logstash工作机制">1.1 logstash工作机制</h3>
<p>在每一个产生日志的节点之上部署一个agent，这个agent通常称为<font color="#f8070d" size=3><code>shipper</code></font>。shipper负责收集日志，并且发送给server端。但是在发送给server端时，为了避免server端可能无法应付大量节点同时发来的信息，会在server端和agent端之间放置一个消息队列，通常称之为<font color="#f8070d" size=3><code>broker</code></font>。这个消息队列比较常见的使用redis。各agent将数据发送至broker，logstash服务器会从broker中依次取出日志拿来在本地做过滤、修改等操作。在完修改后将其发送至ES集群。</p>
<p>对于logstash而言，server端也是插件式的工作的模式，与ES的不同在于，ES的插件是可有可无的。logstash的除核心外的所有功能都是基于插件来完成的。</p>
<h3 id="12-logstash工作流程">1.2 logstash工作流程</h3>
<p>logstash的工作模式类似于管道，从指定位置读入数据，而后过滤数据，最后将其输出到指定位置。事实上logstash的工作流程为：<font color="#f8070d" size=3><code>input | filter | output</code></font>。如无需对数据额外处理，filter可省略。</p>
<h3 id="13-logstash插件分类">1.3 logstash插件分类</h3>
<ul>
<li>input plugin：收集数据</li>
<li>codec plugin：编码插件</li>
<li>filter plugin：过滤</li>
<li>output plugin：输出插件</li>
</ul>
<h2 id="2-logstash安装下载">2 logstash安装下载</h2>
<p>官网地址：https://www.elastic.co/cn/products../../images/Logstash</p>
<p><img loading="lazy" src="../../images/Logstash/da565791.png" alt="image" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="3-logstash的配置使用">3 logstash的配置使用</h2>
<p><font color="#f8070d" size=3><code>/etc../../images/Logstash/conf.d/</code></font>路径下所有以<font color="#f8070d" size=3><code>.conf</code></font>结尾的文件都被当做配置文件使用。对于logstash的配置文件定义，就是定义插件。从哪得到数据。向哪里输出数据。</p>
<h3 id="31-logstash的基本配置框架">3.1 logstash的基本配置框架</h3>
<pre><code class="language-sh">input{
 	....
}

filter{
 	....
}

output{
 	....
}
</code></pre>
<p>简单的</p>
<pre><code class="language-sh">input{
 	stdin {}
}
output { 
 	stdout{
 		codec =&gt;rubydebug
 	}
}
</code></pre>
<p>检查配置文件语法是否正确<font color="#f8070d" size=2><code>logstash -f /etc../../images/Logstash/conf.d/sample.conf --configtest</code></font></p>
<pre><code class="language-sh">$ logstash -f /etc../../images/Logstash/conf.d/sample.conf 
hellow owrd
Settings: Default pipeline workers: 1
Pipeline main started
{
       &quot;message&quot; =&gt; &quot;hellow owrd&quot;, #←信息的完整内容。logstash的信息是在input → filter → output之间流动的。
      &quot;@version&quot; =&gt; &quot;1&quot;, #←版本号。
    &quot;@timestamp&quot; =&gt; &quot;2018-06-04T15:02:47.912Z&quot;, #←事件发生时的时间戳。
          &quot;host&quot; =&gt; &quot;test&quot; #←时间发生的主机。
}
</code></pre>
<h3 id="32-logstash支持的数据类型">3.2 logstash支持的数据类型</h3>
<p>Array：[iteml,item2,&hellip;]
Boolean：true,false
Bytes
Codec：编码器。
Hash: key =&gt;value 
Number
Password：
Path：文件系统路径。
String：字符串。
logstash还支持字段引用，使用<font color="#f8070d" size=3><code>[]</code></font>进行引用</p>
<p>3.3 支持的条件判断
==，!=，&lt;，&lt;=，&gt;，&gt;= 正则匹配=~ !~
in,not in and,or</p>
<h2 id="4-logstash常用的插件">4 Logstash常用的插件</h2>
<p>logstash是高度插件化的，主要分为<font color="#f8070d" size=3><code>input</code></font> <font color="#f8070d" size=3><code>codec</code></font> <font color="#f8070d" size=3><code>filter</code></font> <font color="#f8070d" size=3><code>output</code></font>插件：</p>
<h3 id="41-input插件">4.1 input插件</h3>
<h4 id="411-file">4.1.1 File</h4>
<p>从指定的文件中读取事件流。其工作特性类似于<font color="#f8070d" size=3><code>tail -1f</code></font>，能够将日志文件尾部的一行不断的读取出来。</p>
<p>logstash使用<font color="#f8070d" size=3><code>FileWatch</code></font>（ruby gem库）机制来监听文件变化。filewatch是linux内核中提供的一种功能。filewatch库支持以glob方式展开文件名。因此可以监听多个文件。并且可以将每个文件读取的位置及状态信息保存在<font color="#f8070d" size=3><code>.sincedb</code></font>数据库中。即使重启logstash后也不会从头读取文件的。.sincedb记录了每个被监听的文件的<font color="#f8070d" size=3><code>inode</code></font>,<font color="#f8070d" size=3><code>major number</code></font>,<font color="#f8070d" size=3><code>minor nubmer</code></font>,<font color="#f8070d" size=3><code>pos</code></font>。默认情况下位于启动logstash服务的用户家目录下。也可以自行定义位置。另外，file插件还可以自动识别日志的滚动（日志分割）操作。</p>
<pre><code class="language-sh">input{
    file{
        path =&gt; ['/var/log/messages']
        type =&gt; &quot;system&quot;
        start_postition =&gt; &quot;beginning&quot;
    }
}

output{
    stdout{
        codec =&gt; rubydebug
    }
}
</code></pre>
<p><a href="https://www.elastic.co/guide/en../../images/Logstash/2.4/index.html" target="_blank"
   rel="noopener nofollow noreferrer" >Logstash</a>
<a href="https://www.elastic.co/guide/en../../images/Logstash/2.4/plugins-inputs-file.html" target="_blank"
   rel="noopener nofollow noreferrer" >file</a></p>
<h4 id="412-udp">4.1.2 UDP</h4>
<p>logstash支持通过一个简单的协议来读取信息，如UDP、TCP，说明对方只要通过TCP协议发送时间也是支持的。或者哪怕通过syslog收集事件也是可以的。</p>
<p>logstash通过UDP协议通过网络连接来读取信息，其唯一必须制定的参数为<font color="#f8070d" size=3><code>port</code></font>，用于指定自己监听的端口，agent可以向服务器端端口发送日志信息。而每个被收集数据的主机使用守护进程向server端发送事件。</p>
<pre><code class="language-sh">input{
    udp{
        prot =&gt; 25826
        queue_size =&gt; 2000  #←队列大小
        workers =&gt; 2        #←启动的工作线程数来接收发送的事件。
        buffer_size =&gt;      #←接收缓冲区的大小
    }
}

</code></pre>
<blockquote>
<p><strong>UDP插件实现</strong></p>
</blockquote>
<p>collectd：基于C语言开发的一款高度插件式的主机性能监控程序，以守护进程方式运行，能够收集系统性能相关的各种数据；并且能够基于各种存储机制将收集的结果存储下来。collectd本身所收集到的数据可以通过自身的network插件，将自己在本机所收集到的数据发送至其他主机。</p>
<p>collectd在epel源中</p>
<pre><code class="language-sh">yum install collectd -y
</code></pre>
<p>collectd的大量.so文件为其模块（插件），每一个模块分别实现每一种相应的功能。其配置文件 <font color="#f8070d" size=3><code>/etc/collectd.conf</code></font></p>
<pre><code class="language-sh">Hostname &quot;node3.magedu.com&quot;
LoadPlugin syslog 
LoadPlugin cpu  # cpu性能数据
LoadPlugin df  # 磁盘空间使用情况
LoadPlugin interface # 网络接口
LoadPlugin 1oad  # 服务器负载
LoadPlugin memory  # 内存
LoadPlugin network 
&lt;Plugin network&gt;
    &lt;server &quot;10.0.0.19&quot; &quot;25826&quot;&gt; #10.0.0.19是logstash主机的地址，25826是其监听的udp端口
    &lt;/server&gt;
&lt;/Plugin
</code></pre>
<pre><code class="language-sh">input udp {
    port =&gt; &quot;25826&quot;
    codec =&gt; collectd {}
    type =&gt; &quot;collectd&quot;
}

output{
    stdout{
        codec =&gt; rubydebug
    }
}

</code></pre>
<h4 id="413-redis">4.1.3 redis</h4>
<p>redis插件主要作用在于从redis中获取数据。它支持redis中的两种方式；支持redis channel（发布频道）以及lists两种方式。logstash从redis读数据，可以基于列表方式进行。也可以基于channel方式进行。也就是说redis可以仅工作在正常模式下，不用启用channel机制，也一样能够使logstash从中获取数据。</p>
<p>对于logstash而言，一般要求redis版本在<font color="#f8070d" size=3><code>2.6.0</code></font>之后的版本。</p>
<h3 id="42-filter插件">4.2 filter插件</h3>
<p>filter插件主要用于将event通过output发出之前对其实现某些处理功能。对所收集到的数据当中，期望能够基于某种特定格式进行拆分、组合、过滤其中某些信息，都可以使用filter插件实现。在过滤前后如果有需要，都可以调用codec插件，事先基于某种形式对消息做编码。将处理过数据存往任何可存储位置，需要用到特定于存储位置的输出插件。logstash有众多filter过滤器。</p>
<h4 id="421-grok">4.2.1 grok</h4>
<p>gork是logstash中最重要的插件之一，主要用于分析并结构化文本数据。从web服务器日志读取的日志事件是遵循同种格式的，如：我们只想统计有多少访问IP，但是获取的是一行信息。为了能够方便随后的统计操作，需要将每读到的任何一行日志实现切好。使得对日志分析能够得以进行。目前是logstash中将非结构化日志数据转化为结构化的可查询数据的不二之选。</p>
<p>目前支持处理 <font color="#f8070d" size=3><code>syslog</code></font> <font color="#f8070d" size=3><code>httpd</code></font> <font color="#f8070d" size=3><code>nginx</code></font> 等。默认情况下，logstash提供了120种默认grok模式。这些模式定义在<font color="#f8070d" size=3><code>patterns</code></font>目录下，每个模式都有默认的名称。</p>
<pre><code class="language-sh">rpm -ql logstash|grep &quot;patterns$&quot;;
</code></pre>
<h4 id="422-gork语法格式">4.2.2 gork语法格式</h4>
<p>logstash中grok的语法格式是使用<font color="#f8070d" size=3><code>%{}</code></font>括起的一组信息使用<font color="#f8070d" size=3><code>:</code></font>隔开，其中左半段称之为<font color="#f8070d" size=3><code>SYNTAX</code></font>由半段称之为<font color="#f8070d" size=3><code>SEMANTIC</code></font>。其中SYNTAX表示grok模板预定义模式中已有的模式名称，SEMANTIC，匹配到的文本的自定义的标识符。</p>
<pre><code class="language-sh">%{SYNTAX:SEMANTIC}
</code></pre>
<p>例:</p>
<pre><code class="language-sh">10.0.0.1 GET /index.html 30 0.23

%{IP:client_ip} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
</code></pre>
<p>10.0.0.1 GET /index.html 30 0.23
{
&ldquo;message&rdquo; =&gt; &ldquo;10.0.0.1 GET /index.html 30 0.23&rdquo;,
&ldquo;@version&rdquo; =&gt; &ldquo;1&rdquo;,
&ldquo;@timestamp&rdquo; =&gt; &ldquo;2018-05-31T10:34:37.150Z&rdquo;,
&ldquo;host&rdquo; =&gt; &ldquo;mysql&rdquo;,
&ldquo;client_ip&rdquo; =&gt; &ldquo;10.0.0.1&rdquo;,
&ldquo;method&rdquo; =&gt; &ldquo;GET&rdquo;,
&ldquo;request&rdquo; =&gt; &ldquo;/index.html&rdquo;,
&ldquo;bytes&rdquo; =&gt; &ldquo;30&rdquo;,
&ldquo;duration&rdquo; =&gt; &ldquo;0.23&rdquo;
}</p>
<pre><code class="language-sh">input {
		stdin {}
}

filter {
  grok {
    match =&gt; { 
      &quot;message&quot; =&gt; &quot;%{IP:client_ip} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}&quot; 
    } 
  }
}

output {
		stdout { codec =&gt; rubydebug }
}
</code></pre>
<p>自定义grok模式</p>
<p>grok模式是基于正则表达式的模式编写，其元字符与其他用到正则表达式的工具awk\sed\grep\pcre差别不大</p>
<p>nginx log匹配方式</p>
<pre><code class="language-sh">NGUSERNAME [a-zA-Z\.\@\-\+_%]+
NGUSER %{NGUSERNAME}
NGINXACCESS %{IPORHOST:clientip} - %{NOTSPACE:remote_user} \[%{HTTPDATE:timestamp}\] \&quot;(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\&quot; %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} %{NOTSPACE:http_x_forwarded_for}
</code></pre>
<p><a href="https://blog.csdn.net/joeyon1985/article/details/46639511" target="_blank"
   rel="noopener nofollow noreferrer" >LOGSTASH+ELASTICSEARCH+KIBANA处理NGINX访问日志 - CSDN博客</a></p>
<pre><code class="language-sh">input {
		file {
				path =&gt; ['/var/log/nginx/access.log']
				type =&gt; &quot;nginxlog&quot;
				start_position =&gt; &quot;beginning&quot;
		}
}

filter {
		grok {
				match =&gt; { &quot;message&quot; =&gt; &quot;%{NGINXACCESS}&quot; }
		}
}

output {
		stdout	{
				codec =&gt; rubydebug
		}
}

</code></pre>
<p>action 将数据输出到至elasticsearch中后基于什么方式做操作
hosts es主机集群列表。数组
index 存储在es的哪个索引当中</p>
<p>logstash在使用redis做输入或输出插件时，有两种数据类型，用来保存logstash输出的数据list channel</p>
<p><a href="https://blog.csdn.net/aaronhadoop/article/details/73163025" target="_blank"
   rel="noopener nofollow noreferrer" >ES5.4.0 记录 - CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_21383435/article/details/79323739" target="_blank"
   rel="noopener nofollow noreferrer" >18-elasticsearch集群健康为黄色 - CSDN博客</a></p>
<p>curl -X PUT 172.16.16.18:9200/gongsufanghua-work_record/_settings -d &lsquo;{
&ldquo;index&rdquo; : {
&ldquo;number_of_replicas&rdquo; : 0
}
}&rsquo;</p>
<p><a href="https://segmentfault.com/a/1190000011784259" target="_blank"
   rel="noopener nofollow noreferrer" >logstash-input-jdbc同步mysql数据到elasticsearch - 运维学习之路 - SegmentFault 思否</a></p>
<p><a href="https://blog.csdn.net/u012546526/article/details/78598608" target="_blank"
   rel="noopener nofollow noreferrer" >Elasticsearch yellow unassigned_shards 恢复 replicas 节点恢复 - CSDN博客</a></p>
<p><a href="https://www.jianshu.com/p/62433b9c5c96" target="_blank"
   rel="noopener nofollow noreferrer" >logstash mysql 准实时同步到 elasticsearch - 简书</a></p>
<h2 id="5-conditionals">5 conditionals</h2>
<p>当需要在特定条件下过滤或输出事件。可以使用条件条件判断语句。Logstash中的条件查看和行为与编程语言中的条件相同。条件语句支持if，else if以及else和可以被嵌套。</p>
<p><a href="https://www.elastic.co/guide/en../../images/Logstash/6.4/event-dependent-configuration.html#conditionals" target="_blank"
   rel="noopener nofollow noreferrer" >conditionals</a></p>
<h3 id="51-语法">5.1 语法</h3>
<pre><code class="language-sh">if EXPRESSION {
  ...
} else if EXPRESSION {
  ...
} else {
  ...
}
</code></pre>
<p>logstash支持以下比较运算符：</p>
<ul>
<li><font color="#f8070d" size=3><code>==</code></font> <font color="#f8070d" size=3><code>!=</code></font>、 <font color="#f8070d" size=3><code>&lt;</code></font> <font color="#f8070d" size=3><code>&gt;</code></font> <font color="#f8070d" size=3><code>&lt;=</code></font> <font color="#f8070d" size=3><code>&gt;=</code></font></li>
<li>regexp：<font color="#f8070d" size=3><code>=~</code></font><font color="#f8070d" size=3><code>!~</code></font>（检查右边的模式与左边的字符串值）</li>
<li>包含：<font color="#f8070d" size=3><code>in</code></font> <font color="#f8070d" size=3><code>not in</code></font></li>
<li>布尔运算符是： <font color="#f8070d" size=3><code>and</code></font> <font color="#f8070d" size=3><code>or</code></font> <font color="#f8070d" size=3><code>nand</code></font> <font color="#f8070d" size=3><code>xor（异或）</code></font></li>
<li>支持的一元运算符是：<font color="#f8070d" size=3><code>!</code></font></li>
</ul>
<hr>
<p>注：条件判断语句不包括数学运算符，如下写法logstash会报错</p>
<hr>
<pre><code class="language-sh">if [seg_num]&lt;[total_seg]-1 {
  ...
} 
</code></pre>
<h3 id="52-logstash常用条件判断语句">5.2 logstash常用条件判断语句</h3>
<p>为了避免多个input的数据提交到Elasticsearch后共存于一个索引中。使用index参数来为每个不同的type建立单独的索引。</p>
<pre><code class="language-sh">input{
  file{
    path=&gt; &quot;/usr/local/elasticsearch-2.3.2/logs/elasticsearch.log.*&quot;
    type=&gt; &quot;elasticsearch&quot;
    start_position=&gt; &quot;beginning&quot;
  }
  
  file{
    path=&gt; &quot;/var/log/secure&quot;
    type=&gt; &quot;secure&quot;
    start_position=&gt; &quot;beginning&quot;
  }
}

output{
  if [type] == &quot;elasticsearch&quot; {  
    elasticsearch {
    hosts=&gt; [&quot;192.168.44.129:9200&quot;]
    index=&gt; &quot;elasticsearch-%{+YYYY-MM}&quot;
    }
  }
  if [type] == &quot;secure&quot; {  
    elasticsearch {
    hosts=&gt; [&quot;192.168.44.129:9200&quot;]
    index=&gt; &quot;secure-%{+YYYY-MM}&quot;
    }
  }
}  
</code></pre>
<p>使用in运算符来测试字段是否包含特定字符串，键或（对于列表）元素：</p>
<pre><code class="language-sh">filter {
  if [foo] in [foobar] {
    mutate { add_tag =&gt; &quot;field in field&quot; }
  }
  if [foo] in &quot;foo&quot; {
    mutate { add_tag =&gt; &quot;field in string&quot; }
  }
  if &quot;hello&quot; in [greeting] {
    mutate { add_tag =&gt; &quot;string in field&quot; }
  }
  if [foo] in [&quot;hello&quot;, &quot;world&quot;, &quot;foo&quot;] {
    mutate { add_tag =&gt; &quot;field in list&quot; }
  }
  if [missing] in [alsomissing] {
    mutate { add_tag =&gt; &quot;shouldnotexist&quot; }
  }
  if !(&quot;foo&quot; in [&quot;hello&quot;, &quot;world&quot;]) {
    mutate { add_tag =&gt; &quot;shouldexist&quot; }
  }
}  
</code></pre>
<p>在单个条件中指定多个表达式：</p>
<pre><code class="language-sh">if [loglevel] == &quot;ERROR&quot; and [deployment] == &quot;production&quot; {
    pagerduty {
    ...
    }
}
</code></pre>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
