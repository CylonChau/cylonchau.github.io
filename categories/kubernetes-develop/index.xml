<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>kubernetes develop on Cylon&#39;s Collection</title>
    <link>https://www.oomkill.com/categories/kubernetes-develop/</link>
    <description>Recent content in kubernetes develop on Cylon&#39;s Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 23 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://www.oomkill.com/categories/kubernetes-develop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>源码分析 - Kubernetes中的事件通知机制</title>
      <link>https://www.oomkill.com/2023/06/kubernetes-event/</link>
      <pubDate>Fri, 23 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/06/kubernetes-event/</guid>
      <description>文章分析了Kuberentes事件记录及事件是如何调度的</description>
      <content:encoded><![CDATA[<p>在 Kubernetes 中 事件 ( <em>Event</em> ） 通常被大家认知为是展示集群中发生的情况，通常用作 Pod 的查看，例如为什么 CrashBackOff, 为什么 Pendding，而很少有人知道事件在 Kubernetes 整个系统中的设计是非常巧妙的，可以通过各组件间的传递，使得用户可以知道集群中的情况，文章中将一地揭开Kubernetes to神秘面纱。</p>
<h2 id="为什么需要事件">为什么需要事件</h2>
<p>Kubernetes 在设计时就是 “声明式”，而声明式的最大特点就是 “多组件的协同工作”，而在多组件协同工作时，势必需要传递一些事件，以告知用户任务的状态如何；而事件本身上是一种资源，在很早版本就以及被移入 api/v1 中。下面是 “事件” 资源的定义。</p>
<p>位于 <a href="vendor/k8s.io/api/core/v1/types.go">vendor/k8s.io/api/core/v1/types.go</a> ，因为 <code>vendor/k8s.io</code> 实际上是做了一个软连接，那么真实的实际上位于 <code>{kubernetes_repo}/staging/src/k8s.io/api/core/v1</code></p>
<pre><code class="language-go">type Event struct {
	metav1.TypeMeta `json:&quot;,inline&quot;`
	// 标准的元数据
	metav1.ObjectMeta `json:&quot;metadata&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`

	// 事件涉及的对象
	InvolvedObject ObjectReference `json:&quot;involvedObject&quot; protobuf:&quot;bytes,2,opt,name=involvedObject&quot;`

	// 这里表示的是事件原因，通常为简短的的一种状态名称
	// TODO: provide exact specification for format.
	// +optional
	Reason string `json:&quot;reason,omitempty&quot; protobuf:&quot;bytes,3,opt,name=reason&quot;`

	// 以人类可读取的方式描述，类似于 tcmpdump -A
	// TODO: decide on maximum length.
	// +optional
	Message string `json:&quot;message,omitempty&quot; protobuf:&quot;bytes,4,opt,name=message&quot;`

	// 报告事件的组件，通常包含这个结构体包含 “组件+主机名” 的结构
	// +optional
	Source EventSource `json:&quot;source,omitempty&quot; protobuf:&quot;bytes,5,opt,name=source&quot;`

	// 首次上报事件的事件
	// +optional
	FirstTimestamp metav1.Time `json:&quot;firstTimestamp,omitempty&quot; protobuf:&quot;bytes,6,opt,name=firstTimestamp&quot;`

	// 最近一次记录事件的事件
	// +optional
	LastTimestamp metav1.Time `json:&quot;lastTimestamp,omitempty&quot; protobuf:&quot;bytes,7,opt,name=lastTimestamp&quot;`

	// 事件发生的次数
	// +optional
	Count int32 `json:&quot;count,omitempty&quot; protobuf:&quot;varint,8,opt,name=count&quot;`

    // 事件的类型(Normal, Warning)
	// +optional
	Type string `json:&quot;type,omitempty&quot; protobuf:&quot;bytes,9,opt,name=type&quot;`

	// 首次观察到事件的.
	// +optional
	EventTime metav1.MicroTime `json:&quot;eventTime,omitempty&quot; protobuf:&quot;bytes,10,opt,name=eventTime&quot;`

    // 事件相关的序列，如果事件为单例事件，那么则为nil
	// +optional
	Series *EventSeries `json:&quot;series,omitempty&quot; protobuf:&quot;bytes,11,opt,name=series&quot;`

	// 对事件对象采取的行动
	// +optional
	Action string `json:&quot;action,omitempty&quot; protobuf:&quot;bytes,12,opt,name=action&quot;`

	// Optional secondary object for more complex actions.
	// +optional
	Related *ObjectReference `json:&quot;related,omitempty&quot; protobuf:&quot;bytes,13,opt,name=related&quot;`

	// 发出事件的对应的控制器，也可以理解为组件，因为通常controller-manager 包含多个控制器
    // e.g. `kubernetes.io/kubelet`.
	// +optional
	ReportingController string `json:&quot;reportingComponent&quot; protobuf:&quot;bytes,14,opt,name=reportingComponent&quot;`

	// 控制器实例的ID, e.g. `kubelet-xyzf`.
	// +optional
	ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;`
}
</code></pre>
<h2 id="事件管理器">事件管理器</h2>
<p>通过上面知道了事件这个资源的设计，里面存在一个 ”发出事件的对应的控制器“ 那么必然是作为每一个组件的内置功能，也就是说这可以作为 client-go 中的一个组件。</p>
<p>代码 <a href="vendor/k8s.io/client-go/tools/events/interfaces.go">vendor/k8s.io/client-go/tools/events/interfaces.go</a> 中定义了一个事件管理器，这将定义了如何接收或发送事件到任何地方，例如事件接收器 (<em>EventSink</em>) 或 log</p>
<pre><code class="language-go">type EventBroadcaster interface {
    // 发送从指定的eventBroadcaster接收到的事件
	StartRecordingToSink(stopCh &lt;-chan struct{})

    // 返回一个 EventRecorder 并可以使用发送事件到 EventBroadcaster，并将事件源设置为给定的事件源。
	NewRecorder(scheme *runtime.Scheme, reportingController string) EventRecorder

    // StartEventWatcher 可以使在不使用 StartRecordingToSink 的情况下发送事件
    // 这使得可以通过自定义方式记录事件
    // NOTE: 在使用 eventHandler 接收到的事件时应先进行复制一份。
	// TODO: figure out if this can be removed.
	StartEventWatcher(eventHandler func(event runtime.Object)) func()

    // StartStructuredLogging 可以接收 EventBroadcaster 发送的结构化日志功能
    // 如果需要可以忽略返回值或使用于停止记录
	StartStructuredLogging(verbosity klog.Level) func()

    // 关闭广播
	Shutdown()
}
</code></pre>
<p>EventBroadcaster 的实现只有一个 eventBroadcasterImpl</p>
<pre><code>type eventBroadcasterImpl struct {
   *watch.Broadcaster
   mu            sync.Mutex
   eventCache    map[eventKey]*eventsv1.Event
   sleepDuration time.Duration
   sink          EventSink
}
</code></pre>
<p>这里面最重要的就是 sink，sink就是决定如何去存储事件的一个组件，他返回的是一组 client-go 的 REST 客户端。</p>
<h3 id="事件管理器的设计">事件管理器的设计</h3>
<h3 id="事件生产者">事件生产者</h3>
<p>事件生产者在事件管理器中是作为</p>
<h2 id="控制器">控制器</h2>
<p>service的资源创建很奇妙，继不属于 <code>controller-manager</code> 组件，也不属于 <code>kube-proxy</code> 组件，而是存在于 <code>apiserver</code> 中的一个被成为控制器的组件；而这个控制器又区别于准入控制器。更准确来说，准入控制器是位于kubeapiserver中的组件，而 <strong>控制器</strong> 则是存在于单独的一个包，这里包含了很多kubernetes集群的公共组件的功能，其中就有service。这也就是在操作kubernetes时 当 <code>controller-manager</code> 于  <code>kube-proxy</code> 未工作时，也可以准确的为service分配IP。</p>
<p>首先在构建出apiserver时，也就是代码 <a href="cmd/kube-apiserver/app/server.go">cmd/kube-apiserver/app/server.go</a></p>
<pre><code class="language-go">serviceIPRange, apiServerServiceIP, err := master.ServiceIPRange(s.PrimaryServiceClusterIPRange)
if err != nil {
    return nil, nil, nil, nil, err
}
</code></pre>
<p><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/pkg/master/services.go#L34-L54" target="_blank"
   rel="noopener nofollow noreferrer" >master.ServiceIPRange</a> 承接了为service分配IP的功能，这部分逻辑就很简单了</p>
<pre><code class="language-go">func ServiceIPRange(passedServiceClusterIPRange net.IPNet) (net.IPNet, net.IP, error) {
	serviceClusterIPRange := passedServiceClusterIPRange
	if passedServiceClusterIPRange.IP == nil {
		klog.Warningf(&quot;No CIDR for service cluster IPs specified. Default value which was %s is deprecated and will be removed in future releases. Please specify it using --service-cluster-ip-range on kube-apiserver.&quot;, kubeoptions.DefaultServiceIPCIDR.String())
		serviceClusterIPRange = kubeoptions.DefaultServiceIPCIDR
	}

	size := integer.Int64Min(utilnet.RangeSize(&amp;serviceClusterIPRange), 1&lt;&lt;16)
	if size &lt; 8 {
		return net.IPNet{}, net.IP{}, fmt.Errorf(&quot;the service cluster IP range must be at least %d IP addresses&quot;, 8)
	}

	// Select the first valid IP from ServiceClusterIPRange to use as the GenericAPIServer service IP.
	apiServerServiceIP, err := utilnet.GetIndexedIP(&amp;serviceClusterIPRange, 1)
	if err != nil {
		return net.IPNet{}, net.IP{}, err
	}
	klog.V(4).Infof(&quot;Setting service IP to %q (read-write).&quot;, apiServerServiceIP)

	return serviceClusterIPRange, apiServerServiceIP, nil
}
</code></pre>
<p>而后kube-apiserver为service分为两类</p>
<ul>
<li>apiserver 地址在集群内的service，在代码中表示为 <a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L351" target="_blank"
   rel="noopener nofollow noreferrer" >APIServerServiceIP</a></li>
<li><a href="https://github.com/kubernetes/kubernetes/blob/58178e7f7aab455bc8de88d3bdd314b64141e7ee/cmd/kube-apiserver/app/server.go#L352" target="_blank"
   rel="noopener nofollow noreferrer" >Service</a>，<code>--service-cluster-ip-range</code> 配置指定的ip，通过『逗号』分割可以为两个</li>
</ul>
<p>有了对 service 更好的理解后，接下来开始本系列第二节<a href="https://cylonchau.github.io/kubernetes-without-service.html" target="_blank"
   rel="noopener nofollow noreferrer" >深入理解Kubernetes service - kube-proxy软件架构分析</a></p>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>dual-stack service</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
