<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>kubernetes network on Cylon&#39;s Collection</title>
    <link>https://www.oomkill.com/categories/kubernetes-network/</link>
    <description>Recent content in kubernetes network on Cylon&#39;s Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 28 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://www.oomkill.com/categories/kubernetes-network/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linux网络栈</title>
      <link>https://www.oomkill.com/2022/10/network-stack/</link>
      <pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/10/network-stack/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="linux-架构概述-supa-href11asup">Linux 架构概述 <sup><a href="#1">[1]</a></sup></h2>
<p>本章节简单阐述Linux系统的结构，并讨论子系统中的模块之间以及与其他子系统之间的关系。</p>
<p>Linux内核本身鼓励无用，是作为一个操作系统的一部分参与的，只有为一个整体时他才是一个有用的实体，下图展示了Linux操作系统的分层</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221028214224508.png" alt="image-20221028214224508" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Linux子系统分层图</center>
<center><em>Source：</em>https://docs.huihoo.com/linux/kernel/a1/index.html</center><br>
<p>由图可以看出Linux操作系统由四部分组成：</p>
<ul>
<li>用户应用</li>
<li>OS服务，操作系统的一部分（例如shell）内核编程接口等</li>
<li>内核</li>
<li>硬件控制器，CPU、内存硬件、硬盘和NIC等都数据这部分</li>
</ul>
<h2 id="linux内核阐述">Linux内核阐述</h2>
<p>Linux内核将所有硬件抽象为一致的接口，为用户进程提供了一个虚拟接口，使用户无需知道计算机上安装了哪些物理硬件即可编写进程，并且Linux支持用户进程的多任务处理，每个进程都可以视作为操作系统的唯一进程独享硬件资源。内核负责维护多个用户进程，并协调其对硬件资源的访问，使得每个进程都可以公平的访问资源，并保证进程间安全。</p>
<p>Linux内核主要为五个子系统组成：</p>
<ul>
<li>进程调度器(<em><strong>SCHED</strong></em>)， 控制进程对 CPU 的访问。调度程序执行策略，确保进程可以公平地访问 CPU。</li>
<li>内存管理器 (<em><strong>MM</strong></em>)， 允许多个进程安全地共享操作系统的内存</li>
<li>虚拟文件系统 (<em><strong>VFS</strong></em>)，向所有设备提供通用文件接口来抽象出各种硬件设备</li>
<li>网络接口 (<em><strong>NET</strong></em>)，提供对多种网络标准与各种网络硬件的访问</li>
<li>进程间通信 (<em><strong>IPC</strong></em>)，在单个操作系统上的多种机制进程间通信机制</li>
</ul>
<h2 id="网络子系统架构-supa-href22asup">网络子系统架构 <sup><a href="#2">[2]</a></sup></h2>
<p>网络子系统功能主要是允许 Linux 系统通过网络连接到其他系统。支持多种硬件设备，以及可以使用的多种网络协议。网络子系统抽象了这两个实现细节，以便用户进程和其他内核子系统可以访问网络，而不必知道使用什么物理设备或协议。</p>
<p>子系统模块包含</p>
<ul>
<li>网络设备驱动层 (<em><strong>Network device drivers</strong></em>)，网络设备驱动程序与硬件设备通信。每个硬件设备都有对应的设备驱动程序模块。</li>
<li>独立设备接口层(<em><strong>device independent interface</strong></em>)，设备独立接口提供了所有硬件设备的统一视图，因此在网络子系统之上的级别无需了解硬件信息</li>
<li>网络协议层 (<em><strong>network protocol</strong></em>)，网络协议实现了网络传输的协议</li>
<li>协议独立/无关接口层 (<em><strong>protocol independent interface</strong></em>)，提供了独立于硬件设备的网络接口，为内核内其他子系统访问网络时不依赖特定的协议和硬件接口。</li>
<li>系统调用层 (<em><strong>system call</strong></em>) 用于限制用户进程导出资源的访问</li>
</ul>
<p>网络子系统的结构图如下图所示，</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221029170129050.png" alt="image-20221029170129050" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：网络子系统中的上下文</center>
<center><em>Source：</em>https://docs.huihoo.com/linux/kernel/a1/index.html</center><br>
<p>当网络子系统转换为网络栈时，如下图所示</p>
<p><img loading="lazy" src="https://miro.medium.com/max/700/1*LcGaDm_ZOCbrIerM2UDj0g.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：ISO Stack与TCP/IP Stack</center>
<center><em>Source：</em>https://www.washington.edu/R870/Networking.html</center><br>
<p>当然Linux网络子系统是类似于TCP/IP栈的一种结构，当发生一个网络传输时，数据包会按照所经过的层进行封装。例如应用层应用提供了REST API，那么应用将要传输的数据封装为HTTP协议，然后传递给向下的传输层。传输层是TCP协议就会被添加对应的TCP包头。整个封装过程原始包保持不变，会根据所经过层的不同增加固定格式的包头。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/understandlni_1304.jpg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：数据包传输在每层被封装的过程</center>
<center><em>Source：</em>http://www.embeddedlinux.org.cn/linux_net/0596002556/understandlni-CHP-13-SECT-1.html</center><br>
<p>对于Linux来说TCP/IP 的五层结构则是构成网络子系统的的核心组件，下图是Linux网络栈结构图</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221030005922405.png" alt="image-20221030005922405" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Linux网络栈的结构图</center>
<center><em>Source：</em>https://medium.com/geekculture/linux-networking-deep-dive-731848d791c0</center><br>
<ul>
<li>图中橙色部分是位于TCP/IP的五层结构中的应用层，应用层向下通讯通过 <code>system call</code> 与 socket接口进行交互</li>
<li>蓝色部分是位于内核空间，socket向下则是传输层与网络层</li>
<li>最底层是物理层包含网卡驱动与NIC</li>
</ul>
<p>通过图可以看出，NIC是发送与接收数据包的基本单位，当系统启动时内核通过驱动程序向操作系统注册网卡，当数据包到达网卡时，被放入队列中。内核通过硬中断，运行中断处理程序，为网络帧分配内核数据结构(sk_buff)，并将其拷贝到缓冲区中，此为内核与网卡交互的过程。</p>
<p>网卡硬中断只处理网卡核心数据的读取或发送，网络协议栈中的大部分处理都在软中断中进行处理。内核协议栈将从缓冲区中取出网络帧，通过网络协议栈，从下到上的根据网络栈结构逐层处理这个网络帧。</p>
<h2 id="socket-supa-href44asup">Socket <sup><a href="#4">[4]</a></sup></h2>
<p>Unix Socket是一种使用了Unix文件描述符的IPC机制，在网络栈中是位于内核空间网络栈的一层，是一个用户空间与传输层之间的一个接口，可以为网络连接, 文本文件, 终端或其他；他的行为很像一个文件描述符，因为信息的读写，<code>read()</code>, <code>write()</code>与文件的方式很相似。下图是socket通信模型。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221031203257214.png" alt="image-20221031203257214" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：socket通信模型</center>
<center><em>Source：</em>https://slideplayer.com/slide/10740698/</center><br>
<p>作为用户空间到内核空间的第一层，Socket位于两层之间，由于IPC机制支持不同的通讯协议以及需要对不同的网络协议进行访问，故这些协议实现为位于socket的层，这种情况下，用户空间仅通过系统调用socket接口，而内核空间负责一些其他工作，例如，缓冲区管理，标准协议接口，网络接口与各种不同的网络协议。</p>
<blockquote>
<p>Notes:</p>
<ul>
<li><code>/etc/protocols</code> 定义的协议号</li>
<li><code>/etc/services</code> 定义的服务的端口号</li>
</ul>
</blockquote>
<h2 id="网络栈的工作原理">网络栈的工作原理</h2>
<p>当网络包到达时，网卡（硬中断+DMA）通过DMA将网络数据包放入队列中，告知中断程序硬中断已收到网络数据包。</p>
<h3 id="数据包的发送">数据包的发送</h3>
<p>用户程序发送网络包时，通过网络栈模型自上而下逐层处理帧：</p>
<ul>
<li>应用层：通过系统调用，调用socket API发送网络包，会被限制在内核空间的socket层，socket层将数据包放入到缓冲区内。</li>
<li>传输层：网络栈从socket取出数据包，传输层添加TCP标头</li>
<li>网络层：将IP添加到数据标头，根据MTU大小分片</li>
<li>数据链路层：MAC地址寻址，并添加到帧头尾，将帧放入发送队列，触发软中断通知</li>
<li>物理层：网卡驱动通过DMA从发送队列读取网络帧，通过网卡发送出</li>
</ul>
<h3 id="数据包的接收">数据包的接收</h3>
<p>内核网络栈从缓冲区读取帧，通过网络栈模型自下而上逐层处理帧：</p>
<ul>
<li>数据链路层：
<ul>
<li>检查数据包的有效性</li>
<li>确定网络协议类型 IPV4 or IPV6</li>
<li>去除帧 头, 尾</li>
</ul>
</li>
<li>网络层：
<ul>
<li>取出IP头，确定网络流量的方向（转发或者本机流量）</li>
<li>删除标头，传递给传输层</li>
</ul>
</li>
<li>传输层：取出TCP/UDP协议头，根据源IP, 目的IP, 源端口, 目的端口作为标识找到socket，将数据报文放置socket缓冲区</li>
<li>应用层：应用程序通过socket来读数据</li>
</ul>
<p>下图为网络栈收/发数据的结构图</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221031210517384.png" alt="image-20221031210517384" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Linux网络进程接收网络数据包流程图</center>
<center><em>Source：</em>https://slideplayer.com/slide/10740698/</center><br>
<h2 id="网络子系统分层结构">网络子系统分层结构</h2>
<p>在了解了网络接受网络数据包的流程后，还需要对网络子系统中分层结构进行了解，在该结构中将需要基础掌握一些对于工作与网络子系统中的API的命令是如何调用的。</p>
<p>下图是结合 《深入理解Linux网络技术内幕》第13章 <sup><a href="#3">[3]</a></sup> 中插图13-2与 托马斯格拉夫发表于2019年的文章 &ldquo;How to Make Linux Microservice-Aware with Cilium and eBPF&rdquo; <sup><a href="#5">[5]</a></sup> 的结合旨在让零基础同学可以更好的了解到各API的分层调用</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221104153434745.png" alt="image-20221104153434745" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Linux网络子系统分层调用</center><br>
<p>图中可以看出，是一个基于TCP/IP栈的调用模型，其中应用层包含了常用的工具：</p>
<ul>
<li>配置IP路由：<code>ip</code></li>
<li>ip防火墙（包过滤）：<code>iptables</code></li>
<li>流量整形：<code>tc</code></li>
<li>网络抓包：<code>tcpdump</code></li>
<li>网卡信息：<code>ethtool</code></li>
</ul>
<p>对于云原生网络中，了解完整的分层是非常重要的，这将有利于开发基于eBPF服务。下面就简单的论证下该图</p>
<p>正如图中所示，所有的网络命令都是提供给用户的用户空间API，当发生网络动作时是需要通过内核将数据导入/出，这里使用了系统调用，调用内核提供的导入到用户空间的接口，例如 <code>socket</code>，<code>sysctl</code> 等，更多的接口介绍可以详见《深入理解Linux网络技术内幕》第3章 <sup><a href="#6">[6]</a></sup></p>
<p>到达socket后，继续向下通信时，socket提供了几种级别的接口，这些可以在常见编程语言包中被提供</p>
<ul>
<li><code>AE_PACKAGE</code> / <code>PE_PACKAGE</code>：提供设备级别的API，通俗来讲，就是在网络层之下发送/接受消息的接口，工作于2层，这将允许用户在用户空间实现物理层数据包发送和接收</li>
<li><code>AF_INET</code> / <code>PE_INET</code>：是基于网络层Socket类型，<code>AF_INET</code>是指IPv4，<code>AF_INET6</code> 是IPv6，这里就是IP 地址和端口号。</li>
</ul>
<p>如图所示，对于 <code>PE_PACKAGE</code> 套接字类型而言，Linux在链路层捕捉帧并将其注入至链路层的方式，这样跳过了所有的中间层，例如 <code>tcpdump</code> 与 <code>ethtool</code>， <code>PE_PACKAGE</code> 套接字通过将帧直接交给 <code>dev_queue_xmit</code>。</p>
<ul>
<li><code>dev_queue_xmit</code> 是传输 buffer (<code>sk_buff</code>) 到网络设备中的函数，将封包传递给TC或QoS层，L3封包时调用</li>
</ul>
<p>接下来是iptables，netfilter，是工作与多层协议栈中一系列hook，用户端由命令行工具iptables/nftables控制，可以在数据包经由的数据点上被调用对应的hook函数来改变包的行为。所有的数据包都独立存在于对应的协议栈，经过的数据包会便利所有对应的hook，因为iptables(etables)支持工作于L2的ARP协议。所有的hook都存在与每个网络名称空间内，并且每个网络设备都拥有ingress hook，这也是云原生网络中提到的为什么使用eBPF 跳过netfilter框架可以提升网络性能。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/understandlni_1801.jpg" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Linux 栈中经由netfilter框架示意图</center>
<center><em>Source：</em>http://www.embeddedlinux.org.cn/linux_net/0596002556/understandlni-CHP-18-SECT-1.html</center><br>
<p>接下来就是传统的一些应用，例如telnet，ping都是使用了<code>AE_PACKAGE</code> / <code>PE_PACKAGE</code> 传统联网模式</p>
<p>最后一个点就是 <em><strong>traffic control</strong></em> TC，是工作与L2的一组队列与其机制组成的，通常情况下是一个队列，上面也提到，所有的设备都是使用队列来调度底层设备进入的数据包，liunx中默认的队列是 <code>qdisc </code> 。</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://docs.huihoo.com/linux/kernel/a1/index.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Conceptual Architecture of the Linux Kernel</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://medium.com/geekculture/linux-networking-deep-dive-731848d791c0" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Linux — Networking Deep Dive</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="http://www.embeddedlinux.org.cn/linux_net/0596002556/understandlni-CHP-13-SECT-1.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Network Stack Chapter13</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://notes.shichao.io/tcpv1/ch10/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>User Datagram Protocol (UDP) and IP Fragmentation</strong></em></a></p>
<p><sup id="5">[5]</sup> <a href="https://www.infoq.com/presentations/linux-cilium-ebpf/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>How to Make Linux Microservice-Aware with Cilium and eBPF</strong></em></a></p>
<p><sup id="6">[6]</sup> <a href="http://www.embeddedlinux.org.cn/linux_net/0596002556/understandlni-CHP-3-SECT-1.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Network Stack Chapter13</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>为什么网络是分层的</title>
      <link>https://www.oomkill.com/2022/10/network-unit-in-osi/</link>
      <pubDate>Fri, 28 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/10/network-unit-in-osi/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview-supa-href11asup">Overview <sup><a href="#1">[1]</a></sup></h2>
<p>协议数据单元 <em><strong>Protocol Data Unit</strong></em> (PDU) 是应用于OSI模型中的数据结构，在OSI模型中每一层都会被添加一个header，tailer进行封装，header, tailer加原始报文的组合就是PDU。</p>
<p>在每层中，PDU的名称都是不同的，这也是很多人的疑问，一会数据报文称为数据包，一会数据报文成为数据帧，该文介绍网络中的单元，以了解之间的区别</p>
<h2 id="物理层">物理层</h2>
<p>物理层数据的呈现方式是以 “位” (<em><strong>bit</strong></em>) 为单位的，即0 1，在该层中数据以二进制形式进行传输</p>
<h2 id="数据链路层-supa-href22asup">数据链路层 <sup><a href="#2">[2]</a></sup></h2>
<p>到达数据链路层，实际上可以说进入了TCP/IP栈对底层，而该层的单位为 ”帧“ (<em><strong>frame</strong></em>)，该层中，MAC地址会被封装到数据包中，比如以太网帧，PPP帧都是指该层的数据包</p>
<p>该层中数据帧包含：</p>
<ul>
<li>源MAC</li>
<li>目的MAC</li>
<li>数据，由网络层给出的</li>
<li>数据的总长度</li>
<li>校验序列</li>
</ul>
<h2 id="网络层-supa-href33asup">网络层 <sup><a href="#3">[3]</a></sup></h2>
<p>在网络层中协议数据单元被称为数据 “包&quot; (<em><strong>package</strong></em>) ，是网络间节点通讯的基本单位。该层中IP地址会被封装到数据包内。</p>
<p>该层中数据包包含：</p>
<ul>
<li>标头：源IP，目的IP，协议，数据包编号，帮助数据包匹配网络的位</li>
<li>payload：数据包的主体</li>
<li>标尾：包含几个位，用于告知已到达数据包的末尾与错误检查（循环冗余检查 (<em><strong>CRC</strong></em>)）</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/eyJidWNrZXQiOiJjb250ZW50Lmhzd3N0YXRpYy5jb20iLCJrZXkiOiJnaWZcL3F1ZXN0aW9uNTI1LXBhY2tldC5naWYiLCJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjI5MH0sInRvRm9ybWF0IjoiYXZpZiJ9fQ%3D%3D" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：数据包组成</center>
<center><em>Source：</em>https://computer.howstuffworks.com/question525.htm</center><br>
<p>例如一个电子邮件，假设电子邮件大小尾3500bit，发送时使用1024的固定大小数据包进行发送，那么每个数据包标头为 96bits，标尾为 32bit，剩余 896bits 将用于实际的数据大小。这里为3500bits，会被分为4个数据包，前三个数据包为 896bits，最后一个数据包大小为 812bits。接收端会根据包编号进行解包重组</p>
<h2 id="传输层">传输层</h2>
<h3 id="segment">Segment</h3>
<p>在传输层TCP协议的协议数据单元被称为 ”段“  (<em><strong>Segment</strong></em>) ，上面讲到，IP数据包会以固定大小的数据包进行发送，如果超出大小的会被划分为多个数据包，每个数据包的碎片就被称之为<em><strong>Segment</strong></em>。</p>
<p>数据包分割通常会发生在该层，当发生下列场景时会需要分段</p>
<ul>
<li>数据包大于网络支持的最大传输单元 (<em><strong>MTU</strong></em>)</li>
<li>网络不可靠，将数据包分为更小的包</li>
</ul>
<h3 id="datagram-supa-href44asup">datagram <sup><a href="#4">[4]</a></sup></h3>
<p>在传输层UDP协议的协议数据单元被称为 ”数据报“ (<em><strong>datagram</strong></em>) ，datagram是一种逐层增加的设计，用于无连接通讯</p>
<p>下图是一个UDP数据报被封装位一个IP数据包：IPv4字段值位17 表示udp协议</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/figure_10-1_600.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：udp的IP包</center>
<center><em>Source：</em>https://notes.shichao.io/tcpv1/ch10</center><br>
<p>对于udp数据报的组成包含header与payload，udp的header大小为固定的8字节</p>
<ul>
<li><strong>源端口</strong>：可选</li>
<li><strong>目的端口</strong>：识别接收信息的进程</li>
<li><strong>Length</strong>：udp header + udp payload的长度，最小值为8</li>
<li><strong>checksum</strong>：与lenght一样其实是多余的，因为第三层包含了这两个信息</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/figure_10-2_600.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：udp数据报组成</center>
<center><em>Source：</em>https://notes.shichao.io/tcpv1/ch10</center><br>
<blockquote>
<p>Notes：使用UDP时应注意避免分段，例如帧中MTU为 1500 字节，假设 IPv4 header为 20 字节，UDP header 为 8 字节，则应用程序最多为数据留下 1472 字节以避免碎片</p>
</blockquote>
<h2 id="数据">数据</h2>
<p>对于传输层之上，协议数据单元没有特定的名词，可以统称为协议数据单元或者数据，整个PDU分层结构图如下图所示，其中 T 表示 Trailer，H 表示 Header，通常H包含源地址和目的地址及一些用于管理通信的控制信息。T含错误检查之类的信息或标志 PDU 结束的字段。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/page108.gif" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：PDU分层结构图</center>
<center><em>Source：</em>http://www.telecomworld101.com/Intro2dcRev2/page108.html</center><br>
<blockquote>
<p>Notes：每层的数据字段都由上一层PDU组成，通常情况下，每层只知道自己该层的信息，如网络层仅知道对端网络层，而不知道数据链路层或传输层</p>
</blockquote>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="http://www.telecomworld101.com/Intro2dcRev2/page108.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>Protocol Data Unit</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://www.slashroot.in/difference-between-segments-packets-and-frames" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>difference between segments packets and frames</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://computer.howstuffworks.com/question525.htm" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>What is a packet?</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://notes.shichao.io/tcpv1/ch10/" target="_blank"
   rel="noopener nofollow noreferrer" ><em><strong>User Datagram Protocol (UDP) and IP Fragmentation</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>详述Kubernetes网络模型</title>
      <link>https://www.oomkill.com/2022/08/kubernetes-network-model/</link>
      <pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/08/kubernetes-network-model/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>本文将深入探讨Kubernetes中的网络模型，以及对各种网络模型进行分析。</p>
<h2 id="underlay-network-model">Underlay Network Model</h2>
<h3 id="什么是underlay-network">什么是Underlay Network</h3>
<p>底层网络 <em>Underlay Network</em> 顾名思义是指网络设备基础设施，如交换机，路由器, <em>DWDM</em> 使用网络介质将其链接成的物理网络拓扑，负责网络之间的数据包传输。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/download.png" alt="典型的底层网络" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Underlay network topology</center>
<center><em>Source：</em>https://community.cisco.com/t5/data-center-switches/understanding-underlay-and-overlay-networks/td-p/4295870</center><br>
<p><em>underlay network</em> 可以是二层，也可以是三层；二层 <em>underlay network</em> 的典型例子是以太网 <em>Ethernet</em>，三层是 <em>underlay network</em> 的典型例子是互联网 <em>Internet</em>。</p>
<p>而工作与二层的技术是 <em>vlan</em>，工作在三层的技术是由 <em>OSPF</em>, <em>BGP</em> 等协议组成</p>
<h3 id="kubernetes中的underlay-network">kubernetes中的underlay network</h3>
<p>在kubernetes中，<em>underlay network</em> 是将宿主机作为路由器设备而，Pod 的网络则通过学习成路由条目从而实现跨节点通讯。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220820230021593.png" alt="image-20220820230021593" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：underlay network topology in kubernetes</center><br>
<p>这种模型下典型的有 <em>flannel</em> 的 <em>host-gw</em> 模式与 <em>calico</em> <em>BGP</em> 模式。</p>
<h4 id="flannel-host-gw-supa-href11asup">flannel host-gw <sup><a href="#1">[1]</a></sup></h4>
<p><em>flannel host-gw</em> 模式中每个Node需要在同一个二层网络中，并将Node作为一个路由器，跨节点通讯将通过路由表方式进行，这样方式下将网络模拟成一个<em>underlay network</em>。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/SwitchDiagram1_update_feb16.jpeg" alt="网络交换机图 1" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：layer2 ethernet topology</center>
<center><em>Source：</em>https://www.auvik.com/franklyit/blog/layer-3-switches-layer-2/</center><br>
<blockquote>
<p>Notes：因为是通过路由方式，集群的cidr至少要配置16，因为这样可以保证，跨节点的Node作为一层网络，同节点的Pod作为一个网络。如果不是这种用情况，路由表处于相同的网络中，会存在网络不可达</p>
</blockquote>
<h4 id="calico-bgp-supa-href22asup">Calico BGP <sup><a href="#2">[2]</a></sup></h4>
<p>BGP（<em>Border Gateway Protocol</em>）是去中心化自治路由协议。它是通过维护IP路由表或&rsquo;前缀&rsquo;表来实现AS （<em>Autonomous System</em>）之间的可访问性，属于向量路由协议。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/ti020109.gif" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：BGP network topology</center>
<center><em>Source：</em>https://infocenter.nokia.com/public/7705SAR214R1A/index.jsp?topic=%2Fcom.sar.routing_protocols%</center><br>
<p>与 <em>flannel</em> 不同的是，<em>Calico</em> 提供了的 <em>BGP</em> 网络解决方案，在网络模型上，<em>Calico</em> 与 <em>Flannel host-gw</em> 是近似的，但在软件架构的实现上，<em>flannel</em> 使用 <em>flanneld</em> 进程来维护路由信息；而 <em>Calico</em> 是包含多个守护进程的，其中 <em>Brid</em> 进程是一个 <em>BGP</em> 的客户端 与路由反射器(<em>Router Reflector</em>)，<em>BGP</em> 客户端负责从 <em>Felix</em> 中获取路由并分发到其他 <em>BGP Peer</em>，而反射器在BGP中起了优化的作用。在同一个IBGP中，BGP客户端仅需要和一个 <em>RR</em> 相连，这样减少了<em>AS</em>内部维护的大量的BGP连接。通常情况下，<em>RR</em> 是真实的路由设备，而 <em>Bird</em> 作为 <em>BGP</em> 客户端工作。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/cisco-nx-os-calico-network-design_0.jpeg" alt="相关图片、图表或屏幕截图" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Calico Network Architecture</center>
<center><em>Source：</em>https://www.cisco.com/c/en/us/td/docs/dcn/whitepapers/cisco-nx-os-calico-network-design.html</center><br>
<h4 id="ipvlan--macvlan-supa-href44asup">IPVLAN &amp; MACVLAN <sup><a href="#4">[4]</a></sup></h4>
<p><em>IPVLAN</em> 和 <em>MACVLAN</em> 是一种网卡虚拟化技术，两者之间的区别为， <em>IPVLAN</em> 允许一个物理网卡拥有多个IP地址，并且所有的虚拟接口用同一个MAC地址；而 <em>MACVLAN</em> 则是相反的，其允许同一个网卡拥有多个MAC地址，而虚拟出的网卡可以没有IP地址。</p>
<p>因为是网卡虚拟化技术，而不是网络虚拟化技术，本质上来说属于 <em>Overlay network</em>，这种方式在虚拟化环境中与<em>Overlay network</em> 相比最大的特点就是可以将Pod的网络拉平到Node网络同级，从而提供更高的性能、低延迟的网络接口。本质上来说其网络模型属于下图中第二个。</p>
<ul>
<li>虚拟网桥：创建一个虚拟网卡对(veth pair)，一头栽容器内，一头栽宿主机的root namespaces内。这样一来容器内发出的数据包可以通过网桥直接进入宿主机网络栈，而发往容器的数据包也可以经过网桥进入容器。</li>
<li>多路复用：使用一个中间网络设备，暴露多个虚拟网卡接口，容器网卡都可以介入这个中间设备，并通过MAC/IP地址来区分packet应该发往哪个容器设备。</li>
<li>硬件交换，为每个Pod分配一个虚拟网卡，这样一来，Pod与Pod之间的连接关系就会变得非常清晰，因为近乎物理机之间的通信基础。如今大多数网卡都支持SR-IOV功能，该功能将单一的物理网卡虚拟成多个VF接口，每个VF接口都有单独的虚拟PCIe通道，这些虚拟的PCIe通道共用物理网卡的PCIe通道。</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/7a021d86-virtual-networking-1024x380.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Virtual networking modes: bridging, multiplexing and SR-IOV</center>
<center><em>Source：</em>https://thenewstack.io/hackers-guide-kubernetes-networking/</center><br>
<p>在kubernetes中 <em>IPVLAN</em> 这种网络模型下典型的CNI有，multus 与 danm。</p>
<h5 id="multus">multus</h5>
<p><em>multus</em> 是 intel 开源的CNI方案，是由传统的 <em>cni</em> 与 <em>multus</em>，并且提供了 SR-IOV CNI 插件使 K8s pod 能够连接到 SR-IOV VF 。这是使用了 <em>IPVLAN/MACVLAN</em> 的功能。</p>
<p>当创建新的Pod后，SR-IOV 插件开始工作。配置 VF 将被移动到新的 CNI 名称空间。该插件根据 CNI 配置文件中的 “name” 选项设置接口名称。最后将VF状态设置为UP。</p>
<p>下图是一个 Multus 和 SR-IOV CNI 插件的网络环境，具有三个接口的 pod。</p>
<ul>
<li><em>eth0</em> 是 <em>flannel</em> 网络插件，也是作为Pod的默认网络</li>
<li>VF 是主机的物理端口 <em>ens2f0</em> 的实例化。这是英特尔X710-DA4上的一个端口。 在Pod端的 VF 接口名称为 <em>south0</em> 。</li>
<li>这个VF使用了 DPDK 驱动程序，此 VF 是从主机的物理端口 <em>ens2f1</em> 实例化出的。这个是英特尔® X710-DA4上另外一个端口。 Pod 内的 VF 接口名称为 <em>north0</em>。该接口绑定到 DPDK 驱动程序 <em>vfio-pci</em> 。</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220817154334081.png" alt="image-20220817154334081" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Mutus networking Architecture overlay and SR-IOV</center>
<center><em>Source：</em>https://builders.intel.com/docs/networkbuilders/enabling_new_features_in_kubernetes_for_NFV.pdf</center><br>
<blockquote>
<p><strong>Notes：terminology</strong></p>
<ul>
<li>NIC：network interface card，网卡</li>
<li>SR-IOV：single root I/O virtualization，硬件实现的功能，允许各虚拟机间共享PCIe设备。</li>
<li>VF：Virtual Function，基于PF，与PF或者其他VF共享一个物理资源。</li>
<li>PF：PCIe Physical Function，拥有完全控制PCIe资源的能力</li>
<li>DPDK：Data Plane Development Kit</li>
</ul>
</blockquote>
<p>于此同时，也可以将主机接口直接移动到Pod的网络名称空间，当然这个接口是必须存在，并且不能是与默认网络使用同一个接口。这种情况下，在普通网卡的环境中，就直接将Pod网络与Node网络处于同一个平面内了。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/multus02.png" alt="布鲁吉" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Mutus networking Architecture overlay and ipvlan</center>
<center><em>Source：</em>https://devopstales.github.io/kubernetes/multus/</center><br>
<h5 id="danm">danm</h5>
<p>DANM是诺基亚开源的CNI项目，目的是将电信级网络引入kubernetes中，与multus相同的是，也提供了SR-IOV/DPDK 的硬件技术，并且支持IPVLAN.</p>
<h2 id="overlay-network-model">Overlay Network Model</h2>
<h3 id="什么是overlay">什么是Overlay</h3>
<p>叠加网络是使用网络虚拟化技术，在 <em>underlay</em> 网络上构建出的虚拟逻辑网络，而无需对物理网络架构进行更改。本质上来说，<em>overlay network</em> 使用的是一种或多种隧道协议 (<em>tunneling</em>)，通过将数据包封装，实现一个网络到另一个网络中的传输，具体来说隧道协议关注的是数据包（帧）。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/Example-Overlay-Network-built-on-top-of-an-Internet-style-Underlay.png" alt="图 4：建立在 Internet 样式底层之上的示例 Overlay 网络" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：overlay network topology</center>
<center><em>Source：</em>https://www.researchgate.net/figure/Example-Overlay-Network-built-on-top-of-an-Internet-style-Underlay_fig4_230774628</center><br>
<h3 id="常见的网络隧道技术">常见的网络隧道技术</h3>
<ul>
<li>通用路由封装 ( <em>Generic Routing Encapsulation</em> ) 用于将来自 IPv4/IPv6的数据包封装为另一个协议的数据包中，通常工作与L3网络层中。</li>
<li>VxLAN (<em>Virtual Extensible LAN</em>)，是一个简单的隧道协议，本质上是将L2的以太网帧封装为L4中UDP数据包的方法，使用 <strong>4789</strong> 作为默认端口。<em>VxLAN</em> 也是 <em>VLAN</em> 的扩展对于 4096（$2^{12}$ 位 <em>VLAN ID</em>） 扩展为1600万（$2^{24}$ 位 <em>VNID</em> ）个逻辑网络。</li>
</ul>
<p>这种工作在 <em>overlay</em> 模型下典型的有 <em>flannel</em> 与 <em>calico</em> 中的的 <em>VxLAN</em>, <em>IPIP</em> 模式。</p>
<h3 id="ipip">IPIP</h3>
<p><em>IP in IP</em> 也是一种隧道协议，与 <em>VxLAN</em> 类似的是，<em>IPIP</em> 的实现也是通过Linux内核功能进行的封装。<em>IPIP</em> 需要内核模块 <code>ipip.ko</code>  使用命令查看内核是否加载IPIP模块<code>lsmod | grep ipip</code> ；使用命令<code>modprobe ipip</code> 加载。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/IPIP_Process.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：A simple IPIP network workflow</center>
<center><em>Source：</em>https://ssup2.github.io/theory_analysis/IPIP_GRE_Tunneling/</center><br>
<p>Kubernetes中 <em>IPIP</em> 与 <em>VxLAN</em> 类似，也是通过网络隧道技术实现的。与 <em>VxLAN</em> 差别就是，<em>VxLAN</em> 本质上是一个 UDP包，而 <em>IPIP</em> 则是将包封装在本身的报文包上。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220817163743182.png" alt="image-20220817163743182" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：IPIP in kubernetes</center><br>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220817163814698.png" alt="image-20220817163814698" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：IPIP packet with wireshark unpack</center><br>
<blockquote>
<p>Notes：公有云可能不允许IPIP流量，例如Azure</p>
</blockquote>
<h3 id="vxlan">VxLAN</h3>
<p>kubernetes中不管是 <em>flannel</em> 还是 <em>calico</em> VxLAN的实现都是使用Linux内核功能进行的封装，Linux 对 vxlan 协议的支持时间并不久，2012 年 Stephen Hemminger 才把相关的工作合并到 kernel 中，并最终出现在 kernel 3.7.0 版本。为了稳定性和很多的功能，你可以会看到某些软件推荐在 3.9.0 或者 3.10.0 以后版本的 kernel 上使用 <em>VxLAN</em>。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220815232351298.png" alt="image-20220815232351298" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：A simple VxLAN network topology</center><br>
<p>在kubernetes中vxlan网络，例如 <em>flannel</em>，守护进程会根据kubernetes的Node而维护 <em>VxLAN</em>，名称为 <code>flannel.1</code> 这是 <em>VNID</em>，并维护这个网络的路由，当发生跨节点的流量时，本地会维护对端 <em>VxLAN</em> 设备的MAC地址，通过这个地址可以知道发送的目的端，这样就可以封包发送到对端，收到包的对端 VxLAN设备 <code>flannel.1</code>  解包后得到真实的目的地址。</p>
<p>查看 <em>Forwarding database</em> 列表</p>
<pre><code class="language-bash">$ bridge fdb 
26:5e:87:90:91:fc dev flannel.1 dst 10.0.0.3 self permanent
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220816161418748.png" alt="image-20220816161418748" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：VxLAN in kubernetes</center><br>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220816164301428.png" alt="image-20220816164301428" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：VxLAN packet with wireshark unpack</center><br>
<blockquote>
<p>Notes：VxLAN使用的4789端口，wireshark应该是根据端口进行分析协议的，而flannel在linux中默认端口是8472，此时抓包仅能看到是一个UDP包。</p>
</blockquote>
<p>通过上述的架构可以看出，隧道实际上是一个抽象的概念，并不是建立的真实的两端的隧道，而是通过将数据包封装成另一个数据包，通过物理设备传输后，经由相同的设备（网络隧道）进行解包实现网络的叠加。</p>
<h3 id="weave-vxlan-supa-href33asup">weave vxlan <sup><a href="#3">[3]</a></sup></h3>
<p>weave也是使用了 <em>VxLAN</em> 技术完成的包的封装，这个技术在 <em>weave</em> 中称之为 <em>fastdp (fast data path)</em>，与 <em>calico</em> 和 <em>flannel</em> 中用到的技术不同的，这里使用的是 Linux 内核中的 <em>openvswitch datapath module</em>。与其他的 <em>VxLAN</em> 模型不同的是，weave对网络流量进行了加密。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/weave-net-fdp1-1024x454.png" alt="Weave Net Encapsulation" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：weave fastdp network topology</center>
<center><em>Source：</em>https://www.weave.works/docs/net/latest/concepts/fastdp-how-it-works/</center><br>
<blockquote>
<p>Notes：fastdp工作在Linux 内核版本 3.12 及更高版本，如果低于此版本的例如CentOS7，weave将工作在用户空间，weave中称之为 <em>sleeve mode</em></p>
</blockquote>
<blockquote>
<p><strong>Reference</strong></p>
<p><sup id="1">[1]</sup> <a href="https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md#host-gw" target="_blank"
   rel="noopener nofollow noreferrer" >flannel host-gw</a></p>
<p><sup id="2">[2]</sup> <a href="https://projectcalico.docs.tigera.io/networking/bgp" target="_blank"
   rel="noopener nofollow noreferrer" >calico bgp networking</a></p>
<p><sup id="3">[3]</sup> <a href="https://www.weave.works/docs/net/latest/concepts/router-encapsulation/" target="_blank"
   rel="noopener nofollow noreferrer" >calico bgp networking</a></p>
<p><sup id="4">[4]</sup> <a href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin" target="_blank"
   rel="noopener nofollow noreferrer" >sriov network</a></p>
<p><sup id="4">[5]</sup> <a href="https://github.com/nokia/danm" target="_blank"
   rel="noopener nofollow noreferrer" >danm</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
