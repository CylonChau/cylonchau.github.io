<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>troubleshooting on Cylon&#39;s Collection</title>
    <link>https://www.oomkill.com/categories/troubleshooting/</link>
    <description>Recent content in troubleshooting on Cylon&#39;s Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 13 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.oomkill.com/categories/troubleshooting/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>记录一次ceph集群故障处理记录</title>
      <link>https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/</link>
      <pubDate>Tue, 13 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="处理记录">处理记录</h2>
<p>Ceph版本：octopus</p>
<p>首先遇到問題是，业务端无法挂在 cephfs 查看内核日志发现是 <code> bad authorize reply</code> ，以为是 ceph keyring被替换了</p>
<pre><code>2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</code></pre>
<p>在排查完 keyring 后，手动尝试挂载 cephfs 提示 <code>Input/output error</code> ，此时看出是集群问题了</p>
<pre><code class="language-bash">$ mount -t ceph 10.80.20.100:6789:/tmp /tmp/ceph -o secret=AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ==,name=cephuser
mount error 5 = Input/output error
</code></pre>
<p>因为一开始看到日志是 <em>bad authorize reply</em> 以为是认证错误，重新登录了一下发现是相同的提示，这时查看 ceph status 发现集群异常，除了下面报错外，还有一个 osd down 的状态。</p>
<pre><code>$ ceph health detail
HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data redundancy: 39560/118680 objects degraded (33.333%), 201 pgs degraded, 255 pgs undersized; 4 daemons have recently crashed
[WRN] CEPHADM_HOST_CHECK_FAIL: 1 hosts fail cephadm check
    host localhost failed check: ['podman|docker (/bin/docker) is present', 'systemctl is present', 'lvcreate is present', &quot;No time sync service is running; checked for ['chrony.service', 'chronyd.service', 'systemd-timesyncd.service', 'ntpd.service', 'ntp.service']&quot;, 'ERROR: No time synchronizetion is active']
[WRN] MDS_SLOW_METADATA_IO: 1 MDSs report slow metadata IOs
    mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked &gt; 30 secs, oldest blocked for 4830 secs
[WRN] MDS_SLOW_REQUEST: 1 MDSs report slow requests
    mds.cephfs.localhost.mhlzaj(mds.0): 14 slow metadata IOs are blocked &gt; 30 secs
[WRN] MON_CLOCK_SKEW: clock skew detected on mon.localhost, mon.localhost1
    mon.localhost clock skew 29357.8s &gt; max 0.05s (latency 0.0132089s)
    mon.localhost1 clock skew 29357.8s &gt; max 0.05s (latency 0.0117421s)
[WRN] PG_DEGRADED: Degraded data redundancy: 39501/118680 objects degraded (33.333%), 189 pgs degraded, 241 pgs undersized
    pg 1.0 is stuck undersized for 22m, current state active+undersized+degraded, last acting [1]
    pg 2.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.8 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 2.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 2.1b is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.9 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
    pg 3.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
    pg 3.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
[WRN] RECENT_CRASH: 4 daemons have recently crashed
    osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
    client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</code></pre>
<ul>
<li>
<p>CEPHADM_HOST_CHECK_FAIL：一台或多台主机未通过基本 cephadm 主机检查，该检查验证 (1) 主机可访问并且可以在其中执行 cephadm，以及 (2) 主机满足基本先决条件，例如工作容器运行时（podman 或 docker）和工作时间同步。如果此测试失败，cephadm 将无法管理该主机上的服务。</p>
</li>
<li>
<p>MDS_SLOW_METADATA_IO</p>
</li>
<li>
<p>MDS_SLOW_REQUEST：N条慢请求被阻塞</p>
</li>
<li>
<p>MON_CLOCK_SKEW：运行 ceph-mon 的主机上的时钟未很好同步。如果集群检测到时钟偏差大于 mon_clock_drift_allowed，则会引发此运行状况检查。</p>
</li>
<li>
<p>PG_DEGRADED：一个或多个PG的健康状态受到了损害。一种常见的情况是，某个OSD发生故障或离线，导致PG进入降级状态。在这种情况下，数据副本的可用性会受到影响，并且Ceph集群的性能也可能下降。</p>
</li>
</ul>
<p>首先重启 chronyd 修复了时间同步的问题，因为机房内机器经常出现 chronyd 的服务导致异常，其次重启 osd 服务，让 ceph 做再平衡完成后剩下下面报错。</p>
<p>并且现象有两个：</p>
<ul>
<li>cephfs no such file or director</li>
<li>ceph orch 命令还是没有反应</li>
</ul>
<pre><code>$ ceph health detail
HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data 
[WRN] FS_DEGRADED: 1 filesystem is degraded
    fs cephfs is degraded
[WRN] MDS_SLOW_METADATA_IO: 1 MDSs slow metadata IOs
     mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked &gt; 30 secs, oldest blocked for 930 secs
[WRN] RECENT_CRASH: 4 daemons have recently crashed
    osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
    client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
[WRN] SLOW_OPS: 2 slow ops, oldset one blocked for 474 sec, mon.localhost has slow ops
</code></pre>
<p>通过 search 了一下，查询到 orch 是 MGR 模块</p>
<blockquote>
<p>The orchestrator is a MGR module, have you checked if the containers   are up and running <sup><a href="#1">[1]</a></sup></p>
</blockquote>
<p>此时操作登录对应ceph node，docker restart ceph-mgr的模块，并且重启 mds 模块</p>
<pre><code class="language-bash">$ ceph health detail
[WRN] RECENT_CRASH: 4 daemons have recently crashed
    osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
    client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</code></pre>
<p>此时集群恢复正常，cephfs 恢复</p>
<h2 id="总结">总结</h2>
<p>由于长期没有在处理 ceph 方向问题，对排查有以下生疏：</p>
<ul>
<li>无法挂载时没有及时查看 ceph 集群信息，而是盯着客户端方向日志查询了半天。</li>
<li>对 ceph 故障代码没有了解过，如果有了解，可以很明确的定位问题，而不用耽误2小时。</li>
</ul>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://lists.ceph.io/hyperkitty/list/ceph-users@ceph.io/thread/2OSO26WYFBS4HZ4LPHNMBZUQ6Y3GI6GG/" target="_blank"
   rel="noopener nofollow noreferrer" >ceph orch status hangs forever</a></p>
<p><sup id="2">[2]</sup> <a href="https://docs.ceph.com/en/quincy/rados/operations/health-checks/" target="_blank"
   rel="noopener nofollow noreferrer" >HEALTH CHECKS</a></p>
<p><sup id="3">[3]</sup> <a href="https://docs.ceph.com/en/quincy/cephfs/health-messages/?highlight=MDS_SLOW_METADATA_IO" target="_blank"
   rel="noopener nofollow noreferrer" >CEPHFS HEALTH MESSAGES</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>当cephfs和fscache结合时在K8s环境下的全集群规模故障</title>
      <link>https://www.oomkill.com/2023/11/10-1-ceph-fscache/</link>
      <pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/11/10-1-ceph-fscache/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>本文记录了在 kubernetes 环境中，使用 cephfs 时当启用了 fscache 时，由于网络问题，或者 ceph 集群问题导致的整个 k8s 集群规模的挂载故障问题。</p>
<h2 id="什么是fscache">什么是fscache</h2>
<p>fscache 是网络文件系统的通用缓存，例如 NFS, CephFS都可以使用其进行缓存从而提高 IO</p>
<p>FS-Cache是在访问之前，将整个打开的每个 netfs 文件完全加载到 Cache 中，之后的挂载是从该缓存而不是 netfs 的 inode 中提供</p>
<p>fscache主要提供了下列功能：</p>
<ul>
<li>一次可以使用多个缓存</li>
<li>可以随时添加/删除缓存</li>
<li>Cookie 分为 “卷”, “数据文件”, “缓存”
<ul>
<li>缓存 cookie 代表整个缓存，通常不可见到“网络文件系统”</li>
<li>卷 cookie 来表示一组 文件</li>
<li>数据文件 cookie 用于缓存数据</li>
</ul>
</li>
</ul>
<p>下图是一个 NFS 使用 fscache 的示意图，CephFS 原理与其类似</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/Cache-NFS-Share-Data-with-FS-Cache-1.webp" alt="Cache-NFS-Share-Data-with-FS-Cache-1" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图1：FS-Cache 架构 </center>
<center><em>Source：</em>https://computingforgeeks.com/how-to-cache-nfs-share-data-with-fs-cache-on-linux/</center><br>
<p>CephFS 也是可以被缓存的一种网络文件系统，可以通过其内核模块看到对应的依赖</p>
<pre><code class="language-bash">root@client:~# lsmod | grep ceph
ceph                  376832  1
libceph               315392  1 ceph
fscache                65536  1 ceph
libcrc32c              16384  3 xfs,raid456,libceph
root@client:~# modinfo ceph
filename:       /lib/modules/4.15.0-112-generic/kernel/fs/ceph/ceph.ko
license:        GPL
description:    Ceph filesystem for Linux
author:         Patience Warnick &lt;patience@newdream.net&gt;
author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;
author:         Sage Weil &lt;sage@newdream.net&gt;
alias:          fs-ceph
srcversion:     B2806F4EAACAC1E19EE7AFA
depends:        libceph,fscache
retpoline:      Y
intree:         Y
name:           ceph
vermagic:       4.15.0-112-generic SMP mod_unload
signat:         PKCS#7
signer:        
sig_key:       
sig_hashalgo:   md4
</code></pre>
<p>在启用了fs-cache后，内核日志可以看到对应 cephfs 挂载时 ceph 被注册到 fscache中</p>
<pre><code class="language-bash">[  11457.592011] FS-Cache: Loaded
[  11457.617265] Key type ceph registered
[  11457.617686] libceph: loaded (mon/osd proto 15/24)
[  11457.640554] FS-Cache: Netfs 'ceph' registered for caching
[  11457.640558] ceph: loaded (mds proto 32)
[  11457.640978] libceph: parse_ips bad ip 'mon1.ichenfu.com:6789,mon2.ichenfu.com:6789,mon3.ichenfu.com:6789'
</code></pre>
<blockquote>
<p>当 monitor / OSD 拒绝连接时，所有该节点后续创建的挂载均会使用缓存，除非 umount 所有挂载后重新挂载才可以重新与 ceph mon 建立连接</p>
</blockquote>
<h2 id="cephfs-中的-fscache">cephfs 中的 fscache</h2>
<p>ceph 官方在 2023年11月5日的一篇博客 <sup><a href="#1">[1]</a></sup> 中介绍了，cephfs 与 fscache 结合的介绍。这个功能的加入最显著的成功就是 ceph node 流向 OSD 网络被大大减少，尤其是在读取多的情况下。</p>
<p>这个机制可以在代码 commit 中看到其原理：“在第一次通过文件引用inode时创建缓存cookie。之后，直到我们处理掉inode，我们都不会摆脱cookie” <sup><a href="#2">[2]</a></sup></p>
<h2 id="结合fscache的kubernetes中使用cephfs造成的集群规模故障">结合fscache的kubernetes中使用cephfs造成的集群规模故障</h2>
<p>在了解了上面的基础知识后，就可以引入故障了，下面是故障产生环境的配置</p>
<h3 id="故障发生环境">故障发生环境</h3>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>Centos</td>
<td>7.9</td>
</tr>
<tr>
<td>Ceph</td>
<td>nautilus (14.20)</td>
</tr>
<tr>
<td>Kernel</td>
<td>4.18.16</td>
</tr>
</tbody>
</table>
<h3 id="故障的描述">故障的描述</h3>
<p>当网络出现问题时，如果使用了 cephfs 的 Pod 就会出现大量故障，具体故障表现方式有下面几种</p>
<ul>
<li>
<p>新部署的 Pod 处于 Waiting 状态</p>
</li>
<li>
<p>新部署的 Pod 可以启动成功，但是无法读取 cephfs 的挂载目录，主要故障表现为下面几种形式：</p>
<ul>
<li>ceph mount error 5 = input/output error <sup><a href="#3">[3]</a></sup></li>
<li>cephfs mount failure.permission denied</li>
</ul>
</li>
<li>
<p>旧 Pod 无法被删除</p>
</li>
<li>
<p>新部署的 Pod 无法启动</p>
</li>
</ul>
<blockquote>
<p>注：上面故障引用都是在网络上找到相同报错的一些提示，并不完全切合本文中故障描述</p>
</blockquote>
<p>去对应节点查看日志会发现有下面几个特征</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/IMG_20231108_150726-ink.jpeg" alt="IMG_20231108_150726-ink" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图2-1：故障发生的节点报错</center>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/IMG_20231109_193023-ink.jpeg" alt="IMG_20231109_193023-ink" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图2-2：故障发生的节点报错</center>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/IMG_20231109_192930-ink.jpeg" alt="IMG_20231109_192930-ink" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图2-3：故障发生的节点报错</center>
<pre><code class="language-log">    [ 1815.029831] ceph: mds0 closed our session
    [ 1815.029833] ceph: mds0 reconnect start
    [ 1815.052219] ceph: mds0 reconnect denied
    [ 1815.052229] ceph:  dropping dirty Fw state for ffff9d9085da1340 1099512175611
    [ 1815.052231] ceph:  dropping dirty+flushing Fw state for ffff9d9085da1340 1099512175611
    [ 1815.273008] libceph: mds0 10.99.10.4:6801 socket closed (con state NEGOTIATING)
    [ 1816.033241] ceph: mds0 rejected session
    [ 1829.018643] ceph: mds0 hung
    [ 1880.088504] ceph: mds0 came back
    [ 1880.088662] ceph: mds0 caps renewed
    [ 1880.094018] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm
    [ 1881.100367] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm
    [ 2046.768969] conntrack: generic helper won't handle protocol 47. Please consider loading the specific helper module.
    [ 2061.731126] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm
</code></pre>
<h3 id="故障分析">故障分析</h3>
<p>由上面的三张图我们可以得到几个关键点</p>
<ol>
<li>connection reset</li>
<li>session lost, hunting for new mon</li>
<li>ceph: get_quota_realm()</li>
<li>reconnection denied</li>
<li>mds1 hung</li>
<li>mds1 caps stale</li>
</ol>
<p>这三张图上的日志是一个故障恢复的顺序，而问题节点（通常为整个集群 Node）内核日志都会在刷 <code>ceph: get_quota_realm()</code> 这种日志，首先我们需要确认第一个问题，<code>ceph: get_quota_realm()</code> 是什么原因导致，在互联网上找了一个 linux kernel 关于修复这个问题的提交记录，通过 commit，我们可以看到这个函数产生的原因</p>
<blockquote>
<p>get_quota_realm() enters infinite loop if quota inode has no caps.
This can happen after client gets evicted.  <sup><a href="#4">[4]</a></sup></p>
</blockquote>
<p>这里可以看到，修复的内容是当客户端被驱逐时，这个函数会进入无限 loop ，当 inode 配额没有被授权的用户，常常发生在客户端被驱逐。</p>
<p>通过这个 commit，我们可以确定了后面 4 - 6 问题的疑问，即客户端被 ceph mds 驱逐（加入了黑名单），在尝试重连时就会发生 <code>reconnection denied</code> 接着发生陈腐的被授权认证的用户 (caps stale)。<font color="#f8070d" size=3>接着由于本身没有真实的卸载，而是使用了一个共享的 cookie 这个时候就会发生节点新挂载的目录是没有权限写，或者是  input/output error 的错误</font>，这些错误表象是根据不同情况下而定，比如说被拉黑和丢失的会话。</p>
<h3 id="kubelet的错误日志">kubelet的错误日志</h3>
<p>此时当新的使用了 volumes 去挂载 cephfs时，由于旧的 Pod 产生的工作目录 (/var/lib/kubelet) 下的 Pod 挂载会因为 cephfs caps stale  而导致无法卸载，这是就会存在 “孤儿Pod”，“不能同步 Pod 的状态”，“不能创建新的Pod挂载，因为目录已存在”。</p>
<p>kubelet 日志如下所示：</p>
<pre><code class="language-bash">kubelet_volumes.go:66] pod &quot;5446c441-9162-45e8-e11f46893932&quot; found, but error stat /var/lib/kubelet/pods/5446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxx: permission denied occurred during checking mounted volumes from disk

pod_workers.go:119] Error syncing pod &quot;5446c441-9162-45e8-e11f46893932&quot; (&quot;xxxxx-xxx-xxx-xxxx-xxx_xxxxx(5446c441-9162-45e8-e11f46893932)&quot;, skipping: failed to &quot;StartContainer&quot; for &quot;xxxxx-xxx-xxx&quot; with RunContainerError: &quot;failed to start container \&quot;719346531es654113s3216e1456313d51as132156\&quot;: Error response from daemon: error while createing mount source path '/var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx': mkdir /var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx: file exists&quot;
</code></pre>
<h2 id="问题如何解决">问题如何解决</h2>
<p>首先上面我们阐述了问题出现背景以及原因，要想解决这些错误，要分为两个步骤：</p>
<ol>
<li>首先驱逐 Kubernetes  Node 节点上所有挂载 cephfs 的 Pod，这步骤是为了优雅的结束 fscache 的 cookie cache 机制，使节点可以正常的提供服务</li>
<li>解决使用 fscache 因网络问题导致的会话丢失问题的重连现象</li>
</ol>
<p>这里主要以步骤2来阐述，解决这个问题就是通过两个方式，一个是不使用 fscache，另一个则是不让 mds 拉黑客户端，关闭 fscache 的成本很难，至今没有尝试成功，这里通过配置 ceph 服务使得 ceph mds 不会拉黑因出现网络问题丢失连接的客户端。</p>
<p>ceph 中阐述了驱逐的概念 “当某个文件系统客户端不响应或者有其它异常行为时，有必要强制切断它到文件系统的访问，这个过程就叫做<em>驱逐</em>。”  <sup><a href="#5">[5]</a></sup></p>
<p>要想解决这个问题，ceph 提供了一个参数来解决这个问题，<code>mds_session_blacklist_on_timeout</code></p>
<blockquote>
<p>It is possible to respond to slow clients by simply dropping their MDS sessions, but permit them to re-open sessions and permit them to continue talking to OSDs.  To enable this mode, set <code>mds_session_blacklist_on_timeout</code> to false on your MDS nodes. <sup><a href="#6">[6]</a></sup></p>
</blockquote>
<p>最终在配置后，上述问题解决</p>
<h3 id="附ceph-mds-管理客户端">附：ceph mds 管理客户端</h3>
<p>查看一个客户端的连接</p>
<pre><code class="language-bash">ceph daemon mds.xxxxxxxx session ls |grep -E 'inst|hostname|kernel_version'|grep xxxx
        &quot;inst&quot;: &quot;client.105123 v1:192.168.0.0:0/11243531&quot;,
            &quot;hostname&quot;: &quot;xxxxxxxxxxxxxxxxxx&quot;
</code></pre>
<p>手动驱逐一个客户端</p>
<pre><code class="language-bash">ceph tell mds.0 client evict id=105123
2023-11-12 13:25:23:381 7fa3a67fc700 0 client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231
2023-11-12 13:25:23:421 7fa3a67fc700 0 client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231
</code></pre>
<p>查看 ceph 的配置参数</p>
<pre><code class="language-bash">ceph config dump
WHO     MASK  LEVEL     OPTION                                VALUE RO
  mon         advanced  auth_allow_insecure_global_id_reclaim false
  mon         advanced  mon_allow_pool_delete                 false
  mds         advanced  mds_session_blacklist_on_evict        false
  mds         advanced  mds_session_blacklist_on_timeout      false
</code></pre>
<p>当出现问题无法卸载时应如何解决？</p>
<p>当我们遇到问题时，卸载目录会出现被占用情况，通过 mount 和 fuser 都无法卸载</p>
<pre><code class="language-bash">umount -f /tmp/998
umount： /tmp/998: target is buy.
        (In some cases useful info about processes that use th device is found by losf(8) or fuser(1))
        the device is found by losf(8) or fuser(1)
        
fuser -v1 /root/test
Cannot stat /root/test: Input/output error
</code></pre>
<p>这个时候由于 cephfs 挂载问题会导致整个文件系统不可用，例如 df -h, ls dir 等，此时可以使用 umount 的懒卸载模式 <code>umount -l</code>，这会告诉内核当不占用时被卸载，由于这个问题是出现问题，而不是长期占用，这里用懒卸载后会立即卸载，从而解决了 stuck 的问题。</p>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://ceph.io/en/news/blog/2013/first-impressions-through-fscache-and-ceph/" target="_blank"
   rel="noopener nofollow noreferrer" >First Impressions Through Fscache and Ceph</a></p>
<p><sup id="2">[2]</sup> <a href="https://lwn.net/Articles/563146/" target="_blank"
   rel="noopener nofollow noreferrer" >ceph: persistent caching with fscache</a></p>
<p><sup id="3">[3]</sup> <a href="https://tracker.ceph.com/issues/51191" target="_blank"
   rel="noopener nofollow noreferrer" >Cannot Mount CephFS No Timeout, mount error 5 = Input/output error</a></p>
<p><sup id="4">[4]</sup> <a href="https://patchwork.kernel.org/project/ceph-devel/patch/20190531122802.12814-3-zyan@redhat.com/" target="_blank"
   rel="noopener nofollow noreferrer" >ceph: fix infinite loop in get_quota_realm()</a></p>
<p><sup id="5">[5]</sup> <a href="https://drunkard.github.io/cephfs/eviction/" target="_blank"
   rel="noopener nofollow noreferrer" >Ceph 文件系统客户端的驱逐</a></p>
<p><sup id="6">[6]</sup> <a href="https://docs.ceph.com/en/mimic/cephfs/eviction/#advanced-configuring-blacklisting" target="_blank"
   rel="noopener nofollow noreferrer" >advanced-configuring-blacklisting</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ceph常用命令</title>
      <link>https://www.oomkill.com/2023/09/11-1-ceph-common-cmd/</link>
      <pubDate>Wed, 13 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/09/11-1-ceph-common-cmd/</guid>
      <description></description>
      <content:encoded><![CDATA[<h4 id="测试上传下载对象">测试上传/下载对象</h4>
<p>存取故据时，客户端必须首先连接至RAD05集群上某存储地，而后根据对像名称由相关的中CRUSH规则完成数据对象寻址。于是为了测试集群的数据存储功能，首先创建一个用于测试的存储池mypool，并设定其PG数量为16个。</p>
<pre><code class="language-sh">ceph osd pool create mypool 16 16
</code></pre>
<p>而后，即可将测试文件上传至存储池中。例如下面的<code>rados put</code>命令将/etc/hosts</p>
<p>rados</p>
<p>lspool 显示存储池</p>
<p>rmpool 删除存储池</p>
<p>mkpool 创建存储池</p>
<p>rados mkpool mypool  32 32</p>
<pre><code class="language-sh">rados mkpool {name} {pgnum} {pgpnum}
rados mkpool test 32 32
</code></pre>
<pre><code class="language-sh">$ ceph osd pool create testpool 32 32
pool 'testpool' created
</code></pre>
<p>列出存储池</p>
<pre><code>$ ceph osd pool ls
mypool
rbdpool
testpool

$ rados lspools  
mypool
rbdpool
testpool
</code></pre>
<p>而后即可将测试文件上传到存储池中，例如将<code>rados put</code>命令将<code>/etc/issue</code>文件上传至testpool存储池，对象名称仍然较保留文件名issue，而<code>rados ls</code>可以列出指定存储池中的数据对象</p>
<pre><code class="language-sh">rados put issue /etc/issue --pool=testpool    

$ rados ls --pool=testpool  # --pool 指定放入那个存储池中去
issue
</code></pre>
<p>而<font color="#f8070d" size=3><code>ceph osd map</code></font>可查看获取到存储池中数据对象的具体位置信息（数据和元数据怎么映射存储的)</p>
<pre><code class="language-sh">ceph osd map testpool issue

$ ceph osd map mypool passwd
osdmap e36 pool 'mypool' (1) object 'passwd' -&gt; pg 1.27292a34 (1.14) -&gt; up ([0,3,2], p0) acting ([0,3,2], p0)
</code></pre>
<p>    mypool存储池中的对象<code>passwd</code>被放在pg上<code>1.27292a34</code> 1为存储池编号<code>.</code>后面的编号可以理解为pg的位图。是pg的编号；<code>up ([0,3,2], p0)</code>正常可访问编号0、3、2，副本型存储池，crush算法计算得到，0为主osd。活动集<code>acting ([0,3,2], p0)</code>，此组pg(<code>pg 1.27292a34 (1.14)</code>)之下所有的osd(<code>[0,3,2]</code>)都处于正常活动状态。</p>
<h4 id="删除数据对象">删除数据对象</h4>
<pre><code>ceph osd pool rm testpool --yes-i-really-really-mean-it
</code></pre>
<p>    删除存储池命令存在数据丢失的风险，Ceph于是默认禁止此类操作。管理员需要在<font color="#f8070d" size=3><code>ceph.conf</code></font>配置文件中启用支持删除存储池的操作后，方可使用如下命令删除存储池。</p>
<pre><code class="language-bash">rados rm issue --pool=mypool
</code></pre>
<h3 id="ceph集群的访问接口">ceph集群的访问接口</h3>
<h4 id="ceph块设备接口">Ceph块设备接口</h4>
<p>Ceph块设备，也称为RADOS块设备（简称RBD），是一种基于RADOS存储系统支持超配，（thin-provisioned）、可伸缩的条带化数据存储系统，它通过librbd库与OSD进行交互。RBD为KVM等虚拟化技术和云OS（如OpenStack和CloudStack）提供高可用和无限扩展性的存储后端，这些系统以来与libvirt和QEMU实用程序与RBD进行集。</p>
<p>在集群部署完成以后，就具有了RBD接口，RBD接口关键是在客户端的配置。服务端本身可以直接使用。只需创建出存储池，在存储池中就可以创建块设备。块设备主要表现为存储池当中的镜像或映像文件（image）。</p>
<p>客户端基于librbd库即可将RADOS存储集群用作块设备，不过，用于rbd的存储池需要实现启用rbd功能并进行初始化。例如，创建一个名为rbddata的存储池，在启动rbd功能后对其进行初始化</p>
<p>对于rbdpool而言，创建完成后并不能直接使用，因为三种应用程序需要单独进行启用。相关存储池的应用才可以。</p>
<pre><code class="language-sh">ceph osd pool create rbpool 64 ## 指明pg数量
</code></pre>
<pre><code class="language-sh"># 默认情况下是裸池
$ ceph osd pool application enable rbdpool rbd
enabled application 'rbd' on pool 'rbdpool'

osd pool application enable &lt;poolname&gt; &lt;app&gt; {--yes-i-really-mean-it}             enable use of an application &lt;app&gt; [cephfs,rbd,rgw] on pool &lt;poolname&gt;
</code></pre>
<pre><code class="language-sh">
rbd pool init -p rbddata
</code></pre>
<p>不过，rbd存储池并不能直接用于块设备，而是需要事先在其中按需创建映像（image），以及可用映像、创建快照、将映像回滚到快照和查看快照等管理操作。</p>
<p>创建名为img1的映像</p>
<pre><code class="language-sh">rbd create rbdpool/img --size 1G
</code></pre>
<pre><code class="language-sh">$ rbd ls -p rbdpool
img
img1
</code></pre>
<p>显示映像的相关信息，<code>rbd info</code></p>
<pre><code class="language-sh">$ rbd info rbdpool/img
rbd image 'img':
        size 1 GiB in 256 objects
        order 22 (4 MiB objects)
        id: 38bb6b8b4567
        block_name_prefix: rbd_data.38bb6b8b4567
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features: 
        flags: 
        create_timestamp: Fri Jun 14 17:08:48 2019
</code></pre>
<p>在客户端主机上，用户通过内核级的rbd驱动识别相关设备，即可对其进行分区、创建文件系统并挂载使用。</p>
<p>Rank 层级
MDS MDS在哪台服务器 上
Pool 两个存储池，存储池都位于同一个ceph集群之上，所以看到的空间大小是一样的。</p>
<h4 id="检查集群状态">检查集群状态</h4>
<p>命令：ceph-s</p>
<p>输出信息：</p>
<ul>
<li>集群ID</li>
<li>集群运行状况</li>
<li>监视器地图版本号和监视器仲裁的状态</li>
<li>OSD map版本号和OSD的状态</li>
<li>归置组map版本</li>
<li>归置组和存储池数量</li>
<li>所存储数据理论上的数量和所存储对象的数量</li>
<li>所存储数据的总量</li>
</ul>
<h4 id="获取集群的即时状态">获取集群的即时状态</h4>
<ul>
<li>ceph pg stat</li>
<li>ceph osd pool stat</li>
<li>ceph df</li>
<li>ceph df detail</li>
</ul>
<p>ceph df</p>
<p>输出两端内容：GLOBAL和POOLS</p>
<ul>
<li>GLOBAL：存储量概览</li>
<li>POOLS：存储池列表和每个存储池的理论用量，但出不反应副本、克隆数据或快照</li>
</ul>
<p>GLOBAL段</p>
<ul>
<li>size 集群的整体存储容量</li>
<li>AVAIL 集群中可以使用的可用空间容量</li>
<li>RAW USED 已用的原始存储量</li>
<li>% RAW USED：已用的原始存储量百分比，将此数字与 full ratio和near full ratio搭配使用，可确保您不会用完集群的容量。</li>
<li></li>
</ul>
<h4 id="检查osd和mon的状态">检查OSD和Mon的状态</h4>
<p>可通过执行以下命令来检查OSD，以确保它们已启动里正在运行</p>
<ul>
<li><code>ceph osd stat</code></li>
<li><code>ceph osd dump</code>
还可以根据OSD在CRUSH map中的位置查看OSD</li>
<li><code>ceph osd tree</code>
<ul>
<li>Ceph将列显CRUSH树及主机它的OSD、OSD是否已启动及其权重</li>
</ul>
</li>
</ul>
<pre><code>$ ceph osd tree
ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF 
-1       0.09775 root default                            
-3       0.03897     host stor01                         
 0   hdd 0.01949         osd.0       up  1.00000 1.00000 
 7   hdd 0.01949         osd.7       up  1.00000 1.00000 
-5       0.01959     host stor02                         
 1   hdd 0.00980         osd.1       up  1.00000 1.00000 
 6   hdd 0.00980         osd.6       up  1.00000 1.00000 
-7       0.01959     host stor03                         
 2   hdd 0.00980         osd.2       up  1.00000 1.00000 
 5   hdd 0.00980         osd.5       up  1.00000 1.00000 
-9       0.01959     host stor04                         
 3   hdd 0.00980         osd.3       up  1.00000 1.00000 
 4   hdd 0.00980         osd.4       up  1.00000 1.00000 
</code></pre>
<p>集群中存在多个Mon主机时，应该在启动集群之后读取或写入数据之前检查Mon的种裁状态：事实上，管理员也应该定期检查这种仲裁结果。</p>
<ul>
<li>显示监视器映射：<code>ceph mon stat</code>命令或者<code>ceph mon dump</code></li>
</ul>
<pre><code>$ ceph mon stat
e3: 3 mons at {stor01=10.0.0.4:6789/0,stor02=10.0.0.5:6789/0,stor03=10.0.0.6:6789/0}, election epoch 20, leader 0 stor01, quorum 0,1,2 stor01,stor02,stor03
</code></pre>
<ul>
<li>显示伸裁状态：<code>ceph quorum status</code></li>
</ul>
<h4 id="使用管理套接字">使用管理套接字</h4>
<p>每一个socket文件能够用来直接通过它管理对应的sock背后的守护进程。</p>
<p>Ceph的管理套接字接口常用于查询守护进程。</p>
<ul>
<li>套接字默认保存于<code>/var/run/ceph</code>目录</li>
<li>此接口的使用不能以远程方式进程</li>
</ul>
<p>命令的使用格式</p>
<pre><code class="language-sh">ceph --admin-daemon /var/run/ceph/{socket-name}
</code></pre>
<p>获取使用帮助：</p>
<pre><code>ceph --admin-daemon /var/run/ceph/{socket-name}
</code></pre>
<h4 id="停止或重启ceph集群">停止或重启Ceph集群</h4>
<h5 id="停止">停止</h5>
<ul>
<li>告知Ceph集群不要将osd标记为out，命令<code>ceph osd set noout</code></li>
<li>按如下顺序停止守护进程和节点
<ul>
<li>存储客户端</li>
<li>网关，例如NFS Ganesha或对象网关</li>
<li>元数据服务器</li>
<li>Ceph OSD</li>
<li>Ceph Manager</li>
<li>Ceph Monitor</li>
</ul>
</li>
</ul>
<h5 id="启动">启动</h5>
<ul>
<li>以与停止过程相反的顺序启动节点</li>
<li>Ceph Monitor</li>
<li>Ceph Manager</li>
<li>Ceph OSD</li>
<li>元数据服务器</li>
<li>网关，例如NFS Ganesha或对象网关</li>
<li>存储客户端</li>
<li>删除noout标志，命令<code>ceph osd unset noout</code></li>
</ul>
<h4 id="ceph的配置文件">Ceph的配置文件</h4>
<h5 id="配置文件结构">配置文件结构</h5>
<ul>
<li>ceph配置文件使用ini语法格式</li>
<li>ceph在启动时会依次查找多个不同位置的配置文件，如后找的配置文件与前面发生冲突，会覆盖此前的配置信息</li>
<li>注释可通过&quot;#&quot;,&quot;;&quot;</li>
<li>配置文件主要有以下几个配置项所组成
<ul>
<li><code>[global]</code>:全局配置,影响ceph存储集群中的所有守护进程</li>
<li><code>[osd]</code>: 影响Ceph存储集群中的所有ceph-osd守护进程并覆盖全局中的相同设置</li>
<li><code>[mon]</code>: 影响ceph存储集群中的所有ceph-mon守护进程并覆盖全局中的相同设置</li>
<li><code>[client]</code>: 影响所有客户端，例如，挂载ceph块设备，ceph对象网关等</li>
</ul>
</li>
</ul>
<p>每一个独立的配置项是对所有选项生效的，如<code>[mon]</code>，如有需要对单独的选项进行配置可以使用<code>[mon.id]</code>加上id进行标识。</p>
<ul>
<li>
<p>您可以通过输入由<code>.</code>分隔的类型来指定守护程序的特定实例的配置，您可以指定该实例。 并通过实例ID</p>
</li>
<li>
<p>ceph osd守护进程的实例id总是数字，但它可能是<code>ceph monitors</code>的字母数字</p>
<ul>
<li>例如<code>[mon.a]</code>、<code>[mon.b]</code>、<code>[mon.0]</code>等</li>
</ul>
</li>
<li>
<p>按顺序包含的默认ceph配置文件位置</p>
</li>
<li>
<p>$CEPH_CONF环境变量指定的文件路径路径</p>
</li>
<li>
<p><code>-c</code> /path/ceph.conf 使用<code>-c</code>的命令行选项传递给ceph各应用程序或守护进程的命令行选项</p>
</li>
<li>
<p><code>/etc/ceph/ceph.conf</code></p>
</li>
<li>
<p><code>~/.ceph/config</code></p>
</li>
<li>
<p><code>./ceph.conf</code> 用户当前工作目录</p>
</li>
</ul>
<p>在配置文件配置时，还可以使用元变量来引用配置文件中的其他信息或引用ceph集群中的元数据信息做变量替换的。称作元参数或元变量</p>
<blockquote>
<p>常用的元参数</p>
</blockquote>
<ul>
<li><code>cluster</code>: 当前Ceph集群的名称</li>
<li><code>$type</code>: 当前服务的类型名称，可能会展开为OSD或mon</li>
<li><code>$id</code>: 进程的标识符，例如对osd.0来说，其标识符为0</li>
<li><code>$host</code>：守护进程所在的主机的主机名</li>
<li><code>$name</code>: 其值为<code>$type.$id</code></li>
</ul>
<p>进程的运行时配置</p>
<p>在进程的运行当中，设定<code>osd</code>、<code>mon</code>、<code>mgr</code>等工作特性。</p>
<p>要查看运行时配置，请登录Ceph节点并执行：</p>
<pre><code>ceph daemon {daemon-type}.{id} config show
</code></pre>
<p>获取帮助信息</p>
<pre><code class="language-sh">ceph daemon {daemon-type}.{id} help
</code></pre>
<p>在运行时获取特定配置设置</p>
<pre><code class="language-sh">ceph daemon {daemon-type}.{id} config get {parameter}

# 例如：

ceph daemon osd.0 config get public_addr
</code></pre>
<p>在运行时设置特定配置</p>
<p>设置运行时配置有两种常用方法：</p>
<ul>
<li>使用Ceph mmonitor
<ul>
<li><code>ceph tell {daemon-type}.{daemon id or *} injectargs --{name} {value} [--{name}} {value}]</code></li>
<li>例如：<code>ceph tell osd.0 injectargs '--debug-osd 0/5'</code></li>
</ul>
</li>
<li>使用 administration socket
<ul>
<li><code>ceph daemon {daemon-type}.{id} set {name} {type}</code></li>
<li>例如：<code>ceph osd.0 config set debug_osd 0/5</code></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>踩坑nginx proxy_pass GET 参数传递</title>
      <link>https://www.oomkill.com/2023/05/nginx-proxy_pass/</link>
      <pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/05/nginx-proxy_pass/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="场景">场景</h2>
<p>在配置代理后，GET 请求的变量全部失效，配置如下</p>
<pre><code class="language-conf">location /fw {
    proxy_pass http://127.0.0.1:2952;
}
</code></pre>
<p>我的需求是，<code>/fw/</code> 的都发往 2952端口，但实际情况是404，原因为“在没有指定 URI 的情况下，在1.12版本后会传递原有的URI” 这时会导致一个404错误，因为我的后端接口本身就是 <code>/fw/xxx/</code> 会出现重复</p>
<p>接下来做了一个变量传递</p>
<pre><code class="language-conf">location ~* /fw/(?&lt;section&gt;.*) {
    proxy_pass http://127.0.0.1:2952/fw/$section;
}
</code></pre>
<p>这时存在一个问题，就是 GET 请求的变量无法传递过去</p>
<h2 id="解决">解决</h2>
<p>nginx 官方给出一个样例，说明了，存在某种情况下，nginx 不会确定请求 URI 中的部分参数</p>
<ul>
<li>使用正则表达式时</li>
<li>在 localtion 名称内</li>
</ul>
<p>例如，在这个场景下，proxy_pass 就会忽略原有的请求的URI，而将拼接后的请求转发</p>
<pre><code class="language-conf">location /name/ {
    rewrite    /name/([^/]+) /users?name=$1 break;
    proxy_pass http://127.0.0.1;
}
</code></pre>
<p>那么这服务我遇到的问题，nginx官方给出了使用方式</p>
<p>当在 <code>proxy_pass</code> 中需要变量，可以使用 <code>$request_uri;</code></p>
<p>另外也可以使用 <code>$is_args$args </code>参数 来保证原有的请求参数被传递</p>
<pre><code class="language-conf">location ~* /fw/(?&lt;section&gt;.*) {
    proxy_pass http://127.0.0.1:2952/fw/$section$is_args$args;
}
</code></pre>
<blockquote>
<p>$is_args</p>
<p>“<code>?</code>”  if a request line has arguments, or an empty string otherwise</p>
<p>$args</p>
<p>arguments in the request line</p>
</blockquote>
<p>Reference</p>
<p><a href="http://nginx.org/en/docs/varindex.html" target="_blank"
   rel="noopener nofollow noreferrer" >Alphabetical index of variables</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>解决nginx在docker中报错 [rewrite or internal redirection cycle while internally redirecting to &#34;/index.html]</title>
      <link>https://www.oomkill.com/2023/05/ngx-in-docker-500/</link>
      <pubDate>Thu, 18 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/05/ngx-in-docker-500/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>vue项目部署在裸机Linux上运行正常，部署在docker中nginx出现下列错误</p>
<pre><code>Nginx &quot;rewrite or internal redirection cycle while internally redirecting to &quot;/index.html&quot;
</code></pre>
<p>表现在用户界面 500 Internal Server Error</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/202305201300482.png" alt="image-20230518215938566" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>原因：nginx配置路径不对，改成正确的后恢复</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes Pod网络排错思路</title>
      <link>https://www.oomkill.com/2022/08/pod-network-troubleshooting/</link>
      <pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2022/08/pod-network-troubleshooting/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>本文将引入一个思路：“在Kubernetes集群发生网络异常时如何排查”。文章将引入Kubernetes 集群中网络排查的思路，包含网络异常模型，常用工具，并且提出一些案例以供学习。</p>
<ul>
<li>Pod常见网络异常分类</li>
<li>网络排查工具</li>
<li>Pod网络异常排查思路及流程模型</li>
<li>CNI网络异常排查步骤</li>
<li>案例学习</li>
</ul>
<h2 id="pod网络异常">Pod网络异常</h2>
<p>网络异常大概分为如下几类：</p>
<ul>
<li>
<p><strong>网络不可达</strong>，主要现象为ping不通，其可能原因为：</p>
<ul>
<li>源端和目的端防火墙（<code>iptables</code>, <code>selinux</code>）限制</li>
<li>网络路由配置不正确</li>
<li>源端和目的端的系统负载过高，网络连接数满，网卡队列满</li>
<li>网络链路故障</li>
</ul>
</li>
<li>
<p><strong>端口不可达</strong>：主要现象为可以ping通，但telnet端口不通，其可能原因为：</p>
<ul>
<li>源端和目的端防火墙限制</li>
<li>源端和目的端的系统负载过高，网络连接数满，网卡队列满，端口耗尽</li>
<li>目的端应用未正常监听导致（应用未启动，或监听为127.0.0.1等）</li>
</ul>
</li>
<li>
<p><strong>DNS解析异常</strong>：主要现象为基础网络可以连通，访问域名报错无法解析，访问IP可以正常连通。其可能原因为</p>
<ul>
<li>Pod的DNS配置不正确</li>
<li>DNS服务异常</li>
<li>pod与DNS服务通讯异常</li>
</ul>
</li>
<li>
<p><strong>大数据包丢包</strong>：主要现象为基础网络和端口均可以连通，小数据包收发无异常，大数据包丢包。可能原因为：</p>
<ul>
<li>数据包的大小超过了 <em>dockero</em>，<em>CNI</em> 插件，或者宿主机网卡的 <em>MTU</em> 值。
<ul>
<li>可使用 <code>ping -s</code> 指定数据包大小进行测试</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>CNI异常</strong>：主要现象为Node可以通，但Pod无法访问集群地址，可能原因有：</p>
<ul>
<li><em>kube-proxy</em> 服务异常，没有生成 <em>iptables</em> 策略或者 <em>ipvs</em> 规则导致无法访问</li>
<li>CIDR耗尽，无法为Node注入 <code>PodCIDR</code> 导致 <em>CNI</em> 插件异常</li>
<li>其他 <em>CNI</em> 插件问题</li>
</ul>
</li>
</ul>
<p>那么整个Pod网络异常分类可以如下图所示：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821164836806.png" alt="image-20220821164836806" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Pod network trouble hirarchy</center><br>
<p>总结一下，Pod最常见的网络故障有，网络不可达（ping不通）；端口不可达（telnet不通）；DNS解析异常（域名不通）与大数据包丢失（大包不通）。</p>
<h2 id="常用网络排查工具">常用网络排查工具</h2>
<p>在了解到常见的网络异常后，在排查时就需要使用到一些网络工具才可以很有效的定位到网络故障原因，下面会介绍一些网络排查工具。</p>
<h3 id="tcpdump-supa-href11asup">tcpdump <sup><a href="#1">[1]</a></sup></h3>
<p>tcpdump网络嗅探器，将强大和简单结合到一个单一的命令行界面中，能够将网络中的报文抓取，输出到屏幕或者记录到文件中。</p>
<blockquote>
<p><strong>各系统下的安装</strong></p>
<ul>
<li>Ubuntu/Debian: <code>tcpdump</code>  ；<code>apt-get install -y tcpdump</code></li>
<li>Centos/Fedora: <code>tcpdump</code> ；<code>yum install -y tcpdump</code></li>
<li>Apline：<code>tcpdump </code> ；<code>apk add tcpdump --no-cache</code></li>
</ul>
</blockquote>
<p>查看指定接口上的所有通讯</p>
<p>语法</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>-i [interface]</td>
<td></td>
</tr>
<tr>
<td> -w [flle]</td>
<td>第一个n表示将地址解析为数字格式而不是主机名，第二个N表示将端口解析为数字格式而不是服务名</td>
</tr>
<tr>
<td>-n</td>
<td>不显示IP地址</td>
</tr>
<tr>
<td>-X</td>
<td>hex and ASCII</td>
</tr>
<tr>
<td>-A</td>
<td>ASCII（实际上是以人类可读懂的包进行显示）</td>
</tr>
<tr>
<td>-XX</td>
<td></td>
</tr>
<tr>
<td>-v</td>
<td>详细信息</td>
</tr>
<tr>
<td>-r</td>
<td>读取文件而不是实时抓包</td>
</tr>
<tr>
<td></td>
<td><strong>关键字</strong></td>
</tr>
<tr>
<td><strong>type</strong></td>
<td>host（主机名，域名，IP地址）, net, port, portrange</td>
</tr>
<tr>
<td><strong>direction</strong></td>
<td>src, dst, src or dst , src and ds</td>
</tr>
<tr>
<td><strong>protocol</strong></td>
<td>ether, ip，arp, tcp, udp, wlan</td>
</tr>
</tbody>
</table>
<h4 id="捕获所有网络接口">捕获所有网络接口</h4>
<pre><code class="language-bash">tcpdump -D
</code></pre>
<p>####按IP查找流量</p>
<p>最常见的查询之一 <code>host</code>，可以看到来往于 <code>1.1.1.1</code> 的流量。</p>
<pre><code class="language-bash">tcpdump host 1.1.1.1
</code></pre>
<p>####按源/目的 地址过滤</p>
<p>如果只想查看来自/向某方向流量，可以使用 <code>src</code> 和 <code>dst</code>。</p>
<pre><code class="language-bash">tcpdump src|dst 1.1.1.1
</code></pre>
<h4 id="通过网络查找数据包">通过网络查找数据包</h4>
<p>使用 <code>net</code> 选项，来要查找出/入某个网络或子网的数据包。</p>
<pre><code class="language-bash">tcpdump net 1.2.3.0/24
</code></pre>
<h4 id="使用十六进制输出数据包内容">使用十六进制输出数据包内容</h4>
<p><code>hex</code> 可以以16进制输出包的内容</p>
<pre><code class="language-bash">tcpdump -c 1 -X icmp
</code></pre>
<h4 id="查看特定端口的流量">查看特定端口的流量</h4>
<p>使用 <code>port</code> 选项来查找特定的端口流量。</p>
<pre><code class="language-bash">tcpdump port 3389
tcpdump src port 1025
</code></pre>
<h4 id="查找端口范围的流量">查找端口范围的流量</h4>
<pre><code class="language-bash">tcpdump portrange 21-23
</code></pre>
<h4 id="过滤包的大小">过滤包的大小</h4>
<p>如果需要查找特定大小的数据包，可以使用以下选项。你可以使用 <code>less</code>，<code>greater</code>。</p>
<pre><code class="language-bash">tcpdump less 32
tcpdump greater 64
tcpdump &lt;= 128
</code></pre>
<h4 id="捕获流量输出为文件">捕获流量输出为文件</h4>
<p><code>-w</code>  可以将数据包捕获保存到一个文件中以便将来进行分析。这些文件称为<code>PCAP</code>（PEE-cap）文件，它们可以由不同的工具处理，包括 <code>Wireshark</code> 。</p>
<pre><code class="language-bash">tcpdump port 80 -w capture_file
</code></pre>
<h4 id="组合条件">组合条件</h4>
<p>tcpdump也可以结合逻辑运算符进行组合条件查询</p>
<ul>
<li>
<p><strong>AND</strong>
<em><code>and</code></em> or <code>&amp;&amp;</code></p>
</li>
<li>
<p><strong>OR</strong>
<em><code>or</code></em> or <code>||</code></p>
</li>
<li>
<p><strong>EXCEPT</strong>
<em><code>not</code></em> or <code>!</code></p>
</li>
</ul>
<pre><code class="language-bash">tcpdump -i eth0 -nn host 220.181.57.216 and 10.0.0.1  # 主机之间的通讯
tcpdump -i eth0 -nn host 220.181.57.216 or 10.0.0.1
# 获取10.0.0.1与 10.0.0.9或 10.0.0.1 与10.0.0.3之间的通讯
tcpdump -i eth0 -nn host 10.0.0.1 and \(10.0.0.9 or 10.0.0.3\)
</code></pre>
<h4 id="原始输出">原始输出</h4>
<p>并显示人类可读的内容进行输出包（不包含内容）。</p>
<pre><code class="language-bash">tcpdump -ttnnvvS -i eth0 
tcpdump -ttnnvvS -i eth0 
</code></pre>
<h4 id="ip到端口">IP到端口</h4>
<p>让我们查找从某个IP到端口任何主机的某个端口所有流量。</p>
<pre><code class="language-bash">tcpdump -nnvvS src 10.5.2.3 and dst port 3389
</code></pre>
<h4 id="去除特定流量">去除特定流量</h4>
<p>可以将指定的流量排除，如这显示所有到192.168.0.2的 非ICMP的流量。</p>
<pre><code class="language-bash">tcpdump dst 192.168.0.2 and src net and not icmp
</code></pre>
<p>来自非指定端口的流量，如，显示来自不是SSH流量的主机的所有流量。</p>
<pre><code class="language-bash">tcpdump -vv src mars and not dst port 22
</code></pre>
<h4 id="选项分组">选项分组</h4>
<p>在构建复杂查询时，必须使用单引号 <code>'</code>。单引号用于忽略特殊符号 <code>()</code> ，以便于使用其他表达式（如host, port, net等）进行分组。</p>
<pre><code class="language-bash">tcpdump 'src 10.0.2.4 and (dst port 3389 or 22)'
</code></pre>
<h4 id="过滤tcp标记位">过滤TCP标记位</h4>
<p>TCP RST</p>
<p>The filters below find these various packets because tcp[13] looks at offset 13 in the TCP header, the number represents the location within the byte, and the !=0 means that the flag in question is set to 1, i.e. it’s on.</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 4!=0'
tcpdump 'tcp[tcpflags] == tcp-rst'
</code></pre>
<p>TCP SYN</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 2!=0'
tcpdump 'tcp[tcpflags] == tcp-syn'
</code></pre>
<p>同时忽略SYN和ACK标志的数据包</p>
<pre><code class="language-bash">tcpdump 'tcp[13]=18'
</code></pre>
<p>TCP URG</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 32!=0'
tcpdump 'tcp[tcpflags] == tcp-urg'
</code></pre>
<p>TCP ACK</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 16!=0'
tcpdump 'tcp[tcpflags] == tcp-ack'
</code></pre>
<p>TCP PSH</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 8!=0'
tcpdump 'tcp[tcpflags] == tcp-push'
</code></pre>
<p>TCP FIN</p>
<pre><code class="language-bash">tcpdump 'tcp[13] &amp; 1!=0'
tcpdump 'tcp[tcpflags] == tcp-fin'
</code></pre>
<h4 id="查找http包">查找http包</h4>
<p>查找 <code>user-agent</code> 信息</p>
<pre><code class="language-bash">tcpdump -vvAls0 | grep 'User-Agent:'
</code></pre>
<p>查找只是 <code>GET</code> 请求的流量</p>
<pre><code class="language-bash">tcpdump -vvAls0 | grep 'GET'
</code></pre>
<p>查找http客户端IP</p>
<pre><code class="language-bash">tcpdump -vvAls0 | grep 'Host:'
</code></pre>
<p>查询客户端cookie</p>
<pre><code class="language-bash">tcpdump -vvAls0 | grep 'Set-Cookie|Host:|Cookie:'
</code></pre>
<h4 id="查找dns流量">查找DNS流量</h4>
<pre><code class="language-bash">tcpdump -vvAs0 port 53
</code></pre>
<h4 id="查找对应流量的明文密码">查找对应流量的明文密码</h4>
<pre><code class="language-bash">tcpdump port http or port ftp or port smtp or port imap or port pop3 or port telnet -lA | egrep -i -B5 'pass=|pwd=|log=|login=|user=|username=|pw=|passw=|passwd= |password=|pass:|user:|username:|password:|login:|pass |user '
</code></pre>
<h4 id="wireshark追踪流">wireshark追踪流</h4>
<p>wireshare追踪流可以很好的了解出在一次交互过程中都发生了那些问题。</p>
<p>wireshare选中包，右键选择 “追踪流“ 如果该包是允许的协议是可以打开该选项的</p>
<h4 id="关于抓包节点和抓包设备">关于抓包节点和抓包设备</h4>
<p>如何抓取有用的包，以及如何找到对应的接口，有以下建议</p>
<p><strong>抓包节点</strong>：</p>
<p>通常情况下会在==源端==和==目的端==两端同时抓包，观察数据包是否从源端正常发出，目的端是否接收到数据包并给源端回包，以及源端是否正常接收到回包。如果有丢包现象，则沿网络链路上各节点抓包排查。例如，A节点经过c节点到B节点，先在AB两端同时抓包，如果B节点未收到A节点的包，则在c节点同时抓包。</p>
<p><strong>抓包设备</strong>：</p>
<p>对于 Kubernetes 集群中的Pod，由于容器内不便于抓包，通常视情况在Pod数据包经过的veth设备，<em>docker0</em> 网桥，<em>CNI</em> 插件设备（如cni0，flannel.1 etc..）及Pod所在节点的网卡设备上指定Pod IP进行抓包。选取的设备根据怀疑导致网络问题的原因而定，比如范围由大缩小，从源端逐渐靠近目的端，比如怀疑是 <em>CNI</em> 插件导致，则在 <em>CNI</em> 插件设备上抓包。从pod发出的包逐一经过veth设备，<em>cni0</em> 设备，<em>flannel0</em>，宿主机网卡，到达对端，抓包时可按顺序逐一抓包，定位问题节点。</p>
<blockquote>
<p>需要注意在不同设备上抓包时指定的源目IP地址需要转换，如抓取某Pod时，ping <em>{host}</em> 的包，在 <em>veth</em> 和 <em>cni0</em> 上可以指定 Pod IP抓包，而在宿主机网卡上如果仍然指定Pod IP会发现抓不到包，因为此时Pod IP已被转换为宿主机网卡IP。</p>
</blockquote>
<p>下图是一个使用 <em>VxLAN</em> 模式的 <em>flannel</em> 的跨界点通讯的网络模型，在抓包时需要注意对应的网络接口</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821200501496.png" alt="image-20220821200501496" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：VxLAN in kubernetes</center><br>
<h3 id="nsenter">nsenter</h3>
<p>nsenter是一款可以进入进程的名称空间中。例如，如果一个容器以非 root 用户身份运行，而使用 <code>docker exec</code> 进入其中后，但该容器没有安装 <code>sudo</code> 或未 <code>netstat</code> ，并且您想查看其当前的网络属性，如开放端口，这种场景下将如何做到这一点？<em><strong>nsenter</strong></em> 就是用来解决这个问题的。</p>
<p><strong>nsenter</strong> (<em>namespace enter</em>) 可以在容器的宿主机上使用 <em>nsenter</em> 命令进入容器的命名空间，以容器视角使用宿主机上的相应网络命令进行操作。==当然需要拥有 <em>root</em> 权限==</p>
<blockquote>
<p><strong>各系统下的安装</strong> <sup><a href="#2">[2]</a></sup></p>
<ul>
<li>Ubuntu/Debian: <code>util-linux</code>  ；<code>apt-get install -y util-linux</code></li>
<li>Centos/Fedora: <code>util-linux</code> ；<code>yum install -y util-linux</code></li>
<li>Apline：<code>util-linux</code> ；<code>apk add util-linux --no-cache</code></li>
</ul>
</blockquote>
<p><em>nsenter</em> 的使用语法为，<code>nsenter -t pid -n &lt;commond&gt;</code>，<code>-t</code> 接 进程ID号，<code>-n</code> 表示进入名称空间内，<code>&lt;commond&gt;</code> 为执行的命令。更多的内容可以参考 <sup><a href="#3">[3]</a></sup></p>
<p>实例：如我们有一个Pod进程ID为30858，进入该Pod名称空间内执行 <code>ifconfig</code> ，如下列所示</p>
<pre><code class="language-bash">$ ps -ef|grep tail
root      17636  62887  0 20:19 pts/2    00:00:00 grep --color=auto tail
root      30858  30838  0 15:55 ?        00:00:01 tail -f

$ nsenter -t 30858 -n ifconfig
eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1480
        inet 192.168.1.213  netmask 255.255.255.0  broadcast 192.168.1.255
        ether 5e:d5:98:af:dc:6b  txqueuelen 0  (Ethernet)
        RX packets 92  bytes 9100 (8.8 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 92  bytes 8422 (8.2 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 5  bytes 448 (448.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 5  bytes 448 (448.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

net1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 10.1.0.201  netmask 255.255.255.0  broadcast 10.1.0.255
        ether b2:79:f9:dd:2a:10  txqueuelen 0  (Ethernet)
        RX packets 228  bytes 21272 (20.7 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 216  bytes 20272 (19.7 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>
<h4 id="如何定位pod名称空间">如何定位Pod名称空间</h4>
<p>首先需要确定Pod所在的节点名称</p>
<pre><code class="language-bash">$ kubectl get pods -owide |awk '{print $1,$7}'
NAME NODE
netbox-85865d5556-hfg6v master-machine
netbox-85865d5556-vlgr4 node01
</code></pre>
<p>如果Pod不在当前节点还需要用IP登录则还需要查看IP（可选）</p>
<pre><code class="language-bash">$ kubectl get pods -owide |awk '{print $1,$6,$7}'
NAME IP NODE
netbox-85865d5556-hfg6v 192.168.1.213 master-machine
netbox-85865d5556-vlgr4 192.168.0.4 node01
</code></pre>
<p>接下来，登录节点，获取容器lD，如下列所示，每个pod默认有一个 <em>pause</em> 容器，其他为用户yaml文件中定义的容器，理论上所有容器共享相同的网络命名空间，排查时可任选一个容器。</p>
<pre><code class="language-bash">$ docker ps |grep netbox-85865d5556-hfg6v
6f8c58377aae   f78dd05f11ff                                                    &quot;tail -f&quot;                45 hours ago   Up 45 hours             k8s_netbox_netbox-85865d5556-hfg6v_default_4a8e2da8-05d1-4c81-97a7-3d76343a323a_0
b9c732ee457e   registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1   &quot;/pause&quot;                 45 hours ago   Up 45 hours             k8s_POD_netbox-85865d5556-hfg6v_default_4a8e2da8-05d1-4c81-97a7-3d76343a323a_0
</code></pre>
<p>接下来获得获取容器在节点系统中对应的进程号，如下所示</p>
<pre><code class="language-bash">$ docker inspect --format &quot;{{ .State.Pid }}&quot; 6f8c58377aae
30858
</code></pre>
<p>最后就可以通过 <em>nsenter</em> 进入容器网络空间执行命令了</p>
<h3 id="paping">paping</h3>
<p><strong>paping</strong> 命令可对目标地址指定端口以TCP协议进行连续ping，通过这种特性可以弥补 <em>ping</em> ICMP协议，以及 <em>nmap</em> , <em>telnet</em> 只能进行一次操作的的不足；通常情况下会用于测试端口连通性和丢包率</p>
<p>paping download：<a href="https://code.google.com/archive/p/paping/" target="_blank"
   rel="noopener nofollow noreferrer" >paping</a></p>
<p><em>paping</em> 还需要安装以下依赖，这取决于你安装的 <em>paping</em> 版本</p>
<ul>
<li>RedHat/CentOS：<code>yum install -y libstdc++.i686 glibc.i686</code></li>
<li>Ubuntu/Debian：最小化安装无需依赖</li>
</ul>
<pre><code class="language-bash">$ paping -h
paping v1.5.5 - Copyright (c) 2011 Mike Lovell

Syntax: paping [options] destination

Options:
 -?, --help     display usage
 -p, --port N   set TCP port N (required)
     --nocolor  Disable color output
 -t, --timeout  timeout in milliseconds (default 1000)
 -c, --count N  set number of checks to N
</code></pre>
<h3 id="mtr">mtr</h3>
<p><strong>mtr</strong> 是一个跨平台的网络诊断工具，将 <strong>traceroute</strong> 和 <strong>ping</strong> 的功能结合到一个工具。与 <em>traceroute</em> 不同的是 <em>mtr</em> 显示的信息比起 <em>traceroute</em> 更加丰富：通过 <em>mtr</em> 可以确定网络的条数，并且可以同时打印响应百分比以及网络中各跳跃点的响应时间。</p>
<blockquote>
<p><strong>各系统下的安装</strong> <sup><a href="#2">[2]</a></sup></p>
<ul>
<li>Ubuntu/Debian: <code>mtr</code>  ；<code>apt-get install -y mtr</code></li>
<li>Centos/Fedora: <code>mtr</code> ；<code>yum install -y mtr</code></li>
<li>Apline：<code>mtr</code> ；<code>apk add mtr --no-cache</code></li>
</ul>
</blockquote>
<h4 id="简单的使用示例">简单的使用示例</h4>
<p>最简单的示例，就是后接域名或IP，这将跟踪整个路由</p>
<pre><code class="language-bash">$ mtr google.com

Start: Thu Jun 28 12:10:13 2018
HOST: TecMint                     Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- 192.168.0.1                0.0%     5    0.3   0.3   0.3   0.4   0.0
  2.|-- 5.5.5.211                  0.0%     5    0.7   0.9   0.7   1.3   0.0
  3.|-- 209.snat-111-91-120.hns.n 80.0%     5    7.1   7.1   7.1   7.1   0.0
  4.|-- 72.14.194.226              0.0%     5    1.9   2.9   1.9   4.4   1.1
  5.|-- 108.170.248.161            0.0%     5    2.9   3.5   2.0   4.3   0.7
  6.|-- 216.239.62.237             0.0%     5    3.0   6.2   2.9  18.3   6.7
  7.|-- bom05s12-in-f14.1e100.net  0.0%     5    2.1   2.4   2.0   3.8   0.5
</code></pre>
<p><code>-n</code> 强制 <em>mtr</em> 打印 IP地址而不是主机名</p>
<pre><code class="language-bash">$ mtr -n google.com

Start: Thu Jun 28 12:12:58 2018
HOST: TecMint                     Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- 192.168.0.1                0.0%     5    0.3   0.3   0.3   0.4   0.0
  2.|-- 5.5.5.211                  0.0%     5    0.9   0.9   0.8   1.1   0.0
  3.|-- ???                       100.0     5    0.0   0.0   0.0   0.0   0.0
  4.|-- 72.14.194.226              0.0%     5    2.0   2.0   1.9   2.0   0.0
  5.|-- 108.170.248.161            0.0%     5    2.3   2.3   2.2   2.4   0.0
  6.|-- 216.239.62.237             0.0%     5    3.0   3.2   3.0   3.3   0.0
  7.|-- 172.217.160.174            0.0%     5    3.7   3.6   2.0   5.3   1.4
</code></pre>
<p><code>-b</code> 同时显示IP地址与主机名</p>
<pre><code class="language-bash">$ mtr -b google.com

Start: Thu Jun 28 12:14:36 2018
HOST: TecMint                     Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- 192.168.0.1                0.0%     5    0.3   0.3   0.3   0.4   0.0
  2.|-- 5.5.5.211                  0.0%     5    0.7   0.8   0.6   1.0   0.0
  3.|-- 209.snat-111-91-120.hns.n  0.0%     5    1.4   1.6   1.3   2.1   0.0
  4.|-- 72.14.194.226              0.0%     5    1.8   2.1   1.8   2.6   0.0
  5.|-- 108.170.248.209            0.0%     5    2.0   1.9   1.8   2.0   0.0
  6.|-- 216.239.56.115             0.0%     5    2.4   2.7   2.4   2.9   0.0
  7.|-- bom07s15-in-f14.1e100.net  0.0%     5    3.7   2.2   1.7   3.7   0.9
</code></pre>
<p><code>-c</code> 跟一个具体的值，这将限制 <em>mtr</em> ping的次数，到达次数后会退出</p>
<pre><code class="language-bash">$ mtr -c5 google.com
</code></pre>
<p>如果需要指定次数，并且在退出后保存这些数据，使用 <code>-r</code> flag</p>
<pre><code class="language-bash">$ mtr -r -c 5 google.com &gt;  1
$ cat 1
Start: Sun Aug 21 22:06:49 2022
HOST: xxxxx.xxxxx.xxxx.xxxx Loss%   Snt   Last   Avg  Best  Wrst StDev
  1.|-- gateway                    0.0%     5    0.6 146.8   0.6 420.2 191.4
  2.|-- 212.xx.21.241              0.0%     5    0.4   1.0   0.4   2.3   0.5
  3.|-- 188.xxx.106.124            0.0%     5    0.7   1.1   0.7   2.1   0.5
  4.|-- ???                       100.0     5    0.0   0.0   0.0   0.0   0.0
  5.|-- 72.14.209.89               0.0%     5   43.2  43.3  43.1  43.3   0.0
  6.|-- 108.xxx.250.33             0.0%     5   43.2  43.1  43.1  43.2   0.0
  7.|-- 108.xxx.250.34             0.0%     5   43.7  43.6  43.5  43.7   0.0
  8.|-- 142.xxx.238.82             0.0%     5   60.6  60.9  60.6  61.2   0.0
  9.|-- 142.xxx.238.64             0.0%     5   59.7  67.5  59.3  89.8  13.2
 10.|-- 142.xxx.37.81              0.0%     5   62.7  62.9  62.6  63.5   0.0
 11.|-- 142.xxx.229.85             0.0%     5   61.0  60.9  60.7  61.3   0.0
 12.|-- xx-in-f14.1e100.net  0.0%     5   59.0  58.9  58.9  59.0   0.0
</code></pre>
<p>默认使用的是 ICMP 协议 <code>-i</code> ，可以指定 <code>-u</code>,  <code>-t</code> 使用其他协议</p>
<pre><code class="language-bash">mtr --tcp google.com
</code></pre>
<p><code>-m</code> 指定最大的跳数</p>
<pre><code class="language-bash">mtr -m 35 216.58.223.78
</code></pre>
<p><code>-s</code> 指定包的大小</p>
<h4 id="mtr输出的数据">mtr输出的数据</h4>
<table>
<thead>
<tr>
<th>colum</th>
<th>describe</th>
</tr>
</thead>
<tbody>
<tr>
<td>last</td>
<td>最近一次的探测延迟值</td>
</tr>
<tr>
<td>avg</td>
<td>探测延迟的平均值</td>
</tr>
<tr>
<td>best</td>
<td>探测延迟的最小值</td>
</tr>
<tr>
<td>wrst</td>
<td>探测延迟的最大值</td>
</tr>
<tr>
<td>stdev</td>
<td>标准偏差。越大说明相应节点越不稳定</td>
</tr>
</tbody>
</table>
<h4 id="丢包判断">丢包判断</h4>
<p>任一节点的 <code>Loss%</code>（丢包率）如果不为零，则说明这一跳网络可能存在问题。导致相应节点丢包的原因通常有两种。</p>
<ul>
<li>运营商基于安全或性能需求，人为限制了节点的ICMP发送速率，导致丢包。</li>
<li>节点确实存在异常，导致丢包。可以结合异常节点及其后续节点的丢包情况，来判定丢包原因。</li>
</ul>
<blockquote>
<p>Notes:</p>
<ul>
<li>如果随后节点均没有丢包，则通常说明异常节点丢包是由于运营商策略限制所致。可以忽略相关丢包。</li>
<li>如果随后节点也出现丢包，则通常说明节点确实存在网络异常，导致丢包。对于这种情况，如果异常节点及其后续节点连续出现丢包，而且各节点的丢包率不同，则通常以最后几跳的丢包率为准。如链路测试在第5、6、7跳均出现了丢包。最终丢包情况以第7跳作为参考。</li>
</ul>
</blockquote>
<h4 id="延迟判断">延迟判断</h4>
<p>由于链路抖动或其它因素的影响，节点的 <em>Best</em> 和 <em>Worst</em> 值可能相差很大。而 <em>Avg</em>（平均值）统计了自链路测试以来所有探测的平均值，所以能更好的反应出相应节点的网络质量。而 <em>StDev</em>（标准偏差值）越高，则说明数据包在相应节点的延时值越不相同（越离散）。所以标准偏差值可用于协助判断 <em>Avg</em> 是否真实反应了相应节点的网络质量。例如，如果标准偏差很大，说明数据包的延迟是不确定的。可能某些数据包延迟很小（例如：25ms），而另一些延迟却很大（例如：350ms），但最终得到的平均延迟反而可能是正常的。所以此时 <em>Avg</em> 并不能很好的反应出实际的网络质量情况。</p>
<p>这就需要结合如下情况进行判断：</p>
<ul>
<li>如果 <em>StDev</em> 很高，则同步观察相应节点的 <em>Best</em> 和 <em>wrst</em>，来判断相应节点是否存在异常。</li>
<li>如果<em>StDev</em> 不高，则通过Avg来判断相应节点是否存在异常。</li>
</ul>
<blockquote>
<p>Tips：对于更多的网络工具的使用可以参考这篇<a href="https://www.cnblogs.com/Cylon/p/14946935.html" target="_blank"
   rel="noopener nofollow noreferrer" >文章</a></p>
</blockquote>
<h2 id="pod网络排查流程">Pod网络排查流程</h2>
<p>Pod网络异常时排查思路，可以按照下图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821225201747.png" alt="image-20220821225201747" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：Pod network exception troubleshooting idea</center><br>
<h2 id="案例学习">案例学习</h2>
<h3 id="扩容节点访问service地址不通">扩容节点访问service地址不通</h3>
<p><strong>测试环境k8s节点扩容后无法访问集群clusterlP类型的registry服务</strong></p>
<p>环境信息：</p>
<table>
<thead>
<tr>
<th>IP</th>
<th>Hostname</th>
<th>role</th>
</tr>
</thead>
<tbody>
<tr>
<td>10.153.204.15</td>
<td>yxxx-xxx-xxxfu12</td>
<td>worknode节点（本次扩容的问题节点）</td>
</tr>
<tr>
<td>10.153.203.14</td>
<td>yxxx-xxx-xxxxfu31</td>
<td>master节点</td>
</tr>
<tr>
<td>10.61.187.42</td>
<td>yxxx-xxx-xxxxxxxxf8e9</td>
<td>master节点</td>
</tr>
<tr>
<td>10.61.187.48</td>
<td>yxxx-xxx-xxxxxx61e25</td>
<td>master节点（本次registry服务pod所在<br/>节点）</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p>cni插件：flannel vxlan</p>
</li>
<li>
<p>kube-proxy工作模式为iptables</p>
</li>
<li>
<p>registry服务</p>
<ul>
<li>单实例部署在10.61.187.48:5000</li>
<li>Pod IP：10.233.65.46，</li>
<li>Cluster IP：10.233.0.100</li>
</ul>
</li>
</ul>
<p>现象：</p>
<ul>
<li>
<p>所有节点之间的pod通信正常</p>
</li>
<li>
<p>任意节点和Pod curl registry的Pod 的 <em>IP:5000</em> 均可以连通</p>
</li>
<li>
<p>新扩容节点10.153.204.15 curl registry服务的 Cluster lP 10.233.0.100:5000不通，其他节点curl均可以连通</p>
</li>
</ul>
<p>分析思路：</p>
<ul>
<li>
<p>根据现象1可以初步判断 <em>CNI</em> 插件无异常</p>
</li>
<li>
<p>根据现象2可以判断 <em>registry</em> 的 <em>Pod</em> 无异常</p>
</li>
<li>
<p>根据现象3可以判断 <em>registry</em> 的 <em>service</em> 异常的可能性不大，可能是新扩容节点访问 <em>registry</em> 的 <em>service</em> 存在异常</p>
</li>
</ul>
<p>怀疑方向：</p>
<ul>
<li>问题节点的kube-proxy存在异常</li>
<li>问题节点的iptables规则存在异常</li>
<li>问题节点到service的网络层面存在异常</li>
</ul>
<p>排查过程：</p>
<ul>
<li>排查问题节点的<code> kube-proxy</code></li>
<li>执行 <code>kubectl get pod -owide -nkube-system l grep kube-proxy </code>查看 <em>kube-proxy</em> Pod的状态，问题节点上的 <em>kube-proxy</em> Pod为 <em><strong>running</strong></em> 状态</li>
<li>执行 <code>kubecti logs &lt;nodename&gt; &lt;kube-proxy pod name&gt; -nkube-system</code> 查看问题节点 <em>kube-proxy</em>的Pod日志，没有异常报错</li>
<li>在问题节点操作系统上执行 <code>iptables -S -t nat</code> 查看 <code>iptables </code>规则</li>
</ul>
<p>排查过程：</p>
<p>确认存在到 <em>registry</em> 服务的 Cluster lP <em>10.233.0.100</em> 的 <em>KUBE-SERVICES</em> 链，跳转至 <em>KUBE-SVC-*</em> 链做负载均衡，再跳转至 <em>KUBE-SEP-*</em> 链通过 <em>DNAT</em> 替换为服务后端Pod的IP 10.233.65.46。因此判断iptables规则无异常执行route-n查看问题节点存在访问10.233.65.46所在网段的路由，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821231943753.png" alt="image-20220821231943753" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：10.233.65.46路由</center><br>
<p>查看对端的回程路由</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232111046.png" alt="image-20220821232111046" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：回程路由</center><br>
<p>以上排查证明问题原因不是 <em>cni</em> 插件或者 <em>kube-proxy</em> 异常导致，因此需要在访问链路上抓包，判断问题原因、问题节点执行 <code>curl 10.233.0.100:5000</code>，在问题节点和后端pod所在节点的flannel.1上同时抓包发包节点一直在重传，Cluster lP已 <em>DNAT</em> 转换为后端Pod IP，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821231845672.png" alt="image-20220821231845672" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：抓包过程，发送端</center><br>
<p>后端Pod（ <em>registry</em> 服务）所在节点的 <em>flannel.1</em> 上未抓到任何数据包，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821231730846.png" alt="image-20220821231730846" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：抓包过程，服务端</center><br>
<p>请求 <em>service</em> 的 <em>ClusterlP</em> 时，在两端物理机网卡抓包，发包端如图所示，封装的源端节点IP是10.153.204.15，但一直在重传</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232249344.png" alt="image-20220821232249344" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：包传送过程，发送端</center><br>
<p>收包端收到了包，但未回包，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232338410.png" alt="image-20220821232338410" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：包传送过程，服务端</center><br>
<p>由此可以知道，NAT的动作已经完成，而只是后端Pod（ <em>registry</em> 服务）没有回包，接下来在问题节点执行 <code>curl 10.233.65.46:5000</code>，在问题节点和后端（ <em>registry</em> 服务）Pod所在节点的 <em>flannel.1</em> 上同时抓包，两节点收发正常，发包如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232550959.png" alt="image-20220821232550959" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：正常包发送端</center><br>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821232827589.png" alt="image-20220821232827589" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：正常包接收端</center><br>
<p>接下来在两端物理机网卡接口抓包，因为数据包通过物理机网卡会进行 <em>vxlan</em> 封装，需要抓 <em>vxlan</em> 设备的8472端口，发包端如图所示</p>
<p>发现网络链路连通，==但封装的IP不对==，封装的源端节点IP是10.153.204.228，但是存在问题节点的IP是10.153.204.15</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821233107112.png" alt="image-20220821233107112" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：问题节点物理机网卡接口抓包</center><br>
<p>后端Pod所在节点的物理网卡上抓包，注意需要过滤其他正常节点的请求包，如图所示；发现收到的数据包，源地址是10.153.204.228，但是问题节点的IP是10.153.204.15。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821233258211.png" alt="image-20220821233258211" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：对端节点物理机网卡接口抓包</center><br>
<p>此时问题以及清楚了，是一个Pod存在两个IP，导致发包和回包时无法通过隧道设备找到对端的接口，所以发可以收到，但不能回。</p>
<p>问题节点执行<code>ip addr</code>，发现网卡 <em>enp26s0f0</em>上配置了两个IP，如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821233642903.png" alt="image-20220821233642903" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：问题节点IP</center><br>
<p>进一步查看网卡配置文件，发现网卡既配置了静态IP，又配置了dhcp动态获取IP。如图所示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821233741211.png" alt="image-20220821233741211" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：问题节点网卡配置</center><br>
<p>最终定位原因为问题节点既配置了dhcp 获取IP，又配置了静态IP，导致IP冲突，引发网络异常</p>
<p>解决方法：修改网卡配置文件 <code>/etc/sysconfig/network-scripts/ifcfg-enp26s0f0</code> 里 <code>BOOTPROTO=&quot;dhcp&quot;</code>
为 <code>BOOTPROTO=&quot;none&quot;</code>；重启 <em>docker</em> 和 <em>kubelet</em> 问题解决。</p>
<h3 id="集群外云主机调用集群内应用超时">集群外云主机调用集群内应用超时</h3>
<p>问题现象：Kubernetes 集群外云主机以 http post 方式访问Kubernetes 集群应用接口超时</p>
<p>环境信息：Kubernetes 集群：calicoIP-IP模式，应用接口以nodeport方式对外提供服务</p>
<p>客户端：Kubernetes 集群之外的云主机</p>
<p>排查过程：</p>
<ul>
<li>
<p>在云主机telnet应用接口地址和端口，可以连通，证明网络连通正常，如图所示</p>
</li>
<li>
<p>云主机上调用接口不通，在云主机和Pod所在 Kubernetes节点同时抓包，使用wireshark分析数据包</p>
</li>
</ul>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821234238398.png" alt="image-20220821234238398" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>通过抓包结果分析结果为TCP链接建立没有问题，但是在传输大数据的时候会一直重传 **1514 **大小的第一个数据包直至超时。怀疑是链路两端MTU大小不一致导致（现象：某一个固定大小的包一直超时的情况）。如图所示，1514大小的包一直在重传。</p>
<p>报文1-3 TCP三次握手正常</p>
<p>报文1 info中MSS字段可以看到MSS协商为1460，MTU=1460+20bytes（IP包头）+20bytes（TCP包头）=1500</p>
<p>报文7 k8s主机确认了包4的数据包，但是后续再没有对数据的ACK</p>
<p>报文21-29 可以看到云主机一直在发送后面的数据，但是没有收到k8s节点的ACK，结合pod未收到任何报文，表明是k8s节点和POD通信出现了问题。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20220821234410926.png" alt="image-20220821234410926" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<center>图：wireshark分析</center><br>
<p>在云主机上使用 <code>ping -s</code> 指定数据包大小，发现超过1400大小的数据包无法正常发送。结合以上情况，定位是云主机网卡配置的MTU是1500，<em>tunl0</em> 配置的MTU是1440，导致大数据包无法发送至 <em>tunl0</em> ，因此Pod没有收到报文，接口调用失败。</p>
<p>解决方法：修改云主机网卡MTU值为1440，或者修改calico的MTU值为1500，保持链路两端MTU值一致。</p>
<h3 id="集群pod访问对象存储超时">集群pod访问对象存储超时</h3>
<p>环境信息：公有云环境，Kubernetes 集群节点和对象存储在同一私有网络下，网络链路无防火墙限制k8s集群开启了节点自动弹缩（CA）和Pod自动弹缩（HPA），通过域名访问对象存储，Pod使用集群DNS服务，集群DNS服务配置了用户自建上游DNS服务器</p>
<p>排查过程：</p>
<ul>
<li>
<p>使用nsenter工具进入pod容器网络命名空间测试，ping对象存储域名不通，报错unknown server name，ping对象存储lP可以连通。</p>
</li>
<li>
<p><code>telnet</code> 对象存储80/443端口可以连通。</p>
</li>
<li>
<p><code>paping</code> 对象存储 80/443 端口无丢包。</p>
</li>
<li>
<p>为了验证Pod创建好以后的初始阶段网络连通性，将以上测试动作写入dockerfile，重新生成容器镜像并创pod，测试结果一致。</p>
</li>
</ul>
<p>通过上述步骤，判断Pod网络连通性无异常，超时原因为域名解析失败，怀疑问题如下：</p>
<ul>
<li>集群DNS服务存在异常</li>
<li>上游DNS服务存在异常</li>
<li>集群DNS服务与上游DNS通讯异常</li>
<li>pod访问集群DNS服务异常</li>
</ul>
<p>根据上述方向排查，集群DNS服务状态正常，无报错。测试Pod分别使用集群DNS服务和上游DNS服务解析域名，前者解析失败，后者解析成功。至此，证明上游DNS服务正常，并且集群DNS服务日志中没有与上游DNS通讯超时的报错。定位到的问题：==Pod访问集群DNS服务超时==</p>
<p>此时发现，出现问题的Pod集中在新弹出的 Kubernetes 节点上。这些节点的 <code>kube-proxy</code> Pod状态全部为<em>pending</em>，没有正常调度到节点上。因此导致该节点上其他Pod无法访问包括 dns 在内的所有Kubernetes service。</p>
<p>再进一步排查发现 <code>kube-proxy</code> Pod没有配置priorityclass为最高优先级，导致节点资源紧张时为了将高优先级的应用Pod调度到该节点，将原本已运行在该节点的kube-proxy驱逐。</p>
<p>解决方法：将 <code>kube-proxy</code> 设置 <code>priorityclass</code> 值为 <code>system-node-critical</code> 最高优先级，同时建议应用Pod配置就绪探针，测试可以正常连通对象存储域名后再分配任务。</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://danielmiessler.com/study/tcpdump/#basic-communication" target="_blank"
   rel="noopener nofollow noreferrer" ><em>A tcpdump Tutorial with Examples</em></a></p>
<p><sup id="2">[2]</sup> <a href="https://laramatic.com/how-to-install-nsenter-in-debian-ubuntu-alpine-arch-kali-centos-fedora-raspbian-and-macos/" target="_blank"
   rel="noopener nofollow noreferrer" ><em>How to install nsenter</em></a></p>
<p><sup id="3">[3]</sup> <a href="https://man7.org/linux/man-pages/man1/nsenter.1.html" target="_blank"
   rel="noopener nofollow noreferrer" ><em>man nsenter</em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>Account locked due to 10 failed logins</title>
      <link>https://www.oomkill.com/2021/10/account-locked-due-to-10-failed-logins/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/10/account-locked-due-to-10-failed-logins/</guid>
      <description></description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/dd613b500c634056a17ca2247e8aba0f.png" alt="在这里插入图片描述" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>进入后，找到linux16 开头的一行！将ro改为 <code>rw init=/sysroot/bin/sh</code></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/d505b39b548344158698e416855155ce.png" alt="在这里插入图片描述" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/c2baea4125f74c2bb23769bdb34b1c72.png" alt="在这里插入图片描述" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>查看passwd和 shadow 发现用户并没有锁，于是想到，应该是pam的设置。</p>
<pre><code>pam_tally2.so deny=6 onerr=fail unlock_time=120
</code></pre>
<p>默认log在： <code>/var/log/tallylog</code></p>
<pre><code>chroot /sysroot
# 使用pam_tally2命令解锁
pam_tally2 --user=root --reset
rw init=/sysroot/bin/sh
</code></pre>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://www.cnblogs.com/luckyall/p/6609915.html" target="_blank"
   rel="noopener nofollow noreferrer" >Centos7.x破解密码</a></p>
<p><a href="http://www.jiangjiang.space/2016/11/15/maven-%E7%BC%96%E8%AF%91%E6%97%B6%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/" target="_blank"
   rel="noopener nofollow noreferrer" >pam_tally2锁用户</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>mysql5.6 innodb_large_prefix引起的一个异常</title>
      <link>https://www.oomkill.com/2021/10/mysql5.6-innodb_large_prefix-abnormal/</link>
      <pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/10/mysql5.6-innodb_large_prefix-abnormal/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p>phenomenon： Specified key was too long; max key length is 3072 bytes</p>
</blockquote>
<p><strong>在修改一个数据库字段时，字段容量被限制为了表前缀的大小而不是本身的容量大小</strong></p>
<p>查了一下<code>innodb_large_prefix</code>究竟是什么？</p>
<p>动态行格式<code>DYNAMIC row format </code>支持最大的索引前缀(3072)。由变量<code>innodb_large_prefix</code>进行控制。</p>
<blockquote>
<p>By default, the index key prefix length limit is 767 bytes. See Section 13.1.13, “CREATE INDEX Statement”. For example, you might hit this limit with a column prefix index of more than 255 characters on a TEXT or VARCHAR column, assuming a utf8mb3 character set and the maximum of 3 bytes for each character. When the innodb_large_prefix configuration option is enabled, the index key prefix length limit is raised to 3072 bytes for InnoDB tables that use the DYNAMIC or COMPRESSED row format.</p>
</blockquote>
<p>官方上说在 <code>utf8mb3</code>（<code>most bytes n</code>）如果设置为 <code>varchar(255)</code>时，索引前缀将大于767，可以扩展为3072，但是实际上 varchar的size可以为65535，<code>这个就限制了整个alter table 的操作</code></p>
<p>因为建表是时索引没设置大小，默认是超过255的，后面开启了前缀限制，大小会为3072，此时无法做表修改</p>
<blockquote>
<p><strong>Reference</strong></p>
<p><a href="https://forums.percona.com/t/utf8mb4-error-1709-hy000-index-column-size-too-large-the-maximum-column-size-is-767-bytes/8336" target="_blank"
   rel="noopener nofollow noreferrer" >Utf8mb4 / ERROR 1709 (HY000): Index column size too large. The maximum column size is 767 bytes </a></p>
<p><a href="https://support.cpanel.net/hc/en-us/articles/4403725847959-MySQL-8-innodb-large-prefix" target="_blank"
   rel="noopener nofollow noreferrer" >MySQL8 innodb large prefix</a></p>
<p><a href="https://dba.stackexchange.com/questions/233751/are-innodb-large-prefix-and-innodb-file-format-settings-backwards-compatible" target="_blank"
   rel="noopener nofollow noreferrer" >innodb_file_format settings backwards compatible</a></p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-limits.html" target="_blank"
   rel="noopener nofollow noreferrer" >innodb limits</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>由PIPE size 引起的线上故障</title>
      <link>https://www.oomkill.com/2021/10/pipe-size-problem/</link>
      <pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2021/10/pipe-size-problem/</guid>
      <description></description>
      <content:encoded><![CDATA[<blockquote>
<p><strong>sence</strong>：python中使用subprocess.Popen(cmd, stdout=sys.STDOUT, stderr=sys.STDERR, shell=True) ，stdout, stderr 为None.</p>
</blockquote>
<p>在错误中执行是无法捕获 stderr的内容，后面将上面的改为 <code>subprocess.Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)</code>,发现是可以拿到 <code>stderr</code>, 但是会遇到大量任务hanging，造成线上事故。</p>
<p>为此特意查询<code>subprocess</code>的一些参数的说明。</p>
<blockquote>
<p><code>stdin</code> <code>stdout </code> <code>stderr </code> 如果这些参数为 <code>PIPE</code>, 此时会为一个文件句柄，而传入其他（例如 <code>sys.stdout</code> 、<code>None</code> 等）的则为<code>None</code></p>
</blockquote>
<p>正如这里介绍的一样，<a href="https://docs.python.org/2/library/subprocess.html#subprocess.Popen.stdin" target="_blank"
   rel="noopener nofollow noreferrer" >subprocess</a> 。</p>
<p>而使用 <code>PIPE</code>，却导致程序 hanging。一般来说不推荐使用 <code>stdout=PIPE</code>  <code>stderr=PIPE</code>，这样会导致一个死锁，子进程会将输入的内容输入到 <code>pipe</code>，直到操作系统从buffer中读取出输入的内容。</p>
<p>查询手册可以看到确实是这个问题 <a href="https://docs.python.org/2/library/subprocess.html#subprocess.Popen.communicate" target="_blank"
   rel="noopener nofollow noreferrer" >Refernce</a></p>
<blockquote>
<p><strong>Warning</strong> This will deadlock when using <code>stdout=PIPE</code> and/or <code>stderr=PIPE</code> and the child process generates enough output to a pipe such that it blocks waiting for the OS pipe buffer to accept more data. Use <a href="https://docs.python.org/2/library/subprocess.html#subprocess.Popen.communicate" target="_blank"
   rel="noopener nofollow noreferrer" ><code>communicate()</code></a> to avoid that.</p>
</blockquote>
<p>而在linux中 <code>PIPE</code> 的容量（capacity）是内核中具有固定大小的一块缓冲区，如果用来接收但不消费就会阻塞，所以当用来接收命令的输出基本上100% 阻塞所以会导致整个任务 hanging。<em>（ -Linux2.6.11 ，pipe capacity 和system page size 一样（如， i386 为 4096 bytes ）。 since Linux 2.6.11+，pipe capacity 为 65536  bytes。）</em></p>
<p>关于更多的信息可以参考：<a href="https://linux.die.net/man/7/pipe" target="_blank"
   rel="noopener nofollow noreferrer" >pipe</a></p>
<p>所以如果既要拿到对应的输出进行格式化，又要防止程序hang，可以自己创建一个缓冲区，这样可以根据需求控制其容量，可以有效的避免hanging。列如：</p>
<pre><code class="language-python">cmd = &quot;this is complex command&quot;
outPipe = tempfile.SpooledTemporaryFile(bufsize=10*10000)
fileno = outPipe.fileno()
process = subprocess.Popen(cmd,stdout=fileno,stderr=fileno,shell=True)
</code></pre>
<p>另外，几个参数设置的不通的区别如下：</p>
<p><code>stdout=None</code> 为继承父进程的句柄，通俗来说为标准输出。</p>
<p><code>stderr=STDOUT</code> 重定向错误输出到标准输出</p>
<p><code>stdout=PIPE</code> 将标准输出到linux pipe</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><a href="https://docs.python.org/2/library/subprocess.html#subprocess.Popen.stdin" target="_blank"
   rel="noopener nofollow noreferrer" >subprocess</a></p>
<p><a href="https://stackoverflow.com/questions/25370347/python-subprocess-stderr-stdout-field-is-none-if-created" target="_blank"
   rel="noopener nofollow noreferrer" >subprocess stderr/stdout field is None</a></p>
<p><a href="https://stackoverflow.com/questions/39477003/python-subprocess-popen-hanging" target="_blank"
   rel="noopener nofollow noreferrer" >subprocess-popen-hanging</a></p>
<p><a href="https://unix.stackexchange.com/questions/11946/how-big-is-the-pipe-buffer" target="_blank"
   rel="noopener nofollow noreferrer" >pipe size</a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>Centos7 dbus问题总结</title>
      <link>https://www.oomkill.com/2020/09/centos7-dbus-troubleshooting/</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2020/09/centos7-dbus-troubleshooting/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="authorization-not-available-check-if-polkit">Authorization not available. Check if polkit</h2>
<pre><code>Authorization not available. Check if polkit service is running or see debug message for more information.

dbus.socket failed to listen on sockets: Address family not supported by protocol
Failed to listen on D-Bus System Message Bus Socket.
</code></pre>
<p>这个问题是因为dbus.socket状态异常，所有依赖dbus的启动都会去通过systemcall连接 dbus，当服务不可用时，所有服务无法以systemd方式正常启动/关闭。需要检查dbus.socket是否正常。本地使用需保证unix套接字的监听时启动的</p>
<h2 id="did-not-receive-a-reply">Did not receive a reply</h2>
<pre><code>Failed to open connection to &quot;system&quot; message bus: Did not receive a reply. Possible causes include: the remote application did not send a reply, the message bus security policy blocked the reply, the reply timeout expired, or the network connection was broken.
</code></pre>
<p>这是因为你的配置不对，客户端无法连接上</p>
<h2 id="d-bus-重启后登陆慢">D-Bus 重启后登陆慢</h2>
<pre><code>systemd-logind: Failed to connect to system bus: Connection refused
systemd-logind: Failed to fully start up daemon: Connection refused
systemd: systemd-logind.service: main process exited, code=exited, status=1/FAILURE
systemd: Unit systemd-logind.service entered failed state.
systemd: systemd-logind.service failed.
systemd: systemd-logind.service has no holdoff time, scheduling restart.
systemd: start request repeated too quickly for systemd-logind.service
systemd: Unit systemd-logind.service entered failed state.
systemd: systemd-logind.service failed.

dbus[7782]: [system] Failed to activate service 'org.freedesktop.login1': timed out
dbus-daemon: dbus[7782]: [system] Failed to activate service 'org.freedesktop.login1':   timed out
</code></pre>
<p>参考：<a href="https://serverfault.com/questions/707377/slow-ssh-login-activation-of-org-freedesktop-login1-timed-out" target="_blank"
   rel="noopener nofollow noreferrer" >ssh登陆缓慢</a></p>
<p>systemd-logind主要功能是为每一个登陆session创建一个systemd角度的cgroup管理对象，更方便对session使用cgroup，在dbus服务异常时，systemd-logind会导致登陆缓慢，并不影响正常登陆和ssh登陆。重启dbus.socket后需要也重启systemd-logind</p>
<h2 id="d-bus-开启远程连接">D-Bus 开启远程连接</h2>
<p>编辑 <code>/usr/share/dbus-1/system.conf</code> 或 <code>/etc/dbus-1/session.conf</code></p>
<p>通常情况下生效的是 <code>/etc/dbus-1/system.conf</code> ,需要根据dbus应用是system bus 还是 session bus进行选择配置</p>
<pre><code>&lt;listen&gt;tcp:host=&lt;ip&gt;,bind=*,port=&lt;port&gt;,family=ipv4&lt;/listen&gt;
&lt;listen&gt;unix:path=/run/user/&lt;username&gt;/dbus/user_bus_socket&lt;/listen&gt;
&lt;listen&gt;unix:tmpdir=/tmp&lt;/listen&gt;

&lt;auth&gt;ANONYMOUS&lt;/auth&gt;
&lt;allow_anonymous/&gt;
</code></pre>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://blog.fpmurphy.com/2018/10/using-the-d-bus-interface-to-firewalld.html" target="_blank"
   rel="noopener nofollow noreferrer" >dbus-send使用</a></li>
<li><a href="https://stackoverflow.com/questions/61327052/linux-dbus-remote-tcp-connection-with-systemd-fails" target="_blank"
   rel="noopener nofollow noreferrer" >Linux DBus远程TCP连接失败</a></li>
</ul>
<h2 id="dbus-faq">dbus faq</h2>
<ul>
<li><a href="https://dbus.freedesktop.org/doc/" target="_blank"
   rel="noopener nofollow noreferrer" >faq</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用alpine为基础镜像Q&amp;A</title>
      <link>https://www.oomkill.com/2020/09/alpine-trouble-q-and-a/</link>
      <pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2020/09/alpine-trouble-q-and-a/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="作为go应用存在二进制文件却不能执行">作为go应用存在二进制文件却不能执行</h2>
<p>明明镜像中有对应的二进制文件，但是执行时却提示 <code>not found</code> 或 <code>no such file</code> 或 <code>standard_init_linux.go:211: exec user process caused &quot;no such file or directory&quot;</code></p>
<p>网上常说都是因为windows换行符编码问题。此处实际问题是<strong>该二进制文件是使用动态链接方式编译</strong>.</p>
<p>解决方法：</p>
<pre><code>CGO_ENABLED=0  GOOS=linux  GOARCH=amd64 go build --ldflags &quot;-extldflags -static&quot;
</code></pre>
<p>注意：<code>CGO_ENABLED=0 GOOS=linux GOARCH=amd64</code> 和 <code>cgo_enabled=0 goos=linux goarch=amd64</code> 是有区别的。</p>
<p><strong>保存信息</strong></p>
<p>诸如此类信息都是上述问题</p>
<pre><code>standard_init_linux.go:211: exec user process caused &quot;no such file or directory&quot;

/tmp # ./envoy_end 
/bin/sh: ./envoy_end: not found
</code></pre>
<h2 id="替换为国内源">替换为国内源</h2>
<pre><code>RUN sed -i 's@http://dl-cdn.alpinelinux.org/@https://mirrors.aliyun.com/@g' /etc/apk/repositories
</code></pre>
<h2 id="基于alpine制作php镜像">基于alpine制作PHP镜像</h2>
<ul>
<li>
<p>alpine包搜索 <a href="https://pkgs.alpinelinux.org/" target="_blank"
   rel="noopener nofollow noreferrer" >https://pkgs.alpinelinux.org/</a></p>
</li>
<li>
<p>安装依赖库 <code>apk add  --no-cache  xxx</code></p>
</li>
<li>
<p>基于php apline镜像自行增加或删除扩展。 <a href="https://github.com/docker-library/php" target="_blank"
   rel="noopener nofollow noreferrer" >offcial-repo</a></p>
</li>
<li>
<p>增加扩展可以使用 <code>pecl install xxx</code> 如 <code>pecl install redis</code></p>
</li>
<li>
<p>如果不能使用此种方法安装可以使用，git clone 下来在进行编译，编译成功后 docker-php-ext-enable xxx启动扩展。</p>
</li>
</ul>
<blockquote>
<p>此中方式制作镜像，常见扩展安装完成后，容器大小可控制在100M左右</p>
</blockquote>
<p>参考资料：<a href="https://stackoverflow.com/questions/46221063/what-is-build-deps-for-apk-add-virtual-command" target="_blank"
   rel="noopener nofollow noreferrer" >What is .build-deps for apk add &ndash;virtual command?</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>envoy官方example运行失败问题处理</title>
      <link>https://www.oomkill.com/2020/09/envoy-example-failed/</link>
      <pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2020/09/envoy-example-failed/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="镜像内安装包失败处理">镜像内安装包失败处理</h2>
<p>方法一：修改Dockerfile，在Dockerfile中增加如下</p>
<blockquote>
<p>ubuntu示例</p>
</blockquote>
<pre><code>RUN sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list
RUN sed -i 's/security.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list
</code></pre>
<blockquote>
<p>apline示例</p>
</blockquote>
<pre><code>RUN sed -i 's@http://dl-cdn.alpinelinux.org/@https://mirrors.aliyun.com/@g' /etc/apk/repositories
</code></pre>
<p>方法二：使用http代理，</p>
<p>ubuntu 参考 <a href="https://medium.com/@airman604/getting-docker-to-work-with-a-proxy-server-fadec841194e" target="_blank"
   rel="noopener nofollow noreferrer" >命令行使用代理</a></p>
<h2 id="下载镜像失败处理">下载镜像失败处理</h2>
<p>方法一：docker宿主机使用ss，开启局域网可连接。同局域网中的都可直接连此代理
方法二： docker systemd的 service文件中增加http代理</p>
<p>可看到已经可以成功运行envoy example示例</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20200912185345368-159241184.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="cannot-bind-000080-permission-denied">cannot bind &lsquo;0.0.0.0:80&rsquo;: Permission denied</h2>
<p>docker-compose文件</p>
<pre><code class="language-yaml">version: '3'
services:
  envoy:
    image: envoyproxy/envoy-alpine:v1.15-latest
    volumes:
    - ./envoy.yaml:/etc/envoy/envoy.yaml
    network_mode: &quot;service:mainserver&quot; 
    depends_on:
    - mainserver
  mainserver:
    image: cylonchau/envoy-end:latest
    networks:
      envoymesh:
        aliases:
        - webserver
        - httpserver
        - envoy_end
networks:
  envoymesh: {}
</code></pre>
<p>启动时报错</p>
<pre><code>envoy_1       | [2020-09-06 07:09:48.618][8][critical][main] [source/server/server.cc:101] error initializing configuration '/etc/envoy/envoy.yaml': cannot bind '0.0.0.0:80': Permission denied
envoy_1       | [2020-09-06 07:09:48.618][8][info][main] [source/server/server.cc:704] exiting
envoy_1       | cannot bind '0.0.0.0:80': Permission denied
root_envoy_1 exited with code 1
</code></pre>
<p>参考 <a href="https://github.com/envoyproxy/envoy/issues/11506" target="_blank"
   rel="noopener nofollow noreferrer" >list</a></p>
<pre><code>environment:
- &quot;ENVOY_UID=0&quot;
</code></pre>
]]></content:encoded>
    </item>
    
    <item>
      <title>powercli The SSL connection could not be established, see inner exception. 问题解决</title>
      <link>https://www.oomkill.com/2019/10/powercli-the-ssl-connection-could-not-be-established-see-inner-exception/</link>
      <pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/10/powercli-the-ssl-connection-could-not-be-established-see-inner-exception/</guid>
      <description></description>
      <content:encoded><![CDATA[<pre><code>Connect-VIServer -Server 
</code></pre>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20210801143141181-1047906479.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>这里是“SSL连接不能建立&hellip;&hellip;”这实际上意味着你没有一个有效的证书。如果你想连接到vCenter没有一个有效的证书，您必须允许可以改变你的vCenter证书到受信任的一个，这是正确的解决方案，也可以忽略无效的证书，以规避所有的安全性，但使它现在的工作。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20210801143200019-1825949412.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>忽略无效证书</p>
<pre><code>Set-PowerCLIConfiguration -InvalidCertificateAction:ignore
</code></pre>
<p>再次登陆可正常登陆
<img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1380340-20210801143232415-1036597499.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>tcp.validnode_checking踩过的坑</title>
      <link>https://www.oomkill.com/2018/05/oracle-tcp.validnode_checking/</link>
      <pubDate>Sun, 06 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/05/oracle-tcp.validnode_checking/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>对Oracle 检查ip合法性,就必须在服务器端的sqlnet.ora文件中设置如下参数</p>
<pre><code>TCP.INVITED_NODES=(10.0.0.36,10.0.0.1,10.0.0.35)  
TCP.EXCLUDED_NODES=(10.0.0.2)  
</code></pre>
<p>启动监听出现如下错误</p>
<pre><code>$ lsnrctl status  
  
LSNRCTL for Linux: Version 11.2.0.1.0 - Production on 12-MAR-2018 18:32:13  
  
Copyright (c) 1991, 2009, Oracle.  All rights reserved.  
  
Connecting to (ADDRESS=(PROTOCOL=tcp)(HOST=)(PORT=1521))  
TNS-12541: TNS:no listener  
 TNS-12560: TNS:protocol adapter error  
  TNS-00511: No listener  
   Linux Error: 111: Connection refused
</code></pre>
<p>错误输出并没有打印详细的信息,从lisenter.ora,tnsnames.ora入手,但没有发现文件是错误的。最后检查sqlnet.ora,发现TCP.INVITED_NODES参数有如下约束是官方文档没有给出的</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20210930150150079-1727667534.png" alt="" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><strong><font style="color:red"> tcp.invited_nodes需要满足如下条件才可成功启动监听</font></strong></p>
<ul>
<li>1、需要设置参数TCP.VALIDNODE_CHECKING为YES才能激活该特性。</li>
<li>2、tcp.invited_nodes的值中一定要包括本机地址（127.0.0.1 / 10.0.0.36）或localhost，因为监听需要通过本机ip去访问监听，一旦禁止lsnrct将不能启动或停止监听。</li>
<li>3、不能设置ip段和通配符。</li>
<li>4、此方式只适合tcp/ip协议。</li>
<li>5、此方式是通过监听限制白名单的。</li>
<li>6、针对的是ip地址而不是其他（如用户名等）。</li>
<li>7、此配置适用于9i以上版本。本次踩坑是oracle11gr2。</li>
<li>8、修改配置后需要重启监听才可生效。</li>
</ul>
<p><code>TCP.INVITED_NODES=(10.0.0.36,10.0.0.1)</code></p>
<p>此时在启动监听不会出现报错了。而对与TCP.EXCLUDED_NODES参数并没有以上的限制，需要将禁止访问的ip传参即可。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>jenkins多个slave遇到的坑</title>
      <link>https://www.oomkill.com/2018/05/jenkins-multi-slave-problem/</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/05/jenkins-multi-slave-problem/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="jenkins配置如下">jenkins配置如下</h2>
<p>在Jenkins上添加了两个节点(Slave Node)，且为这两个节点设置了一个相同的标签 &ldquo;windows&rdquo;。创建了一个新Job –  &ldquo;test-windows&rdquo;，选择的是”构建一个自由风格的软件项目”。并且为了使多个slave并行构建，我选择了&quot;只允许绑定到这台机器的job”，在&quot;Label Expression&quot;中选择了&quot;windows&quot;。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025231428874.png" alt="image-20221025231428874" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>然而这种方式并不能实现多个slave并行操作。网上90%说的都不靠谱。</p>
<p>在我使用的过程中，使用了label 去管理多个 Slave，给一个项目的构建指定了这个 label，会发现这个项目的多次构建，都使用同一个 Slave，并没有使用 label 里的其它 Slave去构建。</p>
<p>查了很多资料才发现原来从 jenkins 的调度算法使用了一致性的哈希算法，jenkins根据添加的信息评测出优先级列表，选择优先级最高的Slave去构建，当最优slave不满足条件或者没有可用的 execut时，才会选用下一个slave。</p>
<p>查了很多资料发现构造多配置项目可以选择构建时的slave。这样可以实现多slave并行构建。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025231449517.png" alt="image-20221025231449517" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>multi configuration project比起构建自由风格的软件项目多个Configuration Matrix，在这里可以选择多个slave。这里选择lable的话，还是会使用默认算法从lable中选择最优slave进行构建。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025231504755.png" alt="image-20221025231504755" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>配置完成后再构建时，会同时在多个slave上进行并行构建</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025231536022.png" alt="image-20221025231536022" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>禁止在master上运行job或和业务相关的操作</p>
<p>将 [executors] 设置为0</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/image-20221025231520553.png" alt="image-20221025231520553" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>windows上sqlplus客户端连接oralce数据库中文显示问题</title>
      <link>https://www.oomkill.com/2018/04/sqlplus-windows/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2018/04/sqlplus-windows/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="运行环境">运行环境</h2>
<p>服务器：centos6.8</p>
<p>服务器oracle版本：oracle 11g R2 64位，字符集是ZHS32utf8。</p>
<p>客户端：navicat 12x64  windows8.1x64</p>
<h2 id="问题分析">问题分析</h2>
<p>当在windows客户端使用sqlplus或navicat时如果数据库中文显示“????”</p>
<p>这种情况是在客户端与服务器端字符集不一致时，从客户端输入了汉字信息。输入的这些信息即便是把客户端字符集更改正确，也无法显示汉字。</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20180419215302334-117370933.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20180419215313731-33925628.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>解决方法：退出sqlplus,设置相应的环境变量NLS_LANG</p>
<p>linux：</p>
<pre><code>export NLS_LANG=&quot;SIMPLIFIED CHINESE_CHINA.ZHS16GBK&quot; 
</code></pre>
<p>windows：</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20180419215414786-1063538995.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<h2 id="出现问题">出现问题</h2>
<p>此时。系统cmd命令行使用sqlplus已经正常显示中文，但是navicat中依旧是？？？？</p>
<p>图为cmd命令行访问sqlplus客户端查询</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20180419215432087-917488218.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>图为navicat f6弹出的sqlplus客户端</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20180419215446566-878106693.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>原因是因为Navicat Premium默认自带的instant client，但是其是base lite版本的（Basic Lite： Basic  的精简版本，其中仅带有英文错误消息和 Unicode、ASCII  以及西欧字符集支持），不支持中文字符集，而本文中的服务器端oracle恰好是中文字符集。自带版本不支持。此处需要去oracle官网下载相对应的版本。</p>
<p><a href="http://www.oracle.com/technetwork/database/database-technologies/instant-client/downloads/index.html" target="_blank"
   rel="noopener nofollow noreferrer" >http://www.oracle.com/technetwork/database/database-technologies/instant-client/downloads/index.html</a></p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20180419215502943-2034092654.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>将下载的文件解压覆盖navicat中的instantclient目录里的文件。</p>
<p>此时连接oracle实例提示如下信息</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20180419215518269-1348254446.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
<p>尽管我们下载了64位的版本。却提示如图信息。这是因为Navicat仅支持32位的，因此还需下载一个32位的客户端。替换到instantclient目录中</p>
<p>替换完成后连接实例。f6使用sqlplus查询发现中文已经正常显示</p>
<p><img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed//img/1380340-20180419215532150-1754188108.png" alt="img" onerror="this.onerror=null;this.src='/placeholder.svg';this.className='pe-image-placeholder'" /></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
