<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>storage on Cylon&#39;s Collection</title>
    <link>https://www.oomkill.com/categories/storage/</link>
    <description>Recent content in storage on Cylon&#39;s Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 13 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.oomkill.com/categories/storage/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>记录一次ceph集群故障处理记录</title>
      <link>https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/</link>
      <pubDate>Tue, 13 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2024/02/10-2-troubeshooting-crash/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="处理记录">处理记录</h2>
<p>Ceph版本：octopus</p>
<p>首先遇到問題是，业务端无法挂在 cephfs 查看内核日志发现是 <code> bad authorize reply</code> ，以为是 ceph keyring被替换了</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class="line"><span class="cl">2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class="line"><span class="cl">2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class="line"><span class="cl">2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class="line"><span class="cl">2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class="line"><span class="cl">2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class="line"><span class="cl">2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span><span class="line"><span class="cl">2019-01-30 17:26:58 localhost kernel: libceph: mds0 10.80.20.100:6801 bad authorize reply
</span></span></code></pre></td></tr></table>
</div>
</div><p>在排查完 keyring 后，手动尝试挂载 cephfs 提示 <code>Input/output error</code> ，此时看出是集群问题了</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ mount -t ceph 10.80.20.100:6789:/tmp /tmp/ceph -o <span class="nv">secret</span><span class="o">=</span><span class="nv">AQCoW0dgQk4qGhAAwayKv70OSyyWB3XpZ1JLYQ</span><span class="o">==</span>,name<span class="o">=</span>cephuser
</span></span><span class="line"><span class="cl">mount error <span class="nv">5</span> <span class="o">=</span> Input/output error
</span></span></code></pre></td></tr></table>
</div>
</div><p>因为一开始看到日志是 <em>bad authorize reply</em> 以为是认证错误，重新登录了一下发现是相同的提示，这时查看 ceph status 发现集群异常，除了下面报错外，还有一个 osd down 的状态。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph health detail
</span></span><span class="line"><span class="cl">HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data redundancy: 39560/118680 objects degraded (33.333%), 201 pgs degraded, 255 pgs undersized; 4 daemons have recently crashed
</span></span><span class="line"><span class="cl">[WRN] CEPHADM_HOST_CHECK_FAIL: 1 hosts fail cephadm check
</span></span><span class="line"><span class="cl">    host localhost failed check: [&#39;podman|docker (/bin/docker) is present&#39;, &#39;systemctl is present&#39;, &#39;lvcreate is present&#39;, &#34;No time sync service is running; checked for [&#39;chrony.service&#39;, &#39;chronyd.service&#39;, &#39;systemd-timesyncd.service&#39;, &#39;ntpd.service&#39;, &#39;ntp.service&#39;]&#34;, &#39;ERROR: No time synchronizetion is active&#39;]
</span></span><span class="line"><span class="cl">[WRN] MDS_SLOW_METADATA_IO: 1 MDSs report slow metadata IOs
</span></span><span class="line"><span class="cl">    mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked &gt; 30 secs, oldest blocked for 4830 secs
</span></span><span class="line"><span class="cl">[WRN] MDS_SLOW_REQUEST: 1 MDSs report slow requests
</span></span><span class="line"><span class="cl">    mds.cephfs.localhost.mhlzaj(mds.0): 14 slow metadata IOs are blocked &gt; 30 secs
</span></span><span class="line"><span class="cl">[WRN] MON_CLOCK_SKEW: clock skew detected on mon.localhost, mon.localhost1
</span></span><span class="line"><span class="cl">    mon.localhost clock skew 29357.8s &gt; max 0.05s (latency 0.0132089s)
</span></span><span class="line"><span class="cl">    mon.localhost1 clock skew 29357.8s &gt; max 0.05s (latency 0.0117421s)
</span></span><span class="line"><span class="cl">[WRN] PG_DEGRADED: Degraded data redundancy: 39501/118680 objects degraded (33.333%), 189 pgs degraded, 241 pgs undersized
</span></span><span class="line"><span class="cl">    pg 1.0 is stuck undersized for 22m, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.8 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 2.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 2.1b is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.0 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.1 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.2 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.3 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.4 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.5 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.6 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.7 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.9 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.c is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.d is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.e is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.f is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.10 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.11 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.12 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.13 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.14 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.15 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.16 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.17 is stuck undersized for 4d, current state active+undersized+degraded, last acting [0]
</span></span><span class="line"><span class="cl">    pg 3.18 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.19 is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">    pg 3.1a is stuck undersized for 4d, current state active+undersized+degraded, last acting [1]
</span></span><span class="line"><span class="cl">[WRN] RECENT_CRASH: 4 daemons have recently crashed
</span></span><span class="line"><span class="cl">    osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span><span class="line"><span class="cl">    client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p>CEPHADM_HOST_CHECK_FAIL：一台或多台主机未通过基本 cephadm 主机检查，该检查验证 (1) 主机可访问并且可以在其中执行 cephadm，以及 (2) 主机满足基本先决条件，例如工作容器运行时（podman 或 docker）和工作时间同步。如果此测试失败，cephadm 将无法管理该主机上的服务。</p>
</li>
<li>
<p>MDS_SLOW_METADATA_IO</p>
</li>
<li>
<p>MDS_SLOW_REQUEST：N条慢请求被阻塞</p>
</li>
<li>
<p>MON_CLOCK_SKEW：运行 ceph-mon 的主机上的时钟未很好同步。如果集群检测到时钟偏差大于 mon_clock_drift_allowed，则会引发此运行状况检查。</p>
</li>
<li>
<p>PG_DEGRADED：一个或多个PG的健康状态受到了损害。一种常见的情况是，某个OSD发生故障或离线，导致PG进入降级状态。在这种情况下，数据副本的可用性会受到影响，并且Ceph集群的性能也可能下降。</p>
</li>
</ul>
<p>首先重启 chronyd 修复了时间同步的问题，因为机房内机器经常出现 chronyd 的服务导致异常，其次重启 osd 服务，让 ceph 做再平衡完成后剩下下面报错。</p>
<p>并且现象有两个：</p>
<ul>
<li>cephfs no such file or director</li>
<li>ceph orch 命令还是没有反应</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph health detail
</span></span><span class="line"><span class="cl">HEALTH_WARN 1 host fail cephadm check; 1 MDSs report slow metadata IOs; 1 MDSs report slow requests; clock skew detected on mon.localhost; Degraded data 
</span></span><span class="line"><span class="cl">[WRN] FS_DEGRADED: 1 filesystem is degraded
</span></span><span class="line"><span class="cl">    fs cephfs is degraded
</span></span><span class="line"><span class="cl">[WRN] MDS_SLOW_METADATA_IO: 1 MDSs slow metadata IOs
</span></span><span class="line"><span class="cl">     mds.cephfs.localhost.mhlzaj(mds.0): 20 slow metadata IOs are blocked &gt; 30 secs, oldest blocked for 930 secs
</span></span><span class="line"><span class="cl">[WRN] RECENT_CRASH: 4 daemons have recently crashed
</span></span><span class="line"><span class="cl">    osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span><span class="line"><span class="cl">    client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span><span class="line"><span class="cl">[WRN] SLOW_OPS: 2 slow ops, oldset one blocked for 474 sec, mon.localhost has slow ops
</span></span></code></pre></td></tr></table>
</div>
</div><p>通过 search 了一下，查询到 orch 是 MGR 模块</p>
<blockquote>
<p>The orchestrator is a MGR module, have you checked if the containers   are up and running <sup><a href="#1">[1]</a></sup></p>
</blockquote>
<p>此时操作登录对应ceph node，docker restart ceph-mgr的模块，并且重启 mds 模块</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph health detail
</span></span><span class="line"><span class="cl"><span class="o">[</span>WRN<span class="o">]</span> RECENT_CRASH: <span class="m">4</span> daemons have recently crashed
</span></span><span class="line"><span class="cl">    osd4 crhash on host xxxxxx at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span><span class="line"><span class="cl">    client.xxx.xxxx.hostname.xxxx crashed on host hostname at 20xx-0x-xxT04xx:xx:xx.xxxxxxz
</span></span></code></pre></td></tr></table>
</div>
</div><p>此时集群恢复正常，cephfs 恢复</p>
<h2 id="总结">总结</h2>
<p>由于长期没有在处理 ceph 方向问题，对排查有以下生疏：</p>
<ul>
<li>无法挂载时没有及时查看 ceph 集群信息，而是盯着客户端方向日志查询了半天。</li>
<li>对 ceph 故障代码没有了解过，如果有了解，可以很明确的定位问题，而不用耽误2小时。</li>
</ul>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://lists.ceph.io/hyperkitty/list/ceph-users@ceph.io/thread/2OSO26WYFBS4HZ4LPHNMBZUQ6Y3GI6GG/">ceph orch status hangs forever</a></p>
<p><sup id="2">[2]</sup> <a href="https://docs.ceph.com/en/quincy/rados/operations/health-checks/">HEALTH CHECKS</a></p>
<p><sup id="3">[3]</sup> <a href="https://docs.ceph.com/en/quincy/cephfs/health-messages/?highlight=MDS_SLOW_METADATA_IO">CEPHFS HEALTH MESSAGES</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>当cephfs和fscache结合时在K8s环境下的全集群规模故障</title>
      <link>https://www.oomkill.com/2023/11/10-1-ceph-fscache/</link>
      <pubDate>Sat, 11 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/11/10-1-ceph-fscache/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>本文记录了在 kubernetes 环境中，使用 cephfs 时当启用了 fscache 时，由于网络问题，或者 ceph 集群问题导致的整个 k8s 集群规模的挂载故障问题。</p>
<h2 id="什么是fscache">什么是fscache</h2>
<p>fscache 是网络文件系统的通用缓存，例如 NFS, CephFS都可以使用其进行缓存从而提高 IO</p>
<p>FS-Cache是在访问之前，将整个打开的每个 netfs 文件完全加载到 Cache 中，之后的挂载是从该缓存而不是 netfs 的 inode 中提供</p>
<p>fscache主要提供了下列功能：</p>
<ul>
<li>一次可以使用多个缓存</li>
<li>可以随时添加/删除缓存</li>
<li>Cookie 分为 “卷”, “数据文件”, “缓存”
<ul>
<li>缓存 cookie 代表整个缓存，通常不可见到“网络文件系统”</li>
<li>卷 cookie 来表示一组 文件</li>
<li>数据文件 cookie 用于缓存数据</li>
</ul>
</li>
</ul>
<p>下图是一个 NFS 使用 fscache 的示意图，CephFS 原理与其类似</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/Cache-NFS-Share-Data-with-FS-Cache-1.webp" alt="Cache-NFS-Share-Data-with-FS-Cache-1"  /></p>
<center>图1：FS-Cache 架构 </center>
<center><em>Source：</em>https://computingforgeeks.com/how-to-cache-nfs-share-data-with-fs-cache-on-linux/</center><br>
<p>CephFS 也是可以被缓存的一种网络文件系统，可以通过其内核模块看到对应的依赖</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">root@client:~# lsmod <span class="p">|</span> grep ceph
</span></span><span class="line"><span class="cl">ceph                  <span class="m">376832</span>  <span class="m">1</span>
</span></span><span class="line"><span class="cl">libceph               <span class="m">315392</span>  <span class="m">1</span> ceph
</span></span><span class="line"><span class="cl">fscache                <span class="m">65536</span>  <span class="m">1</span> ceph
</span></span><span class="line"><span class="cl">libcrc32c              <span class="m">16384</span>  <span class="m">3</span> xfs,raid456,libceph
</span></span><span class="line"><span class="cl">root@client:~# modinfo ceph
</span></span><span class="line"><span class="cl">filename:       /lib/modules/4.15.0-112-generic/kernel/fs/ceph/ceph.ko
</span></span><span class="line"><span class="cl">license:        GPL
</span></span><span class="line"><span class="cl">description:    Ceph filesystem <span class="k">for</span> Linux
</span></span><span class="line"><span class="cl">author:         Patience Warnick &lt;patience@newdream.net&gt;
</span></span><span class="line"><span class="cl">author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;
</span></span><span class="line"><span class="cl">author:         Sage Weil &lt;sage@newdream.net&gt;
</span></span><span class="line"><span class="cl">alias:          fs-ceph
</span></span><span class="line"><span class="cl">srcversion:     B2806F4EAACAC1E19EE7AFA
</span></span><span class="line"><span class="cl">depends:        libceph,fscache
</span></span><span class="line"><span class="cl">retpoline:      Y
</span></span><span class="line"><span class="cl">intree:         Y
</span></span><span class="line"><span class="cl">name:           ceph
</span></span><span class="line"><span class="cl">vermagic:       4.15.0-112-generic SMP mod_unload
</span></span><span class="line"><span class="cl">signat:         PKCS#7
</span></span><span class="line"><span class="cl">signer:        
</span></span><span class="line"><span class="cl">sig_key:       
</span></span><span class="line"><span class="cl">sig_hashalgo:   md4
</span></span></code></pre></td></tr></table>
</div>
</div><p>在启用了fs-cache后，内核日志可以看到对应 cephfs 挂载时 ceph 被注册到 fscache中</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>  11457.592011<span class="o">]</span> FS-Cache: Loaded
</span></span><span class="line"><span class="cl"><span class="o">[</span>  11457.617265<span class="o">]</span> Key <span class="nb">type</span> ceph registered
</span></span><span class="line"><span class="cl"><span class="o">[</span>  11457.617686<span class="o">]</span> libceph: loaded <span class="o">(</span>mon/osd proto 15/24<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>  11457.640554<span class="o">]</span> FS-Cache: Netfs <span class="s1">&#39;ceph&#39;</span> registered <span class="k">for</span> caching
</span></span><span class="line"><span class="cl"><span class="o">[</span>  11457.640558<span class="o">]</span> ceph: loaded <span class="o">(</span>mds proto 32<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>  11457.640978<span class="o">]</span> libceph: parse_ips bad ip <span class="s1">&#39;mon1.ichenfu.com:6789,mon2.ichenfu.com:6789,mon3.ichenfu.com:6789&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>当 monitor / OSD 拒绝连接时，所有该节点后续创建的挂载均会使用缓存，除非 umount 所有挂载后重新挂载才可以重新与 ceph mon 建立连接</p>
</blockquote>
<h2 id="cephfs-中的-fscache">cephfs 中的 fscache</h2>
<p>ceph 官方在 2023年11月5日的一篇博客 <sup><a href="#1">[1]</a></sup> 中介绍了，cephfs 与 fscache 结合的介绍。这个功能的加入最显著的成功就是 ceph node 流向 OSD 网络被大大减少，尤其是在读取多的情况下。</p>
<p>这个机制可以在代码 commit 中看到其原理：“在第一次通过文件引用inode时创建缓存cookie。之后，直到我们处理掉inode，我们都不会摆脱cookie” <sup><a href="#2">[2]</a></sup></p>
<h2 id="结合fscache的kubernetes中使用cephfs造成的集群规模故障">结合fscache的kubernetes中使用cephfs造成的集群规模故障</h2>
<p>在了解了上面的基础知识后，就可以引入故障了，下面是故障产生环境的配置</p>
<h3 id="故障发生环境">故障发生环境</h3>
<table>
<thead>
<tr>
<th>软件</th>
<th>版本</th>
</tr>
</thead>
<tbody>
<tr>
<td>Centos</td>
<td>7.9</td>
</tr>
<tr>
<td>Ceph</td>
<td>nautilus (14.20)</td>
</tr>
<tr>
<td>Kernel</td>
<td>4.18.16</td>
</tr>
</tbody>
</table>
<h3 id="故障的描述">故障的描述</h3>
<p>当网络出现问题时，如果使用了 cephfs 的 Pod 就会出现大量故障，具体故障表现方式有下面几种</p>
<ul>
<li>
<p>新部署的 Pod 处于 Waiting 状态</p>
</li>
<li>
<p>新部署的 Pod 可以启动成功，但是无法读取 cephfs 的挂载目录，主要故障表现为下面几种形式：</p>
<ul>
<li>ceph mount error 5 = input/output error <sup><a href="#3">[3]</a></sup></li>
<li>cephfs mount failure.permission denied</li>
</ul>
</li>
<li>
<p>旧 Pod 无法被删除</p>
</li>
<li>
<p>新部署的 Pod 无法启动</p>
</li>
</ul>
<blockquote>
<p>注：上面故障引用都是在网络上找到相同报错的一些提示，并不完全切合本文中故障描述</p>
</blockquote>
<p>去对应节点查看日志会发现有下面几个特征</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/IMG_20231108_150726-ink.jpeg" alt="IMG_20231108_150726-ink"  /></p>
<center>图2-1：故障发生的节点报错</center>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/IMG_20231109_193023-ink.jpeg" alt="IMG_20231109_193023-ink"  /></p>
<center>图2-2：故障发生的节点报错</center>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/IMG_20231109_192930-ink.jpeg" alt="IMG_20231109_192930-ink"  /></p>
<center>图2-3：故障发生的节点报错</center>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">    [ 1815.029831] ceph: mds0 closed our session
</span></span><span class="line"><span class="cl">    [ 1815.029833] ceph: mds0 reconnect start
</span></span><span class="line"><span class="cl">    [ 1815.052219] ceph: mds0 reconnect denied
</span></span><span class="line"><span class="cl">    [ 1815.052229] ceph:  dropping dirty Fw state for ffff9d9085da1340 1099512175611
</span></span><span class="line"><span class="cl">    [ 1815.052231] ceph:  dropping dirty+flushing Fw state for ffff9d9085da1340 1099512175611
</span></span><span class="line"><span class="cl">    [ 1815.273008] libceph: mds0 10.99.10.4:6801 socket closed (con state NEGOTIATING)
</span></span><span class="line"><span class="cl">    [ 1816.033241] ceph: mds0 rejected session
</span></span><span class="line"><span class="cl">    [ 1829.018643] ceph: mds0 hung
</span></span><span class="line"><span class="cl">    [ 1880.088504] ceph: mds0 came back
</span></span><span class="line"><span class="cl">    [ 1880.088662] ceph: mds0 caps renewed
</span></span><span class="line"><span class="cl">    [ 1880.094018] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm
</span></span><span class="line"><span class="cl">    [ 1881.100367] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm
</span></span><span class="line"><span class="cl">    [ 2046.768969] conntrack: generic helper won&#39;t handle protocol 47. Please consider loading the specific helper module.
</span></span><span class="line"><span class="cl">    [ 2061.731126] ceph: get_quota_realm: ino (10000000afe.fffffffffffffffe) null i_snap_realm
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="故障分析">故障分析</h3>
<p>由上面的三张图我们可以得到几个关键点</p>
<ol>
<li>connection reset</li>
<li>session lost, hunting for new mon</li>
<li>ceph: get_quota_realm()</li>
<li>reconnection denied</li>
<li>mds1 hung</li>
<li>mds1 caps stale</li>
</ol>
<p>这三张图上的日志是一个故障恢复的顺序，而问题节点（通常为整个集群 Node）内核日志都会在刷 <code>ceph: get_quota_realm()</code> 这种日志，首先我们需要确认第一个问题，<code>ceph: get_quota_realm()</code> 是什么原因导致，在互联网上找了一个 linux kernel 关于修复这个问题的提交记录，通过 commit，我们可以看到这个函数产生的原因</p>
<blockquote>
<p>get_quota_realm() enters infinite loop if quota inode has no caps.
This can happen after client gets evicted.  <sup><a href="#4">[4]</a></sup></p>
</blockquote>
<p>这里可以看到，修复的内容是当客户端被驱逐时，这个函数会进入无限 loop ，当 inode 配额没有被授权的用户，常常发生在客户端被驱逐。</p>
<p>通过这个 commit，我们可以确定了后面 4 - 6 问题的疑问，即客户端被 ceph mds 驱逐（加入了黑名单），在尝试重连时就会发生 <code>reconnection denied</code> 接着发生陈腐的被授权认证的用户 (caps stale)。<font color="#f8070d" size=3>接着由于本身没有真实的卸载，而是使用了一个共享的 cookie 这个时候就会发生节点新挂载的目录是没有权限写，或者是  input/output error 的错误</font>，这些错误表象是根据不同情况下而定，比如说被拉黑和丢失的会话。</p>
<h3 id="kubelet的错误日志">kubelet的错误日志</h3>
<p>此时当新的使用了 volumes 去挂载 cephfs时，由于旧的 Pod 产生的工作目录 (/var/lib/kubelet) 下的 Pod 挂载会因为 cephfs caps stale  而导致无法卸载，这是就会存在 “孤儿Pod”，“不能同步 Pod 的状态”，“不能创建新的Pod挂载，因为目录已存在”。</p>
<p>kubelet 日志如下所示：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubelet_volumes.go:66<span class="o">]</span> pod <span class="s2">&#34;5446c441-9162-45e8-e11f46893932&#34;</span> found, but error stat /var/lib/kubelet/pods/5446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxx: permission denied occurred during checking mounted volumes from disk
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">pod_workers.go:119<span class="o">]</span> Error syncing pod <span class="s2">&#34;5446c441-9162-45e8-e11f46893932&#34;</span> <span class="o">(</span><span class="s2">&#34;xxxxx-xxx-xxx-xxxx-xxx_xxxxx(5446c441-9162-45e8-e11f46893932)&#34;</span>, skipping: failed to <span class="s2">&#34;StartContainer&#34;</span> <span class="k">for</span> <span class="s2">&#34;xxxxx-xxx-xxx&#34;</span> with RunContainerError: <span class="s2">&#34;failed to start container \&#34;719346531es654113s3216e1456313d51as132156\&#34;: Error response from daemon: error while createing mount source path &#39;/var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx&#39;: mkdir /var/lib/kubelet/pods/5446c441-9162-45446c441-9162-45e8-e11f46893932/volumes/kubernetes.io~cephfs/xxxxxx-xx: file exists&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="问题如何解决">问题如何解决</h2>
<p>首先上面我们阐述了问题出现背景以及原因，要想解决这些错误，要分为两个步骤：</p>
<ol>
<li>首先驱逐 Kubernetes  Node 节点上所有挂载 cephfs 的 Pod，这步骤是为了优雅的结束 fscache 的 cookie cache 机制，使节点可以正常的提供服务</li>
<li>解决使用 fscache 因网络问题导致的会话丢失问题的重连现象</li>
</ol>
<p>这里主要以步骤2来阐述，解决这个问题就是通过两个方式，一个是不使用 fscache，另一个则是不让 mds 拉黑客户端，关闭 fscache 的成本很难，至今没有尝试成功，这里通过配置 ceph 服务使得 ceph mds 不会拉黑因出现网络问题丢失连接的客户端。</p>
<p>ceph 中阐述了驱逐的概念 “当某个文件系统客户端不响应或者有其它异常行为时，有必要强制切断它到文件系统的访问，这个过程就叫做<em>驱逐</em>。”  <sup><a href="#5">[5]</a></sup></p>
<p>要想解决这个问题，ceph 提供了一个参数来解决这个问题，<code>mds_session_blacklist_on_timeout</code></p>
<blockquote>
<p>It is possible to respond to slow clients by simply dropping their MDS sessions, but permit them to re-open sessions and permit them to continue talking to OSDs.  To enable this mode, set <code>mds_session_blacklist_on_timeout</code> to false on your MDS nodes. <sup><a href="#6">[6]</a></sup></p>
</blockquote>
<p>最终在配置后，上述问题解决</p>
<h3 id="附ceph-mds-管理客户端">附：ceph mds 管理客户端</h3>
<p>查看一个客户端的连接</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph daemon mds.xxxxxxxx session ls <span class="p">|</span>grep -E <span class="s1">&#39;inst|hostname|kernel_version&#39;</span><span class="p">|</span>grep xxxx
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;inst&#34;</span>: <span class="s2">&#34;client.105123 v1:192.168.0.0:0/11243531&#34;</span>,
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;hostname&#34;</span>: <span class="s2">&#34;xxxxxxxxxxxxxxxxxx&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>手动驱逐一个客户端</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph tell mds.0 client evict <span class="nv">id</span><span class="o">=</span><span class="m">105123</span>
</span></span><span class="line"><span class="cl">2023-11-12 13:25:23:381 7fa3a67fc700 <span class="m">0</span> client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231
</span></span><span class="line"><span class="cl">2023-11-12 13:25:23:421 7fa3a67fc700 <span class="m">0</span> client.105123 ms_handle_reset on v2:192.168.0.0:6800/112351231
</span></span></code></pre></td></tr></table>
</div>
</div><p>查看 ceph 的配置参数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph config dump
</span></span><span class="line"><span class="cl">WHO     MASK  LEVEL     OPTION                                VALUE RO
</span></span><span class="line"><span class="cl">  mon         advanced  auth_allow_insecure_global_id_reclaim <span class="nb">false</span>
</span></span><span class="line"><span class="cl">  mon         advanced  mon_allow_pool_delete                 <span class="nb">false</span>
</span></span><span class="line"><span class="cl">  mds         advanced  mds_session_blacklist_on_evict        <span class="nb">false</span>
</span></span><span class="line"><span class="cl">  mds         advanced  mds_session_blacklist_on_timeout      <span class="nb">false</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>当出现问题无法卸载时应如何解决？</p>
<p>当我们遇到问题时，卸载目录会出现被占用情况，通过 mount 和 fuser 都无法卸载</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">umount -f /tmp/998
</span></span><span class="line"><span class="cl">umount： /tmp/998: target is buy.
</span></span><span class="line"><span class="cl">        <span class="o">(</span>In some cases useful info about processes that use th device is found by losf<span class="o">(</span>8<span class="o">)</span> or fuser<span class="o">(</span>1<span class="o">))</span>
</span></span><span class="line"><span class="cl">        the device is found by losf<span class="o">(</span>8<span class="o">)</span> or fuser<span class="o">(</span>1<span class="o">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">fuser -v1 /root/test
</span></span><span class="line"><span class="cl">Cannot stat /root/test: Input/output error
</span></span></code></pre></td></tr></table>
</div>
</div><p>这个时候由于 cephfs 挂载问题会导致整个文件系统不可用，例如 df -h, ls dir 等，此时可以使用 umount 的懒卸载模式 <code>umount -l</code>，这会告诉内核当不占用时被卸载，由于这个问题是出现问题，而不是长期占用，这里用懒卸载后会立即卸载，从而解决了 stuck 的问题。</p>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://ceph.io/en/news/blog/2013/first-impressions-through-fscache-and-ceph/">First Impressions Through Fscache and Ceph</a></p>
<p><sup id="2">[2]</sup> <a href="https://lwn.net/Articles/563146/">ceph: persistent caching with fscache</a></p>
<p><sup id="3">[3]</sup> <a href="https://tracker.ceph.com/issues/51191">Cannot Mount CephFS No Timeout, mount error 5 = Input/output error</a></p>
<p><sup id="4">[4]</sup> <a href="https://patchwork.kernel.org/project/ceph-devel/patch/20190531122802.12814-3-zyan@redhat.com/">ceph: fix infinite loop in get_quota_realm()</a></p>
<p><sup id="5">[5]</sup> <a href="https://drunkard.github.io/cephfs/eviction/">Ceph 文件系统客户端的驱逐</a></p>
<p><sup id="6">[6]</sup> <a href="https://docs.ceph.com/en/mimic/cephfs/eviction/#advanced-configuring-blacklisting">advanced-configuring-blacklisting</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph对象存储 - windows上安装s3cmd</title>
      <link>https://www.oomkill.com/2023/09/05-4-s3cmd-in-windows/</link>
      <pubDate>Sun, 24 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/09/05-4-s3cmd-in-windows/</guid>
      <description></description>
      <content:encoded><![CDATA[<p><strong>s3cmd</strong> 是为了管理 Linux 服务器上的 S3 存储桶而创建的。   但我们也在 Windows 服务器上使用这个工具。   本文将帮助您在 Windows 系统中设置 s3cmd</p>
<h2 id="requirment">Requirment</h2>
<p><strong>s3cmd 系统要求：</strong>  s3cmd 需要 Python 2.7 或更高版本才能运行，还需要安装GPG。</p>
<h2 id="步骤1安装-python">步骤1：安装 Python</h2>
<p>从 <a href="https://www.python.org/downloads/">python</a> 官方网站下载并安装 python 2.7 或更高版本并安装。安装python后，将将其加到 <strong>PATH</strong> 环境变量。</p>
<h2 id="步骤-2-在-windows-上安装-gpg">步骤 2： 在 Windows 上安装 GPG</h2>
<p><a href="http://www.gpg4win.org/download.html">Gpg4win</a> (<em>GNU Privacy Guard for Windows</em>) 是一款用于数字加密 (file, email) 的免费软件，可以使用以下链接下载并安装它。</p>
<h2 id="步骤3配置-s3cmd">步骤3：配置 s3cmd</h2>
<p>下载最新的 s3cmd 源代码 <a href="http://s3tools.org/download">从s3cmd 官方页面 </a>并解压；</p>
<p>提取源代码后，使用以下命令设置 s3 环境。  它会询问您的 对象存储的 <em>AccessKey</em> 和 <em>SecretKey</em>，即 GPG 命令的路径</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bat" data-lang="bat"><span class="line"><span class="cl">C<span class="p">:</span><span class="nl">s3cmd</span><span class="c1">&gt; python s3cmd --configure</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Enter new values or accept defaults in brackets with Enter.
</span></span><span class="line"><span class="cl">Refer to user manual for detailed description of all options.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Access key and Secret key are your identifiers for Amazon S3
</span></span><span class="line"><span class="cl">Access Key: XXXXXXXXXXXXXXXXXXXX
</span></span><span class="line"><span class="cl">Secret Key: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Encryption password is used to protect your files from reading
</span></span><span class="line"><span class="cl">by unauthorized persons while in transfer to S3
</span></span><span class="line"><span class="cl">Encryption password: XXXXXXXXX
</span></span><span class="line"><span class="cl"><span class="k">Path</span> to GPG program: C:\Program Files (x86)\GNU\GnuPG\gpg2.exe
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">When using secure HTTPS protocol all communication with Amazon S3
</span></span><span class="line"><span class="cl">servers is protected from 3rd party eavesdropping. This method is
</span></span><span class="line"><span class="cl">slower than plain HTTP and can&#39;t be used if you&#39;re behind a proxy
</span></span><span class="line"><span class="cl">Use HTTPS protocol [No]: Yes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">New settings:
</span></span><span class="line"><span class="cl">  Access Key: XXXXXXXXXXXXXXXXXXXX
</span></span><span class="line"><span class="cl">  Secret Key: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
</span></span><span class="line"><span class="cl">  Encryption password: XXXXXXXXX
</span></span><span class="line"><span class="cl">  <span class="k">Path</span> to GPG program: C:Program Files (x86)GNUGnuPGgpg2.exe
</span></span><span class="line"><span class="cl">  Use HTTPS protocol: True
</span></span><span class="line"><span class="cl">  HTTP Proxy server name:
</span></span><span class="line"><span class="cl">  HTTP Proxy server port: 0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Test access with supplied credentials? [Y/n] Y
</span></span><span class="line"><span class="cl">Please wait, attempting to list all buckets...
</span></span><span class="line"><span class="cl">Success. Your access key and secret key worked fine :-)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Now verifying that encryption works...
</span></span><span class="line"><span class="cl">Success. Encryption and decryption worked fine :-)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Save settings? [y/N] Y
</span></span><span class="line"><span class="cl">Configuration saved to &#39;C:\Users\Administrator\Application Data\s3cmd.ini&#39;
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="步骤4验证">步骤4：验证</h2>
<p>使用以下命令来验证 s3cmd 配置</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">&gt; python c:<span class="se">\s</span>3cmd<span class="se">\s</span>3cmd ls
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph对象存储 - 使用s3cmd管理对象存储</title>
      <link>https://www.oomkill.com/2023/09/05-3-s3cmd/</link>
      <pubDate>Sun, 24 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/09/05-3-s3cmd/</guid>
      <description></description>
      <content:encoded><![CDATA[<p><strong>s3cmd</strong> 是一个 Amazon S3 工具，可以用于创建 s3 bucket、向对象存储中上传，检索和管理数据，在下文将如何在 Linux 上如何安装和使用 “s3cmd” 工具。</p>
<h2 id="在-linux-上安装-s3cmd">在 Linux 上安装 s3cmd</h2>
<p>s3cmd 在 Ubuntu/Debian, Fedora/CentOS/RHEL 这类发行版上的默认软件包存储库中都是可用的，只需在执行对应发行版的安装命令即可安装。</p>
<h3 id="centosrhelfedora">CentOS/RHEL/Fedora</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># centos 8</span>
</span></span><span class="line"><span class="cl">$ sudo dnf install s3cmd 
</span></span><span class="line"><span class="cl"><span class="c1"># centos 7</span>
</span></span><span class="line"><span class="cl">$ sudo yum install s3cmd 
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="ubuntudebian">Ubuntu/Debian</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt-get install s3cmd
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="安装最新版本">安装最新版本</h3>
<p>通常包管理仓库中的版本比较旧，或者使用的 Linux 没有包管理来获取最新版本的 s3cmd，那么可以使用源代码在系统上安装最新版本的 s3cmd，下载地址可以参考附录1  <sup><a href="#1">[1]</a></sup></p>
<p>下面以 2.2 版本进行安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ wget https://sourceforge.net/projects/s3tools/files/s3cmd/2.2.0/s3cmd-2.2.0.tar.gz
</span></span><span class="line"><span class="cl">$ tar xzf s3cmd-2.2.0.tar.gz
</span></span></code></pre></td></tr></table>
</div>
</div><p>使用以下命令和源文件安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ <span class="nb">cd</span> s3cmd-2.2.0 
</span></span><span class="line"><span class="cl">$ sudo python setup.py install 
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="配置-s3cmd">配置 s3cmd</h3>
<p><em>s3cmd</em> 并不仅仅可以管理 AWS s3，也可以管理任意的 S3 对象存储，为了配置 s3cmd 我们需要 <em>Access Key</em> 和 <em>Secret Key</em> 您的 S3 来访问 S3 对象存储，通常 AWS S3 的  <em>Access Key</em> 和 <em>Secret Key</em> 需要到 Amazon security_credential 页面获取 (这里涉及到 AWS 中的用户管理)</p>
<p>使用下列命令配置 s3cmd</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">s3cmd --configure 
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Note：通常这个配置是交互类型的，很多值在自维护的 S3 对象存储中不需要配置，可以一路回车即可</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Enter new values or accept defaults in brackets with Enter.
</span></span><span class="line"><span class="cl">Refer to user manual <span class="k">for</span> detailed description of all options.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Access key and Secret key are your identifiers <span class="k">for</span> Amazon S3
</span></span><span class="line"><span class="cl">Access Key: xxxxxxxxxxxxxxxxxxxxxx
</span></span><span class="line"><span class="cl">Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Encryption password is used to protect your files from reading
</span></span><span class="line"><span class="cl">by unauthorized persons <span class="k">while</span> in transfer to S3
</span></span><span class="line"><span class="cl">Encryption password: xxxxxxxxxx
</span></span><span class="line"><span class="cl">Path to GPG program <span class="o">[</span>/usr/bin/gpg<span class="o">]</span>:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">When using secure HTTPS protocol all communication with Amazon S3
</span></span><span class="line"><span class="cl">servers is protected from 3rd party eavesdropping. This method is
</span></span><span class="line"><span class="cl">slower than plain HTTP and can<span class="s1">&#39;t be used if you&#39;</span>re behind a proxy
</span></span><span class="line"><span class="cl">Use HTTPS protocol <span class="o">[</span>No<span class="o">]</span>: Yes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">New settings:
</span></span><span class="line"><span class="cl">  Access Key: xxxxxxxxxxxxxxxxxxxxxx
</span></span><span class="line"><span class="cl">  Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
</span></span><span class="line"><span class="cl">  Encryption password: xxxxxxxxxx
</span></span><span class="line"><span class="cl">  Path to GPG program: /usr/bin/gpg
</span></span><span class="line"><span class="cl">  Use HTTPS protocol: True
</span></span><span class="line"><span class="cl">  HTTP Proxy server name:
</span></span><span class="line"><span class="cl">  HTTP Proxy server port: <span class="m">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Test access with supplied credentials? <span class="o">[</span>Y/n<span class="o">]</span> Y
</span></span><span class="line"><span class="cl">Please wait, attempting to list all buckets...
</span></span><span class="line"><span class="cl">Success. Your access key and secret key worked fine :-<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Now verifying that encryption works...
</span></span><span class="line"><span class="cl">Success. Encryption and decryption worked fine :-<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Save settings? <span class="o">[</span>y/N<span class="o">]</span> y
</span></span><span class="line"><span class="cl">Configuration saved to <span class="s1">&#39;/root/.s3cfg&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>最终生成的文件在目录 <code>/root/.s3cfg</code> 下</p>
<p>通常需要关注的参数只有几个</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">[default]
</span></span><span class="line"><span class="cl">access_key = &lt;ACCESS KEY FROM PORTAL&gt;
</span></span><span class="line"><span class="cl">host_base = s3-api.us-geo.objectstorage.softlayer.net
</span></span><span class="line"><span class="cl"># 这个要注意，在ceph中使用的是下面格式，表示列出的 host/bucket
</span></span><span class="line"><span class="cl">host_bucket = s3-api.us-geo.objectstorage.softlayer.net/%(bucket)
</span></span><span class="line"><span class="cl">secret_key = &lt;SECRET KEY LISTED IN PORTAL&gt;
</span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://gist.github.com/greyhoundforty/a4a9d80a942d22a8a7bf838f7abbcab2">https://gist.github.com/greyhoundforty/a4a9d80a942d22a8a7bf838f7abbcab2</a></p>
<h2 id="s3cmd-examples">s3cmd examples</h2>
<table>
<thead>
<tr>
<th>说明</th>
<th>命令</th>
</tr>
</thead>
<tbody>
<tr>
<td>列出 bucket 文件</td>
<td>s3cmd ls</td>
</tr>
<tr>
<td>创建存储桶</td>
<td>s3cmd mb s3://tecadmin</td>
</tr>
<tr>
<td>上传文件到 bucket</td>
<td>s3cmd put file.txt s3://tecadmin/</td>
</tr>
<tr>
<td>上传目录到 bucket</td>
<td>s3cmd put -r backup s3://tecadmin/ 需要注意斜杠才表示目录</td>
</tr>
<tr>
<td>下载文件</td>
<td>s3cmd get s3://tecadmin/file.txt</td>
</tr>
<tr>
<td>从 bucket 删除文件</td>
<td>s3cmd del s3://tecadmin/file.txt</td>
</tr>
<tr>
<td>删除一个目录</td>
<td>s3cmd del s3://tastethelinux/Script</td>
</tr>
<tr>
<td>删除 bucket</td>
<td>s3cmd rb s3://tastethelinux</td>
</tr>
<tr>
<td>拷贝 bucket 文件到另一个 bucket</td>
<td>s3cmd cp s3://tastethelinux/tla.txt s3://tastethelinux-example</td>
</tr>
<tr>
<td>移动 bucket 文件到另一个 bucket</td>
<td>s3cmd mv s3://tastethelinux/tla.txt s3://tastethelinux-example/tla_new.txt</td>
</tr>
<tr>
<td>查看存储使用量</td>
<td>s3cmd du s3://tastethelinux/ &ndash;human-readable</td>
</tr>
<tr>
<td>获取 bucket 信息</td>
<td>s3cmd info s3://tastethelinux</td>
</tr>
<tr>
<td>继续上次中断的文件</td>
<td>s3cmd &ndash;continue get s3://tastethelinux/tastethelinux.tar.gz</td>
</tr>
<tr>
<td>尝试运行但不上传</td>
<td>s3cmd &ndash;dry-run</td>
</tr>
<tr>
<td>排除规则 <strong>—exclude / —include</strong> shell 风格通配符</td>
<td>s3cmd sync &ndash;dry-run &ndash;exclude &lsquo;*.txt&rsquo;</td>
</tr>
<tr>
<td>排除规则 <strong>—rexclude / —rinclude</strong> 正则表达式</td>
<td>s3cmd sync &ndash;dry-run &ndash;exclude &lsquo;*.(txt|jpg)&rsquo;</td>
</tr>
<tr>
<td>同步</td>
<td>s3cmd sync  ./  s3://s3tools-demo/some/path/</td>
</tr>
</tbody>
</table>
<p>需要注意的是，<em>s3cmd sync</em> 首先检查 目的 已存在的文件的列表和详细信息，与==本地文件进行比较==，然后仅上传远程不存在或具有不同大小或 md5 校验和的文件。如果您运行了上述所有示例，您将从同步中获得与以下输出类似的输出：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ s3cmd sync  ./  s3://s3tools-demo/some/path/
</span></span><span class="line"><span class="cl">dir2/file2-1.log -&gt; s3://s3tools-demo/some/path/dir2/file2-1.log  <span class="o">[</span><span class="m">1</span> of 2<span class="o">]</span>
</span></span><span class="line"><span class="cl">dir2/file2-2.txt -&gt; s3://s3tools-demo/some/path/dir2/file2-2.txt  <span class="o">[</span><span class="m">2</span> of 2<span class="o">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://sourceforge.net/projects/s3tools/files/s3cmd/"><em><strong>s3cmd Files</strong></em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph对象存储 - 桶策略 Bucket Policy</title>
      <link>https://www.oomkill.com/2023/09/05-2-bucket-policy/</link>
      <pubDate>Mon, 18 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/09/05-2-bucket-policy/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>CEPH RGW 支持 Bucket 的  S3 策略语言，但又不完全类似于 S3 的策略，因为 S3 中策略是基于 AWS 的，某些属性在 CEPH 中并不存在，下面就解开 RGW 关于桶策略的配置。</p>
<p>Bucket Policy (桶策略，下文中统称为 <strong>BP</strong>) 是对象存储中的管理权限和对象存储访问的机制。</p>
<h2 id="policy-language-的组成">Policy Language 的组成</h2>
<p>BP 的格式采用了 JSON 语言，也就是 PL 是基于 JSON 的一种策略语言，他的格式主要为几个元素</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="nt">&#34;Version&#34;</span><span class="p">:</span> <span class="s2">&#34;2012-10-17&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"> <span class="nt">&#34;Statement&#34;</span><span class="p">:</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">   <span class="nt">&#34;Effect&#34;</span><span class="p">:</span> <span class="err">...</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="nt">&#34;Principal&#34;</span><span class="p">:</span> <span class="err">...</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="nt">&#34;Action&#34;</span><span class="p">:</span> <span class="err">...</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="nt">&#34;Resource&#34;</span><span class="p">:</span> <span class="err">...</span>
</span></span><span class="line"><span class="cl"> <span class="p">}]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>该结构由 ==一个== <strong>Version</strong> (表示当前版本) 和 ==一个或多个== <strong>Statement</strong> 数组组成，这些数组定义了希望应用的策略。每个语句数组中都有<strong>Effect</strong>, <strong>Principal</strong>, <strong>Action</strong>, <strong>Resource</strong> 和可选的 <strong>Condition</strong> 元素。</p>
<h3 id="effect">Effect</h3>
<p><em>Effect</em> 部分定义是一个动作，表示是否 <em>Allow</em> 或 <em>Deny</em> 指定资源的访问</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Effect&#34;</span><span class="err">:</span><span class="s2">&#34;Allow&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="principal">Principal</h3>
<p><em>Principal</em> 部分定义了策略应用的 “用户” 或 “实体” (entity) 或 “服务” 等，这里是按照 aws 中子源固定语法组成，当然在 CEPH 中不存在这些资源，那么相对的也是一种固定格式</p>
<p>AWS 将用户分为了三类：</p>
<ul>
<li>AWS 账户 (AWS ACCOUNT)</li>
<li>IAM 用户 (IAM USER)</li>
<li>匿名用户 (anonymous)</li>
</ul>
<p>语法如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Principal&#34;</span><span class="err">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;AWS&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;arn:aws:iam:::user/a0000000-000a-0000-0000-00d0ff0f0000&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="aws-account">AWS Account</h4>
<p>对于该类 “实体”，采用了下面的语法</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;AWS&#34;</span><span class="err">:</span><span class="s2">&#34;account-ARN&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>示例</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Principal&#34;</span><span class="err">:</span><span class="p">{</span><span class="nt">&#34;AWS&#34;</span><span class="p">:</span><span class="s2">&#34;arn:aws:iam::AccountIDWithoutHyphens:root&#34;</span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>或</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Principal&#34;</span><span class="err">:</span><span class="p">{</span><span class="nt">&#34;AWS&#34;</span><span class="p">:[</span><span class="s2">&#34;arn:aws:iam::AccountID1WithoutHyphens:root&#34;</span><span class="p">,</span><span class="s2">&#34;arn:aws:iam::AccountID2WithoutHyphens:root&#34;</span><span class="p">]}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="iam-user">IAM USER</h4>
<p>AWS IAM (<em>AWS Identity and Access Management</em>) ,是 AWS 中用户管理的一种方式，指定 “WHO”, “CAN ACCESS”, “WAHT” (AWS 中的服务和资源、集中管理精细权限)</p>
<p>对于该类 “实体”，采用了下面的语法</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Principal&#34;</span><span class="err">:</span><span class="p">{</span><span class="nt">&#34;AWS&#34;</span><span class="p">:</span><span class="s2">&#34;arn:aws:iam::account-number-without-hyphens:user/username&#34;</span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="匿名用户">匿名用户</h4>
<p>匿名用户就是指对 ”每个人都授予的权限“，可以使用 通配符  (&quot;*&quot;) ，这类权限的配置对象是 ”存储桶中的所有对象均可公开访问“</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Principal&#34;</span><span class="err">:</span><span class="s2">&#34;*&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>或者</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Principal&#34;</span><span class="err">:</span><span class="p">{</span><span class="nt">&#34;AWS&#34;</span><span class="p">:</span><span class="s2">&#34;*&#34;</span><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>上面两类用户实际上权限分散的还是不一样，主要区别如下：</p>
<ul>
<li><code>&quot;Principal&quot;: &quot;*&quot;</code>  并且 <code>Effect: Allow</code> 那么将允许任何人访问该资源。</li>
<li>如果  <code>Effect: Allow</code> 并且 <code>&quot;Principal&quot; : { &quot;AWS&quot; : &quot;*&quot; }</code>，允许 AWS 账户中的 IAM User, Root User.. 访问该资源（通常 不涉及到 CEPH）</li>
</ul>
<h3 id="action">Action</h3>
<p><em>Action</em> 部分定义了 ”对策略授予 (删除) 的权限”。这些操作包括 <em>list bucket</em> 等的功能；需要注意的是，这里 AWS S3 与 CEPH RGW 中定义的权限又不相同，对于 CEPH 中 Action 的权限集合，可以参考 <sup><a href="#1">[1]</a></sup></p>
<h3 id="resource">Resource</h3>
<p><em>Action</em> 部分定义了 “应用于对象存储资源” 例如存储桶和对象，这里的资源类型也是一种基于 aws 资源的固定格式，而在 CEPH 中不存在的部分直接为空，例如：</p>
<ul>
<li>Bucket resources：<code>&quot;arn:aws:s3:::[bucket]&quot;</code></li>
<li>应用所存储同种所有对象或部分对象：<code>&quot;arn:aws:s3:::[bucket]/[object]&quot;</code></li>
</ul>
<p>在上面的策略语言中，将 [bucket] 替换为存储桶的标签，将 [object] 替换为指定所有对象的通配符值 (*) 或对象的路径和名称。</p>
<p>例如下面几个示例</p>
<p>将策略应用到所有对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Resource&#34;</span><span class="err">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;arn:aws:s3:::example-bucket/*&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>指定目录中的所有对象：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Resource&#34;</span><span class="err">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;arn:aws:s3:::example-bucket/folder/*&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>特殊对象</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="s2">&#34;Resource&#34;</span><span class="err">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">  <span class="s2">&#34;arn:aws:s3:::example-bucket/example-file.ext&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="桶策略示例">桶策略示例</h2>
<h3 id="允许任何人查看和下载-bucket-中的对象">允许任何人查看和下载 bucket 中的对象</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="nt">&#34;Version&#34;</span><span class="p">:</span> <span class="s2">&#34;2012-10-17&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"> <span class="nt">&#34;Statement&#34;</span><span class="p">:</span> <span class="p">[{</span>
</span></span><span class="line"><span class="cl">   <span class="nt">&#34;Effect&#34;</span><span class="p">:</span> <span class="s2">&#34;Allow&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="nt">&#34;Principal&#34;</span><span class="p">:</span> <span class="s2">&#34;*&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">   <span class="nt">&#34;Action&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">     <span class="s2">&#34;s3:GetObject&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="s2">&#34;s3:ListBucket&#34;</span>
</span></span><span class="line"><span class="cl">   <span class="p">],</span>
</span></span><span class="line"><span class="cl">   <span class="nt">&#34;Resource&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">     <span class="s2">&#34;arn:aws:s3:::bucket-example/*&#34;</span>
</span></span><span class="line"><span class="cl">   <span class="p">]</span>
</span></span><span class="line"><span class="cl"> <span class="p">}]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="授予指定帐户只能对指定目录的访问权限">授予指定帐户只能对指定目录的访问权限</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="nt">&#34;Version&#34;</span><span class="p">:</span> <span class="s2">&#34;2012-10-17&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"> <span class="nt">&#34;Statement&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Effect&#34;</span><span class="p">:</span> <span class="s2">&#34;Allow&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Principal&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="nt">&#34;AWS&#34;</span><span class="p">:</span> <span class="s2">&#34;arn:aws:iam:::user/a0000000-000a-0000-0000-00d0ff0f0000&#34;</span>
</span></span><span class="line"><span class="cl">     <span class="p">},</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Action&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">       <span class="s2">&#34;s3:ListBucket&#34;</span>
</span></span><span class="line"><span class="cl">     <span class="p">],</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Resource&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">       <span class="s2">&#34;arn:aws:s3:::example-bucket&#34;</span>
</span></span><span class="line"><span class="cl">     <span class="p">]</span>
</span></span><span class="line"><span class="cl">   <span class="p">},</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Effect&#34;</span><span class="p">:</span> <span class="s2">&#34;Allow&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Principal&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="nt">&#34;AWS&#34;</span><span class="p">:</span> <span class="s2">&#34;arn:aws:iam:::user/a0000000-000a-0000-0000-00d0ff0f0000&#34;</span>
</span></span><span class="line"><span class="cl">     <span class="p">},</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Action&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">       <span class="s2">&#34;s3:GetObject&#34;</span>
</span></span><span class="line"><span class="cl">     <span class="p">],</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Resource&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">       <span class="s2">&#34;arn:aws:s3:::example-bucket/test/*&#34;</span>
</span></span><span class="line"><span class="cl">     <span class="p">]</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="允许特定-ip-的访问">允许特定 IP 的访问</h3>
<p>上面在 “策略语言” 中没有提到一个可选参数 “<strong>Condition</strong>”，这在 CEPH 中也是支持的，通过使用 “<strong>Condition</strong>” 您可以选择允许或拒绝来自指定 IP 地址或范围的流量。</p>
<p>下面的示例仅允许来自指定 IP 地址的所有流量：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl"> <span class="nt">&#34;Version&#34;</span><span class="p">:</span> <span class="s2">&#34;2012-10-17&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"> <span class="nt">&#34;Statement&#34;</span><span class="p">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">   <span class="p">{</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Effect&#34;</span><span class="p">:</span> <span class="s2">&#34;Allow&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Principal&#34;</span><span class="p">:</span> <span class="s2">&#34;*&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Action&#34;</span><span class="p">:</span> <span class="s2">&#34;s3:*&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Resource&#34;</span><span class="p">:</span> <span class="s2">&#34;arn:aws:s3:::example-bucket/*&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="nt">&#34;Condition&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">       <span class="nt">&#34;IpAddress&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">         <span class="nt">&#34;aws:SourceIp&#34;</span><span class="p">:</span> <span class="s2">&#34;192.0.2.1/32&#34;</span>
</span></span><span class="line"><span class="cl">       <span class="p">}</span>
</span></span><span class="line"><span class="cl">     <span class="p">}</span>
</span></span><span class="line"><span class="cl">   <span class="p">}</span>
</span></span><span class="line"><span class="cl"> <span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>目前 CEPH 支持的 <em><strong>Condition</strong></em> 字段的值为，参考与 <sup><a href="#1">[1]</a></sup></p>
<ul>
<li>aws:CurrentTime</li>
<li>aws:EpochTime</li>
<li>aws:PrincipalType</li>
<li>aws:Referer</li>
<li>aws:SecureTransport</li>
<li>aws:SourceIp</li>
<li>aws:UserAgent</li>
<li>aws:username</li>
</ul>
<h2 id="应用桶策略">应用桶策略</h2>
<p>桶策略的应用只能通过 <code>s3cmd</code> 命令执行，radosgw-admin 命令并不可以应用，所以要想应用前，需要准备好对应的对象存储相关的配置</p>
<p>语法</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">s3cmd setpolicy <span class="o">[</span>policy-file<span class="o">]</span> s3://<span class="o">[</span>bucket-label<span class="o">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>例如，将文件 “policy.json” 中定义的策略应用到名为 “example-bucket” 的存储桶中</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">s3cmd setpolicy policy.json s3://example-bucket
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://docs.ceph.com/en/latest/radosgw/bucketpolicy/#limitations"><em><strong>LIMITATIONS</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://www.linode.com/docs/products/storage/object-storage/guides/bucket-policies/"><em><strong>Guides - Define Access and Permissions using Bucket Policies</strong></em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>ceph常用命令</title>
      <link>https://www.oomkill.com/2023/09/11-1-ceph-common-cmd/</link>
      <pubDate>Wed, 13 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/09/11-1-ceph-common-cmd/</guid>
      <description></description>
      <content:encoded><![CDATA[<h4 id="测试上传下载对象">测试上传/下载对象</h4>
<p>存取故据时，客户端必须首先连接至RAD05集群上某存储地，而后根据对像名称由相关的中CRUSH规则完成数据对象寻址。于是为了测试集群的数据存储功能，首先创建一个用于测试的存储池mypool，并设定其PG数量为16个。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph osd pool create mypool <span class="m">16</span> <span class="m">16</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>而后，即可将测试文件上传至存储池中。例如下面的<code>rados put</code>命令将/etc/hosts</p>
<p>rados</p>
<p>lspool 显示存储池</p>
<p>rmpool 删除存储池</p>
<p>mkpool 创建存储池</p>
<p>rados mkpool mypool  32 32</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">rados mkpool <span class="o">{</span>name<span class="o">}</span> <span class="o">{</span>pgnum<span class="o">}</span> <span class="o">{</span>pgpnum<span class="o">}</span>
</span></span><span class="line"><span class="cl">rados mkpool <span class="nb">test</span> <span class="m">32</span> <span class="m">32</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ ceph osd pool create testpool <span class="m">32</span> <span class="m">32</span>
</span></span><span class="line"><span class="cl">pool <span class="s1">&#39;testpool&#39;</span> created
</span></span></code></pre></td></tr></table>
</div>
</div><p>列出存储池</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph osd pool ls
</span></span><span class="line"><span class="cl">mypool
</span></span><span class="line"><span class="cl">rbdpool
</span></span><span class="line"><span class="cl">testpool
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rados lspools  
</span></span><span class="line"><span class="cl">mypool
</span></span><span class="line"><span class="cl">rbdpool
</span></span><span class="line"><span class="cl">testpool
</span></span></code></pre></td></tr></table>
</div>
</div><p>而后即可将测试文件上传到存储池中，例如将<code>rados put</code>命令将<code>/etc/issue</code>文件上传至testpool存储池，对象名称仍然较保留文件名issue，而<code>rados ls</code>可以列出指定存储池中的数据对象</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">rados put issue /etc/issue --pool<span class="o">=</span>testpool    
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rados ls --pool<span class="o">=</span>testpool  <span class="c1"># --pool 指定放入那个存储池中去</span>
</span></span><span class="line"><span class="cl">issue
</span></span></code></pre></td></tr></table>
</div>
</div><p>而<font color="#f8070d" size=3><code>ceph osd map</code></font>可查看获取到存储池中数据对象的具体位置信息（数据和元数据怎么映射存储的)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph osd map testpool issue
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ceph osd map mypool passwd
</span></span><span class="line"><span class="cl">osdmap e36 pool <span class="s1">&#39;mypool&#39;</span> <span class="o">(</span>1<span class="o">)</span> object <span class="s1">&#39;passwd&#39;</span> -&gt; pg 1.27292a34 <span class="o">(</span>1.14<span class="o">)</span> -&gt; up <span class="o">([</span>0,3,2<span class="o">]</span>, p0<span class="o">)</span> acting <span class="o">([</span>0,3,2<span class="o">]</span>, p0<span class="o">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>    mypool存储池中的对象<code>passwd</code>被放在pg上<code>1.27292a34</code> 1为存储池编号<code>.</code>后面的编号可以理解为pg的位图。是pg的编号；<code>up ([0,3,2], p0)</code>正常可访问编号0、3、2，副本型存储池，crush算法计算得到，0为主osd。活动集<code>acting ([0,3,2], p0)</code>，此组pg(<code>pg 1.27292a34 (1.14)</code>)之下所有的osd(<code>[0,3,2]</code>)都处于正常活动状态。</p>
<h4 id="删除数据对象">删除数据对象</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ceph osd pool rm testpool --yes-i-really-really-mean-it
</span></span></code></pre></td></tr></table>
</div>
</div><p>    删除存储池命令存在数据丢失的风险，Ceph于是默认禁止此类操作。管理员需要在<font color="#f8070d" size=3><code>ceph.conf</code></font>配置文件中启用支持删除存储池的操作后，方可使用如下命令删除存储池。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">rados rm issue --pool<span class="o">=</span>mypool
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="ceph集群的访问接口">ceph集群的访问接口</h3>
<h4 id="ceph块设备接口">Ceph块设备接口</h4>
<p>Ceph块设备，也称为RADOS块设备（简称RBD），是一种基于RADOS存储系统支持超配，（thin-provisioned）、可伸缩的条带化数据存储系统，它通过librbd库与OSD进行交互。RBD为KVM等虚拟化技术和云OS（如OpenStack和CloudStack）提供高可用和无限扩展性的存储后端，这些系统以来与libvirt和QEMU实用程序与RBD进行集。</p>
<p>在集群部署完成以后，就具有了RBD接口，RBD接口关键是在客户端的配置。服务端本身可以直接使用。只需创建出存储池，在存储池中就可以创建块设备。块设备主要表现为存储池当中的镜像或映像文件（image）。</p>
<p>客户端基于librbd库即可将RADOS存储集群用作块设备，不过，用于rbd的存储池需要实现启用rbd功能并进行初始化。例如，创建一个名为rbddata的存储池，在启动rbd功能后对其进行初始化</p>
<p>对于rbdpool而言，创建完成后并不能直接使用，因为三种应用程序需要单独进行启用。相关存储池的应用才可以。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph osd pool create rbpool <span class="m">64</span> <span class="c1">## 指明pg数量</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># 默认情况下是裸池</span>
</span></span><span class="line"><span class="cl">$ ceph osd pool application <span class="nb">enable</span> rbdpool rbd
</span></span><span class="line"><span class="cl">enabled application <span class="s1">&#39;rbd&#39;</span> on pool <span class="s1">&#39;rbdpool&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">osd pool application <span class="nb">enable</span> &lt;poolname&gt; &lt;app&gt; <span class="o">{</span>--yes-i-really-mean-it<span class="o">}</span>             <span class="nb">enable</span> use of an application &lt;app&gt; <span class="o">[</span>cephfs,rbd,rgw<span class="o">]</span> on pool &lt;poolname&gt;
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rbd pool init -p rbddata
</span></span></code></pre></td></tr></table>
</div>
</div><p>不过，rbd存储池并不能直接用于块设备，而是需要事先在其中按需创建映像（image），以及可用映像、创建快照、将映像回滚到快照和查看快照等管理操作。</p>
<p>创建名为img1的映像</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">rbd create rbdpool/img --size 1G
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd ls -p rbdpool
</span></span><span class="line"><span class="cl">img
</span></span><span class="line"><span class="cl">img1
</span></span></code></pre></td></tr></table>
</div>
</div><p>显示映像的相关信息，<code>rbd info</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd info rbdpool/img
</span></span><span class="line"><span class="cl">rbd image <span class="s1">&#39;img&#39;</span>:
</span></span><span class="line"><span class="cl">        size <span class="m">1</span> GiB in <span class="m">256</span> objects
</span></span><span class="line"><span class="cl">        order <span class="m">22</span> <span class="o">(</span><span class="m">4</span> MiB objects<span class="o">)</span>
</span></span><span class="line"><span class="cl">        id: 38bb6b8b4567
</span></span><span class="line"><span class="cl">        block_name_prefix: rbd_data.38bb6b8b4567
</span></span><span class="line"><span class="cl">        format: <span class="m">2</span>
</span></span><span class="line"><span class="cl">        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
</span></span><span class="line"><span class="cl">        op_features: 
</span></span><span class="line"><span class="cl">        flags: 
</span></span><span class="line"><span class="cl">        create_timestamp: Fri Jun <span class="m">14</span> 17:08:48 <span class="m">2019</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在客户端主机上，用户通过内核级的rbd驱动识别相关设备，即可对其进行分区、创建文件系统并挂载使用。</p>
<p>Rank 层级
MDS MDS在哪台服务器 上
Pool 两个存储池，存储池都位于同一个ceph集群之上，所以看到的空间大小是一样的。</p>
<h4 id="检查集群状态">检查集群状态</h4>
<p>命令：ceph-s</p>
<p>输出信息：</p>
<ul>
<li>集群ID</li>
<li>集群运行状况</li>
<li>监视器地图版本号和监视器仲裁的状态</li>
<li>OSD map版本号和OSD的状态</li>
<li>归置组map版本</li>
<li>归置组和存储池数量</li>
<li>所存储数据理论上的数量和所存储对象的数量</li>
<li>所存储数据的总量</li>
</ul>
<h4 id="获取集群的即时状态">获取集群的即时状态</h4>
<ul>
<li>ceph pg stat</li>
<li>ceph osd pool stat</li>
<li>ceph df</li>
<li>ceph df detail</li>
</ul>
<p>ceph df</p>
<p>输出两端内容：GLOBAL和POOLS</p>
<ul>
<li>GLOBAL：存储量概览</li>
<li>POOLS：存储池列表和每个存储池的理论用量，但出不反应副本、克隆数据或快照</li>
</ul>
<p>GLOBAL段</p>
<ul>
<li>size 集群的整体存储容量</li>
<li>AVAIL 集群中可以使用的可用空间容量</li>
<li>RAW USED 已用的原始存储量</li>
<li>% RAW USED：已用的原始存储量百分比，将此数字与 full ratio和near full ratio搭配使用，可确保您不会用完集群的容量。</li>
<li></li>
</ul>
<h4 id="检查osd和mon的状态">检查OSD和Mon的状态</h4>
<p>可通过执行以下命令来检查OSD，以确保它们已启动里正在运行</p>
<ul>
<li><code>ceph osd stat</code></li>
<li><code>ceph osd dump</code>
还可以根据OSD在CRUSH map中的位置查看OSD</li>
<li><code>ceph osd tree</code>
<ul>
<li>Ceph将列显CRUSH树及主机它的OSD、OSD是否已启动及其权重</li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph osd tree
</span></span><span class="line"><span class="cl">ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF 
</span></span><span class="line"><span class="cl">-1       0.09775 root default                            
</span></span><span class="line"><span class="cl">-3       0.03897     host stor01                         
</span></span><span class="line"><span class="cl"> 0   hdd 0.01949         osd.0       up  1.00000 1.00000 
</span></span><span class="line"><span class="cl"> 7   hdd 0.01949         osd.7       up  1.00000 1.00000 
</span></span><span class="line"><span class="cl">-5       0.01959     host stor02                         
</span></span><span class="line"><span class="cl"> 1   hdd 0.00980         osd.1       up  1.00000 1.00000 
</span></span><span class="line"><span class="cl"> 6   hdd 0.00980         osd.6       up  1.00000 1.00000 
</span></span><span class="line"><span class="cl">-7       0.01959     host stor03                         
</span></span><span class="line"><span class="cl"> 2   hdd 0.00980         osd.2       up  1.00000 1.00000 
</span></span><span class="line"><span class="cl"> 5   hdd 0.00980         osd.5       up  1.00000 1.00000 
</span></span><span class="line"><span class="cl">-9       0.01959     host stor04                         
</span></span><span class="line"><span class="cl"> 3   hdd 0.00980         osd.3       up  1.00000 1.00000 
</span></span><span class="line"><span class="cl"> 4   hdd 0.00980         osd.4       up  1.00000 1.00000 
</span></span></code></pre></td></tr></table>
</div>
</div><p>集群中存在多个Mon主机时，应该在启动集群之后读取或写入数据之前检查Mon的种裁状态：事实上，管理员也应该定期检查这种仲裁结果。</p>
<ul>
<li>显示监视器映射：<code>ceph mon stat</code>命令或者<code>ceph mon dump</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph mon stat
</span></span><span class="line"><span class="cl">e3: 3 mons at {stor01=10.0.0.4:6789/0,stor02=10.0.0.5:6789/0,stor03=10.0.0.6:6789/0}, election epoch 20, leader 0 stor01, quorum 0,1,2 stor01,stor02,stor03
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>显示伸裁状态：<code>ceph quorum status</code></li>
</ul>
<h4 id="使用管理套接字">使用管理套接字</h4>
<p>每一个socket文件能够用来直接通过它管理对应的sock背后的守护进程。</p>
<p>Ceph的管理套接字接口常用于查询守护进程。</p>
<ul>
<li>套接字默认保存于<code>/var/run/ceph</code>目录</li>
<li>此接口的使用不能以远程方式进程</li>
</ul>
<p>命令的使用格式</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph --admin-daemon /var/run/ceph/<span class="o">{</span>socket-name<span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>获取使用帮助：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ceph --admin-daemon /var/run/ceph/{socket-name}
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="停止或重启ceph集群">停止或重启Ceph集群</h4>
<h5 id="停止">停止</h5>
<ul>
<li>告知Ceph集群不要将osd标记为out，命令<code>ceph osd set noout</code></li>
<li>按如下顺序停止守护进程和节点
<ul>
<li>存储客户端</li>
<li>网关，例如NFS Ganesha或对象网关</li>
<li>元数据服务器</li>
<li>Ceph OSD</li>
<li>Ceph Manager</li>
<li>Ceph Monitor</li>
</ul>
</li>
</ul>
<h5 id="启动">启动</h5>
<ul>
<li>以与停止过程相反的顺序启动节点</li>
<li>Ceph Monitor</li>
<li>Ceph Manager</li>
<li>Ceph OSD</li>
<li>元数据服务器</li>
<li>网关，例如NFS Ganesha或对象网关</li>
<li>存储客户端</li>
<li>删除noout标志，命令<code>ceph osd unset noout</code></li>
</ul>
<h4 id="ceph的配置文件">Ceph的配置文件</h4>
<h5 id="配置文件结构">配置文件结构</h5>
<ul>
<li>ceph配置文件使用ini语法格式</li>
<li>ceph在启动时会依次查找多个不同位置的配置文件，如后找的配置文件与前面发生冲突，会覆盖此前的配置信息</li>
<li>注释可通过&quot;#&quot;,&quot;;&quot;</li>
<li>配置文件主要有以下几个配置项所组成
<ul>
<li><code>[global]</code>:全局配置,影响ceph存储集群中的所有守护进程</li>
<li><code>[osd]</code>: 影响Ceph存储集群中的所有ceph-osd守护进程并覆盖全局中的相同设置</li>
<li><code>[mon]</code>: 影响ceph存储集群中的所有ceph-mon守护进程并覆盖全局中的相同设置</li>
<li><code>[client]</code>: 影响所有客户端，例如，挂载ceph块设备，ceph对象网关等</li>
</ul>
</li>
</ul>
<p>每一个独立的配置项是对所有选项生效的，如<code>[mon]</code>，如有需要对单独的选项进行配置可以使用<code>[mon.id]</code>加上id进行标识。</p>
<ul>
<li>
<p>您可以通过输入由<code>.</code>分隔的类型来指定守护程序的特定实例的配置，您可以指定该实例。 并通过实例ID</p>
</li>
<li>
<p>ceph osd守护进程的实例id总是数字，但它可能是<code>ceph monitors</code>的字母数字</p>
<ul>
<li>例如<code>[mon.a]</code>、<code>[mon.b]</code>、<code>[mon.0]</code>等</li>
</ul>
</li>
<li>
<p>按顺序包含的默认ceph配置文件位置</p>
</li>
<li>
<p>$CEPH_CONF环境变量指定的文件路径路径</p>
</li>
<li>
<p><code>-c</code> /path/ceph.conf 使用<code>-c</code>的命令行选项传递给ceph各应用程序或守护进程的命令行选项</p>
</li>
<li>
<p><code>/etc/ceph/ceph.conf</code></p>
</li>
<li>
<p><code>~/.ceph/config</code></p>
</li>
<li>
<p><code>./ceph.conf</code> 用户当前工作目录</p>
</li>
</ul>
<p>在配置文件配置时，还可以使用元变量来引用配置文件中的其他信息或引用ceph集群中的元数据信息做变量替换的。称作元参数或元变量</p>
<blockquote>
<p>常用的元参数</p>
</blockquote>
<ul>
<li><code>cluster</code>: 当前Ceph集群的名称</li>
<li><code>$type</code>: 当前服务的类型名称，可能会展开为OSD或mon</li>
<li><code>$id</code>: 进程的标识符，例如对osd.0来说，其标识符为0</li>
<li><code>$host</code>：守护进程所在的主机的主机名</li>
<li><code>$name</code>: 其值为<code>$type.$id</code></li>
</ul>
<p>进程的运行时配置</p>
<p>在进程的运行当中，设定<code>osd</code>、<code>mon</code>、<code>mgr</code>等工作特性。</p>
<p>要查看运行时配置，请登录Ceph节点并执行：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ceph daemon {daemon-type}.{id} config show
</span></span></code></pre></td></tr></table>
</div>
</div><p>获取帮助信息</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph daemon <span class="o">{</span>daemon-type<span class="o">}</span>.<span class="o">{</span>id<span class="o">}</span> <span class="nb">help</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>在运行时获取特定配置设置</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph daemon <span class="o">{</span>daemon-type<span class="o">}</span>.<span class="o">{</span>id<span class="o">}</span> config get <span class="o">{</span>parameter<span class="o">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 例如：</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ceph daemon osd.0 config get public_addr
</span></span></code></pre></td></tr></table>
</div>
</div><p>在运行时设置特定配置</p>
<p>设置运行时配置有两种常用方法：</p>
<ul>
<li>使用Ceph mmonitor
<ul>
<li><code>ceph tell {daemon-type}.{daemon id or *} injectargs --{name} {value} [--{name}} {value}]</code></li>
<li>例如：<code>ceph tell osd.0 injectargs '--debug-osd 0/5'</code></li>
</ul>
</li>
<li>使用 administration socket
<ul>
<li><code>ceph daemon {daemon-type}.{id} set {name} {type}</code></li>
<li>例如：<code>ceph osd.0 config set debug_osd 0/5</code></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph重新平衡 - Rebalance</title>
      <link>https://www.oomkill.com/2023/09/6-1-ceph-rebalance/</link>
      <pubDate>Sun, 03 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/09/6-1-ceph-rebalance/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="rebalance">Rebalance</h2>
<p>当 Ceph 集群在扩容/缩容后，Ceph会更新 Cluster map, 在更新时会更新 Cluster map 也会更新 “对象的放置” CRUSH 会平均但随机的将对象放置在现有的 OSD 之上，在 Rebalancing 时，只有少量数据进行移动而不是全部数据进行移动，直到达到 OSD 与 对象 之间的平衡，这个过程就叫做 Ceph 的 Rebalance。</p>
<p>需要注意的是，当集群中的 OSD 数量越多，那么在做 Rebalance 时所移动的就越少。例如，在具有 50 个 OSD 的集群中，在添加 OSD 时可能会移动 1/50th 或 2% 的数据。</p>
<p>如下图所示，当前集群有两个 OSD，当在集群中添加一个 OSD，使其数量达到3时，这个时候会触发 Rebalance，所移动的数量为 OSD1 上的 PG3 与 OSD2 上的 PG 6和9</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230830233602421.png" alt="image-20230830233602421"  /></p>
<center>图：Ceph Rebalancing 示意图 </center>
<center><em>Source：</em>https://access.redhat.com/documentation/zh-cn/red_hat_ceph_storage/4/html/architecture_guide/ceph-rebalancing-and-recovery_arch</center><br>
<h3 id="balancer">Balancer</h3>
<p>执行 Rebalance 的模块时 Balancer，其可以优化 OSD 上的放置组 (PG) ，以实现平衡分配。</p>
<p>可以通过命令查看 balancer 的状态</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph balancer status
</span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://docs.ceph.com/en/latest/rados/operations/balancer/">https://docs.ceph.com/en/latest/rados/operations/balancer/</a></p>
<h2 id="backfill">Backfill</h2>
<p>Ceph 回填 (Backfill) 指的是每当删除 OSD 时，Ceph 都会使用 “Backfill” 和 “recovery” 来重新 rebalance 存储集群。这样做是为了根据PG 策略保留数据的多个副本。这两个操作都会占用系统资源，因此当 Ceph 存储集群处于负载状态时，Ceph 的性能将会下降，因为 Ceph 将资源转移到 “回填” 和 “恢复” 过程。</p>
<p>有时为了在删除 OSD 时保持 Ceph 存储可接受的性能，需要先降低 “Backfill” 和 “recovery” 操作的优先级。降低优先级的代价是，较长时间内的数据副本较少，这将会导致数据面临风险。</p>
<p>回填和恢复的发生是发生在 OSD/节点 故障或新增时被触发，如果所有的回填同时发生，会对OSD带来很大的负载，这个现象叫做 ”雷群效应“ (&ldquo;thundering herd&rdquo; effect)</p>
<h3 id="configration-backfill-paramter">Configration backfill paramter</h3>
<p>回填的参数通常位于 OSD 参数下，在 Ceph OSD 中关于 backfill <sup><a href="#1">[1]</a></sup> 的参数如下：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>osd_max_backfills</td>
<td>uint</td>
<td>1</td>
<td>允许回填到单个 OSD 或从单个 OSD 回填的最大数量。请注意，这对于读和写操作是分开应用的。</td>
</tr>
<tr>
<td>osd_backfill_scan_min</td>
<td>int</td>
<td>64</td>
<td>每次回填扫描的最小对象数</td>
</tr>
<tr>
<td>osd_backfill_scan_max</td>
<td>int</td>
<td>512</td>
<td>每次回填扫描的最大对象数</td>
</tr>
<tr>
<td>osd_backfill_retry_interval</td>
<td>float</td>
<td>30.0</td>
<td>重试回填请求之前等待的秒数。</td>
</tr>
</tbody>
</table>
<p><strong>查看当前参数</strong></p>
<p>查看配置之前需要确定 OSD 所在的节点，例如 OSD.1 可以通过 <code>ceph osd tree</code> 获取所有 OSD 列表</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph osd tree
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ID  CLASS  WEIGHT    TYPE NAME       STATUS  REWEIGHT  PRI-AFF
</span></span><span class="line"><span class="cl">-1         13.09845  root default                             
</span></span><span class="line"><span class="cl">-3          4.36615      host PMX1                            
</span></span><span class="line"><span class="cl"> <span class="m">0</span>   nvme   0.72769          osd.0       up   1.00000  1.00000
</span></span><span class="line"><span class="cl"> <span class="m">1</span>   nvme   0.72769          osd.1       up   1.00000  1.00000
</span></span><span class="line"><span class="cl"> <span class="m">2</span>   nvme   0.72769          osd.2       up   1.00000  1.00000
</span></span><span class="line"><span class="cl"> <span class="m">3</span>   nvme   0.72769          osd.3       up   1.00000  1.00000
</span></span><span class="line"><span class="cl"> <span class="m">4</span>   nvme   0.72769          osd.4       up   1.00000  1.00000
</span></span><span class="line"><span class="cl"> <span class="m">5</span>   nvme   0.72769          osd.5       up   1.00000  1.00000
</span></span><span class="line"><span class="cl">-5          4.36615      host PMX2                            
</span></span><span class="line"><span class="cl"> <span class="m">6</span>   nvme   0.72769          osd.6       up   1.00000  1.00000
</span></span><span class="line"><span class="cl"> <span class="m">7</span>   nvme   0.72769          osd.7       up   1.00000  1.00000
</span></span><span class="line"><span class="cl"> <span class="m">8</span>   nvme   0.72769          osd.8       up   1.00000  1.00000
</span></span><span class="line"><span class="cl"> <span class="m">9</span>   nvme   0.72769          osd.9       up   1.00000  1.00000
</span></span><span class="line"><span class="cl"><span class="m">10</span>   nvme   0.72769          osd.10      up   1.00000  1.00000
</span></span><span class="line"><span class="cl"><span class="m">11</span>   nvme   0.72769          osd.11      up   1.00000  1.00000
</span></span><span class="line"><span class="cl">-7          4.36615      host PMX3                            
</span></span><span class="line"><span class="cl"><span class="m">12</span>   nvme   0.72769          osd.12      up   1.00000  1.00000
</span></span><span class="line"><span class="cl"><span class="m">13</span>   nvme   0.72769          osd.13      up   1.00000  1.00000
</span></span><span class="line"><span class="cl"><span class="m">14</span>   nvme   0.72769          osd.14      up   1.00000  1.00000
</span></span><span class="line"><span class="cl"><span class="m">15</span>   nvme   0.72769          osd.15      up   1.00000  1.00000
</span></span><span class="line"><span class="cl"><span class="m">16</span>   nvme   0.72769          osd.16      up   1.00000  1.00000
</span></span><span class="line"><span class="cl"><span class="m">17</span>   nvme   0.72769          osd.17      up   1.00000  1.00000
</span></span></code></pre></td></tr></table>
</div>
</div><p>在拿到 OSD 坐在节点可以通过下面命令查看对应的 OSD 配置</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph daemon osd.1 config get osd_max_backfills
</span></span><span class="line"><span class="cl"><span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;osd_max_backfills&#34;</span>: <span class="s2">&#34;1&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span><span class="line"><span class="cl">$  ceph daemon osd.1 config get osd_recovery_max_active
</span></span><span class="line"><span class="cl"><span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;osd_recovery_max_active&#34;</span>: <span class="s2">&#34;0&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>接下来可以根据 OSD 类型(SSD, HDD, nvme) 的不同，来相应的调整，例如 NVMes 比 HDD 更好的性能，那么可以设置大的回填</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph config show osd.0 osd_recovery_max_active
</span></span><span class="line"><span class="cl">ceph config <span class="nb">set</span> osd osd_max_backfills <span class="m">16</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="recovery">Recovery</h2>
<p>如果 Ceph OSD 守护进程崩溃并重新上线，通常这个OSD会与 PG 中包含更新版本对象的其他 Ceph OSD 守护进程不同步。发生这种情况时，Ceph OSD 守护进程会进入恢复模式 (Recovery)，并寻求获取最新的数据副本并使其映射恢复到最新状态。根据 Ceph OSD daemon 关闭的时间长短，OSD 的对象和 PG 可能会明显过时。此外，如果一个故障域（机架）发生故障，多个 Ceph OSD 守护进程可能会同时恢复在线状态。这会使恢复过程耗时且占用资源。</p>
<p>为了维持操作性能，Ceph 在执行恢复时限制恢复请求数量、线程和对象块大小，这使得 Ceph 在降级状态下也能良好运行。</p>
<p>恢复的参数通常位于 OSD 参数下，在 Ceph OSD 中关于 recovery <sup><a href="#2">[2]</a></sup> 的参数如下：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>类型</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>osd_recovery_delay_start</td>
<td>float</td>
<td>0.0</td>
<td>peer互连完成后，Ceph 将延迟指定的秒数，然后再开始恢复 RADOS 对象。</td>
</tr>
<tr>
<td>osd_recovery_max_active</td>
<td>uint</td>
<td>0</td>
<td>每个 OSD 一次的活动恢复请求数。更多请求将加速恢复，但请求会增加集群的负载</td>
</tr>
<tr>
<td>osd_recovery_max_active_hdd</td>
<td>uint</td>
<td>3</td>
<td>如果主设备是旋转设备（HDD），则每个 OSD 一次的活动恢复请求数。</td>
</tr>
<tr>
<td>osd_recovery_max_active_ssd</td>
<td>uint</td>
<td>10</td>
<td>如果主设备是非旋转设备（即 SSD），则每个 OSD 一次的活动恢复请求数。</td>
</tr>
<tr>
<td>osd_recovery_max_chunk</td>
<td>size</td>
<td>8Mi</td>
<td>恢复操作可以携带的数据块的最大总大小，需要注意单位。</td>
</tr>
<tr>
<td>osd_recovery_max_single_start</td>
<td>uint</td>
<td>1</td>
<td>当 OSD (daemon)恢复时，每个 OSD 新启动的恢复操作的最大数量。</td>
</tr>
<tr>
<td>osd_recovery_sleep</td>
<td>float</td>
<td>0.0</td>
<td>在下一次“恢复”或“回填”操作之前休眠的时间（以秒为单位）。增加此值将减慢恢复操作，而客户端操作受影响较小。</td>
</tr>
<tr>
<td>osd_recovery_sleep_hdd</td>
<td>float</td>
<td>0.1</td>
<td>HDD 下次恢复或回填操作之前的睡眠时间（以秒为单位）。</td>
</tr>
<tr>
<td>osd_recovery_sleep_ssd</td>
<td>float</td>
<td>0.0</td>
<td>SSD 下一次恢复或回填操作之前的睡眠时间（以秒为单位）。</td>
</tr>
<tr>
<td>osd_recovery_sleep_hybrid</td>
<td>float</td>
<td>0.025</td>
<td>当 OSD 数据位于 HDD 上并且 OSD 日志/WAL+DB 位于 SSD 上时，在下一次恢复或回填操作之前休眠的时间（以秒为单位）。</td>
</tr>
<tr>
<td>osd_recovery_priority</td>
<td>uint</td>
<td>5</td>
<td>为恢复工作队列设置的默认优先级。与 Pool 无关</td>
</tr>
</tbody>
</table>
<blockquote>
<p>Ceph backfill 和 recovery 也可以在 Ceph dashboard 中进行配置</p>
</blockquote>
<h3 id="异步恢复">异步恢复</h3>
<p>在 Nautilus 版本之前 “恢复” 动作是同步的，同步最显著的一个特征就是 “同步时会阻止对 RADOS 对象的写入，直到恢复为止”。</p>
<p>回填操作与恢复操作有些不同，回填会临时分配不同的活动集(Active set, PG的一个属性)，并回填活动集之外的 OSD 来允许继续写入</p>
<p>而为了避免 “同步恢复” 的问题 Ceph 提供了一种可以异步恢复的配置，当异步恢复发生时，对活动集成员可继续写入，有关于更多的异步说明，可以参考 Ceph 文档 asynchronous recovery 部分</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://docs.ceph.com/en/reef/rados/configuration/osd-config-ref/#backfilling"><em><strong>backfilling</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://docs.ceph.com/en/latest/rados/configuration/osd-config-ref/#recovery"><em><strong>recovery</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://docs.ceph.com/en/reef/dev/osd_internals/async_recovery/#asynchronous-recovery"><em><strong>ASYNCHRONOUS RECOVERY</strong></em></a></p>
<p><sup id="4">[4]</sup> <a href="https://www.ecstuff4u.com/2021/04/advantages-and-disadvantages-of-san.html"><em><strong>Advantages and Disadvantages of SAN</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>存储概念 - 存储类型对比</title>
      <link>https://www.oomkill.com/2023/08/acquaintance-stroage/</link>
      <pubDate>Tue, 15 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/08/acquaintance-stroage/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="存储选择需要考虑的问题不同的文件访问方式">存储选择需要考虑的问题：不同的文件访问方式?</h2>
<p>在关注存储之前，需要关注下面一些问题：</p>
<p>“应用” 访问数据的方式是什么？</p>
<ul>
<li>一次读取 或 分块读取
<ul>
<li>一个连续的“流”传输最好的方式是什么</li>
</ul>
</li>
<li>有序的 或 随机的</li>
</ul>
<p>“数据的类型是什么”？</p>
<ul>
<li>数据库，Text，视频/音频，图像&hellip;</li>
<li>静态 / 固定 / 动态</li>
</ul>
<p>是否需要数据共享？</p>
<ul>
<li>由应用共享 / 由存储共享</li>
<li>读 / 写</li>
</ul>
<p>共享方面关注的问题？</p>
<ul>
<li>Narrow (只需要更新部分内容，这可以共享特定部分内容，这将不是一个广泛共享) / Broad</li>
</ul>
<p>安全和访问控制：</p>
<ul>
<li>应用什么级别的的安全性？</li>
</ul>
<p>访问性会影响存储的选择：</p>
<ul>
<li>Local / Network</li>
<li>介质：光纤，以太网，SAS，SATA，PCIe&hellip;</li>
</ul>
<p>有了这些问题，就可以引入存储的类型，以便选择最佳的存储（Balance performance and cost ）</p>
<h2 id="das">DAS</h2>
<p><strong>D</strong>irect <strong>A</strong>ttached <strong>S</strong>torage (DAS) 直接附加存储是指，直接连接到服务器存储系统，通俗来讲就是直接连接磁盘，服务器与存储系统之间“<strong>没有经过网络设备</strong>” (如交换机等)，服务器与存储直接由专用的“连接技术”进行连接，如 SCSI, 但现在更常见的是 “eSATA”,  “SAS”, 或 “光纤通道”。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230828210854432.png" alt="image-20230828210854432"  /></p>
<center>图：DAS结构图</center>
<center><em>Source：</em>https://www.pcmag.com/encyclopedia/term/direct-attached-storage</center><br>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/festplattenanschluss-ide-sata-scsi-sas.jpg" alt="festplattenanschluss-ide-sata-scsi-sas.jpg"  /></p>
<center>图：DAS接口类型</center>
<center><em>Source：</em>https://ramsaihan.wordpress.com/2017/10/16/the-sas-sata-scsi-and-ata-in-storage-and-peripheral-communication/</center><br>
<h3 id="外部连接">外部连接</h3>
<p>直连存储也可以通过连接电缆从服务器连接到存储设备，但服务器中必须存在 SAS、以太网或 FC 控制器，只有该服务器可以使用外部磁盘空间。因此直连存储也可以作为是服务器的<strong>扩展</strong></p>
<p>SAS 作为连接介质价格低廉，但距离仅限于几米（最大 5 或 10 米，具体取决于制造商）；光纤通道的传输距离可达数公里，因此也可用作灾备系统。</p>
<p>许多 DAS 系统都有一个内部 RAID 控制器，这些 RAID 将作为一个逻辑磁盘呈现给服务器。这样，用小的物理磁盘也可以生产出大的逻辑硬盘。RAID 控制器的另一个优点是与单个磁盘相比，RAID 5 和 RAID 10 中的 I/O 吞吐量有所增加。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230828211604698.png" alt="image-20230828211604698"  /></p>
<center>图：外部的DAS</center>
<center><em>Source：</em>https://www.storitback.de/service/direct-attached-storage.html</center><br>
<h3 id="das的优缺点">DAS的优缺点</h3>
<p>优点：</p>
<ul>
<li>高可用</li>
<li>更好的数据安全与容错</li>
<li>存储容量扩展更经济</li>
<li>消除网络设置的复杂性</li>
<li>易于管理</li>
</ul>
<p>缺点：</p>
<ul>
<li>有限的分享</li>
<li>利用率差</li>
<li>仅允许有限的使用者(服务器)</li>
<li>单设备有限的接口</li>
</ul>
<h3 id="表现形式">表现形式</h3>
<ul>
<li>在操作系统方面表现为一个裸磁盘(块设备)，如 /dev/sda</li>
<li>在硬件方向表现为一个硬盘设备，如：SSD, HDD, M.2&hellip;</li>
</ul>
<h2 id="nas">NAS</h2>
<p><strong>N</strong>etwork <strong>A</strong>ttached <strong>S</strong>torage (NAS) 网络附加存储是连接到网络的专用文件服务器。NAS 使用以太网和 TCP/IP 等协议，使得 NAS 能够摆脱 SCSI 技术的限制。在表现方面 NAS 是作为一个“网络节点”，也会存在一些 NAS 产品（例如 Network Appliance Filer 和 Auspex 服务器）作为存储设备。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230828225829079.png" alt="image-20230828225829079"  /></p>
<center>图：NAS的表现方式</center>
<center><em>Source：</em>https://dreamlog.tistory.com/565</center><br>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230828230124439.png" alt="image-20230828230124439"  /></p>
<center>图：NAS 与 DAS 的对比</center>
<center><em>Source：</em>https://www.pcmag.com/encyclopedia/term/direct-attached-storage</center><br>
<h3 id="nas-的优缺点">NAS 的优缺点</h3>
<p>优点：</p>
<ul>
<li>对端口没有限制</li>
<li>高度的可扩展性和灵活性</li>
<li>便于安装和维护</li>
<li>多协议</li>
</ul>
<p>缺点：</p>
<ul>
<li>大规模场景下性能会下降（性能取决于协议）</li>
<li>面向文件（文件系统）</li>
<li>因依赖网络，传输速率比 DAS 慢</li>
<li>NAS 由于“共享文件系统”，在安全性的地方会存在弊端</li>
<li>备份和恢复期间会造成网络拥塞</li>
</ul>
<h3 id="表现形式-1">表现形式</h3>
<ul>
<li>在操作系统层面表现为“存储中的一个目录”，如 /data</li>
<li>协议类型：NFS, CIFS</li>
<li>典型产品：
<ul>
<li>NFS</li>
<li>Samba</li>
<li>GlusterFS</li>
<li>CephFS</li>
<li>公有云：EFS(AWS), CFS(tencent), NAS(Aliyun)</li>
</ul>
</li>
</ul>
<h3 id="san">SAN</h3>
<p><strong>S</strong>torage <strong>A</strong>rea <strong>N</strong>etwork (SAN) ，是一种附加远程存储的架构，使其看起来像是本地的，多用于数据中心的存储解决方案，SAN 将存储作为了可通过网络访问的单独设备，该方案融合了 DAS 和 NAS 的灵活性，也增加了配置的复杂性，通常使用与 DAS 类型的协议(SCSI, ISCSI, Fiber Channel) 连接。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230828232107818.png" alt="image-20230828232107818"  /></p>
<center>图：SAN存储结构图</center>
<center><em>Source：</em>https://www.pcmag.com/encyclopedia/term/direct-attached-storage</center><br>
<h3 id="san-的优缺点">SAN 的优缺点</h3>
<p>优点：</p>
<ul>
<li>可以存储大量数据</li>
<li>中心化管理</li>
<li>高度容错</li>
<li>支持动态扩展</li>
<li>快速高效的备份和恢复</li>
</ul>
<p>缺点：</p>
<ul>
<li>硬件成本高(FC 交换机, FC 网络接口, HBA)，价格昂贵</li>
<li>因客户端是多个共享，安全性不好</li>
<li>维护困难</li>
<li>SAN 不适合数据密集型传输，更适用与低流量</li>
<li>依赖高速网络</li>
</ul>
<h3 id="表现形式-2">表现形式</h3>
<ul>
<li>在操作系统方面表现为一个裸磁盘(块设备)，如 /dev/sda</li>
<li>在硬件方向表现为一个磁盘阵列服务器</li>
</ul>
<h2 id="das-vs-nas-vs-san">DAS vs NAS vs SAN</h2>
<p>三种存储类型在架构图上表示如下图所示：</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230828232936991.png" alt="image-20230828232936991"  /></p>
<center>图：存储类型的架构对比</center>
<center><em>Source：</em>https://dreamlog.tistory.com/565</center><br>
<p>对比 DAS, NAS 和 SAN：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>DAS</th>
<th>NAS</th>
<th>SAN</th>
</tr>
</thead>
<tbody>
<tr>
<td>组成</td>
<td>主机 (PC/Server)+存储设备（硬盘）</td>
<td>主机 (PC/Server) <br>文件系统服务 <br>存储设备</td>
<td>磁盘阵列服务器<br/>光纤交换机 <br/>存储设备</td>
</tr>
<tr>
<td>传输介质</td>
<td>电缆(IDE, SATA..)</td>
<td>以太网</td>
<td>光纤 + 光纤交换机 + 电缆 (IDE, SATA)</td>
</tr>
<tr>
<td>类型</td>
<td>块设备</td>
<td>文件系统</td>
<td>块设备</td>
</tr>
<tr>
<td>应用规模</td>
<td>小规模</td>
<td>中</td>
<td>大</td>
</tr>
<tr>
<td>传输速度</td>
<td>介质通道速度</td>
<td>LAN + 介质通道速度双冲因素</td>
<td>通道速度</td>
</tr>
</tbody>
</table>
<h2 id="nas-vs-san">NAS vs SAN</h2>
<p>在通常情况下 NAS与 SAN 常常会进行对比，主要区别在于 SAN 是通道附加的，而 NAS 是网络附加的。</p>
<p>传输协议方面，SAN 与 NAS 技术相似但又不同。SAN 传统上采用低级网络协议来传输磁盘块 NAS 设备通常通过 TCP/IP 传输文件</p>
<p>设备表现方面：NAS 是对数据文件进行操作的单个存储设备，而 SAN 是对磁盘块进行操作的多个设备的本地网络。</p>
<p>网络依赖方面：SAN 通常使用 iSCSI 和光纤通道互连。NAS 通常建立以太网和 TCP/IP 连接。</p>
<p>性能方面：</p>
<ul>
<li>由于文件系统层速度较慢，NAS 通常具有较低的吞吐量和较高的延迟，但高速网络可以弥补 NAS 的性能损失。</li>
<li>SAN 是适用于事务数据库和电子商务网站等高流量环境的高性能设备。FC 或 NVMe 等技术有助于有效地消除此类请求。</li>
</ul>
<p>扩展性方面：NAS可以是 DAS构成的主机，也可以是一个网络磁盘阵列服务器，通常扩展性不是很高；SAN 。其网络架构允许管理员在纵向或横向配置中扩展性能和容量</p>
<p>传输协议方面：</p>
<ul>
<li>NAS 通过以太网交换机的网线直接连接到以太网。NAS 可以使用多种协议连接到服务器，包括 NFS、SMB/CIFS 和 HTTP。</li>
<li>SAN 使用 SCSI 协议或 SAN 驱动设备进行通信。网络是通过使用 SAS /SATA 连接类型或通过将层映射到其他协议来形成的，例如 FC 协议 (FCP) 协议映射 SCSI over FC 或 iSCSI映射 SCSI over TCP/IP 。</li>
</ul>
<h2 id="block-storage">Block Storage</h2>
<p>块存储是一种允许对低级存储设备进行抽象的技术，主要优点是提供<strong>低延迟操作</strong>。块设备视通常为常规磁盘（底层可以是 DAS 或 SAN），在操作系统层面会将其检测为原始磁盘。然后，对其进行格式化以在其上创建文件系统（ext4、XFS、NTFS&hellip;）并开始将其用作可以存储数据的常规设备。</p>
<p>块设备由集群作为较小块的集合进行管理（称为块或简称为块，因此得名）。这些块中的每一个都可以存储在由多台机器组成的存储集群中并存储在唯一的地址下。</p>
<p>块存储由 1 和 0 组成；没有用于跟踪和可视化数据的文件系统或元数据；操作系统必须处理所有块的读/写。此选项的优点是吞吐量性能、低延迟和高 IOPS。通常，块最适合云平台基础设施（管理程序）和数据库，因为它具有高性能的趋势。</p>
<h2 id="file-system">FIle System</h2>
<p>与块存储不同，基于文件的存储（NAS、文件系统）隐藏了与块存储相关的大部分复杂性。NAS 只是在网络上显示为驱动器号或目录，文件系统在存储和管理文件方面表现出色，而块存储缺乏更高级别的数据结构因为它能够轻松地通过网络共享文件，并且具有扩展能力。</p>
<h2 id="object-storage">Object Storage</h2>
<p>对象存储是在云计算行业为了存储大量非<strong>结构化数据</strong>的需求而创建的。数据不使用文件路径，而对象及其元数据的访问是使用标准 HTTP API 完成的</p>
<h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://dreamlog.tistory.com/565"><strong>스토리지 구성 DAS, NAS, SAN, IP-SAN</strong></a></p>
<p><sup id="2">[2]</sup> <a href="https://www.thegioimaychu.vn/blog/giai-phap/nas-vs-san-su-khac-biet-va-cac-truong-hop-su-dung-p1151/"><strong>NAS vs SAN storage: Sự khác biệt và các ứng dụng phù hợp</strong></a></p>
<p><sup id="3">[3]</sup> <a href="https://www.pcmag.com/encyclopedia/term/direct-attached-storage"><strong>direct attached storage</strong></a></p>
<p><sup id="4">[4]</sup> <a href="https://www.ecstuff4u.com/2021/04/advantages-and-disadvantages-of-san.html"><strong>Advantages and Disadvantages of SAN</strong></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>使用cephadm纯离线安装Ceph集群</title>
      <link>https://www.oomkill.com/2023/07/02-1-install-ceph-with-cephadm/</link>
      <pubDate>Sun, 30 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2023/07/02-1-install-ceph-with-cephadm/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="开篇常例---概述">开篇常例 - 概述</h2>
<p>Ceph 是一个广泛使用的开源存储平台。  它提供高性能、可靠性和可扩展性。  Ceph 分布式存储系统提供了对象存储、块存储和文件级存储。  Ceph 旨在提供无单点故障的分布式存储系统。</p>
<p>在本教程中，将通过 ceph-adm 方式在 CentOS 7 上安装和构建 Ceph 集群。该实验的 Ceph 集群需要以下 Ceph 组件：</p>
<ul>
<li><strong>Ceph OSD (ceph-osd)</strong>  -  处理数据存储、数据复制和恢复；通常一个Ceph集群至少需要两台 OSD 服务器 。</li>
<li><strong>Ceph Monitor (ceph-mon)</strong>  -  监视集群状态、OSD 映射和 CRUSH 映射，我们在这里与 cephadm 或 OSD 公用一个节点</li>
<li><strong>Ceph 元数据服务器 (ceph-mds)</strong>  -  这是使用 CephFS 所需的组件。</li>
</ul>
<p>有了上面的条件，我们实验环境所需要的节点如下：</p>
<ul>
<li>三台服务器节点，CentOS 7</li>
</ul>
<blockquote>
<p>注：CentOS 7 可安装最高级别的 ceph 版本就是 O 版</p>
</blockquote>
<p>本教程中的服务器将使用以下主机名和 IP 地址：</p>
<table>
<thead>
<tr>
<th>主机名</th>
<th>IP地址</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>cephadmin</td>
<td>10.0.0.20</td>
<td>作为 ceph 管理节点，以管理与部署 ceph 集群</td>
</tr>
<tr>
<td>osd01</td>
<td>10.0.0.21</td>
<td></td>
</tr>
<tr>
<td>osd02</td>
<td>10.0.0.22</td>
<td></td>
</tr>
<tr>
<td><em>any</em></td>
<td><em>any</em></td>
<td>作为 Ceph Client 的角色</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注：所有 OSD 节点都需要两个分区，一个根（/）分区和一个空分区，稍后用作 Ceph 数据存储。</p>
</blockquote>
<h2 id="requirements">REQUIREMENTS</h2>
<p>使用 <em>cephadm</em> 安装 ceph 集群，所需要的先决条件如下:</p>
<p>必要条件：</p>
<ul>
<li>Python 3，因为 cephadm 是一个 python3 脚本，所以需要每个节点都需要安装 python3</li>
<li>Systemd</li>
<li>Podman or Docker：cephadm 安装的集群是一种以 “容器方式” 运行在对应的 ceph node 之上</li>
<li>LVM2：ceph OSD 是通过 LVM 来使用的，所以需要在每个 OSD 节点之上安装 LVM2</li>
</ul>
<p>非必要条件：</p>
<ul>
<li>chrony or NTP：ceph 强依赖每个节点之上的时间</li>
<li>Internet</li>
<li>域名解析：ceph 集群对于 ceph node 来说是通过 <code>hostname.random_str</code> 识别的的</li>
</ul>
<h2 id="step-1-配置节点">Step 1 配置节点</h2>
<p>此步骤，将配置所有 3 个节点，为安装 Ceph 集群做好准备。  建议在所有节点上按照并运行以下所有命令。  并确保所有节点上都安装了 ssh-server。</p>
<h3 id="创建ceph用户可选">创建ceph用户(可选)</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">useradd -d /home/cephuser -m cephuser 
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> 1<span class="p">|</span>passwd cephuser --stdin
</span></span></code></pre></td></tr></table>
</div>
</div><p>创建新用户后，我们需要为“cephuser” 配置 sudo。  他必须能够以 root 身份运行命令并无需密码即可获得 root 权限。</p>
<p>运行以下命令为用户创建  sudoers 文件并使用  sed  编辑 /etc/sudoers 文件。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;cephuser ALL = (root) NOPASSWD:ALL&#34;</span> <span class="p">|</span> sudo tee /etc/sudoers.d/cephuser
</span></span><span class="line"><span class="cl">chmod <span class="m">0440</span> /etc/sudoers.d/cephuser
</span></span><span class="line"><span class="cl">sed -i s<span class="s1">&#39;/Defaults requiretty/#Defaults requiretty&#39;</span>/g /etc/sudoers
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Note：上述 通过 ceph-deploy 需要配置，cephadm 中没有强制</p>
</blockquote>
<h3 id="安装配置-ntp-服务可选">安装配置 NTP 服务(可选)</h3>
<p>因为分布式存储需要依赖时间，所以需要对所有 OSD 节点的时间保持一致，这里时间同步的软件可以随意选择，</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">yum install -y ntp ntpdate ntp-doc
</span></span><span class="line"><span class="cl">ntpdate 0.us.pool.ntp.org
</span></span><span class="line"><span class="cl">hwclock --systohc
</span></span><span class="line"><span class="cl">systemctl <span class="nb">enable</span> ntpd.service
</span></span><span class="line"><span class="cl">systemctl start ntpd.service
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>可以不准备，随意启动一个服务即可，否则安装会出现如下提示</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ cephadm bootstrap --mon-ip cephadmin
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Verifying podman<span class="p">|</span>docker is present...
</span></span><span class="line"><span class="cl">Verifying lvm2 is present...
</span></span><span class="line"><span class="cl">Verifying <span class="nb">time</span> synchronization is in place...
</span></span><span class="line"><span class="cl">No <span class="nb">time</span> sync service is running<span class="p">;</span> checked <span class="k">for</span> <span class="o">[</span><span class="s1">&#39;chrony.service&#39;</span>, <span class="s1">&#39;chronyd.service&#39;</span>, <span class="s1">&#39;systemd-timesyncd.service&#39;</span>, <span class="s1">&#39;ntpd.service&#39;</span>, <span class="s1">&#39;ntp.service&#39;</span>, <span class="s1">&#39;ntpsec.service&#39;</span>, <span class="s1">&#39;openntpd.service&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">Installing packages <span class="o">[</span><span class="s1">&#39;chrony&#39;</span><span class="o">]</span>...
</span></span><span class="line"><span class="cl">Enabling unit chronyd.service
</span></span><span class="line"><span class="cl">No <span class="nb">time</span> sync service is running<span class="p">;</span> checked <span class="k">for</span> <span class="o">[</span><span class="s1">&#39;chrony.service&#39;</span>, <span class="s1">&#39;chronyd.service&#39;</span>, <span class="s1">&#39;systemd-timesyncd.service&#39;</span>, <span class="s1">&#39;ntpd.service&#39;</span>, <span class="s1">&#39;ntp.service&#39;</span>, <span class="s1">&#39;ntpsec.service&#39;</span>, <span class="s1">&#39;openntpd.service&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl">Repeating the final host check...
</span></span><span class="line"><span class="cl">docker <span class="o">(</span>/usr/bin/docker<span class="o">)</span> is present
</span></span><span class="line"><span class="cl">systemctl is present
</span></span><span class="line"><span class="cl">lvcreate is present
</span></span><span class="line"><span class="cl">Unit chronyd.service is enabled and running
</span></span><span class="line"><span class="cl">Host looks OK
</span></span><span class="line"><span class="cl">Cluster fsid: 19c90bda-2fb8-11ee-9128-000c293e5d57
</span></span><span class="line"><span class="cl">Address: cephadmin is not a valid IP address
</span></span><span class="line"><span class="cl">Verifying IP cephadmin port <span class="m">3300</span> ...
</span></span><span class="line"><span class="cl">Address: cephadmin is not a valid IP address
</span></span><span class="line"><span class="cl">Verifying IP cephadmin port <span class="m">6789</span> ...
</span></span><span class="line"><span class="cl">Address: cephadmin is not a valid IP address
</span></span><span class="line"><span class="cl">Cannot infer CIDR network <span class="k">for</span> mon IP <span class="sb">`</span>cephadmin<span class="sb">`</span> : <span class="s1">&#39;cephadmin&#39;</span> does not appear to be an IPv4 or IPv6 address
</span></span><span class="line"><span class="cl">Cannot infer CIDR network <span class="k">for</span> mon IP <span class="sb">`</span>cephadmin<span class="sb">`</span> : <span class="s1">&#39;cephadmin&#39;</span> does not appear to be an IPv4 or IPv6 address
</span></span><span class="line"><span class="cl">ERROR: Cannot infer CIDR network. Pass --skip-mon-network to configure it later
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="关闭-selinux">关闭 SELInux</h3>
<p>在所有 Ceph Node 节点上关闭 SELInux，可以根据下面命令使用 sed 操作</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sed -i <span class="s1">&#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39;</span> /etc/selinux/config
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="配置-hosts-文件">配置 Hosts 文件</h3>
<p>这里主机名可以根据自己选择进行，如果你有 DNS 服务，那么也可以通过注册在 DNS 内的服务进行</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ tee &gt;&gt; /etc/hosts <span class="s">&lt;&lt; EOF
</span></span></span><span class="line"><span class="cl"><span class="s">10.0.0.20        ceph-octopus-cephadm
</span></span></span><span class="line"><span class="cl"><span class="s">10.0.0.21        ceph-octopus-01
</span></span></span><span class="line"><span class="cl"><span class="s">10.0.0.22        ceph-octopus-02
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="安装依赖">安装依赖</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># centos 7</span>
</span></span><span class="line"><span class="cl">yum install -y python3 lvm2 docker-ce
</span></span><span class="line"><span class="cl"><span class="c1"># centos 8</span>
</span></span><span class="line"><span class="cl">dnf install -y python3 lvm2 podman
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="step2-下载-cephadm-并-修改-cephadm-镜像地址">Step2 下载 cephadm 并 修改 cephadm 镜像地址</h2>
<h3 id="获取-cephadm-脚本">获取 cephadm 脚本</h3>
<p>步骤参考了 ceph 官方安装手册 <sup><a href="#1">[1]</a></sup> ，需要注意的是 <em>cephadm</em> 脚本也是需要按照版本来的</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl --silent --remote-name --location https://github.com/ceph/ceph/raw/octopus/src/cephadm/cephadm
</span></span><span class="line"><span class="cl">chmod +x cephadm
</span></span></code></pre></td></tr></table>
</div>
</div><p>这个步骤主要是为了使 cephadm 可以正常的拉去 ceph 镜像，你可以通过 <code>docker load</code> 方式导入到 ceph node 之上，但是 ceph 镜像必须通过私有镜像进行拉取（存在 reposig 认证）其他 ceph 组件（prometheus, node-exporter..）可以通过 <code>docker load</code> 导入</p>
<p>cephadm 最上面几行写明了要拉去镜像的镜像仓库地址，可以在有互联网机器上下载好，push 到私有镜像仓库中，如果没有私有镜像仓库，可以 run 一个 docker registry ，这个步骤是强制的；其他组件是可以通过 <code>docker load</code> 方式获得</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="err">$</span> <span class="n">head</span> <span class="o">-</span><span class="mi">20</span> <span class="n">cephadm</span> 
</span></span><span class="line"><span class="cl"><span class="c1">#!/usr/bin/python3</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Default container images -----------------------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="n">DEFAULT_IMAGE</span> <span class="o">=</span> <span class="s1">&#39;quay.io/ceph/ceph:v15&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">DEFAULT_IMAGE_IS_MASTER</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="n">DEFAULT_PROMETHEUS_IMAGE</span> <span class="o">=</span> <span class="s1">&#39;quay.io/prometheus/prometheus:v2.18.1&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">DEFAULT_NODE_EXPORTER_IMAGE</span> <span class="o">=</span> <span class="s1">&#39;quay.io/prometheus/node-exporter:v0.18.1&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">DEFAULT_ALERT_MANAGER_IMAGE</span> <span class="o">=</span> <span class="s1">&#39;quay.io/prometheus/alertmanager:v0.20.0&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">DEFAULT_GRAFANA_IMAGE</span> <span class="o">=</span> <span class="s1">&#39;quay.io/ceph/ceph-grafana:6.7.4&#39;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ------------------------------------------------------------------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">LATEST_STABLE_RELEASE</span> <span class="o">=</span> <span class="s1">&#39;octopus&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">DATA_DIR</span> <span class="o">=</span> <span class="s1">&#39;/var/lib/ceph&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">LOG_DIR</span> <span class="o">=</span> <span class="s1">&#39;/var/log/ceph&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">LOCK_DIR</span> <span class="o">=</span> <span class="s1">&#39;/run/cephadm&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">LOGROTATE_DIR</span> <span class="o">=</span> <span class="s1">&#39;/etc/logrotate.d&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">UNIT_DIR</span> <span class="o">=</span> <span class="s1">&#39;/etc/systemd/system&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">LOG_DIR_MODE</span> <span class="o">=</span> <span class="mo">0o770</span>
</span></span><span class="line"><span class="cl"><span class="n">DATA_DIR_MODE</span> <span class="o">=</span> <span class="mo">0o700</span>
</span></span><span class="line"><span class="cl"><span class="n">CONTAINER_INIT</span><span class="o">=</span><span class="kc">False</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="run-docker-registry">Run docker registry</h3>
<p>拉去 docker registry</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker pull registry
</span></span></code></pre></td></tr></table>
</div>
</div><p>镜像保存路径放置在当前工作目录中</p>
<blockquote>
<p>Note: 如果你没有独立的私有镜像仓库，那么请保留 docker registry，直到你不对 ceph 集群进行扩展</p>
</blockquote>
<p>执行下面命令，运行 docker registry</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker run <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --detach <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --name registry <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --hostname registry <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --volume <span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/registry:/var/lib/registry/docker/registry <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --publish 5000:5000 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  --restart unless-stopped <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>  registry:latest
</span></span></code></pre></td></tr></table>
</div>
</div><p>在所有 ceph node 之上执行下面命令，需要自行替换 <code>registry_host</code> 部分</p>
<blockquote>
<p>现象：https://xxx:5000/v2/: http: server gave HTTP response to HTTPS client</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tee /etc/docker/daemon.json <span class="s">&lt;&lt; EOF
</span></span></span><span class="line"><span class="cl"><span class="s">{
</span></span></span><span class="line"><span class="cl"><span class="s">    &#34;insecure-registries&#34;: [&#34;registry_host:5000&#34;]
</span></span></span><span class="line"><span class="cl"><span class="s">}
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="step-3-引导一个新集群">Step 3 引导一个新集群</h2>
<p>在上面步骤都完成后，可以直接去引导一个新集群了</p>
<p>可以选择性执行下面步骤</p>
<blockquote>
<p>这里是安装 ceph 客户端时需要用到的，例如 ceph-common, ceph-fuse 都会用到这些</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ./cephadm add-repo --release octopus
</span></span></code></pre></td></tr></table>
</div>
</div><p>cephadm 命令能够：</p>
<ul>
<li>引导一个新集群</li>
<li>使用 ceph cli 启动容器化的 shell</li>
<li>用于调试容器化的 ceph daemon</li>
</ul>
<p>O 版的安装命令是通过 github 下载，要注意的是，每个版本号的 cephadm 命令不通用</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm</span>
</span></span><span class="line"><span class="cl"><span class="c1"># chmod +x cephadm</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ./cephadm add-repo --release octopus</span>
</span></span><span class="line"><span class="cl"><span class="c1"># install命令旨在将 cephadm 安装到环境变量中</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ./cephadm install </span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="开始引导一个新的-ceph-集群">开始引导一个新的 ceph 集群</h2>
<p>创建 Ceph 集群的第一步是在 Ceph 集群的管理几点上执行命令 <code>cephadm bootstrap</code>，这个命令的行为会创建 Ceph 集群中的第一个 &ldquo;monitor&rdquo; 守护进程，这需要提供一个 “IP地址” 而不可以是 “域名”。</p>
<p>这里将 ceph monitor 部署在管理节点上了，以节省 Node 数量</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ cephadm bootstrap --mon-ip 10.0.0.20
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Verifying podman<span class="p">|</span>docker is present...
</span></span><span class="line"><span class="cl">Verifying lvm2 is present...
</span></span><span class="line"><span class="cl">Verifying <span class="nb">time</span> synchronization is in place...
</span></span><span class="line"><span class="cl">Unit chronyd.service is enabled and running
</span></span><span class="line"><span class="cl">Repeating the final host check...
</span></span><span class="line"><span class="cl">docker <span class="o">(</span>/usr/bin/docker<span class="o">)</span> is present
</span></span><span class="line"><span class="cl">systemctl is present
</span></span><span class="line"><span class="cl">lvcreate is present
</span></span><span class="line"><span class="cl">Unit chronyd.service is enabled and running
</span></span><span class="line"><span class="cl">Host looks OK
</span></span><span class="line"><span class="cl">Cluster fsid: 420ccab4-2fb8-11ee-9f5c-000c293e5d57
</span></span><span class="line"><span class="cl">Verifying IP 10.0.0.20 port <span class="m">3300</span> ...
</span></span><span class="line"><span class="cl">Verifying IP 10.0.0.20 port <span class="m">6789</span> ...
</span></span><span class="line"><span class="cl">Mon IP <span class="sb">`</span>10.0.0.20<span class="sb">`</span> is in CIDR network <span class="sb">`</span>10.0.0.0/24<span class="sb">`</span>
</span></span><span class="line"><span class="cl">Mon IP <span class="sb">`</span>10.0.0.20<span class="sb">`</span> is in CIDR network <span class="sb">`</span>10.0.0.0/24<span class="sb">`</span>
</span></span><span class="line"><span class="cl">Internal network <span class="o">(</span>--cluster-network<span class="o">)</span> has not been provided, OSD replication will default to the public_network
</span></span><span class="line"><span class="cl">Pulling container image quay.io/ceph/ceph:v17...
</span></span><span class="line"><span class="cl">Non-zero <span class="nb">exit</span> code <span class="m">1</span> from /usr/bin/docker pull quay.io/ceph/ceph:v17
</span></span><span class="line"><span class="cl">/usr/bin/docker: stderr Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
</span></span><span class="line"><span class="cl">ERROR: Failed command: /usr/bin/docker pull quay.io/ceph/ceph:v17
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@cephadmin ~<span class="o">]</span><span class="c1"># systemctl start docker </span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>root@cephadmin ~<span class="o">]</span><span class="c1"># cephadm bootstrap --mon-ip 10.0.0.20</span>
</span></span><span class="line"><span class="cl">Verifying podman<span class="p">|</span>docker is present...
</span></span><span class="line"><span class="cl">Verifying lvm2 is present...
</span></span><span class="line"><span class="cl">Verifying <span class="nb">time</span> synchronization is in place...
</span></span><span class="line"><span class="cl">Unit chronyd.service is enabled and running
</span></span><span class="line"><span class="cl">Repeating the final host check...
</span></span><span class="line"><span class="cl">docker <span class="o">(</span>/usr/bin/docker<span class="o">)</span> is present
</span></span><span class="line"><span class="cl">systemctl is present
</span></span><span class="line"><span class="cl">lvcreate is present
</span></span><span class="line"><span class="cl">Unit chronyd.service is enabled and running
</span></span><span class="line"><span class="cl">Host looks OK
</span></span><span class="line"><span class="cl">Cluster fsid: 4d128cbe-2fb8-11ee-8326-000c293e5d57
</span></span><span class="line"><span class="cl">Verifying IP 10.0.0.20 port <span class="m">3300</span> ...
</span></span><span class="line"><span class="cl">Verifying IP 10.0.0.20 port <span class="m">6789</span> ...
</span></span><span class="line"><span class="cl">Mon IP <span class="sb">`</span>10.0.0.20<span class="sb">`</span> is in CIDR network <span class="sb">`</span>10.0.0.0/24<span class="sb">`</span>
</span></span><span class="line"><span class="cl">Mon IP <span class="sb">`</span>10.0.0.20<span class="sb">`</span> is in CIDR network <span class="sb">`</span>10.0.0.0/24<span class="sb">`</span>
</span></span><span class="line"><span class="cl">Internal network <span class="o">(</span>--cluster-network<span class="o">)</span> has not been provided, OSD replication will default to the public_network
</span></span><span class="line"><span class="cl">Pulling container image quay.io/ceph/ceph:v17...
</span></span><span class="line"><span class="cl">Ceph version: ceph version 17.2.6 <span class="o">(</span>d7ff0d10654d2280e08f1ab989c7cdf3064446a5<span class="o">)</span> quincy <span class="o">(</span>stable<span class="o">)</span>
</span></span><span class="line"><span class="cl">Extracting ceph user uid/gid from container image...
</span></span><span class="line"><span class="cl">Creating initial keys...
</span></span><span class="line"><span class="cl">Creating initial monmap...
</span></span><span class="line"><span class="cl">Creating mon...
</span></span><span class="line"><span class="cl">Waiting <span class="k">for</span> mon to start...
</span></span><span class="line"><span class="cl">Waiting <span class="k">for</span> mon...
</span></span><span class="line"><span class="cl">mon is available
</span></span><span class="line"><span class="cl">Assimilating anything we can from ceph.conf...
</span></span><span class="line"><span class="cl">Generating new minimal ceph.conf...
</span></span><span class="line"><span class="cl">Restarting the monitor...
</span></span><span class="line"><span class="cl">Setting mon public_network to 10.0.0.0/24
</span></span><span class="line"><span class="cl">Wrote config to /etc/ceph/ceph.conf
</span></span><span class="line"><span class="cl">Wrote keyring to /etc/ceph/ceph.client.admin.keyring
</span></span><span class="line"><span class="cl">Creating mgr...
</span></span><span class="line"><span class="cl">Verifying port <span class="m">9283</span> ...
</span></span><span class="line"><span class="cl">Waiting <span class="k">for</span> mgr to start...
</span></span><span class="line"><span class="cl">Waiting <span class="k">for</span> mgr...
</span></span><span class="line"><span class="cl">mgr not available, waiting <span class="o">(</span>1/15<span class="o">)</span>...
</span></span><span class="line"><span class="cl">mgr not available, waiting <span class="o">(</span>2/15<span class="o">)</span>...
</span></span><span class="line"><span class="cl">mgr not available, waiting <span class="o">(</span>3/15<span class="o">)</span>...
</span></span><span class="line"><span class="cl">mgr not available, waiting <span class="o">(</span>4/15<span class="o">)</span>...
</span></span><span class="line"><span class="cl">mgr is available
</span></span><span class="line"><span class="cl">Enabling cephadm module...
</span></span><span class="line"><span class="cl">Waiting <span class="k">for</span> the mgr to restart...
</span></span><span class="line"><span class="cl">Waiting <span class="k">for</span> mgr epoch 5...
</span></span><span class="line"><span class="cl">mgr epoch <span class="m">5</span> is available
</span></span><span class="line"><span class="cl">Setting orchestrator backend to cephadm...
</span></span><span class="line"><span class="cl">Generating ssh key...
</span></span><span class="line"><span class="cl">Wrote public SSH key to /etc/ceph/ceph.pub
</span></span><span class="line"><span class="cl">Adding key to root@localhost authorized_keys...
</span></span><span class="line"><span class="cl">Adding host cephadmin...
</span></span><span class="line"><span class="cl">Deploying mon service with default placement...
</span></span><span class="line"><span class="cl">Deploying mgr service with default placement...
</span></span><span class="line"><span class="cl">Deploying crash service with default placement...
</span></span><span class="line"><span class="cl">Deploying prometheus service with default placement...
</span></span><span class="line"><span class="cl">Deploying grafana service with default placement...
</span></span><span class="line"><span class="cl">Deploying node-exporter service with default placement...
</span></span><span class="line"><span class="cl">Deploying alertmanager service with default placement...
</span></span><span class="line"><span class="cl">Enabling the dashboard module...
</span></span><span class="line"><span class="cl">Waiting <span class="k">for</span> the mgr to restart...
</span></span><span class="line"><span class="cl">Waiting <span class="k">for</span> mgr epoch 9...
</span></span><span class="line"><span class="cl">mgr epoch <span class="m">9</span> is available
</span></span><span class="line"><span class="cl">Generating a dashboard self-signed certificate...
</span></span><span class="line"><span class="cl">Creating initial admin user...
</span></span><span class="line"><span class="cl">Fetching dashboard port number...
</span></span><span class="line"><span class="cl">Ceph Dashboard is now available at:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	     URL: https://cephadmin:8443/
</span></span><span class="line"><span class="cl">	    User: admin
</span></span><span class="line"><span class="cl">	Password: eqlf3jh1i1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Enabling client.admin keyring and conf on hosts with <span class="s2">&#34;admin&#34;</span> label
</span></span><span class="line"><span class="cl">Saving cluster configuration to /var/lib/ceph/4d128cbe-2fb8-11ee-8326-000c293e5d57/config directory
</span></span><span class="line"><span class="cl">Enabling autotune <span class="k">for</span> osd_memory_target
</span></span><span class="line"><span class="cl">You can access the Ceph CLI as following in <span class="k">case</span> of multi-cluster or non-default config:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	sudo /usr/local/bin/cephadm shell --fsid 4d128cbe-2fb8-11ee-8326-000c293e5d57 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Or, <span class="k">if</span> you are only running a single cluster on this host:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	sudo /usr/local/bin/cephadm shell 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Please consider enabling telemetry to <span class="nb">help</span> improve Ceph:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	ceph telemetry on
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">For more information see:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	https://docs.ceph.com/docs/master/mgr/telemetry/
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Bootstrap complete.
</span></span></code></pre></td></tr></table>
</div>
</div><p>成功后会看到 ceph dashboard 的界面，默认密码会输出到控制台，第一次登陆会要求修改默认密码</p>
<p>在安装将生成一个最小的 <code>ceph.conf</code> 仅适用于引导阶段的配置文件，通过进入 mon 容器查看</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ docker <span class="nb">exec</span> -it ceph-350494de-d23f-11ea-be85-525400d32681-mon.mon0 cat /etc/ceph/ceph.conf <span class="c1"># </span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="向-ceph-集群导入-osd-node">向 ceph 集群导入 osd node</h2>
<p>向每个 node 导入 ssh key，下面的操作是通过进入管理容器执行的</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ssh-copy-id -f -i /etc/ceph/ceph.pub root@*&lt;new-host&gt;*
</span></span></code></pre></td></tr></table>
</div>
</div><p>添加一个主机到 ceph 集群</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph orch host add *newhost*
</span></span></code></pre></td></tr></table>
</div>
</div><p>部署一个新的 mon，你可以给新加入的主机打上标签</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># ceph orch host label add *&lt;hostname&gt;* mon</span>
</span></span><span class="line"><span class="cl">ceph orch apply mon *&lt;number-of-monitors&gt;*
</span></span></code></pre></td></tr></table>
</div>
</div><p>向集群部署新的组件</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph orch apply mon *&lt;number-of-monitors&gt;*
</span></span><span class="line"><span class="cl">ceph orch apply mon *&lt;host1,host2,host3,...&gt;*
</span></span></code></pre></td></tr></table>
</div>
</div><p>部署 osd damon 在新的主机之上</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph orch daemon add osd *&lt;host&gt;*:*&lt;device-path&gt;*
</span></span><span class="line"><span class="cl">ceph orch daemon add osd host1:/dev/sdb
</span></span></code></pre></td></tr></table>
</div>
</div><p>这是可以列出正在管理的服务器 <code>cephadm</code> 使用 <code>host ls</code> 命令：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph orch host ls 
</span></span></code></pre></td></tr></table>
</div>
</div><p>到此，如果你只使用 RDB 块存储，这里已经部署完成了，如果需要选择使用 文件存储 CephFS，或者对象存储 RGW，可以在另外部署相应的组件，部署的组件是根据按需使用进行部署</p>
<p>osd device 命令也可以列出对应的设备</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph orch device ls 
</span></span></code></pre></td></tr></table>
</div>
</div><p>在 Ceph 中一切存储的基础都是基于 RADOS 集群</p>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="typeerror-__init__-missing-2-required-positional-arguments-hostname-and-addr">TypeError: <strong>init</strong>() missing 2 required positional arguments: &lsquo;hostname&rsquo; and &lsquo;addr&rsquo;</h3>
<p>现象：实际上输入了 <em>hostname</em> 和 <em>addr</em> 也是出现这个问题</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph orch host add ceph-octopus-01
</span></span><span class="line"><span class="cl">Error EINVAL: Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/share/ceph/mgr/mgr_module.py&#34;</span>, line 1756, in _handle_command
</span></span><span class="line"><span class="cl">    <span class="k">return</span> self.handle_command<span class="o">(</span>inbuf, cmd<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/share/ceph/mgr/orchestrator/_interface.py&#34;</span>, line 171, in handle_command
</span></span><span class="line"><span class="cl">    <span class="k">return</span> dispatch<span class="o">[</span>cmd<span class="o">[</span><span class="s1">&#39;prefix&#39;</span><span class="o">]]</span>.call<span class="o">(</span>self, cmd, inbuf<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/share/ceph/mgr/mgr_module.py&#34;</span>, line 462, in call
</span></span><span class="line"><span class="cl">    <span class="k">return</span> self.func<span class="o">(</span>mgr, **kwargs<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/share/ceph/mgr/orchestrator/_interface.py&#34;</span>, line 107, in &lt;lambda&gt;
</span></span><span class="line"><span class="cl">    <span class="nv">wrapper_copy</span> <span class="o">=</span> lambda *l_args, **l_kwargs: wrapper<span class="o">(</span>*l_args, **l_kwargs<span class="o">)</span>  <span class="c1"># noqa: E731</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/share/ceph/mgr/orchestrator/_interface.py&#34;</span>, line 96, in wrapper
</span></span><span class="line"><span class="cl">    <span class="k">return</span> func<span class="o">(</span>*args, **kwargs<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/share/ceph/mgr/orchestrator/module.py&#34;</span>, line 356, in _add_host
</span></span><span class="line"><span class="cl">    <span class="k">return</span> self._apply_misc<span class="o">([</span>s<span class="o">]</span>, False, Format.plain<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/share/ceph/mgr/orchestrator/module.py&#34;</span>, line 1092, in _apply_misc
</span></span><span class="line"><span class="cl">    raise_if_exception<span class="o">(</span>completion<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/share/ceph/mgr/orchestrator/_interface.py&#34;</span>, line 225, in raise_if_exception
</span></span><span class="line"><span class="cl">    <span class="nv">e</span> <span class="o">=</span> pickle.loads<span class="o">(</span>c.serialized_exception<span class="o">)</span>
</span></span><span class="line"><span class="cl">TypeError: __init__<span class="o">()</span> missing <span class="m">2</span> required positional arguments: <span class="s1">&#39;hostname&#39;</span> and <span class="s1">&#39;addr&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>首先先将公钥分发到对应的 CEPH NODE 之上</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CEPH_HOSTNAME</span><span class="o">=</span>root@ceph-octopus-01
</span></span><span class="line"><span class="cl"><span class="c1"># 获取公钥</span>
</span></span><span class="line"><span class="cl">ceph cephadm get-pub-key &gt; /etc/ceph/ceph.pub
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 分发公钥到对应 ceph node</span>
</span></span><span class="line"><span class="cl">ssh-copy-id -f -i /etc/ceph/ceph.pub <span class="si">${</span><span class="nv">CEPH_HOSTNAME</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 尝试使用私钥是否可以连接到 ceph node</span>
</span></span><span class="line"><span class="cl">ceph cephadm get-ssh-config &gt; ssh_config
</span></span><span class="line"><span class="cl">ceph config-key get mgr/cephadm/ssh_identity_key &gt; ~/cephadm_private_key
</span></span><span class="line"><span class="cl">chmod <span class="m">0600</span> ~/cephadm_private_key
</span></span><span class="line"><span class="cl">ssh -F ssh_config -i ~/cephadm_private_key <span class="si">${</span><span class="nv">CEPH_HOSTNAME</span><span class="si">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我解决的方式：实际上版本不对，更新版本就恢复了</p>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://docs.ceph.com/en/octopus/cephadm/install/#install-cephadm"><em><strong>install-cephadm</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://ldapwiki.com/wiki/Common%20Object%20Request%20Broker%20Architecture"><em><strong>Object Request Broker Architecture</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://ldapwiki.com/wiki/Cooperation%20for%20Open%20Systems%20Interconnection%20Networking%20in%20Europe"><em><strong>Cooperation for Open Systems Interconnection Networking in Europe</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph集群安装 - ceph-deploy</title>
      <link>https://www.oomkill.com/2019/11/02-2-install-ceph-with-ceph-deploy/</link>
      <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/11/02-2-install-ceph-with-ceph-deploy/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="环境配置">环境配置</h2>
<p>Ceph 是一个开源去中心化存储平台，专为满足现代存储需求而设计。  Ceph可扩展至 EB 级，并且设计为无单点故障，使其成为需要高度可用的灵活存储的应用程序的理想选择。</p>
<p>下图显示了具有 Ceph 存储的示例 3 节点集群的布局。   两个网络接口可用于增加带宽和冗余，这有助于保持足够的带宽来满足存储要求，而不影响客户端应用程序。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230910161024448.png" alt="image-20230910161024448"  /></p>
<center>图：Ceph存储集群</center>
<center><em>Source：</em>https://www.jamescoyle.net/how-to/1244-create-a-3-node-ceph-storage-cluster</center><br>
<p>图中架构表示了一个无单点故障的 3 节点 Ceph 集群，以提供高度冗余的存储。   每个节点都配置了两个磁盘；   一台运行 Linux 操作系统，另一台将用于 Ceph 存储。   下面的输出显示了可用的存储空间，每个主机上的存储空间完全相同。   <strong>/dev/sda</strong> 是包含操作系统安装的根分区，  <strong>/dev/sdb</strong> 是一个未触及的分区，将用于部署 Ceph 集群，对应的硬件信息如下表所示。</p>
<table>
<thead>
<tr>
<th>主机名</th>
<th>public IP</th>
<th>cluster IP</th>
<th>数据盘</th>
</tr>
</thead>
<tbody>
<tr>
<td>ceph-nautilus01</td>
<td>10.0.0.50</td>
<td>10.0.0.50</td>
<td>/dev/sda<br>/dev/sdb</td>
</tr>
<tr>
<td>ceph-nautilus02</td>
<td>10.0.0.51</td>
<td>10.0.0.51</td>
<td>/dev/sda<br/>/dev/sdb</td>
</tr>
<tr>
<td>ceph-nautilus03</td>
<td>10.0.0.52</td>
<td>10.0.0.52</td>
<td>/dev/sda<br/>/dev/sdb</td>
</tr>
<tr>
<td>ceph-control</td>
<td>10.0.0.49</td>
<td>10.0.0.49</td>
<td>/dev/sda</td>
</tr>
</tbody>
</table>
<h3 id="部署工具">部署工具</h3>
<p><em>ceph-deploy</em> 工具是在 “管理节点” (ceph-admin) 上的目录中运行。</p>
<ul>
<li>ceph-deploy 部署ceph的原生工具 (最后支持版本 octopus 15)
<ul>
<li>借助于ssh来管理目标主机，sudo,和一些 python 模块来完成 ceph 集群的部署和后期维护。</li>
<li>一般讲 <em>ceph-deploy</em>  放置在专用节点，作为 ceph 集群的管理节点。</li>
<li>ceph-deploy 不是一个通用的部署工具，只是用于管理Ceph集群的，专门为用户快速部署并运行一个Ceph集群，这些功能和特性不依赖于其他的编排工具。</li>
<li>它无法处理客户端的配置，因此在部署客户端时就无法使用此工具。</li>
</ul>
</li>
</ul>
<p>下图是来自 ceph 官网的 <em>ceph-deploy</em> 部署工具的一个模型图</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20230910164114089.png" alt="image-20230910164114089"  /></p>
<center>图：ceph-deploy部署模型</center>
<center><em>Source：</em>https://docs.ceph.com/en/nautilus/start/quick-start-preflight/</center><br>
<h3 id="ceph-集群拓扑及网络">CEPH 集群拓扑及网络</h3>
<p>在Ceph内部存在两种流量：</p>
<ul>
<li>Ceph内部各节点之间用来处理OSD之间数据复制，因此为了避免正常向客户端提供服务请求，</li>
<li><strong>public network</strong> 必须，所有客户端都应位于public network</li>
<li><strong>cluster network</strong> 可选</li>
</ul>
<h2 id="ceph-deploy-先决条件配置">ceph-deploy 先决条件配置</h2>
<p>将 Ceph 安装仓库添加到 “管理节点”。然后，安装 <em>ceph-deploy</em>。</p>
<h3 id="debianubuntu">Debian/Ubuntu</h3>
<ol>
<li>添加 release key</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">wget -q -O- <span class="s1">&#39;https://download.ceph.com/keys/release.asc&#39;</span> <span class="p">|</span> sudo apt-key add -
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>将 Ceph deb添加到您的存储库。并替换为安装的 Ceph 版本（例如 nautilus）。例如：</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CEPH_VERSION</span><span class="o">=</span>nautilus
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> deb https://download.ceph.com/debian-<span class="si">${</span><span class="nv">CEPH_VERSION</span><span class="si">}</span>/ <span class="k">$(</span>lsb_release -sc<span class="k">)</span> main <span class="p">|</span> sudo tee /etc/apt/sources.list.d/ceph.list
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>更新仓库并安装 <em>ceph-deploy</em></li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt update
</span></span><span class="line"><span class="cl">sudo apt install ceph-deploy
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="rhelcentos">RHEL/CentOS</h3>
<ol>
<li>安装 epel 源，这里方式很多可以任意选择 “<strong>你所在地区可用的 epel 源</strong>”</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>添加 Ceph rpm 仓库</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CEPH_VERSION</span><span class="o">=</span>nautilus
</span></span><span class="line"><span class="cl">cat <span class="s">&lt;&lt; EOF &gt; /etc/yum.repos.d/ceph.repo
</span></span></span><span class="line"><span class="cl"><span class="s">[ceph]
</span></span></span><span class="line"><span class="cl"><span class="s">name=Ceph packages for $basearch
</span></span></span><span class="line"><span class="cl"><span class="s">baseurl=https://download.ceph.com/rpm-${CEPH_VERSION}/el7/\$basearch
</span></span></span><span class="line"><span class="cl"><span class="s">enabled=1
</span></span></span><span class="line"><span class="cl"><span class="s">priority=2
</span></span></span><span class="line"><span class="cl"><span class="s">gpgcheck=1
</span></span></span><span class="line"><span class="cl"><span class="s">gpgkey=https://download.ceph.com/keys/release.asc
</span></span></span><span class="line"><span class="cl"><span class="s">
</span></span></span><span class="line"><span class="cl"><span class="s">[ceph-noarch]
</span></span></span><span class="line"><span class="cl"><span class="s">name=Ceph noarch packages
</span></span></span><span class="line"><span class="cl"><span class="s">baseurl=https://download.ceph.com/rpm-${CEPH_VERSION}/el7/noarch
</span></span></span><span class="line"><span class="cl"><span class="s">enabled=1
</span></span></span><span class="line"><span class="cl"><span class="s">priority=2
</span></span></span><span class="line"><span class="cl"><span class="s">gpgcheck=1
</span></span></span><span class="line"><span class="cl"><span class="s">gpgkey=https://download.ceph.com/keys/release.asc
</span></span></span><span class="line"><span class="cl"><span class="s">
</span></span></span><span class="line"><span class="cl"><span class="s">[ceph-source]
</span></span></span><span class="line"><span class="cl"><span class="s">name=Ceph source packages
</span></span></span><span class="line"><span class="cl"><span class="s">baseurl=https://download.ceph.com/rpm-${CEPH_VERSION}/el7/SRPMS
</span></span></span><span class="line"><span class="cl"><span class="s">enabled=0
</span></span></span><span class="line"><span class="cl"><span class="s">priority=2
</span></span></span><span class="line"><span class="cl"><span class="s">gpgcheck=1
</span></span></span><span class="line"><span class="cl"><span class="s">gpgkey=https://download.ceph.com/keys/release.asc
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>更新缓存并安装 <em>ceph-deploy</em></li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo yum clean all <span class="o">&amp;&amp;</span> sudo yum makecache
</span></span><span class="line"><span class="cl">sudo yum install ceph-deploy
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="安装集群的预先条件---ceph-node">安装集群的预先条件 - CEPH NODE</h2>
<ul>
<li>管理节点必须拥有所有 ceph node 的无密码登录权限</li>
<li>ntp</li>
<li>开放端口</li>
<li>关闭 selinux</li>
</ul>
<h3 id="创建用于-ceph-deploy-的用户">创建用于 <em>ceph-deploy</em> 的用户</h3>
<p>ceph-deploy 实用程序必须以具有无密码 sudo 权限的用户身份登录 Ceph node，因为它需要在不提示输入密码的情况下安装软件和配置文件。</p>
<p>最新版本的 ceph-deploy 支持 &ndash;username 选项，因此您可以指定任何具有无密码的 sudo 用户（包括 root，但不推荐）。要使用 ceph-deploy &ndash;username {username}，“所指定的用户必须具有对 Ceph Node 的无密码 SSH 访问权限”，因为 ceph-deploy 不会提示您输入密码。</p>
<p>Ceph 官方建议在集群中的所有 Ceph 节点上为 ceph-deploy 创建特定用户。==请不要使用 “ceph” 作为用户名==。整个集群中的统一用户名可能会提高易用性（不是必需的），但您应该避免使用明显的用户名，因为黑客通常会通过暴力破解来使用它们（例如 root, admin, 或使用项目名称）。以下过程将 <code>{username}</code> 替换为您定义的用户名，描述了如何使用无密码 sudo 创建用户。</p>
<ol>
<li>在每个 Ceph Node 创建一个用户</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CEPH_USERNAME</span><span class="o">=</span>ceph
</span></span><span class="line"><span class="cl">sudo useradd -d /home/<span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span> -m <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span>
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> 1<span class="p">|</span>sudo passwd <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span> --stdin
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>对于添加到每个 Ceph Node 的新用户，请确保该用户具有 sudo 权限。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;</span><span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span><span class="s2"> ALL = (root) NOPASSWD:ALL&#34;</span> <span class="p">|</span> sudo tee /etc/sudoers.d/<span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">sudo chmod <span class="m">0440</span> /etc/sudoers.d/<span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="启用-ssh-无密码登录">启用 SSH 无密码登录</h3>
<p>由于 ceph-deploy 不会提示输入密码，因此必须在管理节点上生成 SSH 密钥并将公钥分发到每个 Ceph 节点。 <em>ceph-deploy</em> 将尝试为初始 monitor 生成 SSH 密钥。</p>
<ol>
<li>生成 ssh key，不要使用 sudo 或者是 root 用户</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ssh-keygen
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Generating public/private key pair.
</span></span><span class="line"><span class="cl">Enter file in which to save the key <span class="o">(</span>/ceph-admin/.ssh/id_rsa<span class="o">)</span>:
</span></span><span class="line"><span class="cl">Enter passphrase <span class="o">(</span>empty <span class="k">for</span> no passphrase<span class="o">)</span>:
</span></span><span class="line"><span class="cl">Enter same passphrase again:
</span></span><span class="line"><span class="cl">Your identification has been saved in /ceph-admin/.ssh/id_rsa.
</span></span><span class="line"><span class="cl">Your public key has been saved in /ceph-admin/.ssh/id_rsa.pub.
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li>将 SSH 密钥复制到每个 Ceph Node，将变量 “CEPH_USERNAME” 替换为创建 Ceph 部署用户创建的用户名。</li>
</ol>
<blockquote>
<p>通常情况下，主机名需要解析的，如果没有需要配置在 /etc/hosts 内</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ssh-copy-id <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span>@ceph-nautilus01
</span></span><span class="line"><span class="cl">ssh-copy-id <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span>@ceph-nautilus02
</span></span><span class="line"><span class="cl">ssh-copy-id <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span>@ceph-nautilus03
</span></span><span class="line"><span class="cl"><span class="c1"># 如果是 root 用户执行下面命令</span>
</span></span><span class="line"><span class="cl">sudo -u <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span> ssh-copy-id <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span>@ceph-nautilus01
</span></span><span class="line"><span class="cl">sudo -u <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span> ssh-copy-id <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span>@ceph-nautilus02
</span></span><span class="line"><span class="cl">sudo -u <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span> ssh-copy-id <span class="si">${</span><span class="nv">CEPH_USERNAME</span><span class="si">}</span>@ceph-nautilus03
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>添加主机名到 /etc/hosts (可选)</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ tee &gt;&gt; /etc/hosts <span class="s">&lt;&lt; EOF
</span></span></span><span class="line"><span class="cl"><span class="s">10.0.0.50 ceph-nautilus01
</span></span></span><span class="line"><span class="cl"><span class="s">10.0.0.51 ceph-nautilus02
</span></span></span><span class="line"><span class="cl"><span class="s">10.0.0.52 ceph-nautilus03
</span></span></span><span class="line"><span class="cl"><span class="s">EOF</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="网卡开机自启动">网卡开机自启动</h3>
<p>Ceph OSD Peer 并通过网络向 Ceph monitor 报告。如果默认情况下网络处于关闭状态，则在启用网络之前，Ceph 集群无法在启动期间联机。</p>
<p>在一些 Linux 发行版下（例如 CentOS）上的默认配置默认关闭网络接口。确保在启动过程中网络接口打开，以便 Ceph 守护进程可以通过网络进行通信。(如果你的系统是新装的)</p>
<h3 id="开放所需端口">开放所需端口</h3>
<p>Ceph Monitor (ceph-mon) 默认使用端口 <strong>6789</strong> 进行通信。默认情况下，Ceph OSD 在 <strong>6800:7300</strong> 端口范围内进行通信。详细信息请参见网络配置参考 <sup><a href="#1">[1]</a></sup>。</p>
<h3 id="确保关闭-selinux">确保关闭 SELinux</h3>
<p>在 CentOS 和 RHEL 上，SELinux 默认设置为“Enforcing”。为了简化您的安装，我们建议将 SELinux 设置为 Permissive 或完全禁用，这是 Ceph 官方给出的建议</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo setenforce <span class="m">0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>要持久配置 SELinux（如果 SELinux 存在问题，则建议这样做），请修改 <code>/etc/selinux/config</code> 中的配置文件。</p>
<h3 id="preferences">Preferences</h3>
<p>确保您的 “包管理器” 已安装并启用 “priority/preferences”。在 CentOS 上，您可能需要安装 EPEL。在 RHEL 上，您可能需要启用可选存储库。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo yum install yum-plugin-priorities
</span></span></code></pre></td></tr></table>
</div>
</div><p>例如，在 RHEL 7 服务器上，执行以下命令安装 yum-plugin-priorities 并启用 rhel-7-server-optional-rpms 存储库：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">sudo yum install yum-plugin-priorities --enablerepo=rhel-7-server-optional-rpms
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="初始化新的-ceph-集群">初始化新的 CEPH 集群</h2>
<p>在这里我们创建一个包含一个 Ceph Monitor (<em>ceph-mon</em>) 和 三个 Ceph OSD (<em>osd daemon</em>) 的 Ceph 集群。通常使用 <em>ceph-deploy</em> 部署集群，最佳方式是在 “管理节点” 上创建一个目录，用于维护 ceph-deploy 为集群生成的配置文件和密钥。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mkdir ceph-cluster <span class="o">&amp;&amp;</span> <span class="nb">cd</span> ceph-cluster
</span></span></code></pre></td></tr></table>
</div>
</div><p>需要注意的是，<em>ceph-deploy</em> 将文件输出到当前目录。执行 ceph-deploy 时需要确保位于此目录中。</p>
<p>还需要注意的是，需要使用 “<strong>SSH 免密的那个用户</strong>”</p>
<h3 id="确保集群节点清洁性">确保集群节点清洁性</h3>
<p>如果在任何时候遇到问题后并想重新开始，可以执行下列命令清除所有安装包和配置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph-deploy purge <span class="o">{</span>ceph-node<span class="o">}</span> <span class="o">[{</span>ceph-node<span class="o">}]</span>
</span></span><span class="line"><span class="cl">ceph-deploy purgedata <span class="o">{</span>ceph-node<span class="o">}</span> <span class="o">[{</span>ceph-node<span class="o">}]</span>
</span></span><span class="line"><span class="cl">ceph-deploy forgetkeys
</span></span><span class="line"><span class="cl">rm ceph.*
</span></span></code></pre></td></tr></table>
</div>
</div><p>需要注意的是，执行 ceph-deploy，必须执行第四条命令 <code>rm ceph.*</code></p>
<h3 id="创建集群">创建集群</h3>
<p>创建集群会生成配置文件保存到当前工作目录中，使用 ceph-deploy 执行命令新建一个集群。</p>
<ol>
<li>创建集群</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 语法</span>
</span></span><span class="line"><span class="cl">ceph-deploy new <span class="o">{</span>initial-monitor-node<span class="o">(</span>s<span class="o">)}</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 示例, 指定节点的 hostname, fqdn or hostname:fqdn</span>
</span></span><span class="line"><span class="cl">ceph-deploy new ceph-nautilus01 ceph-nautilus02 ceph-nautilus03
</span></span></code></pre></td></tr></table>
</div>
</div><p>ceph-deploy 会输出到当前文件夹下的文件包含，Ceph 配置文件 (ceph.conf)、ceph-mon 的 keyring 文件 (ceph.mon.keyring) 以及新集群的日志文件。</p>
<ol start="2">
<li>如果主机存在多个网络接口（即公共网络和集群网络是分开的），需要在 Ceph 配置文件的 [global] 部分下添加公共网络设置。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">public <span class="nv">network</span> <span class="o">=</span> <span class="o">{</span>ip-address<span class="o">}</span>/<span class="o">{</span>bits<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 示例</span>
</span></span><span class="line"><span class="cl">public <span class="nv">network</span> <span class="o">=</span> 10.1.2.0/24
</span></span></code></pre></td></tr></table>
</div>
</div><p>或者通过命令指定两个网络的 IP</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph-deploy new ceph-nautilus01 ceph-nautilus02 ceph-nautilus03 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	--cluster-network 172.18.0.0/24 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	--public-network 10.0.0.0/24
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="3">
<li>如果在 IPv6 环境中部署，请将以下内容添加到本地目录中的 ceph.conf 中：</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">echo</span> ms <span class="nb">bind</span> <span class="nv">ipv6</span> <span class="o">=</span> <span class="nb">true</span> &gt;&gt; ceph.conf
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="4">
<li>现在可以安装 ceph 软件包了，执行下列命令</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph-deploy install <span class="o">{</span>ceph-node<span class="o">}</span> <span class="o">[</span>...<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 示例</span>
</span></span><span class="line"><span class="cl">ceph-deploy install ceph-nautilus01 ceph-nautilus02 ceph-nautilus03
</span></span></code></pre></td></tr></table>
</div>
</div><p>在命令执行后，<em>ceph-deploy</em> 将在每个节点上安装 Ceph，所以要确保对应节点需要提前安装好了 ceph yum 仓库文件</p>
<p>也可以通过  <code>--release</code> 指定版本进行安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph-deploy install --release<span class="o">=</span>nautilus ceph-nautilus01 ceph-nautilus02 ceph-nautilus03
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="5">
<li>部署 <em>ceph-mon</em> 并收集密钥：</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph-deploy mon create-initial
</span></span></code></pre></td></tr></table>
</div>
</div><p>​	通常完成步骤5后，本地工作目录应具有以下 keyring 文件：</p>
<ul>
<li><code>ceph.client.admin.keyring</code></li>
<li><code>ceph.bootstrap-mgr.keyring</code></li>
<li><code>ceph.bootstrap-osd.keyring</code></li>
<li><code>ceph.bootstrap-mds.keyring</code></li>
<li><code>ceph.bootstrap-rgw.keyring</code></li>
<li><code>ceph.bootstrap-rbd.keyring</code></li>
<li><code>ceph.bootstrap-rbd-mirror.keyring</code></li>
</ul>
<ol start="6">
<li>使用 <em>ceph-deploy</em> 将配置文件和管理密钥复制到管理节点和 Ceph Node，以便可以在这些节点上使用 ceph CLI 时而无需在每次执行命令时指定 ceph-mon 地址和 keyring文件 (ceph.client.admin.keyring)。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph-deploy admin <span class="o">{</span>ceph-node<span class="o">(</span>s<span class="o">)}</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 示例</span>
</span></span><span class="line"><span class="cl">ceph-deploy admin ceph-nautilus01 ceph-nautilus02 ceph-nautilus03
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="7">
<li>部署 CEPH MANAGER (ceph-mgr)，此步骤仅需要 ==luminous+== 以上版本</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph-deploy mgr create <span class="o">{</span>ceph-node<span class="o">(</span>s<span class="o">)}</span>  *Required only <span class="k">for</span> luminous+ builds, i.e &gt;<span class="o">=</span> 12.x builds*
</span></span><span class="line"><span class="cl"><span class="c1"># 示例</span>
</span></span><span class="line"><span class="cl">ceph-deploy mgr create ceph-nautilus01
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="8">
<li>到步骤8时，只差 OSD 就完成了 RADOS 集群的安装，下面为集群添加 OSD</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph-deploy osd create --data <span class="o">{</span>device<span class="o">}</span> <span class="o">{</span>ceph-node<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 示例</span>
</span></span><span class="line"><span class="cl">ceph-deploy osd create --data /dev/sdb ceph-nautilus01
</span></span><span class="line"><span class="cl">ceph-deploy osd create --data /dev/sdb ceph-nautilus02
</span></span><span class="line"><span class="cl">ceph-deploy osd create --data /dev/sdb ceph-nautilus03
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Note：如果要在 LVM 卷上创建 OSD，则 &ndash;data 的参数必须是 <code>{volume_group}/{lv_name}</code>，而不是卷的块设备的路径</p>
</blockquote>
<ol start="9">
<li>检查集群状态</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ssh node1 sudo ceph health
</span></span><span class="line"><span class="cl"><span class="c1"># or </span>
</span></span><span class="line"><span class="cl">ceph -s
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="no-module-named-pkg_resources">No module named pkg_resources</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph-deploy new ceph-nautilus01 ceph-nautilus02 ceph-nautilus03
</span></span><span class="line"><span class="cl">Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/bin/ceph-deploy&#34;</span>, line 18, in &lt;module&gt;
</span></span><span class="line"><span class="cl">    from ceph_deploy.cli import main
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&#34;</span>, line 1, in &lt;module&gt;
</span></span><span class="line"><span class="cl">    import pkg_resources
</span></span><span class="line"><span class="cl">ImportError: No module named pkg_resources
</span></span></code></pre></td></tr></table>
</div>
</div><p>解决：This issue can be solved by installing <code>yum install -y python-setuptools</code>.</p>
<h3 id="runtimeerror-nosectionerror-no-section-ceph">RuntimeError: NoSectionError: No section: &lsquo;ceph&rsquo;</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,640<span class="o">][</span>ceph-nautilus01<span class="o">][</span>DEBUG <span class="o">]</span>   Installing : ceph-release-1-1.el7.noarch                                  1/1 
</span></span><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,640<span class="o">][</span>ceph-nautilus01<span class="o">][</span>DEBUG <span class="o">]</span> warning: /etc/yum.repos.d/ceph.repo created as /etc/yum.repos.d/ceph.repo.rpmnew
</span></span><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,759<span class="o">][</span>ceph-nautilus01<span class="o">][</span>DEBUG <span class="o">]</span>   Verifying  : ceph-release-1-1.el7.noarch                                  1/1 
</span></span><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,759<span class="o">][</span>ceph-nautilus01<span class="o">][</span>DEBUG <span class="o">]</span> 
</span></span><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,759<span class="o">][</span>ceph-nautilus01<span class="o">][</span>DEBUG <span class="o">]</span> Installed:
</span></span><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,759<span class="o">][</span>ceph-nautilus01<span class="o">][</span>DEBUG <span class="o">]</span>   ceph-release.noarch 0:1-1.el7                                                 
</span></span><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,759<span class="o">][</span>ceph-nautilus01<span class="o">][</span>DEBUG <span class="o">]</span> 
</span></span><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,759<span class="o">][</span>ceph-nautilus01<span class="o">][</span>DEBUG <span class="o">]</span> Complete!
</span></span><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,759<span class="o">][</span>ceph-nautilus01<span class="o">][</span>WARNING<span class="o">]</span> ensuring that /etc/yum.repos.d/ceph.repo contains a high priority
</span></span><span class="line"><span class="cl"><span class="o">[</span>2019-09-11 05:31:42,767<span class="o">][</span>ceph_deploy<span class="o">][</span>ERROR <span class="o">]</span> RuntimeError: NoSectionError: No section: <span class="s1">&#39;ceph&#39;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>解决：CEPH Node 上不要安装 yum 仓库，<em>ceph-deploy</em> 会自动安装，如果存在新的文件会被命名为 <code>ceph.repo.rpmnew</code></p>
<h2 id="向-rados-集群添加-osd">向 RADOS 集群添加 OSD</h2>
<h4 id="列出并擦净磁盘">列出并擦净磁盘</h4>
<p><em>ceph-deploy disk</em> 命令可以检查并列出OSD节点上所有可用的磁盘相关的信息。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph-deploy disk list stor01 stor02 stor03 stor04
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果遇到 <code>Running command: sudo fdisk -l</code> 无输出，原因为系统问中文，修改 <em>en_us.utf8</em> 后可正常显示。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$ ceph-deploy disk list stor01
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.conf<span class="o">][</span>DEBUG <span class="o">]</span> found configuration file at: /home/cephadmin/.cephdeploy.conf
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span> Invoked <span class="o">(</span>2.0.1<span class="o">)</span>: /bin/ceph-deploy disk list stor01
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span> ceph-deploy options:
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  username                      : None
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  verbose                       : False
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  debug                         : False
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  overwrite_conf                : False
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  subcommand                    : list
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  quiet                         : False
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fab61201488&gt;
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  cluster                       : ceph
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  host                          : <span class="o">[</span><span class="s1">&#39;stor01&#39;</span><span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  func                          : &lt;<span class="k">function</span> disk at 0x7fab6144e9b0&gt;
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  ceph_conf                     : None
</span></span><span class="line"><span class="cl"><span class="o">[</span>ceph_deploy.cli<span class="o">][</span>INFO  <span class="o">]</span>  default_release               : False
</span></span><span class="line"><span class="cl"><span class="o">[</span>stor01<span class="o">][</span>DEBUG <span class="o">]</span> connection detected need <span class="k">for</span> sudo
</span></span><span class="line"><span class="cl"><span class="o">[</span>stor01<span class="o">][</span>DEBUG <span class="o">]</span> connected to host: stor01 
</span></span><span class="line"><span class="cl"><span class="o">[</span>stor01<span class="o">][</span>DEBUG <span class="o">]</span> detect platform information from remote host
</span></span><span class="line"><span class="cl"><span class="o">[</span>stor01<span class="o">][</span>DEBUG <span class="o">]</span> detect machine <span class="nb">type</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>stor01<span class="o">][</span>DEBUG <span class="o">]</span> find the location of an executable
</span></span><span class="line"><span class="cl"><span class="o">[</span>stor01<span class="o">][</span>INFO  <span class="o">]</span> Running command: sudo fdisk -l
</span></span></code></pre></td></tr></table>
</div>
</div><p>而后，在管理节点上使用 <em>ceph-deploy</em> 命令 擦除计划专用于OSD磁盘上的所有分区表和数据以便用于OSD，命令格式为<code>ceph-deploy disk zab {osd-server-name} {disk-name}</code>，需要注意的是此步会清除目标设备上的所有数据。</p>
<p>擦除一个磁盘</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph-deploy disk zap
</span></span><span class="line"><span class="cl">ceph-deploy disk zap <span class="o">{</span>ceph_node<span class="o">}</span> /dev/sdb
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Note：如果是未格式化的块设备不需要额外擦除，ceph 集群要求 ceph 管理的块设备必须是未格式化的，如果格式化过的需要擦除</p>
</blockquote>
<p><em>ceph-deploy osd &ndash;help</em>：</p>
<ul>
<li><code>block-db</code> 可以理解为RocksDB数据库，元数据存放的位置</li>
<li><code>block-wal</code> 数据库的数据日志存放的位置</li>
<li><code>filestore</code> 如果使用<code>filestore</code>,明确指定选项<code>--filestore</code> 指明数据放哪，并指明日志放哪（日志指的是文件系统日志）<code>ceph-deploy osd create {node} --filestore --data /path/to/data --journal /path/to/journal</code></li>
<li>bluestore自身没有文件系统，故无需日志，数据库需要日志</li>
</ul>
<h2 id="扩展集群">扩展集群</h2>
<h3 id="添加-osd">添加 OSD</h3>
<p>当一个 “基本集群” 部署好并运行，下一步就是扩展集群。通常会扩展集群，扩展集群存在两种类型，集群组件与OSD，这里主要围绕 扩展 OSD</p>
<p>早期版本的ceph-deploy命令支持在将添加OSD的过程分为两个步骤：准备OSD，激活OSD，但新版本中，此种操作方式已被废除，添加OSD的步骤只能由命令 <em>ceph-deploy osd create create {node} &ndash;data {data-disk}</em> ，一次完成，默认存储引擎为 <strong>bluestore</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph-deploy osd create <span class="o">{</span>node<span class="o">}</span> --data /dev/sdb 
</span></span><span class="line"><span class="cl">ceph-deploy osd create <span class="o">{</span>node<span class="o">}</span> --data /dev/sdc
</span></span></code></pre></td></tr></table>
</div>
</div><p>而后可使用 <em>ceph-deploy osd list {node}</em> 命令列出指定节点上的OSD：</p>
<h3 id="移除-osd-的">移除 OSD 的</h3>
<p>Ceph集群中的一个OSD通常对应一个设备，且运行于专用的守护进程。在某OSD设备出现故障，或管理员出于管理只需确实要移除特定的OSD设备时，需要先停止相关的守护进程，而后再进行移除操作。对于Luminous及其之后的版本来说，停止和移除命令的格式如下</p>
<ul>
<li>停止设备:  <em>ceph osd out {osd-num}</em></li>
<li>停止进程:  <em>sudo systemctl stop ceph-osd@{osd-num}</em></li>
<li>移除设备: <em>ceph osd purge {id} &ndash;yes-i-really-mean-it</em></li>
</ul>
<p>若类似如下的OSD的设备信息存在于ceph.conf配置文件中，管理员在删除OSD之后手动将其删除。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">[osd.1]
</span></span><span class="line"><span class="cl">    host = {hostname}
</span></span></code></pre></td></tr></table>
</div>
</div><p>不过，对于 <em>Luminous</em> 之前的版本来说，管理员需要依次手动执行如下步骤删除OSD设备</p>
<ul>
<li>于CRUSH运行图中移除设备: <em>ceph osd crush remove {name}</em></li>
<li>移除OSD的认证key: <em>ceph auth del osd.{osd-num}</em></li>
<li>移除设备: <em>ceph osd purge {id} &ndash;yes-i-really-mean-it</em></li>
</ul>
<h3 id="扩展-ceph-mon">扩展 ceph-mon</h3>
<p>Ceph 集群需要至少一个 <em>Ceph Monitor</em> 和一个 <em>Ceph Manager</em>，生产环境中，为了实现高可用，Ceph集群通常运行多个监视器，以免单监视器整个存储集群崩溃。Ceph使用 Paxos 算法，改算法是需要至少需要板书以上的监视器（大于n/2，其中n为总监视器数量），才能形成法定人数，尽管此非必须，<strong>奇数个</strong> ceph-mon 往往更好。</p>
<p>使用 <em>ceph-deploy mon add {nodes}</em> 命令可以一次添加一个 ceph-mon 到集群中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph-deploy mon add nautilus02  <span class="c1">## 此处使用短格式名称，长格式名称会报错。</span>
</span></span><span class="line"><span class="cl">ceph-deploy mon add nautilus03
</span></span></code></pre></td></tr></table>
</div>
</div><p>设置完成后，可以在ceph客户端上查看监视器及法定人数的相关信息:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ ceph quorum_status --format json-pretty
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;election_epoch&#34;</span>: 20,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;quorum&#34;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">        0,
</span></span><span class="line"><span class="cl">        1,
</span></span><span class="line"><span class="cl">        <span class="m">2</span>
</span></span><span class="line"><span class="cl">    <span class="o">]</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;quorum_names&#34;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;stor01&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;stor02&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;stor03&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="o">]</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;quorum_leader_name&#34;</span>: <span class="s2">&#34;stor01&#34;</span>,
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;monmap&#34;</span>: <span class="o">{</span> 
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;epoch&#34;</span>: 3,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;fsid&#34;</span>: <span class="s2">&#34;69fb9b55-3fb5-42d0-8cf7-239a3b569791&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;modified&#34;</span>: <span class="s2">&#34;2019-06-06 21:19:41.274199&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;created&#34;</span>: <span class="s2">&#34;2019-06-05 12:35:31.143594&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;features&#34;</span>: <span class="o">{</span>
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;persistent&#34;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;kraken&#34;</span>,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;luminous&#34;</span>,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;mimic&#34;</span>,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;osdmap-prune&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="o">]</span>,
</span></span><span class="line"><span class="cl">            <span class="s2">&#34;optional&#34;</span>: <span class="o">[]</span>
</span></span><span class="line"><span class="cl">        <span class="o">}</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;mons&#34;</span>: <span class="o">[</span>
</span></span><span class="line"><span class="cl">            <span class="o">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;rank&#34;</span>: 0,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;name&#34;</span>: <span class="s2">&#34;stor01&#34;</span>,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;addr&#34;</span>: <span class="s2">&#34;10.0.0.4:6789/0&#34;</span>,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;public_addr&#34;</span>: <span class="s2">&#34;10.0.0.4:6789/0&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="o">}</span>,
</span></span><span class="line"><span class="cl">            <span class="o">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;rank&#34;</span>: 1,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;name&#34;</span>: <span class="s2">&#34;stor02&#34;</span>,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;addr&#34;</span>: <span class="s2">&#34;10.0.0.5:6789/0&#34;</span>,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;public_addr&#34;</span>: <span class="s2">&#34;10.0.0.5:6789/0&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="o">}</span>,
</span></span><span class="line"><span class="cl">            <span class="o">{</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;rank&#34;</span>: 2,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;name&#34;</span>: <span class="s2">&#34;stor03&#34;</span>,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;addr&#34;</span>: <span class="s2">&#34;10.0.0.6:6789/0&#34;</span>,
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;public_addr&#34;</span>: <span class="s2">&#34;10.0.0.6:6789/0&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="o">}</span>
</span></span><span class="line"><span class="cl">        <span class="o">]</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="扩展-manager-节点">扩展 Manager 节点</h3>
<p>Ceph Manager (ceph-mgr) 以 <em>Active/Standy</em> 模式运行，部署其他 ceph-mgr 守护进程可确保在 Active 节点的 ceph-mgr 守护进程故障时，其中一个 Standby 实例可以在不中断服务的情况下接管其任务。</p>
<p>mgr 就是无状态的 web 服务，一般来讲两个足够了。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph-deploy mgr create <span class="o">{</span>ceph_node<span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="启动-rgw">启动 RGW</h3>
<p>RGW (<strong>R</strong>ados <strong>G</strong>ateway) 必要组件，仅在需要用到对象存储兼容 “S3” 和 “Swift” 的 RESTful 接口时才需要部署 RGW 实例，相关的命令为<em>ceph-deploy rgw create (gateway-node)</em> 。</p>
<p>radosgw需要用自用的存储池不能与RBD混合使用，RGW 会在创建时自动初始化出存储池来，RGW 需要有相应服务才能运行起来。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph-deploy rgw create nautilus02
</span></span></code></pre></td></tr></table>
</div>
</div><p>添加完成后，<em>ceph -s</em>  命令的 service 一段中会输出相关信息：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ ceph -s
</span></span><span class="line"><span class="cl">  cluster:
</span></span><span class="line"><span class="cl">    id:     69fb9b55-3fb5-42d0-8cf7-239a3b569791
</span></span><span class="line"><span class="cl">    health: HEALTH_WARN
</span></span><span class="line"><span class="cl">            application not enabled on <span class="m">1</span> pool<span class="o">(</span>s<span class="o">)</span>
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">  services:
</span></span><span class="line"><span class="cl">    mon: <span class="m">3</span> daemons, quorum stor01,stor02,stor03
</span></span><span class="line"><span class="cl">    mgr: stor01<span class="o">(</span>active<span class="o">)</span>, standbys: stor04
</span></span><span class="line"><span class="cl">    osd: <span class="m">8</span> osds: <span class="m">8</span> up, <span class="m">8</span> in
</span></span><span class="line"><span class="cl">    rgw: <span class="m">1</span> daemon active
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">  data:
</span></span><span class="line"><span class="cl">    pools:   <span class="m">7</span> pools, <span class="m">160</span> pgs
</span></span><span class="line"><span class="cl">    objects: <span class="m">196</span>  objects, 1.5 KiB
</span></span><span class="line"><span class="cl">    usage:   <span class="m">21</span> GiB used, <span class="m">79</span> GiB / <span class="m">100</span> GiB avail
</span></span><span class="line"><span class="cl">    pgs:     <span class="m">160</span> active+clean
</span></span></code></pre></td></tr></table>
</div>
</div><p>默认情况下，RGW 实例监听于 TCP 协议的 7480 端口，需要修改，可以通过在运行 RGW 的节点上编辑其主配置文件 ceph.conf 进行修改，相关参数如下所示</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">[client]
</span></span><span class="line"><span class="cl">rgw_frontends = &#34;civetweb port=8080&#34;
</span></span></code></pre></td></tr></table>
</div>
</div><p>RGW 会在 RADOWS 集群上生成包括如下存储池的一系列存储池</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$  ceph osd pool ls
</span></span><span class="line"><span class="cl">mypool
</span></span><span class="line"><span class="cl">rbdpool
</span></span><span class="line"><span class="cl">testpool
</span></span><span class="line"><span class="cl">.rgw.root
</span></span><span class="line"><span class="cl">default.rgw.control
</span></span><span class="line"><span class="cl">default.rgw.meta
</span></span><span class="line"><span class="cl">default.rgw.log
</span></span></code></pre></td></tr></table>
</div>
</div><p>RGW 提供的是兼容 S3 和 Swift 的 REST 接口，客户端通过 HTTP 进行交互，完成数据的增删改查等管理操作。</p>
<h3 id="启用文件系统-cephfs-接口">启用文件系统 (CephFS) 接口</h3>
<p>CephFS 需要至少运行一个 Metadata (MDS) 守护进程 (ceph-mds)，此进程管理与 CephFS 上存储的文件相关的元数据，并协调对 Ceph存储集群的访问。因此，若要使用 CephFS ，需要在存储集群中至少部署一个 MDS 实例，增加 MDS 可以使用命令 <em>ceph-deploy mds create {ceph-node}</em>  完成</p>
<p>还需主义的是，每个 CephFS 都至少需要两个存储池，一个用来存放元数据 (Metadata Pool)，一个存放数据 (Data Pool)。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ ceph-deploy mds create stor01
</span></span></code></pre></td></tr></table>
</div>
</div><p>查看 MDS 的相关状态可以发现，刚添加的 MDS 处于 standby 状态</p>
<p>在运行起来还不够，还没为其创建存储池，故其不能正常工作，处于standby模式。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph mds stat
</span></span><span class="line"><span class="cl">, 1 up:standby
</span></span></code></pre></td></tr></table>
</div>
</div><p>使用 CephFS 之前需要事先于集群中创建一个文件系统，并为其分别指定 “元数据” 和 “数据” 相关的存储池，下面创建一个名为 <em>cephfs</em> 的文件系统用于测试，使用cephfs-metadata为数据存储池，使用cephfs-data为数据存储池。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># 创建存储池</span>
</span></span><span class="line"><span class="cl">ceph osd pool create cephfs-metadata <span class="m">64</span>
</span></span><span class="line"><span class="cl">ceph osd pool create cephfs-data <span class="m">64</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 创建 cephfs</span>
</span></span><span class="line"><span class="cl"><span class="c1">## 语法</span>
</span></span><span class="line"><span class="cl">ceph fs new <span class="o">{</span>fs_name<span class="o">}</span> <span class="o">{</span>meatadata-pool<span class="o">}</span> <span class="o">{</span>data-pool<span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="c1">## 示例</span>
</span></span><span class="line"><span class="cl">ceph fs new cephfs cephfs-metadata cephfs-data
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><em>ceph fs add_data_pool</em>： 额外添加数据池</li>
<li><em>ceph fs new</em>： 创建新文件系统</li>
<li><em>ceph fs status</em>： 查看CephFS 文件系统状态</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ ceph fs status cephfs
</span></span><span class="line"><span class="cl">cephfs - <span class="m">0</span> <span class="nv">clients</span>
</span></span><span class="line"><span class="cl"><span class="o">======</span>
</span></span><span class="line"><span class="cl">+------+--------+--------+---------------+-------+-------+
</span></span><span class="line"><span class="cl"><span class="p">|</span> Rank <span class="p">|</span> State  <span class="p">|</span>  MDS   <span class="p">|</span>    Activity   <span class="p">|</span>  dns  <span class="p">|</span>  inos <span class="p">|</span>
</span></span><span class="line"><span class="cl">+------+--------+--------+---------------+-------+-------+
</span></span><span class="line"><span class="cl"><span class="p">|</span>  <span class="m">0</span>   <span class="p">|</span> active <span class="p">|</span> stor02 <span class="p">|</span> Reqs:    <span class="m">0</span> /s <span class="p">|</span>   <span class="m">10</span>  <span class="p">|</span>   <span class="m">13</span>  <span class="p">|</span>
</span></span><span class="line"><span class="cl">+------+--------+--------+---------------+-------+-------+
</span></span><span class="line"><span class="cl">+-----------------+----------+-------+-------+
</span></span><span class="line"><span class="cl"><span class="p">|</span>       Pool      <span class="p">|</span>   <span class="nb">type</span>   <span class="p">|</span>  used <span class="p">|</span> avail <span class="p">|</span>
</span></span><span class="line"><span class="cl">+-----------------+----------+-------+-------+
</span></span><span class="line"><span class="cl"><span class="p">|</span> cephfs-metadata <span class="p">|</span> metadata <span class="p">|</span> <span class="m">2286</span>  <span class="p">|</span> 21.7G <span class="p">|</span>
</span></span><span class="line"><span class="cl"><span class="p">|</span>   cephfs-data   <span class="p">|</span>   data   <span class="p">|</span>    <span class="m">0</span>  <span class="p">|</span> 21.7G <span class="p">|</span>
</span></span><span class="line"><span class="cl">+-----------------+----------+-------+-------+
</span></span><span class="line"><span class="cl">+-------------+
</span></span><span class="line"><span class="cl"><span class="p">|</span> Standby MDS <span class="p">|</span>
</span></span><span class="line"><span class="cl">+-------------+
</span></span><span class="line"><span class="cl">+-------------+
</span></span><span class="line"><span class="cl">MDS version: ceph version 13.2.6 <span class="o">(</span>7b695f835b03642f85998b2ae7b6dd093d9fbce4<span class="o">)</span> mimic <span class="o">(</span>stable<span class="o">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="reference">Reference</h2>
<p><sup id="1">[1]</sup> <a href="https://docs.ceph.com/en/nautilus/rados/configuration/network-config-ref/"><em><strong>Network Configuration Reference</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://unix.stackexchange.com/questions/379791/ceph-deployerror-runtimeerror-nosectionerror-no-section-ceph"><em><strong>ceph_deploy RuntimeError: NoSectionError: No section: &lsquo;ceph&rsquo;</strong></em></a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph RBD - 初识块存储RBD</title>
      <link>https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/09/03-1-acquaintance-rdb/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="什么是块存储-rbd">什么是块存储 RBD</h2>
<p>Ceph RBD (<strong>R</strong>ADOS <strong>B</strong>lock <strong>D</strong>evice) 是 Ceph 提供的三种存储类型之一 (块存储 RBD, 文件存储 CephFS, 对象存储 RGW)，也是另外两个存储类型 (文件存储 CephFS, 对象存储 RGW) 的底座，位于 RADOS 架构中的最底层，由下图可以看出</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/Screenshot-from-2015-10-28-130109.png" alt=""  /></p>
<center>图：Ceph RADOS架构图</center>
<center><em>Source：</em>https://www.supportsages.com/ceph-part-3-technical-architecture-and-components/</center><br>
<p>RADOS 是可信赖的自动分布式对象存储 (<strong>R</strong>eliable <strong>A</strong>utonomous <strong>D</strong>istributed <strong>O</strong>bject <strong>S</strong>tore) 的简写，通俗来说，RADOS 代表的就是整个 Ceph 集群，数据对象在集群中的存储方式会“将对象复制为多副本” 以实现容错，所以 Ceph 集群的底座就是 RADOS，一个 RADOS 集群的组件通常包含三个，<em>OSD Daemon</em> , <em>MDS</em>, <em>MON</em></p>
<ul>
<li><strong>Object Storage Device</strong> (<em>OSD</em>) Daemon：RADOS集群中负责存储守护进程，与 OSD (数据的物理或逻辑存储单元【通常指一个硬盘】)交互。集群中的每个 Ceph Node 都必须运行 OSD Daemon。对于每个 OSD，可以有一个关联的硬盘 (通常一个OSD Daemon 对应一个存储单元)。</li>
<li><strong>MONITORS</strong> (Mon Daemon)：Monitor (ceph-mon) 不是集群存储组件的一部分，但它通过监视 OSD 状态并生成 “Cluster Map” 而成为 RADOS 不可或缺的一部分。它监视 OSD 并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。一般来说，它充当存储集群中所有 OSD 的 Monitor</li>
<li><strong>Manager</strong> (MGR Daemon)：Manager (ceph-mgr) 是与 ceph-mon 一同运行的守护进程，为外部监控和管理系统提供额外的监视和接口。默认情况下，ceph-mgr 除了确保其正在运行之外不需要其他配置。如果没有运行 ceph-mgr，<code>ceph -s</code> 将会看到一条 WARN；不管是使用什么方式部署的集群 ( ceph-deploy, cephadm)，ceph-mgr 总会 与 ceph-mon 同时运行在一个节点上，也可单独运行在 Ceph Node 之上。</li>
</ul>
<blockquote>
<p>通常 Monitor (ceph-mon) 不构成“<strong>存储</strong>”集群的一部分，只是通过监视 OSD 状态并生成 Cluster map 而成为 ceph存储集群中不可缺少的组件。它通过监视 OSD  并跟踪在给定时间点哪些 OSD 处于运行状态、哪些 OSD 处于对等状态、OSD 的状态等。</p>
</blockquote>
<h3 id="如何确定-ceph-mon-的数量">如何确定 ceph-mon 的数量</h3>
<p>ceph-mon 的数量最好是奇数，并且数量个数时有限限制的，这里在总结一下 ceph monitor 的作用：</p>
<ul>
<li>Monitor 不向客户端提供存储的对象，由于它只是一个监控节点，因此它不保存/管理任何对象，也不属于对象存储的一部分，但它仍然是 RADOS 的一部分，但不用于数据/对象存储。</li>
<li>它维护簇映射状态，即：存储集群客户端/应用程序从 ceph-mon 检索集群映射的副本</li>
<li>通常很奇怪并且数量有限，因为他们的工作相当简单，维护 OSD 的状态。</li>
<li>为分布式决策提供共识，当要针对 OSD 的状态做出特定决策时，它提供了一般规则。  当多个 OSD 要进行对等互连或复制时，监视器会自行决定如何进行对等互连，而不是由 ODS 自行决定。</li>
<li>Ceph OSD 守护进程检查自身状态和其他 OSD 的状态并向监视器报告。</li>
</ul>
<p>中国铁路2020年分享的 1550+ OSD PB 级别存储 ceph-con 数量从 “3” 升级到 “5” 后，趋势稳定 <sup><a href="#1">[1]</a></sup></p>
<h2 id="rbd-存储单元">RBD 存储单元</h2>
<p>RBD 块设备 (RADOS Block Device) 在 Ceph 中被称为 image。image 由 “元数据” 和 ”数据“两部分组成，其中元数据存储在多个特殊的 RADOS 对象中，而数据被自动”条带化“成多个 RADOS 对象进行存储。除了 image 自身的元数据之外，在 image 所属的 存储池 (Pool) 中都还有一组特殊的 RADOS 对象记录 image 关联关系或附加信息等相关的 RBD 管理元数据。所有的数据对象和元数据对象都依据 CRUSH 规则 (CRUSH RULE) 存储在底层的 OSD 设备上，因此 RBD 块设备自动继承了 RADOS 对象的数据冗余保护机制和一致性策略。</p>
<p>对于 RBD Image 官方是这么描述的 <sup><a href="#2">[2]</a></sup></p>
<blockquote>
<p>RBD images are simple block devices that are <strong>striped</strong> over objects and stored in a RADOS object store. The size of the objects the image is striped over must be a power of two.</p>
</blockquote>
<p>由此，我们可以得知，”镜像“ (RBD IMAGE) 的表现就是一个块设备</p>
<h3 id="pool">POOL</h3>
<p>存储池是用于存储对象的逻辑分区，一个存储池中可以放置多个”IMAGE“。在官方给出的，存储池提供了如下特性:</p>
<p>池提供以下功能：</p>
<ol>
<li>
<p>冗余性 (<strong>Resilience</strong>)：可以设置允许失败的 OSD 数量，而不会丢失任何数据。如果您的集群使用 ”复制池“ (replicated )，那么可以容忍的 OSD 故障数量等于复制的数量。</p>
<p>例如：典型配置存储一个对象及其每个 RADOS 对象的两个副本（即size=3，默认值），但您可以根据池的需求配置副本数量。对于纠删码池 (erasure-coded)，冗余性定义为编码块的数量（例如，默认纠删码配置文件中的m = 2）。</p>
</li>
<li>
<p>放置组（Placement Groups，PGs）：您可以为池设置放置组（PGs）的数量。在典型配置中，每个 OSD 的目标 PG 数量约为 100 个 PGs。这提供了合理的负载均衡，而不会消耗过多的计算资源。在设置多个存储池时，请小心为每个存储池和整个集群设置合适数量的 PGs。每个 PG 都属于特定的存储池：当多个存储池使用相同的 OSDs 时，请确保每个 OSD 上的 PG 副本总数在所需的 “每个 OSD 上 PG数量的目标范围内”。要计算适合您的池的PG数量，请使用 pgcalc <sup><a href="#3">[3]</a></sup> 工具。</p>
</li>
<li>
<p>CRUSH规则：当数据存储在池中时，对象及其副本（或在纠删码池中的块）在您的集群中的放置由 CRUSH 规则管理。如果默认规则不适合您的用例，可以为池创建自定义 CRUSH 规则。</p>
</li>
<li>
<p>快照：命令 <code>ceph osd pool mksnap</code> 可创建基于存储池的快照。</p>
</li>
</ol>
<h2 id="librdb-和-librados">librdb 和 librados</h2>
<p>要想使用块设备，就涉及到 RADOS 之上数据的交互，我们已经了解到了 RBD 是为 KVM 等虑拟化技术和云 OS（如 OpenStack 和 CloudStack）提供高性能和无限可扩展性的存储后端，这些系统依赖于 libvirt 和 QEMU 实用程序与 RBD 进行集成。</p>
<p>在Ceph中，提供了一种 librados 的库，它可以使得 客户端 与 Ceph 集群间的交互，在 RADOS 之上 存在一个 &ldquo;librados&quot;库，而 &ldquo;librbd&rdquo; 库则是构建与 “librados” 之上的的库，提供了块存储的功能，下面是两个库在“用途”和“功能”上有一些重要区别：</p>
<ol>
<li><strong>librados</strong>（RADOS客户端库）：
<ul>
<li><strong>用途</strong>：librados是用于与Ceph的底层RADOS存储层交互的库。它提供了与Ceph集群中的对象存储进行直接通信的API，允许应用程序执行各种存储操作，如读取、写入、删除和管理存储对象。</li>
<li><strong>功能</strong>：librados允许应用程序直接访问Ceph集群，而不需要高级抽象，这意味着应用程序可以更精细地控制数据的读写和存储策略。librados可以用于构建自定义数据存储和管理解决方案。</li>
</ul>
</li>
<li><strong>librbd</strong>（RBD客户端库）：
<ul>
<li><strong>用途</strong>：librbd是用于与Ceph中的RBD（Rados Block Device）存储层交互的库。它构建在librados之上，提供了更高级别的抽象，用于处理块设备（镜像）操作，如创建、映射、快照和克隆等。</li>
<li><strong>功能</strong>：librbd简化了块设备操作，并提供了易于使用的API，使应用程序能够轻松地管理RBD镜像。它是构建虚拟化平台、云存储、容器存储等应用的关键组件。</li>
</ul>
</li>
</ol>
<p>要使用 RBD 存储池需要先启用，而却需要注意的是 rbd 存储池并不能直接用于块设备，而是需要事先在其中按需创建映像（image），而 “映像” 代表了真正的块设备</p>
<h2 id="rbb-支持的功能">RBB 支持的功能</h2>
<ul>
<li>快照</li>
<li>排他锁/独占锁</li>
<li>镜像</li>
<li>实时迁移</li>
<li>..</li>
</ul>
<h3 id="快照">快照</h3>
<p>RBD 快照 (<em><strong>snapshot</strong></em>) 是 image 在某个特定时间点的只读逻辑副本，类似于虚拟机的快照，快照也是 RBD 的高级功能之一，使得可以用户创建映像快照以保留时间点状态历史记录。 Ceph还支持快照分层，使您可以快速轻松地克隆镜像（例如虚拟机镜像）。 Ceph RBD 快照的使用通过命令 rbd 和几个更高级的接口进行管理，包括 QEMU, libvirt, OpenStack 和 CloudStack。</p>
<p>由于 Ceph RBD 不知道 “镜像” 内的任何文件系统，因此快照仅是崩溃一致的，除非它们在挂载(<em>mouting</em>) 操作系统内进行协调。因此，我们建议您在拍摄快照之前暂停或停止 I/O。</p>
<blockquote>
<p>如果镜像包含文件系统，则在拍摄快照之前文件系统应处于内部一致状态。即需要停止IO</p>
</blockquote>
<p>快照的操作</p>
<p>ceph 有专门的命令 <code>rbd</code> 可以进行 RBD 相关的操作，例如快照的命令为 <code>rbd snapshot</code></p>
<table>
<thead>
<tr>
<th>命令</th>
<th>语法</th>
<th>样例</th>
</tr>
</thead>
<tbody>
<tr>
<td>创建快照</td>
<td>rbd snap create {pool-name}/{image-name}@{snap-name}</td>
<td>rbd snap create rbd/foo@snapname</td>
</tr>
<tr>
<td>列出快照</td>
<td>rbd snap ls {pool-name}/{image-name}</td>
<td>rbd snap ls rbd/foo</td>
</tr>
<tr>
<td>回滚快照</td>
<td>rbd snap rollback {pool-name}/{image-name}@{snap-name}</td>
<td>rbd snap rollback rbd/foo@snapname</td>
</tr>
<tr>
<td>删除一个快照</td>
<td>rbd snap rm {pool-name}/{image-name}@{snap-name}</td>
<td>rbd snap rm rbd/foo@snapname</td>
</tr>
<tr>
<td>清除所有快照</td>
<td>rbd snap purge {pool-name}/{image-name}</td>
<td>rbd snap purge rbd/foo</td>
</tr>
</tbody>
</table>
<h3 id="列出快照">列出快照</h3>
<h2 id="reference">Reference</h2>
<blockquote>
<p><sup id="1">[1]</sup> <a href="https://www.bilibili.com/video/BV1oe411W7Jj"><em><strong>中国铁路：Ceph在单集群1551个OSD下的挑战</strong></em></a></p>
<p><sup id="2">[2]</sup> <a href="https://docs.ceph.com/en/latest/man/8/rbd/#description"><em><strong>rbd &ndash; manage rados block device (RBD) images</strong></em></a></p>
<p><sup id="3">[3]</sup> <a href="https://old.ceph.com/pgcalc/"><em><strong>pgcalc</strong></em></a></p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph RBD - 关于RBD的操作与管理</title>
      <link>https://www.oomkill.com/2019/07/03-2-rbd-management/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/07/03-2-rbd-management/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>上一章提到了 RBD 块设备相关的基本配置，这章主要描述 RBD 操作部分</p>
<h3 id="ceph块设备接口rdb">ceph块设备接口（RDB）</h3>
<p>Ceph块设备，也称为RADOS块设备（简称RBD），是一种基于RADOS存储系统支持超配（thin-provisioned）、可伸缩的条带化数据存储系统，它通过librbd库与OSD进行交互。RBD为KVM等虑拟化技术和云OS（如OpenStack和CloudStack）提供高性能和无限可扩展性的存储后端，这些系统依赖于libvirt和QEMU实用程序与RBD进行集成。</p>
<p>客户端基于librbd库即可将RADOS存储集群用作块设备，不过，用于rbd的存储池需要实现启用rbd功能并进行初始化。例如，下面的命令创建rbddata的存储池，在启动rbd功能后对其进行初始化。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">ceph osd pool create rbdpool <span class="m">64</span>  <span class="c1"># 创建存储池</span>
</span></span><span class="line"><span class="cl">rbd pool init -p rbdpool <span class="c1"># </span>
</span></span><span class="line"><span class="cl">rbd create rbdpool/img1 --size 2G 
</span></span><span class="line"><span class="cl">rbd ls -p rbdpool
</span></span></code></pre></td></tr></table>
</div>
</div><p>不过rbd存储池并不能直接用于块设备，而是需要事先在其中按需创建影响（image）</p>
<p>RBD是建立在librados之上的客户端，全程为Rados Block Devices，是对rados存储服务做抽象，将rados位于存储池当中所提供的存储空间，使用对象式存储数据的能力的机制。</p>
<p>rbd虚拟磁盘设备要想使linux内核所识别使用，要求linux在内核级支持Ceph，内核级有一个Ceph模块，专门用来驱动Ceph设备的。这个驱动就叫librbd。此模块自己是一个客户端，能够连接至RBD服务上，检索出他所管理的镜像文件。从而镜像文件完全可以被linux主机作为一块完整的硬盘来使用。</p>
<blockquote>
<p><strong>创建存储池镜像</strong></p>
</blockquote>
<ul>
<li><code>--object-size</code>  任何数据存储在存储池中都被切分为固定大小的对象，对象通常称之为条带，切分的对象大小，默认4M</li>
<li><code>--image-feature</code> 指定镜像文件的特性，每种特性代表镜像文件能支持哪些种功能，可被单独启用或禁用。
<ul>
<li><code>layering(+)</code>, 分层克隆</li>
<li><code>exclusive-lock</code> 排它锁，是否支持分布式排它锁，已用来限制一个镜像文件同时只能被一个客户端使用。</li>
<li><code>object-map(+*)</code> 对象映射,是否支持对象位图，主要用于加速导入、导出及已用容量统的。</li>
<li><code>fast-diff(+*)</code>, 快速比较，主要用于做快照之间的比较</li>
<li><code>deep-flatten(+-)</code> , 深层展评</li>
<li><code>journaling</code> 日志，用户在修改image数据时是否记录日志。
shared image</li>
</ul>
</li>
<li><code>--no-progress</code>：不显示创建过程</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd <span class="nb">help</span> create
</span></span><span class="line"><span class="cl">usage: rbd create <span class="o">[</span>--pool &lt;pool&gt;<span class="o">]</span> <span class="o">[</span>--image &lt;image&gt;<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--image-format &lt;image-format&gt;<span class="o">]</span> <span class="o">[</span>--new-format<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--order &lt;order&gt;<span class="o">]</span> <span class="o">[</span>--object-size &lt;object-size&gt;<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--image-feature &lt;image-feature&gt;<span class="o">]</span> <span class="o">[</span>--image-shared<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--stripe-unit &lt;stripe-unit&gt;<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--stripe-count &lt;stripe-count&gt;<span class="o">]</span> <span class="o">[</span>--data-pool &lt;data-pool&gt;<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--journal-splay-width &lt;journal-splay-width&gt;<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--journal-object-size &lt;journal-object-size&gt;<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--journal-pool &lt;journal-pool&gt;<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--thick-provision<span class="o">]</span> --size &lt;size&gt; <span class="o">[</span>--no-progress<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  &lt;image-spec&gt; 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Create an empty image.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ceph osd pool create kube <span class="m">64</span> <span class="m">64</span>
</span></span><span class="line"><span class="cl">ceph osd pool application <span class="nb">enable</span> kube rbd
</span></span><span class="line"><span class="cl">rbd pool init kube
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rbd create --size 2G --pool kube --image vol01 <span class="c1"># 使用各个选项</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rbd create --size 2G kube/vol02  <span class="c1"># &lt;image-spec&gt; </span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>获取对象更详细信息</p>
</blockquote>
<p>PARENT 父镜像</p>
<p>FMT 格式，一般只有v2</p>
<p>PROT</p>
<p>LOCK</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd ls -l --pool kube
</span></span><span class="line"><span class="cl">NAME   SIZE PARENT FMT PROT LOCK 
</span></span><span class="line"><span class="cl">vol01 <span class="m">5</span> GiB          <span class="m">2</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd ls -l --pool kube --format json --pretty-format
</span></span><span class="line"><span class="cl"><span class="o">[</span>
</span></span><span class="line"><span class="cl">    <span class="o">{</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;image&#34;</span>: <span class="s2">&#34;vol01&#34;</span>,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;size&#34;</span>: 5368709120,
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;format&#34;</span>: <span class="m">2</span>
</span></span><span class="line"><span class="cl">    <span class="o">}</span>
</span></span><span class="line"><span class="cl"><span class="o">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>获取镜像文件更详细一部镜像信息，可以使用<code>rbd info</code>命令</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd info kube/vol01
</span></span><span class="line"><span class="cl">rbd image <span class="s1">&#39;vol01&#39;</span>:
</span></span><span class="line"><span class="cl">        size <span class="m">5</span> GiB in <span class="m">1280</span> objects
</span></span><span class="line"><span class="cl">        order <span class="m">22</span> <span class="o">(</span><span class="m">4</span> MiB objects<span class="o">)</span>
</span></span><span class="line"><span class="cl">        id: 5e386b8b4567
</span></span><span class="line"><span class="cl">        block_name_prefix: rbd_data.5e386b8b4567
</span></span><span class="line"><span class="cl">        format: <span class="m">2</span>
</span></span><span class="line"><span class="cl">        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
</span></span><span class="line"><span class="cl">        op_features: 
</span></span><span class="line"><span class="cl">        flags: 
</span></span><span class="line"><span class="cl">        create_timestamp: Mon Jun <span class="m">24</span> 05:42:00 <span class="m">2019</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd info --pool kube --image vol01
</span></span><span class="line"><span class="cl">rbd image <span class="s1">&#39;vol01&#39;</span>:
</span></span><span class="line"><span class="cl">        size <span class="m">5</span> GiB in <span class="m">1280</span> objects
</span></span><span class="line"><span class="cl">        order <span class="m">22</span> <span class="o">(</span><span class="m">4</span> MiB objects<span class="o">)</span>
</span></span><span class="line"><span class="cl">        id: 5e386b8b4567
</span></span><span class="line"><span class="cl">        block_name_prefix: rbd_data.5e386b8b4567
</span></span><span class="line"><span class="cl">        format: <span class="m">2</span>
</span></span><span class="line"><span class="cl">        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
</span></span><span class="line"><span class="cl">        op_features: 
</span></span><span class="line"><span class="cl">        flags: 
</span></span><span class="line"><span class="cl">        create_timestamp: Mon Jun <span class="m">24</span> 05:42:00 <span class="m">2019</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd info --pool kube vol01        
</span></span><span class="line"><span class="cl">rbd image <span class="s1">&#39;vol01&#39;</span>:  <span class="c1"># 镜像文件是谁</span>
</span></span><span class="line"><span class="cl">        size <span class="m">5</span> GiB in <span class="m">1280</span> objects <span class="c1"># 镜像总体是多大空间，被分割成多少个对象</span>
</span></span><span class="line"><span class="cl">        order <span class="m">22</span> <span class="o">(</span><span class="m">4</span> MiB objects<span class="o">)</span> <span class="c1"># 顺序是22，顺序是指块大小的标识序号 # 如64k开始64m结束，4M大小正好排在第22位</span>
</span></span><span class="line"><span class="cl">        id: 5e386b8b4567 <span class="c1"># 镜像文件自身id</span>
</span></span><span class="line"><span class="cl">        block_name_prefix: rbd_data.5e386b8b4567
</span></span><span class="line"><span class="cl">        format: <span class="m">2</span> <span class="c1"># 镜像文件格式</span>
</span></span><span class="line"><span class="cl">        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten <span class="c1"># 启动的特性</span>
</span></span><span class="line"><span class="cl">        op_features:  
</span></span><span class="line"><span class="cl">        flags: 
</span></span><span class="line"><span class="cl">        create_timestamp: Mon Jun <span class="m">24</span> 05:42:00 <span class="m">2019</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="c1"># object-map, fast-diff, deep-flatten种特性在被linux客户端作为磁盘加载到内核中使用时是不被支持的。需将其禁用掉        </span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>禁用或启用某种特性 <code>rbd featrue</code></p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">rbd feature disable<span class="p">|</span><span class="nb">enable</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rbd feature disable kube/vol01 object-map fast-diff deep-flatten
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd info --pool kube vol01
</span></span><span class="line"><span class="cl">rbd image <span class="s1">&#39;vol01&#39;</span>:
</span></span><span class="line"><span class="cl">        size <span class="m">5</span> GiB in <span class="m">1280</span> objects
</span></span><span class="line"><span class="cl">        order <span class="m">22</span> <span class="o">(</span><span class="m">4</span> MiB objects<span class="o">)</span>
</span></span><span class="line"><span class="cl">        id: 5e386b8b4567
</span></span><span class="line"><span class="cl">        block_name_prefix: rbd_data.5e386b8b4567
</span></span><span class="line"><span class="cl">        format: <span class="m">2</span>
</span></span><span class="line"><span class="cl">        features: layering, exclusive-lock
</span></span><span class="line"><span class="cl">        op_features: 
</span></span><span class="line"><span class="cl">        flags: 
</span></span><span class="line"><span class="cl">        create_timestamp: Mon Jun <span class="m">24</span> 05:42:00 <span class="m">2019</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><strong>Ceph客户端使用块设备步骤</strong></p>
</blockquote>
<ul>
<li>创建出镜像文件。</li>
<li>在镜像文件上禁用默认启用的五个特性中的后三个。</li>
<li>在客户端上使用指定命令以指定的用户的身份连入客户端。</li>
<li>配置客户端上yum仓库文件</li>
</ul>
<p>ceph命令默认是以admin为账号的，使用<code>--user</code>指定其他用户。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph -s
</span></span><span class="line"><span class="cl">2019-06-25 10:48:11.283 7f999daa3700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
</span></span><span class="line"><span class="cl">2019-06-25 10:48:11.283 7f999daa3700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication
</span></span><span class="line"><span class="cl">[errno 2] error connecting to the cluster
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ceph --user kube -s
</span></span><span class="line"><span class="cl">  cluster:
</span></span><span class="line"><span class="cl">    id:     69fb9b55-3fb5-42d0-8cf7-239a3b569791
</span></span><span class="line"><span class="cl">    health: HEALTH_WARN
</span></span><span class="line"><span class="cl">            1 pool(s) full
</span></span><span class="line"><span class="cl">            application not enabled on 1 pool(s)
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">  services:
</span></span><span class="line"><span class="cl">    mon: 3 daemons, quorum stor01,stor02,stor03
</span></span><span class="line"><span class="cl">    mgr: stor01(active), standbys: stor04
</span></span><span class="line"><span class="cl">    mds: cephfs-1/1/1 up  {0=stor02=up:active}
</span></span><span class="line"><span class="cl">    osd: 8 osds: 8 up, 8 in
</span></span><span class="line"><span class="cl">    rgw: 1 daemon active
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">  data:
</span></span><span class="line"><span class="cl">    pools:   9 pools, 352 pgs
</span></span><span class="line"><span class="cl">    objects: 254  objects, 3.7 KiB
</span></span><span class="line"><span class="cl">    usage:   49 GiB used, 51 GiB / 100 GiB avail
</span></span><span class="line"><span class="cl">    pgs:     352 active+clean
</span></span></code></pre></td></tr></table>
</div>
</div><p>需要操作系统支持<code>ceph.ko</code>模块</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ modinfo ceph
</span></span><span class="line"><span class="cl">filename:       /lib/modules/3.10.0-957.12.1.el7.x86_64/kernel/fs/ceph/ceph.ko.xz
</span></span><span class="line"><span class="cl">license:        GPL
</span></span><span class="line"><span class="cl">description:    Ceph filesystem <span class="k">for</span> Linux
</span></span><span class="line"><span class="cl">author:         Patience Warnick &lt;patience@newdream.net&gt;
</span></span><span class="line"><span class="cl">author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;
</span></span><span class="line"><span class="cl">author:         Sage Weil &lt;sage@newdream.net&gt;
</span></span><span class="line"><span class="cl">alias:          fs-ceph
</span></span><span class="line"><span class="cl">retpoline:      Y
</span></span><span class="line"><span class="cl">rhelversion:    7.6
</span></span><span class="line"><span class="cl">srcversion:     43DA49DF11334B2A5652931
</span></span><span class="line"><span class="cl">depends:        libceph
</span></span><span class="line"><span class="cl">intree:         Y
</span></span><span class="line"><span class="cl">vermagic:       3.10.0-957.12.1.el7.x86_64 SMP mod_unload modversions 
</span></span><span class="line"><span class="cl">signer:         CentOS Linux kernel signing key
</span></span><span class="line"><span class="cl">sig_key:        2C:7C:17:70:5C:86:D4:20:80:50:D3:F5:54:56:9A:7B:D3:BF:D1:BF
</span></span><span class="line"><span class="cl">sig_hashalgo:   sha256
</span></span></code></pre></td></tr></table>
</div>
</div><p>显示设备map信息</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ rbd showmapped
</span></span><span class="line"><span class="cl">id pool image snap device    
</span></span><span class="line"><span class="cl">0  kube vol01 -    /dev/rbd0 
</span></span></code></pre></td></tr></table>
</div>
</div><p>显示设备挂在到哪里</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd showmapped
</span></span><span class="line"><span class="cl">id pool image snap device    
</span></span><span class="line"><span class="cl"><span class="m">0</span>  kube vol01 -    /dev/rbd0 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">NAME   SIZE PARENT FMT PROT LOCK 
</span></span><span class="line"><span class="cl">vol01 <span class="m">5</span> GiB          <span class="m">2</span>      excl 
</span></span></code></pre></td></tr></table>
</div>
</div><p>卸载挂载</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ umount /dev/rbd0  <span class="c1"># 需要先卸载rbd挂载</span>
</span></span><span class="line"><span class="cl">$ rbd unmap /dev/rbd0 
</span></span><span class="line"><span class="cl">$ rbd showmapped
</span></span></code></pre></td></tr></table>
</div>
</div><p>调整镜像文件空间容量</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$  rbd <span class="nb">help</span> resize 
</span></span><span class="line"><span class="cl">usage: rbd resize <span class="o">[</span>--pool &lt;pool&gt;<span class="o">]</span> <span class="o">[</span>--image &lt;image&gt;<span class="o">]</span> --size &lt;size&gt; 
</span></span><span class="line"><span class="cl">                  <span class="o">[</span>--allow-shrink<span class="o">]</span> <span class="o">[</span>--no-progress<span class="o">]</span> 
</span></span><span class="line"><span class="cl">                  &lt;image-spec&gt; 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Resize <span class="o">(</span>expand or shrink<span class="o">)</span> image.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Positional arguments
</span></span><span class="line"><span class="cl">  &lt;image-spec&gt;         image specification
</span></span><span class="line"><span class="cl">                       <span class="o">(</span>example: <span class="o">[</span>&lt;pool-name&gt;/<span class="o">]</span>&lt;image-name&gt;<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">Optional arguments
</span></span><span class="line"><span class="cl">  -p <span class="o">[</span> --pool <span class="o">]</span> arg    pool name
</span></span><span class="line"><span class="cl">  --image arg          image name
</span></span><span class="line"><span class="cl">  -s <span class="o">[</span> --size <span class="o">]</span> arg    image size <span class="o">(</span>in M/G/T<span class="o">)</span> <span class="o">[</span>default: M<span class="o">]</span> 
</span></span><span class="line"><span class="cl">  --allow-shrink       permit shrinking <span class="c1"># 允许收缩，不要收缩到已有数据占用容量、以下</span>
</span></span><span class="line"><span class="cl">  --no-progress        disable progress output
</span></span><span class="line"><span class="cl">  
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd resize -s 10G kube/vol01
</span></span><span class="line"><span class="cl">Resizing image: 100% complete...done.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd ls -p kube -l
</span></span><span class="line"><span class="cl">NAME    SIZE PARENT FMT PROT LOCK 
</span></span><span class="line"><span class="cl">vol01 <span class="m">10</span> GiB  
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>删除镜像文件</p>
</blockquote>
<p>镜像文件被删除是不可恢复的</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd rm kube/vol01
</span></span><span class="line"><span class="cl">Removing image: 100% complete...done.
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd ls -p kube -l
</span></span><span class="line"><span class="cl">$ 
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>rbd回收站trash</p>
</blockquote>
<p>将镜像挪入回收站中</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd ls -p kube -l
</span></span><span class="line"><span class="cl">NAME   SIZE PARENT FMT PROT LOCK 
</span></span><span class="line"><span class="cl">vol01 <span class="m">2</span> GiB          <span class="m">2</span>           
</span></span><span class="line"><span class="cl">vol02 <span class="m">2</span> GiB          <span class="m">2</span>       
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd trash move kube/vol01
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd ls -p kube -l       
</span></span><span class="line"><span class="cl">NAME   SIZE PARENT FMT PROT LOCK 
</span></span><span class="line"><span class="cl">vol02 <span class="m">2</span> GiB          <span class="m">2</span>           
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd trash list -p kube
</span></span><span class="line"><span class="cl">5ebf6b8b4567 vol01
</span></span></code></pre></td></tr></table>
</div>
</div><p>从trash恢复镜像</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ rbd trash list -p kube
</span></span><span class="line"><span class="cl">5ebf6b8b4567 vol01
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl">$ rbd trash restore -p kube --image vol01 --image-id 5ebf6b8b4567
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd ls -p kube 
</span></span><span class="line"><span class="cl">vol01
</span></span><span class="line"><span class="cl">vol02
</span></span></code></pre></td></tr></table>
</div>
</div><p>如何对镜像文件做快照、克隆</p>
<p>快照是什么？</p>
<p>快照是一种能够瞬间生成数据访问目录，能够支持备份技术的一种数据管理手段。有全量快照与增量快照两种。全量快照一般使用镜像分离技术实现，而增量快照则由<code>COW</code>写时拷贝快照技术<code>ROW</code>写时重定向。
写时复制是指，当写的时候，将数据复制到快照上去修改源数据；写时重定向是指，写的时候直接去写快照，改变快照上的数据，源卷上的数据不变。</p>
<p>snap相关命令</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>snap create (snap add)</td>
<td>创建快照</td>
</tr>
<tr>
<td>snap limit clear</td>
<td>清除快照数量限制</td>
</tr>
<tr>
<td>snap limit set</td>
<td>设定一个镜像文件上所能够创建的快照问价你的上线</td>
</tr>
<tr>
<td>snap list (snap ls)</td>
<td>列出快照</td>
</tr>
<tr>
<td>snap protect</td>
<td>保护快照</td>
</tr>
<tr>
<td>snap purge</td>
<td>清理快照</td>
</tr>
<tr>
<td>snap remove (snap rm)</td>
<td>移除快照</td>
</tr>
<tr>
<td>snap rename</td>
<td>重命名快照</td>
</tr>
<tr>
<td>snap rollback (snap revert)</td>
<td>回滚快照，做快照恢复</td>
</tr>
<tr>
<td>snap unprotect</td>
<td>解除保护，protect的相反操作。</td>
</tr>
</tbody>
</table>
<p>为镜像创建快照</p>
<p><code>rbd snap create</code>只需为哪个镜像创建什么快照就可以了</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">rbd snap create kube/vol01@snap1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd snap list kube/vol01
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">SNAPID NAME   SIZE TIMESTAMP                
</span></span><span class="line"><span class="cl">     <span class="m">4</span> snap1 <span class="m">2</span> GiB Thu Jun <span class="m">27</span> 14:00:36 <span class="m">2019</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>快照恢复</p>
<p>首先先将服务停止（拆除）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">umount /rdb <span class="c1"># 卸载挂载</span>
</span></span><span class="line"><span class="cl">rbd unmap /dev/rbd0 <span class="c1"># 拆除连接</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>回到管理节点做回滚操作</p>
<p>使用<code>rbd snap rollback</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd snap rollback kube/vol01@snap1
</span></span><span class="line"><span class="cl">Rolling back to snapshot: 100% complete...done.
</span></span></code></pre></td></tr></table>
</div>
</div><p>回到客户端再一次将映射挂载访问</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">rbd --user kube map /dev/rbd0 <span class="c1"># 映射</span>
</span></span><span class="line"><span class="cl">mount /dev/rbd0 /rdb <span class="c1"># 挂载</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 查看数据</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ls
</span></span><span class="line"><span class="cl">lost+found  passwd
</span></span></code></pre></td></tr></table>
</div>
</div><p>删除快照</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">rbd snap rm kube.vol01@snap1
</span></span></code></pre></td></tr></table>
</div>
</div><p>限制镜像文件所能够创建的快照数量</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">rbd snap limit <span class="nb">set</span> kube/vol01 --limit <span class="m">2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$  rbd snap create kube/vol01@snap03
</span></span><span class="line"><span class="cl">rbd: failed to create snapshot: <span class="o">(</span>122<span class="o">)</span> Disk quota exceeded
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rbd snap limit clear kube/vol01 <span class="c1"># 清除限制</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>多层快照技术</p>
<p>对原始镜像文件做一次快照，将此快照置于保护模式下。快照所能访问的数据一定是做快照那一刻原始卷上的所有数据的。就算修改原始卷内容，也不会影响快照访问。基于快照再做一层快照。而二级快照是可以当镜像来用的。因此基于保护快照可以创建N个克隆。每一个克隆都可以按照自己的意愿去做希望所做的后续修改操作。</p>
<p>模板池，提供克隆模板的。</p>
<p>工作流程</p>
<ul>
<li>创建镜像</li>
<li>对镜像做快照</li>
<li>对快照做保护</li>
<li>对快照做克隆</li>
</ul>
<p>在管理断创建快照，将此快照当做保护模式下的快照，当做创建克隆时的模板</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd snap create kube/vol01@clonetpl1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd snap ls
</span></span><span class="line"><span class="cl">rbd: image name was not specified
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd snap ls kube/vol01
</span></span><span class="line"><span class="cl">SNAPID NAME        SIZE TIMESTAMP                
</span></span><span class="line"><span class="cl">     <span class="m">4</span> snap1      <span class="m">2</span> GiB Thu Jun <span class="m">27</span> 14:00:36 <span class="m">2019</span> 
</span></span><span class="line"><span class="cl">     <span class="m">5</span> snap02     <span class="m">2</span> GiB Thu Jun <span class="m">27</span> 14:41:25 <span class="m">2019</span> 
</span></span><span class="line"><span class="cl">     <span class="m">8</span> clonetpl1 <span class="m">10</span> GiB Thu Jun <span class="m">27</span> 20:43:48 <span class="m">2019</span> 
</span></span></code></pre></td></tr></table>
</div>
</div><p>置入保护模式</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd snap protect kube/vol01@clonetpl1
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rbd snap unprotect kube/vol01:clonetpl1 <span class="c1"># 解除保护</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>给快照再次做快照，称之为clone。==可支持跨存储池进行克隆==</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd clone kube/vol01@clonetpl1 test/myimg01 <span class="c1"># 是一个镜像，可被当做真正镜像使用</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ rbd ls -p <span class="nb">test</span>
</span></span><span class="line"><span class="cl">myimg01
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>rbd chlidren</code>显示一个快照的子项</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd children kube/vol01@clonetpl1  
</span></span><span class="line"><span class="cl">kube/myimg01
</span></span><span class="line"><span class="cl">test/myimg01
</span></span></code></pre></td></tr></table>
</div>
</div><p>数据展平 flatten</p>
<p>当vol01被删除时，他的上层快照myimg01就无根了，因此需要做数据展平，已确保他所应用的每一个底层快照上的所有数据都被复制到当前镜像中。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ rbd flatten kube/myimg01
</span></span><span class="line"><span class="cl">Image flatten: 100% complete...done.
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph对象存储概述</title>
      <link>https://www.oomkill.com/2019/07/05-1-rgw/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/07/05-1-rgw/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="什么是对象存储">什么是对象存储</h2>
<p>对象存储是一种以非结构化格式（称为对象），简单来说，对象存储是一种将文件存储为对象而不是数据块的存储架构。它是一种将非结构化数据存储在跨位置分布的结构化平面文件系统中的方法。在这种格式中，文件空间由元数据标签组成，支持简单的 API 来描述、读取、删除和定位对象。因此，您可以通过 API 协议直接访问任何设备上保存的数据。此类元数据标签包括有助于更好地识别和分类数据的唯一标识符。</p>
<p>这些元数据标签是高度可定制的，让您可以在需要时通过跟踪和索引文件来轻松组织、访问和检索所有数据。对象存储服务可以在设备级、系统级甚至接口级实现。作为对象存储的数据可确保数据可用性、可搜索性并增强数据安全性，因为它可以保护数据免遭意外删除或损坏。</p>
<h2 id="什么是-ceph-中的对象存储">什么是 CEPH 中的对象存储</h2>
<p>在知道了对象存储不能作为文件系统磁盘由操作系统直接访问，只可以通过应用程序级别的 API 进行访问。Ceph是一个分布式对象存储系统，通过一个 “网关服务” 来提供对象存储接口，这个服务被称为 RADOS Gateway ( <em>RGW</em> )，RGW是构建在 Ceph RADOS 之上，通过在 <strong>librados</strong> 之构建出的一个库 <strong>librgw</strong>，实际上是一个 <em>Civetweb</em> 的服务，rados gateway 内嵌在里面，RGW 为应用程序提供兼容 <em>RESTful</em> 的 <em>S3/Swift</em> 的 API 接口，以在 Ceph 集群中以对象的形式存储数据。Ceph 还支持<strong>多租户</strong>对象存储，可通过 RESTful API 访问。除此之外，RGW 还支持 Ceph 管理 API，可用于使用本机 API 调用来管理 Ceph 存储集群。</p>
<p>librados 是一个构建在 RADOS 集群和 Ceph 集群的中间层，通过这个库，提供了允许用户应用程序通过C、C++、Java、Python 和 PHP绑定直接访问 Ceph 存储集群。Ceph 对象存储还具有多站点 (<em>MultiSite</em>) 能力，即提供灾难恢复的解决方案。</p>
<p>
  <img loading="lazy" src="C:%5cUsers%5cCylon%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5cimage-20230912231513576.png" alt="image-20230912231513576"  /></p>
<center>图：Ceph RGW Structure</center>
<center><em>Source：</em>https://docs.ceph.com/en/octopus/radosgw/</center><br>
<p>安装rgw</p>
<p>rgw 包 ceph-radosgw</p>
<p>ceph中的对象存储是使用HTTP服务与Ceph群集进行交互的（<code>radosgw</code>）由<code>radosgw</code> 提供与<code>OpenStack Swift</code>和<code>Amazon S3</code>兼容的接口，因此<code>radosgw</code>具有自己的管理用户。</p>
<p>![](../../../images/ceph object store/1ae399f8fa9af1042d3e1cbf31828f14eb3fe01a6eb3352f88c3d2a04ac4dc50.png)</p>
<p>ceph-deploy rgw create cn01</p>
<p>修改默认端口</p>
<p>创建用户</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ radosgw-admin user create --uid demo --display-name &#34;seal&#34;
</span></span><span class="line"><span class="cl">{
</span></span><span class="line"><span class="cl">    &#34;user_id&#34;: &#34;demo&#34;,
</span></span><span class="line"><span class="cl">    &#34;display_name&#34;: &#34;seal&#34;,
</span></span><span class="line"><span class="cl">    &#34;email&#34;: &#34;&#34;,
</span></span><span class="line"><span class="cl">    &#34;suspended&#34;: 0,
</span></span><span class="line"><span class="cl">    &#34;max_buckets&#34;: 1000,
</span></span><span class="line"><span class="cl">    &#34;subusers&#34;: [],
</span></span><span class="line"><span class="cl">    &#34;keys&#34;: [
</span></span><span class="line"><span class="cl">        {
</span></span><span class="line"><span class="cl">            &#34;user&#34;: &#34;demo&#34;,
</span></span><span class="line"><span class="cl">            &#34;access_key&#34;: &#34;9REMQZ4789I85QJFW93I&#34;,
</span></span><span class="line"><span class="cl">            &#34;secret_key&#34;: &#34;akYcu5ncvgMxCljTNGrGTHUBbMzjJmETfxtBW7SX&#34;
</span></span><span class="line"><span class="cl">        }
</span></span><span class="line"><span class="cl">    ],
</span></span></code></pre></td></tr></table>
</div>
</div><p>radosgw-admin user list</p>
<p><code>ERROR: S3 error: 416 (InvalidRange)</code></p>
<p>解决办法：</p>
<p>将其他对象存储后台驻留程序（OSD）添加到群集或将“mon_max_pg_per_osd”的默认值增加到300以上。</p>
<p>修改配置文件（管理节点操作）：</p>
<p>$ vim ceph.conf</p>
<p>[mon]
mon allow pool delete = true
mon_max_pg_per_osd = 300</p>
<p>$ ceph-deploy &ndash;overwrite-conf config push c720181 c720182 c720183</p>
<p>重启mon服务（集群mon节点都要操作）：</p>
<p>systemctl restart ceph-mon.target</p>
<p><a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-March/033609.html">416</a></p>
<p>set default pg_num and pgp_num to lower value(8 for example), or set mon_max_pg_per_osd to a high value in ceph.conf</p>
<p><a href="https://stackoverflow.com/questions/48440768/ceph-s3-swift-bucket-create-failed-error-416">416</a></p>
<p><code>403 (SignatureDoesNotMatch)</code> 修改配置文件   <code>signature_v2 = True</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">s3cmd ls 
</span></span><span class="line"><span class="cl">s3cmd ls s3://demob
</span></span><span class="line"><span class="cl"># 创建bucket
</span></span><span class="line"><span class="cl">s3cmd mb s3://demob
</span></span><span class="line"><span class="cl"># 上传目录
</span></span><span class="line"><span class="cl">s3cmd put /etc/ s3://demob/etc --recursive
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph文件系统概述</title>
      <link>https://www.oomkill.com/2019/07/04-1-cephfs/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/07/04-1-cephfs/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>Ceph专门提供了文件系统接口<code>CephFS</code>。CephFS是略微不同于RBD的架构形式，在基础的RADOS Cluster存储集群的基础之上，需要额外运行一个守护进程<code>MDS</code> MetaDataServer 元数据服务器。</p>
<p><code>RADOS Cluster</code>自己是一个对象存储服务，他无法管理传统文件系统上分离去管理元数据和数据的功能。（而且元数据还拥有权限、用户属组、时间戳等。）RADOS Cluster自身是无法实现这个功能的。MDS专用于模拟传统文件系统所应该具有的将数据和元数据分离存储的方式而专门提供的一个服务。MDS只用来管理元数据。</p>
<p>MDS需要工作一个守护进程，客户端必须通过套接字的方式，每一次访问文件时，先去联系到MDS，来获取文件的元数据信息，再到RADOS Cluster以对象方式，将对象模拟成传统文件系统的块，来加载文件数据。</p>
<p>如是挂在CephFS系统的客户端，需先联系到MDS，识别文件系统的各种信息。客户端向挂载目录路径下的任何一个读写操作就相当于由挂载时的内核驱动模块联系到对应的MDS，而后再由客户端之上的模块来联系到RADOS Cluster。为了能够支持CephFS文件系统，也需要内核级模块Ceph.ko模块。挂载过程机制为将内核模块作为文件系统内核的客户端，与文件系统的守护进程进行通讯，必要时将用户数据存取转为对应集群的存储操作。</p>
<p>对于Rados存储集群来讲，存储集群所有数据都会被放在存储池当中，而CephFS管理其数据和元数据分别放置在不同的存储池中。所有元数据都是由MDS管理的。MDS也是客户端，连入CephFS的的metadata，专门用于存储元数据的存储池。</p>
<p>cephfs逻辑</p>
<p>元数据是一类很密集的IO访问。对原数据存储池的操作是放置在存储池当中的，但在本地会使用内存（高速缓存）中完成，过断时间同步到metadata pool中。而数据直接写入data存储池中</p>
<p>meatdata pool只能对mds访问，其他任何客户端时不能被访问的。客户端对元数据的访问必须经由MDS来实现。</p>
<p>当客户端打开一个文本时，首先请求客户端的inode（传统文件系统采用inode来保存文件的元数据）。获得相应授权以后从mds中接收到inode，inode中标示文件的数据究竟放置在data pool的那些对象中。返回对象编号给客户端，客户端基于对象编号访问所有的对象，将数据加载到。</p>
<p>ceph mds  stat</p>
<p><code>mds:  2 up:standby</code> 当没有文件系统时，无法进行选举，故所有的都为standby</p>
<p>CephFS Client访问CephFS集群的方式</p>
<ul>
<li>客户端挂载CephFS，
<ul>
<li>基于内核文件系统完成挂载<code>ceph</code>、<code>libcephfs</code></li>
<li>用户空间文件系统（FUSE <code>Filesystem in USErspace</code>）：libcephfs与ceph集群进行交互。</li>
</ul>
</li>
</ul>
<blockquote>
<p>==<strong>激活CephFS步骤</strong>==</p>
</blockquote>
<ul>
<li>
<p>激活CephFS MDS，至少有一个节点运行ceph-mds守护进程</p>
<ul>
<li><code>ceph-deploy</code>命令完成的</li>
</ul>
</li>
<li>
<p>创建存储池：metadata-pool、data-pool</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ceph osd pool create cephfs_metadata <span class="m">8</span> <span class="m">8</span>
</span></span><span class="line"><span class="cl">ceph osd pool create cephfs_data <span class="m">8</span> <span class="m">8</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>激活文件系统：</p>
<ul>
<li><code>ceph fs new &lt;name&gt; {metadata-pool-name} {pool-name}</code></li>
<li><code>ceph fs status {filesystem-name}</code></li>
<li><code>ceph fs stat</code></li>
</ul>
</li>
<li>
<p>获得必要授权，才能使用服务</p>
<ul>
<li><code>ceph auth get-or-create client.fsclient mon 'allow r' mds 'allow rw' osd 'allow rwx pool=cephfs-data' -o ceph.client.fsclient.keyring</code></li>
</ul>
</li>
<li>
<p>客户端挂载</p>
<ul>
<li>客户端挂载时key和用户名要分开两个不通选项来指定的。</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph-authtool -p -n client.fsclient ceph.client.fsclient.keyring 
</span></span><span class="line"><span class="cl">AQATNhZdmHEBIBAA52a2MESQJS8mMScxPzRkIA==
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ceph auth print-key client.fsclient
</span></span><span class="line"><span class="cl">AQATNhZdmHEBIBAA52a2MESQJS8mMScxPzRkIA==
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ceph auth print-key client.fsclient &gt;fsclient.key # 此key文件是被客户端使用的。需要复制到客户端
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">scp fsclient.key root@172.18.0.6:/etc/ceph
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<ol>
<li>确保安装ceph-common客户端</li>
<li>确保内核中有ceph模块<code>ceph.ko</code>，此模块必须存在才能使用Ceph客户端</li>
<li>确定可获取到Ceph集群的配置文件<code>ceph.conf</code>。</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ mount -t ceph stor01:6789,stor02:6789,stor03:6789:/ /data -o <span class="nv">name</span><span class="o">=</span>fsclient,secretfile<span class="o">=</span>/etc/ceph/fsclient.key
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># stat 跟挂载点可查看文件系统类型</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ stat -f /data
</span></span><span class="line"><span class="cl">  文件：<span class="s2">&#34;/data&#34;</span>
</span></span><span class="line"><span class="cl">    ID：cfb80ce541d5e304 文件名长度：255     类型：ceph
</span></span><span class="line"><span class="cl">块大小：4194304    基本块大小：4194304
</span></span><span class="line"><span class="cl">    块：总计：1061       空闲：1061       可用：1061
</span></span><span class="line"><span class="cl">Inodes: 总计：0          空闲：-1
</span></span></code></pre></td></tr></table>
</div>
</div><p>如需开机自启动需写入<code>/etc/fstab</code>下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">stor01:6789,stor02:6789,stor03:6789:/     /data                   ceph    name=fsclient,secretfile=/etc/ceph/fsclient.key,_netdev,noatime 0 0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">umount /data
</span></span><span class="line"><span class="cl">mount -a # 自动挂载   
</span></span></code></pre></td></tr></table>
</div>
</div><p>如内核级没有提供ceph模块，就无法基于内核文件系统形式挂载和使用ceph客户端， 此时就只能使用<code>ceph-fuse</code>客户端。</p>
<p>fuse不要求内核级必须有相应的内核模块，但要求在用户空间安装一个程序包<code>ceph-fuse</code>ceph以用户空间形式的文件系统逻辑来挂载ceph。无需ceph-common并且一样需提供用于认证的客户端账号。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ ceph-fuse -n client.fsclient -m stor01:6789,stor02:6789,stor03:6789 /data
</span></span><span class="line"><span class="cl">2019-06-29 16:09:14.537 7f42c9515c00 -1 init, <span class="nv">newargv</span> <span class="o">=</span> 0x55baceded440 <span class="nv">newargc</span><span class="o">=</span><span class="m">7</span>
</span></span><span class="line"><span class="cl">ceph-fuse<span class="o">[</span>50438<span class="o">]</span>: starting ceph client
</span></span><span class="line"><span class="cl">ceph-fuse<span class="o">[</span>50438<span class="o">]</span>: starting fuse
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<p>注：内核只要支持应该使用内核级，用户空间级的性能上比不上内核级文件系统。</p>
<hr>
<blockquote>
<p>开机自动挂载</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">none  /data  fuse.ceph ceph.id=fsclient,ceph.conf=/etc/ceph/ceph.conf,_netdev,default 0 0
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="cephfs工作模型">CephFS工作模型</h4>
<p>文件元数据的工作负载通常是一类小而密集的IO请求，因此很难实现类似数据读写IO那样的扩展方式。</p>
<p>分布式文件系统业界提供了将名称空间分割治理的解决方案，通过将文件系统根树及其热点子树分别存储于不同的元数据服务器进行负载均衡，从而赋予了元数据存储线性扩展的可能</p>
<ul>
<li>静态子树分区，固定的不灵活</li>
<li>静态hash分区，对文件名或目录进行hash运算</li>
<li>惰性混编分区，将静态hash和传统文件方式混合使用</li>
<li>动态子树分区，</li>
</ul>
<p></p>
<h3 id="multi-mds">Multi MDS</h3>
<p>多主MDS模式是指CephFS将整个文件系统的名称空间切分为多个子树并配置到多个MDS之上，不过，读写操作的负载均衡策略分别是子树切分和目录副本</p>
<ul>
<li>将写操作负载较重的目录切分成多个子目录以分散负载</li>
<li>为读操作负载较重的目录创建多个副本以均衡负载</li>
</ul>
<p>子树分区和迁移的决策是一个同步过程，各MDS每10秒钟做一次独立的迁移决策，每个MDS并不存在一个一致的名称空间视图，且MDS集群也不存在一个全局调度器负责统一的调度决策</p>
<p>各MDS彼此间通过交换心跳信息（HeartBeat，简称HB）及负载状态来确定是否要进行迁移、如何分区名称空间，以及是否需要目录切分为子树等</p>
<ul>
<li>管理员也可以配置CephFS负载的计算方式从而影响MDS的负载决策，目前，CephFS支持基于CPU负载、文件系统负载及混合此两种的决策机制</li>
</ul>
<p>动态子树分区依赖于共享存储完成热点负载在MDS间的迁移，于是Ceph把MDS的元数据存储于后面的RADOS集群上的专用存储池中，此存储池可由多个MDS共享</p>
<p>·MIDS对元数据的访问并不直接基于RADOS进行，而是为其提供了一个基于内存的缓存区以缓存热点元数据，并且在元数据相关日志条目过期之前将一直存储于内存中</p>
<p>CephFS使用元数据日志来解决容错问题
·元数据日志信息流式存储于CephFS元数据存储池中的元数据日志文件上，类似于LFS（Log-Structured File System）和WAFL（Write Anywhere File Layout）的工作机制，
·CephFS元数据日志文件的体积可以无限增长以确保日志信息能顺序写入RADOS，并额外赋予守护进程修剪冗余或不相关日志条目的能力</p>
<p>每个CephFS都会有一个易读的文件系统名称和一个称为FSCID标识符ID，并且每个CephFS默认情况下都只配置一个Active MDS守护进程</p>
<p>一个MDS集群中可处于Active状态的MDS数量的上限由<code>max_mds</code>参数配置，它控制着可用的rank数量，默认值为1</p>
<ul>
<li>rank是指CephFS上可同时处于Active状态的MDS守护进程的可用编号，其范围从0到<code>max mds-1</code></li>
<li>一个rank编号意味着一个可承载CephFS层级文件系统==目录子树==元数据管理功能的<code>Active</code>状态的<code>ceph-mds</code>守护进程编制，max_mds的值为1时意味着仅有一个0号rank可用。</li>
<li>刚启动的ceph-mds守护进程没有接管任何rank，它随后由MON按需进行分配</li>
<li>一个ceph-mds一次仅可占据一个rank，并且在守护进程终止时将其释放；一个rank只能被一个MDS所占用，被占用后其他MDS就不能再使用它了。</li>
<li>如果MDS名额数量少于进程数量，多余出来的进程只能处于备用模式。</li>
<li>一个rank可以处于下列三种状态中的某一种：
<ul>
<li><strong><code>Up</code></strong>：rank已经由某个ceph-mds守护进程接管</li>
<li><strong><code>Failed</code></strong>：rank未被任何ceph-mds守护进程接管</li>
<li><strong><code>Damaged</code></strong>：rank处于损坏状态，其元数据处于崩溃或丢失状态；在管理员修复问题并对其运行ceph mds repaired”命令之前，处于Damaged状态的rank不能分配给其它任何MDS守护进程</li>
</ul>
</li>
</ul>
<p>当新添加osd时，数据会将pg分配到新的osd上，如果此时影响集群访问。可以添加flag<code> nobackfill：禁止数据回填</code> ，<code>norebalance：禁止重平衡数据。在执行集群维护或者停机时，可以使用该flag</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ceph osd set nobackfill # 增加
</span></span><span class="line"><span class="cl">ceph osd unset nobackfill # 取消
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph安全 - CephX</title>
      <link>https://www.oomkill.com/2019/06/07-1-cephx/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/06/07-1-cephx/</guid>
      <description></description>
      <content:encoded><![CDATA[<p>如果需要与osd打交道，需要通过mon检索集群运行图才能够访问的客户端，通常经由mon认证后才能访问ceph存储。Ceph本身实现了数据服务的认证访问和授权控制机制，CephX协议来实现</p>
<p>CephX Protocol</p>
<p>CephX本身只负责认证和授权检测，不处理通讯过程是否加密。一般来讲需要与moniotr交互的客户端组件（<code>OSD</code>、<code>RBD</code>、<code>RGW</code>等）一般而言都经由CephX认证</p>
<h2 id="cephx认证机制">CephX认证机制</h2>
<p>Ceph使用cephx协议对客户端进行身份认证</p>
<ul>
<li>每个MON都可以对客户端进行身份验正并分发密钥，不存在单点故障和性能瓶颈。（在集群模式下，任何一个monitor在实现认证时是无状态的，每个monitor都能完成身份检验的任务）</li>
<li>MON会返回用于身份验正的数据结构，其包含获取Ceph服务时用到的session key
<ul>
<li>session key通过客户端密钥进行加密，需事先有一个预共享秘钥存在</li>
<li>客户端使用session key向MON请求所需的服务。session key只是拿来做中间通讯使用。</li>
<li>MON向客户端提供一个ticket，用于向实际处理数据的OSD等验正客户端身份。</li>
<li>MON和OSD共享同一个secret，因此OSD会信任由MON发放的ticket</li>
<li>ticket存在有效期限</li>
</ul>
</li>
</ul>
<hr>
<p>注意：</p>
<ul>
<li>CephX身份验正功能仅限制Ceph的各组件之间，它不能扩展到其它非Ceph组件。</li>
<li>它并不解决数据传输加密的问题。</li>
</ul>
<hr>
<p>为了实现Cephx认证，Ceph服务器端一定会为每一个客户端事先生成一个密码</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20240511130549971.png" alt="image-20240511130549971"  /></p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/cylonchau/imgbed/img/image-20240511130516753.png" alt="image-20240511130516753"  /></p>
<h2 id="认证与授权">认证与授权</h2>
<p>无论Ceph客户端时何类型，Ceph都会在存储池中将所有的数据存储为对象</p>
<p>Ceph用户需要拥有存储池访问权限才能读取和写入数据</p>
<p>Ceph用户必须拥有执行全年才能使用Ceph管理命令</p>
<p>相关概念</p>
<blockquote>
<p><strong>用户</strong></p>
</blockquote>
<ul>
<li>用户是指个人或系统参与者（例如应用）</li>
<li>通过创建用户，可以控制谁（或哪个参与者）能够访问Ceph存储集群、以及可访问的存储池及存储池中的数据。</li>
<li>Ceph支持多种类型的用户，单可管理的用户都属于Client类型
<ul>
<li>区分用户种类的原因在于，mon、osd、mds等系统组件也使用cephx协议，但它们非为客户端</li>
<li>通过点号来分隔用户类型和用户名，格式为<code>TYPE.ID</code>，例如：<code>client.admin</code>等</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>授权</strong></p>
</blockquote>
<blockquote>
<p><strong>使能</strong>（Capabilities）</p>
</blockquote>
<ul>
<li>Ceph基于&quot;使能(caps)&ldquo;来描述用户可针对MON、OSD或MDS使用的权限范围或级别。</li>
<li>通用语法格式：<code>daemon-type'allow caps[...]</code></li>
<li>MON使能
<ul>
<li>包括r、w、x和allow profile cap</li>
<li>例如：mon <code>allow rwx</code>，以及mon <code>allow profile</code> osd&rsquo;等。</li>
</ul>
</li>
<li>OSD使能
<ul>
<li>包括r、w、x、<code>class-read</code>、<code>class-write</code>和<code>profile osd</code></li>
<li>此外，OSD使能还允许进行存储池和名称空间设置。如为指定表示对所有OSD所有存储池都获得相关授权</li>
</ul>
</li>
<li>MDS使能
<ul>
<li>只需要allow，或留空</li>
</ul>
</li>
</ul>
<p>各使能的意义</p>
<ul>
<li>
<p><strong><code>allow</code></strong></p>
<ul>
<li>需先于守护进程的访问设置指定</li>
<li>仅对MDS表示rw之意，其它的表示字面意义</li>
</ul>
</li>
<li>
<p><strong><code>r</code></strong>：读取权限，访问MON以检索CRUSH时依赖此使能</p>
</li>
<li>
<p><strong><code>w</code></strong>：对象写入权限</p>
</li>
<li>
<p><strong><code>x</code></strong>：调用类方法（读取和写入）的能力，以及在MON上执行auth操作的能力。</p>
</li>
<li>
<p>class-read：x能力的子集，授予用户调用类读取方法的能力</p>
</li>
<li>
<p>class-write：x的子集，授予用户调用类写入方法的能力</p>
</li>
<li>
<p><strong><code>·*</code></strong>：授予用户对特定守护进程/存储池的读取、写入和执行权限，以及执行管理命令的能力。</p>
</li>
<li>
<p><strong><code>profile osd</code></strong></p>
<ul>
<li>授予用户以某个OSD身份连接到其他OSD或监视器的权限</li>
<li>授予OSD权限，使OSD能够处理复制检测信号流量和状态报告</li>
</ul>
</li>
<li>
<p><strong><code>profile mds</code></strong></p>
<ul>
<li>授予用户以某个MDS身份连接到其他MDS或监视器的权限</li>
</ul>
</li>
<li>
<p><strong><code>profile bootstrap-osd</code></strong></p>
<ul>
<li>授予用户引导OSD的权限</li>
<li>授权给部署工具，使其在OSD加入集群时，引导OSD时有权添加密钥</li>
</ul>
</li>
<li>
<p><strong><code>profile bootstrap-mds</code></strong></p>
<ul>
<li>授予用户引导元数据服务器的权限</li>
<li>授权给部署工具，使其在引导元数据服务器时有权添加密钥</li>
</ul>
</li>
</ul>
<h4 id="用户管理">用户管理</h4>
<ul>
<li>
<p>keyring 保存在ceph集群内部的整体信息，统一的账号存储文件，存放多个用户及其秘钥信息</p>
</li>
<li>
<p><strong>keyring file</strong>：单独被导出的文件，如<code>ceph.client.admin.keyring</code></p>
</li>
<li>
<p>Ceph集群管理员能够直接在Ceph集群中创建、更新和删除用户·创建用户时，可能需要将密钥分发到客户端，以便将密钥添加到==密钥环==</p>
</li>
<li>
<p><strong>列出用户</strong></p>
<ul>
<li>命令：<code>ceph auth list</code></li>
<li>用户标识：<code>TYPE.ID</code>，因此，osd.0表示OSD类型的用户 （osd专用于系统参与者不是真正客户端用户）。，用户ID为0</li>
</ul>
</li>
<li>
<p><strong>检索特定用户</strong></p>
<ul>
<li>命令：<code>ceph auth get TYPE.ID</code>或者<code>ceph auth export TYPE.ID</code></li>
</ul>
</li>
<li>
<p><strong>添加用户</strong></p>
<ul>
<li>
<p><code>ceph auth add</code>：规范方法，它能够创建用户、生成密钥并添加指定的caps
<code>ceph auth get-or-create</code>：简便方法，创建用户并返回密钥文件格式的密钥信息，或者在用户存在时返回用户名及密钥文件格式的密钥信息</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph auth add client.testuser mon &#39;allow r&#39; osd &#39;allow rw pool=rbdpool&#39; 
</span></span><span class="line"><span class="cl">added key for client.testuser
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ceph auth get client.testuser
</span></span><span class="line"><span class="cl">exported keyring for client.testuser
</span></span><span class="line"><span class="cl">[client.testuser]
</span></span><span class="line"><span class="cl">    key = AQBuyw5d+GcJBxAAYF3dkO4qcAA/B/gOt91T1Q==
</span></span><span class="line"><span class="cl">    caps mon = &#34;allow r&#34;
</span></span><span class="line"><span class="cl">    caps osd = &#34;allow rw pool=rbdpool&#34;
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><code>ceph auth get-or-create-key</code>：简便方法，创建用户并返回密钥信息，或者在用户存在时返回密钥信息</p>
</li>
</ul>
</li>
</ul>
<hr>
<p>注意：典型的用户至少对Ceph monitor 具有读取功能，并对Ceph OSD具有读取和写入功能；另外，用户的OSD权限通常应该限制为只能访问特定的存储池，否则，他将具有访问集群中所有存储池的权限</p>
<hr>
<ul>
<li>
<p><strong>列出用户秘钥</strong></p>
<ul>
<li><code>ceph auth print-key {TYPE.ID}</code></li>
</ul>
</li>
<li>
<p><strong>导入用户</strong></p>
<ul>
<li><code>ceph auth import</code> 需要指定秘钥环</li>
</ul>
</li>
<li>
<p><strong>修改用户caps</strong></p>
<ul>
<li><code>ceph auth caps</code></li>
<li>会覆盖用户现有的caps，因此建立事先使用<code>ceph auth get {TYPE.ID}</code>命令查看用户的caps</li>
<li>若是为添加caps，则无需先指定现有的caps</li>
<li>命令格式：<code>ceph auth caps {TYPE.ID} daemon 'allow [r|w|x|*|...] [{pool=pool-name}]</code></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ceph auth caps client.testuser mon &#39;allow rw&#39; osd &#39;allow rw pool=rbdpool&#39;
</span></span><span class="line"><span class="cl">updated caps for client.testuser
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ceph auth get client.testuser
</span></span><span class="line"><span class="cl">exported keyring for client.testuser
</span></span><span class="line"><span class="cl">[client.testuser]
</span></span><span class="line"><span class="cl">        key = AQBuyw5d+GcJBxAAYF3dkO4qcAA/B/gOt91T1Q==
</span></span><span class="line"><span class="cl">        caps mon = &#34;allow rw&#34;
</span></span><span class="line"><span class="cl">        caps osd = &#34;allow rw pool=rbdpool&#34;
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p><strong>删除用户</strong></p>
<ul>
<li><code>ceph auth del {TYPE.ID}</code></li>
</ul>
</li>
</ul>
<h4 id="keyring">keyring</h4>
<p>keyring是一个集合，能够同时存储secret、password、keys、certificates并且使得他们能够被某一个应用程序能用的文件的集合。</p>
<p>任何一个客户端在联系monitor时，会查找本地适用于当前应用程序的keyring文件，以获取自己能够认证的ceph集群的认证信息。</p>
<p><code>ceph-authtool</code>用来创建、修改、查看keyring文件的内容</p>
<p>访问Ceph集群时，客户端会于本地查找密钥环，默认情况下，Ceph会使用以下四个密钥环名称预设密钥环</p>
<ul>
<li>/etc/ceph/{cluster-name}.{user-name}.keyring: 保存单个用户的keyring</li>
<li>/etc/ceph/cluster.keyring： 保存多个用户的keyring</li>
<li>/etc/ceph/keyring</li>
<li>/etc/ceph/keyring.bin：二进制格式，被编码后</li>
</ul>
<p>{cluster-name}是为集群名称，{user-name}是为用户表示{TYPE.ID}
client.admin用户的在名为ceph的集群上的密钥环文件名为<code>ceph.client.admin.keyring</code></p>
<h4 id="管理keyring">管理keyring</h4>
<blockquote>
<p><strong>创建keyring</strong></p>
</blockquote>
<ul>
<li><code>ceph auth add</code>等命令添加的用户还需要额外使用ceph-authtool命令为其创建用户秘钥</li>
<li>ceph客户端通过keyring文件查找用户名并检索秘钥，命令：<code>ceph-authtool --create-keyring /path/to/keyring</code></li>
</ul>
<p>注意</p>
<ul>
<li>keyring文件一般应该保存于<code>/etc/ceph</code>目录中，以便客户端能自动查找</li>
<li>创建包含多个用户的keyring文件时，应该使用`cluster-name.keyring``为文件名</li>
<li>创建仅包含单个用户的kerying文件时，应该使用cluster-name.user-name.keyring作为文件名</li>
</ul>
<p>将用户添加至keyring</p>
<ul>
<li>可将某个用户从包含多个用户的keyring中导出，并保存于一个专用的keyring文件，
<ul>
<li>==命令==：<code>ceph auth get TYPE.ID -o /etc/ceph/cluster-name.user-name.keyring</code></li>
</ul>
</li>
<li>也可将用户的keyring合并至一个统一的keyring文件中，
<ul>
<li>==命令==：<code>ceph-authtool /etc/ceph/cluster-name.keyring -import-key /etc/ceph/cluster- name.user-name.keyring</code></li>
</ul>
</li>
</ul>
<h4 id="使用ceph-authtool命令管理用户">使用ceph-authtool命令管理用户</h4>
<p><code>ceph-authtool</code> 命令可以直接创建用户、授权caps并创建keyring</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ceph-authtool keyringfile 
</span></span><span class="line"><span class="cl">[-C | --create-keyring]
</span></span><span class="line"><span class="cl">[-n | --name entityname] [--gen-key] 
</span></span><span class="line"><span class="cl">[-a | --add-key base64_key] [--cap | --caps capfile]
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><strong>命令选项</strong></p>
</blockquote>
<ul>
<li><code>-C，--create-keyring</code>：创建一个新的密钥环，覆盖任何现有的密钥环文件</li>
<li><code>--gen-key</code>：将为指定的实体名生成新的密钥</li>
<li><code>--add-key</code>：将为密钥环添加编码密钥</li>
<li><code>--cap subsystem capability</code> 将设置给定子系统cap文件的功能</li>
<li><code>--caps capfile</code> 将为所有子系统设置与给定密钥关联的所有功能</li>
</ul>
<hr>
<p>注意：此种方式添加的用户仅存在于keyring文件中，管理员还需要额外将其添加至Ceph集群上</p>
<ul>
<li>命令：<code>ceph auth add {TYPE.ID} -i /PATH/TO/keyring</code></li>
</ul>
<hr>
<blockquote>
<p><strong>创建k8s使用的账号</strong></p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ ceph auth get-or-create client.kube mon <span class="s1">&#39;allow r&#39;</span> osd <span class="s1">&#39;allow * pool=kube&#39;</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>client.kube<span class="o">]</span>
</span></span><span class="line"><span class="cl">        <span class="nv">key</span> <span class="o">=</span> AQAwfA9dff8nGRAAMsF1VlKRPS/NEFOnB057OQ<span class="o">==</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">$ ceph auth get client.kube
</span></span><span class="line"><span class="cl">exported keyring <span class="k">for</span> client.kube
</span></span><span class="line"><span class="cl"><span class="o">[</span>client.kube<span class="o">]</span>
</span></span><span class="line"><span class="cl">        <span class="nv">key</span> <span class="o">=</span> AQAwfA9dff8nGRAAMsF1VlKRPS/NEFOnB057OQ<span class="o">==</span>
</span></span><span class="line"><span class="cl">        caps <span class="nv">mon</span> <span class="o">=</span> <span class="s2">&#34;allow r&#34;</span>
</span></span><span class="line"><span class="cl">        caps <span class="nv">osd</span> <span class="o">=</span> <span class="s2">&#34;allow * pool=kube&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ ceph auth get client.kube -o ./ceph.client.kube.keyring
</span></span><span class="line"><span class="cl">exported keyring <span class="k">for</span> client.kube
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">ceph-authtool --create-keyring cluster.keyring
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ceph-authtool cluster.keyring --import-keyring ./ceph.client.kube.keyring
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">ceph-authtool cluster.keyring --import-keyring ./ceph.client.admin.keyring
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph概念 - 初识Ceph</title>
      <link>https://www.oomkill.com/2019/06/01-1-ceph-acquaintance/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/06/01-1-ceph-acquaintance/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="初识ceph">初识Ceph</h2>
<p>Ceph 是一个开源分布式存储系统系统，它不是一种单一的存储，而是面向云提供一种统一存储平台，包含块存储 RBD, 文件存储 CephFS, 以及对象存储 RGW，这种存储的出现允许用户拜托供应商的绑定，它可以提供块存储到 “云平台”，也可以提供对象存储到 “应用”，并支持理论上的无限扩展性，数千客户端访问 PB 甚至 EB 级别的数据</p>
<h3 id="san-vs-ceph">SAN VS Ceph</h3>
<p>与传统 SAN 存储相比，Ceph 客户端会计算他们所需的数据所在的位置，这消除了存储系统中需要在“中心化查找”的瓶颈。  这使得 Ceph 集群可以在不损失性能的情况下进行扩展。</p>
<h3 id="ceph-集群架构组成">Ceph 集群架构组成</h3>
<p>Ceph 集群核心是 RADOS，而基于 RADOS，构建出多种类型存储，块存储, 文件系统, 对象存储，而一个基础的 Ceph 集群的组件由 &ldquo;Ceph monitor&rdquo; 与 &ldquo;Ceph OSD Daemon&rdquo; 组成</p>
<ul>
<li>Ceph Monitor（进程名称为 ceph-mon，下文中以 ceph-mon 代表 Ceph Monitor） 维护集群映射的主副本。  ceph集群中的monitor，可确保 ceph-mon 守护进程在失败时的高可用性。客户端从 ceph-mon 检索集群映射的副本。</li>
<li>Ceph OSD Daemon 检查”自身“及”其他“ OSD 的状态并报告给 Monitor。</li>
</ul>
<h3 id="ceph-中的常见术语">Ceph 中的常见术语</h3>
<h4 id="application">Application</h4>
<p>用于使用 Ceph 集群的任何 Ceph 外部的应用程序</p>
<h4 id="block-device">Block Device</h4>
<p>也称为 “RADOS 块设备” 或 ”RBD“ ，协调基于块的数据存储的工具，Ceph块设备拆分基于块的应用程序数据   成“块”。   RADOS 将这些块存储为对象。   Ceph 块   设备协调这些对象的存储   存储集群。</p>
<p>也称为 “RADOS Block Device” 或 “RBD”。一种用于协调 Ceph 中基于块的数据的存储的软件。 Ceph 块设备将基于块的应用程序数据拆分为 “Chunk”。 RADOS 将这些块存储为对象。</p>
<p>Chunk 与 Block 是两种不同的概念</p>
<ul>
<li>
<p>Chunk 存储是类似于 Key-Value 存储和对象存储，是一种结构化数据，并固定大小的块</p>
</li>
<li>
<p>Block 通常被提到的上下文是作为硬件接口提供的，通常代表硬件裸设备</p>
</li>
</ul>
<p>所以说 Block Device 是将数据划分为固定大小的 Chunk，存储在 Block 上。</p>
<h4 id="mgr-manager">MGR (Manager)</h4>
<p>Ceph Manager 又称为 Ceph Manager Daemon，进程名称为 <code>ceph-mgr</code>, 是与 Ceph Monitoring 一起运行的守护进程，用于提供监视以及与外部监视和管理系统的接口。自 Luminous 版本 (12) 起，ceph-mgr 没有运行的情况下 Ceph 集群无法正常运行。</p>
<h4 id="mon-monitor">MON (Monitor)</h4>
<p>Ceph Monitor 维护集群状态影视的守护进程，这些“集群状态”包括 Monitor map、Manager map、OSD map 和 CRUSH map。 Ceph 集群必须至少包含三个正在运行的 Monitor，才能实现冗余和高可用性。</p>
<h4 id="osd">OSD</h4>
<p>Ceph <strong>O</strong>bject <strong>S</strong>torage <strong>D</strong>aemon，又被称为 OSD, 在 “research and industry” 中 OSD 表示 ”对象存储设备“，而 Ceph 社区将 OSD 称为 OSD daemon，用于与逻辑磁盘交互的进程。</p>
<h4 id="osd-fsid">OSD fsid</h4>
<p>用于标识 OSD 的唯一标识符。它可以在 OSD 路径中名为 osd_fsid 的文件中找到。术语 “fsid” 与 “uuid” 互换使用</p>
<h4 id="osd-id">OSD id</h4>
<p>定义 OSD 的 integer，它是在创建每个 OSD 期间由监视器生成的。</p>
<h4 id="hybrid-osd">Hybrid OSD</h4>
<p>指同时拥有 HDD 和 SSD 的 OSD</p>
<h4 id="cluster-map">Cluster Map</h4>
<p>由 monitor map、OSD Map、PG Map、MDS Map 和 CRUSH Map 组成的一组 Map，它们共同报告 Ceph 集群的状态。有关详细信息。</p>
<h4 id="crush">CRUSH</h4>
<p>CRUSH <strong>C</strong>ontrolled <strong>R</strong>eplication <strong>U</strong>nder <strong>S</strong>calable <strong>H</strong>ashing 可扩展散列下的受控复制，Ceph 用于计算对象存储位置的算法。</p>
<h4 id="das">DAS</h4>
<p>DAS <strong>D</strong>irect-<strong>A</strong>ttached <strong>S</strong>torage 直接附加存储，无需访问网络直接连接计算机的存储。例如 SSD</p>
<h4 id="lvm-tags">LVM tags</h4>
<p><strong>L</strong>ogical <strong>V</strong>olume <strong>M</strong>anager tags 逻辑卷管理器标签，LVM “卷” 和 “组” 的可扩展元数据。它们用于存储有关设备及其与 OSD 关系的 Ceph 特定信息。</p>
<h4 id="pgs-placement-groups">PGs (Placement Groups)</h4>
<p>“放置组” 是每个逻辑 Ceph Pool 的子集。放置组执行将对象（作为一个组）放置到 OSD 中的功能。 Ceph 在内部以“放置组粒度”来管理数据：这比管理单个RADOS 对象的扩展性将更好。<strong>具有较大数量放置组的集群比具有较少数量放置组的其他相同集群具有更好的平衡性</strong>。</p>
<h4 id="pools">Pools</h4>
<p>池是用于存储对象的逻辑分区。</p>
<h4 id="rados">RADOS</h4>
<p><strong>R</strong>eliable <strong>A</strong>utonomic <strong>D</strong>istributed <strong>O</strong>bject <strong>S</strong>tore 可靠的自动分布对象存储，RADOS 是为可变大小的对象提供可扩展服务的对象存储。 RADOS 对象存储是 <strong>Ceph 集群的核心组件</strong>。</p>
<h4 id="block-storage">Block Storage</h4>
<p>块存储是 Ceph支持的三种存储类型之一。  Ceph 块存储指的是块存储 结合使用时的相关服务和功能 集合</p>
<h4 id="ceph-file-system">Ceph File System</h4>
<p>Ceph File System (CephFS) 是一个兼容 POSIX 的文件系统，构建在 RADOS 之上，可根据按需部署</p>
<h4 id="mds-metadata-server">MDS (Metadata Server)</h4>
<p>Ceph <strong>M</strong>eta<strong>D</strong>ata <strong>S</strong>erver daemon MDS，构建在 RADOS 之上，存储所有文件的元数据作为”文件系统“类型的存储提供给用户，运行的程序名为 <code>ceph-mds</code>，故 也是是否使用 CephFS 的标记</p>
<h4 id="rgw-radow-gateway">RGW (Radow Gateway)</h4>
<p>Ceph 提供兼容 Amazon S3 RESTful API 和 OpenStack Swift API 的组件，可根据按需部署</p>
<h4 id="realm">Realm</h4>
<p>是位于对象存储中的上下文，领域 (Realm) 是一个全局唯一的命名空间，由一个或多个区域组组成。</p>
<h4 id="zone">Zone</h4>
<p>是位于对象存储中的上下文，区域 (zone) 是由一个或多个 RGW 实例组成的逻辑组。&ldquo;zone&rdquo; 的配置状态存储在 &quot;&quot; 中</p>
<h4 id="period">Period</h4>
<p>是位于对象存储中的上下文，Period 是 Realm 的配置状态。该 Period 存储多站点配置的配置状态。当 Period 被更新时，“epoch” 被认为已经改变。</p>
<h4 id="cephx">CephX</h4>
<p>CephX Ceph authentication protocol；CephX 是用于对用户和守护进程进行身份验证。 CephX 的运行方式类似于 Kerberos，但它没有单点故障。</p>
<h4 id="secrets">Secrets</h4>
<p>Secret 是用户访问是需要提供的身份验证的系统用于执行数字身份验证的凭据。</p>
<h2 id="ceph存储集群组成">Ceph存储集群组成</h2>
<p>Ceph 存储集群由多种类型的守护进程组成：</p>
<ul>
<li>Ceph Monitor</li>
<li>Ceph OSD Daemon</li>
<li>Ceph Manager</li>
<li>Ceph Metadata Server</li>
</ul>
<h3 id="osd-object-storage-device">OSD (Object Storage Device)</h3>
<p>对象存储设备 OSD (Object Storage Device) 通常是指==单独的磁盘设备==，每个 Ceph Node 上有一个或多个o OSD，每一个 OSD 是真正存放数据的地方（在文件系统中同样概念为文件与目录）；OSD不是主机。由多个 Ceph Node 组合起来称为 RADOS Cluster</p>
<p>为了使每个 OSD 能够被单独使用和管理，每个 OSD 都会有一个单独的专用的守护进程被称为 <code>ceph-osd</code>。OSD 本身用来存储数据，还包括数据复制、恢复、数据重新均衡、提供监视信息给mon和mgr</p>
<p>一个集群至少有 3个 Ceph OSDs，已确保高可用。选择 OSD 冗余时，应自己指定故障率，故障转移率或故障容忍率。OSD级别故障就在OSD级别冗余，主机级别就跨主机冗余，故障率是机架级别就机架冗余。 在做crush运行图的设定时是应该自己指定的。</p>
<h3 id="mgr-manager-1">(MGR Manager)</h3>
<p>Mgr (Manager) 集群元数据服务器，维护集群映射的主副本。Ceph 监视器集群可确保监视器守护进程发生故障时的高可用性。存储集群客户端从 Ceph Monitor 检索集群映射的副本。</p>
<p>monitor在每一次读数据都是实时查询的，故monitor不适用频繁周期性采集数据的监控操作。在Ceph新版中引入新组建mgr，（早期Ceph版本是没有mgr的）用来专门维护查询类操作，将查询操作按照自己内部空闲方式缓存下来，一旦有监控可及时响应。</p>
<p>mgr是在一类节点上运行的守护进程，一般为两个活以上节点，此类守护进程被称为<code>ceph-mgr</code>。主要功能在于跟踪运行时的指标数据。如磁盘使用率、CPU使用率。以及集群当前状态，此状态不是内部运行状态，而是查询做监控时的状态。包括存储空间利用率、当前性能指标、节点负载（系统级）等</p>
<h3 id="mon-monitor-1">MON (Monitor)</h3>
<p>一个 Ceph 集群内除了存储节点之外，还有另外一种节点 Monitor ，用来管理整个集群的，如有多少个节点，每个节点上有多少个OSD，每个OSD是否健康，他会持有整个集群的运行图（运行状态）。mon是用来集中维护集群元数据而非文件元数据。为了维护整个集群能够正常运行而设定的节点，离开此节点集群内部就无法协调。</p>
<p>在一个主机上运行的守护进程 <code>ceph-mon</code>，守护进程扮演、监视着整个集群所有组件的角色，被称为集群运行图的持有者cluster map（整个集群有多少存储池，每个池中有多少PG，每个PG映射哪个OSD，有多少OSD等等）集群运行图 <code>Cluster Map</code></p>
<p>monitor负责维护整个进群的认证信息并实行认证，认证协议叫 ==CephX== 协议（ceph内部的认证协议）。monitor用来维护认证信息并实行认证。<code>认证中心</code> monitor自身是无状态的，所以实现均衡认证负载。</p>
<p>monitor的高可用自己内部直接使用POSIX协议来实现数据冗余，monitor也是节点级冗余，为了确保各节点数据是强一致的，每个节点都可写，写完后会同步到其他节点。为了避免同时写导致的冲突，使用了分布式一致性协议，monitor就是使用POSIX协议来进行分布式协作的。</p>
<h3 id="mds-metadata-server-1">MDS (Metadata Server)</h3>
<p>==Ceph Metadata Server== 用来代表ceph文件系统而提供的守护进程<code>ceph-mds</code>，如不使用CephFS，此进程是无需启动的。利用底层RADOS存储空间，将存储空间抽象成文件系统，来兼容POSIX file system 提供服务。</p>
<blockquote>
<p>Note: 一个基础的 Ceph 存储集群由，OSD, Monitor, Manager 组成</p>
</blockquote>
<h2 id="ceph集群中其他概念">Ceph集群中其他概念</h2>
<h3 id="客户端接口">客户端接口</h3>
<ul>
<li><strong>Ceph</strong> 存储集群提供了基础的对象数据存储服务，客户端可基于RADOS协议和 librados API 直接与存储系统交互进行对象数据存取</li>
<li><strong>Librados</strong>
<ul>
<li>Librados提供了访问 RADOS 在存储集群支持 “异步” 通信的 API 接口，支持对集群中对象数据的的直接并行访问，用户可通过支持的编程语言开发自定义客户端程序通过RADOS协议与存储系统进行交互</li>
<li>客户端应用程序必须与librados绑定方可连接到RADOS存储集群，因此，用户必须实现安装librados及其依赖后才能编写使用librados的应用程序。</li>
<li>librados API本身使用 C++ 编写的，它额外支持C、Python、Java、和PHP等开发接口</li>
</ul>
</li>
<li>当然，并非所有用户都有能力自定义开发接口以接入RADOS存储集群的需要，为此，Ceph也原生提供了几个较高级别的各户端接口，它们分别是 <em><strong>RADOS GateWay</strong></em> (RGW), <em><strong>Reliable Block Device</strong></em> (RBD) 和 <em><strong>MDS</strong></em> (MetaData Server)，分别为用户提供 RESTFUL、块和 POSIX 文件系统接口</li>
</ul>
<h3 id="管理节点-admin-host">管理节点 (Admin Host)</h3>
<p>Ceph通常是分布式集群，为了便于去管理维护整个集群，通常在Ceph集群中找一个专门的节点用来当管理节点。此节点可以连接至每一个节点用来管理节点上的Ceph守护进程。</p>
<p>Ceph的管理接口是一系列命令行工具，例如 <code>rados</code>,  <code>ceph</code>,  <code>rbd </code>等命令，管理员可以从某个特定的MON节点执行管理操作，但也有人更倾向于使用专用的管理节点。</p>
<blockquote>
<p>Note: 在早期 (Ceph-deploy) 部署的集群，通常管理节点是必要的，但使用 cephadm 部署的集群，实际上管理管理节点可以在任何位置</p>
</blockquote>
<h3 id="pool">Pool</h3>
<p>Ceph所提供的存储空间（没有目录之类一说）是将“所有对象都是存储在数据平面上”，因此，所有对象都不能同名。RADOS 将他的存储空间切分为多个分区以便好进行管理。每一个分区叫做一个“<strong>存储池</strong>”。存储池的大小是取决于底层的存储空间的。与真正意义上的分区不是一回事。在Ceph中每一个存储池存放的数据了也可能会太大，所以存储池也可以进一步划分（可选）。被称为名称空间（先切分为存储池，每个存储池可进一步被划分成名称空间） 两级逻辑组件。</p>
<h3 id="第三级-pg">第三级 PG</h3>
<p>每一个存储池内部会有多个PG（<strong>P</strong>lacement <strong>G</strong>roups 规置组）存在。Pool 与 pg 都是抽象的概念。</p>
<h3 id="object">Object</h3>
<p>对象是自带元数据的组件，</p>
<ul>
<li>对象id，每一个对象应有一个对象ID在集群内部来引用对象</li>
<li>数据</li>
<li>元数据 key vlaue类型的数据</li>
</ul>
<p>这些类型打包成一起存储，被称为一个对象。RADOS集群会吧真正存储的每一个文件，切分成N个对象来进行存储的。（切分个数与默认对象切分大小有关）。</p>
<p>每一个对象都是被单独管理的，都拥有自己的标识符。因此， 同一个文件的object有可能被映射到不同的PG上，PG提交给主机的，由主机负责将对象存储在磁盘（OSD）被存储在不同的OSD上。</p>
<h3 id="文件存储到rados集群">文件存储到RADOS集群</h3>
<p>一般要接入RADOS集群必须通过客户端来实现(LIBRADOS、RBD、CephFS、RADOSGW)，才能接入到集群中来。</p>
<p>当将文件存入Ceph中时，需要通过某一类客户端接入，客户端接入时，需要借助于 Ceph 存储API接口将其切分为固定大小的存储对象(Date object)，此数据对象究竟被放置在哪个 OSD 上存放这中间是靠 crush 来完成的。数据对象被存放在哪个存储池上是固定的。存储池需创建才可使用。但PG是虚拟的中间层。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Ceph算法 - crush</title>
      <link>https://www.oomkill.com/2019/06/08-1-ceph-crush/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/06/08-1-ceph-crush/</guid>
      <description></description>
      <content:encoded><![CDATA[<h3 id="关于存储池">关于存储池</h3>
<p>从某种意义上来讲，RADOS所提供的存储空间的管理接口，不应该将其放置在同一个平面当中，因此将其切割成多个不同的&quot;逻辑存储空间&quot;，称之为存储池。</p>
<p>RADOS存储集群提供的基础存储服务需要由&quot;存储池（pool）&ldquo;分割为逻辑存储区域，此类的逻辑区域亦是对象数据的名称空间。</p>
<ul>
<li>实践中，管理员可以为特定的应用程序存储不同类型数据的需求分别创建专用的存储池，例如rbd存储池，rgw存储池等，也可以为某个项目或某个用户创建专有的存储池。</li>
<li>存储池还可以再进一步细分为一至多个名称空间（namespace）。同一个存储池内，无论属于哪个、哪些名称空间，数据都是被存储池中的PG进行存放，虽然处于不同名称空间，但可能处于同一个PG之上。</li>
<li>客户端(包括rbd和rgw等）存取数据时，需要事先指定存储池名称、用户名和密钥等信息完成认证，而后将一直维持与其指定的存储池的连接，于是也可以吧存储池看做是客户端的IO接口。</li>
</ul>
<p>存储池类型</p>
<ul>
<li>副本池（replicated）：任何一个数据对象存储在此类存储池中其冗余机制是通过创建多个数据对象副本来实现的，而副本数量是用户在创建存储池时指定。如，创建存储池时没指定类型，就是副本池，默认副本数量为3个（1主两从），统称副本数量。把每个对象在集群中存储为多个副本，其中存储于主OSD的为主副本，副本数量在创建存储池时由管理员指定；副本池类型为Ceph为默认的存储池类型。但是此存储池是非常浪费存储空间的。副本池对读IO有很好的附带表现</li>
<li>纠删码池（erasure code）：使用校验码可计算回数据。把各对象存储为<code>N=K+M</code>个块，其中，K为数据块数量，M为编码块数量，因此存储池的尺寸为<code>K+M</code>；纠删码块的数据就 是允许冗余的级别。如4+2，即允许最多两个数据块丢失。不是所有的应有都能支持纠删码池，如rbd必须使用副本池。radosGW可以使用纠删码池。</li>
</ul>
<p>副本池IO</p>
<ul>
<li>将一个数据对象存储为多副本</li>
<li>写入操作时，Ceph客户端使用CRUSH算法来计算对象的PG ID和Primary OSD</li>
<li>主OSD根据设定的副本教、对象的名称、存储池名称和集群运行图（Cluster Map）计算出PG的各铺助OSD，而后由主OSD将数据同步给这些辅助OSD。</li>
</ul>
<p></p>
<p>对于有着三个副本的存储池来讲，任何一个PG都会选择三个OSD，因此，副本池所关联的OSD数量通常与冗余量相同。OSD，成为一个活动集。如图所示，其中一个OSD为主OSD负责读写操作，另外两个OSD负责从主OSD同步数据。当三个副本都存完，才能的到存储完成的消息的。客户的只需与主OSD通信，同步过程是OSD内部自行实现的。</p>
<p>纠删码池IO</p>
<p>纠删码是一种前向纠错码（FEC）代码</p>
<ul>
<li>通过将K块的数据转换为N块，假设N=K+M，则其中的M代表纠删码算法添加的额外活冗余的块数量以提供冗余机制（即编码块），而N则表示在纠删码编码之后要创建的块的总数，其可以故障的块数为M（即N-K）个。</li>
<li>类似于RAID5</li>
</ul>
<p>纠删码池减少了确保数据持久性所需的磁盘空间量，但计算量上却比副本存储池要更贵一些
RGW可以使用纠删码池，但RDB不支持。</p>
<p>例如，把包含数据<code>ABCDEFGHI</code>的对象NYAN保存到存储池中，假设纠删码算法会将内容分割为三个数据块：第一个包含<code>ABC</code>，第二个为<code>DEF</code>，最后一个为<code>GHI</code>，并为这三个数据块额外创建两个编码块：第四个<code>YXY</code>和第五个<code>GQC</code>，此时纠删码算法会通过计算哪家出<code>GHI</code>、和<code>YXY</code></p>
<p>副本池所有从OSD没有次序之分，只有主和从两类角色之分，各个从没有次序。纠删码池是有次序的，这个顺序代表数据拼凑起来的数据。因此，每个分片应指定其在整个数据文件的偏移量是多少。</p>
<p></p>
<h3 id="归置组">归置组</h3>
<p>归置组（PlacementGroup）是用于跨OSD将数据存储在某个存储池中的内部数据结构。</p>
<ul>
<li>相对于存储池来说，PG是一个虚拟组件，它是对象映射到存储池时使用的虚拟的层</li>
<li>出于规模伸缩及性能方面的考虑，Ceph将存储池细分为归置组，吧每个单独的对象映射到归置组，并将归置组分配给一个主OSD</li>
<li>存储池由一系列的归置组组成，而CRUSH算法则根据集群运行图和集群状态，将各PG<font style="background:#ffc104;" size=2>均匀</font>、<font style="background:#ffc104;" size=2>伪随机</font>地分布到急群众的OSD之上。</li>
<li>若某OSD失败或需要对集群进行重新平衡，Ceph则移动或复制整个归置组而无需单独寻址每个对象。</li>
</ul>
<p>归置组在OSD守护进程和Ceph客户端之间生成了一个中间层，CRUSH算法负责将每个对象动态映射到一个归置组，然后再将每个归置组动态映射到一个或多个OSD守护进程，从而能够支持在新的OSD设备上线时动态进行数据重新平衡。</p>
<p>归置组作用</p>
<p>在存储池中存放100w数据对象，而使用100个归置组，一组内存放1w对象。归置组是从新平衡和恢复时的基本单元。使得数据单元不至于以文件或对象为单位。</p>
<h4 id="归置组计数">归置组计数</h4>
<p>归置组的数量有管理员在创建存储池是指定，而后由crush负责创建和使用</p>
<ul>
<li>通常，PG的数量应该是数据的合理粒度的子集
<ul>
<li>例如，一个包含256个PG的存储池意味着每个PG包含大约1/256的存储池数据</li>
</ul>
</li>
<li>当需要将PG从一个OSD移动到另一个OSD时，PG的数量会对性能产生影响
<ul>
<li>PG数量过少，Ceph将不得不同时移动相当数量的数据，其产生的网络负载将对集群的正常性能输出产生负面影响。</li>
<li>而在过多的PG数量场景中在移动极少量的数据时，Ceph将会占用过多的CPU和RAM，从而对集群的计算资源产生负面影响。</li>
</ul>
</li>
<li>PG数量在集群分发数据和重新平衡时扮演着重要作用
<ul>
<li>在所有OSD之间进行数据持久存储及完成数据分布会需要较多的归置组，但是它们的数量应该减少到最大性能所需的最小数量值，以节省CPU和内存资源</li>
<li>一般说来，对于有着超过50个OSD的RADOS集群，建议每个OSD大约有50-100个PG以平衡资源使用，取的更好的数据持久性和数据分布，更大规模的集群中，每个OSD大约可持有100-200个PG</li>
<li>至少应该使用多少个PG，可通过下面的公式计算后，将其值以类似于四舍五入到最近的2的N次幂
<ul>
<li><code>(Total OSDs * PGPerOSD/Replication factor =&gt; Total PGs  )</code></li>
<li>可以使用的PG数量 = 总OSD数量* 每个OSD可以有多少个PG/复制因子（副本数量）</li>
</ul>
</li>
<li>一个RADOS集群上可能会存在多个存储池，因此管理员还需要考虑所有存储池上的PG分布后每个OSD需要映射的PG数量</li>
<li>PG数量一定是2的N次方倍，这样进行hash计算时，速度才会更快。Ceph要求每一个OSD上最多不能超过256个PG</li>
</ul>
</li>
</ul>
<h4 id="归置组状态">归置组状态</h4>
<p>依据PG当前的工作特性活工作进程所阶段，它总是处于某个活某些个“状态中”，最常见的状态应该为<code>active+clean</code></p>
<p>PG的常见状态</p>
<ul>
<li><strong>Active</strong>
<ul>
<li>主OSD和各辅助OSD均处于就绪状态，可正常服务于客户端IO请求</li>
<li>一般Peering操作过程完成后即会转入Active状态</li>
</ul>
</li>
<li><strong>Clean</strong>
<ul>
<li>主OSD和各辅助OSD均处于就绪状态，所有对象的副本数量均符合期望，并且PG的活动集和上行集为同一组OSD。</li>
<li>活动集(Acting Set)：由PG当前的主OSD和所有的处于活动状态的辅助OSD组成，这组OSD负责执行此PG上数据对象的存取操作I/O</li>
<li>上行集(Up Set)，根据CRUSH的工作方式，集群拓扑架构的变动将可能导致PG相应的OSD变动活扩展至其他的OSD之上，这个新的OSD集也成为PG的“（Up Set）”，其映射到的新OSD集可能不分地与原有OSD集重合，也可能会完全不相干；上行集OSD需要从当前的活动集OSD上复制数据对象，在所有对象同步完成后，上行集便成为新的活动集，而PG也将转为“活动（active）”状态。</li>
</ul>
</li>
<li><strong>Peering</strong>
<ul>
<li>如数据不一致，需将数据复制过去，这个复制数据过程就称之为对等过程。</li>
<li>一个PG中的所有OSD必须就它们持有的数据对象状态达成一致，而“对等（Peering）”即为其OSD从不一致转为一致的过程。</li>
</ul>
</li>
<li><strong>Degraded</strong>
<ul>
<li>在某OSD标记为“down”时，所有映射到此OSD的PG即转入“降级（degraded）”状态</li>
<li>此OSD重新启动并完成Perring操作后，PG将重新转回clean</li>
<li>一旦OSD标记为down的时间超过5分钟，它将被标记出集群，而后Ceph将对降级状态的PG启动回复操作，直到所有因此而降级的PG重回clean状态</li>
<li>在其内部OSD上某对象不可用活悄然崩溃时，PG也会被标记为降级状态，知道对象从某个权威副本上正确恢复。</li>
</ul>
</li>
<li><strong>Stale</strong> 过期
<ul>
<li>每个OSD都要周期性的向RADOS集群中的监视器报告其作为主OSD所持有的所有PG的最新统计数据，因任何原因导致某个主OSD无法正常向监视器发送此类报告，或者由其他OSD报告某个OSD已经down掉，则所有以此OSD的PG将立刻被标记为stale状态。</li>
</ul>
</li>
<li><strong>Undersized</strong></li>
<li>PG中的副本数少于其存储池定义的个数时即转入undersized状态，回复和回填操作在随后会启动已修复其副本为期望值</li>
<li><strong>Scrubbing</strong> 一致性保障的非常重要机制，文件完整性检查
<ul>
<li>各OSD还需要周期性的检查其所持有的数据对象的完整性，以确保所有对等OSD上的数据一致；处于此类检查过程中的PG便会被标记为<code>scrubbing</code>状态，这也通常被称作light scrubs、shallow scrubs或者simply scrubs。</li>
<li>另外，PG还需偶尔需要进行<code>deep scrubs</code>检查以确保同一对象在相关的各OSD上能按位匹配，此时PG将处于<code>scrubbing+deep</code>状态。</li>
</ul>
</li>
<li><strong>Recovering</strong> 恢复
<ul>
<li>添加一个新的OSD至存储集群中或某OSD宕掉时，PG则由可能会被CRUSH重新映射进而将持有与此不同的OSD集，而这些处于内部数据同步过程中的PG则被标记为recovering状态；</li>
</ul>
</li>
<li><strong>Backfilling</strong> 回填
<ul>
<li>新OSD加入存储集群后，Ceph则会进入数据重新均衡的状态，即一些数据对象会在进程后台从现有OSD移到新的OSD之上，此操作过程为backfill。</li>
</ul>
</li>
</ul>
<h4 id="crush">CRUSH</h4>
<p>把对象直接映射到OSD之上会导致二者之间的紧密耦合关系，变动底层OSD就会牵一发而动全身，因此在OSD设备变动时不可避免地对整个集群产生扰动</p>
<p>于是Ceph将一个对象映射进RADOS集群的过程分为两步</p>
<ul>
<li>先是以一致性哈希算法将对象名称映射到PG</li>
<li>而后是将PG ID基于CRUSH算法映射到OSD</li>
</ul>
<p>此两个过程都以“实时计算”的方式完成，而非畅通的查表方式，从而有效规避了任何组件被”中心化“的可能性，使得集群规模扩展不在受限。</p>
<p>这个实时计算操作用到的算法就是CRUSH</p>
<ul>
<li>Controlled Replication Uder Scalable Hashing</li>
<li>他是一种数据分布式算法，类似于一致性哈希算法，用于为RADOS存储集群控制数据分布。</li>
<li></li>
</ul>
<h4 id="客户端io的简要工作流程">客户端IO的简要工作流程</h4>
<p>存取对象时，客户端从Ceph监视器检索出集群运行图，绑定到指定的存储池，并对存储池上PG内的对象执行IO操作。</p>
<ul>
<li>存储池的<font color="#f8070d" size=2>CRUSH规则集</font>和<font color="#f8070d" size=2>PG的数量</font>是决定Ceph如何放置数据的关键性因素</li>
<li>基于最新版本的集群运行图，客户端能够了解到集群中的所有监视器和OSD以及他们各自的当前状态。</li>
<li>不过，客户端对目标对象的位置却一无所知。</li>
</ul>
<p>执行对象的存取操作时，客户端需要输入的是</font>和<font color="#f8070d" size=2>对象标识</font>和</font>和<font color="#f8070d" size=2>存储池名</font>。至于存储到哪个PG、被映射到哪个OSD则是由算法来完成的。</p>
<ul>
<li>客户端需要在存储池中存储命名对象时，他将对象名称、对象名册的hash码、存储池中的PG数量和存储池名称作为输入，而后由CRUSH计算出PG的ID及此PG的主OSD。</li>
<li>通过将对象的标识进行一致性hash运算得到的哈希值与PG位图掩吗进行“与”运算得到目标PG，从而得出目标PG的ID（pg_id），完成有Object至PG的映射。</li>
<li>而后，CRUSH算法便将以此pg_id、CRUSH运行图和归置规则（Placement Rules）为输入参数再次进行计算，并输出一个确定且有序的目标存储向量列表（OSD列表），从而完成从PG至OSD的映射。</li>
</ul>
<p>Ceph客户端使用以下步骤来计算PG ID</p>
<ul>
<li>客户端输入存储池名称及对象名称。例如<code>pool = pool1</code>以及<code>object-id = obj1</code></li>
<li>获取对象名称并通过一致性hash算法对其进行hash运算，即hash(o)，其中o为对象名称</li>
<li>将计算出的对象标识hash码与PG位图掩吗进行“与”运算获得目标PG的标识符，即PG ID，例如1701
<ul>
<li>计算公式为<code>pgid=func(hash(o)&amp;m,r)</code>其中，变量o是对象标识符，变量m是当前存储池中PG的位图掩吗，变量r是指复制因子，用于确定目标PG中OSD数量。</li>
</ul>
</li>
<li>CRUSH根据集群运行图计算出与目标PG对应的有序的OSD集合，并确定出其主OSD</li>
<li>客户端渠道存储池名称对应的数字表示，例如存储池“pool1”的数字表示11</li>
<li>客户端将存储池的ID添加到PG ID，例如，11.1701</li>
<li>客户端通过直接与PG映射到的主OSD通信来执行诸如写入、读取或删除之类的对象操作。</li>
</ul>
<p></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Cloud基础设施 - 初识Ceph</title>
      <link>https://www.oomkill.com/2019/06/01-2-cloud-base/</link>
      <pubDate>Sun, 30 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2019/06/01-2-cloud-base/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="初识ceph">初识Ceph</h2>
<p>Ceph 是一个开源分布式存储系统系统，它不是一种单一的存储，而是面向云提供一种统一存储平台，包含块存储 RBD, 文件存储 CephFS, 以及对象存储 RGW，这种存储的出现允许用户拜托供应商的绑定，它可以提供块存储到 “云平台”，也可以提供对象存储到 “应用”，并支持理论上的无限扩展性，数千客户端访问 PB 甚至 EB 级别的数据</p>
<h3 id="san-vs-ceph">SAN VS Ceph</h3>
<p>与传统 SAN 存储相比，Ceph 客户端会计算他们所需的数据所在的位置，这消除了存储系统中需要在“中心化查找”的瓶颈。  这使得 Ceph 集群可以在不损失性能的情况下进行扩展。</p>
<h3 id="ceph-集群架构组成">Ceph 集群架构组成</h3>
<p>Ceph 集群核心是 RADOS，而基于 RADOS，构建出多种类型存储，块存储, 文件系统, 对象存储，而一个基础的 Ceph 集群的组件由 &ldquo;Ceph monitor&rdquo; 与 &ldquo;Ceph OSD Daemon&rdquo; 组成</p>
<ul>
<li>Ceph Monitor（进程名称为 ceph-mon，下文中以 ceph-mon 代表 Ceph Monitor） 维护集群映射的主副本。  ceph集群中的monitor，可确保 ceph-mon 守护进程在失败时的高可用性。客户端从 ceph-mon 检索集群映射的副本。</li>
<li>Ceph OSD Daemon 检查”自身“及”其他“ OSD 的状态并报告给 Monitor。</li>
</ul>
<h3 id="ceph-中的常见术语">Ceph 中的常见术语</h3>
<h4 id="application">Application</h4>
<p>用于使用 Ceph 集群的任何 Ceph 外部的应用程序</p>
<h4 id="block-device">Block Device</h4>
<p>也称为 “RADOS 块设备” 或 ”RBD“ ，协调基于块的数据存储的工具，Ceph块设备拆分基于块的应用程序数据   成“块”。   RADOS 将这些块存储为对象。   Ceph 块   设备协调这些对象的存储   存储集群。</p>
<p>也称为 “RADOS Block Device” 或 “RBD”。一种用于协调 Ceph 中基于块的数据的存储的软件。 Ceph 块设备将基于块的应用程序数据拆分为 “Chunk”。 RADOS 将这些块存储为对象。</p>
<p>Chunk 与 Block 是两种不同的概念</p>
<ul>
<li>
<p>Chunk 存储是类似于 Key-Value 存储和对象存储，是一种结构化数据，并固定大小的块</p>
</li>
<li>
<p>Block 通常被提到的上下文是作为硬件接口提供的，通常代表硬件裸设备</p>
</li>
</ul>
<p>所以说 Block Device 是将数据划分为固定大小的 Chunk，存储在 Block 上。</p>
<h4 id="mgr-manager">MGR (Manager)</h4>
<p>Ceph Manager 又称为 Ceph Manager Daemon，进程名称为 <code>ceph-mgr</code>, 是与 Ceph Monitoring 一起运行的守护进程，用于提供监视以及与外部监视和管理系统的接口。自 Luminous 版本 (12) 起，ceph-mgr 没有运行的情况下 Ceph 集群无法正常运行。</p>
<h4 id="mon-monitor">MON (Monitor)</h4>
<p>Ceph Monitor 维护集群状态影视的守护进程，这些“集群状态”包括 Monitor map、Manager map、OSD map 和 CRUSH map。 Ceph 集群必须至少包含三个正在运行的 Monitor，才能实现冗余和高可用性。</p>
<h4 id="osd">OSD</h4>
<p>Ceph <strong>O</strong>bject <strong>S</strong>torage <strong>D</strong>aemon，又被称为 OSD, 在 “research and industry” 中 OSD 表示 ”对象存储设备“，而 Ceph 社区将 OSD 称为 OSD daemon，用于与逻辑磁盘交互的进程。</p>
<h4 id="osd-fsid">OSD fsid</h4>
<p>用于标识 OSD 的唯一标识符。它可以在 OSD 路径中名为 osd_fsid 的文件中找到。术语 “fsid” 与 “uuid” 互换使用</p>
<h4 id="osd-id">OSD id</h4>
<p>定义 OSD 的 integer，它是在创建每个 OSD 期间由监视器生成的。</p>
<h4 id="hybrid-osd">Hybrid OSD</h4>
<p>指同时拥有 HDD 和 SSD 的 OSD</p>
<h4 id="cluster-map">Cluster Map</h4>
<p>由 monitor map、OSD Map、PG Map、MDS Map 和 CRUSH Map 组成的一组 Map，它们共同报告 Ceph 集群的状态。有关详细信息。</p>
<h4 id="crush">CRUSH</h4>
<p>CRUSH <strong>C</strong>ontrolled <strong>R</strong>eplication <strong>U</strong>nder <strong>S</strong>calable <strong>H</strong>ashing 可扩展散列下的受控复制，Ceph 用于计算对象存储位置的算法。</p>
<h4 id="das">DAS</h4>
<p>DAS <strong>D</strong>irect-<strong>A</strong>ttached <strong>S</strong>torage 直接附加存储，无需访问网络直接连接计算机的存储。例如 SSD</p>
<h4 id="lvm-tags">LVM tags</h4>
<p><strong>L</strong>ogical <strong>V</strong>olume <strong>M</strong>anager tags 逻辑卷管理器标签，LVM “卷” 和 “组” 的可扩展元数据。它们用于存储有关设备及其与 OSD 关系的 Ceph 特定信息。</p>
<h4 id="pgs-placement-groups">PGs (Placement Groups)</h4>
<p>“放置组” 是每个逻辑 Ceph Pool 的子集。放置组执行将对象（作为一个组）放置到 OSD 中的功能。 Ceph 在内部以“放置组粒度”来管理数据：这比管理单个RADOS 对象的扩展性将更好。<strong>具有较大数量放置组的集群比具有较少数量放置组的其他相同集群具有更好的平衡性</strong>。</p>
<h4 id="pools">Pools</h4>
<p>池是用于存储对象的逻辑分区。</p>
<h4 id="rados">RADOS</h4>
<p><strong>R</strong>eliable <strong>A</strong>utonomic <strong>D</strong>istributed <strong>O</strong>bject <strong>S</strong>tore 可靠的自动分布对象存储，RADOS 是为可变大小的对象提供可扩展服务的对象存储。 RADOS 对象存储是 <strong>Ceph 集群的核心组件</strong>。</p>
<h4 id="block-storage">Block Storage</h4>
<p>块存储是 Ceph支持的三种存储类型之一。  Ceph 块存储指的是块存储 结合使用时的相关服务和功能 集合</p>
<h4 id="ceph-file-system">Ceph File System</h4>
<p>Ceph File System (CephFS) 是一个兼容 POSIX 的文件系统，构建在 RADOS 之上，可根据按需部署</p>
<h4 id="mds-metadata-server">MDS (Metadata Server)</h4>
<p>Ceph <strong>M</strong>eta<strong>D</strong>ata <strong>S</strong>erver daemon MDS，构建在 RADOS 之上，存储所有文件的元数据作为”文件系统“类型的存储提供给用户，运行的程序名为 <code>ceph-mds</code>，故 也是是否使用 CephFS 的标记</p>
<h4 id="rgw-radow-gateway">RGW (Radow Gateway)</h4>
<p>Ceph 提供兼容 Amazon S3 RESTful API 和 OpenStack Swift API 的组件，可根据按需部署</p>
<h4 id="realm">Realm</h4>
<p>是位于对象存储中的上下文，领域 (Realm) 是一个全局唯一的命名空间，由一个或多个区域组组成。</p>
<h4 id="zone">Zone</h4>
<p>是位于对象存储中的上下文，区域 (zone) 是由一个或多个 RGW 实例组成的逻辑组。&ldquo;zone&rdquo; 的配置状态存储在 &quot;&quot; 中</p>
<h4 id="period">Period</h4>
<p>是位于对象存储中的上下文，Period 是 Realm 的配置状态。该 Period 存储多站点配置的配置状态。当 Period 被更新时，“epoch” 被认为已经改变。</p>
<h4 id="cephx">CephX</h4>
<p>CephX Ceph authentication protocol；CephX 是用于对用户和守护进程进行身份验证。 CephX 的运行方式类似于 Kerberos，但它没有单点故障。</p>
<h4 id="secrets">Secrets</h4>
<p>Secret 是用户访问是需要提供的身份验证的系统用于执行数字身份验证的凭据。</p>
<p>存储上下文中的锁定是您能够与硬件连接的最小尺寸。每当您从磁盘读取或写入磁盘时，无论需要读取多少个块，您都会读取此数量的次数。默认 NTFS 块大小（也称为簇大小、分配单元）为 4096 字节 (4KB)。如果您有一个正好 4096 字节长的文件，那么您将从磁盘读取一个块。如果是 4097 字节，那么您会读取两个块。您无法读取部分块，因此即使文件实际上没有消耗整个块，存储文件系统也会清空该块的其余部分。查看实际情况的一个简单方法是在硬盘驱动器上创建一个空白文本文件，查看属性以及“大小”（0 字节）和“磁盘大小”（4096 字节）之间的差异。</p>
<p>Ceph是一个对象（object）式存储系统，它把每一个待管理的
在（例如一个文件）切分为一到多个固定大小的（ceph底层管理机制）对象数据，在Ceph之上，每一个对象，是一个基础的原子管理单元（不可再切分的单元），并以其为原子单元完成数据存取</p>
<ul>
<li>对象数据的底层存储服务是由多个主机（host）组成的存储集群，该集群也就称之为RADOS（Reliable Automatic Distributed Object Store）存储集群，即可靠、自动化、分布式对象存储系统。</li>
<li>librados是RADOS存储集群的API，它支持C、C++、Java、Python、Ruby和PHP等编程语言。</li>
</ul>
<p>存储设备</p>
<ul>
<li>DAS 直接附加存储：IDE、SATA、SCSI、SAS、USB</li>
<li>NAS 网络附加存储（文件系统级别）：NFS、CIFS</li>
<li>SAN 存储区域网络，与NAS不同的是，SAN提供的接口是块级别的接口。多数使用SCSI FC SAN ISCSI（internet SCSI）</li>
</ul>
<p>数据是由元数据和内容（数据组成）组成，而元数据是确保路由的。传统文件系统ext有inode信息存储在元数据区，一部分空间存元数据，一部分空间存数据。数据称为数据块 data block。 当客户端试图访问某个文件时，根据给定文件路径或文件名层层查找，查询inode表。inode中记录的有在数据块空间中那些编号块存放当前文件数据。</p>
<p>元数据找到关联数据的存储路由表，也记录了当前文件的属性信息、如权限、属组等。这是一个标准的POSIX文件系统所应该具有的基本兼容的属性。</p>
<p>如将数据分散到多个节点上去存储时，就不能按照传统的机制在一个分区上组织元数据和数据，我们只能讲数据和元数据分开存放。每一个节点只提供存储空间，元数据存放在固定节点。</p>
<p>在存数据时，客户端将请求按照指定大小规模，每一块当做一个独立的文件，然后进行路由和调度，从而完成分散存储的目的。从而利用多个节点的网络和磁盘IO的存储能力。当用户访问时，需要联系元数据服务器，由元数据服务器返回相关数据 信息后从并行从这些节点上加载到数据块，而后根据元数据给出逻辑，由客户端组合起来，就可得到完整数据。</p>
<p>元数据和数据服务器分离在不通额主机之上，还要完成服务器的角色划分。在传统以Google为代表的设计体系结构当中，存放元数据的服务器被称之为名称服务器（NameNode）。存放数据的服务器被称为DataNode。</p>
<p>文件系统的元数据是一类非常密集、但IO量非常小的数据。因此为了高性能、高效通常需要存储在内存当中。为此需要一种机制同步存储在磁盘上，当文件被修改元数据也会被修改。大量的修改操作都是随机的。随机IO通常会非常慢。为了能够高效的使非常密集的文件元数据的修改操作能够快速高效的存在磁盘上，一般不会直接修改元数据的。而是将修改操作的操作请求记录下来。</p>
<p>将随机IO转成顺序IO能够快速同步磁盘进行持久保存。此方法缺点在宕机恢复时，只能通过回放记录在文件中的指令才能将数据还原回来。故其速度很慢。</p>
<p>因此将所有数据的持久存储都存放在第三方存储服务上，</p>
<p>对于数据存储级别的高可用思路</p>
<ul>
<li>节点级冗余</li>
<li>数据级冗余 对每一个块 做副本</li>
<li>分片级冗余
primary shard
replica shard</li>
</ul>
<p>HDFS Hadoop FileSystem</p>
<h3 id="对象存储">对象存储</h3>
<p>每一文件对象，在存储文件时，每一个对象自身大小不固定，是按照文件自身大小来存储的。不区分数据与元数据。每一个数据流自带数据和元数据。数据和数据流是存放在一起的。因此，每一个数据流就叫一个数据对象。数据是直接存放在磁盘之上的，而不在区分数据和元数据。每一对象都有自己自我管理的格式。</p>
<p>Ceph在存数据时，为了避免有一个元数据中心服务器成为整个系统的瓶颈所在，采用了计算方式来解决问题。在存储文件时，对文件做Hash计算映射到对应节点上去。Ceph没有中心节点，没有元数据服务器，任何对象存取都是通过实时计算得到的</p>
<p>从根本上来讲，Ceph在底层是RADOS的存储集群，由多个节点组成。其存储服务只是API叫做librados，Ceph在接口之上提供了几个抽象接口以便可以在传统意义上使用存储服务的逻辑实现存储功能。除用户自己在librados之上还开发了3个接口</p>
<ul>
<li>cephfs</li>
<li>RDB 将Ceph所提供的存储空间模拟成一个个块设备使用的，每个块设备成为一个image，相当于磁盘。块接口，相当于传统硬盘</li>
<li>RADOSGW 更抽象的，能跨互联网使用的对象存储。与Ceph内部对象不一样。Ceph内部的对象是固定大小的存储块，通常只在Ceph集群中使用，基于RPC协议工作。基于互联网的云存储，是基于resetful风格的接口提供的文件存储。每一个文件是一个对象，文件大小各不相同。</li>
</ul>
<h3 id="crush-1">crush</h3>
<p>Ceph内部通过计算方式完成对象路由的计算综合性算法。</p>
<h2 id="ceph存储集群组成">Ceph存储集群组成</h2>
<p>Ceph存储集群，一般可归置为几个组件组成</p>
<h3 id="osd-object-stroage-device-对象存储设备">osd <code>object stroage device</code> 对象存储设备</h3>
<p>    每个主机上有多个osd，每一个osd通常是指==单独的磁盘设备==。osd是真正存放数据的地方。如果使用files会是一个目录。对象存储设备而不是主机。多个主机组合起来称为RADOS Cluster（RADOS存储集群）</p>
<p>    为了使每个osd能够被单独使用和管理，每个osd都会有一个单独的专用的守护进程<code>ceph-osd</code>。osd本身用来存储数据，还包括数据复制、恢复、数据重新均衡、提供监视信息给mon和mgr</p>
<p>    一个集群至少有3个Ceph OSDs，已确保高可用。选择OSD冗余时，应自己指定故障率，故障转移率或故障容忍率。OSD级别故障就在OSD级别冗余，主机级别就跨主机冗余，故障率是机架级别就机架冗余。 在做crush运行图的设定时是应该自己指定的。</p>
<h3 id="monitor-集群元数据服务器">monitor 集群元数据服务器</h3>
<p>    集群内除了存储节点之外，还有另外一种节点 monitor （monitor监视器），用来管理整个集群的，如有多少个节点，每个节点上有多少个OSD，每个OSD是否健康，他会持有整个集群的运行图（运行状态）。mon是用来集中维护集群元数据而非文件元数据。为了维护整个集群能够正常运行而设定的节点，离开此节点集群内部就无法协调。</p>
<p>    在一个主机上运行的守护进程 <code>ceph-mon</code>，守护进程扮演、监视着整个集群所有组件的角色，被称为集群运行图的持有者cluster map（整个集群有多少存储池，每个池中有多少PG，每个PG映射哪个OSD，有多少OSD等等）集群运行图 <code>Cluster Map</code></p>
<p>    monitor负责维护整个进群的认证信息并实行认证，认证协议叫==CephX==协议（ceph内部的认证协议）。monitor用来维护认证信息并实行认证。<code>认证中心</code> monitor自身是无状态的，所以实现均衡认证负载。</p>
<p>    monitor的高可用自己内部直接使用POSIX协议来实现数据冗余，monitor也是节点级冗余，为了确保各节点数据是强一致的，每个节点都可写，写完后会同步到其他节点。为了避免同时写导致的冲突，使用了分布式一致性协议，monitor就是使用POSIX协议来进行分布式协作的。</p>
<h3 id="mgr-manager-1">mgr manager</h3>
<p>    monitor在每一次读数据都是实时查询的，故monitor不适用频繁周期性采集数据的监控操作。在Ceph新版中引入新组建mgr，（早期Ceph版本是没有mgr的）用来专门维护查询类操作，将查询操作按照自己内部空闲方式缓存下来，一旦有监控可及时响应。</p>
<p>    mgr是在一类节点上运行的守护进程，一般为两个活以上节点，此类守护进程被称为<code>ceph-mgr</code>。主要功能在于跟踪运行时的指标数据。如磁盘使用率、CPU使用率。以及集群当前状态，此状态不是内部运行状态，而是查询做监控时的状态。包括存储空间利用率、当前性能指标、节点负载（系统级）等</p>
<h3 id="mds-非必要组件">mds (非必要组件)</h3>
<p>    ==Ceph Metadata Server== 用来代表ceph文件系统而提供的守护进程<code>ceph-mds</code>，如不使用CephFS，此进程是无需启动的。利用底层RADOS存储空间，将存储空间抽象成文件系统，来兼容POSIX file system 提供服务。</p>
<hr>
<p>mgr ods mon才是一个完整的RADOS Cluster</p>
<hr>
<h3 id="管理节点-admin-host">管理节点 Admin Host</h3>
<p>    Ceph通常是分布式集群，为了便于去管理维护整个集群，通常在Ceph集群中找一个专门的节点用来当管理节点。此节点可以连接至每一个节点用来管理节点上的Ceph守护进程。</p>
<p>    Ceph的场馆用管理接口是一句命令行工具程序，例如<code>rados</code> <code>ceph</code> <code>rbd</code>等命令，管理员可以从某个特定的MON节点执行管理操作，单也有人更倾向于使用专用的管理节点</p>
<p>    事实上，专用的管理节点有助于在Ceph相关的程序升级或硬件维护期间为管理员提供一个完整的、独立的并隔离于存储集群之外的操作系统，从而避免因重启意外中断而导致维护操作异常中断。</p>
<h3 id="pool">pool</h3>
<p>    Ceph把他所提供的存储空间（没有目录之类一说）所有对象都是存储在数据平面上，因此，所有对象都不能同名。RADOS将他的存储空间切分为多个分区以便好进行管理。每一个分区叫做一个存储池。存储池的大小是取决于底层的存储空间的。与真正意义上的分区不是一回事。在Ceph中每一个存储池存放的数据了也可能会太大，所以存储池也可以进一步划分（可选）。被称为名称空间（先切分为存储池，每个存储池可进一步被划分成名称空间） 两级逻辑组件。</p>
<h3 id="第三级-pg">第三级 PG</h3>
<p>每一个存储池内部会有多个PG（placement groups 规置组）存在。pool与pg都是抽象的概念。</p>
<h3 id="object">object</h3>
<p>对象是自带元数据的组件，</p>
<ul>
<li>对象id，每一个对象应有一个对象ID在集群内部来引用对象</li>
<li>数据</li>
<li>元数据 key vlaue类型的数据</li>
</ul>
<p>    这些类型打包成一起存储，被称为一个对象。RADOS集群会吧真正存储的每一个文件，切分成N个对象来进行存储的。（切分个数与默认对象切分大小有关）。</p>
<p>    每一个对象都是被单独管理的，都拥有自己的标识符。因此， 同一个文件的object有可能被映射到不同的PG上，PG提交给主机的，由主机负责将对象存储在磁盘（OSD）被存储在不同的OSD上。</p>
<h3 id="文件存储到rados集群">文件存储到RADOS集群</h3>
<p>一般要接入RADOS集群必须通过客户端来实现(LIBRADOS、RBD、CephFS、RADOSGW)，才能接入到集群中来。</p>
<p>当将文件存入Ceph中时，需要通过某一类客户端接入，客户端接入时，需要借助于Ceph存储API接口将其切分为固定大小的存储对象(Date object)，此数据对象究竟被放置在哪个OSD上存放这中间是靠crush来完成的。数据对象被存放在哪个存储池上是固定的。存储池需创建才可使用。但PG是虚拟的中间层。</p>
<p>PG作用</p>
<h4 id="crush算法第一步">crush算法第一步</h4>
<p>任何一个对象被存进RADOS集群中时，一定是向某一个存储池请求的。而后将对象的名字做一致性哈希计算（系列计算），计算完之后，通过顺时针找到落在最近PG。对象是归类在PG中的。每一个对象一定属于某个存储池内的某个PG（但PG不是真实存在的）。接下来还需将PG存到OSD上。</p>
<h4 id="crush算法第二步">crush算法第二步</h4>
<p>需将PG根据存储池的冗余副本数量和存储池的类型找到足量的OSD来存。是以PG为单位进行管理的，所以被称为主PG或活动PG和副本PG，一个PG中的所有对象是统一被管理的（存放在同一个OSD），如果要基于副本管理，写需写入主OSD，由主OSD同步到从OSD之上（同步过程是OSD自己内部管理）。但存储池和crush算法一定要能够确定出哪个是主的那些是从。一般来讲，传统的存储池是1主2从的存储。故其空间利用率是极低的。</p>
<p>Ceph也支持另外的存储池，纠删码存储池，类似于RAID5。存储数据时也会选出多个OSD来，多个OSD不是做副本方式存储，而是将数据切成多块之后每一个OSD上存一部分，另外一些OSD存放校验码，任何一个OSD坏了可以用校验码来计算出来。</p>
<p>故Ceph整体使用分为两部 <code>将对象映射给PG</code>，<code>将PG映射给OSD</code> 此过程是由crush算法来完成的。</p>
<p>最核心的组建RADOS Cluster以对象化的方式吧每一个文件切分成固定大小对象基于对象进行管理的。每一个对象要被基于crush算法实时计算之后映射到集群中某一个OSD之上。而每个集群上有多少个OSD，各个处于什么状态是由monitor负责维护和管理的，甚至监视器要维护整个集群的状态，如OSD、PG等等。故monitor是整个集群的元数据服务器。而不是文件元数据服务器。Ceph没有文件元数据服务器。所有数据访问都是通过实时计算路由的。因此整个集群可以无限制扩展</p>
<h3 id="集群运行图">集群运行图</h3>
<p></p>
<h3 id="数据抽象接口客户端中间层">数据抽象接口（客户端中间层）</h3>
<ul>
<li><strong>Ceph</strong>存储集群提供了基础的对象数据存储服务，客户端可基于RADOS协议和librados API直接与存储系统交互进行对象数据存取</li>
<li><strong>Librados</strong>
<ul>
<li>Librados提供了访问RAD0OS在存储集群支持异步通信的API接口，支持对集群中对象数据的的直接并行访问，用户可通过支持的编程语言开发自定义客户端程序通过RADOS协议与存储系统进行交互</li>
<li>客户端应用程序必须与librados绑定方可连接到RADOS存储集群，因此，用户必须实现安装librados及其依赖后才能编写使用librados的应用程序。</li>
<li>librados API本身使用C++编写的，它额外支持C、Python、Java、和PHP等开发接口</li>
</ul>
</li>
<li>当然，并非所有用户都有能力自定义开发接口以接入RADOS存储集群的需要，为此，Ceph也原生提供了几个较高级别的各户端接口，它们分别是<code>RADOS GateWay</code>（RGW）、<code>Reliable Block Device</code>（RBD）和<code>MDS</code>（MetaData Server），分别为用户提供RESTful、块和POSIX文件系统接口。</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>网络共享 - centos7安装vsftpd</title>
      <link>https://www.oomkill.com/2016/09/vsftp-network-filesystem/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2016/09/vsftp-network-filesystem/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="配置防火墙开启ftp服务器需要的端口">配置防火墙，开启FTP服务器需要的端口</h2>
<p>CentOS 7.0默认使用的是firewall作为防火墙，这里改为iptables防火墙。</p>
<ol>
<li><strong>关闭firewall</strong>：</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">systemctl stop firewalld.service 	    <span class="c1">#停止firewall</span>
</span></span><span class="line"><span class="cl">systemctl disable firewalld.service 	<span class="c1">#禁止firewall开机启动</span>
</span></span></code></pre></td></tr></table>
</div>
</div><ol start="2">
<li><strong>安装iptables防火墙</strong></li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">yum install iptables-services 					<span class="c1"># 安装</span>
</span></span><span class="line"><span class="cl">vi /etc/sysconfig/iptables 						<span class="c1"># 编辑防火墙配置文件</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Firewall configuration written by system-config-firewall</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Manual customization of this file is not recommended.</span>
</span></span><span class="line"><span class="cl">*filter
</span></span><span class="line"><span class="cl">:INPUT ACCEPT <span class="o">[</span>0:0<span class="o">]</span>
</span></span><span class="line"><span class="cl">:FORWARD ACCEPT <span class="o">[</span>0:0<span class="o">]</span>
</span></span><span class="line"><span class="cl">:OUTPUT ACCEPT <span class="o">[</span>0:0<span class="o">]</span>
</span></span><span class="line"><span class="cl">-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
</span></span><span class="line"><span class="cl">-A INPUT -p icmp -j ACCEPT
</span></span><span class="line"><span class="cl">-A INPUT -i lo -j ACCEPT
</span></span><span class="line"><span class="cl">-A INPUT -m state --state NEW -m tcp -p tcp --dport <span class="m">22</span> -j ACCEPT
</span></span><span class="line"><span class="cl">-A INPUT -m state --state NEW -m tcp -p tcp --dport <span class="m">21</span> -j ACCEPT
</span></span><span class="line"><span class="cl">-A INPUT -m state --state NEW -m tcp -p tcp --dport 10060:10090 -j ACCEPT
</span></span><span class="line"><span class="cl">-A INPUT -j REJECT --reject-with icmp-host-prohibited
</span></span><span class="line"><span class="cl">-A FORWARD -j REJECT --reject-with icmp-host-prohibited
</span></span><span class="line"><span class="cl">COMMIT
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">:wq! <span class="c1">#保存退出</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">systemctl restart iptables.service <span class="c1">#最后重启防火墙使配置生效</span>
</span></span><span class="line"><span class="cl">systemctl <span class="nb">enable</span> iptables.service <span class="c1">#设置防火墙开机启动</span>
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<blockquote>
<p><strong>说明</strong>：</p>
<p>21端口是ftp服务端口；10060到10090是Vsftpd被动模式需要的端口，可自定义一段大于1024的tcp端口。</p>
</blockquote>
<hr>
<h2 id="关闭selinux">关闭SELINUX</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">vi /etc/selinux/config
</span></span><span class="line"><span class="cl"><span class="c1">#SELINUX=enforcing 	    # 注释掉</span>
</span></span><span class="line"><span class="cl"><span class="c1">#SELINUXTYPE=targeted   # 注释掉</span>
</span></span><span class="line"><span class="cl"><span class="nv">SELINUX</span><span class="o">=</span>disabled 		<span class="c1"># 增加</span>
</span></span><span class="line"><span class="cl">:wq! <span class="c1">#保存退出</span>
</span></span><span class="line"><span class="cl">setenforce <span class="m">0</span>            <span class="c1"># 使配置立即生效</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="安装vsftpd">安装vsftpd</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 安装vsftpd</span>
</span></span><span class="line"><span class="cl">yum install -y vsftpd 
</span></span><span class="line"><span class="cl"><span class="c1"># 安装vsftpd虚拟用户配置依赖包</span>
</span></span><span class="line"><span class="cl">yum install -y psmisc net-tools systemd-devel libdb-devel perl-DBI 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">systemctl start vsftpd.service <span class="c1"># 启动</span>
</span></span><span class="line"><span class="cl">systemctl <span class="nb">enable</span> vsftpd.service <span class="c1"># 设置vsftpd开机启动</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="配置vsftp服务器">配置vsftp服务器</h2>
<p><strong>备份默认配置文件</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">cp /etc/vsftpd/vsftpd.conf /etc/vsftpd/vsftpd.conf-bak
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>执行以下命令进行设置</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sed -i <span class="s2">&#34;s/anonymous_enable=YES/anonymous_enable=NO/g&#34;</span> <span class="s1">&#39;/etc/vsftpd/vsftpd.conf&#39;</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s/#anon_upload_enable=YES/anon_upload_enable=NO/g&#34;</span> <span class="s1">&#39;/etc/vsftpd/vsftpd.conf&#39;</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s/#anon_mkdir_write_enable=YES/anon_mkdir_write_enable=YES/g&#34;</span> <span class="s1">&#39;/etc/vsftpd/vsftpd.conf&#39;</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s/#chown_uploads=YES/chown_uploads=NO/g&#34;</span> <span class="s1">&#39;/etc/vsftpd/vsftpd.conf&#39;</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s/#async_abor_enable=YES/async_abor_enable=YES/g&#34;</span> <span class="s1">&#39;/etc/vsftpd/vsftpd.conf&#39;</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s/#ascii_upload_enable=YES/ascii_upload_enable=YES/g&#34;</span> <span class="s1">&#39;/etc/vsftpd/vsftpd.conf&#39;</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s/#ascii_download_enable=YES/ascii_download_enable=YES/g&#34;</span> <span class="s1">&#39;/etc/vsftpd/vsftpd.conf&#39;</span>
</span></span><span class="line"><span class="cl">sed -i <span class="s2">&#34;s/#ftpd_banner=Welcome to blah FTP service./ftpd_banner=Welcome to FTP service./g&#34;</span> <span class="s1">&#39;/etc/vsftpd/vsftpd.conf&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> -e <span class="s2">&#34;use_localtime=YES\nlisten_port=21\nchroot_local_user=YES\nidle_session_timeout=300
</span></span></span><span class="line"><span class="cl"><span class="s2">\ndata_connection_timeout=1\nguest_enable=YES\nguest_username=vsftpd
</span></span></span><span class="line"><span class="cl"><span class="s2">\nuser_config_dir=/etc/vsftpd/vconf\nvirtual_use_local_privs=YES
</span></span></span><span class="line"><span class="cl"><span class="s2">\npasv_min_port=10060\npasv_max_port=10090
</span></span></span><span class="line"><span class="cl"><span class="s2">\naccept_timeout=5\nconnect_timeout=1&#34;</span> &gt;&gt; /etc/vsftpd/vsftpd.conf
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>这是配置好的配置文件</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">anonymous_enable</span><span class="o">=</span>NO
</span></span><span class="line"><span class="cl"><span class="nv">local_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">write_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">local_umask</span><span class="o">=</span><span class="m">022</span>
</span></span><span class="line"><span class="cl"><span class="nv">anon_upload_enable</span><span class="o">=</span>NO
</span></span><span class="line"><span class="cl"><span class="nv">anon_mkdir_write_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">dirmessage_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">xferlog_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">connect_from_port_20</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">chown_uploads</span><span class="o">=</span>NO
</span></span><span class="line"><span class="cl"><span class="nv">xferlog_std_format</span><span class="o">=</span>YES 	
</span></span><span class="line"><span class="cl"><span class="nv">async_abor_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">ascii_upload_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">ascii_download_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">ftpd_banner</span><span class="o">=</span>Welcome to FTP service.
</span></span><span class="line"><span class="cl"><span class="nv">listen</span><span class="o">=</span>NO
</span></span><span class="line"><span class="cl"><span class="nv">listen_ipv6</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">pam_service_name</span><span class="o">=</span>vsftpd
</span></span><span class="line"><span class="cl"><span class="nv">userlist_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">tcp_wrappers</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">use_localtime</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">listen_port</span><span class="o">=</span><span class="m">21</span>
</span></span><span class="line"><span class="cl"><span class="nv">chroot_local_user</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">idle_session_timeout</span><span class="o">=</span><span class="m">300</span>
</span></span><span class="line"><span class="cl"><span class="nv">data_connection_timeout</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="nv">guest_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">guest_username</span><span class="o">=</span>vsftpd
</span></span><span class="line"><span class="cl"><span class="nv">user_config_dir</span><span class="o">=</span>/etc/vsftpd/vconf
</span></span><span class="line"><span class="cl"><span class="nv">virtual_use_local_privs</span><span class="o">=</span>NO
</span></span><span class="line"><span class="cl"><span class="nv">pasv_min_port</span><span class="o">=</span><span class="m">10060</span>
</span></span><span class="line"><span class="cl"><span class="nv">pasv_max_port</span><span class="o">=</span><span class="m">10090</span>
</span></span><span class="line"><span class="cl"><span class="nv">accept_timeout</span><span class="o">=</span><span class="m">5</span>
</span></span><span class="line"><span class="cl"><span class="nv">connect_timeout</span><span class="o">=</span><span class="m">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="建立虚拟用户名单文件">建立虚拟用户名单文件</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">touch /etc/vsftpd/virtualUsers
</span></span><span class="line"><span class="cl"><span class="c1"># 编辑虚拟用户名单文件：（第一行账号，第二行密码，注意：不能使用root做用户名，系统保留）</span>
</span></span><span class="line"><span class="cl">vi /etc/vsftpd/virtualUsers
</span></span><span class="line"><span class="cl">public
</span></span><span class="line"><span class="cl"><span class="m">123456</span>
</span></span><span class="line"><span class="cl">admin
</span></span><span class="line"><span class="cl"><span class="m">111111</span>
</span></span><span class="line"><span class="cl">:wq! 	<span class="c1">#保存退出</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="生成虚拟用户数据文件">生成虚拟用户数据文件</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">db_load -T -t <span class="nb">hash</span> -f /etc/vsftpd/virtusers /etc/vsftpd/virtualUsers.db
</span></span><span class="line"><span class="cl">chmod <span class="m">600</span> /etc/vsftpd/virtualUsers.db 	<span class="c1">#设定PAM验证文件，并指定对虚拟用户数据库文件进行读取</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="在etcpamdvsftpd的文件头部加入以下信息">在/etc/pam.d/vsftpd的文件头部加入以下信息</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 修改前先备份 </span>
</span></span><span class="line"><span class="cl">cp /etc/pam.d/vsftpd /etc/pam.d/vsftpdbak
</span></span><span class="line"><span class="cl">vi /etc/pam.d/vsftpd
</span></span><span class="line"><span class="cl">auth sufficient /lib64/security/pam_userdb.so <span class="nv">db</span><span class="o">=</span>/etc/vsftpd/virtualUsers
</span></span><span class="line"><span class="cl">account sufficient /lib64/security/pam_userdb.so <span class="nv">db</span><span class="o">=</span>/etc/vsftpd/virtualUsers
</span></span></code></pre></td></tr></table>
</div>
</div><hr>
<blockquote>
<p>注：</p>
</blockquote>
<p>在后面加入无效
如果系统为32位，上面改为lib，否则配置失败</p>
<hr>
<h2 id="新建系统用户vsftpd">新建系统用户vsftpd</h2>
<p>用户目录为/home/ftp 用户登录终端设为/bin/false(即使之不能登录系统)</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">useradd vsftpd -d /home/ftp -s /bin/false
</span></span><span class="line"><span class="cl">chown vsftpd:vsftpd /home/ftp -R
</span></span><span class="line"><span class="cl">chown www:www /home/www-R 	<span class="c1"># 如果虚拟用户的宿主用户为www，需要这样设置。</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="建立虚拟用户个人vsftp的配置文件">建立虚拟用户个人Vsftp的配置文件</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 创建虚拟用户用户的权限配置文件</span>
</span></span><span class="line"><span class="cl">mkdir /etc/vsftpd/vconf
</span></span><span class="line"><span class="cl"><span class="c1"># 进入该目录</span>
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /etc/vsftpd/vconf
</span></span><span class="line"><span class="cl"><span class="c1"># 创建public与admin两个用户的配置文件</span>
</span></span><span class="line"><span class="cl">touch public admin
</span></span><span class="line"><span class="cl"><span class="c1"># 编辑用户admin配置文件，其他的跟这个配置文件类似</span>
</span></span><span class="line"><span class="cl">vi admin
</span></span><span class="line"><span class="cl"><span class="nv">local_root</span><span class="o">=</span>/home/ftp/
</span></span><span class="line"><span class="cl"><span class="nv">write_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">anon_world_readable_only</span><span class="o">=</span>NO
</span></span><span class="line"><span class="cl"><span class="nv">anon_upload_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">anon_mkdir_write_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">anon_other_write_enable</span><span class="o">=</span>YES
</span></span><span class="line"><span class="cl"><span class="nv">allow_writeable_chroot</span><span class="o">=</span>YES
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="最后重启vsftpd服务器">最后重启vsftpd服务器</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">systemctl restart vsftpd.service
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="备注">备注</h2>
<ul>
<li>guest_username=vsftpd  #指定虚拟用户的宿主用户（就是我们前面新建的用户）</li>
<li>guest_username=www  #如果ftp目录是指向网站根目录，用来上传网站程序，可以指定虚拟用户的宿主用户为nginx运行账户www，可以避免很多权限设置问题。</li>
<li></li>
</ul>
<h2 id="vsftpd配置文件详解">vsftpd配置文件详解</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span><span class="lnt">121
</span><span class="lnt">122
</span><span class="lnt">123
</span><span class="lnt">124
</span><span class="lnt">125
</span><span class="lnt">126
</span><span class="lnt">127
</span><span class="lnt">128
</span><span class="lnt">129
</span><span class="lnt">130
</span><span class="lnt">131
</span><span class="lnt">132
</span><span class="lnt">133
</span><span class="lnt">134
</span><span class="lnt">135
</span><span class="lnt">136
</span><span class="lnt">137
</span><span class="lnt">138
</span><span class="lnt">139
</span><span class="lnt">140
</span><span class="lnt">141
</span><span class="lnt">142
</span><span class="lnt">143
</span><span class="lnt">144
</span><span class="lnt">145
</span><span class="lnt">146
</span><span class="lnt">147
</span><span class="lnt">148
</span><span class="lnt">149
</span><span class="lnt">150
</span><span class="lnt">151
</span><span class="lnt">152
</span><span class="lnt">153
</span><span class="lnt">154
</span><span class="lnt">155
</span><span class="lnt">156
</span><span class="lnt">157
</span><span class="lnt">158
</span><span class="lnt">159
</span><span class="lnt">160
</span><span class="lnt">161
</span><span class="lnt">162
</span><span class="lnt">163
</span><span class="lnt">164
</span><span class="lnt">165
</span><span class="lnt">166
</span><span class="lnt">167
</span><span class="lnt">168
</span><span class="lnt">169
</span><span class="lnt">170
</span><span class="lnt">171
</span><span class="lnt">172
</span><span class="lnt">173
</span><span class="lnt">174
</span><span class="lnt">175
</span><span class="lnt">176
</span><span class="lnt">177
</span><span class="lnt">178
</span><span class="lnt">179
</span><span class="lnt">180
</span><span class="lnt">181
</span><span class="lnt">182
</span><span class="lnt">183
</span><span class="lnt">184
</span><span class="lnt">185
</span><span class="lnt">186
</span><span class="lnt">187
</span><span class="lnt">188
</span><span class="lnt">189
</span><span class="lnt">190
</span><span class="lnt">191
</span><span class="lnt">192
</span><span class="lnt">193
</span><span class="lnt">194
</span><span class="lnt">195
</span><span class="lnt">196
</span><span class="lnt">197
</span><span class="lnt">198
</span><span class="lnt">199
</span><span class="lnt">200
</span><span class="lnt">201
</span><span class="lnt">202
</span><span class="lnt">203
</span><span class="lnt">204
</span><span class="lnt">205
</span><span class="lnt">206
</span><span class="lnt">207
</span><span class="lnt">208
</span><span class="lnt">209
</span><span class="lnt">210
</span><span class="lnt">211
</span><span class="lnt">212
</span><span class="lnt">213
</span><span class="lnt">214
</span><span class="lnt">215
</span><span class="lnt">216
</span><span class="lnt">217
</span><span class="lnt">218
</span><span class="lnt">219
</span><span class="lnt">220
</span><span class="lnt">221
</span><span class="lnt">222
</span><span class="lnt">223
</span><span class="lnt">224
</span><span class="lnt">225
</span><span class="lnt">226
</span><span class="lnt">227
</span><span class="lnt">228
</span><span class="lnt">229
</span><span class="lnt">230
</span><span class="lnt">231
</span><span class="lnt">232
</span><span class="lnt">233
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-zed" data-lang="zed"><span class="line"><span class="cl"><span class="n">allow_anon_ssl</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">只有</span><span class="n">ss1_enable激活了才可以启用此项</span><span class="err">。如果设置为</span><span class="n">YES</span><span class="err">，匿名用户将容许使用安全的</span><span class="n">SSL连接服务器</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">anon_mkdir_write_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果设为</span><span class="n">YES</span><span class="err">，匿名用户将容许在指定的环境下创建新目录。如果此项要生效，那么配置</span><span class="n">write_enable必须被激活</span><span class="err">，并且匿名用户必须在其父目录有写权限。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">anon_other_write_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果设置为</span><span class="n">YES</span><span class="err">，匿名用户将被授予较大的写权限，例如删除和改名。一般不建议这么做，除非想完全授权。也可以和</span><span class="n">cmds_allowed配合来实现控制</span><span class="err">，这样可以达到文件续传功能。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">anon_upload_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果设为</span><span class="n">YES</span><span class="err">，匿名用户就容许在指定的环境下上传文件。如果此项要生效，那么配置</span><span class="n">write_enable必须激活</span><span class="err">。并且匿名用户必须在相关目录有写权限。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">anon_world_readable_only</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">启用的时候，匿名用户只容许下载完全可读的文件，这也就容许了</span><span class="n">ftp用户拥有对文件的所有权</span><span class="err">，尤其是在上传的情况下。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">anonymous_enable</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">控制是否容许匿名用户登录。如果容许，那么“</span><span class="n">ftp</span><span class="err">”和“</span><span class="n">anonymous</span><span class="err">”都将被视为“</span><span class="n">anonymous</span><span class="err">&#34;而容许登录。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">ascii_download_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">启用时，用户下载时将以</span><span class="n">ASCII模式传送文件</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">ascii_upload_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">启用时，用户上传时将以</span><span class="n">ASCII模式传送文件</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">async_abor_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">启用时，一个特殊的</span><span class="n">FTP命令</span><span class="err">&#34;</span><span class="n">async</span><span class="w"> </span><span class="n">ABOR</span><span class="err">”将容许使用。只有不正常的</span><span class="n">FTP客户端要使用这一点</span><span class="err">。而且，这个功能又难于操作，所以，默认是把它关闭了。但是，有些客户端在取消一个传送的时候会被挂死</span><span class="p">(</span><span class="err">注：估计是客户端无响应了</span><span class="p">)</span><span class="err">，那你只有启用这个功能才能避免这种情况。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">background</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">启用时，并且</span><span class="n">VSFTPD是</span><span class="err">“</span><span class="n">listen</span><span class="err">”模式启动的</span><span class="p">(</span><span class="err">注：就是</span><span class="n">standalone模式</span><span class="p">)</span><span class="err">，</span><span class="n">VSFTPD将把监听进程置于后台</span><span class="err">。但访问</span><span class="n">VSFTPD时</span><span class="err">，控制台将立即被返回到</span><span class="n">SHELL</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">check_shell</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">注意：这个选项只对非</span><span class="n">PAM结构的VSFTPD才有效</span><span class="err">。如果关闭，</span><span class="nn">VSFTPD将不检查/etc/</span><span class="n">shells以判定本地登录的用户是否有一个可用的SHELL</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">chmod_enable</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">启用时，将容许使用</span><span class="n">SITE</span><span class="w"> </span><span class="n">CHMOD命令</span><span class="err">。注意，这只能用于本地用户。匿名用户绝不能使用</span><span class="n">SITE</span><span class="w"> </span><span class="n">CHMOD</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">chown_uploads</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果启用，所以匿名用户上传的文件的所有者将变成在</span><span class="n">chown_username里指定的用户</span><span class="err">。这对管理</span><span class="n">FTP很有用</span><span class="err">，也许也对安全有益。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">chroot_list_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果激活，你要提供一个用户列表，表内的用户将在登录后被放在其</span><span class="n">home目录</span><span class="err">，锁定在虚根下</span><span class="p">(</span><span class="err">注：进入</span><span class="n">FTP后</span><span class="err">，</span><span class="n">PWD一下</span><span class="err">，可以看到当前目录是&#34;</span><span class="o">/</span><span class="err">&#34;</span><span class="p">,</span><span class="w"> </span><span class="err">这就是虚根。是</span><span class="n">FTP的根目录</span><span class="err">，并非</span><span class="n">FTP服务器系统的根目录</span><span class="p">)</span><span class="err">。如果</span><span class="n">chroot_local_user设为YES后</span><span class="err">，其含义会发生一点变化。在这种情况下，这个列表内的用户将不被锁定在虚根下。默认情况下，这个列表文件是</span><span class="o">/</span><span class="nn">etc/</span><span class="n">vsftpd</span><span class="p">.</span><span class="n">chroot_list</span><span class="p">,</span><span class="w"> </span><span class="err">但你也可以通过修改</span><span class="n">chroot_list_file来改变默认值</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">chroot_local_user</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果设为</span><span class="n">YES</span><span class="err">，本地用户登录后将被</span><span class="p">(</span><span class="err">默认地</span><span class="p">)</span><span class="err">锁定在虚根下，并被放在他的</span><span class="n">home目录下</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">警告：这个配置项有安全的意味，特别是如果用户有上传权限或者可使用</span><span class="n">SHELL的话</span><span class="err">。在你确定的前提下，再启用它。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">注意，这种安全暗示并非只存在于</span><span class="n">VSFTPD</span><span class="err">，其实是广泛用于所有的希望把用户锁定在虚根下的</span><span class="n">FTP软件</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">connect_from_port_20</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">这用来控制服务器是否使用20端口号来做数据传输。为安全起见，有些客户坚持启用。相反，关闭这一项可以让</span><span class="n">VSFTPD更加大众化</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">deny_email_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果激活，你要提供一个关于匿名用户的密码</span><span class="n">E</span><span class="o">-</span><span class="n">MAIL表</span><span class="p">(</span><span class="err">注：我们都知道，匿名用户是用邮件地址做密码的</span><span class="p">)</span><span class="err">以阻止以这些密码登录的匿名用户。默认情况下，这个列表文件是</span><span class="o">/</span><span class="nn">etc/</span><span class="n">vsftpd</span><span class="p">.</span><span class="n">banner_emails</span><span class="err">，但你也可以通过设置</span><span class="n">banned_email_file来改变默认值</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">dirlist_enable</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果设置为</span><span class="n">NO</span><span class="err">，所有的列表命令</span><span class="p">(</span><span class="err">注：如</span><span class="n">ls</span><span class="p">)</span><span class="err">都将被返回“</span><span class="kd">permission</span><span class="w"> </span><span class="n">denied</span><span class="err">”提示。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">dirmessage_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果启用，</span><span class="n">FTP服务器的用户在首次进入一个新目录的时候将显示一段信息</span><span class="err">。默认情况下，会在这个目录中查找</span><span class="p">.</span><span class="n">message文件</span><span class="err">，但你也可以通过更改</span><span class="n">message_file来改变默认值</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">download_enable</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果设为</span><span class="n">NO</span><span class="err">，下载请求将返回“</span><span class="kd">permission</span><span class="w"> </span><span class="n">denied</span><span class="err">”。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">dual_log_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果启用，两个</span><span class="n">LOG文件会各自产生</span><span class="err">，默认的是</span><span class="o">/</span><span class="nn">var/log/xferlog和/var/log/</span><span class="n">vsftpd</span><span class="p">.</span><span class="n">log</span><span class="err">。前一个是</span><span class="n">wu</span><span class="o">-</span><span class="n">ftpd格式的LOG</span><span class="err">，能被通用工具分析。后一个是</span><span class="n">VSFTPD的专用LOG格式</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">force_dot_files</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果激活，即使客户端没有使用“</span><span class="n">a</span><span class="err">”标记，</span><span class="p">(</span><span class="n">FTP里</span><span class="p">)</span><span class="err">以</span><span class="p">.</span><span class="err">开始的文件和目录都会显示在目录资源列表里。但是把&#34;</span><span class="p">.</span><span class="err">&#34;和&#34;</span><span class="p">..</span><span class="err">&#34;不会显示。</span><span class="p">(</span><span class="err">注：即</span><span class="n">LINUX下的当前目录和上级目录不会以</span><span class="err">‘</span><span class="p">.</span><span class="err">’或‘</span><span class="p">..</span><span class="err">’方式显示</span><span class="p">)</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">force_local_data_ssl</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">只有在</span><span class="n">ssl_enable激活后才能启用</span><span class="err">。如果启用，所有的非匿名用户将被强迫使用安全的</span><span class="n">SSL登录以在数据线路上收发数据</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">force_local_logins_ssl</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">只有在</span><span class="n">ssl_enable激活后才能启用</span><span class="err">。如果启用，所有的非匿名用户将被强迫使用安全的</span><span class="n">SSL登录以发送密码</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">guest_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果启用，所有的非匿名用户登录时将被视为”游客“，其名字将被映射为</span><span class="n">guest_username里所指定的名字</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">hide_ids</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果启用，目录资源列表里所有用户和组的信息将显示为&#34;</span><span class="n">ftp</span><span class="err">&#34;</span><span class="p">.</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">listen</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果启用，</span><span class="n">VSFTPD将以独立模式</span><span class="p">(</span><span class="n">standalone</span><span class="p">)</span><span class="err">运行，也就是说可以不依赖于</span><span class="n">inetd或者类似的东东启动</span><span class="err">。直接运行</span><span class="n">VSFTPD的可执行文件一次</span><span class="err">，然后</span><span class="n">VSFTPD就自己去监听和处理连接请求了</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">listen_ipv6</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">类似于</span><span class="n">listen参数的功能</span><span class="err">，但有一点不同，启用后</span><span class="n">VSFTPD会去监听IPV6套接字而不是IPV4的</span><span class="err">。这个设置和</span><span class="n">listen的设置互相排斥</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">local_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">用来控制是否容许本地用户登录。如果启用，</span><span class="o">/</span><span class="nn">etc/</span><span class="n">passwd里面的正常用户的账号将被用来登录</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">log_ftp_protocol</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">启用后，如果</span><span class="n">xferlog_std_format没有被激活</span><span class="err">，所有的</span><span class="n">FTP请求和反馈信息将被纪录</span><span class="err">。这常用于调试</span><span class="p">(</span><span class="n">debugging</span><span class="p">)</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">ls_recurse_enable</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果启用，&#34;</span><span class="n">ls</span><span class="w"> </span><span class="o">-</span><span class="n">R</span><span class="err">&#34;将被容许使用。这是为了避免一点点安全风险。因为在一个大的站点内，在目录顶层使用这个命令将消耗大量资源。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">no_anon_password</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果启用，</span><span class="n">VSFTPD将不会向匿名用户询问密码</span><span class="err">。匿名用户将直接登录。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">no_log_lock</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">启用时，</span><span class="n">VSFTPD在写入LOG文件时将不会把文件锁住</span><span class="err">。这一项一般不启用。它对一些工作区操作系统问题，如</span><span class="n">Solaris</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Veritas文件系统共存时有用</span><span class="err">。因为那在试图锁定</span><span class="n">LOG文件时</span><span class="err">，有时候看上去象被挂死</span><span class="p">(</span><span class="err">无响应</span><span class="p">)</span><span class="err">了。</span><span class="p">(</span><span class="err">注：这我也不是很理解。所以翻译未必近乎原意。原文如下：</span><span class="n">It</span><span class="w"> </span><span class="n">exists</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">workaround</span><span class="w"> </span><span class="n">operating</span><span class="w"> </span><span class="n">system</span><span class="w"> </span><span class="n">bugs</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Solaris</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Veritas</span><span class="w"> </span><span class="n">filesystem</span><span class="w"> </span><span class="n">combination</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">been</span><span class="w"> </span><span class="n">observed</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">sometimes</span><span class="w"> </span><span class="n">exhibit</span><span class="w"> </span><span class="n">hangs</span><span class="w"> </span><span class="n">trying</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">lock</span><span class="w"> </span><span class="n">log</span><span class="w"> </span><span class="n">files</span><span class="p">.)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">one_process_model</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果你的</span><span class="n">LINUX核心是2</span><span class="p">.</span><span class="err">4的，那么也许能使用一种不同的安全模式，即一个连接只用一个进程。只是一个小花招，但能提高</span><span class="n">FTP的性能</span><span class="err">。请确定需要后再启用它，而且也请确定你的站点是否会有大量的人同时访问。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">passwd_chroot_enable</span><span class="w"> </span><span class="p">(</span><span class="err">注：这段自己看，无语</span><span class="p">...)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">　　</span><span class="n">if</span><span class="w"> </span><span class="n">enabled</span><span class="p">,</span><span class="w"> </span><span class="n">along</span><span class="w"> </span><span class="n">with</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">　　</span><span class="p">.</span><span class="n">BR</span><span class="w"> </span><span class="n">chroot_local_user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">　　</span><span class="p">,</span><span class="w"> </span><span class="n">then</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">chroot</span><span class="p">()</span><span class="w"> </span><span class="n">jail</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="n">may</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">specified</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">per</span><span class="o">-</span><span class="n">user</span><span class="w"> </span><span class="n">basis</span><span class="p">.</span><span class="w"> </span><span class="n">Each</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">　　</span><span class="n">user</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">jail</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">derived</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">home</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">string</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="o">/</span><span class="nn">etc/</span><span class="n">passwd</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">　　</span><span class="n">occurrence</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">/</span><span class="p">.</span><span class="o">/</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">home</span><span class="w"> </span><span class="n">directory</span><span class="w"> </span><span class="n">string</span><span class="w"> </span><span class="n">denotes</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">jail</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">at</span><span class="w"> </span><span class="n">that</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">　　</span><span class="n">particular</span><span class="w"> </span><span class="n">location</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">path</span><span class="p">.</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">　　</span><span class="err">默认值：</span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">pasv_enable</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果你不想使用被动方式获得数据连接，请设为</span><span class="n">NO</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">pasv_promiscuous</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果你想关闭被动模式安全检查</span><span class="p">(</span><span class="err">这个安全检查能确保数据连接源于同一个</span><span class="n">IP地址</span><span class="p">)</span><span class="err">的话，设为</span><span class="n">YES</span><span class="err">。确定后再启用它</span><span class="p">(</span><span class="err">注：原话是：只有你清楚你在做什么时才启用它</span><span class="o">!</span><span class="p">)</span><span class="err">合理的用法是：在一些安全隧道配置环境下，或者更好地支持</span><span class="n">FXP时</span><span class="p">(</span><span class="err">才启用它</span><span class="p">)</span><span class="err">。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">port_enable</span><span class="w"> </span><span class="n">YES</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果你想关闭以端口方式获得数据连接时，请关闭它。</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">port_promiscuous</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果你想关闭端口安全检查</span><span class="p">(</span><span class="err">这个检查可以确保对外的</span><span class="p">(</span><span class="n">outgoing</span><span class="p">)</span><span class="err">数据线路只通向客户端</span><span class="p">)</span><span class="err">时，请关闭它。确认后再做</span><span class="o">!</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">run_as_launching_user</span><span class="w"> </span><span class="n">NO</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="err">#</span><span class="w"> </span><span class="err">如果你想让一个用户能启动</span><span class="n">VSFTPD的时候</span><span class="err">，可以设为</span><span class="n">YES</span><span class="err">。当</span><span class="n">ROOT用户不能去启动VSFTPD的时候会很有用</span><span class="p">(</span><span class="err">注：应该不是说</span><span class="n">ROOT用户没有权限启动VSFTPD</span><span class="err">，而是因为别的，例如安全限制，而不能以</span><span class="n">ROOT身份直接启动VSFTPD</span><span class="p">)</span><span class="err">。强烈警告</span><span class="o">!!</span><span class="err">别启用这一项，除非你完全清楚你在做什么</span><span class="p">(</span><span class="o">:</span><span class="err">无语</span><span class="p">....)</span><span class="o">!!!</span><span class="err">随意地启动这一项会导致非常严重的安全问题，特别是</span><span class="n">VSFTPD没有或者不能使用虚根技术来限制文件访问的时候</span><span class="p">(</span><span class="err">甚至</span><span class="n">VSFTPD是被ROOT启动的</span><span class="p">)</span><span class="err">。有一个愚蠢的替代方案是启用</span><span class="n">deny_file</span><span class="err">，将其设置为</span><span class="p">{</span><span class="cm">/*,*..*}等，但其可靠性却不能和虚根相比，也靠不住。
</span></span></span><span class="line"><span class="cl"><span class="cm">如果启用这一项，其他配置项的限制也会生效。例如，非匿名登录请求，上传文件的所有权的转换，用于连接的20端口和低于1024的监听端口将不会工作。其他一些配置项也可能被影响。
</span></span></span><span class="line"><span class="cl"><span class="cm">secure_email_list_enable NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果你想只接受以指定E-MAIL地址登录的匿名用户的话，启用它。这一般用来在不必要用虚拟用户的情况下，以较低的安全限制去访问较低安全级别的资源。如果启用它，匿名用户除非用在email_password_file里指定的E-MAIL做为密码，否则不能登录。这个文件的格式是一个密码一行，而且没有额外的空格(注：whitespace,译为空格，不知道是否正确)。
</span></span></span><span class="line"><span class="cl"><span class="cm">默认的文件名是：/etc/vsftpd.email_passwords.
</span></span></span><span class="line"><span class="cl"><span class="cm">session_support NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这将配置是否让VSFTPD去尝试管理登录会话。如果VSFTPD管理会话，它会尝试并更新utmp和wtmp。它也会打开一个pam会话(pam_session)，直到LOGOUT才会关闭它，如果使用PAM进行认证的话。如果你不需要会话纪录，或者想VSFTPD运行更少的进程，或者让它更大众化，你可以关闭它。注：utmp和wtmp只在有PAM的环境下才支持。
</span></span></span><span class="line"><span class="cl"><span class="cm">setproctitle_enable NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，VSFTPD将在系统进程列表中显示会话状态信息。换句话说，进程名字将变成VSFTPD会话当前正在执行的动作(等待，下载等等)。为了安全目的，你可以关闭这一项。
</span></span></span><span class="line"><span class="cl"><span class="cm">ssl_enable NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，vsftpd将启用openSSL，通过SSL支持安全连接。这个设置用来控制连接(包括登录)和数据线路。同时，你的客户端也要支持SSL才行。注意：小心启用此项.VSFTPD不保证OpenSSL库的安全性。启用此项，你必须确信你安装的OpenSSL库是安全的。
</span></span></span><span class="line"><span class="cl"><span class="cm">ssl_sslv2 NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 要激活ssl_enable才能启用它。如果启用，将容许SSL V2协议的连接。TLS V1连接将是首选。
</span></span></span><span class="line"><span class="cl"><span class="cm">ssl_sslv3 NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 要激活ssl_enable才能启用它。如果启用，将容许SSL V3协议的连接。TLS V1连接将是首选。
</span></span></span><span class="line"><span class="cl"><span class="cm">ssl_tlsv1 YES
</span></span></span><span class="line"><span class="cl"><span class="cm"># 要激活ssl_enable才能启用它。如果启用，将容许TLS V1协议的连接。TLS V1连接将是首选。
</span></span></span><span class="line"><span class="cl"><span class="cm">syslog_enable NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，系统log将取代vsftpd的log输出到/var/log/vsftpd.log.FTPD的了log工具将不工作。
</span></span></span><span class="line"><span class="cl"><span class="cm">tcp_wrappers NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，vsftpd将被tcp_wrappers所支持。进入的(incoming)连接将被tcp_wrappers访问控制所反馈。如果tcp_wrappers设置了VSFTPD_LOAD_CONF环境变量，那么vsftpd将尝试调用这个变量所指定的配置。
</span></span></span><span class="line"><span class="cl"><span class="cm">ext_userdb_names NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 默认情况下，在文件列表中，数字ID将被显示在用户和组的区域。你可以编辑这个参数以使其使用数字ID变成文字。为了保证FTP性能，默认情况下，此项被关闭。
</span></span></span><span class="line"><span class="cl"><span class="cm">tilde_user_enable NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，vsftpd将试图解析类似于~chris/pics的路径名(一个&#34;~&#34;(tilde)后面跟着个用户名)。注意，vsftpd有时会一直解析路径名&#34;~&#34;和&#34;~/&#34;(在这里，～被解析成内部登录目录)。～用户路径(～user paths)只有在当前虚根下找到/etc/passwd文件时才被解析。
</span></span></span><span class="line"><span class="cl"><span class="cm">use_localtime NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，vsftpd在显示目录资源列表的时候，在显示你的本地时间。而默认的是显示GMT(格林尼治时间)。通过MDTM FTP命令来显示时间的话也会被这个设置所影响。
</span></span></span><span class="line"><span class="cl"><span class="cm">use_sendfile YES
</span></span></span><span class="line"><span class="cl"><span class="cm"># 一个内部设定，用来测试在你的平台上使用sendfile()系统呼叫的相关好处(benefit).
</span></span></span><span class="line"><span class="cl"><span class="cm">userlist_deny YES
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这个设置在userlist_enable被激活后能被验证。如果你设置为NO，那么只有在userlist_file里明确列出的用户才能登录。如果是被拒绝登录，那么在被询问密码前，用户就将被系统拒绝。
</span></span></span><span class="line"><span class="cl"><span class="cm">userlist_enable NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，vsftpd将在userlist_file里读取用户列表。如果用户试图以文件里的用户名登录，那么在被询问用户密码前，他们就将被系统拒绝。这将防止明文密码被传送。参见userlist_deny。
</span></span></span><span class="line"><span class="cl"><span class="cm">virtual_use_local_privs NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，虚拟用户将拥有和本地用户一样的权限。默认情况下，虚拟用户就拥有和匿名用户一样的权限，而后者往往有更多的限制(特别是写权限)。
</span></span></span><span class="line"><span class="cl"><span class="cm">write_enable NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这决定是否容许一些FTP命令去更改文件系统。这些命令是STOR, DELE, RNFR, RNTO, MKD, RMD, APPE 和 SITE。
</span></span></span><span class="line"><span class="cl"><span class="cm">xferlog_enable NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，一个log文件将详细纪录上传和下载的信息。默认情况下，这个文件是/var/log/vsftpd.log，但你也可以通过更改vsftpd_log_file来指定其默认位置。
</span></span></span><span class="line"><span class="cl"><span class="cm">xferlog_std_format NO
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果启用，log文件将以标准的xferlog格式写入(wu-ftpd使用的格式)，以便于你用现有的统计分析工具进行分析。但默认的格式具有更好的可读性。默认情况下，log文件是在/var/log/xferlog。但是，你可以通过修改xferlog_file来指定新路径。
</span></span></span><span class="line"><span class="cl"><span class="cm">数字选项
</span></span></span><span class="line"><span class="cl"><span class="cm"># 以下是数字配置项。这些项必须设置为非负的整数。为了方便umask设置，容许输入八进制数，那样的话，数字必须以0开始。
</span></span></span><span class="line"><span class="cl"><span class="cm">accept_timeout 60
</span></span></span><span class="line"><span class="cl"><span class="cm"># 超时，以秒为单位，设定远程用户以被动方式建立连接时最大尝试建立连接的时间。
</span></span></span><span class="line"><span class="cl"><span class="cm">anon_max_rate 0　(无限制)
</span></span></span><span class="line"><span class="cl"><span class="cm"># 对于匿名用户，设定容许的最大传送速率，单位：字节/秒。
</span></span></span><span class="line"><span class="cl"><span class="cm">anon_umask 077
</span></span></span><span class="line"><span class="cl"><span class="cm"># 为匿名用户创建的文件设定权限。注意：如果你想输入8进制的值，那么其中的0不同于10进制的0。
</span></span></span><span class="line"><span class="cl"><span class="cm">connect_timeout 60
</span></span></span><span class="line"><span class="cl"><span class="cm"># 超时。单位：秒。是设定远程用户必须回应PORT类型数据连接的最大时间。
</span></span></span><span class="line"><span class="cl"><span class="cm">data_connection_timeout 300
</span></span></span><span class="line"><span class="cl"><span class="cm"># 超时，单位：秒。设定数据传输延迟的最大时间。时间一到，远程用户将被断开连接。
</span></span></span><span class="line"><span class="cl"><span class="cm">file_open_mode 0666
</span></span></span><span class="line"><span class="cl"><span class="cm"># 对于上传的文件设定权限。如果你想被上传的文件可被执行，umask要改成0777。
</span></span></span><span class="line"><span class="cl"><span class="cm">ftp_data_port 20
</span></span></span><span class="line"><span class="cl"><span class="cm"># 设定PORT模式下的连接端口(只要connect_from_port_20被激活)。
</span></span></span><span class="line"><span class="cl"><span class="cm">idle_session_timeout 300
</span></span></span><span class="line"><span class="cl"><span class="cm"># 超时。单位：秒。设置远程客户端在两次输入FTP命令间的最大时间。时间一到，远程客户将被断开连接。
</span></span></span><span class="line"><span class="cl"><span class="cm">listen_port 21
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果vsftpd处于独立运行模式，这个端口设置将监听的FTP连接请求。
</span></span></span><span class="line"><span class="cl"><span class="cm">local_max_rate　0(无限制)
</span></span></span><span class="line"><span class="cl"><span class="cm">#　为本地认证用户设定最大传输速度，单位：字节/秒。
</span></span></span><span class="line"><span class="cl"><span class="cm">local_umask 077
</span></span></span><span class="line"><span class="cl"><span class="cm"># 设置本地用户创建的文件的权限。注意：如果你想输入8进制的值，那么其中的0不同于10进制的0。
</span></span></span><span class="line"><span class="cl"><span class="cm">max_clients 0(无限制)
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果vsftpd运行在独立运行模式，这里设置了容许连接的最大客户端数。再后来的用户端将得到一个错误信息。
</span></span></span><span class="line"><span class="cl"><span class="cm">max_per_ip 0(无限制)
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果vsftpd运行在独立运行模式，这里设置了容许一个IP地址的最大接入客户端。如果超过了最大限制，将得到一个错误信息。
</span></span></span><span class="line"><span class="cl"><span class="cm">pasv_max_port 0(使用任何端口)
</span></span></span><span class="line"><span class="cl"><span class="cm"># 指定为被动模式数据连接分配的最大端口。可用来指定一个较小的范围以配合防火墙。
</span></span></span><span class="line"><span class="cl"><span class="cm">pasv_min_port 0(使用任何端口)
</span></span></span><span class="line"><span class="cl"><span class="cm"># 指定为被动模式数据连接分配的最小端口。可用来指定一个较小的范围以配合防火墙。
</span></span></span><span class="line"><span class="cl"><span class="cm">trans_chunk_size 0(让vsftpd自行选择)
</span></span></span><span class="line"><span class="cl"><span class="cm"># 你一般不需要改这个设置。但也可以尝试改为如8192去减小带宽限制的影响。
</span></span></span><span class="line"><span class="cl"><span class="cm">以下是STRING 配置项
</span></span></span><span class="line"><span class="cl"><span class="cm">anon_root 无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 设置一个目录，在匿名用户登录后，vsftpd会尝试进到这个目录下。如果失败则略过。
</span></span></span><span class="line"><span class="cl"><span class="cm">banned_email_file /etc/vsftpd.banned_emails
</span></span></span><span class="line"><span class="cl"><span class="cm"># deny_email_enable启动后，匿名用户如果使用这个文件里指定的E-MAIL密码登录将被拒绝。
</span></span></span><span class="line"><span class="cl"><span class="cm">banner_file 无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 设置一个文本，在用户登录后显示文本内容。如果你设置了ftpd_banner，ftpd_banner将无效。
</span></span></span><span class="line"><span class="cl"><span class="cm">chown_username ROOT
</span></span></span><span class="line"><span class="cl"><span class="cm"># 改变匿名用户上传的文件的所有者。需设定chown_uploads。
</span></span></span><span class="line"><span class="cl"><span class="cm">chroot_list_file /etc/vsftpd.chroot_list
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这个项提供了一个本地用户列表，表内的用户登录后将被放在虚根下，并锁定在home目录。这需要chroot_list_enable项被启用。如果chroot_local_user项被启用，这个列表就变成一个不将列表里的用户锁定在虚根下的用户列表了。
</span></span></span><span class="line"><span class="cl"><span class="cm">cmds_allowed 无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 以逗号分隔的方式指定可用的FTP命令(post　login. USER, PASS and QUIT 是始终可用的命令)。
</span></span></span><span class="line"><span class="cl"><span class="cm">其他命令将被屏蔽。这是一个强有力的locking down一个FTP服务器的手段。例如：cmds_allowed=PASV,RETR,QUIT(只允许检索文件)
</span></span></span><span class="line"><span class="cl"><span class="cm">cmds_allowed=ABOR,APPE,CWD,CDUP,FEAT,LIST,MKD,MDTM,PASS,PASV,PWD,QUIT,RETR,REST,
</span></span></span><span class="line"><span class="cl"><span class="cm">STOR,STRU,TYPE,USER(支持上传和下载的断点续传等命令)。
</span></span></span><span class="line"><span class="cl"><span class="cm">详细参考：http://www.nsftools.com/tips/RawFTP.htm
</span></span></span><span class="line"><span class="cl"><span class="cm">deny_file 无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这可以设置一个文件名或者目录名式样以阻止在任何情况下访问它们。并不是隐藏它们，而是拒绝任何试图对它们进行的操作(下载，改变目录层，和其他有影响的操作)。这个设置很简单，而且不会用于严格的访问控制-文件系统权限将优先生效。然而，这个设置对确定的虚拟用户设置很有用。
</span></span></span><span class="line"><span class="cl"><span class="cm">#  特别是如果一个文件能多个用户名访问的话(可能是通过软连接或者硬连接)，那就要拒绝所有的访问名。
</span></span></span><span class="line"><span class="cl"><span class="cm"># 建议你为使用文件系统权限设置一些重要的安全策略以获取更高的安全性。如deny_file={*.mp3,*.mov,.private}
</span></span></span><span class="line"><span class="cl"><span class="cm">dsa_cert_file 无(有一个RSA证书就够了)
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这个设置为SSL加密连接指定了DSA证书的位置。
</span></span></span><span class="line"><span class="cl"><span class="cm">email_password_file /etc/vsftpd.email_passwords
</span></span></span><span class="line"><span class="cl"><span class="cm"># 在设置了secure_email_list_enable后，这个设置可以用来提供一个备用文件。
</span></span></span><span class="line"><span class="cl"><span class="cm">ftp_username ftp
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这是用来控制匿名FTP的用户名。这个用户的home目录是匿名FTP区域的根。
</span></span></span><span class="line"><span class="cl"><span class="cm">ftpd_banner 无(默认的界面会被显示)
</span></span></span><span class="line"><span class="cl"><span class="cm"># 当一个连接首次接入时将现实一个欢迎界面。
</span></span></span><span class="line"><span class="cl"><span class="cm">guest_username ftp
</span></span></span><span class="line"><span class="cl"><span class="cm"># 参见相关设置guest_enable。这个设置设定了游客进入后，其将会被映射的名字。
</span></span></span><span class="line"><span class="cl"><span class="cm">hide_file 无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 设置了一个文件名或者目录名列表，这个列表内的资源会被隐藏，不管是否有隐藏属性。但如果用户知道了它的存在，将能够对它进行完全的访问。hide_file里的资源和符合hide_file指定的规则表达式的资源将被隐藏。vsftpd的规则表达式很简单，例如hide_file={*.mp3,.hidden,hide*,h?}
</span></span></span><span class="line"><span class="cl"><span class="cm">listen_address 无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果vsftpd运行在独立模式下，本地接口的默认监听地址将被这个设置代替。需要提供一个数字化的地址。
</span></span></span><span class="line"><span class="cl"><span class="cm">listen_address6 无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 如果vsftpd运行在独立模式下，要为IPV6指定一个监听地址(如果listen_ipv6被启用的话)。需要提供一个IPV6格式的地址。
</span></span></span><span class="line"><span class="cl"><span class="cm">local_root无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 设置一个本地(非匿名)用户登录后，vsftpd试图让他进入到的一个目录。如果失败，则略过。
</span></span></span><span class="line"><span class="cl"><span class="cm">message_file .message
</span></span></span><span class="line"><span class="cl"><span class="cm"># 当进入一个新目录的时候，会查找这个文件并显示文件里的内容给远程用户。dirmessage_enable需启用。
</span></span></span><span class="line"><span class="cl"><span class="cm">nopriv_user nobody
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这是vsftpd做为完全无特权的用户的名字。这是一个专门的用户，比nobody更甚。用户nobody往往用来在一些机器上做一些重要的事情。
</span></span></span><span class="line"><span class="cl"><span class="cm">pam_service_name ftp
</span></span></span><span class="line"><span class="cl"><span class="cm"># 设定vsftpd将要用到的PAM服务的名字。
</span></span></span><span class="line"><span class="cl"><span class="cm">pasv_address 无(地址将取自进来(incoming)的连接的套接字)
</span></span></span><span class="line"><span class="cl"><span class="cm"># 当使用PASV命令时，vsftpd会用这个地址进行反馈。需要提供一个数字化的IP地址。
</span></span></span><span class="line"><span class="cl"><span class="cm">rsa_cert_file /usr/share/ssl/certs/vsftpd.pem
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这个设置指定了SSL加密连接需要的RSA证书的位置。
</span></span></span><span class="line"><span class="cl"><span class="cm">secure_chroot_dir  /usr/share/empty
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这个设置指定了一个空目录，这个目录不容许ftp　user写入。在vsftpd不希望文件系统被访问时，目录为安全的虚根所使用。
</span></span></span><span class="line"><span class="cl"><span class="cm">ssl_ciphers DES-CBC3-SHA
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这个设置将选择vsftpd为加密的SSL连接所用的SSL密码。详细信息参见ciphers。
</span></span></span><span class="line"><span class="cl"><span class="cm">user_config_dir 无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这个强大的设置容许覆盖一些在手册页中指定的配置项(基于单个用户的)。用法很简单，最好结合范例。如果你把user_config_dir改为/etc/vsftpd_user_conf，那么以chris登录，vsftpd将调用配置文件/etc/vsftpd_user_conf/chris。
</span></span></span><span class="line"><span class="cl"><span class="cm">user_sub_token 无
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这个设置将依据一个模板为每个虚拟用户创建home目录。例如，如果真实用户的home目录通过guest_username为/home/virtual/$USER 指定，并且user_sub_token设置为 $USER ，那么虚拟用户fred登录后将锁定在/home/virtual/fred下。
</span></span></span><span class="line"><span class="cl"><span class="cm">userlist_file /etc/vsftpd.user_list
</span></span></span><span class="line"><span class="cl"><span class="cm"># 当userlist_enable被激活，系统将去这里调用文件。
</span></span></span><span class="line"><span class="cl"><span class="cm">vsftpd_log_file /var/log/vsftpd.log
</span></span></span><span class="line"><span class="cl"><span class="cm"># 只有xferlog_enable被设置，而xferlog_std_format没有被设置时，此项才生效。这是被生成的vsftpd格式的log文件的名字。
</span></span></span><span class="line"><span class="cl"><span class="cm"># dual_log_enable和这个设置不能同时启用。如果你启用了syslog_enable，那么这个文件不会生成，而只产生一个系统log.
</span></span></span><span class="line"><span class="cl"><span class="cm">xferlog_file /var/log/xferlog
</span></span></span><span class="line"><span class="cl"><span class="cm"># 这个设置是设定生成wu-ftpd格式的log的文件名。只有启用了xferlog_enable和xferlog_std_format后才能生效。但不能和dual_log_enable同时启用。
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="数字代码">数字代码</h3>
<table>
<thead>
<tr>
<th style="text-align:center">数字代码</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">110</td>
<td>重新启动标记应答。</td>
</tr>
<tr>
<td style="text-align:center">120</td>
<td>服务在多久时间内ready。</td>
</tr>
<tr>
<td style="text-align:center">125</td>
<td>数据链路埠开启，准备传送。</td>
</tr>
<tr>
<td style="text-align:center">150</td>
<td>文件状态正常，开启数据连接端口。</td>
</tr>
<tr>
<td style="text-align:center">200</td>
<td>命令执行成功。</td>
</tr>
<tr>
<td style="text-align:center">202</td>
<td>命令执行失败。</td>
</tr>
<tr>
<td style="text-align:center">211</td>
<td>系统状态或是系统求助响应。</td>
</tr>
<tr>
<td style="text-align:center">212</td>
<td>目录的状态。</td>
</tr>
<tr>
<td style="text-align:center">213</td>
<td>文件的状态。</td>
</tr>
<tr>
<td style="text-align:center">214</td>
<td>求助的讯息。</td>
</tr>
<tr>
<td style="text-align:center">215</td>
<td>名称系统类型。</td>
</tr>
<tr>
<td style="text-align:center">220</td>
<td>新的联机服务ready。</td>
</tr>
<tr>
<td style="text-align:center">221</td>
<td>服务的控制连接埠关闭，可以注销。</td>
</tr>
<tr>
<td style="text-align:center">225</td>
<td>数据连结开启，但无传输动作。</td>
</tr>
<tr>
<td style="text-align:center">226</td>
<td>关闭数据连接端口，请求的文件操作成功。</td>
</tr>
<tr>
<td style="text-align:center">227</td>
<td>进入passive</td>
</tr>
<tr>
<td style="text-align:center">230</td>
<td>使用者登入。</td>
</tr>
<tr>
<td style="text-align:center">250</td>
<td>请求的文件操作完成。</td>
</tr>
<tr>
<td style="text-align:center">257</td>
<td>显示目前的路径名称。</td>
</tr>
<tr>
<td style="text-align:center">331</td>
<td>用户名称正确，需要密码。</td>
</tr>
<tr>
<td style="text-align:center">332</td>
<td>登入时需要账号信息。</td>
</tr>
<tr>
<td style="text-align:center">350</td>
<td>请求的操作需要进一部的命令。</td>
</tr>
<tr>
<td style="text-align:center">421</td>
<td>无法提供服务，关闭控制连结。</td>
</tr>
<tr>
<td style="text-align:center">425</td>
<td>无法开启数据链路。</td>
</tr>
<tr>
<td style="text-align:center">426</td>
<td>关闭联机，终止传输。</td>
</tr>
<tr>
<td style="text-align:center">450</td>
<td>请求的操作未执行。</td>
</tr>
<tr>
<td style="text-align:center">451</td>
<td>命令终止：有本地的错误。</td>
</tr>
<tr>
<td style="text-align:center">452</td>
<td>未执行命令：磁盘空间不足。</td>
</tr>
<tr>
<td style="text-align:center">500</td>
<td>格式错误，无法识别命令。</td>
</tr>
<tr>
<td style="text-align:center">501</td>
<td>参数语法错误。</td>
</tr>
<tr>
<td style="text-align:center">502</td>
<td>命令执行失败。</td>
</tr>
<tr>
<td style="text-align:center">503</td>
<td>命令顺序错误。</td>
</tr>
<tr>
<td style="text-align:center">504</td>
<td>命令所接的参数不正确。</td>
</tr>
<tr>
<td style="text-align:center">530</td>
<td>未登入。</td>
</tr>
<tr>
<td style="text-align:center">532</td>
<td>储存文件需要账户登入。</td>
</tr>
<tr>
<td style="text-align:center">550</td>
<td>未执行请求的操作。</td>
</tr>
<tr>
<td style="text-align:center">551</td>
<td>请求的命令终止，类型未知。</td>
</tr>
<tr>
<td style="text-align:center">552</td>
<td>请求的文件终止，储存位溢出。</td>
</tr>
<tr>
<td style="text-align:center">553</td>
<td>未执行请求的的命令，名称不正确。</td>
</tr>
</tbody>
</table>
]]></content:encoded>
    </item>
    
    <item>
      <title>网络共享 - centos7安装samba</title>
      <link>https://www.oomkill.com/2016/09/samba-network-filesystem/</link>
      <pubDate>Thu, 22 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2016/09/samba-network-filesystem/</guid>
      <description></description>
      <content:encoded><![CDATA[<h2 id="安装samba服务">安装samba服务</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">yum install samba -y
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="配置samba服务">配置samba服务</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">cp /etc/samba/smb.cnf{,.`date +%F`} #&lt;== 修改前备份
</span></span><span class="line"><span class="cl">vim /etc/samba/smb.cnf
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="smb配置文件">smb配置文件</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1">#=================== Global Settings[全局选项] ==============================</span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>global<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">workgroup</span> <span class="o">=</span> WORKGROUP <span class="c1">#&lt;==设定Samba Server所要加入的工作组或域</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">server <span class="nv">string</span> <span class="o">=</span> Samba Server Version %v <span class="c1">#&lt;==设定注释，宏%v表示显示Samba的版本号</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">netbios <span class="nv">name</span> <span class="o">=</span> zhi <span class="c1">#&lt;==设置Samba Server的NetBIOS名称</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">map to <span class="nv">guest</span> <span class="o">=</span> bad user <span class="c1">#&lt;==开启匿名访问 </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ----------------- Logging Options [日志选项]-----------------------------</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#设置日志文件存储位置及名称，宏%m(主机名),表示对每台访问Samba Server的机器都单独记录一个日志文件</span>
</span></span><span class="line"><span class="cl">log <span class="nv">file</span> <span class="o">=</span> /var/log/samba/log.%m   
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">max log <span class="nv">size</span> <span class="o">=</span> <span class="m">50</span> <span class="c1">#&lt;==设置Samba Server日志文件的最大容量，单位为KB，0代表不限制</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ---------------- Standalone Server Options[独立运行进程] ---------------------</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 共享级别 share被弃用</span>
</span></span><span class="line"><span class="cl"><span class="nv">security</span> <span class="o">=</span> share
</span></span><span class="line"><span class="cl">passdb <span class="nv">backend</span> <span class="o">=</span> tdbsam   <span class="c1">#&lt;==建立安全账户管理数据库</span>
</span></span><span class="line"><span class="cl"><span class="c1"># =================== share settings[共享参数] =================== </span>
</span></span><span class="line"><span class="cl"><span class="o">[</span>web<span class="o">]</span>
</span></span><span class="line"><span class="cl"><span class="nv">comment</span> <span class="o">=</span> Public Stuff <span class="c1">#&lt;==定义说明信息</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">path</span> <span class="o">=</span> /data/web <span class="c1">#&lt;==共享目录路径</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">public</span> <span class="o">=</span> yes <span class="c1">#&lt;==匿名访问</span>
</span></span><span class="line"><span class="cl">             
</span></span><span class="line"><span class="cl"><span class="nv">writable</span> <span class="o">=</span> yes <span class="c1">#&lt;==可写</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">read</span> <span class="nv">only</span> <span class="o">=</span>yes    <span class="c1">#&lt;==read和writable后面的替换前面的属性</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">printable</span> <span class="o">=</span> no 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">write <span class="nv">list</span> <span class="o">=</span> +staff <span class="c1">#&lt;==允许写入该共享的用户 @是组</span>
</span></span><span class="line"><span class="cl"><span class="nv">browseable</span><span class="o">=</span>yes <span class="c1">#&lt;==用来指定该共享是否可以浏览</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><em><strong>share参数被去除</strong></em></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">WARNING: Ignoring invalid value &#39;share&#39; for parameter &#39;security&#39;
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="配置中问题">配置中问题</h2>
<h3 id="window无法打开smb共享">window无法打开smb共享</h3>
<p>windows中电脑必须在控制面板中开启次功能</p>
<p>关闭后在局域网内找不到任何电脑，并不能访问smb共享</p>
<h3 id="权限无法访问解决方法">权限无法访问解决方法</h3>
<p>解决方法：（该方法在/etc/samba/smb.conf中有提到）</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># Set SELinux labels only on files and directories you have created. Use the
</span></span><span class="line"><span class="cl"># chcon command to temporarily change a label:
</span></span><span class="line"><span class="cl"># chcon -t samba_share_t /path/to/directory
</span></span></code></pre></td></tr></table>
</div>
</div><p>所以执行以上命令：即可解决问题。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1">#chcon -t samba_share_t /path/to/directory</span>
</span></span></code></pre></td></tr></table>
</div>
</div>]]></content:encoded>
    </item>
    
    <item>
      <title>网络文件系统 - NFS</title>
      <link>https://www.oomkill.com/2016/04/nfs-network-filesystem/</link>
      <pubDate>Wed, 13 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.oomkill.com/2016/04/nfs-network-filesystem/</guid>
      <description></description>
      <content:encoded><![CDATA[<h3 id="nfs介绍">NFS介绍</h3>
<h4 id="什么是nfs">什么是NFS</h4>
<p>NFS是 <strong>Network File System</strong> 网络文件系统。它的主要功能是通过网络（一般是局域网）让不同的主机系统之间可以共享文件或目录。NFS客户端（一般为应用服务器，例如Web）可以通过挂载（mount）的方式将NFS服务器端共享的数据目录挂载到NFS客户端本地系统中（就是某一个挂载点下）。从客户端本地看，NFS服务器端共享的目录就好像是客户端自己的磁盘分区或者目录一样，而实际上却是远端的NFS服务器的目录。</p>
<p>一般情况下，Windows网络共享服务或samba服务用于办公局域网共享，而互联网中小型网站集群架构后端常用NFS进行数据共享，如果是大型网站，那么有可能还会用到更复杂的分布式文件系统，例如：Moosefs（mfs）、GlusterFS、FastDFS。</p>
<h4 id="nfs的历史介绍">NFS的历史介绍</h4>
<p>第一个网络文件系统被称为File Access Listener，由Digital Equipment Corporation（DEC）在1976年开发。</p>
<p>NFS是第一个构建于IP协议之上的现代网络文件系统。在20世纪80年代，它首先作为实验的文件系统，由Sun Microsystems在内部完成开发。NFS协议归属于Request for Comments（RFC）标准，并且随后演化为NFSv2。作为一个标准，由于NFS与其他客户端和服务器的互操作能力很好而发展快速。</p>
<p>之后，标准继续演化，成为NFSv3，在RFC1813中有定义。这一新的协议比以前的版本具有更好的可扩展性，支持大文件（超过2GB），异步写入，并且将TCP作为传输协议，为文件系统在更广泛的网络中使用铺平了道路。在2000年，RFC 3010（由RFC 3530修订）将NFS带入企业级应用。此时，Sun引入了具有较高安全性、带有状态协议的NFSv4（NFS之前的版本都是无状态的）。今天，NFS版本的4.1（由RFC 5661定义）增加了对跨越分布式服务器并行访问的支持（称为PNFS extension）。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/35f42ce1.png" alt="image"  /></p>
<p>NFS系统已经历了近30年的发展。它代表了一个非常稳定的（及可移植）网络文件系统，具备可扩展、高性能等特性，并达到了企业级应用质量标准。由于网络速度的增加和延迟的降低，NFS系统一直是通过网络提供文件系统服务的有竞争力的选择，特别是在中小型互联网企业中，应用十分广泛。</p>
<h4 id="nfs在企业中的应用场景">NFS在企业中的应用场景</h4>
<p>在企业集群架构的工作场景中，NFS网络文件系统一般被用来<font style="background:#ffc104;" size=2>存储共享视频、图片、附件等静态资源</font>文件，通常网站用户上传的文件都会放到NFS共享里，例如：BBS产品的图片、附件、头像（注意网站BBS程序不要放在NFS共享里），然后前端所有的节点访问这些静态资源时都可读取NFS存储上的资源。NFS是当前互联网系统架构中最常用的数据存储服务之一，前面说过，中小型网站公司应用频率更高，大公司或门户除了使用NFS外，还可能会使用更为复杂的分布式文件系统，比如Moosefs（mfs）、GlusterFS、FastDFS等。</p>
<h4 id="企业生产集群为什么需要共享存储角色">企业生产集群为什么需要共享存储角色</h4>
<p>例如：A用户传图片到Web1服务器，然后让B用户访问这张图片，结果B用户访问的请求分发到了Web2，因为Web2上没有这张图片，这就导致它无法看到A用户上传的图片，如果此时有一个共享存储，A用户上传图片的请求无论是分发到Web1还是Web2上，最终都会存储到共享存储上，而在B用户访问图片时，无论请求分发到Web1还是Web2上，最终也都会去共享存储上找，这样就可以访问到需要的资源了。这个共享存储的位置可以通过开源软件和商业硬件实现，互联网中小型集群架构会用普通PC服务器配置NFS网络文件系统实现。</p>
<p><strong>当集群中没有NFS共享存储时，用户访问图片的情况如图所示。</strong></p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/b027c360.png" alt="image"  /></p>
<p><strong>如果集群中有NFS共享存储，用户访问图片的情况如图所示。</strong></p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/63a3062b.png" alt="image"  /></p>
<p>中小型互联网企业一般不会买硬件存储，因为太贵，大公司如果业务发展很快的话，可能会临时买硬件存储顶一下网站的压力，当网站并发继续加大时，硬件存储的扩展相对就会很费劲，且价格成几何级数增加。例如：淘宝网就曾替换掉了很多硬件设备，比如，用LVS+Haproxy替换了netscaler负载均衡设备，用FastDFS、TFS配合PC服务器替换了netapp、emc等商业存储设备，去IOE正在成为互联网公司的主流。</p>
<h3 id="nfs系统原理介绍">NFS系统原理介绍</h3>
<h4 id="nfs系统挂载结构图解与介绍">NFS系统挂载结构图解与介绍</h4>
<p>在NFS服务器端设置好一个共享目录/video后，其他有权限访问NFS服务器端的客户端都可以将这个共享目录/video挂载到客户端本地的某个挂载点（其实就是一个目录，这个挂载点目录可以自己随意指定），不同客户端的挂载点可以不相同。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/cd1b46ce.png" alt="image"  /></p>
<p>客户端正确挂载完毕后，就可以通过NFS客户端的挂载点所在的/v/video或/video目录查看到NFS服务器端/video共享出来的目录下的所有数据。在客户端上查看时，NFS服务器端的/video目录就相当于客户端本地的磁盘分区或目录，几乎感觉不到使用上的区别，根据NFS服务器端授予的NFS共享权限以及共享目录的本地系统权限，只要在指定的NFS客户端操作挂载/v/video或/video的目录，就可以将数据轻松地存取到NFS服务器端上的/video目录中了。</p>
<h4 id="什么是rpc">什么是RPC</h4>
<p>因为NFS支持的功能相当多，而不同的功能都会使用不同的程序来启动，每启动一个功能就会启用一些端口来传输数据，因此，NFS的功能所对应的端口无法固定，它会随机取用一些未被使用的端口来作为传输之用，其中CentOS 5.x的随机端口都小于1024，而CentOS 6.x的随机端口都是较大的。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/d588df18.png" alt="image"  /></p>
<p>因为端口不固定，这样一来就会造成NFS客户端与NFS服务器端的通信障碍，因为NFS客户端必须要知道NFS服务器端的数据传输端口才能进行通信，才能交互数据。</p>
<p>要解决上面的困扰，就需要通过远程过程调用RPC（Remote Proce-dure Call）服务来帮忙了，NFS的RPC服务最主要的功能就是记录每个NFS功能所对应的端口号，并且在NFS客户端请求时将该端口和功能对应的信息传递给请求数据的NFS客户端，从而确保客户端可以连接到正确的NFS端口上去，达到实现数据传输交互数据目的。这个RPC服务类似NFS服务器端和NFS客户端之间的一个中介，流程如图10-7所示。</p>
<p>拿房屋中介打个比喻吧：假设我们要找房子，这里的我们就相当于NFS客户端，中介介绍房子，就相当于RPC服务，房子所有者房东就相当于NFS服务，租房的人找房子，就要找中介，中介要预先存有房子主人的信息，才能将房源信息告诉租房的人。</p>
<p><strong>那么RPC服务如何知道每个NFS的端口呢？</strong></p>
<p>当NFS服务器端启动服务时会随机取用若干端口，并主动向RPC服务注册取用的相关端口及功能信息，如此一来，RPC服务就知道NFS每个端口对应的NFS功能了，然后RPC服务使用固定的111端口来监听NFS客户端提交的请求，并将正确的NFS端口信息回复给请求的NFS客户端，这样一来，NFS客户端就可以与NFS服务器端进行数据传输了。</p>
<p>在启动NFS Server之前，首先要启动RPC服务（CentOS 5.8下为portmap服务，CentOS 6.6下为rpcbind服务，下同），否则NFS Server就无法向RPC服务注册了。另外，如果RPC服务重新启动，原来已经注册好的NFS端口数据就会丢失，因此，此时RPC服务管理的NFS程序也需要重新启动以重新向RPC注册。要特别注意的是，一般修改NFS配置文件后，是不需要重启NFS的，直接在命令行执行/etc/init.d/nfs reload或exportfs-rv即可使修改的/etc/exports生效。</p>
<h4 id="nfs的工作流程原理">NFS的工作流程原理</h4>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/2b45f2f3.png" alt="image"  /></p>
<p>当访问程序通过NFS客户端向NFS服务器端存取文件时，其请求数据流程大致如下：</p>
<ul>
<li>1）首先用户访问网站程序，由程序在NFS客户端上发出存取NFS文件的请求，这时NFS客户端（即执行程序的服务器）的RPC服务（rpcbind服务）就会通过网络向NFS服务器端的RPC服务（rpcbind服务）的111端口发出NFS文件存取功能的询问请求。</li>
<li>2）NFS服务器端的RPC服务（rpcbind服务）找到对应的已注册的NFS端口后，通知NFS客户端的RPC服务（rpcbind服务）。</li>
<li>3）此时NFS客户端获取到正确的端口，并与NFS daemon联机存取数据。</li>
<li>4）NFS客户端把数据存取成功后，返回给前端访问程序，告知用户存取结果，作为网站用户，就完成了一次存取操作。</li>
</ul>
<p>因为NFS的各项功能都需要向RPC服务（rpcbind服务）注册，所以只有RPC服务才能获取到NFS服务的各项功能对应的端口号（port number）、PID、NFS在主机所监听的IP等信息，而NFS客户端也只能通过向RPC服务询问才能找到正确的端口。也就是说，NFS需要有RPC服务的协助才能成功对外提供服务。从上面的描述，我们不难推断，无论是NFS客户端还是NFS服务器端，当要使用NFS时，都需要首先启动RPC服务，NFS服务必须在RPC服务启动之后启动，客户端无需启动NFS服务，但需要启动RPC服务。</p>
<hr>
<blockquote>
<p><strong>注意</strong>： <font style="background:#ffc104;" size=2>NFS的RPC服务，在CentOS 5.X下名称为portmap，在CentOS 6.X下名称为rpcbind。</font></p>
</blockquote>
<hr>
<h3 id="安装nfs服务">安装NFS服务</h3>
<h4 id="搭建nfs环境准备">搭建NFS环境准备</h4>
<h5 id="克隆虚拟机">克隆虚拟机</h5>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/cd7140c2.png" alt="image"  /></p>
<p></p>
<h5 id="克隆的虚拟机存在的网络问题">克隆的虚拟机存在的网络问题</h5>
<p><strong>原因分析</strong>：</p>
<p>使用VM的克隆功能，会为新产生的虚拟机配置一个与原始虚拟机网卡MAC地址不同的网卡。对于CentOS这样的Linux系统，会把运行时的网卡MAC地址记入/etc/udev/rules.d/70-persistent-net.rules文件中。这样克隆好的新系统里也保存了这个记录。</p>
<p>当新系统启动时，由于vm已经为其配置了不同的MAC地址，因此系统会在启动扫描硬件时把这个新的MAC地址的网卡当做是eth1，并且增加记入上述文件中。而此时配置文件里的/etc/sysconfig/network-scripts/ifcfg-eth0里记录的还是原来的MAC地址，而这个MAC地址在新系统里是不存在的，所以无法启动。</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/66aff6b4.png" alt="image"  /></p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/64cd4219.png" alt="image"  /></p>
<p><strong>解决方法1</strong></p>
<ul>
<li>删除里面的uuid，因为这个是唯一的值</li>
<li>更改HWaddr为eth1的值</li>
</ul>
<p>重启系统后</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/1fd6599d.png" alt="image"  /></p>
<p>解决方法2
1.编辑eth0的配置文件：<code>vi /etc/sysconfig/network-scripts/ifcfg-eth0</code> ,删除HWADDR地址那一行及UUID的行</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ cat /etc/sysconfig/network-scripts/ifcfg-eth0
</span></span><span class="line"><span class="cl">DEVICE=eth0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">IPV6INIT=no
</span></span><span class="line"><span class="cl">USERCTL=no
</span></span><span class="line"><span class="cl">HWADDR=00:0c:29:08:28:9f
</span></span><span class="line"><span class="cl">UUID=cee39dbb-6a10-4425-9daf-768b6e79a9c9
</span></span></code></pre></td></tr></table>
</div>
</div><p>2.清空 <code>/etc/udev/rules.d/70-persistent-net.rules</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">&gt;/etc/udev/rules.d/70-persistent-net.rules
</span></span></code></pre></td></tr></table>
</div>
</div><p>3.重启系统</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/624006dc.png" alt="image"  /></p>
<hr>
<blockquote>
<p><font color="#0215cd" size=2><strong>注： 两种方法更改后重启网卡是不行的，必须重启系统</strong></font></p>
</blockquote>
<hr>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/c6480f95.png" alt="image"  /></p>
<h4 id="安装nfs">安装NFS</h4>
<p>要部署NFS服务，需要安装下面的软件包：</p>
<ul>
<li><code>nfs-utils</code>：NFS服务的主程序，包括<code>rpc.nfsd</code>、<code>rpc.mountd</code>这两个daemons和相关文档说明，以及执行命令文件等。</li>
<li><code>rpcbind</code>：CentOS 6.X下面RPC的主程序。NFS可以视为一个RPC程序，在启动任何一个RPC程序之前，需要做好端口和功能的对应映射工作，这个映射工作就是由rpcbind服务来完成的。因此，在提供NFS服务之前必须先启动rpcbind服务才行。</li>
</ul>
<h5 id="nfs安装的三种方式">NFS安装的三种方式</h5>
<p>检查软件是否安装</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">rpm -qa nfs-utils rpcbind
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>yum安装</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">yum install nfs-utils rpcbind -y
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>通过系统光盘里的rpm包安装，命令</strong> <code>nfs-utils-1.2.3-64.el6.x86_64</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">yum grouplistgrep -i nfs
</span></span><span class="line"><span class="cl">yum groupinstall &#34;NFS file server&#34; -y
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="启动nfs相关服务">启动NFS相关服务</h4>
<p><strong>启动rpcbind服务</strong></p>
<p>因为NFS及其辅助程序都是基于RPC（Remote Procedure Call）协议的（使用的端口为111），所以首先要确保系统中运行了rpcbind服务。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">/etc/init.d/rpcbind start 
</span></span><span class="line"><span class="cl">/etc/init.d/rpcbind status	#&lt;==检查rpcbind服务状态
</span></span><span class="line"><span class="cl">rpcinfo -p localhost       	#&lt;==rpcbind服务未启动检查rpcinfo信息的报错
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li><a href="https://yd.baidu.com/view/38b81d255fbfc77da369b13c?cn=24-105,24-592&amp;pn=15">https://yd.baidu.com/view/38b81d255fbfc77da369b13c?cn=24-105,24-592&amp;pn=15</a></li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ps -ef|egrep &#34;rpc|nfs&#34;
</span></span><span class="line"><span class="cl">rpcuser     901      1  0 06:43 ?        00:00:00 rpc.statd &lt;==检查文件一致性
</span></span><span class="line"><span class="cl">rpc        1071      1  0 07:24 ?        00:00:00 rpcbind
</span></span><span class="line"><span class="cl">root       1101      2  0 07:24 ?        00:00:00 [rpciod/0]
</span></span><span class="line"><span class="cl">root       1110      1  0 07:24 ?        00:00:00 rpc.rquotad
</span></span><span class="line"><span class="cl">root       1115      1  0 07:24 ?        00:00:00 rpc.mountd
</span></span><span class="line"><span class="cl">root       1122      2  0 07:24 ?        00:00:00 [nfsd4]
</span></span><span class="line"><span class="cl">root       1123      2  0 07:24 ?        00:00:00 [nfsd4_callbacks]
</span></span><span class="line"><span class="cl">root       1124      2  0 07:24 ?        00:00:00 [nfsd]
</span></span><span class="line"><span class="cl">root       1125      2  0 07:24 ?        00:00:00 [nfsd]
</span></span><span class="line"><span class="cl">root       1126      2  0 07:24 ?        00:00:00 [nfsd]
</span></span><span class="line"><span class="cl">root       1127      2  0 07:24 ?        00:00:00 [nfsd]
</span></span><span class="line"><span class="cl">root       1128      2  0 07:24 ?        00:00:00 [nfsd]
</span></span><span class="line"><span class="cl">root       1129      2  0 07:24 ?        00:00:00 [nfsd]
</span></span><span class="line"><span class="cl">root       1130      2  0 07:24 ?        00:00:00 [nfsd]
</span></span><span class="line"><span class="cl">root       1131      2  0 07:24 ?        00:00:00 [nfsd]
</span></span><span class="line"><span class="cl">root       1158      1  0 07:24 ?        00:00:00 rpc.idmapd
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>问：如何让rpcbind比nfs先启动？</strong></p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/01fbb104.png" alt="image"  /></p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/7fda5764.png" alt="image"  /></p>
<p>一般都是将启动命令放入rc.local中</p>
<h3 id="配置nfs">配置NFS</h3>
<p>nfs配置文件</p>
<hr>
<blockquote>
<p><strong>提示</strong>：NFS默认配置文件/etc/exports其实是存在的，但是没有内容，需要用户自行配置</p>
</blockquote>
<hr>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ cat /etc/exports 
</span></span><span class="line"><span class="cl"># NFS共享的目录   共享给谁(权限)  NFS客户端地址1（参数1,参数2） NFS客户端地址2（参数1,参数2）
</span></span><span class="line"><span class="cl">/data   	      192.168.59.*(rw,sync)
</span></span><span class="line"><span class="cl">/data 			  10.1.1.1(rw,sync) 
</span></span></code></pre></td></tr></table>
</div>
</div><center>NFS地址配置的详细说明</center>
<table>
<thead>
<tr>
<th>客户端地址</th>
<th>具体地址</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>授权单一客户端访问NFS</td>
<td>10.0.0.30</td>
<td>一般情况下生产环境中此配置不多</td>
</tr>
<tr>
<td>授权整个网段可访问NFS</td>
<td>10.0.0.0/24</td>
<td>其中24位255.255.255.0，指定网段为生产环境中最常见的配置。配置简单，维护方便</td>
</tr>
<tr>
<td>授权整个网段可访问NFS</td>
<td>10.0.0.0/24</td>
<td>其中24位255.255.255.0，指定网段为生产环境中最常见的配置。配置简单，维护方便</td>
</tr>
<tr>
<td>授权整个网段可访问NFS</td>
<td>10.0.0.*</td>
<td>指定网段的另外写法（不推荐使用）</td>
</tr>
<tr>
<td>授权某个域名客户端访问</td>
<td>nfs.bodboy.com</td>
<td>生产环境中一般情况下不常用。（域名可在hosts文件中解析）</td>
</tr>
<tr>
<td>授权整个域名客户端访问</td>
<td>*.oldboy.com</td>
<td>生产环境中一般情况下不常用</td>
</tr>
</tbody>
</table>
<center> /etc/exports 文件格式配置实例说明</center>
<table>
<thead>
<tr>
<th>常用格式说明</th>
<th>要共享的目录  客户端IP地址或IP段（参数1，参数2&hellip;）</th>
</tr>
</thead>
<tbody>
<tr>
<td>配置例一</td>
<td>/data 10.0.0.0/24(rw,sync) #&lt;&ndash; 允许客户端读写，并且数据同步写到服务器端磁盘里，注意，<code>24</code>和 <code>(</code> 之间不能有空格</td>
</tr>
<tr>
<td>配置例二</td>
<td>/data  10.0.0.0/24(rw,sync,all_squash,anonuid=2000,anongid=2000) #&lt;&ndash;允许客户端读写，并且数据同步写到服务器端的磁盘里，并指定客户端的用户UID和GID。早期生产环境的一种配置，适合多客户端共享一个NFS服务单目录，如果所有服务器的nfsnobody账户UID都是65534，则本例没什么必要了。早期CentOS5.5的系统默认情况下nfsbobody的UID不一定是65534，此时如果这些服务器共享一个NFS目录，就会出现访问权限问题</td>
</tr>
<tr>
<td>配置例三</td>
<td><code>/home/oldbody  10.0.0.0/24(ro)</code> #&lt;&ndash; 只读并共享<br><br>用途：例如在生产环境中，开发人员有查看盛传服务器日志的需求，但有不希望给开发生产服务器的权限，那么就可以给开发提供某个测试服务器NFS客户端上查看某个生产服务器日志目录（NFS共享）的权限，当然这不是唯一的放法，例如可以把程序记录的日志发送到测试服务器供开发查看或者通过收集日志等其他方式展现</td>
</tr>
</tbody>
</table>
<center>NFS常用配置参数权限</center>
<table>
<thead>
<tr>
<th>参数选项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ro</code></td>
<td>只读访问</td>
</tr>
<tr>
<td><code>rw</code></td>
<td>读写访问</td>
</tr>
<tr>
<td><code>sync</code></td>
<td>请求或写入数据时，数据同步写入到NFS服务器的硬盘后才返回。<br><strong>优点</strong>，数据安全不会丢，缺点，性能比不启用该参数要差</td>
</tr>
<tr>
<td>async</td>
<td>写入数据会先写到内存缓存区，直到硬盘有空挡才会再写入磁盘，这样可以提升写入效率！风险若为服务器宕机或不正常关机，会损失缓冲区中未写入磁盘的数据（解决方法：服务器主板点出或加UPS不间断电源）</td>
</tr>
<tr>
<td><code>all_squash</code></td>
<td>不管访问NFS服务器共享目录的用户身份如歌，它的权限都将被压缩成匿名用户，同时它的UID和GID都会变成nfsnobody账号身份。在早期多个NFS客户端同时读写NFS服务器数据时，这个参数很有用<br><strong>在生产中配置NFS的重要技巧</strong>：<br>1)确保所有客户端服务器对NFS共享目录具备想用的用户访问权限<br><code>a.all_squash</code>把所有客户端都压缩成固定的匿名用户（UID相同）<br> b.就是anonuid，anongid指定的UID和GID用户 <br> 2)所有的客户端和服务器端都需要有一个相同的UID和GID的用户，即nfsnobody（UID必须相同）</td>
</tr>
<tr>
<td><code>no_all_squash</code></td>
<td>访问NFS服务器共享目录的用户如果是root的话，它对该共享目录具有root权限。这个配置原是为无盘客户端准备的。用户应该避免使用</td>
</tr>
<tr>
<td><code>root_squash</code></td>
<td>如果访问NFS服务器共享目录是root，则它的权限将被压缩成匿名用户，同时它的UID和GID通常会变成nfsnobody账号身份</td>
</tr>
<tr>
<td><code>anonuid=xxx</code></td>
<td>参数以anon*开头即指anonymous匿名用户，这个用户的UID设置值通常为nfsnobody的UID值，当然也可以自行设置这个UID值。但是，UID必须存在于/etc/passwd中。在多NFS客户端时，如多台webserver共享一个NFS目录，通过这个参数可以使得不同的NFS客户端写入的数据对所有NFS客户端保持永阳的用户权限，即为配置的匿名UID对应用户权限，这个参数很有用，一般默认即可</td>
</tr>
<tr>
<td><code>anonuid</code></td>
<td>同uid</td>
</tr>
</tbody>
</table>
<h4 id="配置服务器端">配置服务器端</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ /etc/init.d/nfs reload = exportfs -rv（平滑生效）
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>reload = exportfs -rv</code>（平滑生效）位置</p>
<p>
  <img loading="lazy" src="https://cdn.jsdelivr.net/gh/CylonChau/imgbed/img/c2860959.png" alt="image"  /></p>
<p><strong>检查挂载信息</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ showmount -e 127.0.0.1
</span></span><span class="line"><span class="cl">Export list for 127.0.0.1:
</span></span><span class="line"><span class="cl">/data 192.168.59.*
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>执行挂载命令</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ mount -t nfs 192.168.59.133:/data /mnt
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>查看挂载结果</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ df -h
</span></span><span class="line"><span class="cl">Filesystem            Size  Used Avail Use% Mounted on
</span></span><span class="line"><span class="cl">/dev/sda3             9.1G  1.5G  7.2G  17% /
</span></span><span class="line"><span class="cl">tmpfs                 242M     0  242M   0% /dev/shm
</span></span><span class="line"><span class="cl">/dev/sda1             190M   27M  153M  16% /boot
</span></span><span class="line"><span class="cl">192.168.59.133:/data  9.1G  1.5G  7.2G  17% /mnt
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="配置客户端">配置客户端</h4>
<p><strong>开启rpcbind</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ /etc/init.d/rpcbind start
</span></span><span class="line"><span class="cl">正在启动 rpcbind：[确定]
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>将rpcbind服务设置为开机自启动</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">$  vi /etc/rc.local 
</span></span><span class="line"><span class="cl"><span class="c1">#!/bin/sh</span>
</span></span><span class="line"><span class="cl"><span class="c1">#</span>
</span></span><span class="line"><span class="cl"><span class="c1"># This script will be executed *after* all the other init scripts.</span>
</span></span><span class="line"><span class="cl"><span class="c1"># You can put your own initialization stuff in here if you don&#39;t</span>
</span></span><span class="line"><span class="cl"><span class="c1"># want to do the full Sys V style init stuff.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">touch /var/lock/subsys/local
</span></span><span class="line"><span class="cl">/etc/init.d/rpcbind start
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>查看挂载信息</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ showmount -e 192.168.59.133
</span></span><span class="line"><span class="cl">Export list for 192.168.59.133:
</span></span><span class="line"><span class="cl">/data 192.168.59.*
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>设置挂载</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ mount -t nfs 192.168.59.133:/data /mnt
</span></span><span class="line"><span class="cl">$ df -h
</span></span><span class="line"><span class="cl">Filesystem            Size  Used Avail Use% Mounted on
</span></span><span class="line"><span class="cl">/dev/sda3             9.1G  1.4G  7.2G  17% /
</span></span><span class="line"><span class="cl">tmpfs                 242M     0  242M   0% /dev/shm
</span></span><span class="line"><span class="cl">/dev/sda1             190M   27M  153M  16% /boot
</span></span><span class="line"><span class="cl">192.168.59.133:/data  9.1G  1.5G  7.2G  17% /mnt
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>切换到挂载的目录</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ cd /mnt
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>查看nfs服务器文件列表</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ ls
</span></span><span class="line"><span class="cl">a.log
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>解决不能写的问题</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$ vi /var/lib/nfs/etab 
</span></span><span class="line"><span class="cl">/data   192.168.59.*(rw,sync,wdelay,hide,nocrossmnt,secure,root_squash,no_all_squash,no_subtree_check,secure_locks,acl,anonuid=65534,anongid=65534,sec=sys,rw,root_squash,no_all_squash)
</span></span><span class="line"><span class="cl">$ grep 65534 /etc/passwd
</span></span><span class="line"><span class="cl">nfsnobody❌65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin
</span></span><span class="line"><span class="cl"># 将目录所属用户设置为nfsnobody
</span></span><span class="line"><span class="cl">$ chown -R nfsnobody /data
</span></span><span class="line"><span class="cl"># 查看设置结果
</span></span><span class="line"><span class="cl">$ ls -l /
</span></span><span class="line"><span class="cl">drwxr-xr-x.   2 nfsnobody root  4096 4月   8 21:54 data
</span></span><span class="line"><span class="cl"># 在客户端创建一个文件
</span></span><span class="line"><span class="cl">$ touch b.txt
</span></span><span class="line"><span class="cl"># 在服务器端查看
</span></span><span class="line"><span class="cl">$ ls
</span></span><span class="line"><span class="cl">b.txt
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>重启后挂载失效</strong></p>
<p>在 <code>/etc/rc.loacl</code> 中挂载，不要在fstab中，因为linux启动过程，fstab先启动。网络是后启动的，网络磁盘是挂不上的</p>
<p><strong>挂载服务器断</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">mount -t nfs 192.168.59.133:/data /mnt
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ cat /proc/mounts
</span></span><span class="line"><span class="cl">rootfs / rootfs rw 0 0
</span></span><span class="line"><span class="cl">proc /proc proc rw,relatime 0 0
</span></span><span class="line"><span class="cl">sysfs /sys sysfs rw,seclabel,relatime 0 0
</span></span><span class="line"><span class="cl">devtmpfs /dev devtmpfs rw,seclabel,relatime,size=235908k,nr_inodes=58977,mode=755 0 0
</span></span><span class="line"><span class="cl">devpts /dev/pts devpts rw,seclabel,relatime,gid=5,mode=620,ptmxmode=000 0 0
</span></span><span class="line"><span class="cl">tmpfs /dev/shm tmpfs rw,seclabel,relatime 0 0
</span></span><span class="line"><span class="cl">/dev/sda3 / ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0
</span></span><span class="line"><span class="cl">none /selinux selinuxfs rw,relatime 0 0
</span></span><span class="line"><span class="cl">devtmpfs /dev devtmpfs rw,seclabel,relatime,size=235908k,nr_inodes=58977,mode=755 0 0
</span></span><span class="line"><span class="cl">/proc/bus/usb /proc/bus/usb usbfs rw,relatime 0 0
</span></span><span class="line"><span class="cl">/dev/sda1 /boot ext4 rw,seclabel,relatime,barrier=1,data=ordered 0 0
</span></span><span class="line"><span class="cl">none /proc/sys/fs/binfmt_misc binfmt_misc rw,relatime 0 0
</span></span><span class="line"><span class="cl">sunrpc /var/lib/nfs/rpc_pipefs rpc_pipefs rw,relatime 0 0
</span></span><span class="line"><span class="cl">nfsd /proc/fs/nfsd nfsd rw,relatime 0 0
</span></span><span class="line"><span class="cl">192.168.59.133:/data/ /mnt nfs4 rw,relatime,vers=4,rsize=4096,wsize=1024,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.59.133,minorversion=0,local_lock=none,addr=192.168.59.133 0 0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ cd /mnt
</span></span><span class="line"><span class="cl">$ umount /mnt
</span></span><span class="line"><span class="cl">umount.nfs: /mnt: device is busy
</span></span><span class="line"><span class="cl">umount.nfs: /mnt: device is busy
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">$ umount -lf /mnt
</span></span><span class="line"><span class="cl">$ df -h
</span></span><span class="line"><span class="cl">Filesystem      Size  Used Avail Use% Mounted on
</span></span><span class="line"><span class="cl">/dev/sda3       9.1G  1.5G  7.2G  17% /
</span></span><span class="line"><span class="cl">tmpfs           242M     0  242M   0% /dev/shm
</span></span><span class="line"><span class="cl">/dev/sda1       190M   27M  153M  16% /boot
</span></span></code></pre></td></tr></table>
</div>
</div><center>NFS挂载参数说明</center>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数功能</th>
<th>默认参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>fg/bg</td>
<td>当在客户端执行挂载时，可选择时前台（fg）还是后台（bg）执行。若在前台执行，则mount会持续尝试挂载，直到成功或挂载时间超时为止，若在后台执行，则mount会在后台持续多次进行mount，而不会影响到前台的其他程序操作。如果网络联机不稳定，或是服务器常常需要开关机，建议使用bg比较妥当</td>
<td>fg</td>
</tr>
<tr>
<td>soft/hard</td>
<td>当NFS客户端以soft挂载服务器时，若网络或服务器出现问题，造成客户端与服务器之间无法传输资料，客户端就会一直尝试，直到timeout（超时时间）后显示错误才停止。若使用soft mount的话，可能会在timeout出现时造成资料丢失，故一般不建议使用。若用hard模式挂载硬盘时，刚与soft相反，此时客户端会一直尝试连线倒服务器，若服务器有回应就继续刚才的操作，若没有回应NFS客户端会一直尝试，此时无法umount或kill，所以常常会配合intr使用</td>
<td>hard</td>
</tr>
<tr>
<td>intr</td>
<td>当使用hard挂载的资源timeout后，若有指定intr参数，可以在timeout后把它中断掉，这避免出问题时系统整个被NFS锁死，建议使用intr</td>
<td>无</td>
</tr>
<tr>
<td>rsize/wsize</td>
<td>读出（rsize）与写入（wsize）的区块大小（block size），这个设置值可以影响客服端与服务器传输数据的传冲存储量，一般来说，如果在局域网内，别且客户端与服务器端都具有足够的内存，这个值可以设置大一点，比如65535（bytes），提升缓冲区块将提升NFS文件系统的传输能力。但设置的值也不要太大，最好以网络能够传输的最大值为限</td>
<td>CentOS5X：默认值<br><code>rsize=1024</code><br><code>wsize=1024</code><br>CentOS6：默认值<br><code>rsize=131072</code><br><code>wsize=131072</code></td>
</tr>
<tr>
<td>proto</td>
<td>使用UDP协定来传输资料，在LAN中会有比较好的性能。若要跨越Internet的话，使用proto=tcp多传输的数据会有比较好的纠错能力</td>
<td>proto=tcp</td>
</tr>
</tbody>
</table>
<p>通过man nfs查看上述参数信息。如果追求极致，可以用如下参数挂载</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">mount -t nfs -o fg,hard,intr,rsize=1111,wsize=1111 10.0.0.7:/data /mnt
</span></span></code></pre></td></tr></table>
</div>
</div><p>如果考虑简单、易用为原则，直接选择默认值</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">mount -t nfs 10.0.0.7:/data /mnt
</span></span></code></pre></td></tr></table>
</div>
</div><center>mount -o 参数对应的选项</center>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数意义</th>
<th>系统默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td><font color="#f8070d" size=3><code>suid/nosuid</code></font></td>
<td>当挂载的文件系统上有任何SUID的程序时，只要使用nosuid就能够取消设置SUID功能</td>
<td>suid</td>
</tr>
<tr>
<td><font color="#f8070d" size=3><code>rw/ro</code></font></td>
<td>可以指定文件系统是只读或可写</td>
<td>rw</td>
</tr>
<tr>
<td>dev/nodev</td>
<td>是否可以保留装置文件的特殊功能？一般来说只有/dev才会有特殊的装置，因此可选择nodev</td>
<td>dev</td>
</tr>
<tr>
<td><font color="#f8070d" size=3><code>exec/noexec</code></font></td>
<td>是否具有执行文件权限？如果想要挂载的文件时普通的资源区（例如：图片、附件），那个可以选择noexec</td>
<td>exec</td>
</tr>
<tr>
<td>user/nouser</td>
<td>是否允许用户拥有文件的挂载与卸载功能？如果要保护文件系统，最好不要为用户提供挂载与卸载功能</td>
<td>nouer</td>
</tr>
<tr>
<td>auto/noauto</td>
<td>这个auto是指“mount -a”时会不会被挂载的项目，如果不需要这个分区随时被挂载，可以设置为noauto	auto</td>
<td></td>
</tr>
<tr>
<td><font color="#f8070d" size=3><code>default</code></font></td>
<td>这个是fstab里面的默认值，包括rw、suid、dev、exec、auto、nouser、async默认情况下大部分都是默认值</td>
<td></td>
</tr>
<tr>
<td><font color="#f8070d" size=3><code>noatime/atime</code></font></td>
<td>atime:在每一次数据访问时，会永不更新访问文件的inode时间戳，在高并发情况下，建议通过加上noatime来取消这个默认项，已到提升I/O性能，优化I/O的目的<br>noatime：访问文件不更新文件的inode时间戳，高并发环境，推荐显示应用该选项，可提高磁盘I/O性能</td>
<td>atime</td>
</tr>
<tr>
<td><font color="#f8070d" size=3><code>sync/async</code></font></td>
<td>该参数与async相反。有I/O操作时，会同步处理I/O即吧数据同步写入硬盘。次参数会牺牲一点I/O性能，但是，换来的时断电后数据的安全性。 <br> sync：涉及文件I/O的操作都是异步处理，即不会写到磁盘，此参数会提高性能，但会降低数据安全。一般情况，生产环境不推荐使用。除非对性能要求很高，对数据可靠性不要求场合</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>问</strong>：在企业生产环境中，NFS客户端挂载有没有必要加某些参数，如：noexec、nosuid、bg、soft、rsize、wsize等参数，有书上说建议加rsize、wszie这两个参数</p>
<p>这个问题属于mount挂载优化内容（有些参数也适合其他文件系统），一般来说要适当加挂载参数，但是，最好先做测试，用数据来说话，才能更好的确定到底是挂载还是不挂载。</p>
<h5 id="有关安全挂载参数选项">有关安全挂载参数选项</h5>
<p>在工作环境，一般来说，NFS服务器共享的只是普通静态数据（图片、附件、视频）不需要执行suid、exec等权限。挂载的这个文件系统时能作为数据存取只用，无法执行程序，对于客户端来讲增加了安全性，例如：很多木马篡改文件都是由上传入口上传的程序倒存储目录然后执行的。
因此在挂载的时候用下面的命令很有必要</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">mount -t nfs -o noexec,nosuid,nodev,rw 10.0.0.7:/data /mnt
</span></span></code></pre></td></tr></table>
</div>
</div><p>注：通过mount -o指定挂载参数与/etc/fstab里指定挂载参数的效果是一样的。</p>
<blockquote>
<p><strong>挂载性能优化参数</strong></p>
</blockquote>
<p><strong>禁止更新目录及文件时间戳挂载</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">mount -t nfs -o noatime,nodiratime 10.0.0.7:/data /mnt
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>安全加优化的挂载方法如下</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">mount -t nfs -o nosuid,noexec,nodev,noatime,nodiratime,intr,rsize=131072,wsize=131072 10.0.0.7:/data /mnt
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><strong>NFS优缺点</strong></p>
</blockquote>
<p><strong>优点</strong>：</p>
<ul>
<li>简单易上手，容易掌握</li>
<li>数据可见</li>
<li>部署快速，维护简单，且可控，满足需求就是最好的</li>
<li>可靠，从软件层面上来看，数据可靠性高，经久耐用。数据实在文件系统之上的。</li>
<li>服务非常稳定</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>存在单点故障，服务器宕机，所有的客户端都不能访问共享目录</li>
<li>在大数据高并发的场合，NFS效率、性能有限（2000W/日以下的PV网站不是瓶颈。除非网站架构设计太差）</li>
<li>安全性一般等</li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
